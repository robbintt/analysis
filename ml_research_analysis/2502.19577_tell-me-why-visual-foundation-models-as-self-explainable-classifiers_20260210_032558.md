---
ver: rpa2
title: 'Tell me why: Visual foundation models as self-explainable classifiers'
arxiv_id: '2502.19577'
source_url: https://arxiv.org/abs/2502.19577
tags:
- prototypes
- prototype
- metrics
- interpretability
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ProtoFM, a novel approach to building self-explainable
  visual classifiers by combining frozen visual foundation models (VFMs) with a prototypical
  architecture. The key innovation is a lightweight head (~1M parameters) trained
  on top of frozen VFMs that achieves both competitive classification accuracy and
  superior interpretability.
---

# Tell me why: Visual foundation models as self-explainable classifiers

## Quick Facts
- arXiv ID: 2502.19577
- Source URL: https://arxiv.org/abs/2502.19577
- Reference count: 26
- Key outcome: ProtoFM achieves state-of-the-art interpretability (mean explainability score 0.92) while maintaining competitive classification accuracy on multiple datasets

## Executive Summary
ProtoFM is a novel approach that combines frozen visual foundation models with a prototypical architecture to create self-explainable image classifiers. The method uses a student-teacher training scheme with specialized losses to ensure consistent prototype assignments across image augmentations while maintaining strong local feature representations. By leveraging the spatial alignment capabilities of pretrained VFMs and constraining the classification head to use only positive weights, ProtoFM produces faithful explanations that decompose predictions into weighted sums of interpretable visual concepts. The model demonstrates competitive accuracy on benchmark datasets while significantly outperforming existing interpretable methods on comprehensive interpretability metrics.

## Method Summary
ProtoFM builds upon frozen visual foundation models (VFMs) like DINOv2 by adding a lightweight prototypical head (~1M parameters) that learns to assign image patches to interpretable prototypes. The architecture uses a student-teacher training scheme where both original and augmented images are processed in parallel, with the teacher branch updated via exponential moving average. Multiple specialized losses ensure consistent prototype assignments, prevent model collapse, and maintain discriminative power. The final classifier uses only positive weights, ensuring each class prediction is a non-negative combination of prototype activations, making the decision process inherently interpretable.

## Key Results
- Achieves mean explainability score of 0.92 on FunnyBirds benchmark, surpassing all baselines
- Maintains competitive classification accuracy (86.3% on CUB, 93.6% on CARS) while being fully interpretable
- Outperforms specialized interpretable models on correctness and contrastivity metrics while achieving superior overall interpretability
- Ablation studies show assignment loss and alignment loss are most critical for consistency and preventing prototype collapse

## Why This Works (Mechanism)

### Mechanism 1
Freezing the VFM backbone preserves spatial alignment capabilities critical for prototype localization. VFMs pretrained with patch-level objectives (e.g., DINOv2's dense prediction tasks) develop strong local representations where individual patch embeddings encode semantically meaningful spatial information. By freezing these weights and only training a lightweight projection head, ProtoFM inherits this spatial fidelity without disrupting it through downstream gradient updates.

### Mechanism 2
The student-teacher architecture with EMA enforces consistent prototype assignments across augmented views. Two parallel branches process original and augmented images. The teacher prototype weights are updated via exponential moving average (EMA) of student weights. By computing assignment masks for both views and enforcing consistency through the assignment loss, the model learns to assign the same patches to the same prototypes regardless of geometric/color transformations.

### Mechanism 3
The alignment loss prevents prototype collapse by anchoring assignments to backbone feature correspondences. The correspondence distillation loss compares intra-sample and inter-sample feature correlations from the frozen backbone with prototype assignment patterns. This ensures prototypes don't all collapse to a single pattern by requiring assignment structure to reflect the underlying feature geometry.

## Foundational Learning

- **Prototypical Part Networks**: Understanding ProtoPNet's class-specific prototypes vs. ProtoPool's shared prototypes clarifies why ProtoFM uses positive-weighted linear classification. Quick check: Can you explain why constraining classification weights to be positive enhances interpretability?

- **Visual Foundation Models (VFMs) - DINOv2, CLIP, SAM**: ProtoFM's efficiency depends on leveraging frozen VFMs. Understanding what makes DINOv2's patch-level features suitable (self-supervised dense prediction) vs. CLIP's more global representations informs backbone selection. Quick check: Why might DINOv2 be preferred over CLIP for tasks requiring fine-grained spatial localization?

- **Co-12 Interpretability Properties**: The paper evaluates against correctness, completeness, contrastivity, consistency, stability, and compactness. Understanding these desiderata is essential for interpreting the benchmark results. Quick check: What is the difference between correctness (faithfulness) and consistency in the context of prototype explanations?

## Architecture Onboarding

- **Component map**: Frozen VFM Backbone (DINOv2-ViT-B/14) -> Projector z -> Prototype Bank P^s,t (N=300) -> Assignment Module -> Top-k Aggregation -> Classification Head (positive weights) -> Teacher Branch (EMA)

- **Critical path**: Image → VFM → patch embeddings → Projector → prototype-space embeddings → Cosine similarity → assignment masks → Top-k mean → prototype presence vector h → Positive-weighted linear layer → class scores

- **Design tradeoffs**: Prototype count (N) affects expressiveness vs compactness (paper uses 300). Backbone choice impacts spatial vs generalization performance. DINOv2 better for spatial tasks; CLIP better for generalization. Temperature τ controls assignment sharpness.

- **Failure signatures**: Prototype collapse (all patches to single prototype), semantic gap (inconsistent concepts), low SEC score (prototypes explain <50% of prediction). Monitor prototype diversity and consistency metrics.

- **First 3 experiments**: 1) Backbone ablation: Compare frozen DINOv2 vs CLIP vs trainable ResNet-50 on FunnyBirds accuracy and mX. 2) Loss component analysis: Train variants removing each loss term to replicate Table 3. 3) Prototype count scaling: Train with N∈{50, 150, 300, 500} and plot accuracy vs size metrics.

## Open Questions the Paper Calls Out

1. **Textual grounding**: Can textual descriptions be effectively incorporated into ProtoFM to provide semantic grounding for the learned visual prototypes? Current model relies on visual exemplars which may not fully clarify abstract semantic meaning.

2. **Clinical workflow impact**: Does ProtoFM improve diagnostic decision-making compared to black-box baselines in clinical settings? Current evaluation uses proxy metrics rather than human-in-the-loop performance or user trust studies.

3. **Consistency optimization**: Is it possible to improve ProtoFM's prototype consistency to match specialized models like PIP-Net without compromising correctness and contrastivity? Current trade-off shows high consistency models often fail at highlighting discriminative features.

## Limitations

- **Domain dependency**: Interpretability heavily relies on frozen VFM backbone spatial capabilities, which may degrade on domains where pretrained VFMs lack relevant spatial priors (medical imaging, satellite imagery).

- **Hyperparameter sensitivity**: Model depends on multiple tuned loss weights and architectural choices without systematic sensitivity analysis across diverse domains.

- **Evaluation specificity**: While FunnyBirds provides comprehensive metrics, results may not generalize beyond the five tested datasets without cross-domain validation.

## Confidence

- **High Confidence**: Classification accuracy claims (86.3% on CUB, 93.6% on CARS) - standard metrics with clear baselines
- **Medium Confidence**: Interpretability scores on FunnyBirds - novel metric set with limited external validation
- **Low Confidence**: Generalization claims across domains - insufficient cross-dataset ablation studies

## Next Checks

1. **Domain transfer validation**: Train ProtoFM on medical imaging (RSNA) and satellite imagery to verify spatial alignment preservation outside natural images.

2. **Hyperparameter robustness**: Systematically vary prototype count (N=50-500) and loss weights (λ1-λ4) to identify Pareto frontiers for accuracy vs. interpretability trade-offs.

3. **Comparison with explainable fine-tuning**: Compare ProtoFM against fine-tuned VFM approaches using attention visualization or Grad-CAM to isolate the benefit of prototypical architecture versus VFM freezing.