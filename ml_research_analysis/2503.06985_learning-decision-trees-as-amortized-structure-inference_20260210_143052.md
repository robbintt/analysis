---
ver: rpa2
title: Learning Decision Trees as Amortized Structure Inference
arxiv_id: '2503.06985'
source_url: https://arxiv.org/abs/2503.06985
tags:
- decision
- learning
- trees
- tree
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of learning decision trees for
  tabular data, which has an intractably large search space. Existing methods rely
  on greedy heuristics, while deep learning approaches struggle due to the lack of
  natural structure in tabular data.
---

# Learning Decision Trees as Amortized Structure Inference
## Quick Facts
- arXiv ID: 2503.06985
- Source URL: https://arxiv.org/abs/2503.06985
- Authors: Mohammed Mahfoud; Ghait Boukachab; MichaÅ‚ Koziarski; Alex Hernandez-Garcia; Stefan Bauer; Yoshua Bengio; Nikolay Malkin
- Reference count: 40
- Primary result: DT-GFN outperforms state-of-the-art decision tree and deep learning methods on classification benchmarks, robustness to distribution shifts, and anomaly detection

## Executive Summary
This paper addresses the fundamental challenge of learning decision trees for tabular data, which has an intractably large search space. Traditional approaches rely on greedy heuristics while deep learning methods struggle due to the lack of natural structure in tabular data. The authors propose DT-GFN, a hybrid amortized structure inference approach that formulates decision tree construction as a sequential planning problem. By training a deep reinforcement learning policy (GFlowNet) to sample decision trees from the Bayesian posterior, DT-GFN yields interpretable models with shorter description lengths.

The key innovation lies in treating decision tree construction as a planning problem rather than an optimization problem. This approach enables the model to explore the vast space of possible tree structures more effectively than greedy methods, while maintaining interpretability through the discrete tree structure. The GFlowNet policy learns to sample trees that balance accuracy with model complexity, resulting in models that are both accurate and compact.

## Method Summary
DT-GFN formulates decision tree construction as a sequential planning problem, where each step involves selecting a split based on the current node's data distribution. A GFlowNet policy is trained to sample decision trees from the Bayesian posterior, which represents the distribution of all possible trees weighted by their likelihood given the data. The policy is trained using a combination of supervised learning (from ground truth trees) and reinforcement learning (to explore the space of possible trees). During inference, the trained policy samples multiple trees, which are then combined through ensemble methods to produce final predictions.

## Key Results
- DT-GFN outperforms state-of-the-art decision tree and deep learning methods on standard classification benchmarks
- The approach demonstrates superior robustness to distribution shifts compared to baseline methods
- DT-GFN shows consistent scaling behavior, where larger ensembles lead to better ensemble prediction and systematic generalization
- The learned models have shorter description lengths, indicating better balance between accuracy and complexity

## Why This Works (Mechanism)
The mechanism behind DT-GFN's success lies in its ability to amortize the expensive structure search across multiple inference tasks. By training a GFlowNet policy to sample trees directly from the posterior distribution, the approach avoids the local optima problem that plagues greedy methods. The policy learns to navigate the tree space efficiently, balancing exploration of novel structures with exploitation of high-performing regions. This amortized inference approach enables faster and more effective tree construction during inference time.

## Foundational Learning
- **Bayesian inference**: Understanding how to compute and sample from posterior distributions is crucial for DT-GFN's approach to tree selection
- **GFlowNets**: Knowledge of flow-based generative models is needed to understand how the policy learns to sample valid tree structures
- **Sequential decision making**: The tree construction process is framed as a sequential decision problem, requiring understanding of MDP/POMDP concepts
- **Ensemble methods**: DT-GFN combines multiple sampled trees, so familiarity with bagging, boosting, and other ensemble techniques is important
- **Information theory**: Concepts like description length and entropy are used to evaluate model complexity and generalization

## Architecture Onboarding
**Component Map**: Data -> Preprocessor -> GFlowNet Policy -> Tree Sampler -> Ensemble Predictor
**Critical Path**: The core inference path involves data flowing through the GFlowNet policy to generate tree structures, which are then evaluated and combined in the ensemble
**Design Tradeoffs**: The approach trades computational efficiency during training (GFlowNet training) for efficiency during inference (fast tree sampling), versus traditional methods that are fast to train but may get stuck in local optima
**Failure Signatures**: Poor performance might manifest as the GFlowNet policy collapsing to a limited set of tree structures, or the ensemble failing to improve with additional trees
**Three First Experiments**:
1. Train DT-GFN on a simple binary classification dataset and visualize the sampled tree structures
2. Compare ensemble performance as a function of ensemble size (1, 4, 8, 16 trees)
3. Evaluate the impact of different GFlowNet architectures (e.g., varying depth and width) on final performance

## Open Questions the Paper Calls Out
None

## Limitations
- The approach requires computationally intensive training of GFlowNet policies for each dataset
- Analysis focuses on relatively small ensemble sizes (up to 16 trees), leaving uncertainty about performance at larger scales
- Claims about systematic generalization require more extensive testing across different domain shifts

## Confidence
**High confidence**: The core methodology of using GFlowNets for amortized decision tree sampling is technically sound and well-validated through comparisons with baseline methods.

**Medium confidence**: Claims about improved interpretability and robustness to distribution shifts are supported by the presented experiments, though additional validation on more diverse datasets would strengthen these findings.

**Low confidence**: The claim about systematic generalization requires more extensive testing across different domain shifts and out-of-distribution scenarios to be fully substantiated.

## Next Checks
1. Evaluate DT-GFN's performance on larger ensemble sizes (e.g., 32-64 trees) to verify consistent scaling behavior.
2. Test the approach on more diverse tabular datasets, particularly those with mixed feature types and larger feature spaces.
3. Conduct ablation studies to quantify the contribution of different GFlowNet architectural choices and training procedures to the final performance.