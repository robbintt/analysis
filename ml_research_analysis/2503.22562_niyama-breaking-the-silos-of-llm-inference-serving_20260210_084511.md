---
ver: rpa2
title: 'Niyama : Breaking the Silos of LLM Inference Serving'
arxiv_id: '2503.22562'
source_url: https://arxiv.org/abs/2503.22562
tags:
- requests
- niyama
- load
- latency
- request
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Niyama introduces a QoS-aware LLM inference serving system that
  addresses inefficiencies in current siloed deployments by enabling fine-grained
  workload co-scheduling on shared infrastructure. It dynamically adapts scheduling
  decisions using three key innovations: dynamic chunking that opportunistically increases
  throughput by exploiting deadline slack in non-interactive requests, hybrid prioritization
  that smoothly interpolates between EDF and SRPF to balance deadline adherence and
  fairness across load conditions, and eager relegation that selectively degrades
  service for low-priority requests during overload to maintain QoS for critical workloads.'
---

# Niyama : Breaking the Silos of LLM Inference Serving

## Quick Facts
- **arXiv ID:** 2503.22562
- **Source URL:** https://arxiv.org/abs/2503.22562
- **Reference count:** 32
- **Primary result:** 32% GPU reduction vs. siloed deployments while maintaining QoS

## Executive Summary
Niyama introduces a QoS-aware LLM inference serving system that addresses inefficiencies in current siloed deployments by enabling fine-grained workload co-scheduling on shared infrastructure. It dynamically adapts scheduling decisions using three key innovations: dynamic chunking that opportunistically increases throughput by exploiting deadline slack in non-interactive requests, hybrid prioritization that smoothly interpolates between EDF and SRPF to balance deadline adherence and fairness across load conditions, and eager relegation that selectively degrades service for low-priority requests during overload to maintain QoS for critical workloads. The system reduces GPU requirements by 32% compared to state-of-the-art siloed deployments while maintaining QoS guarantees. Under extreme load conditions, Niyama reduces SLO violations by an order of magnitude compared to current strategies, achieving zero deadline violations for high-priority requests while gracefully degrading service for the remaining workload.

## Method Summary
Niyama extends Sarathi/vLLM with three key innovations: (1) dynamic chunking that computes safe chunk sizes by exploiting deadline slack in non-interactive requests, (2) hybrid prioritization that interpolates between EDF and SRPF based on load conditions, and (3) eager relegation that proactively moves requests likely to miss deadlines to a separate queue. The system uses a random forest predictor trained on Vidur simulator profiles to estimate chunk sizes. It was evaluated on ShareGPT, Azure Conv, and Azure Code datasets with three QoS tiers (interactive: TTFT=6s, TBT=50ms; non-interactive: TTLT=600s/1800s) across varying QPS loads on Llama3-8B and Qwen-7B models.

## Key Results
- 32% reduction in GPU requirements compared to state-of-the-art siloed deployments
- Order-of-magnitude reduction in SLO violations under extreme load conditions
- Zero deadline violations for high-priority requests during overload scenarios
- Graceful degradation of service for remaining workload while protecting critical traffic

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Opportunistically increasing chunk sizes during non-interactive "deadline slack" periods may recover throughput without violating latency SLOs.
- **Mechanism:** The system calculates "slack" as the difference between a running request's next token deadline and the current time. If interactive requests have accumulated slack, the scheduler dynamically increases the prefill chunk size for co-scheduled requests, processing more tokens per iteration.
- **Core assumption:** LLM prefill execution time is sufficiently predictable to safely utilize slack without overshooting deadlines.
- **Evidence anchors:**
  - [abstract] "...dynamic chunking mechanism to improve overall throughput while maintaining strict QoS guarantees."
  - [section 3.3] "Niyama employs dynamic chunking to opportunistically maximize the chunk size... by exploiting any available slack."
  - [corpus] AccelGen and AdaServe similarly target heterogeneous SLOs, suggesting chunk adaptation is a converging solution direction.
- **Break condition:** If prediction error is high, aggressive chunking will violate Time Between Tokens (TBT) SLOs for interactive users.

### Mechanism 2
- **Claim:** A hybrid scheduling policy interpolating between Earliest Deadline First (EDF) and Shortest Remaining Prompt First (SRPF) appears necessary to handle load variance.
- **Mechanism:** At low load, the scheduler favors EDF (deadline adherence). As load increases, it shifts weight toward SRPF (processing shorter jobs to clear queue). This is modeled via a linear combination of deadline and remaining prefill tokens.
- **Core assumption:** Request length correlates with processing time sufficiently well that prioritizing short requests alleviates queue pressure.
- **Evidence anchors:**
  - [abstract] "...hybrid prioritization policy that balances fairness and efficiency..."
  - [section 3.4] "Niyama smoothly interpolates between EDF and SRPF to compute the priority of a request."
  - [corpus] Corpus papers focus largely on colocation/speculative decoding; specific hybrid EDF/SRPF interpolation is unique to this text.
- **Break condition:** Under extreme overload where even short jobs exceed capacity, this hybrid policy degrades similarly to standard SRPF (starving long requests).

### Mechanism 3
- **Claim:** Proactively isolating requests likely to miss deadlines ("eager relegation") stabilizes the system for high-priority traffic.
- **Mechanism:** A "violation checker" estimates if a request will miss its deadline in the current iteration. If so, it is moved to a "relegated queue" rather than stalling the main queue. This sacrifices specific requests to prevent cascading head-of-line blocking.
- **Core assumption:** The system can accurately identify "doomed" requests before they consume critical GPU cycles.
- **Evidence anchors:**
  - [abstract] "...selectively degrades service for low-priority requests during overload to maintain QoS..."
  - [section 3.4] "...if a request has already violated its TTFT / TTLT deadline, or is about to violate it... Niyama de-prioritizes this request."
  - [corpus] Weak direct evidence in corpus for "eager relegation"; related work focuses on admission control rather than mid-flight relegation.
- **Break condition:** If the predictor is overly aggressive in relegation, valid requests are unnecessarily degraded, reducing effective goodput.

## Foundational Learning

- **Concept:** Prefill vs. Decode Phases
  - **Why needed here:** Niyama relies on the distinct compute-bound (Prefill) and memory-bound (Decode) characteristics of LLMs to calculate slack and chunk sizes.
  - **Quick check question:** Can you explain why the prefill phase is more amenable to chunking than the decode phase?

- **Concept:** Chunked Prefills (Sarathi)
  - **Why needed here:** This paper builds directly upon Sarathi's chunking mechanism rather than standard monolithic batching.
  - **Quick check question:** How does splitting a prefill request into chunks prevent stalling ongoing decode operations?

- **Concept:** Time to First Token (TTFT) vs. Time Between Tokens (TBT)
  - **Why needed here:** Niyama optimizes for different metrics based on QoS class (Interactive vs. Non-interactive).
  - **Quick check question:** Which metric is critical for a chatbot user experience, and which for a batch summarization job?

## Architecture Onboarding

- **Component map:** Prefill Queue -> Hybrid Selector -> Violation Checker -> (Main Queue or Relegated Queue) -> Dynamic Chunking Predictor
- **Critical path:** The scheduling loop where the **Selector** picks a request → **Checker** validates viability → **Chunker** sizes the batch. A bug in the predictor here causes either SLO violations or throughput collapse.
- **Design tradeoffs:**
  - **Alpha (α) Parameter:** Setting α too high prioritizes short jobs (SRPF), risking starvation for long requests; too low risks overload collapse (EDF behavior).
  - **Relegation Threshold:** Aggressive relegation protects high-priority SLAs but increases "error" rate for potentially viable requests.
- **Failure signatures:**
  - **Head-of-Line Blocking:** If dynamic chunking fails to pause for decodes, TBT spikes.
  - **Cascading Violations:** If eager relegation is disabled, a single long request can delay all subsequent requests, causing a latency spike that persists long after the request finishes.
- **First 3 experiments:**
  1. **Baseline Comparison:** Run the `ShareGPT` dataset with fixed vs. dynamic chunking to isolate the throughput gains from slack exploitation.
  2. **Load Stress Test:** Increase QPS on a single GPU replica to find the "knee" where Hybrid Prioritization outperforms pure EDF.
  3. **Relegation Ablation:** Disable the violation checker under 150% load to observe the "cascading violation" effect described in Figure 5.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the hybrid prioritization parameter α be dynamically adjusted in real-time to automatically balance fairness and deadline adherence as load conditions fluctuate?
- Basis in paper: [inferred] The paper notes the importance of tuning α as load increases (Figure 12) but describes it as a configurable parameter or deployment setting rather than an automated control variable.
- Why unresolved: The paper demonstrates the sensitivity of performance to α but leaves the mechanism for adapting it in a live production environment as an open implementation detail.
- What evidence would resolve it: An adaptive control algorithm that adjusts α based on real-time metrics (e.g., queue length, violation rates) and demonstrates stable performance across dynamic workloads without manual tuning.

### Open Question 2
- Question: Can the prioritization of non-interactive requests be improved by replacing the historical over-approximation of decode lengths with input-content-aware prediction models?
- Basis in paper: [inferred] Section 3.4 states that for non-interactive requests, the system uses a "simple prediction" based on historical mean plus two standard deviations.
- Why unresolved: Relying on historical statistics may result in inaccurate deadline estimates for specific requests with variable output lengths, potentially leading to sub-optimal scheduling.
- What evidence would resolve it: A comparative analysis showing that a model-based estimator (e.g., based on prompt complexity) reduces TTLT violations or improves goodput compared to the statistical heuristic.

### Open Question 3
- Question: Do the efficiency gains from dynamic chunking and co-scheduling persist when scaling to significantly larger models (e.g., 70B+ parameters) where memory bandwidth constraints differ?
- Basis in paper: [inferred] The evaluation in Section 4 is restricted to Llama3-8B and Qwen-7B models.
- Why unresolved: Larger models have different compute-to-memory ratios and KV-cache pressures, which may impact the effectiveness of the chunk size tuning and slack exploitation strategies.
- What evidence would resolve it: Evaluation results on 70B+ models (e.g., Llama-3-70B) demonstrating similar GPU reduction percentages and SLO maintenance as observed in the 8B experiments.

## Limitations

- The paper does not provide a breakdown of computational overhead for hybrid priority computation and violation checking under extreme load conditions
- Random forest predictor generalization to diverse LLM architectures beyond the tested Llama3-8B and Mixtral-8x7B models is not demonstrated
- The adaptive alpha mechanism lacks specific update functions or threshold values, making exact reproduction difficult

## Confidence

**High Confidence:**
- Dynamic chunking mechanism exploiting deadline slack is well-supported by experimental results showing 32% GPU reduction
- Basic architecture of combining Sarathi's chunking with QoS-aware scheduler is sound

**Medium Confidence:**
- Specific performance improvement values likely accurate for tested configurations but may not generalize to different models
- General approach of hybrid prioritization between EDF and SRPF is valid though specific adaptation strategy remains underspecified

**Low Confidence:**
- Long-term stability under sustained extreme overload conditions is not demonstrated
- Predictor's generalization to unseen models and exact overhead characteristics under maximum load are not fully characterized

## Next Checks

1. **Overhead Characterization:** Measure end-to-end latency overhead of Niyama scheduler components (priority computation, violation checking, relegation) under maximum load conditions (150% capacity) on production hardware, comparing against baseline Sarathi implementation.

2. **Predictor Generalization Test:** Train and evaluate random forest chunk size predictor on diverse LLM architectures beyond Llama3-8B and Mixtral-8x7B, then measure prediction accuracy and resulting SLO violation rates on these unseen models.

3. **Alpha Adaptation Validation:** Implement and test multiple alpha update strategies (threshold-based, proportional control, reinforcement learning) to determine which provides most stable performance across varying load conditions, validating chosen strategy prevents cascading violations under sustained overload.