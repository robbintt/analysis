---
ver: rpa2
title: 'WAON: Large-Scale and High-Quality Japanese Image-Text Pair Dataset for Vision-Language
  Models'
arxiv_id: '2510.22276'
source_url: https://arxiv.org/abs/2510.22276
tags:
- japanese
- image
- dataset
- waon
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces WAON, a large-scale (155M examples) and high-quality
  Japanese image-text pair dataset constructed from Common Crawl. The dataset is built
  using a pipeline incorporating language identification, image quality filtering,
  NSFW filtering, deduplication, and SigLIP score-based filtering to ensure data quality.
---

# WAON: Large-Scale and High-Quality Japanese Image-Text Pair Dataset for Vision-Language Models

## Quick Facts
- arXiv ID: 2510.22276
- Source URL: https://arxiv.org/abs/2510.22276
- Reference count: 0
- A large-scale Japanese image-text dataset (155M examples) with high-quality filtering and cultural benchmarks

## Executive Summary
This paper introduces WAON, a large-scale Japanese image-text pair dataset constructed from Common Crawl using a comprehensive filtering pipeline. The dataset contains 155 million examples and incorporates multiple quality control measures including language identification, image quality filtering, NSFW filtering, deduplication, and SigLIP score-based filtering. To evaluate its effectiveness, the authors created WAON-Bench, a manually curated Japanese cultural image classification benchmark with 374 classes. Experimental results demonstrate that fine-tuning SigLIP2 on WAON achieves state-of-the-art performance on Japanese cultural benchmarks more efficiently than using ReLAION, addressing the critical need for high-quality Japanese vision-language data.

## Method Summary
WAON was constructed using Common Crawl data with a multi-stage filtering pipeline. The process begins with language identification to ensure Japanese text, followed by image quality assessment and NSFW filtering to remove inappropriate content. Deduplication removes redundant pairs, and SigLIP score-based filtering ensures high-quality image-text alignment. The dataset is evaluated using WAON-Bench, a manually curated benchmark specifically designed for Japanese cultural image classification with 374 distinct classes. The effectiveness of WAON is validated through fine-tuning experiments comparing SigLIP2 performance when trained on WAON versus ReLAION.

## Key Results
- WAON contains 155 million high-quality Japanese image-text pairs constructed from Common Crawl
- Fine-tuning SigLIP2 on WAON achieves state-of-the-art results on Japanese cultural benchmarks
- WAON demonstrates more efficient performance gains compared to ReLAION for Japanese vision-language tasks

## Why This Works (Mechanism)
The effectiveness of WAON stems from its comprehensive filtering pipeline that ensures high data quality while maintaining scale. By incorporating multiple filtering stages (language identification, image quality, NSFW detection, deduplication, and SigLIP-based alignment scoring), the dataset achieves a balance between size and quality that previous Japanese vision-language datasets lack. The culturally-specific WAON-Bench benchmark provides a rigorous evaluation framework tailored to Japanese cultural content, allowing for accurate assessment of model performance on culturally-relevant tasks.

## Foundational Learning

### Japanese Vision-Language Model Training
**Why needed:** Japanese language presents unique challenges for vision-language models due to its complex writing system (kanji, hiragana, katakana) and cultural specificity
**Quick check:** Models trained on multilingual datasets often underperform on Japanese-specific tasks due to lack of cultural context

### SigLIP Score-Based Filtering
**Why needed:** Ensures high-quality image-text alignment by measuring semantic similarity between visual and textual content
**Quick check:** Low SigLIP scores indicate poor alignment between images and their textual descriptions

### Cultural Image Classification Benchmarks
**Why needed:** Standard vision benchmarks often lack cultural specificity required for evaluating Japanese cultural understanding
**Quick check:** 374-class benchmark provides granular evaluation of Japanese cultural knowledge

## Architecture Onboarding

### Component Map
Common Crawl Data -> Language Filter -> Image Quality Filter -> NSFW Filter -> Deduplication -> SigLIP Filter -> WAON Dataset

### Critical Path
The SigLIP score-based filtering represents the critical path for ensuring high-quality image-text alignment, as it directly impacts the dataset's effectiveness for training vision-language models.

### Design Tradeoffs
Scale vs. Quality: The multi-stage filtering pipeline prioritizes quality but may exclude some valid examples. Cultural Specificity: The 374-class benchmark provides detailed cultural evaluation but may not cover all Japanese cultural domains.

### Failure Signatures
- Low SigLIP scores indicate poor image-text alignment
- High false positive rates in NSFW filtering may unnecessarily reduce dataset size
- Language identification errors could include non-Japanese content

### First Experiments
1. Evaluate SigLIP score distribution across filtered examples to assess filtering effectiveness
2. Measure vocabulary coverage of Japanese text in the dataset
3. Compare performance on WAON-Bench across different filtering threshold levels

## Open Questions the Paper Calls Out
None identified in the provided materials.

## Limitations
- Dataset quality heavily depends on automated filtering pipeline effectiveness without human validation
- Cultural specificity of the 374-class benchmark may not comprehensively represent all Japanese cultural content
- Real-world performance on diverse Japanese vision-language tasks remains untested

## Confidence

- **High confidence**: Dataset size claim (155M examples) and dataset existence
- **Medium confidence**: High-quality designation based on filtering pipeline description
- **Medium confidence**: Efficiency improvement claims compared to ReLAION
- **Medium confidence**: State-of-the-art results on Japanese cultural benchmarks

## Next Checks

1. Conduct human evaluation on a stratified sample of 1,000 examples to verify data quality and filtering effectiveness
2. Test fine-tuning efficiency claims across multiple VLM architectures beyond SigLIP2
3. Evaluate model performance on real-world Japanese vision-language tasks beyond the curated WAON-Bench benchmark