---
ver: rpa2
title: 'Benign-to-Toxic Jailbreaking: Inducing Harmful Responses from Harmless Prompts'
arxiv_id: '2505.21556'
source_url: https://arxiv.org/abs/2505.21556
tags:
- masked
- toxicity
- toxic
- jailbreak
- clean
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Benign-to-Toxic (B2T) jailbreaking is proposed to induce harmful
  outputs from harmless prompts in large vision-language models (LVLMs). Unlike prior
  Toxic-Continuation methods that rely on already-toxic inputs, B2T optimizes adversarial
  images to force toxic outputs from benign conditioning, breaking safety alignment
  directly.
---

# Benign-to-Toxic Jailbreaking: Inducing Harmful Responses from Harmless Prompts

## Quick Facts
- arXiv ID: 2505.21556
- Source URL: https://arxiv.org/abs/2505.21556
- Reference count: 40
- Primary result: Benign-to-Toxic (B2T) jailbreaking induces toxic outputs from harmless prompts in large vision-language models, outperforming prior methods by 10–40 percentage points in attack success rate.

## Executive Summary
This paper introduces Benign-to-Toxic (B2T) jailbreaking, a novel attack paradigm that induces harmful responses from harmless prompts in large vision-language models (LVLMs). Unlike prior work that relies on toxic inputs, B2T optimizes adversarial images to force toxic outputs from benign conditioning text, directly breaking safety alignment. Experiments on five safety benchmarks with four LVLMs show B2T consistently outperforms prior approaches and demonstrates strong black-box transferability. The method also enhances text-based jailbreaks when combined with them, exposing a fundamental vulnerability in multimodal safety alignment.

## Method Summary
The B2T method optimizes a single adversarial image perturbation (bounded by ℓ∞-norm ε=32/255) to induce toxic outputs from benign text prompts. A mixed loss function probabilistically combines a Benign-to-Toxic loss (maximizing toxic token generation from safe conditioning) and a Toxic-Continuation loss (extending toxic contexts). The optimization uses Projected Gradient Descent over 4,000–5,000 steps, with a mixing parameter τ (0.1 or 0.2) controlling the balance. The adversarial image is universal and transferable across models and prompts, exploiting a fundamental weakness in LVLM safety alignment.

## Key Results
- B2T achieves 10–40 percentage point higher attack success rates than prior Toxic-Continuation methods across five safety benchmarks.
- The adversarial image generalizes across diverse prompts and transfers effectively to unseen LVLM architectures (up to 53.3% higher ASR in black-box settings).
- B2T enhances text-based jailbreaks when combined, further increasing their effectiveness.
- Input-level defenses like JPEG compression only partially mitigate the attacks, indicating robustness.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Optimizing adversarial images to map benign conditioning text to toxic target tokens forces the perturbation to directly break the model's safety alignment, without relying on any toxic signal in the prompt.
- Mechanism: The B2T loss maximizes the probability of generating a toxic token when conditioned on a benign phrase, compelling the adversarial image to subvert the model's refusal mechanisms.
- Core assumption: Prior jailbreak successes in the Toxic-Continuation setting are significantly aided by the presence of toxicity in the conditioning prompt, rather than solely by the adversarial image's ability to break alignment.
- Evidence anchors: The abstract and section text explicitly state that the image alone must break the model's safety mechanisms when conditioned on benign text. However, corpus neighbors do not explicitly validate this mechanism.
- Break condition: This mechanism's effectiveness will diminish if LVLMs are trained or fine-tuned with robust safety alignment that prevents toxic mapping from any non-toxic input, regardless of visual perturbations.

### Mechanism 2
- Claim: A combined loss function that probabilistically mixes B2T and Toxic-Continuation objectives achieves superior jailbreak performance by leveraging their complementary strengths.
- Mechanism: At each training step, a Bernoulli variable samples between the B2T loss (for breaking alignment from benign inputs) and the standard Toxic-Continuation loss (for generating fluent toxic text).
- Core assumption: Breaking safety alignment (B2T) and extending a toxic context fluently (Toxic-Continuation) are separable tasks that can be synergistically combined.
- Evidence anchors: Section text describes the mixing process and its purpose. However, corpus evidence is weak or missing.
- Break condition: An improper mixing ratio (τ) could fail, either by not applying enough B2T loss to break alignment or by degrading output fluency.

### Mechanism 3
- Claim: A single adversarial image optimized with the B2T objective can act as a universal jailbreak trigger, generalizing across diverse prompts and transferring in black-box settings to other LVLM architectures.
- Mechanism: The B2T objective learns a perturbation that targets a fundamental vulnerability in how LVLMs process multimodal inputs, rather than overfitting to a specific prompt or model's quirks.
- Core assumption: Current LVLMs share common, fundamental weaknesses in their multimodal alignment that can be exploited by a universal visual perturbation.
- Evidence anchors: The abstract and section text claim high black-box transferability. Corpus neighbors on cross-modal vulnerabilities lend indirect support.
- Break condition: The transferability will be reduced or eliminated if LVLMs adopt significantly different vision-language fusion architectures or implement more robust, modality-specific safety defenses.

## Foundational Learning

- **Concept: Safety Alignment in LVLMs**
  - Why needed here: The paper argues that current alignment techniques are vulnerable to B2T attacks. Understanding what alignment is (and where it resides in the model) is a prerequisite for diagnosing this failure.
  - Quick check question: Can you explain the goal of safety alignment (e.g., via RLHF) in Large Vision-Language Models and identify which model components are typically fine-tuned?

- **Concept: Autoregressive Next-Token Prediction**
  - Why needed here: Both the baseline and proposed jailbreak methods formulate their attacks as optimizing the loss from the model's standard next-token prediction task.
  - Quick check question: In an autoregressive LLM, how is the conditional probability for the next token calculated, and what role does the "conditioning prompt" play in this calculation?

- **Concept: Adversarial Perturbations (ℓ∞-norm Constraint)**
  - Why needed here: The B2T method optimizes a visual perturbation under a strict ℓ∞-norm bound (ε). This is a core technical detail for implementation and reproducibility.
  - Quick check question: What does an ℓ∞-norm constraint of ε = 32/255 mean for the maximum allowed change to an individual pixel in an image?

## Architecture Onboarding

- **Component map:** Vision Encoder (e.g., CLIP-ViT) -> Vision-Language Projector (e.g., Q-Former, linear layer) -> Large Language Model (e.g., Vicuna, LLaMA) -> Safety Alignment (Implicit in LLM weights)

- **Critical path:**
  1. **Data Prep:** Create pairs of benign conditioning text and toxic target tokens.
  2. **Forward Pass:** Pass a perturbed image (I + δ) and the benign conditioning text through the frozen LVLM.
  3. **Loss Calculation:** Compute the B2T loss (cross-entropy) between the model's predicted next tokens and the toxic targets, potentially mixed with a Toxic-Continuation loss.
  4. **Perturbation Update:** Use Projected Gradient Descent (PGD) to update the image perturbation δ based on the loss, then clip the update to ensure it remains within the ε-bound.

- **Design tradeoffs:**
  - **τ (B2T loss ratio):** A higher τ more aggressively breaks alignment but can produce less fluent text. A lower τ prioritizes fluent toxic continuation but may fail to initiate misalignment.
  - **ε (Perturbation budget):** A larger budget (e.g., 64/255) increases attack success but makes the perturbation more perceptible to humans. A smaller budget is stealthier but less effective.
  - **Universality:** Training a single universal image is highly efficient and generalizable but may not achieve the peak attack success rate of a prompt-specific perturbation.

- **Failure signatures:**
  - **Low ASR on benchmarks with benign prompts:** The perturbation is not effectively breaking alignment.
  - **High ASR but incoherent/gibberish outputs:** The B2T loss is too high (τ is too large), sacrificing fluency.
  - **Attack success collapses under JPEG compression:** The perturbation is overfitting to high-frequency noise that compression removes.

- **First 3 experiments:**
  1. **Baseline Reproduction:** Replicate the main result by training a B2T perturbation on LLaVA-1.5 with ε=32/255 and τ=0.2. Evaluate the Attack Success Rate (ASR) on AdvBench using the Detoxify evaluator.
  2. **τ Ablation Study:** Run an ablation by varying the mixing parameter τ from 0.0 to 0.5. Plot the ASR against an output fluency score (e.g., from GPT-4o) to empirically find the optimal trade-off.
  3. **Black-Box Transfer Test:** Train a perturbation on InstructBLIP and evaluate its ASR on MiniGPT-4 and LLaVA-1.5 (black-box targets) to test the transferability claim.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the Benign-to-Toxic (B2T) optimization paradigm be effectively adapted for discrete text-based jailbreaks?
  - Basis in paper: The authors explicitly state in Section 6: "Future work could explore B2T extensions to text, with attention to the challenges posed by its discrete modality."
  - Why unresolved: The current work optimizes continuous image perturbations; applying the B2T loss (predicting toxic tokens from benign prefixes) to discrete text tokens presents optimization challenges distinct from those solved by existing methods like GCG.
  - What evidence would resolve it: A successful gradient-based or discrete optimization algorithm that uses the B2T objective to generate adversarial text suffixes that reliably induce toxicity from benign prompts.

- **Open Question 2:** Would curriculum learning or adaptive scheduling of the mixing parameter $\tau$ improve the balance between attack success rate and output fluency?
  - Basis in paper: Section 6 suggests that "Strategies like curriculum learning or adaptive $\tau$ scheduling may help balance ASR and fluency."
  - Why unresolved: The current implementation uses a static $\tau$ (0.1 or 0.2) determined by ablation, which risks either insufficient misalignment (low $\tau$) or incoherent outputs (high $\tau$), leaving the optimal dynamic control policy undefined.
  - What evidence would resolve it: Experiments demonstrating that dynamic adjustment of $\tau$ during optimization yields perturbations that achieve higher toxicity scores while maintaining superior grammatical coherence compared to static baselines.

- **Open Question 3:** Can model-level defenses, such as adversarial training, effectively immunize Large Vision-Language Models (LVLMs) against B2T attacks?
  - Basis in paper: While the paper tests input-level defenses like JPEG compression (Appendix D), it does not evaluate model-level hardening. The authors describe the vulnerability as a "fundamental weakness" in multimodal alignment, implying a need to explore if standard safety training pipelines can be updated to resist B2T.
  - Why unresolved: The transferability of B2T suggests the perturbations exploit deep structural features of the embedding space; it is unknown if current fine-tuning protocols can close this specific "benign-to-toxic" gap without degrading standard capabilities.
  - What evidence would resolve it: A study evaluating the Attack Success Rate (ASR) of B2T perturbations on LVLMs fine-tuned with B2T-generated adversarial examples included in the safety training data.

## Limitations

- **Reproducibility of Results:** The paper lacks critical implementation details, such as the exact benign phrases and toxic targets used for training, which are essential for independent replication.
- **Generalizability of Universal Perturbation:** The evaluation focuses on a limited set of LVLMs (4 models), and the claim of a universal vulnerability needs validation across a broader range of architectures.
- **Safety Evaluator Reliability:** The paper relies on automated safety evaluators whose reliability and potential biases are not discussed, which could affect the reported attack success rates.

## Confidence

- **High Confidence:** The core technical contribution of the B2T paradigm and the ablation study on τ are clearly defined and well-executed.
- **Medium Confidence:** Claims about superior performance and black-box transferability are supported by experiments but are limited by unknown reproducibility factors and a small sample of target models.
- **Low Confidence:** The discussion of a "fundamental vulnerability" in multimodal safety alignment is speculative, as the evaluation is on a limited set of models and the underlying mechanism is not fully explored.

## Next Checks

1. **Implement and Validate the B2T Loss:** Recreate the Benign-to-Toxic (B2T) loss calculation in a controlled setting. Given a benign prompt and a toxic target word, verify that the optimization process can indeed increase the probability of the toxic token being generated. This is the foundational mechanism of the paper.

2. **Expand Black-Box Transferability Testing:** Evaluate the transferability of a B2T-learned perturbation beyond the four LVLMs tested in the paper. Specifically, test on models with different architectural designs for vision-language fusion (e.g., models using different attention mechanisms or projector layers) to assess the claim of a "universal" vulnerability.

3. **Analyze and Visualize the Perturbation:** Generate and analyze the final adversarial perturbation. Visualize the perturbation (as done in the paper's supplement) and perform ablation studies to identify which visual features are most critical for the attack. This will help validate the proposed mechanism and identify potential defenses.