---
ver: rpa2
title: 'CAD2DMD-SET: Synthetic Generation Tool of Digital Measurement Device CAD Model
  Datasets for fine-tuning Large Vision-Language Models'
arxiv_id: '2508.21732'
source_url: https://arxiv.org/abs/2508.21732
tags:
- image
- images
- dataset
- object
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces CAD2DMD-SET, a synthetic data generation tool
  designed to improve Large Vision-Language Models' (LVLMs) ability to read digital
  measurement devices (DMDs) in challenging real-world conditions. By leveraging 3D
  CAD models, advanced rendering, and image composition, the tool generates diverse,
  VQA-labeled synthetic DMD datasets.
---

# CAD2DMD-SET: Synthetic Generation Tool of Digital Measurement Device CAD Model Datasets for fine-tuning Large Vision-Language Models

## Quick Facts
- **arXiv ID:** 2508.21732
- **Source URL:** https://arxiv.org/abs/2508.21732
- **Reference count:** 40
- **Primary result:** CAD2DMD-SET improves LVLM performance on DMD tasks by 200% ANLS without degrading other task performance

## Executive Summary
This paper introduces CAD2DMD-SET, a synthetic data generation tool that addresses a critical limitation in Large Vision-Language Models (LVLMs): their inability to reliably read digital measurement devices (DMDs) in real-world conditions. By leveraging 3D CAD models and advanced rendering techniques, the tool generates diverse, VQA-labeled synthetic datasets that significantly improve LVLM performance on DMD recognition tasks. The authors validated their approach using a real-world benchmark (DMDBench) of 1,000 annotated images, demonstrating that fine-tuning InternVL with CAD2DMD-SET data improved ANLS scores by 200% without compromising performance on other tasks.

## Method Summary
CAD2DMD-SET generates synthetic datasets by compositing rendered 3D CAD models of digital measurement devices onto realistic backgrounds. The tool creates variations in device state (different readings), lighting conditions, and environmental contexts to produce diverse training data. Each synthetic image is automatically labeled with Visual Question Answering (VQA) format responses, enabling direct fine-tuning of LVLMs. The generated datasets are designed to address the gap between controlled training data and challenging real-world scenarios where DMDs appear in varied lighting, angles, and contexts.

## Key Results
- InternVL fine-tuned with CAD2DMD-SET data achieved a 200% increase in Average Normalized Levenshtein Similarity (ANLS) score
- Performance improvements were demonstrated on a real-world validation set (DMDBench) with 1,000 annotated images
- Fine-tuning did not degrade performance on other LVLM tasks, maintaining general capability

## Why This Works (Mechanism)
The tool works by addressing the data scarcity problem for DMD-related tasks in LVLMs. By synthetically generating diverse training examples that include various device states, lighting conditions, and backgrounds, the model learns robust representations that generalize to real-world scenarios. The VQA labeling format ensures the fine-tuning process aligns with how LVLMs process and respond to visual questions, making the adaptation process efficient and effective.

## Foundational Learning
- **3D CAD modeling and rendering**: Needed to create realistic device representations from different angles and states; Quick check: Can generate consistent device images across multiple viewpoints
- **Image composition techniques**: Required to seamlessly integrate rendered devices into realistic backgrounds; Quick check: Composited images appear natural without obvious artifacts
- **VQA format labeling**: Essential for LVLM fine-tuning compatibility; Quick check: Generated labels match expected answer formats for LVLM training
- **Synthetic data diversity**: Critical for preventing overfitting to specific conditions; Quick check: Generated dataset covers sufficient variation in lighting, angles, and contexts

## Architecture Onboarding
**Component Map:** CAD Models -> Rendering Engine -> Image Composition -> VQA Labeling -> LVLM Fine-tuning
**Critical Path:** The pipeline follows a sequential flow where 3D CAD models are rendered, composited onto backgrounds, labeled with VQA responses, and then used for LVLM fine-tuning
**Design Tradeoffs:** Prioritizes diversity and realism over perfect photorealism to maximize generalization, accepting some synthetic artifacts in exchange for broader coverage of real-world conditions
**Failure Signatures:** Models may fail on devices with novel form factors not represented in the synthetic data, or in extreme lighting conditions beyond the diversity range of generated examples
**First Experiments:** 1) Generate a small synthetic dataset and test LVLM performance on a limited DMD task set; 2) Compare fine-tuning with synthetic vs real data on a validation set; 3) Test model robustness across different lighting and angle variations

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Generalizability to other DMD types and environmental conditions beyond those tested remains unverified
- Computational cost and efficiency for large-scale deployment are not discussed
- Lack of extensive ablation studies to isolate the impact of individual pipeline components

## Confidence
- **Major claim (200% ANLS improvement):** High confidence, supported by substantial quantitative results
- **Tool applicability to other LVLM tasks:** Medium confidence, validation focused primarily on specific DMD use case
- **No degradation on other tasks:** Medium confidence, supported by results but would benefit from broader benchmark testing

## Next Checks
1. Test CAD2DMD-SET on a broader range of digital measurement devices and environmental conditions to assess generalizability
2. Conduct ablation studies to determine the contribution of each synthetic data generation component to performance improvements
3. Evaluate computational efficiency and scalability of the tool for large-scale real-world deployment scenarios