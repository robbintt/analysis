---
ver: rpa2
title: Local-to-Global Logical Explanations for Deep Vision Models
arxiv_id: '2601.13404'
source_url: https://arxiv.org/abs/2601.13404
tags:
- explanations
- explanation
- image
- class
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the need for interpretable explanations of
  deep vision models by introducing a neurosymbolic framework that generates human-understandable
  concept-level explanations. The method produces local explanations for individual
  images and global explanations for entire classes using logical formulas in monotone
  disjunctive normal form (MDNF), where each clause represents minimally sufficient
  combinations of annotated objects or parts.
---

# Local-to-Global Logical Explanations for Deep Vision Models

## Quick Facts
- arXiv ID: 2601.13404
- Source URL: https://arxiv.org/abs/2601.13404
- Reference count: 27
- Introduces neurosymbolic framework generating human-understandable concept-level explanations for deep vision models

## Executive Summary
This work addresses the need for interpretable explanations of deep vision models by introducing a neurosymbolic framework that generates human-understandable concept-level explanations. The method produces local explanations for individual images and global explanations for entire classes using logical formulas in monotone disjunctive normal form (MDNF), where each clause represents minimally sufficient combinations of annotated objects or parts. The approach was evaluated on ADE20K and Pascal-Parts datasets with VGG-19 and ViT-B architectures, demonstrating that the generated explanations maintain high fidelity while being significantly more interpretable than traditional saliency-based methods.

## Method Summary
The framework generates explanations through a two-stage process: local explanations for individual images are computed via beam search that finds minimal object sets sufficient for classification, while global explanations are generated using greedy set cover algorithms to create class-level logical formulas. The local explanations identify the minimal set of objects or parts that are sufficient for a model's classification decision, expressed as MDNF clauses. These local explanations are then aggregated into global explanations that characterize entire classes, with each clause representing a minimally sufficient combination of annotated objects. The approach was evaluated on ADE20K and Pascal-Parts datasets using VGG-19 and ViT-B architectures, measuring both explanation coverage and accuracy.

## Key Results
- Achieved coverage of approximately 85% of validation decisions with only 20 clauses
- Multi-class explanation list accuracy of 75.69% for ViT-B on ADE20K dataset
- Generated explanations are significantly more interpretable than traditional saliency-based methods while maintaining high fidelity

## Why This Works (Mechanism)
The method works by decomposing complex classification decisions into interpretable logical combinations of objects and parts. By using annotated segmentation masks as ground truth concepts, the framework can identify which specific objects or object parts are minimally sufficient for classification decisions. The beam search algorithm efficiently finds these minimal sufficient sets for individual images, while the greedy set cover algorithm aggregates these local explanations into global class-level patterns. The use of monotone disjunctive normal form ensures that explanations are both expressive enough to capture complex decision boundaries and simple enough to be human-understandable.

## Foundational Learning
- **Monotone Disjunctive Normal Form (MDNF)**: A logical representation where each clause is a conjunction of positive literals (object presence) combined by disjunction - needed to express sufficient conditions without negations for interpretability
- **Beam Search**: Heuristic search algorithm that explores the most promising paths - needed to efficiently find minimal sufficient object sets for local explanations
- **Greedy Set Cover**: Approximation algorithm for finding minimal cover sets - needed to aggregate local explanations into global class-level patterns
- **Annotated Segmentation Masks**: Ground truth labels indicating object and part locations - needed as the concept vocabulary for explanation generation
- **Fidelity Metrics**: Quantitative measures of how well explanations match model decisions - needed to validate that explanations accurately represent model behavior
- **Coverage Metrics**: Measures of how many model decisions can be explained - needed to assess the comprehensiveness of the explanation framework

## Architecture Onboarding

**Component Map:**
Annotated Images -> Beam Search (Local Explanations) -> MDNF Clauses -> Greedy Set Cover (Global Explanations) -> Class-Level Explanations

**Critical Path:**
Image segmentation masks → Object/part extraction → Beam search for minimal sufficient sets → MDNF clause generation → Greedy set cover aggregation → Final explanations

**Design Tradeoffs:**
- Accuracy vs interpretability: More complex logical formulas could capture more decisions but reduce human understanding
- Annotation cost vs explanation quality: Detailed segmentation masks enable precise explanations but require significant labeling effort
- Computational cost vs completeness: Beam search with larger widths finds better explanations but increases runtime
- Local vs global coverage: More local explanations improve global coverage but increase explanation complexity

**Failure Signatures:**
- Low coverage indicates explanations miss significant portions of model decisions
- Poor fidelity suggests explanations don't accurately reflect model behavior
- Overly complex clauses indicate failure to find truly minimal sufficient sets
- Inconsistent global explanations suggest local explanation errors propagating upward

**First 3 Experiments:**
1. Generate local explanations for a diverse set of validation images and verify minimal sufficiency through ablation studies
2. Aggregate local explanations into global class patterns and measure coverage on held-out validation data
3. Compare explanation fidelity and interpretability against baseline saliency-based methods using quantitative metrics

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on annotated object and part segmentation masks presents practical constraints and limits real-world applicability
- Performance may degrade on complex scenes with occlusions or unusual object configurations due to dependence on explicit object presence
- Evaluation focuses on classification accuracy of explanations rather than human studies to validate actual interpretability
- Computational cost of beam search and greedy set cover algorithms may become prohibitive for larger datasets or more complex models

## Confidence

**High confidence:**
- Local explanation generation via beam search is technically sound with clear algorithmic definitions
- Global explanation generation via greedy set cover is well-established and appropriate for this application
- Quantitative evaluation metrics are properly defined and computed

**Medium confidence:**
- Claims of improved interpretability are supported by quantitative metrics but lack human-centered validation studies
- Comparison with baseline methods demonstrates superiority on measured metrics, though baselines may not represent the full spectrum of explanation methods
- The relationship between annotation quality and explanation quality could be more thoroughly explored

## Next Checks

1. Conduct user studies with domain experts to validate whether the logical formulas genuinely improve human understanding compared to traditional saliency maps
2. Test the framework on more challenging datasets with complex scenes, severe occlusions, and unusual object configurations to assess robustness limits
3. Evaluate computational scalability by measuring runtime performance on larger datasets and more complex model architectures, particularly for the beam search component