---
ver: rpa2
title: 'FusionBench: A Unified Library and Comprehensive Benchmark for Deep Model
  Fusion'
arxiv_id: '2406.03280'
source_url: https://arxiv.org/abs/2406.03280
tags:
- fusion
- tasks
- merging
- performance
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'FusionBench is the first comprehensive benchmark and unified library
  for deep model fusion, addressing inconsistent and inadequate evaluations of model
  fusion techniques. It provides a modular framework with three primary components:
  Algorithm Module, Model Pool Module, and Task Pool Module, supporting easy implementation
  and testing of new fusion techniques across computer vision and natural language
  processing domains.'
---

# FusionBench: A Unified Library and Comprehensive Benchmark for Deep Model Fusion

## Quick Facts
- arXiv ID: 2406.03280
- Source URL: https://arxiv.org/abs/2406.03280
- Reference count: 38
- Primary result: First comprehensive benchmark for deep model fusion, showing model mixing methods (WEMoE, SMILE) achieve 89.2-89.3% average accuracy on CLIP-ViT-B/32 across 8 tasks

## Executive Summary
FusionBench addresses the lack of standardized evaluation in deep model fusion by providing a comprehensive benchmark and unified library. The framework supports three core fusion categories—ensemble, merging, and mixing—across computer vision and natural language processing tasks. Experimental results demonstrate that model mixing approaches consistently outperform parameter averaging and task arithmetic, achieving near-multi-task learning performance while maintaining inference efficiency. The benchmark reveals both the potential and limitations of current fusion techniques, including sensitivity to distribution shifts and challenges with negative transfer on unseen tasks.

## Method Summary
FusionBench implements a modular framework with three primary components: Algorithm Module (fusion implementations), Model Pool Module (model management), and Task Pool Module (evaluation datasets). The library uses Hydra-based YAML configuration for experiment control and supports lazy loading to manage memory for large models. Methods are organized into ensemble (model averaging), merging (parameter-space combination), and mixing (MoE-based approaches) categories. Evaluation spans CLIP-ViT, ResNet-50, GPT-2, and Flan-T5 models fine-tuned on diverse tasks including SUN397, Cars, MNIST, and GLUE benchmarks. The benchmark emphasizes reproducible evaluation through standardized metrics and dataset configurations.

## Key Results
- Model mixing methods (WEMoE, SMILE) consistently achieve the best performance with 89.2-89.3% average accuracy on CLIP-ViT-B/32
- Layer-wise adaptive merging (AdaMerging) significantly outperforms task-wise variants, showing 82.6% vs 68.7% on ViT-B/32
- Simple parameter averaging achieves 66.5% average accuracy, validating the effectiveness of naive fusion approaches
- Traditional multi-task learning still outperforms most fusion methods, indicating room for improvement
- All methods show significant performance degradation under corruption (pixelation drops WEMoE to 28.0% average accuracy)

## Why This Works (Mechanism)

### Mechanism 1
Task vectors from independently fine-tuned models are nearly orthogonal, enabling effective knowledge combination with minimal interference. Task vectors (τ = θ_ft − θ_pre) capture task-specific modifications to pre-trained weights. When these vectors occupy distinct subspaces, merging operations can preserve task-specific knowledge without destructive interference between tasks. Core assumption: The pre-trained initialization provides a shared representation that task-specific modifications build upon orthogonally. Evidence: Task vectors are nearly orthogonal (section D.1), simple averaging achieves 66.5% average accuracy (section E.3.1). Break condition: When tasks share significant parameter-space overlap, orthogonality assumption fails.

### Mechanism 2
Layer-wise adaptive merging outperforms task-wise or uniform merging strategies by accounting for heterogeneous parameter importance across layers. Different layers encode different levels of abstraction; early layers capture general features while later layers encode task-specific representations. Layer-wise AdaMerging learns separate merging coefficients per layer through test-time adaptation, allowing appropriate weighting of task-specific contributions. Core assumption: The optimal fusion strategy varies across layers and can be approximated from test-time data. Evidence: Layer-wise AdaMerging achieves 82.6% vs task-wise's 68.7% (section E.3.1). Break condition: When test data is severely out-of-distribution, adaptive methods may overfit.

### Mechanism 3
Model mixing methods (MoE-based approaches) achieve superior performance by constructing heterogeneous architectures with expanded parameter capacity. Unlike parameter averaging which constrains the merged model to the original parameter space, MoE-based methods construct routing mechanisms that select from multiple expert subnetworks derived from input models. This increases model capacity while maintaining per-task specialization. Core assumption: The computational overhead of expanded models is acceptable, and routing functions can be learned effectively with limited data. Evidence: WEMoE achieves 89.2% average accuracy, closest to Individual Fine-tuned baseline of 90.3% (section E.3.1). Break condition: When deployment requires strict inference budget constraints matching original model size.

## Foundational Learning

- **Concept: Model Fusion Taxonomy (Ensemble vs. Merging vs. Mixing)**
  - Why needed here: FusionBench organizes all algorithms into three categories with fundamentally different cost-benefit profiles. Understanding this taxonomy is essential for method selection.
  - Quick check question: Given two fine-tuned models, if you must output a single model with the same parameter count as the inputs, which category applies?

- **Concept: Task Vectors and Mode Connectivity**
  - Why needed here: Many merging methods operate on task vectors (θ_ft − θ_pre) rather than raw parameters. The assumption that these vectors reside in approximately linear, connected regions of the loss landscape underlies most fusion approaches.
  - Quick check question: What is the mathematical definition of a task vector, and what does its near-orthogonality imply for multi-task fusion?

- **Concept: Test-Time Adaptation (TTA)**
  - Why needed here: Adaptive methods like AdaMerging require understanding how to optimize fusion coefficients at inference time without access to training data.
  - Quick check question: How does TTA differ from standard fine-tuning, and what risks does it introduce for distribution shift?

## Architecture Onboarding

- **Component map:**
  Algorithm Module -> Model Pool Module -> Task Pool Module -> Results JSON

- **Critical path:**
  1. Define experiment via YAML config (method, modelpool, taskpool)
  2. CLI invokes `fusion_bench` which instantiates components
  3. Algorithm executes `run()` on modelpool
  4. Optional: taskpool evaluates merged model
  5. Results saved to JSON report

- **Design tradeoffs:**
  - `LazyStateDict` reduces memory for large models but adds loading latency per-model
  - Test-time adaptive methods require unlabeled data and add inference overhead
  - MoE-based mixing increases parameter count 1.5-4× depending on expert count

- **Failure signatures:**
  - Negative transfer on unseen tasks (RESISC45 accuracy drops below pre-trained baseline)
  - Corruption sensitivity: pixelation corruption causes near-random performance across all methods
  - LoRA adapter merging fails silently if adapters not properly integrated before fusion

- **First 3 experiments:**
  1. Reproduce simple averaging baseline on 8-task CLIP-ViT-B/32 to validate installation (target: ~66.5% average accuracy)
  2. Compare layer-wise vs. task-wise AdaMerging on same benchmark to observe the 13.9% gap (82.6% vs 68.7%)
  3. Evaluate generalization by training on 6 tasks, testing on 2 unseen (Table 14/15 setup) to quantify transfer degradation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can negative transfer be mitigated when fused models are evaluated on unseen tasks?
- Basis in paper: The authors observe that "a negative transfer is observed... on the RESISC45 dataset, where the merged models exhibit lower accuracy compared to the pre-trained model," indicating transferred knowledge "may not be beneficial or even harmful to this specific unseen task."
- Why unresolved: The paper documents the phenomenon but does not propose or evaluate methods to detect or prevent negative transfer during fusion.
- What evidence would resolve it: Experiments comparing fusion methods with explicit negative transfer detection/avoidance mechanisms on held-out task benchmarks.

### Open Question 2
- Question: What regularization techniques can prevent adaptive fusion methods from overfitting to corrupted test distributions?
- Basis in paper: The authors state "adaptive methods may overfit to certain tasks, leading to a decrease in overall performance. This suggests that adaptive methods may need to be further regularized to improve generalization and robustness."
- Why unresolved: While the problem is identified, no specific regularization strategies are proposed or tested for test-time adaptation methods like AdaMerging or WEMoE.
- What evidence would resolve it: Comparative studies of regularized adaptive methods versus baselines on corruption benchmarks (e.g., impulse noise, pixelation).

### Open Question 3
- Question: How can multi-task model fusion close the performance gap with traditional multi-task learning?
- Basis in paper: The authors observe "traditional MTL outperforms most multi-task model fusion methods, which indicates that traditional MTL is still a strong baseline for multi-task learning, and there is room for improvement."
- Why unresolved: The paper establishes the gap but does not investigate theoretical or practical approaches to match MTL performance without joint training access.
- What evidence would resolve it: Development of fusion methods that statistically match or exceed traditional MTL baselines across the eight CLIP tasks.

### Open Question 4
- Question: How can fusion robustness be maintained under severe corruptions like pixelation?
- Basis in paper: The authors note "The performance of all methods drops significantly on certain types of corruptions, such as pixelation and impulse noise. This highlights the challenge of maintaining robustness under severe distribution shifts and the need for further research in this direction."
- Why unresolved: Current methods show severe degradation (WEMoE drops to 28.0% average under pixelation), and no solutions are proposed.
- What evidence would resolve it: Fusion methods demonstrating maintained performance (e.g., <10% degradation) under pixelation corruption.

## Limitations
- Evaluation is limited to CLIP-based vision models and specific NLP architectures, leaving generalization to other domains unclear
- The paper lacks explicit details on hyperparameters for test-time adaptation methods, making exact reproduction difficult
- No systematic analysis of failure modes beyond observed performance degradation under corruption

## Confidence
- **High Confidence:** The core finding that model mixing (WEMoE, SMILE) outperforms parameter averaging and task arithmetic is well-supported by experimental results across multiple benchmarks
- **Medium Confidence:** Claims about orthogonality of task vectors are supported by observation but lack rigorous statistical validation
- **Low Confidence:** Generalization claims to unseen tasks are based on limited experiments (6→2 task transfer) without ablation studies on task similarity

## Next Checks
1. Reproduce the 13.9% gap between layer-wise (82.6%) and task-wise (68.7%) AdaMerging on ViT-B/32 to validate the importance of fine-grained fusion strategies
2. Test negative transfer by evaluating merged models on tasks outside the training set distribution, measuring degradation below pre-trained baselines
3. Characterize corruption sensitivity by systematically evaluating all methods under noise levels from Table 16, documenting performance collapse thresholds