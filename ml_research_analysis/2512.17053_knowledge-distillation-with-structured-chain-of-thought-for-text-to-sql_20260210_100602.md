---
ver: rpa2
title: Knowledge Distillation with Structured Chain-of-Thought for Text-to-SQL
arxiv_id: '2512.17053'
source_url: https://arxiv.org/abs/2512.17053
tags:
- reasoning
- structured
- student
- query
- distillation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Struct-SQL, a knowledge distillation framework
  that transfers structured reasoning from large language models to small language
  models for the Text-to-SQL task. The method distills a formal query execution plan
  as a structured teaching signal, contrasting with standard unstructured Chain-of-Thought
  distillation.
---

# Knowledge Distillation with Structured Chain-of-Thought for Text-to-SQL

## Quick Facts
- arXiv ID: 2512.17053
- Source URL: https://arxiv.org/abs/2512.17053
- Authors: Khushboo Thaker; Yony Bresler
- Reference count: 36
- Key outcome: 8.1% absolute improvement in execution accuracy for Text-to-SQL via structured query plan distillation

## Executive Summary
This paper proposes Struct-SQL, a knowledge distillation framework that transfers structured reasoning from large language models to small language models for the Text-to-SQL task. The method distills a formal query execution plan as a structured teaching signal, contrasting with standard unstructured Chain-of-Thought distillation. Evaluated on the BIRD mini-dev benchmark, the distilled small model achieves an 8.1% absolute improvement in execution accuracy over the unstructured baseline, reducing syntactic errors such as schema hallucination. The approach enables accurate, private deployment of small models while addressing enterprise constraints of cost and security.

## Method Summary
Struct-SQL uses knowledge distillation from GPT-4o (teacher) to Qwen3-4B or Mistral-7B (student) for Text-to-SQL generation. The teacher generates structured query execution plans (QP-CoT) that decompose SQL generation into sequential schema-aware steps, followed by the final SQL. The student is fine-tuned on the complete sequence using QLoRA with a sequence-level distillation loss. Training data is constructed from 1,000 stratified samples from the BIRD benchmark, filtered to ensure syntactic validity and correct execution. The framework is evaluated on the BIRD mini-dev benchmark with execution accuracy as the primary metric.

## Key Results
- 8.1% absolute improvement in execution accuracy over unstructured CoT baseline
- Reduction in syntactic errors from 21.2% to 16.8%, with 'No Such Column' errors dropping from 19.0% to 15.8%
- Training efficiency: 2.24 epochs, 29 minutes on single GPU (H200) with QLoRA
- Token overhead: 3.6× more inference tokens than unstructured approach (362 vs 99 avg)

## Why This Works (Mechanism)

### Mechanism 1: Structured Reasoning Reduces Schema Hallucination
Distilling structured query execution plans reduces syntactic errors in SLMs by enforcing explicit schema traversal steps. The query plan decomposes SQL generation into sequential steps (table scanning → column selection → filtering → joining → grouping), forcing the model to ground each operation against the schema before proceeding.

### Mechanism 2: Query Plan Provides Intermediate Supervision Signal
Training on teacher-generated query plans + SQL provides a denser learning signal than SQL alone or unstructured CoT + SQL. The student model learns to predict both the structured plan and final SQL, minimizing loss over the complete sequence, creating explicit gradient signal for intermediate reasoning steps.

### Mechanism 3: Generalization via Complexity-Stratified Sampling
The distillation dataset's stratified sampling across SQL complexity categories improves generalization to out-of-domain schemas. Training data is explicitly balanced across single-table (29.5%), subqueries (22.9%), joins/set operations (39.8%), and combined joins+subqueries (7.8%).

## Foundational Learning

- **Query Execution Plans**: Why needed - The entire Struct-SQL framework uses database query plans as the structured reasoning template. Quick check - Can you explain why a database optimizer might reorder JOIN operations, and how this affects the logical plan the LLM generates?

- **Knowledge Distillation Loss Functions**: Why needed - The framework uses sequence-level distillation rather than output-only distillation. Quick check - What is the difference between distilling only Y_T (final output) vs. R_T ⊕ Y_T (reasoning + output), and why would the latter require more training tokens?

- **SQL Error Taxonomy (Syntactic vs. Semantic)**: Why needed - The paper's core claim hinges on reducing syntactic errors specifically. Quick check - If a generated query executes without error but returns wrong rows, is this a SYN or SEM error? What if it references a non-existent column?

## Architecture Onboarding

- **Component map**: Teacher (GPT-4o) → QP-CoT Prompt → Structured Plan + SQL (Z_T) → Dataset Construction → Validity Filter → 1,000 samples → Student (Qwen3-4B / Mistral-7B) ← QLoRA Training (L_KD on Z_T) → Inference: QP-CoT Prompt → Autoregressive Plan → SQL

- **Critical path**: Dataset quality filtering. Samples are only admitted if GPT-4o's SQL is "syntactically valid and yields the correct result upon execution." Teacher errors propagate directly.

- **Design tradeoffs**: Token overhead (3.6× more tokens for 8.1% EX gain); JOIN performance (ReasonSQL outperforms Struct-SQL on JOIN queries); Training efficiency (29 min vs 110+ min for larger datasets).

- **Failure signatures**: High 'No Such Column/Table' errors → model not learning schema adherence; High 'Empty Output' → plan is syntactically correct but logically wrong; JOIN performance degradation → structured template may constrain flexible entity linking.

- **First 3 experiments**:
  1. Run student model with QP-CoT prompting (no fine-tuning) on your domain's BIRD-mini-dev equivalent to measure pre-distillation syntactic error rate
  2. Replicate Table 1's stratified sampling but with 500 samples instead of 1,000 to validate whether gains hold at lower data regimes
  3. Isolate JOIN queries from your evaluation set and compare Struct-SQL vs. ReasonSQL to determine if your domain exhibits the same JOIN disadvantage

## Open Questions the Paper Calls Out

- Can structured reasoning distillation based on execution plans be effectively generalized to other complex reasoning tasks outside of Text-to-SQL?
- Does integrating Struct-SQL with unstructured reasoning strategies yield a superior distillation signal compared to either method in isolation?
- Why does the Struct-SQL framework underperform compared to unstructured CoT specifically on queries requiring multi-entity JOIN operations?

## Limitations

- The paper demonstrates strong empirical improvements but lacks controlled ablation studies isolating the specific contribution of structured reasoning versus other factors
- Schema hallucination reduction claims are based on correlation rather than causal intervention studies
- JOIN query performance degradation under Struct-SQL versus unstructured approaches suggests the method may not be universally optimal across all SQL patterns

## Confidence

- **High confidence**: The 8.1% absolute improvement in execution accuracy over unstructured baselines is well-supported by controlled experiments on the BIRD mini-dev benchmark
- **Medium confidence**: The attribution of syntactic error reduction specifically to structured query plans is plausible but requires additional causal validation
- **Medium confidence**: The efficiency claims (2.24 epochs, 29 minutes training) are credible given the small dataset size, though generalization to larger domains remains unproven

## Next Checks

1. Conduct a controlled ablation experiment comparing Struct-SQL against an unstructured CoT baseline with identical teacher quality, dataset size, and prompt engineering to isolate the structural reasoning effect
2. Perform domain transfer testing on enterprise schemas outside the BIRD benchmark to validate generalization claims and identify failure patterns
3. Implement a hybrid approach that applies structured reasoning only to specific query types (joins, complex aggregations) while using unstructured CoT for simpler patterns to address the JOIN performance gap identified in Figure 4a