---
ver: rpa2
title: 'MineTheGap: Automatic Mining of Biases in Text-to-Image Models'
arxiv_id: '2512.13427'
source_url: https://arxiv.org/abs/2512.13427
tags:
- prompts
- prompt
- bias
- images
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MineTheGap, a method for automatically discovering
  text prompts that induce biased outputs in text-to-image models. The approach uses
  a genetic algorithm driven by a novel bias score that measures the gap between generated
  images and LLM-generated textual variations.
---

# MineTheGap: Automatic Mining of Biases in Text-to-Image Models

## Quick Facts
- **arXiv ID**: 2512.13427
- **Source URL**: https://arxiv.org/abs/2512.13427
- **Reference count**: 40
- **Key outcome**: Introduces MineTheGap, a method using genetic algorithms and a novel bias score to automatically discover text prompts that induce biased outputs in text-to-image models.

## Executive Summary
MineTheGap presents an automated approach for discovering text prompts that elicit biased outputs from text-to-image models. The method employs a genetic algorithm guided by a custom bias score that measures the gap between generated images and LLM-generated textual variations. Unlike existing approaches that rely on predefined bias categories, MineTheGap discovers biases directly from the model's behavior by comparing image distributions to plausible interpretations of the prompt. The approach is validated on occupational gender bias, achieving strong correlation with ground-truth demographics, and demonstrates consistent discovery of model-specific biases across four different TTI models.

## Method Summary
MineTheGap uses a genetic algorithm to iteratively evolve text prompts that minimize a custom bias score. The algorithm maintains a population of 15 prompts, each scored based on the similarity gap between 15 generated images and 15 LLM-generated text variations using CLIP embeddings. The score is computed as the average of the lower 25th percentile of maximum similarities in the cross-similarity matrix, normalized by the mean similarity. The GA iteratively selects the top 5 prompts, generates 2 mutations per selected prompt using the LLM, and injects 5 new random prompts to maintain diversity. This process continues until convergence, with validation against ground-truth demographic data showing strong correlation (œÅ=0.72) for occupational gender bias.

## Key Results
- Achieves Spearman correlation of 0.72 with Bureau of Labor Statistics gender ratios for occupational bias detection
- Consistently discovers prompts that elicit strong, model-specific biases across different TTI models
- Demonstrates ability to uncover biases without relying on predefined bias categories

## Why This Works (Mechanism)
The method works by framing bias discovery as an optimization problem where the genetic algorithm searches for prompts that maximize the gap between what the model generates and what the LLM considers plausible interpretations. The bias score effectively measures how much the generated images deviate from expected interpretations by comparing CLIP embeddings of images against text variations. By minimizing this score, the algorithm identifies prompts where the model's output distribution diverges significantly from plausible interpretations, revealing underlying biases in the model's training or representation.

## Foundational Learning
MineTheGap builds on genetic algorithm optimization techniques and representation learning through CLIP embeddings. The approach leverages the observation that text-to-image models can exhibit systematic biases when prompted in specific ways, and that these biases can be discovered through iterative search rather than manual investigation. The use of LLM-generated text variations as a proxy for plausible interpretations represents a novel approach to defining bias without requiring predefined categories. The method also assumes that CLIP embedding similarity is a meaningful metric for comparing visual and textual representations in the context of bias detection.

## Architecture Onboarding
The MineTheGap framework requires access to a text-to-image generation model, an LLM for prompt mutation and text variation generation, and CLIP models for embedding computation. The genetic algorithm operates independently of the specific TTI model being analyzed, making it applicable to various architectures. The system maintains a population of prompts and iteratively refines them based on the bias score. No specific architectural modifications to the TTI models are required - the method works as an external analysis tool that probes model behavior through prompt engineering.

## Open Questions the Paper Calls Out
- How does the method perform on biases beyond occupational gender bias?
- What is the relationship between discovered biases and the training data distribution?
- How sensitive is the bias score to different choices of LLM for text variation generation?
- Can the method identify intersectional biases that combine multiple attributes?
- What is the computational cost of applying this method to larger models or more diverse bias categories?

## Limitations
- Currently validated primarily on occupational gender bias, limiting generalizability claims
- Requires access to ground-truth demographic data for validation in specific bias categories
- Computational cost of generating multiple images and text variations per prompt iteration
- The bias score depends on the quality of LLM-generated text variations as proxies for plausible interpretations
- May not discover all types of biases, particularly subtle or context-dependent ones
- Performance may vary depending on the specific TTI model architecture and training approach

## Confidence
The method demonstrates strong empirical results for occupational gender bias detection with correlation of 0.72 against ground-truth data. The approach is well-justified theoretically through the use of genetic algorithms for optimization and CLIP embeddings for representation comparison. However, validation is limited to one specific type of bias, and the generalizability to other bias categories remains to be thoroughly established. The computational requirements and sensitivity to LLM choice for text variation generation are potential concerns that warrant further investigation.

## Next Checks
- Test the method on additional bias categories beyond occupational gender
- Evaluate sensitivity to different LLM choices for text variation generation
- Analyze computational costs for larger-scale bias discovery
- Investigate the relationship between discovered biases and model training data
- Examine whether the method can identify intersectional or compound biases
- Validate performance across different TTI model architectures and training approaches