---
ver: rpa2
title: 'GMM-COMET: Continual Source-Free Universal Domain Adaptation via a Mean Teacher
  and Gaussian Mixture Model-Based Pseudo-Labeling'
arxiv_id: '2601.11161'
source_url: https://arxiv.org/abs/2601.11161
tags:
- domain
- adaptation
- source
- target
- teacher
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces the first study on continual source-free universal
  domain adaptation (SF-UniDA), where a model must adapt sequentially to multiple
  unlabeled target domains with both domain and category shifts. The proposed method,
  GMM-COMET, combines Gaussian mixture model-based pseudo-labeling with a mean teacher
  framework to improve stability over long adaptation sequences.
---

# GMM-COMET: Continual Source-Free Universal Domain Adaptation via a Mean Teacher and Gaussian Mixture Model-Based Pseudo-Labeling

## Quick Facts
- arXiv ID: 2601.11161
- Source URL: https://arxiv.org/abs/2601.11161
- Reference count: 11
- This work introduces the first study on continual source-free universal domain adaptation (SF-UniDA), where a model must adapt sequentially to multiple unlabeled target domains with both domain and category shifts.

## Executive Summary
This paper proposes GMM-COMET, a method for continual source-free universal domain adaptation that combines Gaussian mixture model-based pseudo-labeling with a mean teacher framework. The approach addresses the challenge of adapting to multiple unlabeled target domains sequentially while handling both domain and category shifts. By integrating GMM-based pseudo-labeling for robust unknown class detection and mean teacher framework for stability, the method prevents excessive drift from the source model while maintaining adaptation capability. Experiments on DomainNet, CIFAR-10-C, and CIFAR-100-C demonstrate that GMM-COMET is the only method consistently improving upon the source-only baseline across all category shift scenarios (partial, open, and open-partial set).

## Method Summary
GMM-COMET adapts a pre-trained source model to unlabeled target domains in a source-free setting using a mean teacher framework with GMM-based pseudo-labeling. The method employs a ResNet-50 backbone with a 256-dim projection head, using a teacher model updated via exponential moving average (EMA) of student weights. Pseudo-labels are assigned using a GMM in a 64-dim projected feature space, with dual thresholds on OOD scores (Mahalanobis distance or entropy) to reject unknown classes. The training objective combines contrastive loss, entropy minimization, and consistency losses between student-teacher and student-source model features. The method operates in a continual learning setting, adapting sequentially to multiple target domains without forgetting previously learned knowledge.

## Key Results
- GMM-COMET is the only method consistently improving upon the source-only baseline across all category shift scenarios (partial, open, and open-partial set).
- The method achieves the best overall performance on DomainNet, CIFAR-10-C, and CIFAR-100-C benchmarks.
- GMM-COMET sets a strong baseline for future research in continual source-free universal domain adaptation.

## Why This Works (Mechanism)

### Mechanism 1: GMM-based pseudo-labeling
The Gaussian Mixture Model estimates likelihoods in a 64-dim projected feature space, assigning pseudo-labels based on class-conditional likelihoods while filtering samples using dual thresholds on OOD scores. This robust handling of category shifts works under the assumption that known classes form distinct clusters that can be modeled by Gaussian distributions, distinct from unknown class samples.

### Mechanism 2: Mean Teacher framework
A teacher model updated via exponential moving average of student weights provides stable reference distributions that resist noise from immediate student gradients. This framework stabilizes long-term adaptation under the assumption that the EMA smoothing factor is tuned to balance adaptation speed with noise filtering.

### Mechanism 3: Consistency losses
Two consistency losses prevent catastrophic forgetting by constraining feature drift: one enforces L2 consistency between student features and the frozen source model features, while another enforces consistency between student and teacher features. This regularizes the model to retain source knowledge while adapting, based on the assumption that source feature space contains valid knowledge that must be preserved.

## Foundational Learning

### Concept: Universal Domain Adaptation (UniDA)
**Why needed here:** Unlike standard DA, UniDA assumes the label space differs between source and target (Partial, Open, Open-Partial sets). You must understand that the model isn't just correcting covariate shift but must also reject "unknown" classes.
**Quick check question:** How does the model handle a target sample belonging to a class not seen during source training?

### Concept: Test-Time Adaptation (TTA) / Source-Free DA
**Why needed here:** The system operates without access to the original source data. Adaptation happens entirely on the incoming unlabeled target batch during inference.
**Quick check question:** What information is available to the model during the adaptation phase?

### Concept: Catastrophic Forgetting in Continual Learning
**Why needed here:** The model adapts sequentially. Without specific mechanisms (like the proposed consistency losses), adapting to Domain B will degrade performance on Domain A (or the source knowledge).
**Quick check question:** Why can't we just fine-tune the model continuously on the target stream?

## Architecture Onboarding

### Component map:
Backbone (ResNet-50) -> Projection Head (256-dim) -> Classifier -> GMM Module (stores μ, Σ for source classes) -> Student-Teacher Pair (identical architectures) -> OOD Detector (Mahalanobis distance or entropy)

### Critical path:
1. Receive Batch -> Student Forward Pass -> Project Features
2. Teacher Forward Pass -> Update GMM Statistics
3. Compute OOD Scores & Pseudo-labels
4. Calculate Total Loss (Lc + Le + Lcon)
5. Update Student via SGD; Update Teacher via EMA

### Design tradeoffs:
- **OOD Metric:** Mahalanobis distance works best for separated clusters (CIFAR-10); Entropy works better for overlapping classes (DomainNet)
- **Feature Dimension (FDr):** 64 dims selected as trade-off between memory efficiency and retaining enough info for clustering
- **Thresholding:** Adaptive initialization (first 50 batches) vs fixed thresholds

### Failure signatures:
- **Model Collapse:** All target samples mapped to a single class cluster. (Mitigation: Check Contrastive Loss Lc)
- **Trivial Solution:** Model marks everything as "Unknown." (Check OOD thresholds τu)
- **Catastrophic Forgetting:** Performance degrades on later domains. (Check consistency weights λ2, λ3)

### First 3 experiments:
1. **Validation of GMM initialization:** Run on a single domain to verify the adaptive threshold finding (Ninit=50) correctly separates known vs unknown samples
2. **Ablation on Consistency:** Run the full continual sequence with and without Lcon,src to quantify the retention of source knowledge
3. **Stress Test:** Evaluate on a sequence with extreme domain gaps (e.g., Sketch to Real) to verify stability limits

## Open Questions the Paper Calls Out

### Open Question 1
**Question:** How can the framework be extended to handle scenarios where the target label space evolves dynamically over time (continual category shift), rather than remaining static?
**Basis in paper:** The authors state, "We leave the analysis of changing category shifts in addition to (or instead of) the continual domain shifts open for future work."
**Why unresolved:** The current method assumes Yt is constant across the entire stream, which limits applicability in environments where new categories emerge sequentially.
**What evidence would resolve it:** A modification of GMM-COMET that successfully identifies and integrates novel classes appearing in later domains without catastrophically forgetting previous ones.

### Open Question 2
**Question:** Is it possible to move beyond rejection to classify or cluster samples from unknown classes into distinct groups?
**Basis in paper:** The conclusion identifies a promising direction: "move beyond rejection and enable the classification or clustering of samples from unknown classes."
**Why unresolved:** The current architecture maps all unknown samples to a single "unknown" class index (|Ys|+1), lacking the mechanism to differentiate between multiple unseen categories.
**What evidence would resolve it:** An extended output layer and loss function that achieves high clustering accuracy on target-private classes without degrading known-class performance.

### Open Question 3
**Question:** Can the mean teacher mechanism be made adaptive to prevent it from slowing convergence in simpler or less dynamic shift scenarios?
**Basis in paper:** The ablation study notes that in simpler settings like DomainNet PDA, "the exponential moving average inherent to the mean teacher can instead slow down adaptation."
**Why unresolved:** The current fixed momentum factor (αMT) offers stability for complex shifts but introduces unnecessary lag when rapid adaptation is feasible.
**What evidence would resolve it:** A dynamic momentum schedule that adjusts update speeds based on domain complexity, improving accuracy on simple shifts while maintaining stability on difficult ones.

## Limitations
- **Feature space assumption fragility:** The GMM-OOD framework assumes source classes form separable Gaussian clusters in the projected space, which may fail under severe domain shifts or class overlaps.
- **Hyperparameter sensitivity:** The method requires careful tuning of multiple hyperparameters (thresholds, consistency weights, EMA decay, GMM dimensionality), with sensitivity across diverse domain shifts remaining unquantified.
- **Limited scalability evaluation:** Experiments focus on 4-domain sequences (DomainNet) and single-domain corruptions (CIFAR-C), with unknown behavior on longer sequences with more diverse domains.

## Confidence
- **High confidence:** The experimental results showing GMM-COMET outperforming source-only baselines across all category shift scenarios are well-supported by the reported metrics.
- **Medium confidence:** The mechanism explanations are reasonable given the methodology description, but some specific implementation details (GMM initialization, numerical stability) are not fully specified.
- **Low confidence:** Claims about GMM-COMET being the "first" study in continual SF-UniDA cannot be independently verified without exhaustive literature review.

## Next Checks
1. **Cross-dataset generalization:** Test GMM-COMET on a dataset with more than 4 domains (e.g., Office-Home or WILDS) to validate scalability beyond the current experimental scope.
2. **Ablation on OOD metric selection:** Systematically compare Mahalanobis vs entropy OOD detection across all dataset pairs to verify the claimed metric-dataset pairing rationale.
3. **Memory efficiency analysis:** Measure and report the memory overhead of storing GMM statistics (μ, Σ) for all source classes across adaptation sequences to quantify the practical computational cost.