---
ver: rpa2
title: 'UniTTS: An end-to-end TTS system without decoupling of acoustic and semantic
  information'
arxiv_id: '2505.17426'
source_url: https://arxiv.org/abs/2505.17426
tags:
- audio
- data
- arxiv
- distilcodec
- speech
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DistilCodec, a method to distill a multi-codebook
  neural audio codec into a single-codebook version with 32,768 codes while achieving
  near 100% codebook utilization. Leveraging DistilCodec, the authors propose UniTTS,
  an end-to-end TTS system that jointly models acoustic and semantic information without
  separation.
---

# UniTTS: An end-to-end TTS system without decoupling of acoustic and semantic information

## Quick Facts
- arXiv ID: 2505.17426
- Source URL: https://arxiv.org/abs/2505.17426
- Reference count: 40
- Primary result: UniTTS achieves 4.80 fidelity, 4.97 stability, 4.94 naturalness, and 4.60 emotional expressiveness MOS scores while maintaining LLM-level text capabilities

## Executive Summary
UniTTS introduces a novel approach to text-to-speech synthesis by integrating acoustic and semantic information without decoupling them. The system leverages DistilCodec, a method to distill a multi-codebook neural audio codec into a single-codebook version with 32,768 codes while achieving near 100% codebook utilization. Built on a Qwen2.5-7B backbone, UniTTS processes interleaved text and audio prompts through a three-stage training pipeline, maintaining strong language modeling capabilities while generating high-quality speech. The system demonstrates impressive performance across multiple evaluation metrics including fidelity, stability, naturalness, and emotional expressiveness.

## Method Summary
UniTTS employs a novel end-to-end approach that avoids separating acoustic and semantic information during processing. The core innovation is DistilCodec, which distills a multi-codebook neural audio codec into a single-codebook version with 32,768 codes while achieving near 100% codebook utilization. The system uses a Qwen2.5-7B backbone and is trained through three stages: pre-training, supervised fine-tuning, and alignment. UniTTS accepts interleaved text and audio prompts, maintaining the text processing capabilities of a large language model while generating speech. This unified architecture allows for more natural integration of semantic and acoustic information compared to traditional cascaded TTS systems.

## Key Results
- UniTTS-LPO achieves MOS scores of 4.80 in fidelity, 4.97 in stability, 4.94 in naturalness, and 4.60 in emotional expressiveness
- DistilCodec reaches 98.2% codebook usage on speech and 99.9% on universal audio with low perplexity
- The system maintains LLM-level text capabilities while generating high-quality speech
- UniTTS demonstrates effective universal audio reconstruction across different audio domains

## Why This Works (Mechanism)
The unified architecture enables direct modeling of the relationship between semantic content and acoustic features, eliminating information loss that occurs during decoupling in traditional cascaded systems. By maintaining a single codebook with 32,768 codes and achieving near-perfect utilization, DistilCodec ensures efficient representation of both speech and universal audio signals. The three-stage training approach (pre-training, supervised fine-tuning, and alignment) allows the model to progressively learn the complex mapping between text/audio inputs and speech outputs while preserving the language modeling capabilities of the Qwen2.5-7B backbone.

## Foundational Learning
**Neural Audio Codecs**: Why needed - Efficient compression and reconstruction of audio signals; Quick check - Verify codebook utilization rates exceed 95% across different audio types
**Multi-head Attention**: Why needed - Capturing long-range dependencies in both text and audio sequences; Quick check - Confirm attention patterns show coherent alignment between semantic and acoustic features
**Knowledge Distillation**: Why needed - Transferring knowledge from complex multi-codebook systems to simpler single-codebook architectures; Quick check - Validate that distilled models maintain or improve performance metrics
**Prompt-based Learning**: Why needed - Handling interleaved text and audio inputs in a unified manner; Quick check - Test system response to various input format combinations
**End-to-end Training**: Why needed - Eliminating information loss from cascaded processing; Quick check - Compare performance metrics against cascaded TTS baselines
**LLM Integration**: Why needed - Maintaining strong language understanding while adding speech generation capabilities; Quick check - Evaluate text-only performance alongside speech tasks

## Architecture Onboarding

**Component Map**: Qwen2.5-7B backbone -> DistilCodec encoder -> Joint semantic-acoustic modeling -> Speech generation decoder

**Critical Path**: Input processing → Semantic encoding → Joint acoustic-semantic modeling → Codebook quantization → Speech synthesis

**Design Tradeoffs**: The unified approach sacrifices some modularity compared to cascaded systems but gains in information preservation and natural integration. The single-codebook design simplifies the architecture but requires careful distillation to maintain performance. Using a large LLM backbone provides strong text capabilities but increases computational requirements.

**Failure Signatures**: Poor codebook utilization indicates inadequate distillation; degraded text performance suggests interference between speech and language modeling; instability in speech generation may result from misalignment between semantic and acoustic representations.

**First Experiments**:
1. Test codebook utilization rates on diverse audio samples to verify near-100% utilization claim
2. Evaluate text-only performance to confirm LLM capabilities are preserved
3. Compare MOS scores against established TTS systems using identical evaluation protocols

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation relies heavily on MOS scores without comprehensive comparative baselines against established TTS systems
- Three-stage training pipeline complexity may affect reproducibility and introduces hyperparameter sensitivity
- Computational requirements for Qwen2.5-7B backbone may limit practical deployment scenarios
- Limited analysis of performance across diverse speaker characteristics, languages, and acoustic conditions

## Confidence

**High confidence**: Technical feasibility of DistilCodec for achieving high codebook utilization rates (98.2% and 99.9% metrics demonstrate effectiveness)

**Medium confidence**: End-to-end integration claims due to lack of ablation studies quantifying individual training stage contributions

**Medium confidence**: MOS evaluation results given absence of statistical significance testing and comparison with competitive baselines

**Low confidence**: Generalizability claims without evidence of performance across diverse speaker characteristics, languages, and acoustic conditions

## Next Checks

1. Conduct head-to-head comparisons with established TTS systems (e.g., VITS, Glow-TTS) using identical evaluation metrics and datasets to benchmark UniTTS's performance improvements

2. Perform ablation studies removing each training stage (pre-training, supervised fine-tuning, alignment) to quantify their individual contributions to final performance and identify potential bottlenecks

3. Test the system's robustness across diverse acoustic conditions including background noise, varying speaking rates, emotional variability, and cross-lingual scenarios to validate the claimed universal audio reconstruction capabilities