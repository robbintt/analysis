---
ver: rpa2
title: Data-Efficient Multi-Agent Spatial Planning with LLMs
arxiv_id: '2502.18822'
source_url: https://arxiv.org/abs/2502.18822
tags:
- rollout
- arxiv
- taxi
- llms
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores using large language models (LLMs) to efficiently
  solve multi-agent taxi routing problems by leveraging world knowledge and prompting
  techniques. The approach involves converting environmental states and actions into
  natural language prompts for an LLM, which outputs decisions on where taxis should
  move or pick up passengers.
---

# Data-Efficient Multi-Agent Spatial Planning with LLMs

## Quick Facts
- arXiv ID: 2502.18822
- Source URL: https://arxiv.org/abs/2502.18822
- Reference count: 40
- One-line primary result: Fine-tuned LLMs achieve state-of-the-art multi-agent taxi routing with 50x fewer environmental interactions than previous methods

## Executive Summary
This paper presents a novel approach to multi-agent spatial planning by leveraging large language models (LLMs) to solve taxi routing problems. The method converts environmental states and actions into natural language prompts, allowing an LLM to output decisions on taxi movements and passenger pickups. The approach demonstrates that LLMs can achieve strong performance with minimal environmental interactions through a combination of effective prompting strategies and fine-tuning.

The research shows that fine-tuning a Llama 3-8B model with a rollout algorithm significantly outperforms previous state-of-the-art methods while requiring substantially less training data. The study also reveals that incorporating shortest-path information into prompts substantially improves performance, and LLMs can adapt to environmental factors like weather through simple semantic prompts. The approach achieves the lowest average passenger waiting time across all load levels compared to existing methods.

## Method Summary
The approach frames multi-agent taxi routing as a sequential decision-making problem where each taxi can move to adjacent locations or pick up/drop off passengers. The environment state and action space are converted into natural language descriptions that serve as prompts for an LLM. The model outputs decisions by generating text that indicates the chosen action for each taxi. The method employs a fine-tuning pipeline that uses a rollout algorithm to generate training data, where the LLM iteratively generates actions and the environment provides resulting states. Key techniques include using shortest-path information in prompts to improve spatial reasoning and applying semantic prompts to adapt to environmental factors. The approach is evaluated against traditional methods including graph neural networks and reinforcement learning baselines.

## Key Results
- Fine-tuned Llama 3-8B achieves state-of-the-art performance with 50x fewer environmental interactions than previous methods
- Incorporating shortest-path information into prompts significantly improves LLM decision-making accuracy
- Zero-shot LLM performance is surprisingly strong, outperforming some traditional planning approaches without fine-tuning
- LLMs can adapt to environmental factors like weather through simple semantic prompt modifications

## Why This Works (Mechanism)
The method works by leveraging LLMs' ability to understand and reason about natural language descriptions of spatial relationships and planning tasks. By converting the multi-agent planning problem into a language understanding task, the approach taps into the LLM's world knowledge and reasoning capabilities. The fine-tuning process allows the model to learn domain-specific patterns while retaining its general reasoning abilities. The rollout algorithm provides high-quality training data that captures the sequential nature of the planning problem. Including shortest-path information in prompts gives the LLM explicit spatial context that improves its decision-making, while semantic prompts enable the model to adapt to environmental changes without requiring additional training.

## Foundational Learning
**Natural language prompting** - Converting states and actions to text-based prompts allows LLMs to process spatial planning tasks using their language understanding capabilities. This is needed because it transforms the problem into a format LLMs are designed to handle. Quick check: Verify that prompt format preserves all necessary spatial and state information.

**Fine-tuning with rollout algorithms** - Using generated trajectories as training data allows the LLM to learn from high-quality examples without requiring extensive real environmental interactions. This is needed to bridge the gap between zero-shot performance and optimal decision-making. Quick check: Ensure generated trajectories capture diverse scenarios and edge cases.

**Spatial reasoning through explicit path information** - Including shortest-path data in prompts provides the LLM with crucial spatial context that improves routing decisions. This is needed because LLMs may struggle with graph traversal reasoning without explicit spatial cues. Quick check: Compare performance with and without path information across different graph topologies.

## Architecture Onboarding

**Component map:** Environment -> State Encoder -> Prompt Generator -> LLM -> Action Decoder -> Environment

**Critical path:** The LLM inference loop forms the critical path where each decision depends on the previous action's outcome. The pipeline processes each taxi's decision sequentially within each timestep, with the LLM generating actions based on the current state description and shortest-path information.

**Design tradeoffs:** The approach trades computational cost of LLM inference against the need for fewer environmental interactions. Using natural language as an intermediate representation adds overhead but enables zero-shot learning and easier adaptation to new conditions. The fine-tuning process requires upfront compute but reduces long-term interaction costs.

**Failure signatures:** Performance degradation occurs when prompts fail to capture relevant spatial relationships, when the LLM overfits to training trajectories, or when environmental conditions differ significantly from training scenarios. The method may also struggle with very large agent populations due to token limitations.

**First experiments:** 1) Compare zero-shot performance across different LLM sizes to identify optimal base model. 2) Test prompting strategies with varying levels of path information detail. 3) Evaluate fine-tuning stability across different training data quantities.

## Open Questions the Paper Calls Out
**Open Question 1:** Can LLMs effectively serve as predictors of future requests or as value functions to evaluate generated plans in multi-agent routing contexts?
- Basis in paper: Section 7 states, "LLMs could be used to predict future requests or as value functions to evaluate generated plans" as a primary interest for future work.
- Why unresolved: The current study utilizes LLMs strictly for policy generation (action selection) rather than state evaluation or demand forecasting.
- What evidence would resolve it: Experiments comparing LLM-based demand forecasting accuracy against traditional time-series models and evaluating the correlation between LLM value estimates and ground-truth rollout costs.

**Open Question 2:** What frameworks can optimize the trade-off between the compute expense of LLM inference and the cost of environmental interactions?
- Basis in paper: Section 7 notes that "compute expense of large-model inference may now surpass that of environmental interactions, necessitating new frameworks for this paradigm."
- Why unresolved: The paper identifies this cost inversion but does not propose or test specific frameworks to manage the trade-off.
- What evidence would resolve it: A study introducing a hybrid agent that dynamically switches between cheap heuristics and expensive LLM planning based on uncertainty or operational constraints.

**Open Question 3:** Why do advanced prompting strategies like Few-Shot and Self-Consistency fail to outperform Zero-Shot prompting in this spatial planning domain?
- Basis in paper: Section 5.6 reports that few-shot and CoT-SC "do not perform better than zero-shot," hypothesizing "improper generalization" without providing verification.
- Why unresolved: The counter-intuitive result is observed in the ablation study, but the mechanism causing the performance degradation remains unexplained.
- What evidence would resolve it: An analysis of the model's attention maps or reasoning chains to demonstrate how provided examples bias the model against optimal graph traversal strategies.

## Limitations
- Generalization beyond taxi routing domain remains unproven, with unclear transfer to other multi-agent planning problems
- Evaluation focuses primarily on waiting time, lacking analysis of computational overhead and scalability to larger agent populations
- Limited exploration of sensitivity to prompt engineering quality and potential performance degradation with more complex environments

## Confidence
- Fine-tuned LLMs achieve state-of-the-art performance with 50x fewer environmental interactions: **High confidence**
- Incorporating shortest-path information improves performance: **Medium confidence**
- LLMs can adapt to environmental factors through semantic prompts: **Low confidence**

## Next Checks
1. Test the approach on multi-agent pathfinding problems with different topologies and constraints to evaluate cross-domain generalization of the prompting strategy.

2. Conduct ablation studies to determine which components of the fine-tuning pipeline contribute most to performance gains, particularly isolating the impact of rollout algorithm integration versus LLM fine-tuning itself.

3. Evaluate inference latency and computational requirements compared to traditional planning algorithms to assess practical deployment considerations for real-time multi-agent systems.