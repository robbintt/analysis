---
ver: rpa2
title: 'Sarang at DEFACTIFY 4.0: Detecting AI-Generated Text Using Noised Data and
  an Ensemble of DeBERTa Models'
arxiv_id: '2502.16857'
source_url: https://arxiv.org/abs/2502.16857
tags:
- text
- data
- detection
- dataset
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of detecting AI-generated text,
  focusing on the Defactify 4.0 shared task. The authors propose an ensemble-based
  approach using DeBERTa models trained on a noised dataset to improve robustness
  and generalization.
---

# Sarang at DEFACTIFY 4.0: Detecting AI-Generated Text Using Noised Data and an Ensemble of DeBERTa Models

## Quick Facts
- arXiv ID: 2502.16857
- Source URL: https://arxiv.org/abs/2502.16857
- Reference count: 22
- Achieved first place in DEFACTIFY 4.0 shared task with F1 scores of 1.0 (Task-A) and 0.9531 (Task-B)

## Executive Summary
This paper tackles the challenge of detecting AI-generated text by introducing a noise-driven ensemble approach for the DEFACTIFY 4.0 shared task. The authors inject random "junk" words (3-8 characters) into 10% of training data to force the model to learn robust, invariant features rather than overfitting to surface-level patterns. Their ensemble combines models trained on both clean and noised datasets, achieving state-of-the-art performance with F1 scores of 1.0 for AI vs. human classification and 0.9531 for identifying specific LLM generators.

## Method Summary
The approach centers on data augmentation through noise injection and ensemble learning. The team created a "noised dataset" by inserting random character strings into text samples at a 10% rate. They trained three DeBERTa-v3-small models: one on noised data (Task-A), and two for Task-B - one on noised data and another sequentially fine-tuned on original then noised data. The Task-B predictions were combined using a 60:40 weighted ensemble of the noised-only and sequentially fine-tuned models.

## Key Results
- Achieved F1 score of 1.0 for Task-A (AI vs. human classification)
- Achieved F1 score of 0.9531 for Task-B (specific LLM identification)
- Outperformed all previous individual model attempts through ensemble approach
- Successfully addressed overfitting problem in Task-B, improving from baseline F1 of ~0.28

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Injecting random noise forces the model to learn robust, invariant features rather than overfitting to surface-level lexical patterns.
- **Mechanism:** By corrupting 10% of the training text with garbled strings, the model cannot rely on perfect sequential fluency and must instead prioritize higher-level semantic consistency and structural patterns to classify the text, acting as a regularization layer.
- **Core assumption:** The semantic signature of "humanness" or specific LLM characteristics persists even when the local token distribution is corrupted.
- **Evidence anchors:** [abstract] "By injecting 10% junk words into the training data, the model becomes more resilient to disturbances and better captures invariant features." [section 4.2] "This approach mimics real-world scenarios where noisy or corrupted data is often encountered, enabling the model to learn more resilient representations."
- **Break condition:** If the detection task relies heavily on specific n-gram statistics (e.g., watermarking) rather than semantic structure, noise injection might destroy the very signal the model needs to detect.

### Mechanism 2
- **Claim:** A weighted ensemble of models trained on different data distributions (clean vs. noisy) captures a broader decision boundary than a single model.
- **Mechanism:** One model learns standard features from clean text, while the other learns robust features from corrupted text. Averaging their predictions (60:40 weighting) smooths out the variance and corrects the "overfitting to clean data" issue observed in the baseline.
- **Core assumption:** The errors made by the clean-trained model and the noise-trained model are at least partially uncorrelated.
- **Evidence anchors:** [abstract] "The ensemble combines models fine-tuned on both original and noisy datasets." [section 5] "Utilizing an ensemble of models fine-tuned on original and noisy data allowed us to capture diverse patterns, ultimately outperforming all previous individual model attempts."
- **Break condition:** If the "clean" model is already perfectly generalizing, the ensemble would merely add computational overhead without accuracy gain.

### Mechanism 3
- **Claim:** Sequential fine-tuning (Original → Noisy) allows a model to acquire nuanced patterns that direct training on noisy data misses.
- **Mechanism:** This curriculum-like approach first establishes a baseline understanding of the task on clean data, then adapts that knowledge to handle perturbations. The paper suggests this captures "distinct patterns" compared to direct noisy training.
- **Core assumption:** Knowledge learned from clean data is transferable and beneficial when subsequently adapting to noise, rather than conflicting with it.
- **Evidence anchors:** [section 4.2] "Further finetune deberta-v3-small on noisy train data which was earlier finetuned on original train dataset." [section 5] "Although this method did not outperform... it captured distinct patterns... These unique patterns proved valuable when integrating the outputs."
- **Break condition:** If "catastrophic forgetting" occurs, the model might lose the precise clean features during the second noisy training phase.

## Foundational Learning

- **Concept: Data Augmentation / Noising**
  - **Why needed here:** The core innovation of this paper is not a new architecture, but a specific data augmentation strategy (junk word injection) to solve the generalization problem in detection.
  - **Quick check question:** If you inject noise into a sentence, does the ground-truth label (Human vs. AI) change? (Answer: No, which is why this acts as regularization).

- **Concept: Ensemble Learning (Weighted Averaging)**
  - **Why needed here:** The winning submission relied on combining predictions. Understanding how to weight models (60:40 split) is crucial to reproducing the result.
  - **Quick check question:** Why use a weighted average (60:40) instead of a simple average? (Answer: To prioritize the model with higher validation confidence/performance).

- **Concept: Fine-tuning Transformers (DeBERTa)**
  - **Why needed here:** DeBERTa-v3-small is the engine. Understanding that it uses disentangled attention helps explain why it can handle the "complex patterns" mentioned in the abstract.
  - **Quick check question:** What is the difference between "pre-training" and "fine-tuning" in the context of this paper?

## Architecture Onboarding

- **Component map:**
  Raw Text -> Noise Generator (10% junk words) -> DeBERTa-v3-small Backbone -> Classification Head (Binary/Multiclass) -> Weighted Ensemble Combiner

- **Critical path:**
  1. Create "Noised Dataset" by modifying the "Original Dataset"
  2. Train Model A on Noised Dataset
  3. Train Model B on Original Dataset, then continue training on Noised Dataset (Sequential/Double)
  4. Inference: Pass input through both Model A and Model B
  5. Combine logits (0.6 * Model A + 0.4 * Model B) for final prediction

- **Design tradeoffs:**
  - **Noise Level:** 10% noise was chosen to balance robustness vs. semantic destruction. Higher noise might degrade performance on clean text; lower noise might not fix overfitting.
  - **Model Size:** Authors chose `deberta-v3-small` over `base` or `xsmall`. The paper implies `small` hit the "sweet spot" where `base` overfitted and `xsmall` underfit.

- **Failure signatures:**
  - **Task-B Overfitting:** The paper explicitly notes that without noise, models achieved high training accuracy but failed on validation for Task-B (identifying specific LLMs).
  - **Memorization:** If the model learns to detect the specific "junk words" rather than the underlying text style, it will fail on real-world clean data (though results suggest this did not happen).

- **First 3 experiments:**
  1. **Baseline Check:** Fine-tune `deberta-v3-small` on the raw, un-noised dataset to confirm the overfitting problem on Task-B (F1 ~0.28 reported).
  2. **Noise Injection Ablation:** Fine-tune a fresh model on the 10% noised dataset. Compare Task-B F1 score against the baseline (target: >0.90).
  3. **Ensemble Assembly:** Implement the weighted ensemble of the "Noised-only" model and the "Double Fine-tuned" model. Check if F1 exceeds the single model score of 0.9454.

## Open Questions the Paper Calls Out

- **Open Question 1:** How do dynamic data noising strategies compare to the static 10% junk word injection used in this study? [explicit] The conclusion states that future work will "explore further the potential of dynamic data noising strategies." This is unresolved because the current implementation uses a fixed probability and static method for noise injection; it is unknown if adapting noise levels during training offers better regularization.

- **Open Question 2:** Does integrating larger DeBERTa variants (e.g., base or large) into the ensemble yield significant performance gains over the DeBERTa-v3-small ensemble? [explicit] The authors list exploring "ensemble techniques with more variants of the DeBERTa models" as a specific avenue for future work. This is unresolved because the winning submission relied exclusively on the 'small' variant, leaving the potential benefits of larger architectures untested in this specific noise-driven ensemble context.

- **Open Question 3:** Does training on random character-level "junk" noise effectively generalize to sophisticated semantic adversarial attacks or paraphrasing? [inferred] The paper aims to address reliability against "user-modified versions" of text, but the proposed noise injection consists of random "garbled words" (Table 3) rather than semantic perturbations. This is unresolved because resilience to non-sensical character noise may not translate to robustness against meaningful text manipulation used to evade detection in real-world scenarios.

- **Open Question 4:** What is the impact of varying the noise injection ratio (currently fixed at 10%) on the model's generalization capability? [inferred] The methodology specifies injecting "10% junk" without providing an ablation study or justification for this specific threshold. This is unresolved because it is unclear if 10% is the optimal balance for regularization or if higher/lower ratios could further reduce the overfitting observed in Task-B.

## Limitations
- **Noise injection specifics:** While the paper claims 10% junk word injection, the exact algorithm for determining insertion points and counts is not fully specified, which could lead to variations in reproducibility.
- **Ensemble weighting details:** The paper does not explicitly state whether the 60:40 weighting applies to raw logits or softmax probabilities, nor the exact method for the "double finetune" learning rate schedule in the second phase.
- **Sequential fine-tuning validation:** The effectiveness of the sequential fine-tuning approach (Original → Noisy) is not directly validated against direct noisy training in the results section, leaving some uncertainty about its actual contribution.

## Confidence
- **High confidence:** The effectiveness of noise-driven training for improving generalization on Task-B (identifying specific LLMs) is well-supported by the reported F1 score jump from ~0.28 to 0.9454 and the winning performance.
- **Medium confidence:** The superiority of the weighted ensemble over single models is supported by results, but the specific 60:40 ratio and the contribution of the sequential fine-tuning variant are less certain without ablation studies.
- **Low confidence:** The exact noise injection algorithm and its precise impact on different text lengths or styles is not detailed enough to fully verify the claimed robustness.

## Next Checks
1. **Noise injection validation:** Implement and test the exact noise injection algorithm (random string length 3-8, 10% insertion rate) on a small subset of the data, and verify that model performance degrades if noise is omitted, confirming its role as regularization.
2. **Ensemble ablation study:** Reproduce the ensemble but also test a simple average (50:50) and individual model performance to quantify the marginal benefit of the weighted combination and confirm the 60:40 split is optimal.
3. **Sequential vs. direct noisy training:** Train two separate Task-B models—one with sequential fine-tuning (Original → Noisy) and one with only noisy training—and compare their F1 scores to isolate the contribution of the curriculum-like approach.