---
ver: rpa2
title: 'Empaths at SemEval-2025 Task 11: Retrieval-Augmented Approach to Perceived
  Emotions Prediction'
arxiv_id: '2506.04409'
source_url: https://arxiv.org/abs/2506.04409
tags:
- emotion
- majority
- label
- vote
- emotions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents EmoRAG, a retrieval-augmented system for perceived
  emotion detection in multilingual text. The approach uses a database of training
  examples, a retriever to fetch similar examples, and an ensemble of large language
  models to predict emotions.
---

# Empaths at SemEval-2025 Task 11: Retrieval-Augmented Approach to Perceived Emotions Prediction

## Quick Facts
- arXiv ID: 2506.04409
- Source URL: https://arxiv.org/abs/2506.04409
- Reference count: 2
- System achieves average F1-micro score of 0.638 and F1-macro of 0.590 across 28 languages in SemEval-2025 Task 11

## Executive Summary
This paper presents EmoRAG, a retrieval-augmented system for perceived emotion detection in multilingual text. The approach uses a database of training examples, a retriever to fetch similar examples, and an ensemble of large language models to predict emotions. Predictions are aggregated using label-specific F1-weighted voting. The system achieves strong results across 28 languages without requiring model fine-tuning, outperforming several monolingual baselines.

## Method Summary
The EmoRAG system employs a retrieval-augmented generation approach that combines a database of training examples with a retriever to find similar examples, then uses an ensemble of large language models to predict emotions. The system aggregates predictions using label-specific F1-weighted voting. Unlike fine-tuning approaches, this method leverages existing LLMs and a retrieval mechanism to adapt to multilingual emotion detection tasks, achieving strong performance across 28 languages without the computational overhead of model adaptation.

## Key Results
- Average F1-micro score of 0.638 across 28 languages
- Average F1-macro score of 0.590 across 28 languages
- Outperforms several monolingual baselines without requiring model fine-tuning

## Why This Works (Mechanism)
The retrieval-augmented approach works by leveraging existing training examples to provide context for emotion prediction. When presented with a new text, the system retrieves similar examples from its database and uses these alongside the input text to generate emotion predictions. The ensemble of LLMs provides diverse perspectives, while F1-weighted voting ensures that predictions are weighted by their historical accuracy for each emotion label. This approach avoids the computational cost of fine-tuning while still achieving strong multilingual performance.

## Foundational Learning
- Retrieval-augmented generation: Why needed - Provides relevant context without fine-tuning; Quick check - Measure retrieval precision and recall on validation set
- Ensemble voting with label-specific weights: Why needed - Accounts for varying label difficulty; Quick check - Compare weighted vs unweighted voting performance
- Multilingual emotion detection: Why needed - Real-world applications require cross-lingual understanding; Quick check - Test performance across language families

## Architecture Onboarding

Component Map:
Database of training examples -> Retriever -> LLM ensemble -> Label-specific F1-weighted voting -> Final emotion prediction

Critical Path:
Input text → Retriever (finds similar examples) → LLM ensemble (generates predictions) → F1-weighted voting (aggregates predictions) → Final output

Design Tradeoffs:
- Fixed database vs. dynamic expansion: Trade-off between stability and adaptability
- Multiple LLMs vs. single fine-tuned model: Trade-off between diversity and computational efficiency
- F1-weighted voting vs. simple majority: Trade-off between accuracy and simplicity

Failure Signatures:
- Poor retrieval performance on low-resource languages
- LLM bias toward certain emotion categories
- Voting mechanism overweighting confident but incorrect predictions

First Experiments:
1. Test retrieval precision on a held-out validation set
2. Compare ensemble performance against individual LLM performance
3. Evaluate impact of F1-weighted voting vs. simple majority voting

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions.

## Limitations
- Reliance on fixed training example database raises scalability concerns
- Performance on very low-resource languages not explicitly differentiated
- Complexity of ensemble voting mechanism obscures individual model contributions

## Confidence
- High confidence in English and major European language performance metrics
- Medium confidence in overall multilingual performance due to limited individual language breakdowns
- Low confidence in claims about generalizability to unseen domains or emotional contexts

## Next Checks
1. Conduct ablation studies to quantify the specific contribution of the retrieval component versus the LLM ensemble alone
2. Test the system's performance on emotion categories not well-represented in the training data
3. Evaluate retrieval effectiveness by measuring how often top-k retrieved examples improve prediction accuracy across different language families