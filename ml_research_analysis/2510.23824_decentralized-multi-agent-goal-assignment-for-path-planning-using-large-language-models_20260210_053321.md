---
ver: rpa2
title: Decentralized Multi-Agent Goal Assignment for Path Planning using Large Language
  Models
arxiv_id: '2510.23824'
source_url: https://arxiv.org/abs/2510.23824
tags:
- agents
- assignment
- goal
- agent
- decentralized
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study demonstrates that large language models, when provided
  with structured prompts and explicit quantitative information, can serve as highly
  effective decentralized agents for goal assignment in multi-agent grid environments.
  GPT-4.1-based agents, in particular, achieved makespans close to the optimal solver
  without centralized planning and consistently outperformed both greedy and random
  assignment strategies, especially as problem complexity increased.
---

# Decentralized Multi-Agent Goal Assignment for Path Planning using Large Language Models

## Quick Facts
- arXiv ID: 2510.23824
- Source URL: https://arxiv.org/abs/2510.23824
- Reference count: 15
- LLM-based agents with structured prompts achieve near-optimal makespans without centralized planning

## Executive Summary
This study demonstrates that large language models, when provided with structured prompts and explicit quantitative information, can serve as highly effective decentralized agents for goal assignment in multi-agent grid environments. GPT-4.1-based agents, in particular, achieved makespans close to the optimal solver without centralized planning and consistently outperformed both greedy and random assignment strategies, especially as problem complexity increased. These results highlight the crucial role of prompt design and input structure in enhancing reasoning capabilities of LLMs for collaborative tasks. Our work provides new benchmarks for language-model-driven coordination and points to promising directions for further integrating LLM-based agents into scalable, decentralized multi-agent systems.

## Method Summary
The approach involves comparing greedy (nearest-goal by BFS), random assignment, optimal solver (brute-force), and LLM-based agents (GPT-4.1, LLaVA) for decentralized goal assignment in 20×20 grid scenarios with 2-6 agents and goals. LLM agents receive grid images, scenario descriptions, positions, and optionally agent-goal distance tables, using chain-of-thought prompts with structured reasoning checklists. Conflicts are resolved by agent index ordering without negotiation. Makespan (timesteps until all agents reach goals) is the primary metric, with performance compared against optimal assignment computed via brute-force centralized search.

## Key Results
- GPT-4.1 with explicit distance tables achieved makespans close to optimal solver without centralized planning
- Performance degraded to near-greedy baseline when distance tables were removed from prompts
- GPT-4.1 consistently outperformed LLaVA and both greedy and random assignment strategies

## Why This Works (Mechanism)

### Mechanism 1
Providing explicit quantitative information (agent-goal distance tables) to LLM agents substantially improves assignment quality compared to relying on spatial inference from visual grid representations alone. Distance tables reduce the reasoning burden on the LLM by externalizing path cost computation, allowing the model to focus on assignment optimization rather than distance estimation. Without this data, the LLM must infer distances from grid images—an error-prone process that degrades decisions. The LLM can reliably compare numeric costs when presented in tabular form and integrate this into its ranking logic.

### Mechanism 2
Chain-of-thought (CoT) prompting with explicit reasoning checklists elicits more globally informed assignment decisions from LLM agents. CoT forces the model to articulate intermediate reasoning steps—listing goals, drafting assignments, computing longest paths, trying alternatives—before committing. This structured deliberation surfaces team-level reasoning rather than local greedy choices. The LLM follows the checklist steps faithfully and does not shortcut to immediate answers.

### Mechanism 3
Deterministic conflict resolution by agent index ordering enables feasible decentralized assignment without iterative negotiation. Each agent independently produces a ranked preference list. A fixed priority rule (lowest index wins conflicts) guarantees unique assignments post-hoc. This eliminates coordination overhead while ensuring consistency. Agents accept the priority ordering and do not attempt strategic manipulation of rankings based on predicted conflict outcomes.

## Foundational Learning

- **Concept: Makespan Minimization**
  - **Why needed here:** The core objective function. Makespan = max arrival time across all agents. Understanding this clarifies why greedy (minimizing individual cost) can fail globally.
  - **Quick check question:** If Agent 1 reaches its goal in 3 steps and Agent 2 in 7 steps, what is the makespan?

- **Concept: Decentralized vs. Centralized Assignment**
  - **Why needed here:** This paper's central distinction. Decentralized = agents decide independently with limited communication; centralized = single planner with global control.
  - **Quick check question:** In this paper, is conflict resolution centralized or decentralized? What about preference generation?

- **Concept: Assignment Problem / Hungarian Algorithm**
  - **Why needed here:** The optimal baseline uses brute-force search over assignments. The Hungarian algorithm is the classical polynomial-time solution for bipartite matching with costs.
  - **Quick check question:** Why does this paper use brute-force rather than Hungarian for optimal baseline computation?

## Architecture Onboarding

- **Component map:** Environment Generator → State Encoder → LLM Agent(s) → Conflict Resolver → Path Executor → Evaluator
- **Critical path:** Prompt design → LLM inference quality → Preference ranking accuracy → Conflict resolution → Assignment feasibility → Makespan. The prompt-to-ranking step is the highest-leverage failure point.
- **Design tradeoffs:**
  - Distance tables vs. visual-only: Tables improve performance (+2.5 makespan steps) but require pre-computation; visual-only is more general but unreliable
  - Re-rank every step vs. once: Re-ranking slightly improves results (15.12 vs. 15.50) but increases API calls and latency
  - GPT-4.1 vs. LLaVA: GPT-4.1 far superior; LLaVA exhibited unstable strategy shifts and performed worse than random
- **Failure signatures:**
  - High makespan with distance tables: Check prompt formatting; model may ignore or misparse table
  - Unstable rankings across steps: Observed with LLaVA; indicates model lacks coherent planning—switch to more capable model
  - Assignment conflicts unresolved: Conflict resolver not triggered; verify index ordering logic is applied
- **First 3 experiments:**
  1. Ablate distance tables: Run GPT-4.1 with and without explicit distance info on same 20 scenarios. Confirm ~2.5-step makespan gap replicates
  2. Scale agent count: Test 2, 4, 6, 8 agents. Plot performance gap vs. team size. Verify flat scaling for GPT-4.1 + distances
  3. Swap conflict resolution: Replace index-based priority with random tiebreaking. Measure variance in makespan to assess sensitivity to resolution rule

## Open Questions the Paper Calls Out

### Open Question 1
Can LLM-based decentralized assignment maintain near-optimal performance when scaled to significantly larger agent teams (e.g., >10 or >50 agents)? The authors explicitly state in the Conclusion that "Future work should explore larger team sizes" as the study was limited to a maximum of six agents. The computational cost and reasoning capabilities of LLMs were only validated on small-scale problems (2–6 agents), and it is unclear if the prompt-based reasoning scales linearly or degrades with increased coordination complexity.

### Open Question 2
How does the LLM-based assignment protocol perform under partial observability or dynamic environmental conditions? The Discussion notes that "Real-world scenarios may introduce dynamic obstacles... or partial observability," which were not addressed in the current static, fully observable setup. The agents in this study relied on complete global knowledge (grid layout, all positions) and static obstacles, assumptions that rarely hold in real-world robotics.

### Open Question 3
Can more sophisticated, LLM-driven negotiation protocols outperform the rigid, index-based conflict resolution used in this study? The Conclusion suggests exploring "richer agent communication protocols" and the Discussion notes agents "did not participate in ... ongoing negotiation." Conflicts were resolved deterministically by agent index priority, which is rigid; it remains unknown if allowing LLMs to negotiate directly would improve global efficiency or cause instability.

### Open Question 4
Does integrating LLM reasoning directly into the path-planning phase (navigation) yield better global outcomes than decoupled assignment? The Discussion highlights that LLM agents "did not participate in navigation," acting only on assignment. The current system separates goal assignment (LLM) from path execution (BFS), potentially missing optimization opportunities that require reasoning about specific routes or bottlenecks.

## Limitations
- Prompt engineering sensitivity: Substantial performance gaps hinge on precise prompt formatting and instruction-following
- Model selection: Results anchored to GPT-4.1 but exact variant unspecified, affecting reproducibility
- Scaling uncertainty: Claims about scalability beyond 2-6 agents not empirically validated
- Conflict resolution fairness: Fixed index ordering may not be optimal for heterogeneous agent capabilities

## Confidence

- **High**: GPT-4.1 + explicit distance tables consistently outperforms greedy/random baselines and achieves near-optimal makespans in tested configurations
- **Medium**: The critical role of distance tables in improving LLM reasoning is well-supported, but exact magnitude of improvement could vary with prompt tuning
- **Low**: Claims about scalability and robustness beyond the 2-6 agent range, or in environments with heterogeneous agent capabilities, are not empirically validated

## Next Checks

1. **Ablate distance tables**: Run GPT-4.1 with and without explicit distance info on the same 20 scenarios. Confirm the ~2.5-step makespan gap replicates under controlled prompt variations
2. **Scale agent count**: Test 2, 4, 6, 8 agents. Plot performance gap vs. team size to verify flat scaling for GPT-4.1 + distances and detect any degradation
3. **Swap conflict resolution**: Replace index-based priority with random tiebreaking. Measure variance in makespan to assess sensitivity to the resolution rule and identify potential fairness issues