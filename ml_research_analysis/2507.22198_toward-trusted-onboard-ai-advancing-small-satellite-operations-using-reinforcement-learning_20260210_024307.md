---
ver: rpa2
title: 'Toward Trusted Onboard AI: Advancing Small Satellite Operations using Reinforcement
  Learning'
arxiv_id: '2507.22198'
source_url: https://arxiv.org/abs/2507.22198
tags:
- satellite
- agent
- onboard
- control
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This research developed a Reinforcement Learning (RL) algorithm
  for autonomous command automation onboard a 3U CubeSat, focusing on macro Control
  Action Reinforcement Learning (CARL). The RL agent was trained using Proximal Policy
  Optimization (PPO) in a high-fidelity digital twin simulation, learning to issue
  high-level actions like adjusting attitude for solar pointing based on live telemetry.
---

# Toward Trusted Onboard AI: Advancing Small Satellite Operations using Reinforcement Learning

## Quick Facts
- **arXiv ID:** 2507.22198
- **Source URL:** https://arxiv.org/abs/2507.22198
- **Reference count:** 36
- **Primary result:** Developed RL algorithm for autonomous command automation onboard a 3U CubeSat, successfully containerized and interfaced with onboard systems, though full deployment phases remain future work

## Executive Summary
This research develops a Reinforcement Learning (RL) algorithm for autonomous command automation onboard a 3U CubeSat using macro Control Action Reinforcement Learning (CARL). The RL agent was trained using Proximal Policy Optimization (PPO) in a high-fidelity digital twin simulation, learning to issue high-level actions like adjusting attitude for solar pointing based on live telemetry. While full deployment phases remain future work, the development and integration phases demonstrated key results: the RL model was successfully containerized and interfaced with onboard systems, though initial inference outputs showed inconsistent behavior in extreme scenarios. Improved training with increased simulation difficulty yielded more responsive agent decisions.

## Method Summary
The approach uses Proximal Policy Optimization (PPO) via the Ray library within the BSK-RL (Basilisk) Gymnasium environment to train a macro action RL agent. The agent operates on a 24-dimensional observation space including attitude representations (Direction Cosine Matrix, Modified Rodrigues Parameters), angular velocity, position/velocity, battery charge, and reaction wheel speed. The policy outputs three high-level macro actions (Drift, Charge, Desaturate) which are translated into flight software command sequences via a decompilation layer. Training ran on a single desktop with NVIDIA 4090 GPU, with checkpoints saved every 5 episodes representing 6 months of simulated operations. The inference pipeline was containerized using Docker for onboard deployment.

## Key Results
- Successfully containerized RL inference pipeline for onboard deployment on 3U CubeSat
- Training with increased simulation difficulty improved agent responsiveness to survival scenarios
- Initial inference testing showed inconsistent behavior in extreme telemetry conditions, requiring further refinement
- Established phased framework for building trust in onboard AI from simulation to safe inference on orbit

## Why This Works (Mechanism)

### Mechanism 1
- Claim: High-level macro actions enable RL agents to command spacecraft without direct hardware coupling.
- Mechanism: The agent outputs abstract actions (Drift, Charge, Desaturate) which are translated into flight software command sequences via a decompilation layer. This decouples policy decisions from hardware-specific implementations.
- Core assumption: The translation layer correctly maps macro actions to safe low-level instructions across all reachable states.
- Evidence anchors:
  - [abstract] "The agent uses this information to produce high-level actions... which are then translated into control algorithms and executed through lower-level instructions."
  - [Page 5, Table 2] Maps each macro action to equivalent operator instructions with explicit pseudocode.
  - [corpus] Weak direct support; neighboring papers focus on low-level attitude control rather than hierarchical action spaces.
- Break condition: If the translation layer has edge cases or the flight software implementation diverges from pseudocode assumptions, unsafe commands could result.

### Mechanism 2
- Claim: Phased deployment builds operator trust by enabling safe comparison of predictions against real behavior before granting command authority.
- Mechanism: The trained policy runs in an isolated inference environment, receiving live telemetry and generating predictions that are logged but not executed. Operators compare these predictions against actual satellite behavior to validate model logic.
- Core assumption: The observation abstraction layer (compiled telemetry → policy input format) preserves information needed for correct decisions.
- Evidence anchors:
  - [Page 6] "The current version generates only the theoretical actions the agent would take if it were in control, creating a safe test environment... with no operational risk."
  - [Page 3] Phases 5-6 describe progression from recommendation engine to full autonomy as operator confidence grows.
  - [corpus] No direct corpus validation of this specific trust-building pipeline.
- Break condition: If telemetry abstraction loses critical state information, the agent's predictions may appear correct but would fail if granted command authority.

### Mechanism 3
- Claim: Increasing simulation difficulty forces the agent to learn proactive survival strategies.
- Mechanism: By reducing battery capacity and reaction wheel speed thresholds, terminal states become more frequent during training. This increases the density of negative reward signals, pushing the policy toward anticipatory behavior rather than reactive drift.
- Core assumption: The digital twin's failure modes accurately reflect real-world failure dynamics.
- Evidence anchors:
  - [Page 7] "The simulation environment's difficulty was increased by reducing battery capacity and reaction wheel maximum rotational speed thresholds. This strategy slightly improved the agent's reactiveness by forcing it to learn more proactive survival strategies."
  - [Page 4] Survival-based reward function with terminal states (battery depletion, wheel saturation) yielding rewards < -1.
  - [corpus] Assumption: Related work (Kyuroson et al., Naik et al.) shows DRL success in simulation but lacks on-orbit validation.
- Break condition: If the simulation's failure dynamics don't match reality, the agent may learn strategies that are brittle or counterproductive on orbit.

## Foundational Learning

- **Concept:** Modified Rodrigues Parameters (MRP) and Direction Cosine Matrices (DCM)
  - Why needed here: The observation space includes attitude representations (σ_B/N, [BN]) that the agent must interpret to determine spacecraft orientation relative to inertial frame and sun vector.
  - Quick check question: Given a DCM and sun vector in body frame, can you compute the rotation needed to point solar panels at the sun?

- **Concept:** Proximal Policy Optimization (PPO) algorithm
  - Why needed here: PPO is the training algorithm; understanding its stability properties and reward signal requirements is essential for debugging poor policy convergence.
  - Quick check question: If your agent converges to a policy that always selects the same action regardless of state, what reward shaping interventions might help?

- **Concept:** Docker multi-stage builds and cross-compilation
  - Why needed here: The inference pipeline must run on constrained onboard hardware with different CPU architecture than development machines; image size directly impacts uplink feasibility.
  - Quick check question: Your Docker image is 2GB but the satellite's uplink window allows only 50MB per pass—what build strategies can reduce deployed size?

## Architecture Onboarding

- **Component map:**
  - Training stack: BSK-RL environment → Ray distributed training → PPO checkpoints → extracted policy file
  - Inference stack: Live telemetry → feature extraction script → policy inference → macro action output → (future) command translation layer
  - Deployment: Docker container on satellite OBC, communicating via mounted directories and open ports with host bus computer

- **Critical path:**
  1. Define observation space matching both telemetry availability and simulation state variables
  2. Build digital twin with mission-specific parameters (mass, power, orbit)
  3. Train until checkpoint shows survival behavior in sanity-check scenarios
  4. Export policy and build inference container
  5. Validate on ground engineering model with real telemetry format
  6. Deploy to flight asset in recommendation-only mode

- **Design tradeoffs:**
  - Simulation fidelity vs. training time: Higher-fidelity models (e.g., including magnetorquers) are more realistic but harder to integrate and debug.
  - Observation abstraction vs. policy flexibility: Reduced input space accelerates convergence but may lose information needed for edge-case handling.
  - Container update frequency vs. risk: Hot-swappable config files enable rapid iteration but increase surface for deployment errors.

- **Failure signatures:**
  - Agent always selects "Drift" regardless of state → reward function doesn't sufficiently penalize inaction; increase terminal state frequency.
  - Telemetry columns misaligned with policy input → add keyword-based translation layer, not numeric indexing.
  - Unit mismatches between simulation and telemetry → integrate unyt library for compile-time unit checking.
  - Docker image too large for uplink → multi-stage builds, architecture-specific base images, dependency pruning.

- **First 3 experiments:**
  1. Sanity-check visualization: Plot policy action selections across a 2D grid of battery vs. wheel saturation, with nominal values for other dimensions. Confirm intuitive behavior at extremes (low battery → Charge action dominant).
  2. Ground-model telemetry replay: Feed historical telemetry through the inference pipeline and compare predicted actions against logged operator commands. Identify systematic divergences.
  3. Container integration test: Deploy the inference container to the ground engineering model, verify port communication and file mounting work correctly before any flight upload.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can reward functions be evolved beyond survival-based schemes to induce complex, mission-relevant adaptive behaviors in satellite agents?
- **Basis in paper:** [explicit] The "Future Work" section states the need to address "limitations of the current reward structure" by moving beyond the "existing survival-based reward function" to facilitate complex behaviors.
- **Why unresolved:** The current agent trained using a survival reward (avoiding battery depletion) demonstrated unresponsive behavior ("consistently drifts"), failing to adapt actions even when environment difficulty was increased.
- **What evidence would resolve it:** Demonstration of an agent successfully executing non-survival tasks (e.g., targeted imaging, complex maneuvering) in the digital twin without compromising basic safety constraints.

### Open Question 2
- **Question:** What mechanisms allow for the granular, selective promotion of specific model functions from a recommendation engine to full autonomous control?
- **Basis in paper:** [explicit] The "Future Work" section notes that to enable the "gradual promotion process," model functionality must be "split into portions that can be promoted separately from other model functions."
- **Why unresolved:** The paper outlines the deployment phases (recommendation to autonomy) conceptually but admits the "deployment phases... were not implemented herein."
- **What evidence would resolve it:** A software architecture and operational protocol that allows operators to independently certify and activate specific macro-actions (e.g., "Charge" vs. "Desaturate") while keeping others in a recommendation-only state.

### Open Question 3
- **Question:** What are the performance trade-offs between using mission-specific digital twins versus generic training environments for reinforcement learning models?
- **Basis in paper:** [explicit] The "Future Work" section proposes "systematic comparisons between satellite-specified and generic algorithms" to "quantify how closely the training environment must resemble operational conditions."
- **Why unresolved:** It is currently unclear if the overhead of building high-fidelity digital twins (as done for the LIME satellite) is strictly necessary for convergence or if more general environments yield transferable policies.
- **What evidence would resolve it:** Comparative metrics of agent performance and convergence speed when trained on a high-fidelity twin versus a generalized simulation, evaluated on the specific flight hardware.

### Open Question 4
- **Question:** How does the performance of Proximal Policy Optimization (PPO) compare to alternative learning or non-learning algorithms for this specific onboard control architecture?
- **Basis in paper:** [explicit] The "Future Work" section identifies the need to "expand on the experimental framework" by integrating "alternative learning approaches" and eventually "non-reinforcement learning algorithms" for benchmarking.
- **Why unresolved:** This study utilized PPO almost exclusively due to its stability; the suitability of other Deep Reinforcement Learning (DRL) or classical control methods within the same containerized inference pipeline remains untested.
- **What evidence would resolve it:** A comparative analysis benchmarking PPO against other algorithms (e.g., DQN, classical control) on the "sanity check" scenarios described in the results.

## Limitations
- No on-orbit validation completed; all results stem from simulation training
- Translation layer mapping macro actions to low-level commands remains untested
- Phases 4-6 (actual command authority) are future work, leaving core autonomy hypothesis unverified

## Confidence
**High Confidence:** The simulation training framework and PPO implementation are technically sound, with clear methodology and reproducible results within the digital twin environment.

**Medium Confidence:** The containerized inference pipeline and phased trust-building approach are well-conceptualized, though practical challenges (like the telemetry abstraction layer) introduce potential failure modes that could undermine the safety claims.

**Low Confidence:** The fundamental claim that this approach enables trusted autonomous spacecraft operations remains unproven, as no actual command execution or on-orbit testing has been completed.

## Next Checks
1. **Translation Layer Safety Analysis:** Conduct formal verification of the macro-to-micro command mapping across all possible state combinations, focusing on edge cases that could trigger unsafe commands.

2. **Ground Hardware-in-the-Loop Testing:** Deploy the inference pipeline to a flight-equivalent engineering model, feeding it live telemetry to validate that predicted actions match expected behavior before granting any command authority.

3. **Continuous Integration Testing Pipeline:** Implement automated testing that compares simulation-trained policy decisions against ground truth spacecraft responses across representative mission scenarios, measuring drift between predicted and actual outcomes.