---
ver: rpa2
title: 'OPTIMA: Optimal One-shot Pruning for LLMs via Quadratic Programming Reconstruction'
arxiv_id: '2512.13886'
source_url: https://arxiv.org/abs/2512.13886
tags:
- optima
- thanos
- anda
- sparsegpt
- pruning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: OPTIMA introduces a one-shot post-training pruning method that
  achieves both high accuracy and computational efficiency for large language models.
  It reformulates layer-wise weight reconstruction as column-wise quadratic programs
  that share a common Hessian matrix, enabling per-column globally optimal updates.
---

# OPTIMA: Optimal One-shot Pruning for LLMs via Quadratic Programming Reconstruction

## Quick Facts
- **arXiv ID:** 2512.13886
- **Source URL:** https://arxiv.org/abs/2512.13886
- **Reference count:** 30
- **Primary result:** Achieves up to 3.97% absolute accuracy gains over baseline mask selectors through one-shot post-training pruning

## Executive Summary
OPTIMA introduces a one-shot post-training pruning method for large language models that achieves both high accuracy and computational efficiency. The key innovation is reformulating layer-wise weight reconstruction as column-wise quadratic programs that share a common Hessian matrix, enabling per-column globally optimal updates. This approach can be integrated with existing mask selectors and consistently improves zero-shot accuracy across multiple model families and sparsity regimes.

## Method Summary
OPTIMA performs layer-wise weight reconstruction by solving column-wise quadratic programs (QPs) with a shared Hessian matrix estimated from calibration data. For each layer, it accumulates the Hessian $H = X^T X$ incrementally, then solves constrained QPs per column to find the optimal weight adjustment that minimizes reconstruction error given the pruning mask. The method uses a first-order primal-dual solver (rAPDHG) that exploits the shared-Hessian structure for parallel execution on accelerators.

## Key Results
- Achieves up to 3.97% absolute accuracy gains over baseline mask selectors
- Prunes an 8B-parameter transformer in 40 hours using 60GB of memory on NVIDIA H100
- Consistently improves zero-shot accuracy across multiple model families and sparsity regimes

## Why This Works (Mechanism)

### Mechanism 1: Shared-Hessian Column-wise Decomposition
OPTIMA recovers model accuracy by solving for the globally optimal weight update relative to the reconstruction objective. The method decomposes the layer-wise reconstruction problem into independent QPs for each column of the weight matrix, sharing the same Hessian matrix $H = X^T X$ because all columns receive identical input activations. This enables finding the mathematically optimal adjustment for non-pruned weights to compensate for removed weights.

### Mechanism 2: Constraint Enforcement via Variable Bounds
Instead of using explicit constraint matrices, OPTIMA encodes pruning masks as tight variable bounds. By setting the bounds of $\Delta w_i$ to $-w_i$ for pruned indices, it effectively locks these variables to their desired values without the system overhead of equality matrices, making constrained QP solving feasible on accelerators.

### Mechanism 3: Accelerated Batched QP Solving
The shared-Hessian structure enables batching thousands of micro-QPs for parallel execution. The solver reuses $H$ across all column-wise problems, with critical operations reducing to matrix-vector products with the shared Hessian. This allows processing entire weight matrices in large batches rather than sequential column-by-column updates.

## Foundational Learning

- **Concept: Least-Squares Reconstruction (OBS/OBD)**
  - Why needed here: OPTIMA is essentially an efficient solver for the Optimal Brain Surgeon reconstruction objective. Understanding that the goal is to minimize $\|XW - X\hat{W}\|^2$ (output error) rather than just weight magnitude is central.
  - Quick check question: Why does minimizing the magnitude of weights (L1/L2) not guarantee minimal *output* error compared to OBS-style reconstruction?

- **Concept: Quadratic Programming (QP)**
  - Why needed here: The core contribution reformulates weight update as a QP. You need to distinguish the objective (quadratic function of weights) from the constraints (linear equalities/inequalities).
  - Quick check question: In the formulation $\min \Delta w^T H \Delta w$, what does $H$ represent physically regarding the loss landscape?

- **Concept: Calibration Data in Post-Training Quantization/Pruning**
  - Why needed here: The Hessian $H$ is not computed from the full dataset but estimated from a small "calibration" set.
  - Quick check question: How does the size and distribution of the calibration dataset affect the estimation of the Hessian $H$?

## Architecture Onboarding

- **Component map:** Hessian Accumulator -> Mask Selector -> QP Solver (rAPDHG) -> Weight Updater

- **Critical path:** The Hessian Accumulation and QP Solver loop. The solver is iterative; if tolerance is too tight, this loop dominates runtime. Hessian accumulation is memory-bound if not implemented incrementally.

- **Design tradeoffs:**
  - Constrained vs. Unconstrained QP: Chose Constrained using bounds because Unconstrained reformulation creates variable-sized matrices that are "difficult to batch" on GPUs.
  - Optimality vs. Speed: Slower than Wanda but faster than exact OBS. Targets middle ground: mathematical optimality of reconstruction step, but only for fixed mask.
  - Memory: Uses 60GB peak memory (H100) to hold $H$ and batched QPs.

- **Failure signatures:**
  - Solver Divergence: If Hessian is ill-conditioned, rAPDHG might not converge within 100,000 steps.
  - Mask Drift: If QP solver numerical tolerance is loose, weights meant to be 0 might become non-zero.
  - OOM on Large Layers: For layers with massive input dimensions $d$, $H \in \mathbb{R}^{d \times d}$ may not fit in memory.

- **First 3 experiments:**
  1. Unit Test: Verify QP solver on single layer. Check if $\|XW - X\hat{W}\|$ decreases after update compared to just applying mask.
  2. Scaling Profiling: Measure runtime of Hessian accumulation vs. QP solving. Does solver scale linearly with column count?
  3. Ablation on Tolerance: Run pruning with loose vs. tight QP tolerance (e.g., $10^{-2}$ vs $10^{-4}$) to find "knee" in curve where accuracy gains diminish relative to solver steps.

## Open Questions the Paper Calls Out

- **Question 1:** How does the interaction between specific mask selection heuristics and OPTIMA reconstruction affect performance in semi-structured sparsity regimes for larger models (>7B parameters)?
  - Basis: Appendix A notes varied results on Qwen-2.5 family under 2:4 semi-structured sparsity suggest "a complex interaction between mask selection heuristics and OPTIMA reconstruction."
  - Evidence needed: Systematic ablation study on models larger than 7B comparing different mask selectors under 2:4 constraints.

- **Question 2:** Can the reconstruction optimality of OPTIMA be maintained or improved by integrating the mask selection and weight update steps?
  - Basis: Section 2 states finding optimal mask is NP-hard, motivating decoupled approach. Paper demonstrates optimality only for weight update given fixed mask.
  - Evidence needed: Algorithm allowing iterative mask refinement within OPTIMA framework compared against current one-shot decoupled baseline.

- **Question 3:** Does OPTIMA's optimal weight reconstruction reduce the necessity for or effectiveness of subsequent PEFT methods like QLoRA?
  - Basis: Paper focuses entirely on zero-shot performance "without fine-tuning." Leaves unstated whether 3.97% gain closes gap to dense model completely.
  - Evidence needed: Benchmarks of OPTIMA-pruned models versus baseline-pruned models after applying equal amount of LoRA or QLoRA fine-tuning on downstream tasks.

## Limitations
- Reliance on Hessian estimation from small calibration set introduces statistical uncertainty in reconstruction quality
- Scalability to layers with significantly larger input dimensions than tested 8k-16k range is not demonstrated
- Numerical stability of tight bounds mechanism under float16/bfloat16 precision remains an implicit assumption

## Confidence
- **High Confidence:** Shared-Hessian column-wise decomposition enabling parallel QP solving; method's ability to improve accuracy over baseline mask selectors
- **Medium Confidence:** Optimality of per-column QP solutions given estimated Hessian; memory efficiency claim
- **Low Confidence:** Robustness of tight bounds mechanism under numerical precision limits; scalability to architectures with much larger layer dimensions

## Next Checks
1. **Calibration Data Sensitivity:** Systematically vary size and distribution of calibration dataset (32, 64, 128, 256 samples from different domains) and measure impact on final pruned model accuracy and Hessian condition number.

2. **Solver Numerical Stability:** Intentionally set loose QP solver tolerances (e.g., $10^{-2}$) and measure drift in pruned weights. Also test solver with float16 vs bfloat16 to identify precision-related convergence issues.

3. **Scalability Boundary:** Test method on layer with artificially inflated input dimension to identify practical limit where $d \times d$ Hessian memory requirement negates batching benefits.