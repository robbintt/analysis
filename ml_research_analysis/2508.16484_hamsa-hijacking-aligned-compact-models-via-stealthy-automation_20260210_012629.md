---
ver: rpa2
title: 'HAMSA: Hijacking Aligned Compact Models via Stealthy Automation'
arxiv_id: '2508.16484'
source_url: https://arxiv.org/abs/2508.16484
tags:
- prompts
- jailbreak
- arxiv
- attack
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: HAMSA introduces an automated red-teaming framework for generating
  stealthy jailbreak prompts against aligned compact LLMs, addressing the limitations
  of manual and low-quality automated methods. The core method employs a multi-stage
  evolutionary search with temperature-controlled variability to iteratively refine
  prompts while maintaining natural language fluency.
---

# HAMSA: Hijacking Aligned Compact Models via Stealthy Automation

## Quick Facts
- arXiv ID: 2508.16484
- Source URL: https://arxiv.org/abs/2508.16484
- Authors: Alexey Krylov; Iskander Vagizov; Dmitrii Korzh; Maryam Douiba; Azidine Guezzaz; Vladimir Kokh; Sergey D. Erokhin; Elena V. Tutubalina; Oleg Y. Rogov
- Reference count: 40
- Primary result: Automated red-teaming framework achieving 97% success rate on aligned compact LLMs using policy puppetry templates and RAG-enhanced evolutionary search

## Executive Summary
HAMSA introduces an automated red-teaming framework for generating stealthy jailbreak prompts against aligned compact LLMs, addressing the limitations of manual and low-quality automated methods. The core method employs a multi-stage evolutionary search with temperature-controlled variability to iteratively refine prompts while maintaining natural language fluency. A Policy Puppetry Template disguises malicious instructions as benign policy files (XML, INI, JSON), and RAG-enhanced lifelong learning enables strategy transfer across prompts. Evaluated on English and Arabic datasets, HAMSA achieved success rates up to 97% with mean scores above 9.4, outperforming baselines. Notably, Arabic prompts showed higher adversarial potency (2.24 harmfulness score) than English (1.28), highlighting increased vulnerability in less-resourced dialects.

## Method Summary
HAMSA operates through a multi-stage evolutionary framework that combines population-based search with RAG-enhanced lifelong learning. The system uses a Policy Puppetry Template to disguise malicious instructions as benign configuration files, bypassing token-level safety filters while preserving semantic harmfulness. The evolutionary search iteratively refines candidate prompts using temperature-controlled variability to balance exploration and coherence preservation. Successful attack strategies are stored in a RAG library and retrieved for transfer across semantically similar prompts. The framework was evaluated against GigachatLite using Qwen-7B, Mistral-7B, and Vicuna-7B as attacking models, with Gigachat-MAX serving as the scorer.

## Key Results
- Success rates up to 97% on aligned compact LLMs
- Mean harmfulness scores above 9.4 on 10-point scale
- Arabic prompts showed higher adversarial potency (2.24 harmfulness) than English (1.28)
- Policy Puppetry Template achieved consistent evasion across XML, INI, and JSON formats
- RAG-based strategy transfer improved attack efficiency on semantically similar queries

## Why This Works (Mechanism)

### Mechanism 1: Policy Puppetry Template Evasion
- Claim: Disguising malicious instructions as benign configuration files (XML, INI, JSON) bypasses token-level safety filters while preserving semantic harmfulness.
- Mechanism: The formatting transformation Π(·) wraps adversarial content in structured markup that appears syntactically benign to surface-level detectors, exploiting the gap between structural parsing and semantic safety evaluation in aligned models.
- Core assumption: Safety training has weaker coverage for configuration-file-like inputs compared to natural language prompts.
- Evidence anchors:
  - [abstract] "employs a multi-stage evolutionary search... capable of bypassing alignment safeguards while maintaining natural language fluency"
  - [section 3] "Π(·) denotes a formatting transformation that disguises adversarial instructions as benign policy or configuration files"
  - [corpus] Weak direct evidence; related work (Graph of Attacks with Pruning) similarly uses structural obfuscation but doesn't validate this specific template approach.
- Break condition: Safety filters trained explicitly on configuration-file-style inputs; perplexity thresholds tuned for structured formats.

### Mechanism 2: Temperature-Controlled Evolutionary Search
- Claim: Iterative mutation with temperature-guided variability discovers adversarial prompts that balance attack success with fluency preservation.
- Mechanism: Population-based optimization applies mutation operators (paraphrasing, format obfuscation) where temperature controls exploration-exploitation tradeoff—higher temperature for diverse candidates, lower for coherent refinements.
- Core assumption: The search space contains regions where fluency and adversarial strength co-occur.
- Evidence anchors:
  - [abstract] "candidate prompts are iteratively refined using a population-based strategy augmented with temperature-controlled variability to balance exploration and coherence preservation"
  - [section 3] Eq. 4 formalizes: J^(t+1) = σ(μ(J^t), S) where μ denotes mutation operators and σ is selection
  - [corpus] ForgeDAN paper validates evolutionary frameworks for jailbreaks but doesn't isolate temperature effects.
- Break condition: Temperature extremes cause either incoherent output (too high) or search stagnation (too low); threshold τ misaligned with actual harmfulness.

### Mechanism 3: RAG-Based Strategy Transfer
- Claim: Retrieving semantically similar successful attacks accelerates jailbreak discovery on new queries.
- Mechanism: After WarmUp builds initial strategy library D, Lifelong phase retrieves top-K strategies via cosine similarity over response embeddings (Eq. 6). Algorithm 1 selects strategies based on score differentials, enabling cross-query generalization.
- Core assumption: Successful attack patterns transfer across semantically similar malicious intents.
- Evidence anchors:
  - [abstract] "integrates RAG for lifelong adaptation and transfer of successful attack strategies"
  - [section 3] "Successful responses are re-embedded and saved back to D, continuously enriching the library"
  - [corpus] Limited direct validation; RICoTA paper discusses in-the-wild conversation testing but doesn't address RAG-based strategy transfer.
- Break condition: Embedding similarity doesn't correlate with attack transferability; library contamination from low-quality strategies; domain shift between WarmUp and test queries.

## Foundational Learning

- **Concept: Evolutionary/Genetic Algorithms**
  - Why needed here: HAMSA frames red-teaming as discrete optimization (Eq. 3) with mutation, selection, and fitness scoring. Understanding population-based search is essential for tuning temperature and iteration limits.
  - Quick check question: Can you explain how mutation operators differ from selection mechanisms in genetic algorithms?

- **Concept: LLM Safety Alignment (RLHF, Instruction Tuning)**
  - Why needed here: The attack targets alignment safeguards. You need to understand what RLHF optimizes for to reason about why puppetry templates evade detection.
  - Quick check question: What objective does RLHF optimize, and why might it leave gaps for structured-format inputs?

- **Concept: Retrieval-Augmented Generation (RAG)**
  - Why needed here: Lifelong iteration depends on embedding-based retrieval. Understanding dense retrieval, cosine similarity, and dynamic library updates is critical.
  - Quick check question: How does embedding similarity relate to semantic similarity, and what are failure modes when they diverge?

## Architecture Onboarding

- **Component map:**
  Attacking Model (LLM_attk: Qwen-7B / Mistral-7B / Vicuna-7B) -> Policy Puppetry Template -> Target Model (GigachatLite) -> Scorer LLM (Gigachat-MAX) -> RAG Library -> Summarizer LLM

- **Critical path:**
  1. WarmUp: 20 seed prompts → generate attacks → score → store successful triplets in RAG
  2. Lifelong: For each new query, retrieve top-K strategies → apply mutations → score → update RAG
  3. Test: Freeze RAG, disable summarizer, run standardized attack per query

- **Design tradeoffs:**
  - Temperature: Higher → more exploration but risk incoherence; lower → stable but may miss diverse attacks
  - RAG library size: Larger → better coverage but slower retrieval and potential noise
  - Termination threshold (τ): Higher → stricter success criteria but more iterations; lower → faster but lower-quality harmful outputs

- **Failure signatures:**
  - High iteration count with low success rate → temperature too low or initial population poor
  - Low fluency scores despite high attack success → temperature too high, mutation too aggressive
  - RAG retrieval returns irrelevant strategies → embedding model mismatch or library contamination
  - Consistent refusal responses → puppetry template detected; try alternative format (XML vs JSON)

- **First 3 experiments:**
  1. **Baseline validation**: Run WarmUp only (no Lifelong) on 20 seed prompts; verify attack success ≥40% and scorer calibration (spot-check scores against manual review).
  2. **Temperature sweep**: Fix all other parameters, test temperature ∈ {0.5, 0.7, 1.0, 1.3} on subset of 50 prompts; plot success rate vs. fluency tradeoff.
  3. **RAG ablation**: Compare (a) full Lifelong with RAG, (b) Lifelong without RAG updates, (c) no retrieval (random strategy); measure cross-topic transfer success on held-out categories.

## Open Questions the Paper Calls Out
- None

## Limitations
- Dataset Representativeness: Limited details on dataset diversity, prompt complexity distribution, and domain coverage; Arabic subset contains only 100 prompts
- Transferability Claims: Mixed results showing some successful transfer while others fail completely, suggesting imperfect embedding-similarity correlation
- Safety Filter Generalization: Assumes safety training gaps for structured-format inputs that may not hold for models with enhanced detection mechanisms

## Confidence
- **High Confidence**: Core evolutionary search framework with temperature control works as described; technically sound and reproducible
- **Medium Confidence**: Policy Puppetry Template evasion mechanism shows effectiveness but relies on assumptions about safety training gaps needing broader validation
- **Medium Confidence**: RAG-based strategy transfer demonstrates potential but shows inconsistent performance across prompt categories
- **Low Confidence**: Claims about Arabic prompts showing higher adversarial potency based on small sample (100 prompts) lacking systematic analysis of dialectal variations

## Next Checks
1. **Cross-Model Safety Filter Testing**: Test Policy Puppetry Templates against 5+ different aligned models (including OpenAI, Anthropic, and open-source alternatives) to validate whether structured-format evasion generalizes beyond the specific target model used in the study.

2. **Temperature Robustness Analysis**: Conduct systematic temperature sweeps (0.1 to 2.0 in 0.1 increments) across 100+ prompts to quantify the relationship between temperature settings, success rates, and fluency preservation, identifying optimal ranges for different prompt types.

3. **Longitudinal Strategy Transfer Study**: Track strategy library evolution over extended attack sessions (1000+ prompts) to measure RAG-based transfer efficiency decay rates, contamination effects, and identify conditions under which stored strategies become counterproductive.