---
ver: rpa2
title: 'GHaLIB: A Multilingual Framework for Hope Speech Detection in Low-Resource
  Languages'
arxiv_id: '2512.22705'
source_url: https://arxiv.org/abs/2512.22705
tags:
- hope
- urdu
- speech
- english
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents GHaLIB, a multilingual framework for hope speech
  detection in low-resource languages. The authors address the challenge of detecting
  hope speech in languages like Urdu, which has been underrepresented in NLP research.
---

# GHaLIB: A Multilingual Framework for Hope Speech Detection in Low-Resource Languages

## Quick Facts
- arXiv ID: 2512.22705
- Source URL: https://arxiv.org/abs/2512.22705
- Reference count: 40
- Achieves 95.2% F1 for Urdu binary classification on PolyHope-M 2025

## Executive Summary
This paper presents GHaLIB, a multilingual framework for hope speech detection in low-resource languages. The authors address the challenge of detecting hope speech in languages like Urdu, which has been underrepresented in NLP research. The proposed method leverages transformer models, including XLM-RoBERTa, mBERT, EuroBERT, and UrduBERT, with simple preprocessing and classification training. Experiments on the PolyHope-M 2025 benchmark show strong performance, achieving F1-scores of 95.2% for Urdu binary classification and 65.2% for Urdu multi-class classification. Similar competitive results are reported for Spanish, German, and English. The framework demonstrates the potential of applying multilingual models to low-resource environments, facilitating the identification of hope speech and promoting constructive digital discourse.

## Method Summary
GHaLIB employs a pipeline that routes input text by language to specialized encoders (UrduBERT for Urdu, EuroBERT for German/Spanish) whose embeddings are concatenated and processed by XLM-RoBERTa-base. The framework uses weighted cross-entropy loss with 1.5× weight on the hope class to address class imbalance, and applies post-hoc threshold tuning on validation data. Training employs Optuna hyperparameter search with 30 trials on a 70/15/15 stratified split of the PolyHope-M 2025 corpus. The model uses XLM-RoBERTa tokenization with 128-token truncation/padding.

## Key Results
- 95.2% F1-score for Urdu binary classification
- 65.2% F1-score for Urdu multi-class classification
- Competitive performance across Spanish, German, and English

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Language-specific encoders paired with a multilingual transformer backbone improve classification for low-resource languages.
- Mechanism: Input text is first routed by language to specialized encoders (UrduBERT for Urdu, EuroBERT for German/Spanish) whose embeddings are concatenated and processed by XLM-RoBERTa-base. This captures morphological and syntactic patterns that purely multilingual models may overlook.
- Core assumption: Language-specific pretraining encodes structural features that complement cross-lingual representations.
- Evidence anchors:
  - [section IV] "The pipeline begins by identifying the language of the input and assigning it to a proper contextual encoder... These encoders generate dense embeddings, which are subsequently concatenated and fed into a context-based classifier using XLM-RoBERTa-base."
  - [section V] "Language-specific models were found to be the most benefit for morphologically rich and lower-resource languages, because they can capture morphological variations and syntax structures that multilingual models often overlook."
  - [corpus] Neighbor papers report similar findings for transformer-based multilingual hope detection (FMR range 0.51–0.60), though direct comparative baselines for this specific encoder-fusion design are not available in the corpus.
- Break condition: If target language lacks a dedicated encoder, the marginal gain over vanilla XLM-RoBERTa may diminish.

### Mechanism 2
- Claim: Weighted cross-entropy loss with higher weight on the hope class mitigates false negatives under class imbalance.
- Mechanism: A custom WeightedTrainer applies 1.5× weight to the positive (hope) class in cross-entropy loss, biasing gradient updates to reduce missed hope instances.
- Core assumption: The majority "Not Hope" class otherwise dominates learning, leading to high false-negative rates.
- Evidence anchors:
  - [section IV] "To handle the class imbalance, in particular the risk for false negatives, the custom WeightedTrainer comes with a cross-entropy loss where the weight is 1.5× for the positive (hope) class."
  - [section III] "The label distributions reflect an evident imbalance in both the English and Urdu datasets; Not Hope dominates the samples."
  - [corpus] Related work on multilingual hope speech mentions class imbalance as a challenge, but does not provide ablations on loss weighting for this framework.
- Break condition: If the dataset is well-balanced or false positives are costlier than false negatives, this weighting may hurt precision.

### Mechanism 3
- Claim: Post-hoc threshold tuning on validation data improves the false negative / false positive trade-off without changing model ranking.
- Mechanism: After training, classification thresholds between 0.3 and 0.8 are tuned on the validation set to adjust decision boundaries.
- Core assumption: The default 0.5 threshold is suboptimal for imbalanced or asymmetric-cost settings.
- Evidence anchors:
  - [section IV] "After training, the model classification threshold, ranging between 0.3 and 0.8, is tuned on the validation set to optimally balance the trade-off between false negatives and false positives."
  - [section V] "Tuning thresholds on the validation data yielded small gains in the balance of false negatives to false positives, while producing no overall change in model rankings."
  - [corpus] No direct corpus evidence comparing threshold tuning vs. fixed threshold for this task.
- Break condition: If validation distribution diverges significantly from test distribution, tuned thresholds may not generalize.

## Foundational Learning

- Concept: **Transformer tokenization and subword vocabularies**
  - Why needed here: The pipeline uses XLM-RoBERTa tokenization with 128-token truncation/padding; understanding subword segmentation is essential for debugging preprocessing issues.
  - Quick check question: Can you explain why "unrealistic" might be split into multiple tokens and how that affects embedding aggregation?

- Concept: **Class imbalance and weighted loss**
  - Why needed here: The framework explicitly applies 1.5× weight to the hope class; knowing how loss weighting shifts gradients helps diagnose unexpected precision/recall patterns.
  - Quick check question: If you increase the hope-class weight from 1.5× to 3.0×, what would you expect to happen to recall and precision?

- Concept: **Threshold tuning vs. probability calibration**
  - Why needed here: The authors tune thresholds post-training; distinguishing this from calibrated probabilities prevents misinterpreting raw scores as confidences.
  - Quick check question: After threshold tuning, does a score of 0.6 above threshold 0.55 represent the same confidence as 0.6 above threshold 0.4?

## Architecture Onboarding

- Component map:
  - Language router → language-specific encoder (UrduBERT / RoBERTa / EuroBERT) → embedding concatenation → XLM-RoBERTa-base classifier → threshold layer → prediction
  - Training: WeightedTrainer with 1.5× hope-class weight, Optuna hyperparameter search (30 trials)
  - Data: PolyHope-M 2025 corpus → stratified 70/15/15 split → XLM-RoBERTa tokenizer (max 128 tokens)

- Critical path:
  1. Text input → language identification
  2. Tokenization with language-appropriate tokenizer
  3. Encoder forward pass → pooled embeddings
  4. Classification head → logits → softmax → threshold comparison → label

- Design tradeoffs:
  - XLM-RoBERTa-base chosen over -large for GPU memory constraints (2×16GB T4), accepting slight accuracy drop for feasibility
  - Language-specific encoders add complexity but improve low-resource language performance
  - Simple preprocessing prioritized over complex augmentation to maintain reproducibility

- Failure signatures:
  - Low recall on hope classes despite high accuracy: check class weights and threshold settings
  - Poor performance on code-mixed inputs: language router may misassign, bypassing correct encoder
  - Large train-validation gap: potential overfitting or distribution shift; verify stratification

- First 3 experiments:
  1. Replicate baseline: Run XLM-RoBERTa-base (no language-specific encoders) on Urdu binary classification to establish a comparison point against the reported 95.0% F1 with UrduBERT.
  2. Ablate weighted loss: Train with uniform class weights (1.0× for both classes) and compare false negative rates to the 1.5× weighted configuration.
  3. Threshold sweep: On a fixed model, evaluate thresholds from 0.3 to 0.8 on validation data and plot precision-recall trade-offs to verify the authors' claim of "small gains."

## Open Questions the Paper Calls Out
None

## Limitations
- Generalizability to truly low-resource languages remains unproven beyond tested high-resource languages
- Reproducibility concerns due to lack of detailed experimental parameters and random seeds
- Reliance on language-specific encoders limits immediate applicability to languages without pretrained models

## Confidence

**High confidence**:
- The encoder-fusion architecture combining language-specific encoders with XLM-RoBERTa is technically sound
- The use of weighted cross-entropy loss to address class imbalance is a standard, well-validated approach
- The overall framework design is logically coherent and implementable

**Medium confidence**:
- The claim of "strong performance" achieving 95.2% F1 for Urdu binary classification, as no baseline comparisons are provided for the specific PolyHope-M 2025 benchmark
- The assertion that the framework facilitates "constructive digital discourse" - this is aspirational rather than empirically demonstrated
- The generalizability to other low-resource languages beyond the tested set

**Low confidence**:
- The scalability claim for truly low-resource languages lacking dedicated pretrained encoders
- The reproducibility of exact results without detailed experimental parameters
- The optimal threshold tuning strategy without comparative metrics

## Next Checks
1. **Ablation study on language-specific encoders**: Run Urdu binary classification using only XLM-RoBERTa-base (no UrduBERT fusion) and compare against the reported 95.0% F1. This would quantify the actual contribution of the language-specific encoder component and validate whether the 1.2% improvement is consistent.

2. **Weighted loss ablation**: Train the same model architecture with uniform class weights (1.0× for both classes) and compare false negative rates, precision, and recall to the 1.5× weighted configuration. This would determine whether the weighted loss provides meaningful improvement beyond standard training.

3. **Threshold tuning impact analysis**: On a fixed model, systematically evaluate classification performance across thresholds from 0.3 to 0.8, reporting F1-score, precision, and recall at each threshold. This would quantify whether threshold tuning provides "small gains" or more substantial improvements, and help determine optimal operating points for different use cases.