---
ver: rpa2
title: 'Galton''s Law of Mediocrity: Why Large Language Models Regress to the Mean
  and Fail at Creativity in Advertising'
arxiv_id: '2509.25767'
source_url: https://arxiv.org/abs/2509.25767
tags:
- original
- extreme
- expansion
- creative
- mild
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models generate fluent but generic text, especially
  in creative domains like advertising. We formalize this as Galton-style regression
  to the mean in language and test it using a staged forgetting and expansion protocol
  on 1,045 ad concepts.
---

# Galton's Law of Mediocrity: Why Large Language Models Regress to the Mean and Fail at Creativity in Advertising

## Quick Facts
- **arXiv ID**: 2509.25767
- **Source URL**: https://arxiv.org/abs/2509.25767
- **Reference count**: 16
- **Primary result**: LLMs generate fluent but generic text in creative domains like advertising, regressing to the mean without explicit creative guidance.

## Executive Summary
Large language models (LLMs) excel at generating fluent text but struggle with creativity, especially in domains like advertising. This paper formalizes this tendency as a "Galton-style regression to the mean" in language. Through a staged forgetting and expansion protocol on 1,045 ad concepts, the authors show that creative elements (metaphors, emotions, visual cues) are the first to disappear, while factual content remains. During expansion, outputs become longer but fail to recover original meaning or creativity. Providing explicit creative markers (metaphors, emotional hooks, visual details) partially mitigates this regression, improving semantic alignment and stylistic balance, though outputs still rely on familiar tropes.

## Method Summary
The authors tested Galton's Law by applying a staged forgetting and expansion protocol to 1,045 ad concepts from the 55mV database. First, they progressively removed creative elements (metaphors, emotions, visual cues) during "forgetting," observing that these disappeared before factual content. They measured semantic similarity, surface overlap, and lexical diversity, which all dropped sharply. Next, during "expansion," they prompted the model to lengthen the ads. Outputs became longer and more varied but failed to recover original meaning or creativity, with similarity metrics plateauing at low levels. Finally, they provided explicit creative markers (extracted by an LLM) to guide expansion, which improved semantic alignment and stylistic balance, though outputs still relied on familiar tropes.

## Key Results
- Creative elements (metaphors, emotions, visual cues) disappeared first during forgetting, while factual content remained.
- During expansion, outputs became longer but failed to recover original meaning or creativity, with cosine similarity plateauing at 0.22–0.29 and METEOR at 0.13–0.19.
- Providing explicit creative markers improved semantic alignment and stylistic balance, raising cosine similarity to 0.24–0.37 and METEOR to 0.13–0.26, though outputs still relied on familiar tropes.

## Why This Works (Mechanism)
Assumption: The mechanism likely involves the statistical nature of LLMs trained on next-token prediction. Creative elements are typically less frequent and more diverse in training corpora, making them statistically less probable during generation. The model's tendency to maximize likelihood leads to more generic, high-frequency patterns that represent the "mean" of its training distribution. This creates a bias toward safe, conventional outputs that lack the originality of the source material.

## Foundational Learning
Assumption: The foundational learning likely relates to how LLMs are trained on vast corpora of internet text, which contains more conventional and widely-shared content than truly creative works. This creates an implicit bias toward average, non-creative language patterns. The model learns to prioritize coherence and plausibility over novelty, as these are more strongly reinforced in the training objective of next-token prediction.

## Architecture Onboarding
Unknown: The paper does not provide specific guidance on how to adapt existing LLM architectures or fine-tuning approaches to better preserve creative elements. Based on the results, it appears that simply scaling up model size or training data does not inherently solve the regression-to-mean problem in creative domains.

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Do the observed regression patterns generalize to creative domains beyond advertising, such as literary fiction, product design, or scientific ideation?
- Basis in paper: [explicit] Conclusion lists "domain specificity (ads)" as a key limitation.
- Why unresolved: The study evaluated only advertising concepts from the 55mV database; no other creative domains were tested.
- What evidence would resolve it: Replicating the forgetting-expansion protocol on diverse creative corpora (e.g., short stories, design briefs, research hypotheses) with domain-appropriate creative markers.

### Open Question 2
- Question: Can training objectives explicitly designed to reward diversity and penalize clichés reduce regression to mediocrity?
- Basis in paper: [explicit] Discussion states future work could "strengthen creativity recovery by designing training objectives that reward diversity and penalize clichés."
- Why unresolved: Current models use next-token prediction and RLHF for safety, which suppress low-probability creative continuations; no alternative training scheme was evaluated.
- What evidence would resolve it: Fine-tuning models with diversity-enhancing or cliché-penalizing loss terms, then measuring whether creative markers persist longer during forgetting and recover more faithfully during expansion.

### Open Question 3
- Question: Would human creative professionals rate expanded outputs differently than the automated metrics and LLM judges used in this study?
- Basis in paper: [inferred] Conclusion notes "metric/LLM-judge dependence" as a limitation, and the qualitative analysis relied on hybrid LLM-human verification for only 25-100 ads.
- Why unresolved: The primary evaluation uses cosine similarity, METEOR, entropy, and 4-gram uniqueness; human judgment was limited in scope.
- What evidence would resolve it: Large-scale human evaluation by advertising creatives comparing original vs. expanded outputs on dimensions like originality, emotional resonance, and cultural relevance.

### Open Question 4
- Question: Do human-extracted creative markers guide expansion more effectively than LLM-extracted markers?
- Basis in paper: [inferred] Methodology describes markers as "extracted by an LLM with a structured prompt" (Appendix C.5), but does not compare against human curation.
- Why unresolved: The information advantage of markers is noted as a limitation, but the extraction source (human vs. LLM) was not varied.
- What evidence would resolve it: A controlled comparison where identical ads are expanded using human-curated markers versus LLM-extracted markers, measuring alignment and creative recovery.

## Limitations
- The study focused exclusively on advertising concepts from the 55mV database, limiting generalizability to other creative domains.
- Only GPT-4 was tested; results may not generalize to other models or future versions.
- Evaluation relied on automated metrics (cosine similarity, METEOR, lexical diversity), which may not fully capture nuanced aspects of creativity; human evaluation was limited.

## Confidence
- **High confidence**: The staged forgetting and expansion protocol yielded consistent, statistically significant degradation in semantic and stylistic features, providing strong internal validity.
- **Medium confidence**: The experimental design is robust, but the scope is limited to advertising concepts and controlled prompt conditions; generalization to other creative domains or real-world workflows remains uncertain.

## Next Checks
1. **Cross-model replication**: Repeat the forgetting/expansion protocol on multiple LLMs (e.g., Claude, Llama, Gemini) to assess whether regression to the mean is a general LLM phenomenon or model-specific.
2. **Human evaluation**: Conduct blinded human ratings of creative quality and originality on forgotten vs. guided outputs to validate automated metric findings.
3. **Real-world workflow test**: Integrate the creative cue protocol into a full advertising campaign generation pipeline and measure downstream creative impact and client reception.