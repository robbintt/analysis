---
ver: rpa2
title: 'Quantifying surprise in clinical care: Detecting highly informative events
  in electronic health records with foundation models'
arxiv_id: '2507.22798'
source_url: https://arxiv.org/abs/2507.22798
tags:
- asmt
- rate
- respiratory
- spo2
- heart
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method to quantify the information content
  of clinical events in electronic health records using foundation models. The approach
  leverages a pretrained transformer to estimate context-aware information for each
  event in a patient's hospitalization timeline.
---

# Quantifying surprise in clinical care: Detecting highly informative events in electronic health records with foundation models

## Quick Facts
- **arXiv ID:** 2507.22798
- **Source URL:** https://arxiv.org/abs/2507.22798
- **Reference count:** 0
- **Primary result:** Foundation model surprisal identifies clinically significant events associated with mortality and long length of stay

## Executive Summary
This paper introduces a method to quantify the information content of clinical events in electronic health records using foundation models. The approach leverages a pretrained transformer to estimate context-aware information for each event in a patient's hospitalization timeline. The authors show that highly informative events, as identified by their model, are significantly associated with negative patient outcomes such as inpatient mortality and long length of stay. Additionally, they demonstrate that removing the most informative events from a timeline degrades the performance of downstream prognostic models, while removing less informative events has minimal impact. This work highlights the potential of foundation models to surface clinically significant events and improve interpretability in healthcare AI systems.

## Method Summary
The authors develop a Llama-3.2-style transformer (67.3M parameters) trained from scratch on EHR data from MIMIC-IV and UCMC in CLIF format. Events are tokenized using a "category-value" strategy where continuous values are binned into deciles, resulting in a 208-token vocabulary. The model performs causal language modeling to predict the next token given the patient's history. Information content is quantified using Shannon self-information (surprisal), calculated as the negative log-likelihood of each event given its context. The method is validated by examining associations between high-informativeness events and outcomes, and by testing whether removing these events degrades downstream prognostic model performance.

## Key Results
- High-surprisal events identified by the foundation model show strong association with negative outcomes including inpatient mortality and long length of stay
- Removing the most informative events from a patient timeline significantly degrades downstream prognostic model performance (ROC-AUC drops), while removing less informative events has minimal impact
- The foundation model identifies surprising events that rule-based approaches would consider within normal ranges, suggesting its ability to detect clinically significant deviations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** High "surprisal" (negative log-likelihood) identifies clinically significant events better than static rule-based thresholds.
- **Mechanism:** The model is trained to minimize the expected context-aware information (cross-entropy) of the training data. During inference, if a token $x_t$ has low probability $p(x_t|x_{<t})$ given the patient's history, it yields high information content ($-\log_2 p$). This mathematically flags events that deviate from the learned distribution of clinical trajectories.
- **Core assumption:** The training data distribution $\hat{p}$ sufficiently approximates the true distribution of valid clinical sequences, implying that "surprise" correlates with clinical significance rather than noise.
- **Evidence anchors:**
  - [Abstract]: "flag anomalous events that rule-based approaches would consider within a normal range."
  - [Section 3.3]: Defines context-aware information $I_p(x_u|x_{<u}) = -\log_2 p(x_u|x_{<u})$.
  - [Corpus]: Weak direct evidence; related EHR models (e.g., PULSE-ICU) focus on prediction tasks rather than information quantification.
- **Break condition:** High levels of data entry noise or systematic distribution shift (e.g., a new hospital with different protocols) could cause high surprisal for non-clinical reasons, breaking the alignment between "surprise" and "importance."

### Mechanism 2
- **Claim:** Token information content correlates with the magnitude of change in the patient's latent representation.
- **Mechanism:** As a transformer processes a sequence, it builds a representation $R(x_{1:t})$. Informative (unexpected) tokens force the attention mechanism to update the hidden state significantly to accommodate the new, unpredicted reality, resulting in a larger path length $\Delta$ in the latent space.
- **Core assumption:** The model's latent space is smooth and semantically organized such that Euclidean distance (or path length) corresponds to meaningful shifts in patient state.
- **Evidence anchors:**
  - [Section 3.6]: Defines path length $\Delta_{u:v}$ and jump magnitude $\Delta_t$.
  - [Section 4.3]: Reports a regression $\hat{\beta} = 0.548, p < 0.001$ between informativeness and representation change, though $R^2$ is 0.212.
  - [Corpus]: Not explicitly covered in neighbor abstracts.
- **Break condition:** If the model is under-trained or the embedding space is isotropic/random, representation shifts may not correlate with information.

### Mechanism 3
- **Claim:** Removing high-information events degrades downstream prognostic performance, establishing a causal link between "surprise" and outcome prediction.
- **Mechanism:** A linear probe (logistic regression) uses the FM's representation to predict outcomes (e.g., mortality). If high-information tokens are removed, the representation $R(x)$ lacks the critical updates provided by those tokens, reducing the probe's accuracy. Removing low-information tokens retains the "gist" of the trajectory.
- **Core assumption:** The linear probe relies on the specific information encoded by the surprising events, and the FM does not simply memorize outcomes independent of the sequence.
- **Evidence anchors:**
  - [Abstract]: "removing the most informative events... degrades the performance... while removing less informative events has minimal impact."
  - [Table 1]: Shows significant drops in ROC-AUC when top 20-40% informative events are redacted ($p < 0.01$).
  - [Corpus]: MedFuse discusses embedding fusion for time series, implying feature importance is a known challenge in EHR, supporting the need for selection.
- **Break condition:** The downstream task is trivial or learned entirely from static demographics (prefix), making event redaction irrelevant.

## Foundational Learning

- **Concept: Shannon Self-Information**
  - **Why needed here:** This is the mathematical definition of "surprise" used in the paper. You must understand that low probability = high information.
  - **Quick check question:** If a model predicts a token with probability 1.0 (100%), what is its information content in bits?

- **Concept: Causal Language Modeling (CLM)**
  - **Why needed here:** The architecture (GPT-style) predicts the next token based strictly on past context. This unidirectional flow is required to calculate the conditional probability of an event given prior history.
  - **Quick check question:** Why would a bidirectional model (like BERT) be unsuitable for calculating the sequential "surprise" of a new lab result arriving in real-time?

- **Concept: Category-Value Tokenization**
  - **Why needed here:** The paper uses a specific tokenization scheme (e.g., `LAB_glucose` + `Q9`) rather than raw floats. Understanding this is key to interpreting the vocabulary and the model's inputs.
  - **Quick check question:** How does binning a continuous value into deciles (Q0-Q9) potentially lose information versus using raw values?

## Architecture Onboarding

- **Component map:** EHR Data (CLIF format) -> Tokenizer (Category + Decile bins) -> Vocabulary (208 tokens) -> Llama-3.2 Transformer (8 layers, 1024 hidden size, 67M params) -> Output Layer (Logits over Vocabulary) -> Softmax probabilities -> Surprisal Calculator (-log p) -> Representation Extractor (Last hidden state) -> Linear Probe (Logistic Regression)

- **Critical path:** The calculation of $p(x_t|x_{<t})$. The model must be loaded in inference mode to generate logits for the *next* token at every step $t$ in the sequence.

- **Design tradeoffs:**
  - **Decile Binning:** Reduces vocabulary size and handles sparsity, but destroys fine-grained numerical distinctions (e.g., glucose 250 vs 260 might be in the same bin).
  - **No Time Tokens:** The model relies on sequence order, not explicit timestamps. This simplifies the model but may fail to capture the urgency of rapid successive events vs. sparse events.

- **Failure signatures:**
  - **Uniform Surprisal:** If the model is under-trained or the vocabulary is too small, it may output uniform probabilities, making all events look equally "surprising."
  - **Prefix Dominance:** If the model relies mostly on static demographics (age/race) in the prefix, redacting events later in the sequence will show no change in downstream AUC, breaking the paper's hypothesis.

- **First 3 experiments:**
  1. Verify Tokenization: Take a sample patient timeline from MIMIC-IV, apply the category-value tokenization (calculating deciles from training stats), and ensure the sequence matches the paper's format.
  2. Surprisal Correlation: Train a small instance of the model, calculate the surprisal for a validation set, and check if high-surprisal tokens visually correspond to abnormal lab values or death (as per Fig 2-4).
  3. Redaction Test: Extract 24-hour representations from the frozen model. Train a logistic regression on "Original" vs. "Top 20% Redacted" timelines to verify the AUC drop reported in Table 1.

## Open Questions the Paper Calls Out

- **Can foundation model-derived informativeness metrics effectively reduce clinical alarm fatigue in a live deployment compared to standard rule-based thresholds?**
  - **Basis in paper:** [explicit] The Discussion states the metric "could provide a data-driven solution to clinical alarm fatigue by implementing dynamic alerting systems that prioritize notifications based on contextual surprise."
  - **Why unresolved:** The current work validates the association between high-informativeness events and outcomes retrospectively but does not test the efficacy of suppressing low-information alerts in a real-time clinical workflow.
  - **What evidence would resolve it:** A prospective trial measuring clinician response times and "alarm fatigue" metrics when using the FM-based filter versus standard static thresholds.

- **How can the model disambiguate between "surprise" caused by data entry errors versus surprise caused by genuine unexpected changes in patient condition?**
  - **Basis in paper:** [explicit] The Introduction notes that divergence indicates "practice variation," "unexpected change in patient condition," or "issues of data quality," and the Discussion proposes the method could "identify contextually implausible entries."
  - **Why unresolved:** The paper treats all high-information events similarly; it does not propose or validate a mechanism to separate "clean" surprising events (pathology) from "noisy" ones (typos).
  - **What evidence would resolve it:** A study performing manual chart review on high-informativeness events to calculate the precision with which the model flags data entry errors versus clinical deterioration.

## Limitations
- The foundational assumption that surprisal correlates with clinical importance relies heavily on the model's training distribution matching real clinical practice, which may not hold across different clinical settings or time periods.
- The use of decile binning for continuous values introduces systematic information loss that may affect surprisal calculations for borderline cases.
- The downstream degradation test demonstrates statistical significance but doesn't establish clinical relevance of the AUC drops or compare against simpler baselines.

## Confidence
- **High Confidence:** The mathematical framework for calculating context-aware information (Shannon self-information) is sound and correctly implemented. The tokenization approach and CLM architecture are appropriate for the stated task.
- **Medium Confidence:** The association between high-surprisal events and negative outcomes is demonstrated statistically but requires further validation to confirm clinical utility. The redaction experiment methodology is sound, but the practical significance of the performance degradation needs clarification.
- **Low Confidence:** The interpretation that representation path length directly reflects semantic change in patient state, and that surprisal reliably identifies the most clinically important events across diverse settings, lacks sufficient empirical support.

## Next Checks
1. **Distribution Shift Validation:** Test the surprisal metric on a temporally distinct validation set from the same hospital system or a different hospital system to quantify how well the surprisal-outcome correlation holds under distribution shift.

2. **Information Loss Quantification:** Systematically vary the binning strategy (different numbers of bins, adaptive binning) and measure the impact on both surprisal calculations and downstream task performance to characterize the tradeoff between vocabulary size and information fidelity.

3. **Baseline Comparison:** Implement a simple rule-based surprisal measure using static event frequencies (without the FM) and compare its ability to identify outcome-associated events against the foundation model approach to establish the added value of the FM.