---
ver: rpa2
title: Contrastive Decoding Mitigates Score Range Bias in LLM-as-a-Judge
arxiv_id: '2510.18196'
source_url: https://arxiv.org/abs/2510.18196
tags:
- score
- b-inst
- contrastive
- range
- llama
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies a new bias in LLM-as-a-judge systems called
  "score range bias," where judges produce outputs that are sensitive to the predefined
  score ranges, leading to inconsistent evaluations. The bias is observed across different
  model sizes and families (Llama-3 and Qwen-2.5), with models tending to favor specific
  scores regardless of the quality of the summaries.
---

# Contrastive Decoding Mitigates Score Range Bias in LLM-as-a-Judge

## Quick Facts
- arXiv ID: 2510.18196
- Source URL: https://arxiv.org/abs/2510.18196
- Reference count: 35
- Primary result: Contrastive decoding achieves up to 11.3% relative improvement in Spearman correlation with human judgments by mitigating score range bias

## Executive Summary
This paper identifies a critical bias in LLM-as-a-judge systems called "score range bias," where judges produce outputs sensitive to predefined score ranges rather than actual summary quality. The bias manifests as systematic preferences for specific scores (e.g., Score 4 for Llama-3, Score 2 for Qwen-2.5) regardless of input quality. The proposed solution uses contrastive decoding, which combines a main model and assistant model from the same family to cancel shared bias patterns while preserving quality-discriminative signals. This approach achieves significant improvements in evaluation consistency across different score ranges.

## Method Summary
The method uses contrastive decoding to mitigate score range bias by computing log p_main − λ · log p_asst, where the main model (larger, same family as assistant) generates logits that are adjusted by subtracting weighted logits from the assistant model. The technique assumes both models share similar bias patterns that can be canceled through subtraction. Hyperparameters λ (weighting factor) and temperature t are tuned per model-range combination on a held-out development set. The approach was tested on the SummEval benchmark using Llama-3.1 and Qwen-2.5 model families across four score ranges (0-4, 1-5, 2-6, 3-7).

## Key Results
- Score range bias exists across different model sizes and families, with models favoring specific scores regardless of summary quality
- Contrastive decoding achieves up to 11.3% relative improvement in Spearman correlation with human judgments
- The bias mitigation is consistent across different score ranges and evaluation dimensions (coherence, relevance, consistency)
- Family-shared bias patterns enable effective cancellation when using assistant models from the same family

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM judges exhibit systematic score range bias where token logit distributions favor specific numerical scores regardless of input quality.
- Mechanism: The first output token's logit distribution shows strong, consistent peaks at certain scores that do not shift proportionally with actual summary quality, creating evaluation outputs that reflect model priors rather than reasoned assessment.
- Core assumption: The bias originates from tokenization patterns and training distribution effects rather than deliberate evaluation reasoning.
- Evidence anchors: [abstract] "LLM judge outputs are highly sensitive to pre-defined score ranges", [section 4.2, Figure 3] Qwen-2.5 models show highest logits for Score 2 across sizes while human annotations peak at Score 3.

### Mechanism 2
- Claim: Models from the same family encode similar score range biases with magnitudes that scale with model size.
- Mechanism: Shared pre-training data and architecture create analogous inductive biases in token prediction, with larger models showing reduced but persistent bias patterns.
- Core assumption: Family similarity in bias patterns is bidirectional and can be leveraged for cancellation.
- Evidence anchors: [abstract] "similar biases exist among models from the same family", [section 4.2] "Llama family models (3B and 8B) tend to output score of 4 and Qwen 2.5 family models tend to output score of 2".

### Mechanism 3
- Claim: Contrastive decoding removes shared bias components while preserving quality-discriminative signal.
- Mechanism: The adjusted score log p_main − λlog p_asst subtracts the assistant model's logit distribution (weighted by λ) from the main model, canceling common bias while preserving differential signals where the larger model has better quality discrimination.
- Core assumption: The main model has superior quality assessment capability but shares similar bias patterns with the assistant.
- Evidence anchors: [abstract] "achieving up to 11.3% relative improvement on average in Spearman correlation with human judgments", [section 3, Equation 1] Explicit formula with λ inclusion "to align the logit distribution between the two models".

## Foundational Learning

- Concept: Logit distributions and temperature scaling
  - Why needed here: Understanding how contrastive decoding operates on log probabilities and how temperature controls distribution sharpness before subtraction.
  - Quick check question: Given logits [1.0, 2.0, 3.0] for scores [1, 2, 3] and temperature T=2.0, which score has highest probability after softmax?

- Concept: Rank correlation metrics (Spearman, Pearson, Kendall)
  - Why needed here: The paper reports alignment with human judgments using three correlation measures; understanding their differences clarifies what improvement means.
  - Quick check question: Why might Spearman correlation be more appropriate than Pearson when evaluating LLM judge alignment with human ordinal rankings?

- Concept: Direct assessment vs pairwise comparison in LLM evaluation
  - Why needed here: The paper specifically addresses direct assessment (assigning scores without references), where score range bias is most problematic.
  - Quick check question: What structural difference makes pairwise comparison less susceptible to score range bias than direct assessment?

## Architecture Onboarding

- Component map:
  Input processor -> Main model -> Assistant model -> Temperature module -> Contrastive module -> Output parser

- Critical path:
  1. Identical prompt → both models simultaneously
  2. Extract first-token logits from each model
  3. Apply temperature scaling to both
  4. Compute contrastive log-probabilities
  5. Softmax and extract highest-probability score token

- Design tradeoffs:
  - λ magnitude: Higher λ removes more bias but risks removing quality signal; paper uses λ∈[0.01, 1.0]
  - Temperature: Higher values smooth distributions (more exploration), lower values sharpen (more exploitation); paper uses t∈[0.5, 5.0]
  - Assistant model size: Smaller assistants have lower compute but may have less aligned bias patterns; 1B vs 3B showed marginal differences

- Failure signatures:
  - Negative correlation: Assistant model bias pattern inverted relative to main model
  - Parsing failures: Model outputs non-numeric text despite prompt instructions (paper sets to lowest score)
  - Range-specific degradation: Performance collapses on non-standard ranges (e.g., 3-7) without contrastive decoding

- First 3 experiments:
  1. Baseline bias quantification: Run main model with greedy decoding across all score ranges on SummEval; plot output score distributions vs human annotations to visualize bias.
  2. Hyperparameter grid search: On 10% held-out dev set, sweep λ∈[0.01, 0.1, 0.5, 1.0] and t∈[0.5, 1.0, 2.0, 3.0, 4.0, 5.0] for each score range; select by Spearman correlation.
  3. Ablation on assistant selection: Compare same-family assistant (Llama-8B + Llama-3B) vs different-family assistant (Llama-8B + Qwen-3B) to validate family-shared bias hypothesis; expect cross-family to fail or degrade.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the mitigation of score range bias via contrastive decoding generalize to evaluation tasks other than text summarization?
- Basis in paper: [explicit] The conclusion explicitly identifies "tasks beyond summarization" as a necessary future research direction.
- Why unresolved: The study exclusively utilized the SummEval dataset for summarization, leaving other generative tasks untested.
- What evidence would resolve it: Applying the contrastive decoding method to direct assessment tasks in translation, RAG answer generation, or dialogue.

### Open Question 2
- Question: Is contrastive decoding effective for models specifically fine-tuned for evaluation tasks, such as Prometheus?
- Basis in paper: [explicit] Section 4.1 states the authors leave "models like Prometheus... as future work" because suitable model sizes were unavailable.
- Why unresolved: The experiments relied on general instruction-tuned models rather than judge-specialized models, which may exhibit different bias characteristics.
- What evidence would resolve it: Experiments applying contrastive decoding to judge-specific fine-tuned models of varying sizes to see if shared family biases persist.

### Open Question 3
- Question: Can lightweight approaches like prompt engineering achieve bias mitigation comparable to contrastive decoding without the extra inference cost?
- Basis in paper: [explicit] The conclusion recommends "investigating alternative lightweight approaches e.g., prompt engineering on evaluation rubrics."
- Why unresolved: Contrastive decoding requires two forward passes, increasing test-time compute; prompt engineering offers a cheaper, untested alternative.
- What evidence would resolve it: Comparative analysis of score range bias between contrastive decoding and carefully optimized rubric-based prompts.

### Open Question 4
- Question: Does the efficacy of this method scale to models with parameters significantly larger than 14B?
- Basis in paper: [explicit] The conclusion lists "scaling to models beyond 14B parameters" as a future research direction.
- Why unresolved: Computational budget constraints limited the study to models up to 14B parameters (Qwen-2.5-14B).
- What evidence would resolve it: Replicating the contrastive decoding experiments using large-scale models (e.g., 70B+ parameters) to verify consistent bias mitigation.

## Limitations

- The mechanism for why contrastive decoding cancels shared bias patterns lacks rigorous mathematical proof, relying on empirical observations rather than theoretical guarantees.
- The evaluation focuses solely on direct assessment in summarization tasks, leaving performance on other evaluation paradigms or domains untested.
- The family-shared bias hypothesis, while supported by observed patterns in Llama and Qwen models, lacks systematic cross-family validation and may not generalize to all model families.

## Confidence

- High Confidence: The existence of score range bias itself is well-supported by empirical observations across multiple model sizes and families.
- Medium Confidence: The mechanism explaining why contrastive decoding works is plausible but relies on assumptions about model family similarities that weren't rigorously tested.
- Low Confidence: Claims about general applicability to other evaluation tasks, domains, or model families extend beyond the empirical evidence.

## Next Checks

1. Cross-Domain Validation: Test contrastive decoding on non-summarization tasks (code evaluation, dialogue quality, mathematical reasoning) to determine if score range bias is domain-specific or universal.

2. Ablation on Model Families: Systematically test contrastive decoding with assistant models from different families (e.g., Llama-8B + Qwen-3B vs. Llama-8B + Llama-3B) to quantify the importance of family-shared bias patterns.

3. Theoretical Analysis of Logit Cancellation: Develop a mathematical framework showing under what conditions subtracting similarly-biased logit distributions cancels shared components while preserving differential signals.