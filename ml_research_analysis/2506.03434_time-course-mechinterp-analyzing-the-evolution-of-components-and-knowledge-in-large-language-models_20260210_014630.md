---
ver: rpa2
title: 'Time Course MechInterp: Analyzing the Evolution of Components and Knowledge
  in Large Language Models'
arxiv_id: '2506.03434'
source_url: https://arxiv.org/abs/2506.03434
tags:
- proper
- heads
- count
- ffns
- relation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work analyzes the evolution of factual knowledge circuits
  in OLMo-7B during pre-training using Information Flow Routes to trace attention
  heads and FFNs across 40 training snapshots. Components are classified into general,
  entity, relation-answer, and fact-answer roles, revealing that LLMs first rely on
  stable general-purpose components before gradually developing specialized ones.
---

# Time Course MechInterp: Analyzing the Evolution of Components and Knowledge in Large Language Models

## Quick Facts
- **arXiv ID**: 2506.03434
- **Source URL**: https://arxiv.org/abs/2506.03434
- **Reference count**: 40
- **Primary result**: Analyzes how attention heads and FFNs evolve in roles during OLMo-7B pre-training using Information Flow Routes across 40 snapshots

## Executive Summary
This paper investigates how factual knowledge circuits form and evolve during pre-training of OLMo-7B. Using Information Flow Routes to trace computational pathways across 40 training snapshots, the authors classify components into general, entity, relation-answer, and fact-answer roles. They find that LLMs initially rely on stable general-purpose components before gradually developing specialized ones, with attention heads showing high turnover and FFNs remaining largely stable. The study reveals that location-based relations are learned faster than name-based relations, demonstrating how task complexity shapes acquisition dynamics.

## Method Summary
The authors analyze 40 OLMo-7B checkpoints (S1-20B to S40-838B tokens) using Information Flow Routes (IFRs) to trace which attention heads and FFNs contribute to factual knowledge predictions. For each of 160 validated facts across 10 relations, they extract IFRs at three token positions (SUBJECT, END, ANSWER) and compute component activation scores. Components are classified into proper roles using hierarchical exclusion (e.g., proper entity roles exclude general components), and their stability is measured via intersection-over-union (IoU) with final-model components. Role transitions are tracked using Markov chains to understand specialization dynamics.

## Key Results
- LLMs first depend on broad, general-purpose components that later specialize as training progresses
- Attention heads show high turnover with frequent role changes, especially in early and late layers, while FFNs remain largely stable
- Location-based relations converge to high accuracy earlier in training than name-based relations
- Components classified as general show high IoU with final-model components, while answer-specific attention heads have IoU of only 0.2

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Component Specialization
Early training relies on general-purpose components that gradually specialize into task-specific roles as the model achieves reliable prediction accuracy. Attention heads begin with broad, context-independent contributions (general role). As training progresses and accuracy exceeds ~80% (around S7-S8), components diversify into specialized roles—entity, relation-answer, and fact-answer. The Markov chain modeling shows specialized heads tend to transition toward more general roles over time, though net specialization increases because emergence outpaces reversion.

### Mechanism 2: Differential Stability Between Attention Heads and FFNs
FFNs provide a relatively stable processing backbone while attention heads exhibit frequent reallocation, particularly in answer-specific roles. With only 32 FFNs versus 1024 attention heads, FFNs show high IoU consistency and rarely maintain specialized functions—they tend to revert to general roles. Attention heads, especially answer-specific ones, display high turnover (IoU of only 0.2 between the final checkpoint and its predecessor), indicating dynamic repurposing.

### Mechanism 3: Task Complexity Modulates Acquisition Speed and Circuit Dynamics
Relation types with smaller candidate vocabularies (locations) are acquired faster and show more stable circuit formation than those with larger vocabularies (person names). Location-based relations reach 80% top-1 accuracy at S5 versus S14 for name-based relations. Name-based tasks engage more attention heads in mid-layers (10-18) and exhibit higher switching rates. The paper attributes this to fewer prominent locations versus many prominent person names in training data, making correct ranking harder for NAME relations.

## Foundational Learning

- **Concept: Information Flow Routes (IFRs)**
  - Why needed here: The paper uses IFRs (Ferrando & Voita, 2024) as the primary method for tracing computational pathways through the transformer, identifying which attention heads and FFNs contribute to predictions at each snapshot.
  - Quick check question: How does the ALTI-based contribution threshold (θ) determine which edges are retained in an IFR?

- **Concept: Circuit Analysis and Minimal Subgraphs**
  - Why needed here: The paper frames factual knowledge retrieval as emerging from circuits—minimal computational subgraphs of attention heads and FFNs that reproduce task behavior. Understanding this framing is essential for interpreting role classifications.
  - Quick check question: What distinguishes a "minimal faithful" circuit from the full model computation for a factual recall task?

- **Concept: Role Classification Taxonomy (General → Entity → Relation-Answer → Fact-Answer)**
  - Why needed here: The paper defines five roles with hierarchical exclusion (e.g., "proper" entity roles exclude general-role components). These classifications drive all IoU and transition analyses.
  - Quick check question: Why does the "proper" designation (e.g., H_e = J_e − J_g) matter for avoiding overcounting general components?

## Architecture Onboarding

- **Component map**: OLMo-7B: 32 layers × 32 attention heads (1024 total) + 32 FFNs (one per layer)
- **Critical path**: Load OLMo-7B snapshots S5K–S200K (40 snapshots) → Build probing dataset (160 facts over 10 relations) → Extract IFRs per fact/snapshot → Compute component role activation scores → Classify into proper roles → Compute IoU vs. final model → Track role transitions using Markov chains
- **Design tradeoffs**: Higher FFN threshold (0.9 vs. 0.1) may artificially inflate "general" classifications; IFRs chosen over activation patching for scalability but may miss finer-grained interactions; aggregating FFNs as layer-level units precludes neuron-level analysis
- **Failure signatures**: Answer-specific attention heads with persistently low IoU (0.2) indicate ongoing plasticity; high switching rates in mid-layers (10-18) for NAME relations suggest unstable circuits; FFNs oscillating between general and relation-answer roles around S7-S8 indicate transitional uncertainty
- **First 3 experiments**:
  1. **Single-snapshot IFR replication**: Extract IFRs for 10 facts from one relation type at S20-419B using the paper's threshold and verify role classifications
  2. **Head-tracking across checkpoints**: Select 5 attention heads classified as answer-specific at S20 and track their IoU and role transitions across S20 → S25 → S30 → S35 → S40
  3. **LOC vs. NAME learning curve comparison**: Plot top-1 and top-10 accuracy for CITY_IN_COUNTRY versus BOOKS_WRITTEN across 10 snapshots to verify the ~9-step acquisition gap (S5 vs. S14 for 80% accuracy)

## Open Questions the Paper Calls Out

- **Open Question 1**: Do finer-grained neuron-level dynamics within FFNs exhibit similar hierarchical specialization patterns to attention heads, or do they show different turnover and stability characteristics?
- **Open Question 2**: How well do the observed component evolution patterns generalize across different model architectures, scales, and training regimes?
- **Open Question 3**: Do the observed attention head specialization dynamics and FFN stability patterns predict model behavior during fine-tuning, pruning, or continual learning?

## Limitations
- High FFN activation threshold (0.9) may artificially inflate "general" classifications, obscuring true FFN specialization patterns
- Aggregating FFNs at layer level prevents neuron-level analysis, potentially missing fine-grained specialization patterns
- The 160-fact probing dataset covers only 10 relations and may not generalize to broader knowledge domains or natural query distributions

## Confidence
- **High Confidence**: Differential stability between attention heads and FFNs (IoU of 0.2 vs. high stability for FFNs)
- **Medium Confidence**: Hierarchical specialization hypothesis (general → specific roles with net specialization increase)
- **Medium Confidence**: Task complexity effect on acquisition timing (LOC vs. NAME relations showing different learning curves)

## Next Checks
1. **Threshold Sensitivity Analysis**: Re-run IFR extraction with varying path thresholds (θ = 0.05, 0.1, 0.2) and FFN activation thresholds (0.7, 0.8, 0.9) to assess classification stability
2. **Cross-Relation Generalization Test**: Apply the analytical framework to different relation types (e.g., numerical facts, temporal relations) to verify whether LOC vs. NAME dynamics generalize
3. **Fine-grained FFN Analysis**: Track individual neurons within FFNs rather than treating them as monolithic layer units to test whether apparent stability masks underlying specialization patterns