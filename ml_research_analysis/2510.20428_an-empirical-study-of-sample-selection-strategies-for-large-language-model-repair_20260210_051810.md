---
ver: rpa2
title: An Empirical Study of Sample Selection Strategies for Large Language Model
  Repair
arxiv_id: '2510.20428'
source_url: https://arxiv.org/abs/2510.20428
tags:
- repair
- saps
- selection
- data
- sample
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of efficiently repairing toxic
  behaviors in large language models (LLMs) while preserving general capabilities.
  The authors systematically evaluate five sample selection strategies for behavioral
  repair, including random sampling, K-Center, gradient-norm-based selection (GraNd),
  stratified coverage (CCS), and a novel Semantic-Aware Prioritized Sampling (SAPS)
  method that targets boundary samples in semantic space.
---

# An Empirical Study of Sample Selection Strategies for Large Language Model Repair

## Quick Facts
- **arXiv ID**: 2510.20428
- **Source URL**: https://arxiv.org/abs/2510.20428
- **Authors**: Xuran Li; Jingyi Wang
- **Reference count**: 40
- **Primary result**: Semantic-Aware Prioritized Sampling (SAPS) achieves the best balance between detoxification, utility preservation, and efficiency by prioritizing boundary samples in semantic space

## Executive Summary
This study systematically evaluates five sample selection strategies for repairing toxic behaviors in large language models while preserving general capabilities. The authors introduce SAPS, a boundary-aware sampling method that clusters repair data in semantic embedding space and prioritizes samples farthest from cluster centroids. Experimental results demonstrate that SAPS achieves superior or comparable repair outcomes using substantially less data than full-dataset approaches, with computational efficiency far exceeding gradient-based and coverage-stratified alternatives. The research establishes that selection strategy significantly impacts repair effectiveness and should be treated as a tunable component in repair pipelines.

## Method Summary
The study evaluates five sample selection strategies (random, K-Center, GraNd, CCS, and SAPS) across three model scales (GPT-2 variants and Pythia) and five repair methods (DAPT, DPO, IRepair with and without KL regularization). SAPS works by embedding repair data, reducing dimensionality, clustering, and selecting samples with largest distances to cluster centroids. Models are evaluated on toxicity reduction using RealToxicityPrompts, language modeling quality via perplexity on WikiText-2 and LAMBADA, and composite metrics (RPS, RES, OPS) that balance repair effectiveness against computational efficiency.

## Key Results
- SAPS consistently outperforms or matches full-dataset repair while using substantially less data across multiple model-method combinations
- Computational efficiency varies dramatically: SAPS requires seconds while CCS/GraNd require thousands of seconds for sample selection
- Selection strategy effectiveness is modulated by model scale and repair method architecture, with larger models and preference-based methods showing less sensitivity to selection quality
- Boundary sample emphasis varies by method: 100% boundary optimal for DAPT/KL, 25% for IRepair without KL regularization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Selecting boundary samples (peripheral to cluster centroids) provides disproportionate corrective signal for behavioral repair compared to representative center samples.
- Mechanism: SAPS clusters repair data in semantic embedding space, then prioritizes samples with largest distance to their cluster centroids. These boundary cases encode ambiguous or rare behaviors and exert stronger corrective gradients during repair optimization.
- Core assumption: Samples near decision boundaries carry higher diagnostic value for fixing undesirable behaviors than prototypical examples.
- Evidence anchors:
  - [abstract] "SAPS approach that prioritizes boundary samples identified through clustering"
  - [Section IV-D] "For DAPT on GPT-2 XL, toxicity decreases from 21.78 with no boundary samples to 11.27 when the dataset consists entirely of boundary points"
  - [corpus] Weak direct corpus support; boundary-focused selection in active learning (Nguyen & Smeulders, Ertekin et al.) provides indirect grounding but not for LLM repair specifically
- Break condition: If undesirable behaviors are encoded in prototypical regions rather than boundaries, boundary sampling would underperform center sampling.

### Mechanism 2
- Claim: Lightweight, model-agnostic selection (SAPS, random) outperforms gradient-based methods (GraNd) and coverage-stratified methods (CCS) in repair efficiency because high selection overhead negates downstream gains.
- Mechanism: GraNd requires per-sample gradient computation through the full model; CCS requires iterative scoring and stratified binning. Their selection time can exceed the repair time itself (e.g., CCS requires >13,000 seconds for selection on GPT-2 Large IRepair, vs. ~4 seconds for SAPS).
- Core assumption: The marginal improvement in repair quality from sophisticated selection does not justify computational cost when simpler methods achieve comparable RPS.
- Evidence anchors:
  - [Section IV-B] "CCS requires more than 13000 seconds merely to identify samples for IRepair on GPT-2 Large, a duration that already exceeds the subsequent repair phase itself"
  - [Table V] SAPS total time (selection + repair) consistently lower than full dataset baseline across methods
  - [corpus] "Sensitivity of Stability" paper notes adaptive data selection can be unstable in transfer learning; does not directly address computational overhead in repair
- Break condition: If repair budgets are unconstrained and quality gains are critical regardless of cost, high-overhead methods may become viable.

### Mechanism 3
- Claim: Selection strategy effectiveness is modulated by model scale and repair method architecture—larger models and preference-based methods (DPO) are less sensitive to selection quality.
- Mechanism: Larger models (Pythia-2.8B) have greater capacity to extract signal from diverse or noisy subsets, reducing dependence on curated selection. DPO optimizes via relative preference comparisons rather than absolute representational content, making it robust to sample composition.
- Core assumption: Method-specific learning dynamics determine selection sensitivity; continued pre-training methods derive signal from sample content, while preference methods derive signal from pairwise comparisons.
- Evidence anchors:
  - [Section IV-A] "For GPT-2 XL under DPO, perplexity on Lambada varies only modestly between 28.90 and 29.65, and toxicity rates remain clustered in a narrow band between 36% and 41% across strategies"
  - [Section IV-C] "For the larger Pythia model, a higher data proportion of 70% to 90% is generally beneficial, as its greater capacity allows it to convert more data into better performance"
  - [corpus] No direct corpus validation of scale-selection interaction in repair context
- Break condition: If model scale increases to extreme sizes (100B+ parameters), even random sampling may become universally sufficient, eroding SAPS advantage.

## Foundational Learning

- **Coreset Selection / Data Pruning**
  - Why needed here: The entire paper is about selecting small subsets (Dp) from full repair datasets (D) that preserve repair effectiveness. Understanding how to construct representative or informative subsets is foundational.
  - Quick check question: Can you explain why K-Center greedy selects iteratively by maximum minimum distance rather than random sampling within clusters?

- **Post-Hoc Model Repair vs. Fine-Tuning**
  - Why needed here: The paper distinguishes repair (targeted behavioral correction) from indiscriminate fine-tuning. Repair methods like IRepair localize to specific layers; this affects how selection strategies interact with the optimization process.
  - Quick check question: What is the difference between full-parameter repair (DAPT) and component-aware repair (IRepair), and why might selection matter more for one than the other?

- **Perplexity as Language Modeling Quality Proxy**
  - Why needed here: The paper evaluates repair not just on toxicity reduction but on preserving general capabilities via perplexity on WikiText-2 and LAMBADA. Understanding what perplexity measures (and doesn't) is essential for interpreting OPS/RPS tradeoffs.
  - Quick check question: Why might a repair method that minimizes toxicity simultaneously increase perplexity, and what does the OPS metric capture about this tradeoff?

## Architecture Onboarding

- **Component map:**
  ```
  Repair Data (D) → Embedding Encoder E(·) → Dimensionality Reduction R(·) 
       → Clustering C(·, K) → Boundary Distance Ranking → SAPS Subset (Dp)
       → Repair Method (DAPT/DPO/IRepair) → Repaired Model (f_Dp)
  ```

- **Critical path:** The clustering-to-boundary-selection pipeline (SAPS) is the novel contribution. If clustering fails to capture semantic structure, or if boundary samples don't encode repair-relevant information, downstream repair degrades. The choice of external encoder E(·) matters—it must provide semantically meaningful embeddings without depending on the model being repaired.

- **Design tradeoffs:**
  - **Selection overhead vs. repair quality:** SAPS adds ~3-10 seconds overhead; CCS/GraNd add thousands of seconds. Only justified if RPS/OPS improvements are substantial.
  - **Data proportion vs. saturation:** For GPT-2 XL, 30-50% data often achieves peak RPS; for Pythia-2.8B, 70-90% is beneficial. Tuning required per model-method pair.
  - **Boundary emphasis vs. coverage:** Pure boundary sampling (100%) works for DAPT/KL-regularized methods; IRepair (without KL) performs best with 25% boundary—method-specific calibration needed.

- **Failure signatures:**
  - **CCS/GraNd selection time exceeds repair time:** Indicates selection overhead dominates; switch to SAPS or random.
  - **RPS < 0 (negative values in Table IV):** Selection subset produces worse repair than full data; selection strategy is inappropriate for that model-method combination.
  - **Perplexity spikes while toxicity drops modestly:** Over-repair on detoxification at cost of general capabilities; reduce data proportion or switch to KL-regularized repair method.

- **First 3 experiments:**
  1. **Baseline sanity check:** Run full-dataset repair (no selection) on your target model with DAPT and IRepair. Establish toxicity, perplexity, and runtime baselines.
  2. **SAPS vs. Random at 50%:** Implement SAPS (off-the-shelf sentence encoder + K-means + distance ranking) and compare against random sampling at α=0.5. Measure RPS, OPS, and total runtime.
  3. **Boundary proportion sweep:** For your best-performing repair method from step 2, vary boundary sample ratio from 0% to 100% in 25% increments. Identify optimal mixture for your model scale.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can an automated framework be developed to dynamically select or hybridize data prioritization strategies based on model scale and repair objectives?
- Basis in paper: [explicit] The authors propose future work on "Automated Strategy Selection" and "Hybrid Selection Strategies" to handle the context-dependence of optimal choices (Section V.B).
- Why unresolved: Current findings show optimal strategies (e.g., SAPS vs. Random) vary significantly by model capacity and repair method, requiring manual tuning.
- What evidence would resolve it: A meta-learning or rule-based system that successfully recommends optimal strategies for unseen model-repair pairs.

### Open Question 2
- Question: Do the efficiency and effectiveness advantages of boundary-aware sampling (SAPS) for detoxification transfer to other alignment tasks such as reducing social bias or improving factual accuracy?
- Basis in paper: [explicit] The authors explicitly call for "Extension to Other Safety and Alignment Tasks" (Section V.B), noting the current study is limited to toxicity.
- Why unresolved: It is unclear if "boundary samples" for toxicity correlate with critical samples for bias or factuality, which may have different semantic distributions.
- What evidence would resolve it: Empirical validation of SAPS on benchmarks like TruthfulQA or BBQ showing consistent performance gains over baselines.

### Open Question 3
- Question: How does the performance of selection strategies change when selection hyperparameters (e.g., sampling ratio) are jointly optimized with repair hyperparameters (e.g., learning rates)?
- Basis in paper: [inferred] The authors note in "Threats to Validity" (Section VI.A) that achieving optimal performance may require "joint optimization of repair and selection parameters," which remains unexplored.
- Why unresolved: The study fixed selection ratios (e.g., 50%) while tuning repair methods, potentially missing optimal trade-offs between data volume and training intensity.
- What evidence would resolve it: Experiments demonstrating that co-tuning the sampling ratio α alongside learning rates yields superior or more stable convergence than tuning them sequentially.

## Limitations

- The study's conclusions depend heavily on specific repair methods and model scales tested, with optimal boundary sample ratios requiring method-specific calibration
- Computational efficiency claims assume standard clustering implementations and may degrade with extremely large datasets
- Boundary sampling advantages for detoxification may not transfer to other alignment tasks like social bias or factual accuracy

## Confidence

- **High Confidence**: SAPS consistently outperforms or matches full-dataset repair while using substantially less data (supported by multiple model-method combinations in Tables III-V)
- **Medium Confidence**: Selection strategy impact varies by repair method architecture (DAPT vs. DPO sensitivity differences appear robust but method-specific calibration needs broader validation)
- **Medium Confidence**: Computational efficiency advantages of lightweight methods hold for tested scales but may shift at extreme model sizes (>100B parameters)

## Next Checks

1. Test SAPS on a preference-based repair method not evaluated in the paper (e.g., Direct Preference Optimization variants) to verify boundary sampling benefits extend beyond the specific DPO implementation studied.
2. Evaluate selection strategy performance on multilingual toxicity repair datasets to assess whether semantic clustering generalizes across languages and cultural contexts.
3. Measure SAPS performance degradation when using different embedding encoders (e.g., sentence transformers vs. CLIP) to quantify sensitivity to the semantic representation choice.