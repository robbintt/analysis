---
ver: rpa2
title: 'Planted in Pretraining, Swayed by Finetuning: A Case Study on the Origins
  of Cognitive Biases in LLMs'
arxiv_id: '2507.07186'
source_url: https://arxiv.org/abs/2507.07186
tags:
- bias
- biases
- instruction
- 'true'
- pretraining
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) exhibit cognitive biases, but it was
  unclear whether these biases originate from pretraining, instruction finetuning,
  or training randomness. To disentangle these factors, we conducted a two-step causal
  experiment.
---

# Planted in Pretraining, Swayed by Finetuning: A Case Study on the Origins of Cognitive Biases in LLMs

## Quick Facts
- arXiv ID: 2507.07186
- Source URL: https://arxiv.org/abs/2507.07186
- Authors: Itay Itzhak; Yonatan Belinkov; Gabriel Stanovsky
- Reference count: 30
- Primary result: Pretraining is the dominant source of cognitive biases in LLMs, not instruction finetuning or training randomness

## Executive Summary
This study investigates whether cognitive biases in large language models originate from pretraining, instruction finetuning, or training randomness. Through a two-step causal experiment design, the authors systematically disentangle these factors. They first assess the impact of training randomness by fine-tuning models with different random seeds, then introduce cross-tuning—swapping instruction datasets between models—to isolate pretraining effects. The results demonstrate that pretraining is the primary source of cognitive biases, with models sharing the same pretrained backbone exhibiting more similar bias patterns than those sharing only finetuning data.

## Method Summary
The authors conducted two-step causal experiments using OLMo-7B and T5-11B base models. First, they fine-tuned each base model on Flan and Tulu-2 instruction datasets with three different random seeds to assess training randomness effects. Second, they implemented cross-tuning by swapping instruction datasets between the pretrained models (OLMo on Flan vs Tulu, T5 on Flan vs Tulu). Bias evaluation used 32 cognitive bias metrics, with scores normalized to [-1, 1]. Clustering analysis compared whether bias vectors grouped by pretraining identity or instruction dataset. LoRA finetuning with rank 64-512 was used, validated against MMLU performance to ensure capability preservation.

## Key Results
- Training randomness introduces only minor variability in bias scores, with directionally stable trends when aggregated across seeds
- Models with the same pretrained backbone exhibit more similar bias patterns than those sharing only finetuning data
- Clustering analysis confirmed that models group more strongly by pretraining identity than by instruction dataset, even in community-finetuned models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cognitive biases are primarily "planted" during pretraining and encoded as a latent bias, which finetuning surfaces or modifies rather than creates.
- Mechanism: The model acquires fundamental behavioral tendencies during pretraining. Finetuning on instruction data acts as a lightweight modifier that cannot easily override these deeply rooted latent patterns.
- Core assumption: The "Latent Bias" acquired during pretraining is the dominant causal factor for "Observed Bias," distinct from finetuning noise.
- Evidence anchors:
  - [abstract] "models with the same pretrained backbone exhibit more similar bias patterns than those sharing only finetuning data."
  - [section] Section 1 Figure 1 caption: "Our results indicate that pretraining is the leading cause of biases in LMs, shaping the latent bias that later affects observed biases."
- Break condition: If finetuning depth or scale were significantly increased, the latent pretraining bias might be overwritten.

### Mechanism 2
- Claim: Cross-tuning (swapping instruction datasets between base models) isolates the causal influence of pretraining by controlling for the data variable.
- Mechanism: By finetuning Model A on Data B (originally used for Model B) and vice versa, one creates a 2x2 factorial design. If resulting bias vectors cluster by model architecture rather than data, pretraining is the dominant driver.
- Core assumption: The effect of instruction data is additive or superficial and does not interact chaotically with specific pretraining architectures to create entirely new bias patterns.
- Evidence anchors:
  - [abstract] "We introduce cross-tuning – swapping instruction datasets between models to isolate bias sources."
  - [section] Section 3.3: "In this setup, each pretrained model is fine-tuned on the instruction dataset originally used for the other model... If the origin of the biases is the pretrained model, then finetuning it on a different instruction dataset should not drastically change the emergent bias trends."
- Break condition: If instruction data contains strong, contradictory bias signals (e.g., explicit debiasing data), it might disrupt the pretraining-based clustering.

### Mechanism 3
- Claim: Aggregating bias scores across multiple random seeds filters out training stochasticity ("noise") to reveal stable bias trends.
- Mechanism: Random initialization and data order perturbations cause variance. Averaging scores across seeds cancels out this noise, revealing the consistent direction of latent bias.
- Core assumption: Training randomness acts as zero-mean noise on the bias score rather than a systematic bias shifter.
- Evidence anchors:
  - [abstract] "bias scores showed modest variability across random seeds, but directionally stable trends when aggregated."
  - [section] Section 5.1: "Our analysis shows that averaging bias scores across seeds yields strong correlations with the scores of the fully finetuned model."
- Break condition: If a specific random seed triggers a mode collapse or catastrophic forgetting, aggregation might obscure rather than reveal the true bias.

## Foundational Learning

- Concept: **Latent vs. Observed Bias**
  - Why needed here: To understand that what we measure (Observed) is a noisy projection of internal state (Latent), and that interventions should target the Latent layer (pretraining) rather than just the Observed layer.
  - Quick check question: If two models have different Observed Biases but share a pretrained backbone, does this paper suggest their Latent Biases are likely similar or different?

- Concept: **LoRA (Low-Rank Adaptation)**
  - Why needed here: The study relies on LoRA to approximate full finetuning for resource efficiency; understanding this approximation is critical for validating the "secondary role of finetuning" claim.
  - Quick check question: Why does the paper verify MMLU scores when using LoRA, and what risk does this mitigation address?

- Concept: **Bias Vectors**
  - Why needed here: The paper treats bias not as a scalar but as a high-dimensional vector (scores across 32 biases) to perform clustering analysis.
  - Quick check question: In the clustering analysis, does grouping by "pretraining identity" or "instruction dataset" yield tighter clusters?

## Architecture Onboarding

- Component map: Pretrained Backbones -> LoRA Adaptation Layers -> Instruction Datasets -> 32 Cognitive Bias Metrics -> Clustering Analysis
- Critical path:
  1. Select two base models with divergent initial bias patterns (OLMo vs T5)
  2. Apply Cross-Tuning: Finetune each on the other's instruction set
  3. Measure Bias Vectors across 32 dimensions
  4. Perform Clustering (K-Means or label-based) to see if vectors group by base model or data
- Design tradeoffs:
  - **LoRA vs. Full Finetuning**: Using LoRA allows controlled experimentation but may underrepresent the capacity of full finetuning to alter pretraining dynamics
  - **Data Downsampling**: Flan was downsampled to match Tulu size; this controls for data volume but might lose signal from the original Flan scale
- Failure signatures:
  - **High Seed Variance**: If standard deviation of bias scores across seeds is high, the causal signal is lost in noise
  - **Capability Degradation**: If LoRA configuration drops MMLU performance significantly, the model is no longer a valid proxy for the production model
- First 3 experiments:
  1. **Seed Stability Test**: Finetune a single model (e.g., OLMo) on Tulu with 3 seeds; compute standard deviation of bias scores to establish a noise floor
  2. **Cross-Tuning Swap**: Finetune OLMo on Flan and T5 on Tulu; compare the resulting bias vectors against the original (OLMo-Tulu, T5-Flan) to see if the vector shifts toward the data or stays with the base model
  3. **Clustering Validation**: Run K-Means on the bias vectors of all 4 models; verify if clusters align perfectly with Pretraining ID (OLMo vs T5) rather than Data ID (Flan vs Tulu)

## Open Questions the Paper Calls Out

- Question: Which specific pretraining pipeline components (corpus composition, linguistic framing, tokenization, filtering, sampling strategies) causally determine particular cognitive bias patterns?
  - Basis in paper: [explicit] "Our findings point to pretraining as the dominant source, but the mechanisms remain underspecified. Factors such as corpus composition, linguistic framing, tokenization, filtering, and sampling strategies likely contribute to the emergence of bias."
  - Why unresolved: The paper identifies pretraining as the primary source but does not isolate which pretraining design choices drive which biases.
  - What evidence would resolve it: Controlled ablation studies pretraining models with systematically varied corpora or tokenizers, then measuring bias score changes.

- Question: Can bias-aware objectives or data curation during pretraining mitigate cognitive biases without degrading downstream capabilities?
  - Basis in paper: [explicit] "Early-stage interventions offer promising avenues for mitigation... designing objectives that explicitly penalize biased behavior."
  - Why unresolved: The paper proposes pretraining-stage interventions but provides no experimental validation of their effectiveness or trade-offs.
  - What evidence would resolve it: Pretraining with explicit bias penalties and comparing both bias scores and task performance (e.g., MMLU) to standard pretraining baselines.

- Question: Does full finetuning have greater capacity than LoRA to alter pretraining-established bias patterns?
  - Basis in paper: [inferred] The authors acknowledge using LoRA due to costs; prior work (Shuttleworth et al., 2024) suggests LoRA may be an "illusion of equivalence" to full finetuning.
  - Why unresolved: All controlled experiments used LoRA, leaving uncertainty about whether full finetuning could override pretraining biases more effectively.
  - What evidence would resolve it: Head-to-head comparison of full finetuning vs. LoRA on identical instruction data, measuring bias vector shifts.

## Limitations

- The study uses LoRA rather than full finetuning, which may not fully capture the capacity of instruction tuning to reshape pretrained biases
- The cross-tuning design assumes instruction data effects are additive rather than interactive with pretraining architecture
- Community-finetuned models provide weaker evidence for pretraining dominance due to limited available variants and potential confounding factors

## Confidence

- **High Confidence**: Pretraining is the primary source of cognitive biases in LLMs (supported by consistent clustering results across multiple experimental setups)
- **Medium Confidence**: Training randomness introduces only minor variability in bias scores (limited by single model seed experiment; could vary with different architectures)
- **Medium Confidence**: Cross-tuning successfully isolates pretraining effects (relies on assumption that instruction data effects are superficial; not explicitly validated)

## Next Checks

1. **Full Finetuning Replication**: Repeat the core experiments using full finetuning instead of LoRA to verify that the pretraining dominance finding holds under more extensive adaptation

2. **Instruction Dataset Strength Test**: Introduce an explicitly debiased instruction dataset in the cross-tuning design to test whether strong bias signals in finetuning data can override pretraining patterns

3. **Architectural Transferability**: Test whether models with similar pretraining architectures (but different weights) exhibit similar bias patterns when finetuned on identical instruction data, to distinguish architecture vs weight effects