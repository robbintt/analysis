---
ver: rpa2
title: 'VeritasFi: An Adaptable, Multi-tiered RAG Framework for Multi-modal Financial
  Question Answering'
arxiv_id: '2510.10828'
source_url: https://arxiv.org/abs/2510.10828
tags:
- arxiv
- retrieval
- financial
- chunks
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: VeritasFi is a hybrid RAG framework for financial QA that addresses
  the challenges of multi-modal document processing and domain-to-entity adaptation.
  It combines a multi-modal preprocessing pipeline (CAKC) that transforms heterogeneous
  financial data into structured text, a tripartite hybrid retrieval engine (deep
  semantic search, high-frequency memory bank, and real-time tool use), and a two-stage
  domain-to-entity re-ranking strategy.
---

# VeritasFi: An Adaptable, Multi-tiered RAG Framework for Multi-modal Financial Question Answering

## Quick Facts
- arXiv ID: 2510.10828
- Source URL: https://arxiv.org/abs/2510.10828
- Reference count: 40
- VeritasFi achieves 62.9% average end-to-end accuracy on Zeekr and 64.0% on Lotus datasets, outperforming baselines by 18.4-38.4 percentage points

## Executive Summary
VeritasFi is a hybrid RAG framework designed to address the challenges of multi-modal financial question answering. It processes heterogeneous financial data (text, tables, figures) through a multi-modal preprocessing pipeline (CAKC) that transforms documents into structured text, employs a tripartite hybrid retrieval engine combining deep semantic search, high-frequency memory bank, and real-time tool use, and uses a two-stage domain-to-entity re-ranking strategy for rapid company-specific adaptation. The framework demonstrates superior performance on proprietary financial QA datasets, achieving 18.4-38.4 percentage point improvements over baselines like GraphRAG and LightRAG.

## Method Summary
The framework processes SEC filings through CAKC: MinerU parsing converts documents into structured chunks, GPT-4o transforms tables/figures into descriptive text, and semantic enhancements (de-duplication, co-reference resolution, metadata generation) create machine-readable inputs. Retrieval combines parallel BM25, FAISS, and metadata search with Chroma DB indexing. A two-stage re-ranker training strategy first builds a general model on abstracted data (masking entity names), then fine-tunes on company-specific data with LLM-generated hard negatives using FlagEmbedding with LoRA fine-tuning.

## Key Results
- Achieves 62.9% average end-to-end accuracy on Zeekr and 64.0% on Lotus datasets
- Outperforms baselines by 18.4-38.4 percentage points on proprietary financial QA benchmarks
- Multi-path retrieval improves evidence hit rates by 2-5%, while two-stage re-ranking enhances precision by 4.5-7.7 percentage points
- Rapid adaptation to new companies through automated data annotation

## Why This Works (Mechanism)

### Mechanism 1
Transforming multi-modal financial data into unified textual narratives and semantically enriched chunks improves retrieval grounding compared to raw linearized text. The CAKC pipeline parses documents via MinerU, then uses GPT-4o to rewrite tables/figures into descriptive text, followed by de-duplication, co-reference resolution, and metadata attachment. This reduces modality gaps and context fragmentation. Core assumption: The LLM transformation accurately captures numerical relationships without hallucination. Evidence: [Abstract] describes "multi-modal preprocessing pipeline that seamlessly transforms heterogeneous data into a coherent, machine-readable format." [Section 3.1] details "figures are converted into structured captions... while tables are rewritten as textual statements." Break condition: If the summarization LLM hallucinates trends in charts or omits key table footnotes, the retriever surfaces plausible but factually incorrect grounding.

### Mechanism 2
A two-stage, domain-to-entity re-ranking strategy enables rapid adaptation to specific companies while maintaining general financial reasoning. The system trains a general re-ranker on abstracted data (masking entity names) to learn "financial logic" independent of specific companies, then fine-tunes on company-specific data with LLM-generated hard negatives. Core assumption: The LLM auto-annotator generates sufficiently high-quality relevance labels (accuracy >92% mentioned in Appendix). Evidence: [Abstract] mentions "two-stage training strategy for its re-ranking component... initially constructs a general... model, followed by rapid fine-tuning on company-specific data." [Section 6.3] reports "Model after DAR achieves higher accuracy on both datasets, yielding boosts of over 2.5 in NDCG@5... surpassing both Stage 1 and Direct SFT." Break condition: If hard negatives selected during auto-annotation are actually relevant but subtle, the contrastive loss degrades the model's ability to recognize implicit connections.

### Mechanism 3
Tripartite Hybrid Retrieval improves evidence coverage by routing queries to specialized handlers (semantic, cache, tools) in parallel. The query preprocessor decomposes complex queries while the system simultaneously queries deep multi-path retrieval (BM25, Dense, Metadata), high-frequency memory bank (cached key-value answers), and external tools (APIs). Core assumption: The query decomposition router correctly classifies intent to leverage appropriate path. Evidence: [Abstract] describes "tripartite hybrid retrieval engine that operates in parallel... combining deep multi-path retrieval... real-time data acquisition... and an expert-curated memory bank." [Section 4.2.1] states "Each retriever targets a different aspect of the query-chunk relationship to maximize recall." Break condition: If the router misclassifies a complex, multi-hop reasoning query as a "high-frequency" simple lookup, the system retrieves superficial answers from cache instead of synthesizing deep evidence.

## Foundational Learning

- **Concept: Multi-Modal Linearization**
  - Why needed here: VeritasFi converts images/tables to text rather than processing them directly. Understanding prompt engineering to preserve numerical integrity during conversion is essential.
  - Quick check question: When converting a complex financial table to text, what specific data attributes (row headers, time periods) must be explicitly preserved to ensure the LLM doesn't lose relational context?

- **Concept: Contrastive Learning for Retrieval (Hard Negatives)**
  - Why needed here: The re-ranker trains using hard negatives (retrieved but irrelevant chunks) rather than random chunks. Understanding this distinction is vital for debugging training data quality.
  - Quick check question: Why would training a re-ranker using only "random chunks" as negatives result in poor performance on confusing financial documents?

- **Concept: RAG Fusion / Ensemble Retrieval**
  - Why needed here: The system combines BM25 (keywords), Dense (semantic), and Metadata retrieval. Understanding when each excels is crucial for optimization.
  - Quick check question: In financial contexts, when would BM25 outperform a Dense embedding model (e.g., searching for specific ticker symbols or legal citations)?

## Architecture Onboarding

- **Component map:**
  - Ingestion (CAKC): MinerU (Parser) -> GPT-4o (Textualizer) -> De-duper/Co-ref Resolver -> Chroma/Relational DB
  - Serving (THR): Query Preprocessor -> Router -> [Dense/BM25/Metadata Retrievers] + [Memory Cache] + [Tools]
  - Specialization (DAR): LLM Annotator (generating training data) -> FlagEmbedding (LoRA Fine-tuning)

- **Critical path:** The Domain-to-Entity Adaptation (DAR) module is critical. Raw retrieval brings high recall but low precision (noise), and the specialized re-ranker is the primary filter determining final answer quality. If this component fails or isn't tuned for the target company, the system hallucinates.

- **Design tradeoffs:**
  - Latency vs. Accuracy: Running 3 parallel retrieval streams and complex re-ranker increases latency compared to simple vector search but improves accuracy
  - Automation vs. Noise: Using LLM to auto-annotate training data allows rapid scaling to new companies but introduces noisy labels compared to human annotation

- **Failure signatures:**
  - "Context Window Stuffing": Chunk Bundling module might aggressively expand chunks, exceeding context window limits if similarity thresholds are too low
  - "Cache Staleness": High-Frequency Memory Bank relies on expert verification; if not updated, system confidently returns outdated financial figures
  - "Abstraction Leakage": If Stage 1 abstraction isn't perfect, general re-ranker might overfit to specific company names rather than financial concepts, reducing transferability

- **First 3 experiments:**
  1. **Modality Ablation:** Ingest sample report with CAKC enabled vs. disabled (raw text). Measure retrieval F1 score for questions derived from tables/figures to quantify multi-modal lift.
  2. **Re-ranker Overfit Test:** Train re-ranker using only Stage 1 (General) vs. Stage 1+2 (Specific). Compare performance on held-out set of cross-company questions to ensure specialization didn't destroy general reasoning.
  3. **Router Latency Profiling:** Measure end-to-end latency for "Memory Bank" path vs. "Multi-Path Retrieval" path. Verify Memory Bank provides intended low-latency shortcut for high-frequency queries.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the two-stage domain-to-entity adaptation strategy generalize effectively to other specialized domains beyond finance (e.g., legal, healthcare)?
  - Basis in paper: [explicit] Conclusion states "Future work could explore extending this framework to other specialized domains (e.g., legal, healthcare)."
  - Why unresolved: Entity abstraction and automated annotation pipeline designed specifically for financial filings; other domains have different terminology, document structures, and reasoning patterns requiring different abstraction strategies.
  - What evidence would resolve it: Results from applying same two-stage training strategy with domain-appropriate abstraction methods on legal (court documents) or healthcare (clinical notes) datasets.

- **Open Question 2:** How can the framework incorporate advanced reasoning capabilities to handle complex multi-hop queries over temporal financial data?
  - Basis in paper: [explicit] Conclusion explicitly mentions incorporating "advanced reasoning capabilities to handle complex multi-hop queries over temporal financial data" as future work.
  - Why unresolved: Current framework retrieves and re-ranks chunks but doesn't explicitly model temporal relationships or perform multi-hop reasoning chains across documents from different time periods.
  - What evidence would resolve it: Performance improvements on benchmark requiring temporal reasoning (comparing metrics across fiscal years) after integrating temporal reasoning module.

- **Open Question 3:** What is the impact of LLM-generated annotation noise on re-ranker specialization quality, and at what error rate does performance degrade significantly?
  - Basis in paper: [inferred] Stage 2 adaptation relies on LLM-generated annotations with 92.8-95.5% accuracy; paper doesn't analyze how annotation errors propagate to re-ranker performance.
  - Why unresolved: While annotation accuracy is reported, no ablation study examines sensitivity of final QA performance to annotation quality variations.
  - What evidence would resolve it: Controlled experiments with artificially injected annotation noise at different rates (5%, 10%, 20% error) measuring downstream NDCG and end-to-end accuracy.

## Limitations
- Relies on GPT-4o for multi-modal transformation, introducing cost and potential fidelity risks - accuracy fundamentally limited by LLM's ability to faithfully convert complex financial tables/figures without hallucination
- Company-specific adaptation through automated annotation assumes high-quality LLM labeling, but 92% accuracy claim lacks independent verification
- Evaluation uses proprietary datasets (Zeekr, Lotus) that are not publicly available, limiting reproducibility and external validation
- System complexity with parallel retrieval streams, multi-stage re-ranking, and automated data annotation creates operational overhead that may not scale efficiently for organizations without substantial ML infrastructure

## Confidence
- **High Confidence:** Empirical results showing 18.4-38.4 percentage point improvements over baselines are well-supported by reported metrics and ablation studies. Mechanism of using abstracted data for general re-ranker training followed by company-specific fine-tuning is theoretically sound and technically detailed.
- **Medium Confidence:** Claims about rapid adaptation to new companies rely heavily on automated annotation quality, which is asserted but not independently validated in main paper. Tripartite retrieval architecture's benefits are demonstrated but could benefit from more granular analysis of individual component contributions.
- **Low Confidence:** Specific prompt engineering for GPT-4o's multi-modal transformation and exact thresholds for chunk bundling and similarity matching are not fully specified, making precise reproduction challenging. Evaluation on proprietary datasets limits generalizability claims.

## Next Checks
1. **Modality Ablation Study:** Systematically compare retrieval performance with CAKC enabled vs. disabled on publicly available financial documents with tables/figures to quantify multi-modal lift independently of proprietary datasets.
2. **Annotation Quality Audit:** Implement human validation pipeline for LLM-generated relevance labels used in Stage 2 training, measuring precision/recall against expert annotations on sample of company-specific data.
3. **Cross-Company Transferability Test:** Evaluate general re-ranker (Stage 1 only) on questions from companies not seen during training to verify that entity abstraction effectively enables domain transfer rather than overfitting to specific patterns.