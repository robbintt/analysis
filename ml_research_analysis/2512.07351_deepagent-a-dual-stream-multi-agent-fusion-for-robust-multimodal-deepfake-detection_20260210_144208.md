---
ver: rpa2
title: 'DeepAgent: A Dual Stream Multi Agent Fusion for Robust Multimodal Deepfake
  Detection'
arxiv_id: '2512.07351'
source_url: https://arxiv.org/abs/2512.07351
tags:
- visual
- detection
- deepfake
- audio
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces DeepAgent, a dual-agent deepfake detection
  framework that integrates visual and multimodal audio-visual analysis to address
  the challenge of deepfake content verification. The system employs two complementary
  agents: Agent-1 uses a lightweight CNN-based architecture to identify visual artifacts
  in videos, while Agent-2 detects audio-visual inconsistencies by combining acoustic
  features, Whisper-generated transcriptions, and OCR-based frame text.'
---

# DeepAgent: A Dual Stream Multi Agent Fusion for Robust Multimodal Deepfake Detection

## Quick Facts
- arXiv ID: 2512.07351
- Source URL: https://arxiv.org/abs/2512.07351
- Authors: Sayeem Been Zaman; Wasimul Karim; Arefin Ittesafun Abian; Reem E. Mohamed; Md Rafiqul Islam; Asif Karim; Sami Azam
- Reference count: 40
- Primary result: Dual-agent deepfake detection framework achieves 94.35% test accuracy on combined Celeb-DF and FakeAVCeleb datasets

## Executive Summary
DeepAgent introduces a dual-agent framework for deepfake detection that combines visual artifact analysis with audio-visual semantic consistency checking. The system employs two complementary agents: Agent-1 uses a lightweight CNN-based architecture to identify visual artifacts in videos, while Agent-2 detects audio-visual inconsistencies by combining acoustic features, Whisper-generated transcriptions, and OCR-based frame text. Their outputs are fused through a Random Forest meta-classifier that aggregates probabilistic decisions from both agents. The framework achieves a test accuracy of 94.35% on combined Celeb-DF and FakeAVCeleb datasets, with Agent-2 reaching 93.69% accuracy on FakeAVCeleb and the meta-classifier achieving 97.49% accuracy on DeepFakeTIMIT through cross-dataset validation.

## Method Summary
DeepAgent is a dual-agent deepfake detection framework that processes videos through two parallel streams. Agent-1 extracts frames at uniform intervals, resizes to 224×224 grayscale, and processes them through a 5-block AlexNet-style CNN (64→128→256→256→128 filters) to detect visual artifacts. Agent-2 extracts audio at 16kHz, computes 13-dimensional MFCC features, generates transcriptions via Whisper, extracts frame text via EasyOCR, and computes lexical similarity between speech and visual text. The 14-dimensional feature vector feeds a 4-layer DNN. Both agents output probability scores that are standardized and fed to a Random Forest meta-classifier (100 trees) for final binary classification. The framework was trained on Celeb-DF, FakeAVCeleb, and evaluated on DeepFakeTIMIT.

## Key Results
- Test accuracy of 94.35% on combined Celeb-DF and FakeAVCeleb datasets
- Agent-2 achieves 93.69% accuracy on FakeAVCeleb dataset using audio-visual consistency features
- Meta-classifier achieves 97.49% accuracy on DeepFakeTIMIT through cross-dataset validation
- Cross-dataset performance varies significantly (81.56% on FakeAVCeleb vs 97.49% on DeepFakeTIMIT)

## Why This Works (Mechanism)

### Mechanism 1: Specialized Agent Decomposition with Complementary Decision Boundaries
Decomposing detection into a visual-only agent and an audio-visual consistency agent yields complementary error patterns that fusion can exploit. Agent-1 learns spatial artifact patterns via a shallow CNN optimized for texture, while Agent-2 learns cross-modal inconsistency patterns via MFCC acoustic embeddings plus lexical similarity between Whisper transcripts and OCR text. Their outputs form a 2D meta-feature vector for a Random Forest. Core assumption: Visual-only and cross-modal inconsistencies are approximately independent failure modes; deepfakes that evade one will be caught by the other.

### Mechanism 2: Cross-Modal Semantic Consistency via Lexical Alignment
Measuring lexical overlap between speech transcripts and visual text provides a scalar signal of audio-visual desynchronization characteristic of deepfakes. Agent-2 extracts a 13-dim MFCC mean vector from resampled audio, transcribes speech via Whisper, extracts frame text via EasyOCR, and computes normalized intersection similarity. The 14-dim feature feeds a 4-layer DNN with dropout. Core assumption: Deepfakes exhibit measurable lexical mismatch between spoken content and visible text/lip motion; real videos maintain higher consistency.

### Mechanism 3: Decision-Level Meta-Fusion via Random Forest Ensemble
Fusing agent predictions at the decision level improves robustness by aggregating learned decision boundaries without requiring gradient-compatible representations. Each agent outputs a probability score. These are standardized and input to a Random Forest with T=100 trees trained on bootstrap samples. Final prediction: weighted average of tree outputs. Core assumption: The meta-classifier can learn to weight agent reliability contextually; Random Forest's non-linear decision boundaries capture interaction patterns between agent outputs.

## Foundational Learning

- **Mel-Frequency Cepstral Coefficients (MFCCs)**
  - Why needed here: Agent-2's audio pipeline depends on understanding how MFCCs compress spectral envelope into 13 coefficients; the paper assumes this captures synthetic speech artifacts.
  - Quick check question: Can you explain why MFCCs use a Mel-scale filterbank rather than linear frequency bins, and what the DCT step achieves?

- **AlexNet-style CNN Architecture**
  - Why needed here: Agent-1 is a modified AlexNet; understanding the 5-block conv structure, receptive field growth, and why shallow networks may capture texture artifacts better than very deep networks is essential for debugging.
  - Quick check question: What is the inductive bias of a shallow CNN with large initial kernels (11×11) vs. modern ResNet-style architectures with small stacked kernels?

- **Random Forest Ensemble Aggregation**
  - Why needed here: The meta-classifier is the final decision layer; understanding bootstrap aggregation, feature importance, and why RF handles small meta-feature spaces (2D) well is critical.
  - Quick check question: Why does Random Forest with 100 trees reduce variance compared to a single decision tree, and what happens if agent outputs are perfectly correlated?

## Architecture Onboarding

- **Component map:**
  Video file → frame extraction + audio extraction → Agent-1 CNN processing → Agent-2 MFCC+Whisper+OCR processing → probability outputs → standardization → Random Forest meta-classifier → final binary label

- **Critical path:**
  1. Video preprocessing (frame extraction at 5-frame intervals + audio extraction)
  2. Parallel agent inference (Agent-1 on frames, Agent-2 on audio+frames)
  3. Meta-feature construction and RF prediction
  4. Threshold at 0.5 for binary output

- **Design tradeoffs:**
  - Agent-1 uses grayscale (not RGB) to reduce memory; paper notes no consistent accuracy gain from color.
  - Late fusion (decision-level) vs. early/feature-level fusion trades off end-to-end gradient flow for modularity and interpretability.
  - Agent-2's 14-dim representation is extremely compact; may lose information compared to full audio spectrograms or video embeddings.

- **Failure signatures:**
  - Low OCR text detection → lexical similarity defaults toward zero → Agent-2 relies heavily on MFCC alone.
  - High compression or low resolution → visual artifacts blur → Agent-1 recall drops (observed: Fake class recall 74% vs Real 96%).
  - Dataset shift (FakeAVCeleb → DeepFakeTIMIT) → meta-classifier accuracy varies significantly (81.56% vs 97.49%).

- **First 3 experiments:**
  1. **Agent isolation test:** Run Agent-1 and Agent-2 independently on a held-out subset of FakeAVCeleb; compute confusion matrices to identify systematic failure modes per agent and verify complementarity.
  2. **Ablation on meta-features:** Train the Random Forest using only Agent-1 output, only Agent-2 output, and both; quantify the fusion gain and check for correlation between agent predictions.
  3. **Cross-dataset stress test:** Train the full pipeline on FakeAVCeleb and evaluate on DeepFakeTIMIT without retraining the meta-classifier; measure accuracy drop and analyze whether one agent degrades more than the other.

## Open Questions the Paper Calls Out

### Open Question 1
Would an adaptive fusion mechanism with uncertainty-aware weighting improve DeepAgent's performance when one modality provides unreliable signals? The current Random Forest meta-classifier treats both agents' outputs equally without modulating confidence based on input quality or signal reliability per sample. Evidence: Comparative study implementing uncertainty-weighted fusion against the current static Random Forest approach, evaluated on noisy or partially corrupted multimodal inputs.

### Open Question 2
How does DeepAgent's detection performance vary across demographic subgroups (age, gender, ethnicity)? No stratified analysis was conducted, and benchmark datasets may have demographic imbalances that mask differential error rates. Evidence: Per-subgroup performance metrics across available demographic attributes, with statistical tests for significant differences in error rates.

### Open Question 3
Can DeepAgent maintain detection accuracy under targeted adversarial attacks on individual modalities? No adversarial evaluation was conducted; the framework's vulnerability to perturbed audio, adversarially optimized video frames, or coordinated cross-modal attacks remains unknown. Evidence: Evaluation under standard adversarial attack protocols applied to visual frames, audio waveforms, or both modalities simultaneously.

### Open Question 4
Why does the meta-classifier achieve lower accuracy (81.56%) on FakeAVCeleb than Agent-2 alone (93.69%), and can this fusion degradation be corrected? The paper does not explain this performance inversion or investigate whether the Random Forest overfits, the meta-feature space is poorly separable, or Agent-1 predictions introduce noise. Evidence: Ablation studies varying meta-classifier type, analysis of agent agreement rates, and visualization of the 2D meta-feature space to assess separability.

## Limitations
- Dataset bias: The authors acknowledge that datasets used are largely celebrity-focused and may not reflect the full range of age, gender, ethnicity, and speaking styles encountered in practice.
- No subgroup analysis: The framework does not report subgroup performance or analyze potential biases in false positive and false negative rates across demographic groups.
- Partial adversarial robustness: The deployment context and threat model are only partially explored, with no evaluation against targeted adversarial attacks on individual modalities.

## Confidence

- **High Confidence**: The visual artifact detection mechanism via shallow CNN (Agent-1) is well-supported by ablation results and standard practice in deepfake forensics.
- **Medium Confidence**: The lexical similarity feature for audio-visual consistency shows measurable performance gains in ablation but depends heavily on OCR and ASR quality that varies across datasets.
- **Medium Confidence**: The Random Forest meta-classifier fusion provides robust aggregation, though the decision-level fusion approach is less explored in deepfake literature compared to feature-level fusion.

## Next Checks

1. Conduct agent isolation testing on a held-out subset to verify complementary error patterns between visual-only and audio-visual consistency detection.
2. Perform ablation studies on the 14-dimensional Agent-2 feature space to quantify the contribution of lexical similarity versus MFCC-only features.
3. Execute cross-dataset validation without retraining the meta-classifier to measure generalization limits and identify which agent degrades more severely under dataset shift.