---
ver: rpa2
title: 'From Loop Nests to Silicon: Mapping AI Workloads onto AMD NPUs with MLIR-AIR'
arxiv_id: '2510.14871'
source_url: https://arxiv.org/abs/2510.14871
tags:
- mlir-air
- data
- memory
- spatial
- channel
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MLIR-AIR introduces a compiler intermediate representation that
  bridges high-level AI workloads with spatial accelerators by providing explicit
  abstractions for asynchronous execution, data movement, and compute scheduling.
  The framework enables the compiler to transform structured loop nests into tiled,
  statically scheduled programs that exploit locality, overlap communication with
  computation, and minimize runtime control overhead.
---

# From Loop Nests to Silicon: Mapping AI Workloads onto AMD NPUs with MLIR-AIR

## Quick Facts
- **arXiv ID:** 2510.14871
- **Source URL:** https://arxiv.org/abs/2510.14871
- **Reference count:** 40
- **Primary result:** MLIR-AIR achieves up to 78.7% compute efficiency on matrix multiplication workloads through explicit abstractions for asynchronous execution and data movement.

## Executive Summary
MLIR-AIR introduces a compiler intermediate representation that bridges high-level AI workloads with spatial accelerators by providing explicit abstractions for asynchronous execution, data movement, and compute scheduling. The framework enables the compiler to transform structured loop nests into tiled, statically scheduled programs that exploit locality, overlap communication with computation, and minimize runtime control overhead. On matrix multiplication workloads, MLIR-AIR achieves up to 78.7% compute efficiency and generates performance nearly identical to hand-optimized implementations. For the LLaMA 2 multi-head attention block, the compiler produces fused implementations using approximately 150 lines of code, demonstrating scalability to complex AI kernels.

## Method Summary
The paper presents a compilation framework that maps AI workloads onto AMD NPUs using MLIR-AIR as an intermediate representation. The method involves lowering high-level framework code (PyTorch/TensorFlow) through MLIR dialects, applying tiling and dependency analysis, inferring channel abstractions for data movement, and finally lowering to the AMD AIE backend. The framework was evaluated on matrix multiplication workloads across various dimensions and on a LLaMA 2 multi-head attention block, with execution on Ryzen AI 7840 NPUs via the XRT runtime.

## Key Results
- MLIR-AIR achieves up to 78.7% compute efficiency on matrix multiplication workloads, matching hand-optimized implementations
- The framework generates fused LLaMA 2 multi-head attention implementations using approximately 150 lines of high-level MLIR code
- End-to-end latency for fused MHA kernels shows 2.24× speedup over unfused implementations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Explicit token-based dependency tracking enables fine-grained asynchronous execution while preserving correctness.
- Mechanism: The ACDG (Asynchronous Control and Dataflow Graph) embeds operation-level dependencies directly into the IR via `air.token` values. These tokens encode RAW, WAR, and WAW dependencies as SSA values, allowing the compiler to reason about and optimize execution schedules while leveraging MLIR's native dominance analysis for automatic correctness verification.
- Core assumption: Dependencies can be statically captured at compile time and do not require runtime resolution for correctness.
- Evidence anchors:
  - [abstract] "MLIR-AIR's token-based asynchronous control flow and channel-based communication abstractions allow fine-grained dependency tracking and hardware pipelining"
  - [section 5.3.1] "air.token construct provides fine-grained control over execution dependencies in asynchronous workloads... integrates with MLIR's native SSA dominance and verification infrastructure"
  - [corpus] Weak corpus evidence; related NPU compilation papers focus on scheduling but not token-based synchronization specifically.
- Break condition: If workloads have dynamic data-dependent control flow that cannot be represented statically in the ACDG, the mechanism would require runtime fallback or fail.

### Mechanism 2
- Claim: Decoupled put/get channel abstraction enables overlapping communication with computation by localizing synchronization to memory hierarchy boundaries.
- Mechanism: `air.channel.put` and `air.channel.get` operations are decoupled and placed in code regions local to their respective memory allocations. They are linked via a symbolic `air.channel` that enforces backpressure-based synchronization. This allows the compiler to schedule data movement independently from compute, overlapping execution when hardware buffering permits.
- Core assumption: Hardware supports stream buffering that allows backpressure synchronization without blocking the producer entirely.
- Evidence anchors:
  - [abstract] "channel-based communication abstractions allow fine-grained dependency tracking and hardware pipelining, enabling efficient mapping to AMD NPUs"
  - [section 5.2.2] "decoupling enables fine-grained control over dataflow, allowing communication to be scheduled alongside computation when desired, or independently when beneficial"
  - [section 7.4] Figure 6 shows ACDG transformation from coupled memcpy to decoupled put/get, with dashed arrow representing "data stream back pressure; an overlapping schedule across two sides is enabled if stream buffering is supported in hardware"
  - [corpus] Related work on NPU scheduling (NVR, Neptune) addresses data movement but does not explicitly validate the channel abstraction pattern.
- Break condition: If the target hardware lacks adequate stream buffer depth, backpressure would serialize execution, negating the overlapping benefit.

### Mechanism 3
- Claim: Spatial tiling with `air.herd` enables predictable, deterministic execution by guaranteeing atomic dispatch and independent forward progress for worker groups.
- Mechanism: `air.herd` defines a group of work units executing concurrently on a physically contiguous grid of compute units. The atomic scheduling constraint ensures all workers are dispatched only when all resources are available, and each worker can make independent forward progress. This eliminates nondeterminism common in GPU wavefront scheduling.
- Core assumption: The spatial architecture can guarantee physical contiguity and independent forward progress for allocated worker groups.
- Evidence anchors:
  - [section 5.1.3] "air.herd operations are scheduled atomically: they are only scheduled when resources for all the workers are available, and must enable independent forward progress"
  - [section 7] Describes AMD NPU architecture where "the absence of caches... and the emphasis on computing on local tiles of data (eliminating memory access latency variation) means the architecture is characterized by extremely deterministic behaviour"
  - [corpus] Related NPU papers (NVR, Neptune) mention spatial scheduling but do not provide comparative validation of atomic dispatch guarantees.
- Break condition: If resource requirements exceed available contiguous blocks or if architecture cannot guarantee independent forward progress, compilation or execution fails.

## Foundational Learning

- **SSA (Static Single Assignment) Form**
  - Why needed here: `air.token` values are SSA values encoding dependencies. Understanding SSA dominance is essential to grasp how MLIR-AIR guarantees correctness through its verification infrastructure.
  - Quick check question: Given a control flow graph with a merge point, can you identify where φ-functions would be inserted and why this matters for dependency tracking?

- **Polyhedral Compilation Model**
  - Why needed here: MLIR-AIR leverages polyhedral analysis for loop transformations, broadcast detection via affine sets, and spatial mapping. The paper references affine access relations for data movement optimization.
  - Quick check question: Can you express a simple 2D loop nest with affine access patterns as a polyhedral domain and explain what transformations preserve correctness?

- **Spatial vs. Temporal Architectures**
  - Why needed here: The paper contrasts thread-centric GPU models with spatial architectures that require explicit placement. Understanding this distinction is critical for evaluating AIR's design choices.
  - Quick check question: What is the fundamental difference in how a GPU scheduler vs. a spatial NPU scheduler handles resource allocation and forward progress guarantees?

## Architecture Onboarding

- **Component map:**
  - Frontend Integration: PyTorch/TensorFlow → Torch-MLIR/TOSA → SCF/LinAlg dialects
  - AIR Dialect Layer: `air.launch` → `air.segment` → `air.herd` hierarchy; `air.channel` (put/get); `air.token` (async dependencies)
  - ACDG Analysis: Token extraction, dependency graph construction, loop-carried dependency handling
  - Lowering Pipeline: Tiling → Broadcast detection → Channel inference → MemRef splitting → MLIR-AIE lowering
  - Backend: MLIR-AIE → Peano/Vitis → XRT/ROCr runtime execution

- **Critical path:**
  1. **Input ingestion:** High-level framework code → MLIR dialects preserving loop structure
  2. **Tiling pass:** Identify parallel subregions, introduce `scf.parallel` → map to `air.herd`
  3. **Dependency analysis:** Extract ACDG, annotate with `air.token` values, handle loop-carried dependencies
  4. **Channel inference:** Decouple `air.memcpy` into `air.channel.put`/`air.channel.get` pairs
  5. **Hardware lowering:** Map `air.herd` → AIE cores, `air.channel` → DMA engines/BDs, `air.token` → tile-local locks
  6. **Runtime dispatch:** Generate binary via XRT, execute with dependency-resolved scheduling

- **Design tradeoffs:**
  - **Abstraction level vs. performance:** AIR sits between Triton (high-level) and MLIR-AIE (close-to-metal). Higher abstraction reduces programmer effort but may limit micro-optimizations.
  - **Static vs. dynamic scheduling:** AIR shifts scheduling decisions to compile-time for predictability, trading runtime flexibility for deterministic execution.
  - **Atomic herd dispatch vs. fine-grained scheduling:** Atomic dispatch guarantees independent forward progress but may reduce utilization if resource availability is fragmented.

- **Failure signatures:**
  - **Dependency analysis failure:** If ACDG cannot resolve dependencies (e.g., dynamic control flow), compilation halts or produces incorrect execution order.
  - **Channel mapping failure:** If hardware lacks sufficient DMA channels or buffer depth for inferred `air.channel` operations, backend lowering fails.
  - **Herd size violation:** If `air.herd` dimensions exceed physically contiguous resources, MLIR-AIE lowering reports mapping failure.
  - **Memory space mismatch:** If MemRef allocations do not respect hierarchy annotations (L1/L2/L3), runtime produces access violations.

- **First 3 experiments:**
  1. **Vector addition baseline:** Implement the element-wise add example from Appendix B using AIR Python bindings. Verify correct output, examine generated IR to trace `air.herd` formation and `air.channel` decoupling.
  2. **Matrix multiplication scaling:** Run tiled matrix multiplication (Listing 6 pseudocode) across herd configurations (2×2, 2×4, 4×4). Measure throughput vs. problem size, compare against MLIR-AIE hand-optimized baseline to validate the ~78.7% efficiency claim.
  3. **Kernel fusion with LLaMA MHA:** Implement a simplified multi-head attention block using fused kernels. Profile dispatch overhead reduction (targeting ~2× speedup per Table 3) and verify `air.channel` merging reduces DMA channel contention.

## Open Questions the Paper Calls Out

- **Question:** Can MLIR-AIR automatically determine optimal tiling factors and strategies without relying on manual heuristics?
  - Basis in paper: [inferred] The paper states in Section 9.2 that tiling sizes were "fixed... chosen heuristically and not fine-tuned," suggesting the automation of this step remains unaddressed.
  - Why unresolved: The current implementation depends on user-specified or simple heuristic tiling parameters to achieve high efficiency.
  - What evidence would resolve it: An autotuning compiler pass that dynamically selects tiling configurations achieving efficiency comparable to or better than the current hand-tuned baselines.

- **Question:** How can the air.launch abstraction be extended to support coordinated execution across multiple heterogeneous devices or hosts?
  - Basis in paper: [explicit] Section 10 explicitly states, "We plan to explore how an appropriate runtime might use air.launch to enable multi-device dispatch."
  - Why unresolved: The current framework focuses on single-device dispatch and lacks runtime mechanisms for cross-device or cross-host coordination.
  - What evidence would resolve it: A runtime implementation demonstrating a single AIR program dispatching coordinated tasks across a cluster of distinct accelerators.

- **Question:** Does the proposed token-based synchronization and dataflow model scale effectively to full Transformer models utilizing all available spatial compute resources?
  - Basis in paper: [inferred] The LLaMA 2 case study in Section 9.3 was limited to a "single AIE core" and "does not yet exploit spatial parallelism across multiple AIE cores."
  - Why unresolved: The complexity of managing synchronization and data movement across a full NPU array for complex models remains unverified.
  - What evidence would resolve it: Performance benchmarks of a complete LLaMA 2 inference running across the entire NPU tile array without resource contention or deadlock.

## Limitations

- Static dependency analysis assumes dependencies can be fully resolved at compile time, with no clear strategy for handling dynamic control flow
- Hardware abstraction fidelity remains unproven beyond AMD NPUs, with scalability to other spatial architectures unverified
- Channel buffer requirements are not specified, potentially limiting performance on memory-constrained edge deployments

## Confidence

**High Confidence Claims:**
- The token-based SSA dependency tracking mechanism is well-grounded in MLIR's established compiler infrastructure
- The 78.7% compute efficiency for matrix multiplication is supported by direct comparison against hand-optimized MLIR-AIE implementations
- The channel decoupling mechanism for overlapping communication with computation follows established dataflow principles

**Medium Confidence Claims:**
- The scalability demonstration with LLaMA 2 MHA using ~150 lines of code, while compelling, lacks detailed performance breakdown across the various fused operations
- The claim of deterministic execution due to spatial architecture characteristics is supported by architecture description but not empirically validated against nondeterministic alternatives
- The abstraction's portability claim is theoretically sound but only tested on AMD NPUs

**Low Confidence Claims:**
- The performance comparison to existing NPU compilers (NVR, Neptune) is absent, making relative performance claims difficult to evaluate
- The specific efficiency gains from channel merging and broadcast detection optimizations are not quantified separately from overall results
- The impact of AIR's higher abstraction level on programmer productivity versus low-level optimization capability is not measured

## Next Checks

1. **Dynamic Control Flow Stress Test:** Implement a workload with data-dependent loop bounds or conditional execution paths. Measure whether MLIR-AIR can compile it successfully, or if compilation fails, document the specific dependency resolution errors to understand the framework's limitations.

2. **Hardware Buffer Sensitivity Analysis:** Systematically vary simulated DMA buffer depths in the backend lowering and measure the impact on achieved overlapping communication efficiency. Characterize the minimum buffer requirements for achieving the claimed performance benefits.

3. **Cross-Architecture Portability Experiment:** Attempt to retarget the same MLIR-AIR program to a different spatial architecture (e.g., Xilinx AIE or an FPGA overlay). Document which AIR constructs (herds, channels, tokens) map directly versus requiring manual intervention or fallback mechanisms.