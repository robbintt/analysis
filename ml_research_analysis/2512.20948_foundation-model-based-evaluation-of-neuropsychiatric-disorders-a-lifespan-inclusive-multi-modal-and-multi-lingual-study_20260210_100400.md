---
ver: rpa2
title: 'Foundation Model-based Evaluation of Neuropsychiatric Disorders: A Lifespan-Inclusive,
  Multi-Modal, and Multi-Lingual Study'
arxiv_id: '2512.20948'
source_url: https://arxiv.org/abs/2512.20948
tags:
- speech
- detection
- multi-modal
- fusion
- depression
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces FEND, a foundation model-based evaluation\
  \ framework for detecting neuropsychiatric disorders such as Alzheimer\u2019s disease\
  \ (AD), depression, and autism spectrum disorder (ASD). The framework integrates\
  \ speech and text modalities using pre-trained foundation models (e.g., WavLM for\
  \ speech, E5 for text) and evaluates their performance across 13 multi-lingual datasets\
  \ spanning English, Chinese, Greek, French, and Dutch."
---

# Foundation Model-based Evaluation of Neuropsychiatric Disorders: A Lifespan-Inclusive, Multi-Modal, and Multi-Lingual Study

## Quick Facts
- arXiv ID: 2512.20948
- Source URL: https://arxiv.org/abs/2512.20948
- Reference count: 40
- Key outcome: Foundation models (WavLM, E5) detect neuropsychiatric disorders; multi-modal fusion improves AD/depression but not ASD; multilingual models outperform English-centric ones cross-lingually.

## Executive Summary
This paper introduces FEND, a foundation model-based evaluation framework for detecting neuropsychiatric disorders such as Alzheimer's disease (AD), depression, and autism spectrum disorder (ASD). The framework integrates speech and text modalities using pre-trained foundation models (e.g., WavLM for speech, E5 for text) and evaluates their performance across 13 multi-lingual datasets spanning English, Chinese, Greek, French, and Dutch. FEND employs both mono-modal and multi-modal fusion approaches, using classical fusion methods such as attention, tensor fusion, and memory fusion networks. Key findings include: multi-modal fusion improves AD and depression detection but underperforms in ASD due to dataset heterogeneity; modality imbalance is prevalent, with fusion failing to surpass the best mono-modal models; cross-corpus experiments show robust performance in consistent task and language settings but degradation in multi-lingual and task-heterogeneous scenarios. WavLM-Large excels in speech tasks, while E5-Large and multilingual models like mE5-L are effective for text. The study highlights the importance of language-specific models and tailored fusion strategies for different disorders. FEND provides extensive benchmarks and insights into performance-influencing factors, advancing automated, lifespan-inclusive, and multi-lingual neuropsychiatric disorder assessment.

## Method Summary
FEND uses frozen pre-trained foundation models (WavLM-Large for speech, E5-Large for text) to extract representations from audio and transcripts. These are mapped to 128-dim vectors, sequences downsampled to 200 frames via a merging layer, and fed to a 3-layer MLP classifier (256 hidden units, ReLU, dropout 0.2). Multi-modal fusion employs 8 classical methods (MFN, GraphMFN, MulT, MFM, TFN, LMF, MMIM, Attention). Training uses Adam (LR=1e-3, weight decay=1e-5) for 80 epochs. Evaluation metrics include Weighted Accuracy (WA), Unweighted Accuracy (UA), and Weighted F1 (WF1) across 13 public datasets in 5 languages.

## Key Results
- Multi-modal fusion improves AD and depression detection but underperforms for ASD due to dataset heterogeneity.
- Modality imbalance is prevalent: fusion often fails to surpass the best mono-modal models.
- Cross-lingual generalization requires language-specific or multilingual pre-trained models; English-centric models degrade severely on non-English tasks.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-trained foundation models capture transferable acoustic and linguistic representations for neuropsychiatric disorder detection without task-specific fine-tuning.
- Mechanism: Large-scale self-supervised pre-training (e.g., WavLM's denoising masked speech modeling, E5's contrastive learning) produces representations encoding prosodic, semantic, and syntactic patterns. When these representations are fed through a simple MLP classifier with frozen backbone weights, the quality of the pre-trained features—not classifier complexity—determines performance.
- Core assumption: Acoustic and linguistic abnormalities in AD, depression, and ASD are sufficiently represented in pre-training corpora or generalize from related patterns.
- Evidence anchors:
  - [abstract]: "Leveraging 13 multi-lingual datasets... we systematically evaluate multi-modal fusion performance"
  - [Section III-A]: "We employ a simple three-layer MLP as the classifier... This design ensures that the effectiveness of foundation models is assessed without the complexity of advanced classifiers"
  - [Section V-A]: "WavLM-Large achieves the best performance in the speech modality... attributed to its large-scale pre-training and advanced denoising mechanisms"
- Break condition: If pre-training corpora lack diversity in disordered speech patterns or the target language, representations may be uninformative (as seen with English-centric models on Greek/Chinese data).

### Mechanism 2
- Claim: Multi-modal fusion improves detection for AD and depression but degrades or fails to surpass mono-modal baselines for ASD.
- Mechanism: Speech and text modalities provide complementary information—acoustic features capture prosody, pitch, fluency; text features capture semantic content, syntactic complexity, lexical diversity. Fusion methods (attention, tensor fusion, memory networks) combine these at sequence or utterance level. However, when one modality contains substantially more discriminative information (speech dominates ASD detection), fusion can be distracted by the weaker modality.
- Core assumption: Both modalities provide non-redundant, task-relevant signal; fusion architecture can learn to weight modalities appropriately.
- Evidence anchors:
  - [abstract]: "Multi-modal fusion excels in AD and depression detection but underperforms in ASD due to dataset heterogeneity"
  - [Section V-B]: "Multi-modal fusion is effective for AD and depression detection... ASD detection remains challenging: Multi-modal fusion fails to surpass mono-modal baselines"
  - [Section V-C]: "Modality imbalance... where multi-modal models optimize for the dominant modality while neglecting others"
- Break condition: If modalities are highly imbalanced (one dominates), or if dataset heterogeneity creates inconsistent cross-modal patterns (ASD), fusion introduces noise rather than signal.

### Mechanism 3
- Claim: Cross-lingual generalization requires language-specific or multilingual pre-trained models; English-centric models degrade severely on non-English tasks.
- Mechanism: Text foundation models encode language-specific semantic and syntactic patterns during pre-training. When the target language differs from pre-training corpora, representations are impoverished, causing the text modality to become the weak link in fusion. Multilingual models (mE5-L, mGTE-B) with broader language coverage produce meaningful representations across languages.
- Core assumption: Language coverage in pre-training correlates with downstream representation quality for that language.
- Evidence anchors:
  - [Section V-A1]: "On the ADReSS-M dataset... training set is overwhelmingly English... test set is entirely Greek... E5-Large WF1 drops to 61.2%. In contrast, the multilingual model mE5-L shows substantially better performance with a WF1 of 72.3%"
  - [Section V-E4]: "English-centric models struggle to generate meaningful representations for non-English languages, leading to a low-quality text modality"
  - [corpus]: Related work on multilingual depression detection (Leveraging LLMs for Multilingual Depression Detection, arxiv 2504.04891) confirms language-matched models outperform cross-lingual transfer, supporting this mechanism.
- Break condition: If target language has minimal representation in multilingual model pre-training (e.g., Dutch in mGTE-B underperforms on ASDBank), even multilingual models may fail.

## Foundational Learning

- **Concept: Self-supervised foundation models (SSL)**
  - Why needed here: The entire FEND framework relies on frozen pre-trained encoders (WavLM, HuBERT, E5, etc.) that were trained via SSL objectives. Understanding that these models learn general representations without labeled medical data is essential for interpreting why frozen features work.
  - Quick check question: Can you explain why WavLM's "denoising masked speech modeling" objective might help it capture prosodic features relevant to depression detection?

- **Concept: Multi-modal fusion strategies (early vs. late, attention-based vs. tensor-based)**
  - Why needed here: FEND benchmarks 8 fusion methods (MFN, GraphMFN, MulT, MFM, TFN, LMF, MMIM, Attention). Understanding the tradeoffs—sequence-level captures temporal interactions but requires alignment; utterance-level is simpler but loses granularity—is critical for method selection.
  - Quick check question: Given a dataset where speech and text are not temporally aligned, which fusion categories from FEND would be inapplicable?

- **Concept: Modality imbalance in multi-modal learning**
  - Why needed here: A central finding is that fusion often fails to beat the best mono-modal model because one modality dominates optimization. Recognizing this failure mode and knowing mitigation techniques (OGM-GE, PMR) is essential for practical deployment.
  - Quick check question: If you observe that a multi-modal model achieves 85% accuracy but the speech-only baseline achieves 87%, what hypothesis should you test first?

## Architecture Onboarding

- **Component map:**
  Input (Speech Audio / Text Transcript)
      ↓
  Foundation Model Encoder (Frozen: WavLM-Large / E5-Large / mE5-L)
      ↓
  Representation Vectors (128-dim, downsampled to 200 seq length)
      ↓
  [Mono-modal path] → MLP Classifier (3-layer, 256 hidden)
  [Multi-modal path] → Fusion Module (Attention/TFN/MFN/MulT/etc.) → Classifier
      ↓
  Binary/Multi-class Output (AD / Depression / ASD)

- **Critical path:** The representation extraction from foundation models is the bottleneck. The paper uses frozen backbones with a uniform training protocol (lr=1e-3, 80 epochs, dropout=0.2). Any modification to this path (unfreezing, different pooling, dimension changes) will affect comparability with the benchmark.

- **Design tradeoffs:**
  1. **Frozen vs. fine-tuned backbones:** Frozen ensures fair comparison and isolates representation quality; fine-tuning may improve performance but introduces confounds.
  2. **Fusion method selection:** Attention-based (MulT) robust across tasks; tensor fusion (TFN) strong for AD; but no single method dominates all.
  3. **Language model selection:** English-only datasets → E5-Large; multilingual/cross-lingual → mE5-L; Chinese-specific → Qwen2.5-7B or Baichuan2.7B.

- **Failure signatures:**
  - Fusion accuracy < best mono-modal accuracy → modality imbalance; try OGM-GE or PMR.
  - Cross-lingual performance collapse → language mismatch; switch to multilingual text encoder.
  - ASD fusion underperforms mono-modal → dataset heterogeneity; fusion may not be appropriate; rely on speech-only.

- **First 3 experiments:**
  1. **Reproduce mono-modal benchmark:** Select one dataset (e.g., ADReSS for AD), run WavLM-Large (speech) and E5-Large (text) through the frozen encoder + MLP pipeline. Verify WF1 matches reported values (~81-84%).
  2. **Test multi-modal fusion with and without imbalance mitigation:** On a dataset where fusion underperforms (e.g., MODMA), compare baseline fusion vs. OGM-GE/PMR-modified fusion. Confirm mitigation closes the gap.
  3. **Cross-corpus generalization test:** Train on DAIC-WOZ (English depression), test on EATD (Chinese depression). Observe degradation; then swap E5-Large for mE5-L or Qwen2.5-7B and measure improvement.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can novel, domain-specific multi-modal fusion architectures be developed that consistently outperform mono-modal baselines across all neuropsychiatric disorders?
- **Basis in paper:** [explicit] The authors state in Section VI that there is a "pressing need to develop and open-source multi-modal fusion methods specifically tailored for neuropsychiatric disease detection" because current methods are adapted from emotion recognition and often fail to capture unique disorder patterns.
- **Why unresolved:** The study found that multi-modal fusion failed to surpass the best mono-modal models on several datasets (e.g., ASDBank, ADReSS-M), largely due to modality imbalance and the use of generic fusion strategies.
- **What evidence would resolve it:** A new fusion architecture specifically designed for speech-text neuropsychiatric assessment that achieves statistically significant improvements over the WavLM-Large and E5-Large mono-modal baselines reported in the benchmark.

### Open Question 2
- **Question:** To what extent does Automatic Speech Recognition (ASR) quality limit the performance of text-based foundation models in multi-lingual disorder detection?
- **Basis in paper:** [explicit] The authors acknowledge that "transcription errors are a potential source of noise" and suggest in Section VI that "developing models that are more robust to ASR errors... represents a significant avenue for future research."
- **Why unresolved:** While the study utilized powerful ASR systems (Whisper, SenseVoice), it did not isolate the impact of transcription errors on the subsequent text embeddings, leaving a potential performance bottleneck unquantified.
- **What evidence would resolve it:** A comparative analysis of model performance using ground-truth human transcripts versus ASR-generated transcripts across the multi-lingual datasets (e.g., Chinese, Greek, Dutch) included in FEND.

### Open Question 3
- **Question:** Do parameter-efficient fine-tuning (PEFT) or end-to-end training paradigms yield significant performance gains over the frozen feature extraction approach used in FEND?
- **Basis in paper:** [explicit] In the conclusion, the authors explicitly list "Investigating advanced training paradigms, such as full end-to-end fine-tuning or parameter-efficient fine-tuning (PEFT)" as a future direction to explore the "upper bounds of performance."
- **Why unresolved:** The FEND framework deliberately froze the backbones of foundation models to ensure fair comparability, meaning the potential improvements gained by adapting the pre-trained models to the specific pathology of AD, depression, or ASD remain unknown.
- **What evidence would resolve it:** Experiments applying LoRA or full fine-tuning to the top-performing models (e.g., WavLM-Large) and comparing the results against the frozen MLP baseline provided in the paper.

## Limitations
- Fusion method superiority is task- and dataset-specific; no single approach consistently outperforms others across all disorders and datasets.
- Language model dependency limits cross-lingual generalization; English-centric models degrade severely on non-English tasks.
- Dataset heterogeneity limits fusion applicability; ASD datasets exhibit high variability, making cross-modal pattern learning unreliable.

## Confidence
- **High Confidence**: Mono-modal performance of frozen foundation models (WavLM-Large for speech, E5-Large for text) on English datasets; cross-lingual degradation with English-centric models; modality imbalance as a consistent failure mode.
- **Medium Confidence**: Task-specific effectiveness of fusion methods; superiority of multilingual models over English-centric models for cross-lingual tasks; generalizability of findings to clinical settings.
- **Low Confidence**: Fusion performance on ASD across diverse datasets; real-world applicability of benchmarks without fine-tuning; robustness of results to changes in preprocessing pipeline (e.g., sequence downsampling method).

## Next Checks
1. **Reproduce mono-modal benchmarks**: Implement frozen WavLM-Large and E5-Large pipelines on ADReSS (AD) and DAIC-WOZ (depression) datasets. Verify WF1 scores match reported values (~81-84%) to confirm representation extraction quality.
2. **Test fusion with modality rebalancing**: On MODMA (where fusion underperforms mono-modal), compare baseline fusion vs. OGM-GE/PMR-modified fusion. Confirm that rebalancing closes the performance gap, validating modality imbalance as the failure mode.
3. **Cross-corpus generalization with language-matched models**: Train on DAIC-WOZ (English depression), test on EATD (Chinese depression). Measure performance drop with E5-Large; then substitute mE5-L or Qwen2.5-7B and quantify improvement to validate language-specific representation requirements.