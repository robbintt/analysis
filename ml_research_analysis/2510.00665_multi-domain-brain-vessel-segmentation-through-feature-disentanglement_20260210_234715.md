---
ver: rpa2
title: Multi-Domain Brain Vessel Segmentation Through Feature Disentanglement
arxiv_id: '2510.00665'
source_url: https://arxiv.org/abs/2510.00665
tags:
- segmentation
- domain
- vessel
- brain
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of segmenting brain vessels
  across multiple imaging modalities (MRA, CTA, MRV) without requiring domain-specific
  model design or data harmonization. The authors propose a semi-supervised domain
  adaptation framework that employs feature disentanglement to manipulate vessel appearances
  while preserving spatial information like shapes and locations.
---

# Multi-Domain Brain Vessel Segmentation Through Feature Disentanglement

## Quick Facts
- arXiv ID: 2510.00665
- Source URL: https://arxiv.org/abs/2510.00665
- Authors: Francesco Galati; Daniele Falcetta; Rosa Cortese; Ferran Prados; Ninon Burgos; Maria A. Zuluaga
- Reference count: 23
- Primary result: Achieves Dice scores of 69.9% for multi-center MRA, 74.5% for MRA-to-CTA, and 67.5% for MRA-to-MRV adaptation

## Executive Summary
This paper addresses the challenge of segmenting brain vessels across multiple imaging modalities (MRA, CTA, MRV) without requiring domain-specific model design or data harmonization. The authors propose a semi-supervised domain adaptation framework that employs feature disentanglement to manipulate vessel appearances while preserving spatial information like shapes and locations. The method achieves state-of-the-art performance across all cross-modal adaptation scenarios, particularly excelling at bridging the challenging gap between arterial and venous structures.

## Method Summary
The proposed framework consists of two phases: Phase 1 learns a disentangled latent space using adversarial training with StyleGAN2, while Phase 2 trains an encoder for label-preserving image-to-image translation using cycle-consistency and segmentation losses. A novel label preservation mechanism ensures translations maintain vessel geometry and positioning. The method requires only 3 annotated target slices for effective adaptation and outperforms existing approaches across MRA-to-CTA, MRA-to-MRV, and multi-center MRA scenarios.

## Key Results
- Achieves Dice scores of 69.9% for multi-center MRA adaptation
- Achieves Dice scores of 74.5% for MRA-to-CTA adaptation
- Achieves Dice scores of 67.5% for MRA-to-MRV adaptation, outperforming state-of-the-art approaches

## Why This Works (Mechanism)

### Mechanism 1
Feature disentanglement enables selective manipulation of vessel appearance while preserving spatial geometry. Path length regularization transforms the StyleGAN2 latent space W into a semantically organized space where different directions correspond to independent, controllable aspects of variation. This allows the encoder to map domain-specific features (intensities, textures) separately from domain-invariant features (vessel shapes, locations, densities).

### Mechanism 2
Two-phase training isolates adversarial instability to Phase 1, enabling stable cross-domain translation in Phase 2. Phase 1 trains G and D adversarially to build a unified disentangled latent space. Phase 2 freezes G and trains E with cycle-consistency and segmentation losses. Removing D in Phase 2 prevents penalization of "hybrid" translations that don't match either domain's pure distribution—critical for artery-to-vein translation.

### Mechanism 3
Label preservation mechanism ensures translations maintain vessel geometry by enforcing segmentation consistency. A label-synthesis branch predicts segmentation masks for both source reconstruction and cross-domain translation, optimized with Dice + cross-entropy losses using the same source annotation. This forces latent codes to share spatial features, excluding position/shape from translation.

## Foundational Learning

- **StyleGAN2 Architecture and Latent Space W+**: The generator builds on StyleGAN2's mapping network and style injection to create a semantically rich, disentangled latent space. Understanding W+ (extended latent space with per-layer style controls) is essential for debugging translation quality.
  - Quick check: Can you explain how path length regularization differs from R1 regularization, and why the former enables disentanglement?

- **Cycle-Consistency for Unpaired Image Translation**: Phase 2 uses cycle-consistency (source→target→source) with reconstruction losses to enforce that translations are reversible and meaningful without paired data.
  - Quick check: What would happen if cycle-consistency were removed—would the model still preserve vessel geometry?

- **Semi-Supervised Domain Adaptation Setup**: The framework requires labeled source data (N=35 volumes), unlabeled target data (M volumes), and minimal labeled target data (m=3 slices). Understanding this split is critical for planning data collection.
  - Quick check: Why does the paper use m=3 slices rather than m=0 (fully unsupervised) for target annotations?

## Architecture Onboarding

- **Component map**: 
  - Input images → Encoder (E) → Latent codes (W+) → Generator (G) → Output images
  - Generator (G) → Label-synthesis branch (G_lsb) → Segmentation masks
  - Domain flag (d) conditions Encoder (E) for source vs. target domain

- **Critical path**:
  1. Phase 1: Train G + D adversarially (250k iterations) with L_adv + L_R1 + L_pl
  2. Pre-train E + G_lsb on source-only data (15k iterations)
  3. Phase 2: Train E + G_lsb with cycle-consistency and segmentation losses (20k iterations); G frozen except G_lsb
  4. Inference: For target image x_t, generate both ˆx_t (reconstruction) and ˆx_s (translation); average their segmentation predictions

- **Design tradeoffs**:
  - 2.5D vs. 3D: Processing 2D slices reduces memory but may affect vessel continuity across slices; clDice metric suggests topology is preserved
  - Residual connections: Ablation shows Dice drops from 72.2% to 14.4% when removed—critical for spatial information flow
  - Intensity inversion: Required for unsupervised MRA-to-MRV (veins are dark, arteries bright); not needed in semi-supervised setting
  - Domain-specific batch normalization (DSBN): Provides ~2.9% Dice improvement; recommended for multi-domain training

- **Failure signatures**:
  - Vessel displacement: Translations resize brain or shift vessel positions; indicates insufficient label preservation
  - Vessel vanishing: Fewer vessels in translation than input; suggests style transfer dominates over content preservation
  - Partial transformation: Only some veins transform to artery-like appearance; indicates encoder can't link distant vessel types without guidance

- **First 3 experiments**:
  1. Reproduce intra-domain performance on OASIS-3: Train Phase 1 + Phase 2 with source-only data; target Dice ~73.7% on held-out MRA slices. Validates pipeline before cross-domain experiments.
  2. Ablate residual connections: Remove skip connections from E→G; expect dramatic Dice drop (>50 points). Confirms residual connections are critical infrastructure.
  3. Test m=0 vs. m=3 target annotations on MRA-to-MRV: Compare unsupervised (no target labels) vs. semi-supervised (3 slices) settings. Paper shows m=0 with intensity inversion fails (Dice 0.1%); m=3 achieves 67.5%. Validates that minimal target supervision is essential for cross-vessel-type adaptation.

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on path length regularization for disentanglement lacks strong corpus validation compared to alternative approaches
- Two-phase training strategy assumes sufficient disentanglement from Phase 1, with limited empirical validation
- Requirement for minimal target annotations (m=3 slices) may limit real-world applicability
- 2.5D approach may miss vessel continuity across slices that 3D models could capture

## Confidence
- **High confidence**: Intra-domain MRA segmentation performance (~73.7% Dice) and ablation results showing critical importance of residual connections (>50-point Dice drop)
- **Medium confidence**: Cross-domain adaptation claims (74.5% MRA-to-CTA, 67.5% MRA-to-MRV) given limited comparison to single-domain baselines
- **Medium confidence**: Label preservation mechanism effectiveness, though vessel displacement issues in baselines support the need for geometry preservation

## Next Checks
1. **Ablate the two-phase training strategy**: Remove Phase 2 freezing and train G + D + E together; compare stability and translation quality to the proposed approach.
2. **Test on additional modalities**: Apply the framework to MRV-to-CTA and CTA-to-MRV translations to verify generalization beyond the MRA-centric evaluation.
3. **Vary m=3 annotation requirement**: Systematically test m=0, m=1, m=5, m=10 target slices to quantify the minimum supervision needed for effective cross-vessel-type adaptation.