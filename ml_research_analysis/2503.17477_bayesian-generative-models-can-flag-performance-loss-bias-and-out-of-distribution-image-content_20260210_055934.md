---
ver: rpa2
title: Bayesian generative models can flag performance loss, bias, and out-of-distribution
  image content
arxiv_id: '2503.17477'
source_url: https://arxiv.org/abs/2503.17477
tags:
- skin
- uncertainty
- bias
- generative
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the need for uncertainty quantification (UQ)
  in generative models, particularly variational autoencoders (VAEs), to detect performance
  loss, bias, and out-of-distribution (OOD) content in medical imaging. The authors
  propose SLUG, a scalable UQ method that extends the Sketched Lanczos Uncertainty
  (SLU) algorithm using stochastic trace estimators to handle high-dimensional image
  data.
---

# Bayesian generative models can flag performance loss, bias, and out-of-distribution image content

## Quick Facts
- arXiv ID: 2503.17477
- Source URL: https://arxiv.org/abs/2503.17477
- Authors: Miguel López-Pérez; Marco Miani; Valery Naranjo; Søren Hauberg; Aasa Feragen
- Reference count: 32
- This work addresses the need for uncertainty quantification (UQ) in generative models, particularly variational autoencoders (VAEs), to detect performance loss, bias, and out-of-distribution (OOD) content in medical imaging.

## Executive Summary
This paper introduces SLUG (Sketched Lanczos Uncertainty Global), a scalable uncertainty quantification method for detecting performance degradation, racial bias, and out-of-distribution content in dermatological image generation. The method extends Sketched Lanczos Uncertainty to handle high-dimensional image data using stochastic trace estimators, providing both image-level and pixel-wise uncertainty scores. Experiments on Fitzpatrick17k, PASSION, and ISIC datasets demonstrate that SLUG effectively identifies racial bias across skin tone subgroups and flags OOD elements like rulers or ink, offering a practical solution for safeguarding clinical deployment of generative models.

## Method Summary
SLUG extends the Sketched Lanczos Uncertainty (SLU) algorithm by incorporating stochastic trace estimators to scale uncertainty quantification to high-dimensional image data. It computes uncertainty scores at both image and pixel levels, making it suitable for flagging reconstruction errors, racial bias, and out-of-distribution artifacts. The method is applied to a residual VAE trained with ELBO and perceptual loss, using datasets split by Fitzpatrick skin tone scale. Implementation uses JAX/Flax with specific training parameters including 1000 epochs, batch size 64, and 500 stochastic trace samples for SLUG computation.

## Key Results
- SLUG strongly correlates with reconstruction error and racial bias across skin tone subgroups
- Outperforms standard VAE encoder variances for uncertainty quantification
- Effectively highlights OOD content (rulers, ink) in dermatological images

## Why This Works (Mechanism)
The method works by approximating the predictive covariance of the generative model using the Sketched Lanczos algorithm combined with stochastic trace estimation. This approach captures uncertainty in the decoder's predictions without computing the full high-dimensional covariance matrix. The combination of perceptual loss and the SLUG uncertainty measure creates a sensitive detector for both systematic bias (racial) and incidental anomalies (OOD content), addressing the fundamental challenge that standard VAE uncertainties fail to capture meaningful reconstruction uncertainty in high-dimensional spaces.

## Foundational Learning

### VAE Uncertainty Quantification
**Why needed:** Standard VAEs use encoder variance as uncertainty, which poorly correlates with reconstruction quality in high-dimensional spaces
**Quick check:** Verify that standard VAE encoder variance shows weak correlation with reconstruction error on test set

### Sketched Lanczos Algorithm
**Why needed:** Direct computation of predictive covariance is intractable for high-dimensional images
**Quick check:** Confirm SLUG computation completes with S=500 samples on 128×128 images

### Hutchinson Stochastic Trace Estimator
**Why needed:** Provides unbiased trace estimation of implicit matrices without explicit formation
**Quick check:** Verify trace estimates converge with increasing number of samples

## Architecture Onboarding

### Component Map
VAE Encoder -> Latent Space -> VAE Decoder -> SLUG Uncertainty Estimator -> Output Uncertainty Map

### Critical Path
Data preprocessing (resize to 128×128) → VAE training (ELBO + Perceptual Loss) → SLUG computation (Lanczos + trace estimator) → Uncertainty analysis

### Design Tradeoffs
SLUG trades computational complexity for accurate uncertainty estimation; the 500-sample stochastic estimator provides good approximation but increases runtime compared to simpler methods

### Failure Signatures
- If SLUG shows no correlation with MSE, the Lanczos implementation likely has errors
- If uncertainty maps appear uniform, the trace estimator variance may be too low
- If training fails to converge, perceptual loss weight may be unbalanced with ELBO

### First Experiments
1. Train VAE on Fitzpatrick17k Mixed set and verify reconstruction quality
2. Compute SLUG scores on held-out test sets and check correlation with MSE
3. Generate pixel-wise uncertainty maps for images containing OOD content

## Open Questions the Paper Calls Out

### Open Question 1
**Question:** What are the true underlying causes of performance degradation and racial bias in dermatological VAEs beyond simple subgroup underrepresentation?
**Basis in paper:** [explicit] The authors state, "Discovering the true causes of bias therefore remains an important open challenge" after observing that bias persists even when training exclusively on darker skin tones.
**Why unresolved:** Representation rates do not fully explain the error gap; the authors hypothesize "higher variability" or "relaxed inclusion criteria" but do not verify these factors.
**What evidence would resolve it:** Ablation studies controlling for image complexity (e.g., texture variance) and data collection protocols across skin tones to isolate the specific drivers of the error.

### Open Question 2
**Question:** Can pixel-wise uncertainty quantification be scaled efficiently for high-resolution 2D images or 3D medical imaging volumes?
**Basis in paper:** [explicit] The authors identify this as the "main limitation," noting that "computation of pixelwise uncertainty does not scale well, posing a bottleneck for high-resolution or 3D images."
**Why unresolved:** The current stochastic trace estimator implementation is computationally prohibitive for the dense per-pixel mapping required by larger volumetric data.
**What evidence would resolve it:** Demonstration of a modified SLUG algorithm operating on 3D CT or MRI data with tractable memory usage and runtime.

### Open Question 3
**Question:** Can the SLUG method be adapted for modern latent diffusion architectures to detect bias in generative foundation models?
**Basis in paper:** [inferred] The introduction notes VAEs are often combined with other models like Stable Diffusion, yet the method is validated strictly on standard VAEs.
**Why unresolved:** The mathematical formulation relies on the VAE decoder's Jacobian and GGN matrix; it is unclear if this approximates uncertainty effectively in the iterative denoising loops of diffusion models.
**What evidence would resolve it:** Experiments applying SLUG (or a variant thereof) to the latent space of a diffusion model to check for similar correlations with reconstruction error and bias.

## Limitations
- Unspecified VAE architecture hyperparameters (latent dimension, residual block depth)
- Unknown perceptual loss backbone network
- Computationally prohibitive for high-resolution or 3D medical imaging volumes
- Limited validation to standard VAE architectures only

## Confidence
- High confidence: SLUG can detect OOD content in dermatological images
- Medium confidence: SLUG correlates with reconstruction error and racial bias
- Medium confidence: SLUG outperforms standard VAE encoder variances for UQ

## Next Checks
1. Verify SLUG correlates positively with MSE on held-out test sets; standard VAE encoder variance should show weaker correlation
2. Replicate the bias detection claim by measuring SLUG score differences between FST 1-2 and FST 5-6 subgroups in the Mixed set
3. Implement the full SLU algorithm with S=500 stochastic samples and compare runtime and uncertainty estimates against the standard Hutchinson estimator baseline