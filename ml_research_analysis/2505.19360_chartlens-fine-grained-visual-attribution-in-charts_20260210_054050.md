---
ver: rpa2
title: 'ChartLens: Fine-grained Visual Attribution in Charts'
arxiv_id: '2505.19360'
source_url: https://arxiv.org/abs/2505.19360
tags:
- chart
- charts
- visual
- attribution
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of visual attribution in chart
  understanding by multimodal large language models (MLLMs), where models often generate
  hallucinations that conflict with visual data. The authors introduce ChartLens,
  a novel algorithm that combines heuristic-guided segmentation with SAM (Segment
  Anything Model) and LineFormer to identify chart elements, then uses set-of-marks
  prompting with MLLMs for fine-grained visual attribution.
---

# ChartLens: Fine-grained Visual Attribution in Charts

## Quick Facts
- arXiv ID: 2505.19360
- Source URL: https://arxiv.org/abs/2505.19360
- Reference count: 40
- Authors: Manan Suri; Puneet Mathur; Nedim Lipka; Franck Dernoncourt; Ryan A. Rossi; Dinesh Manocha
- One-line primary result: ChartLens achieves 26-66% improvements over baselines across bar, line, and pie charts using fine-grained visual attribution

## Executive Summary
This paper addresses the challenge of visual attribution in chart understanding by multimodal large language models (MLLMs), where models often generate hallucinations that conflict with visual data. The authors introduce ChartLens, a novel algorithm that combines heuristic-guided segmentation with SAM (Segment Anything Model) and LineFormer to identify chart elements, then uses set-of-marks prompting with MLLMs for fine-grained visual attribution. They also present ChartVA-Eval, a benchmark containing 1200+ samples with real-world charts from diverse domains like finance and policy, featuring fine-grained attribution annotations. ChartLens achieves 26-66% improvements over baselines across bar, line, and pie charts, demonstrating robust performance in grounding textual responses to specific visual elements in charts.

## Method Summary
ChartLens is a post-hoc fine-grained visual attribution system for charts that identifies specific visual elements supporting an MLLM's textual response. The method uses heuristic-guided preprocessing (Otsu thresholding, contour detection) refined by SAM for bar and pie charts, or LineFormer for line charts. These segmented regions are labeled with alphanumeric marks, then presented to GPT-4o via set-of-marks (SoM) prompting with chain-of-thought examples for validation and attribution. The system is evaluated on ChartVA-Eval, a benchmark of 1200+ real-world charts from finance and policy domains, using IoU≥0.9 matching for precision/recall/F1 metrics on bar/pie charts and detection rate/average area for line charts.

## Key Results
- ChartLens achieves 26-66% improvement in F1 scores over baselines across bar, line, and pie charts
- Outperforms zero-shot GPT-4o (3.30-22.77 F1), Kosmos-2 (10.86-29.21 F1), and LISA (7.91-19.28 F1) on attribution tasks
- Reduces Chart% area by 3-50 times compared to LISA for line chart attribution
- Demonstrates 69.28 F1 on AITQA, 64.14 F1 on ChartQA, and 34.65 F1 on PlotQA datasets

## Why This Works (Mechanism)

### Mechanism 1
Combining heuristic-guided segmentation with SAM refinement produces more accurate chart element masks than either approach alone. Classical computer vision (Otsu thresholding, contour detection, solidity/area filtering) generates initial candidate regions → SAM receives n sampled points from each candidate as prompts → SAM produces refined masks that suppress noise through weaker IoU scores on non-coherent elements. Core assumption: Heuristics provide sufficiently accurate initial proposals that SAM can correct; SAM's spatial coherence detection distinguishes chart elements from background artifacts. Evidence: abstract states "combines heuristic-guided segmentation with SAM"; section 5.1 confirms "more flexible and accurate segmentation process"; ChartCitor (arxiv:2502.00989) addresses similar attribution challenges but does not use SAM-based refinement. Break condition: Low-contrast images or unusual chart styles where heuristics fail to generate viable SAM prompts.

### Mechanism 2
Set-of-Marks (SoM) prompting with labeled regions enables MLLMs to perform fine-grained visual attribution that direct coordinate prediction cannot achieve. Segmented regions receive alphanumeric labels → marked image presented to MLLM with chain-of-thought prompt → model performs validation (consistency check) then attribution (identifying specific labeled elements) → output references discrete marks rather than continuous coordinates. Core assumption: MLLMs have emergent grounding capabilities activated by explicit visual anchors; the model can reason about labeled regions without fine-tuning. Evidence: abstract states "uses set-of-marks prompting with MLLMs for fine-grained visual attribution"; section 5.2 explains "SoM prompting is effective because it enables explicit localization within the image"; Yang et al. 2023 introduced SoM for general visual grounding. Break condition: Charts with too many elements (label space exhaustion); overlapping elements where marks create visual clutter.

### Mechanism 3
LineFormer-based segmentation with domain partitioning handles line chart attribution better than point-based SAM prompting. LineFormer (transformer architecture) extracts line instances using global context → each detected line divided into equally spaced segments along horizontal domain → segments serve as fine-grained attribution marks → MLLM prompted to identify point pairs bounding the attribution region. Core assumption: Lines have sufficient structural coherence for transformer-based instance segmentation; segmenting by domain extent preserves semantic meaning for reasoning tasks. Evidence: section 5.1 states "Lines present unique challenges... LineFormer effectively addresses these challenges"; section 6.2 shows "ChartLens reduces Chart% area by ≈ 3-50 times". Break condition: Dense multi-line charts with >5-6 intersecting lines; line charts with non-uniform x-axis spacing where equal segmentation loses precision.

## Foundational Learning

- **Segment Anything Model (SAM) with point prompts**: Understanding how point-based prompting works (sample points from heuristic regions → SAM generates masks). *Why needed here*: Core segmentation refinement step; poor point selection yields bad masks. *Quick check question*: Given 3 points inside a candidate bar region, will SAM extend the mask to include adjacent grid lines?

- **Set-of-Marks visual prompting**: How alphanumeric overlays on image regions improve MLLM grounding. *Why needed here*: Enables the attribution step; without understanding SoM, the prompting strategy is opaque. *Quick check question*: If a chart has 47 bars but SoM uses labels 1-9 and A-Z (36 total), how does the system handle overflow?

- **Post-hoc vs. inline attribution**: Decoupling attribution from generation allows plug-and-play with any chart QA system. *Why needed here*: Design rationale for the overall architecture. *Quick check question*: Why does post-hoc attribution potentially miss elements that influenced the model's reasoning but weren't strictly necessary for the answer?

## Architecture Onboarding

- **Component map**: Chart Image → [Heuristic Preprocessor: Otsu, contours, filtering] → [SAM Refiner: point prompts → masks] OR [LineFormer for line charts] → Mark Generator (labels, bounding boxes) → SoM Prompt Constructor → MLLM (GPT-4o) → Attribution Parser → Output (element IDs + validation flag)

- **Critical path**: Segmentation quality → mark labeling clarity → MLLM grounding accuracy. The heuristic→SAM pipeline for bars/pie is the most failure-prone segment; LineFormer for lines is more robust but limited to that chart type.

- **Design tradeoffs**: (1) Single MLLM call with chain-of-thought vs. separate validation/attribution calls—paper uses single call for efficiency but risks error propagation. (2) IoU≥0.9 matching threshold for evaluation—high threshold rewards precision over recall. (3) Synthetic chart augmentation for AITQA—increases diversity but may not reflect real-world chart complexity.

- **Failure signatures**: (1) Low F1 on PlotQA vs. AITQA/ChartQA (34.65 vs. 69.28/64.14) suggests difficulty with multi-attribution questions. (2) LISA achieves high detection rate on lines but with 40-63% chart area coverage—indicates coarse, non-specific grounding. (3) Zero-shot GPT-4o bounding box prompting fails (F1 3.30-22.77) without visual marks.

- **First 3 experiments**:
  1. Reproduce bar chart segmentation on ChartQA subset (100 samples): compare heuristic-only vs. heuristic+SAM pipeline on IoU with ground truth bars.
  2. Ablation on number of SAM prompt points per candidate (n=1, 3, 5, 10): measure mask IoU vs. compute time to find optimal sampling density.
  3. Test SoM label space limits: generate synthetic charts with 20, 35, 50, 75 elements and measure attribution F1 degradation as labels exceed alphanumeric capacity.

## Open Questions the Paper Calls Out
The paper explicitly identifies several promising directions for future work, including improving robustness across chart styles, extending the approach to handle text-based reasoning alongside visual attribution, and exploring how to generalize beyond the three chart types currently supported. The authors acknowledge that their current approach focuses primarily on visual chart elements and does not account for textual components like captions, labels, or titles. They also note that inaccuracies in the segmentation process may result in imperfect or incomplete attributions, suggesting the need for better error detection and recovery mechanisms.

## Limitations
- Current approach focuses primarily on visual chart elements and does not account for textual components like captions, labels, or titles
- Performance degradation on PlotQA (34.65 F1) suggests difficulty with multi-attribution questions requiring more complex reasoning
- Limited to three chart types (bar, line, pie) with no evaluation on scatter plots, heatmaps, or other visualization formats

## Confidence
- **High confidence**: SAM refinement improving over heuristic-only segmentation (strong empirical evidence with specific IoU improvements)
- **Medium confidence**: Set-of-Marks prompting enabling fine-grained attribution (novel application, moderate empirical support)
- **Low confidence**: Generalization to highly stylized or non-standard chart types (no testing on infographics, scatter plots, or domain-specific visualizations)

## Next Checks
1. **Label space stress test**: Systematically evaluate attribution performance as chart element count increases from 10 to 100 elements, measuring F1 degradation and identifying the point where alphanumeric label exhaustion causes performance collapse.
2. **SAM prompt density optimization**: Conduct controlled experiments varying the number of sampled points per candidate region (n=1, 3, 5, 10) to establish the optimal tradeoff between segmentation accuracy and computational cost.
3. **Cross-dataset robustness analysis**: Apply ChartLens to InfoChartQA (infographic charts) and ChartCitor (non-standard visualizations) to assess generalization beyond the standard bar/line/pie chart formats used in the primary evaluation.