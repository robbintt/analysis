---
ver: rpa2
title: 'ZeroSumEval: Scaling LLM Evaluation with Inter-Model Competition'
arxiv_id: '2504.12562'
source_url: https://arxiv.org/abs/2504.12562
tags:
- https
- arxiv
- games
- language
- game
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces ZeroSumEval, a novel competition-based evaluation\
  \ protocol for Large Language Models (LLMs) that leverages zero-sum games to provide\
  \ dynamic, scalable, and robust benchmarks resistant to saturation. Unlike traditional\
  \ static benchmarks that often suffer from data contamination, overfitting, and\
  \ high costs, ZeroSumEval employs a diverse suite of games\u2014including Chess,\
  \ Poker, MathQuiz, Gandalf, Liar's Dice, Debate, and PyJail\u2014to evaluate various\
  \ AI capabilities such as strategic reasoning, knowledge application, and creativity."
---

# ZeroSumEval: Scaling LLM Evaluation with Inter-Model Competition

## Quick Facts
- arXiv ID: 2504.12562
- Source URL: https://arxiv.org/abs/2504.12562
- Reference count: 26
- ZeroSumEval introduces competition-based evaluation using zero-sum games to benchmark LLM capabilities across diverse scenarios

## Executive Summary
ZeroSumEval presents a novel evaluation framework that addresses the limitations of traditional static benchmarks by employing zero-sum games as dynamic evaluation environments. The framework evaluates LLMs across seven distinct game types including Chess, Poker, MathQuiz, Gandalf, Liar's Dice, Debate, and PyJail, measuring capabilities ranging from strategic reasoning to creativity and jailbreaking resistance. By leveraging automated verification and prompt abstraction, ZeroSumEval provides scalable and robust evaluation resistant to overfitting and data contamination. Extensive experiments with over 7,000 simulations across 13 models reveal that while frontier models excel in common games, they struggle with novel question generation and creative reasoning tasks.

## Method Summary
ZeroSumEval operates by having LLMs compete against each other in zero-sum game environments, where success requires outperforming the opponent rather than simply achieving absolute correctness. The framework employs automated verification to ensure solution validity and uses prompt abstraction to maintain evaluation consistency across different model families. Each game targets specific capabilities: strategic reasoning (Chess, Poker), knowledge application (MathQuiz, Gandalf), creativity (Debate), and security robustness (PyJail, Liar's Dice). The evaluation protocol includes both competitive gameplay and adversarial components where models attempt to jailbreak or outmaneuver opponents. Performance is measured through win rates, solution validity, and success in breaking opponent defenses, providing a multifaceted assessment of model capabilities.

## Key Results
- GPT-4o and Claude-3.7-sonnet achieve highest win rates in common games but fail at novel question generation tasks
- Thinking models show superior jailbreaking success rates but are most vulnerable to instruction-following attacks
- Instruct models with Chain of Thought prompting generally outperform native thinking models in competitive scenarios
- PyJail success rates for breaking self-generated sandboxes remain below 2%, highlighting generation difficulties

## Why This Works (Mechanism)
The framework leverages competitive dynamics to create adaptive evaluation scenarios that resist benchmark saturation. By pitting models against each other in zero-sum games, ZeroSumEval forces models to demonstrate not just correctness but strategic superiority. The automated verification ensures solution validity while prompt abstraction maintains consistency across model families. This combination creates scalable, dynamic benchmarks that can continuously challenge models as they improve, preventing the overfitting issues common in static benchmarks.

## Foundational Learning
- Zero-sum game theory: Required for understanding competitive dynamics where one player's gain equals another's loss
  - Quick check: Verify that the sum of payoffs equals zero across all game outcomes
- Automated verification protocols: Essential for scalable evaluation without human intervention
  - Quick check: Test verification accuracy on known solutions across different game types
- Prompt abstraction techniques: Needed to maintain consistency when evaluating diverse model architectures
  - Quick check: Compare performance consistency when using abstracted vs. model-specific prompts

## Architecture Onboarding
Component map: Game Engine -> Model Interface -> Automated Verifier -> Score Calculator -> Result Aggregator

Critical path: Game Engine generates scenarios → Model Interface submits responses → Automated Verifier validates solutions → Score Calculator computes outcomes → Result Aggregator stores performance metrics

Design tradeoffs: The framework balances between comprehensive evaluation coverage and computational efficiency, choosing automated verification over human evaluation to enable large-scale testing while accepting potential verification errors

Failure signatures: Models failing to generate valid solutions, automated verifier producing false negatives, score calculation errors in complex multi-round games

First experiments:
1. Run single-round Chess games with two models to verify basic game engine functionality
2. Test automated verifier accuracy using known correct/incorrect solutions from MathQuiz
3. Validate score calculation consistency across multiple game types with predefined outcomes

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Why do instruct models prompted with Chain of Thought (CoT) generally outperform native "thinking" models in competitive game environments, contrary to the expectation that explicit reasoning traces improve performance?
- Basis in paper: The authors note in Section 4.2 that "instruct models with CoT prompting generally perform better across games" (Figure 6b), describing this finding as "surprising" given the general belief that CoT improves outcomes.
- Why unresolved: The paper identifies the phenomenon but only speculates that the "chain of thought can be incorrect" in specific instances; it does not provide a theoretical or empirical explanation for why native reasoning capabilities might fail compared to prompted ones in this context.
- What evidence would resolve it: A comparative error analysis of the reasoning traces generated by native thinking models versus CoT-prompted instruct models to identify where the logic diverges or fails.

### Open Question 2
- Question: Is there an inherent tradeoff between a model's creative reasoning capabilities and its robustness to instruction-following attacks?
- Basis in paper: In Section 4.1, the authors observe that while thinking models are the most successful at jailbreaking opponents (infiltrating), they are also the most susceptible to being coerced into revealing secrets, suggesting a "tradeoff between creativity and instruction following."
- Why unresolved: The paper documents the correlation between attacking success and defensive failure in thinking models but does not determine if this is a structural limitation of the architecture or a result of specific training objectives.
- What evidence would resolve it: Experiments varying the "creativity" temperature or reasoning depth of models to see if defensive robustness decreases linearly as offensive creativity increases.

### Open Question 3
- Question: What specific mechanisms prevent state-of-the-art models from generating valid, solvable, and challenging test cases (e.g., math problems or PyJail sandboxes) for opponents?
- Basis in paper: The abstract and Section 4.1 highlight that while models can play common games, they "struggle to play games that require creating novel and challenging questions," with PyJail success rates for breaking self-generated sandboxes being < 2%.
- Why unresolved: The paper demonstrates the failure of the verification step (models failing to solve their own challenges) but leaves open whether this is due to a lack of meta-cognitive self-consistency or simply the difficulty of the generation task itself.
- What evidence would resolve it: An analysis of the "invalid" states: determining if models generate unsolvable tasks, or if they simply lose the solution context during the verification phase.

## Limitations
- Limited model diversity with only 13 models tested across the evaluation framework
- Focus on zero-sum games may not capture all relevant LLM capabilities and real-world applications
- Automated verification processes may introduce bias or errors in complex game scenarios
- The framework requires substantial computational resources for large-scale evaluation

## Confidence
- Dynamic benchmark resistance to saturation: Medium - Framework shows promise but needs broader validation
- Frontier model performance limitations: Medium - Results based on specific game contexts, not comprehensive
- Automated verification reliability: Medium - Efficiency gains come with potential accuracy trade-offs

## Next Checks
1. Test the framework's scalability by evaluating at least 50 additional models across different size categories and training approaches, including specialized models for specific domains
2. Conduct cross-task validation by applying the framework to non-game scenarios and comparing results with established benchmarks to verify generalizability
3. Perform ablation studies to determine the impact of automated verification on evaluation accuracy, including human verification of a subset of results to establish ground truth confidence levels