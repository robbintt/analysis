---
ver: rpa2
title: 'CaughtCheating: Is Your MLLM a Good Cheating Detective? Exploring the Boundary
  of Visual Perception and Reasoning'
arxiv_id: '2507.00045'
source_url: https://arxiv.org/abs/2507.00045
tags:
- reasoning
- arxiv
- mllms
- clues
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces CaughtCheating, a benchmark designed to evaluate
  the detective-level visual perception and reasoning capabilities of multimodal large
  language models (MLLMs). The benchmark consists of 100 real-world images collected
  from social media, where users request detection of subtle, context-dependent suspicious
  clues that contradict claims in shared photos.
---

# CaughtCheating: Is Your MLLM a Good Cheating Detective? Exploring the Boundary of Visual Perception and Reasoning

## Quick Facts
- arXiv ID: 2507.00045
- Source URL: https://arxiv.org/abs/2507.00045
- Reference count: 40
- Primary result: Even the best-performing model (GPT-o3) achieves only 26.0% accuracy in detecting subtle, context-dependent clues in real-world images

## Executive Summary
This work introduces CaughtCheating, a benchmark designed to evaluate the detective-level visual perception and reasoning capabilities of multimodal large language models (MLLMs). The benchmark consists of 100 real-world images collected from social media, where users request detection of subtle, context-dependent suspicious clues that contradict claims in shared photos. The images are split evenly into Clued and Unclued categories, with detailed annotations including primary questions, deterministic and non-deterministic clues, and decomposed questions for in-depth analysis. Extensive experiments show that even the best-performing model, GPT-o3, achieves only 26.0% accuracy and 17.2% IoU in detecting deterministic clues, with an overall F1 score of 23.9%. These results reveal that current MLLMs struggle with tasks requiring nuanced detective-level reasoning, as they often fail to identify subtle cues or relate them to social context. The study highlights the need for improved visual perception and reasoning in MLLMs to match human-level detective capabilities.

## Method Summary
The study creates a benchmark called CaughtCheating consisting of 100 real-world images from social media where users request detection of suspicious clues in photos that may contradict claims. The images are annotated with primary questions, deterministic clues (clear evidence), non-deterministic clues (ambiguous evidence), and decomposed sub-questions. Researchers evaluate 12 leading MLLMs including GPT-o3, Gemini, and Claude on their ability to identify clues and reason about their significance. Performance is measured using accuracy, Intersection over Union (IoU), and F1 scores across both clued and unclued images, with particular focus on the detection of deterministic clues that represent clear evidence.

## Key Results
- GPT-o3, the top-performing model, achieves only 26.0% accuracy in detecting deterministic clues
- Average IoU for deterministic clue detection across all models is just 17.2%
- Overall F1 score for clue detection is 23.9%, indicating poor performance on detective-level reasoning tasks
- Models struggle particularly with non-deterministic clues requiring contextual interpretation

## Why This Works (Mechanism)
The benchmark works by presenting MLLMs with realistic scenarios where subtle visual cues contradict explicit claims in social media images. Models must integrate visual perception with contextual reasoning to identify suspicious elements, mimicking the cognitive processes humans use when evaluating the authenticity of shared photos. The challenge lies in detecting clues that are not explicitly highlighted and understanding their social implications, requiring both fine-grained visual analysis and higher-order reasoning about human behavior and deception.

## Foundational Learning
- **Visual perception in MLLMs**: Why needed - to identify subtle visual cues that may indicate deception; Quick check - can the model detect objects, relationships, and anomalies in complex scenes
- **Contextual reasoning**: Why needed - to understand how visual clues relate to social claims and cultural contexts; Quick check - can the model connect visual evidence to implied narratives
- **Multimodal integration**: Why needed - to combine visual information with textual prompts and social context; Quick check - does the model maintain coherence between visual and textual reasoning
- **Detective-level reasoning**: Why needed - to evaluate whether clues are suspicious and warrant further investigation; Quick check - can the model assess the significance of evidence beyond mere detection
- **Social media context awareness**: Why needed - to understand typical social media behaviors and claim-validation dynamics; Quick check - does the model recognize common patterns in shared photo contexts

## Architecture Onboarding

**Component map**: Input Image -> Visual Encoder -> Feature Extraction -> Reasoning Module -> Output Detection

**Critical path**: The visual encoder must accurately process image details, while the reasoning module must integrate these features with contextual understanding to identify suspicious clues and evaluate their significance.

**Design tradeoffs**: Models must balance between detailed visual analysis (which may capture subtle clues) and broader contextual reasoning (which may miss fine details), with current architectures tending to optimize for one at the expense of the other.

**Failure signatures**: Models consistently miss non-deterministic clues requiring cultural context, fail to connect subtle visual anomalies to social implications, and struggle with images where clues are intentionally obscured or require multiple-step reasoning.

**First experiments**:
1. Test model performance on synthetic images with varying clue explicitness to isolate visual perception from contextual reasoning
2. Evaluate models on benchmark images with explicit textual hints to measure baseline visual detection capabilities
3. Compare performance across different cultural contexts to assess generalization of detective reasoning

## Open Questions the Paper Calls Out
The paper does not explicitly call out additional open questions beyond the main findings about MLLM limitations in detective-level reasoning.

## Limitations
- Small dataset size (only 100 images) may limit generalizability and increase variance in performance estimates
- Real-world social media images introduce uncontrolled variability in image quality, context, and cultural interpretations
- Binary classification of clues as "deterministic" versus "non-deterministic" may oversimplify real-world complexity
- Evaluation metrics focus on explicit clue detection rather than nuanced reasoning capabilities

## Confidence
- **High confidence**: Current MLLMs struggle with detective-level visual perception and reasoning tasks, as evidenced by consistently low performance across multiple models and metrics
- **Medium confidence**: The benchmark effectively evaluates detective-level reasoning, though small dataset size and uncontrolled variables may affect results
- **Medium confidence**: Low performance reflects fundamental MLLM limitations rather than benchmark design issues

## Next Checks
1. Replicate the study with a significantly larger dataset (minimum 500 images) to improve statistical reliability and assess whether performance trends hold with increased sample size
2. Conduct cross-cultural validation by testing the benchmark with diverse human annotators from different cultural backgrounds to establish whether the task difficulty reflects model limitations or cultural/contextual ambiguities
3. Implement ablation studies varying image quality, context information, and clue explicitness to determine which factors most significantly impact MLLM performance and whether targeted improvements in specific capabilities could yield substantial gains