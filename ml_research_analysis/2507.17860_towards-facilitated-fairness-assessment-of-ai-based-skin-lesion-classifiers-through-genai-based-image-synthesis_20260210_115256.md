---
ver: rpa2
title: Towards Facilitated Fairness Assessment of AI-based Skin Lesion Classifiers
  Through GenAI-based Image Synthesis
arxiv_id: '2507.17860'
source_url: https://arxiv.org/abs/2507.17860
tags:
- skin
- synthetic
- type
- fairness
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study develops a diffusion-based synthetic data generation
  pipeline for fairness auditing of melanoma classifiers. By training LightningDiT
  on ISIC dermoscopic images with demographic conditioning, the authors produce attribute-controlled
  synthetic cohorts that enable intersectional subgroup analysis.
---

# Towards Facilitated Fairness Assessment of AI-based Skin Lesion Classifiers Through GenAI-based Image Synthesis

## Quick Facts
- arXiv ID: 2507.17860
- Source URL: https://arxiv.org/abs/2507.17860
- Reference count: 40
- Three melanoma classifiers tested on synthetic and real data show consistent TPR disparity patterns across sex, age, and skin type, validating synthetic data for fairness auditing.

## Executive Summary
This study develops a diffusion-based synthetic data generation pipeline to audit fairness in melanoma classifiers. By training a LightningDiT model on ISIC dermoscopic images with demographic conditioning, the authors produce attribute-controlled synthetic cohorts that enable intersectional subgroup analysis. Testing three public melanoma classifiers on real and synthetic data reveals consistent TPR disparity patterns across sex, age, and Fitzpatrick skin type, confirming synthetic images' utility for systematic fairness assessment.

## Method Summary
The method trains a LightningDiT latent diffusion transformer on 500K ISIC dermoscopic images, conditioning generation on demographic attributes via CLIP text embeddings. The model generates balanced synthetic cohorts covering all combinations of sex, age groups, and skin types. These synthetic images are then used to audit three pretrained melanoma classifiers (DeepGuide, MelaNet, SkinLesionDensenet), measuring TPR disparity (ΔTPR) across demographic groups and comparing results to real data benchmarks.

## Key Results
- Synthetic images reproduce similar fairness trends as real data: MelaNet shows sex ΔTPR=0.0115 and skin-type ΔTPR=0.1322 on synthetic data matching real patterns
- Intersectional analysis enabled by balanced synthetic cohorts reveals consistent bias patterns across all demographic combinations
- Domain alignment matters: classifiers trained on ISIC (MelaNet, SkinLesionDensenet) show better real-synthetic correlation than those trained on different data (DeepGuide)

## Why This Works (Mechanism)

### Mechanism 1: Distributional Alignment via Latent Diffusion
The diffusion model learns P(X,A) where X is image and A is demographic attributes, sampling from conditional distributions P(X|A). When classifiers process these samples, their errors reflect learned correlations between X and A from training data, mirroring real-world biases.

### Mechanism 2: Intersectional Coverage via Controlled Prompting
Full-factorial prompt design creates balanced cohorts for under-represented subgroups, reducing statistical noise in fairness metrics. The 2×8×6 factorial design generates equal batches for every combination, preventing majority groups from dominating analysis.

### Mechanism 3: Proxy Evaluation via Generator-Detector Alignment
Fairness audits are most reliable when synthetic generator and target classifier originate from the same data distribution. Performance drops primarily due to distributional shifts between generator's training data and classifier's training data.

## Foundational Learning

- **Concept: Latent Diffusion Models (LDMs)** - Why needed: LightningDiT is an LDM that compresses images to latent space before noise removal. Quick check: Why use VA-VAE instead of standard VAE? (Check Section 3.1 regarding semantic structure).

- **Concept: True Positive Rate (TPR) Disparity (ΔTPR)** - Why needed: Specific fairness metric defined as max(TPR) - min(TPR) across groups. Quick check: Why calculate ΔTPR only on positive cases rather than using Accuracy or F1-score?

- **Concept: Classifier-Free Guidance (CFG)** - Why needed: Sampling technique balancing prompt adherence versus image quality/diversity. Quick check: Did higher CFG improve FID for skin lesions? What does this imply about model stability?

## Architecture Onboarding

- **Component map:** ISIC Archive -> Preprocessing (256×256 resize) -> VA-VAE Encoding -> CLIP Text Embeddings -> LightningDiT Generation -> Euler Sampler (400 steps, CFG=4) -> Decode to Pixels -> Classifiers -> TPR Calculation

- **Critical path:** Train VA-VAE, then DiT on ISIC (80K steps, batch 1024) -> Generate full-factorial sampling (81,600 images) -> Run through 3 pretrained classifiers -> Compute per-group TPR and ΔTPR

- **Design tradeoffs:** Sampling Quality vs. Speed (400 steps optimal but expensive vs. default 200) vs. Control vs. Realism (controls metadata attributes but fails to control raw pixel intensity)

- **Failure signatures:** Age Non-Convergence (TPR curves for synthetic Age data fail to converge with real data) vs. Nearest Neighbor Overlap (generated images look identical to training images)

- **First 3 experiments:** 1) Hyperparameter Sweep (CFG 4-8, Steps 200-400) on small subset to confirm FID optimal points vs. Appendix Table 1 vs. 2) Attribute Isolation Test (vary only Skin Type while holding Age/Sex constant) vs. 3) Sanity Check (run MelaNet on synthetic "Melanoma" images)

## Open Questions the Paper Calls Out

1. Can demographically balanced synthetic cohorts be used effectively to train fairer melanoma classifiers, rather than just auditing existing ones? (Section 5.5 explicitly states next step is to use synthetic images for training fairly)

2. Can concept-based fairness evaluation methods identify the specific visual features driving observed disparities in synthetic audits? (Section 5.3 notes lack of method for discovering causes of unfairness)

3. Does extending synthetic generation to multiple diagnostic categories enable reliable calculation of equalized odds? (Section 5.4 aims to extend generator to multiple categories for additional fairness criteria)

4. Can Vision-Language Models act as reliable automated judges to filter low-quality synthetic images before fairness auditing? (Section 5.2 suggests VLM-as-a-judge for quality evaluation)

## Limitations
- Distributional alignment mechanism effectiveness not thoroughly validated in corpus
- Intersectional coverage depends on CLIP encoder's ability to disentangle attributes
- Proxy evaluation requires same data distribution source, limiting generalizability

## Confidence
- High confidence: Core mechanism of using synthetic data for fairness auditing is validated by consistent TPR disparity patterns
- Medium confidence: Intersectional coverage approach is promising but depends on generative model's ability to produce rare combinations
- Low confidence: Distributional alignment mechanism's effectiveness needs more empirical support

## Next Checks
1. Perform domain adaptation study to quantify impact of distributional shifts between generator and classifier training data
2. Conduct ablation study varying prompt granularity to test robustness of attribute control
3. Compare fairness trends using alternative metrics (Equal Opportunity Difference, Demographic Parity Difference) to verify ΔTPR robustness