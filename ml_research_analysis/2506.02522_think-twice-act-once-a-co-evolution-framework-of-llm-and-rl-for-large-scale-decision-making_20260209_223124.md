---
ver: rpa2
title: 'Think Twice, Act Once: A Co-Evolution Framework of LLM and RL for Large-Scale
  Decision Making'
arxiv_id: '2506.02522'
source_url: https://arxiv.org/abs/2506.02522
tags:
- learning
- usage
- llms
- policy
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Agents Co-Evolution (ACE), a framework that
  combines large language models (LLMs) and reinforcement learning (RL) for large-scale
  decision-making in industrial settings like power grid operations. The key innovation
  is a dual-role trajectory refinement mechanism where LLMs act as both Policy Actor
  (refining suboptimal actions via multi-step reasoning) and Value Critic (performing
  temporal credit assignment through trajectory-level reward shaping) during RL training.
---

# Think Twice, Act Once: A Co-Evolution Framework of LLM and RL for Large-Scale Decision Making

## Quick Facts
- **arXiv ID**: 2506.02522
- **Source URL**: https://arxiv.org/abs/2506.02522
- **Reference count**: 40
- **Primary result**: ACE framework achieves up to 145% improvement over expert-guided RL baselines in power grid operations with action spaces exceeding 60K discrete actions

## Executive Summary
This paper introduces Agents Co-Evolution (ACE), a framework that combines large language models (LLMs) and reinforcement learning (RL) for large-scale decision-making in industrial settings like power grid operations. The key innovation is a dual-role trajectory refinement mechanism where LLMs act as both Policy Actor (refining suboptimal actions via multi-step reasoning) and Value Critic (performing temporal credit assignment through trajectory-level reward shaping) during RL training. This enables effective guidance while maintaining real-time performance during deployment. ACE demonstrates state-of-the-art performance across three L2RPN competition environments with action spaces exceeding 60K discrete actions, achieving up to 145% improvement over expert-guided RL baselines. The framework requires significantly fewer samples (287-682 LLM refinements vs. 100K+ samples for baselines) while maintaining competitive real-time performance (38.7s test time vs. 46.1s for expert-guided RL).

## Method Summary
ACE uses Soft Actor-Critic (SAC) as its RL backbone with a dual-role LLM module that acts as both Policy Actor and Value Critic. The LLM Actor refines low-reward transitions (r < r̄) through multi-step reasoning and environment validation, while the LLM Critic performs trajectory-level reward shaping by identifying key decision points. A mixed buffer combines RL and LLM-refined transitions with reward-based prioritization, enabling bidirectional improvement between LLM and RL. The framework requires only 287-682 LLM refinements compared to 100K+ samples for traditional approaches, using Qwen2-7B or GPT-4 as the LLM backbone with LoRA fine-tuning every 100 refined samples.

## Key Results
- Achieves up to 145% improvement over expert-guided RL baselines across three L2RPN environments
- Requires only 287-682 LLM refinements versus 100K+ samples for traditional RL approaches
- Maintains competitive real-time performance (38.7s test time vs. 46.1s for expert-guided RL)
- Demonstrates state-of-the-art performance in power grid topology optimization with action spaces exceeding 60K discrete actions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: LLMs can improve RL sample efficiency in large action spaces through selective action refinement on low-reward transitions
- **Mechanism**: The LLM (as Policy Actor) identifies transitions where reward r < r̄, converts state-action pairs to natural language, generates refined actions via multi-step reasoning, and validates these through environment simulation before storing in a separate LLM buffer. This targets RL's early-stage exploration weakness where Q-function estimates and policy exploration are unreliable in vast action spaces (>60K discrete actions).
- **Core assumption**: LLMs possess sufficient domain reasoning to propose better actions than early-stage RL policies; text-based state representations preserve decision-relevant information.
- **Evidence anchors**: [abstract] "the Actor refines suboptimal actions via multi-step reasoning and environment validation"; [section 3.2] "For mini-batches sampled from the replay buffer DRL, we identify transitions where the reward r < r̄, marking them as inappropriate decisions that require refinement."
- **Break condition**: If LLM-refined actions consistently yield lower rewards than RL actions after validation, the mechanism degrades to noise injection.

### Mechanism 2
- **Claim**: LLMs can perform trajectory-level credit assignment that approximates non-parametric TD(λ) with better handling of long-term dependencies
- **Mechanism**: The LLM (as Value Critic) processes complete episode trajectories, identifies key decision points through counterfactual reasoning, and assigns discretized reward adjustments {-2K, -K, +K, +2K}. This differs from exponential decay assumptions in standard eligibility traces by using semantic reasoning to identify causally important states.
- **Core assumption**: LLMs can infer causal relationships between actions and delayed outcomes from trajectory text descriptions; discretized adjustments prevent reward instability.
- **Evidence anchors**: [abstract] "the Critic performs temporal credit assignment through trajectory-level reward shaping"; [section 3.2] "This mechanism can be viewed as an extension of implicit multi-step TD(λ): LLMs approximate eligibility traces through trajectory-level reasoning"
- **Break condition**: If K values are too large relative to base rewards, or if LLM identifies spurious correlations, Q-value estimation becomes unstable.

### Mechanism 3
- **Claim**: Reward-weighted experience mixing enables bidirectional improvement between LLM and RL with minimal LLM calls
- **Mechanism**: A mixed buffer Dmix combines RL and LLM-refined transitions with sampling distribution weighted by reward improvement (Equation 8). This buffer serves dual purposes: (1) RL trains with prioritized high-quality experiences, (2) LLM fine-tunes on task-specific successful trajectories via LoRA. The co-evolution is sample-efficient because LLM is only queried 287-682 times vs. 100K+ RL samples.
- **Core assumption**: LLM-refined experiences that yield higher rewards than RL actions contain learnable patterns transferable to future refinement; RL policies can generalize from limited high-quality demonstrations.
- **Evidence anchors**: [abstract] "RL agent enhances LLMs' task-specific decision-making with high-quality fine-tuning datasets generated via prioritized experience replay"; [section 4.4] "ACE requires only 287 LLM refinements to achieve state-of-the-art performance, compared to 100K samples needed by traditional approaches"
- **Break condition**: If β (LLM buffer ratio) is too high, RL overfits to LLM biases; if too low, LLM guidance is insufficient.

## Foundational Learning

- **Concept**: Soft Actor-Critic (SAC) with entropy regularization
  - **Why needed here**: ACE uses SAC as its RL backbone; understanding off-policy learning, Q-function updates (Equation 2), and entropy-exploration trade-offs is essential for debugging why early-stage policies need LLM guidance.
  - **Quick check question**: Can you explain why SAC's entropy term helps in large action spaces but still struggles with 60K+ discrete actions?

- **Concept**: Experience replay and prioritization
  - **Why needed here**: ACE's mixed buffer mechanism builds on standard replay; reward-based importance weights (Equation 8) require understanding how sampling distributions affect policy gradient estimates.
  - **Quick check question**: What happens to policy learning if high-reward LLM refinements are always sampled but represent a narrow distribution of states?

- **Concept**: LLM fine-tuning with LoRA (Low-Rank Adaptation)
  - **Why needed here**: ACE performs online LLM fine-tuning every 100 samples; understanding parameter-efficient fine-tuning helps assess memory/compute tradeoffs and potential catastrophic forgetting.
  - **Quick check question**: Why might frequent LoRA updates on small batches (200 samples) fail to improve LLM guidance quality?

## Architecture Onboarding

- **Component map**: RL Module (SAC agent) -> LLM Actor (fLLM) -> LLM Critic (gLLM) -> LLM Buffer (DLLM) -> Mixed Buffer (Dmix) -> Fine-tuning Module (LoRA)

- **Critical path**:
  1. RL interacts with environment → stores transitions in DRL
  2. fLLM samples low-reward transitions → generates refined actions → validates via simulation → stores in DLLM
  3. gLLM samples high-impact trajectories → adjusts rewards → updates DLLM
  4. Training loop samples from Dmix with reward-weighted priorities → updates Q-function and policy
  5. Periodic LLM fine-tuning on Dmix

- **Design tradeoffs**:
  - **Inference cost vs. guidance quality**: GPT-4 requires no SFT but has API costs; Qwen2-7B requires SFT infrastructure but improves over time
  - **Query frequency vs. compute overhead**: Higher fLLM/gLLM frequency improves early convergence but increases training time (1h 48m for 508 inferences)
  - **Bad-case threshold (r)**: r=0.3 includes 510 samples (more compute, potential noise); r=-0.3 includes 83 samples (faster but slower convergence)

- **Failure signatures**:
  - **Survival rate plateaus at ~77%**: Indicates LLM SFT is not occurring or ineffective
  - **Episode rewards decrease after gLLM activation**: K values too large, causing Q-value instability
  - **Test time explodes (>1000s)**: LLM being used during deployment instead of offline-only
  - **Refined rewards consistently below RL actions**: LLM lacks domain knowledge; requires better prompts or SFT

- **First 3 experiments**:
  1. **Ablate fLLM only**: Remove Policy Actor, keep Value Critic. Expected: ~30% reward drop (48.3 vs 69.8 on WCCI 2020), confirming action refinement is the primary driver.
  2. **Vary bad-case threshold**: Test r ∈ {-0.3, 0, 0.3} with fixed query interval. Expected: r=0 balances sample count and quality; extremes show 6% degradation.
  3. **Test generalization to unseen scenarios**: Train on 288 scenarios, evaluate on 576. Expected: Performance gap between high/low query frequencies narrows, suggesting reduced activation frequency is viable with diverse training data.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can improved semantic encoding of actions bridge the performance gap for action-based trajectory selection in the LLM Value Critic?
  - **Basis in paper**: [explicit] The authors observe in Table 3 that action-based trajectory selection yields "significantly limited performance" compared to state or reward-based selection, attributing it to LLMs struggling with "abstract topological changes."
  - **Why unresolved**: It is undetermined whether this failure is inherent to LLMs' inability to reason over topology or simply a failure of the specific action-to-text conversion function ($T_a$) used in the experiment.
  - **What evidence would resolve it**: Ablation studies using descriptive action encodings (e.g., natural language descriptions of switches) versus the raw indices used in the paper to see if performance aligns with state-based methods.

- **Open Question 2**: How robust is the ACE framework in scenarios where the "multi-round reasoning" validation step is computationally infeasible?
  - **Basis in paper**: [inferred] Section 4.3 describes a "multi-round reasoning" trick that validates LLM proposals via Grid2Op simulation, retrying up to five times if rewards are inferior.
  - **Why unresolved**: The framework's success appears partially dependent on this expensive validation loop. In industrial settings where high-fidelity simulation is slow, this "Think Twice" phase could become a training bottleneck.
  - **What evidence would resolve it**: Evaluating the convergence speed and final performance of ACE when the simulation budget for multi-round validation is restricted or removed entirely.

- **Open Question 3**: Does the co-evolution mechanism remain stable in environments with sparse rewards where the RL agent cannot generate high-quality fine-tuning data for the LLM early on?
  - **Basis in paper**: [inferred] Section 3.3 relies on the RL agent generating "high-quality fine-tuning datasets" via prioritized experience replay to update the LLM.
  - **Why unresolved**: If the RL agent explores poorly in early stages (a known issue in sparse reward settings), the LLM may be fine-tuned on low-quality or noisy data, potentially causing the co-evolution to collapse.
  - **What evidence would resolve it**: Testing ACE on a sparse reward variant of the power grid or a different industrial control task to analyze if the LLM's guidance degrades without initial high-quality RL trajectories.

## Limitations
- The neural network architectures for RL policy and Q-function are underspecified beyond embedding dimensions
- The LLM reward shaping mechanism lacks theoretical grounding for discretized credit assignment stability
- The selection criteria for "BAD actions" in trajectory processing are vaguely defined
- The claim that LLMs can perform meaningful causal credit assignment through text processing remains empirically supported but theoretically unproven

## Confidence
- **High Confidence**: The core sample efficiency claims (287-682 LLM refinements vs. 100K+ RL samples) are well-supported by ablation studies and comparison baselines. The survival rate improvements (77% vs 39.5%) are directly measurable and consistent across experiments.
- **Medium Confidence**: The dual-role LLM mechanism (Actor and Critic) shows clear performance benefits, but the specific contributions of action refinement versus reward shaping are confounded. The claim that LLMs approximate TD(λ) through trajectory reasoning is plausible but lacks rigorous theoretical justification.
- **Low Confidence**: The assertion that LLMs can reliably perform causal credit assignment through natural language processing of trajectories is the weakest claim. The discretized reward adjustments and selection of "key decision points" are described qualitatively but not validated for correctness or consistency.

## Next Checks
1. **Mechanism Isolation Test**: Run ACE with fLLM disabled (only gLLM active) and gLLM disabled (only fLLM active) to quantify the independent contribution of action refinement versus reward shaping. This will determine whether the trajectory-level credit assignment provides meaningful value beyond selective action refinement.

2. **Reward Shaping Stability Analysis**: Systematically vary K values (0.1, 0.2, 0.4, 0.8) and measure Q-value variance, policy convergence speed, and final performance. Include a control where reward shaping is applied randomly to non-causal transitions to test whether observed improvements are due to genuine credit assignment or noise injection.

3. **Generalization Robustness Test**: Train ACE on a subset of scenarios (e.g., 50% of available data) and evaluate on completely unseen grid configurations and fault patterns. Compare performance degradation against traditional RL baselines to validate whether the co-evolution approach truly learns transferable decision-making principles rather than overfitting to specific scenarios.