---
ver: rpa2
title: 'Flexible Blood Glucose Control: Offline Reinforcement Learning from Human
  Feedback'
arxiv_id: '2501.15972'
source_url: https://arxiv.org/abs/2501.15972
tags:
- reward
- glucose
- patient
- blood
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PAINT, a novel RL framework for learning
  flexible insulin dosing policies from patient records in type 1 diabetes management.
  PAINT uses a sketch-based approach for reward learning, where patients annotate
  their historical data with a continuous reward signal to reflect desired outcomes.
---

# Flexible Blood Glucose Control: Offline Reinforcement Learning from Human Feedback

## Quick Facts
- arXiv ID: 2501.15972
- Source URL: https://arxiv.org/abs/2501.15972
- Reference count: 19
- Primary result: Achieves 15% reduction in glycaemic risk over commercial benchmarks through patient preference sketching

## Executive Summary
This paper introduces PAINT, a novel RL framework for learning flexible insulin dosing policies from patient records in type 1 diabetes management. PAINT uses a sketch-based approach for reward learning, where patients annotate their historical data with a continuous reward signal to reflect desired outcomes. A reward model is trained from this labelled data to inform a safety-constrained offline RL algorithm that restricts actions to safe strategies while enabling preference tuning. Evaluation shows PAINT achieves common glucose goals through simple state labelling, reducing glycaemic risk by 15% over commercial benchmarks. Action labelling enables incorporating patient expertise to pre-empt meals (+10% time-in-range post-meal) and address device errors (-1.6% variance post-error).

## Method Summary
PAINT combines offline RL with human feedback by first training a reward model on patient-annotated glucose/insulin trajectories, then using this model to fine-tune a safety-constrained policy. The framework trains a prior policy on a safe Magni risk function, then modifies TD3+BC to anchor the behavioral cloning term to this prior rather than raw actions. Patient preferences are expressed through continuous sketching of historical data, with stratified sampling improving reward model training on imbalanced labels. The system balances safety constraints with preference tuning through a λ parameter controlling the strength of the prior policy anchor.

## Key Results
- Achieves 15% reduction in glycaemic risk over commercial benchmarks through simple state labelling
- Incorporates patient expertise for pre-meal dosing (+10% time-in-range post-meal) via action labelling
- Handles device errors effectively (-1.6% variance post-error) through error-aware preference learning
- Demonstrates robustness under realistic conditions including limited samples, labelling errors, and patient variability

## Why This Works (Mechanism)

### Mechanism 1
Continuous reward sketching enables precise preference communication for insulin dosing policies. Patients annotate historical glucose/insulin trajectories with a drawn continuous signal (-1 to +1), expressing proximity to desired states. A neural network reward model is trained via MSE loss to predict these labels, then labels the full dataset. Stratified sampling across reward strata improves learning on imbalanced/discrete signals. Core assumption: Patients can reliably identify goal states or desirable actions in retrospect, even without knowing how to achieve them.

### Mechanism 2
Modifying the behavioral cloning term in TD3+BC to anchor to a safety-focused prior policy prevents dangerous preference-driven actions. The agent is pre-trained with a verifiably safe reward (Magni risk function). During preference tuning, the behavioral cloning term is changed from `(π(s) - a)^2` (action-matching) to `(π(s) - π_priori(s))^2` (policy-anchoring). A patient-tunable λ controls preference strength vs. safety constraint. Core assumption: The prior policy trained on Magni risk is sufficiently safe and performant across diverse patient scenarios.

### Mechanism 3
Stratified reward sampling during reward model training improves performance on imbalanced or discrete reward labels. Mini-batches are constructed by sampling equally from k uniformly-spaced, non-overlapping reward strata, ensuring coverage across the reward spectrum during training. Core assumption: Imbalanced reward distributions harm reward model generalization; uniform exposure corrects this.

## Foundational Learning

- **Offline Reinforcement Learning (TD3+BC)**: Enables policy learning from static patient datasets without risky online exploration. Quick check: Can you explain why the behavioral cloning term in TD3+BC prevents out-of-distribution actions?

- **Reward Learning from Human Feedback (RLHF)**: Grounds the policy in patient-specific preferences without requiring hand-crafted reward functions. Quick check: What is the limitation of pairwise preference comparisons for long-horizon tasks like insulin dosing?

- **Constrained Policy Optimization / Safe RL**: Ensures preference-driven tuning does not lead to dangerous glucose excursions. Quick check: How does anchoring to a prior policy differ from standard Lagrangian constraint methods?

## Architecture Onboarding

- **Component map**: Historical patient trajectories + preference-labeled subset -> Reward Model (neural network) -> Prior Policy (TD3+BC on Magni risk) -> Tuned Policy (safety-constrained TD3+BC anchored to prior) -> λ Slider (preference strength control)

- **Critical path**: 1) Collect historical glucose/insulin data from patient 2) Patient annotates subset (~10k samples / ~3 weeks) via sketching interface 3) Train reward model on labeled data (MSE loss, stratified sampling) 4) Train prior policy on Magni risk (safety-focused) 5) Tune policy using reward model + prior-anchored BC term 6) Deploy; patient adjusts λ to calibrate preference strength

- **Design tradeoffs**: Labeling burden vs. performance (1,000 labeled samples yield gains, more improve accuracy); Safety vs. flexibility (higher λ increases preference influence but reduces safety margin); Prior policy quality (poorly-tuned prior limits achievable improvements)

- **Failure signatures**: Preference misalignment (noisy/adversarial labels); Action extrapolation failure (limited demonstrator diversity); Over-constrained policy (too-low λ yields nearly identical behavior to prior); Sensor artifact mishandling (compression lows or CGM errors)

- **First 3 experiments**: 1) Baseline validation: Train PAINT on simulated UVA/Padova patients with simple binary reward labels; verify TIR improvement over PID benchmark 2) Safety anchor stress test: Inject adversarial preference labels; confirm policy does not diverge from safe behavior due to prior anchoring 3) Sample efficiency sweep: Vary labeled data from 250 to 90,000 samples; plot reward/TIR/TBR vs. label count to identify minimum viable labeling burden

## Open Questions the Paper Calls Out

- **Cross-population transfer**: How does PAINT perform with real T1D patients providing actual preference feedback, rather than simulated annotations? The authors state "This work illustrates PAINT's potential in real-world T1D management" and all experiments used simulated patients with simulated feedback functions.

- **Extrapolation limits**: How can offline RL's limited extrapolation ability be overcome when the demonstrator dataset lacks action diversity in critical regions? The paper notes the agent struggles to pre-empt meals because the PID demonstrator has low variance in insulin dosing prior to eating.

- **Domain generalization**: How does PAINT generalise to other safety-critical domains requiring preference adaptation? The framework is described as applicable to "any domain requiring precise user-adaptive control of RL policies subject to pre-defined constraint," but evaluation was limited to the T1D glucose control task only.

## Limitations
- Safety guarantees hinge on the Magni risk-trained prior being universally safe across diverse patient populations, yet its performance under extreme scenarios (illness, pregnancy) is not validated
- Framework's inability to propose novel strategies outside demonstrator variance (e.g., aggressive pre-meal dosing) is acknowledged but not quantified
- Claims about rapid preference learning (few thousand labels) lack comparison to alternative annotation schemes or transfer learning approaches

## Confidence

- **High confidence**: TIR improvements over PID (15%) and post-meal gains (+10%) are well-supported by ablation studies across multiple patient cohorts
- **Medium confidence**: Safety claims rely on simulation assumptions; real-world validation with patient heterogeneity is needed
- **Low confidence**: Claims about rapid preference learning (few thousand labels) lack comparison to alternative annotation schemes or transfer learning approaches

## Next Checks
1. **Cross-population transfer test**: Evaluate PAINT on patients with atypical glucose patterns (dawn phenomenon, exercise sensitivity) to verify prior policy universality
2. **Annotation burden optimization**: Compare sketching vs. simpler feedback mechanisms (thumbs up/down, sliders) for equivalent performance to identify minimal viable labeling effort
3. **Adversarial robustness audit**: Systematically test PAINT's response to maliciously crafted preference labels to quantify safety margin under worst-case user behavior