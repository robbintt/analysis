---
ver: rpa2
title: LLM Output Homogenization is Task Dependent
arxiv_id: '2509.21267'
source_url: https://arxiv.org/abs/2509.21267
tags:
- prompt
- diversity
- temperature
- general
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduces a task-anchored framework to evaluate and
  mitigate output homogenization in large language models. It presents a taxonomy
  of eight task categories, each with distinct concepts of functional diversity, and
  introduces task-anchored functional diversity as a metric.
---

# LLM Output Homogenization is Task Dependent

## Quick Facts
- arXiv ID: 2509.21267
- Source URL: https://arxiv.org/abs/2509.21267
- Authors: Shomik Jain; Jack Lanchantin; Maximilian Nickel; Karen Ullrich; Ashia Wilson; Jamelle Watson-Daniels
- Reference count: 40
- Primary result: Task-anchored framework increases functional diversity for task categories where homogenization is undesired while preserving it where desired

## Executive Summary
This paper addresses the problem of output homogenization in large language models by introducing a task-anchored framework that evaluates and mitigates this issue. The authors present a taxonomy of eight task categories, each with distinct concepts of functional diversity, and introduce task-anchored functional diversity as a metric. Their proposed task-anchored sampling technique increases functional diversity for tasks where it's desired while maintaining homogenization where it's appropriate, outperforming general sampling methods. Crucially, the results challenge the common assumption of a diversity-quality tradeoff by showing that task-dependent quality metrics can achieve both high diversity and high quality simultaneously.

## Method Summary
The authors introduce a two-step task-anchored sampling approach: first classifying prompts into one of eight task categories using an LLM classifier, then applying task-specific system prompts or in-context regeneration prompts based on the classified category. They evaluate functional diversity using an LLM-judge that identifies path-connected components of responses, measuring the number of functionally unique responses out of five generated per prompt. Quality is assessed using task-specific grading checklists rather than generic reward models. The framework was tested across 344 prompts from six datasets using multiple LLM judges (GPT-4o, Claude-4-Sonnet, Gemini-2.5-Flash) and different generation models with temperature 0.5-1.0 and nucleus 0.9.

## Key Results
- Task-anchored sampling significantly increases functional diversity for underspecified and subjective tasks while maintaining homogenization for well-specified objective tasks
- The approach outperforms general prompt-based strategies in eliciting responses with diverse solution strategies and creative elements
- Improved functional diversity is achieved without significant quality loss when using task-dependent quality metrics, challenging the perceived diversity-quality tradeoff
- Classification accuracy varies dramatically by model size (46-56% for smaller models vs 85%+ for larger ones), creating practical deployment constraints

## Why This Works (Mechanism)

### Mechanism 1: Task-Contingent Reward Definition
The system maps prompts to one of eight categories and activates corresponding definitions of "functional diversity." In "Well-Specified" tasks, it enforces homogenization of the final answer; in "Creative" tasks, it enforces diversity of narrative elements. This prevents conflict where generic diversity methods encourage hallucination in factual tasks. The core assumption is that the model can accurately map inputs to the taxonomy.

### Mechanism 2: Inference-Time Instruction Steering
Rather than relying on stochastic sampling, the method injects explicit constraints (e.g., "Generate responses with different problem-solving strategies") to guide the model to sample from distinct, high-probability regions of the latent space. This assumes the model can follow complex, meta-cognitive instructions about its own generation process.

### Mechanism 3: Evaluation Re-framing
By evaluating quality using task-specific "grading checklists" and diversity using "functional diversity," the paper demonstrates that high diversity does not inherently lower quality. Generic reward models tend to penalize unusual phrasing often found in diverse responses, whereas checklist-based evaluation focuses on task completion.

## Foundational Learning

- **Concept: Functional vs. Lexical Diversity**
  - Why needed: The core premise relies on distinguishing between "using different words" (lexical) and "meaning something different" (functional)
  - Quick check: If two math solutions use completely different variables and phrasing but arrive at the same answer via the same algebraic trick, are they functionally diverse according to this paper?

- **Concept: RLHF/DPO Collapse**
  - Why needed: To understand the "problem" the paper is solving - standard alignment techniques often push models toward a single "safe" or "preferred" response style
  - Quick check: Why does standard Reinforcement Learning from Human Feedback (RLHF) tend to reduce the variety of valid responses for subjective tasks?

- **Concept: LLM-as-a-Judge Evaluation**
  - Why needed: The architecture relies on using an LLM to evaluate both "functional diversity" and "quality"
  - Quick check: What is the specific risk of using a generic reward model (like Athene-RM) to evaluate the quality of diverse creative writing responses in this framework?

## Architecture Onboarding

- **Component map:** User Prompt -> Task Classifier -> Prompt Selector -> Generator -> Evaluator
- **Critical path:** The Task Classifier is the single point of failure. If a "Creative Writing" prompt is misclassified as "Well-Specified Objective," the system will artificially restrict diversity.
- **Design tradeoffs:** Latency vs. Accuracy (separate LLM call adds latency but improves relevance), Complexity vs. Generalizability (8-category taxonomy covers many cases but requires manual extension), Cost (LLM-judges more expensive than embedding distances)
- **Failure signatures:** "Hallucinated Diversity" (model claims different strategy but repeats same logic), "Over-Homogenization" (outputs identical due to misclassification), "Checklist Mismatch" (judge misses key requirements)
- **First 3 experiments:** 1) Run 100-200 diverse prompts through classifier and check precision/recall for each category, 2) Compare "General System Prompting" vs "Task-Anchored System Prompting" on held-out set measuring functional diversity, 3) Generate responses for 20 creative prompts and correlate human ratings with LLM-judge scores using task-anchored checklists

## Open Questions the Paper Calls Out

- **Open Question 1:** Does task-anchored functional diversity, as evaluated by LLM-judges, align with human perceptions of meaningful response differences? The authors call for a user study to confirm alignment with human judgments.

- **Open Question 2:** Can the task-anchored taxonomy and functional diversity concepts be effectively adapted for non-English languages? The authors identify their taxonomy as "English-centric" and limit evaluation to English prompts.

- **Open Question 3:** Can task-anchored considerations be embedded directly into a model's learning or reasoning process to prevent behaviors like confabulation? The authors propose exploring embedding these into learning or Chain-of-Thought reasoning.

## Limitations

- The eight-category taxonomy may not generalize to all prompt types without manual extension, with limited quantitative evidence of coverage for domains outside the 344-prompt evaluation set
- Reliance on LLM-judges for both functional diversity assessment and quality grading introduces potential systematic bias, though the ensemble approach mitigates individual model variance
- Classification accuracy variance across model sizes (46-56% for smaller models vs 85%+ for larger ones) creates practical deployment constraints, with limited analysis of misclassification error cascades

## Confidence

- **High Confidence:** Experimental results demonstrating task-anchored sampling outperforms generic approaches for increasing functional diversity
- **Medium Confidence:** Claim that task-dependent quality metrics eliminate the diversity-quality tradeoff requires stronger validation across diverse human evaluators
- **Low Confidence:** Generalizability of the eight-category taxonomy to real-world applications is uncertain, with limited systematic evaluation beyond curated datasets

## Next Checks

1. **Taxonomy Coverage Validation:** Test the eight-category taxonomy on 100+ diverse real-world prompts from production LLM applications. Measure classification accuracy and identify prompt types that fall outside the taxonomy.

2. **Human Evaluator Alignment:** Conduct a human evaluation study comparing LLM-judge assessments with human ratings for both functional diversity and quality. Focus on agreement rates for task-anchored vs. generic evaluation approaches.

3. **Misclassification Impact Analysis:** Systematically induce task classification errors and measure the downstream impact on functional diversity and quality scores. Quantify the sensitivity of the task-anchored approach to classifier errors.