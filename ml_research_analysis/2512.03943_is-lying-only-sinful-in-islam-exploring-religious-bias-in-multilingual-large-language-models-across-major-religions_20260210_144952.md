---
ver: rpa2
title: Is Lying Only Sinful in Islam? Exploring Religious Bias in Multilingual Large
  Language Models Across Major Religions
arxiv_id: '2512.03943'
source_url: https://arxiv.org/abs/2512.03943
tags:
- bias
- religious
- bengali
- english
- islam
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study systematically evaluates religious bias in multilingual
  large language models using BRAND, a dataset of 2,417 religious norms across four
  South Asian religions in English and Bengali. Testing three prompt types across
  five LLMs reveals consistent performance disparities: models achieve near-perfect
  accuracy for Islamic norms but struggle with Buddhism and Hinduism, especially in
  Bengali.'
---

# Is Lying Only Sinful in Islam? Exploring Religious Bias in Multilingual Large Language Models Across Major Religions

## Quick Facts
- arXiv ID: 2512.03943
- Source URL: https://arxiv.org/abs/2512.03943
- Reference count: 40
- Key outcome: Models achieve near-perfect accuracy for Islamic norms but struggle with Buddhism and Hinduism, especially in Bengali, with consistent bias patterns across five multilingual LLMs.

## Executive Summary
This study systematically evaluates religious bias in multilingual large language models using BRAND, a dataset of 2,417 religious norms across four South Asian religions in English and Bengali. Testing three prompt types across five LLMs reveals consistent performance disparities: models achieve near-perfect accuracy for Islamic norms but struggle with Buddhism and Hinduism, especially in Bengali. Misclassification patterns show strong bias toward Islam and Hinduism, with Bengali norms more frequently misclassified than English ones. Label classification reveals models often default to "Normal" when distinguishing between "Expected" and "Taboo" norms. The findings highlight how multilingual LLMs amplify religious bias, particularly in low-resource languages, and underscore the need for culturally inclusive datasets and bias mitigation strategies.

## Method Summary
The study evaluates religious bias using the BRAND dataset containing 2,417 religious norms across four South Asian religions (Islam 26.9%, Hinduism 27.4%, Christianity 24.4%, Buddhism 21.3%) in English and Bengali. Five LLMs (Llama 3 70B, Mistral Saba 24B, Gemma 3 4B, Gemini 2.0 Flash, Qwen 3 32B) were tested using three prompt types: label classification, religion identification, and scope identification. Models were accessed via free-tier APIs with temperature set to 0 for deterministic outputs, and responses were constrained to single words. Evaluation measured accuracy by religion, label, and language, with misclassification patterns analyzed to reveal bias distributions.

## Key Results
- Models achieve near-perfect accuracy for Islamic norms but struggle significantly with Buddhism and Hinduism, especially in Bengali
- Bengali norms are more frequently misclassified than English norms across all religions
- Models systematically default to "Normal" classifications when distinguishing between "Expected" and "Taboo" norms
- 0% misclassification toward Buddhism indicates learned exclusion rather than accurate recognition

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Training data composition drives religious bias patterns differently across languages
- Mechanism: Models internalize statistical associations between languages and dominant religions in their training corpora—Bengali text contains proportionally more Islamic content while English contains more Christian content, creating language-specific default associations
- Core assumption: The observed performance disparities stem primarily from training data imbalances rather than architectural biases
- Evidence anchors: [abstract] "models achieve near-perfect accuracy for Islamic norms but struggle with Buddhism and Hinduism, especially in Bengali"; [section 5.1] "English training corpora contain more balanced religious representation from global sources...while low-resource languages like Bengali exhibit more concentrated religious representation due to limited available training data"

### Mechanism 2
- Claim: Models adopt a neutrality default when lacking contextual religious understanding
- Mechanism: When models cannot distinguish between nuanced categories (Expected vs. Normal vs. Taboo), they systematically default to "Normal" classifications as a safety-oriented fallback, reflecting inadequate representation of religious normative frameworks
- Core assumption: The "Normal" default represents uncertainty avoidance rather than accurate theological understanding
- Evidence anchors: [abstract] "Label classification reveals models often default to 'Normal' when distinguishing between 'Expected' and 'Taboo' norms"; [section 4.1.2] "models predominantly misclassify Normal norms as Expected, with misclassification rates ranging from 82.6% to 94.3%"

### Mechanism 3
- Claim: High accuracy on dominant categories masks systematic exclusion of minority religions
- Mechanism: Models achieve high accuracy by learning to confidently predict overrepresented categories while effectively removing underrepresented religions (like Buddhism in both languages) from their prediction space—the 0% Buddhism misclassification rate indicates not superior recognition but learned exclusion
- Core assumption: Accuracy metrics alone insufficiently capture representational bias; error distribution patterns reveal structural biases
- Evidence anchors: [abstract] "Misclassification patterns show strong bias toward Islam and Hinduism"; [section 5.4] "The 0% misclassification toward Buddhism exemplifies this paradox...Buddhism has fallen below the confidence threshold required for prediction"

## Foundational Learning

- Concept: Multilingual LLM representations
  - Why needed here: Understanding that models process different languages through shared representations that can amplify or distort culturally-specific content
  - Quick check question: Can you explain why translating the same norm between Bengali and English produces different model predictions?

- Concept: Representational bias vs. performance disparity
  - Why needed here: Distinguishing between models performing poorly on certain categories versus systematically misrepresenting or excluding them
  - Quick check question: Why does 0% misclassification toward Buddhism indicate bias rather than accurate recognition?

- Concept: Label classification with normative categories
  - Why needed here: Understanding the theological distinctions between Expected (encouraged), Normal (permissible), and Taboo (prohibited) norms across religions
  - Quick check question: Why might a model struggle to distinguish "Expected" from "Normal" when the boundary varies by religious context?

## Architecture Onboarding

- Component map:
  - BRAND dataset (2,417 entries × 2 languages × 13 features including Label, Scope, Religion)
  - Three prompt templates (religious norm classification, religion identification, scope identification)
  - Five evaluated models (Llama 3 70B, Mistral Saba 24B, Gemma 3 4B, Gemini 2.0 Flash, Qwen 3 32B)
  - Evaluation pipeline measuring accuracy by religion, label, and language

- Critical path:
  1. Dataset creation → validation by religion experts → translation to both languages
  2. Prompt engineering → temperature set to 0 for deterministic outputs → single-word response constraints
  3. Model inference → accuracy calculation → misclassification pattern analysis

- Design tradeoffs:
  - Using AI-generated content (70%) plus scholarly sources (30%) for dataset diversity vs. potential circularity
  - Free-tier API access limits testing scope but enables reproducibility
  - Single-word responses reduce noise but eliminate explanatory context
  - Focusing on four religions/two languages limits generalizability but enables depth

- Failure signatures:
  - Near-perfect accuracy for one religion + poor accuracy for others indicates overrepresentation bias
  - Consistent "Normal" predictions across Expected/Taboo inputs indicates contextual understanding failure
  - Language-dependent accuracy shifts (Bengali → English improvement) signals training data imbalance
  - 0% misclassification toward any category suggests learned exclusion, not accuracy

- First 3 experiments:
  1. Replicate the three prompt types on additional low-resource languages (e.g., Urdu, Tamil) to test whether language-religion associations generalize
  2. Add debiasing interventions (contextual religious label replacement, adversarial training) and measure impact on both accuracy and error distribution patterns
  3. Extend dataset to include intra-religious variation (sects, regional practices) to test whether models recognize religious diversity or default to monolithic representations

## Open Questions the Paper Calls Out

- Question: Do the language-specific bias patterns observed (Bengali-Islam association, English-Christianity association) generalize to other low-resource languages beyond Bengali?
- Question: What are the mechanisms underlying differential model behavior across languages in religious classification tasks?
- Question: How can debiasing strategies be effectively designed to address religious bias while preserving theological accuracy across faith traditions?
- Question: Do these bias patterns persist across different model architectures, including commercial APIs and newer model families not tested in this study?

## Limitations

- The dataset focuses on only four South Asian religions in two languages, limiting generalizability to other religious traditions
- AI-generated content comprises 70% of the dataset, raising potential circular bias concerns
- Single-word response constraints may not reflect real-world usage patterns where models provide contextual explanations
- Free-tier API limitations prevented comprehensive testing of larger models or extended prompt variations

## Confidence

**High Confidence**: Models show near-perfect accuracy for Islamic norms while struggling with Buddhism and Hinduism, particularly in Bengali, across five models and three prompt types.

**Medium Confidence**: Models default to "Normal" classifications when distinguishing between Expected and Taboo norms, though the underlying mechanism requires further validation.

**Low Confidence**: 0% misclassification toward Buddhism indicates learned exclusion rather than accurate recognition, requiring additional evidence for full validation.

## Next Checks

1. Apply the same evaluation framework to additional low-resource languages (Urdu, Tamil, Punjabi) to determine whether language-religion association bias generalizes beyond Bengali-English or represents a specific corpus artifact.

2. Implement and test two debiasing strategies—contextual religious label replacement and adversarial training—to measure impact on both overall accuracy and error distribution patterns.

3. Extend the BRAND dataset to include intra-religious distinctions (Sunni vs. Shia Islam, Vaishnavism vs. Shaivism, Theravada vs. Mahayana Buddhism) to test whether models recognize religious diversity within traditions or default to monolithic representations.