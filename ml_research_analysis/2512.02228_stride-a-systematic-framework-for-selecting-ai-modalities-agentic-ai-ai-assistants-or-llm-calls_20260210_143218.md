---
ver: rpa2
title: 'STRIDE: A Systematic Framework for Selecting AI Modalities -- Agentic AI,
  AI Assistants, or LLM Calls'
arxiv_id: '2512.02228'
source_url: https://arxiv.org/abs/2512.02228
tags:
- stride
- task
- agentic
- reasoning
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: STRIDE provides a systematic framework for selecting between LLM
  calls, AI assistants, and agentic AI based on task characteristics. The method decomposes
  tasks into subtasks, quantifies reasoning depth, tool needs, and state requirements,
  and computes an Agentic Suitability Score to guide modality selection.
---

# STRIDE: A Systematic Framework for Selecting AI Modalities -- Agentic AI, AI Assistants, or LLM Calls

## Quick Facts
- **arXiv ID**: 2512.02228
- **Source URL**: https://arxiv.org/abs/2512.02228
- **Reference count**: 4
- **Primary result**: Systematic framework to classify tasks as requiring LLM calls, AI assistants, or agentic AI based on complexity analysis.

## Executive Summary
STRIDE provides a systematic framework for selecting between LLM calls, AI assistants, and agentic AI based on task characteristics. The method decomposes tasks into subtasks, quantifies reasoning depth, tool needs, and state requirements, and computes an Agentic Suitability Score to guide modality selection. Evaluated on 30 real-world tasks, STRIDE achieved 92% accuracy in modality selection, reduced unnecessary agent deployments by 45%, and cut resource costs by 37% compared to always deploying agents. Expert validation confirmed its practical utility across SRE, compliance, and automation domains.

## Method Summary
STRIDE is a five-stage pipeline that classifies tasks into three AI modalities: LLM calls, AI assistants, or agentic AI. It first decomposes tasks into a directed acyclic graph (DAG) of subtasks using LLM-based parsing, then computes an Agentic Suitability Score (ASS) based on reasoning depth, tool requirements, state management, and risk level. The True Dynamism Score (TDS) isolates workflow-driven variability from tool and model noise. Self-reflection requirements are assessed as a gating condition for agentic AI. Finally, a knowledge base maps the combined ASS, TDS, and reflection signals to a modality recommendation, with weights tuned via grid search and reinforcement learning with expert feedback.

## Key Results
- Achieved 92% accuracy in selecting appropriate AI modality across 30 real-world tasks
- Reduced unnecessary agent deployments by 45% compared to baseline
- Cut resource costs by 37% while maintaining task completion quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing tasks into a DAG reveals structural complexity that predicts modality needs.
- Mechanism: STRIDE breaks down tasks into subtasks and maps dependencies; higher node count and edge density correlate with deeper reasoning and tool orchestration, pushing scores toward agentic AI.
- Core assumption: Subtask graph structure is a reliable proxy for execution-time complexity and autonomy requirements (not proven; evidence is correlational).
- Evidence anchors: [abstract] "STRIDE integrates structured task decomposition..."; [section] "Tasks are decomposed into a directed acyclic graph (DAG) of subtasks..."; [corpus] No direct corpus evidence for DAG-based modality selection (weak/missing).
- Break condition: If decomposition is shallow or dependencies are misidentified for a task class, the Agentic Suitability Score will misclassify (e.g., underestimating complexity for multi-document summarization).

### Mechanism 2
- Claim: The True Dynamism Score isolates workflow-driven variability, avoiding over-engineering from model or tool noise.
- Mechanism: TDS computes W(s) (workflow variability) weighted positively, V(s) (tool volatility) and M(s) (model instability) weighted negatively/normalized, so only tasks with high workflow dynamism trigger agentic recommendations.
- Core assumption: Workflow-induced variability uniquely benefits from autonomous decision-making; tool/model variability can be handled by non-agentic means (retry, prompt tuning). This is plausible but not proven causal.
- Evidence anchors: [abstract] "...quantifies reasoning depth, tool needs, and state requirements, and computes an Agentic Suitability Score..."; [section] "By distinguishing sources of variability, STRIDE avoids over-engineering and activates agentic AI only when autonomous reasoning materially improves task outcomes."; [corpus] Corpus papers discuss adaptivity and autonomy but do not directly validate TDS mechanism.
- Break condition: If tool or model volatility is conflated with workflow dynamism (e.g., an unstable API that requires adaptive orchestration), TDS may under-recommend agents.

### Mechanism 3
- Claim: Self-reflection requirements act as a gating condition for agentic AI when mid-execution correction is needed.
- Mechanism: STRIDE evaluates conditional branching, nondeterministic tools, and validation needs; if TDS ≥ θ and any of these are true, SR(s)=1 triggers reflection hooks (ReAct, re-planning), pushing toward agentic classification.
- Core assumption: Tasks requiring reflection during execution fundamentally need autonomous agents; assistants cannot handle mid-execution correction adequately.
- Evidence anchors: [abstract] "...self-reflection requirement analysis..."; [section] "Self-reflection is required when subtasks involve mid-execution decision points or validation of nondeterministic tools."; [corpus] Related work (Reflexion) shows reflection improves agent performance, but does not prove it is a causal necessity for modality selection.
- Break condition: If reflection is useful but not strictly necessary (e.g., a well-designed assistant with scripted recovery), STRIDE may over-recommend agents.

## Foundational Learning

- Concept: Directed Acyclic Graphs (DAGs) for task dependencies
  - Why needed here: STRIDE represents decomposed subtasks and their ordering/data dependencies as a DAG to compute complexity metrics.
  - Quick check question: Given subtasks A→B→C, what happens to the DAG if B also depends on an external tool output?

- Concept: Variance attribution (model vs tool vs workflow)
  - Why needed here: The True Dynamism Score requires separating sources of variability to avoid misclassifying stochastic or volatile non-workflow factors as agent-requiring dynamism.
  - Quick check question: If an API response time varies due to rate limits, which variability type is this, and should it increase TDS?

- Concept: Self-reflection / meta-cognition in agents
  - Why needed here: STRIDE uses reflection requirements (mid-execution decisions, error recovery) as a key signal for agentic necessity.
  - Quick check question: In a ReAct-style loop, what triggers a "re-think" step, and how would STRIDE detect this need from a task description?

## Architecture Onboarding

- Component map: Task Decomposition Engine (LLM + prompting) → DAG builder → Reasoning & Tool Scoring module → Dynamism Attribution module → Self-Reflection Assessment → Recommendation Engine
- Critical path: Task description → Decomposition → DAG → per-subtask ASS + TDS + SR → aggregate to task profile x_T → classify modality → output recommendation
- Design tradeoffs:
  - Interpretability vs generality: Heuristic scoring functions are transparent but may not generalize to all domains.
  - Weight tuning: Grid search and RL for weights (wr, wt, ws, wρ, α, β, γ) can optimize for specific domains but risks overfitting.
  - Human-in-the-loop calibration: Expert feedback improves alignment but adds latency and subjectivity.
- Failure signatures:
  - Underestimation of complexity in borderline tasks (e.g., multi-document summarization with low TDS but high interdependency).
  - Misattribution of tool volatility as workflow dynamism leading to false negatives.
  - Over-reliance on historical patterns in knowledge base K leading to stale recommendations.
- First 3 experiments:
  1. Replicate the 30-task evaluation with held-out tasks from SRE and compliance to test generalization and measure accuracy, over-engineering reduction, and resource savings.
  2. Ablation study: disable TDS and/or self-reflection components on a subset of tasks to quantify each component's contribution (following Table 4).
  3. Domain shift test: apply STRIDE to a new domain (e.g., customer support ticket routing) without re-tuning weights, and measure accuracy drop; then calibrate weights via grid search to assess adaptation cost.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can STRIDE be effectively extended to evaluate multimodal tasks involving vision and audio inputs?
- Basis in paper: [explicit] The conclusion states, "Looking ahead, we will extend evaluation... to include multimodal tasks (vision/audio)."
- Why unresolved: The current framework and evaluation are limited to text-based modalities (SRE, compliance, automation).
- What evidence would resolve it: A modified STRIDE framework that successfully scores and selects modalities for image or audio-based tasks with comparable accuracy.

### Open Question 2
- Question: Can reinforcement learning (RL) methods outperform the current grid search approach for tuning STRIDE's weighting parameters?
- Basis in paper: [explicit] The authors list plans to "integrate reinforcement learning for weight tuning" as future work.
- Why unresolved: The current methodology relies on grid search and expert feedback, which may be less adaptive or optimal than learned policies.
- What evidence would resolve it: Comparative analysis showing RL-tuned weights achieving higher modality selection accuracy or resource savings than the static grid search baseline.

### Open Question 3
- Question: Does STRIDE maintain its efficiency and accuracy when validated at full enterprise scale?
- Basis in paper: [explicit] The conclusion identifies the need to "validate STRIDE at enterprise scale" beyond the 30-task evaluation.
- Why unresolved: The paper acknowledges the evaluation set, while deep, was "modest in size," leaving high-volume performance unproven.
- What evidence would resolve it: Results from a longitudinal study involving thousands of diverse tasks in a live production environment showing sustained cost reductions.

### Open Question 4
- Question: How can the True Dynamism Score (TDS) be refined to prevent underestimating dynamism in complex, non-agentic tasks?
- Basis in paper: [inferred] Section 4.3 notes that errors arose in "borderline scenarios, such as multi-document summarization, where dynamism was underestimated."
- Why unresolved: The current heuristic scoring sometimes fails to distinguish complex static workflows from dynamic ones.
- What evidence would resolve it: An updated scoring mechanism that correctly classifies high-complexity tasks without falsely recommending agentic AI.

## Limitations

- Framework generalizability limited by 30-task evaluation across SRE, compliance, and automation domains
- Causal vs correlational evidence gap: DAG structure and TDS may not be causally responsible for modality needs
- Weight optimization transparency lacking: final values for weights, coefficients, and threshold not reported

## Confidence

- **High Confidence**: Task decomposition into DAGs can reveal structural complexity; distinguishing sources of variability helps avoid over-engineering; self-reflection requirements are relevant to agentic decision-making
- **Medium Confidence**: ASS accurately predicts modality needs across diverse task types; TDS correctly isolates workflow-driven dynamism from tool/model noise; 37% resource cost reduction is achievable with STRIDE-guided selection
- **Low Confidence**: STRIDE's framework generalizes to novel domains without retraining; exact scoring functions and thresholds are optimal across all contexts; expert validation represents consensus across the AI practitioner community

## Next Checks

1. **Cross-Domain Generalization Test**: Apply STRIDE to 10-15 tasks from a new domain (e.g., customer service, software development) without retraining weights. Measure accuracy drop and compare against baseline (always deploy agents) to quantify generalization cost.

2. **Component Ablation Study**: Systematically disable TDS calculation and self-reflection assessment on a subset of tasks. Compare modality selection accuracy and resource usage to determine each component's marginal contribution, replicating the methodology suggested in Table 4.

3. **Weight Sensitivity Analysis**: Perform a grid search over reasonable ranges for wr, wt, ws, wρ, α, β, γ, and θ. Identify which parameters most affect modality classification and test whether small perturbations cause significant accuracy changes, indicating overfitting to the 30-task dataset.