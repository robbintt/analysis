---
ver: rpa2
title: 'Towards geological inference with process-based and deep generative modeling,
  part 1: training on fluvial deposits'
arxiv_id: '2510.14445'
source_url: https://arxiv.org/abs/2510.14445
tags:
- training
- samples
- cited
- https
- sample
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Generative modeling has been used for decades to predict subsurface
  physical properties while quantifying uncertainty, but struggles with complex geological
  structures like fluvial deposits due to their continuity. This study investigates
  whether generative adversarial networks (GANs) - a deep learning approach - can
  generate realistic 3D fluvial deposits by training on data from a process-based
  model.
---

# Towards geological inference with process-based and deep generative modeling, part 1: training on fluvial deposits

## Quick Facts
- arXiv ID: 2510.14445
- Source URL: https://arxiv.org/abs/2510.14445
- Authors: Guillaume Rongier; Luk Peeters
- Reference count: 40
- Primary result: GANs can generate diverse, high-quality 3D fluvial deposits that honor geological laws, with 2D architectural elements transferring directly to 3D.

## Executive Summary
This study investigates whether generative adversarial networks (GANs) can generate realistic 3D fluvial deposits by training on data from a process-based model. An ablation study demonstrates that architectural elements developed for large 2D images transfer directly to 3D fluvial deposits with comparable cell counts. Training remains stable using residual blocks and regularization, producing samples that reproduce non-stationarity and detailed features without mode collapse or memorization. The use of process-based models provides auxiliary properties like deposition time, enabling validation through geological principles such as the law of superposition.

## Method Summary
The method trains a 3D GAN to generate fluvial deposits (128×128×16 cells) with two continuous properties: coarse sediment fraction and deposition time. The FluvDepoSet dataset contains 20,200 3D realizations from the CHILD process-based model. The GAN architecture extends DCGAN with residual blocks, spectral normalization, and lazy R1 regularization. Training uses binary cross entropy with logits, Adam optimizer (β1=0, β2=0.99), mixed precision training, and batch size 64. Performance is evaluated using sliced Wasserstein distance to validation samples and the fraction of cells honoring the law of superposition.

## Key Results
- Architectural elements from 2D image generation (residual blocks, spectral normalization) transfer directly to 3D fluvial deposits with comparable total cell counts
- Training remains stable with these components, producing samples that reproduce non-stationarity and detailed features
- Generated samples honor the law of superposition (younger deposits above older ones) without requiring a separate validation dataset
- The most parsimonious architecture combines DCGAN with residual blocks, binary cross entropy with logits, and either spectral normalization or lazy R1 regularization

## Why This Works (Mechanism)

### Mechanism 1: 2D-to-3D Architecture Transfer via Equivalent Spatial Complexity
The representational challenge scales with total cells rather than dimensionality. A 128×128×16 3D volume (262,144 cells) presents similar complexity to a 512×512 2D image (262,144 cells), allowing 2D-proven stabilization techniques to remain effective. The limited structural diversity in fluvial deposits reduces optimization difficulty compared to general image datasets.

### Mechanism 2: Process-Based Auxiliary Properties Enable Physics-Aware Validation
Process-based models naturally produce temporal metadata like deposition time. Training the GAN to generate both physical properties and deposition time creates an internal consistency check: in undeformed strata, younger cells must overlie older ones. The fraction of cells honoring superposition converges to 1.0 as training improves.

### Mechanism 3: Residual Blocks + Regularization as Minimum Viable Stabilizers
Residual connections enable gradient flow through deeper networks, while spectral normalization constrains discriminator Lipschitz constant, preventing one network from overpowering the other. This combination creates a stable adversarial equilibrium sufficient for the specific geological structure targeted.

## Foundational Learning

- **Generative Adversarial Network (GAN) minimax game**: Understanding that GAN training is a two-player game seeking Nash equilibrium, not standard optimization. Loss curves don't directly indicate quality.
- **Residual connections (skip connections within blocks)**: The ablation study shows this is the critical component that enables stable training; without it, DCGAN collapses.
- **Spectral normalization / gradient penalty regularization**: Constrains discriminator capacity to prevent it from overpowering the generator; lazy R1 is a computationally cheaper variant.

## Architecture Onboarding

- **Component map**: Latent vector (100-dim) → Generator (fully convolutional with residual blocks, transposed convolutions) → 3D volume (128×128×16) → Discriminator (residual blocks with bottleneck, spectral normalization) → Real/Fake classification
- **Critical path**: 1) Prepare training data from process-based realizations, 2) Implement 3D GAN architecture with residual blocks and spectral normalization, 3) Train monitoring sliced Wasserstein distance and superposition fraction, 4) Run until both metrics stabilize
- **Design tradeoffs**: Latent vector size (8-512) shows minimal impact on quality; smaller latent spaces may be more entangled for inversion; batch size limited to 64 due to memory constraints
- **Failure signatures**: Mode collapse (samples cluster in MDS visualization), training collapse (Wasserstein distance oscillates), memorization (generated samples nearly identical to training samples), superposition violations (fraction below 0.9)
- **First 3 experiments**: 1) Replicate architecture on small subset to verify training stability, 2) Test latent interpolation to assess entanglement, 3) Generate samples with different latent shapes to test out-of-distribution extrapolation

## Open Questions the Paper Calls Out

1. **Does the training stability and sample quality of GANs transfer to larger 3D images and multimodal datasets representing diverse depositional environments?**
   - Based on: Abstract and conclusion noting this remains to be seen
   - Why unresolved: Study restricted to specific 3D size (128×128×16) and single depositional environment
   - Evidence needed: Successful training on larger volumes and varied geological settings

2. **How do GANs compare to diffusion models and variational autoencoders (VAEs) in terms of sample quality, diversity, and computational cost for geological modeling?**
   - Based on: Conclusion noting further comparison is needed
   - Why unresolved: Study focused exclusively on validating GAN architectures
   - Evidence needed: Comparative benchmark evaluating these architectures against the "generative learning trilemma"

3. **What architectural strategies are required to enable deep generative models to properly extrapolate in non-stationary directions?**
   - Based on: Discussion noting current extrapolation attempts fail with disconnected pieces
   - Why unresolved: Current models statistically reproduce geometries without understanding underlying geological principles
   - Evidence needed: Model generating geologically plausible continuations of non-stationary features without artifacts

## Limitations
- Limited to a single depositional environment (fluvial deposits), creating uncertainty about performance in more complex geological settings
- Tested only on 3D images up to 128×128×16 cells, so scalability to larger volumes remains unknown
- Does not compare performance against other deep generative models like diffusion models or VAEs

## Confidence

- **High**: 3D GAN training stability with architectural elements from 2D; ability to generate samples honoring geological laws using process-based auxiliary properties
- **Medium**: Scalability to larger 3D volumes (>128×128×16); performance on multimodal geological datasets; generalizability to structurally deformed settings
- **Low**: Impact of latent space entanglement on downstream inversion tasks

## Next Checks
1. **Scale test**: Train the same architecture on 256×256×32 samples to verify 2D-to-3D transfer breaks down at larger volumes
2. **Diversity stress test**: Train on a multimodal dataset combining fluvial, deltaic, and aeolian environments to test mode collapse resistance
3. **Structural deformation test**: Generate samples in a folded setting and validate using Walther's law instead of superposition to assess architectural adaptability