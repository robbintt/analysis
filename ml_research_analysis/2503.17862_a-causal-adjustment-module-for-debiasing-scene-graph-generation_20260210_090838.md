---
ver: rpa2
title: A Causal Adjustment Module for Debiasing Scene Graph Generation
arxiv_id: '2503.17862'
source_url: https://arxiv.org/abs/2503.17862
tags:
- object
- causal
- relationships
- adjustment
- distribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the long-tail problem in Scene Graph Generation
  (SGG), where models tend to predict tail relationships as head ones. While existing
  debiasing methods attribute this bias solely to the long-tail distribution of relationships,
  this paper identifies the skewed distributions of objects and object pairs as deeper
  underlying causes.
---

# A Causal Adjustment Module for Debiasing Scene Graph Generation

## Quick Facts
- **arXiv ID**: 2503.17862
- **Source URL**: https://arxiv.org/abs/2503.17862
- **Reference count**: 40
- **Primary result**: State-of-the-art mean recall (mR@K) and significantly improved zero-shot recall (zR@K) in unbiased Scene Graph Generation

## Executive Summary
This paper addresses the long-tail problem in Scene Graph Generation (SGG), where models exhibit strong bias toward predicting frequent relationships while failing on rare ones. The authors identify that existing debiasing methods overlook the skewed distributions of objects and object pairs as root causes. They propose the Mediator-based Causal Chain Model (MCCM) and Causal Adjustment Module (CAModule), which model the causal relationships among objects, object pairs, and relationships using a mediator variable for co-occurrence distribution. Experiments demonstrate that CAModule achieves state-of-the-art performance across multiple backbones and benchmarks, particularly excelling at the challenging zero-shot recall metric.

## Method Summary
The proposed method introduces a mediator-based causal chain model (MCCM) that restructures the causal flow from objects to pairs through a co-occurrence distribution. The CAModule is a lightweight Transformer encoder that takes statistical distributions (object frequency, co-occurrence, pair frequency, and relationship frequency) as input and produces triplet-level adjustment factors. These factors are applied multiplicatively to the model's logits to correct biased predictions. The approach also includes semantic similarity-based inference rules for generating zero-shot relationships by composing unseen object pairs.

## Key Results
- CAModule achieves state-of-the-art mean recall rates across various SGG backbones
- Significant improvement in zero-shot recall rate (zR@K) by composing zero-shot relationships
- Effective performance on challenging benchmarks including VG150, GQA, and Open Images V6

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Triplet-level logit adjustments correct bias more effectively than relationship-level adjustments because bias exists at the object-pair level, not just the predicate level.
- **Mechanism:** Instead of applying a single adjustment factor to all instances of a relationship (e.g., "on"), the model calculates a unique factor for every specific <subject, predicate, object> triplet. This accounts for the fact that a head relationship like "on" can form a tail triplet when paired with rare objects (e.g., <bag, on, arm>).
- **Core assumption:** The long-tail distribution of relationships is a symptom of the underlying skewed distributions of objects and object pairs, and adjusting for these finer granularities captures the true causal structure of the bias.
- **Evidence anchors:** [Section 3.1] "This observation suggests that, even for triplets composed of the same relationship, we should apply distinct adjustment factors... rather than a single adjustment factor."

### Mechanism 2
- **Claim:** Introducing a "Co-occurrence Distribution" (C) as a mediator between Objects (O) and Pairs (P) prevents the model from hallucinating plausible-but-non-existent pairs.
- **Mechanism:** The Mediator-based Causal Chain Model (MCCM) inserts variable C (O → C → P) to quantify the probability of two objects appearing together in the same scene, downweighting pairs that are theoretically possible but statistically rare.
- **Core assumption:** Statistical co-occurrence in the training data is a valid proxy for semantic plausibility and causal compatibility between objects.
- **Evidence anchors:** [Section 3.2] "MCCM introduces a mediator variable, the co-occurrence distribution C, between O and P... optimizing O → P to O → C → P."

### Mechanism 3
- **Claim:** Zero-shot relationships can be composed by inferring missing object pairs using semantic similarity rules.
- **Mechanism:** The model uses inference rules based on GloVe embeddings to identify semantically similar objects that can form the same relationships, extending the Object Pair distribution to cover unseen combinations.
- **Core assumption:** Semantic similarity in embedding space equates to functional interchangeability in visual relationships.
- **Evidence anchors:** [Section 3.4] "...inference rules... based on the principle that objects with similar attributes may form the same relationships."

## Foundational Learning

- **Concept: Structural Causal Models (SCM)**
  - **Why needed here:** The paper formalizes SGG as a directed acyclic graph of variables ($O, C, P, R$). Understanding SCM notation (e.g., $do(O=o)$) is required to interpret why the "Mediator" variable changes the causal flow from $O \to P$ to $O \to C \to P$.
  - **Quick check question:** Can you explain the difference between "mediation" and "confounding" in the context of Object and Pair distributions?

- **Concept: Long-Tailed Recognition & Debiasing**
  - **Why needed here:** The core problem is the trade-off between overall Recall (R@K) and mean Recall (mR@K). Understanding why "balancing" a dataset often hurts head-class performance is crucial to appreciating why this paper emphasizes "triplet-level" adjustments over simple resampling.
  - **Quick check question:** Why does standard cross-entropy loss fail on long-tailed distributions, and how does logit adjustment theoretically address this?

- **Concept: Transformers for Tabular/Statistical Data**
  - **Why needed here:** The CAModule is a Transformer, but it processes statistical vectors (O, C, P, R) rather than images or text sequences.
  - **Quick check question:** How does the self-attention mechanism in the CAModule learn interactions between the "Object Distribution" vector and the "Relationship Distribution" vector?

## Architecture Onboarding

- **Component map:** $O \to C \to P \to R$ statistical distributions → Linear embeddings (128-dim) → $T_{OC}$ → $T_{OCP}$ → $T_{CPPR}$ → Triplet-level adjustment factors ($\tilde{A}$) → Logit multiplication
- **Critical path:** The most sensitive step is the extraction of sub-distributions ($O_B, C_B, P_B, R_B$) for a given batch (Eq. 25). The module must correctly index the global statistical matrices to retrieve the specific co-occurrence and pair probabilities for the objects present in the current image batch.
- **Design tradeoffs:**
  - Explicit vs. Implicit Adjustment: Traditional methods use explicit inverse-frequency formulas. This module learns the adjustment via a Transformer, providing more flexibility but requiring training data to cover the statistical space.
  - Granularity: Triplet-level factors provide precision but require storing a larger adjustment matrix than relationship-level methods.
- **Failure signatures:**
  - Zero-shot Drop: If zR@K is 0, check the threshold $\beta$ (Eq. 40) and the similarity metric $\alpha$; the inference rules may be too strict or the GloVe embeddings misaligned.
  - High Recall, Low Mean Recall: If R@K is high but mR@K is low, the CAModule is likely not weighting the tail triplets sufficiently, suggesting the causal path $O \to C \to P$ is not effectively suppressing frequent bias.
- **First 3 experiments:**
  1. Baseline vs. Vanilla Logit Adjustment: Run the backbone (e.g., MotifsNet), then add standard inverse-frequency logit adjustment. Confirm it fails (as per Table 7) to validate the paper's premise.
  2. Ablation on Mediator C: Remove the $C$ variable (reverting to $O \to P$) and measure the drop in mR@K to quantify the value of co-occurrence modeling.
  3. Hyperparameter Sensitivity ($\alpha, \beta$): Sweep the similarity threshold $\beta$ (Fig 9) to find the "sweet spot" where zero-shot pair recall (zpR@) increases without introducing implausible noise.

## Open Questions the Paper Calls Out
None

## Limitations
- The method assumes co-occurrence statistics from the training set generalize to test distributions, which may fail in domain-shifted scenarios.
- Semantic similarity via GloVe embeddings may not capture visual attribute distinctions critical for relationship inference.
- Zero-shot pair generation relies heavily on attribute similarity, which may not hold for all object categories or cultural contexts.

## Confidence
- **High confidence**: Triplet-level adjustment factors improve mR@K over relationship-level methods (empirical validation via Table 5, 7).
- **Medium confidence**: Mediator variable C effectively models co-occurrence bias (mechanism plausible but requires more ablation studies).
- **Low confidence**: Semantic similarity-based zero-shot pair generation generalizes robustly (methodologically sound but weak empirical validation in corpus).

## Next Checks
1. **Ablation Study**: Remove the co-occurrence mediator (C) and measure the degradation in mR@K to quantify its contribution.
2. **Domain Transfer Test**: Evaluate CAModule performance on a domain-shifted dataset (e.g., Open Images V6) to test co-occurrence generalization.
3. **Zero-Shot Pair Quality**: Manually inspect a sample of inferred zero-shot pairs to assess semantic plausibility beyond recall metrics.