---
ver: rpa2
title: On-device Streaming Discrete Speech Units
arxiv_id: '2506.01845'
source_url: https://arxiv.org/abs/2506.01845
tags:
- speech
- dsus
- window
- size
- full
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of deploying discrete speech
  units (DSUs) in real-time, on-device streaming speech applications. DSUs are advantageous
  due to their high transmission efficiency and compatibility with language models,
  but their extraction from self-supervised speech models (S3Ms) is computationally
  intensive and requires full-length audio.
---

# On-device Streaming Discrete Speech Units

## Quick Facts
- arXiv ID: 2506.01845
- Source URL: https://arxiv.org/abs/2506.01845
- Reference count: 0
- Primary result: 50% FLOPs reduction with 6.5% CER increase on ML-SUPERB 1h dataset

## Executive Summary
This paper addresses the computational challenges of extracting discrete speech units (DSUs) from self-supervised speech models (S3Ms) for real-time, on-device streaming applications. DSUs offer high transmission efficiency and compatibility with language models but typically require full-length audio and intensive computation. The authors propose architectural optimizations including reduced attention window sizes and fewer model layers, combined with learnable weighting schemes and fine-tuning, to enable efficient streaming DSU extraction while maintaining reasonable recognition accuracy.

## Method Summary
The authors reduce computational complexity by shrinking the attention window and decreasing the number of layers in the S3M architecture. They introduce learnable weights to prioritize important components and apply fine-tuning to preserve DSU quality despite the reduced model size. The approach is evaluated on HuBERT and Whisper models, demonstrating that these modifications enable streaming DSU extraction with approximately 50% fewer floating-point operations while maintaining acceptable performance levels.

## Key Results
- 50% reduction in FLOPs through architectural modifications
- 6.5% relative increase in character error rate on ML-SUPERB 1h dataset
- Successful streaming DSU extraction demonstrated on HuBERT and Whisper models

## Why This Works (Mechanism)
The mechanism works by reducing the computational burden of S3Ms through architectural simplification while maintaining sufficient information for DSU extraction. By decreasing attention window sizes, the model processes only local context rather than entire utterances, enabling streaming capability. Layer reduction and learnable weighting allow the model to focus computational resources on the most critical components for DSU generation, while fine-tuning adapts the compressed model to maintain recognition quality.

## Foundational Learning

**Self-supervised speech models (S3Ms)**: Neural networks trained on unlabeled speech data that learn general speech representations. Needed because labeled speech data is expensive to obtain, and quick check is whether the model can generate meaningful representations without explicit supervision.

**Discrete speech units (DSUs)**: Quantized representations of speech that can be efficiently transmitted and processed. Required for reducing bandwidth and enabling language model integration, with quick check being the preservation of phonetic information after quantization.

**Attention mechanisms**: Components that weigh the importance of different input elements when generating outputs. Critical for capturing long-range dependencies in speech, with quick check being the trade-off between receptive field size and computational cost.

**Model compression**: Techniques to reduce the size and computational requirements of neural networks while preserving performance. Essential for on-device deployment, with quick check being the balance between efficiency gains and accuracy degradation.

**Fine-tuning**: Process of adapting a pre-trained model to a specific task using task-specific data. Necessary to optimize the compressed model for DSU extraction, with quick check being the amount of adaptation data needed for acceptable performance.

## Architecture Onboarding

Component map: Audio input -> Reduced-window S3M encoder -> Layer selection -> Weighted combination -> Quantization -> DSU output

Critical path: The bottleneck lies in the S3M encoder's attention computation, which is mitigated by reducing window sizes and layer count. The weighted combination and quantization stages become more critical as they must preserve information despite model compression.

Design tradeoffs: The primary tradeoff is between computational efficiency and recognition accuracy. Smaller attention windows enable streaming but may miss long-range context. Fewer layers reduce computation but may lose hierarchical speech information. The learnable weights and fine-tuning help navigate these tradeoffs.

Failure signatures: Performance degradation typically manifests as increased CER, particularly for longer utterances where context windows are insufficient. The model may also produce inconsistent DSUs for similar phonetic content when attention windows are too small.

First experiments:
1. Measure CER degradation as a function of attention window size reduction
2. Evaluate DSU quality using metrics like BERTScore against full-model outputs
3. Test streaming latency improvements with reduced layer counts

## Open Questions the Paper Calls Out
None

## Limitations
- Results limited to HuBERT and Whisper architectures; generalization to other S3Ms unknown
- Computational savings may be offset by training requirements for learnable weights and fine-tuning
- Real-world on-device performance metrics (latency, memory, power) not reported
- Streaming capability demonstrated theoretically but not validated in practical deployment scenarios

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Model compression effectiveness | High |
| Streaming capability demonstration | Medium |
| On-device deployment practicality | Low |

## Next Checks

1. Evaluate the proposed optimizations on additional S3M architectures (Wav2Vec 2.0, WavLM) and larger speech datasets (LibriSpeech, Common Voice) to assess generalization.

2. Measure real-time on-device performance metrics including actual latency, memory consumption, and power usage on representative mobile/embedded hardware platforms.

3. Test the streaming DSU extraction with downstream applications (speech recognition, translation) to quantify the impact on end-to-end task performance in streaming scenarios.