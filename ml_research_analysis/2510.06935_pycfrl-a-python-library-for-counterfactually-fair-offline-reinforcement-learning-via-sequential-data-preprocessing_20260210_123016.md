---
ver: rpa2
title: 'PyCFRL: A Python library for counterfactually fair offline reinforcement learning
  via sequential data preprocessing'
arxiv_id: '2510.06935'
source_url: https://arxiv.org/abs/2510.06935
tags:
- data
- policy
- train
- learning
- actions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PyCFRL is a Python library for counterfactually fair offline reinforcement
  learning via sequential data preprocessing. The library addresses fairness concerns
  in sequential decision-making by implementing a novel data preprocessing algorithm
  that ensures counterfactual fairness.
---

# PyCFRL: A Python library for counterfactually fair offline reinforcement learning via sequential data preprocessing

## Quick Facts
- arXiv ID: 2510.06935
- Source URL: https://arxiv.org/abs/2510.06935
- Reference count: 1
- Primary result: PyCFRL achieves counterfactual unfairness level of 0.042 while maintaining competitive value (7.358) compared to baseline methods

## Executive Summary
PyCFRL is a Python library implementing counterfactually fair offline reinforcement learning through sequential data preprocessing. The library addresses fairness concerns in sequential decision-making by constructing new state variables that are causally independent of sensitive attributes, enabling fair policy learning across different groups. The core method ensures that policy decisions remain unchanged when counterfactual sensitive attribute values are considered, while maintaining competitive performance. PyCFRL provides tools to evaluate both policy value and counterfactual unfairness levels, with a data example showing significant fairness improvements over baseline methods.

## Method Summary
PyCFRL implements a sequential data preprocessing algorithm that constructs counterfactually fair state variables by estimating how sensitive attributes influence states and creating new representations that remove this dependency. The method uses cross-validation preprocessing to prevent information leakage and separate reward preprocessing to optimize value while maintaining fairness guarantees. The library includes modules for reading trajectory data, preprocessing states and rewards, training policies using fitted Q-iteration, simulating environments, and evaluating both value and counterfactual unfairness. The approach is demonstrated on a dataset with 500 individuals and 10 time steps, achieving substantial fairness improvements while preserving policy performance.

## Key Results
- Achieved counterfactual unfairness level of 0.042 (compared to 0.407 and 0.446 for baseline methods)
- Maintained competitive value of 7.358 (vs 8.606 for full baseline)
- Processing time of 378.6 seconds for N=500, T=10 with 5-fold cross-validation
- Significant 90% reduction in counterfactual unfairness while only incurring 14% value reduction

## Why This Works (Mechanism)

### Mechanism 1: Counterfactual State Decoupling via Sequential Preprocessing
- Claim: Removing sensitive attribute influence from state variables causes downstream policies to become counterfactually fair
- Mechanism: SequentialPreprocessor estimates counterfactual states under different sensitive attribute values and constructs new state variables that are causally independent of the sensitive attribute
- Core assumption: Structural causal model correctly specifies how sensitive attributes influence states
- Evidence anchors: Abstract states new state variables are "not impacted by sensitive attributes"; companion paper provides theoretical foundation
- Break condition: Misspecified causal model or large finite-sample estimation error degrades fairness guarantees

### Mechanism 2: Cross-Validation Preprocessing to Prevent Information Leakage
- Claim: Using separate folds for preprocessor training and data preprocessing reduces overfitting
- Mechanism: 5-fold cross-validation ensures preprocessor doesn't "see" data it transforms, preventing encoding of sensitive attribute information
- Core assumption: Cross-validation adequately prevents leakage of sensitive attribute information
- Evidence anchors: Section 4.2 explicitly states cross_folds=5 is used to reduce overfitting with limited trajectory data
- Break condition: Very small datasets may still have leakage even with 5-fold cross-validation

### Mechanism 3: Policy Value Preservation via Reward Preprocessing
- Claim: Separate reward preprocessing improves policy value while maintaining fairness guarantees
- Mechanism: State preprocessing ensures counterfactual fairness while reward preprocessing optimizes value through a different objective
- Core assumption: Reward preprocessing doesn't re-introduce dependence on sensitive attributes
- Evidence anchors: Section 1 states reward preprocessing has "different purpose to improve value rather than ensure CF"; trade-off demonstrated in Section 4.6
- Break condition: If reward preprocessing creates dependence on sensitive attributes, CF guarantees may be violated

## Foundational Learning

- **Concept: Counterfactual Fairness (CF)**
  - Why needed here: CF is the core fairness criterion requiring reasoning about what would have happened with different sensitive attribute values
  - Quick check question: If an individual's sensitive attribute changed but all exogenous factors remained identical, would the policy assign the same action probability?

- **Concept: Offline Reinforcement Learning (Fitted Q-Iteration)**
  - Why needed here: PyCFRL operates on fixed datasets, not interactive environments, requiring understanding of FQI's limitations
  - Quick check question: Can FQI learn optimal actions that were never taken in the training data?

- **Concept: Structural Causal Models (SCMs)**
  - Why needed here: The preprocessing algorithm requires specifying how sensitive attributes causally influence states
  - Quick check question: Does the sensitive attribute directly cause state variables, or are there confounders?

## Architecture Onboarding

- **Component map:** reader → preprocessor → agents (FQI) → evaluation
- **Critical path:** 1) Load trajectories via read_trajectory_from_csv() → 2) Train/test split (80/20) → 3) Initialize SequentialPreprocessor with z_space, num_actions, cross_folds → 4) Call train_preprocessor() → 5) Initialize FQI with preprocessor, train with preprocess=False → 6) Train SimulatedEnvironment on full data → 7) Evaluate via FQE and model-based CF metric
- **Design tradeoffs:** Value vs. Fairness (14% value reduction for 90% fairness improvement); Cross-folds vs. Data Efficiency (higher folds reduce overfitting but increase computation); Neural network vs. interpretability (default 'nn' provides flexibility but may have non-convergence issues)
- **Failure signatures:** CF metric not near 0 on test data indicates preprocessing failure; non-convergence warnings suggest neural network issues; value severely degraded (>30%) may indicate over-regularization
- **First 3 experiments:** 1) Baseline replication: Run provided data example to verify CF≈0.042 and value≈7.358; 2) Hyperparameter sensitivity: Vary cross_folds (1,3,5,10) to observe impact on CF metric and value; 3) Environment mismatch test: Train preprocessor on one environment, evaluate on differently-configured SyntheticEnvironment to test robustness

## Open Questions the Paper Calls Out

- **Open Question 1:** How can the library be extended to accommodate offline datasets where individuals have variable-length episodes?
  - Basis: Current implementation requires same number of time steps for all individuals; extending to variable-length episodes can improve flexibility

- **Open Question 2:** Can the preprocessing pipeline be effectively integrated with standard offline RL libraries like d3rlpy or environment interfaces like Gymnasium?
  - Basis: PyCFRL can further combine preprocessor with popular offline RL algorithm libraries such as d3rlpy, or connect evaluation functions with established RL environment libraries such as gym

- **Open Question 3:** Does generalizing the algorithm to non-additive counterfactual state reconstruction improve the versatility or performance of the library?
  - Basis: Generalization to non-additive counterfactual states reconstruction can make PyCFRL more versatile

## Limitations
- Current implementation requires all individuals to have the same number of time steps, limiting flexibility for variable-length episodes
- Neural network architecture details and hyperparameters are unspecified, potentially affecting reproducibility
- The reported CF metric of 0.042 (not 0) indicates practical limitations in achieving perfect counterfactual fairness due to finite-sample errors
- Cross-validation preprocessing lacks direct corpus evidence for its effectiveness in fairness contexts

## Confidence

**Uncertainties:** Key unknowns include neural network architecture details (layers, units, activation functions, learning rates) for preprocessor, FQI, and environment models, which could affect exact reproducibility of the reported CF metric (0.042) and value (7.358). The sample data file location and generation method are also unclear.

**Confidence Labels:**
- **High Confidence:** The core mechanism of counterfactual state decoupling via sequential preprocessing is well-founded in companion theoretical work; the value-fairness trade-off demonstration is clearly presented
- **Medium Confidence:** The cross-validation preprocessing approach is reasonable but lacks direct corpus evidence for fairness contexts; the dual preprocessing strategy is logical but not theoretically guaranteed
- **Low Confidence:** Exact reproducibility requires specification of neural network hyperparameters and computing environment details not provided in the paper

## Next Checks
1. Replicate the baseline experiment with N=500, T=10 to verify CF metric ≈ 0.042 and value ≈ 7.358, comparing against "Full" and "Unaware" baselines
2. Perform hyperparameter sensitivity analysis by varying cross_folds (1, 3, 5, 10) to establish the overfitting-generalization trade-off for different data scales
3. Test environment robustness by training the preprocessor on one environment configuration and evaluating on a differently-configured SyntheticEnvironment to assess distribution shift resilience