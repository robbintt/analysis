---
ver: rpa2
title: Resource-Friendly Dynamic Enhancement Chain for Multi-Hop Question Answering
arxiv_id: '2506.17692'
source_url: https://arxiv.org/abs/2506.17692
tags:
- question
- reasoning
- retrieval
- answer
- documents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DEC, a multi-hop QA framework designed to
  improve performance and efficiency for lightweight LLMs. DEC first decomposes complex
  questions into a logically coherent reasoning chain, then dynamically rewrites sub-questions
  with context-aware query rewriting, and finally uses a keyword-enhanced retrieval
  strategy to improve document recall.
---

# Resource-Friendly Dynamic Enhancement Chain for Multi-Hop Question Answering

## Quick Facts
- arXiv ID: 2506.17692
- Source URL: https://arxiv.org/abs/2506.17692
- Authors: Binquan Ji; Haibo Luo; Yifei Lu; Lei Hei; Jiaqi Wang; Tingjing Liao; Lingyu Wang; Shichao Wang; Feiliang Ren
- Reference count: 37
- Multi-hop QA framework that matches state-of-the-art while reducing token consumption by up to 27%

## Executive Summary
DEC introduces a resource-efficient multi-hop question answering framework designed specifically for lightweight LLMs. The approach decomposes complex questions into logical reasoning chains, dynamically rewrites sub-questions with context awareness, and employs keyword-enhanced retrieval for precise document selection. The framework demonstrates significant improvements in both performance and efficiency, achieving up to 12.1% better semantic accuracy while reducing reasoning chain length by 27% compared to baseline approaches.

## Method Summary
The DEC framework operates through three core stages: question decomposition into logically coherent reasoning chains, context-aware query rewriting for sub-questions, and keyword-enhanced retrieval for precise document selection. The keyword extraction module plays a crucial role in ensuring targeted retrieval with minimal computational overhead. The system is designed to work effectively with smaller LLMs (8B parameters) while maintaining competitive performance against larger models, making it particularly suitable for resource-constrained environments.

## Key Results
- Achieves up to 12.1% improvement in semantic accuracy over baselines on Llama-3.1-8B-Instruct
- Reduces reasoning chain length by 27% while maintaining or improving performance
- Demonstrates strong generalization to larger models like Qwen2.5-14B-Instruct
- Shows superior performance in identifying unanswerable questions across all tested datasets

## Why This Works (Mechanism)
The framework's effectiveness stems from its staged approach that breaks down complex multi-hop reasoning into manageable components. By first decomposing questions into logical chains, the system can handle complexity incrementally rather than requiring the model to maintain multiple reasoning threads simultaneously. The dynamic rewriting component adapts queries based on retrieved context, ensuring relevance and precision. The keyword extraction module provides targeted retrieval that minimizes noise and computational overhead while maintaining recall quality.

## Foundational Learning

### Question Decomposition
- **Why needed**: Multi-hop questions require tracking multiple reasoning steps; decomposition prevents cognitive overload in smaller models
- **Quick check**: Verify that decomposed sub-questions maintain logical coherence and can be answered independently

### Context-Aware Query Rewriting
- **Why needed**: Static queries may miss relevant information; dynamic adaptation improves retrieval precision
- **Quick check**: Test rewritten queries against retrieval performance metrics to ensure improvement

### Keyword Extraction for Retrieval
- **Why needed**: Reduces noise in document selection while maintaining precision, crucial for resource efficiency
- **Quick check**: Compare retrieval precision/recall before and after keyword extraction implementation

## Architecture Onboarding

### Component Map
Question Decomposition -> Context-Aware Query Rewriting -> Keyword-Enhanced Retrieval -> Answer Generation

### Critical Path
The most resource-intensive component is the keyword extraction module, which directly impacts retrieval quality and overall efficiency. The dynamic rewriting stage adds computational overhead but significantly improves precision, making it essential for maintaining performance despite smaller model size.

### Design Tradeoffs
The framework trades some precision in individual steps for overall efficiency gains. By focusing on lightweight components and reducing token consumption, DEC sacrifices the ability to handle extremely complex reasoning chains that larger models might manage natively.

### Failure Signatures
- Poor decomposition leads to circular reasoning or missing logical steps
- Ineffective keyword extraction results in either low recall (missing relevant documents) or high noise (retrieving irrelevant content)
- Dynamic rewriting failures manifest as queries that drift from original intent or become too generic

### 3 First Experiments
1. Compare performance with and without keyword extraction on a subset of HotpotQA questions
2. Test dynamic rewriting effectiveness by measuring retrieval precision improvement
3. Evaluate decomposition quality by measuring logical coherence between sub-questions

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, though the limitations section suggests areas for future investigation, particularly regarding scalability to larger models and performance on more complex real-world questions.

## Limitations
- Performance claims rely on comparisons with potentially outdated baselines
- Focus on small LLMs (8B, 14B parameters) limits generalizability to larger models
- Keyword extraction may struggle with ambiguous or context-dependent queries
- Ablation studies don't explore interaction effects under resource constraints

## Confidence

### Major Claims Assessment

- **DEC framework effectiveness**: High confidence - Consistent improvements across three datasets with statistically significant gains
- **Keyword extraction contribution**: High confidence - Clear impact demonstrated through ablation studies with measurable precision improvements
- **Dynamic rewriting benefits**: Medium confidence - Improvements shown but exact contribution relative to other components needs further validation
- **Generalization to larger models**: Low confidence - Limited testing on Qwen2.5-14B-Instruct; performance on 70B+ models unexplored

## Next Checks

1. Test DEC on larger language models (70B+ parameters) to verify scalability and efficiency gains persistence

2. Conduct cross-dataset validation using complex, real-world multi-hop questions beyond curated benchmarks

3. Perform stress testing with adversarial queries designed to challenge keyword extraction and dynamic rewriting components, focusing on context-dependent or ambiguous questions