---
ver: rpa2
title: How well LLM-based test generation techniques perform with newer LLM versions?
arxiv_id: '2601.09695'
source_url: https://arxiv.org/abs/2601.09695
tags:
- test
- generation
- tests
- coverage
- class
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the effectiveness of recent LLM-based test
  generation techniques by evaluating four state-of-the-art tools (HITS, SymPrompt,
  TestSpark, and CoverUp) against a simple Plain-LLM approach using current LLM versions.
  The study replicates these methods on 393 Java classes and 3,657 methods, comparing
  their performance across line coverage, branch coverage, and mutation score metrics.
---

# How well LLM-based test generation techniques perform with newer LLM versions?

## Quick Facts
- arXiv ID: 2601.09695
- Source URL: https://arxiv.org/abs/2601.09695
- Authors: Michael Konstantinou; Renzo Degiovanni; Mike Papadakis
- Reference count: 40
- One-line primary result: Plain-LLM approach outperforms four state-of-the-art tools by 17.72%-20.92% on coverage and mutation metrics while maintaining comparable cost.

## Executive Summary
This paper evaluates four state-of-the-art LLM-based test generation tools (HITS, SymPrompt, TestSpark, CoverUp) against a simple Plain-LLM approach using current LLM versions (GPT-4o-mini, DeepSeek V3, LLaMA 3.3 70B). The study replicates these methods on 393 Java classes and 3,657 methods, comparing their performance across line coverage, branch coverage, and mutation score metrics. The surprising result is that the Plain-LLM approach, using simple prompting without sophisticated engineering components, outperforms all four state-of-the-art tools. Additionally, the paper demonstrates that granularity significantly impacts both effectiveness and cost, with method-level prompting generating more thorough tests than class-level prompting, and a hybrid strategy achieving similar effectiveness while reducing LLM queries by approximately 20%. The study also identifies that approximately 20% of generated tests fail to compile and 43% fail to pass assertions, suggesting that improving test repair mechanisms could significantly enhance overall performance.

## Method Summary
The study evaluates four SOTA LLM-based test generation tools (HITS, SymPrompt, TestSpark, CoverUp) and a Plain-LLM approach on 6 Java projects containing 393 classes and 3,657 methods. The Plain-LLM method uses zero-shot prompting with up to 5 repair iterations, while the tools add preprocessing components like slicing and symbolic path extraction. All approaches are executed via ChatUniTest with temperature 0.1. The evaluation measures line coverage, branch coverage (JaCoCo), mutation score (PIT), compilation rate, passing rate, and LLM query counts. Three configurations are tested: class-level, method-level, and hybrid (class first, then uncovered methods). Statistical significance is assessed using the Mann-Whitney U test (p<0.05).

## Key Results
- Plain-LLM achieved 49.95% line coverage, 35.33% branch coverage, and 33.82% mutation score, outperforming the best previous approaches by 17.72%, 19.80%, and 20.92% respectively
- Method-level prompting generates approximately 4x more tests than class-level, achieving higher coverage
- Hybrid approach (class first, then uncovered methods) reduces LLM queries by ~20% while maintaining similar effectiveness
- Approximately 20% of generated tests fail to compile and 43% fail to pass assertions

## Why This Works (Mechanism)

### Mechanism 1: Model Capability Obsolescence
Prior test generation engineering techniques were designed to compensate for weaker model reasoning in GPT-3.5-era LLMs. With current models (GPT-4o-mini, DeepSeek V3, LLaMA 3.3 70B), improved reasoning and code generation capabilities allow simple zero-shot prompting to achieve comparable or better results without additional engineering scaffolding.

### Mechanism 2: Granularity-Based Coverage and Cost Tradeoff
Class-level prompts request complete test suites in one query, producing fewer but broader tests. Method-level prompts target individual methods, generating ~4x more tests with deeper exploration. The hybrid approach first generates class-level tests, then selectively targets uncovered methods.

### Mechanism 3: Test Validity Bottleneck Limits Practical Utility
The primary bottleneck is not coverage but validity—approximately 20% of tests fail to compile and 43% fail assertions due to syntactic errors, semantic issues, or improper mocking. When LLMs encounter errors, they often generate helper classes or override production code rather than fix tests.

## Foundational Learning

- **Test Adequacy Metrics (Line/Branch Coverage, Mutation Score)**: The paper's entire effectiveness evaluation relies on these three metrics. Understanding what they measure—and their limitations—is essential to interpret results.
- **Zero-Shot vs. Engineered Prompting**: The paper's central claim is that zero-shot ("Plain-LLM") prompting now outperforms engineered approaches (slicing, symbolic path extraction, coverage feedback). Understanding the difference is prerequisite to understanding why this result is surprising.
- **LLM Query Cost and Temperature**: The paper evaluates efficiency via query count (directly proportional to API cost) and uses temperature 0.1 to reduce nondeterminism. Without this context, efficiency comparisons lack grounding.

## Architecture Onboarding

- **Component map**: Plain-LLM Core (prompt → up to 5 repair iterations → minimal parsing) -> Tool Wrappers (HITS, SymPrompt, TestSpark, CoverUp) (preprocessing → LLM) -> Evaluation Layer (JaCoCo/PIT → post-processing)
- **Critical path**: Select CUT → Generate tests → Compile tests (filter failures) → Execute tests (filter assertion failures) → Compute coverage/mutation metrics
- **Design tradeoffs**: Method-level vs. Class-level (higher coverage vs. lower query cost); Plain-LLM vs. Engineered (simplicity vs. predictable behavior); Repair iterations (more fixes vs. increased cost)
- **Failure signatures**: Non-compiling tests (~20%) due to missing imports/type mismatches; Non-passing tests (~43%) from incorrect oracles/mocking; Hallucinated scaffolding (empty classes/overrides)
- **First 3 experiments**: 1) Baseline replication: Run Plain-LLM on 3-5 classes, measure coverage and rates; 2) Granularity ablation: Compare method-level, class-level, and hybrid approaches; 3) Failure analysis: Manually inspect 20 non-passing tests to categorize root causes

## Open Questions the Paper Calls Out

### Open Question 1
Can dedicated test-repair oracles recover the significant coverage potential lost due to the high rate of non-compiling and non-passing tests? Current tools simply filter out invalid tests rather than attempting repair.

### Open Question 2
How can LLMs be better guided to synthesize necessary helper classes without hallucinating empty placeholders or overriding existing production code? The paper indicates generating accurate helper components is unexplored.

### Open Question 3
What are the optimal heuristics for switching between class-level and method-level prompting to maximize coverage while minimizing LLM queries? The hybrid approach saves 20% of queries but is "not fully optimized."

## Limitations
- Benchmark may not represent full diversity of software complexity, particularly in domains with heavy external dependencies or multi-threaded code
- Temperature 0.1 may suppress exploration of edge cases that could reveal weaknesses in plain prompting
- Study does not report variance across multiple runs, which could indicate whether performance gaps are consistent

## Confidence
- **High Confidence**: Validity bottleneck (compilation/passing rates) is well-supported and corroborated by research corpus
- **Medium Confidence**: Plain prompting outperforms sophisticated techniques, but depends on benchmark representativeness
- **Low Confidence**: Conclusion about obsolescence of test generation engineering techniques is overstated

## Next Checks
1. Test approaches on broader Java projects including complex dependency structures, multi-threading, or different coding patterns
2. Run multiple independent executions with same temperature to quantify variance and confirm performance differences
3. Perform systematic categorization of non-compiling and non-passing tests to identify dominant error types