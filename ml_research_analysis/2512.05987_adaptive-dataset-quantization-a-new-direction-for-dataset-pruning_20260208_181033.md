---
ver: rpa2
title: 'Adaptive Dataset Quantization: A New Direction for Dataset Pruning'
arxiv_id: '2512.05987'
source_url: https://arxiv.org/abs/2512.05987
tags:
- quantization
- dataset
- pruning
- compression
- adaptive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel dataset quantization approach to
  reduce storage and communication costs for large-scale datasets in resource-constrained
  edge devices. Unlike traditional dataset pruning and distillation methods that focus
  on inter-sample redundancy, the proposed method compresses each image by reducing
  redundant or less informative content within samples while preserving essential
  features.
---

# Adaptive Dataset Quantization: A New Direction for Dataset Pruning

## Quick Facts
- **arXiv ID**: 2512.05987
- **Source URL**: https://arxiv.org/abs/2512.05987
- **Reference count**: 31
- **Primary result**: Introduces dataset quantization that compresses each image's precision while maintaining model training performance, achieving significant compression (up to 99.3%) with minimal accuracy loss.

## Executive Summary
This paper introduces Adaptive Dataset Quantization (ADQ), a novel approach to compress large-scale datasets for resource-constrained edge devices by reducing redundant or less informative content within samples while preserving essential features. Unlike traditional dataset pruning that removes samples, ADQ compresses each image's precision through linear symmetric quantization with adaptive bit-width allocation based on gradient sensitivity. The method maintains model training performance while achieving significant dataset compression, outperforming traditional quantization and dataset pruning baselines under the same compression ratios.

## Method Summary
ADQ first applies linear symmetric quantization to obtain initial quantization ranges and scales for each sample. Then, an adaptive quantization allocation algorithm distributes different quantization ratios for samples with varying precision requirements, maintaining a constant total compression ratio. For each sample, quantization range is computed from maximum absolute value, then pixel values are mapped to signed integers. Sensitivity scores are computed as gradient deviation between original and quantized inputs. Samples are sorted by sensitivity and split into high/low groups with different bit-widths under global budget constraints. During training, quantized samples are dequantized on-the-fly.

## Key Results
- At 93.75% compression ratio, ADQ achieves 82.15% accuracy on CIFAR-10 and 57.69% on CIFAR-100
- Outperforms strong baselines: D2 (78.55%, 52.79%) and TDDS (77.97%, 52.15%) at same compression
- Combines effectively with dataset pruning for extreme compression (99.3% total): 68.73% on CIFAR-10 vs 44.50% for pruning alone
- Adaptive allocation consistently outperforms fixed allocation across all tested compression ratios

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Linear symmetric quantization reduces intra-sample redundancy while preserving semantic content sufficient for model training.
- **Mechanism**: For each image sample, compute max absolute value to derive a sample-specific scale factor, then map pixel values to signed integers in range [-Q, Q] where Q = 2^(b-1) - 1. Dequantization reconstructs approximate values during training via scale multiplication.
- **Core assumption**: Neural networks trained on reconstructed (dequantized) images will generalize comparably to full-precision training if the quantization error does not corrupt gradient signals significantly.
- **Evidence anchors**:
  - [abstract] "It first applies linear symmetric quantization to obtain an initial quantization range and scale for each sample."
  - [section 4.1] Equation 3-6 define the quantization/dequantization pipeline with scale factor s_d = (m_d + ε) / (2^b - 1).
- **Break condition**: Quantization bit-width drops below a threshold where reconstruction error introduces systematic bias in feature representations, causing gradient divergence during training.

### Mechanism 2
- **Claim**: Adaptive bit-width allocation based on quantization sensitivity improves performance over uniform quantization at equivalent compression ratios.
- **Mechanism**: Compute sensitivity score S(d) as 1 - cosine_similarity(g_orig, g_quant), where g_orig and g_quant are gradients w.r.t. model parameters for original and quantized inputs. Sort samples by S(d), split into high-sensitivity (D_high) and low-sensitivity (D_low) subsets, assign b_high > b_low bits respectively under global bit-budget constraint.
- **Core assumption**: Gradient deviation correlates with downstream training utility loss; samples with larger gradient deviation benefit more from higher precision preservation.
- **Evidence anchors**:
  - [abstract] "an adaptive quantization allocation algorithm is introduced to distribute different quantization ratios for samples with varying precision requirements."
  - [section 4.2] "We find empirically that setting |D_high| = |D_low| (i.e., equal split) achieves a good balance."
  - [section 5.4, Table 6] Adaptive allocation outperforms fixed allocation at all compression ratios (e.g., 93.75% compression: 82.15% vs 78.37% on CIFAR-10).
- **Break condition**: Sensitivity estimation noise dominates signal (e.g., unstable gradients early in training), causing misallocation of bit-widths and net performance degradation.

### Mechanism 3
- **Claim**: ADQ is orthogonal to dataset pruning and can be combined for extreme compression ratios (>95%).
- **Mechanism**: First apply dataset pruning (e.g., CCS-AUM) to select a coreset, then apply adaptive quantization to the retained samples. Total compression ratio = pruning ratio × quantization compression ratio.
- **Core assumption**: Intra-sample redundancy and inter-sample redundancy are independent compression dimensions with multiplicative storage savings.
- **Evidence anchors**:
  - [section 5.3] "our approach is orthogonal to dataset pruning, we can combine adaptive dataset quantization with dataset pruning to achieve further compression."
  - [section 5.3, Table 2] At 99.3% total compression (90% pruning + 93.75% quantization), ADQ + CCS-AUM achieves 68.73% on CIFAR-10 vs CCS-AUM alone at 44.50%.
- **Break condition**: Pruning removes samples that would have been assigned high bit-widths under ADQ, creating a selection bias that reduces diversity in the quantized subset.

## Foundational Learning

- **Concept**: Linear Symmetric Quantization
  - **Why needed here**: Core compression operation; maps continuous values to discrete integer representations with symmetric range around zero.
  - **Quick check question**: Given pixel values in range [-1, 1] and 4-bit quantization, what is the maximum quantization error?

- **Concept**: Gradient-Based Sensitivity Analysis
  - **Why needed here**: Determines which samples require higher precision; relies on computing gradients w.r.t. model parameters for sensitivity scoring.
  - **Quick check question**: Why use cosine distance rather than L2 distance for comparing gradient vectors across samples of varying magnitudes?

- **Concept**: Dataset Pruning vs. Distillation vs. Quantization
  - **Why needed here**: Positions ADQ within existing dataset reduction taxonomy; clarifies that pruning removes samples, distillation synthesizes samples, quantization compresses sample precision.
  - **Quick check question**: At 50% compression ratio, how many samples remain after pruning vs. what bit-width is used after uniform quantization?

## Architecture Onboarding

- **Component map**: Sensitivity Estimator -> Allocation Engine -> Quantization Module -> Dequantization Layer -> Training Pipeline
- **Critical path**:
  1. Load pre-trained reference model (frozen) for sensitivity computation
  2. For each sample: quantize → dequantize → compute gradients → calculate S(d)
  3. Sort all samples by S(d), split at median
  4. Assign b_high to D_high, b_low to D_low per budget constraint
  5. Store quantized dataset with per-sample bit-width metadata
  6. During training: dequantize on-the-fly during data loading

- **Design tradeoffs**:
  - **Two-group vs. multi-group allocation**: Paper empirically finds 2 groups optimal (Table 5a); more groups increase overhead without accuracy gains
  - **Equal split vs. threshold-based split**: Equal split (|D_high| = |D_low|) chosen for simplicity; threshold-based may improve but adds hyperparameter
  - **Pre-computed vs. dynamic sensitivity**: Pre-computed sensitivity is one-time cost (~1 epoch forward/backward pass); dynamic would be prohibitive

- **Failure signatures**:
  - **Accuracy collapse at extreme compression**: If b_low = 0 (samples dropped) and D_low contains informative samples, accuracy drops sharply
  - **Sensitivity estimator instability**: If reference model is undertrained or dataset has high label noise, S(d) rankings become unreliable
  - **Dequantization overhead**: If dequantization is not fused with data loading, I/O becomes bottleneck on fast GPUs

- **First 3 experiments**:
  1. **Baseline quantization sweep**: Apply uniform 8/6/4/2-bit quantization to CIFAR-10, train ResNet-18, measure accuracy drop vs. compression ratio to establish Pareto frontier.
  2. **Sensitivity distribution analysis**: Compute S(d) for all CIFAR-10 samples using pre-trained ResNet-18; visualize histogram and identify outliers (samples with very high/low sensitivity).
  3. **Adaptive vs. fixed ablation**: At 75% compression, compare (b_high=10, b_low=6) adaptive allocation vs. uniform 8-bit; measure accuracy difference and per-class breakdown to identify which classes benefit most from adaptation.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can performance be improved by making the quantization parameters learnable per image rather than using fixed linear symmetric quantization?
- **Basis in paper**: [explicit] The conclusion states, "This can be improved by making the quantization ratio learnable for each image, potentially boosting dataset quantization performance."
- **Why unresolved**: The current method employs a static linear symmetric quantization approach with a fixed range, which may not be optimal for every sample.
- **What evidence would resolve it**: A comparative study showing higher accuracy or efficiency when using an end-to-end learnable quantization scheme for the dataset.

### Open Question 2
- **Question**: How can a unified framework jointly optimize both dataset quantization and model quantization?
- **Basis in paper**: [explicit] The authors aim to "develop a unified framework that integrates both dataset and model quantization to further improve the efficiency of AI systems."
- **Why unresolved**: The current work treats dataset quantization as a preprocessing step, ignoring the interplay with the quantization of the model parameters during training.
- **What evidence would resolve it**: A joint optimization method that demonstrates superior efficiency and accuracy compared to applying dataset and model quantization independently.

### Open Question 3
- **Question**: Is the Adaptive Dataset Quantization method effective for data modalities beyond natural images, such as medical imaging or synthetic datasets?
- **Basis in paper**: [explicit] The authors note, "We plan to extend our method beyond natural image datasets to other domains, such as medical imaging and synthetic datasets."
- **Why unresolved**: The current experiments are restricted to CIFAR and ImageNet, which possess specific redundancy characteristics (e.g., textures, backgrounds) that may differ from specialized domains.
- **What evidence would resolve it**: Successful application and maintenance of test accuracy on benchmarks like medical imaging datasets (e.g., ChestX-ray) or synthetic data.

## Limitations

- The sensitivity estimation protocol is underspecified, particularly regarding which model state and bit-width to use for gradient computation, creating significant uncertainty for faithful reproduction.
- The assumption that gradient deviation correlates with training utility lacks theoretical grounding and may not hold for datasets with high label noise or early in training.
- Extreme compression regimes (93.75-99.3%) may be brittle due to aggressive 0-bit allocation for 50% of samples, potentially disproportionately affecting certain classes.

## Confidence

- **High confidence**: The core quantization mechanism (linear symmetric quantization) and its implementation details are well-specified and standard. The experimental results on CIFAR-10 at moderate compression ratios (50-87.5%) appear reproducible based on the provided methodology.
- **Medium confidence**: The adaptive allocation algorithm's effectiveness is demonstrated empirically, but the sensitivity estimation protocol's sensitivity to hyperparameters and model choice is unclear. The orthogonal combination with pruning is logically sound but not extensively validated.
- **Low confidence**: The extreme compression regime (93.75-99.3%) results may be brittle due to the aggressive 0-bit allocation for 50% of samples, which could disproportionately affect certain classes or data subsets.

## Next Checks

1. **Sensitivity computation protocol**: Reproduce the sensitivity score distribution on CIFAR-10 using different reference model states (random init, pretrained, partially trained) to quantify variability in S(d) rankings.
2. **Bit-width allocation robustness**: Test adaptive allocation with 3+ groups versus the reported 2-group split at 75% compression to verify the claimed optimality of equal splitting.
3. **Class-wise sensitivity analysis**: Compute per-class average sensitivity scores and examine whether certain classes are systematically assigned lower bit-widths, potentially explaining accuracy drops in extreme compression.