---
ver: rpa2
title: 'SPAP: Structured Pruning via Alternating Optimization and Penalty Methods'
arxiv_id: '2505.03373'
source_url: https://arxiv.org/abs/2505.03373
tags:
- pruning
- spap
- arxiv
- sparsity
- structured
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SPAP addresses structured pruning for LLMs by formulating a mixed-integer
  optimization problem and solving it via a penalty method combined with alternating
  minimization. The approach targets MLP layers, using theoretical analysis to show
  relaxation preserves optimality, then employs a penalty framework to make pruning
  decisions and an alternating minimization algorithm for efficient weight updates.
---

# SPAP: Structured Pruning via Alternating Optimization and Penalty Methods

## Quick Facts
- arXiv ID: 2505.03373
- Source URL: https://arxiv.org/abs/2505.03373
- Reference count: 9
- Primary result: Achieves 1.29× inference speedup and 23% memory reduction at 30% sparsity while maintaining lower perplexity than state-of-the-art methods

## Executive Summary
SPAP introduces a structured pruning method for large language models that formulates the problem as mixed-integer optimization and solves it through a combination of penalty methods and alternating minimization. The approach targets MLP layers using a GLU architecture, demonstrating superior performance compared to existing methods like FASP, CFSP, and FLAP. Theoretical analysis shows that relaxing binary pruning variables to continuous values preserves optimality, enabling efficient optimization through penalty methods.

## Method Summary
SPAP addresses structured pruning through a penalty method that converts hard bilinear constraints into tractable soft penalties. The algorithm alternates between updating pruning masks and weights, using closed-form solutions for weight updates and gradient descent for mask updates. The method specifically targets MLP layers with GLU architecture, using 128 calibration samples from WikiText2 to estimate input-output activations. After pruning decisions are made, an alternating minimization procedure refines the remaining weights to minimize reconstruction error.

## Key Results
- Achieves 1.29× inference speedup and 23% memory reduction at 30% sparsity on LLaMA-3.1-8B
- Maintains lower perplexity (28.58 vs 29.45) than FASP, CFSP, and FLAP at 30% sparsity
- Demonstrates superior zero-shot accuracy across multiple tasks including ARC, PIQA, and WinoGrande
- Outperforms competitors across multiple model families including LLaMA-3/3.1/3.2 and Qwen2.5

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Relaxing binary pruning variables to continuous [0,1] does not sacrifice optimality.
- Mechanism: The bilinear constraint W·diag(s)=0 creates complementary structure where any optimal relaxed solution contains implicit binary decisions that can be exactly recovered.
- Core assumption: Bilinear constraint is strictly enforced and sparsity constraint correctly specifies target pruning level.
- Evidence: Theoretical analysis in Theorem 1 with constructive proof; no corpus validation provided.

### Mechanism 2
- Claim: Penalty method progressively enforces bilinear constraints through increasing penalty parameter ρ.
- Mechanism: Quadratic penalty term ρ/2 Σ s_i∥W[:,i]∥² creates differentiable pressure toward constraint satisfaction, converting hard constraints into soft penalties.
- Core assumption: Sufficiently large ρ yields equivalent solutions to constrained problem; soft updates maintain optimization stability.
- Evidence: Equations 5-12 in section 3.2; penalty approaches mentioned in neighbor paper but different context.

### Mechanism 3
- Claim: Alternating minimization with closed-form W_down updates outperforms vanilla gradient descent.
- Mechanism: Least-squares subproblem admits closed-form solution W_down = YZ^T(ZZ^T)^{-1} when other variables are fixed, exploiting problem structure.
- Core assumption: Subproblem is well-conditioned or regularized; gradient updates provide sufficient exploration.
- Evidence: Section 3.3 equations 13-15; ablation in Table 5 shows SPAP with GD achieves higher perplexity than full SPAP.

## Foundational Learning

- Concept: Mixed-Integer Programming (MIP) Relaxation
  - Why needed: Understanding why relaxing {0,1}^n → [0,1]^n preserves optimality under specific constraint structures
  - Quick check: Given W·diag(s)=0 with s ∈ [0,1]^n, if s_3=0.7 and W[:,3] nonzero, is this feasible?

- Concept: Penalty Methods in Constrained Optimization
  - Why needed: Converting hard bilinear constraints into tractable penalty terms requires understanding trade-off between constraint satisfaction and objective minimization
  - Quick check: As penalty parameter ρ → ∞, what happens to feasible solutions of penalized problem?

- Concept: Alternating Minimization / Block Coordinate Descent
  - Why needed: Algorithm alternates between closed-form and gradient-based updates; convergence properties require understanding when alternating approaches are preferred
  - Quick check: Why might alternating minimization converge faster than joint gradient descent on all variables?

## Architecture Onboarding

- Component map: MLP Block with GLU (W_up, W_gate, W_down projections) -> Penalty Module (scores columns, soft-updates s, increases ρ) -> Weight Recovery Module (alternating updates)

- Critical path:
  1. Collect 128 calibration activations from WikiText2 via forward pass
  2. Initialize s, W, ρ; iterate K times: compute scores, update s with momentum, update W via closed-form, increase ρ
  3. Hard threshold s to binary; physically remove pruned columns/rows
  4. Run 20 iterations of alternating minimization to update remaining weights

- Design tradeoffs:
  - Score parameter t: Higher emphasizes column norms; lower incorporates input activation magnitude
  - Soft update α: Higher = slower mask changes, more stable but potentially slower convergence
  - Iterations K: More iterations increase computation without guaranteed improvement
  - Calibration samples: 128 samples sufficient; more samples increase memory/compute linearly

- Failure signatures:
  - Perplexity NaN or explosion: Numerical instability in matrix inversion - add δI perturbation
  - No speedup at claimed sparsity: Verify actual parameter reduction matches target
  - Degraded zero-shot accuracy: Check MLP-heavy vs attention-heavy model responses

- First 3 experiments:
  1. Sanity check on OPT-125M: Run at 20% sparsity, verify perplexity matches Table 1 (~29.33)
  2. Ablation of alternating minimization: Compare SPAP vs "SPAP w/o update" vs "SPAP w/ GD" on Qwen2.5-0.5B at 30% sparsity
  3. Profiling inference speedup: Measure CUDA time and peak memory for LLaMA-3.1-8B at 30% sparsity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can SPAP formulation be extended to structured pruning of attention heads or other non-MLP components?
- Basis: Paper states SPAP is "specifically targeting MLP layers" focusing on GLU architecture but does not apply method to attention mechanisms
- Why unresolved: Optimization model and structural correspondence are derived specifically for up/gate/down projection structure of MLP blocks
- Resolution evidence: Derivation for attention weight matrices or experimental results showing efficacy when pruning attention heads

### Open Question 2
- Question: How sensitive is SPAP's performance to size and domain of calibration dataset?
- Basis: Section 4.1 mentions using "128 randomly selected calibration samples" while section 2 critiques SliceGPT for requiring "large calibration datasets"
- Why unresolved: Objective function relies on input-output activations estimated from limited samples; variance under different data regimes unexplored
- Resolution evidence: Ablation study reporting perplexity and zero-shot accuracy across varying sample sizes and source domains

### Open Question 3
- Question: Is alternating minimization algorithm guaranteed to converge to global optimum of relaxed problem?
- Basis: Section 3.2 states algorithm solves penalized model "approximately" via alternating optimization; Theorem 1 proves optimal solution exists but convergence guarantees not provided
- Why unresolved: Problem remains non-convex due to bilinear term handled by penalty; alternating minimization can converge to local minima
- Resolution evidence: Theoretical convergence proof or empirical analysis showing objective value and solution quality across random initializations

## Limitations
- Theoretical relaxation optimality claim requires empirical validation in practical MLP pruning scenarios
- Alternating minimization benefit lacks direct comparison to other alternating optimization approaches in pruning literature
- Performance depends heavily on unspecified hyperparameter choices requiring empirical tuning

## Confidence
- High confidence: SPAP achieves measured speedup and memory reduction on LLaMA and Qwen models as reported in Tables 3-4
- Medium confidence: Theoretical relaxation preserves optimality claim is mathematically sound but requires practical validation
- Medium confidence: Alternating minimization provides measurable performance benefit over gradient descent based on controlled ablation studies
- Low confidence: Specific hyperparameter values needed for optimal performance are not provided and may significantly impact results

## Next Checks
1. Verify relaxation optimality claim by testing whether optimal relaxed solutions contain sufficient implicit binary decisions to recover exact binary solutions across multiple random MLP configurations

2. Systematically vary t, α, τ, and ρ^(0) to determine their impact on perplexity and pruning quality, establishing whether default values are robust or sensitive to model architecture

3. Measure actual inference speedup and memory reduction on different GPU architectures (RTX 4090, A100) to confirm claimed 1.29× speedup and 23% memory reduction are consistent across hardware configurations