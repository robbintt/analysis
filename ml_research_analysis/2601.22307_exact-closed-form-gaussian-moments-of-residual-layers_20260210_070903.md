---
ver: rpa2
title: Exact closed-form Gaussian moments of residual layers
arxiv_id: '2601.22307'
source_url: https://arxiv.org/abs/2601.22307
tags:
- unscented
- network
- activation
- weights
- architecture
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper closes a long-standing gap by deriving exact closed-form\
  \ Gaussian moments for propagating uncertainty through neural network layers. The\
  \ authors achieve exact moment matching for probit, GeLU, ReLU, Heaviside, and sine\
  \ activation functions\u2014including residual connections\u2014by computing analytical\
  \ expressions for layer-wise mean and full covariance propagation."
---

# Exact closed-form Gaussian moments of residual layers

## Quick Facts
- arXiv ID: 2601.22307
- Source URL: https://arxiv.org/abs/2601.22307
- Reference count: 40
- Primary result: Exact closed-form Gaussian moments for propagating uncertainty through neural network layers with probit, GeLU, ReLU, Heaviside, and sine activations.

## Executive Summary
This paper derives exact closed-form Gaussian moments for propagating uncertainty through neural network layers, closing a long-standing gap in the field. The authors achieve exact moment matching for several activation functions including probit, GeLU, ReLU, Heaviside, and sine by computing analytical expressions for layer-wise mean and full covariance propagation. Their method shows orders-of-magnitude improvements in KL divergence accuracy compared to linearization, mean-field, and unscented transforms, with applications in uncertainty propagation for regression and classification as well as variational Bayesian inference.

## Method Summary
The method computes exact first and second moments (mean and covariance) of a Gaussian distribution propagated through neural network layers. For a layer function g(x) = σ(Ax + b) + Cx + d, the moments depend on three analytically derived functions: Mσ (mean contribution of σ), Kσ (covariance between σ outputs), and Lσ (covariance between σ output and input). These are computed exactly for probit, GeLU, ReLU, Heaviside, and sine using Gaussian integrals, Stein's lemma, and dominated convergence. The method chains these exact single-layer moment formulas through deep networks, using a variational Gaussian approximation that achieves orders-of-magnitude better accuracy than existing methods.

## Key Results
- Orders-of-magnitude improvements in KL divergence error metric, up to a millionfold, over linearization, mean-field, and unscented transforms
- Competitive statistical calibration for uncertainty propagation in regression and classification tasks
- Hundredfold KL divergence improvements in variational Bayesian inference over state-of-the-art deterministic methods
- Theoretical error bounds using recursive decomposition with Lipschitz constants and non-normality terms

## Why This Works (Mechanism)

### Mechanism 1
The first and second moments of a Gaussian distribution propagated through a single layer can be computed exactly in closed form. For layer function g(x) = σ(Ax + b) + Cx + d, the moments depend on analytically derived functions Mσ, Kσ, and Lσ, which are exact for probit, GeLU, ReLU, Heaviside, and sine using Gaussian integrals and Stein's lemma. Core assumption is Gaussian input to each layer. Exactness is for a single layer; multi-layer propagation uses variational Gaussian approximation introducing approximation error.

### Mechanism 2
Chaining exact single-layer moment formulas through deep networks yields a Gaussian approximation orders of magnitude more accurate than existing methods. Starting from Gaussian input, each layer's output is approximated by a Gaussian with exact moments, avoiding compounding errors from linearization or independent neuron assumptions. Core assumption is that layer-wise Gaussian approximation is sufficiently accurate. Can fail on networks producing highly non-Gaussian outputs from Gaussian inputs.

### Mechanism 3
The accuracy of layer-wise Gaussian approximation can be theoretically bounded using recursive decomposition involving Lipschitz constants and "non-normality" term. The Wasserstein distance between true and approximate distributions at layer k is bounded by Lipschitz constant times error at layer k-1 plus term measuring non-Gaussianity. Core assumption is activation function smoothness for theoretical bounds. Current bounds are loose but attribute error sources.

## Foundational Learning

**Concept: Gaussian Moment Matching**
Why needed: The entire method is built on propagating first two moments of a Gaussian distribution. Matching moments is common way to fit one distribution to another.
Quick check: If random variable Y is approximated by Gaussian with same mean and covariance, what divergence metric is this approximation minimizing?

**Concept: Stein's Lemma (for Gaussian Variables)**
Why needed: Key analytical tool used to derive Lσ and Kσ functions, especially for GeLU. Connects covariances to expectations of derivatives.
Quick check: For jointly Gaussian X1, X2, what does Stein's lemma say about Cov(X1, f(X2)) in terms of derivative of f?

**Concept: The Role of Activation Functions in Non-Gaussianity**
Why needed: Method's error depends on how much activation function makes Gaussian input non-Gaussian. Smooth activations produce less non-normality than discontinuous ones.
Quick check: Why might network with ReLU or Heaviside activations be more challenging case for moment-matching Gaussian approximation?

## Architecture Onboarding

**Component map:**
Input Distribution N(μ, Σ) -> Layer Function g(x) = σ(Ax+b) + Cx+d -> Core Analytical Engines (Mσ, Kσ, Lσ) -> Propagation Pipeline (Lemma 2.4 formulas) -> Output Gaussian N(μ_out, Σ_out)

**Critical path:** Implementing and correctly integrating Mσ, Kσ, and Lσ functions for chosen activation. For GeLU, involves bivariate normal CDF and derivatives; for sine, involves exponential functions.

**Design tradeoffs:**
- Accuracy vs. Complexity: Exact moments for supported activations vs. custom derivation for new activations
- Layer-wise Approximation vs. True Distribution: Analytical tractability vs. ignoring higher-order statistics
- Supported Architectures: Handles residual connections (C matrix) but not attention mechanisms or complex architectures

**Failure signatures:**
- Highly Non-Gaussian Internal Distributions: Intermediate layer outputs strongly non-Gaussian (multi-modal, heavy-tailed)
- Networks with Hard Thresholds: Heaviside can create degenerate true outputs while approximation predicts broad Gaussian
- Numerical Instability: Computing bivariate normal CDFs and derivatives for extreme correlation values causes precision issues

**First 3 experiments:**
1. Validate Single-Layer Exactness: Propagate Gaussian through one layer with ReLU/GeLU, compare analytic mean/covariance to Monte Carlo estimates. Error should be at machine precision.
2. Benchmark Multi-Layer Accuracy: On randomly initialized 2-3 layer network, compare final analytic Gaussian's KL divergence from Monte Carlo "pseudo-true" Gaussian against baseline methods.
3. Test on Real Application: Implement uncertainty-aware regression on California Housing by propagating input noise through trained network, comparing predictive log-likelihood and calibration to baseline ignoring input uncertainty.

## Open Questions the Paper Calls Out

**Open Question 1**
Can preliminary analysis of stochastic feedforward neurons be expanded into robust training and inference methodology for stochastic neural networks? The paper provides moment-matching approximation for specific stochastic neuron model but doesn't formalize training or application. Resolution would require full training framework with loss functions and backpropagation rules.

**Open Question 2**
Do closed-form Gaussian moments exist for standard logistic sigmoid activation function, or is approximation strictly necessary? The paper provides exact solutions for probit, GeLU, ReLU, Heaviside, and sine, but treats logistic sigmoid as exception requiring approximation via Φ. Resolution would require mathematical derivation of Mσ, Kσ, Lσ functions for σ(x) = (1+e^{-x})^{-1}.

**Open Question 3**
Can theoretical error bounds on Wasserstein distance between true distribution and Gaussian approximation be tightened for deep networks? Current bounds are loose, relying on generic constants and Lipschitz estimates. Resolution would require refined theoretical analysis resulting in tighter bound scaling precisely with network depth and nonlinearity.

## Limitations
- Exactness limited to single-layer propagation; multi-layer accuracy depends on layer-wise Gaussian approximation quality
- Theoretical error bounds exist but are loose and primarily serve to attribute error sources
- Method supports only standard residual architectures, not attention mechanisms or other modern building blocks
- Numerical stability requires careful implementation of bivariate normal CDF and derivatives

## Confidence

**High confidence**: Single-layer moment formulas are mathematically exact for stated activation functions; Monte Carlo experiments validate orders-of-magnitude KL improvements; practical applications show competitive statistical calibration.

**Medium confidence**: Multi-layer Gaussian approximation accuracy and error bounds; scalability to deeper networks; robustness across diverse network architectures.

**Low confidence**: Behavior with novel activation functions requiring custom moment derivations; performance on networks with strong non-Gaussian internal representations; quantitative tightness of theoretical error bounds.

## Next Checks

1. **Validate Single-Layer Exactness**: Propagate Gaussian through one layer with ReLU/GeLU and compare analytic moments against Monte Carlo estimates. Error should be at machine precision, confirming Lemma 2.4 implementation.

2. **Benchmark Multi-Layer Accuracy**: On randomly initialized 2-3 layer network, measure final KL divergence against linearization/unscented baselines. Verify reported order-of-magnitude improvements hold across different input covariance scales.

3. **Test Numerical Stability**: Implement Φ₂ quadrature with cancellation-avoidance integrals. Run stress tests with extreme input variances and correlation values to ensure covariance matrices remain positive-definite and computations don't underflow/overflow.