---
ver: rpa2
title: 'BPQA Dataset: Evaluating How Well Language Models Leverage Blood Pressures
  to Answer Biomedical Questions'
arxiv_id: '2503.04155'
source_url: https://arxiv.org/abs/2503.04155
tags:
- measurements
- blood
- questions
- clinical
- medical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether language models (LMs) can effectively
  interpret and use clinical measurements, specifically blood pressure readings (BPs),
  in medical question-answering (QA) tasks. The researchers created a new dataset
  called BPQA, containing 100 medical QA pairs verified by medical students and designed
  to rely on BPs.
---

# BPQA Dataset: Evaluating How Well Language Models Leverage Blood Pressures to Answer Biomedical Questions

## Quick Facts
- arXiv ID: 2503.04155
- Source URL: https://arxiv.org/abs/2503.04155
- Reference count: 17
- Primary result: Larger language models (GPT-3.5, MedAlpaca) benefit more from blood pressure data than smaller models (BERT, BioBERT) in medical QA tasks

## Executive Summary
This study introduces the BPQA dataset to evaluate whether language models can effectively use blood pressure readings in medical question answering. The dataset contains 100 verified medical QA pairs designed to depend on BP measurements. Researchers tested four models (BERT, BioBERT, MedAlpaca, GPT-3.5) across five dataset variants with and without BP data and labels. Results show that larger models significantly outperform smaller ones when BP data is available, and domain-specific models improve when BP measurements are augmented with context-specific labels. The work highlights the importance of specialized benchmarks for clinical AI and suggests that retrieval augmentation could help models generalize better to minority patient populations.

## Method Summary
The researchers created BPQA, a dataset of 100 medical QA pairs verified by medical students, where answers depend on blood pressure readings. They evaluated four language models (BERT, BioBERT, MedAlpaca, GPT-3.5) using zero-shot prompting across five dataset variants: original with BP data, BP-free version, labeled BP data, BP-free with labels, and human-labeled context-specific BP data. Encoder models used fill-mask classification while decoder models used text generation with constrained Yes/No choices. Performance was measured by classification accuracy, comparing how models leveraged raw BP measurements versus relying on spurious correlations.

## Key Results
- Larger models (GPT-3.5, MedAlpaca) show greater performance gains from BP inclusion compared to smaller models (BERT, BioBERT)
- Domain-specific models (BioBERT, MedAlpaca) improve when BP measurements include context-specific categorical labels
- Standard medical QA benchmarks fail to test numerical measurement interpretation, as removing BPs has negligible impact on performance
- Human-labeled context-specific BP labels improve GPT-3.5's performance on special patient contexts by 6%

## Why This Works (Mechanism)

### Mechanism 1: Scale-Dependent Numerical Reasoning
Larger decoder-based models (GPT-3.5, MedAlpaca) possess internalized medical heuristics allowing them to map raw numerical values (e.g., "155/65 mmHg") to clinical risk profiles without external tools. Smaller encoder models (BERT, BioBERT) struggle with this numerical reasoning, showing minimal performance change when BP data is removed.

### Mechanism 2: Semantic Grounding via Context-Specific Labeling
Domain-specific models benefit from augmenting raw BP measurements with explicit categorical labels (e.g., "normal", "high") that match specific patient contexts. This acts as retrieval augmentation, offloading the threshold-mapping task from the model and bridging the gap between raw numbers and clinical meaning.

### Mechanism 3: Isolation of Clinical Variables
By creating variants where BP is the only variable changed, the dataset isolates specific failure modes in interpreting clinical measurements that are masked in broader benchmarks. This strict measurement reveals whether models use actual numerical reasoning or rely on spurious correlations.

## Foundational Learning

- **Zero-Shot Evaluation**: Required to measure "inherent" capability without fine-tuning bias; use structured prompts to force binary classification without prior examples.
  - Quick check: Does high zero-shot accuracy on BPQA-free indicate strong numerical reasoning or strong reliance on spurious correlations?

- **Context-Aware Normalcy Ranges**: Medical data is not static; BP of 130/90 is "high" generally but "normal" for pregnant women. Systems must account for patient demographics before applying logic.
  - Quick check: Why would a "high" label based on CDC guidelines hurt performance for a pregnant patient in BPQA-human-label?

- **Encoder vs. Decoder Architectures**: The performance split between encoders (BERT) and decoders (GPT) requires understanding how masked tokens vs. generated text processing affects numerical reasoning in zero-shot settings.
  - Quick check: Why might a generative decoder model (MedAlpaca) handle clinical context ambiguity better than a masked-language model (BioBERT) in this zero-shot setting?

## Architecture Onboarding

- **Component map**: Input Layer (4 dataset variants) -> Model Layer (Encoder vs. Decoder selector) -> Prompting Strategy (Fill-mask for Encoders; Text-generation with Yes/No constraints for Decoders) -> Evaluation (Accuracy comparison)

- **Critical path**: 1) Generate BPQA dataset with verified medical ground truth 2) Create ablation variants (removing BPs, adding labels) 3) Run inference and compare delta between BPQA and BPQA-free

- **Design tradeoffs**: Small dataset size (100 pairs) ensures medical verification and strict dependency on BP but limits statistical power; general labels are scalable but inaccurate for minorities; human labels are accurate but not scalable

- **Failure signatures**: "Shortcut" failure (model maintains >50% accuracy on BPQA-free, guessing using context words); "Label Overload" failure (BERT performance drops when labels are added); "Context Mismatch" failure (GPT-3.5 performance drops when generic labels are applied to special contexts)

- **First 3 experiments**: 1) Ablation Test: Run target model on BPQA vs. BPQA-free; negligible difference indicates inability to leverage numerical vitals 2) Label Sensitivity Test: Compare performance on BPQA-label vs. BPQA-human-label on "Special Context" subset 3) Scale Comparison: Benchmark small encoder vs. large decoder to confirm scale as determining factor for numerical reasoning

## Open Questions the Paper Calls Out

### Open Question 1
Does the performance benefit of label augmentation for domain-specific models generalize to other clinical measurements beyond blood pressure?
The study isolated blood pressure as a case study; it is unknown if the finding that domain-specific LMs benefit from labels holds true for other complex measurements (e.g., lab values, respiratory rates) with different numerical formats and contextual dependencies.

### Open Question 2
Can modifying tokenizers to handle medical numerical formats improve language model performance on clinical QA tasks?
Standard tokenizers may struggle to preserve quantitative semantics in formatted strings like "130/85 mmHg," but it has not been tested whether a tokenizer optimized for these medical patterns would enhance models' ability to reason with the data.

### Open Question 3
Can patient-context-aware retrieval augmentation effectively automate context-specific labeling to improve generalization for minority patients?
While manual human-labeling improved GPT-3.5's performance for special contexts, it is unclear if an automated retrieval system can dynamically provide this context to replace manual intervention effectively.

## Limitations
- Dataset size (n=100) ensures medical verification but limits statistical power and generalizability
- Study relies on synthetic clinical scenarios rather than real patient data, potentially missing real-world complexity
- Exact prompt templates for decoder models are not fully specified, creating reproducibility gaps

## Confidence

- **High Confidence**: Larger models (GPT-3.5, MedAlpaca) benefit more from BP inclusion than smaller models (BERT, BioBERT) - well-supported by ablation analysis
- **Medium Confidence**: Label augmentation improves domain-specific model performance - supported but based on single dataset needing external validation
- **Medium Confidence**: BPQA-free performance above chance indicates shortcut reasoning - plausible but not definitively proven

## Next Checks

1. **Scale Sensitivity Validation**: Test additional models across parameter spectrum (GPT-3.5 vs. GPT-4, smaller decoder variants) to confirm monotonic relationship between model scale and BP measurement utilization

2. **Real-World Data Transfer**: Evaluate same models on real clinical records with actual patient BPs and outcomes to test whether synthetic dataset performance translates to practical utility

3. **Demographic Generalization**: Test model performance across diverse patient demographics (age, pregnancy status, comorbidities) to validate context-specific label importance and potential bias in clinical reasoning