---
ver: rpa2
title: 'Critique-Coder: Enhancing Coder Models by Critique Reinforcement Learning'
arxiv_id: '2509.22824'
source_url: https://arxiv.org/abs/2509.22824
tags:
- arxiv
- reasoning
- data
- critique
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Critique Reinforcement Learning (CRL) is introduced to address
  the lack of critique and reflection mechanisms in standard RL-based code generation.
  CRL trains models to generate critiques for (question, solution) pairs, with rewards
  based on the correctness of the critique's judgment.
---

# Critique-Coder: Enhancing Coder Models by Critique Reinforcement Learning

## Quick Facts
- **arXiv ID:** 2509.22824
- **Source URL:** https://arxiv.org/abs/2509.22824
- **Authors:** Chi Ruan; Dongfu Jiang; Yubo Wang; Wenhu Chen
- **Reference count:** 6
- **Primary result:** CRL improves coding accuracy by +4.8 points and general reasoning by +6.1 points on benchmarks.

## Executive Summary
Critique Reinforcement Learning (CRL) introduces a critique mechanism to standard RL-based code generation by training models to evaluate (question, solution) pairs with binary judgments. The approach integrates 20% CRL data with standard RL training, creating a hybrid learning paradigm that enhances both coding accuracy and transferable reasoning abilities. On LiveCodeBench v5, the 4B model achieves 59.0 accuracy (+4.8 over baseline), outperforming larger models like DeepCoder-14B and GPT-o1. The method demonstrates strong generalization to non-code logical reasoning tasks, showing that critique training provides complementary skills beyond direct code generation.

## Method Summary
CRL trains models to generate binary judgments (True/False) for (question, solution) pairs, with rewards based on judgment correctness. The approach uses GRPO optimization with a hybrid dataset containing 20% CRL data and 80% standard RL data. Candidate solutions are generated and executed to create ground truth labels (True if pass rate >80% on test cases). Training proceeds in two phases: 16k context with scaled CRL rewards (×0.8), then 32k context when rewards stabilize. The method builds upon Qwen3-4B/8B base models with thinking mode enabled, using asymmetric clipping and group sampling for policy updates.

## Key Results
- LiveCodeBench v5: 4B model reaches 59.0 accuracy (+4.8 over baseline)
- Outperforms larger models: DeepCoder-14B and GPT-o1
- BBEH logic tasks: +6.1 average improvement over base model
- 20% CRL ratio optimal; 50% and 100% degrade performance

## Why This Works (Mechanism)

### Mechanism 1: Critique-Regularized Policy Optimization
Incorporating binary rewards for critiquing solutions improves reasoning and verification capabilities that transfer to better code generation. The model generates internal reasoning to verify correctness, reinforcing error detection abilities. The core assumption is that judgment generation relies on shared reasoning steps with solution generation. Evidence shows CRL improves both coding and logic benchmarks. Break condition: if the model learns to guess labels without reasoning, transfer may fail.

### Mechanism 2: Hybrid Data Balancing (The 20% Rule)
Substituting 20% of RL data with CRL data provides complementary learning signals without causing format shift. Standard RLVR optimizes for solution generation while CRL optimizes for evaluation. The 20% ratio prevents over-fitting to critique format while maintaining generation proficiency. Evidence shows 50% and 100% CRL mixtures degrade code generation. Break condition: exceeding ~50% CRL causes loss of code syntax proficiency.

### Mechanism 3: Transferable Verification Skills
Verification and logical analysis skills from code critique transfer to non-code logical reasoning tasks. Critiquing requires analyzing control flow, edge cases, and logical consistency - domain-agnostic skills. Reinforcing these patterns improves performance on logical benchmarks despite code-only training. Core assumption: logical verification is a general skill. Evidence shows +6.1 improvement on BBEH logic tasks. Break condition: if reasoning is too syntax-specific, transfer to natural language logic diminishes.

## Foundational Learning

- **Concept: Group Relative Policy Optimization (GRPO)**
  - Why needed: Specific optimizer that computes advantages relative to other outputs in a group, requiring multiple outputs per prompt (G=8).
  - Quick check: Are you sampling a group of outputs (G > 1) for each prompt to compute relative advantage, or just a single output?

- **Concept: Reinforcement Learning with Verifiable Rewards (RLVR)**
  - Why needed: Baseline paradigm using pass/fail signals from test cases; CRL introduces secondary verifiable signal (judgment correctness).
  - Quick check: Is your reward signal determined by code execution against tests (RL) or binary correctness of textual judgment (CRL)?

- **Concept: Format Shift / Mode Collapse**
  - Why needed: Key risk where excessive CRL training causes model to forget solution generation and only output judgments.
  - Quick check: Does training data mix strictly preserve majority generation examples (RL) to prevent loss of code writing ability?

## Architecture Onboarding

- **Component map:** Input Router → RL Pipeline (Question → Solution) / CRL Pipeline (Question + Solution → Judgment) → Reward Server (R_rl or R_crl) → Policy Model (Qwen3-4B/8B) → GRPO Updater

- **Critical path:**
  1. CRL Dataset Construction: Generate candidate solutions, execute to find pass rates, label True/False (>80% threshold)
  2. Hybrid Batching: Maintain 20% CRL / 80% RL mix during GRPO updates
  3. Iterative Context Lengthening: 16k → 32k as training stabilizes (DeepCoder protocol)

- **Design tradeoffs:**
  - Constraint: Pure CRL improves reasoning but degrades coding syntax
  - Choice: Use 20% CRL ratio to gain reasoning benefits without syntax degradation
  - Reward Scaling: CRL rewards scaled by 0.8 in 16k phase to prevent domination

- **Failure signatures:**
  - Self-Critique Failure: Model cannot use its own critiques to select best solution at test time
  - Performance Drop: EvalPlus drops while BBEH rises indicates exceeding optimal CRL ratio (>50%)

- **First 3 experiments:**
  1. Baseline Verification: Train Qwen3-4B on pure RLVR to establish LiveCodeBench baseline (~56.6)
  2. Ablation on Ratio: Train 0%, 20%, and 50% CRL models; verify 20% yields peak LiveCodeBench, 50% degrades generation
  3. Transfer Check: Evaluate 20% CRL on BBEH logic tasks; confirm +4.0 improvement correlates with coding gains

## Open Questions the Paper Calls Out

### Open Question 1
Why does CRL improve reasoning capabilities but fail to enable effective self-critique during test-time scaling? The paper identifies that while CRL enhances reasoning, attempts to use the model to critique its own candidate solutions for selection did not yield performance improvements, indicating a lack of "genuine self-critical ability." This issue stems from the binary reward design or a fundamental disconnect between judging correctness and estimating solution rank. Resolution would require comparing critique confidence scores against actual pass rates or testing ranking-based rewards.

### Open Question 2
Is the optimal 20% CRL data mixing ratio universally applicable, or dependent on base model capabilities or dataset difficulty? While 20% is best for Qwen3-4B/8B, the paper doesn't test if this ratio holds for smaller models or easier datasets where the mismatch between critique and generation might differ in impact. Resolution would require sensitivity analysis across different model sizes (1B vs 8B) or varying dataset complexities.

### Open Question 3
Does optimizing solely for binary judgment label ("True"/"False") compromise the fidelity of the model's generated critique reasoning? The reward function is strictly dependent on final judgment token matching ground truth, offering no incentive for accuracy of preceding critique text. The paper notes increased "thinking" length but it's unclear if the model learns to generate sound logical arguments or merely rationalizes to reach the correct binary label. Resolution would require human or LLM evaluation of critique text quality compared to process-based rewards.

## Limitations
- Cannot perform effective self-critique at test time - requires external feedback
- 20% CRL ratio may be dataset-specific and not generalize to all coding datasets
- Relies heavily on test-case execution, making it dependent on reliable, comprehensive test suites

## Confidence
- **High Confidence:** Empirical improvements on LiveCodeBench and BBEH benchmarks are well-documented and reproducible
- **Medium Confidence:** Mechanism claim that critique training reinforces intermediate reasoning is plausible but not directly observed
- **Low Confidence:** Claim about avoiding "format shift" is demonstrated but underlying optimization dynamics remain unclear

## Next Checks
1. **Ablation on Test Suite Quality:** Vary test case quality and comprehensiveness to determine if CRL benefits depend on perfect oracles versus approximate feedback
2. **Cross-Dataset Transfer:** Apply 20% CRL methodology to different coding datasets (HumanEval or MBPP) to test generalizability
3. **Self-Evaluation Capability:** Test whether fine-tuning CRL model to generate its own critiques during inference can improve solution quality through iterative refinement