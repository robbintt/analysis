---
ver: rpa2
title: 'KG-QAGen: A Knowledge-Graph-Based Framework for Systematic Question Generation
  and Long-Context LLM Evaluation'
arxiv_id: '2505.12495'
source_url: https://arxiv.org/abs/2505.12495
tags:
- person
- what
- name
- role
- position
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces KG-MULQA, a knowledge-graph-based framework
  for generating systematic question-answer pairs to evaluate long-context large language
  models. It leverages structured RDF representations of annotated financial credit
  agreements to extract QA pairs across three complexity dimensions: multi-hop reasoning,
  set operations, and answer plurality.'
---

# KG-QAGen: A Knowledge-Graph-Based Framework for Systematic Question Generation and Long-Context LLM Evaluation

## Quick Facts
- **arXiv ID:** 2505.12495
- **Source URL:** https://arxiv.org/abs/2505.12495
- **Reference count:** 40
- **Key outcome:** Introduces KG-MULQA, a knowledge-graph-based framework generating systematic QA pairs to evaluate long-context LLMs, with a dataset of 20,139 pairs across three complexity dimensions.

## Executive Summary
This paper presents KG-MULQA, a novel framework for generating systematic question-answer pairs to evaluate long-context large language models. The framework leverages structured RDF representations of annotated financial credit agreements to extract QA pairs across three complexity dimensions: multi-hop reasoning, set operations, and answer plurality. Using this approach, the authors construct a dataset of 20,139 QA pairs, the largest among existing long-context benchmarks. Evaluation of 16 LLMs shows performance degrades with increasing complexity, especially on set-based reasoning and implicit relation extraction. Human evaluation confirms that even top models struggle with tasks requiring deeper document understanding, highlighting KG-MULQA's effectiveness in revealing model limitations in long-context comprehension.

## Method Summary
KG-MULQA converts annotated financial credit agreements into RDF Knowledge Graphs using 7 entity types and a detailed annotation schema. SPARQL queries are applied to these graphs to generate QA pairs across three complexity levels: single-hop (Level 1), multi-hop (Level 2), and complex logic with set operations (Level 3). The framework produces 20,139 QA pairs from 170 credit agreements. Evaluation uses a two-stage prompt pipeline with vLLM/LangChain, chunking documents to fit 128k token contexts. Primary metrics include F1 score, LLM-as-a-Judge scoring (1-5), and human evaluation for ground-truth correlation.

## Key Results
- KG-MULQA-D dataset contains 20,139 QA pairs, the largest long-context benchmark dataset available
- Performance degrades systematically with complexity: GPT-4o achieves 81.4% F1 on Level 1 but only 55.7% on Level 3
- Set operation failures are the most common error mode, with models often listing all items instead of computing differences
- Implicit relation extraction shows significant model limitations, with full document context outperforming oracle snippets for hard questions

## Why This Works (Mechanism)

### Mechanism 1: Grounding Complexity in Graph Topology
The framework maps unstructured text to RDF graphs, enabling programmatic control of question difficulty via graph traversal depth and logic. Semantic structure is converted to subject-predicate-object triples, with SPARQL queries traversing specific path lengths and applying logic. A "hard" question is mechanically defined as a longer path (e.g., 3 hops) with complex logic (e.g., set difference), forcing models to replicate multi-step graph traversal over long text.

### Mechanism 2: Decoupling Semantic Drift from Retrieval Failure
Deterministic generation of ground-truth answers via SPARQL allows distinguishing between retrieval inability and reasoning manipulation. Since the "Gold" answer is generated via code rather than human intuition, errors can be precisely categorized. If a model retrieves correct entities but fails the set operation, it's flagged as a "Set Operation Failure" rather than a retrieval error.

### Mechanism 3: Stress-Testing Context Utilization via Implicit Links
Forcing models to answer questions based on implicit relationships (derived from document structure like signature blocks) rather than explicit text tests robustness beyond simple keyword matching. The annotation schema captures visually implied relations, requiring models to infer relationships not verbally stated, significantly increasing task difficulty for embedding-based retrievers.

## Foundational Learning

- **Concept:** RDF Triples & SPARQL
  - **Why needed here:** This is the substrate of the framework. You cannot understand how questions are generated without understanding how the system stores "Organization → hasEmployee → Person" and queries that structure.
  - **Quick check question:** How would you write a SPARQL query to find all entities connected to "Organization X" by *any* predicate?

- **Concept:** Set Theory (Intersection ∩ and Difference \)
  - **Why needed here:** The "Hard" difficulty level is primarily driven by requiring the LLM to perform mental set operations (e.g., "What is role A but NOT role B?").
  - **Quick check question:** If Document A lists {1, 2, 3} and Document B lists {2, 3, 4}, what is the result of A \ B?

- **Concept:** "Lost in the Middle" Phenomenon
  - **Why needed here:** The evaluation relies on long-context performance. Understanding that models bias attention to the start and end of prompts is crucial for interpreting the retrieval error analysis.
  - **Quick check question:** Where should you ideally place the most critical instructions in a 100k-token prompt?

## Architecture Onboarding

- **Component map:** HTML/PDF Credit Agreements -> Annotation Layer (Label Studio) -> Graph Builder (RDF/Turtle) -> QA Generator (SPARQL) -> Evaluation Suite (LangChain/vLLM)
- **Critical path:** The Annotation Schema (Section 2.2). If schema definitions are not strictly followed, the Knowledge Graph construction will fail, leading to invalid QA pairs.
- **Design tradeoffs:**
  - Template Depth vs. Yield: Highly complex templates (Level 5) are rarely applicable to real documents because necessary entity combinations don't exist, trading complexity for dataset size
  - Chunking vs. Coherence: Documents are chunked to fit 128k context, potentially severing cross-chunk dependencies, though merging prompt attempts reconciliation
- **Failure signatures:**
  - High "Not Found" Rate: Usually indicates context window is flooded or model is conservative; check chunking strategy
  - High Edit Distance / Low F1: Indicates model is "hallucinating" or retrieving semantically similar but factually wrong entities
  - Set Operation Error: Model lists all items instead of the filtered set
- **First 3 experiments:**
  1. Graph Validation: Run SPARQL queries on sample RDF file to verify Level 1 questions map 1:1 to obvious text facts
  2. Chunking Ablation: Evaluate GPT-4o on "Easy" questions with full context vs. 50% chunk size to establish baseline for retrieval degradation
  3. Set Logic Isolation: Test model on Level 2 questions with zero hops (purely set-based on single entity) to isolate logical reasoning from multi-hop retrieval

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the KG-MULQA framework be effectively adapted to generate multi-level QA pairs for domains with different structural characteristics, such as legal contracts or medical records?
- **Basis in paper:** [explicit] The authors state in the "Limitations" section that "The created QA pairs are based on financial credit agreements only," and while Appendix F shows a medical ablation, it is limited in scope.
- **Why unresolved:** The current framework relies on a specific ontology for credit agreements; it is unclear if the three complexity dimensions capture reasoning difficulty as effectively in other domains.
- **What evidence would resolve it:** Successful construction and model evaluation of a KG-MULQA dataset derived from a large-scale corpus of non-financial documents, showing similar complexity-based performance degradation.

### Open Question 2
- **Question:** What specific training or architectural modifications are necessary to overcome systematic LLM failures in set-based comparisons and implicit relation extraction?
- **Basis in paper:** [explicit] The authors conclude in Section 4.2 that "Addressing these shortcomings will likely involve a combination of targeted fine-tuning... and stronger in-context reasoning prompts."
- **Why unresolved:** The paper identifies and categorizes failure modes but does not experiment with or propose concrete solutions to fix them.
- **What evidence would resolve it:** Demonstrating that a model fine-tuned on set-logic tasks or implicit-relation datasets shows statistically significant improvement on the "Hard" subset of the KG-MULQA-D benchmark compared to baselines.

### Open Question 3
- **Question:** To what extent does the removal of document structure (formatting, layout) in "oracle" retrieval settings impair an LLM's ability to reason over implicit information?
- **Basis in paper:** [inferred] Appendix D.8 notes that the "Full Document" setting outperforms the "Oracle" setting for hard questions, and authors attribute errors to "Implicit Information Gaps" where models ignore visual or positional indicators.
- **Why unresolved:** The paper establishes that full context is better than snippets for complex queries but does not isolate whether this is due to volume of text or loss of implicit structural cues present in full document.
- **What evidence would resolve it:** An ablation study evaluating model performance on full documents where formatting/layout cues have been systematically removed or flattened versus the original formatted text.

## Limitations
- The framework's reliance on highly structured financial documents may limit generalizability to other domains, as performance on more narrative or less structured documents is unknown
- The "implicit relation" stress-testing mechanism depends on consistent document formatting, which may diminish if credit agreements evolve to include more explicit textual links
- LLM-as-a-Judge evaluation introduces subjectivity, as the exact prompt engineering and calibration of Gemini-2.0-Pro's scoring is not fully specified

## Confidence

- **High:** The core claim that KG-MULQA systematically generates QA pairs with controllable complexity (multi-hop, set operations, plurality) is well-supported by the RDF-SPARQL pipeline description and validation results
- **Medium:** The claim that KG-MULQA is the largest long-context benchmark is likely true based on dataset size, but direct comparison with other benchmarks' full scope is not provided
- **Medium:** The claim that performance degradation is specifically due to tested complexity dimensions is supported, but disentangling this from general long-context retrieval issues is difficult without broader ablation study

## Next Checks

1. **Cross-Domain Generalization:** Apply the KG-MULQA framework to a different document type (e.g., legal contracts or technical manuals) and evaluate whether the same complexity gradient and failure modes are observed
2. **Prompt Engineering Ablation:** Systematically vary the LLM-as-a-Judge prompt (e.g., different scoring instructions or exemplar answers) to quantify the variance introduced by the automated evaluation step
3. **Implicit Relation Robustness:** Create a modified KG-MULQA dataset where a subset of implicit relations are made explicit through annotation. Re-run the evaluation to measure the specific performance gap attributable to implicit vs. explicit link extraction