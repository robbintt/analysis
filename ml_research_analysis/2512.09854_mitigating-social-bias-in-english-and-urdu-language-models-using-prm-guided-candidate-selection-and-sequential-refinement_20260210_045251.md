---
ver: rpa2
title: Mitigating Social Bias in English and Urdu Language Models Using PRM-Guided
  Candidate Selection and Sequential Refinement
arxiv_id: '2512.09854'
source_url: https://arxiv.org/abs/2512.09854
tags:
- urdu
- bias
- english
- language
- utility
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses social bias in large language models, focusing
  on single-word generation in English and Urdu. It proposes a preference-ranking
  model (PRM)-guided inference-time mitigation framework with three methods: baseline
  generation, PRM-Select (best-of-N sampling), and PRM-Sequential (iterative refinement).'
---

# Mitigating Social Bias in English and Urdu Language Models Using PRM-Guided Candidate Selection and Sequential Refinement

## Quick Facts
- arXiv ID: 2512.09854
- Source URL: https://arxiv.org/abs/2512.09854
- Reference count: 4
- Single-word generation tasks in English and Urdu show that Urdu exhibits significantly higher baseline bias than English

## Executive Summary
This paper addresses social bias in large language models, focusing on single-word generation in English and Urdu. It proposes a preference-ranking model (PRM)-guided inference-time mitigation framework with three methods: baseline generation, PRM-Select (best-of-N sampling), and PRM-Sequential (iterative refinement). Using GPT-3.5 for generation and GPT-4o-mini for scoring, the study evaluates 200 English and 200 Urdu prompts across bias and utility dimensions. Results show that Urdu exhibits significantly higher baseline bias than English, reflecting structural inequities in multilingual LLM training. PRM-Select effectively reduces bias and nearly eliminates cross-lingual gaps, while PRM-Sequential achieves maximal fairness but slightly reduces Urdu utility due to overcorrection. The work demonstrates that lightweight, inference-time debiasing methods can substantially improve fairness, especially for underrepresented languages.

## Method Summary
The study evaluates bias mitigation in English and Urdu language models using three inference-time methods. Baseline generation uses standard sampling without intervention. PRM-Select generates N candidates and selects the highest-scoring one according to a preference-ranking model. PRM-Sequential iteratively refines outputs by generating candidates, selecting the best, and repeating the process. GPT-3.5 generates single-word completions for 200 prompts in each language, while GPT-4o-mini serves as the PRM to score candidates. Bias is measured using a combination of direct bias metrics and intersectional bias assessments, while utility is evaluated through semantic relevance and coherence measures. The study compares performance across both languages to identify cross-lingual gaps and the effectiveness of mitigation strategies.

## Key Results
- Urdu exhibits significantly higher baseline bias than English, reflecting structural inequities in multilingual LLM training
- PRM-Select effectively reduces bias and nearly eliminates cross-lingual gaps between English and Urdu
- PRM-Sequential achieves maximal fairness but slightly reduces Urdu utility due to overcorrection

## Why This Works (Mechanism)
The PRM-guided approach works by leveraging a separate model to rank and select among multiple candidate generations, effectively filtering out biased outputs while maintaining utility. By using a preference-ranking model trained to identify and score for fairness, the system can systematically prefer less biased alternatives during inference. The sequential refinement approach allows for iterative improvement by repeatedly applying the selection process, though this can lead to overcorrection when pushed too far. The effectiveness across both English and Urdu demonstrates that inference-time mitigation can address systemic training biases even in underrepresented languages.

## Foundational Learning
- Preference-ranking models (PRMs): AI models trained to score and rank candidates based on specific criteria like fairness; needed to provide automated bias evaluation during generation
- Inference-time mitigation: Techniques applied during text generation rather than during model training; needed because retraining large models is computationally expensive and may not address all bias scenarios
- Cross-lingual bias analysis: Comparing bias across languages to identify disparities in model performance; needed to reveal structural inequities in multilingual model training
- Single-word generation evaluation: Focusing on simple generation tasks to isolate and measure bias more precisely; needed because bias can manifest differently across task complexity
- Best-of-N sampling: Generating multiple candidates and selecting the best according to a scoring model; needed to improve output quality while reducing bias
- Iterative refinement: Repeated application of generation and selection to progressively improve outputs; needed to achieve maximal fairness improvements

## Architecture Onboarding

**Component Map**
GPT-3.5 (generation) -> GPT-4o-mini (PRM scoring) -> Candidate selection -> Output

**Critical Path**
Generation -> PRM scoring -> Selection/refinement -> Evaluation

**Design Tradeoffs**
- Single-word vs. multi-token generation: Simpler evaluation but may not capture full bias complexity
- Proprietary vs. open-source models: Higher performance but less transparency and reproducibility
- Iterative refinement depth: More iterations improve fairness but risk overcorrection and reduced utility

**Failure Signatures**
- Overcorrection in PRM-Sequential leading to reduced utility
- Cross-lingual gaps persisting despite mitigation
- PRM scoring introducing new biases or missing subtle forms of bias

**First Experiments**
1. Compare PRM-Select and PRM-Sequential performance on additional languages beyond English and Urdu
2. Test the framework with open-source generation and scoring models to assess dependency on proprietary systems
3. Evaluate the methods on multi-token generation tasks to determine effectiveness beyond single-word generation

## Open Questions the Paper Calls Out
None

## Limitations
- Results depend on proprietary models (GPT-3.5 and GPT-4o-mini), limiting reproducibility and raising concerns about inherent model biases
- Study focuses only on single-word generation, which may not reflect bias patterns in more complex text generation tasks
- Sample size of 200 prompts per language may be insufficient to capture full diversity of bias scenarios, particularly for Urdu's complex sociolinguistic landscape
- Analysis focuses on binary gender bias without examining intersectional dimensions or other forms of social bias
- PRM-Sequential approach shows overcorrection in Urdu, suggesting calibration issues for different linguistic contexts

## Confidence
- High Confidence: Urdu exhibits higher baseline bias than English, reflecting known inequities in multilingual LLM training
- Medium Confidence: PRM-Select effectively reduces bias and cross-lingual gaps, but results depend on specific models and prompts
- Medium Confidence: PRM-Sequential achieves maximal fairness but causes overcorrection in Urdu, requiring further investigation
- Low Confidence: Generalizability to other languages, bias types, or generation tasks remains uncertain

## Next Checks
1. Replicate the study using open-source generation and scoring models to reduce dependency on proprietary systems and better understand the robustness of the mitigation methods
2. Expand the prompt set to include diverse sociolinguistic contexts and intersectional bias scenarios, particularly for Urdu, to ensure findings are representative
3. Evaluate the PRM-guided methods on multi-token or sentence-level generation tasks to assess their effectiveness beyond single-word generation