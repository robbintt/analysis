---
ver: rpa2
title: Efficient Action-Constrained Reinforcement Learning via Acceptance-Rejection
  Method and Augmented MDPs
arxiv_id: '2503.12932'
source_url: https://arxiv.org/abs/2503.12932
tags:
- action
- policy
- aram
- learning
- acrl
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes ARAM, a framework that adapts standard deep
  RL algorithms to action-constrained settings using acceptance-rejection sampling
  and augmented MDPs. The method avoids computationally expensive QP operations and
  complex generative models by first sampling actions from an unconstrained policy,
  then accepting only those within feasible sets, and finally guiding the policy toward
  feasible regions through penalty signals in an augmented MDP.
---

# Efficient Action-Constrained Reinforcement Learning via Acceptance-Rejection Method and Augmented MDPs

## Quick Facts
- arXiv ID: 2503.12932
- Source URL: https://arxiv.org/abs/2503.12932
- Reference count: 26
- Authors: Wei Hung; Shao-Hua Sun; Ping-Chun Hsieh
- One-line primary result: ARAM achieves faster training, higher valid action rates, and lower per-action inference times than state-of-the-art ACRL methods across locomotion and resource allocation benchmarks.

## Executive Summary
This paper introduces ARAM, a framework for efficient action-constrained reinforcement learning that avoids computationally expensive quadratic programming (QP) operations. The method uses acceptance-rejection sampling to enforce constraints without projection layers, and guides the policy toward feasible regions through an augmented MDP with self-loop transitions and penalty signals. ARAM is implemented on top of multi-objective SAC with dual replay buffers, allowing it to scale to high-dimensional problems while maintaining fast inference times. The approach demonstrates superior performance on locomotion tasks (Hopper, HalfCheetah, Ant) and resource allocation problems (NSFnet, bike sharing) compared to state-of-the-art ACRL methods.

## Method Summary
ARAM addresses action-constrained RL by combining three key mechanisms: (1) acceptance-rejection sampling where actions from an unconstrained policy are accepted only if they satisfy constraints, avoiding QP operations; (2) an augmented MDP that adds self-loop transitions and penalty signals for infeasible actions, guiding the policy toward feasible regions; and (3) multi-objective RL with preference-conditioned policies that eliminate the need for hyperparameter tuning of penalty weights. The method uses dual replay buffers to separately store feasible and infeasible transitions, and is implemented on SAC with a shared critic network for all preferences. The approach achieves constraint satisfaction during both training and deployment while maintaining competitive performance on standard benchmarks.

## Key Results
- ARAM achieves faster training convergence than DPre+ and SPre+ on HopperVel and HalfCheetah while maintaining higher valid action rates
- Per-action inference time is significantly lower than projection-based methods (no QP operations required)
- ARAM scales effectively to high-dimensional problems (Ant with 20D actions) where acceptance-rejection sampling remains computationally efficient
- The method shows robustness across different constraint types including torque limits, L2-norm bounds, and capacity constraints

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Acceptance-rejection sampling enforces action constraints without solving quadratic programs.
- Mechanism: The unconstrained policy π_ϕ serves as a proposal distribution. Actions are sampled and checked against the feasible set C(s); those outside C(s) are rejected and resampling occurs. This replaces projection-based correction with a simple membership test.
- Core assumption: The acceptance rate is sufficiently high that resampling does not become a computational bottleneck.
- Evidence anchors:
  - [abstract]: "we leverage the classic acceptance-rejection method, where we treat the unconstrained policy as the proposal distribution and derive a modified policy with feasible actions"
  - [Section 4.1]: "ARM is computationally efficient as it only requires checking if an action satisfies the constraints. As a result, ARM largely obviates the need to solve QPs"
  - [corpus]: Weak direct validation; related ACRL papers focus on projection layers and generative models, not acceptance-rejection.
- Break condition: If acceptance rate approaches zero (e.g., randomly initialized policy in a small feasible region), the resampling loop may not terminate in bounded time.

### Mechanism 2
- Claim: An augmented MDP with self-loop transitions and penalty signals guides the policy toward feasible action regions, improving acceptance rate.
- Mechanism: AUTO-MDP ˜M adds (i) self-loop transition for infeasible actions a ∉ C(s), keeping the agent in the same state, and (ii) a penalty reward [0, −K] for infeasible actions vs. [r(s,a), 0] for feasible ones. The policy learns to avoid penalty by shifting mass toward C(s).
- Core assumption: The penalty weight K and preference λ are sufficient to shape policy behavior without excessive conservatism.
- Evidence anchors:
  - [abstract]: "construct an augmented two-objective MDP, which include additional self-loop state transitions and a penalty signal for the rejected actions"
  - [Section 4.2]: Proposition 1 proves optimal policies of the original constrained MDP remain optimal in AUTO-MDP for any λ ∈ Λ.
  - [corpus]: Neighbor papers mention penalty/constraint violation signals generally, but no direct validation of the AUTO-MDP construction.
- Break condition: If λ_r ≫ λ_c, penalty may be too weak to improve acceptance; if λ_c ≫ λ_r, policy may become overly conservative and sacrifice return.

### Mechanism 3
- Claim: Multi-objective RL with preference-conditioned policies eliminates the need for hyperparameter tuning of the penalty weight.
- Mechanism: MOSAC learns a single preference-conditioned policy π_ϕ(·|λ) and Q-function for all λ, using a shared critic. Dual replay buffers (Dr for feasible, Da for infeasible) balance gradient updates. Preference λ is sampled from a distribution ρ_λ during training.
- Core assumption: Shared learning across preferences transfers useful knowledge and improves sample efficiency.
- Evidence anchors:
  - [Section 4.3]: "MOSAC learns a preference-dependent policy π_ϕ(a|s; λ) and the corresponding vector-valued Q function by a preference-dependent critic network"
  - [Figure 5 / Appendix C.3]: Ablation shows MORL outperforms single-objective SAC with fixed preferences in both return and valid action rate.
  - [corpus]: No direct corpus validation of this specific MOSAC-based implementation.
- Break condition: If the preference distribution ρ_λ is poorly chosen, the policy may underperform at deployment-time λ values.

## Foundational Learning

- Concept: Acceptance-Rejection Sampling
  - Why needed here: Core mechanism for constraint enforcement without QP. Understanding proposal vs. target distributions and acceptance probability is essential.
  - Quick check question: Given proposal g and target f, what is the acceptance probability for sample x?

- Concept: Multi-Objective MDPs and Linear Scalarization
  - Why needed here: AUTO-MDP produces vector-valued rewards; the learner must scalarize via preference λ to optimize.
  - Quick check question: How does the optimal policy set change as λ varies over the probability simplex?

- Concept: Off-Policy Actor-Critic with Entropy Regularization (SAC)
  - Why needed here: ARAM is instantiated on SAC; understanding the critic loss, policy loss, and entropy coefficient α is required for implementation.
  - Quick check question: In SAC, what role does the soft value function V(s) play in the policy gradient?

## Architecture Onboarding

- Component map:
  - Environment -> ARM sampler -> Constraint oracle -> Action
  - Action -> Environment step -> Transition storage (Dr/Da) -> Mini-batch sampling
  - Mini-batch -> Critic update -> Policy update -> New policy

- Critical path:
  1. Environment step → sample action via ARM (resample if infeasible)
  2. Store transition in Dr (if feasible) or Da (if infeasible with penalty)
  3. Sample mini-batch from both buffers
  4. Update critic via scalarized TD loss (Eq. 4)
  5. Update policy via scalarized policy gradient (Eq. 5)

- Design tradeoffs:
  - Higher penalty K → faster acceptance rate improvement but potentially lower return if over-penalized.
  - Buffer ratio η → controls how many infeasible transitions are used per update; too high may destabilize learning early on.
  - Max resampling attempts → bounds inference time but risks falling back to projection.

- Failure signatures:
  - Valid action rate stuck near zero: Policy not receiving sufficient penalty gradient; check Da sampling and λ_c weight.
  - Return degrades while acceptance improves: Over-conservative policy; reduce λ_c or K.
  - Inference time spikes: Acceptance rate too low; add max resampling cutoff with projection fallback.

- First 3 experiments:
  1. Validate ARM sampler in isolation: Measure acceptance rate vs. feasible set volume on a toy 2D constraint (e.g., circle). Confirm bounded inference time with max attempts.
  2. Ablate penalty weight K and preference λ: Sweep K ∈ {0.05, 0.1, 0.2} and λ ∈ {[0.9,0.1], [0.5,0.5], [0.1,0.9]} on HalfCheetah. Plot return vs. valid action rate.
  3. Compare dual-buffer vs. single-buffer: Run ARAM with only Dr vs. Dr+Da on HopperVel. Verify that dual-buffer improves acceptance rate convergence without harming return.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effectively does the ARAM framework generalize to strictly on-policy reinforcement learning algorithms?
- Basis in paper: [explicit] The authors state ARAM "can be combined with any standard deep RL algorithm" (Page 2), yet the practical implementation relies on a dual-buffer design specific to off-policy methods (Page 7).
- Why unresolved: On-policy algorithms do not utilize replay buffers, so it is unclear how the "augmented replay buffer" for rejected transitions would be integrated into the update cycle of algorithms like PPO or TRPO.
- What evidence would resolve it: Empirical results applying ARAM to an on-policy algorithm on the provided ACRL benchmarks.

### Open Question 2
- Question: Can the target distribution be learned or adapted dynamically to maximize sample efficiency?
- Basis in paper: [inferred] Appendix C.5 analyzes fixed target distributions (Gaussian vs. Student-t) and notes the "flexibility in the choice of target distribution," but does not explore adaptive mechanisms.
- Why unresolved: The paper tests static heuristics for the target distribution $\pi^\dagger$, leaving open the possibility that a dynamic distribution could better track the changing policy during training.
- What evidence would resolve it: A comparison of the current fixed heuristics against a method that learns the target distribution parameters concurrently with the policy.

### Open Question 3
- Question: What are the performance limits of ARAM in environments with extremely high-dimensional action spaces or vanishingly small feasible sets?
- Basis in paper: [inferred] The paper identifies "low acceptance rate" as a critical issue (Page 5) and demonstrates scalability only up to 20 dimensions (Appendix C.6).
- Why unresolved: While AUTO-MDP guides the policy toward feasible regions, acceptance-rejection sampling theoretically becomes inefficient as the volume of the feasible set shrinks relative to the ambient space in very high dimensions.
- What evidence would resolve it: Evaluation of ARAM on tasks with action dimensions significantly greater than 20 (e.g., 50D+) or highly sparse constraints.

## Limitations

- Acceptance-rejection sampling may become computationally prohibitive when the feasible set is small relative to the unconstrained policy's support, potentially requiring fallback mechanisms.
- The AUTO-MDP construction, while theoretically sound, may not scale well to high-dimensional action spaces where constraint checking itself becomes expensive.
- Multi-objective RL component adds implementation complexity and requires careful tuning of the preference distribution ρ_λ, which is not extensively validated across different constraint geometries.

## Confidence

- Acceptance-rejection mechanism: High confidence in theoretical validity and practical effectiveness based on empirical results
- AUTO-MDP construction: Medium confidence in practical impact on learning efficiency, though sensitivity analysis shows reasonable robustness
- Overall performance claims: Medium confidence given comparisons primarily against ACRL methods that solve QPs or use generative models, but ablation studies are limited in scope

## Next Checks

1. Measure acceptance rate distribution across training steps for each benchmark task, identifying any cases where acceptance drops below 5% for extended periods.
2. Conduct a systematic ablation of the AUTO-MDP penalty weight K and preference λ values, including cases where λ_r ≫ λ_c and λ_c ≫ λ_r, to quantify the impact on both valid action rate and return.
3. Implement and compare against a fallback mechanism that uses projection-based constraint enforcement when ARM resampling exceeds a threshold, measuring the trade-off between computational cost and constraint satisfaction.