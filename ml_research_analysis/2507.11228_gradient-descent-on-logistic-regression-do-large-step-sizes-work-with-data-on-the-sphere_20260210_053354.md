---
ver: rpa2
title: 'Gradient Descent on Logistic Regression: Do Large Step-Sizes Work with Data
  on the Sphere?'
arxiv_id: '2507.11228'
source_url: https://arxiv.org/abs/2507.11228
tags:
- step
- dataset
- convergence
- have
- logistic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies gradient descent (GD) convergence on logistic\
  \ regression with large step sizes when data is restricted to the unit sphere. While\
  \ separable data allows arbitrarily large step sizes, non-separable data requires\
  \ step sizes below the stability threshold 2/\u03BB."
---

# Gradient Descent on Logistic Regression: Do Large Step-Sizes Work with Data on the Sphere?

## Quick Facts
- arXiv ID: 2507.11228
- Source URL: https://arxiv.org/abs/2507.11228
- Reference count: 22
- One-line primary result: In 1D logistic regression with unit-norm data, GD converges globally for all step sizes below stability threshold (γ < 2) through oscillatory dynamics, but higher dimensions allow counterexamples with cycling behavior.

## Executive Summary
This paper investigates whether restricting logistic regression data to the unit sphere (||xi|| = 1) is sufficient to guarantee global convergence of gradient descent (GD) for step sizes up to the stability threshold (γ < 2, where η = γ/λ and λ is the Hessian eigenvalue at the solution). While separable data allows arbitrarily large step sizes, non-separable data requires step sizes below 2/λ. The authors prove that in one dimension with equal-magnitude data, global convergence is guaranteed for all γ < 2 through oscillatory dynamics when γ ∈ (1, 2). However, they construct counterexamples in higher dimensions demonstrating that global convergence fails for any γ < 2, even with unit-norm data, using a "schmearing" technique that embeds 2D cycling trajectories onto high-dimensional spheres.

## Method Summary
The paper analyzes gradient descent on logistic regression L(w) = (1/n)Σᵢ log(1 + exp(-yᵢw^Txᵢ)) with step size η = γ/λ, where λ is the largest eigenvalue of the Hessian at the minimizer w*. For 1D analysis, the authors use equal-magnitude data with xi ∈ {+1, -1} and prove convergence through contraction analysis, distinguishing monotonic convergence (γ ≤ 1) from oscillatory convergence (γ ∈ (1, 2)). For higher dimensions, they construct counterexamples by "schmearing" a 2D cycling dataset onto a d-dimensional unit sphere via padding with ±siej where si = √(1 - ||xi||²), creating block-diagonal Hessian structures that preserve cycling behavior when d is sufficiently large.

## Key Results
- 1D convergence: GD converges globally for all γ < 2 when data restricted to unit sphere with equal magnitudes, with oscillatory convergence for γ ∈ (1, 2)
- Higher-dimensional counterexamples: For any γ < 2, there exist non-separable logistic regression problems on the unit sphere where GD converges to a cycle rather than the optimum
- Schmearing construction: The counterexamples are built by embedding 2D cycling trajectories into high-dimensional spaces while maintaining unit-norm constraints

## Why This Works (Mechanism)

### Mechanism 1
- Claim: In 1D logistic regression with equal-magnitude data, GD converges globally for all step sizes below the stability threshold through oscillatory dynamics when γ ∈ (1, 2).
- Mechanism: The GD map T(w) develops two stationary points (local max/min) on opposite sides of w* when γ > 1. Iterates cross w* repeatedly, with contraction guaranteed in either one step (when moving right-to-left) or two steps (when crossing twice), creating damped oscillations that converge linearly once in the oscillatory neighborhood.
- Core assumption: Data restricted to unit sphere (|xi| = 1 in 1D), non-separable case, logistic loss structure with sigmoid's decreasing derivative for w > 0.
- Evidence anchors: [abstract] "We prove that this is true in a one dimensional space... convergence is oscillatory when γ ∈ (1,2)." [section 2] Lemma 1-3 establish one-step and two-step contraction; Equation (9) quantifies linear rate: wt+2k − w* < (1 − (2−γ)/γ)^k(wt − w*).

### Mechanism 2
- Claim: Two-step contraction compensates for potential one-step expansion when iterates cross from right to left of the optimum.
- Mechanism: When wt > w* but wt+1 < w* (crossover), the average sigmoid derivative R(wt) > 1/η triggers the crossing. The return step leverages R(wt+1) > R(wt) (due to sigmoid's symmetry and monotonicity properties), ensuring (1 − ηR(wt+1))(1 − ηR(wt)) < 1. The product is bounded by 1 − γ(2−γ)R(wt)²/σ'(w*)² < 1.
- Core assumption: Logistic loss with sigmoid nonlinearity; R(w) increases as w approaches 0 from either side due to σ' maximized at 0.
- Evidence anchors: [section 2, Lemma 3] Explicit bound: wt+2 − w* / (wt − w*) ≤ 1 − γ(2−γ)R(wt)²/σ'(w*)² < 1. [section 2, Lemma 2] Double crossing guarantee: if wt > w* and wt+1 < w*, then wt+2 > w*.

### Mechanism 3
- Claim: Higher-dimensional counterexamples exist despite spherical data constraint via "schmearing"—embedding a 2D cycling trajectory into a high-dimensional sphere.
- Mechanism: Start with known 2D counterexample from Meng et al. (2024). Duplicate it 2(d−2) times, padding each with ±siej in extra dimensions where si = √(1 − ||xi||²). This makes all vectors unit-norm. The Hessian becomes block-diagonal: top-left preserves λb (base curvature), bottom-right contributes cb/(d−2) where cb is dataset-dependent constant. For large enough d, λb dominates, preserving the cycle in the first two coordinates.
- Core assumption: Base 2D dataset with cycle exists; cb < λb(d−2) achievable; symmetry in padded coordinates makes w* have zeros in last d−2 dimensions.
- Evidence anchors: [section 3, Theorem 2] "For any γ < 2, there exists a non-separable logistic regression problem in some dimension d > 1, on which there exists a GD trajectory... that converges to a cycle." [section 3, Equation 13-15] Block Hessian structure and dimension condition λb ≥ cb/(d−2).

## Foundational Learning

- Concept: Stability threshold (2/λ) vs. classical smoothness bound (2/L)
  - Why needed here: The paper's entire premise is that convergence can occur for step sizes between 2/L and 2/λ, where λ is local curvature at solution (typically λ ≪ L for logistic regression). Understanding this distinction is essential for interpreting γ < 2 as "below stability threshold."
  - Quick check question: Given a logistic regression problem with L = 10 (global smoothness) and λ = 1 (Hessian eigenvalue at solution), what step size range permits convergence per classical theory vs. this paper's threshold?

- Concept: Implicit bias of GD on separable data
  - Why needed here: Paper contrasts non-separable case with separable case where "directions of {wt} always converge to the maximum-margin separating hyperplane... under arbitrarily large step sizes." This motivates why non-separable convergence is harder.
  - Quick check question: Why does the maximum-margin property emerge in separable logistic regression but not provide guarantees in the non-separable setting?

- Concept: Fixed points and cycles in discrete dynamical systems
  - Why needed here: The paper frames GD as map T(w) = w − η∇L(w) and analyzes its fixed-point stability. When γ crosses 1, the fixed point at w* can still be stable but convergence becomes oscillatory; at γ = 2, period-2 cycles emerge.
  - Quick check question: If a GD map T has T(w*) = w* and T'(w*) = −1.5, what type of convergence behavior would you expect near w*?

## Architecture Onboarding

- Component map: Loss function L(w) = (1/n)Σᵢ log(1 + exp(−yᵢw^Txᵢ)) -> GD operator T(w) = w − η∇L(w) where η = γ/λ -> Stability parameter γ ∈ (0, 2) -> Data constraint ∥xi∥ = 1 (unit sphere) -> Hessian at solution ∇²L(w*) determines λ

- Critical path:
  1. Compute λ at solution (requires knowing w* or approximating)
  2. Set η = γ/λ with target γ < 2
  3. Initialize w0 (any point works in 1D; in higher dimensions, convergence depends on data geometry)
  4. Iterate: wt+1 = wt − (γ/λ)∇L(wt)
  5. Monitor: Check if ‖wt+1 − wt‖ suggests oscillation (alternating signs around w* in 1D) or cycling (periodic patterns in ‖wt‖ in higher dimensions)

- Design tradeoffs:
  - Larger γ (closer to 2): Faster asymptotic