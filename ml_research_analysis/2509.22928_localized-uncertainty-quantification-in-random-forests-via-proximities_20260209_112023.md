---
ver: rpa2
title: Localized Uncertainty Quantification in Random Forests via Proximities
arxiv_id: '2509.22928'
source_url: https://arxiv.org/abs/2509.22928
tags:
- points
- random
- prediction
- intervals
- coverage
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a localized uncertainty quantification method
  for random forests using proximity measures between training and test points. The
  approach forms localized distributions of out-of-bag (OOB) errors around nearby
  points, enabling prediction intervals for regression and trust scores for classification.
---

# Localized Uncertainty Quantification in Random Forests via Proximities

## Quick Facts
- arXiv ID: 2509.22928
- Source URL: https://arxiv.org/abs/2509.22928
- Reference count: 21
- Method provides localized uncertainty quantification for random forests using proximity-weighted OOB errors

## Executive Summary
This paper introduces RF-FIRE and RF-ICE, two methods for localized uncertainty quantification in random forests. RF-FIRE constructs prediction intervals for regression by forming local distributions of out-of-bag (OOB) errors around nearby training points, while RF-ICE computes trust scores for classification by measuring classification accuracy among nearby training points. Both methods leverage RF-GAP proximities to define neighborhoods and outperform existing approaches in balancing interval width with coverage for regression and in identifying unclassifiable points for classification.

## Method Summary
The method computes RF-GAP proximities between training points during random forest training, then uses these to identify k-nearest neighbors for each test point. For regression (RF-FIRE), it forms prediction intervals using quantiles of local OOB error distributions from neighbors. For classification (RF-ICE), it computes trust scores as proximity-weighted classification accuracy. The neighborhood size k controls the coverage-width tradeoff, with empirical results suggesting 3-5% of the dataset size is sufficient for target coverage.

## Key Results
- RF-FIRE achieves narrower prediction intervals than competing methods while maintaining desired coverage
- RF-ICE Conformity achieves 0.965 AUC in accuracy-rejection curves, outperforming probability difference (0.929) and tree conformity (0.730)
- Both methods improve model accuracy by identifying and excluding unclassifiable points
- RF-FIRE prediction intervals are equivalent to RF-Intervals when k=n, demonstrating the method's generalization

## Why This Works (Mechanism)

### Mechanism 1
- Localized OOB error distributions approximate test error distributions better than global error estimates
- Core assumption: Training and test data arise from the same generative distribution; OOB residuals are exchangeable with test residuals
- Evidence anchors: Abstract mentions localized OOB error distributions; Page 3 explains test points close in proximity to OOB points exhibit similar behavior

### Mechanism 2
- Neighborhood size k controls the coverage-width trade-off without requiring the full training set
- Core assumption: Local neighborhoods preserve sufficient signal for quantile estimation
- Evidence anchors: Page 4 notes 3-5% of dataset size is sufficient for target coverage; Page 3 states RF-FIRE equals RF-Intervals when k=n

### Mechanism 3
- Proximity-weighted classification accuracy identifies unclassifiable points better than probability-based uncertainty
- Core assumption: Misclassifications cluster in the decision space; nearby errors indicate local unpredictability
- Evidence anchors: Page 4 reports RF-ICE Conformity provided highest average AUC values (0.965 ± 0.068); Page 5 Table II shows RF-ICE outperforms probability difference and tree conformity

## Foundational Learning

- **Out-of-Bag (OOB) Error**
  - Why needed here: OOB errors serve as the internal validation set forming the basis of all uncertainty estimates
  - Quick check question: Can you explain why OOB points approximate test performance without additional holdout data?

- **Proximity Matrices in Random Forests**
  - Why needed here: RF-GAP proximities define the neighborhood structure for localizing uncertainty
  - Quick check question: How does RF-GAP differ from traditional terminal-node co-occurrence proximities?

- **Quantile Regression vs. Residual-Based Intervals**
  - Why needed here: RF-FIRE competes with QRF; understanding both clarifies when each is appropriate
  - Quick check question: Why would residual-based intervals be narrower than quantile regression intervals for the same coverage?

## Architecture Onboarding

- Component map: Trained RF -> RF-GAP Proximity Matrix (W) -> Local k-NN Selection -> OOB Residual Distribution D_k -> Quantile Extraction -> Prediction Interval / Trust Score

- Critical path:
  1. Compute OOB predictions and residuals during RF training (no extra passes)
  2. Build RF-GAP proximities between training points (W ∈ R^(n×n)) and test-to-train (W ∈ R^(n×n_test))
  3. For each test point, retrieve k neighbors with highest proximity
  4. Extract α/2 and 1−α/2 quantiles from neighbors' OOB residuals
  5. For classification, compute ECR or Conformity scores from proximity-weighted accuracy

- Design tradeoffs:
  - k selection: Lower k = narrower intervals, potentially under-covered; higher k = wider intervals, better coverage guarantee. Cross-validate k on OOB data
  - ECR vs. Conformity: ECR yields interpretable accuracy-like scores; Conformity better separates misclassified points (higher AUC)
  - Dynamic vs. fixed k: Dynamic k = all non-zero proximity neighbors (varies per point); fixed k = consistent neighborhood size across dataset

- Failure signatures:
  - Coverage significantly below target → increase k or check for distribution shift
  - All intervals uniform width → k too large (approaching n); reduce k
  - Trust scores near 0.5 for all points → proximities not discriminative; verify RF-GAP computation
  - High variance in interval widths with poor coverage → insufficient OOB samples per tree; increase tree count

- First 3 experiments:
  1. Coverage calibration sweep: On a validation split, sweep k ∈ {50, 100, 200, 500, n} and plot empirical coverage vs. mean interval width. Identify smallest k achieving target coverage
  2. Benchmark comparison: Compare RF-FIRE BIS scores against QRF and RF-Intervals on 3 OpenML-CTR23 datasets. Confirm RF-FIRE's narrower intervals at matched coverage
  3. Accuracy-rejection validation: For classification, plot accuracy-rejection curves for RF-ICE Conformity vs. probability difference on 3 OpenML-CC18 datasets. Verify higher AUC for RF-ICE

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the theoretical coverage guarantees for RF-FIRE prediction intervals when using finite neighborhood sizes (k < n), as opposed to the asymptotic guarantees known for the full OOB distribution (k = n)?
- Basis in paper: The paper demonstrates empirical coverage across datasets but does not derive finite-sample or local coverage bounds, leaving practitioners without formal calibration guarantees
- Why unresolved: Authors show empirical coverage but lack theoretical analysis establishing coverage bounds for RF-FIRE under specific assumptions
- What evidence would resolve it: A theoretical analysis establishing coverage bounds for RF-FIRE under specific assumptions about k, data distribution, and forest structure

### Open Question 2
- Question: How robust are RF-FIRE intervals and RF-ICE trust scores to distribution shift between training and test data?
- Basis in paper: The method assumes "training and test data arise from the same generative model" (Section IV), with no experiments evaluating performance when this assumption is violated
- Why unresolved: High-stakes applications often encounter distribution shift; the paper does not investigate whether local proximity-weighted error distributions remain reliable when test points lie outside the training support
- What evidence would resolve it: Experiments on datasets with induced distribution shift (e.g., shifted features, novel class regions) measuring coverage degradation and trust score calibration

### Open Question 3
- Question: What is the principled methodology for selecting the neighborhood size k to optimally balance interval width, coverage, and local adaptivity across diverse datasets?
- Basis in paper: The paper notes k "can be determined dynamically... or treated as a dataset-dependent hyperparameter chosen through cross-validation" and empirically finds 3–5% of dataset size sufficient, but offers no systematic selection criteria or theoretical justification
- Why unresolved: Practitioners lack guidance on k selection for new datasets; the trade-off between local adaptivity (low k) and coverage stability (high k) remains heuristic
- What evidence would resolve it: A theoretical or empirical study deriving dataset-adaptive k selection rules (e.g., based on intrinsic dimensionality, noise level, or forest structure) with validation across benchmark suites

## Limitations

- Distribution shift sensitivity: Performance degrades with covariate or concept drift as method assumes similar training and test distributions
- Proximity quality dependence: RF-GAP proximities may not capture semantic similarity in high-dimensional or sparse feature spaces
- Neighborhood size selection: The 3-5% heuristic lacks rigorous justification and may not generalize across datasets

## Confidence

- RF-FIRE mechanism (High): Strong empirical support from experiments and clear theoretical connection to RF-Intervals
- Coverage-width tradeoff (Medium): Supported by single empirical observation; requires broader validation
- RF-ICE superiority (Medium): Statistical significance testing not reported; AUC differences may not be practically meaningful
- Cross-dataset generalizability (Low): Experiments limited to OpenML repositories; performance on real-world noisy data unknown

## Next Checks

1. **Distribution shift robustness**: Evaluate RF-FIRE and RF-ICE on datasets with known covariate shifts (e.g., UCI Shift datasets) to quantify performance degradation
2. **Hyperparameter sensitivity analysis**: Systematically vary k (from 1% to 20% of n) and tree count (from 100 to 1000) to identify breaking points and optimal configurations
3. **Alternative proximity comparison**: Benchmark RF-GAP against alternative similarity measures (Euclidean, cosine, learned embeddings) on datasets with ground-truth similarity information