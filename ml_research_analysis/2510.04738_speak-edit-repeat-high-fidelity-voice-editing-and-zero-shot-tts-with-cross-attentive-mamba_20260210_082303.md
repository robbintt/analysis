---
ver: rpa2
title: 'Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive
  Mamba'
arxiv_id: '2510.04738'
source_url: https://arxiv.org/abs/2510.04738
tags:
- audio
- speech
- editing
- mamba
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces MA VE, a novel autoregressive architecture
  for text-conditioned voice editing and high-fidelity text-to-speech synthesis that
  combines Mamba state-space models with cross-attention mechanisms. The key innovation
  is replacing the quadratic-complexity self-attention in Transformers with linear-complexity
  Mamba layers while maintaining precise text-acoustic alignment through cross-attention,
  enabling efficient modeling of long audio sequences with strong linguistic conditioning.
---

# Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba

## Quick Facts
- arXiv ID: 2510.04738
- Source URL: https://arxiv.org/abs/2510.04738
- Authors: Baher Mohammad; Magauiya Zhussip; Stamatios Lefkimmiatis
- Reference count: 40
- Primary result: Achieves human-parity speech naturalness in editing (57.2% indistinguishability) with 6× memory reduction versus Transformer baselines

## Executive Summary
This paper introduces MA VE, a novel autoregressive architecture for text-conditioned voice editing and high-fidelity text-to-speech synthesis that combines Mamba state-space models with cross-attention mechanisms. The key innovation is replacing the quadratic-complexity self-attention in Transformers with linear-complexity Mamba layers while maintaining precise text-acoustic alignment through cross-attention, enabling efficient modeling of long audio sequences with strong linguistic conditioning. MA VE achieves human-parity naturalness in speech editing, with 57.2% of listeners rating edited speech as indistinguishable from the original, and outperforms state-of-the-art models like VoiceCraft and FluentSpeech in both speaker similarity and naturalness for zero-shot TTS.

## Method Summary
MA VE employs a hybrid architecture that integrates Mamba state-space models with cross-attention mechanisms to achieve efficient, high-fidelity speech synthesis. The model uses Mamba layers to process acoustic sequences with linear complexity instead of the quadratic complexity of traditional self-attention, enabling scalable modeling of long audio sequences. Cross-attention layers maintain precise alignment between text and acoustic representations, preserving linguistic conditioning while avoiding the computational overhead of full self-attention. The autoregressive decoder generates mel-spectrograms conditioned on both the Mamba-processed acoustic features and the cross-attended text representations. This design achieves comparable or superior speech quality to Transformer-based baselines while significantly reducing memory requirements during inference.

## Key Results
- Achieves human-parity naturalness in speech editing with 57.2% of listeners unable to distinguish edited speech from original recordings
- Outperforms VoiceCraft and FluentSpeech in zero-shot TTS for both speaker similarity and naturalness metrics
- Demonstrates approximately 6× memory reduction compared to VoiceCraft during inference while maintaining comparable latency

## Why This Works (Mechanism)
The architecture succeeds by addressing the fundamental trade-off between computational efficiency and linguistic alignment in speech synthesis. Mamba layers provide linear-time processing of long acoustic sequences, avoiding the quadratic complexity bottleneck of self-attention that limits Transformer scalability. The cross-attention mechanism selectively attends to text representations at each decoding step, maintaining precise phoneme-to-timestep alignment without requiring full self-attention over the entire sequence. This combination enables the model to capture both long-range acoustic dependencies through Mamba's state-space modeling and fine-grained linguistic structure through targeted cross-attention, achieving high-fidelity synthesis with substantially reduced computational requirements.

## Foundational Learning
**Mamba State-Space Models**
- *Why needed*: Enable linear-time processing of long sequences by replacing quadratic self-attention with selective state updates
- *Quick check*: Verify the model maintains temporal coherence in generated speech sequences compared to Transformer baselines

**Cross-Attention Mechanisms**
- *Why needed*: Preserve precise text-to-acoustic alignment while avoiding full self-attention complexity
- *Quick check*: Evaluate phoneme-level alignment accuracy between input text and generated speech

**Autoregressive Generation**
- *Why needed*: Ensures coherent, natural-sounding speech by generating each timestep conditioned on previous outputs
- *Quick check*: Compare sample quality and naturalness between autoregressive and parallel-generation variants

## Architecture Onboarding

**Component Map**
Mamba encoder -> Cross-attention layers -> Autoregressive decoder -> Mel-spectrogram output

**Critical Path**
Input text → Text encoder → Cross-attention with Mamba-processed audio features → Autoregressive mel-spectrogram generation → Vocoder

**Design Tradeoffs**
Linear Mamba complexity vs. potential loss of global self-attention context; cross-attention precision vs. additional computational overhead; autoregressive coherence vs. parallel generation speed

**Failure Signatures**
Speech artifacts indicating poor long-range acoustic modeling; misaligned phoneme-timestep correspondence; reduced speaker identity preservation in zero-shot scenarios

**First Experiments**
1. Benchmark Mamba-only vs. cross-attention augmented versions on text-acoustic alignment quality
2. Measure memory usage and latency across different sequence lengths to validate 6× reduction claim
3. Conduct ablation studies removing cross-attention to quantify its contribution to naturalness scores

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Human-parity naturalness claims are specific to editing scenarios and may not generalize to all TTS applications
- Cross-attention mechanism adds complexity that may affect long-term dependency modeling compared to pure self-attention architectures
- Generalization capabilities beyond LibriSpeech dataset remain untested for diverse acoustic conditions and speaker demographics

## Confidence
- **High Confidence**: Technical implementation details, architectural innovations, and memory efficiency improvements
- **Medium Confidence**: Subjective evaluation results (naturalness, speaker similarity), relative performance comparisons with baselines
- **Medium Confidence**: Cross-attention mechanism effectiveness for text-acoustic alignment

## Next Checks
1. Conduct comprehensive ablation studies isolating the contributions of Mamba layers versus cross-attention mechanisms to quantify their individual impacts on speech quality and efficiency.

2. Evaluate MA VE's zero-shot TTS performance across diverse datasets (VCTK, LibriTTS, multilingual corpora) to assess generalization beyond LibriSpeech, including speakers with different accents, ages, and recording conditions.

3. Perform controlled listening tests comparing MA VE with additional baseline architectures (Glow-TTS, Grad-TTS) using standardized protocols to validate the human-parity naturalness claims and ensure results are not dataset-specific.