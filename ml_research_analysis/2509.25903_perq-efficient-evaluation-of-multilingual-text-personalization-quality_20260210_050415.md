---
ver: rpa2
title: 'PerQ: Efficient Evaluation of Multilingual Text Personalization Quality'
arxiv_id: '2509.25903'
source_url: https://arxiv.org/abs/2509.25903
tags:
- personalization
- quality
- texts
- text
- metric
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces PerQ, an efficient evaluation metric for multilingual
  text personalization quality. The authors address the problem of using resource-intensive
  large language models (LLMs) for meta-evaluation of personalization quality, which
  is common due to the lack of dedicated metrics.
---

# PerQ: Efficient Evaluation of Multilingual Text Personalization Quality

## Quick Facts
- arXiv ID: 2509.25903
- Source URL: https://arxiv.org/abs/2509.25903
- Reference count: 14
- PerQ is a fine-tuned classifier that achieves strong correlation with LLM evaluations while requiring 96× less computation and 12× less GPU memory

## Executive Summary
The paper introduces PerQ, an efficient evaluation metric for multilingual text personalization quality. The authors address the problem of using resource-intensive large language models (LLMs) for meta-evaluation of personalization quality, which is common due to the lack of dedicated metrics. PerQ is a fine-tuned classifier trained on majority-voted scores from three diverse LLMs, achieving strong correlation with LLM evaluations (Spearman > 0.8) while requiring significantly less computational resources. For example, PerQ can provide predictions 96× faster than using three LLMs, with 12× less GPU memory. A case study demonstrates PerQ's usability in comparing personalization capabilities across different LLMs, languages, and platforms, confirming findings from expensive LLM-based evaluations at a fraction of the cost.

## Method Summary
The authors developed PerQ by training a classifier on personalization quality scores obtained from three different LLMs (GPT-3.5, GPT-4, Claude 3) via majority voting. The metric was trained on a dataset of 25,200 personalized texts across seven languages (English, German, French, Italian, Slovak, Russian, Hungarian), each with four different platform-specific personalization levels. The model was fine-tuned on the collected labels and evaluated using Spearman correlation with the LLM predictions, achieving strong performance while requiring significantly less computational resources than direct LLM evaluation.

## Key Results
- PerQ achieves Spearman correlation > 0.8 with LLM-based evaluations
- PerQ is 96× faster and requires 12× less GPU memory than using three LLMs
- Case study confirms PerQ can effectively compare personalization capabilities across different LLMs, languages, and platforms
- Strong performance maintained across seven different languages in the tested dataset

## Why This Works (Mechanism)
PerQ works by distilling the collective judgment of multiple LLMs into a more efficient classifier model. By training on majority-voted scores from three diverse LLMs, the metric captures consensus judgments about personalization quality while avoiding the computational cost of running multiple LLMs for each evaluation. The approach leverages the observation that LLM meta-evaluations, while expensive, can provide high-quality labels when aggregated, which can then be used to train a much more efficient classifier that approximates their collective judgment.

## Foundational Learning

**LLM-based evaluation**: Using LLMs as judges for text quality assessment - needed because traditional metrics struggle with subjective aspects like personalization; quick check: verify the LLM judges produce consistent scores across multiple runs.

**Majority voting**: Aggregating multiple independent judgments to improve reliability - needed to reduce individual LLM biases and noise; quick check: confirm that majority votes align better with human judgments than individual LLM scores.

**Fine-tuning classifiers**: Adapting pre-trained models to specific evaluation tasks - needed to create task-specific metrics from general-purpose models; quick check: test whether fine-tuning on LLM labels transfers to human preferences.

**Multilingual evaluation**: Assessing text quality across multiple languages - needed because personalization often requires cultural and linguistic adaptation; quick check: verify consistent performance across all target languages.

## Architecture Onboarding

**Component map**: Input text -> PerQ classifier -> 4-point personalization score

**Critical path**: Text preprocessing -> Feature extraction -> Classifier prediction -> Output score

**Design tradeoffs**: The authors chose a fine-tuned classifier over prompt engineering or other lightweight approaches to achieve better accuracy while maintaining efficiency gains over full LLM evaluation.

**Failure signatures**: Performance degradation when texts contain complex cultural references, ambiguous personalization cues, or when evaluating languages outside the training set.

**First experiments**: 1) Test PerQ on texts from languages not in training set, 2) Compare PerQ scores with human annotations on a small sample, 3) Measure correlation between PerQ and individual LLM scores to check for overfitting.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does PerQ performance generalize to languages outside the seven tested, particularly for low-resource languages or those with significantly different syntactic structures?
- Basis in paper: [explicit] The authors state in the Limitations section that the study is limited to 7 languages and they are "unsure about generalization of our findings to other languages."
- Why unresolved: The training and testing data were restricted to English, German, French, Italian, Slovak, Russian, and Hungarian.
- What evidence would resolve it: Evaluation of the trained metric on a dataset of personalized texts in languages not present in the training set (e.g., Japanese, Arabic, Swahili).

### Open Question 2
- Question: What is the actual correlation between PerQ predictions and human judgments on the specific multilingual platform-targeted task, as opposed to the estimated correlation derived from a different dataset?
- Basis in paper: [inferred] The paper estimates a 0.725 correlation with human judgments using the PerDisNews dataset (English-only, targeting user groups), but notes the main dataset lacks human annotations ("missing human annotations") and targets different "personalization" aspects (platforms vs. groups).
- Why unresolved: Collecting human annotations for the 25,200 multilingual texts was deemed infeasible by the authors.
- What evidence would resolve it: A human annotation study on a subset of the generated multilingual platform-specific texts to calculate the direct Spearman correlation with PerQ scores.

### Open Question 3
- Question: Can the proposed metric training methodology be effectively transferred to evaluate other subjective text aspects (e.g., fluency, hallucinations) or different scoring scales without significant loss in accuracy?
- Basis in paper: [explicit] The Limitations section notes the focus on a specific 4-point personalization scale and states, "evaluation of other aspects of the texts or usage of different evaluation scale might be different."
- Why unresolved: The experiments only validated the approach for the specific task of platform-based personalization quality.
- What evidence would resolve it: Applying the same training pipeline (LLM majority vote distillation) to train metrics for distinct dimensions like coherence or factual consistency and comparing their performance to current LLM-based evaluators.

## Limitations
- Performance evaluated only against the same LLM evaluators used for training data generation, raising concerns about potential overfitting
- Computational efficiency claims presented without detailed benchmarking methodology or consideration of different hardware configurations
- Case study demonstrating utility lacks statistical rigor in validating whether observed differences are significant
- Limited to seven languages, with uncertain generalization to other languages or low-resource languages

## Confidence

- **High confidence**: PerQ achieves computational efficiency gains over direct LLM evaluation (based on clear timing and memory comparisons)
- **Medium confidence**: PerQ correlates strongly with LLM evaluations for personalization quality (though validation against truly independent evaluators would strengthen this claim)
- **Low confidence**: PerQ captures meaningful aspects of personalization quality beyond what it was trained to mimic from LLMs

## Next Checks

1. Evaluate PerQ against a held-out set of human-annotated personalization quality judgments to determine if it captures human perceptions beyond LLM preferences

2. Test PerQ's generalization by evaluating it on LLMs not used in training data generation to assess whether it truly measures personalization quality or merely memorizes specific evaluator patterns

3. Conduct ablation studies varying training data size, diversity of source LLMs, and voting mechanisms to determine the minimum viable configuration for maintaining strong performance while maximizing efficiency gains