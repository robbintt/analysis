---
ver: rpa2
title: 'Time To Impeach LLM-as-a-Judge: Programs are the Future of Evaluation'
arxiv_id: '2506.10403'
source_url: https://arxiv.org/abs/2506.10403
tags:
- programs
- response
- arxiv
- evaluation
- pajama
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "PAJAMA synthesizes executable judging programs from LLM-generated\
  \ logic, reducing API costs by ~3500x compared to LLM-as-a-judge while improving\
  \ consistency by 15.83% and reducing biased responses by 23.7%. When distilled into\
  \ a reward model, PAJAMA outperforms LLM-as-a-judge-distilled models on RewardBench\u2019\
  s CHAT-HARD subset (+2.19% on Prometheus, +8.67% on JudgeLM) and maintains accuracy\
  \ across 52 programs, improving accuracy from 59% (3 programs) to 82.2% without\
  \ performance plateau."
---

# Time To Impeach LLM-as-a-Judge: Programs are the Future of Evaluation

## Quick Facts
- arXiv ID: 2506.10403
- Source URL: https://arxiv.org/abs/2506.10403
- Authors: Tzu-Heng Huang; Harit Vishwakarma; Frederic Sala
- Reference count: 40
- Key outcome: PAJAMA reduces API costs by ~3500x compared to LLM-as-a-judge while improving consistency by 15.83% and reducing biased responses by 23.7%

## Executive Summary
PAJAMA introduces a novel evaluation paradigm that synthesizes executable judging programs from LLM-generated logic, addressing the high costs and biases of traditional LLM-as-a-judge methods. By generating a fixed set of Python judging functions (52 in the paper) and executing them locally, PAJAMA achieves dramatic cost reductions while maintaining or improving accuracy. The approach uses weak supervision aggregation to combine noisy program outputs into reliable preference labels, and can be distilled into a portable reward model. Experimental results show PAJAMA outperforms LLM-as-a-judge across multiple benchmarks, with particular improvements in consistency and reduction of positional and reference biases.

## Method Summary
PAJAMA synthesizes executable judging programs using GPT-4o based on six criteria (Structure, Relevance, Readability, Bias, Factuality, Safety). These programs score LLM responses and output discretized preferences (+1/-1). A weak supervision model (Snorkel) learns program reliability weights and aggregates outputs to infer consensus preference labels. The aggregated labels can then be used to fine-tune a smaller reward model (Gemma-2B-it) for local deployment. The method achieves significant cost reduction by executing pre-synthesized programs rather than making per-sample API calls, and improves consistency by replacing stochastic LLM judgments with deterministic program execution.

## Key Results
- Reduces API costs by ~3500x compared to direct LLM-as-a-judge scoring
- Improves accuracy from 59% (3 programs) to 82.2% (52 programs) on RewardBench
- Outperforms LLM-as-a-judge-distilled models on RewardBench's CHAT-HARD subset (+2.19% on Prometheus, +8.67% on JudgeLM)
- Reduces biased response win rate from 43.97% to 20.26% while improving consistency from 48.36% to 64.19%

## Why This Works (Mechanism)

### Mechanism 1: Cost Decoupling via Program Synthesis
The LLM generates a small, fixed number of judging programs (52) that can be executed locally on arbitrary dataset sizes. API costs scale with program count, not evaluation samples, achieving ~3500x cost reduction.

### Mechanism 2: Weak Supervision Aggregation Reduces Noise
Multiple noisy program outputs are aggregated via weak supervision, where a probabilistic model learns reliability weights for each program and infers consensus labels, outperforming naïve majority voting.

### Mechanism 3: Deterministic Program Execution Mitigates LLM Biases
Programs apply fixed rubrics without access to candidate order or stylistic cues that trigger LLM biases, eliminating position bias and reducing gender and rich-content biases compared to LLM-as-a-judge.

## Foundational Learning

- **Weak Supervision / Data Programming**
  - Why needed here: Core to aggregating noisy program outputs into reliable preference labels without ground truth
  - Quick check question: Can you explain why modeling label correlations improves over majority voting?

- **LLM-as-a-Judge Paradigm**
  - Why needed here: Provides baseline understanding of what PAJAMA replaces and why (cost, bias, inflexibility)
  - Quick check question: What are four documented failure modes of LLM-as-a-judge per the paper?

- **Reward Model Distillation**
  - Why needed here: PAJAMA labels can train a smaller reward model for local deployment; understanding this pipeline is essential
  - Quick check question: Why might a distilled model generalize beyond its training programs?

## Architecture Onboarding

- **Component map:**
  1. Program Synthesis Prompting → 2. Program Execution Engine → 3. Discretization Layer → 4. Weak Supervision Aggregator → 5. Optional Distillation

- **Critical path:**
  Prompt design → Program synthesis → Execution on evaluation set → Weak supervision aggregation → (Optional) Reward model distillation

- **Design tradeoffs:**
  - More programs → higher accuracy but diminishing returns; paper shows no plateau at 52 programs
  - Criteria diversity → reduces correlation but requires careful prompt engineering
  - Distillation → enables local deployment but may lose interpretability of raw program logic
  - Third-party libraries → add capability but introduce un-audited dependencies

- **Failure signatures:**
  - Low accuracy on CHAT-HARD subset: programs may underfit complex reasoning
  - Safety subset degradation: rule-based logic struggles with nuanced policy
  - High inter-program correlation: weak supervision gains collapse
  - Execution errors: malformed Python or library mismatches

- **First 3 experiments:**
  1. **Baseline Replication**: Run PAJAMA's 52 programs on held-out Prometheus slice. Verify 82.2% accuracy vs majority-vote baseline.
  2. **Ablation on Program Count**: Sample 3, 10, 25, 52 programs. Plot accuracy vs. count to confirm scaling trend.
  3. **Bias Injection Test**: Apply position/reference bias perturbations to responses. Measure consistency and biased win rate; compare to Table 2 benchmarks.

## Open Questions the Paper Calls Out

### Open Question 1
Can program-based judges be adapted to handle subjective safety evaluations where they currently underperform LLM-as-a-judge? The current criteria rely on explicit logic or BERT models that may fail to capture nuance of emotional intent or complex safety policies as effectively as generative LLMs.

### Open Question 2
Does PAJAMA's performance plateau or continue to scale with the number of synthesized judging programs? The study only tested up to 52 programs; it is undetermined if signal saturation occurs at higher numbers or if weak supervision aggregation fails with too many noisy sources.

### Open Question 3
What is the impact of using more sophisticated program synthesis methods on the quality of aggregated judgment? The simplicity of the prompt may limit the complexity and diversity of resulting logic, potentially capping the effectiveness of the weak supervision ensemble.

## Limitations
- Safety subset degradation (2.7% drop) indicates rule-based logic struggles with nuanced policy domains
- Third-party library dependencies (BERT models, embedding models) introduce potential bias and compatibility risks
- Generalization claim across 52 programs lacks theoretical bounds for diminishing returns

## Confidence
- **High confidence**: Cost reduction mechanism, basic accuracy improvements, bias reduction metrics
- **Medium confidence**: Generalization across all 52 programs without performance plateau, weak supervision aggregation superiority
- **Low confidence**: Safety subset performance, long-term stability of synthesized programs, robustness to out-of-distribution queries

## Next Checks
1. **Correlation Analysis**: Measure pairwise agreement rates among all 52 programs. If average agreement exceeds 80%, weak supervision benefits may be overstated.
2. **Out-of-Distribution Testing**: Apply PAJAMA programs to mathematical reasoning or code generation tasks not explicitly covered by the six criteria. Measure accuracy degradation.
3. **Library Dependency Audit**: Replace BERT and embedding models with simpler alternatives (e.g., keyword matching). Quantify performance impact to assess sensitivity to un-audited components.