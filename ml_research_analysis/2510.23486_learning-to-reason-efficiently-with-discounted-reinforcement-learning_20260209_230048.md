---
ver: rpa2
title: Learning to Reason Efficiently with Discounted Reinforcement Learning
arxiv_id: '2510.23486'
source_url: https://arxiv.org/abs/2510.23486
tags:
- policy
- reasoning
- arxiv
- blackwell
- optimal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the problem of efficient reasoning in large
  language models, which often produce overly long chains of thought that increase
  computational cost and latency. The authors propose a principled approach using
  discounted reinforcement learning to encourage concise yet accurate reasoning.
---

# Learning to Reason Efficiently with Discounted Reinforcement Learning

## Quick Facts
- arXiv ID: 2510.23486
- Source URL: https://arxiv.org/abs/2510.23486
- Reference count: 18
- Primary result: Discounted RL achieves similar accuracy to undiscounted approaches while significantly reducing response length across multiple benchmarks

## Executive Summary
This work addresses the problem of efficient reasoning in large language models, which often produce overly long chains of thought that increase computational cost and latency. The authors propose a principled approach using discounted reinforcement learning to encourage concise yet accurate reasoning. By applying a discount factor close to 1 to the environment (correctness) reward while leaving intrinsic formatting rewards undiscounted, they create an objective that simultaneously maximizes accuracy and minimizes expected response length.

## Method Summary
The authors propose using discounted reinforcement learning where environment rewards (task correctness) are discounted by a factor γ close to 1, while intrinsic rewards (formatting) remain undiscounted. This creates an objective that encourages concise reasoning without sacrificing accuracy. They theoretically analyze Blackwell optimal policies in deterministic MDPs with binary terminal rewards, showing that optimal policies exist within restricted policy classes. Empirically, they implement this approach using GRPO with discounted rewards across multiple mathematical reasoning benchmarks, demonstrating that shorter reasoning traces can be achieved without accuracy loss.

## Key Results
- On GSM8K, achieved 91.07% accuracy with 170.08 tokens versus 91.06% accuracy with 217.60 tokens for undiscounted baseline
- Consistent performance across multiple benchmarks (MATH, AMC 2023, AIME 2025, MINERVA, OLYMPIAD)
- Generalizes across different model sizes (Qwen2.5 7B/14B, Llama 3 8B, Phi-4)
- Theoretical analysis shows Blackwell optimal policies exist within restricted policy classes

## Why This Works (Mechanism)
The approach works by creating a multi-objective optimization problem where the discounted environment reward encourages shorter reasoning chains while the undiscounted intrinsic formatting rewards prevent degenerate solutions (empty responses). The discount factor γ close to 1 preserves accuracy incentives while providing a gentle pressure toward conciseness. The softmax deployment policy with appropriately chosen Blackwell discount factor ensures that greedy policies achieve both maximum accuracy and minimum mean response length in the theoretical setting.

## Foundational Learning

**Reinforcement Learning with Environment Rewards**
- Why needed: To optimize for task correctness through trial-and-error interaction
- Quick check: Verify reward shaping doesn't create unintended incentives

**Discounted Returns**
- Why needed: To mathematically encode preference for shorter reasoning chains
- Quick check: Ensure discount factor is close enough to 1 to preserve accuracy

**Blackwell Optimality**
- Why needed: To guarantee existence of policies that are both accurate and concise
- Quick check: Confirm MDP structure satisfies theoretical assumptions

**Intrinsic vs Extrinsic Rewards**
- Why needed: To prevent degenerate solutions while optimizing for task performance
- Quick check: Validate formatting rewards are sufficient to maintain solution structure

## Architecture Onboarding

**Component Map**
Input → LLM with Reasoning Policy → Environment (Task) → Reward Function (Discounted Env + Undiscounted Intrinsic) → GRPO Update → Updated Policy

**Critical Path**
1. Generate reasoning trace
2. Compute discounted environment reward and undiscounted intrinsic reward
3. Apply GRPO update with combined reward
4. Deploy updated policy for inference

**Design Tradeoffs**
- Discount factor γ close to 1 preserves accuracy but provides weaker length pressure
- Undiscounted formatting rewards prevent empty responses but may not fully control length
- Theoretical assumptions (deterministic MDPs, binary rewards) may not hold in practice

**Failure Signatures**
- Very short responses with poor accuracy (discount factor too aggressive)
- No length reduction despite training (discount factor too close to 1)
- Degenerate empty responses (insufficient intrinsic rewards)

**First 3 Experiments**
1. Vary discount factor γ on GSM8K to find optimal accuracy-length tradeoff
2. Remove intrinsic formatting rewards to test necessity for preventing degenerate solutions
3. Apply approach to non-mathematical reasoning tasks to test domain generalization

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical analysis assumes deterministic MDPs with binary terminal rewards, which may not capture real-world reasoning complexity
- Approach may be sensitive to reward design choices, particularly the balance between environment and intrinsic rewards
- Empirical evaluation focuses primarily on mathematical reasoning tasks, leaving generalization to other domains uncertain

## Confidence
- **High confidence**: Empirical results showing consistent accuracy preservation while reducing token counts across multiple benchmarks and model sizes
- **Medium confidence**: Theoretical claims about Blackwell optimal policies existing within restricted policy classes, as they rely on simplifying assumptions about MDP structure
- **Medium confidence**: Generalizability of the approach to reasoning tasks beyond mathematical problem-solving, given the evaluation focus

## Next Checks
1. Test the approach on non-mathematical reasoning tasks (e.g., commonsense reasoning, logical inference, or multi-hop QA) to assess domain generalization
2. Conduct ablation studies on the intrinsic formatting rewards to determine their necessity and impact on performance across different task types
3. Evaluate the approach on more diverse model architectures and training paradigms (e.g., models trained with different alignment techniques or reward models) to test robustness to implementation variations