---
ver: rpa2
title: Could Thinking Multilingually Empower LLM Reasoning?
arxiv_id: '2504.11833'
source_url: https://arxiv.org/abs/2504.11833
tags:
- language
- multilingual
- languages
- reasoning
- english
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether multilingual reasoning can enhance
  large language model (LLM) performance beyond the typical "English bias." The authors
  evaluate three LLMs on two reasoning tasks (GPQA and MGSM) using human-translated
  prompts in 17 languages. By aggregating outputs across multiple languages, they
  show that multilingual reasoning can significantly increase the upper bound of performance
  (by up to ~10 Acc@k points) compared to English-only reasoning.
---

# Could Thinking Multilingually Empower LLM Reasoning?

## Quick Facts
- **arXiv ID**: 2504.11833
- **Source URL**: https://arxiv.org/abs/2504.11833
- **Reference count**: 20
- **Primary result**: Multilingual reasoning can boost LLM reasoning upper bounds by up to 10 Acc@k points, but common selection methods fail to fully exploit this advantage.

## Executive Summary
This paper investigates whether multilingual reasoning can overcome the "English bias" in large language models (LLMs). The authors evaluate three LLMs (Qwen2.5-72B, LLaMA3.1-70B, R1-Distill-LLaMA-70B) on two reasoning tasks (GPQA and MGSM) using human-translated prompts in 17 languages. They demonstrate that aggregating outputs across multiple languages significantly increases the theoretical upper bound of performance (Acc@k), but common selection strategies like majority voting and LLM-as-a-judge fail to achieve these gains. The study reveals that while multilingual reasoning shows promise, the practical challenge lies in developing effective answer selection methods.

## Method Summary
The authors evaluate multilingual reasoning by translating prompts into 17 languages and aggregating model outputs. For each question, they generate responses in multiple languages using chain-of-thought prompting, then measure both the theoretical upper bound (Acc@k - at least one correct answer among k candidates) and practical performance (Vote@k - majority voting accuracy). They compare multilingual settings against English-only approaches using different seeds and paraphrases. The evaluation includes human-translated GPQA and MGSM data from BenchMAX, with machine translation variants tested for robustness.

## Key Results
- Multilingual reasoning increases the upper bound of performance by up to 10 Acc@k points compared to English-only reasoning
- Majority voting and LLM-as-a-judge selection methods fail to achieve these theoretical gains
- Machine translation quality is sufficient to achieve the multilingual advantage, with no significant benefit from human translation
- A small set of 4 diverse languages provides most of the multilingual benefit

## Why This Works (Mechanism)
The paper hypothesizes that multilingual reasoning works through error independence - different languages compensate for each other's errors because the model's failures are not identical across languages. This creates a diverse set of candidate answers where the correct answer is more likely to be present among the multilingual outputs. The key insight is that this diversity provides an upper bound on performance that exceeds what's achievable through English-only reasoning with multiple seeds.

## Foundational Learning

- **Concept: Acc@k vs. Majority Voting**
  - Why needed here: These metrics quantify the potential (Acc@k) versus realized performance (Voting) of multilingual aggregation. The paper's core finding is the large gap between them.
  - Quick check question: If 17 answers are generated and only 1 is correct, what is Acc@17? What would majority voting likely return?

- **Concept: LLM-as-a-Judge Bias**
  - Why needed here: A key result is that using an LLM to select the best answer introduces a bias toward high-resource languages, failing to select correct answers in lower-resource languages.
  - Quick check question: Why would a judge LLM prefer a well-articulated but incorrect English answer over a poorly phrased but correct answer in Swahili?

- **Concept: Multilingual Error Independence**
  - Why needed here: The paper hypothesizes that languages "compensate for each other's errors." This requires that the model's failures are not identical across languages.
  - Quick check question: If a model fails a math problem due to an inherent lack of knowledge, would translating the question to Spanish likely fix the error? Why or why not?

## Architecture Onboarding

- **Component map**: Translation Engine -> Candidate Generator -> Answer Selector
- **Critical path**: The main blocker is the Answer Selector. The Generator reliably produces diverse, high-potential candidates, but the Selector cannot identify the best one.
- **Design tradeoffs**:
  - Translation Quality: Machine translation provides no significant gain over human translation, so machine translation is preferred for cost and scalability
  - Language Set Size: A small, diverse set (~4 languages) provides most of the gain. Scaling to 17 languages increases cost linearly but only marginally improves the upper bound
  - Selection Method: Majority voting is cheap but fails because the correct answer is often not the majority. A sophisticated verifier is needed but does not yet exist
- **Failure signatures**:
  - Collapsed Performance: If Vote@k and Acc@k scores are similar, the multilingual advantage is not being leveraged
  - Language Bias: If the final selected answer is overwhelmingly from one language regardless of input, the Selector is biased
  - Diminishing Returns: If adding languages beyond 3-4 does not increase Acc@k, the added languages are too similar to existing ones
- **First 3 experiments**:
  1. Upper Bound Probe: For your target model and dataset, measure Acc@k and Vote@k for k=4 languages. The gap between these two numbers is the optimization target
  2. Translation Quality Ablation: Repeat the probe using both human and machine-translated prompts for the same languages
  3. Selection Bias Audit: Run the LLM-as-a-judge selection on a set of candidates where you know the correct answer's language

## Open Questions the Paper Calls Out

- **Open Question 1**: How can a stable answer selection method be developed to effectively close the gap between the theoretical upper bound of multilingual reasoning (Acc@k) and practical accuracy (Vote@k)?
  - Basis in paper: The authors conclude that common selection strategies fail to realize theoretical gains, noting that a stable method remains "elusive"
  - Why unresolved: The paper demonstrates that majority voting is sensitive to language combination and introduces noise, while LLM judges exhibit language bias rather than focusing on correctness
  - What evidence would resolve it: A novel selection algorithm that consistently achieves accuracy comparable to the multilingual Acc@k upper bound

- **Open Question 2**: Do the high upper bounds of multilingual reasoning and the associated selection challenges persist in smaller language models (under 70 billion parameters)?
  - Basis in paper: The authors explicitly state in the Limitations section that the focus on large models (>70B) may not represent the capabilities or challenges in smaller architectures
  - Why unresolved: The experimental scope is restricted to three specific large models
  - What evidence would resolve it: Evaluation of the Multilingual, Repeat, and Paraphrase settings on smaller parameter models

- **Open Question 3**: Can the optimal "advantage language" for a specific query be predicted based on question difficulty or semantic structure prior to inference?
  - Basis in paper: Section 5.1 notes that "language correctness correlates with question difficulty" and that specific languages serve as "key advantage languages"
  - Why unresolved: While the authors identify that different languages perform better on different difficulty levels, they do not provide a method to predict the best language for a specific input a priori
  - What evidence would resolve it: A classifier or routing mechanism that predicts the best language for a query and achieves higher accuracy than random language selection

## Limitations
- The study focuses exclusively on models with >70B parameters, limiting generalizability to smaller architectures
- The practical utility of multilingual reasoning is demonstrated through theoretical upper bounds but not realized due to selection method limitations
- The experiments are limited to two specific reasoning tasks (GPQA and MGSM), which may not generalize to all reasoning domains

## Confidence
- **High Confidence**: The Acc@k metric as an upper bound and the finding that machine translation is sufficient are well-supported by experiments
- **Medium Confidence**: The claim that Vote@k and LLM-as-a-judge methods fail to achieve the upper bound is demonstrated, but the reasons are inferred rather than rigorously tested
- **Low Confidence**: The practical utility of multilingual reasoning for end-users is not quantified - the paper shows potential but not how to realize it

## Next Checks
1. Error Correlation Analysis: For a fixed set of languages, measure the correlation of errors across languages for the same question to validate the error independence hypothesis
2. Judge Calibration: Run the LLM-as-a-judge selection on a dataset where the correct answer's language is known to isolate language bias from reasoning ability
3. Cost-Effectiveness Benchmark: For the same computational budget (e.g., 4x inference), compare the performance of 1 model (×4 seeds) versus 4 languages (×1 seed each) on a held-out test set