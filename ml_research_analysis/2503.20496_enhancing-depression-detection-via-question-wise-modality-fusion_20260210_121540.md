---
ver: rpa2
title: Enhancing Depression Detection via Question-wise Modality Fusion
arxiv_id: '2503.20496'
source_url: https://arxiv.org/abs/2503.20496
tags:
- questmf
- depression
- text
- modality
- audio
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses the challenge of improving depression severity
  detection using multimodal data (text, audio, video) by proposing a novel framework
  that predicts scores for each individual question in the PHQ-8 questionnaire, rather
  than just a total score. This "Question-wise Modality Fusion" (QuestMF) approach
  uses separate fusion models for each question to capture the variable contribution
  of each modality depending on the question, along with a new "Imbalanced Ordinal
  Log-Loss" (ImbOLL) function designed for imbalanced ordinal classification.
---

# Enhancing Depression Detection via Question-wise Modality Fusion

## Quick Facts
- arXiv ID: 2503.20496
- Source URL: https://arxiv.org/abs/2503.20496
- Reference count: 40
- Primary result: Question-wise modality fusion with ImbOLL loss achieves CCC of 0.685 on E-DAIC dataset

## Executive Summary
This study introduces QuestMF, a framework for depression severity detection that predicts individual PHQ-8 question scores rather than just total scores. The approach uses separate fusion models for each question, capturing the variable contribution of text, audio, and video modalities depending on the specific symptom being assessed. A novel Imbalanced Ordinal Log-Loss (ImbOLL) function is proposed to handle the ordinal nature and class imbalance of PHQ-8 scores. Results show the framework matches state-of-the-art performance while providing better interpretability for clinical applications.

## Method Summary
QuestMF predicts scores for each of 8 PHQ-8 questions separately using turn-level multimodal features. Each question has its own fusion model that combines text, audio, and video representations through cross-attention mechanisms. Single-modality encoders use BiLSTM and self-attention to process turn-level features. The Imbalanced Ordinal Log-Loss (ImbOLL) function addresses ordinal classification with class imbalance. Models are trained separately: first single-modality encoders, then fusion modules with ImbOLL loss. The total PHQ-8 score is obtained by summing individual question predictions.

## Key Results
- Achieves CCC of 0.685 on E-DAIC test set, matching state-of-the-art performance
- Question-wise approach improves interpretability by identifying specific symptoms for each individual
- Text modality performs best for symptom questions 1-5, while audio-visual modalities are more important for questions 6-8
- Model shows consistent performance across validation folds with CCC ranging from 0.610 to 0.730

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Question-specific fusion models capture differential modality relevance per PHQ-8 item.
- **Mechanism:** Rather than a single fusion module predicting total PHQ-8 score, QuestMF trains 8 separate fusion models—each learns which modalities matter most for that specific symptom (e.g., audio-visual cues for fidgetiness vs. text for appetite questions).
- **Core assumption:** Depression symptoms manifest differently across modalities; a one-size-fits-all fusion obscures question-specific signal.
- **Evidence anchors:**
  - [abstract] "variable contribution of each modality for each question in the questionnaire"
  - [section 1] "a question on being fidgety may require more attention to audio-visual modalities. On the other hand, text transcripts may contribute more significantly to a question regarding a person's appetite"
  - [corpus] Limited direct validation; CAF-Mamba (arXiv 2601.21648) explores cross-modal attention but not question-wise decomposition.
- **Break condition:** If modality contributions are uniform across questions, the added complexity yields no gain over single-fusion baselines.

### Mechanism 2
- **Claim:** Ordinal classification with ImbOLL improves learning on skewed PHQ-8 score distributions.
- **Mechanism:** PHQ-8 item scores (0–3) are ordered but imbalanced (few patients score 2–3). ImbOLL modifies the ordinal log-loss by weighting rare classes more heavily, penalizing misclassifications proportionally to both class distance and class rarity.
- **Core assumption:** Treating scores as continuous (regression) loses ordinal structure; standard classification ignores both order and imbalance.
- **Evidence anchors:**
  - [abstract] "Imbalanced Ordinal Log-Loss (ImbOLL) function designed for imbalanced ordinal classification"
  - [section 4.3] Equation 4 defines L_ImbOLL with distance term α and inverse-frequency weight β
  - [corpus] No corpus papers evaluate ordinal loss functions for depression; standard practice remains regression or vanilla classification.
- **Break condition:** If the dataset were balanced, ImbOLL would reduce to standard OLL with marginal benefit.

### Mechanism 3
- **Claim:** Turn-level aggregation with cross-attention fusion captures temporal multimodal interactions better than frame- or session-level approaches.
- **Mechanism:** Each modality encoder pools features per dialogue turn (not per word or entire session), then BiLSTM and self-attention encode turn sequences. Cross-attention layers exchange information across modalities before final MLP classification.
- **Core assumption:** Turns provide the right granularity—coarser than frames (which cut words), finer than whole sessions (which lose structure).
- **Evidence anchors:**
  - [section 2.2] "fusion at the frame level, however, can cut words... fusion at the topic level is too course-grained. Thus, we use turn-level fusion"
  - [section 4.1] architecture diagram and description of turn-based encoding
  - [corpus] MMFformer (arXiv 2508.06701) uses transformer-based multimodal fusion but operates at different granularity; no direct comparison to turn-level.
- **Break condition:** If interview turns are poorly segmented or highly variable in length, turn-level representations become noisy.

## Foundational Learning

- **Concept: Ordinal classification vs. regression**
  - Why needed here: PHQ-8 scores are ordered categories, not continuous values; predicting "1.5" is meaningless clinically.
  - Quick check question: Can you explain why predicting a score of 1.7 on a 0–3 scale is problematic for symptom interpretation?

- **Concept: Cross-attention in multimodal fusion**
  - Why needed here: The fusion module uses modality A as query and modality B as key/value to learn how one modality should attend to another.
  - Quick check question: In the notation "T → A," which modality provides the query and which provides the key/value?

- **Concept: Class imbalance handling via inverse-frequency weighting**
  - Why needed here: High PHQ-8 scores are rare; without weighting, the model optimizes for majority class (0–1) and ignores minorities (2–3).
  - Quick check question: If class y appears 10 times in 1000 samples, what is the weight w(y) according to Equation 3?

## Architecture Onboarding

- **Component map:**
  - Text/Audio/Video encoders: Turn encoder → Mean pooling → BiLSTM (hidden=50) → Self-attention (4 heads) → Flatten → MLP (256 hidden, ReLU)
  - Fusion module: Cross-attention layers (6 combinations for 3 modalities) → Self-attention per modality → Concat → Flatten → MLP
  - Loss function: ImbOLL (α=1, β=0.5) applied per-question; scores summed for total PHQ-8

- **Critical path:**
  1. Train single-modality encoders first (freeze turn encoders, train BiLSTM/attention/MLP)
  2. Load pre-trained encoders, train fusion modules with ImbOLL
  3. For fusion involving text, freeze text encoder weights during fusion training

- **Design tradeoffs:**
  - Higher dropout (0.8) in fusion layers vs. overfitting risk on 163 training sessions
  - Question-wise models (8× parameters) vs. interpretability and symptom-specific accuracy
  - BiLSTM over transformers for audio for computational efficiency on short turn sequences

- **Failure signatures:**
  - CCC near 0 or negative on validation → single-modality encoders may not have converged; check per-modality CCC
  - All predictions concentrated in classes 0–1 → ImbOLL β too low or class imbalance not addressed
  - Video model outputs constant scores (as noted for Q8) → insufficient visual features or collapsed attention

- **First 3 experiments:**
  1. Baseline replication: Train "Total" framework (MSE, single fusion) to establish benchmark CCC/RMSE on E-DAIC split
  2. Ablation on loss functions: Compare QuestMF with MSE vs. OLL vs. ImbOLL on validation CCC to isolate loss contribution
  3. Modality contribution audit: For each question, run Text-only, Text+Audio, Text+Video, and full fusion; identify which modalities matter per question (per Figure 5)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the QuestMF framework maintain its performance and generalizability when applied to larger, more diverse clinical datasets?
- **Basis in paper:** [explicit] The authors explicitly state that the small size of the E-DAIC dataset (163 training sessions) leads to potential overfitting and that "results and analysis obtained in this work need to be verified with bigger and more diverse datasets in the future."
- **Why unresolved:** Privacy concerns currently restrict the availability of large-scale multimodal datasets for mental health, forcing reliance on small samples.
- **What evidence would resolve it:** Replicating the study on a dataset with thousands of participants across various demographic groups to ensure results are not biased by the specific E-DAIC cohort.

### Open Question 2
- **Question:** Is QuestMF effective for tracking symptom changes in longitudinal therapy data involving multiple sessions?
- **Basis in paper:** [explicit] The authors note that E-DAIC contains only initial interviews and "does not contain real therapy session interviews," meaning they "cannot test the effectiveness of our model in such real world situations."
- **Why unresolved:** Depression tracking requires observing temporal dynamics across sessions, which is impossible with single-intake datasets like E-DAIC.
- **What evidence would resolve it:** Applying the framework to a longitudinal corpus where model predictions are correlated with symptom changes across multiple therapy visits.

### Open Question 3
- **Question:** Would a dynamic architecture that selects specific modalities per question outperform the current fixed fusion approach?
- **Basis in paper:** [inferred] The authors analyze modality importance per question (e.g., Text best for Q1) but refrain from building a framework that filters modalities, citing the need for more data to generalize this analysis.
- **Why unresolved:** It is unclear if the observed modality importance is a statistical artifact of the small validation set or a robust pattern.
- **What evidence would resolve it:** Implementing a "question-specific modality selection" mechanism and evaluating whether it significantly improves Concordance Correlation Coefficient (CCC) scores over the standard fusion approach.

## Limitations

- Requires 8 separate fusion models, significantly increasing computational complexity compared to single-fusion baselines
- Small dataset size (163 training sessions) raises concerns about overfitting despite dropout regularization
- Paper lacks ablation studies showing individual contributions of question-wise modeling versus the novel ImbOLL loss function

## Confidence

- **High confidence** in the mechanism of ordinal classification benefits for PHQ-8 scoring (Mechanism 2)
- **Medium confidence** in the question-wise fusion approach providing meaningful improvements over single-fusion models
- **Medium confidence** in the turn-level aggregation being optimal compared to other temporal granularities

## Next Checks

1. **Ablation study**: Compare QuestMF with ImbOLL against QuestMF with standard ordinal loss and MSE loss to isolate the contribution of each innovation
2. **Generalization test**: Evaluate on additional depression datasets (e.g., DAIC-WOZ) to assess robustness beyond E-DAIC
3. **Clinical utility validation**: Conduct user study with clinicians to verify whether question-wise symptom predictions actually improve intervention planning compared to total score predictions