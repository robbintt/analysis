---
ver: rpa2
title: Circuit Representation Learning with Masked Gate Modeling and Verilog-AIG Alignment
arxiv_id: '2502.12732'
source_url: https://arxiv.org/abs/2502.12732
tags:
- masked
- circuit
- mgvga
- logic
- representation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of learning effective circuit
  representations that capture both structural details and functional properties for
  electronic design automation tasks. Traditional masked graph modeling approaches
  fail for circuits due to logical equivalence constraints, while graph neural networks
  struggle to capture abstract circuit functions.
---

# Circuit Representation Learning with Masked Gate Modeling and Verilog-AIG Alignment

## Quick Facts
- **arXiv ID**: 2502.12732
- **Source URL**: https://arxiv.org/abs/2502.12732
- **Reference count**: 19
- **Primary result**: MGVGA significantly outperforms state-of-the-art methods on quality of result prediction (NDCG@3 improving from 0.334 to 0.540) and logic equivalence identification (F1-score improving from 0.424 to 0.470) across multiple circuit benchmarks.

## Executive Summary
This paper addresses the challenge of learning effective circuit representations that capture both structural details and functional properties for electronic design automation tasks. Traditional masked graph modeling approaches fail for circuits due to logical equivalence constraints, while graph neural networks struggle to capture abstract circuit functions. The authors propose MGVGA, a constrained masked modeling paradigm that combines masked gate modeling (MGM) in latent space with Verilog-AIG alignment (VGA) using large language models. MGM preserves logical equivalence by reconstructing masked gates using latent representations as constraints, while VGA leverages LLM understanding of Verilog code to guide function learning. Experiments show MGVGA significantly outperforms state-of-the-art methods on quality of result prediction and logic equivalence identification across multiple circuit benchmarks.

## Method Summary
MGVGA is a self-supervised learning framework for circuit representation that combines masked gate modeling (MGM) with Verilog-AIG alignment (VGA). The method uses And-Inverter Graphs (AIGs) as input, applying a DeepGCN encoder-decoder architecture. MGM masks gates in latent space rather than in the original circuit, reconstructing masked gate attributes (type and degree) to preserve logical equivalence. VGA performs cross-attention alignment between masked AIG representations and LLM-extracted Verilog embeddings, enabling the model to learn functional semantics. The model is trained jointly on gate type prediction and gate-level degree prediction, using 810,000 AIGs and 64,826 Verilog-AIG pairs. Evaluation is performed on downstream tasks including quality of result prediction and logic equivalence identification across 10 held-out circuit designs.

## Key Results
- MGVGA achieves NDCG@3 of 0.540 on quality of result prediction, significantly outperforming state-of-the-art (0.334) and VGAE (0.287).
- For logic equivalence identification, MGVGA achieves F1-score of 0.470, surpassing state-of-the-art (0.424) and VGAE (0.387).
- The VGA component provides critical functional supervision, with ablation showing F1-score drops from 0.470 to 0.433 when removed.
- Optimal masking ratios are 0.3 for MGM and 0.5 for VGA, with higher ratios degrading performance.

## Why This Works (Mechanism)

### Mechanism 1
Masking gates in latent space rather than in the original circuit preserves logical equivalence during self-supervised reconstruction. The GNN encoder first processes the complete AIG to produce latent representations X where logical structure is already embedded. Masking is then applied to X, not the input graph. Unmasked gate representations serve as soft constraints during decoding because they have already aggregated features from all gates through message passing.

### Mechanism 2
Cross-attention alignment between masked AIG representations and LLM-extracted Verilog embeddings transfers functional knowledge from language models to graph neural networks. A bidirectional LLM encodes Verilog code into embeddings XV. The masked AIG representation queries this Verilog embedding space via cross-attention, forcing the GNN to reconstruct masked gates under functional constraints from the LLM's understanding.

### Mechanism 3
Joint training on gate type prediction and gate-level degree prediction forces the model to learn both semantic gate functions and structural connectivity patterns. Two reconstruction heads operate on decoded representations: a classification head predicting AND/NOT/PI/PO types via cross-entropy, and a regression head predicting in-degree and out-degree via MSE.

## Foundational Learning

- **Concept: And-Inverter Graphs (AIGs)**
  - Why needed here: AIGs are the canonical representation used throughout MGVGA. Understanding that circuits decompose into AND gates, NOT gates, and primary inputs/outputs is essential for grasping both the masking and reconstruction logic.
  - Quick check question: Given a circuit with 3 inputs and 2 outputs, what is the minimum set of node types in its AIG representation?

- **Concept: Masked Autoencoders (MAE/BEiT paradigm)**
  - Why needed here: MGVGA adapts the masked modeling paradigm from vision/language to circuits. Without understanding how masking forces representation learning through reconstruction, the motivation for MGM's latent-space innovation is unclear.
  - Quick check question: Why does random masking in BERT or MAE work, but random masking of circuit gates fails to preserve logical equivalence?

- **Concept: Logical Equivalence vs. Logical Correctness**
  - Why needed here: The paper's core insight hinges on this distinction. A reconstructed circuit can be logically correct (produces valid outputs) but not equivalent to the original (produces different outputs for same inputs). This is why traditional masking fails.
  - Quick check question: If you mask a gate and replace it with a functionally valid but different gate, is the result logically equivalent to the original?

## Architecture Onboarding

- **Component map**: Input AIG → GNN Encoder (DeepGCN, 7 layers) → MGM path: Mask in latent space → Decoder → [Type Head, Degree Head] → VGA path: Mask in input → Encoder → Cross-Attention(X_masked, X_verilog) → Decoder → [Type Head, Degree Head] → Verilog Code → Bidirectional LLM (gte-Qwen2-7B) → Adaptive Pooling → X_verilog

- **Critical path**: The VGA cross-attention block is the key integration point. The masked AIG representation must successfully query the Verilog embedding space. If this alignment fails, the model reverts to purely structural learning without functional grounding.

- **Design tradeoffs**: Masking ratios optimal at 0.3 for MGM and 0.5 for VGA. VGA works with higher masking because Verilog provides strong functional constraints; MGM fails with higher masking because no external supervision exists. The paper deliberately does NOT mask edges, reducing complexity from O(N²) to O(N) but potentially missing higher-order structural patterns.

- **Failure signatures**: NDCG@3 < 0.35 on QoR prediction suggests MGM is not learning structural features (check masking ratio, encoder depth). F1-score < 0.40 on logic equivalence identification suggests VGA alignment is weak (verify Verilog-AIG pair quality, LLM embedding extraction).

- **First 3 experiments**: 1) Reproduce ablation: Train MGM-only (no VGA) and verify NDCG@3 ≈ 0.34 and F1 ≈ 0.43 match Table 4 before debugging full system. 2) Masking ratio sweep: For your target circuit sizes, validate optimal ratios (0.3/0.5) hold or identify domain-specific optima using the Grid in Table 3 as starting points. 3) LLM embedding sanity check: Extract Verilog embeddings for a few circuits and verify they cluster by functional similarity (e.g., arithmetic circuits near each other) before training VGA.

## Open Questions the Paper Calls Out

### Open Question 1
**Question**: Can the MGVGA framework be extended to reconstruct the adjacency matrix to capture global topological features without incurring the prohibitive $O(N^2)$ computational complexity that the authors currently avoid?
**Basis in paper**: [Explicit] The authors state in Appendix A.1.2 that they do not mask or reconstruct the adjacency matrix $A$ because doing so is "computationally infeasible for large circuits" and makes the problem "NP-hard due to multiple possible solutions."
**Why unresolved**: The current method relies on degree prediction to capture connectivity implicitly. While efficient, it may miss higher-order structural patterns inherent in the full adjacency matrix, limiting the model's understanding of global circuit topology.
**What evidence would resolve it**: A modification of the decoder that approximates adjacency reconstruction with linear or log-linear complexity, demonstrating improved performance on tasks requiring global structural reasoning without increasing training time exponentially.

### Open Question 2
**Question**: How can the inference overhead of MGVGA be minimized to ensure net positive speedups in SAT solving for simpler subcircuits or "easier cases"?
**Basis in paper**: [Explicit] The authors note in Appendix A.3.2 that while MGVGA accelerates complex SAT solving, "the SAT solver with GNN is less effective for easier cases, as the model inference process accounts for a significant portion of the total runtime."
**Why unresolved**: The computational cost of running the GNN encoder and constraint blocks (cross-attention) may outweigh the time saved by reducing the SAT search space for small problems, limiting the method's applicability to a broad range of circuit complexities.
**What evidence would resolve it**: A latency analysis showing that a distilled or quantized version of MGVGA provides a net reduction in total runtime compared to baseline solvers across the full distribution of subcircuit complexities, not just the hard instances.

### Open Question 3
**Question**: Is the Verilog-AIG Alignment (VGA) performance robust to noisy or hallucinated functional embeddings from the Large Language Model teacher?
**Basis in paper**: [Inferred] The paper assumes the LLM (`gte-Qwen2-7B-instruct`) has an "excellent understanding" and "comprehensive understanding" of Verilog. However, if the LLM's embedding $X_V$ is inaccurate for complex or non-standard code, the cross-attention constraint in Equation 5 could enforce incorrect functional alignments.
**Why unresolved**: The methodology relies entirely on the LLM's output as ground truth for functional constraints. There is no mechanism described to detect or correct for LLM hallucinations or misunderstandings of the Verilog logic.
**What evidence would resolve it**: An ablation study evaluating the downstream task performance (e.g., Logic Equivalence Identification) when the LLM embeddings are intentionally perturbed or when using a weaker/smaller LLM, compared to the current state-of-the-art performance.

## Limitations
- The VGA approach depends critically on high-quality Verilog-AIG pairs, but the paper doesn't thoroughly characterize the coverage or quality distribution of these pairs across circuit families.
- The evaluation focuses on specific EDA tasks (QoR prediction and logic equivalence) without exploring broader circuit analysis applications.
- The computational cost of running the GNN encoder and constraint blocks (cross-attention) may outweigh the time saved for simpler circuits.

## Confidence
- **High confidence**: The core mechanism of masking in latent space versus input space (MGM) - this is well-supported by both theoretical reasoning and ablation studies.
- **Medium confidence**: The VGA cross-attention alignment effectiveness - while ablation shows clear benefit, the dependency on specific LLM quality and Verilog-AIG pair availability introduces variability.
- **Medium confidence**: The superiority of joint gate type and degree prediction over adjacency reconstruction - reasonable but lacks extensive ablation on alternative structural objectives.

## Next Checks
1. **Architecture fidelity verification**: Implement and train the DeepGCN encoder-decoder with only MGM (no VGA) to verify the reported baseline performance (NDCG@3 ≈ 0.34, F1 ≈ 0.43) matches Table 4 before proceeding to full MGVGA implementation.

2. **Masking ratio sensitivity analysis**: Conduct a systematic sweep of masking ratios (0.1 to 0.7) for both MGM and VGA on a representative subset of circuits to validate whether the optimal ratios (0.3/0.5) generalize across different circuit sizes and types.

3. **LLM embedding quality assessment**: Extract and analyze the Verilog embeddings from the gte-Qwen2-7B-instruct model to verify that functionally similar circuits (e.g., different adder implementations) produce nearby embeddings in the embedding space before using them for VGA training.