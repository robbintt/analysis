---
ver: rpa2
title: 'What I cannot execute, I do not understand: Training and Evaluating LLMs on
  Program Execution Traces'
arxiv_id: '2503.05703'
source_url: https://arxiv.org/abs/2503.05703
tags:
- scratchpad
- output
- steps
- prediction
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Execution Tuning (E.T.), a training approach
  that explicitly models real-world program execution traces to improve code reasoning
  in large language models (LLMs). Unlike traditional code datasets that treat code
  as static text, E.T.
---

# What I cannot execute, I do not understand: Training and Evaluating LLMs on Program Execution Traces

## Quick Facts
- arXiv ID: 2503.05703
- Source URL: https://arxiv.org/abs/2503.05703
- Reference count: 15
- Primary result: Execution Tuning achieves ~80% accuracy on CruxEval and MBPP nested loops by training LLMs on execution traces at three granularities.

## Executive Summary
This paper introduces Execution Tuning (E.T.), a training approach that explicitly models real-world program execution traces to improve code reasoning in large language models (LLMs). Unlike traditional code datasets that treat code as static text, E.T. leverages dynamic execution information through three trace granularities: direct output prediction, line-level, and instruction-level. The method introduces dynamic scratchpads—self-contained state representations updated at each step—which excel at long executions (up to 14,000 steps) by reducing the number of intermediate predictions needed. Models trained with E.T. achieve ~80% accuracy on CruxEval and MBPP, significantly outperforming direct output prediction baselines.

## Method Summary
Execution Tuning trains LLMs on execution traces rather than static code. The approach generates ~300k Python functions with ~1.5M executions using LLM-generated unit tests and fuzzing. A custom tracer captures program state at three granularities (direct output, line-level, instruction-level bytecode) while recording locals, globals, stack, and iterator states. Three scratchpad variants represent this state: regular (full state per line), compact (diff of changed variables), and dynamic (single updated state supporting step-skipping). The method fine-tunes Llama 3.1 8B Instruct for 7.5k steps with batch size 1024 and max sequence length 8192 tokens. At inference, models predict intermediate states iteratively until function return, with confidence-calibrated step-skipping (Line-n/Instruction-n) enabling shorter execution paths.

## Key Results
- ~80% accuracy on CruxEval and MBPP nested loops with trace modeling vs. ~49% with direct output prediction
- Dynamic scratchpads enable accurate prediction on long executions (up to 14,000 steps) by reducing intermediate predictions
- Line-n step-skipping reduces required steps from 14k to 1.5k for Collatz conjecture while maintaining accuracy
- Instruction-level granularity achieves 98.8% state accuracy vs. 96.3% for line-level, but requires 7x more steps
- NLL-based confidence calibration enables inference-time tradeoffs between step count and accuracy

## Why This Works (Mechanism)

### Mechanism 1: Intermediate State Materialization
- Claim: Explicitly predicting intermediate execution states improves final output prediction accuracy compared to direct output prediction.
- Mechanism: Training the model to emit intermediate variable states as a "scratchpad" creates verifiable reasoning steps that reduce compounding errors. Each state prediction becomes a checkpoint the model can self-correct against.
- Core assumption: The model learns to simulate a virtual machine state transition function, mapping (current_state, code) → next_state.
- Evidence anchors:
  - [abstract] "obtaining ∼80% accuracy on CruxEval and MBPP" with trace modeling vs. direct prediction
  - [section 3.2, Table 2] Direct output prediction achieves 49.3% on CruxEval; Scratchpad F.T. achieves 78.7%
  - [corpus] Nye et al. (2021) scratchpad work cited as foundational; no direct corpus papers replicate this specific result
- Break condition: Fails when intermediate states themselves require complex reasoning (e.g., string indexing where tokenization creates ambiguity)—errors compound rather than self-correct.

### Mechanism 2: Dynamic Scratchpad State Compression
- Claim: Maintaining a single self-contained state representation (vs. accumulating history) enables accurate prediction on very long executions (>1000 steps).
- Mechanism: Rather than storing all prior states, the model learns to update one canonical state dictionary. This eliminates context window overflow and forces the model to encode only information necessary for future predictions.
- Core assumption: The model can learn to discard information that doesn't affect future control flow or final outputs.
- Evidence anchors:
  - [abstract] "dynamic scratchpads...excel at long executions (up to 14,000 steps) by reducing the number of intermediate predictions needed"
  - [section 3.4, Table 6] Line-1 correctly predicts Collatz(3038) requiring 619 chained predictions; Line-n reduces to 106.1 average steps
  - [corpus] Weak direct evidence; corpus papers focus on different compression/execution approaches
- Break condition: Fails when state information implicitly encoded in execution history is needed but not explicitly represented (e.g., iterator disambiguation in nested loops—paper notes this requires explicit iteration count encoding).

### Mechanism 3: Confidence-Calibrated Multi-Step Prediction
- Claim: Training models to predict N steps ahead (N∈{1..10}) enables inference-time tradeoffs between step count and accuracy, with negative log-likelihood (NLL) serving as a confidence signal.
- Mechanism: The model learns to predict state after N lines/instructions. At inference, the model evaluates multiple N values and selects based on confidence. Sometimes N>1 is more confident than N=1 because the model "sees" simpler intermediate paths.
- Core assumption: Model confidence (NLL) correlates with prediction correctness across different step sizes.
- Evidence anchors:
  - [section 3.2] "n = 1 is not always the prediction in which the model is most confident"
  - [section 3.1, Figure 4] Shows calibration: accuracy decreases and NLL increases as N increases
  - [corpus] No corpus papers directly test NLL-based step selection; this appears novel to E.T.
- Break condition: Calibration breaks down on longer executions (Binary Counter with Line-n drops to 1/9 accuracy vs. 8/9 for Line-1), suggesting NLL is unreliable for complex control flow.

## Foundational Learning

- **Concept: Python bytecode and stack-based execution**
  - Why needed here: Instruction-level granularity requires understanding opcodes like `LOAD_FAST`, `CALL_METHOD`, and how the operand stack operates.
  - Quick check question: Given `x = a + b`, can you list the bytecode instructions that execute this statement?

- **Concept: Control flow vs. data flow in program execution**
  - Why needed here: The paper explicitly separates control flow accuracy (predicting next line/instruction) from variable state accuracy—these fail independently.
  - Quick check question: In a loop with no variable changes, is control flow prediction sufficient for correct output prediction?

- **Concept: Scratchpad reasoning in transformers**
  - Why needed here: The approach builds on Nye et al.'s scratchpad technique; understanding why intermediate token generation helps is prerequisite.
  - Quick check question: Why does generating intermediate steps in the output sequence help a transformer, given it has no hidden state?

## Architecture Onboarding

- **Component map:**
  Python function collection → Input generation (LLM + fuzzing) → Custom tracer (sys.settrace) → Trace representation layer → Fine-tuning (Llama 3.1 8B) → Inference (iterative state prediction)

- **Critical path:** The tracer is the bottleneck. It must correctly handle: (1) stepping into user-defined aux functions but not library calls, (2) tracking iterator state (requires stack inspection), (3) filtering C-level events. Paper notes an AST transformation was needed for MBPP nested for-loops.

- **Design tradeoffs:**
  - Line-level vs. Instruction-level: Line-level is more interpretable; instruction-level achieves higher full-state accuracy (98.8% vs. 96.3%) but requires 7x more steps.
  - Compact vs. Dynamic scratchpad: Compact wins on short executions (79.7% vs. 73.3% on CruxEval); Dynamic wins on long executions.
  - Step-skipping (Line-n) vs. single-step (Line-1): Skipping reduces steps by ~65% but loses accuracy unless combined with search.

- **Failure signatures:**
  - String indexing errors: Tokenization makes character counting inconsistent across iterations with dynamic scratchpad (model must recount from scratch each step).
  - Built-in method outputs: Models fail on methods like `.istitle()` where outputs are memorized rather than computed.
  - Nested iterator ambiguity: Requires explicit iteration count encoding; without it, `for c in ('a','b','a')` cannot distinguish first 'a' from second.

- **First 3 experiments:**
  1. **Reproduce CruxEval baseline:** Fine-tune Llama 3.1 8B instruct on direct output prediction only. Target: ~49% accuracy. This validates your training pipeline.
  2. **Ablate scratchpad type on a held-out subset:** Compare Scratchpad vs. Compact Scratchpad vs. Dynamic (Line-1) on 100 CruxEval examples. Expect Compact ≈80%, Dynamic ≈73%. Confirms representation choices.
  3. **Long-execution stress test:** Run Collatz(n=3038) with Line-1. If you get correct output in ~619 steps, your state-chaining is working. If it fails before 100 steps, debug state accumulation or control flow prediction first.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can Execution Tuning (E.T.) be successfully extended to low-level languages like C, specifically to model complex memory phenomena such as pointer aliasing? The conclusion suggests extending to C for understanding pointer aliasing, but the current study focuses exclusively on Python which abstracts away memory management.

- **Open Question 2:** Does training on execution traces that include exceptions and runtime errors improve a model's robustness and debugging capabilities? The paper suggests looking at more challenging datasets with exceptions, but the current data pipeline discards inputs yielding runtime errors, limiting the model to "happy path" executions only.

- **Open Question 3:** Can advanced search strategies or better confidence calibration close the performance gap between the "Line-n" heuristic and the optimal Dijkstra upper bound in long executions? While the model's individual predictions are accurate enough to solve the task, the mechanism for selecting which prediction step to take (based on Negative Log-Likelihood) fails to find the correct path in complex cases like the Binary Counter.

## Limitations

- The evaluation relies on synthetic datasets (CruxEval, MBPP nested loops) rather than real-world programming tasks, leaving unclear whether improvements transfer to practical code generation.
- The method requires execution traces that capture complete program state, which may not be feasible for complex, interactive, or stateful applications.
- While E.T. improves reasoning for specific algorithmic patterns, downstream effects on standard coding benchmarks are modest, suggesting the approach may be more valuable for specialized reasoning tasks than general-purpose coding.

## Confidence

- **High confidence**: The core experimental results on CruxEval and MBPP nested loops are well-supported by the data, with clear baselines and controlled ablations. The ~80% accuracy improvements are robust across multiple scratchpad variants and granularities.
- **Medium confidence**: The claims about dynamic scratchpad superiority on long executions are supported by the Collatz and binary counter experiments, but the evaluation is limited to synthetic tasks. The mechanism by which dynamic state compression enables 14k-step predictions is plausible but not fully explained.
- **Low confidence**: The assertion that E.T. improves multi-step reasoning capabilities for general programming tasks is weakly supported, as downstream effects on standard benchmarks are modest. The calibration claims for NLL-based step selection are preliminary and break down on complex control flow.

## Next Checks

1. **Downstream benchmark transfer**: Evaluate E.T.-trained models on standard coding benchmarks like HumanEval, CodeContests, and APPS to quantify real-world impact beyond synthetic datasets.

2. **Generalization to different languages and paradigms**: Test whether execution trace training transfers to other programming languages (e.g., Java, C++) and paradigms (e.g., object-oriented, functional) that have different state representation needs.

3. **Robustness to state complexity**: Systematically evaluate performance degradation as state complexity increases (nested data structures, recursion, closures) to identify where the approach breaks down and whether alternative state representations are needed.