---
ver: rpa2
title: A Model of Causal Explanation on Neural Networks for Tabular Data
arxiv_id: '2512.21746'
source_url: https://arxiv.org/abs/2512.21746
tags:
- variables
- causal
- variable
- data
- cennet
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes CENNET, a causal explanation method for neural
  network predictions on tabular data that addresses the problems of pseudo-correlation
  and indirect causal factors in existing explainability methods. CENNET uses structural
  causal models (SCMs) combined with neural networks, analyzing the nearest neighbor
  latent unit (NNLU) to extract characteristic correlated variables (CCVs) that directly
  influence predictions.
---

# A Model of Causal Explanation on Neural Networks for Tabular Data

## Quick Facts
- arXiv ID: 2512.21746
- Source URL: https://arxiv.org/abs/2512.21746
- Reference count: 40
- Primary result: CENNET outperforms existing explainability methods on synthetic and quasi-real tabular datasets while providing causal explanations

## Executive Summary
This paper introduces CENNET, a novel method for causal explanation of neural network predictions on tabular data. The approach addresses fundamental limitations in existing explainability methods by distinguishing between correlated and causally relevant features. CENNET combines structural causal models with neural networks, analyzing nearest neighbor latent units to extract characteristic correlated variables that directly influence predictions. The method provides both global and local explanations through a total explanation power index that quantifies the causal contribution of each feature.

## Method Summary
CENNET operates by first training a neural network on tabular data, then analyzing the latent space representations of predictions. For each prediction, the method identifies the nearest neighbor latent unit (NNLU) and extracts characteristic correlated variables (CCVs) that share similar latent representations. These CCVs are evaluated for their causal relevance through an entropy-based analysis combined with neuron activation weights. The total explanation power (TEP) index is computed as a weighted sum of neuron activations and entropy measures, providing a quantitative assessment of each feature's causal contribution. This approach enables CENNET to provide explanations that go beyond simple correlations to identify features that actually influence model predictions.

## Key Results
- CENNET statistically outperforms LIME, SHAP, and ACV in ranking important variables on synthetic datasets
- The method achieves faster computation times in most settings compared to baseline techniques
- Experimental results demonstrate superior performance particularly for non-additive and combinatorial effects in quasi-real datasets

## Why This Works (Mechanism)
CENNET works by leveraging the structural properties of neural networks in latent space. When features are causally related to predictions, they create distinct patterns in the latent representations that can be identified through nearest neighbor analysis. The method exploits the fact that causally relevant features tend to have similar activation patterns across nearby latent units, while spurious correlations do not maintain this consistency. By focusing on characteristic correlated variables within the NNLU framework, CENNET can distinguish between features that merely correlate with predictions and those that actually drive them through causal pathways.

## Foundational Learning
- **Structural Causal Models (SCMs)**: Formal framework for representing causal relationships; needed to distinguish correlation from causation in feature importance; quick check: verify d-separation holds in synthetic causal graphs
- **Latent Space Analysis**: Understanding how neural networks represent features in hidden layers; crucial for identifying causal patterns beyond input space correlations; quick check: visualize t-SNE embeddings of latent representations
- **Entropy-based Feature Selection**: Information-theoretic measure for quantifying feature relevance; used to assess the informational contribution of CCVs; quick check: compare entropy scores across different feature subsets
- **Nearest Neighbor Search in High Dimensions**: Algorithm for finding similar latent representations; fundamental to identifying characteristic correlated variables; quick check: measure recall@k for known similar instances
- **Neuron Activation Analysis**: Understanding how individual neurons contribute to predictions; essential for computing total explanation power; quick check: visualize activation patterns for different feature combinations
- **Causal vs. Spurious Correlation**: Theoretical distinction between features that cause outcomes versus those that merely correlate; central to CENNET's value proposition; quick check: verify method's performance on confounded vs. unconfounded datasets

## Architecture Onboarding

**Component Map**
Input Features -> Neural Network -> Latent Space -> Nearest Neighbor Search -> NNLU Identification -> CCV Extraction -> Entropy Analysis -> TEP Computation -> Causal Explanations

**Critical Path**
The critical path flows from input features through the neural network to latent space representations, where nearest neighbor search identifies relevant latent units. CCV extraction and entropy analysis then determine which features have causal explanatory power. The TEP computation aggregates these analyses into final explanations.

**Design Tradeoffs**
- **Accuracy vs. Interpretability**: Using complex neural networks provides better predictive performance but requires sophisticated explanation methods
- **Local vs. Global Explanations**: CENNET provides both, but the computational cost increases for global analysis across the entire dataset
- **Causal vs. Correlational Analysis**: The method prioritizes causal explanations but may miss important correlational patterns that are practically useful

**Failure Signatures**
- Poor performance on datasets with high levels of confounding where causal relationships are obscured
- Degraded accuracy when nearest neighbor search fails to find meaningful similar latent representations
- Overfitting to spurious correlations in small or biased training datasets

**First Experiments**
1. Apply CENNET to a synthetic dataset with known causal structure (e.g., chain graph) and verify recovered causal relationships match ground truth
2. Compare TEP scores against established causal inference benchmarks on semi-synthetic datasets
3. Evaluate computational efficiency by measuring explanation generation time across datasets of increasing size and dimensionality

## Open Questions the Paper Calls Out
The paper identifies several open questions regarding the generalizability of CENNET to highly nonlinear, high-dimensional datasets with complex causal structures. It also raises questions about the method's robustness against adversarial attacks that exploit causal pathway assumptions, and how well the approach scales to datasets with thousands of features.

## Limitations
- Reliance on nearest neighbor analysis may introduce bias when training data lacks sufficient diversity to represent all causal relationships
- The assumption that correlated variables in NNLU can be treated as causal factors remains unverified for complex real-world datasets
- The TEP index may oversimplify causal relationships by reducing them to weighted sums of neuron activations and entropy measures

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| CENNET outperforms baseline techniques on synthetic datasets with known ground truth | High |
| Computational efficiency improvements are well-documented | High |
| Performance on quasi-real datasets where ground truth is not fully known | Medium |
| Interpretability of extracted CCVs in practical applications | Medium |
| Generalization to highly nonlinear, high-dimensional datasets | Low |
| Robustness against adversarial attacks exploiting causal assumptions | Low |

## Next Checks
1. Test CENNET on datasets with known causal structures (e.g., TÃ¼bingen cause-effect pairs) to verify the accuracy of extracted causal explanations against ground truth
2. Conduct ablation studies removing the SCM component to quantify its contribution to explanation quality versus standard correlation analysis
3. Evaluate the method's performance when training data contains significant confounding variables to assess its ability to distinguish true causal factors from spurious correlations