---
ver: rpa2
title: 'OmniDrive-R1: Reinforcement-driven Interleaved Multi-modal Chain-of-Thought
  for Trustworthy Vision-Language Autonomous Driving'
arxiv_id: '2512.14044'
source_url: https://arxiv.org/abs/2512.14044
tags:
- reasoning
- arxiv
- driving
- reward
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: OmniDrive-R1 introduces an end-to-end Vision-Language Model framework
  for autonomous driving, addressing the critical challenge of object hallucination
  in safety-critical domains. The model unifies perception and reasoning through an
  interleaved Multi-modal Chain-of-Thought (iMCoT) mechanism, enabling dynamic acquisition
  of fine-grained visual evidence during reasoning without relying on external detection
  tools.
---

# OmniDrive-R1: Reinforcement-driven Interleaved Multi-modal Chain-of-Thought for Trustworthy Vision-Language Autonomous Driving

## Quick Facts
- **arXiv ID:** 2512.14044
- **Source URL:** https://arxiv.org/abs/2512.14044
- **Reference count:** 40
- **Key outcome:** State-of-the-art performance on DriveLMM-o1 benchmark, improving overall reasoning score from 51.77% to 80.35%

## Executive Summary
OmniDrive-R1 introduces an end-to-end Vision-Language Model framework for autonomous driving that addresses the critical challenge of object hallucination in safety-critical domains. The model unifies perception and reasoning through an interleaved Multi-modal Chain-of-Thought (iMCoT) mechanism, enabling dynamic acquisition of fine-grained visual evidence during reasoning without relying on external detection tools. The core innovation is a reinforcement-driven visual grounding capability enabled by the Clip-GRPO algorithm, which uses an annotation-free, process-based grounding reward based on CLIP's cross-modal consistency to ensure alignment between visual focus and textual reasoning.

## Method Summary
The OmniDrive-R1 framework combines Vision-Language Model capabilities with autonomous driving requirements through a novel reinforcement learning approach. The model employs an interleaved Multi-modal Chain-of-Thought (iMCoT) mechanism that dynamically acquires visual evidence during reasoning, eliminating the need for external detection tools. The Clip-GRPO algorithm provides reinforcement learning-based visual grounding using an annotation-free reward derived from CLIP's cross-modal consistency metrics. This approach ensures that visual attention during reasoning remains aligned with textual understanding, addressing the hallucination problem common in autonomous driving applications.

## Key Results
- Achieves 80.35% overall reasoning score on DriveLMM-o1 benchmark (vs. 51.77% baseline)
- Improves final answer accuracy from 37.81% to 73.62% compared to Qwen2.5VL-7B baseline
- Demonstrates competitive zero-shot spatial reasoning performance on SURDS benchmark

## Why This Works (Mechanism)
The model's effectiveness stems from its ability to dynamically acquire visual evidence during reasoning rather than relying on pre-processed detection outputs. The iMCoT mechanism interleaves perception and reasoning steps, allowing the model to refine its understanding iteratively. The Clip-GRPO algorithm provides a novel reward signal based on cross-modal consistency, enabling the model to learn effective visual grounding without requiring manual annotations. This combination addresses the hallucination problem by ensuring that visual evidence directly supports the reasoning process.

## Foundational Learning
- **Vision-Language Model Integration**: Why needed - Autonomous driving requires understanding both visual scenes and natural language instructions; Quick check - Model must process camera inputs and generate driving decisions
- **Reinforcement Learning for Grounding**: Why needed - Traditional supervised learning cannot effectively teach visual attention; Quick check - Model learns to focus on relevant visual regions through reward feedback
- **Cross-modal Consistency**: Why needed - Ensures alignment between visual perception and textual reasoning; Quick check - CLIP-based reward measures agreement between visual focus and textual content
- **Multi-modal Chain-of-Thought**: Why needed - Complex driving decisions require step-by-step reasoning; Quick check - Model can break down complex queries into manageable sub-tasks

## Architecture Onboarding
- **Component Map**: Vision Encoder -> iMCoT Reasoning Engine -> CLIP-based Grounding Reward -> Reinforcement Learning Update -> Vision Encoder
- **Critical Path**: Visual input → Dynamic evidence acquisition → Multi-modal reasoning → Cross-modal consistency reward → Model parameter update
- **Design Tradeoffs**: Annotation-free training vs. potential reward signal noise; Dynamic evidence acquisition vs. computational overhead; End-to-end learning vs. modular system maintainability
- **Failure Signatures**: Visual attention drift from textual context; Over-reliance on certain visual regions; Inconsistent reasoning across similar scenarios
- **First Experiments**: 1) Test iMCoT mechanism with static visual inputs; 2) Evaluate Clip-GRPO reward signal stability; 3) Assess baseline performance without reinforcement learning

## Open Questions the Paper Calls Out
None

## Limitations
- Performance evaluation limited to DriveLMM-o1 benchmark and SURDS dataset, restricting generalizability assessment
- Single baseline comparison (Qwen2.5VL-7B) without broader state-of-the-art benchmarking
- Clip-GRPO algorithm's effectiveness depends on CLIP's cross-modal consistency, which may not generalize across diverse driving scenarios
- No real-world deployment or safety validation beyond controlled benchmark environments

## Confidence
- **High confidence**: iMCoT mechanism's ability to dynamically acquire visual evidence during reasoning
- **Medium confidence**: State-of-the-art performance claims on DriveLMM-o1 benchmark
- **Medium confidence**: Zero-shot spatial reasoning performance on SURDS

## Next Checks
1. Test generalization across multiple autonomous driving benchmarks beyond DriveLMM-o1, including real-world deployment scenarios
2. Compare performance against a broader range of state-of-the-art vision-language models for autonomous driving
3. Conduct ablation studies isolating the contribution of the Clip-GRPO algorithm versus other model components to quantify its specific impact on grounding performance