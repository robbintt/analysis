---
ver: rpa2
title: Verifying Memoryless Sequential Decision-making of Large Language Models
arxiv_id: '2510.06756'
source_url: https://arxiv.org/abs/2510.06756
tags:
- llms
- sequential
- policy
- state
- decision-making
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a method for formally verifying memoryless
  sequential decision-making policies based on large language models (LLMs) against
  safety properties expressed in probabilistic computation tree logic (PCTL). The
  core approach incrementally constructs the reachable portion of the Markov decision
  process (MDP) under the LLM policy, encoding states as natural language prompts,
  parsing the LLM's output into actions, and expanding only the resulting successor
  states.
---

# Verifying Memoryless Sequential Decision-making of Large Language Models

## Quick Facts
- arXiv ID: 2510.06756
- Source URL: https://arxiv.org/abs/2510.06756
- Reference count: 40
- Key outcome: Method for formally verifying memoryless LLM-based sequential decision-making policies against PCTL safety properties using incremental MDP construction and Storm model checking

## Executive Summary
This paper introduces a formal verification approach for memoryless sequential decision-making policies based on large language models (LLMs). The method constructs the reachable portion of the Markov decision process (MDP) under the LLM policy incrementally, encoding states as natural language prompts and parsing LLM outputs into actions. The resulting model is checked with the Storm model checker to verify safety properties expressed in probabilistic computation tree logic (PCTL). Experiments demonstrate that deterministic LLM policies can be formally verified, though they generally underperform deep reinforcement learning baselines on standard grid-world benchmarks.

## Method Summary
The core approach incrementally constructs the reachable portion of the MDP under the LLM policy by encoding states as natural language prompts, parsing the LLM's output into actions, and expanding only the resulting successor states. This incremental construction enables efficient exploration of the state space while maintaining formal guarantees. The resulting MDP model is then checked using the Storm model checker to determine whether the policy satisfies specified PCTL safety properties. The tool supports open-source LLMs via Ollama and allows users to specify tasks in the PRISM language, facilitating continuous benchmarking and verification of new LLM releases.

## Key Results
- Deterministic LLM policies can be formally verified against PCTL safety properties
- Verified policies generally underperform deep reinforcement learning baselines on grid-world benchmarks
- The tool enables seamless verification of new LLM releases and user-defined tasks in PRISM language
- Incremental MDP construction approach makes verification tractable for practical grid-world problems

## Why This Works (Mechanism)
The approach works by leveraging the LLM's ability to map natural language state descriptions to action choices, then formally modeling this mapping as a policy within an MDP framework. By incrementally constructing only the reachable portion of the MDP, the method avoids the exponential state space explosion that would occur with exhaustive enumeration. The Storm model checker provides formal guarantees about policy safety properties by exhaustively analyzing the constructed MDP model. This combination of LLM-based policy generation with formal verification methods enables mathematically rigorous safety analysis of AI decision-making systems.

## Foundational Learning
- **Markov Decision Processes (MDPs)**: Mathematical framework for modeling sequential decision-making under uncertainty; needed to represent the decision problem formally for verification; quick check: verify understanding of states, actions, transition probabilities, and rewards
- **Probabilistic Computation Tree Logic (PCTL)**: Temporal logic for specifying properties of probabilistic systems; needed to express safety properties that can be verified; quick check: can express "reach a goal state with probability â‰¥ 0.9 while avoiding unsafe states"
- **Incremental MDP Construction**: Technique for building the state space only as needed during exploration; needed to make verification tractable for large state spaces; quick check: understand how successor states are generated and explored
- **Natural Language Prompt Engineering**: Skill of crafting effective prompts to elicit desired LLM behavior; needed to encode states and parse actions correctly; quick check: can design prompts that consistently produce valid action outputs
- **Model Checking with Storm**: Algorithmic verification method for finite-state probabilistic systems; needed to automatically verify PCTL properties on the constructed MDP; quick check: understand basic concepts of probabilistic verification

## Architecture Onboarding

**Component Map:**
User Task -> PRISM Specification -> LLM Policy Construction -> Incremental MDP Building -> Storm Model Checker -> Verification Result

**Critical Path:**
1. User defines task in PRISM language
2. System constructs LLM-based policy by mapping states to prompts
3. Incremental MDP construction explores reachable states
4. Storm model checker verifies PCTL properties
5. System returns verification result

**Design Tradeoffs:**
- Memoryless vs. history-dependent policies (simplicity vs. expressiveness)
- Incremental vs. exhaustive state space construction (tractability vs. completeness)
- Open-source vs. proprietary LLMs (accessibility vs. performance)
- Natural language encoding vs. symbolic state representation (flexibility vs. precision)

**Failure Signatures:**
- Policy verification fails due to incomplete state space exploration
- LLM produces invalid or inconsistent action outputs
- Storm model checker times out on large MDPs
- PCTL properties cannot be expressed in the required formalism

**3 First Experiments:**
1. Verify a simple grid-world policy against basic safety properties (reach goal, avoid traps)
2. Compare verification results across different open-source LLMs of varying sizes
3. Test the tool's ability to verify a policy on a new task specified in PRISM language

## Open Questions the Paper Calls Out
None

## Limitations
- Experimental evaluation limited to grid-world benchmarks, which may not capture real-world complexity
- Focus on memoryless policies excludes many practical applications requiring history-dependent decision-making
- Open-source LLMs used in experiments may underperform compared to state-of-the-art proprietary models
- Comparison with deep reinforcement learning baselines shows LLM policies generally underperform

## Confidence
- **High confidence**: The core verification methodology (incremental MDP construction, PCTL property checking) is technically sound and well-implemented
- **Medium confidence**: The experimental results showing LLM policies' performance relative to DRL baselines, given the limited benchmark scope
- **Medium confidence**: The tool's practical utility for continuous benchmarking of LLM releases, pending broader real-world validation

## Next Checks
1. Evaluate the approach on larger, more complex sequential decision-making benchmarks beyond grid-world environments
2. Test the tool with state-of-the-art proprietary LLMs to assess scalability and performance differences
3. Extend the methodology to handle policies with memory (history-dependent policies) and assess the impact on verification complexity and performance