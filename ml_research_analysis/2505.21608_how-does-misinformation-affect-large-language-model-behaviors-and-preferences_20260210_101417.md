---
ver: rpa2
title: How does Misinformation Affect Large Language Model Behaviors and Preferences?
arxiv_id: '2505.21608'
source_url: https://arxiv.org/abs/2505.21608
tags:
- misinformation
- deterding
- llms
- university
- stanford
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models remain vulnerable to misinformation, struggling
  to discern conflicting claims across knowledge domains and textual styles. This
  paper introduces MISBENCH, a comprehensive benchmark containing 10.3 million misinformation
  examples spanning factual, temporal, and semantic conflicts across six textual styles
  including news reports, blogs, and technical language.
---

# How does Misinformation Affect Large Language Model Behaviors and Preferences?

## Quick Facts
- arXiv ID: 2505.21608
- Source URL: https://arxiv.org/abs/2505.21608
- Reference count: 40
- Large language models remain vulnerable to misinformation, struggling to discern conflicting claims across knowledge domains and textual styles.

## Executive Summary
This paper investigates how large language models (LLMs) handle misinformation across different knowledge domains and textual styles. The authors introduce MISBENCH, a comprehensive benchmark containing 10.3 million misinformation examples spanning factual, temporal, and semantic conflicts across six textual styles. Experiments reveal that while LLMs can identify contextual inconsistencies, they are particularly susceptible to factual contradictions and ambiguous semantic constructs, with susceptibility varying by task complexity and presentation style. Building on these insights, the authors propose Reconstruct to Discriminate (RtD), which enhances misinformation detection by reconstructing entity descriptions from external knowledge sources and comparing them with original claims.

## Method Summary
The method involves claim extraction from Wikidata and 2WikiMultihopQA, conflict construction across three types (factual, temporal, semantic), stylization using LLaMA-3-70B across six styles, and quality control through NLI entailment checking and semantic matching. The RtD method identifies subject entities, retrieves Wikipedia descriptions, generates supporting evidence through reconstruction, and compares with original claims. Evaluation uses vLLM with specific prompts and metrics including Success Rate%, Memorization Ratio, and Evidence Tendency scores.

## Key Results
- LLMs demonstrate vulnerability to misinformation across all three conflict types (factual, temporal, semantic)
- Susceptibility varies significantly by textual style, with formal styles more convincing in single-hop tasks and narrative styles in multi-hop reasoning
- RtD improves detection success rates by up to 20.6% on Gemma2-9B and 6.0% on Qwen2.5-14B

## Why This Works (Mechanism)

### Mechanism 1: Contextual Inconsistency Detection Without Prior Knowledge
- Claim: LLMs can identify misinformation through contextual inconsistencies even when lacking specific factual knowledge
- Core assumption: Inconsistency patterns in misinformation are learnable from pre-training on coherent text
- Evidence anchors: LLMs demonstrate comparable abilities in discerning misinformation without prior knowledge (average 12.6% drop)
- Break condition: When misinformation is internally consistent and stylistically matches training distribution

### Mechanism 2: Comparative Discrimination Over Single-Judgment
- Claim: LLMs perform better at comparative analysis between conflicting evidence than single-source truth evaluation
- Core assumption: The relative signal (difference between correct and incorrect) is stronger than absolute judgment signals
- Evidence anchors: LLMs achieve notably higher MR when evaluating contradictory evidence compared to single-evidence scenarios
- Break condition: When both conflicting sources appear equally authoritative

### Mechanism 3: External Knowledge Reconstruction (RtD)
- Claim: Reconstructing entity descriptions from authoritative sources and comparing with claims improves misinformation detection
- Core assumption: External sources (Wikipedia) are more reliable than model's internal knowledge for entity-level facts
- Evidence anchors: RtD improves detection success rates by up to 20.6% on Gemma2-9B and 6.0% on Qwen2.5-14B
- Break condition: When external sources are outdated, ambiguous, or the entity is not well-documented

## Foundational Learning

- Concept: Knowledge Conflict Taxonomy (Factual, Temporal, Semantic)
  - Why needed here: Misinformation vulnerability varies by conflict type; semantic conflicts are most challenging because they exploit polysemy and context ambiguity
  - Quick check question: Given "Apple announced a new product in 2025," would a claim that "Apple is a fruit company" be factual, temporal, or semantic conflict?

- Concept: Memorization Ratio (MR) vs. Evidence Tendency (TendCM)
  - Why needed here: These metrics distinguish between "relying on internal knowledge" vs. "choosing between presented evidence"—critical for diagnosing failure modes
  - Quick check question: If MR=0.15 and TendCM=0.42 for the same model, what does this indicate about its behavior?

- Concept: Stylistic Susceptibility (Formal vs. Narrative)
  - Why needed here: LLMs are more susceptible to formal/objective style in single-hop tasks but narrative/subjective style in multi-hop reasoning—style is not neutral
  - Quick check question: Would a fabricated news report or a fabricated blog post be more convincing to an LLM on a multi-hop reasoning task?

## Architecture Onboarding

- Component map: Claim Extraction -> Conflict Construction -> Stylization -> Quality Control -> RtD Pipeline
- Critical path: Claim extraction → Conflict construction → Stylization → Quality control. RtD adds retrieval+reconstruction as augmentation
- Design tradeoffs:
  - Synthetic data enables scale (10.3M) but may not cover all real-world patterns
  - Multiple-choice format constrains response space for cleaner evaluation but reduces ecological validity
  - External knowledge as ground truth assumes recency and accuracy
- Failure signatures:
  - High susceptibility to semantic conflicts (ambiguous entities exploit polysemy)
  - Style-dependent performance drops (formal style on single-hop, narrative on multi-hop)
  - Simply feeding retrieved descriptions shows limited improvement vs. RtD reconstruction
- First 3 experiments:
  1. Establish baseline Success Rate% on MISBENCH for target model across three conflict types, with and without prior knowledge
  2. Measure MR and TendCM to diagnose whether failures stem from knowledge gaps or contextual susceptibility
  3. Implement RtD: compare (+Desc) direct retrieval vs. reconstruction-based comparison; expect larger gains on semantic conflicts

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the integration of visual content and metadata modify LLMs' susceptibility to the textual misinformation styles (e.g., narrative vs. technical) identified in MISBENCH?
- Basis in paper: The authors state in the Limitations section that the study focuses primarily on text-based content and that "future work should consider the impact of metadata, visual content, and other forms of information"
- Why unresolved: The current experimental setup isolates text to analyze linguistic patterns, ignoring the multi-modal nature of real-world misinformation
- What evidence would resolve it: An evaluation of LLM performance on an extended version of MISBENCH that pairs misinformation texts with misleading or corroborating images and source metadata

### Open Question 2
- Question: To what extent does the overlap between synthetic MISBENCH data and LLMs' pre-training corpora artificially inflate the reported robustness or vulnerability metrics?
- Basis in paper: The authors acknowledge that "conflict pairs may be extracted from pre-training corpora," but "the sheer volume of data makes it difficult to efficiently identify" this contamination
- Why unresolved: It is unclear if the models are reasoning through the misinformation or recognizing it from training data, which could distort the "Memorization Ratio" metric
- What evidence would resolve it: A decontamination analysis comparing model performance on data identified as overlapping with pre-training sets versus wholly novel synthetic data

### Open Question 3
- Question: Does the Reconstruct to Discriminate (RtD) method's reliance on external knowledge retrieval hinder its effectiveness for long-tail entities with sparse Wikipedia descriptions?
- Basis in paper: The RtD method depends on retrieving entity descriptions from external sources (Wikipedia/Wikidata). While the method improves detection, the paper does not analyze performance on entities that lack rich descriptions
- Why unresolved: RtD may fail or hallucinate when the external evidence it relies on is missing or insufficient, a scenario likely for obscure entities
- What evidence would resolve it: An ablation study evaluating RtD's success rate specifically on a subset of low-frequency or "tail" entities compared to head entities

## Limitations
- Synthetic MISBENCH data may not fully capture real-world misinformation complexity
- RtD's reliance on Wikipedia assumes accuracy and currency, which may not hold for rapidly evolving topics
- Multiple-choice evaluation format may overestimate true misinformation detection capability

## Confidence
- **High Confidence**: LLMs are vulnerable to misinformation across all conflict types with style-dependent susceptibility
- **Medium Confidence**: RtD shows measurable improvement over direct retrieval, but magnitude may be influenced by dataset construction
- **Medium Confidence**: LLMs can detect contextual inconsistencies without prior knowledge, but with significant limitations

## Next Checks
1. Apply RtD to a dataset of manually curated real-world misinformation examples to assess generalization
2. Evaluate model performance on misinformation involving entities with rapidly changing information to test Wikipedia-grounded assumption
3. Convert a subset of MISBENCH questions to open-ended format to evaluate whether multiple-choice performance overestimates true capability