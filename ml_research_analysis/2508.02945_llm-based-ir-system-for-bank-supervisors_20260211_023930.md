---
ver: rpa2
title: LLM-based IR-system for Bank Supervisors
arxiv_id: '2508.02945'
source_url: https://arxiv.org/abs/2508.02945
tags:
- findings
- system
- uni00000013
- retrieval
- finding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents an IR system to assist bank supervisors in drafting
  consistent measures for new findings by retrieving relevant historical findings
  and associated measures. The system combines lexical (BM25L+), semantic (fine-tuned
  sentence transformer), and fuzzy CRR article matching techniques to rank and retrieve
  findings.
---

# LLM-based IR-system for Bank Supervisors

## Quick Facts
- arXiv ID: 2508.02945
- Source URL: https://arxiv.org/abs/2508.02945
- Reference count: 8
- Primary result: Hybrid IR model achieves MAP@100=0.83 and MRR@100=0.92 for bank supervision findings retrieval

## Executive Summary
This paper presents an information retrieval system designed to assist bank supervisors in drafting consistent measures for new findings by retrieving relevant historical findings and associated measures. The system combines lexical (BM25L+), semantic (fine-tuned sentence transformer), and fuzzy CRR article matching techniques to rank and retrieve findings. A Monte Carlo methodology validates performance on partially labeled data, yielding strong results that surpass standalone lexical or semantic models, demonstrating the effectiveness of integrating multiple retrieval strategies for the prudential domain.

## Method Summary
The system retrieves historical bank supervision findings to help draft consistent measures for new cases. It uses a hybrid approach combining BM25L+ for lexical matching, a fine-tuned sentence transformer (SentTRF+TSDAE) for semantic matching, and fuzzy matching on CRR article metadata. The hybrid model averages similarity scores from lexical and semantic components, while CRR metadata filtering refines the candidate pool. The system is evaluated on a confidential dataset of ~7,000 ECB findings using MAP@100 and MRR@100 metrics.

## Key Results
- Hybrid model achieves MAP@100=0.83 and MRR@100=0.92 on validation data
- System outperforms standalone lexical (MAP@100=0.74) and semantic models
- CRR fuzzy matching improves performance across all models when added
- TSDAE fine-tuning provides domain adaptation benefits for semantic embeddings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combining lexical and semantic retrieval signals improves ranking accuracy over single-signal approaches in this domain.
- Mechanism: Lexical models capture exact term overlap while semantic models capture contextual meaning. Their similarity scores are averaged to produce a hybrid ranking that leverages both signals.
- Core assumption: Relevant historical findings share both key terms/phrases and underlying regulatory concepts with the input finding.
- Evidence anchors:
  - "The system combines lexical (BM25L+), semantic (fine-tuned sentence transformer), and fuzzy CRR article matching techniques to rank and retrieve findings."
  - Table 4 shows the Hybrid model achieving MAP@100=0.83 and MRR@0.93, outperforming individual models.
  - "A Hybrid Approach to Information Retrieval and Answer Generation for Regulatory Texts" similarly combines lexical and semantic techniques for regulatory documents.
- Break condition: If findings use divergent terminology for the same concept (low lexical overlap) or if the semantic model fails to capture domain-specific nuances.

### Mechanism 2
- Claim: Unsupervised domain adaptation via TSDAE improves sentence embeddings for this specialized regulatory corpus.
- Mechanism: A Transformer-based Sequential Denoising AutoEncoder (TSDAE) is trained to reconstruct original findings from noise-corrupted versions (50% token deletion), forcing the model to learn robust semantic representations specific to prudential banking text without labeled data.
- Core assumption: The reconstruction task teaches the model to capture meaningful semantic structure inherent to the regulatory domain.
- Evidence anchors:
  - "Enhanced by a Transformer-based Denoising AutoEncoder for fine-tuning..."
  - "The domain-adapted Sentence Transformer, SentTRF+TSDAE, substantially benefits from fuzzy matching, achieving a MAP of 0.82 and an MRR of 0.92."
  - No direct corpus papers specifically address TSDAE in regulatory IR; evidence is primarily internal to the paper.
- Break condition: If the noise corruption strategy is too aggressive or too mild, or if the domain corpus is too small or unrepresentative.

### Mechanism 3
- Claim: Fuzzy matching on CRR article metadata refines the search space and boosts precision by leveraging regulatory structure.
- Mechanism: Findings are linked to Capital Requirements Regulation (CRR) articles. The system computes Jaccard similarity and hierarchical similarity between the CRR article sets of the input and candidate findings, filtering candidates that don't meet thresholds (≥1/3 for both).
- Core assumption: Findings addressing similar non-compliance issues will be linked to overlapping or hierarchically related CRR articles.
- Evidence anchors:
  - "...CRR fuzzy set matching techniques, the IR system ensures the retrieval of findings that closely align with current cases."
  - Table 4 shows performance improvements across all models when fuzzy CRR matching is added (e.g., Hybrid model MAP rises from 0.74 to 0.83).
  - Corpus evidence for fuzzy CRR matching in this specific domain is weak/not present in provided neighbors.
- Break condition: If CRR article tagging is incomplete, incorrect, or if the regulatory structure changes significantly.

## Foundational Learning

- **Information Retrieval Evaluation Metrics (MAP, MRR)**: To quantify system performance. MAP@100 measures average precision across the top 100 retrieved items, rewarding systems that rank relevant items higher. MRR@100 measures where the first relevant item appears.
  - Quick check: Why might a system have high MAP but lower MRR, and vice versa?

- **Sentence Transformers and Embedding Spaces**: The core of the semantic component. These models map text to dense vectors where cosine similarity approximates semantic relatedness.
  - Quick check: How does a Sentence Transformer differ from a word-level embedding model like GloVe in producing document representations?

- **Hybrid Retrieval Systems**: The system fuses three signals. Understanding why and how to combine lexical, semantic, and metadata signals is key.
  - Quick check: What are two potential risks when averaging similarity scores from models with different scales and error profiles?

## Architecture Onboarding

- **Component map**: Input Finding → CRR Metadata Filter → Parallel Lexical & Semantic Scoring on Filtered Candidates → Score Fusion → Top-k Ranked Findings → Associated Measures
- **Critical path**: Input Finding → CRR Metadata Filter → Parallel Lexical & Semantic Scoring on Filtered Candidates → Score Fusion → Top-k Ranked Findings → Associated Measures
- **Design tradeoffs**: Increased complexity vs. accuracy. TSDAE requires compute for fine-tuning. CRR filtering risks excluding novel edge cases. Hybrid fusion assumes equal importance of lexical and semantic signals.
- **Failure signatures**: (1) High lexical overlap but irrelevant semantics; (2) Semantically similar but different CRR focus; (3) Correct CRR articles but poor text match
- **First 3 experiments**:
  1. **Ablation study**: Measure Hybrid model performance with and without the CRR filtering stage to quantify its impact.
  2. **Baseline comparison**: Compare Hybrid model against BM25L+ and SentTRF+TSDAE individually on a held-out set of manually labeled test queries.
  3. **Domain adaptation analysis**: Compare SentTRF+TSDAE against the off-the-shelf SentTRF model to measure the gain from TSDAE fine-tuning.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the mapping between findings and measures ($\theta: F \to P_{\ge 1}(M)$) be effectively approximated using a Neural Machine Translation (NMT) approach with generative Large Language Models?
- Basis in paper: Section 7 explicitly proposes "rewriting the problem as a Neural Machine Translation (NMT) task" to suggest measures dynamically rather than just retrieving them.
- Why unresolved: The current system retrieves existing historical measures but lacks the capability to generate or suggest new measures based on the input findings, which requires solving the complexities of conditional text generation in a legal context.
- What evidence would resolve it: Successful implementation and validation of a generative model that can translate a finding into a consistent, legally sound measure that aligns with historical data better than retrieval alone.

### Open Question 2
- Question: How can the IR system maintain accuracy and consistency when regulatory changes alter the relevance or validity of historical precedents?
- Basis in paper: Section 7 identifies the "evolving regulatory landscape" as a limitation, noting that changing regulations make it difficult to locate similar findings and necessitate an "auxiliary system that adapts to regulatory changes."
- Why unresolved: The current system relies on a static database of historical findings. It does not possess a mechanism to detect or adjust for concept drift when regulations (CRR) are updated, potentially retrieving obsolete measures.
- What evidence would resolve it: Development of a continuous learning framework or a dynamic weighting mechanism that identifies and down-weights findings based on regulatory obsolescence, validated through temporal hold-out sets.

### Open Question 3
- Question: Would a learned weighting strategy outperform the simple arithmetic mean used to combine lexical and semantic similarity scores in the Hybrid model?
- Basis in paper: Section 4.2 defines the Hybrid model using a simple average of the similarity matrices ($\Sigma_{Hybrid} = (\Sigma_{BM25L+} + \Sigma_{SentTRF}) / 2$), assuming equal importance for both signals.
- Why unresolved: Equal weighting may be suboptimal if specific query types (e.g., those relying heavily on specific legal citations vs. descriptive text) benefit more from one retrieval strategy over the other.
- What evidence would resolve it: A comparative ablation study showing that a learning-to-rank algorithm or weighted reciprocal rank fusion yields statistically significant improvements in MAP/MRR over the simple averaging method.

## Limitations

- Dataset secrecy and internal validation: The core dataset (7,000 ECB findings) and labeled relevance pairs are internal and confidential, limiting verifiability of reported performance metrics.
- Corpus representativeness and CRR tagging completeness: The effectiveness of CRR fuzzy matching hinges on complete and accurate CRR article tagging of findings, which may not be available in other domains.
- Hybrid score fusion assumptions: The system assumes lexical and semantic similarity scores are equally important and linearly combinable, ignoring potential scale differences and correlation between errors.

## Confidence

- **High confidence**: The general design pattern of combining lexical (BM25L+) and semantic (sentence transformer) retrieval signals for hybrid ranking is well-supported by prior work in the domain and internal ablation results.
- **Medium confidence**: The specific claim that TSDAE fine-tuning substantially improves embeddings for this prudential domain is primarily supported by internal comparisons and lacks direct external validation.
- **Low confidence**: The assertion that the CRR fuzzy matching stage is essential for boosting precision is based on the paper's internal results and lacks strong supporting evidence from related literature.

## Next Checks

1. **Reproduce hybrid performance on open proxy data**: Obtain a public regulatory or legal document corpus with metadata tags (e.g., U.S. CFR sections or EU directives), implement the BM25L+, fine-tuned semantic model, and CRR-style metadata filter, then measure hybrid vs. individual model performance to verify the claimed gains hold on different data.

2. **Perform ablation on the CRR filtering stage**: Systematically remove the fuzzy CRR matching filter from the Hybrid pipeline and re-evaluate MAP@100 and MRR@100. Compare performance with a lowered similarity threshold (e.g., 0.1 or 0.0) to quantify the contribution of metadata filtering and diagnose aggressive filtering failures.

3. **Validate domain adaptation impact of TSDAE**: Compare the performance of the base `all-MiniLM-L6-v2` embeddings against the TSDAE fine-tuned `SentTRF+TSDAE` model on a manually labeled subset of test queries. Measure if the fine-tuning consistently improves semantic retrieval in the prudential domain or if gains are marginal/noisy.