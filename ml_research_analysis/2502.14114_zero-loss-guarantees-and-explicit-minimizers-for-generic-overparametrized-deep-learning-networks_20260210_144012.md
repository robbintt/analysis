---
ver: rpa2
title: Zero loss guarantees and explicit minimizers for generic overparametrized Deep
  Learning networks
arxiv_id: '2502.14114'
source_url: https://arxiv.org/abs/2502.14114
tags:
- rank
- zero
- loss
- full
- then
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of achieving zero loss in overparametrized
  deep learning networks for supervised learning with L2 cost. The authors prove that
  generic training data ensures the attainability of zero loss by providing an explicit
  construction of zero-loss minimizers without invoking gradient descent.
---

# Zero loss guarantees and explicit minimizers for generic overparametrized Deep Learning networks

## Quick Facts
- arXiv ID: 2502.14114
- Source URL: https://arxiv.org/abs/2502.14114
- Reference count: 20
- Key outcome: Generic training data ensures zero loss in strongly overparametrized deep networks via explicit construction without gradient descent

## Executive Summary
This paper addresses the fundamental question of when zero loss is achievable in overparametrized deep learning networks for supervised learning with L2 cost. The authors prove that generic training data ensures zero loss by providing an explicit construction of zero-loss minimizers without invoking gradient descent. They establish sufficient conditions for zero loss in deep networks with equal layer dimensions and diffeomorphic activations, showing that while strongly overparameterized networks are generally solvable, increasing depth can deteriorate gradient descent efficiency by causing rank loss in the training Jacobian.

## Method Summary
The paper presents an explicit closed-form construction of zero-loss minimizers for strongly overparametrized deep networks. The method works recursively backward from the output layer, using the inverse activation to propagate target values through hidden layers while solving linear regression at the input layer. The construction requires equal layer dimensions Mℓ = M > N, full-rank data matrix X⁽⁰⁾, and a diffeomorphic activation σ. For networks with L hidden layers, the output layer is set to project onto the Q target dimensions, and the remaining parameters are computed via backward recursion using σ⁻¹.

## Key Results
- Generic training data (full-rank X⁽⁰⁾) guarantees existence of zero-loss minimizers in strongly overparametrized networks
- Explicit construction works recursively backward through layers without invoking gradient descent
- Increasing depth can deteriorate gradient descent efficiency by causing rank loss in the training Jacobian
- Full-rank weight matrices W₂, ..., W_{L+1} preserve Jacobian rank but constrain initialization

## Why This Works (Mechanism)

### Mechanism 1
Zero loss minimizers can be explicitly constructed for strongly overparametrized networks with generic training data. The construction works recursively backward from the output layer, solving linear regression at the input layer while using the inverse activation to propagate target values backward through hidden layers. The output layer is set to project onto the Q target dimensions. This fails if X⁽⁰⁾ is rank-deficient or intermediate layer activations fall outside the domain of σ⁻¹.

### Mechanism 2
Full rank of the training Jacobian D ensures well-behaved gradient flow. When D ∈ ℝᴺˣᴷ is full rank, gradient descent trajectories are deformable to linear interpolation in output space. Rank loss redirects gradient flow unpredictably and violates simplicity guarantees. This fails if any W_j (j ≥ 2) loses rank or if activation derivatives vanish across an entire layer's outputs.

### Mechanism 3
Increasing depth L can slow gradient descent convergence even when zero loss minimizers exist. The gradient involves products of σ' raised to powers up to L. For mollified ReLU-like activations with σ'(x) → 0⁺ as x → −∞, gradients can become arbitrarily small along certain trajectories, causing numerical ill-conditioning. This is mitigated if σ' is uniformly bounded below away from zero.

## Foundational Learning

- **Concept**: Strong overparametrization (M > N, K > N)
  - Why needed here: The paper's zero-loss guarantees and explicit constructions require more parameters than training points
  - Quick check question: Given M input dimension, N training samples, and L hidden layers each of width Mℓ, what is the total parameter count K via equation (3)?

- **Concept**: Local diffeomorphism and submersion
  - Why needed here: The explicit construction relies on σ⁻¹ existing locally; gradient flow analysis requires ∇σ ≠ 0 (submersion) to avoid Jacobian rank loss
  - Quick check question: Is standard ReLU a diffeomorphism on ℝ⁺? Is it a submersion at x = 0?

- **Concept**: Training Jacobian D = ∂f/∂θ
  - Why needed here: The rank of this N × K matrix determines whether gradient descent trajectories are well-behaved
  - Quick check question: For a linear model f(x) = ⟨x, θ⟩ with N data points, what is D and when is it full rank?

## Architecture Onboarding

- **Component map**: X⁽⁰⁾ ∈ ℝᴹˣᴺ (input) → X⁽¹⁾ → ... → X⁽ᴸ⁾ → X⁽ᴸ⁺1⁾ ∈ ℝᵠ (output)
- **Critical path**:
  1. Verify data matrix X⁽⁰⁾ has full column rank N
  2. Choose activation σ that is locally invertible
  3. Initialize W₂, ..., W_L as arbitrary full-rank matrices
  4. Construct W_1 via pseudo-inverse of X⁽⁰⁾; construct W_{L+1} and biases via backward recursion

- **Design tradeoffs**:
  - Width vs. depth: Large width alone suffices for zero loss; depth adds representation power but introduces gradient flow pathologies
  - Activation choice: Strictly monotone activations enable explicit construction but may have small gradients in saturation regions
  - Rank maintenance: Full-rank weight matrices preserve Jacobian rank but constrain initialization strategies

- **Failure signatures**:
  - Gradient stalling: Loss plateaus with tiny updates → check activation derivative magnitudes across layers
  - Jacobian rank loss: Sudden trajectory deviation in output space → check for rows of ∇σ|_A(X) approaching zero
  - Construction failure: σ⁻¹ domain error during explicit construction → intermediate values exited ℝ⁺_M

- **First 3 experiments**:
  1. Replicate the explicit construction on synthetic data with full-rank X⁽⁰⁾ ∈ ℝ¹⁰ˣ⁵ and verify C[θ*] = 0
  2. Compare gradient descent trajectories vs. explicit solution by initializing near the constructed minimizer
  3. Jacobian rank monitoring during training: Track rank(D) across epochs for varying depths L ∈ {2, 5, 10} with fixed width M = 50

## Open Questions the Paper Calls Out

### Open Question 1
What is the geometry of the rank-deficient set in parameter space, and how does it specifically impact gradient flow trajectories? The paper establishes that rank loss creates obstructions and that these regions have specific codimensions, but it does not characterize the topological structure of these sets or how they redirect the flow.

### Open Question 2
What is the quantitative probability that a gradient flow path encounters rank-deficient regions in strongly overparameterized networks? While the paper suggests the probability is low because M is large, it provides no rigorous bounds or statistical measures of avoidance.

### Open Question 3
To what extent does practical (discrete) gradient descent diverge from ideal gradient flow when crossing rank-deficient barriers? The paper notes that while ideal gradient flow is redirected by rank loss, practical implementations "almost surely 'miss' or 'tunnel through'" these regions, implying a disconnect in theoretical modeling.

## Limitations
- The explicit construction's robustness to rank-deficient data remains unclear
- The choice of activation functions with bounded inverses versus unbounded ones may significantly affect numerical stability
- The theoretical rank preservation conditions for the training Jacobian rely on strong assumptions about full-rank weight matrices

## Confidence

- **High confidence**: Zero loss is achievable for generic training data in strongly overparametrized networks with equal layer dimensions
- **Medium confidence**: Increasing depth deteriorates gradient descent efficiency through Jacobian rank loss
- **Low confidence**: The explicit construction scales well to very deep networks with realistic activation functions

## Next Checks

1. Test the explicit construction on progressively more ill-conditioned data matrices (condition numbers ranging from 10² to 10⁸) to quantify the transition from "generic" to problematic cases
2. Compare gradient descent convergence rates for networks of depth L=2, 5, 10 with identical width and initialization, measuring both training time and final loss
3. Implement the explicit construction with different activation functions (tanh, sigmoid, mollified ReLU) on the same synthetic dataset to compare numerical stability and construction success rates