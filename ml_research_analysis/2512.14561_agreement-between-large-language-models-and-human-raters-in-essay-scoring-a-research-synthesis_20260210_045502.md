---
ver: rpa2
title: 'Agreement Between Large Language Models and Human Raters in Essay Scoring:
  A Research Synthesis'
arxiv_id: '2512.14561'
source_url: https://arxiv.org/abs/2512.14561
tags:
- https
- essay
- scoring
- studies
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This synthesis examined agreement between large language models
  (LLMs) and human raters in automated essay scoring. Among 65 studies published from
  2022 to 2025, reported agreement indices such as Quadratic Weighted Kappa, Pearson
  correlation, and Spearman's rho generally ranged from 0.30 to 0.80, indicating moderate
  to good agreement.
---

# Agreement Between Large Language Models and Human Raters in Essay Scoring: A Research Synthesis

## Quick Facts
- **arXiv ID**: 2512.14561
- **Source URL**: https://arxiv.org/abs/2512.14561
- **Reference count**: 16
- **Key outcome**: Moderate to good agreement (0.30-0.80) between LLMs and human raters in automated essay scoring across 65 studies

## Executive Summary
This synthesis examined agreement between large language models (LLMs) and human raters in automated essay scoring through 65 studies published from 2022 to 2025. Reported agreement indices including Quadratic Weighted Kappa, Pearson correlation, and Spearman's rho generally ranged from 0.30 to 0.80, indicating moderate to good alignment between AI and human scoring. The research landscape shows GPT-4 as the most frequently used model, with most studies focusing on English-language writing and L2 contexts. While findings suggest LLMs can serve as useful tools for essay scoring, the wide variation in study design, scoring rubrics, and reporting practices highlights the need for standardized measurement approaches.

## Method Summary
The synthesis analyzed 65 studies published between 2022 and 2025 that compared large language models with human raters for automated essay scoring. Studies reported various agreement indices including Quadratic Weighted Kappa, Pearson correlation, and Spearman's rho. The analysis examined patterns across different models, languages, and educational contexts, with particular attention to methodological variations that might influence agreement levels.

## Key Results
- Agreement between LLMs and human raters ranged from 0.30 to 0.80 across studies, indicating moderate to good alignment
- GPT-4 was the most frequently studied model in automated essay scoring research
- Most studies focused on English-language writing and L2 contexts, with limited research on younger learners and diverse languages

## Why This Works (Mechanism)
The moderate to good agreement between LLMs and human raters likely stems from LLMs' ability to capture complex linguistic features and semantic patterns that align with human scoring criteria. LLMs demonstrate strong performance in understanding essay structure, coherence, and content relevance, which are key dimensions in holistic scoring approaches. Their training on diverse text corpora enables them to recognize quality indicators that human raters also value, though the exact cognitive processes differ between AI and human scoring.

## Foundational Learning
None

## Architecture Onboarding
None

## Open Questions the Paper Calls Out
None

## Limitations
- Wide variation in study designs, scoring rubrics, and reporting practices prevents direct comparisons between studies
- Overrepresentation of English-language and L2 writing contexts limits generalizability to other languages and younger learners
- Lack of standardized measurement protocols across the 65 included studies
- Absence of systematic analysis of model-specific performance differences beyond GPT-4 dominance

## Confidence
- **High**: LLMs show moderate to good agreement with human raters when aggregated across diverse studies
- **Medium**: GPT-4 is the most frequently studied model and English/L2 contexts dominate the literature
- **Low**: Agreement levels are comparable to traditional AES systems without direct comparative evidence

## Next Checks
1. Conduct direct head-to-head comparisons between top-performing LLMs and established AES systems using identical datasets and rubrics
2. Implement standardized reporting protocols for agreement metrics, including inter-rater reliability of human scores as a baseline reference
3. Expand research to include diverse language populations and K-12 age groups with systematic sampling across educational levels