---
ver: rpa2
title: Privacy-Preserving Chest X-ray Report Generation via Multimodal Federated Learning
  with ViT and GPT-2
arxiv_id: '2505.21715'
source_url: https://arxiv.org/abs/2505.21715
tags:
- federated
- learning
- client
- data
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses privacy concerns in medical report generation
  by proposing a multimodal federated learning framework for chest X-ray report generation.
  The system employs a Vision Transformer (ViT) for image encoding and GPT-2 for report
  generation, enabling decentralized training without sharing raw patient data.
---

# Privacy-Preserving Chest X-ray Report Generation via Multimodal Federated Learning with ViT and GPT-2

## Quick Facts
- **arXiv ID:** 2505.21715
- **Source URL:** https://arxiv.org/abs/2505.21715
- **Reference count:** 40
- **Primary result:** Federated learning framework achieves comparable report generation quality to centralized approaches while preserving patient privacy

## Executive Summary
This study addresses critical privacy concerns in medical report generation by proposing a multimodal federated learning framework for chest X-ray report generation. The system employs a Vision Transformer (ViT) for image encoding and GPT-2 for report generation, enabling decentralized training without sharing raw patient data. Three federated learning aggregation strategies—FedAvg, Krum Aggregation, and a novel Loss-aware Federated Averaging (L-FedAvg)—were evaluated on the IU-Xray dataset. Among these, Krum Aggregation demonstrated superior performance, achieving the highest ROUGE-1 F1 score of 0.306, ROUGE-2 F1 score of 0.1411, BERTScore F1 of 0.8731, and RaTEScore of 62.24. These results indicate that federated learning can match or surpass centralized models in generating clinically relevant and semantically rich radiology reports while preserving data privacy.

## Method Summary
The framework consists of a multimodal federated learning architecture where local institutions train a Vision Transformer for chest X-ray image encoding and a GPT-2 model for report generation independently. Three aggregation strategies were implemented: FedAvg for averaging model parameters across participants, Krum Aggregation for robust aggregation resistant to malicious updates, and Loss-aware Federated Averaging (L-FedAvg) which incorporates local loss information during aggregation. The IU-Xray dataset was used for evaluation, with models trained in a federated manner across multiple clients. Performance was assessed using standard metrics including ROUGE scores for n-gram overlap, BERTScore for semantic similarity, and RaTEScore for radiology-specific evaluation.

## Key Results
- Krum Aggregation achieved the highest ROUGE-1 F1 score of 0.306 among federated approaches
- BERTScore F1 reached 0.8731, indicating strong semantic similarity between generated and reference reports
- RaTEScore of 62.24 demonstrates clinically relevant report generation quality
- Federated approaches showed comparable or superior performance to centralized baselines while maintaining data privacy

## Why This Works (Mechanism)
The success of this framework stems from leveraging the complementary strengths of Vision Transformers for robust visual feature extraction and GPT-2 for natural language generation. The federated learning approach enables model training across distributed datasets without data sharing, addressing critical privacy constraints in healthcare. By incorporating multiple aggregation strategies, the framework can adapt to different institutional requirements and data distributions. The multimodal design allows the model to learn meaningful associations between chest X-ray features and clinical terminology, resulting in semantically coherent and clinically relevant reports that maintain patient privacy throughout the training process.

## Foundational Learning

**Vision Transformers (ViT)** - why needed: To extract hierarchical visual features from chest X-ray images without relying on convolutional architectures; quick check: Verify patch embedding dimensions match input requirements.

**Transformer-based Language Models** - why needed: To generate coherent, clinically relevant text sequences conditioned on visual features; quick check: Confirm attention mask prevents future token leakage during generation.

**Federated Learning Aggregation** - why needed: To combine model updates from distributed clients while preserving data locality; quick check: Validate aggregation strategy maintains model convergence across heterogeneous clients.

**Medical Report Evaluation Metrics** - why needed: To assess both lexical and semantic quality of generated radiology reports; quick check: Ensure metric implementations handle clinical terminology and report structure appropriately.

**Multimodal Fusion** - why needed: To integrate visual and textual representations for coherent report generation; quick check: Verify cross-modal attention mechanisms properly align image features with textual context.

## Architecture Onboarding

**Component Map:** Chest X-ray Image -> ViT Encoder -> Visual Features -> GPT-2 Decoder -> Generated Report

**Critical Path:** Image preprocessing → ViT encoding → Multimodal attention fusion → GPT-2 generation → Report output

**Design Tradeoffs:** The framework trades computational overhead from federated aggregation for privacy preservation, and sacrifices some potential performance gains from centralized training for compliance with data protection regulations. The choice of GPT-2 over larger language models balances generation quality with computational efficiency suitable for federated deployment.

**Failure Signatures:** Performance degradation when client data distributions are highly heterogeneous, communication bottlenecks during frequent aggregation rounds, convergence issues with aggressive local training epochs, and potential security vulnerabilities in aggregation protocols.

**Three First Experiments:**
1. Baseline centralized training on combined dataset to establish upper performance bounds
2. Federated training with FedAvg aggregation across identical data distributions to verify convergence
3. Robustness testing with malicious clients to evaluate Krum Aggregation's resistance to adversarial updates

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to single dataset (IU-Xray), restricting generalizability to other clinical settings
- Performance metrics show room for improvement compared to state-of-the-art centralized approaches
- Scalability and performance in heterogeneous, multi-institutional settings not thoroughly evaluated
- Computational overhead and communication costs of federated learning not quantified

## Confidence
- **High:** Privacy-preserving nature of federated approach, relative performance of aggregation strategies on IU-Xray dataset
- **Medium:** Clinical utility and real-world applicability of generated reports (lacks physician evaluation)
- **Low:** Scalability claims and performance in heterogeneous, multi-institutional settings (limited experimental scope)

## Next Checks
1. External validation on multiple, diverse chest x-ray datasets to assess generalizability
2. Physician-in-the-loop evaluation to determine clinical acceptability and potential diagnostic impact of generated reports
3. Large-scale simulation of federated learning across geographically distributed institutions with varying data distributions to evaluate scalability and robustness of aggregation strategies