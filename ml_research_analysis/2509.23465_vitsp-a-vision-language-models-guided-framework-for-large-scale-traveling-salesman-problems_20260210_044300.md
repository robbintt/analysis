---
ver: rpa2
title: 'ViTSP: A Vision Language Models Guided Framework for Large-Scale Traveling
  Salesman Problems'
arxiv_id: '2509.23465'
source_url: https://arxiv.org/abs/2509.23465
tags:
- optimality
- runtime
- seconds
- instances
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes ViTSP, a vision language model (VLM)-guided
  framework for solving large-scale Traveling Salesman Problems (TSPs). ViTSP leverages
  pre-trained VLMs to visually identify promising subproblems from TSP instances,
  which are then optimized using exact solvers to improve the global solution.
---

# ViTSP: A Vision Language Models Guided Framework for Large-Scale Traveling Salesman Problems
## Quick Facts
- **arXiv ID:** 2509.23465
- **Source URL:** https://arxiv.org/abs/2509.23465
- **Reference count:** 40
- **Primary result:** ViTSP achieves average optimality gaps below 0.2% on large-scale TSP instances, outperforming learning-based methods and reducing LKH-3's gaps by 12-100% under same runtime budget

## Executive Summary
This paper introduces ViTSP, a novel framework that leverages vision-language models (VLMs) to solve large-scale Traveling Salesman Problems (TSPs). The approach uses pre-trained VLMs to visually identify promising subproblems within TSP instances, which are then optimized using exact solvers to improve global solutions. Unlike traditional learning-based approaches that require extensive training and struggle with out-of-distribution instances, ViTSP bypasses dedicated model training and maintains effectiveness across diverse TSP instances. The framework demonstrates how pre-trained generative models can effectively complement operations research solvers in combinatorial optimization.

## Method Summary
ViTSP employs a two-stage approach to solve large-scale TSP instances. First, it uses pre-trained vision-language models to visually analyze TSP instances and identify promising subproblems that are likely to contain high-quality solution segments. These subproblems are then extracted and solved using exact optimization solvers. The solutions from subproblems are integrated back into the global solution through a refinement process. This approach leverages the VLM's ability to recognize structural patterns in TSP instances without requiring any training on TSP-specific data, making it adaptable to various TSP instances without retraining.

## Key Results
- Achieves average optimality gaps below 0.2% on TSPLIB instances ranging from 1k to 88k nodes
- Reduces LKH-3's optimality gaps by 12-100% under the same runtime budget
- Outperforms existing learning-based methods on very-large-scale TSP instances
- Demonstrates effectiveness without requiring training on TSP-specific data

## Why This Works (Mechanism)
ViTSP works by leveraging VLMs' ability to understand visual representations of TSP instances and identify structurally important regions that are likely to contain high-quality solution segments. The VLM can recognize patterns such as cluster formations, boundary regions, and dense areas that traditional heuristics might miss. By focusing exact solvers on these promising subproblems, the framework achieves better solutions within the same computational budget. The approach is particularly effective for very large instances where traditional solvers struggle to explore the solution space comprehensively.

## Foundational Learning
- **Vision-Language Models:** Pre-trained models that can process both visual and textual information
  - *Why needed:* To visually analyze TSP instances and identify promising subproblems without requiring TSP-specific training
  - *Quick check:* Model should correctly identify high-quality subregions in TSP visualizations

- **Exact TSP Solvers:** Algorithms that guarantee optimal solutions for subproblems
  - *Why needed:* To obtain provably optimal solutions for identified subproblems
  - *Quick check:* Solver should return optimal solutions for subproblems within reasonable time

- **TSP Instance Visualization:** Converting TSP instances into visual representations
  - *Why needed:* To enable VLMs to process TSP data as images
  - *Quick check:* Visual representation should preserve key structural features of the TSP instance

## Architecture Onboarding
- **Component Map:** VLM Visual Analysis -> Subproblem Extraction -> Exact Solver -> Solution Integration
- **Critical Path:** VLM inference → subproblem identification → exact solver optimization → global solution refinement
- **Design Tradeoffs:** Using pre-trained VLMs eliminates training overhead but may not capture TSP-specific optimizations; focusing on subproblems reduces computational cost but requires effective integration strategy
- **Failure Signatures:** VLM misidentifies subproblems leading to suboptimal local solutions; integration step fails to incorporate subproblem solutions effectively
- **First Experiments:**
  1. Test VLM's ability to identify high-quality subproblems on small TSP instances with known optimal solutions
  2. Evaluate solution quality improvement when integrating subproblem solutions into global solutions
  3. Measure computational overhead of VLM inference versus pure solver runtime

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to TSPLIB instances, which represent only a specific subset of real-world TSP applications
- Performance on asymmetric TSP, time-dependent TSP, and TSP variants with additional constraints remains unexplored
- Computational overhead of VLM inference is not thoroughly characterized
- Scalability to extremely large instances beyond 88k nodes and behavior with different VLM architectures is not investigated

## Confidence
- **High:** The core methodology of using VLMs to identify subproblems for exact solvers is clearly articulated and experimentally validated on standard benchmark instances
- **Medium:** The claim that ViTSP maintains effectiveness across diverse TSP instances is supported by TSPLIB experiments but lacks validation on truly out-of-distribution instances
- **Low:** Claims about bypassing dedicated model training leading to better generalization lack comparative analysis with other zero-shot or few-shot learning approaches

## Next Checks
1. Evaluate ViTSP on asymmetric TSP instances and TSP variants with additional constraints (capacity, time windows) to assess generalization beyond symmetric TSP
2. Conduct ablation studies comparing VLM inference time versus pure solver runtime, and test different VLM model sizes to understand the trade-off between solution quality improvement and computational overhead
3. Test the framework's performance on out-of-distribution instances not represented in TSPLIB, including randomly generated instances with varying characteristics and real-world routing problems from logistics applications