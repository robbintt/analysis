---
ver: rpa2
title: Accelerating Inference for Multilayer Neural Networks with Quantum Computers
arxiv_id: '2510.07195'
source_url: https://arxiv.org/abs/2510.07195
tags:
- quantum
- circuit
- then
- lemma
- complexity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents the first fully coherent quantum implementation
  of multilayer neural networks with non-linear activation functions, specifically
  focusing on ResNet-like architectures with residual blocks containing multi-filter
  2D convolutions, sigmoid activations, skip-connections, and layer normalizations.
  The work analyzes inference complexity under three quantum data access regimes:
  with efficient quantum access to both inputs and weights (achieving polylogarithmic
  complexity in input dimension), with efficient quantum access to weights only (providing
  quartic speedup over classical methods), and with no quantum access assumptions
  (yielding quadratic speedup).'
---

# Accelerating Inference for Multilayer Neural Networks with Quantum Computers

## Quick Facts
- **arXiv ID:** 2510.07195
- **Source URL:** https://arxiv.org/abs/2510.07195
- **Reference count:** 40
- **Primary result:** First fully coherent quantum implementation of multilayer neural networks with non-linear activations, achieving polylogarithmic, quartic, or quadratic speedups

## Executive Summary
This paper presents the first fully coherent quantum implementation of multilayer neural networks with non-linear activation functions, specifically targeting ResNet-like architectures. The work introduces a modular "Vector-Encoding" framework that enables quantum matrix-vector arithmetic while maintaining coherence throughout network layers without intermediate measurements. The authors analyze inference complexity under three quantum data access regimes: with efficient quantum access to both inputs and weights (achieving polylogarithmic complexity), with weights only (providing quartic speedup), and without quantum access assumptions (yielding quadratic speedup). Key technical contributions include a novel quantum algorithm for multiplying arbitrary dense matrices with element-wise squared vectors without Frobenius norm dependence, and a QRAM-free block-encoding for 2D multi-filter convolutions.

## Method Summary
The paper develops a modular "Vector-Encoding" (VE) framework for quantum neural network inference, avoiding the exponential scaling issues of previous approaches. The method involves constructing quantum circuits for 2D convolutions using block-encoding techniques, implementing non-linear activation functions (specifically sigmoid) through polynomial approximation of the error function, and incorporating skip connections and layer normalizations to maintain norm preservation. The approach maintains coherence throughout the network layers without intermediate measurements or tomography, enabling provable speedups for classification tasks. The architecture is analyzed under three data access regimes with varying assumptions about quantum memory capabilities, providing a comprehensive complexity analysis that accounts for error propagation and norm stability challenges inherent to quantum implementations.

## Key Results
- First fully coherent quantum implementation of multilayer neural networks with non-linear activations
- Achieves polylogarithmic complexity in input dimension when quantum access is available to both inputs and weights
- Provides quartic speedup over classical methods when only weights have efficient quantum access
- Introduces novel quantum algorithm for multiplying dense matrices with element-wise squared vectors without Frobenius norm dependence
- Maintains norm preservation through residual blocks using skip connections and spectral normalization

## Why This Works (Mechanism)
The approach works by leveraging quantum superposition to perform parallel operations on all input components simultaneously, combined with block-encoding techniques to efficiently represent and manipulate large matrices. The modular VE framework allows complex operations to be broken down into simpler quantum primitives that can be composed coherently. Skip connections and normalization layers are incorporated to prevent exponential norm decay that would otherwise occur with repeated non-linear transformations. The polynomial approximation of non-linear functions enables their implementation using quantum arithmetic circuits while maintaining the overall unitary structure required for coherence.

## Foundational Learning
**Block-Encoding (Def 2)**: A unitary matrix representation of a linear operator that can be implemented with controlled operations - needed for efficiently representing large weight matrices in quantum circuits; quick check: verify that applying the block-encoding to a quantum state produces the expected linear transformation.

**Vector-Encoding (Def 3)**: A quantum state preparation technique that encodes classical vectors into quantum amplitudes - needed for initializing input data in quantum neural networks; quick check: confirm that measuring the quantum state yields the original classical vector with appropriate probabilities.

**QRAM Oracle (Def A.1/A.2)**: Quantum access to classical data with polylogarithmic complexity - needed for the asymptotic speedups claimed in Regimes 1 and 2; quick check: verify that the oracle provides access to data elements in O(polylog N) time complexity.

**Spectral Norm**: The maximum singular value of a matrix - needed for bounding the error propagation through network layers; quick check: compute spectral norms of example weight matrices to verify they remain bounded.

**Polynomial Approximation of erf**: Taylor series expansion of the error function - needed for implementing sigmoid activation in quantum circuits; quick check: compare polynomial approximation error against required precision bounds for small network instances.

## Architecture Onboarding

**Component Map:** QRAM Oracles → Block-Encoding → Vector-Encoding → Matrix-Vector Product → Non-linear Activation → Skip Connection → Normalization → Output

**Critical Path:** The critical path is the sequential application of quantum operations through each network layer: input encoding → convolution block-encoding → non-linear activation (polynomial approximation) → residual addition → normalization. Each layer's output becomes the next layer's input, with error accumulating multiplicatively.

**Design Tradeoffs:** The main tradeoff is between circuit depth and error tolerance. While maintaining coherence enables provable speedups, the polynomial degree for non-linear activation grows with network depth, increasing circuit complexity. Skip connections help with norm preservation but add overhead. The three regimes represent tradeoffs between data access assumptions and achievable speedups.

**Failure Signatures:** Norm decay (exponential decrease in vector magnitude through layers), approximation error accumulation (output deviates from classical expectation beyond epsilon), and oracle access violations (complexity exceeding polylogarithmic bounds). These manifest as measurement outcomes that don't match theoretical predictions or excessive resource requirements.

**First Experiments:**
1. Verify Vector-Encoding and Block-Encoding definitions by implementing basic primitives for small dimensions (N=4, 8) and checking output states.
2. Construct and test the General Skip Norm Block with polynomial approximation of erf to ensure norm preservation.
3. Assemble a shallow network (k=1 or 2) for Regime 2 and run inference to verify output state matches classical expectation within error bounds.

## Open Questions the Paper Calls Out

**Open Question 1:** Is it possible to coherently enact sequences of non-linear transformations without exponentially increasing circuit depth? The paper demonstrates that input-dependent unitary definitions force exponential scaling with layer count, but a formal proof of impossibility or a novel algorithmic solution remains open.

**Open Question 2:** Can a "practically passive" QRAM be physically realized? Current active QRAM architectures require energy scaling with problem dimension, negating quantum speedups. A robust, low-energy QRAM interface is needed to realize the asymptotic benefits in Regimes 1 and 2.

**Open Question 3:** Can quantum norm-preservation techniques inspire new classical algorithms? The paper's methods for tracking vector norms through residual blocks could theoretically improve classical inference algorithms, but this direction remains unexplored.

**Open Question 4:** Can Quantum Phase Estimation techniques enable coherent non-linearities without exponential depth? While QPE might solve the depth issue, it would likely trade it for prohibitive error-dependence, but this specific tradeoff hasn't been rigorously analyzed.

## Limitations
- Oracle complexity assumptions (QRAM with polylogarithmic access) are not hardware-implementable with current technology
- Error analysis assumes perfect oracle access and doesn't account for realistic noise models in near-term devices
- Physical error correction overhead (magic state distillation, routing) is omitted from complexity analysis

## Confidence
- **Theoretical framework validity:** High - mathematical proofs appear rigorous and well-structured
- **Practical implementation timelines:** Low - dependent on fault-tolerant quantum computers that don't yet exist
- **Error bound accuracy:** Medium - assumes ideal oracle access and doesn't model realistic quantum noise

## Next Checks
1. Implement and benchmark basic Vector-Encoding and Block-Encoding primitives for small matrices (N=4, 8) to verify claimed polylogarithmic scaling in practice
2. Simulate the General Skip Norm Block with polynomial approximation of erf to measure actual error accumulation versus theoretical bounds
3. Analyze spectral norm preservation properties by testing different weight matrix configurations to identify failure modes in norm decay