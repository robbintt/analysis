---
ver: rpa2
title: Fine-Tuning Large Multimodal Models for Automatic Pronunciation Assessment
arxiv_id: '2509.15701'
source_url: https://arxiv.org/abs/2509.15701
tags:
- score
- assessment
- accuracy
- pronunciation
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores fine-tuning Large Multimodal Models (LMMs)
  for Automatic Pronunciation Assessment (APA) across multiple granularities and aspects.
  Using Qwen2-Audio-7B-Instruct, the authors fine-tune the model on the Speechocean762
  dataset and a private corpus, combining LoRA adaptation with Simple Preference Optimization
  (SimPO) and cross-entropy loss.
---

# Fine-Tuning Large Multimodal Models for Automatic Pronunciation Assessment

## Quick Facts
- arXiv ID: 2509.15701
- Source URL: https://arxiv.org/abs/2509.15701
- Reference count: 0
- Key outcome: Fine-tuning Qwen2-Audio-7B-Instruct with LoRA and SimPO achieves competitive APA performance (PCC ~0.9 at word/sentence levels) but struggles with phoneme-level assessment (PCC ~0.38).

## Executive Summary
This paper explores fine-tuning Large Multimodal Models (LMMs) for Automatic Pronunciation Assessment (APA) across multiple granularities and aspects. Using Qwen2-Audio-7B-Instruct, the authors fine-tune the model on the Speechocean762 dataset and a private corpus, combining LoRA adaptation with Simple Preference Optimization (SimPO) and cross-entropy loss. The approach significantly outperforms zero-shot baselines, achieving competitive results with public and commercial systems at word and sentence levels, with PCC values reaching 0.9. However, phoneme-level assessment remains challenging. Notably, PCC and Spearman's rank Correlation Coefficient (SCC) show large discrepancies (PCC ~0.9, SCC ~0.6), suggesting SCC may be a more suitable evaluation metric for APA tasks. The findings highlight both the potential and limitations of LMMs in APA, particularly for fine-grained modeling and rank-aware evaluation.

## Method Summary
The authors fine-tune Qwen2-Audio-7B-Instruct using LoRA (rank=8) on the Speechocean762 dataset and a private corpus, combining SimPO with cross-entropy loss for parameter-efficient adaptation. Training data is generated through simulation, creating preference pairs by randomly adjusting scores by 2-4 points while maintaining consistency across granularities. The model is evaluated on phoneme, word, and sentence levels using PCC, SCC, and RMSE metrics, demonstrating competitive performance against zero-shot baselines and commercial systems at higher granularities while highlighting challenges at the phoneme level.

## Key Results
- Fine-tuned LMM achieves PCC ~0.9 at word and sentence levels, competitive with commercial APA systems
- Significant PCC-SCC discrepancy (PCC ~0.9 vs SCC ~0.6) suggests SCC better captures ordinal consistency for APA
- Phoneme-level assessment remains challenging (PCC ~0.38) despite improvements from fine-tuning
- LoRA+SimPO outperforms zero-shot baselines and achieves competitive results on single-granularity tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Parameter-efficient fine-tuning via LoRA adapts LMMs to APA tasks while preserving pretrained knowledge.
- **Mechism:** LoRA injects low-rank trainable matrices into target modules (rank=8 in this work), enabling the model to learn task-specific pronunciation scoring patterns without full model updates. The frozen backbone maintains general audio-language understanding while adapters specialize for assessment.
- **Core assumption:** The pretrained Qwen2-Audio model possesses sufficient acoustic-linguistic representations that can be redirected toward scoring objectives through low-rank perturbations.
- **Evidence anchors:** Fine-tuning significantly outperforms zero-shot settings and achieves competitive results on single-granularity tasks; related work confirms LoRA fine-tuning on Phi-4-multimodal for APA/MDD without complex joint training.

### Mechanism 2
- **Claim:** Simple Preference Optimization (SimPO) improves score calibration by enforcing relative ranking consistency without reference model overhead.
- **Mechanism:** SimPO trains on synthetic preference pairs where positive samples have scores 2-4 points higher than negatives. The loss function ($L = L_{SimPO} + \lambda L_{CE}$) combines preference margins ($\gamma=0.5$) with cross-entropy to maintain instruction-following while learning ordinal score relationships.
- **Core assumption:** Simulated preference pairs (via score perturbation with consistency propagation to higher granularities) adequately approximate human expert scoring preferences.
- **Evidence anchors:** All SimPO training data is generated through simulation; using only SimPO sometimes caused deviations from prompt instructions, so it's combined with cross-entropy loss.

### Mechanism 3
- **Claim:** Multi-granularity prompting enables unified assessment but degrades fine-grained performance due to output sequence complexity.
- **Mechanism:** A single prompt instructs simultaneous evaluation at sentence (5 aspects), word (3 aspects), and phoneme (1 aspect) levels. The model must generate structured output with correct formatting. This creates a length/complexity burden that disproportionately affects phoneme-level accuracy.
- **Core assumption:** The instruction-following capability of Qwen2-Audio can reliably parse and execute multi-level scoring instructions without aspect interference.
- **Evidence anchors:** Table 1 shows phoneme-level PCC at only 0.38/0.34 (PCC/SCC) vs. sentence-level accuracy at 0.74/0.70; experimental results show word-level accuracy, total score, and sentence-level prosody were significantly improved without predicting phoneme-level scores.

## Foundational Learning

- **Concept: Pearson vs. Spearman Correlation for Ordinal Scoring**
  - **Why needed here:** The paper reports PCC ≈ 0.9 but SCC ≈ 0.6, revealing that linear correlation alone overestimates model performance on ordinal scoring tasks where rank order matters more than score distances.
  - **Quick check question:** If a model predicts scores [8, 6, 4, 2] vs. ground truth [10, 7, 4, 1], which correlation captures the ordinal consistency vs. linear fit difference?

- **Concept: Preference Optimization (DPO → SimPO)**
  - **Why needed here:** SimPO removes DPO's reference model requirement by using average log-likelihood as the reward, enabling more scalable training on synthetic preference pairs for score calibration.
  - **Quick check question:** Why does SimPO add cross-entropy loss rather than using preference loss alone?

- **Concept: Multi-Granularity Scoring Cascades**
  - **Why needed here:** APA requires consistent scores across levels—phoneme errors should propagate to word scores, which should reflect in sentence totals. The paper's simulated data enforces this cascade during preference pair generation.
  - **Quick check question:** If phoneme scores change by +3, how should word and sentence total scores update to maintain consistency?

## Architecture Onboarding

- **Component map:**
  Audio Input → Qwen2-Audio-7B-Instruct (frozen backbone) → LoRA Adapters (rank=8, trainable) → Structured Text Output (sentence/word/phoneme scores) → SimPO + CE Loss for fine-tuning

- **Critical path:**
  1. Format training data with comprehensive prompts (reference text + phone sequence)
  2. Generate SimPO preference pairs via score perturbation with cascade consistency
  3. Fine-tune with LoRA (3 epochs, lr=1e-4, warmup 10%, batch=1 grad_accum=8)
  4. Evaluate using PCC, SCC, RMSE on held-out test set

- **Design tradeoffs:**
  - Multi-granularity vs. single-granularity: Multi-task training simplifies deployment but degrades phoneme-level accuracy (PCC drops from ~0.6 to ~0.38). Single-aspect training achieves competitive results (sentence fluency PCC 0.79 vs. 0.73).
  - PCC vs. SCC reporting: PCC shows stronger numbers but SCC better reflects ordinal ranking quality; report both.
  - SimPO data source: Synthetic preferences enable scalable training but may not capture expert score distribution tails.

- **Failure signatures:**
  - Phoneme-level PCC < 0.4: Indicates model struggling with fine-grained acoustic discrimination; consider single-granularity training or specialized phoneme models.
  - NaN for completeness scores: Severe label imbalance (only 8/2500 training samples with scores < 10); cannot learn this aspect without data augmentation.
  - Negative stress correlation (-0.01): Model failing on word-level stress scoring; requires additional balanced data (private data improves to 0.15).
  - SCC significantly lower than PCC: Model predictions have reasonable linear correlation but poor ordinal ranking consistency.

- **First 3 experiments:**
  1. Reproduce single-granularity baseline: Fine-tune on sentence-level accuracy only, compare PCC/SCC against multi-granularity setup to quantify interference effects.
  2. Ablate SimPO contribution: Train with LoRA only (no preference optimization) to isolate SimPO's calibration effect on score distribution alignment.
  3. Test phoneme-level with forced alignment inputs: Provide oracle phoneme boundaries from external aligner to determine whether failure is acoustic (representation) vs. segmentation (tokenization) related.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What architectural or training modifications could enable LMMs to achieve competitive phoneme-level pronunciation assessment performance?
- Basis in paper: "phoneme-level assessment remains challenging" and "Future work on LLMs should focus on enhancing phoneme-level modeling"
- Why unresolved: Fine-tuning with LoRA and SimPO yielded only slight phoneme-level improvements (PCC 0.38), far below word/sentence levels; the root cause of LMMs' difficulty with fine-grained phoneme scoring is not identified.
- What evidence would resolve it: Experiments comparing phoneme-level performance across different model architectures, tokenization strategies, or intermediate phoneme-aware pretraining objectives that achieve PCC/SCC comparable to word-level results (>0.5).

### Open Question 2
- Question: How can LMMs be trained or fine-tuned to improve ordinal consistency in pronunciation scoring, as measured by Spearman's rank Correlation Coefficient?
- Basis in paper: PCC reaches ~0.9 while SCC remains ~0.6, and authors state "SCC, which measures ordinal consistency without assuming linearity... may be a more appropriate metric for APA"
- Why unresolved: The discrepancy indicates models achieve high linear correlation but poor rank preservation; standard training objectives (cross-entropy, SimPO) do not explicitly optimize for ordinal consistency.
- What evidence would resolve it: Training experiments with rank-aware loss functions that close the gap between PCC and SCC (e.g., SCC approaching PCC levels) without degrading overall performance.

### Open Question 3
- Question: Why does multi-granularity joint assessment degrade performance compared to single-granularity single-aspect evaluation, and can this gap be closed?
- Basis in paper: Single-granularity tasks "significantly outperform" and "yield significantly better results than multi-granularity and multi-aspect assessment" (Table 3 vs Tables 1-2), but no explanation for this trade-off is provided.
- Why unresolved: The paper does not investigate whether the degradation stems from prompt complexity, output length, token interference, or capacity limitations in the LoRA-adapted model.
- What evidence would resolve it: Ablation studies isolating prompt length, output format complexity, and multi-task interference that identify the bottleneck and demonstrate methods to recover single-granularity performance in joint evaluation settings.

## Limitations

- Severe label imbalance for completeness scores (only 8/2500 training samples with scores < 10) makes this aspect essentially unlearnable
- Large PCC-SCC discrepancy (PCC ~0.9 vs SCC ~0.6) indicates linear correlation metrics significantly overstate model performance on ordinal scoring tasks
- Multi-granularity approach degrades phoneme-level performance to PCC ~0.38, suggesting the unified model cannot effectively handle fine-grained acoustic discrimination

## Confidence

**High Confidence:** The LoRA fine-tuning mechanism and its parameter-efficient adaptation of pretrained LMMs is well-established and reproducible. The SimPO+CE loss combination effectively addresses prompt compliance issues observed with SimPO alone.

**Medium Confidence:** The PCC-SCC discrepancy interpretation as evidence that SCC is more suitable for APA evaluation is compelling but requires domain-specific validation. The synthetic preference generation method appears reasonable but lacks direct comparison to human expert preferences.

**Low Confidence:** The claim that Qwen2-Audio-7B-Instruct is "the first LMM-based APA model to evaluate multiple granularities and aspects simultaneously" cannot be verified without comprehensive literature review. The exact phoneme sequence formatting requirements for optimal performance remain underspecified.

## Next Checks

1. **Single-Granularity Ablation Study:** Fine-tune separate models for each granularity (phoneme-only, word-only, sentence-only) and compare their PCC/SCC performance against the multi-granularity model to quantify the interference effect and determine optimal task decomposition.

2. **Human Preference Validation:** Generate a small set of synthetic preference pairs and have human experts score the same audio samples to measure the correlation between synthetic and human preference patterns, validating whether the simulation adequately captures expert scoring behavior.

3. **Phoneme-Level Oracle Test:** Replace the model's phoneme segmentation with oracle phoneme boundaries from a forced aligner and retrain the phoneme-level scorer to isolate whether performance degradation stems from acoustic representation limitations or segmentation/tokenization issues.