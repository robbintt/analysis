---
ver: rpa2
title: Ensemble based Closed-Loop Optimal Control using Physics-Informed Neural Networks
arxiv_id: '2510.18195'
source_url: https://arxiv.org/abs/2510.18195
tags:
- control
- system
- optimal
- ensemble
- signal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents a multistage ensemble framework for solving
  the Hamilton-Jacobi-Bellman (HJB) equation using physics-informed neural networks
  (PINNs) to enable optimal control of nonlinear systems. The approach addresses the
  computational challenges of traditional numerical solutions to the HJB equation
  by leveraging ensemble learning and automatic differentiation to approximate the
  optimal cost-to-go function and derive corresponding control signals.
---

# Ensemble based Closed-Loop Optimal Control using Physics-Informed Neural Networks

## Quick Facts
- arXiv ID: 2510.18195
- Source URL: https://arxiv.org/abs/2510.18195
- Reference count: 40
- Ensemble PINN framework solves HJB equation for optimal control without stabilizer terms

## Executive Summary
This work presents a multistage ensemble framework for solving the Hamilton-Jacobi-Bellman (HJB) equation using physics-informed neural networks (PINNs) to enable optimal control of nonlinear systems. The approach addresses computational challenges of traditional numerical solutions by leveraging ensemble learning and automatic differentiation to approximate optimal cost-to-go functions and derive corresponding control signals. The method successfully demonstrates closed-loop control of a two-state continuous nonlinear system with infinite time horizon, handling noisy, perturbed system states and varying initial conditions.

## Method Summary
The method involves training an ensemble of PINNs on both boundary data and HJB equation residuals, using a warm-start phase followed by HJB-PINN loss optimization. Three control strategies are evaluated: individual control for each ensemble member, mean control for all members, and outlier-excluding mean control. The framework uses automatic differentiation to compute gradients of the learned cost function for deriving optimal control signals. Implementation requires generating analytical data on a uniform mesh, implementing the two-stage training process (warm-start then HJB loss), and running closed-loop simulations with the three control policies.

## Key Results
- Ensemble approach stabilizes system dynamics to zero even with perturbed initial conditions
- MSE between analytical and learned solutions is effectively zero in most regions
- Outlier-excluding mean control policy maintains stability while excluding control signals from uncontrollable systems
- Method achieves optimal control without relying on stabilizer terms or finite-horizon approximations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A multistage training strategy allows the HJB equation to be solved without explicit stabilizer terms.
- **Mechanism:** The framework decouples learning into two phases: first, a warm-start phase conditions network weights using sub-optimal cost-to-go data (MSE loss); second, a fine-tuning phase enforces the HJB physics-informed loss. This initialization prevents the optimizer from settling into inescapable local minima caused by the non-unique solutions of the steady-state HJB equation.
- **Core assumption:** The sub-optimal data generated offline (e.g., via analytical approximations or ODE solvers) provides a sufficiently accurate basin of attraction for the true optimal solution.
- **Evidence anchors:**
  - [abstract] "Our framework does not use stabilizer terms..."
  - [section 4.3.6] "Warm-starting the network weights on sub-optimal data helps the network not start at a local minima within the manifold... that it cannot escape."
  - [corpus] (Weak/General) Neighbors like *2505.21842* discuss PINNs for HJB but often rely on different stabilization or finite-horizon approximations; this paper's specific mechanism is the removal of stabilizers via warm-starting.
- **Break condition:** If the initial sub-optimal data is too inaccurate or the loss landscape is too flat, the warm-start may prime the network in a region that converges to a non-optimal steady state.

### Mechanism 2
- **Claim:** Automatic differentiation of the network output yields a differentiable control signal for closed-loop feedback.
- **Mechanism:** A feedforward neural network approximates the scalar Value function (Cost-to-Go, $J$). By construction, the optimal control signal $u$ is a function of the costate $\lambda$ (the gradient of $J$ with respect to the state $x$). The framework utilizes automatic differentiation (autograd) to compute this gradient $\nabla_x J$ directly from the network, allowing the calculation of instantaneous optimal control $u$ without training a separate "actor" network.
- **Core assumption:** The network learns a smooth representation of the Value function, such that $\nabla_x J$ is well-defined and stable (hence the use of Tanh activation).
- **Evidence anchors:**
  - [abstract] "...learn the optimal cost-to-go, and subsequently the corresponding optimal control signal..."
  - [section 4.3.4] "Modern automatic differentiation libraries... allow direct computation of $\nabla_x \hat{J}$."
  - [corpus] *2503.02171* ("Is Bellman Equation Enough...") questions if Bellman residuals alone suffice for control, highlighting the necessity of this gradient mechanism to actually derive the policy.
- **Break condition:** If the activation function is non-smooth (e.g., ReLU) or the network is overfitted, the gradient $\nabla_x J$ will be noisy or zero, causing erratic control signals.

### Mechanism 3
- **Claim:** Ensemble averaging with outlier exclusion maintains system trajectory under state perturbation.
- **Mechanism:** Instead of relying on a single PINN, the system queries $N$ independent networks. The control signal is the mean of the ensemble, but the framework applies the Multivariate Chauvenet Criterion to identify and exclude outlier states (and thus control signals) that deviate too far from the ensemble mean. This filters out divergence caused by random noise or poor local network performance.
- **Core assumption:** The majority of the ensemble remains in-distribution and correct; errors are uncorrelated across ensemble members.
- **Evidence anchors:**
  - [abstract] "...robust control even in the presence of perturbations... validated through numerical experiments..."
  - [section 4.4.3] "...detect outliers... states are flagged as outliers if $P(x^{(j)}) < C$."
  - [corpus] *2506.00731* ("iPINNER") supports the general efficacy of combining ensembles (Kalman filters) with PINNs to handle noisy observations.
- **Break condition:** If the perturbation is systemic rather than random (affecting all networks identically) or if the perturbation pushes the system outside the training domain $\Omega$ for the majority of the ensemble, the mean control will fail.

## Foundational Learning

- **Concept:** Hamilton-Jacobi-Bellman (HJB) Equation
  - **Why needed here:** This is the governing PDE of the system. Unlike supervised learning where labels are data, here the "physics" is the HJB residual, which must equal zero for an optimal trajectory.
  - **Quick check question:** Can you explain why the time derivative term $\dot{V}$ vanishes in the HJB equation when dealing with an infinite time horizon?

- **Concept:** Costate ($\lambda$) and Pontryagin's Minimum Principle
  - **Why needed here:** The paper does not output $u$ directly; it outputs $J$. You must understand that $\lambda = \nabla_x J$ acts as the "sensitivity" of the cost to the state, which is mathematically required to compute the optimal control $u^*$.
  - **Quick check question:** How does the relationship $\lambda = \nabla_x J$ allow a network learning a scalar cost to generate a vector control signal?

- **Concept:** Stiffness and Spectral Bias in PINNs
  - **Why needed here:** The paper explicitly avoids "stabilizer terms" and uses specific hyperparameters (Tanh, shallow layers) to ensure convergence. Understanding why neural networks struggle with high-frequency or discontinuous PDE residuals is key to debugging this architecture.
  - **Quick check question:** Why does the paper recommend Tanh over ReLU activation functions for this specific control problem?

## Architecture Onboarding

- **Component map:**
  1. **Input:** State vector $x \in \mathbb{R}^n$.
  2. **Backbone:** Feedforward Network (Input $\to$ 2 Hidden Layers $\to$ Scalar $\hat{J}$).
  3. **Differentiator:** `torch.autograd` (Computes $\nabla_x \hat{J}$).
  4. **Control Law:** Algebraic equation using $\nabla_x \hat{J}$ to compute $u$.
  5. **Ensemble Manager:** Aggregates $N=20$ networks; applies Chauvenet outlier filter.

- **Critical path:**
  1. Generate sub-optimal trajectory data (offline).
  2. **Warm-start:** Train all $N$ networks to MSE convergence on this data.
  3. **Fine-tune:** Switch loss to HJB Physics-Informed Loss + Boundary Loss (AdamW optimizer).
  4. **Inference:** Run closed-loop simulation; at each step $t$, compute $\hat{u}$ for all networks, filter outliers, apply mean control $\bar{u}$.

- **Design tradeoffs:**
  - **Stabilizer-free vs. Warm-start:** The paper trades off "online" stability constraints for "offline" data generation effort to simplify the loss landscape.
  - **Ensemble vs. Singular:** High robustness (Ensemble) vs. computational speed (Singular).
  - **Network Depth:** Limited to 2 hidden layers (shallow) to prevent vanishing gradients when differentiating for the costate.

- **Failure signatures:**
  - **Loss Explosion:** If HJB loss terms are not normalized or if boundary conditions are weak, the loss may diverge.
  - **Flat Control:** If the network uses ReLU, gradients in the hidden layers may die, resulting in $\nabla_x \hat{J} \approx 0$, causing the controller to output zero regardless of state.
  - **Trajectory Drift:** If the initial state $x_0$ is far outside the training domain $\Omega$, the PINN will hallucinate cost values, leading to unstable control.

- **First 3 experiments:**
  1. **Sanity Check (No Control vs. Individual):** Run the system with $u=0$ vs. a single trained PINN controller to verify the network has learned a valid stabilizing policy (replicate Figure 1c).
  2. **Noise Robustness (Ensemble vs. Singular):** Inject Gaussian noise ($\mu=0, \sigma=0.01$) into state observations. Compare trajectory variance between a single network controller and the Ensemble Mean controller.
  3. **Ablation on Warm-Start:** Train a "cold-start" model (random initialization) directly on the HJB loss. Compare convergence time and success rate against the warm-started model to validate the necessity of the two-stage process.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the proposed framework be successfully extended to complex control benchmarks, such as cart-pole or inverted pendulum systems?
- Basis in paper: [explicit] The Conclusion states, "Future work possibilities include extending this framework to more complex problems such as cart pole and inverted pendulum."
- Why unresolved: The current experiments are limited to a two-state continuous nonlinear system; the framework has not yet been validated on the higher-dimensional or unstable dynamics typical of classic control benchmarks.
- What evidence would resolve it: Successful stabilization and cost minimization results when applying the ensemble PINN method to Gymnasium environments like "CartPole-v1" or "Pendulum-v1".

### Open Question 2
- Question: How sensitive are the ensemble control policies to increasing magnitudes of noise and state perturbations?
- Basis in paper: [explicit] The Conclusion notes, "a sensitivity study would shed light on the extent to which the three control policies present in this work can handle increasing amounts of noise in the data."
- Why unresolved: The experiments only tested perturbations drawn from a specific random normal distribution ($\mu=0.0, \sigma=0.01$), leaving the breaking points of the controller unknown.
- What evidence would resolve it: A parametric study varying the standard deviation of the noise and reporting the resulting control performance (e.g., MSE of the state trajectory or Hamiltonian).

### Open Question 3
- Question: Is the proposed method viable without the warm-start phase that relies on pre-computed analytical data?
- Basis in paper: [inferred] Section 5.1 details that the training process "begins with a warm-start phase, where each network... is pre-trained on suboptimal cost-to-go data." The paper does not demonstrate if the HJB PINN loss alone is sufficient to converge from a random initialization.
- Why unresolved: It is unclear if the method can solve the control problem from scratch for systems where no analytical approximation (even a sub-optimal one) is available to seed the network weights.
- What evidence would resolve it: Comparing convergence rates and final control accuracy between randomly initialized networks and warm-started networks on the same dynamical system.

## Limitations
- Method relies on analytical suboptimal cost-to-go data for warm-starting, requiring prior knowledge of system dynamics
- Chauvenet criterion assumes uncorrelated errors across ensemble members, which may fail under systemic perturbations
- Validated only on a two-state nonlinear system with infinite time horizon, limiting generalizability to higher dimensions or finite-horizon problems

## Confidence
- **High confidence:** The ensemble framework successfully stabilizes trajectories under noise (Section 4.4.2 results are quantitative and reproducible)
- **Medium confidence:** The warm-start mechanism's necessity is theoretically sound but the specific suboptimal data generation procedure has unspecified parameters (Tsit5 solver settings, initial conditions)
- **Low confidence:** Claims about computational efficiency compared to traditional numerical HJB solvers are not directly benchmarked

## Next Checks
1. Test the outlier exclusion mechanism under correlated perturbations (e.g., systematic drift) to verify its robustness assumptions
2. Evaluate performance with ReLU activation functions to quantify the importance of smooth gradients for control stability
3. Implement the framework on a higher-dimensional system (3+ states) to assess scalability and computational requirements