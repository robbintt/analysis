---
ver: rpa2
title: 'MMSI-Video-Bench: A Holistic Benchmark for Video-Based Spatial Intelligence'
arxiv_id: '2512.10863'
source_url: https://arxiv.org/abs/2512.10863
tags:
- spatial
- video
- reasoning
- wang
- benchmark
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MMSI-Video-Bench is a fully human-annotated benchmark designed
  to evaluate video-based spatial intelligence in multimodal large language models.
  It operationalizes a four-level framework covering Perception, Planning, Prediction,
  and Cross-Video Reasoning through 1,106 questions grounded in 1,278 video clips
  from 25 diverse datasets and in-house recordings.
---

# MMSI-Video-Bench: A Holistic Benchmark for Video-Based Spatial Intelligence

## Quick Facts
- arXiv ID: 2512.10863
- Source URL: https://arxiv.org/abs/2512.10863
- Reference count: 40
- Primary result: Human-AI performance gap of ~60% on video-based spatial intelligence benchmark

## Executive Summary
MMSI-Video-Bench is a fully human-annotated benchmark designed to evaluate video-based spatial intelligence in multimodal large language models. It operationalizes a four-level framework covering Perception, Planning, Prediction, and Cross-Video Reasoning through 1,106 questions grounded in 1,278 video clips from 25 diverse datasets and in-house recordings. Each question is carefully designed and reviewed by 3D vision experts with explanatory rationales to ensure precise, unambiguous grounding. The benchmark reveals a striking human-AI performance gap: current models perform near chance, with the best reasoning model lagging humans by nearly 60%. Fine-grained error analysis exposes systematic failures in geometric reasoning, motion grounding, long-horizon prediction, and cross-video correspondence. Neither 3D spatial cues nor chain-of-thought prompting yields meaningful gains. The benchmark establishes a challenging testbed for advancing video-based spatial reasoning capabilities.

## Method Summary
The benchmark consists of 1,106 multiple-choice questions (4-6 options each) grounded in 1,278 video clips from 25 public datasets plus 140 in-house recordings. Videos are uniformly sampled to 50 frames for evaluation, with timestamp metadata preserved. Expert annotators (3D vision specialists) create questions across five categories: Spatial Construction, Motion Understanding, Planning, Prediction, and Cross-Video Reasoning. Each question includes ground-truth answer and explanatory rationale. Evaluation uses exact-match accuracy on two tracks: Uniform-50 (50 uniformly sampled frames) and Sufficient-Coverage (full annotated frame set). Proprietary models are accessed via official APIs; open-source models are evaluated on 8×A100 GPUs using standardized prompt templates.

## Key Results
- Human performance: 96.4% accuracy (near ceiling)
- Best model performance: 35.3% accuracy (Qwen2.5-VL-72B-Chat)
- Random baseline: 24.1% accuracy
- Performance gap: ~60% between humans and best models
- 3D spatial cues and chain-of-thought prompting yield <1% improvement

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A four-level hierarchical framework exposes progressive capability dependencies in video-based spatial intelligence.
- Mechanism: Tasks are organized into Perception → Planning → Prediction → Cross-Video Reasoning. Lower-level failures cascade upward; models cannot predict or plan without first constructing accurate spatial representations from partial observations.
- Core assumption: Spatial intelligence is hierarchical, not modular—weakness in any layer propagates.
- Evidence anchors:
  - [abstract] "operationalizes a four-level framework, Perception, Planning, Prediction, and Cross-Video Reasoning"
  - [section 3.1] "After understanding spatio-temporal information, the next step is decision-making based on video understanding"
  - [corpus] Weak corpus signal on hierarchy specifically; related benchmarks (MMSI-Bench, SpatialScore) focus on spatial evaluation but not this exact decomposition.

### Mechanism 2
- Claim: Uniform temporal sampling captures broader evidence than consecutive sampling, which is critical for reasoning-intensive spatial tasks.
- Mechanism: Spatial reasoning often requires integrating information from temporally distant frames (e.g., "How does the dog move when out of sight?"). Consecutive segments miss key events; uniform sampling increases probability of capturing them.
- Core assumption: Critical spatial information is distributed non-locally across video timeline.
- Evidence anchors:
  - [section 5] "Uniform sampling substantially outperforms consecutive sampling, demonstrating that broad temporal coverage is essential"
  - [section 5] "AKS fails to provide improvements... key frames cannot be directly determined from semantic similarity alone"
  - [corpus] No direct corpus corroboration on sampling strategy effects for spatial benchmarks.

### Mechanism 3
- Claim: Spatial fine-tuning on narrow datasets creates capabilities that do not generalize to diverse, reasoning-intensive benchmarks.
- Mechanism: Models like Spatial-MLLM and VLM3R optimize for specific spatial datasets but may lose instruction-following or general reasoning ability, leading to degradation on holistic benchmarks (VLM3R: 4.97% vs. base 28.48%).
- Core assumption: Generalization requires exposure to diverse spatial reasoning patterns, not just architectural modifications.
- Evidence anchors:
  - [section 4.3] "both Spatial-MLLM and VLM3R suffer from degradation, particularly in instruction-following ability"
  - [section 7.1] "models fail to effectively leverage the 3D spatial cues... ignored or not correctly associated with video content"
  - [corpus] SpatialScore notes fragmented spatial evaluations, supporting diversity argument.

## Foundational Learning

- Concept: **Geometric Reasoning over Video Frames**
  - Why needed here: The dominant error type (Geometric Reasoning Error) in Spatial Construction tasks; models must infer front/behind, near/far relations across frames.
  - Quick check question: Given two frames from different camera positions, can you determine if object A is to the left or right of object B from the camera's perspective?

- Concept: **Cross-Frame Identity Persistence**
  - Why needed here: ID Mapping Errors occur when models fail to track entities through occlusion or rapid motion; required for Motion Understanding and Cross-Video Reasoning.
  - Quick check question: If an object leaves frame and re-enters, how do you know it's the same object?

- Concept: **Prompt-Evidence Alignment**
  - Why needed here: Planning/Prediction tasks require connecting hypothetical conditions or goals to visual evidence; Prompt Alignment Error is a major failure mode.
  - Quick check question: Given a prompt "If I turn left at the door, what will I see?", how do you map the conditional to specific frames?

## Architecture Onboarding

- Component map: Video frame encoder + timestamp embedding -> Latent geometric understanding -> Frame sampling strategy + cross-frame attention -> LLM backbone for multi-step spatial inference -> Multiple-choice answer with optional rationale

- Critical path: Frame sampling → Spatial feature extraction → Cross-frame relation modeling → Reasoning over combined representation → Answer selection. Failure at any stage propagates; Geometric Reasoning breaks at relation modeling, Prompt Alignment breaks at reasoning.

- Design tradeoffs:
  - More frames → higher compute, potential redundancy (Sufficient-Coverage underperforms Uniform-50 for many models)
  - Specialized spatial modules → may harm general instruction-following (VLM3R degradation)
  - External 3D cues (VGGT) → adds noise if reconstruction fails on complex/dynamic scenes

- Failure signatures:
  - Near-chance accuracy → model not using video evidence (language prior dominance)
  - High Geometric Reasoning Error → spatial representation failure
  - High Prompt Alignment Error → reasoning module not integrating visual evidence with task constraints
  - Performance drop with more frames → redundancy overwhelming attention mechanism

- First 3 experiments:
  1. Ablate frame count (1, 10, 50, 100) with uniform sampling to calibrate temporal coverage needs for your model architecture.
  2. Evaluate base vs. spatially fine-tuned checkpoints on a held-out diverse spatial subset to measure generalization gap before deployment.
  3. Test error-type stratification: run 100-sample diagnostic per category (Spatial Construction, Motion, Planning, Prediction, Cross-Video) to identify which module to target for improvement.

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset Coverage: The specific composition of the 140 in-house videos is not disclosed, limiting understanding of how well the benchmark generalizes to real-world video scenarios.
- Generalization Claims: Performance degradation for spatially fine-tuned models is observed, but the evaluation doesn't definitively distinguish between overfitting to spatial tasks vs. loss of general instruction-following ability.
- Temporal Sampling Optimization: The evaluation doesn't explore intermediate sampling strategies that might better balance coverage and efficiency than the tested extremes.

## Confidence

- **High Confidence**: The benchmark construction methodology (expert annotation, hierarchical framework, diverse dataset coverage) is clearly specified and reproducible. The human-AI performance gap is well-established through direct comparison.
- **Medium Confidence**: The claim that spatial intelligence is hierarchical is supported by task organization and error propagation patterns, but lacks direct corpus evidence of hierarchical dependencies in MLLM performance.
- **Medium Confidence**: The claim that uniform sampling outperforms alternatives is demonstrated empirically but lacks theoretical grounding or exploration of why this strategy works for spatial reasoning specifically.
- **Low Confidence**: The conclusion that spatial fine-tuning harms generalization is based on two specific models. The mechanism (loss of instruction-following) is plausible but not definitively proven across different fine-tuning approaches.

## Next Checks

1. **Error Type Validation**: Conduct a blind error categorization study where three independent annotators label model errors from the same 100-sample diagnostic sets. Calculate inter-annotator agreement to validate the error taxonomy and identify potential conflation between categories.

2. **Fine-tuning Ablation Study**: Evaluate spatially fine-tuned models on a held-out diverse spatial subset and on non-spatial reasoning tasks. This would distinguish whether performance degradation stems from overfitting to spatial patterns or loss of general instruction-following capability.

3. **Sampling Strategy Optimization**: Test intermediate temporal sampling strategies (e.g., dense sampling with attention-based frame selection, or curriculum sampling from coarse to fine temporal coverage) to determine if better trade-offs exist between coverage, efficiency, and spatial reasoning performance.