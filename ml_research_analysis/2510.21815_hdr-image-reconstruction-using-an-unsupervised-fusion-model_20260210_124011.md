---
ver: rpa2
title: HDR Image Reconstruction using an Unsupervised Fusion Model
arxiv_id: '2510.21815'
source_url: https://arxiv.org/abs/2510.21815
tags:
- image
- images
- fusion
- range
- dynamic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of reconstructing High Dynamic
  Range (HDR) images from multiple Low Dynamic Range (LDR) exposures. It proposes
  an unsupervised deep learning model that learns to fuse under- and overexposed images
  without requiring ground-truth HDR data.
---

# HDR Image Reconstruction using an Unsupervised Fusion Model

## Quick Facts
- **arXiv ID**: 2510.21815
- **Source URL**: https://arxiv.org/abs/2510.21815
- **Reference count**: 26
- **Primary result**: Unsupervised deep learning model achieves MEF-SSIM of 0.9921 for HDR reconstruction from LDR exposure pairs

## Executive Summary
This paper proposes an unsupervised deep learning approach for reconstructing High Dynamic Range (HDR) images from multiple Low Dynamic Range (LDR) exposures. The core innovation is a convolutional neural network that learns to fuse under- and overexposed images without requiring ground-truth HDR data. The model predicts per-pixel weight maps to optimally combine complementary exposure information, using a customized weighted Structural Similarity Index Measure (SSIM) loss function with a γ parameter derived from perceptual attributes like variance and gradient.

## Method Summary
The method uses a VGG16-inspired encoder-decoder CNN architecture where inputs are converted to grayscale and stacked as a 2-channel tensor. The encoder extracts hierarchical features through 10 convolutional layers with batch normalization and ReLU activations, while the decoder generates per-pixel weight maps through 10 convolutional layers with upsampling. These weights are applied to the original RGB exposures via weighted summation to produce the fused HDR image. The key innovation is the weighted SSIM loss with a γ parameter computed from local variance and gradient, enabling the network to learn optimal fusion without ground-truth supervision.

## Key Results
- Hybrid variance-gradient γ formulation achieves highest MEF-SSIM score of 0.9921
- Outperforms other γ configurations including variance-only (0.9566) and well-exposedness-only (0.8267)
- Demonstrates superior detail preservation and structural consistency compared to traditional fusion methods
- Validated on SICE dataset using 250×250 patch extraction from under/overexposed image pairs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Per-pixel adaptive weight maps enable optimal fusion of complementary exposure information without ground-truth supervision
- Mechanism: Encoder-decoder CNN predicts spatially-varying weight maps W_n for each input exposure, applied via weighted summation to selectively preserve well-exposed regions
- Core assumption: Network can learn to identify informative regions through gradient signal from weighted SSIM loss
- Break condition: Low exposure diversity between inputs prevents weight maps from exploiting complementary information

### Mechanism 2
- Claim: Weighted SSIM loss with γ parameter provides trainable signal balancing structural fidelity between under and overexposed inputs
- Mechanism: Loss computes SSIM between fused output and each input separately, weighting contributions by γ_w based on local variance
- Core assumption: Local variance correlates with perceptual informativeness and structural detail worth preserving
- Break condition: Mis-specified γ (e.g., well-exposedness alone) over-emphasizes smooth luminance at cost of texture detail

### Mechanism 3
- Claim: Hybrid variance-gradient γ formulation outperforms single-attribute formulations by combining global contrast sensitivity with local edge detection
- Mechanism: Multiplicatively combines variance and gradient, then normalizes to ensure high γ values only when both agree a region is informative
- Core assumption: Variance and gradient provide complementary cues that better approximate human perception than either alone
- Break condition: High noise (variance from noise) or excessive smoothing (low gradient) produces unreliable γ values

## Foundational Learning

- **Encoder-Decoder CNN Architectures**: Understanding skip connections, feature pyramid compression, and decoder upsampling is essential for debugging weight map quality. Quick check: Can you explain why decoder reverses encoder filter sizes and what spatial information is lost at each pooling layer?

- **Structural Similarity Index Measure (SSIM)**: Core loss function is modified SSIM; understanding luminance, contrast, and structure components explains why standard SSIM fails for multi-exposure inputs. Quick check: Why does SSIM compare structural similarity rather than pixel-wise error, and how does this relate to perceptual quality?

- **Multi-Exposure Fusion Fundamentals**: Paper builds on classical MEF with adaptive weights; understanding well-exposedness, gradient-based weighting, and their limitations motivates deep learning approach. Quick check: Given underexposed image preserving bright regions and overexposed image preserving dark regions, how would you manually design a fusion rule?

## Architecture Onboarding

- **Component map**: Input (2 grayscale images, 250×250 patches) -> Encoder (10 conv layers, 64→512 filters) -> Decoder (10 conv layers, 512→64 filters) -> Output (Softmax weight maps W_1, W_2) -> Fusion (weighted sum of RGB inputs) -> Loss (weighted SSIM with γ)

- **Critical path**: 1) Input preprocessing and patch extraction 2) Encoder feature extraction at multiple scales 3) Decoder weight map generation with softmax normalization 4) γ computation from input exposures 5) Weighted SSIM loss calculation and backpropagation

- **Design tradeoffs**: Grayscale encoder reduces computation but loses color cues for weight prediction; unsupervised training eliminates need for HDR ground truth but relies entirely on loss design quality; static scene assumption avoids ghosting but limits real-world applicability

- **Failure signatures**: Over-smoothed outputs indicate γ using well-exposedness alone (MEF-SSIM ~0.83); lost texture detail suggests γ using variance alone (MEF-SSIM ~0.96); bright region saturation indicates weight maps not suppressing overexposed regions properly

- **First 3 experiments**: 1) Baseline reproduction: Train with variance-only γ, verify MEF-SSIM ~0.9566 2) γ ablation: Compare all six γ formulations on same test split, expect variance-gradient to achieve ~0.99 3) Failure case analysis: Test on low dynamic range and high-noise images, document where hybrid γ degrades

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How can the unsupervised fusion model be adapted to handle dynamic scenes with motion between exposures to effectively suppress ghosting artifacts?
- **Basis in paper**: Authors explicitly state in Introduction they "focus on static scenes where such motion artifacts are absent"
- **Why unresolved**: Current methodology assumes perfect alignment; any movement would result in ghosting that weighted fusion logic does not address
- **What evidence would resolve it**: Modification (e.g., adding optical flow or attention masks) yielding high MEF-SSIM scores on moving subject datasets without double edges

### Open Question 2
- **Question**: Does converting input images to grayscale for the encoder impair the model's ability to reconstruct accurate chromatic information in the final HDR output?
- **Basis in paper**: Section 3.1.1 notes inputs are "converted to grayscale to reduce computational cost" while fusion operates on RGB channels
- **Why unresolved**: Unclear if ignoring color information during feature extraction and weight prediction leads to color bleeding or loss of saturation
- **What evidence would resolve it**: Ablation studies comparing grayscale encoder against RGB encoder to quantify differences in color accuracy metrics

### Open Question 3
- **Question**: Would replacing handcrafted γ formulations with a learnable attention mechanism yield superior reconstruction fidelity compared to the tested "variance + gradient" combination?
- **Basis in paper**: Authors manually derive γ from three perceptual attributes and conclude "variance–gradient" is best, but don't test if network could learn optimal fusion metric independently
- **Why unresolved**: Optimal weighting strategy might be complex non-linear function not captured by simple combinations of tested perceptual priors
- **What evidence would resolve it**: Experiments replacing explicit γ calculation with convolutional attention block demonstrating if learned weight map can outperform 0.9921 MEF-SSIM score

## Limitations
- Assumes static scenes without motion between exposures, excluding common ghosting artifacts
- Performance relies on specific kernel sizes for local variance/gradient calculations which are unspecified
- Claims of "high-quality HDR reconstruction" lack perceptual studies and comparison to state-of-the-art supervised methods
- May fail when input exposures have low dynamic range or high noise content

## Confidence
- **High Confidence**: Architectural design (encoder-decoder CNN producing weight maps) is well-specified and theoretically sound
- **Medium Confidence**: Empirical results showing hybrid variance-gradient outperforming other γ formulations are internally consistent but lack external validation
- **Low Confidence**: Claim of "high-quality HDR reconstruction" not substantiated by perceptual studies or comparison to supervised methods

## Next Checks
1. **Dataset generalization test**: Evaluate pre-trained model on different HDR dataset (e.g., HDR+, Adobe HDR) to verify MEF-SSIM performance is not dataset-specific
2. **Ground-truth validation**: Obtain small set of ground-truth HDR images and measure reconstruction error (PSNR, HDR-VDP-2) to complement unsupervised MEF-SSIM metric
3. **Exposure diversity robustness**: Systematically vary exposure ratio between input images (1-stop, 3-stop, 5-stop differences) and document MEF-SSIM degradation to establish operational limits