---
ver: rpa2
title: Latent Stochastic Interpolants
arxiv_id: '2506.02276'
source_url: https://arxiv.org/abs/2506.02276
tags:
- latent
- stochastic
- generative
- space
- encoder
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Latent Stochastic Interpolants (LSI), a framework
  that extends Stochastic Interpolants to jointly learn encoder, decoder, and generative
  models in a latent space. The key innovation is deriving an Evidence Lower Bound
  (ELBO) in continuous time that enables end-to-end training while preserving SI's
  flexibility and likelihood control.
---

# Latent Stochastic Interpolants

## Quick Facts
- **arXiv ID**: 2506.02276
- **Source URL**: https://arxiv.org/abs/2506.02276
- **Reference count**: 40
- **Primary result**: Achieves FID 3.76 at 128×128 resolution while reducing sampling FLOPs by up to 73.6% vs. observation-space diffusion models

## Executive Summary
Latent Stochastic Interpolants (LSI) introduces a framework that extends Stochastic Interpolants to jointly learn encoder, decoder, and generative models in a latent space. The key innovation is deriving an Evidence Lower Bound (ELBO) in continuous time that enables end-to-end training while preserving SI's flexibility and likelihood control. LSI constructs a diffusion bridge between a prior and an encoder-defined posterior, allowing simulation-free training. Experiments on ImageNet show LSI achieves FID scores comparable to observation-space models while requiring significantly fewer FLOPs during sampling.

## Method Summary
LSI jointly trains an encoder that maps images to latent codes, a latent generative model that evolves these codes through a diffusion bridge, and a decoder that reconstructs images from latents. The method derives a continuous-time ELBO that permits simulation-free training by constructing a variational posterior using Doob's h-transform between a fixed prior and the encoded data point. This allows the model to sample latent trajectories without expensive numerical SDE integration during training. The architecture partitions parameters into encoder, decoder, and latent SI model, with sampling shifted to the lower-dimensional latent space for computational efficiency.

## Key Results
- Achieves FID 3.76 at 128×128 resolution on ImageNet, comparable to observation-space diffusion models
- Reduces sampling FLOPs by up to 73.6% compared to observation-space models
- Joint training improves performance over independently trained components, with optimal FID achieved at specific β values balancing reconstruction and generative terms
- Supports classifier-free guidance and flexible sampling with controllable diversity

## Why This Works (Mechanism)

### Mechanism 1
Joint optimization of encoder, decoder, and latent generative model is enabled by deriving a continuous-time ELBO that permits simulation-free training. The paper constructs a variational posterior using a Diffusion Bridge between a fixed prior $z_0$ and the encoded data point $z_1$. By assuming a linear SDE for the bridge, the conditional density $p(z_t|z_1, z_0)$ becomes Gaussian, allowing $z_t$ to be sampled in closed form rather than requiring expensive numerical integration during training.

### Mechanism 2
Computational efficiency during sampling is achieved by shifting the iterative generative process to a lower-dimensional latent space. The architecture partitions parameters into Encoder (E), Decoder (D), and Latent SI model (L). During sampling, E is unused, D runs once, but L runs $N$ times. Since L operates on compressed latents with fewer channels than observation-space models, FLOPs per step are significantly lower.

### Mechanism 3
Joint training aligns the latent space specifically for the generative process, outperforming fixed pre-trained representations. The loss function includes a weighting term $\beta$ that balances reconstruction against the generative flow matching term. When $\beta > 0$, gradients from the generative term backpropagate into the encoder, forcing the latent manifold to deform into a shape that is easier for the interpolant to traverse.

## Foundational Learning

- **Concept: Stochastic Differential Equations (SDEs) & Wiener Process**
  - Why needed: The entire generative process is defined via an SDE. Understanding drift ($h_t$) and dispersion ($\sigma_t$) terms is required to grasp how noise is injected and removed.
  - Quick check: How does the dispersion coefficient $\sigma_t$ affect the "straightness" of the trajectory in Eq. 12?

- **Concept: Variational Inference & ELBO**
  - Why needed: LSI is fundamentally an optimization of an Evidence Lower Bound. You must understand why we maximize a lower bound rather than the likelihood directly.
  - Quick check: In Eq. 3, which term encourages the latent space to capture information, and which term regularizes the path dynamics?

- **Concept: Diffusion Bridges (Doob's h-transform)**
  - Why needed: This is the mathematical trick used to condition a stochastic process to hit a specific endpoint ($z_1$). It is the mechanism allowing the paper to define a path between observed data and a prior without simulation.
  - Quick check: Why does Eq. 5 add a term involving $\nabla \ln p(z_1|z_t)$ to the standard SDE dynamics?

## Architecture Onboarding

- **Component map:** Encoder (2 downsample conv groups + 2 downsamples → 16×16 latent) → Latent SI Model (16 self-attention transformer blocks at 16×16) → Decoder (2 upsample groups → image)

- **Critical path:**
  1. Training: Sample $x_1$ → Encode to $z_1$ → Sample $z_0$ (Prior) → Sample $z_t$ (Interpolant Eq. 12) → Predict $h_\theta(z_t, t)$ → Compute ELBO Loss
  2. Sampling: Sample $z_0$ → Solve SDE/ODE (Eq. 18) using $h_\theta$ to get $z_1$ → Decode $z_1$ to $x_1$

- **Design tradeoffs:**
  - Parameter $\beta$: Controls trade-off between reconstruction quality and FID. High $\beta$ = better FID, worse PSNR
  - Parameterization: "InterpFlow" (Eq. 17) recommended over "OrigFlow" or "NoisePred" to avoid high variance gradients
  - Encoder Noise ($c$): Stochastic encoders ($c > 0$) perform better than deterministic ones ($c=0$)

- **Failure signatures:**
  - Training Instability: NaNs or divergent loss, likely due to $\sqrt{1-t}$ denominator in naive parameterizations. Fix: Switch to InterpFlow parameterization
  - Poor Reconstruction: FID degrades despite low loss on generative term. Fix: Lower $\beta$ to prioritize reconstruction
  - Blurry Samples: Likely $c$ (encoder noise scale) is too low or latent bottleneck is too aggressive

- **First 3 experiments:**
  1. Verify Latent Code Utility: Train with $\beta \to 0$ vs. $\beta > 0$ on CIFAR10 to confirm joint training advantage
  2. Parameterization Stability: Compare gradient variance of "OrigFlow" vs. "InterpFlow" on single batch overfitting task
  3. Sampler Flexibility: Generate samples using probability flow ODE ($\gamma_t=0$) vs. SDE ($\gamma_t > 0$) to verify controllable diversity

## Open Questions the Paper Calls Out

- **Question**: How does the variance preserving (VP) interpolant schedule compare empirically to the linear schedule in terms of sample quality and training stability?
  - Basis: The authors state they derived expressions for VP choices but do not explore this empirically
  - Why unresolved: The paper derives the math for VP schedules but restricts experiments to the linear schedule, leaving potential benefits unknown
  - What evidence would resolve it: Training identical LSI models using VP formulation on ImageNet and comparing FID scores against linear baseline

- **Question**: Can the variational posterior approximation be generalized to non-linear SDEs without sacrificing simulation-free training?
  - Basis: The Conclusion notes simplifying assumptions for variational posterior approximation do not seem to limit empirical performance
  - Why unresolved: The linearity assumption restricts flexibility. While the authors claim it works, they do not test if removing this restriction improves performance
  - What evidence would resolve it: Deriving simulation-free objective for specific non-linear posterior SDEs and comparing generative performance

- **Question**: Why does a fixed noise scale in the encoder appear to outperform learned variance in terms of FID?
  - Basis: Text states "fixed c models achieved higher FID" while Figure 1 suggests fixed $c$ achieves lower (better) FID than learned baseline, creating ambiguity
  - Why unresolved: Unclear if suboptimal performance of learned variance is due to optimization difficulties or fundamental mismatch with bridge construction
  - What evidence would resolve it: Ablation study analyzing geometry of latent space (KL divergence to prior) for fixed vs. learned encoder variances

## Limitations
- Linear drift assumption for diffusion bridge may be too restrictive for complex data distributions, potentially limiting expressiveness compared to fully learned diffusion models
- Paper does not explicitly validate that computational efficiency gains are preserved when accounting for encoder and decoder overhead during training
- Class conditioning mechanism is not fully specified, which may affect reproducibility of class-conditional FID results

## Confidence
- **High confidence**: Mechanism for computational efficiency through latent space sampling is well-supported by architecture design and empirical FLOPs comparison
- **Medium confidence**: Theoretical derivation of ELBO in continuous time appears sound, but direct experimental validation of simulation-free training advantage is limited
- **Medium confidence**: Claim that joint training improves performance over independently trained components is supported by FID trends with β, but underlying mechanism could benefit from more direct ablation studies

## Next Checks
1. **Bridge linearity validation**: Systematically test LSI performance with non-linear drift alternatives on a controlled dataset to quantify expressiveness trade-off
2. **Efficiency accounting**: Measure total training FLOPs including encoder/decoder updates to verify claimed efficiency gains hold across full training pipeline
3. **Parameter sensitivity**: Conduct formal ablation study varying σ, β, and encoder noise scale c to identify optimal configurations and failure boundaries