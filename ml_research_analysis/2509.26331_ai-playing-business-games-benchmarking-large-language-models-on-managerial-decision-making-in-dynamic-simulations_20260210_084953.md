---
ver: rpa2
title: 'AI Playing Business Games: Benchmarking Large Language Models on Managerial
  Decision-Making in Dynamic Simulations'
arxiv_id: '2509.26331'
source_url: https://arxiv.org/abs/2509.26331
tags:
- llms
- management
- business
- decisions
- decision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study benchmarks large language models (LLMs) on long-term
  strategic decision-making using a dynamic 12-month retail business simulation. Five
  leading LLMs (Gemini, ChatGPT, Meta AI, Mistral AI, and Grok) were evaluated in
  identical conditions using a reproducible Excel-based simulator.
---

# AI Playing Business Games: Benchmarking Large Language Models on Managerial Decision-Making in Dynamic Simulations

## Quick Facts
- arXiv ID: 2509.26331
- Source URL: https://arxiv.org/abs/2509.26331
- Reference count: 40
- Five leading LLMs tested in identical 12-month retail business simulation, with Gemini models outperforming others in profitability and revenue

## Executive Summary
This study evaluates large language models as virtual CEOs managing a retail company over 12 months using a dynamic business simulation. Five leading LLMs (Gemini, ChatGPT, Meta AI, Mistral AI, and Grok) were tested in identical conditions using an open-source Excel-based simulator, making monthly decisions on pricing, orders, marketing, staffing, and forecasting. Results show significant performance variation, with Gemini models demonstrating superior profitability and revenue generation while maintaining better strategic coherence. All models struggled with long-term profitability, with most collapsing after 3-4 periods, highlighting current limitations in sustained strategic reasoning. The study provides a novel, open-access benchmark framework for evaluating AI in complex, multi-step business scenarios.

## Method Summary
The study used a sequential decision-making approach where LLMs received previous period's financial report and output 10 decisions including price, order units, marketing, hiring/dismissal, dividends, loans, training, R&D, sales forecast, and income forecast. These decisions were entered into an open-source Excel simulator (OnePlayer2024r1protectedblank8.xlsx) downloaded from SourceForge, with results copied back to the LLM for the next period. Performance was measured through profit, revenue, market share, and strategic coherence metrics over 12 months. The experiment used a reproducible framework with initial and subsequent prompt templates provided in appendices, though exact model versions and parsing methods were not fully specified.

## Key Results
- Gemini models significantly outperformed other LLMs in profitability and revenue metrics
- Most LLMs experienced a "point of no return" collapse with sales dropping to zero around months 3-5
- Non-Gemini models showed erratic decision patterns and poor strategic coherence
- All LLMs struggled with long-term profitability despite initial strong performance

## Why This Works (Mechanism)
The simulation framework tests LLMs' ability to maintain strategic coherence across extended decision sequences in a dynamic environment. By requiring monthly decisions that affect future states, the study reveals whether LLMs can learn from past outcomes and adapt strategies accordingly. The Excel-based simulator provides a controlled, reproducible environment where all models face identical market conditions and competitive pressures, enabling fair comparison of decision-making capabilities.

## Foundational Learning
- **Business simulation dynamics**: Understanding how pricing, inventory, and marketing decisions interact over time - needed to interpret why models succeed or fail; quick check: trace how a price change affects revenue and inventory in subsequent periods
- **Strategic coherence assessment**: Evaluating whether decision sequences follow logical patterns - needed to distinguish good from erratic decision-making; quick check: plot decision variables over time to identify patterns
- **LLM decision parsing**: Extracting structured decisions from free-form text outputs - needed for accurate data collection; quick check: verify that all 10 decision types are correctly extracted from each LLM response
- **Performance metric calculation**: Computing profitability, market share, and other KPIs from simulator outputs - needed for objective comparison; quick check: manually verify KPI calculations against raw simulator data
- **Point of no return phenomenon**: Understanding why sales collapse in certain periods - needed to diagnose fundamental LLM limitations; quick check: identify when and why each model's sales trajectory becomes unrecoverable
- **Prompt engineering impact**: Recognizing how prompt wording affects decision quality - needed for reproducibility; quick check: compare decision outputs using slightly modified prompts

## Architecture Onboarding
**Component Map**: Excel simulator <- LLM decisions -> LLM reasoning -> Financial report -> Repeat monthly for 12 periods

**Critical Path**: LLM prompt → decision extraction → Excel input → simulator calculation → financial report → next prompt

**Design Tradeoffs**: Single-run vs. statistical significance (chose single-run for reproducibility); free-form vs. structured LLM outputs (chose free-form for natural interaction but required manual parsing); simplified vs. realistic business model (chose simplified for accessibility but may miss real-world complexity)

**Failure Signatures**: Sales dropping to zero around periods 3-5; erratic price/order fluctuations; failure to maintain positive cash flow; inability to recover from initial poor decisions

**First 3 Experiments**:
1. Test structured JSON output format for LLM decisions to eliminate parsing ambiguity
2. Run multiple independent trials per model to establish statistical significance
3. Modify prompt to include explicit long-term goal reminders and observe coherence improvement

## Open Questions the Paper Calls Out
- Would LLMs show improved strategic coherence with repeated sessions and learning across simulation runs, or is long-term incoherence an inherent limitation of current architectures?
- How would LLM decision-making change in competitive multi-agent simulations where multiple LLMs compete simultaneously rather than against a static environment?
- Can LLMs effectively serve as strategic advisors to human managers rather than autonomous decision-makers, and what division of labor optimizes outcomes?
- What prompt engineering modifications would improve LLM coherence in extended simulations?

## Limitations
- Single-run experiment design limits statistical reliability and generalizability of results
- Simplified business simulator may not capture real-world strategic complexity
- Manual parsing of free-form LLM outputs introduces potential human interpretation bias
- Unclear exact model specifications and prompt modifications across sessions

## Confidence
- **High confidence**: Gemini performance ranking; strategy incoherence patterns in non-Gemini models; general collapse pattern after 3-4 periods
- **Medium confidence**: Specific numerical performance values; subjective claims about "coherence and adaptability"
- **Low confidence**: Fundamental LLM strategic reasoning capabilities; exact reproducibility given unspecified details

## Next Checks
1. Execute 30+ independent runs per model using the same Excel simulator to establish statistical significance
2. Reverse-engineer the Excel simulator's underlying formulas to determine if collapse is LLM or simulator artifact
3. Implement standardized structured output format (JSON schema) for LLM decisions to eliminate parsing ambiguity