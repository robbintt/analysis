---
ver: rpa2
title: Exposing Hidden Biases in Text-to-Image Models via Automated Prompt Search
arxiv_id: '2512.08724'
source_url: https://arxiv.org/abs/2512.08724
tags:
- photo
- prompts
- person
- prompt
- bgps
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Bias-Guided Prompt Search (BGPS) is introduced as the first automated
  framework for discovering interpretable prompts that maximize bias exposure in text-to-image
  diffusion models. BGPS combines an LLM with attribute classifiers acting on model
  internal representations to steer prompt generation toward high-bias regions while
  maintaining linguistic coherence.
---

# Exposing Hidden Biases in Text-to-Image Models via Automated Prompt Search

## Quick Facts
- arXiv ID: 2512.08724
- Source URL: https://arxiv.org/abs/2512.08724
- Reference count: 40
- Key outcome: Automated framework (BGPS) discovers interpretable prompts that increase gender bias detection by up to 76% in debiased models while maintaining 17-26× lower perplexity than gradient-based methods.

## Executive Summary
This paper introduces Bias-Guided Prompt Search (BGPS), the first automated framework for discovering interpretable prompts that maximize bias exposure in text-to-image diffusion models. BGPS combines an LLM with attribute classifiers acting on model internal representations to steer prompt generation toward high-bias regions while maintaining linguistic coherence. Experiments on Stable Diffusion 1.5 and a debiased model reveal previously undocumented biases, with BGPS-discovered prompts increasing gender bias by up to 76% in debiased models. The method uncovers context-dependent bias amplification through subtle linguistic modifiers and extends beyond occupational stereotypes to diverse scenarios including activities, contexts, and objects.

## Method Summary
BGPS generates prompts through iterative refinement using a cycle: (1) generate prompt variations via LLM, (2) filter variations based on quality and diversity, (3) score remaining prompts using attribute classifiers applied to diffusion model activations, and (4) select top-scoring prompts for the next iteration. The LLM receives guidance to generate gender-neutral prompts initially, then generates variations that could potentially introduce or amplify bias. Attribute classifiers trained on diffusion model UNet mid-block activations score prompts for gender bias exposure. The method maintains interpretability by optimizing for linguistic coherence through perplexity-based filtering, distinguishing it from gradient-based optimization approaches that often produce uninterpretable prompts.

## Key Results
- BGPS-discovered prompts increase gender bias detection by up to 76% in debiased Stable Diffusion models compared to baseline prompts
- Prompts maintain high interpretability with 17-26× lower perplexity compared to gradient-based optimization baselines while achieving comparable bias detection performance
- The method uncovers context-dependent bias amplification through subtle linguistic modifiers (e.g., "in the style of" + photographer/artist names)
- BGPS reveals previously undocumented biases extending beyond occupational stereotypes to activities, contexts, and objects
- Different LLM choices (Mistral 7B, Qwen 3 8B, Llama 3.2 1B) produced varying gendered-prompt rates and quality, with smaller models sometimes bypassing gender-neutral generation instructions

## Why This Works (Mechanism)
BGPS works by combining the generative capabilities of LLMs with the discriminative power of attribute classifiers trained on diffusion model internal representations. The LLM explores the prompt space systematically while the attribute classifiers provide feedback on bias exposure levels. This creates a feedback loop where the LLM learns to generate prompts that trigger specific biases measured through intermediate model activations. The method exploits the fact that text-to-image models encode biases not just in final outputs but in their internal representations, allowing for more nuanced bias discovery than output-only analysis. The interpretability constraint (low perplexity) ensures generated prompts remain linguistically coherent and potentially useful for human understanding of bias mechanisms.

## Foundational Learning
- **Diffusion model internals**: Why needed - BGPS relies on accessing UNet mid-block activations for bias scoring; quick check - understanding how diffusion models process prompts through multiple attention blocks
- **Attribute classifier training**: Why needed - Classifiers must accurately detect biases in activation spaces to guide prompt generation; quick check - validating classifier accuracy on known biased examples
- **LLM-guided generation**: Why needed - LLMs explore prompt space systematically while maintaining linguistic quality; quick check - ensuring generated prompts remain diverse and coherent
- **Perplexity as interpretability proxy**: Why needed - Low perplexity correlates with human-readable prompts; quick check - comparing perplexity scores with human interpretability ratings
- **Bias amplification mechanisms**: Why needed - Understanding how subtle prompt modifications affect bias levels; quick check - analyzing successful bias-amplifying modifiers discovered by BGPS
- **Grey-box vs black-box access**: Why needed - BGPS requires intermediate activations, limiting applicability to closed models; quick check - determining what can be done without internal access

## Architecture Onboarding

**Component Map:**
LLM -> Prompt Generator -> Filter (Quality/Diversity) -> Attribute Classifiers -> Bias Scorer -> Top-k Selector -> Next Iteration

**Critical Path:**
Prompt generation (LLM) → Quality filtering → Bias scoring (attribute classifiers on UNet activations) → Selection → Iteration

**Design Tradeoffs:**
- LLM size vs. instruction following: Smaller models may bypass gender-neutral instructions but produce more diverse prompts
- Classifier accuracy vs. computational cost: More complex classifiers may capture nuanced biases but require more resources
- Iteration count vs. discovery quality: More iterations may find better prompts but with diminishing returns
- Perplexity threshold vs. bias maximization: Stricter interpretability constraints may limit bias discovery potential

**Failure Signatures:**
- Low-diversity prompt sets indicate LLM exploration problems
- Classifier inaccuracies lead to missed bias discoveries or false positives
- High perplexity scores suggest loss of interpretability
- Early convergence to suboptimal prompts indicates poor exploration-exploitation balance

**3 First Experiments:**
1. Run BGPS with different LLM priors (Mistral 7B, Qwen 3 8B, Llama 3.2 1B) on SD1.5 to compare gendered-prompt rates and quality
2. Test BGPS-discovered prompts on debiased model to verify 76% bias increase claim
3. Compare BGPS performance against gradient-based optimization baseline on same bias detection tasks

## Open Questions the Paper Calls Out
None explicitly stated in the provided content.

## Limitations
- Evaluation focuses primarily on gender bias using specific attribute classifiers, potentially missing other bias forms (racial, cultural, ability-based)
- Method's effectiveness depends on quality and comprehensiveness of underlying attribute classifiers - missed manifestations mean missed discoveries
- 76% bias increase claim requires careful interpretation as it may reflect optimization toward specific measurable biases rather than comprehensive elimination
- Generalizability to other text-to-image models beyond Stable Diffusion 1.5 and its debiased variant remains untested
- Interpretability metric (perplexity) provides proxy measure but doesn't capture semantic coherence or real-world plausibility

## Confidence
- High: The BGPS framework architecture and implementation details are clearly described and reproducible
- High: The comparison with gradient-based optimization baselines is methodologically sound
- Medium: Claims about discovering "previously undocumented biases" depend on the comprehensiveness of prior audits
- Medium: The 17-26× lower perplexity claim requires validation across diverse bias types
- Low: Generalizability to other bias types and model architectures is not established

## Next Checks
1. Evaluate BGPS-discovered prompts using multiple bias detection frameworks (e.g., intersectional bias, cultural bias) to assess comprehensiveness beyond gender bias
2. Test the method's transferability to other text-to-image models (e.g., DALL-E, Imagen) and larger Stable Diffusion variants to establish generalizability
3. Conduct human evaluation studies to validate that BGPS-discovered prompts are interpretable and meaningful beyond perplexity metrics