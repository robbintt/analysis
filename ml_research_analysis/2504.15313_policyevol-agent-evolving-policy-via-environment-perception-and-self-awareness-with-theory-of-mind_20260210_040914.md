---
ver: rpa2
title: 'PolicyEvol-Agent: Evolving Policy via Environment Perception and Self-Awareness
  with Theory of Mind'
arxiv_id: '2504.15313'
source_url: https://arxiv.org/abs/2504.15313
tags:
- game
- name
- card
- chips
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PolicyEvol-Agent is a large language model (LLM)-based framework
  for multi-agent games with incomplete information, featuring policy evolution via
  environment perception and self-awareness with Theory of Mind. The agent dynamically
  adjusts its behavioral policy by integrating game experiences and iteratively updating
  strategies without parameter tuning.
---

# PolicyEvol-Agent: Evolving Policy via Environment Perception and Self-Awareness with Theory of Mind

## Quick Facts
- arXiv ID: 2504.15313
- Source URL: https://arxiv.org/abs/2504.15313
- Reference count: 36
- Primary result: Outperformed Suspicion-Agent and traditional RL methods in Leduc Hold'em, winning significantly more chips after 100 games

## Executive Summary
PolicyEvol-Agent is an LLM-based framework for multi-agent games with incomplete information that dynamically evolves its behavioral policy through environment perception and self-awareness with Theory of Mind (ToM) reasoning. The agent generates multifaceted beliefs about both the environment and itself, enabling more rational decision-making without parameter tuning. Through iterative strategy updates based on game experiences, PolicyEvol-Agent demonstrates strategic human-like behaviors such as bluffing and flexible folding, achieving superior performance compared to traditional RL-based methods and the state-of-the-art Suspicion-Agent baseline in Leduc Hold'em poker.

## Method Summary
PolicyEvol-Agent leverages large language models to reason about incomplete information games through a belief-generation framework that incorporates environment perception and self-awareness. The agent uses Theory of Mind to infer opponents' strategies and beliefs while simultaneously maintaining self-awareness of its own position and potential actions. Policy evolution occurs through iterative updates based on game experiences, with the LLM generating multifaceted beliefs that inform strategic decision-making. The framework operates without requiring parameter tuning, instead relying on the LLM's reasoning capabilities to adapt strategies dynamically across game iterations.

## Key Results
- Won significantly more chips than Suspicion-Agent and traditional RL methods after 100 Leduc Hold'em games
- Demonstrated effective policy evolution, showing improved performance over time through iterative strategy updates
- Ablation studies confirmed the importance of belief generation and planning modules to overall agent performance

## Why This Works (Mechanism)
The framework succeeds by combining LLM-based reasoning with iterative policy evolution. Through Theory of Mind, the agent can infer opponents' beliefs and strategies while maintaining self-awareness of its own position. This dual-perspective reasoning enables more sophisticated decision-making than traditional RL approaches. The policy evolution mechanism allows the agent to learn from experience without parameter tuning, continuously refining its strategy based on game outcomes. The integration of environment perception with self-awareness creates a feedback loop where beliefs about both self and opponents inform and update each other, leading to more rational and adaptive behavior.

## Foundational Learning

**Incomplete Information Games**: Games where players lack full knowledge of the game state or opponents' strategies, requiring reasoning under uncertainty.
*Why needed*: Most real-world strategic interactions involve incomplete information, making this the appropriate framework for realistic multi-agent scenarios.
*Quick check*: Can the agent handle situations where opponents' cards or intentions are hidden?

**Theory of Mind (ToM) Reasoning**: The ability to attribute mental states to others and reason about their beliefs, intentions, and knowledge.
*Why needed*: Enables the agent to model opponents' thinking processes and predict their actions based on inferred beliefs.
*Quick check*: Does the agent correctly infer when opponents are bluffing or holding strong hands?

**Policy Evolution**: The iterative refinement of decision-making strategies based on experience and outcomes.
*Why needed*: Allows continuous improvement without manual parameter tuning or retraining.
*Quick check*: Does performance improve monotonically over successive games?

**Belief Generation**: Creating probabilistic representations of uncertain game states and opponent strategies.
*Why needed*: Essential for decision-making under incomplete information where certainty is impossible.
*Quick check*: Are generated beliefs consistent with observed game actions and outcomes?

## Architecture Onboarding

**Component Map**: Environment Perception -> Belief Generation -> ToM Reasoning -> Policy Evolution -> Action Selection

**Critical Path**: The agent first perceives the game state, generates beliefs about both environment and self, applies ToM reasoning to infer opponents' mental states, evolves its policy based on these beliefs and past experiences, then selects actions accordingly.

**Design Tradeoffs**: Uses LLM reasoning for flexibility and human-like strategic thinking versus computational efficiency of traditional RL methods. Avoids parameter tuning but requires significant computational resources for each decision.

**Failure Signatures**: 
- Poor performance against non-RL opponents suggests issues with ToM reasoning accuracy
- Stagnant performance indicates problems with policy evolution mechanism
- Inconsistent decisions reveal belief generation failures
- Overconfidence in uncertain situations points to inadequate uncertainty modeling

**Three First Experiments**:
1. Test against a random agent to verify basic game-playing capability
2. Evaluate against a fixed-strategy opponent to assess adaptive reasoning
3. Compare performance with and without ToM reasoning to isolate its contribution

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Performance generalizability to other incomplete information games beyond Leduc Hold'em remains unproven
- Policy evolution mechanism lacks rigorous analysis of convergence properties and stability across different game dynamics
- Effectiveness of Theory of Mind component depends heavily on LLM's reasoning capabilities, which may vary across implementations

## Confidence

**Performance claims**: High - Well-documented experimental results against established baselines
**Policy evolution mechanism**: Medium - Shows improvement but lacks systematic analysis of underlying dynamics
**Theory of Mind integration**: Medium - Demonstrated effectiveness but not thoroughly validated against alternatives

## Next Checks

1. Test PolicyEvol-Agent on diverse incomplete information games beyond Leduc Hold'em to evaluate generalizability
2. Conduct ablation studies isolating individual components (perception, self-awareness, ToM reasoning) to quantify their contributions
3. Implement version using smaller, more efficient LLM to assess performance dependence on model scale