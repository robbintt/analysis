---
ver: rpa2
title: 'Let it Calm: Exploratory Annealed Decoding for Verifiable Reinforcement Learning'
arxiv_id: '2510.05251'
source_url: https://arxiv.org/abs/2510.05251
tags:
- arxiv
- sampling
- exploration
- training
- temperature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Exploratory Annealed Decoding (EAD), a temperature-annealing
  schedule for improved exploration in RLVR. By starting with a high temperature and
  gradually cooling, EAD encourages broad exploration early in generation while preserving
  sample quality later, addressing the trade-off between exploration and exploitation.
---

# Let it Calm: Exploratory Annealed Decoding for Verifiable Reinforcement Learning

## Quick Facts
- arXiv ID: 2510.05251
- Source URL: https://arxiv.org/abs/2510.05251
- Reference count: 19
- Primary result: EAD consistently improves sample efficiency and final performance across multiple RLVR algorithms and model sizes

## Executive Summary
This paper proposes Exploratory Annealed Decoding (EAD), a temperature-annealing schedule for improved exploration in RLVR. By starting with a high temperature and gradually cooling, EAD encourages broad exploration early in generation while preserving sample quality later, addressing the trade-off between exploration and exploitation. Empirically, EAD consistently improves sample efficiency and final performance across multiple RLVR algorithms and model sizes, outperforming fixed-temperature sampling baselines on the Numina-Math dataset. It also shows effectiveness at inference time and is compatible with various RL algorithms such as DAPO, GRPO, and EntropyMech. The method is lightweight, plug-and-play, and offers a robust path to enhancing LLM reasoning.

## Method Summary
EAD implements a token-level temperature schedule τ_t = max{1 + τ_max - e^(t/d), τ_min} that starts high for exploration and decays exponentially to concentrate exploration when uncertainty is highest. The method includes truncated importance sampling to correct off-policy drift caused by aggressive annealing. It is compatible with DAPO, GRPO, and EntropyMech algorithms, using default parameters τ_max=1.2, d_0=25, and τ_min=0.1 for smaller models or 0.8 for larger models. The schedule adapts to training progression with d_s = min(d_0 + 5s, 40000).

## Key Results
- EAD consistently improves Pass@16 scores across multiple RLVR algorithms (DAPO, GRPO, EntropyMech) and model sizes (1B, 1.5B, 7B) on Numina-Math
- EAD shows stable training with reduced entropy collapse compared to fixed-temperature baselines
- The method demonstrates effectiveness at inference time through Majority@N improvements
- EAD maintains sample quality (Best@16) while improving worst-case performance (Worst@16)

## Why This Works (Mechanism)

### Mechanism 1: Position-Dependent Entropy Decay
- Exploration value is highest at early tokens because expected conditional entropy decreases monotonically as generation progresses
- Early tokens face higher uncertainty and determine semantic direction; high-temperature exploration yields diverse semantic branches
- Core assumption: Entropy decay trend reliably predicts where exploration is most productive across prompt distributions
- Evidence: Entropy inequality proof, Fig. 1 showing entropy shrinking with positions, Fig. 2 early branching experiment on MMLU

### Mechanism 2: Annealed Temperature Aligns Exploration with Generation Dynamics
- Decaying temperature schedule improves explore-exploit trade-off by concentrating exploration when uncertainty is high
- High temperature flattens softmax distribution increasing diversity; decay sharpens distribution for coherence
- Core assumption: Exponential decay form and hyperparameters generalize across model sizes and tasks
- Evidence: Full formula in Section 4, Fig. 3 visualizing schedules, global-step-aware decay implementation

### Mechanism 3: Truncated Importance Sampling Corrects Off-Policy Drift
- Aggressive annealing creates off-policy discrepancy between behavior and target policies
- TIS bounds variance by truncating importance sampling ratios to prevent gradient explosion
- Core assumption: Truncation bias is acceptable given variance reduction
- Evidence: Proposition B.1 proving variance increases as τ moves from 1, Fig. 11 showing clip fraction/gradient surge without correction, Fig. 4 showing TIS sustains Worst@16 gains

## Foundational Learning

- **Concept: Temperature Sampling and Entropy**
  - Why needed here: EAD manipulates τ to control entropy; higher τ flattens softmax (more uniform = more entropy/diversity), lower τ sharpens it (more deterministic)
  - Quick check question: Given logits [2.0, 1.0, 0.5], compute probabilities at τ=1.0 vs τ=2.0. Which has higher entropy?

- **Concept: Off-Policy RL and Importance Sampling**
  - Why needed here: Annealing creates behavior policy ≠ target policy; IS ratios correct the gradient expectation. TIS bounds extreme ratios
  - Quick check question: If behavior policy samples action a with probability 0.5 but target policy assigns 0.1, what is the IS ratio? What happens if the ratio is 1000?

- **Concept: RLVR and Verifiable Rewards**
  - Why needed here: RLVR uses binary, rule-based rewards instead of learned reward models to avoid reward hacking
  - Quick check question: Why would a learned reward model assign high scores to incorrect but plausible solutions? How does a verifiable reward avoid this?

## Architecture Onboarding

- **Component map**: Temperature Scheduler -> Rollout Sampler -> Reward Verifier -> Advantage Calculator -> IS Ratio Calculator -> Policy Optimizer

- **Critical path**: 
  1. Sample G rollouts per prompt with EAD schedule (log behavior probabilities)
  2. Verify each response → binary rewards
  3. Compute advantages (mean-std normalization across batch)
  4. For each token, compute truncated IS ratio
  5. Backpropagate weighted, clipped policy gradient
  6. Update d_s = min(d_0 + 5s, 40000) as training progresses

- **Design tradeoffs**:
  - τ_max (default 1.2): Higher = more exploration but risk of incoherence
  - τ_min: Lower-capability models (1B) use 0.1; higher-capability (7B) use 0.8 to avoid plausible-wrong solutions
  - Decay rate d (default d_0=25): Larger = longer exploration phase; interacts with response length
  - TIS threshold ε: Lower = more bias, less variance; tune if gradients explode
  - Template cutoff c (default 10): Fix τ=1.0 for prompt-specific prefixes to avoid interference

- **Failure signatures**:
  - Clip fraction surge → IS ratios too extreme; reduce τ_max or tighten ε
  - Gradient norm spike → Off-policy drift; add or lower TIS truncation
  - Entropy collapse (plateau) → Model stuck in local optimum; verify d_s is increasing with global step
  - Pass@k plateaus early → Exploration insufficient; increase τ_max or d

- **First 3 experiments**:
  1. Baseline comparison: Run DAPO with EAD vs fixed τ∈{0.6, 1.0, 1.2} on Numina-Math subset; track Pass@16, Worst@16, entropy over training
  2. TIS ablation: Run EAD with and without truncated IS; monitor clip fraction, gradient norm, and Worst@16 to confirm stability mechanism
  3. Cross-algorithm validation: Plug EAD into GRPO and EntropyMech on same data; verify gains generalize beyond DAPO

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can an adaptive, prompt-specific temperature schedule outperform the uniform schedule used in this study?
- Basis in paper: Authors state in Section 7 that while they used a uniform schedule, "developing such a mechanism [adaptive schedule] is nontrivial," and they leave this for "future investigation"
- Why unresolved: Developing an adaptive mechanism is difficult because prompt distributions shift during iterative RL optimization, and tracking statistics per prompt adds system complexity and overhead
- What evidence would resolve it: A method that dynamically adjusts temperature based on real-time prompt statistics that demonstrates superior performance over the static schedule without prohibitive computational cost

### Open Question 2
- Question: How does EAD perform under systematic scaling laws regarding model size and dataset volume?
- Basis in paper: Section 7 notes that the "scaling behavior of our method is not fully explored because of limited computational resources," identifying a systematic scaling study as an "important next step"
- Why unresolved: Experiments were limited to 1B-7B parameters; it is unknown if sample efficiency gains persist or diminish at much larger scales
- What evidence would resolve it: Empirical results from training significantly larger models with EAD, analyzing Pass@k improvements relative to compute budget

### Open Question 3
- Question: Why does EAD (and RLVR generally) yield limited gains on base models compared to instruction-tuned models?
- Basis in paper: Section 5.1 mentions that experiments on the Llama-3.2-1B base model showed "limited gains across all methods," and authors "defer a deeper investigation to future work"
- Why unresolved: Paper establishes the gap but does not analyze underlying mechanics of why base models fail to benefit from this exploration strategy
- What evidence would resolve it: Ablation study identifying necessary pre-training or intermediate fine-tuning conditions required for EAD to be effective on base models

### Open Question 4
- Question: Does EAD provide compounding benefits when combined with other advanced, orthogonal exploration algorithms?
- Basis in paper: Section 7 states that while EAD is compatible with some algorithms, "a comprehensive study combining it with other advanced exploration-promoting RLVR algorithms... remains a promising direction"
- Why unresolved: Unclear if EAD's sampling-level modifications are redundant when combined with algorithm-level exploration techniques
- What evidence would resolve it: Experiments integrating EAD with orthogonal methods (like tree-search rollouts) to determine if performance improvements are additive or interfering

## Limitations

- Several implementation details remain unspecified including TIS truncation threshold ε, number of rollouts G per prompt, and DAPO clipping bounds
- Optimal temperature parameters appear model-dependent, suggesting sensitivity requiring task-specific tuning in practice
- While EAD shows strong performance, the claim of being "plug-and-play" across algorithms is validated for DAPO and GRPO but only briefly for EntropyMech

## Confidence

- **High confidence**: Core mechanism of position-dependent entropy decay and its relationship to exploration value (Mechanism 1) is well-supported by entropy inequality proof and empirical validation
- **Medium confidence**: Effectiveness of exponential temperature annealing schedule (Mechanism 2) is supported by consistent empirical improvements but specific hyperparameters may require task-specific tuning
- **Low confidence**: Claim that EAD offers a "robust path" for enhancing LLM reasoning at inference time is mentioned but not extensively validated

## Next Checks

1. **TIS sensitivity analysis**: Systematically vary the truncated importance sampling threshold ε (e.g., ε∈{10, 100, 1000}) and measure its impact on training stability metrics (clip fraction, gradient norm) and final Pass@16 performance

2. **Cross-task generalization test**: Apply EAD to a non-mathematical reasoning task (e.g., code generation or multi-hop reasoning) with different reward structures to validate whether the "explore early" heuristic holds when entropy decay patterns differ from math problems

3. **Inference-time ablation study**: Compare EAD-modified models against fixed-temperature baselines using Majority@N and Best@N metrics on a held-out test set, varying N from 1 to 32 to quantify claimed inference-time benefits