---
ver: rpa2
title: Mechanistic Interpretability of Brain-to-Speech Models Across Speech Modes
arxiv_id: '2602.01247'
source_url: https://arxiv.org/abs/2602.01247
tags:
- speech
- across
- causal
- patching
- modes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study applies mechanistic interpretability to brain-to-speech
  decoding models across vocalized, mimed, and imagined speech modes. The authors
  use cross-mode activation patching and tri-modal interpolation to test how internal
  representations mediate performance differences.
---

# Mechanistic Interpretability of Brain-to-Speech Models Across Speech Modes

## Quick Facts
- arXiv ID: 2602.01247
- Source URL: https://arxiv.org/abs/2602.01247
- Reference count: 17
- Authors: Maryam Maghsoudi; Ayushi Mishra
- Key outcome: Cross-mode transfer in brain-to-speech models is mediated by compact, layer-specific subspaces with vocalized representations uniquely effective for restoring mimed and imagined speech decoding

## Executive Summary
This study applies mechanistic interpretability to brain-to-speech decoding models across vocalized, mimed, and imagined speech modes. The authors use cross-mode activation patching and tri-modal interpolation to test how internal representations mediate performance differences. They discover that cross-mode transfer is mediated by compact, layer-specific subspaces rather than diffuse network-wide activity. Specifically, vocalized representations can causally restore high-quality decoding for imagined and mimed inputs, while the reverse substitution catastrophically degrades performance. Causal localization reveals bottlenecks: a small set of convolutional channels and a short segment of recurrent states dominate transfer effects.

## Method Summary
The authors employ mechanistic interpretability techniques including cross-mode activation patching and tri-modal interpolation to investigate brain-to-speech decoding models. They test these methods across three speech modes (vocalized, mimed, and imagined) to understand how internal representations mediate performance differences. The approach involves systematically substituting activations between modes to identify causal relationships and determine which components are critical for cross-mode transfer.

## Key Results
- Cross-mode transfer is mediated by compact, layer-specific subspaces rather than diffuse network-wide activity
- Vocalized representations can causally restore high-quality decoding for imagined and mimed inputs
- Neuron-level analysis shows improvements arise from small, consistent subsets of neurons rather than single units

## Why This Works (Mechanism)
The mechanism underlying cross-mode transfer success centers on the existence of shared representational subspaces that can be activated through specific neural pathways. The vocalized mode contains rich, complete representations that can effectively bootstrap the sparser representations in mimed and imagined modes. This works because the internal structure of the network has learned to compress and encode speech information in a way that preserves essential features across modalities, with certain layers and channels serving as critical bottlenecks for this transfer.

## Foundational Learning
- Activation patching: Understanding how substituting neural activations between different conditions affects model behavior - needed to identify causal relationships between internal representations and outputs; quick check: can you explain how activation patching differs from feature attribution methods?
- Tri-modal interpolation: The mathematical framework for blending representations across three different modes - needed to test continuous relationships between speech modes; quick check: can you describe the interpolation equation and its constraints?
- Mechanistic interpretability: The methodology for reverse-engineering neural network behavior - needed to move beyond black-box predictions to understand internal mechanisms; quick check: can you list three different mechanistic interpretability techniques and their purposes?
- Layer-specific subspaces: The concept that different neural network layers encode different types of information - needed to understand where cross-mode transfer occurs; quick check: can you explain why earlier vs later layers might serve different functions in this context?
- Causal localization: Methods for identifying which specific components cause particular behaviors - needed to pinpoint bottlenecks in the transfer process; quick check: can you describe how ablation studies complement activation patching?
- Neuron-level analysis: The practice of examining individual neuron contributions to overall behavior - needed to understand whether single units or populations drive transfer effects; quick check: can you explain the difference between single-neuron and population-level interpretations?

## Architecture Onboarding

Component Map: EEG/Neural signals -> DeepResNet layers -> LSTM recurrent states -> Output layer -> Speech reconstruction

Critical Path: Input signal processing (DeepResNet) -> Temporal integration (LSTM) -> Final projection to speech space

Design Tradeoffs: The authors chose DeepResNet + LSTM architecture for its ability to capture both spatial and temporal features, but this creates potential bottlenecks where information must flow through specific channels. The tradeoff between model complexity and interpretability was managed by using established architectures that allow for mechanistic analysis.

Failure Signatures: Catastrophic degradation when applying reverse substitutions (mimed/imagined to vocalized) indicates that the network has learned asymmetric representations. Poor performance in cross-mode transfer suggests insufficient shared representational space or overly specialized mode-specific processing.

First Experiments:
1. Test cross-mode patching at different layer depths to identify where transfer effects are strongest
2. Apply tri-modal interpolation with varying interpolation weights to map the continuous manifold
3. Perform neuron-level ablation studies to identify critical populations for cross-mode transfer

## Open Questions the Paper Calls Out
None

## Limitations
- Limited dataset (13 participants) may not capture full variability in brain-to-speech representations
- Controlled speech conditions may not generalize to naturalistic, spontaneous speech
- Specific architectural choices (DeepResNet + LSTM) may not transfer to other brain-to-speech model families

## Confidence
- High: Cross-mode transfer is mediated by compact, layer-specific subspaces
- Medium: Vocalized representations are uniquely effective for restoring mimed and imagined speech decoding
- Low: Speech modes lie on a shared continuous causal manifold

## Next Checks
1. Replicate the causal patching experiments across diverse brain-to-speech architectures (e.g., Transformers, attention-based models) to test architectural generalizability
2. Conduct within-subject replication with naturalistic, spontaneous speech tasks to validate controlled experimental findings
3. Apply the mechanistic interpretability pipeline to brain data from different recording modalities (e.g., ECOG, MEG) to assess modality-specific representational differences