---
ver: rpa2
title: Continuum-armed Bandit Optimization with Batch Pairwise Comparison Oracles
arxiv_id: '2505.22361'
source_url: https://arxiv.org/abs/2505.22361
tags:
- algorithm
- uni00000016
- pairwise
- bandit
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a continuum-armed bandit optimization framework
  with batch pairwise comparison oracles, where decisions are made by comparing pairs
  of actions over multiple time periods to obtain noisy function value differences.
  The authors develop algorithms combining discretization, local polynomial approximation,
  tournament successive elimination, and batched LinUCB to address the unique challenges
  of biased, consistent-only-in-aggregate comparisons.
---

# Continuum-armed Bandit Optimization with Batch Pairwise Comparison Oracles

## Quick Facts
- arXiv ID: 2505.22361
- Source URL: https://arxiv.org/abs/2505.22361
- Reference count: 40
- Primary result: Achieves regret bounds matching minimax lower bounds up to poly-logarithmic factors for smooth functions with pairwise comparison oracles

## Executive Summary
This paper introduces a continuum-armed bandit optimization framework that operates with batch pairwise comparison oracles, where decisions are made by comparing pairs of actions over multiple time periods to obtain noisy function value differences. The authors develop algorithms that combine discretization, local polynomial approximation, tournament successive elimination, and batched LinUCB to address the unique challenges of biased, consistent-only-in-aggregate comparisons. For smooth functions, they achieve regret bounds matching minimax lower bounds up to poly-logarithmic factors, with scaling of $O(T^{(k+d)/(2k+d)}\text{polylog}(T))$ for Hölder smoothness class $\Sigma_d(k,M)$. For strongly concave functions, a proximal gradient descent approach yields $O(\sqrt{T})$ regret. Applications to joint pricing/inventory and network revenue management demonstrate improved dependency on problem parameters compared to state-of-the-art methods.

## Method Summary
The framework transforms continuum optimization into a discrete search problem by partitioning the domain into small cubes, then approximating the smooth function locally using polynomial regression. For each cube, a modified LinUCB algorithm finds local optima using the pairwise oracle. A tournament successive elimination mechanism compares cube champions to concentrate computational budget on promising regions. For strongly concave functions, the approach estimates gradients via finite differences and applies proximal gradient descent with geometrically increasing batch sizes to achieve $\sqrt{T}$ regret.

## Key Results
- Achieves regret bounds of $O(T^{(k+d)/(2k+d)}\text{polylog}(T))$ for Hölder smooth functions, matching minimax lower bounds
- For strongly concave functions, obtains $O(\sqrt{T})$ regret through gradient-based optimization
- Demonstrates improved dependency on problem parameters compared to state-of-the-art methods in inventory management applications
- Theoretical framework generalizes existing optimization approaches with biased comparison oracles

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The algorithm transforms a non-parametric continuum optimization problem into a linear bandit problem within local partitions.
- **Mechanism:** By partitioning the domain $X$ into $J^d$ small cubes, the smooth function $f$ is approximated linearly using a polynomial feature map $\phi_j(x)$ (Eq. 3.1). This allows the use of a modified LinUCB algorithm (BatchLinUCB) to find local optima within a specific cube $X_j$, treating the unknown function as a linear combination of polynomial basis functions.
- **Core assumption:** The objective function $f$ belongs to the Hölder smoothness class $\Sigma_d(k, M)$, ensuring the approximation error $|f(x) - \langle \theta_j, \phi_j(x) \rangle|$ is bounded by $O(J^{-k})$.
- **Evidence anchors:**
  - [Lemma 1] bounds the approximation error by $(d+k)kM \times J^{-k}$.
  - [Algorithm 1] implements BatchLinUCB using this linear approximation.
- **Break condition:** If the function is not sufficiently smooth (low $k$) or the partition size $J$ is too coarse, the approximation error term $C_2$ will dominate, causing the UCB confidence bounds to fail to contain the true function value.

### Mechanism 2
- **Claim:** Global optimization is achieved by treating local cubes as competitors in a "tournament" to eliminate sub-optimal regions.
- **Mechanism:** The "Tournament Successive Elimination" (Algorithm 3) iteratively compares "champion" points from different cubes using the pairwise oracle. If a cube's champion loses a comparison by a margin greater than the error tolerance $\epsilon_\zeta + C'_2$, that cube is discarded. This gradually concentrates computational budget on the region containing the global optimum $x^*$.
- **Core assumption:** The pairwise oracle is $\gamma_1, \gamma_2$-consistent (Definition 1), meaning estimates converge to the true difference as batch size $n$ increases.
- **Evidence anchors:**
  - [Algorithm 3] Lines 11-19 describe the tournament and elimination logic.
  - [Lemma 4] proves that with high probability, the optimal cube $j^*$ is never eliminated.
- **Break condition:** If the oracle consistency (Definition 1) is violated (e.g., bias does not decrease with more samples), the tournament may incorrectly eliminate the optimal cube.

### Mechanism 3
- **Claim:** For strongly concave functions, the framework achieves $\sqrt{T}$ regret by effectively estimating gradients via finite differences.
- **Mechanism:** Algorithm 4 uses a proximal gradient descent approach. Instead of direct function values, it estimates the gradient by querying the oracle with pairs $(x_t, x_t \pm h_t e_j)$. Crucially, it uses geometrically increasing epoch lengths ($\beta_\tau$). This ensures that as the algorithm converges toward the optimum, the gradient estimation becomes more precise (smaller error), stabilizing the convergence.
- **Core assumption:** The function is strongly concave ($f \in \Gamma_d(\sigma)$) and the domain has an interior maximizer.
- **Evidence anchors:**
  - [Theorem 2] establishes the $\tilde{O}(\sqrt{T})$ regret bound.
  - [Algorithm 4] details the gradient estimation step (Lines 7-9).
- **Break condition:** If the function is not strongly concave (e.g., it is merely smooth), the fixed-step gradient descent logic fails, and one must revert to the slower Tournament algorithm (Mechanism 1/2).

## Foundational Learning

- **Concept: Local Polynomial Regression**
  - **Why needed here:** The core of Mechanism 1 relies on approximating an unknown smooth function locally using polynomials. Understanding Taylor expansions and feature maps $\phi_j(x)$ is required to implement the "BatchLinUCB" subroutine.
  - **Quick check question:** Can you explain why the error term in Lemma 1 scales with $J^{-k}$, and what happens to this error if $J$ (partition granularity) increases?

- **Concept: Linear Bandits (LinUCB)**
  - **Why needed here:** The paper reduces the local optimization problem to a linear bandit to handle the "biased, consistent-only-in-aggregate" oracle. Familiarity with ridge regression and confidence ellipsoids in LinUCB is necessary to tune the $C_1$ parameters.
  - **Quick check question:** In Algorithm 1, why is the "baseline" action $x_s$ required for the linear model update, and how does the doubling trick in Algorithm 2 mitigate the regret incurred by a poor baseline?

- **Concept: Finite-Difference Gradient Estimation**
  - **Why needed here:** Mechanism 3 depends on estimating gradients from pairwise differences. Understanding the bias-variance tradeoff in choosing the step size $h_\tau$ is critical.
  - **Quick check question:** In Algorithm 4, why must the batch size $\beta_\tau$ increase geometrically to maintain a valid regret bound as the iterate $x_\tau$ approaches the optimum?

## Architecture Onboarding

- **Component map:**
  1. **Oracle Wrapper:** Implements $O(n, x, x')$ (Definition 1). Must handle the batching logic ($n$ periods) and return the aggregated comparison signal.
  2. **Partition Manager:** Divides the domain $[0,1]^d$ into $J^d$ cubes and manages the "active set" $A_\zeta$.
  3. **Local Solver (BatchLinUCB):** Estimates the linear parameter $\theta_j$ and selects actions within a specific cube (Algorithm 1).
  4. **Tournament Controller:** Manages the comparison rounds between cube champions and executes elimination (Algorithm 3).

- **Critical path:**
  The implementation of the **Oracle Wrapper** is the most critical bottleneck. The theory assumes the oracle returns an estimate where variance decreases as $O(1/n)$. If your specific application (e.g., inventory control) has a more complex bias profile, the Lemma 2 bounds may not hold.

- **Design tradeoffs:**
  - **Partition Size ($J$):** A larger $J$ reduces approximation error (better for complex functions) but exponentially increases the number of cubes to search (tournament complexity). The paper suggests $J = \lceil T^{1/(2k+d)} \rceil$.
  - **Batch Size ($N_\zeta$ vs. $\beta_\tau$):** Larger batches reduce oracle noise but increase the "regret per query." You must balance batch size against the local sub-optimality gap.

- **Failure signatures:**
  - **Regret Flatlines:** If regret stops decreasing, check if the Oracle consistency parameters ($\gamma_1, \gamma_2$) are underestimated in your $C_1$ calculation, causing the confidence bounds to be too tight.
  - **Premature Elimination:** If the tournament eliminates the optimal region early, the tolerance $\epsilon_\zeta$ is likely too tight for the current sample size $N_\zeta$.

- **First 3 experiments:**
  1. **Oracle Unit Test:** Verify the implemented Oracle satisfies Definition 1 by querying fixed pairs $(x, x')$ with varying $n$ and plotting the convergence of the estimate to the true difference.
  2. **Synthetic Function (Smooth):** Run Algorithm 3 on a known smooth function (e.g., $f_1$ from the paper, a Wendland function). Tune $k$ and $J$ to match the theoretical regret bound $O(T^{(k+d)/(2k+d)})$.
  3. **Ablation on Strong Concavity:** Run Algorithm 4 on a non-concave smooth function vs. a strongly concave one to observe the breakdown of the gradient descent approach and verify the necessity of the structure assumption.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the algorithmic framework be extended to function classes with growth conditions (e.g., controlling Lebesgue measure growth of level sets), and what would the resulting regret bounds be?
- **Basis in paper:** [explicit] The conclusion states: "It would be interesting to extend our results to other important function classes, such as those equipped with growth conditions that regulate how fast the Lebesgue measure of level sets of objective functions could grow."
- **Why unresolved:** The current analysis only covers Hölder smoothness and strong concavity. Growth conditions have been shown to significantly impact optimal regret rates in standard continuum-armed bandit settings, but combining them with biased pairwise comparison oracles requires new analytical techniques.
- **What evidence would resolve it:** Deriving regret bounds for objective functions satisfying growth conditions under the pairwise comparison oracle, ideally matching known lower bounds from the standard bandit literature.

### Open Question 2
- **Question:** Can the joint pricing and inventory replenishment problem be solved with provable regret bounds when both censored demand and fixed ordering costs are present?
- **Basis in paper:** [explicit] The conclusion notes: "It is an interesting yet challenging question to extend algorithms and analysis in this paper to problem settings with both demand censoring and inventory ordering costs, which remains as far as we know an open question in the field." Table 1 also highlights this gap.
- **Why unresolved:** Fixed ordering costs introduce non-convexity and combinatorial structure that complicate the optimization landscape, while censored demand limits feedback. The combination creates fundamental difficulties not present in either setting alone.
- **What evidence would resolve it:** A regret analysis for an algorithm addressing both censored demand and fixed costs, with explicit dependence on time horizon T and number of products d.

### Open Question 3
- **Question:** Can the algorithms be made adaptive to unknown smoothness parameters k and M while maintaining near-optimal regret?
- **Basis in paper:** [inferred] Remark 1 states that knowledge of smoothness parameters is "mathematically necessary" and cites Locatelli and Carpentier (2018) for a negative result on adaptivity. However, the question remains whether partial adaptivity (e.g., to k but not M) or different oracle models might allow adaptivity.
- **Why unresolved:** The algorithm parameters C1, C2, and the partition granularity J all depend explicitly on k and M. Without this knowledge, both the local polynomial approximation quality and confidence bounds become mismatched to the true function class.
- **What evidence would resolve it:** An algorithm achieving near-optimal regret without prior knowledge of smoothness parameters, or a formal impossibility result specific to the pairwise comparison oracle setting.

## Limitations

- The framework requires smoothness assumptions that may not hold for real-world functions, limiting practical applicability beyond synthetic test cases.
- The tournament elimination mechanism scales exponentially with dimension due to the $J^d$ initial cubes, creating computational bottlenecks in high-dimensional settings.
- The pairwise oracle consistency assumption (variance decreasing as $O(1/n)$) may not hold for all applications, particularly those with complex bias profiles.

## Confidence

**High**: The regret bounds for smooth functions are mathematically proven and the algorithm structure (discretization + local linear approximation + tournament elimination) is sound. The $O(\sqrt{T})$ bound for strongly concave functions follows from established gradient descent theory.

**Medium**: The algorithm's practical performance depends on proper tuning of constants $C_1, C_2$ and the choice of partition size $J$. The paper mentions these are "conservative" and scaled by unspecified constants in practice.

**Low**: The claim that this framework "generalizes many existing optimization frameworks" lacks concrete examples beyond the inventory management application. The bridge between theory and real-world deployment remains unclear.

## Next Checks

1. **Oracle Consistency Validation**: Implement the pairwise comparison oracle with known bias characteristics and systematically vary the batch size $n$ to empirically verify the consistency assumptions (Definition 1). Plot the convergence of estimates and test whether the assumed $O(1/n)$ variance scaling holds.

2. **High-Dimensional Scaling Test**: Implement Algorithm 3 on synthetic smooth functions in dimensions $d=5, 10, 20$ to empirically measure how the regret scales with dimension compared to the theoretical prediction. This will reveal whether the exponential dependence on $J^d$ is practical.

3. **Non-Strongly-Concave Function Test**: Run Algorithm 4 on functions that are smooth but not strongly concave (e.g., a quadratic with condition number > 1). Measure whether the algorithm degrades gracefully or fails catastrophically, validating the necessity of the strong concavity assumption for the $\sqrt{T}$ regret bound.