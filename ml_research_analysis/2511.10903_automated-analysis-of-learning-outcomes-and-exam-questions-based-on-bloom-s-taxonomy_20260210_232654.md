---
ver: rpa2
title: Automated Analysis of Learning Outcomes and Exam Questions Based on Bloom's
  Taxonomy
arxiv_id: '2511.10903'
source_url: https://arxiv.org/abs/2511.10903
tags:
- bloom
- learning
- augmentation
- data
- taxonomy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the automation of Bloom's Taxonomy classification
  for exam questions and learning outcomes using a variety of machine learning and
  deep learning approaches. The authors compare traditional ML models (Naive Bayes,
  Logistic Regression, SVM), RNN architectures (LSTM, BiLSTM, GRU, BiGRU), transformer
  models (BERT, RoBERTa), and large language models (OpenAI, Gemini, Llama, Claude)
  on a small dataset of 600 sentences across six cognitive categories.
---

# Automated Analysis of Learning Outcomes and Exam Questions Based on Bloom's Taxonomy

## Quick Facts
- arXiv ID: 2511.10903
- Source URL: https://arxiv.org/abs/2511.10903
- Authors: Ramya Kumar; Dhruv Gulwani; Sonit Singh
- Reference count: 36
- Primary result: SVM with synonym-based data augmentation achieved 94% accuracy on Bloom's Taxonomy classification of exam questions

## Executive Summary
This paper investigates the automation of Bloom's Taxonomy classification for exam questions and learning outcomes using multiple machine learning approaches. The authors compare traditional ML models (Naive Bayes, Logistic Regression, SVM), RNN architectures (LSTM, BiLSTM, GRU, BiGRU), transformer models (BERT, RoBERTa), and large language models (OpenAI, Gemini, Llama, Claude) on a small dataset of 600 sentences across six cognitive categories. SVM with synonym-based data augmentation achieved the highest performance (94% accuracy, recall, and F1-score) with minimal overfitting. Transformer-based models showed mixed results, with RoBERTa outperforming BERT. Zero-shot LLM evaluations demonstrated that OpenAI and Gemini achieved approximately 0.72-0.73 accuracy. The findings highlight the challenges of applying complex deep models to small datasets and emphasize the effectiveness of simpler algorithms with appropriate data augmentation strategies.

## Method Summary
The study uses 600 labeled sentences (100 per Bloom's Taxonomy category) for multi-class classification. Preprocessing includes lowercasing, punctuation removal, tokenization, WordNet lemmatization, and stopword removal. Data augmentation applies synonym replacement (~10% rate) via WordNet. Traditional ML models use CountVectorizer with SMOTE and GridSearchCV; RNNs use Keras tokenizer with optional GloVe embeddings and early stopping; transformers use HuggingFace implementations with AdamW optimization; LLMs are evaluated zero-shot via API calls. The 80/20 stratified train-test split is used throughout.

## Key Results
- SVM with synonym augmentation achieved 94% accuracy, recall, and F1-score, outperforming all other approaches
- Transformer models showed severe overfitting (BERT: 0.35 accuracy without augmentation, 0.47 with augmentation)
- RoBERTa initially showed better generalization but eventually overfit as training progressed
- Zero-shot LLM evaluation achieved 0.72-0.73 accuracy with OpenAI and Gemini, demonstrating reasonable performance without task-specific training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Synonym-based data augmentation improves generalization for Bloom's Taxonomy classification on small datasets, with stronger effects on traditional ML models than deep architectures.
- Mechanism: Random replacement of eligible words (length >3, non-stopwords) with WordNet synonyms increases lexical diversity by ~10% without changing semantic meaning. This artificially expands the training distribution, forcing models to learn more robust decision boundaries rather than memorizing specific lexical patterns.
- Core assumption: Synonym replacement preserves the cognitive level of questions; semantic drift from poorly paired synonyms does not materially change classification-relevant features.
- Evidence anchors:
  - [abstract] "Among traditional ML approaches, Support Vector Machines (SVM) with data augmentation achieved the best overall performance, reaching 94 percent accuracy."
  - [section] "Augmentation also helped significantly to Logistic Regression and raised the accuracy of the model, which was 0.78 to 0.91."
  - [corpus] Neighbor papers on automated question generation (OneClickQuiz, DeepQuestion) similarly use augmentation strategies for educational NLP, suggesting cross-validation of the approach in the domain.
- Break condition: Augmentation effectiveness degrades if synonym quality is poor (semantic drift), if dataset is extremely small (<100 samples per class), or if target labels depend on specific lexical cues that synonyms alter.

### Mechanism 2
- Claim: Model complexity must be matched to dataset scale; overparameterized models overfit severely on small educational corpora, while simpler models with appropriate regularization achieve better generalization.
- Mechanism: Deep models (RNNs, BERT) have high parameter counts that can memorize training data when samples are limited (600 sentences = ~80-100 per class after split). Training accuracies exceeded 0.95 while validation dropped to 0.35-0.73. SVM's margin-based optimization with limited parameters creates more stable boundaries under data scarcity.
- Core assumption: The performance gap between training and validation reflects overfitting rather than distribution shift between train/test splits.
- Evidence anchors:
  - [abstract] "RNN models and BERT suffered from severe overfitting, while RoBERTa initially overcame it but began to show signs as training progressed."
  - [section] "BERT was overfit and its accuracy was poor (0.35 without augmentation; 0.47 with augmentation)."
  - [corpus] Limited direct corpus evidence on overfitting dynamics; neighbor papers focus on generation/evaluation rather than model scaling laws for educational data.
- Break condition: Breaks if dataset scales up significantly (>5,000-10,000 samples), if regularization techniques (dropout, weight decay, early stopping) are aggressively tuned, or if pre-training domain closely matches target educational content.

### Mechanism 3
- Claim: Zero-shot LLM classification leverages implicit cognitive understanding from pre-training, achieving moderate accuracy (72-73%) without task-specific training, useful when labeled data or compute is unavailable.
- Mechanism: Large-scale pre-training on diverse corpora embeds knowledge of question structure, cognitive verbs, and educational patterns. When prompted with Bloom's category definitions, LLMs map input questions to these categories using learned semantic relationships rather than fitted parameters.
- Core assumption: LLM pre-training distributions include sufficient educational content and cognitive framework knowledge; prompt formulation adequately conveys category boundaries.
- Evidence anchors:
  - [abstract] "Zero-shot evaluations of large language models (LLMs) indicated that OpenAI and Gemini performed best among the tested LLMs, achieving approximately 0.72-0.73 accuracy."
  - [section] "This performance is more impressive since there is no task-specific training at all, which implies that the performance of LLMs is highly innate with respect to the ability to reason at a cognitive level."
  - [corpus] EduEval benchmark paper validates LLM cognitive evaluation in educational contexts; DeepQuestion framework uses Bloom's taxonomy for LLM evaluation, suggesting domain-appropriate pre-training coverage.
- Break condition: Breaks for smaller or less capable models (LLaMA 3.1 via Ollama: 0.42 accuracy), with poorly designed prompts, or when domain-specific jargon falls outside pre-training distribution.

## Foundational Learning

- Concept: **Bloom's Taxonomy cognitive hierarchy**
  - Why needed here: The six categories (Knowledge, Comprehension, Application, Analysis, Synthesis, Evaluation) are the classification targets; understanding their ordering and distinguishing features is essential for interpreting model errors (adjacent-level confusions).
  - Quick check question: Given the question "Compare and contrast two algorithms," which Bloom level applies, and which adjacent level might a classifier confuse it with?

- Concept: **Overfitting detection via train-validation gap**
  - Why needed here: The paper's central finding hinges on identifying overfitting (train accuracy ~0.95, validation 0.35-0.73); you must recognize this pattern to select appropriate models for small datasets.
  - Quick check question: If a model shows 98% training accuracy and 65% validation accuracy, what two interventions might help?

- Concept: **Zero-shot vs. fine-tuned evaluation**
  - Why needed here: LLM results are zero-shot (no gradient updates), while transformer results are fine-tuned; comparing these fairly requires understanding what each paradigm tests.
  - Quick check question: Why might a zero-shot LLM (72% accuracy) be preferable to a fine-tuned BERT (47% accuracy) in a production setting?

## Architecture Onboarding

- Component map:
  - Preprocessing pipeline: Lowercase → punctuation removal → tokenization → lemmatization (WordNet) → stopword removal → filter tokens (length ≤2)
  - Feature extraction: Bag-of-Words (CountVectorizer) + optional POS tags for ML; Keras tokenizer (10k vocab, pad to 100) + optional GloVe embeddings (300d) for RNNs; HuggingFace tokenizers for transformers
  - Model families: (1) Traditional ML with SMOTE + GridSearchCV; (2) RNNs with Adam + early stopping; (3) Transformers with AdamW + warmup; (4) LLMs via API calls with structured prompts
  - Augmentation module: Synonym replacement via WordNet at ~10% rate

- Critical path: Start with augmented SVM baseline → evaluate data scale → only progress to transformers/LLMs if dataset >5K samples or SVM fails to meet requirements. SVM with augmentation is the default recommendation.

- Design tradeoffs:
  - SVM vs. LLM: SVM requires labeled data and training compute but achieves 94% accuracy; LLM requires neither but caps at ~73% with API costs per inference.
  - Augmentation quality vs. quantity: Aggressive augmentation risks semantic drift; conservative (~10%) provides modest gains without label corruption.
  - RoBERTa vs. BERT: RoBERTa's optimized pre-training helps on small data but eventually overfits; BERT fails more severely.

- Failure signatures:
  - Train >> validation accuracy (>15% gap): Overfitting; reduce model complexity or add augmentation/regularization.
  - Adjacent-category confusion in confusion matrix: Likely feature overlap; consider POS tagging or domain-specific features.
  - LLM accuracy <60% (e.g., LLaMA/Claude): Prompt engineering issue or model capability mismatch; try different prompts or switch models.

- First 3 experiments:
  1. Replicate SVM + augmentation baseline on your dataset; verify 80/20 stratified split and SMOTE application; target 90%+ accuracy as viability threshold.
  2. Ablate augmentation: Train SVM with and without synonym replacement to quantify augmentation contribution on your specific data.
  3. Zero-shot LLM evaluation with OpenAI/Gemini using the paper's prompt format; compare against SVM to determine if training-free approach meets your accuracy requirements.

## Open Questions the Paper Calls Out
None

## Limitations
- The primary limitation is the small dataset size (600 sentences total), which constrains generalizability to larger or more diverse educational corpora
- The specific sentences and exact labels are not publicly available, making full replication impossible without access to the original data
- The exact LLM prompt formulations and GridSearchCV hyperparameter grids are unspecified, creating ambiguity in reproducing reported results

## Confidence
- High confidence: SVM with synonym augmentation achieving 94% accuracy is well-supported by controlled experiments and clear performance gaps between augmented and non-augmented conditions
- Medium confidence: Zero-shot LLM performance claims (72-73% accuracy) are reasonable given the stated results, though the lack of prompt details introduces uncertainty about exact conditions
- Low confidence: Claims about transformer model behavior (BERT overfitting vs. RoBERTa initial success) are plausible but could vary significantly with different hyperparameter settings or larger datasets

## Next Checks
1. Verify augmentation effectiveness by training SVM with and without synonym replacement on your specific dataset to quantify the contribution of this technique
2. Test overfitting detection by monitoring training vs. validation accuracy gaps across all model families, particularly for transformer-based approaches
3. Conduct prompt sensitivity analysis for zero-shot LLM evaluation by testing multiple prompt formulations to establish the stability of reported accuracy levels