---
ver: rpa2
title: 'TRepLiNa: Layer-wise CKA+REPINA Alignment Improves Low-Resource Machine Translation
  in Aya-23 8B'
arxiv_id: '2510.06249'
source_url: https://arxiv.org/abs/2510.06249
tags:
- language
- hindi
- treplina
- layer
- repina
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates layer-wise cross-lingual alignment for improving
  low-resource machine translation from underrepresented languages to high-resource
  languages using the Aya-23 8B model. The authors propose TRepLiNa, a method combining
  Centered Kernel Alignment (CKA) for cross-lingual representation similarity with
  REPINA regularization to stabilize high-resource language features during training.
---

# TRepLiNa: Layer-wise CKA+REPINA Alignment Improves Low-Resource Machine Translation in Aya-23 8B

## Quick Facts
- arXiv ID: 2510.06249
- Source URL: https://arxiv.org/abs/2510.06249
- Reference count: 24
- Primary result: TRepLiNa improves low-resource translation by aligning mid-level transformer layers using CKA+REPINA

## Executive Summary
This work investigates layer-wise cross-lingual alignment for improving low-resource machine translation from underrepresented languages to high-resource languages using the Aya-23 8B model. The authors propose TRepLiNa, a method combining Centered Kernel Alignment (CKA) for cross-lingual representation similarity with REPINA regularization to stabilize high-resource language features during training. Through experiments on four low-resource Indian language pairs with Hindi/English pivots under zero-shot, few-shot, and fine-tuning settings, the study finds that aligning mid-level transformer layers (approximately layers 10–15) yields the strongest performance gains, with TRepLiNa consistently favoring layer 15 in data-scarce conditions.

## Method Summary
TRepLiNa implements layer-wise cross-lingual alignment by combining machine translation loss with CKA-based similarity objectives and REPINA regularization. The method operates on QLoRA fine-tuned Aya-23 8B, performing parallel forward passes to extract hidden states from both low-resource and high-resource language inputs at a target layer. CKA measures structural similarity between these representations while REPINA anchors the high-resource features to prevent degradation. The combined loss function balances translation quality with alignment strength, using hyperparameters λ for CKA weight and μ for REPINA weight.

## Key Results
- Mid-level transformer layers (10-15) provide optimal alignment points for LRL→HRL translation
- TRepLiNa improves weighted composite scores (0.6×BLEU+0.4×ChrF) over baselines
- Method shows particular strength for typologically distant language pairs like Mundari→Hindi and Santali→English
- Performance varies for closely related pairs like Bhili→Hindi where over-alignment can degrade results

## Why This Works (Mechanism)
TRepLiNa works by forcing the low-resource language representations to structurally align with high-resource language representations at specific transformer layers while preserving the high-resource language's inherent features through REPINA regularization. This dual objective enables the model to learn a shared representational space that benefits low-resource translation without compromising the high-resource language's quality. The layer-wise approach targets mid-level representations where semantic features are sufficiently abstract for cross-lingual transfer but still retain enough language-specific information to maintain translation quality.

## Foundational Learning

- **Concept: Centered Kernel Alignment (CKA) as a Similarity Loss**
  - **Why needed here:** CKA is the core alignment tool. It measures the similarity between two sets of representations (matrices) even if they have different dimensions, by comparing the structure of their sample-to-sample similarity matrices. Understanding that the loss `L_CKA = 1 - CKA(H_A, H_B)` forces the model to make LRL activations structurally similar to HRL activations is critical.
  - **Quick check question:** If two sets of hidden states have a CKA score of 1.0, what does that imply about their relationship, and what would the alignment loss `L_CKA` be?

- **Concept: Representation Projection Invariance (REPINA)**
  - **Why needed here:** REPINA is the regularizer that prevents the HRL from being corrupted. It works by creating a "reference" representation from a forward pass (often with adapters disabled, using `stop_gradient`) and adding a loss term that penalizes the squared distance between the current HRL representation and this fixed reference.
  - **Quick check question:** In the TRepLiNa loss `L = L_MT + λL_CKA + µL_REPINA`, what is the effect of setting `µ = 0`? What happens to the HRL representations as training progresses?

- **Concept: QLoRA Fine-Tuning**
  - **Why needed here:** The method is implemented on top of QLoRA, a parameter-efficient fine-tuning technique. This involves loading a base model in 4-bit quantization and training small Low-Rank Adapter (LoRA) modules. The alignment loss affects the gradients that update only these LoRA weights, not the full model.
  - **Quick check question:** Why is this method considered "low-cost"? How many parameters are actually being modified by the alignment and task losses during a typical training run?

## Architecture Onboarding

**Component Map:**
The TRepLiNa system is a training modification applied to a standard decoder-only LLM (Aya-23 8B) setup with QLoRA.
*   **Inputs:** A batch of parallel sentences: source (LRL) and target (HRL).
*   **Base Model:** Aya-23 8B with 4-bit quantization and LoRA adapters enabled on projection modules (`[q,k,v,o,gate,up,down]`).
*   **Alignment Target:** A specific transformer block output at layer `ℓ` (e.g., `ℓ=15`).
*   **Loss Aggregation:** A joint loss calculator that combines the standard causal language modeling loss (`L_MT`), the CKA alignment loss (`L_CKA`), and the REPINA anchoring loss (`L_REPINA`).

**Critical Path:**
1.  **Parallel Forward Passes:** The system must perform (at least) two types of forward passes for each training step.
    *   A standard pass for the machine translation task (source LRL -> target HRL).
    *   An "alignment-only" pass to extract hidden states `H_A` (from LRL input) and `H_B` (from HRL input) at the target layer `ℓ`.
2.  **REPINA Reference Pass:** To compute `L_REPINA`, a separate forward pass on the HRL input is required with the LoRA adapters *disabled* to get the stable reference representation `H_pre_B`.
3.  **Loss Computation & Backprop:** Compute `L_CKA` between `H_A` and `H_B`, and `L_REPINA` between `H_B` and `H_pre_B`. Sum these with `L_MT`. Backpropagate the total loss, which updates only the LoRA adapter weights.

**Design Tradeoffs:**
*   **Alignment Strength (λ):** A higher CKA weight `λ` forces faster alignment but risks over-regularization. The paper finds that smaller datasets (1k) benefit from higher `λ` (0.05), while larger datasets (20k) require lower `λ` (0.01) to avoid eroding language-specific features.
*   **Anchor Strength (µ):** A higher REPINA weight `µ` better preserves HRL features but can slow down or conflict with task learning. For typologically close languages (e.g., Bhili-Hindi), REPINA-only was sometimes better, suggesting a tradeoff between alignment and feature preservation.
*   **Layer Selection:** Mid-layers (10-15) are best for LRL→HRL. The appendix suggests later layers (e.g., 20) might be needed for HRL→LRL. An uninformed choice of layer `ℓ` could yield no benefit.

**Failure Signatures:**
*   **Performance Collapse on HRL:** If `µ` is too low or `λ` is too high, the HRL translation quality may degrade as its representations are pulled toward the LRL. This is the "representation drift" TRepLiNa is designed to prevent, but it can still fail if hyperparameters are poorly chosen.
*   **Stagnant LRL Performance:** If `µ` is too high, the HRL anchor may be too rigid, preventing the model from adapting its shared space to accommodate the LRL, resulting in minimal improvement over the baseline.
*   **Over-Alignment:** For closely related language pairs, a strong CKA term can wash out beneficial, language-specific nuances, leading to performance slightly below a simpler REPINA-only baseline (as seen in the Bhili→Hindi result).

**First 3 Experiments:**
1.  **Baselines & Layer Sweep:** Replicate the "Step 1" experiment from the paper. On a small subset (1k pairs), run three configurations for one epoch: `NoAlign` (standard QLoRA), `CKA-only`, and `TRepLiNa` (CKA+REPINA). Sweep layers `ℓ ∈ {1, 10, 15, 20, 30}` on a single language pair (e.g., Mundari→Hindi) to confirm that `ℓ=15` is indeed the optimal alignment layer.
2.  **Hyperparameter Sensitivity Check:** Using the best layer from experiment 1, fix `µ=0.05` and vary `λ ∈ {0.01, 0.05, 0.1}`. Plot the final BLEU/ChrF score. This validates the paper's claim about the tradeoff between alignment strength and dataset size (even on a small dataset, check if 0.1 is "too large" as suggested).
3.  **Ablation on HRL Stabilization:** For the best configuration from experiment 2, run two comparative runs: one with `TRepLiNa` (µ=0.05) and one with `CKA-only` (µ=0). Evaluate not just on the LRL→HRL task, but also on a pure HRL→HRL task (e.g., Hindi→English) using the same checkpoint. This directly tests the mechanism that REPINA prevents HRL degradation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the optimal alignment layer for TRepLiNa consistently shift to higher layers (e.g., layer 20) when translating from high-resource to low-resource languages (HRL→LRL)?
- Basis in paper: [explicit] Appendix C notes that preliminary sweeps for HRL→LRL show performance peaking at layer 20, unlike the LRL→HRL peak at layers 10–15, and states that rigorous verification is left to future work.
- Why unresolved: The main experiments focused exclusively on LRL→HRL translation due to the computational cost of fine-tuning and the asymmetric task profile.
- What evidence would resolve it: Full Step-2 (multi-epoch) experiments on HRL→LRL pairs to confirm if the higher layer peak is robust and consistent across different language families.

### Open Question 2
- Question: Can alternative similarity objectives, such as contrastive InfoNCE or cosine similarity, outperform CKA in preserving beneficial language-specific features during alignment?
- Basis in paper: [explicit] The Limitations section states the authors "do not explore other similarity objectives (cosine, contrastive InfoNCE)," restricting the study solely to CKA.
- Why unresolved: It remains unclear if CKA is the optimal metric for all language pairs or if alternative objectives might mitigate the performance degradation observed in typologically close pairs like Bhili-Hindi.
- What evidence would resolve it: Comparative ablation studies substituting the CKA loss with contrastive or cosine-based losses on the same MMLoSo benchmark.

### Open Question 3
- Question: Would an adaptive scheduling strategy for the alignment coefficient (lambda) prevent the over-alignment and performance loss seen in typologically close language pairs?
- Basis in paper: [inferred] The authors note TRepLiNa underperforms on Bhili→Hindi likely due to "over-alignment" washing out features, while the Limitations section admits to using fixed coefficients "without scheduler/tuning."
- Why unresolved: Fixed weights may force unnecessary alignment throughout training for languages that already share significant representational space in the base model.
- What evidence would resolve it: Experiments implementing decaying or layer-specific lambda schedules for close language pairs to determine if dynamic weighting recovers performance.

## Limitations

- The method shows performance degradation for closely related language pairs due to over-alignment
- Layer selection heuristic lacks theoretical grounding for why specific layers provide optimal cross-lingual transfer
- Computational setup requires significant resources despite QLoRA implementation

## Confidence

**High Confidence Claims:**
- TRepLiNa consistently improves performance over no-alignment baselines across multiple low-resource language pairs
- Mid-level transformer layers (10-15) provide optimal alignment points for LRL→HRL translation
- REPINA regularization effectively prevents HRL representation drift during cross-lingual alignment
- The method shows particular strength for typologically distant language pairs

**Medium Confidence Claims:**
- Layer 15 is universally optimal across all low-resource conditions (varies by data size and language pair)
- The CKA+REPINA combination is superior to either component alone in all scenarios
- QLoRA implementation details (4-bit NF4, specific LoRA dimensions) are optimal for this task

**Low Confidence Claims:**
- The method generalizes to language pairs beyond the Indian language subset tested
- The computational cost is truly "low" compared to alternative approaches
- The weighted composite score (0.6×BLEU+0.4×ChrF) provides the most meaningful evaluation across all scenarios

## Next Checks

1. **Language Family Generalization Test**: Apply TRepLiNa to language pairs from different families (e.g., Turkic→Slavic, or Niger-Congo→Indo-European) to verify if the mid-layer alignment heuristic (layers 10-15) remains optimal across diverse linguistic typologies.

2. **Representation Drift Quantification**: Implement a systematic measurement of HRL representation changes during training by comparing pre-alignment and post-alignment HRL representations using both CKA and direct Euclidean distance metrics, confirming that REPINA successfully anchors representations while CKA drives alignment.

3. **Zero-Shot Transfer Evaluation**: Test the method's zero-shot cross-lingual transfer capability by training on LRL→HRL pairs and evaluating on unseen HRL→LRL pairs, measuring whether the alignment improves bidirectional transfer beyond what standard fine-tuning achieves.