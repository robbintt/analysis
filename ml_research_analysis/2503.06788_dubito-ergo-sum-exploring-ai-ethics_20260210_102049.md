---
ver: rpa2
title: 'Dubito Ergo Sum: Exploring AI Ethics'
arxiv_id: '2503.06788'
source_url: https://arxiv.org/abs/2503.06788
tags:
- ethics
- moral
- https
- what
- doubt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper argues that AI lacks the capacity for moral agency because
  it cannot doubt itself. It reviews various ethical frameworks and concludes that
  sensory grounding, understanding, and the ability to doubt are essential for morality.
---

# Dubito Ergo Sum: Exploring AI Ethics

## Quick Facts
- arXiv ID: 2503.06788
- Source URL: https://arxiv.org/abs/2503.06788
- Authors: Viktor Dorfler; Giles Cuthbert
- Reference count: 0
- Primary result: AI lacks moral agency because it cannot doubt itself

## Executive Summary
This philosophical paper argues that AI systems cannot possess moral agency because they fundamentally lack the capacity for self-doubt, which the authors identify as essential for moral decision-making. Through phenomenological and critical interpretivist approaches, the authors review traditional ethical frameworks (deontology, consequentialism, virtue ethics) and conclude that current AI architectures are insufficient for moral reasoning. They propose that rather than embedding ethics into AI, systems should be designed to assist human moral decision-making by providing useful input for self-doubt and scanning for emerging patterns.

## Method Summary
The paper employs a phenomenological approach combined with critical interpretivism and abductive reasoning to analyze AI ethics conceptually. The authors use everyday examples and thought experiments rather than empirical data or experiments. They examine existing ethical frameworks and argue that sensory grounding, understanding, and the ability to doubt are prerequisites for morality that current AI systems cannot satisfy.

## Key Results
- AI systems lack the capacity for self-doubt, which the authors argue is essential for moral agency
- Current ethical frameworks (deontology, consequentialism, virtue ethics) are inadequate for AI implementation due to exception handling and bias amplification
- AI amplifies existing ethical problems rather than creating new ones, suggesting a role for AI in assisting human moral decision-making rather than replacing it

## Why This Works (Mechanism)
The paper's central argument rests on the philosophical claim that doubt is a prerequisite for moral agency. The mechanism proposed is that moral decisions require metacognition - the ability to question one's own judgments and consider alternative perspectives. Since current AI systems operate deterministically and lack genuine uncertainty or self-reflection, they cannot engage in the type of moral reasoning that humans use.

## Foundational Learning
- **Phenomenological approach**: Why needed - provides framework for understanding lived experience and consciousness; Quick check - can you identify the subjective experience being analyzed?
- **Critical interpretivism**: Why needed - allows examination of how meaning is constructed in ethical contexts; Quick check - can you distinguish between objective facts and interpretive frameworks?
- **Abductive reasoning**: Why needed - enables inference to best explanation in ethical dilemmas; Quick check - can you evaluate competing explanations for moral behavior?

## Architecture Onboarding
**Component Map**: Human Moral Reasoning -> AI Input/Pattern Detection -> Human Moral Decision
**Critical Path**: Human identifies ethical dilemma → AI provides pattern analysis → Human incorporates insights into decision
**Design Tradeoffs**: 
- Direct ethical programming (deontology) vs. pattern assistance approach
- Complete AI autonomy vs. human-in-the-loop decision making
- Speed/efficiency vs. moral certainty

**Failure Signatures**:
- AI systems making definitive moral judgments without acknowledging uncertainty
- Over-reliance on statistical patterns without contextual understanding
- Absence of self-correction mechanisms when AI encounters novel ethical situations

**First Experiments**:
1. Design prompts to test whether LLMs can express genuine uncertainty about moral judgments versus parroting uncertainty phrases
2. Compare AI moral reasoning patterns against human responses on standardized ethics tests
3. Test AI system's ability to recognize when it lacks sufficient information for ethical decisions

## Open Questions the Paper Calls Out
**Open Question 1**: How can AI systems be designed to autonomously identify their ethical limitations and request human assistance?
- Basis in paper: [explicit] The authors state the "far trickier next one is how to get AI to identify its limitations, and request human assistance."
- Why unresolved: The paper is conceptual and does not propose technical solutions for this implementation gap.
- What evidence would resolve it: A functional protocol triggering human review when AI encounters ambiguous ethical scenarios.

**Open Question 2**: What specific variables and sample sizes are required for Deep Learning to statistically model moral decisions?
- Basis in paper: [explicit] The text explicitly asks, "what number of learning examples would be needed... What variables would need to be considered?"
- Why unresolved: The authors pose these to highlight the complexity of current ML, leaving the quantification unanswered.
- What evidence would resolve it: Empirical studies defining the dimensionality of moral features necessary for reliable ethical classification.

**Open Question 3**: How can "external pointers" be developed to signal when AI requires human intervention due to a lack of "felt sense"?
- Basis in paper: [explicit] The authors argue "we need to figure out how to provide external pointers when AI needs to involve a human in the process."
- Why unresolved: This is proposed as a necessary mechanism for safe operation but remains an identified problem to be solved.
- What evidence would resolve it: Interface protocols that translate AI uncertainty into actionable alerts for human operators.

## Limitations
- No empirical validation of the core thesis that self-doubt is essential for moral agency
- No specific AI systems or benchmarks proposed for testing the claims about AI capabilities
- Conceptual definitions of "understanding," "doubt," and "moral agency" remain abstract and unoperationalized

## Confidence
- High confidence in the claim that current AI systems lack human-like self-doubt and metacognition
- Medium confidence in the analysis of existing ethical frameworks as applied to AI systems
- Low confidence in the assertion that doubt is necessary for moral agency, as this represents a philosophical position

## Next Checks
1. Design prompt-based experiments testing whether current LLMs can express genuine uncertainty about moral judgments versus parroting uncertainty phrases
2. Compare AI moral reasoning patterns against human responses on standardized ethics tests (e.g., Moral Machine, Defining Issues Test) to identify systematic differences
3. Conduct a systematic literature review of empirical studies on AI moral decision-making to identify what evidence exists for or against the paper's claims about AI capabilities