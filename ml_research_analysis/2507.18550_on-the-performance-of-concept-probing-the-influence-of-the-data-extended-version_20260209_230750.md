---
ver: rpa2
title: 'On the Performance of Concept Probing: The Influence of the Data (Extended
  Version)'
arxiv_id: '2507.18550'
source_url: https://arxiv.org/abs/2507.18550
tags:
- probing
- concept
- data
- hassymbol
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how the data used to train concept probing
  models affects their performance in image classification tasks. Concept probing
  helps interpret neural networks by training additional classifiers to map internal
  model representations to human-defined concepts of interest.
---

# On the Performance of Concept Probing: The Influence of the Data (Extended Version)

## Quick Facts
- arXiv ID: 2507.18550
- Source URL: https://arxiv.org/abs/2507.18550
- Reference count: 40
- Key outcome: Probes achieve 97.3% of maximum performance with just 250 training samples for relevant concepts, with data reuse showing no negative impact

## Executive Summary
This paper investigates how the data used to train concept probing models affects their performance in image classification tasks. Concept probing helps interpret neural networks by training additional classifiers to map internal model representations to human-defined concepts of interest. The authors examine four main dimensions: training data size, original model size, data reuse, and data quality.

Key findings include: (1) Probes achieve 97.3% of maximum performance with just 250 training samples for relevant concepts, with linear and mapping network probes showing particular effectiveness; (2) Surprisingly, increasing original model size slightly improves probe performance rather than degrading it; (3) Reusing data from the original model's training set has no negative impact on probe performance; (4) Probes show robustness to moderate label noise (up to 20%) but performance degrades significantly beyond that.

## Method Summary
The authors evaluate concept probing performance across four key dimensions: training data size (ranging from 50 to 5000 samples), original model size (varying ResNet and ViT architectures), data reuse (training probes on original model's training data vs. held-out data), and data quality (introducing label noise). They test three probe architectures - Logistic Regression, Ridge Regression, and MapNN - across multiple datasets including CIFAR-10, ImageNet, and modified datasets with injected concepts. The evaluation measures test accuracy of probes trained to detect human-defined concepts within intermediate model representations.

## Key Results
- Probes achieve 97.3% of maximum performance with just 250 training samples for relevant concepts
- Increasing original model size slightly improves probe performance rather than degrading it
- Reusing original model's training data has no negative impact on probe performance
- Probes show robustness to moderate label noise (up to 20%) but degrade significantly beyond that

## Why This Works (Mechanism)

### Mechanism 1: Representation Distillation for Relevant Concepts
- **Claim:** Probing models achieve high accuracy with limited data because the original neural network has already distilled task-relevant information into its internal activations.
- **Mechanism:** If a concept $C$ is relevant to the original model's task, the model $f$ likely preserves information about $C$ within its intermediate layers $f_u(x)$. The probe $g$ acts as a simple decoder that maps these pre-processed activations to the concept label, rather than learning the concept from raw pixels. This reliance on pre-distilled features allows the probe to reach ~97.3% of maximum performance with only 250 samples.
- **Core assumption:** The concept being probed is functionally relevant to the original model's training objective.
- **Evidence anchors:**
  - [Section 4] "The results show that the accuracy of the probes grows quickly for relevant concepts... Non-relevant concepts seem to require additional training samples."
  - [Section 2] "Concept probing works by training additional classifiers to map the internal representations of a model into human-defined concepts of interest."
  - [Corpus] *Concept Probing: Where to Find Human-Defined Concepts* suggests the efficacy is tied to the location (layer) where these representations are stored.
- **Break condition:** If the concept is non-relevant to the original task, the model may have discarded the information during compression, causing the probe to fail or require significantly more data.

### Mechanism 2: Activation Invariance to Data Provenance
- **Claim:** Reusing the original model's training data to train the probe does not degrade probe performance because the activation patterns of training data generalize well to test data.
- **Mechanism:** One might hypothesize that a model $f$ produces distinct activation patterns for seen (training) vs. unseen data. However, the paper suggests the mapping from activation to concept is stable. The probe learns a generalizable function $g: f_u(x) \to c$ that applies regardless of whether $x$ was seen during $f$'s training.
- **Core assumption:** The original model has not severely overfit to its training data such that activation distributions differ drastically between train and test instances.
- **Evidence anchors:**
  - [Section 6] "The performance of the trained probes $g$ remains mostly unchanged, even when all of $g$'s training data came from their respective original model's training set."
  - [Section 6] "T-test showing that there is no statistically significant evidence that the amount of reuse affects performance."
  - [Corpus] *Probing Neural Topology of Large Language Models* supports the idea that functional co-activations form stable structures that can be probed.
- **Break condition:** If the original model $f$ has memorized training data (overfitting) without learning general features, the activations $f_u(x_{train})$ might be idiosyncratic, causing the probe to overfit to artifacts rather than the concept.

### Mechanism 3: Robustness via Redundancy and Regularization
- **Claim:** Probing models, particularly those with regularization (e.g., MapNN), maintain performance even under moderate label noise and model scaling.
- **Mechanism:** The authors suggest that scaling up the original model increases redundant features, which may provide additional signal for the probe. Conversely, mapping network probes use L1 regularization and feature selection (input reduce), which filters out noise and selects the most informative units, making them robust to dimensionality changes.
- **Core assumption:** The signal-to-noise ratio in the activations remains favorable even as model capacity increases or labels become noisy.
- **Evidence anchors:**
  - [Section 5] "Accuracy of the probing models seems to slightly increase with the scaling up... MapNN seems mostly unaffected by the scaling down."
  - [Section 7] "Probes show robustness to moderate label noise... introducing 20% of noise led to a relative performance reduction of 9.3%."
  - [Corpus] *Discovering Chunks in Neural Embeddings* implies that neural activations contain recurring, stable entities (chunks) that can be identified despite noise.
- **Break condition:** Performance degrades significantly beyond 30% label noise, suggesting the signal is overwhelmed by noise.

## Foundational Learning

- **Concept: Internal Representations (Activations)**
  - **Why needed here:** Concept probing does not look at input pixels; it looks at the "thought process" (activations) of the model. Understanding that these are high-dimensional vectors extracted from hidden layers is fundamental.
  - **Quick check question:** If you extract activations from the final layer of a ResNet, what do they represent compared to activations from an early convolutional layer?

- **Concept: Relevancy (Task-Relevance)**
  - **Why needed here:** The paper explicitly distinguishes between "relevant" and "non-relevant" concepts. A probe only works efficiently if the original model actually uses that concept for its task.
  - **Quick check question:** Would a "wheel" detector (probe) perform better on a model trained to classify cars or a model trained to classify landscape paintings? Why?

- **Concept: Regularization (L1/Feature Selection)**
  - **Why needed here:** The MapNN probe uses L1 regularization to identify which specific neurons encode a concept. This differentiates it from standard linear probes that look at all neurons at once.
  - **Quick check question:** Why might a probe that selects only 5% of the neurons (MapNN) be more robust to model size changes than a probe that uses all neurons (Logistic)?

## Architecture Onboarding

- **Component map:** Original Model ($f$) -> Activation Extractor -> Probe Model ($g$) -> Concept Dataset
- **Critical path:**
  1. Select a layer/unit $u$ of model $f$ to probe (Paper suggests final blocks are best for relevant concepts)
  2. Build dataset $D_g$ by running images through $f$ and extracting activations
  3. Train $g$ on $D_g$ with regularization (L1 or Ridge) to map activations to concepts

- **Design tradeoffs:**
  - **Linear Probe (Logistic/Ridge):** Fast, simple, but may struggle if the concept is non-linearly encoded in the layer
  - **MapNN:** Uses "input reduce" to find specific units; more robust to noise and model scaling, but computationally more intensive to train
  - **Data Reuse:** Safe to use training set images for probing to save data gathering costs (per paper findings), provided the original model isn't severely overfitted

- **Failure signatures:**
  - **Stuck at 50% accuracy:** Concept is likely "non-relevant" or the layer $u$ has compressed the concept away
  - **High Variance:** Training data is too small (<250 samples) or label noise is >20%
  - **Sudden Drop in Accuracy:** You may have selected a layer too early in the network (features not abstract enough) or too late (too compressed)

- **First 3 experiments:**
  1. **Sample Efficiency Test:** Train a probe on a relevant concept using 50, 250, and 1000 samples. Verify the "elbow" at ~250 samples mentioned in Section 4.
  2. **Relevancy Ablation:** Probe for a "relevant" attribute (e.g., color) vs. a "non-relevant" attribute (e.g., background object) to verify the performance gap.
  3. **Noise Injection:** Flip 20% of training labels for a concept and observe if the probe degrades gracefully (confirming robustness).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do data reuse strategies negatively impact probe generalization metrics other than test accuracy, such as robustness to out-of-distribution samples?
- Basis in paper: [explicit] Section 6 states that while test accuracy was unaffected, "effects of reusing the original model's training data might manifest in other ways that are not measurable by a probe's test accuracy."
- Why unresolved: The authors only measured standard test accuracy and did not evaluate robustness or distributional shift metrics.
- What evidence would resolve it: Experiments measuring probe performance on adversarial examples or out-of-distribution datasets when trained on reused data.

### Open Question 2
- Question: How does the specific distribution of label noise (e.g., class-conditional bias vs. uniform error) quantitatively degrade concept probing performance?
- Basis in paper: [explicit] Section 7 notes that natural noise caused a larger performance drop than artificial noise, speculating this is because "the noise... is not random and mainly affects the positive samples."
- Why unresolved: The study compared random artificial noise against one instance of natural noise but did not systematically vary noise distributions.
- What evidence would resolve it: Controlled experiments introducing varying degrees of systematic bias (e.g., flipping only positive labels) to model degradation curves.

### Open Question 3
- Question: Do the observed relationships between data size and probe performance persist in non-visual domains such as natural language processing?
- Basis in paper: [inferred] The paper focuses exclusively on "concept probing in the context of image classification tasks" (Abstract/Section 3), leaving other modalities unexplored.
- Why unresolved: Representation structures differ significantly between CNNs/ViTs and language models, potentially altering data efficiency requirements.
- What evidence would resolve it: Replication of the training data size and quality experiments using large language models (LLMs) and textual benchmarks.

## Limitations

- **Major Uncertainties:** The paper's findings on model scaling show performance improvements with larger models, which contradicts the conventional wisdom that larger models create harder-to-interpret intermediate representations. This surprising result needs further investigation, particularly regarding whether this holds across diverse architectures beyond the specific models tested.
- **Label Noise Scope:** The robustness claims to label noise (up to 20%) were tested on "relevant" concepts only - the behavior for non-relevant concepts under noise remains unclear.
- **Data Reuse Assumptions:** While statistically significant differences weren't found, the study may have limited power to detect smaller negative effects on generalization metrics beyond standard test accuracy.

## Confidence

- **High Confidence:** Sample efficiency findings (97.3% performance with 250 samples) - well-supported by extensive experiments across multiple datasets and concepts
- **Medium Confidence:** Data reuse safety - while statistically significant differences weren't found, the study may have limited power to detect smaller negative effects
- **Low Confidence:** Model scaling benefits - counterintuitive finding that contradicts established interpretability research; requires replication with broader model diversity

## Next Checks

1. **Cross-architecture scaling validation:** Test whether larger models consistently improve probe performance across architectures not originally studied (e.g., ConvNext, EfficientNet variants)
2. **Noise impact on non-relevant concepts:** Systematically test label noise effects on both relevant and non-relevant concepts to determine if robustness patterns differ
3. **Overfitting edge cases:** Specifically test probes on models known to be severely overfit to their training data to validate the data reuse assumption under extreme conditions