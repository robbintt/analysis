---
ver: rpa2
title: Few-Shot Classification and Anatomical Localization of Tissues in SPECT Imaging
arxiv_id: '2502.06632'
source_url: https://arxiv.org/abs/2502.06632
tags:
- classification
- learning
- prototypical
- prnet
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a few-shot learning approach for classifying
  and localizing tissues in SPECT imaging. The authors adapted Prototypical Networks
  with a pre-trained ResNet-18 backbone to classify ventricles, myocardium, and liver
  tissues, achieving 96.67% training and 93.33% validation accuracy.
---

# Few-Shot Classification and Anatomical Localization of Tissues in SPECT Imaging

## Quick Facts
- arXiv ID: 2502.06632
- Source URL: https://arxiv.org/abs/2502.06632
- Reference count: 8
- Primary result: 96.67% training and 93.33% validation accuracy for 3-class tissue classification

## Executive Summary
This paper presents a two-pronged approach for few-shot learning in SPECT imaging: tissue classification using Prototypical Networks and anatomical landmark localization using an adapted PRNet. The classification task identifies ventricles, myocardium, and liver tissues from 2D SPECT slices using a ResNet-18 backbone, achieving high accuracy with only 12 training images. The localization task employs self-supervised learning to predict anatomical coordinates without explicit landmark labels, using an encoder-decoder architecture with skip connections. Both approaches demonstrate the potential of deep learning for medical imaging tasks with limited labeled data.

## Method Summary
The study combines few-shot classification and anatomical localization for SPECT imaging. For classification, Prototypical Networks with a pre-trained ResNet-18 backbone aggregate class prototypes from support sets and classify query images based on Euclidean distances to these prototypes. For localization, the paper adapts PRNet from 3D to 2D by replacing 3D convolutions with 2D equivalents and adding skip connections. PRNet uses self-supervised learning through random double-cropping of patches, predicting relative offsets between patch pairs while reconstructing original patches as auxiliary supervision. Both models are trained using Adam optimizer with learning rates of 0.001.

## Key Results
- Prototypical Network achieved 96.67% training accuracy and 93.33% validation accuracy for 3-class tissue classification
- PRNet achieved a training loss of 1.395 while accurately reconstructing patches and capturing spatial relationships
- The framework successfully demonstrates few-shot learning capabilities with only 12 SPECT images
- Both approaches show promise for improving deep learning frameworks in medical imaging

## Why This Works (Mechanism)

### Mechanism 1: Metric-Space Prototype Aggregation for Few-Shot Classification
Prototypical Networks enable tissue classification with minimal labeled examples by representing each class as a single prototype vector in learned embedding space. The ResNet-18 backbone maps input images to a metric space where intra-class distance is minimized and inter-class distance is maximized. Classification reduces to computing Euclidean distances from query embeddings to class prototypes (mean of support embeddings), followed by softmax over negative distances. This approach assumes tissue classes form separable clusters in embedding space when aggregated via mean vectors, leveraging the pre-trained backbone's transferable low-level features.

### Mechanism 2: Self-Supervised Spatial Relationship Learning via Double Cropping
PRNet learns anatomical landmark localization without explicit landmark labels by predicting relative offsets between randomly sampled patch pairs. Random double cropping selects two points in an image, extracts patches around each, and the encoder maps patches to anatomical coordinates. The network minimizes the difference between predicted offset and ground-truth offset while the decoder reconstructs original patches as auxiliary supervision, enforcing spatial information preservation in the latent representation. This mechanism assumes anatomical structure consistency across images such that learned spatial relationships transfer to unseen images.

### Mechanism 3: Dimensionality Reduction from 3D to 2D for Domain Adaptation
The paper adapts 3D architectures to 2D processing to enable transfer of spatial relationship learning from CT/MRI to SPECT imaging. The original PRNet architecture for 3D volumes is modified by replacing 3D convolutional layers with 2D equivalents while preserving skip connections for spatial resolution. This adaptation assumes 2D slices contain sufficient anatomical information for localization tasks and that information loss from discarding the third dimension does not critically impair the task.

## Foundational Learning

- Concept: Metric Learning and Embedding Spaces
  - Why needed here: Prototypical Networks rely on the premise that similar inputs cluster together in embedding space. Understanding how distance metrics and embedding quality affect classification is essential for debugging few-shot failures.
  - Quick check question: Given three tissue embeddings—ventricle [0.2, 0.8], myocardium [0.7, 0.3], liver [0.25, 0.75]—which pair has the smallest Euclidean distance, and what does this imply about class separability?

- Concept: Self-Supervised Learning and Pretext Tasks
  - Why needed here: PRNet uses reconstruction and relative position prediction as pretext tasks. Understanding how auxiliary objectives shape representations helps assess whether the model learns meaningful anatomy or exploits artifacts.
  - Quick check question: If a model achieves low reconstruction loss but high localization error on validation data, what does this indicate about the relationship between the pretext task and downstream task?

- Concept: Episode-Based Training for Few-Shot Learning
  - Why needed here: Unlike standard batch training, few-shot learning trains on "episodes" containing support and query sets sampled from classes. This mimics test conditions and prevents memorization.
  - Quick check question: In a 3-way 5-shot episode with 10 query images per class, how many total forward passes are required per episode, and what is the expected random baseline accuracy?

## Architecture Onboarding

- Component map:
  - Prototypical Network branch: Input (segmented tissue mask) → ResNet-18 backbone (pre-trained, fine-tuned) → Embedding vector → Distance computation to class prototypes → Softmax classification
  - PRNet branch: Input (2D SPECT slice) → Random double-crop sampler → Dual encoder pathway → FC layers (anatomical coordinates) → Offset prediction + Decoder (patch reconstruction) with skip connections
  - Shared infrastructure: Adam optimizer (lr=0.001 for both), episodic batch loader for Prototypical Network, iteration-based training for PRNet

- Critical path:
  1. Data preparation: Segment tissue masks from 12 SPECT images
  2. Prototypical Network training: Sample episodes (3 support, 6 query per class), compute prototypes, minimize negative log-probability
  3. PRNet training: Extract random patch pairs, compute ground-truth offsets, minimize L_ssl = L_dis + L_rec
  4. Validation: Report accuracy for classification; reconstruction visualization for localization

- Design tradeoffs:
  - Prototype aggregation (mean vs. median): Mean is sensitive to outliers; median may be more robust for high-variance classes
  - Backbone depth (ResNet-18 vs. deeper): Deeper networks may capture more expressive features but increase overfitting risk with 12 images
  - Skip connection placement: Earlier skip connections preserve fine spatial details but increase memory; later skip connections capture semantic information
  - Assumption: The paper does not ablate these choices; performance differences are unknown

- Failure signatures:
  - Prototypical Network: High training accuracy with low validation accuracy suggests overfitting to support set; large gap between prototype distances indicates poor embedding quality
  - PRNet: Low reconstruction loss with visually incoherent reconstructions suggests model has learned to minimize pixel-wise loss without capturing structure; high distance loss indicates failure to learn spatial relationships
  - General: If validation metrics are not reported, generalization cannot be assessed

- First 3 experiments:
  1. Baseline probe: Train Prototypical Network with leave-one-image-out cross-validation across the 12 SPECT images to estimate generalization variance. Report mean and standard deviation of validation accuracy.
  2. Embedding visualization: Project tissue embeddings using t-SNE or UMAP colored by class. Assess cluster separation and identify potential outlier samples that may distort prototypes.
  3. PRNet localization quantification: Define a held-out test set with annotated landmark coordinates. Report mean absolute error (MAE) between predicted and ground-truth offsets to establish a quantitative baseline beyond reconstruction loss.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided text. However, the conclusion suggests future work integrating spatial relationships into segmentation frameworks and extending the approach to full 3D volumetric data.

## Limitations
- Extremely small dataset (12 images) raises significant concerns about overfitting and generalizability to diverse patient populations
- Lack of quantitative localization metrics for PRNet beyond reconstruction loss prevents assessment of anatomical accuracy
- Key architectural details are underspecified, including input dimensions, patch sizes, and exact loss formulations
- Manual or semi-automated segmentation process is not described, introducing potential variability

## Confidence
- High confidence: The Prototypical Network architecture and training procedure are well-established in the literature with plausible reported accuracy
- Medium confidence: The PRNet self-supervised learning mechanism is theoretically sound but lacks quantitative localization metrics and has underspecified architecture details
- Low confidence: Generalizability to clinical practice given the tiny dataset, impact of 3D-to-2D adaptation on localization accuracy, and effectiveness for tissue classes with high pathological variability

## Next Checks
1. Perform leave-one-image-out cross-validation on the 12 SPECT images and report mean ± standard deviation of validation accuracy to quantify performance variance.
2. Implement quantitative evaluation of PRNet by annotating landmark coordinates on a held-out test set and reporting mean absolute error between predicted and ground-truth anatomical offsets.
3. Generate t-SNE or UMAP visualizations of tissue embeddings from the Prototypical Network to visually assess class separation and identify potential outliers.