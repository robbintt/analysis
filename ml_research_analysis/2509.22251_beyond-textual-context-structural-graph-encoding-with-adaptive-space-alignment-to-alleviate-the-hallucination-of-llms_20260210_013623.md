---
ver: rpa2
title: 'Beyond Textual Context: Structural Graph Encoding with Adaptive Space Alignment
  to alleviate the hallucination of LLMs'
arxiv_id: '2509.22251'
source_url: https://arxiv.org/abs/2509.22251
tags:
- llms
- graph
- knowledge
- information
- structural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the hallucination problem in large language
  models (LLMs) by integrating structural and semantic information from knowledge
  graphs (KGs). The proposed SSKG-LLM framework employs a Knowledge Graph Retrieval
  (KGR) module to extract relevant subgraphs, a Knowledge Graph Encoding (KGE) module
  to jointly model structural and semantic patterns using GraphLM, and a Knowledge
  Graph Adaptation (KGA) module to align KG embeddings with LLM input spaces through
  cross-attention.
---

# Beyond Textual Context: Structural Graph Encoding with Adaptive Space Alignment to alleviate the hallucination of LLMs

## Quick Facts
- arXiv ID: 2509.22251
- Source URL: https://arxiv.org/abs/2509.22251
- Authors: Yifang Zhang; Pengfei Duan; Yiwen Yang; Shengwu Xiong
- Reference count: 8
- **Primary result**: SSKG-LLM achieves ~8% improvement over baselines by integrating structural and semantic KG information into LLM reasoning.

## Executive Summary
This paper addresses the hallucination problem in large language models by integrating structural and semantic information from knowledge graphs (KGs). The proposed SSKG-LLM framework employs a Knowledge Graph Retrieval (KGR) module to extract relevant subgraphs, a Knowledge Graph Encoding (KGE) module to jointly model structural and semantic patterns using GraphLM, and a Knowledge Graph Adaptation (KGA) module to align KG embeddings with LLM input spaces through cross-attention. Extensive experiments on CommonsenseQA, SIQA, and TruthfulQA datasets demonstrate that SSKG-LLM consistently outperforms existing methods, achieving approximately 8% improvement across tasks. The results validate the effectiveness of incorporating both structural and semantic information from KGs to enhance factual reasoning abilities of LLMs.

## Method Summary
SSKG-LLM is a framework that integrates knowledge graph (KG) information into large language models (LLMs) to reduce hallucinations. It operates in three stages: (1) Knowledge Graph Retrieval (KGR) extracts relevant subgraphs from ConceptNet/Wikidata using entity linking and DFS serialization; (2) Knowledge Graph Encoding (KGE) converts subgraphs to Levi graphs and encodes them using frozen GraphLM; (3) Knowledge Graph Adaptation (KGA) aligns KG embeddings with LLM space via cross-attention, and the adapted representations are concatenated with LLM queries and fine-tuned using LoRA. The approach preserves structural relationships that are lost in pure text-based retrieval-augmented generation.

## Key Results
- SSKG-LLM achieves approximately 8% improvement over baselines across CommonsenseQA, SIQA, and TruthfulQA datasets.
- Ablation study shows removing KGA causes a ~7% accuracy drop on SIQA, demonstrating the critical role of semantic alignment.
- DFS traversal outperforms Random Walk and BFS in KGR module, validating the importance of preserving reasoning chains.
- GraphLM outperforms GNNs in KGE module by better balancing structural and semantic information.

## Why This Works (Mechanism)

### Mechanism 1