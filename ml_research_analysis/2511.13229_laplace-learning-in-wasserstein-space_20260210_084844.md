---
ver: rpa2
title: Laplace Learning in Wasserstein Space
arxiv_id: '2511.13229'
source_url: https://arxiv.org/abs/2511.13229
tags:
- such
- convergence
- learning
- probability
- then
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper extends graph-based semi-supervised learning to infinite-dimensional
  Wasserstein spaces, specifically investigating Laplace Learning on Wasserstein submanifolds.
  The key contribution is establishing variational convergence from discrete graph
  p-Dirichlet energies to their continuum counterparts as sample size increases, thereby
  generalizing existing results from Euclidean spaces.
---

# Laplace Learning in Wasserstein Space

## Quick Facts
- arXiv ID: 2511.13229
- Source URL: https://arxiv.org/abs/2511.13229
- Authors: Mary Chriselda Antony Oliver; Michael Roberts; Carola-Bibiane Schönlieb; Matthew Thorpe
- Reference count: 40
- Key outcome: Extends graph-based semi-supervised learning to infinite-dimensional Wasserstein spaces, proving Γ-convergence of discrete p-Dirichlet energies to continuum functionals on Wasserstein submanifolds

## Executive Summary
This paper establishes theoretical foundations for semi-supervised learning on probability measures by extending Laplace Learning to the infinite-dimensional Wasserstein space. The authors prove that as sample size increases, discrete graph-based p-Dirichlet energies Γ-converge to continuum functionals defined on Wasserstein submanifolds, providing asymptotic consistency guarantees. They characterize the Laplace-Beltrami operator on these submanifolds and demonstrate the framework's effectiveness through numerical experiments on synthetic and real 3D object data.

## Method Summary
The method constructs a graph on empirical probability measures using either linearized Wasserstein distances or full optimal transport metrics. For linearized approximation, a reference measure is chosen and transport maps are computed to linearize the geometry. The p-Dirichlet energy is minimized subject to label constraints using a discrete p-Laplacian operator. The theoretical analysis shows this discrete energy converges to a continuum functional as sample size grows, with the convergence rate depending on the dimension of the underlying parameter space.

## Key Results
- Proves Γ-convergence of discrete graph p-Dirichlet energies to continuum functionals on Wasserstein submanifolds
- Establishes explicit characterization of the Laplace-Beltrami operator on Wasserstein submanifolds
- Demonstrates classification accuracy exceeding 99% on synthetic Gaussian data with sufficient samples
- Achieves competitive accuracy on ModelNet10 3D object dataset using limited labeled data

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The discrete graph p-Dirichlet energy Γ-converges to a continuum functional on Wasserstein submanifolds, providing theoretical guarantees for semi-supervised learning consistency.
- **Mechanism:** The proof proceeds through three intermediary functionals: (1) exact feature vectors on Λ_n, (2) parameter space S_n, and (3) continuum limit on S. The TL^p_{d_{W2}} topology bridges discrete and continuum spaces by using optimal transport maps T_n: Λ → Λ_n^{(m)} to extend discrete functions to the continuum domain for comparison.
- **Core assumption:** Assumption B.2: Existence of a linear map B_θ mapping tangent vectors on S to tangent vectors in Wasserstein space, satisfying boundedness and continuity conditions. Also requires the bi-Lipschitz embedding E: S → Λ.
- **Evidence anchors:**
  - [abstract] "proving variational convergence of a discrete graph p-Dirichlet energy to its continuum counterpart"
  - [Section 4.2, Theorem 4.7] Proves E_{ε_n,m_n,n} Γ-converges to E_∞ as n→∞ with probability one
  - [corpus] Related work "Robust Graph-Based Semi-Supervised Learning via $p$-Conductances" shows p-Laplacian methods are actively studied, but this paper uniquely extends to Wasserstein spaces
- **Break condition:** If the embedding E is not bi-Lipschitz, or if B_θ violates the boundedness conditions in Assumption B.2(a)-(d), the metric equivalence between d_{W2} on Λ and Euclidean distance on S fails.

### Mechanism 2
- **Claim:** The Laplace-Beltrami operator on Wasserstein submanifolds admits an explicit characterization linking discrete graph operators to continuum differential operators.
- **Mechanism:** Proposition 4.10 derives Δ_Λ through the first variation of the continuum energy E_∞. The operator involves integration over h ∈ R^d with the kernel η(‖B_θ(h)‖_{L^2(E(θ))}) and density ρ_S, connecting the discrete p-Laplacian L_{ε_n,m_n,n}^{(p)} to the weighted continuum operator.
- **Core assumption:** Assumption C.1: The density ρ_S on the parameter space S must be Lipschitz continuous and strictly positive. Assumption E.1-E.4 on the weight function η (non-increasing, continuous, positive at zero, compact support).
- **Evidence anchors:**
  - [abstract] "characterize the Laplace–Beltrami operator on a submanifold of the Wasserstein space"
  - [Section 4.3, Proposition 4.10] Explicit form: Δ_Λf(μ) involves integration of h·∇_θ[η(‖B_θ(h)‖_{L^2})ρ_S²|∇(f∘E)·h|^{p-2}...]
  - [corpus] "Learning functions through Diffusion Maps" uses diffusion geometry for manifold learning, but doesn't address Wasserstein geometry
- **Break condition:** If η lacks compact support or fails to be positive at zero, the energy may not capture local neighborhood structure properly. If ρ_S is not bounded away from zero, the operator becomes ill-defined.

### Mechanism 3
- **Claim:** Classification accuracy improves with sample size due to empirical Wasserstein distance convergence rates.
- **Mechanism:** Theorem 3.1 establishes that d_{W_2}(μ, μ^{(m)}) scales as ñ_q_k(m) (rates like m^{-1/k} for k≥5). As m,n → ∞ with proper scaling of ε_n, the empirical distributions converge, making graph neighborhoods more representative of true manifold geometry.
- **Core assumption:** Assumption F.1-F.2: ε_n must satisfy ε_n ≫ q_d(n) and ε_n ≫ ñ_q_k(mn) to ensure graph connectivity while maintaining locality. The ratio m_n/n must be bounded.
- **Evidence anchors:**
  - [Section 5.1] On synthetic Gaussians, accuracy reaches 98-99% for n≥1600 with only 20-60% labeled data
  - [Section 3.1, Theorem 3.1] Provides explicit convergence rates for empirical measures in W_2 and W_∞ distances
  - [corpus] "Wasserstein distance based semi-supervised manifold learning" uses Wasserstein metrics but focuses on image classification, not theoretical convergence
- **Break condition:** If ε_n decays too fast relative to q_d(n), the graph becomes disconnected. If ε_n decays too slowly, the energy fails to localize, losing ability to detect non-linear class boundaries.

## Foundational Learning

- **Concept: Γ-convergence**
  - **Why needed here:** The main theoretical tool proving that minimizers of the discrete energy converge to minimizers of the continuum functional. Without this, there's no guarantee that solving the graph-based problem approximates the manifold learning objective.
  - **Quick check question:** Can you explain why Γ-convergence plus compactness implies convergence of minimizers, whereas pointwise convergence of energies does not?

- **Concept: Wasserstein distance and optimal transport**
  - **Why needed here:** Defines the metric structure on the space of probability measures. The 2-Wasserstein distance d_{W_2} between empirical distributions forms the basis for edge weights in the graph.
  - **Quick check question:** Given two empirical measures μ^{(m)} = (1/m)Σ δ_{x_i} and ν^{(m)} = (1/m)Σ δ_{y_i}, how would you compute d_{W_2}(μ^{(m)}, ν^{(m)})?

- **Concept: Sobolev spaces on metric measure spaces**
  - **Why needed here:** The paper defines W^{1,p}(Λ, d_{W_2}, P_Λ) using a Lipschitz-type characterization (Definition 3.7) rather than derivatives, which is necessary since Λ is an infinite-dimensional Wasserstein submanifold.
  - **Quick check question:** State the Hajłasz characterization of Sobolev spaces that doesn't use derivatives explicitly.

## Architecture Onboarding

- **Component map:**
  - Empirical probability measures μ_i^{(m)} -> Linearized Wasserstein distances d_{LW_2} -> ε-connected graph with weights W_{ij}^{(m)} = (1/ε^d)η(d_{W_2}/ε) -> Discrete p-Dirichlet energy E_{ε,m,n} -> Minimizer f_n -> Classification via thresholding

- **Critical path:**
  1. Parameter selection: ε_n must satisfy lower bounds for connectivity (Assumption F.1-F.2)
  2. Transport map computation: Linearized OT maps T_i^{(m)} from reference μ_1^{(m)}
  3. Graph weight construction
  4. Solve constrained optimization (Equation 38)

- **Design tradeoffs:**
  - Full W_2 distance vs. linearized approximation: Full distance is theoretically correct but O(m³ log m); linearization is O(m²) but assumes structure around reference measure
  - ε-graph vs. k-NN graph: ε-graph has theoretical guarantees; k-NN is more robust to density variations
  - Choice of p: p=2 gives standard Laplacian; p>2 provides robustness to noise; the paper handles general p≥1

- **Failure signatures:**
  - **Disconnected graph:** ε_n too small; manifests as isolated components in weight matrix
  - **Over-smoothing:** ε_n too large; label boundaries become blurred
  - **Curse of dimensionality in base space:** When k (ambient dimension of Ω) is large, m must be exponentially large for good empirical approximation

- **First 3 experiments:**
  1. **Synthetic validation:** Replicate the 2D Gaussian experiment with n=100-1600 samples, varying labeled fraction (20-80%). Verify accuracy increases with n. Check graph connectivity by examining the number of connected components.
  2. **Sensitivity to ε scaling:** For fixed n=800, vary ε around the connectivity threshold. Plot accuracy vs. ε to identify the sweet spot and observe failure modes (disconnection for small ε, over-smoothing for large ε).
  3. **Linear vs. full W_2:** On a small subset (n=100), compare classification accuracy using full Sinkhorn-based W_2 distance vs. linearized approximation. Quantify the computational speedup vs. accuracy tradeoff.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the convergence of the discrete graph Laplacian to the continuum Laplace–Beltrami operator be rigorously proven in the Wasserstein space setting?
- Basis in paper: [explicit] The authors state on Page 4, "It is beyond the scope of the paper to prove convergence of the Laplace–Beltrami operator... but... it is reasonable to conjecture (with a possibly more restrictive lower bound on $\varepsilon$) convergence of the graph Laplacian to a continuum Laplace–Beltrami operator in this setting."
- Why unresolved: The current work establishes Gamma-convergence of the energy functionals, which is sufficient for the convergence of minimizers but does not imply spectral or pointwise convergence of the operators themselves.
- What evidence would resolve it: A proof demonstrating the pointwise consistency or spectral convergence of the discrete operators defined in Section 4.3 to the continuum operator $\Delta_{\Lambda}$.

### Open Question 2
- Question: Can the Laplace Learning framework be extended to handle unbalanced probability measures with varying total mass?
- Basis in paper: [explicit] Page 15 identifies this as a limitation and states: "Extending the framework to unbalanced variants — such as the Hellinger–Kantorovich (Wasserstein–Fisher–Rao) distance... constitutes a natural direction for future research."
- Why unresolved: The current theoretical framework and numerical implementation restrict inputs to measures of equal total mass supported on the same number of points ($m$).
- What evidence would resolve it: The derivation of a variational consistency result (Gamma-convergence) for the graph p-Dirichlet energy defined using unbalanced transport metrics.

### Open Question 3
- Question: Can the convergence rates for the empirical 2-Wasserstein distance be improved for general domains with Lipschitz boundaries?
- Basis in paper: [explicit] Remark 3.2 on Page 9 notes that while improved rates are known for specific cases (like the cube or torus), "We conjecture that similar improved rates could hold for general domains with Lipschitz boundary by adapting the arguments in [10]."
- Why unresolved: The authors currently rely on rates for general domains that may be suboptimal (e.g., $m^{-1/2}$ for $k=4$) compared to the expected $m^{-1/k}$ scaling found in specialized literature.
- What evidence would resolve it: A rigorous proof establishing the rate of convergence $d_{W_2}(\mu, \mu^{(m)}) \le C m^{-1/k}$ (up to logarithmic factors) for domains with Lipschitz boundaries.

## Limitations
- The framework requires specific geometric assumptions including bi-Lipschitz embedding between parameter space and Wasserstein submanifold
- Linearized Wasserstein approximation introduces approximation error that may accumulate in high-dimensional settings
- Computational complexity remains high despite linearization, particularly for large datasets
- The method is limited to balanced probability measures of equal total mass

## Confidence

**High Confidence:** The Γ-convergence result under stated assumptions, the Laplace-Beltrami operator characterization, and the numerical improvement with sample size on synthetic data.

**Medium Confidence:** The asymptotic consistency claims for real-world datasets, the robustness of the linearized approximation across diverse data types, and the practical utility of the theoretical convergence rates.

**Low Confidence:** The assumption verification for arbitrary real datasets, the behavior in extremely high-dimensional settings (k > 10), and the scalability of the method beyond moderate-sized problems (n > 10⁴).

## Next Checks

1. **Assumption verification experiment:** Design a systematic test to verify Assumption B.2 (the B_θ linear map conditions) on real datasets. Use the ModelNet10 data to estimate whether the embedding E: S → Λ satisfies bi-Lipschitz conditions empirically.

2. **High-dimensional stress test:** Generate synthetic data in higher dimensions (k = 5, 10, 20) following the same manifold structure but increasing ambient dimension. Track how the required sample size m scales with k to empirically validate the curse of dimensionality claims.

3. **Approximation error quantification:** Implement both the full Sinkhorn-based W₂ distance and the linearized approximation for a subset of the ModelNet10 dataset. Measure the classification accuracy difference and compute the exact computational speedup to quantify the approximation trade-off.