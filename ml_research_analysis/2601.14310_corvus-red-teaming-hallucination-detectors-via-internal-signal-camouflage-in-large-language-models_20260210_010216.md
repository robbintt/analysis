---
ver: rpa2
title: 'CORVUS: Red-Teaming Hallucination Detectors via Internal Signal Camouflage
  in Large Language Models'
arxiv_id: '2601.14310'
source_url: https://arxiv.org/abs/2601.14310
tags:
- corvus
- telemetry
- answer
- attention
- hidden
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies how internal telemetry signals used by single-pass\
  \ hallucination detectors can be manipulated through model-side adversarial adaptation.\
  \ It introduces CORVUS, a red-teaming procedure that fine-tunes lightweight LoRA\
  \ adapters to camouflage detector-visible telemetry\u2014specifically token entropy,\
  \ hidden log-volume, and attention diagonality\u2014under teacher forcing, including\
  \ an embedding-space FGSM stress test for attention."
---

# CORVUS: Red-Teaming Hallucination Detectors via Internal Signal Camouflage in Large Language Models

## Quick Facts
- arXiv ID: 2601.14310
- Source URL: https://arxiv.org/abs/2601.14310
- Authors: Nay Myat Min; Long H. Pham; Hongyu Zhang; Jun Sun
- Reference count: 9
- Primary result: Lightweight LoRA adapters trained to camouflage internal telemetry degrade hallucination detectors' AUROC by up to 27 points and collapse TPR at fixed FPR thresholds.

## Executive Summary
This paper introduces CORVUS, a red-teaming framework that fine-tunes lightweight LoRA adapters to camouflage internal telemetry signals used by single-pass hallucination detectors. By targeting token entropy, hidden log-volume, and attention diagonality under teacher forcing, CORVUS reduces the separability of hallucinated vs. faithful outputs without requiring hallucination labels. Trained on 1,000 out-of-distribution Alpaca instructions, the adapters transfer to the FAVA-Annotation benchmark and significantly degrade both training-free (LLM-Check) and probe-based (SEP, ICR-probe) detectors across four open-weight models. The results demonstrate brittleness in internal-signal-based detection and highlight the need for external grounding or cross-model validation defenses.

## Method Summary
CORVUS fine-tunes LoRA adapters (rank=64, α=128) attached to attention projection modules (q_proj, k_proj) to manipulate three telemetry signals: token entropy (TE), hidden log-volume (HV), and attention diagonality (AD). The training objective combines cross-entropy loss with telemetry-shaping terms computed under teacher forcing. An FGSM embedding perturbation (ε=1e-2) provides an additional attention-shaping signal via ADadv. The model is trained for one epoch on 1,000 out-of-distribution Alpaca instruction-response pairs, targeting only ~0.5% of parameters. The resulting adapters transfer zero-shot to FAVA-Annotation, degrading detector performance without significant generation utility loss.

## Key Results
- Degrades both training-free detectors (LLM-Check) and probe-based detectors (SEP, ICR-probe), reducing AUROC by up to 27 points and collapsing TPR at fixed FPR thresholds.
- Trained on 1,000 out-of-distribution Alpaca instructions (<0.5% trainable parameters), CORVUS transfers to FAVA-Annotation across Llama-2, Vicuna, Llama-3, and Qwen2.5.
- Shows substantial drops in AUROC and calibration shifts, indicating brittleness of internal-signal–based detection to adaptive model-side attacks.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Single-pass hallucination detectors rely on separable internal telemetry (token entropy, hidden log-volume, attention diagonality) to distinguish hallucinated from faithful outputs; CORVUS reduces this separability.
- **Mechanism**: CORVUS trains LoRA adapters to reshape TE, HV, and AD signals under teacher forcing. By minimizing token entropy (βte=0.07), maximizing hidden log-volume (βhv=0.05), and increasing attention diagonality on an FGSM-perturbed pass (λadv=0.5), it shifts the joint distribution of telemetry features so that clean-calibrated detector thresholds fail.
- **Core assumption**: Detectors trained on clean models cannot adapt quickly to model-side telemetry shifts; separability is not robust to adaptive perturbations.
- **Evidence anchors**:
  - [abstract]: "degrades both training-free detectors (e.g., LLM-Check) and probe-based detectors (e.g., SEP, ICR-probe), reducing AUROC by up to 27 points and collapsing TPR at fixed FPR thresholds."
  - [section 7]: "Detection succeeds when the between-class separation Δ is large... CORVUS modifies the model... so that... the induced telemetry T(x) for hallucinated and faithful instances becomes less separable."
  - [corpus]: Limited direct evidence; neighbor papers discuss detection via internal representations (HIDE, FaithSCAN) but do not evaluate adaptive model-side attacks. FMR scores ~0.55-0.60 suggest moderate relatedness to detection, not adversarial robustness.
- **Break condition**: Detectors that can retrain or recalibrate online on adapted telemetry, or those relying on external grounding (retrieval), may restore separability.

### Mechanism 2
- **Claim**: Lightweight LoRA adapters (targeting q_proj, k_proj) are sufficient to manipulate high-level telemetry signals without degrading generation utility.
- **Mechanism**: LoRA adds low-rank matrices (rank 64, α=128) to attention projections; gradients from the CORVUS loss (Eq. 7) flow through teacher-forced forward passes to update only these matrices. The FGSM embedding perturbation (ε=1e-2) provides an additional attention-shaping signal via ADadv, improving transfer to paraphrased prompts.
- **Core assumption**: High-dimensional internal signals (HV, AD) can be steered by low-rank updates without disrupting cross-entropy loss (LCE) or surface fluency.
- **Evidence anchors**:
  - [section 3.7]: "All telemetry terms are computed under teacher forcing over the answer window... The FGSM direction is computed by differentiating the surrogate detector score −AD with respect to the input embeddings."
  - [section 4]: "We attach LoRA adapters with rank r=64 and scaling α=128, and target only the attention projection modules (q_proj, k_proj). All base model parameters are frozen."
  - [corpus]: Not directly addressed; neighbor papers focus on detection, not adapter-based evasion.
- **Break condition**: If critical telemetry components depend on non-attention layers (e.g., MLP, residuals), q/k-only LoRA may be insufficient; higher rank or broader module coverage may be needed.

### Mechanism 3
- **Claim**: CORVUS trained on 1,000 Alpaca instruction–response pairs transfers to FAVA-Annotation without further tuning.
- **Mechanism**: The loss is label-free and telemetry-directed; it does not require hallucination labels. By shaping general properties (entropy, hidden dispersion, attention concentration) that are shared across datasets, the learned adapters generalize to out-of-distribution prompts.
- **Core assumption**: TE/HV/AD telemetry patterns exploited by detectors are task-agnostic; their shifts are not dependent on specific prompt semantics.
- **Evidence anchors**:
  - [abstract]: "Trained on 1,000 out-of-distribution Alpaca instructions (<0.5% trainable parameters), CORVUS transfers to FAVA-Annotation across Llama-2, Vicuna, Llama-3, and Qwen2.5."
  - [section 3.2]: "CORVUS trains only on an out-of-distribution instruction set and optimizes telemetry terms that transfer to the auditing setting."
  - [corpus]: Not verified externally; neighbor papers do not report cross-dataset transfer for adversarial attacks.
- **Break condition**: Strong distribution shift (e.g., different domains, languages) may reduce transfer; some per-dataset calibration of loss coefficients may be required.

## Foundational Learning

- **Concept**: **Teacher Forcing in Decoder-Only LLMs**
  - **Why needed here**: CORVUS computes all telemetry under teacher forcing on fixed (prompt, answer) pairs; understanding logit alignment and causal masking is essential for implementing TE/HV/AD extraction.
  - **Quick check question**: Given a prompt of length p and answer of length a, which logits index the answer-window predictive distributions?

- **Concept**: **LoRA (Low-Rank Adaptation)**
  - **Why needed here**: CORVUS uses LoRA to modify attention projections while freezing base weights; knowing how rank, scaling, and module targeting affect gradient flow is critical for reproducing or extending the method.
  - **Quick check question**: If LoRA rank is 8 instead of 64, would you expect stronger, weaker, or similar telemetry steering, and why?

- **Concept**: **FGSM (Fast Gradient Sign Method) in Embedding Space**
  - **Why needed here**: The AD-targeted FGSM step constructs a perturbed pass that provides an additional attention-shaping signal; understanding how embedding perturbations affect attention is necessary for tuning ε and interpreting ADadv.
  - **Quick check question**: Why does FGSM here increase a surrogate detector score rather than decrease it, and how does that affect AD?

## Architecture Onboarding

- **Component map**: Alpaca instructions → LoRA adapters (q_proj, k_proj) → clean pass → TE/HV/AD extraction → FGSM pass → ADadv → combined loss → updated LoRA parameters

- **Critical path**:
  1. Tokenize prompt+answer; identify answer window W.
  2. Run clean pass; extract logits, hidden states, attentions.
  3. Compute TE (mean categorical entropy), HV (log-det of centered hidden Gram), AD (mean log of attention diagonal).
  4. Compute FGSM perturbation on embeddings via ∇E(−Σ ADℓ); run perturbed pass; extract ADadv.
  5. Compute L; backprop; update LoRA parameters.

- **Design tradeoffs**:
  - **Rank/α**: Higher rank increases steering capacity but may affect model capacity and transfer; rank=64, α=128 is a moderate choice.
  - **ε (FGSM step)**: Larger ε strengthens attention shaping but may harm utility; default ε=1e-2 balances transfer and fluency.
  - **Loss coefficients**: βte, βhv, λadv control emphasis on each telemetry family; attention (ADadv) is the primary lever for LLM-Check evasion.

- **Failure signatures**:
  - **Degenerate generation**: If LoRA over-shapes telemetry, models may produce repetitive or refusal-style outputs; check median GPT-2 perplexity shift (e.g., >5 points).
  - **No telemetry shift**: If AUROC does not drop, check gradient flow (are LoRA parameters updating?) and FGSM magnitude (is ε too small?).
  - **Utility collapse**: If cross-entropy loss rises sharply, reduce βte, βhv, or λadv.

- **First 3 experiments**:
  1. **Reproduce Table 1 on one model**: Train CORVUS on Llama-2-7B-Chat with default hyperparameters; evaluate PPL, Window-Entropy, LLM-Check (Hidden/Attn), SEP, ICR-probe on FAVA-Annotation; report AUROC, TPR@5%FPR, F1.
  2. **Ablate telemetry terms**: Train variants with only TE, only HV, only AD, TE+HV, etc.; quantify contribution to AUROC drops per Table 3.
  3. **Sweep FGSM ε**: Train with ε=0, 1.5e-3, 5e-3, 1e-2; evaluate attention-score AUROC and paraphrase transfer to isolate AD-targeted contribution per Table 4.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can the telemetry camouflage objective be combined with correctness objectives to improve model factuality rather than merely hiding errors?
- **Basis in paper**: [explicit] The Conclusion states, "Developing attacks that improve factuality... are left for future work."
- **Why unresolved**: The current CORVUS implementation optimizes solely for evasion (reducing detector visibility) using a label-free objective on OOD data, without modifying the underlying factual correctness of the model's outputs.
- **What evidence would resolve it**: A modified training regime that includes a factual consistency loss, demonstrating that the model can both evade detectors and increase accuracy on benchmarks like FAVA-Annotation.

### Open Question 2
- **Question**: Do retrieval-based or external-knowledge hallucination defenses maintain robustness against internal signal camouflage?
- **Basis in paper**: [explicit] The Abstract concludes there is a need for "defenses that incorporate external grounding," and the Limitations section notes that "more advanced defenses (e.g., retrieval-based fact-checking) were out of scope."
- **Why unresolved**: The study focused strictly on single-pass internal telemetry detectors; it is unknown if reshaping internal activations affects the model's ability to maintain consistency with retrieved external evidence.
- **What evidence would resolve it**: Evaluating CORVUS-adapted models against external grounding methods (e.g., RAG-specific detectors or AlignScore) to see if internal signal manipulation disrupts external fact-checking metrics.

### Open Question 3
- **Question**: Can probe-based detectors be hardened against this attack via adversarial training?
- **Basis in paper**: [explicit] The Conclusion suggests developing "more robust hallucination defenses... by incorporating adversarial training" and "adversary-aware auditing."
- **Why unresolved**: In the paper's evaluation, probe-based detectors (like ICR-probe and SEP) were trained only on clean-model telemetry, rendering them vulnerable to the distribution shifts induced by LoRA adapters.
- **What evidence would resolve it**: Retraining probe-based detectors on a mixture of clean and CORVUS-adapted telemetry, then verifying if the detector recovers its AUROC performance against the attack.

## Limitations

- **Distribution Shift and Transfer Robustness**: While CORVUS demonstrates transfer from Alpaca to FAVA-Annotation, the robustness of this transfer across different domains, languages, or instruction types remains uncertain. The 1,000 training pairs represent a small fraction (<0.5% trainable parameters) of the model's total capacity, raising questions about scalability and generalizability.
- **Detector-Specific Vulnerabilities**: The effectiveness of CORVUS varies significantly across detectors—particularly between training-free methods (LLM-Check) and probe-based approaches (SEP, ICR-probe). The paper shows that SEP and ICR-probe are more resistant to evasion, suggesting that some internal-signal detectors may be inherently more robust.
- **Utility Preservation Trade-offs**: CORVUS claims to preserve generation utility while manipulating telemetry, but the paper only reports modest perplexity changes (<3 points). The relationship between telemetry manipulation intensity and generation quality degradation is not fully characterized, particularly for longer or more complex outputs.

## Confidence

- **High Confidence**: The core mechanism of using LoRA adapters to manipulate internal telemetry signals (token entropy, hidden log-volume, attention diagonality) is well-supported by the experimental results, particularly the ablation studies showing the relative contribution of each term to detector degradation.
- **Medium Confidence**: The claim of zero-shot transfer from Alpaca to FAVA-Annotation is supported by the reported results, but the underlying reasons for successful transfer are not fully explained. The paper attributes this to task-agnostic telemetry patterns, but empirical validation across diverse domains is limited.
- **Low Confidence**: The assertion that CORVUS represents a fundamental vulnerability in all single-pass hallucination detectors may be overstated. The results show significant variation in detector resilience, and the paper does not fully explore whether certain detector architectures or calibration strategies could mitigate the attack.

## Next Checks

1. **Cross-Domain Transfer Evaluation**: Test CORVUS-trained adapters on FAVA-Annotation variants from different domains (e.g., scientific, medical, legal) to quantify transfer robustness and identify failure modes when distribution shift is substantial.

2. **Detector Architecture Vulnerability Mapping**: Systematically evaluate CORVUS against a broader range of detector architectures, including those with external grounding mechanisms or online recalibration capabilities, to map the vulnerability landscape more comprehensively.

3. **Telemetry-Utility Trade-off Analysis**: Conduct a systematic study varying the loss coefficients (βte, βhv, λadv) and FGSM step size (ε) to characterize the precise relationship between telemetry manipulation intensity and generation quality degradation, including effects on output diversity and factual consistency.