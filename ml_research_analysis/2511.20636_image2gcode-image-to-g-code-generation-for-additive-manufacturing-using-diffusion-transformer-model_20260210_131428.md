---
ver: rpa2
title: 'Image2Gcode: Image-to-G-code Generation for Additive Manufacturing Using Diffusion-Transformer
  Model'
arxiv_id: '2511.20636'
source_url: https://arxiv.org/abs/2511.20636
tags:
- manufacturing
- page
- geometric
- diffusion
- design
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Image2Gcode: Image-to-G-code Generation for Additive Manufacturing Using Diffusion-Transformer Model

## Quick Facts
- **arXiv ID:** 2511.20636
- **Source URL:** https://arxiv.org/abs/2511.20636
- **Reference count:** 25
- **Primary result:** None specified in the paper

## Executive Summary
Image2Gcode presents a diffusion-transformer model that generates printer-ready G-code toolpaths directly from 2D images, bypassing traditional CAD and STL workflows. The system uses a frozen DinoV2 vision encoder combined with a 1D U-Net decoder conditioned through cross-attention to transform images into (X,Y,E) keypoint sequences representing toolpaths. The approach achieves reduced travel distances compared to heuristic slicers while maintaining dimensional accuracy in physical prints.

## Method Summary
The model processes 224×224 grayscale slice images through a frozen DinoV2-Small encoder (layers 1-10 frozen, 11-12 fine-tuned) to extract 256 patch tokens. These features condition a 1D U-Net decoder via cross-attention, where keypoint features serve as queries and DinoV2 tokens provide keys/values. The diffusion process denoises Gaussian noise over 500 steps using x₀-prediction with a cosine schedule. Training uses masked L2 reconstruction loss with SNR-based timestep weighting on sequences padded to N_max. The Slice-100K dataset provides 100K+ STL-G-code pairs, with 10% held for validation.

## Key Results
- Dimensional accuracy validation through physical print testing
- Travel distance analysis showing reduced travel compared to heuristic slicers
- Qualitative toolpath comparison demonstrating data-driven strategy diversity

## Why This Works (Mechanism)

### Mechanism 1
Pretrained vision encoders transfer geometric understanding to manufacturing domains without task-specific pretraining. DinoV2's self-supervised representations encode edge structures, geometric primitives, and spatial relationships learned from 142M generic images. When frozen and combined with cross-attention, these features condition toolpath generation on slice geometry. The final two transformer layers are fine-tuned to adapt high-level features while preserving low-level geometric representations. Assumption: Generic visual features transfer to manufacturing-specific geometry without catastrophic forgetting.

### Mechanism 2
Cross-attention conditioning enables coherent sequence generation while respecting global spatial constraints. The 1D U-Net's keypoint features serve as queries (Q) while all 256 DinoV2 patch tokens provide keys and values (K, V). This allows each trajectory point to attend to the full spatial field of the slice image. Multi-scale cross-attention at multiple U-Net depths injects hierarchical geometric context—from coarse structure to fine boundary details. Assumption: Attention can learn the implicit mapping between visual regions and their corresponding toolpath priorities without explicit spatial supervision.

### Mechanism 3
Diffusion models can learn distributions over valid manufacturing toolpaths rather than memorizing single solutions. The iterative denoising process (T=500 steps) transforms Gaussian noise into coherent trajectories. By training with x₀-prediction and masked L2 loss over variable-length sequences, the model learns the manifold of feasible toolpaths. The stochastic sampling enables diverse valid outputs—same geometry can yield different infill patterns (diagonal hatching vs. concentric shells). Assumption: The training distribution (Slice-100K) sufficiently covers the space of valid toolpaths for generalization.

## Foundational Learning

- **Denoising Diffusion Probabilistic Models (DDPMs)**: Core generative mechanism. Understanding forward corruption (adding noise) and reverse denoising (predicting clean data) is essential for debugging generation quality and adjusting inference parameters. Quick check: Can you explain why predicting x₀ directly differs from predicting noise ε, and what this implies for training stability?

- **Cross-Attention in Conditional Generation**: Enables the toolpath decoder to query visual features. Critical for understanding how geometric constraints propagate from encoder to generated sequences. Quick check: Given query dimension N (trajectory length) and key/value dimension P (256 patch tokens), what is the attention matrix shape and its memory complexity?

- **G-code Representation and Slicing**: The model outputs (X, Y, E) keypoints that must be post-processed into valid G-code. Understanding coordinate systems, extrusion modes, and toolpath semantics is required for debugging physical print failures. Quick check: What does monotonically increasing E-values indicate, and what happens if the model predicts non-monotonic extrusion?

## Architecture Onboarding

- **Component map**: Input Image (224×224) → [DinoV2-Small Encoder] → 256 patch tokens (384-dim each) → [1D U-Net] → Keypoint sequence [N_max × 3] → [Post-processing] → Validated G-code

- **Critical path**: DinoV2 feature extraction → Cross-attention fusion → Denoising prediction at high noise levels (early timesteps). Errors in visual encoding propagate through all subsequent denoising steps.

- **Design tradeoffs**:
  - Slice-based vs. volumetric: 2D slices reduce memory/compute but cannot enforce inter-layer coherence or handle topological changes
  - x₀-prediction vs. ε-prediction: Direct prediction simplifies loss but may be less stable at high noise levels
  - Frozen vs. fine-tuned encoder: Freezing preserves pretrained features but limits adaptation; partial fine-tuning balances both
  - Assumption: 500 diffusion steps chosen empirically—fewer steps may trade quality for speed

- **Failure signatures**:
  - Non-monotonic E-values → extruder retracting during deposition, causing under-extrusion
  - Disconnected perimeters → attention failed to capture global connectivity; check cross-attention weights at deep layers
  - Infill extends beyond boundary → encoder-geometry alignment failure; verify preprocessing (edge detection, contour extraction)
  - High-curvature artifacts → insufficient keypoints or denoising收敛 too early

- **First 3 experiments**:
  1. Ablate encoder fine-tuning: Freeze all DinoV2 layers vs. fine-tune last 2 vs. fine-tune all. Measure geometric fidelity (boundary IoU) on held-out geometries. Expect: full fine-tuning may overfit to training distribution.
  2. Vary diffusion steps at inference: Test T ∈ {100, 250, 500, 1000}. Measure toolpath coherence and travel distance. Expect: fewer steps faster but may produce artifacts at boundaries.
  3. Test distribution shift robustness: Evaluate on photographs → sketches → CAD renders. Track where quality degrades. Expect: photographs harder due to lighting/texture; edge preprocessing is the bottleneck.

## Open Questions the Paper Calls Out

### Open Question 1
Can a hierarchical generation pipeline enable the framework to handle complex 3D geometries with significant topological changes between layers or intricate internal cavities? The current slice-based formulation cannot directly handle complex 3D geometries with significant topological changes, and a hierarchical generation pipeline is proposed as a future direction.

### Open Question 2
How can conditioning mechanisms be extended to allow explicit user control over manufacturing parameters such as infill density, pattern type, and mechanical performance requirements? The paper suggests incorporating explicit control over these parameters would enable task-specific toolpath optimization, but current framework learns infill strategies implicitly from data distribution.

### Open Question 3
Does the observed reduction in travel distance in generated toolpaths compromise or improve the structural integrity and mechanical performance of the printed parts? While dimensional accuracy is validated, comprehensive mechanical testing would be required to fully characterize whether reduced travel distance translates to equivalent or improved structural performance.

### Open Question 4
Can multi-agent LLM systems be effectively integrated to translate high-level natural language specifications into valid conditioning signals for the diffusion model? The paper suggests LLM integration where agents could interpret user specifications and translate them into conditioning signals, but mapping abstract linguistic intent to specific diffusion parameters remains conceptual.

## Limitations
- Current formulation cannot handle complex 3D geometries with significant topological changes between layers
- Limited user control over manufacturing parameters such as infill density and pattern type
- Mechanical performance of parts produced with data-driven toolpaths has not been quantified against standard slicers

## Confidence
- Mechanism 1: High - well-supported by encoder architecture and transfer learning literature
- Mechanism 2: Medium - cross-attention design is clear but attention's ability to capture complex spatial relationships is assumed
- Method reproducibility: Medium - several critical hyperparameters (N_max, SNR weighting formula, cross-attention layer count) are unspecified

## Next Checks
1. Verify DinoV2 encoder feature distributions between synthetic slices and real-world images to assess domain shift
2. Measure travel distance reduction and boundary accuracy on held-out geometries to validate geometric fidelity claims
3. Conduct physical print validation on diverse geometries (simple vs. complex) to test real-world robustness