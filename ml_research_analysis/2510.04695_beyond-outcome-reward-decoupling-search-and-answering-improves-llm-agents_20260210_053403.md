---
ver: rpa2
title: 'Beyond Outcome Reward: Decoupling Search and Answering Improves LLM Agents'
arxiv_id: '2510.04695'
source_url: https://arxiv.org/abs/2510.04695
tags:
- search
- reward
- agent
- training
- recall
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of training search-augmented large
  language model agents that interleave reasoning and retrieval before answering questions.
  The authors find that agents trained with only outcome-based rewards (e.g., exact
  match) exhibit multiple search deficiencies including failure to invoke tools, invalid
  queries, and redundant searches, ultimately degrading answer quality.
---

# Beyond Outcome Reward: Decoupling Search and Answering Improves LLM Agents

## Quick Facts
- arXiv ID: 2510.04695
- Source URL: https://arxiv.org/abs/2510.04695
- Reference count: 13
- Key result: DeSA-trained agents achieve 64.5% search recall vs 59.5% for outcome-only baselines, with 8.0%/5.6% gains in EM accuracy on 3B/7B models

## Executive Summary
This paper addresses the problem of training search-augmented LLM agents that interleave reasoning and retrieval before answering questions. The authors find that agents trained with only outcome-based rewards (e.g., exact match) exhibit multiple search deficiencies including failure to invoke tools, invalid queries, and redundant searches, ultimately degrading answer quality. They propose DeSA (Decoupling Search-and-Answering), a two-stage training framework that explicitly separates search optimization from answer generation. In Stage 1, agents are trained to improve search effectiveness with retrieval recall-based rewards, while in Stage 2, outcome rewards are used to optimize final answer generation. Across seven QA benchmarks, DeSA-trained agents substantially improve search behaviors and outperform strong single-stage training approaches.

## Method Summary
DeSA implements a two-stage GRPO training framework. Stage 1 optimizes for recall reward - training the agent to retrieve documents containing the ground truth answer using binary signals. Stage 2 initializes from Stage 1's best checkpoint and optimizes for exact match reward on final answers. The critical transition occurs when validation EM peaks then declines (around 50-100 steps), indicating the policy is starting to optimize for recall at the expense of answer quality. The method uses Qwen2.5-3B/7B-Instruct models with E5 retrievers and Wikipedia corpus, training on NQ/HotpotQA with evaluation on 7 benchmarks.

## Key Results
- DeSA achieves 64.5% search recall compared to 59.5% for outcome-only baselines
- Outperforms single-stage approaches by 8.0% on 3B model and 5.6% on 7B model
- Reduces deficient search rate from 23.36% to 14.60% on NQ+HotpotQA
- Shows consistent improvements across seven QA benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Outcome-only rewards create credit assignment failures in multi-step search trajectories.
- **Mechanism:** The exact match reward provides binary feedback only at trajectory termination. With multiple interleaved reasoning-search-answer steps, the agent cannot determine which intermediate search actions contributed to success or failure, leading to exploration of low-value behaviors (no search, duplicate queries, invalid tool calls).
- **Core assumption:** Reward sparsity directly causes behavioral collapse rather than exploration inefficiency alone.
- **Evidence anchors:**
  - [abstract]: "outcome-only supervision provides sparse, delayed feedback, it may suffer from the well-known credit assignment challenges"
  - [section 4.2]: "training with EM reward alone does not effectively translate the outcome reward signal into the optimization of intermediate search actions"
  - [corpus]: HiPRAG paper confirms "suboptimal search behaviors exist widely, such as over-search and under-search" under outcome rewards
- **Break condition:** If dense per-step rewards (e.g., retrieval quality scores) produce similar deficient behaviors, credit assignment is not the primary failure mode.

### Mechanism 2
- **Claim:** Decoupling recall and outcome rewards prevents gradient interference during policy updates.
- **Mechanism:** Simultaneous optimization of $R_{recall}$ and $R_{EM}$ creates conflicting gradients—recall rewards encourage exhaustive search while EM rewards may benefit from selective precision. Sequential training allows the policy to first converge to a search-competent region before fine-tuning for answer extraction.
- **Core assumption:** The reward functions are not perfectly aligned; optimizing both jointly creates non-convex optimization landscape with local minima.
- **Evidence anchors:**
  - [section 6.3.2]: "mixing the search reward (recall) and outcome reward (EM) creates confusing optimization pressures"
  - [section 5]: "explicitly separates these two objectives and applies tailored supervision sequentially"
  - [corpus]: Process vs. Outcome Reward paper finds "process-based rewards are essential for training agentic RAG systems"
- **Break condition:** If a properly weighted composite reward $R = \alpha \cdot R_{recall} + (1-\alpha) \cdot R_{EM}$ matches DeSA performance, gradient interference is not the causal mechanism.

### Mechanism 3
- **Claim:** Recall-based rewards in Stage 1 provide curriculum learning that constrains the search policy to viable regions before answer optimization.
- **Mechanism:** Stage 1 training with $R_{recall}$ produces a policy that successfully retrieves relevant documents (62.5% recall after Stage 1 vs 45.4% baseline). This creates a better initialization for Stage 2, avoiding local minima where the agent learns to answer from parametric knowledge alone.
- **Core assumption:** The search skill acquisition phase must reach a minimum competency threshold before answer optimization can improve final accuracy.
- **Evidence anchors:**
  - [section 6.2 Figure 5]: After Stage 1, "deficient search rate drops significantly below the Search-R1 baseline (14.60 vs. 23.36), while its recall already surpasses it"
  - [section 6.2]: "Stage 2 training also allows the agent to adapt its search behavior from exhaustive search to more accurate"
  - [corpus]: Repurposing Synthetic Data paper shows fine-grained supervision "improves search agent performance" over sparse rewards
- **Break condition:** If randomly initialized Stage 2 training (without Stage 1) with outcome rewards converges to similar performance given sufficient compute, curriculum structure is not the causal factor.

## Foundational Learning

- **Concept: Credit Assignment in RL**
  - **Why needed here:** The paper's core diagnosis is that outcome-only rewards fail because intermediate search actions cannot be correctly attributed credit for final outcomes.
  - **Quick check question:** Can you explain why sparse rewards make it harder for an agent to learn which specific actions in a 10-step trajectory led to success?

- **Concept: Sequential Decision-Making under Partial Observability**
  - **Why needed here:** The search agent operates in a POMDP where each search reveals partial information, and the agent must maintain history $H_t$ across steps.
  - **Quick check question:** Why does the agent need to condition its next action on $(q, a_0, d_0, a_1, d_1, ...)$ rather than just the current query?

- **Concept: Reward Shaping and Dense Feedback**
  - **Why needed here:** DeSA's Stage 1 uses recall as a dense process reward that provides feedback at trajectory end but directly measures intermediate search quality.
  - **Quick check question:** How does $R_{recall}$ differ from $R_{EM}$ in terms of when feedback is available and what behavior it directly incentivizes?

## Architecture Onboarding

- **Component map:**
  User Query → [LLM Policy π_θ] → <search>query</search> → [Retriever E5] → Top-3 passages → History H_t ← information ←

- **Critical path:**
  1. Implement GRPO with group sampling (G=5 responses per prompt)
  2. Build search environment returning top-3 passages per query
  3. Implement recall reward checker: `ground_truth ∈ Aggregate(all_retrieved_chunks)`
  4. Train Stage 1 until EM score peaks then drops (transition signal)
  5. Load Stage 1 checkpoint, train Stage 2 with EM reward only

- **Design tradeoffs:**
  - **Simple $R_{recall}$ vs composite rewards:** Adding penalties for duplicate/invalid queries (Section 6.3.1) reduces deficient behaviors but restricts recall exploration. The simpler reward generalizes better.
  - **Transition point selection:** Too early → insufficient search skills; too late → EM collapse requires recovery. Paper uses "last pre-collapse checkpoint" (≈50-100 steps).
  - **Retriever choice:** Using E5 with Wikipedia corpus; changing retriever may require re-tuning transition point.

- **Failure signatures:**
  - Stage 1 overtraining: EM curve peaks then crashes while recall continues rising (Figure 6a)
  - Missing recall ground truth: If answer entity is not in corpus, $R_{recall}$ is always 0
  - Invalid search tag format: Agent generates `<search>query/search>` instead of `<search>query</search>`
  - Duplicate query loops: Agent issues identical query 3+ times consecutively

- **First 3 experiments:**
  1. **Reproduce failure modes:** Train Qwen2.5-3B-Instruct with EM-only reward on NQ+Hotpot