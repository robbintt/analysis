---
ver: rpa2
title: 'Bridging Unsupervised and Semi-Supervised Anomaly Detection: A Theoretically-Grounded
  and Practical Framework with Synthetic Anomalies'
arxiv_id: '2506.13955'
source_url: https://arxiv.org/abs/2506.13955
tags:
- anomalies
- synthetic
- data
- anomaly
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper bridges unsupervised and semi-supervised anomaly detection\
  \ by introducing synthetic anomalies into the training process. The key innovation\
  \ is a mathematical framework showing that synthetic anomalies improve anomaly modeling\
  \ in low-density regions and enable optimal convergence guarantees for neural network\
  \ classifiers\u2014the first theoretical result for semi-supervised AD."
---

# Bridging Unsupervised and Semi-Supervised Anomaly Detection: A Theoretically-Grounded and Practical Framework with Synthetic Anomalies

## Quick Facts
- arXiv ID: 2506.13955
- Source URL: https://arxiv.org/abs/2506.13955
- Reference count: 40
- One-line primary result: Synthetic anomalies improve anomaly modeling in low-density regions and enable optimal convergence guarantees for neural network classifiers—the first theoretical result for semi-supervised AD.

## Executive Summary
This paper bridges unsupervised and semi-supervised anomaly detection by introducing synthetic anomalies into the training process. The key innovation is a mathematical framework showing that synthetic anomalies improve anomaly modeling in low-density regions and enable optimal convergence guarantees for neural network classifiers—the first theoretical result for semi-supervised AD. Empirically, adding synthetic anomalies consistently improves performance across five diverse datasets (tabular, image, and text), benefiting both known and unknown anomalies. The method generalizes beyond the theoretical model to other classification-based AD approaches like ES and DROCC, though less effectively for autoencoder-based methods.

## Method Summary
The framework adds synthetic anomalies sampled uniformly from the data support, training a binary classifier to distinguish normal data from combined known and synthetic anomalies. This approach validates the generalizability of synthetic anomalies in AD and opens avenues for future research in other AD settings.

## Key Results
- Synthetic anomalies improve anomaly modeling in low-density regions and enable optimal convergence guarantees for neural network classifiers
- Adding synthetic anomalies consistently improves performance across five diverse datasets (tabular, image, and text)
- Benefits both known and unknown anomalies while generalizing to other classification-based AD approaches

## Why This Works (Mechanism)
Synthetic anomalies improve anomaly modeling by providing additional training signal in low-density regions where anomalies are rare. The uniform sampling ensures coverage of the data support, helping the classifier learn better decision boundaries. The theoretical framework shows that this approach enables optimal convergence guarantees for neural network classifiers in semi-supervised AD settings.

## Foundational Learning
- **Binary classification for AD**: Why needed - transforms AD into a supervised learning problem; Quick check - verify binary cross-entropy loss implementation
- **Uniform sampling for synthetic anomalies**: Why needed - ensures coverage of data support; Quick check - confirm synthetic samples fall within [0,1]^d
- **Mixture modeling**: Why needed - combines normal data with known and synthetic anomalies; Quick check - verify mixture weights sum to 1
- **Shallow ReLU networks**: Why needed - provides theoretical convergence guarantees; Quick check - confirm network depth ≤ 3
- **Logistic loss with weight decay**: Why needed - enables convergence analysis; Quick check - verify regularization term in loss function

## Architecture Onboarding
- **Component map**: Data preprocessing -> Model training -> Validation -> Testing
- **Critical path**: Data normalization → Synthetic anomaly generation → Binary classifier training → Early stopping on validation loss
- **Design tradeoffs**: Uniform vs. adversarial synthetic anomalies; shallow vs. deep networks; amount of synthetic data vs. known anomaly signal dilution
- **Failure signatures**: Vanishing gradients (stalled training loss), dilution of known anomaly signal (degraded known-anomaly AUPR), overfitting to known anomalies (high validation AUPR on known but low on unknown)
- **First experiments**: 1) Train with n' = n + n⁻ synthetic anomalies on NSL-KDD; 2) Compare performance with and without synthetic anomalies on Thyroid dataset; 3) Test different synthetic anomaly counts (n', 2n', 3n') on Arrhythmia dataset

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can the theoretical guarantees for semi-supervised AD be extended to settings where contamination is present in normal data?
- Basis in paper: [explicit] The conclusion states: "We envision future work that extends our theoretical insights to other AD settings (e.g., when contamination is present in normal data)."
- Why unresolved: The current theoretical framework assumes clean normal data; contaminated normals violate the assumed data generating process and would require modified assumptions about the noise condition and density estimation.
- What evidence would resolve it: Extending Theorem 2's convergence guarantees to handle a mixture of normal and anomalous points in the "normal" training set, with corresponding bounds on excess risk under contamination.

### Open Question 2
- Question: Are different mathematical formulations of semi-supervised AD fundamentally equivalent or do they lead to different learning dynamics?
- Basis in paper: [explicit] Appendix A states: "A future research direction is to understand if different approaches of semi-supervised AD are the same or fundamentally different."
- Why unresolved: The authors propose a mixture-based formulation (h2 = s̃h⁻ + (1-s̃)), but alternative formulations exist, such as constrained optimization approaches that bound known anomaly misclassification without equivalence proofs.
- What evidence would resolve it: A theoretical analysis comparing the decision boundaries and convergence properties of mixture-based formulations versus constrained optimization formulations under identical data conditions.

### Open Question 3
- Question: How can synthetic anomalies be generated more effectively (e.g., adversarially) while maintaining theoretical convergence guarantees?
- Basis in paper: [explicit] Appendix A states: "Future work should understand how synthetic anomalies can complement existing methods, such as understanding how to generate synthetic anomalies more effectively (e.g., DROCC uses synthetic anomalies that are adversarial), along with theoretical guarantees."
- Why unresolved: The current theoretical guarantees depend on synthetic anomalies being uniformly sampled; adversarial or targeted synthetic anomalies would violate the uniform distribution assumption used in the proofs.
- What evidence would resolve it: Extending the theoretical framework to accommodate non-uniform synthetic anomaly distributions, potentially requiring new concentration bounds and approximation error analyses.

### Open Question 4
- Question: How should anomaly supervision be designed for autoencoder-based architectures to benefit from synthetic anomalies?
- Basis in paper: [explicit] The discussion states: "Supervision for negatives (i.e., anomalies) must be more carefully designed in autoencoders (e.g., in contrastive learning), but we leave this to future work."
- Why unresolved: Autoencoder-based methods (ABC, DeepSAD) showed mixed or negative results when adding synthetic anomalies, suggesting the reconstruction-based objective conflicts with the classification-based synthetic anomaly supervision.
- What evidence would resolve it: A modified training objective or architecture that jointly leverages reconstruction-based anomaly scoring and synthetic anomaly supervision in a unified framework, with empirical validation showing consistent improvements across autoencoder methods.

## Limitations
- Several critical hyperparameters are unspecified (learning rate, batch size, weight decay value, optimizer type)
- Exact network architecture details are unclear (layer widths distribution, bias usage)
- Mixture parameters s and ŝ used in experiments are not explicitly stated
- Method shows less effectiveness for autoencoder-based approaches compared to classification-based methods
- Performance for multimodal data depends heavily on quality of pretrained embeddings

## Confidence
- **High confidence**: Theoretical framework showing synthetic anomalies improve anomaly modeling and enable optimal convergence guarantees
- **Medium confidence**: Empirical results showing consistent improvements across five diverse datasets
- **Medium confidence**: Generalizability claims to other classification-based AD approaches

## Next Checks
1. Replicate the NSL-KDD experiments with varying synthetic anomaly counts (n' = n + n⁻, 2×(n + n⁻), 3×(n + n⁻)) to verify the dilution effect and potential benefits for difficult datasets
2. Test the method on an autoencoder-based approach (e.g., reconstruction-based) to confirm the claimed reduced effectiveness compared to classification-based methods
3. Implement the framework with different mixture parameters s and ŝ to determine their impact on known vs. unknown anomaly detection performance