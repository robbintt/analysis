---
ver: rpa2
title: 'Uncertainty propagation through trained multi-layer perceptrons: Exact analytical
  results'
arxiv_id: '2601.16830'
source_url: https://arxiv.org/abs/2601.16830
tags:
- analytical
- uncertainty
- expressions
- gaussian
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper provides exact analytical expressions for uncertainty
  propagation through single-hidden-layer MLPs with ReLU activation functions when
  inputs follow a multivariate Gaussian distribution. The key contributions are: Closed-form
  analytical expressions for the mean and variance of the output without resorting
  to infinite series expansions, unlike previous work.'
---

# Uncertainty propagation through trained multi-layer perceptrons: Exact analytical results

## Quick Facts
- arXiv ID: 2601.16830
- Source URL: https://arxiv.org/abs/2601.16830
- Reference count: 20
- Single-hidden-layer MLPs with ReLU activation and Gaussian inputs admit closed-form uncertainty propagation

## Executive Summary
This paper derives exact analytical expressions for the mean and variance of outputs from single-hidden-layer MLPs with ReLU activations when inputs follow a multivariate Gaussian distribution. Unlike previous approaches that rely on infinite series expansions, the authors provide closed-form formulas using standard and bivariate Gaussian integrals. The method was validated on a lithium-ion battery state-of-health prediction task, showing excellent agreement with Monte Carlo sampling and confirming the expected $1/\sqrt{n}$ error scaling.

## Method Summary
The analytical approach exploits the fact that affine transformations of Gaussian vectors remain Gaussian, while ReLU activations produce rectified Gaussian distributions with known moments. For a single-hidden-layer MLP with input $V \sim \mathcal{N}(\lambda, \Lambda)$, the method computes the mean $\mu$ and covariance $\Sigma$ of the pre-activation hidden layer, then uses Theorems 1-2 to calculate the first two moments of the rectified outputs. These moments propagate linearly through the final affine layer to yield exact output statistics. The approach was validated against Monte Carlo sampling on an EIS dataset for lithium-ion SOH prediction.

## Key Results
- Closed-form expressions for output mean and variance without infinite series expansions
- RMSE between analytical and Monte Carlo estimates decreases below $10^{-5}$ at $10^6$ samples
- Error scales as $1/\sqrt{n}$ as expected for Monte Carlo methods
- Method provides more accurate, transparent, and reproducible uncertainty quantification than sampling approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The mean and variance of the output of a single-hidden-layer MLP with ReLU activation can be computed exactly when the input follows a multivariate Gaussian distribution.
- Mechanism: Affine transformations of Gaussian vectors preserve Gaussianity. The pre-activation hidden vector $W = A^T V + c$ is multivariate Gaussian with analytically computable mean $\mu = A^T \lambda + c$ and covariance $\Sigma = A^T \Lambda A$. Propagation through the elementwise ReLU produces a rectified multivariate Gaussian; closed-form expressions for its first two moments exist as functions of standard 1D and 2D Gaussian integrals (pdf $\phi$, cdf $\Phi$, and bivariate cdf $\Phi_2$).
- Core assumption: The input $V \in \mathbb{R}^m$ is multivariate Gaussian with known mean $\lambda$ and covariance $\Lambda$; the network has exactly one hidden layer with ReLU activations; $\sigma_i > 0$ and $|\rho_{ij}| < 1$ for Theorems 1 and 2 (corner cases handled separately).
- Evidence anchors:
  - [abstract] "We give analytical results for propagation of uncertainty through trained MLPs with a single hidden layer and ReLU activation functions... we obtain exact expressions without resort to a series expansion."
  - [section] Theorem 1 provides $E[X_i]$ and $E[X_i^2]$; Theorem 2 provides $E[X_i X_j]$ in closed form.
  - [corpus] Weak direct support; no neighbor paper provides this exact ReLU-plus-Gaussian mechanism.
- Break condition: Inputs are non-Gaussian (or mixtures), the network has more than one hidden layer, the activation is not ReLU (e.g., sigmoid, tanh, GELU), or weights/covariances yield degenerate cases not covered by the standard bivariate integrals.

### Mechanism 2
- Claim: After computing the mean $\gamma$ and covariance $\Gamma$ of the rectified hidden layer, the output mean and variance follow from simple linear-algebraic propagation through the final affine layer.
- Mechanism: Given $Y = \beta^T X + d$, where $X = W_+$, the output mean is $E[Y] = \beta^T \gamma + d$ and the variance is $\text{Var}(Y) = \beta^T \Gamma \beta$.
- Core assumption: $\gamma$ and $\Gamma$ have been correctly computed from Mechanism 1; the output layer is purely affine with no additional nonlinearity.
- Evidence anchors:
  - [section] "Then the mean of Y is $\beta^T \gamma + d$ and the variance of Y is $\beta^T \Gamma \beta$."
  - [abstract] Implicit in the claim that mean and variance are provided analytically.
  - [corpus] No direct corpus neighbor confirms this linear propagation step; corpus support is weak.
- Break condition: Additional hidden layers or nonlinearities exist between the rectified hidden layer and the output, or the task is classification with softmax/cross-entropy (not regression).

### Mechanism 3
- Claim: The analytical expressions converge to Monte Carlo estimates as the number of samples increases, with error scaling as $1/\sqrt{n}$.
- Mechanism: By repeatedly sampling inputs from the specified Gaussian and propagating through the fixed MLP, empirical estimates of the output mean and variance approach the analytical values. Root-mean-square error (RMSE) between analytical and Monte Carlo estimates decreases proportionally to $1/\sqrt{n}$.
- Core assumption: The MLP weights are fixed and deterministic; the random samples are drawn correctly from the specified multivariate Gaussian; numerical evaluation of Gaussian integrals is sufficiently accurate.
- Evidence anchors:
  - [section] "The error was proportional to $1/\sqrt{n}$ as expected... the gradient of the lines of best fit... are respectively -0.5036 and -0.4966."
  - [section] Table 1 shows RMSE for both mean and variance decreasing by roughly an order of magnitude for each tenfold increase in Monte Carlo trials, reaching below $10^{-5}$ at $10^6$ samples.
  - [corpus] Weak; no neighbor paper directly reports this validation or scaling.
- Break condition: Biased or flawed sampling routines, insufficient Monte Carlo samples, numerical errors in computing $\Phi$ or $\Phi_2$, or implementation mistakes in the analytical formulas.

## Foundational Learning

- Concept: Rectified Gaussian distribution
  - Why needed here: The core mathematical object is $X = \max(0, W)$ where $W \sim \mathcal{N}(\mu, \Sigma)$; understanding how ReLU changes the moments is essential.
  - Quick check question: For $W \sim \mathcal{N}(\mu, \sigma^2)$, can you write $E[\max(0, W)]$ and $E[\max(0, W)^2]$ in terms of $\phi$ and $\Phi$?

- Concept: Affine transformation of a multivariate Gaussian
  - Why needed here: The first layer maps the Gaussian input to another Gaussian; knowing how mean and covariance transform under affine maps is prerequisite.
  - Quick check question: If $V \sim \mathcal{N}(\lambda, \Lambda)$, what are the mean and covariance of $W = A^T V + c$?

- Concept: Bivariate Gaussian CDF $\Phi_2$
  - Why needed here: The cross-moment $E[X_i X_j]$ for correlated ReLU outputs involves the bivariate normal CDF; understanding its behavior near $\rho = \pm 1$ helps handle corner cases.
  - Quick check question: How does $\Phi_2(x, y; \rho)$ behave as $\rho \to 1$ and $\rho \to -1$?

## Architecture Onboarding

- Component map: Input layer $V$ -> First affine layer $W = A^T V + c$ -> ReLU layer $X = W_+$ -> Output layer $Y = \beta^T X + d$

- Critical path:
  1. From input statistics $\lambda, \Lambda$ and first-layer weights $A, c$, compute $\mu$ and $\Sigma$.
  2. Using Theorems 1 and 2, compute $\gamma_i = E[X_i]$ and $\Gamma_{ij} = \text{Cov}(X_i, X_j)$ for all hidden units.
  3. Compute output mean $E[Y] = \beta^T \gamma + d$ and output variance $\text{Var}(Y) = \beta^T \Gamma \beta$.

- Design tradeoffs:
  - Exactness vs. scope: This approach is exact only for single-hidden-layer ReLU MLPs with Gaussian inputs; extending to deeper networks or other activations introduces approximations.
  - Computation vs. width: Evaluating bivariate Gaussian CDFs for all pairs of hidden units is $O(p^2)$; for very wide layers, Monte Carlo may be more practical despite its sampling noise.
  - Interpretability vs. generality: Analytical formulas provide transparency into how input uncertainties propagate, but require stronger assumptions than sampling-based methods.

- Failure signatures:
  - NaNs or infinities in $\gamma_i$ or $\Gamma_{ij}$ when $\sigma_i = 0$ or $|\rho_{ij}| = 1$; handle via Appendix B's corner-case formulas.
  - Substantial disagreement between analytical and Monte Carlo estimates, indicating bugs in implementation or violated assumptions.
  - Numerical instability in bivariate Gaussian CDF routines for extreme correlations or large standardized means.

- First 3 experiments:
  1. Validate Theorems 1-2 on a small network with $p=2$ hidden units: choose $\mu, \Sigma$, compute analytical $E[X_i], E[X_i^2], E[X_i X_j]$, compare against Monte Carlo estimates with $10^6$ samples.
  2. Train a single-hidden-layer ReLU MLP on a toy regression dataset with Gaussian inputs; propagate input uncertainty analytically and via Monte Carlo, plot RMSE vs. number of MC samples, and confirm $1/\sqrt{n}$ scaling.
  3. Intentionally violate an assumption (e.g., use non-Gaussian input, add a second hidden layer, or use a non-ReLU activation) and observe divergence between analytical and Monte Carlo results to confirm break conditions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can exact analytical expressions be derived for uncertainty propagation through MLPs with multiple hidden layers?
- Basis in paper: [explicit] The Conclusion states that extending the results to MLPs with more hidden layers involves fully characterizing the multivariate distribution after the second convolution, which is described as a "more challenging task."
- Why unresolved: The output of the first hidden layer is a rectified Gaussian, meaning the input to subsequent layers is no longer Gaussian, breaking the core assumption required for the current derivation.
- What evidence would resolve it: A derivation of closed-form mean and variance expressions for a network with two or more hidden layers that does not rely on Gaussian approximations at intermediate layers.

### Open Question 2
- Question: How does the computational complexity of the analytical approach compare to sampling-based methods like Monte Carlo?
- Basis in paper: [explicit] The Conclusion explicitly identifies comparing the computational complexity of the analytical approach with sampling-based approaches as "future work."
- Why unresolved: While analytically exact, calculating the 2D Gaussian integrals for covariance terms may become computationally intensive relative to the network width, potentially outweighing the speed of sampling methods for certain architectures.
- What evidence would resolve it: A theoretical big-O analysis or empirical benchmarks comparing execution time versus accuracy between the analytical method and Monte Carlo sampling across various network widths.

### Open Question 3
- Question: Can exact uncertainty propagation methods be developed for input distributions other than multivariate Gaussian?
- Basis in paper: [explicit] The Conclusion notes that extending the results to other input distributions "is likely to require different mathematical techniques."
- Why unresolved: The current proofs heavily rely on the specific statistical properties of the Gaussian distribution (e.g., the moments of the rectified Gaussian).
- What evidence would resolve it: The derivation of exact output moments for input data following non-Gaussian distributions, such as uniform or Poisson distributions.

## Limitations
- Exact method limited to single-hidden-layer ReLU MLPs with Gaussian inputs
- Computational cost scales quadratically with hidden layer width
- Requires numerical evaluation of bivariate Gaussian CDFs which can be unstable for extreme correlations

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Propagation through single ReLU layer (Theorems 1-2) | High |
| Linear-algebraic output layer propagation | High |
| Corner-case handling (Appendix B) | Medium |
| Practical validation on EIS dataset | Medium |

## Next Checks

1. **Robustness to Degenerate Cases**: Systematically test the corner-case formulas (σᵢ = 0, |ρᵢⱼ| = 1) on synthetic data to verify Theorem 3 and Appendix B.1-B.2 produce correct limits.

2. **Computational Scaling Experiment**: Measure runtime of analytical vs. Monte Carlo approaches for varying hidden layer widths (p = 10, 50, 100, 200) to identify the crossover point where sampling becomes more efficient.

3. **Generalization Experiment**: Attempt to apply the analytical framework to a multi-layer perceptron (2 hidden layers) or a network with a different activation (e.g., tanh) to explicitly demonstrate the break conditions and quantify approximation errors.