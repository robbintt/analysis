---
ver: rpa2
title: 'Activation Oracles: Training and Evaluating LLMs as General-Purpose Activation
  Explainers'
arxiv_id: '2512.15674'
source_url: https://arxiv.org/abs/2512.15674
tags:
- activation
- training
- oracle
- activations
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Activation Oracles, a method for training
  language models to interpret their own internal activations and answer arbitrary
  natural language questions about them. The authors scale up prior work by training
  on diverse datasets including system prompt interpretation, binary classification,
  and self-supervised context prediction tasks.
---

# Activation Oracles: Training and Evaluating LLMs as General-Purpose Activation Explainers

## Quick Facts
- arXiv ID: 2512.15674
- Source URL: https://arxiv.org/abs/2512.15674
- Authors: Adam Karvonen; James Chua; Clément Dumas; Kit Fraser-Taliente; Subhash Kantamneni; Julian Minder; Euan Ong; Arnab Sen Sharma; Daniel Wen; Owain Evans; Samuel Marks
- Reference count: 40
- Key outcome: Training LLMs as general-purpose activation interpreters achieves strong out-of-distribution generalization on auditing tasks, with diversified training data yielding consistent improvements.

## Executive Summary
This paper introduces Activation Oracles, a method for training language models to interpret their own internal activations and answer arbitrary natural language questions about them. The authors scale up prior work by training on diverse datasets including system prompt interpretation, binary classification, and self-supervised context prediction tasks. They evaluate Activation Oracles on four downstream auditing tasks requiring generalization to fine-tuned models, finding that even narrowly-trained models can succeed and that training data diversity yields consistent improvements. On secret-keeping benchmarks where models conceal information, Activation Oracles match or exceed white-box baselines on all three tasks and the best overall baseline on two of three. On emergent misalignment detection, they achieve comparable performance to model-diffing baselines. The results demonstrate that diversified training enables language models to generalize to out-of-distribution activation interpretation tasks and provide a simple, accessible alternative to specialized interpretability techniques.

## Method Summary
Activation Oracles train language models to answer arbitrary natural language questions about their own activations (LatentQA task). The training approach combines three task types: SPQA from Pan et al. (2024), binary classification tasks, and self-supervised context prediction. Models are trained using LoRA adapters with norm-matched additive steering injected after transformer layer 1. The steering method scales steering vectors to match activation norms, preventing activation explosion. Training uses AdamW optimizer with linear warmup and decay, on datasets totaling ~1M examples across Qwen3-8B, Gemma-2-9B-IT, and Llama-3.3-70B-Instruct models. The approach is evaluated on secret-keeping benchmarks (Taboo, User Gender, Secret Side Constraint), emergent misalignment detection, PersonaQA, and held-out classification tasks requiring generalization to fine-tuned models.

## Key Results
- Activation Oracles achieve strong out-of-distribution generalization on auditing tasks, matching or exceeding white-box baselines on secret-keeping benchmarks
- Training data diversity consistently improves performance across multiple model scales and task types
- Even narrowly-trained Activation Oracles can succeed on downstream auditing tasks, though diversified training yields better results
- The method provides a simple, accessible alternative to specialized interpretability techniques while maintaining competitive performance

## Why This Works (Mechanism)
Activation Oracles work by training models to interpret their own internal representations through a mixture of diverse tasks. The norm-matched additive steering prevents activation explosion while allowing controlled manipulation of model behavior. By exposing models to varied interpretation tasks during training, they develop general capabilities to extract and explain information from activations, enabling successful generalization to unseen auditing scenarios. The approach leverages the model's existing understanding of its own representations, requiring only targeted fine-tuning rather than architectural modifications.

## Foundational Learning
- **LatentQA task**: Training models to answer questions about their own activations using natural language. Needed to enable interpretable explanations of model internals. Quick check: Model can correctly answer questions about activations from held-out examples.
- **Norm-matched steering**: Adding scaled steering vectors to activations to control model behavior without causing activation explosion. Needed to maintain training stability while enabling behavior control. Quick check: Activation norms remain stable across layers during training.
- **Out-of-distribution generalization**: Model's ability to apply learned interpretation skills to novel tasks and fine-tuned models. Needed to demonstrate practical utility for auditing. Quick check: Performance on held-out classification tasks matches or exceeds baseline methods.

## Architecture Onboarding

**Component Map**: Input activations -> Norm-matched steering injection -> LoRA adapter training -> Activation interpretation output

**Critical Path**: Activation extraction at specific layers → Steering injection at layer 1 → LoRA fine-tuning → Natural language output generation

**Design Tradeoffs**: 
- LoRA vs full fine-tuning: LoRA provides parameter efficiency but may limit maximum performance
- Steering injection layer: Layer 1 chosen for stability, but other layers could provide different trade-offs
- Dataset diversity: More diverse training improves generalization but increases computational cost

**Failure Signatures**:
- Activation norm explosion (>100,000x growth) indicates incorrect steering implementation
- High training loss (>10% increase) suggests layer placement issues or steering method errors
- Poor OOD generalization indicates insufficient training data diversity or improper task formulation

**First Experiments**:
1. Verify activation norm stability with norm-matched steering at layer 1 across 10 training steps
2. Test steering injection at layer 0 vs layer 1 to confirm ~10% performance difference
3. Compare replacement-based steering vs additive norm-matching for activation stability

## Open Questions the Paper Calls Out
None

## Limitations
- Oracle prompt engineering details are not fully specified, making exact reproduction challenging
- Training data diversity benefits are shown qualitatively but lack controlled ablation studies
- Evaluation relies on manual annotations for some tasks, introducing potential subjectivity
- Performance depends critically on precise implementation details for steering method

## Confidence
High confidence in diversified training benefits and secret-keeping benchmark performance
Medium confidence in overall OOD generalization claims
Low confidence in emergent misalignment detection results due to limited detail

## Next Checks
1. Reproduce norm-matched steering injection at layer 1 with exact hyperparameters to verify activation norm stability and training loss convergence
2. Conduct ablation study comparing SPQA-only training versus full mixture training on held-out classification task
3. Implement and test exact oracle prompt templates on secret-keeping benchmarks to confirm performance parity with reported results