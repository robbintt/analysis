---
ver: rpa2
title: What Shapes a Creative Machine Mind? Comprehensively Benchmarking Creativity
  in Foundation Models
arxiv_id: '2510.04009'
source_url: https://arxiv.org/abs/2510.04009
tags:
- creativity
- creative
- tasks
- story
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces C2-Eval, a comprehensive benchmark for evaluating
  creativity in foundation models across two regimes: convergent (structured tasks
  like code generation) and divergent (open-ended tasks like storytelling). It measures
  creativity using three dimensions: Usefulness, Originality, and Surprise (U-O-S).'
---

# What Shapes a Creative Machine Mind? Comprehensively Benchmarking Creativity in Foundation Models

## Quick Facts
- arXiv ID: 2510.04009
- Source URL: https://arxiv.org/abs/2510.04009
- Reference count: 37
- Primary result: C2-Eval benchmark reveals creativity doesn't scale monotonically with model size and reasoning models excel at creative tasks

## Executive Summary
This paper introduces C2-Eval, a comprehensive benchmark for evaluating creativity in foundation models across convergent (structured) and divergent (open-ended) tasks. The benchmark measures creativity through three dimensions - Usefulness, Originality, and Surprise (U-O-S) - and evaluates 20+ models using human annotators. The study reveals that reasoning models outperform others in creative tasks, creative instructions generally improve performance but can harm reasoning models on convergent tasks, and creative capability doesn't scale monotonically with model size.

## Method Summary
The authors developed C2-Eval as a unified framework for evaluating creativity in foundation models, measuring it through three dimensions: Usefulness, Originality, and Surprise. The benchmark spans two regimes - convergent tasks (like code generation and poetry completion) and divergent tasks (like storytelling and drawing). Extensive human evaluations were conducted with over 1,100 annotators providing 60,000+ judgments. The framework also tests the impact of creative instructions on model performance and examines how creativity scales with model size.

## Key Results
- Reasoning models (like DeepSeek, GPT-4) consistently outperform other model families in creative tasks
- Creative instructions improve overall creativity but can hurt reasoning models on convergent tasks
- Creative capability doesn't scale monotonically with model size, showing non-linear relationships
- Convergent and divergent creativity show different scaling patterns across model families

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its comprehensive coverage of creative tasks and its grounding in established social science theories of creativity. By measuring three distinct dimensions (Usefulness, Originality, Surprise) and covering both convergent and divergent creative regimes, the framework captures the multifaceted nature of creativity. The use of human annotators provides nuanced evaluation that automated metrics cannot capture, particularly for subjective aspects like originality and surprise.

## Foundational Learning

**Creativity Theory**: Understanding the three dimensions of creativity (Usefulness, Originality, Surprise) is essential for proper benchmark design and interpretation. Quick check: Can you explain why all three dimensions are needed rather than just measuring originality?

**Foundation Model Capabilities**: Knowledge of different model families (reasoning, general-purpose, etc.) and their strengths helps interpret performance differences. Quick check: What distinguishes reasoning models from general-purpose models in terms of architecture?

**Human Evaluation Methods**: Understanding how crowdworker evaluations work and their limitations is crucial for assessing the reliability of results. Quick check: What are the main sources of noise in human creativity evaluations?

## Architecture Onboarding

**Component Map**: Task generation -> Human evaluation (U-O-S scoring) -> Model comparison -> Scaling analysis -> Instruction impact assessment

**Critical Path**: The core evaluation pipeline follows: Task generation → Human annotation (U-O-S scores) → Statistical analysis of model differences → Identification of scaling patterns

**Design Tradeoffs**: The benchmark trades comprehensiveness for focus, covering 9 creative tasks rather than attempting exhaustive coverage. This enables deeper analysis but may miss important creative domains.

**Failure Signatures**: Models may excel at originality but fail on usefulness, or vice versa. Reasoning models may underperform on convergent tasks when given creative instructions, suggesting a tension between creative and analytical thinking.

**3 First Experiments**:
1. Run C2-Eval on a small language model to establish baseline creative capabilities
2. Test creative instructions on a reasoning model to observe the performance drop on convergent tasks
3. Compare human vs. LLM-as-judge evaluations on the same tasks to assess alignment

## Open Questions the Paper Calls Out
None identified in the provided materials.

## Limitations
- Human evaluation relies on crowdworkers who may have inconsistent interpretations of creativity across tasks and cultural contexts
- Benchmark focuses on English-language tasks, potentially limiting generalizability to other languages
- Creative instructions lack theoretical grounding for why specific prompts enhance creativity
- Covers only a subset of creative domains, missing areas like visual arts, music composition, and physical design

## Confidence

**High confidence**: Benchmark framework design, core experimental methodology, general trends in model performance
**Medium confidence**: Specific numerical rankings of models, effect sizes of creative instructions, cross-cultural validity of human evaluations
**Low confidence**: Generalizability to non-English contexts, long-term stability of creative capabilities, applicability to emerging model architectures

## Next Checks
1. Conduct cross-cultural validation studies with non-English speaking annotators to assess the robustness of the U-O-S framework across different cultural contexts
2. Test the benchmark on visual and multimodal foundation models to extend evaluation beyond text-based tasks
3. Implement longitudinal studies tracking creative performance as models continue to scale and evolve, particularly focusing on the non-monotonic relationship between size and creativity