---
ver: rpa2
title: The Human Visual System Can Inspire New Interaction Paradigms for LLMs
arxiv_id: '2504.10101'
source_url: https://arxiv.org/abs/2504.10101
tags:
- information
- llms
- visual
- understanding
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This position paper proposes that large language models (LLMs)
  can be understood through the metaphor of human visual perception, addressing the
  problem of misleading metaphors (like "LLMs-as-minds") that lead to misplaced trust
  and difficulty distinguishing hallucinations from useful outputs. The paper draws
  parallels between how our eyes sample visual information through saccades and fixations
  and how LLMs process language through attention mechanisms.
---

# The Human Visual System Can Inspire New Interaction Paradigms for LLMs

## Quick Facts
- arXiv ID: 2504.10101
- Source URL: https://arxiv.org/abs/2504.10101
- Reference count: 11
- Key outcome: Proposes understanding LLMs through visual perception metaphor to address misleading metaphors and improve trust calibration

## Executive Summary
This position paper proposes that large language models can be understood through the metaphor of human visual perception, addressing the problem of misleading metaphors that lead to misplaced trust and difficulty distinguishing hallucinations from useful outputs. The paper draws parallels between how our eyes sample visual information through saccades and fixations and how LLMs process language through attention mechanisms. It argues that both systems create an illusion of complete understanding while actually interpolating over gaps in information. The authors propose three research directions inspired by visual perception theory: developing shared representations, exploring information-seeking behaviors, and defining informational affordances.

## Method Summary
The paper proposes a conceptual framework for LLM interaction inspired by visual perception mechanisms. It suggests creating "saccade maps" of concept space by visualizing LLM attention patterns, modeling information-seeking behaviors analogous to visual saccade types (top-down, bottom-up, systematic), and defining "informational affordances" similar to Gibson's measurable affordances in physical space. The approach requires grounding artifacts for shared representations, attention visualization modules, exploration pattern classifiers, and quantifiable affordance indicators.

## Key Results
- LLMs create an illusion of complete understanding similar to visual perception's interpolation over gaps
- Three research directions proposed: shared representations, information-seeking behaviors, and informational affordances
- Visual perception metaphor provides productive framework for developing trust calibration mechanisms

## Why This Works (Mechanism)

### Mechanism 1
- Claim: If LLM attention patterns can be visualized analogously to visual saccade maps, users could audit where models "fixate" in concept space and identify gaps or hallucinations.
- Mechanism: The paper proposes mapping LLM attention across information space (similar to salience mapping in vision research) to reveal which conceptual regions the model relies on vs. interpolates over.
- Core assumption: Attention weights in transformers correlate meaningfully with conceptual "fixation" that can be interpreted by humans.
- Evidence anchors:
  - [abstract] "draws parallels between how our eyes sample visual information through saccades and fixations and how LLMs process language through attention mechanisms"
  - [section 2.6.1] "Can we visualise this and reveal misleading or incorrect information and areas where we need to prompt them to assimilate other information to generate correct responses?"
  - [corpus] Weak direct evidence—neighbor papers focus on metaphor detection/generation, not attention visualization for auditing.
- Break condition: If attention patterns are too distributed or high-dimensional to map onto human-interpretable concept regions.

### Mechanism 2
- Claim: If LLM users employ different information-seeking strategies (top-down task-driven, bottom-up stimulus-driven, systematic approaches), interaction effectiveness may improve by matching strategy to task type.
- Mechanism: The paper analogizes three saccade types to LLM interaction modes—goal-driven queries (top-down), exploratory browsing prompted by surprising outputs (bottom-up), and habitual interaction patterns (systematic).
- Core assumption: Human-LLM information-seeking behavior can be categorized into these three modes and optimized accordingly.
- Evidence anchors:
  - [abstract] "exploring information-seeking behaviors (top-down, bottom-up, and systematic approaches)"
  - [section 3.1] "The differentiation in visual perception between bottom-up, top-down or systematic saccades can be a productive line of inquiry in thinking about interactions with LLMs"
  - [corpus] Neighbor paper "An Enactivist Approach to Human-Computer Interaction" touches on agency and affordances but doesn't validate this tripartite model for LLMs.
- Break condition: If LLM interactions don't exhibit separable patterns corresponding to these categories.

### Mechanism 3
- Claim: If "informational affordances" can be mathematically defined for LLM concept spaces (analogous to Gibson's landing judgment for pilots), users could calibrate trust by assessing what action possibilities the model's representations support.
- Mechanism: Translate Gibson's measurable affordances (e.g., landing judgment as a defined point orienting pilot motion) into quantifiable indicators of LLM conceptual coverage adequacy for specific tasks.
- Core assumption: Abstract information spaces have measurable properties analogous to physical affordances that indicate suitability for particular user goals.
- Evidence anchors:
  - [abstract] "defining informational affordances"
  - [section 4.2] "Gibson et al. (1955) mathematically defined this particular point in order to measure and experiment with it... How might we translate this notion of affordances and possibly quantify it"
  - [corpus] Limited—neighbor papers discuss affordances in HCI contexts but not this specific quantification approach.
- Break condition: If information space dimensionality prevents meaningful affordance definitions (paper notes: LLM landscape "likely to have a much higher intrinsic dimensionality" than visual space).

## Foundational Learning

- Concept: **Saccade mapping / Salience mapping**
  - Why needed here: Core metaphor—the paper proposes visualizing LLM "information saccades" analogous to eye-tracking maps.
  - Quick check question: Can you explain how a salience map reveals what visual information is actually processed vs. assumed?

- Concept: **Change blindness**
  - Why needed here: Demonstrates that perception interpolates over gaps—key to understanding why both vision and LLMs can seem comprehensive while missing information.
  - Quick check question: What does change blindness reveal about the "illusion of complete perception"?

- Concept: **Gibson's affordances**
  - Why needed here: Foundational for the proposed "informational affordances" research direction.
  - Quick check question: How does Gibson's definition of affordances differ from viewing perception as learning from past experience?

## Architecture Onboarding

- Component map:
  Grounding artifact -> Attention visualization module -> Exploration pattern classifier -> Affordance indicators

- Critical path:
  1. Identify or create grounding artifact for domain
  2. Extract and visualize attention patterns across relevant tokens/concepts
  3. Classify exploration mode based on interaction patterns
  4. Map affordance indicators against user task requirements

- Design tradeoffs:
  - **Information asymmetry**: LLM-to-human ratio (~300M:1) vastly exceeds eye-to-brain ratio (~3000:1)—visualization must compress far more aggressively
  - **Ground truth availability**: Physical world provides shared reference; LLMs need explicit grounding artifacts
  - **Dimensionality**: Visual space is 3D + time; information space has "much higher intrinsic dimensionality"

- Failure signatures:
  - **Penrose Triangle pattern**: Locally consistent exchanges that are globally inconsistent (Section 2.2, Figure 1)
  - **Distribution shift without signal**: Model generates confidently on out-of-distribution concepts
  - **Cultural blind spots**: WEIRD bias in training data distorts representations (Section 2.3)

- First 3 experiments:
  1. **Saccade map prototype**: Visualize attention patterns for LLM responses on a grounded artifact (e.g., small codebase); identify "fixation" vs. "interpolation" regions
  2. **Exploration pattern logging**: Instrument LLM interactions to classify top-down/bottom-up/systematic patterns; analyze correlation with task success
  3. **Affordance indicator definition**: For a bounded domain (e.g., mathematical proofs), define measurable indicators analogous to Gibson's landing judgment that predict response reliability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is it possible to construct a "saccade map" of the abstract concept space that LLMs leverage, analogous to how visual salience maps track eye fixations?
- Basis in paper: [explicit] Section 2.6.1 asks, "Concretely, we ask whether it is possible to make a saccade map of abstract concept space that LLMs are leveraging in their responses?"
- Why unresolved: Current interpretability methods do not effectively map token relationships into a navigable spatial representation that reveals coverage gaps or the depth of knowledge assimilation.
- What evidence would resolve it: Visualization tools that map attention mechanisms to concept clusters, successfully revealing regions of hallucination or sparse information that users can then audit.

### Open Question 2
- Question: Can a mechanism similar to visual "grabbiness" be implemented in LLM interfaces to automatically alert users to distributional shifts or data scarcity?
- Basis in paper: [explicit] Section 3.4 asks, "Can we design something like 'grabbiness' in LLMs that can pull focus to distributional shift, signalling the user to investigate and add in context?"
- Why unresolved: LLMs currently lack intrinsic signaling mechanisms to indicate when a query falls outside their training distribution or when the context provided is insufficient for reliable generation.
- What evidence would resolve it: An interface feature that successfully detects context drift and triggers a user intervention, demonstrably reducing errors in high-stakes tasks.

### Open Question 3
- Question: Can "informational affordances" be mathematically defined and quantified in the LLM information space to help users assess concept assimilation?
- Basis in paper: [explicit] Section 4.2 asks, "Can we mathematically define concepts in the information space of LLMs... at a level at which we could recognise and audit them?"
- Why unresolved: Unlike physical affordances (e.g., "stand-on-able"), abstract informational properties lack formal definitions that allow for the measurement of an LLM's readiness for a specific domain task.
- What evidence would resolve it: A formalized metric, akin to Gibson's "landing judgment," that correlates with the model's actual utility and reliability for specific user goals.

## Limitations

- The core metaphor between visual perception and LLM processing remains largely speculative without empirical validation
- Proposed research directions lack specific implementation details and validation methods
- The claim that attention patterns meaningfully correspond to human-interpretable conceptual fixations lacks experimental support

## Confidence

- **Medium**: The general analogy between visual sampling and LLM attention is conceptually sound and supported by existing literature on attention mechanisms and change blindness
- **Low**: The proposed visualization techniques for saccade maps and the mathematical formalization of informational affordances lack specific implementation details and validation
- **Medium**: The distinction between top-down, bottom-up, and systematic information-seeking strategies appears reasonable based on established cognitive science literature, though its applicability to LLM interactions remains untested

## Next Checks

1. **Attention Pattern Validation**: Extract attention weights from a standard transformer model across multiple layers and attempt to project them into a human-interpretable concept space. Test whether human evaluators can identify meaningful "fixation" regions versus interpolation gaps.

2. **Interaction Pattern Classification**: Instrument real LLM interactions to classify user queries into top-down, bottom-up, and systematic categories based on linguistic and behavioral features. Measure whether these categories correlate with task success rates and user satisfaction.

3. **Affordance Indicator Development**: For a bounded domain (e.g., mathematical proofs or code review), define measurable indicators analogous to Gibson's landing judgment that predict when an LLM's conceptual coverage is sufficient for reliable task completion. Validate these indicators against ground truth performance.