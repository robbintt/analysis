---
ver: rpa2
title: 'SLAP: Scalable Language-Audio Pretraining with Variable-Duration Audio and
  Multi-Objective Training'
arxiv_id: '2601.12594'
source_url: https://arxiv.org/abs/2601.12594
tags:
- audio
- slap
- training
- loss
- encoder
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses limitations in contrastive language-audio pretraining
  (CLAP), including small training datasets, fixed short audio durations, and the
  inability to capture fine-grained audio features. The proposed Scalable Language-Audio
  Pretraining (SLAP) method scales training to 109 million audio-text pairs, supports
  variable-duration audio up to 30 seconds using a redesigned Transformer-based audio
  encoder, and integrates contrastive, self-supervised, and captioning losses in a
  unified single-stage training framework.
---

# SLAP: Scalable Language-Audio Pretraining with Variable-Duration Audio and Multi-Objective Training

## Quick Facts
- arXiv ID: 2601.12594
- Source URL: https://arxiv.org/abs/2601.12594
- Reference count: 0
- Key result: State-of-the-art zero-shot audio-text retrieval (47.2% R@1 on AudioCaps, 27.2% on Clotho) and zero-shot audio classification (95.5% top-1 on ESC-50, 80.5% on GTZAN) using 109M synthetic audio-text pairs and multi-objective training.

## Executive Summary
SLAP introduces a scalable language-audio pretraining framework that addresses key limitations of existing CLAP models: small datasets, fixed short audio durations, and inability to capture fine-grained audio features. By scaling to 109 million audio-text pairs with variable-duration support (up to 30 seconds), integrating contrastive, self-supervised, and captioning losses in a unified single-stage training framework, and using a redesigned Transformer-based audio encoder, SLAP achieves new state-of-the-art results across multiple audio-text retrieval and classification benchmarks. The method demonstrates that dense, multi-objective training can learn richer audio representations that generalize effectively to downstream tasks.

## Method Summary
SLAP trains a Transformer-based audio encoder jointly with contrastive, self-supervised, and captioning objectives on 109M audio-text pairs. The audio encoder processes variable-duration mel-spectrograms using sequence packing with FlashAttention, supporting clips up to 30 seconds. The multi-objective loss combines contrastive alignment (LCLAP), masked autoencoding with EMA teacher (LSSL), and captioning generation (LCAP) in a single-stage training framework. The model uses 2D RoPE, alternating local/global attention, and RMSNorm, trained from scratch without AudioSet initialization.

## Key Results
- Zero-shot audio-text retrieval: 47.2% R@1 on AudioCaps (vs. 41.1% prior SOTA), 27.2% R@1 on Clotho (vs. 20.8% prior SOTA)
- Zero-shot audio classification: 95.5% top-1 accuracy on ESC-50 (vs. 91.8% prior SOTA), 80.5% on GTZAN (vs. 73.2% prior SOTA)
- Audio captioning: 13.7% CIDEr on AudioCaps, competitive with specialized models despite using a shallow decoder

## Why This Works (Mechanism)

### Mechanism 1: Multi-Objective Dense Representation Learning
The combination of contrastive, self-supervised, and captioning losses creates richer dense audio representations than contrastive learning alone. LCLAP aligns global audio-text embeddings, LSSL forces the encoder to learn local acoustic structure through masked reconstruction, and LCAP grounds dense patch features in semantics via captioning. The weighted sum trains all objectives jointly, with LSSL contributing most substantially to retrieval performance.

### Mechanism 2: Sequence Packing with FlashAttention
Variable-duration audio training is made efficient through sequence packing and FlashAttention. Mel-spectrograms are patchified and concatenated into packed sequences, with FlashAttention natively handling variable-length attention. This eliminates padding waste while maintaining per-sample isolation through careful attention mask design. The approach supports durations up to 30 seconds without architectural modifications.

### Mechanism 3: Scale Over Initialization
Training from scratch on 109M synthetic audio-text pairs matches or exceeds models initialized from AudioSet-pretrained encoders. Scale compensates for caption noise through redundancy, allowing the model to learn audio-specific representations rather than inheriting vision-domain biases. While synthetic captions may introduce noise, the massive dataset size appears to dilute this effect.

## Foundational Learning

- **Contrastive learning (CLAP/CLIP-style)**: Essential for understanding the core training signal that aligns paired audio and text in a shared embedding space. Quick check: Given a batch of 4 audio-text pairs, can you write out which entries in the similarity matrix are positive vs. negative for the contrastive loss?

- **Masked autoencoding / self-distillation (DINO-style)**: Critical for grasping LSSL, which uses an EMA teacher and cross-entropy on prototype scores for masked patches. Quick check: Why does the teacher receive unmasked inputs while the student receives masked inputs? What would happen if both received the same masking?

- **Sequence packing and FlashAttention**: Necessary for understanding how variable-duration training achieves efficiency through packed sequences. Quick check: In a packed sequence containing three audio clips of lengths 10, 20, and 15 patches, which positions should attend to which others to maintain per-sample isolation?

## Architecture Onboarding

- **Component map**: Waveform → 16kHz mel-spectrogram → 16×16 patches → linear projection → 12-layer ViT encoder → MAP pooling → global embedding; Text encoder (ModernBERT-base) → MAP → global text embedding; Dense audio+text → 8-layer decoder → caption tokens

- **Critical path**: Preprocessing (resample → mel → patchify → pack) → forward pass (packed audio → student encoder → MAP for LCLAP, patch features for LSSL/LCAP; unmasked audio → teacher encoder → prototypes; text → encoder → MAP) → decoder (dense audio+text → caption tokens) → loss aggregation

- **Design tradeoffs**: Local vs. global attention (local windows reduce compute; global blocks preserve long-range modeling); captioning weight (γ=0.5 balances auxiliary benefit vs. contrastive dominance); synthetic captions (enable scale but require fine-tuning on cleaner data)

- **Failure signatures**: Poor retrieval on long audio (packing bug or attention mask error); SSL loss not decreasing (EMA schedule or teacher/student data split issue); generic captioning (decoder undertrained or γ too low)

- **First 3 experiments**: 1) Reproduce ablation baseline (contrastive only) on small subset to verify multi-objective benefit; 2) Packing correctness check with synthetic batch and attention mask verification; 3) Duration sensitivity test comparing zero-shot retrieval on AudioCaps vs. Clotho without fine-tuning

## Open Questions the Paper Calls Out

- **LLM Integration**: Can replacing the shallow text decoder with a pretrained large language model bridge the performance gap with state-of-the-art audio captioning models while maintaining unified training benefits?

- **Caption Quality Ceiling**: Does noise and hallucination in synthetic captions impose a ceiling on zero-shot generalization that cannot be overcome solely by scaling data volume?

- **SSL in Fine-Tuning**: Would incorporating target-specific self-supervised learning during fine-tuning close the performance gap with models like M2D2 on audio tagging tasks?

## Limitations
- Dataset dependency: The 109M MovieGen Audio pairs are proprietary, making direct reproduction difficult
- Downstream generalization: Performance on domains beyond audio-text retrieval and classification (e.g., source separation, medical audio) remains untested
- Caption quality impact: Limited systematic analysis of how caption noise affects downstream performance despite using synthetic data

## Confidence
- **High confidence**: Multi-objective training improves dense audio representations (supported by ablation showing consistent metric drops when removing objectives)
- **High confidence**: Sequence packing with FlashAttention enables efficient variable-duration training (directly supported by architectural description and Clotho results)
- **Medium confidence**: Synthetic captions scale effectively (demonstrated on 109M pairs, but no ablation on caption quality)
- **Medium confidence**: Training from scratch outperforms AudioSet initialization (stated as departure from CLAP norm, but no direct comparison)

## Next Checks
1. Ablation on synthetic caption quality: Retrain on WavCaps (human-verified) vs. generated captions; compare downstream performance to isolate noise impact
2. Duration transfer test: Evaluate zero-shot retrieval on Clotho (15-30s) vs. AudioCaps (10s) without fine-tuning; large gap indicates packing or attention issues
3. FlashAttention packing verification: Create synthetic batch with known durations; verify attention masks correctly isolate samples and gradients don't cross boundaries