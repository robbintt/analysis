---
ver: rpa2
title: Semantic Decomposition and Selective Context Filtering -- Text Processing Techniques
  for Context-Aware NLP-Based Systems
arxiv_id: '2502.14048'
source_url: https://arxiv.org/abs/2502.14048
tags:
- context
- arxiv
- systems
- input
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces two novel text processing techniques\u2014\
  Semantic Decomposition and Selective Context Filtering\u2014designed to improve\
  \ the integration and performance of Large Language Models (LLMs) in context-aware\
  \ systems. Semantic Decomposition sequentially breaks down input prompts into structured,\
  \ hierarchical schemas, enabling systems to parse and process natural language commands\
  \ more effectively."
---

# Semantic Decomposition and Selective Context Filtering -- Text Processing Techniques for Context-Aware NLP-Based Systems

## Quick Facts
- arXiv ID: 2502.14048
- Source URL: https://arxiv.org/abs/2502.14048
- Reference count: 34
- Introduces two novel text processing techniques for improving context-aware NLP systems: Semantic Decomposition (sequential hierarchical schema traversal) and Selective Context Filtering (embedding-based context pruning)

## Executive Summary
This paper introduces two novel text processing techniques—Semantic Decomposition and Selective Context Filtering—designed to improve the integration and performance of Large Language Models (LLMs) in context-aware systems. Semantic Decomposition sequentially breaks down input prompts into structured, hierarchical schemas, enabling systems to parse and process natural language commands more effectively. Selective Context Filtering uses embedding-based methods to systematically remove irrelevant context segments, enhancing coherence and reducing computational overhead. Experiments using synthetic datasets and benchmark datasets show that these techniques improve consistency in schema adherence and context selection, with smaller models and vector-based filtering sometimes outperforming larger models. The work offers practical methods to enhance LLM-based system pipelines, though optimal context management and further evaluation are identified as future research directions.

## Method Summary
The paper proposes two complementary techniques for context-aware NLP systems. Semantic Decomposition breaks down natural language prompts through a hierarchical schema using Context-Free Grammars to constrain LLM outputs, ensuring structured, sequential classification at each depth level. Selective Context Filtering employs embedding similarity (using text-embedding-3-small) to systematically prune irrelevant context segments before they reach the LLM pipeline, reducing computational overhead and potential noise. The methods are evaluated using synthetic datasets (SynPrompt and SynAsst) and benchmark datasets (OASST1/OASST2), measuring Exponential Consistency Index (ECI) across multiple LLM configurations (gpt-4o, gpt-4o-mini) and filtering approaches.

## Key Results
- Semantic Decomposition achieved ECI scores up to 0.918 (depth 1, C.D. condition) with gpt-4o-mini outperforming gpt-4o on 5 of 7 benchmarks
- Vector-based context filtering (text-embedding-3-small) achieved 0.700-0.754 accuracy vs. 0.602-0.631 for LLM-based selection on synthetic data
- Smaller models (gpt-4o-mini, text-embedding-3-small) consistently outperformed larger models on both techniques, though the reason remains unexplained

## Why This Works (Mechanism)

### Mechanism 1: Semantic Decomposition via Hierarchical Schema Traversal
- **Claim:** Structured, sequential decomposition of prompts into predefined schema nodes may improve classification consistency compared to single-pass inference.
- **Mechanism:** An LLM is constrained via Context-Free Grammars (CFGs) to output discrete classification labels at each depth level of a hierarchy. The prompt is re-summarized at each level, condensing relevant information before the next classification step. This mimics Chain-of-Thought refinement while bounding reasoning within a designer-specified schema.
- **Core assumption:** Breaking classification into sequential, constrained decisions reduces ambiguity compared to single-shot classification; CFG-constrained outputs prevent format drift.
- **Evidence anchors:**
  - [abstract] "Semantic Decomposition sequentially breaks down input prompts into structured, hierarchical schemas, enabling systems to parse and process natural language commands more effectively."
  - [Section 4.1] Describes sequential traversal down schema depth, using Structured Outputs with CFGs to guarantee parsable output.
  - [corpus] Related work on componentization (arXiv:2601.15164) supports decomposition into manipulable units, though no direct comparison exists.
- **Break condition:** If schema classes are ambiguous or overlapping, sequential decomposition may propagate errors down the hierarchy; overly fine-grained schemas increase inference cost without accuracy gains.

### Mechanism 2: Selective Context Filtering via Embedding-Based Relevance Scoring
- **Claim:** Vector embedding similarity can systematically filter irrelevant context segments, potentially reducing noise and computational overhead in downstream LLM tasks.
- **Mechanism:** Each context segment is embedded using models like `text-embedding-3-small`. Embeddings are compared against the current task/query to compute relevance scores. Segments below a threshold are pruned before passing context to the LLM pipeline.
- **Core assumption:** Semantic similarity in embedding space correlates with task relevance; irrelevant context degrades coherence.
- **Evidence anchors:**
  - [abstract] "Selective Context Filtering uses embedding-based methods to systematically remove irrelevant context segments, enhancing coherence and reducing computational overhead."
  - [Table 3] Vector-based methods achieved 0.700–0.754 accuracy vs. 0.602–0.631 for LLM-based selection, though only on synthetic data.
  - [corpus] No direct corpus comparison for this specific filtering approach; related work focuses on RAG re-ranking rather than pipeline-level filtering.
- **Break condition:** If relevant context is semantically dissimilar to the query (e.g., negation, counterfactuals), embedding-based filtering may incorrectly prune it.

### Mechanism 3: Smaller Model Superiority in Constrained Tasks (Conditional/Observational)
- **Claim:** Under specific constrained classification and filtering tasks, smaller models (e.g., gpt-4o-mini) produced more consistent outputs than larger models (e.g., gpt-4o) in the reported experiments.
- **Mechanism:** Hypothesis not resolved in paper. Possible factors: smaller models may be less prone to overthinking simple classifications; constrained output schemas may reduce the advantage of larger reasoning capacity.
- **Core assumption:** This is an empirical observation, not a proven mechanism.
- **Evidence anchors:**
  - [Section 5.3] "Surprisingly, gpt-4o-mini produces more consistent evaluations across the board despite being a 'weaker' model... We currently do not know the reasoning behind why the smaller models perform better."
  - [Table 1] gpt-4o-mini outscored gpt-4o on 5 of 7 Semantic Decomposition benchmarks.
  - [corpus] No corroborating corpus evidence for this phenomenon.
- **Break condition:** Effect may not generalize to different tasks, domains, or model families; requires replication.

## Foundational Learning

- **Context-Free Grammars (CFGs) for Structured Outputs**
  - Why needed here: Semantic Decomposition relies on CFGs to guarantee LLM outputs conform to a parsable schema at each hierarchy level.
  - Quick check question: Can you explain how a CFG constrains token generation during decoding?

- **Vector Embeddings and Cosine Similarity**
  - Why needed here: Selective Context Filtering uses embedding similarity to rank and filter context segments.
  - Quick check question: Given two text segments, how would you compute their semantic similarity using pre-trained embeddings?

- **Context Window and Attention Scaling**
  - Why needed here: The paper's motivation stems from transformer attention's O(n²) scaling; filtering techniques aim to reduce effective context length.
  - Quick check question: Why does quadratic attention scaling make long-context processing expensive?

## Architecture Onboarding

- **Component map:**
  Input Router -> Schema Traverser -> Context Buffer -> Embedding Filter -> Response Generator

- **Critical path:**
  1. Raw prompt → Semantic Decomposition (schema classification + summarization)
  2. Context Buffer → Embedding Filter → Pruned context
  3. Classified intent + pruned context → Response Generator → Output

- **Design tradeoffs:**
  - **Decomposition depth vs. latency:** More hierarchy levels increase LLM calls but provide finer-grained control
  - **Filter threshold vs. recall:** Aggressive pruning reduces noise but risks losing relevant context; conservative thresholds retain more but increase token costs
  - **LLM vs. embedding filtering:** LLM-based selection more expressive but less consistent; embedding-based faster and more stable

- **Failure signatures:**
  - Schema misclassification at root propagates errors downstream
  - Over-aggressive filtering removes critical context (check: filtered segments should be logged for audit)
  - Overly long schema enumerations cause CFG constraint violations or timeout
  - Context injection at every step (C.D. condition) reduced consistency in experiments—monitor for ambiguity injection

- **First 3 experiments:**
  1. **Baseline consistency test:** Implement Depth 1 decomposition only; measure ECI across 50 prompts with and without CFG constraints to quantify structured output benefit.
  2. **Threshold sweep:** Run Selective Context Filtering on SynAsst with threshold values [0.3, 0.5, 0.7]; plot accuracy vs. retained context ratio to find optimal operating point.
  3. **Model size ablation:** Compare gpt-4o-mini vs. gpt-4o on same classification task with fixed schema; replicate smaller-model superiority finding and document latency/cost tradeoffs.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why do smaller models (gpt-4o-mini, text-embedding-3-small) consistently outperform larger models on Semantic Decomposition and Selective Context Filtering tasks?
- Basis in paper: [explicit] "We currently do not know the reasoning behind why the smaller models perform better on our techniques, so more future work still needs to be done with regards to analysis in this aspect."
- Why unresolved: The paper reports the counterintuitive finding but lacks mechanistic analysis of why parameter count inversely correlates with performance on these structured tasks.
- What evidence would resolve it: Ablation studies comparing model architectures, probing intermediate representations during decomposition/filtering, and analysis of attention patterns on structured vs. unstructured tasks.

### Open Question 2
- Question: What is the optimal balance of context tokens to inject at each stage of evaluation to maximize consistency without introducing ambiguity?
- Basis in paper: [inferred] "We posit that a balance needs to be achieved in how much context tokens needs to be passed on during all stages of evaluation" and results show pre-decomposed context reduces consistency in some cases.
- Why unresolved: Experiments showed that both too much and too little context hurt performance, but no systematic study of optimal token ratios was conducted.
- What evidence would resolve it: Controlled experiments varying context injection ratios at each pipeline stage, measuring ECI scores across different token budgets.

### Open Question 3
- Question: How do alternative RAG algorithms and decomposition schemas compare to the proposed methods across diverse model architectures?
- Basis in paper: [explicit] "Future work will aim at refining and evaluating these techniques further, exploring additional decomposition techniques, utilizing other RAG algorithms, and testing on a wider set of models."
- Why unresolved: Experiments were limited to GPT models and two OpenAI embedding models; schema was fixed; only synthetic and OASST datasets were tested.
- What evidence would resolve it: Benchmarks across open-source LLMs (Llama, Mistral, etc.), alternative embedding approaches, and real-world domain-specific datasets with varying decomposition schemas.

## Limitations
- The surprising superiority of smaller models (gpt-4o-mini) over larger ones (gpt-4o) for constrained classification tasks remains unexplained and may not generalize beyond the tested schema and domains
- Optimal context filtering thresholds and embedding similarity parameters were not systematically validated across diverse real-world datasets
- The study used synthetic datasets and benchmark subsets rather than fully deployed production systems, limiting external validity

## Confidence
- **High confidence**: Semantic Decomposition via CFG-constrained sequential traversal improves classification consistency compared to single-pass inference (supported by ECI improvements and controlled synthetic experiments)
- **Medium confidence**: Embedding-based Selective Context Filtering outperforms LLM-based selection on consistency metrics in constrained synthetic tasks (supported by quantitative results, but limited to synthetic data)
- **Low confidence**: The claim that smaller models perform better than larger models for these specific tasks (empirical observation without mechanistic explanation)

## Next Checks
1. Replicate the smaller-model superiority finding across 3+ diverse classification schemas and real-world dialogue datasets to test generalizability
2. Conduct ablation studies varying context filtering thresholds (0.3-0.9) on production-like datasets to establish optimal operating points and measure recall-precision tradeoffs
3. Implement schema drift detection to monitor whether sequential decomposition propagates initial classification errors through hierarchy levels in long-running conversations