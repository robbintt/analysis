---
ver: rpa2
title: Decoders Laugh as Loud as Encoders
arxiv_id: '2509.04779'
source_url: https://arxiv.org/abs/2509.04779
tags:
- humor
- jokes
- language
- zhang
- were
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigated whether Large Language Models (LLMs) can
  understand humor by classifying jokes into six categories: absurdity, dark, irony,
  wordplay, social commentary, and non-jokes. A dataset of 1,392 examples was curated,
  cleaned, and split into train/validation/test sets.'
---

# Decoders Laugh as Loud as Encoders

## Quick Facts
- arXiv ID: 2509.04779
- Source URL: https://arxiv.org/abs/2509.04779
- Reference count: 9
- Primary result: Fine-tuned GPT-4o decoder achieves 0.85 F1-macro, matching the best fine-tuned encoder (RoBERTa, 0.86) on humor classification

## Executive Summary
This study investigates whether Large Language Models can understand humor by classifying jokes into six categories: absurdity, dark, irony, wordplay, social commentary, and non-jokes. A dataset of 1,392 examples was curated, cleaned to remove overlapping humor elements, and split into train/validation/test sets. Multiple models were tested, including fine-tuned encoders (BERT, RoBERTa variants), zero/few-shot encoder-decoders (BART, Flan-T5), and zero/few-shot decoders (Llama, Gemma, Qwen, Mistral, GPT variants). The key finding is that a fine-tuned GPT-4o decoder achieved an F1-macro score of 0.85, statistically matching the performance of the best fine-tuned encoder (RoBERTa at 0.86), demonstrating that fine-tuned decoders can understand humor as well as encoders.

## Method Summary
The study collected jokes from multiple online sources and non-jokes from a Kaggle dataset, then manually filtered the data to remove jokes containing multiple humor mechanisms (especially wordplay overlaps) to create a strict multi-class problem. The final dataset of 1,392 examples was split into 80/10/10 stratified sets. Encoders were fine-tuned using HuggingFace Trainer with seeds 42, 1337, 2025, early stopping, and F1-macro optimization. The decoder (GPT-4o) was fine-tuned via OpenAI API with 3 epochs, batch size 4, and learning rate multiplier 2. All models were evaluated using F1-macro to account for class imbalance.

## Key Results
- Fine-tuned GPT-4o decoder achieved 0.8522 F1-macro, matching fine-tuned RoBERTa's 0.8566
- Zero-shot and few-shot decoders performed significantly worse than fine-tuned models
- Larger encoder models generally performed better, except for RoBERTa and ALBERT which showed no size-based performance gap
- Training loss for GPT-4o reached near-zero rapidly, suggesting potential overfitting on the small dataset

## Why This Works (Mechanism)

### Mechanism 1: Fine-Tuning Alignment for Generative Classifiers
Fine-tuning adapts decoder weights to map humor concepts to output tokens, closing the performance gap observed in zero-shot settings. This works because high classification accuracy implies understanding rather than superficial correlation. The approach fails when tasks require dense vector retrieval rather than discrete label generation.

### Mechanism 2: Ambiguity Reduction via Strict Pre-processing
Manual filtering removes overlapping humor elements, reducing noise and boundary confusion during training. This allows models to learn distinct features for specific humor types rather than generalized "funny" signals. The method breaks when test sets contain complex jokes relying on multiple mechanisms.

### Mechanism 3: Metric Selection for Imbalanced Domains
F1-Macro averages scores across all classes, forcing models to perform well on minority classes. This prevents models from achieving high accuracy by simply predicting the negative class. The approach is insufficient when applications prioritize specific error types over balanced metrics.

## Foundational Learning

- **Concept: Encoder vs. Decoder Architectures**
  - Why needed: The study proves decoders can perform classification, traditionally an encoder task
  - Quick check: Explain why bidirectional encoders are theoretically superior for classification than unidirectional decoders, and how fine-tuning mitigates this

- **Concept: Overfitting in Low-Data Regimes**
  - Why needed: The small dataset (1,392 samples) raises overfitting concerns
  - Quick check: What does training loss of 0 mean for a large model on a small dataset, and how does the validation loss curve inform memorization risk?

- **Concept: Zero-shot vs. Few-shot vs. Fine-tuning**
  - Why needed: The paper benchmarks all three approaches
  - Quick check: Why might few-shot examples fail to improve performance on tasks requiring deep semantic understanding compared to updating model weights?

## Architecture Onboarding

- **Component map:** Raw Scraping -> Manual Filtering (Remove Wordplay overlap) -> Balancing (Negative Sampling) -> Split (Train/Val/Test) -> Fine-tuning (Encoders via HuggingFace, Decoder via OpenAI API) -> Evaluation (F1-Macro via custom script)

- **Critical path:** The manual cleaning of data to separate "Wordplay" from other categories. Without this strict separation, multi-class labels would be too noisy for the model to converge on the reported 0.85 F1 score.

- **Design tradeoffs:**
  - Simplicity vs. Realism: Simplified multi-class task boosts scores but reduces ability to handle real-world multi-faceted humor
  - Compute vs. Accuracy: Fine-tuned GPT-4o matches RoBERTa, but RoBERTa is likely cheaper for inference

- **Failure signatures:**
  - Flan-T5 performed worse in few-shot than zero-shot, suggesting inability to handle long prompt context
  - GPT-4o training loss hit near-zero quickly, implying potential memorization rather than generalization

- **First 3 experiments:**
  1. Reproduce the RoBERTa baseline using the provided split and F1-Macro metric
  2. Test GPT-4o on uncleaned raw jokes to validate the importance of preprocessing
  3. Double the smallest class (Social Commentary) using augmentation to test data scaling effects

## Open Questions the Paper Calls Out

- **Question 1:** Does successful humor classification indicate genuine understanding or surface-level pattern recognition?
  - Basis: Authors assume classification quality implies understanding but provide no validation
  - Resolution: Additional tasks like joke explanation generation or humor rewriting tested against human evaluations

- **Question 2:** Why do RoBERTa and ALBERT show no performance gap between base and large variants?
  - Basis: Authors note this anomaly but offer no hypothesis
  - Resolution: Controlled ablation studies comparing base vs. large variants on humor-specific subtasks

- **Question 3:** How does fine-tuned decoder performance scale with training data quantity and diversity?
  - Basis: GPT-4o reached near-zero training loss rapidly; dataset was limited to 1,392 examples
  - Resolution: Systematic experiments varying training set sizes with balanced distributions

## Limitations

- Aggressive manual preprocessing creates a simplified problem that may not generalize to real-world humor detection
- Small dataset size (1,392 examples) raises questions about generalization beyond specific jokes used
- Limited hyperparameter control for GPT-4o fine-tuning makes exact reproduction difficult
- No human evaluation of model's "understanding" vs. pattern matching
- Rapid training loss convergence suggests potential overfitting

## Confidence

- **High Confidence:** Core architectural finding that fine-tuned decoders can match fine-tuned encoders on this classification task
- **Medium Confidence:** Interpretation that performance demonstrates "understanding" of humor is reasonable but not definitively proven
- **Low Confidence:** Claims about scaling to production systems are premature given dataset limitations

## Next Checks

1. Conduct ablation study testing model performance on original, uncleaned jokes to quantify preprocessing impact
2. Perform human evaluation of model's incorrect predictions to distinguish semantic ambiguity from model limitations
3. Create synthetic test set with overlapping humor mechanisms to stress-test generalization claims