---
ver: rpa2
title: 'When lies are mostly truthful: automated verbal deception detection for embedded
  lies'
arxiv_id: '2501.07217'
source_url: https://arxiv.org/abs/2501.07217
tags:
- lies
- embedded
- deception
- deceptive
- truthful
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study explores the detection of embedded lies\u2014statements\
  \ that mix truthful and deceptive information\u2014using a novel dataset of 2,088\
  \ statements. Participants wrote truthful and deceptive accounts of autobiographical\
  \ events, with embedded lies annotated and rated for deceptiveness, centrality,\
  \ and source."
---

# When lies are mostly truthful: automated verbal deception detection for embedded lies

## Quick Facts
- arXiv ID: 2501.07217
- Source URL: https://arxiv.org/abs/2501.07217
- Reference count: 40
- Primary result: Automated detection of embedded lies achieved 64% accuracy, with embedded lies comprising ~1/3 of deceptive statements and showing minimal linguistic differences from truthful accounts

## Executive Summary
This study addresses the challenge of detecting embedded lies—statements that mix truthful and deceptive information—using a novel dataset of 2,088 autobiographical statements. Participants wrote both truthful and deceptive accounts of events, with embedded lies annotated and rated for deceptiveness, centrality, and source. Machine learning models, including fine-tuned language models, achieved 64% accuracy in classifying truthful statements versus those with embedded lies. The research reveals that embedded lies typically constitute 1/3 of deceptive statements, are largely derived from personal experiences, and show minimal linguistic differences from truthful counterparts. The study also demonstrates that pre-trained deception models struggle with embedded lies due to distributional shifts, and that individual differences influence self-rated deceptiveness but not embedded lie behavior.

## Method Summary
The study employed a within-subjects experimental design where 261 participants wrote truthful and deceptive accounts of autobiographical events. Participants self-annotated their deceptive statements to identify embedded lies and rated their deceptiveness, centrality, and source. The dataset was processed using linguistic feature extraction (LIWC-22, DeCLaRatiVE, GPT-embeddings) and evaluated using machine learning models including Random Forest, distilBERT, FLAN-T5, and Llama-3-8B fine-tuned with QLoRA. Performance was assessed through 5-10 fold cross-validation, with accuracy, F1 metrics, and explainability via correlation analysis between model confidence and embedded lie count.

## Key Results
- Automated detection achieved 64% accuracy (chance = 50%) in classifying truthful vs. embedded deceptive statements
- Embedded lies comprised approximately 1/3 of deceptive statement length (mean ratio = 0.32)
- 35.86% of embedded lies relied on personal past experiences involving participants directly
- Fine-tuned Llama-3-8B achieved best performance, while pre-trained deception models dropped to 56% accuracy with 65% false negative rate
- Minimal linguistic differences between truthful and embedded deceptive statements (effect sizes d = 0.26 for social words, d = -0.17 for memory words)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Embedded lies resist detection because they structurally approximate truthful narratives through strategic truth-fabrication blending
- Mechanism: Liars anchor deceptive content in autobiographical memory (46.27% from direct personal experience), yielding statements that share linguistic and semantic properties with truthful accounts. The 2/3 truthful scaffolding minimizes discriminative signal across linguistic features
- Core assumption: Detection difficulty scales with proportion of truthful substrate (only correlational evidence provided)
- Evidence anchors: [abstract] "Typical deceptive statements consisted of 2/3 truthful information and 1/3 embedded lies"; [results] "35.86% of embedded lies... relied on personal past experiences"
- Break condition: If liars must fabricate larger proportions (>50%) or draw from non-experiential sources, detection accuracy should increase

### Mechanism 2
- Claim: Fine-tuned language models extract residual signal from subtle linguistic differences that traditional feature engineering misses
- Mechanism: Llama-3-8B captures distributed representations where truthful statements show higher social referencing and deceptive statements show elevated cognitive process markers, though differences are minimal (≤2 significant n-grams per event)
- Core assumption: Transformer attention patterns exploit consistent but weak discriminative patterns (specific mechanisms uncharacterized)
- Evidence anchors: [results] "Linguistic analysis... revealed several differences... However, these effect sizes were small"; [table 7] minimal significant n-gram differences
- Break condition: If linguistic differences fall below model sensitivity threshold (<20% embedded lies), accuracy should approach chance

### Mechanism 3
- Claim: Pre-trained deception models fail to generalize because they overfit to fabrication patterns absent in partially truthful statements
- Mechanism: Deception language model (FLAN-T5) dropped from 79.31% to 56% accuracy with asymmetric recall loss (truthful: 76%, deceptive: 35%), indicating learned association with wholly fabricated narratives
- Core assumption: Training data distribution mismatch causes performance drop (inference plausible but not conclusively proven)
- Evidence anchors: [results] "The deception language model... dropped to 56% accuracy... 65% of embedded lies were misclassified as truthful"; [discussion] preserved truthful recall argues against overfitting
- Break condition: If models are trained on graded deception continua rather than binary labels, generalization should improve

## Foundational Learning

- **Within-subjects experimental design**
  - Why needed: Controls for individual linguistic baselines, strengthens causal inference about veracity effects
  - Quick check question: Why does a within-subjects design reduce linguistic confounders compared to between-subjects designs in deception research?

- **Fine-tuning vs. zero-shot inference**
  - Why needed: Explains why adaptation to embedded lies requires parameter updates on target-domain data
  - Quick check question: What does the performance gap between fine-tuned and zero-shot models suggest about the distribution shift from fabrication to embedded lies?

- **Explainability via correlation analysis**
  - Why needed: The paper uses Spearman correlations between model confidence and embedded lie count as proxy-based explainability
  - Quick check question: Why is correlation between class probability and embedded lie count only weak evidence (rho = 0.10) for a causal detection mechanism?

## Architecture Onboarding

- Component map: Data layer (2,088 statements) -> Feature extraction (LIWC-22, DeCLaRatiVE, GPT-embeddings) -> Model layer (RF, distilBERT, FLAN-T5, Llama-3-8B) -> Evaluation (5-10 fold CV, accuracy/F1, explainability via correlation)

- Critical path: Data annotation quality -> within-subject pairing integrity -> fine-tuning hyperparameters (learning rate 1e-4, batch size 2, epochs 3) -> cross-validation fold stratification

- Design tradeoffs:
  - Self-annotation vs. external labeling: Faster and scalable but introduces subjectivity in what counts as "deceptive"
  - Model complexity vs. interpretability: Llama-3-8B achieves best performance but explainability is limited to output correlations
  - Binary classification vs. regression: Treats detection as binary rather than quantifying deception degree on continuum

- Failure signatures:
  - Accuracy at or near chance (50%) -> likely training data leakage or insufficient fine-tuning
  - High truthful recall, low deceptive recall (<40%) -> model learned fabrication patterns absent in embedded lies
  - Large train-test accuracy gap (>15%) -> overfitting; ensure within-subject pairing keeps both statements in same fold

- First 3 experiments:
  1. Baseline replication: Train RF on LIWC features with 10-fold nested CV; expect 55-58% accuracy
  2. Fine-tuning sweep: Vary embedded lie ratios in training data (e.g., only statements with >30% lies vs. full dataset)
  3. Cross-context generalization: Train on 10 events, test on held-out event to assess event-specific vs. generalizable deception patterns

## Open Questions the Paper Calls Out

- **Can automated systems effectively distinguish between fully fabricated statements, statements with embedded lies, and completely truthful statements?**
  - Basis: [explicit] Authors propose "a potential avenue for future research could involve... having three versions of the statement: truthful, embedded lies, and fully deceptive"
  - Why unresolved: Current study only utilized binary classification
  - What evidence would resolve it: Three-condition design testing model accuracy across full spectrum of fabrication

- **Can predictive models identify the specific location (word or phrase) of an embedded lie within a narrative?**
  - Basis: [explicit] Authors suggest dataset allows for "sequence classification task, allowing for the prediction of how and where lies are embedded within truthful narratives"
  - Why unresolved: Current paper focused on document-level classification rather than token-level detection
  - What evidence would resolve it: Training and evaluating sequence labeling model using provided annotations

- **Do traditional verbal cues of deception (e.g., Reality Monitoring, Verifiability Approach) retain discriminative power when applied to embedded lies?**
  - Basis: [explicit] Authors ask whether "traditional theoretical frameworks of deception... work well when applied to embedded lies"
  - Why unresolved: Study found minimal linguistic differences suggesting standard cues may be ineffective
  - What evidence would resolve it: Targeted analysis testing specific theoretical cues against the dataset

- **Does the subjectivity of self-annotated ground truth limit the reliability of automated detection models?**
  - Basis: [inferred] Authors admit self-reported annotations may lead to "subjective interpretations... [reducing] the consistency and reliability of the data"
  - Why unresolved: Unclear if 64% accuracy ceiling was due to task difficulty or annotation noise
  - What evidence would resolve it: Comparing model performance on current dataset against independently verified dataset

## Limitations

- Self-annotation approach introduces potential subjectivity in what constitutes deceptive content, though standardization through word ratios attempted
- 64% accuracy ceiling suggests fundamental limits to detecting embedded lies rather than model inadequacies, but correlational explainability analysis provides only weak evidence for proposed mechanism
- Within-subjects design may create linguistic dependencies that artificial between-subjects datasets could better isolate

## Confidence

- **High confidence**: Embedded lies comprise approximately 1/3 of deceptive statements with minimal linguistic differences from truthful accounts (supported by effect sizes d = 0.26 for social words, d = -0.17 for memory words)
- **Medium confidence**: Generalization failure of pre-trained deception models to embedded lies (65% false negative rate), though inference that this isn't overfitting is plausible but not conclusively proven
- **Low confidence**: Specific mechanism by which fine-tuned LLMs detect embedded lies, as explainability analysis only shows weak correlations without revealing attention patterns or feature importance

## Next Checks

1. Conduct external annotation validation where independent raters identify embedded lies in a subset of statements to assess inter-rater reliability against self-annotations
2. Test model performance on progressively increasing proportions of embedded lies (e.g., 20%, 40%, 60%) to empirically establish relationship between fabrication degree and detectability
3. Train models on fully fabricated lies only versus mixed embedded lies to determine whether observed performance drop stems from distributional shift or inherent detection difficulty of embedded deception