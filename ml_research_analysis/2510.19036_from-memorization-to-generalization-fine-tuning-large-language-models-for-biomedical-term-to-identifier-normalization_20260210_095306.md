---
ver: rpa2
title: 'From Memorization to Generalization: Fine-Tuning Large Language Models for
  Biomedical Term-to-Identifier Normalization'
arxiv_id: '2510.19036'
source_url: https://arxiv.org/abs/2510.19036
tags:
- gene
- term
- fine-tuning
- terms
- identifier
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study evaluated large language models (LLMs) for mapping
  biomedical terms to standardized identifiers across three ontologies: Human Phenotype
  Ontology (HPO), Gene Ontology (GO), and gene symbol-protein mappings (GENE). Fine-tuning
  Llama 3.1 8B showed strong memorization gains for GO (up to 77% accuracy improvement)
  and GENE (13.9% generalization to unseen pairs), but minimal gains for HPO.'
---

# From Memorization to Generalization: Fine-Tuning Large Language Models for Biomedical Term-to-Identifier Normalization

## Quick Facts
- arXiv ID: 2510.19036
- Source URL: https://arxiv.org/abs/2510.19036
- Reference count: 40
- Fine-tuning Llama 3.1 8B showed strong memorization gains for GO (up to 77% accuracy improvement) and GENE (13.9% generalization to unseen pairs), but minimal gains for HPO.

## Executive Summary
This study investigates how large language models map biomedical terms to standardized ontology identifiers, revealing that fine-tuning success depends critically on identifier popularity and lexicalization. Popular identifiers are more likely encountered during pretraining, enhancing memorization through frequency-based strengthening of factual salience. Lexicalized identifiers (like gene symbols) enable semantic generalization to unseen pairs, while arbitrary identifiers (like HP codes) restrict learning to rote memorization. The work demonstrates that LLMs can learn to generalize biomedical mappings when identifiers carry semantic meaning, but only memorize when identifiers are opaque codes.

## Method Summary
The study fine-tuned Llama 3.1 8B using LoRA adapters on three biomedical ontologies: Human Phenotype Ontology (HPO), Gene Ontology (GO cellular component), and gene symbol-protein mappings (GENE). Training data consisted of 200 frequency-balanced term-identifier pairs per terminology, expanded to 1,000 examples through prompt templates. Six separate models were trained (3 terminologies × 2 directions) with 20 epochs, LoRA rank=64, and learning rate 1e-5. Baseline performance was evaluated on Llama 3.1 8B, 70B, and GPT-4o. Memorization was measured as training set accuracy gains, while generalization was assessed on held-out validation sets. Embedding analyses compared cosine similarities between term-identifier pairs to evaluate lexicalization effects.

## Key Results
- Fine-tuning improved memorization most for GO (77% accuracy gain) and least for HPO (no gain), with GENE intermediate (15.7% gain)
- Generalization occurred for GENE (13.9% accuracy gain on unseen pairs) but not for HPO or GO
- Embedding analyses showed tight semantic alignment between gene symbols and protein names but weak alignment for GO or HPO identifiers
- Term→identifier accuracy consistently exceeded identifier→term accuracy across all models and terminologies

## Why This Works (Mechanism)

### Mechanism 1: Popularity as a Proxy for Factual Salience
Pretraining frequency shapes the internal strength with which facts are stored in model weights. Fine-tuning more readily reinforces high-salience associations than establishes new ones for rare facts. Models exposed more frequently to gene symbols than GO or HPO identifiers during pretraining showed greater memorization gains.

### Mechanism 2: Lexicalization Enables Semantic Generalization
Lexicalized identifiers acquire embeddings during pretraining that align with their associated natural-language terms. Fine-tuning exploits this pre-existing semantic structure to activate latent associations beyond specific training examples, while arbitrary identifiers lack this semantic foundation.

### Mechanism 3: Autoregressive Directionality Bias
During pretraining, models predict the next token given preceding context. Natural-language terms typically precede their identifiers in biomedical corpora, so term→identifier mappings align with learned statistical dependencies while the reverse does not.

## Foundational Learning

- **Concept: Factual Salience**
  - Why needed here: Understanding why some term–identifier pairs are more easily memorized requires grasping how pretraining frequency translates into internal knowledge strength
  - Quick check question: Given two terms, one appearing 1000 times and another 10 times in the pretraining corpus, which would you expect to show greater memorization gains after fine-tuning?

- **Concept: Lexicalization**
  - Why needed here: The core distinction between arbitrary and lexicalized identifiers determines whether fine-tuning supports generalization or only rote memorization
  - Quick check question: Which identifier is more likely to be lexicalized in an LLM's embedding space: "GO:0005634" or "TP53"? Why?

- **Concept: Autoregressive Directionality**
  - Why needed here: The consistent asymmetry in mapping accuracy reflects how LLMs are trained and has practical implications for task design
  - Quick check question: If you need to build a biomedical normalization system that maps identifiers to terms, what performance challenges might you expect compared to mapping terms to identifiers?

## Architecture Onboarding

- **Component map**: Pretrained LLM (Llama 3.1 8B) -> LoRA adapters -> Ontology datasets (HPO/GO/GENE) -> PMC/annotation frequency data -> Embedding extraction modules

- **Critical path**: 1) Curate frequency-balanced training/validation sets 2) Generate diverse prompts for each term–identifier pair 3) Fine-tune LoRA adapters 4) Evaluate accuracy on training and validation sets 5) Analyze popularity proxies and lexicalization

- **Design tradeoffs**: Parameter-efficient fine-tuning reduces resource costs but may yield different memorization–generalization dynamics than full fine-tuning. Frequency-balanced sampling mitigates long-tail effects but may not reflect real-world prevalence.

- **Failure signatures**: Near-zero gains on both training and validation sets suggest low popularity and non-lexicalized identifiers. Strong training gains but negligible validation gains indicate effective memorization without semantic alignment.

- **First 3 experiments**: 1) Replicate setup for new biomedical terminology (SNOMED CT) 2) Systematically vary lexicalized vs. arbitrary identifier ratios in synthetic pairs 3) Evaluate alternative fine-tuning methods (full fine-tuning, different adapters) on memorization–generalization trade-off

## Open Questions the Paper Calls Out

1. **Mechanism of Embedding Geometry**: Does fine-tuning primarily re-weight existing token probabilities or reshape embedding geometry? The study assumes the former but only analyzed base model embeddings.

2. **Knowledge Degradation Optimization**: Can fine-tuning approaches be optimized to reduce knowledge degradation while preserving generalization gains? Only one PEFT configuration was tested.

3. **Framework Generalizability**: Does the popularity–lexicalization framework generalize to terminologies with different identifier structures (e.g., ICD-10, SNOMED CT)? Only three ontologies were tested.

4. **Lexicalization Threshold**: Can the lexicalization threshold be crossed during fine-tuning, or is it determined entirely during pretraining? The study suggests pretraining determines this but cannot definitively prove it.

## Limitations
- Corpus-level gaps: PMC annotation counts may not accurately reflect true pretraining exposure frequencies
- Generalizability boundaries: Results based on three ontologies and one model architecture may not transfer broadly
- Lexicalization quantification: No rigorous threshold established for what constitutes "lexicalized" versus "arbitrary" identifiers

## Confidence

**High Confidence**:
- Fine-tuning improves memorization for popular identifiers across all three ontologies
- GENE gene symbols show semantic generalization while HPO and GO identifiers do not
- Lexicalized identifiers exhibit higher embedding alignment than arbitrary identifiers
- Directional asymmetry (term→identifier > identifier→term) is consistent across models and terminologies

**Medium Confidence**:
- Popularity proxies accurately predict memorization gains
- Lexicalization enables semantic generalization rather than rote memorization
- Pretraining frequency patterns drive autoregressive directionality bias

**Low Confidence**:
- Specific numerical estimates of pretraining exposure from PMC counts
- Universal applicability of lexicalization effects across all biomedical terminologies
- Causal relationship between embedding alignment and generalization capability

## Next Checks
1. **Synthetic Identifier Experiment**: Create controlled datasets with varying ratios of lexicalized vs. arbitrary identifiers while holding popularity constant to isolate lexicalization effects.

2. **Cross-Model Validation**: Replicate the study using different model families (Mistral, Gemma) and fine-tuning methods (full fine-tuning, QLoRA variants) to test generalizability.

3. **Pretraining Corpus Analysis**: For open-weight models with available pretraining data, directly measure the frequency of specific term-identifier pairs rather than relying on PMC proxies.