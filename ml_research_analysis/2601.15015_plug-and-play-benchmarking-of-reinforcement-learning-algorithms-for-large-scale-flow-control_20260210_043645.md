---
ver: rpa2
title: Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale
  Flow Control
arxiv_id: '2601.15015'
source_url: https://arxiv.org/abs/2601.15015
tags:
- control
- learning
- flow
- reinforcement
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "FluidGym is the first standalone, fully differentiable RL benchmark\
  \ for active flow control, built entirely in PyTorch with no external CFD solver\
  \ dependencies. It provides standardized, GPU-accelerated environments for both\
  \ single- and multi-agent RL, supporting 2D and 3D fluid dynamics tasks including\
  \ cylinder flow, Rayleigh-B\xE9nard convection, airfoil flow, and turbulent channel\
  \ flow."
---

# Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control

## Quick Facts
- arXiv ID: 2601.15015
- Source URL: https://arxiv.org/abs/2601.15015
- Reference count: 40
- First standalone, fully differentiable RL benchmark for active flow control built entirely in PyTorch with no external CFD solver dependencies

## Executive Summary
FluidGym is a comprehensive PyTorch-based RL benchmark for active flow control (AFC) that enables plug-and-play experimentation across 13 environments including cylinder flow, Rayleigh-Bénard convection, airfoil flow, and turbulent channel flow in both 2D and 3D. The benchmark uniquely combines GPU-accelerated simulation via the PICT solver, full differentiability for gradient-based control, multi-agent support through PettingZoo, and standardized evaluation protocols with pre-defined train/val/test splits. Experiments with PPO and SAC across all environments show SAC generally outperforms PPO at higher difficulty levels, while D-MPC using reward gradients demonstrates effective control without policy learning.

## Method Summary
FluidGym provides standardized RL environments built entirely in PyTorch using the GPU-accelerated PICT solver, eliminating external CFD dependencies. The benchmark supports both single-agent and multi-agent RL with Gymnasium and PettingZoo APIs, includes 2D and 3D fluid dynamics tasks, and offers full differentiability for gradient-based methods. Training uses default Stable-Baselines3 hyperparameters with 5 random seeds (3 for 3D environments), and evaluation follows a standardized protocol using pre-defined train/val/test splits with 10 initial domains each. The differentiable simulation enables D-MPC control without policy learning, and all environments, datasets, and trained models are publicly released.

## Key Results
- SAC outperforms PPO on 10 of 13 environments, with the largest gap at higher difficulty levels (medium/hard)
- D-MPC using reward gradients achieves competitive drag reduction (~8%) without policy learning
- Transfer learning shows 2D-trained policies sometimes outperform 3D-trained baselines on cylinder flow tasks
- MARL agents coordinate effectively for RBC convection and TCF channel flow control

## Why This Works (Mechanism)

### Mechanism 1
- Claim: End-to-end differentiability enables gradient-based control without policy learning
- Mechanism: The PICT solver computes simulation gradients via a hybrid DtO/OtD approach, allowing backpropagation through step() to reward. D-MPC exploits this by optimizing action sequences via gradient ascent through the differentiable simulator at each control step, using only reward gradient signal—no policy network, value function, or exploration required
- Core assumption: The reward landscape is sufficiently smooth and unimodal enough for local gradient ascent to find effective actions within the receding horizon
- Evidence anchors: D-MPC achieves competitive drag reduction (~8%) compared to PPO and SAC on CylinderJet2D-easy-v0; FluidGym is the only benchmark combining full differentiability across all environments

### Mechanism 2
- Claim: GPU-native simulation eliminates external CFD coupling overhead, enabling scalable RL experimentation
- Mechanism: FluidGym runs entirely in PyTorch on top of GPU-accelerated PICT solver with custom CUDA kernels for PISO algorithm, removing process-level coupling, file I/O, and interface glue code between Python RL libraries and external solvers
- Core assumption: Users have CUDA-capable GPUs; CPU-only execution is not currently supported
- Evidence anchors: FluidGym is the only benchmark combining no external solver, full differentiability, MARL support, and 3D capabilities; DRLinFluids, drlFoam, DRLFluent all require external CFD coupling layers

### Mechanism 3
- Claim: Standardized initial conditions and evaluation protocols reduce variance in algorithm comparisons
- Mechanism: Pre-computed train/val/test splits (10 initial domains each) are downloaded and cached on first use. env.reset() applies consistent random perturbations and rollout steps based on seeded RNG, ensuring identical initial conditions across runs. Mean per-step reward (rather than cumulative return) is reported to avoid episode-length confounding
- Core assumption: Ten initial domains per split provide sufficient coverage of the state space; random perturbations adequately diversify within-episode conditions
- Evidence anchors: env.reset() creates standardized and reproducible train/val/test protocol; many prior works lack standardized training and evaluation procedures

## Foundational Learning

- Concept: Markov Decision Processes (MDPs) and Partial Observability (POMDPs)
  - Why needed here: FluidGym exposes observations from virtual sensors, not full flow states. Understanding that RL agents operate on partial observations (ot = O(st, at)) rather than ground-truth states is essential for interpreting agent behavior and designing observation spaces
  - Quick check question: Given sensor observations at locations shown in Figure 9, can the agent infer the full velocity field? Why might partial observability matter for generalization?

- Concept: Navier-Stokes Equations and Dimensionless Parameters
  - Why needed here: Environment difficulty is controlled via Reynolds number (Re) for cylinder/airfoil, Rayleigh number (Ra) for convection, and friction Reynolds (Reτ) for channel flow. These parameters govern turbulence intensity and flow regime transitions
  - Quick check question: If Re increases from 100 to 500 for cylinder flow (Table 4), what qualitative change in vortex shedding would you expect, and why does this make control harder?

- Concept: Gradient-Based Optimization Through Simulators
  - Why needed here: D-MPC and differentiable RL methods require understanding how gradients flow through time-stepping. The hybrid DtO/OtD approach in PICT means algorithmic structure uses DtO while linear solves use OtD
  - Quick check question: In D-MPC (Algorithm 1), why is the environment state reset and gradients detached before each optimization iteration? What would happen if gradients accumulated across iterations?

## Architecture Onboarding

- Component map: PETSC backend -> PICT solver -> FluidEnv core -> Gymnasium/PettingZoo wrappers -> RL agents
- Critical path:
  1. Install via pip (no CFD dependencies)
  2. Instantiate environment (e.g., gym.make("CylinderJet2D-easy-v0"))
  3. First reset() triggers download of initial domains
  4. RL loop: action → env.step(action) → (observation, reward, done, info)
  5. For MARL: use PettingZoo API; each agent receives local observations/actions
  6. For gradient-based methods: enable requires_grad on actions, backprop through step()

- Design tradeoffs:
  - GPU-only vs. CPU fallback: GPU acceleration required for practical training times; CPU support would improve accessibility but is not implemented
  - Default SB3 hyperparameters vs. tuned: Authors use off-the-shelf hyperparameters for comparability, which may underrepresent algorithm potential
  - Mean per-step reward vs. cumulative return: Chosen to avoid episode-length confounding; valid because episodes have fixed length within each environment
  - Single unified solver vs. physics-specialized backends: PICT handles all environments; may lack domain-specific optimizations compared to specialized solvers

- Failure signatures:
  - CUDA out of memory: 3D environments (especially Airfoil3D) require large VRAM; reduce batch size or use gradient checkpointing
  - Slow convergence on hard difficulty: SAC outperforms PPO at higher Re/Ra; if PPO stalls, switch algorithms or increase sample count
  - MARL coordination failure: If agents don't synchronize (e.g., RBC convection rolls don't form), check reward decomposition (local vs. global weight β) and observation window size
  - D-MPC divergence: If gradient magnitudes explode, reduce learning rate α or shorten horizon H; chaotic flows may require smaller timesteps

- First 3 experiments:
  1. Baseline SARL on CylinderJet2D-easy-v0: Train PPO and SAC with default SB3 hyperparameters for 50k steps, 5 seeds. Compare mean test reward and drag reduction to Table 8 values (PPO: 5.6%, SAC: 6.7%). Verify installation and reproduce paper baselines
  2. Gradient-based control with D-MPC: Implement Algorithm 1 on CylinderJet2D across all three difficulty levels. Vary horizon H (10, 20, 40) and learning rate α (0.05, 0.1, 0.2). Compare to RL baselines to understand when gradient-only methods are competitive
  3. MARL transfer across domain sizes: Train MA-PPO and MA-SAC on TCFSmall3D-both, evaluate on TCFLarge3D-both. Reproduce Figure 8 finding that small-domain policies outperform large-domain training. Investigate whether equivariance holds across Reτ levels

## Open Questions the Paper Calls Out

- How do gradient-based differentiable RL methods compare to model-free RL across FluidGym environments?
  - Basis in paper: Limitations section states "evaluating gradient-based methods, e.g., DPC (Drgoˇna et al., 2022) and differentiable RL (Xu et al., 2022; Xing et al., 2025)... is a natural next step"
  - Why unresolved: D-MPC is only demonstrated as proof-of-concept on CylinderJet2D; no systematic comparison with differentiable RL methods was conducted
  - What evidence would resolve it: Benchmarking DPC and differentiable RL variants against PPO/SAC on all FluidGym environments with metrics for sample efficiency, final performance, and wall-clock training time

- How does algorithm-specific hyperparameter tuning affect the observed PPO vs SAC performance gap at higher difficulty levels?
  - Basis in paper: Limitations notes "baseline algorithms are evaluated using standard hyperparameters from off-the-shelf libraries, which promotes comparability but may not reflect each algorithm's optimal performance"
  - Why unresolved: SAC consistently outperforms PPO at medium/hard difficulties, but this gap may reflect default hyperparameters rather than inherent algorithmic differences
  - What evidence would resolve it: Hyperparameter sweeps for both algorithms showing IQM performance profiles across environments and difficulty levels

- What is the sample efficiency advantage of differentiable control (D-MPC) over model-free RL across turbulence intensities?
  - Basis in paper: D-MPC achieves comparable drag reduction to PPO/SAC on CylinderJet2D-easy without policy learning, but is only evaluated on one environment
  - Why unresolved: The scalability of gradient-only control to more turbulent regimes (medium/hard) and 3D flows remains untested
  - What evidence would resolve it: D-MPC performance curves across all cylinder and RBC environments with varying Rayleigh/Reynolds numbers

## Limitations
- Exact MLPOLICY architectures for RL agents are not specified, affecting reproducibility of reported performance
- Multi-agent implementation details (local observation windowing, reward decomposition across different environments) are underspecified
- D-MPC implementation beyond Algorithm 1 lacks details on optimizer choice and action initialization strategy

## Confidence
- High Confidence: GPU-accelerated simulation mechanics and differentiability, MARL environment structure, core benchmark design principles
- Medium Confidence: SAC vs. PPO performance comparisons, multi-agent MARL results due to unspecified architectural details
- Medium Confidence: D-MPC gradient-based control effectiveness due to missing implementation specifics

## Next Checks
1. Replicate PPO and SAC training on CylinderJet2D-easy-v0 with default SB3 hyperparameters for 50k steps, comparing mean reward and drag reduction to Table 8 values (PPO: 5.6%, SAC: 6.7%)
2. Implement D-MPC on CylinderJet2D across difficulty levels with varying horizons (H=10,20,40) and learning rates (α=0.05,0.1,0.2), comparing to RL baselines
3. Train MA-PPO and MA-SAC on TCFSmall3D-both, evaluate on TCFLarge3D-both, and test whether small-domain policies transfer better than large-domain training