---
ver: rpa2
title: 'GPTKB v1.5: A Massive Knowledge Base for Exploring Factual LLM Knowledge'
arxiv_id: '2507.05740'
source_url: https://arxiv.org/abs/2507.05740
tags:
- gptkb
- knowledge
- entity
- gptkbp
- entities
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GPTKB v1.5 is a 100-million-triple knowledge base built from GPT-4.1
  for $14k using recursive LLM knowledge materialization. The system enables large-scale
  analysis of LLM factual knowledge through link-based exploration, SPARQL querying,
  and cross-model comparison.
---

# GPTKB v1.5: A Massive Knowledge Base for Exploring Factual LLM Knowledge

## Quick Facts
- arXiv ID: 2507.05740
- Source URL: https://arxiv.org/abs/2507.05740
- Authors: Yujia Hu; Tuan-Phong Nguyen; Shrestha Ghosh; Moritz Müller; Simon Razniewski
- Reference count: 2
- 100-million-triple knowledge base built from GPT-4.1 for $14k using recursive LLM knowledge materialization

## Executive Summary
GPTKB v1.5 is a 100-million-triple knowledge base built from GPT-4.1 for $14k using recursive LLM knowledge materialization. The system enables large-scale analysis of LLM factual knowledge through link-based exploration, SPARQL querying, and cross-model comparison. The KB contains 6.1M entities, 381K relations, and 32K classes with 75.5% triple accuracy. The methodology uses BFS-based traversal with LLM-based NER and post-hoc consolidation. The public demonstrator (gptkb.org) provides entity browsing, SPARQL endpoint, and RDF dump. Key analyses include nationality and gender bias detection, symmetry checks, and EU citizen counts.

## Method Summary
The system uses BFS-based recursive prompting starting from "Vannevar Bush" to elicit subject-predicate-object triples from GPT-4.1. LLM-based NER identifies new entities in triple objects, which are enqueued for subsequent elicitation. Post-hoc consolidation applies greedy clustering to merge relations and classes based on label embedding similarity. Constrained decoding ensures triple format compliance. The resulting KB is stored in Virtuoso and exposed via SPARQL endpoint and web interface.

## Key Results
- 100M triples, 6.1M entities, 381K canonical relations, 32K classes
- 75.5% triple accuracy with automated web validation
- $14,136 cost for full construction over 18 days
- Strong Western/English-language nationality bias (374k American vs 45k Indian)
- Only 16.2% of spouse relations are bidirectional

## Why This Works (Mechanism)

### Mechanism 1: Recursive Knowledge Materialization via BFS Traversal
Recursive prompting with BFS traversal produces a densely interlinked knowledge base that overcomes availability bias in sample-based evaluations. Starting from a seed entity, the LLM is prompted for subject-predicate-object triples. Named entities in object positions are identified via LLM-based NER and enqueued for subsequent elicitation. This BFS-style expansion continues recursively, building a graph where each node's triples are elicited before exploring its neighbors. The LLM's factual knowledge is structurally connected—entities reference other entities that the model also knows about, enabling graph traversal to surface latent knowledge.

### Mechanism 2: Constrained Decoding with Post-hoc Canonicalization
Combining constrained decoding (forcing triple format) with post-hoc embedding-based clustering yields a usable schema without manual ontology engineering. Constrained decoding ensures outputs remain in triple format. Post-hoc consolidation applies greedy clustering to merge relations and classes based on label embedding similarity, reducing 936k raw relations to 381k canonical relations and 220k classes to 32k. Surface form variations (e.g., "bornIn" vs. "birthplace") share sufficient embedding similarity to cluster automatically, and semantic equivalence can be determined without human judgment.

### Mechanism 3: Materialized KB Enables SQL-like Analytical Queries on LLM Knowledge
Converting LLM knowledge to a persistent KB enables scalable statistical analysis that would be prohibitively expensive via repeated prompting. Once triples are materialized and stored in Virtuoso, SPARQL queries execute in milliseconds rather than requiring API calls. This permits aggregate analyses (e.g., counting citizens per country, measuring spouse symmetry) across millions of triples. The materialized KB is representative of the LLM's complete factual knowledge—the BFS traversal from "Vannevar Bush" reaches a representative sample of entities.

## Foundational Learning

- **RDF Triples and SPARQL**
  - Why needed here: The entire system outputs subject-predicate-object triples stored in a Virtuoso triple store. Understanding RDF serialization (Turtle syntax) and SPARQL query patterns (GROUP BY, OPTIONAL, FILTER) is essential to both use and extend the system.
  - Quick check question: Can you write a SPARQL query that counts distinct predicates used across all triples?

- **Breadth-First Search (BFS) for Graph Construction**
  - Why needed here: The knowledge elicitation process explicitly uses BFS traversal. Understanding why BFS (vs. DFS) was chosen relates to controlling expansion depth and managing the exploration frontier.
  - Quick check question: What would happen to the entity distribution if the system used DFS instead of BFS from the same seed?

- **Named Entity Recognition (NER)**
  - Why needed here: NER determines which object values become new graph nodes. Errors in NER propagate through the entire recursive process—missed entities reduce recall; false positives create hallucinated nodes.
  - Quick check question: If NER incorrectly identifies "April 28, 2012" as a named entity, what downstream effects would you expect?

## Architecture Onboarding

- **Component map:**
  [Seed Entity] → [Knowledge Prompting] → [Triple Extraction]
                                            ↓
  [Subject Queue] ← [LLM-based NER] ← [Triple Objects]
        ↓
  [Parallel API Calls] → [Raw Triple Set]
                              ↓
                    [Knowledge Consolidation]
                    /         |          \
           [Class Clustering] [Relation Clustering] [Entity Deduplication]
                              ↓
                    [Taxonomy Construction]
                              ↓
                    [RDF Serialization (Turtle)]
                              ↓
              [Virtuoso Triple Store] ←→ [Django Web App + Nginx]
                              ↓
              [SPARQL Endpoint + Entity Browser + RDF Dump]

- **Critical path:** The recursive loop (Prompt → NER → Enqueue → Prompt) is the rate-limiting step. The paper reports 18 days and $14k for 100M triples. Any optimization must target this loop—reducing API calls per entity or improving parallelization without distorting BFS order.

- **Design tradeoffs:**
  - **BFS vs. parallelization:** Parallel crawling distorts BFS search order, requiring post-hoc recomputation of shortest paths. The system stores `bfsLayer` and `bfsParent` as meta-relations to recover structural insights.
  - **Model quality vs. cost:** GPT-4.1 costs more than GPT-4o-mini but reduced hallucinations from 60% to ~20% and increased accuracy by 44 percentage points.
  - **Canonicalization threshold:** Higher embedding similarity thresholds preserve more relations (936k → less reduction); lower thresholds merge aggressively (381k final) but risk false positives.

- **Failure signatures:**
  - **Output skew:** v1.1 had entities with 100k+ triples (mostly hallucinated). Symptom: single entities dominating triple counts.
  - **Asymmetry:** Only 16.2% of spouse relations are bidirectional. Symptom: `A spouse B` exists but `B spouse A` does not.
  - **Hallucinated entities:** For "Christos Christodoulopoulos," GPT-4o-mini invented a Greek football player. Symptom: entities with no web verification.

- **First 3 experiments:**
  1. **Seed entity sensitivity analysis:** Run the pipeline from 3 different seed entities (e.g., "Marie Curie," "Confucius," "Lionel Messi") and compare entity distributions, nationality bias, and schema coverage. Hypothesis: Western seeds produce Western-biased KBs.
  2. **Symmetry injection experiment:** Modify the prompt to explicitly request bidirectional relations (e.g., "If A is spouse of B, also state B is spouse of A"). Measure whether symmetry increases from 16.2% baseline without inflating hallucinations.
  3. **Consolidation threshold sweep:** Re-run consolidation with embedding similarity thresholds at 0.7, 0.8, 0.9, and 0.95. Plot resulting relation counts vs. manual precision sampling on 100 random merged pairs. Identify the knee point.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can recursive knowledge materialization achieve high symmetry in bidirectional relations, or is asymmetry inherent to auto-regressive LLMs?
- Basis in paper: The paper shows only 16.2% of spouse relations are bidirectional (65,339/402,333), stating "asymmetry is also dominant in GPT-4.1's factual knowledge" and referencing the reversal curse literature.
- Why unresolved: The methodology does not attempt symmetry enforcement; the paper only measures asymmetry post-hoc as an analytical insight.
- What evidence would resolve it: A modified construction pipeline that explicitly prompts for inverse relations or validates symmetry during consolidation, comparing resulting bidirectional coverage rates.

### Open Question 2
- Question: How does the choice of seed entity affect the coverage and topical distribution of the resulting knowledge base?
- Basis in paper: The entire KB is built from a single seed (Vannevar Bush) via BFS traversal, but the paper does not analyze how different seeds might yield different knowledge domains or coverage patterns.
- Why unresolved: Only one seed was used; no ablation or comparison across multiple seeds was conducted.
- What evidence would resolve it: Running parallel constructions from diverse seeds (e.g., non-Western entities, scientific vs. cultural figures) and comparing entity overlap, class distributions, and triple content.

### Open Question 3
- Question: Can automated web-based validation reliably distinguish between "plausible" and "true" triples, and how should semantically incomprehensible outputs be handled?
- Basis in paper: The evaluation notes that "the truth of some triples remains undecidable, mostly because parts of them are semantically incomprehensible," and reports discrepancies between automated (75.5% true) and manual (75% true) assessments.
- Why unresolved: The validation framework cannot adjudicate unclear or nonsensical outputs, and the "plausible" category remains poorly characterized.
- What evidence would resolve it: A human annotation study on the "plausible" and "unverifiable" categories with inter-annotator agreement analysis, plus error taxonomy of incomprehensible outputs.

### Open Question 4
- Question: To what extent does the observed nationality bias reflect LLM training data versus the BFS traversal order and seed entity selection?
- Basis in paper: The paper documents strong Western/English-language nationality bias (374,263 American vs. far fewer non-Western nationalities) and notes it is "at a much stronger bias than existing resources like Wikidata," but does not disentangle cause.
- Why unresolved: No controlled experiments varying traversal parameters or comparing against randomized sampling approaches.
- What evidence would resolve it: Comparing nationality distributions across KBs built with different seeds, traversal strategies, or explicit debiasing interventions in the prompting.

## Limitations

- The BFS traversal starting from "Vannevar Bush" may create sampling bias, potentially favoring Western/English-language knowledge domains.
- The 75.5% accuracy claim depends on automated web validation without detailed error analysis of false positive types.
- The consolidation process reduces 936k raw relations to 381k canonical relations using unspecified embedding similarity thresholds, making it difficult to assess false positive rates in clustering.

## Confidence

- **High confidence:** The system architecture and methodology are clearly specified. The BFS traversal approach, SPARQL endpoint functionality, and bias analysis results (nationality, gender, symmetry) are reproducible from the described pipeline.
- **Medium confidence:** The 75.5% accuracy figure and cost breakdown ($14,136 for 100M triples) are reported but depend on unspecified implementation details of the validation framework and prompt templates.
- **Low confidence:** The completeness and representativeness of the KB relative to GPT-4.1's full factual knowledge, and the robustness of the consolidation thresholds across different domains or seed entities.

## Next Checks

1. **NER error propagation analysis:** Run the pipeline with a controlled seed entity set and systematically measure how NER errors (both false positives and false negatives) affect the final KB structure and size. Compare against ground-truth entity lists for sample entities.

2. **Seed entity bias validation:** Replicate the KB construction from three diverse seed entities (Western, Eastern, and non-English domains) and quantify differences in entity distribution, relation coverage, and nationality bias statistics to assess the sampling bias introduced by the BFS traversal.

3. **Consolidation threshold sensitivity:** Execute the consolidation pipeline across a range of embedding similarity thresholds (0.7, 0.8, 0.9, 0.95) and manually validate 100 randomly selected merged pairs at each threshold to identify the point where precision drops below acceptable levels while maintaining practical relation count reduction.