---
ver: rpa2
title: Where is the multimodal goal post? On the Ability of Foundation Models to Recognize
  Contextually Important Moments
arxiv_id: '2601.16333'
source_url: https://arxiv.org/abs/2601.16333
tags:
- moments
- important
- multimodal
- video
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the ability of multimodal foundation models
  to identify important moments in football games. The authors create a new dataset
  called MOMENTS by leveraging highlight reels and full game videos to obtain labeled
  important and non-important moments without manual annotation.
---

# Where is the multimodal goal post? On the Ability of Foundation Models to Recognize Contextually Important Moments

## Quick Facts
- arXiv ID: 2601.16333
- Source URL: https://arxiv.org/abs/2601.16333
- Authors: Aditya K Surikuchi; Raquel Fernández; Sandro Pezzelle
- Reference count: 40
- Primary result: Multimodal foundation models perform near chance level on recognizing contextually important football moments

## Executive Summary
This paper investigates whether current multimodal foundation models can identify contextually important moments in football games. The authors introduce MOMENTS, a dataset of 100,000+ labeled moments extracted from football videos using SSIM-based localization without manual annotation. Despite testing multiple state-of-the-art models across seven modality combinations, performance remains near chance level (MCC ≈ 0), with no significant advantage for multimodal over unimodal models. Analysis reveals that models tend to rely on visual modality for important moments and textual commentary for non-important moments, suggesting limitations in cross-modal integration.

## Method Summary
The MOMENTS dataset was created by automatically extracting important and non-important moments from full football game videos and highlight reels using hierarchical SSIM matching. The method identifies 100/127 highlight pairs successfully, with the remaining 27 pairs excluded due to localization failure. Seven modality combinations were tested across multiple off-the-shelf models: audio, language, video, audio+language, audio+video, language+video, and all three modalities. Performance was evaluated using accuracy, F1, and Matthews Correlation Coefficient (MCC), with internal analysis conducted via modality contribution scores (ΔZ).

## Key Results
- Models achieve near chance-level performance (MCC ≈ 0) across all modality combinations
- No significant advantage for multimodal models over unimodal ones
- Visual modality drives positive classification confidence for important moments (VIM > LIM)
- Textual modality drives positive confidence for non-important classification (LNIM > VNIM)
- Many multimodal predictions show lower confidence than optimal unimodal predictions, indicating fusion collapse

## Why This Works (Mechanism)

### Mechanism 1: Modality-Specific Dominance for Different Moment Types
- Models disproportionately rely on visual signals for important moments and linguistic signals for non-important moments
- Contribution scores (ΔZ) computed via logit differences show visual modality drives positive classification confidence for important moments, while textual commentary drives confidence for non-important classification
- Core assumption: Logit differences proxy internal feature attribution; the eye–voice span (EVS) latency (~3s) aligns commentary with visual events
- Evidence anchors:
  - [abstract]: "models tend to rely on visual modality for important moments and textual modality for non-important moments"
  - [section 5.1]: Figure 5 shows contribution scores with VIM > LIM for important moments and LNIM > VNIM for non-important moments
  - [corpus]: Weak direct corpus support; related work on abstract concept recognition (arXiv:2508.20765) notes models struggle with non-concrete video understanding but does not address modality dominance
- Break condition: If models were effectively fusing modalities, contribution scores would be balanced across modalities for both classes rather than polarized

### Mechanism 2: Hierarchical Frame Localization via SSIM
- Highlight moments can be automatically extracted from full-game videos using multi-scale Structural Similarity Index Measure without manual annotation
- Three-step hierarchical matching reduces complexity from O(H×G) to tractable levels: (1) second-level representative frame comparison, (2) pruning via well-localized boundaries, (3) fine-grained frame matching with 0.8 SSIM threshold
- Core assumption: Highlight frames are subsequences of full-game frames with acceptable visual distortion from overlays/scorecards
- Evidence anchors:
  - [abstract]: "leveraging highlight reels and full game videos to obtain labeled important and non-important moments without manual annotation"
  - [section 3.1]: SSIM threshold of 0.8; Figure 2 shows >50% frames localized for 100/127 pairs after step 2
  - [corpus]: No direct corpus evidence for this specific SSIM-based localization approach
- Break condition: If highlight reels used substantially different camera angles or editing, SSIM matching would fail; the 27/127 pairs with poor localization suggest this occurs

### Mechanism 3: Cross-Modal Integration Failure (Modality Collapse)
- Current multimodal architectures exhibit "collapse towards one modality" rather than synergistic fusion
- Figure 6 shows many points below the diagonal (multimodal confidence < unimodal confidence), indicating multimodal inputs introduce noise rather than complementary signal for contextually important moments
- Core assumption: Effective fusion should yield multimodal confidence ≥ optimal unimodal confidence
- Evidence anchors:
  - [abstract]: "no significant advantage for multimodal models over unimodal ones"
  - [section 5.2]: "lack of many points in top left region above the diagonal" and "collapse towards one of the modalities"
  - [corpus]: Unified Interaction Foundational Model paper (arXiv:2509.06025) similarly notes foundation models "fail to grasp the holistic nature of structured interactions"
- Break condition: If static projector-based fusion were sufficient, multimodal settings would outperform unimodal consistently across moment types

## Foundational Learning

- Concept: **Matthews Correlation Coefficient (MCC)**
  - Why needed here: MCC accounts for all four confusion matrix cells, avoiding F1's true-positive bias in binary classification evaluation
  - Quick check question: Given a model with 75% accuracy on a balanced dataset, what additional information does MCC provide that accuracy alone cannot?

- Concept: **Modality Contribution via Logit Differences (ΔZ)**
  - Why needed here: Enables internal model analysis beyond downstream metrics by quantifying how each modality shifts prediction confidence
  - Quick check question: If ΔZ for visual modality is +3.81 and ΔZ for language modality is -0.18 for the same sample, what does this imply about the model's reliance pattern?

- Concept: **Static vs. Dynamic Multimodal Fusion**
  - Why needed here: The paper critiques static projector-based fusion (fixed information flow) and advocates for sample-level dynamic routing (modularity)
  - Quick check question: Why might a static fusion strategy fail when some samples have conflicting signals across modalities?

## Architecture Onboarding

- Component map: A -> L -> V -> Classification head (where A=audio, L=language, V=video)
- Critical path:
  1. Moment extraction via SSIM hierarchical localization (Section 3.1)
  2. Modality alignment accounting for EVS latency (+3s audio extension)
  3. Prompt engineering for off-the-shelf model evaluation (two prompts tested)
  4. Contribution score computation via Equation 2 for modality analysis

- Design tradeoffs:
  - SSIM vs. learned features: SSIM is deterministic but may miss semantic similarity; DINOv2 (mentioned but not used) could capture higher-level features
  - Duration matching: Non-important moments sampled from Gamma distribution fitted to important moment durations—ensures comparability but may miss natural variation
  - Prompt sensitivity: Models tested with two prompts; modality ordering effects observed (six permutations for ALV)

- Failure signatures:
  - MCC ≈ 0 with accuracy ≈ 0.5 indicates chance-level performance
  - Multimodal ΔZ < max(unimodal ΔZ) indicates collapse
  - Contribution scores heavily skewed toward one modality indicate ineffective fusion

- First 3 experiments:
  1. Baseline replication: Train logistic regression classifiers on n-gram text, MFCC audio, and SwinTransformer video features to establish performance bounds
  2. Modality ablation: Compare each model's performance across all seven modality combinations to identify dominant modality patterns
  3. Contribution score analysis: Compute ΔZ-based contribution scores (Equation 2) for correctly vs. incorrectly classified samples to diagnose fusion failures

## Open Questions the Paper Calls Out
None

## Limitations
- SSIM-based localization successfully processes 100/127 pairs but fails on 27 pairs (21.3%), suggesting potential generalization issues across different football broadcasts
- Modality ordering effects could be influenced by prompt sensitivity rather than fundamental modality dominance patterns
- Football-specific context may limit generalizability to other domains requiring abstract concept understanding

## Confidence
- **High Confidence**: MOMENTS dataset creation methodology (SSIM-based localization with 127 pairs), chance-level performance across all models (MCC ≈ 0), modality contribution analysis showing visual dominance for important moments
- **Medium Confidence**: Claim that multimodal models show "no significant advantage" over unimodal models, the specific modality ordering effects observed in contribution scores
- **Low Confidence**: Generalizability of modality dominance patterns to other abstract concept recognition tasks beyond football highlights

## Next Checks
1. **Cross-Domain Validation**: Evaluate the same models on the "Looking Beyond the Obvious" dataset for abstract concept recognition to test whether modality dominance patterns persist across different video understanding tasks.

2. **SSIM Robustness Testing**: Systematically vary the SSIM threshold (0.7, 0.75, 0.85, 0.9) and document localization success rates to determine the optimal threshold and identify failure modes.

3. **Fusion Architecture Comparison**: Implement and test a simple dynamic routing fusion mechanism (e.g., sample-level modality gating) versus the static projector approach to quantify performance improvements in cross-modal integration.