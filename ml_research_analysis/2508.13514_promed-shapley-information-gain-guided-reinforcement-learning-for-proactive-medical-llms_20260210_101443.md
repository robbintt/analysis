---
ver: rpa2
title: 'ProMed: Shapley Information Gain Guided Reinforcement Learning for Proactive
  Medical LLMs'
arxiv_id: '2508.13514'
source_url: https://arxiv.org/abs/2508.13514
tags:
- information
- patient
- medical
- question
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of medical LLMs operating in a
  reactive paradigm during interactive consultations, where they answer without seeking
  additional information, leading to misdiagnosis. The authors propose ProMed, a reinforcement
  learning framework that shifts LLMs toward a proactive paradigm by enabling them
  to ask clinically valuable questions before decision-making.
---

# ProMed: Shapley Information Gain Guided Reinforcement Learning for Proactive Medical LLMs

## Quick Facts
- arXiv ID: 2508.13514
- Source URL: https://arxiv.org/abs/2508.13514
- Reference count: 40
- ProMed achieves 6.29% average improvement over state-of-the-art methods on medical diagnostic tasks

## Executive Summary
This paper addresses the fundamental limitation of reactive medical LLMs that answer patient queries without proactively seeking additional diagnostic information, leading to suboptimal medical decisions. The authors propose ProMed, a reinforcement learning framework that shifts medical LLMs toward a proactive consultation paradigm by enabling them to ask clinically valuable questions before diagnosis. The core innovation is the Shapley Information Gain (SIG) reward mechanism, which quantifies the utility of questions by combining information-theoretic measures with contextual importance using Shapley values from cooperative game theory.

The framework demonstrates significant improvements in medical diagnostic accuracy through a two-stage training pipeline that first initializes the model using high-reward trajectories generated by Monte Carlo Tree Search (MCTS), then refines the policy using SIG-augmented reinforcement learning. Experiments on two newly curated partial-information medical benchmarks show ProMed outperforming existing methods by 6.29% on average while delivering a 54.45% improvement over reactive approaches.

## Method Summary
ProMed employs a novel reinforcement learning framework that enables medical LLMs to proactively ask diagnostic questions rather than reactively answering patient queries. The core innovation is the Shapley Information Gain (SIG) reward, which quantifies question utility by combining information gain with contextual importance via Shapley values from cooperative game theory. The framework operates through a two-stage training pipeline: (1) SIG-Guided Model Initialization uses MCTS to construct high-reward interaction trajectories for supervised warm-up, and (2) SIG-Augmented Policy Optimization integrates SIG into GRPO with a novel reward distribution mechanism that assigns higher rewards to informative questions. The approach shifts medical LLMs from reactive to proactive consultation paradigms, enabling them to gather additional information before making diagnostic decisions.

## Key Results
- ProMed achieves an average 6.29% improvement over state-of-the-art methods on medical diagnostic benchmarks
- Delivers a 54.45% gain over the reactive consultation paradigm
- Demonstrates robust generalization to out-of-domain cases on newly curated partial-information medical benchmarks

## Why This Works (Mechanism)
The Shapley Information Gain mechanism works by quantifying the marginal contribution of each question to the overall diagnostic information gain, weighted by its contextual importance. Unlike traditional information gain measures that treat all questions equally, SIG uses cooperative game theory principles to evaluate how each question's contribution changes when combined with others. The MCTS initialization phase generates high-quality interaction trajectories that serve as supervision for the initial training phase, ensuring the model learns from optimal consultation patterns. The two-stage training pipeline allows the model to first learn good question-answering patterns through supervised learning on MCTS-generated data, then refine these patterns through reinforcement learning with the SIG reward.

## Foundational Learning
**Reinforcement Learning with Proximal Policy Optimization (PPO/GRPO)**: Needed to train the model to make sequential decisions about which questions to ask; quick check - verify the policy gradient updates are properly clipped to prevent instability.

**Monte Carlo Tree Search (MCTS)**: Required for generating high-quality interaction trajectories during initialization; quick check - ensure the exploration-exploitation balance produces diverse yet relevant question sequences.

**Shapley Values from Cooperative Game Theory**: Essential for quantifying each question's marginal contribution to diagnostic accuracy; quick check - validate that the Shapley value calculations correctly capture the counterfactual importance of questions.

**Information Gain Theory**: Fundamental for measuring how much uncertainty reduction each question provides; quick check - confirm the information gain metric aligns with clinical diagnostic value.

**Partial-Information Environments**: Necessary for creating realistic medical consultation scenarios where complete information isn't available upfront; quick check - verify the partial information masking doesn't create unrealistic diagnostic scenarios.

## Architecture Onboarding

**Component Map**: Patient Query -> Question Selection Module -> Information Gathering -> Diagnostic Decision -> Reward Calculation (SIG) -> Policy Update

**Critical Path**: The most critical path is the question selection and information gathering sequence, where the model must choose which questions to ask and interpret the responses to make accurate diagnoses. This path directly determines the SIG reward and subsequent policy updates.

**Design Tradeoffs**: The framework trades computational complexity during training (MCTS and Shapley value calculations) for improved diagnostic accuracy and more natural consultation flows. The two-stage training approach balances the need for high-quality initialization data with the flexibility of reinforcement learning fine-tuning.

**Failure Signatures**: 
- Low SIG rewards across multiple episodes suggest the model is asking irrelevant or redundant questions
- High variance in reward distribution may indicate unstable MCTS trajectory generation
- Poor generalization to out-of-domain cases could reveal overfitting to specific medical domains

**First 3 Experiments**:
1. Compare SIG-augmented training versus standard RL without SIG rewards to isolate the benefit of the information gain mechanism
2. Test MCTS-generated trajectories versus random question sequences during initialization to quantify trajectory quality impact
3. Evaluate model performance with varying levels of partial information to determine the threshold where proactive questioning becomes most valuable

## Open Questions the Paper Calls Out
None specified in the provided materials.

## Limitations
- Reliance on two newly curated partial-information medical benchmarks that lack independent validation
- Heavy dependence on MCTS trajectory quality during initialization phase without adequate robustness analysis
- Impressive performance claims (54.45% gain) lack clear context about baseline model capabilities and clinical relevance

## Confidence

**SIG reward mechanism validity**: Medium - The theoretical foundation is sound, but empirical validation across diverse medical scenarios is limited

**Performance claims (6.29% average improvement)**: Low-Medium - Based on proprietary benchmarks without external verification

**Generalization claims**: Low - Limited out-of-domain testing with unspecified domains

## Next Checks
1. Release and validate the MedQA-P and MedCAT-P benchmarks on independent medical expert panels to confirm question relevance and diagnostic value
2. Conduct ablation studies removing the MCTS initialization phase to quantify its contribution versus the SIG reward mechanism alone
3. Test ProMed's performance on established medical question-answering datasets with complete information to verify it doesn't degrade when full context is available