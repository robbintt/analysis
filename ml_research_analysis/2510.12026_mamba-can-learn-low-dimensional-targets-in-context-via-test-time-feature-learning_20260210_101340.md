---
ver: rpa2
title: Mamba Can Learn Low-Dimensional Targets In-Context via Test-Time Feature Learning
arxiv_id: '2510.12026'
source_url: https://arxiv.org/abs/2510.12026
tags:
- mamba
- learning
- in-context
- have
- logd
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes Mamba's in-context learning (ICL) capability
  through a theoretical lens, focusing on tasks defined by low-dimensional nonlinear
  target functions. The authors introduce a framework combining input embeddings,
  a simplified Mamba architecture, and a two-stage gradient-based pretraining algorithm.
---

# Mamba Can Learn Low-Dimensional Targets In-Context via Test-Time Feature Learning

## Quick Facts
- arXiv ID: 2510.12026
- Source URL: https://arxiv.org/abs/2510.12026
- Authors: Junsoo Oh; Wei Huang; Taiji Suzuki
- Reference count: 40
- Primary result: Mamba achieves test-time feature learning for low-dimensional nonlinear targets with sample complexity matching nonlinear Transformers while maintaining computational efficiency

## Executive Summary
This paper presents a theoretical analysis of Mamba's in-context learning (ICL) capabilities, focusing on tasks defined by low-dimensional nonlinear target functions. The authors develop a framework that combines input embeddings, a simplified Mamba architecture, and a two-stage gradient-based pretraining algorithm. They prove that Mamba can perform test-time feature learning by extracting relevant task features directly from context examples, establishing sample complexity bounds that improve upon linear Transformers and match nonlinear Transformers. The key insight is that Mamba's nonlinear gating mechanism enables feature extraction, overcoming limitations of purely linear recurrent updates.

## Method Summary
The authors introduce a simplified Mamba architecture with a single gating function and analyze its in-context learning capability through a theoretical framework. The method involves two stages: first, a gradient-based pretraining algorithm learns a generative model for the input data, and second, Mamba performs test-time feature learning on low-dimensional nonlinear target functions. The theoretical analysis focuses on single-index models and establishes sample complexity bounds that depend on the generative exponent rather than the information exponent. The pretraining algorithm assumes access to a generative model with specific structural properties, and the analysis provides guarantees for Mamba's performance on synthetic datasets.

## Key Results
- Mamba achieves test-time feature learning by extracting relevant task features from context examples
- Sample complexity bounds improve upon linear Transformers and match nonlinear Transformers
- Mamba's performance is comparable to softmax Transformers while maintaining computational efficiency

## Why This Works (Mechanism)
Mamba's ability to perform in-context learning stems from its nonlinear gating mechanism, which enables test-time feature learning. Unlike linear recurrent models that can only learn linear combinations of features, Mamba's gating allows it to extract and adapt to the relevant low-dimensional nonlinear structure present in the context examples. This mechanism overcomes the fundamental limitation of linear models in handling nonlinear target functions, making it possible to achieve sample complexity comparable to nonlinear Transformers.

## Foundational Learning
- **Single-index models**: Nonlinear functions of a single linear combination of features; needed to model low-dimensional nonlinear targets in ICL tasks; quick check: verify target function depends on one linear combination
- **Generative exponents**: Parameters controlling the complexity of the data distribution; needed to establish sample complexity bounds; quick check: confirm generative exponent is bounded
- **Information exponents**: Measure of task complexity; needed to compare sample complexity across different architectures; quick check: verify information exponent is properly defined for target functions
- **Test-time feature learning**: Process of extracting relevant features from context examples during inference; needed to explain Mamba's ICL capability; quick check: confirm features are extracted from context
- **Nonlinear gating**: Mechanism allowing selective information flow; needed to enable feature extraction in Mamba; quick check: verify gating function is nonlinear
- **Sample complexity**: Number of examples needed for accurate learning; needed to establish theoretical guarantees; quick check: confirm sample complexity bounds are polynomial

## Architecture Onboarding

**Component Map:**
Input Embeddings -> Simplified Mamba Block -> Gating Mechanism -> Output Layer

**Critical Path:**
Context examples are first embedded, then processed through the simplified Mamba block where the gating mechanism extracts relevant features, and finally the output layer produces predictions based on these learned features.

**Design Tradeoffs:**
The simplified architecture trades off model complexity for theoretical tractability, making it possible to establish formal guarantees. The nonlinear gating mechanism provides the essential capability for feature extraction but adds computational overhead compared to linear recurrent models. The two-stage pretraining approach requires careful initialization but enables better generalization.

**Failure Signatures:**
- Poor performance on high-dimensional targets where the single-index assumption breaks down
- Sensitivity to initialization of the generative model in the pretraining stage
- Suboptimal feature extraction when context examples are noisy or contain outliers
- Computational inefficiency when the gating mechanism becomes too complex

**3 First Experiments:**
1. Test Mamba's ICL performance on synthetic single-index model tasks with varying generative exponents
2. Compare sample complexity of Mamba versus linear Transformers on low-dimensional nonlinear targets
3. Evaluate robustness of Mamba's feature extraction to noisy context examples

## Open Questions the Paper Calls Out
None

## Limitations
- Simplified Mamba architecture may not capture behavior of practical implementations with multiple attention blocks
- Analysis restricted to single-index models, limiting generalization to more complex task distributions
- Pretraining algorithm assumes access to generative models with specific structural properties
- Empirical validation limited to synthetic datasets with narrow task diversity

## Confidence
- High confidence in theoretical framework and sample complexity bounds for simplified Mamba architecture
- Medium confidence in applicability to practical Mamba implementations
- Medium confidence in empirical results given limited task diversity
- Low confidence in generalization beyond single-index models

## Next Checks
1. Extend experiments to include diverse natural language tasks and compare Mamba's in-context learning performance against standard benchmarks
2. Test the theoretical predictions with more realistic Mamba architectures incorporating multiple blocks and normalization layers
3. Analyze robustness to outliers and noisy context examples to validate the theoretical generalization bounds under practical conditions