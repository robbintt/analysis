---
ver: rpa2
title: Self-attention vector output similarities reveal how machines pay attention
arxiv_id: '2512.21956'
source_url: https://arxiv.org/abs/2512.21956
tags:
- attention
- context
- similarity
- token
- head
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces a new method for quantifying information
  processing in transformer architectures by analyzing the vector space emerging from
  self-attention layers. The authors developed a context similarity matrix that measures
  the scalar product between token vectors output by each attention head, providing
  a more fundamental view of how tokens are transformed as they propagate through
  the architecture.
---

# Self-attention vector output similarities reveal how machines pay attention

## Quick Facts
- arXiv ID: 2512.21956
- Source URL: https://arxiv.org/abs/2512.21956
- Authors: Tal Halevi; Yarden Tzach; Ronit D. Gross; Shalom Rosner; Ido Kanter
- Reference count: 0
- Primary result: Introduces context similarity matrices to quantify transformer information processing beyond attention weights

## Executive Summary
This study introduces a novel method for quantifying information processing in transformer architectures by analyzing the vector space emerging from self-attention layers. The authors developed a context similarity matrix that measures the scalar product between token vectors output by each attention head, providing a more fundamental view of how tokens are transformed as they propagate through the architecture. The analysis reveals that transformer layers progressively refine representations from lexical meaning to syntactic context, with different attention heads specializing in complementary linguistic features.

The research demonstrates that final layers focus attention on sentence separator tokens, suggesting a practical approach to text segmentation based on semantic features. The context similarity matrix provides a quantifiable language for assessing learning performance in transformers, though the authors note that further research is needed to develop a comprehensive signal-to-noise ratio framework for these architectures.

## Method Summary
The methodology extracts attention head outputs (not attention weights) from BERT-12, computing context similarity matrices via dot products of these output vectors. For each head, the output (128 × 64) is multiplied by its transpose to create a 128×128 context similarity matrix. High values indicate both vector alignment and magnitude similarity, revealing which token representations are being similarly transformed. The analysis tracks similarity distance distributions across layers and identifies head-specific focus tokens, with diagonal elements zeroed to emphasize token-to-token relationships.

## Key Results
- In final layers, attention maps focus on sentence separator tokens, suggesting a practical approach to text segmentation based on semantic features
- Different attention heads within the same layer specialize in different linguistic characteristics - some identify token repetitions while others recognize tokens of common appearance and their surrounding context
- As layers progress, context similarity shifts from long-range to short-range patterns, with final layers showing preference for strong similarities within the same sentence
- Each head tends to focus on a unique token from the text and builds similarity pairs centered around it

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The context similarity matrix captures information processing more meaningfully than attention weights alone.
- Mechanism: Each attention head output is multiplied by its transpose to create a 128×128 context similarity matrix. High values indicate both vector alignment and magnitude similarity.
- Core assumption: Vector similarity in the output space reflects meaningful linguistic relationships rather than arbitrary numerical artifacts.
- Evidence anchors: Prior studies show attention weights alone are insufficient for explaining learning processes; context similarity reveals distinct patterns between token vector pairs.

### Mechanism 2
- Claim: Transformer layers progressively refine representations from lexical meaning to syntactic context through predictable spatial patterns.
- Mechanism: Early layers show evenly distributed similarities; middle layers concentrate along the diagonal (local relationships); final layers spread similarities within sentence boundaries.
- Core assumption: The observed similarity evolution reflects hierarchical linguistic abstraction rather than training noise.
- Evidence anchors: Context similarity shifts from long-range to short-range patterns, with final layers showing preference for strong similarities within the same sentence.

### Mechanism 3
- Claim: Different attention heads specialize in complementary linguistic features.
- Mechanism: Each head develops a unique "focus token" per input and builds high-similarity pairs around it. Specialization is consistent across texts.
- Core assumption: Head specialization emerges from training dynamics and serves non-redundant functional roles.
- Evidence anchors: Each head tends to focus on a unique token and builds similarity pairs centered around it, with behavior independent of the text presented.

## Foundational Learning

- Concept: Dot product measures both alignment and magnitude
  - Why needed here: The entire methodology depends on understanding that scalar products capture directional alignment AND vector size.
  - Quick check question: Why would two vectors have a high dot product even if they point in different directions?

- Concept: Attention weights ≠ attention head outputs
  - Why needed here: The paper explicitly argues that attention maps don't explain learning; the post-attention output vectors do.
  - Quick check question: What additional information does the head output contain that the attention map lacks?

- Concept: Long-range vs. short-range token correlations
  - Why needed here: The paper tracks how median similarity distance shifts from ~60+ tokens (early layers) to <20 tokens (final layers).
  - Quick check question: In a 128-token sequence with 3-4 sentences, what distance would indicate "same-sentence" vs. "cross-sentence" correlations?

## Architecture Onboarding

- Component map:
  ```
  Tokenized input (128 tokens)
       ↓
  Embedding layer (lexical + positional, 768-dim)
       ↓
  12 Transformer blocks
       └─ Each block: 12 attention heads
            ├─ Q, K, V projections (64-dim each)
            ├─ Attention map: softmax(Q·K^T / √d)
            ├─ Head output: attention_map × V
            └─ Context similarity matrix: output · output^T
       ↓
  Concatenated heads → FFN → next block
       ↓
  Final layer outputs (sentence separators highly attended)
  ```

- Critical path:
  1. Extract attention head outputs from each layer
  2. Compute context similarity matrix per head: head_output @ head_output.T
  3. Zero the diagonal to emphasize off-diagonal patterns
  4. Track similarity distance distributions across layers
  5. Identify head-specific focus tokens

- Design tradeoffs:
  - Diagonal zeroing removes self-similarity but required to see token-to-token relationships
  - Threshold 0.3 for "high attention" columns: exploits observed gap in normalized attention distribution
  - Sequence length fixed at 128: standardizes analysis but may miss longer-range phenomena
  - Assumption: Unnormalized dot product is meaningful (not cosine similarity)

- Failure signatures:
  - No layer-wise pattern shift (similar distributions at layer 1 and 11) → potential training collapse
  - All heads show identical focus tokens → redundant specialization
  - No sentence-separator attention in final layers → positional encoding issue
  - High-similarity pairs with no linguistic relationship → noise-dominated signal

- First 3 experiments:
  1. **Baseline replication**: Load pre-trained BERT-12, run 100 sample texts, compute context similarity matrices for layers 1, 6, and 11; verify the long→short→medium range pattern.
  2. **Head specialization test**: For each head in layer 11, extract the most common token appearing in high-similarity pairs across 500 inputs; check if heads specialize on different tokens per input.
  3. **Architecture transfer**: Apply the same analysis to GPT-2 encoder layers or RoBERTa; assess whether sentence-separator attention and range patterns are universal or BERT-specific.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a comprehensive Signal-to-Noise Ratio (SNR) framework be developed for transformer architectures using context similarity matrices?
- Basis in paper: The authors state "the need for a quantifiable language for Signal-to-Noise ratio (SNR) persists" and note that "translating this tokenized accuracy into a vectorized signal and noise requires further extensive research."
- Why unresolved: The context similarity matrix reveals noise elements through high-similarity pairs with no direct linguistic comprehension, but no formal SNR quantification method exists yet.
- What evidence would resolve it: A formal metric distinguishing linguistically meaningful similarity patterns from noise, validated across multiple text samples and architectures.

### Open Question 2
- Question: How can transformer architectures achieve contextual comprehension of long-range correlations across larger texts?
- Basis in paper: The discussion asks "how can contextual comprehension of long-range correlations be achieved? Although the model is capable of capturing medium-range correlations within sentences, it has not yet demonstrated the ability to handle long-range contextual relationships across larger texts."
- Why unresolved: Final layers show preference for short-range, within-sentence similarities; only one head showed long-range capability, and only for identical tokens.
- What evidence would resolve it: Architectural modifications demonstrating consistent long-range dependency capture, measured via context similarity matrices on documents exceeding single-paragraph length.

### Open Question 3
- Question: Is the context similarity matrix behavior universal across different transformer architectures?
- Basis in paper: The authors state "a more comprehensive analysis of the context similarity matrices of other architectures is required in order to assess its universality" and suggest comparing "encoder-decoder architectures, such as translation systems."
- Why unresolved: This study only analyzed BERT-12; different architectures may exhibit different vector space dynamics.
- What evidence would resolve it: Replication of the context similarity analysis on GPT, T5, and other architectures showing consistent patterns of layer-wise similarity evolution.

### Open Question 4
- Question: Is the current attention-mechanism architecture optimal, or are more efficient alternatives needed?
- Basis in paper: The authors raise "the question of whether the presented attention-mechanism architecture is optimal" and note that "developing more efficient and smarter alternatives to advanced attention techniques that support long-range interactions is necessary."
- Why unresolved: Noise elements persist in the vector space, and the authors question whether large embedding dimensions are necessary.
- What evidence would resolve it: Comparative studies of alternative architectures achieving equivalent or superior NLP performance with reduced computational costs and cleaner similarity patterns.

## Limitations
- The study exclusively focuses on BERT-12 without validation across diverse transformer architectures or tasks
- The methodology assumes that vector similarity in attention head outputs meaningfully captures linguistic relationships, but this correlation remains empirically validated only within BERT
- The analysis uses fixed sequence length of 128 tokens, which may not capture longer-range dependencies or behaviors in models designed for extended contexts

## Confidence

**High Confidence Claims:**
- The mathematical framework for computing context similarity matrices is sound and reproducible
- BERT-12 exhibits predictable attention patterns with sentence separator tokens in final layers
- Different attention heads show measurable specialization patterns in similarity analysis

**Medium Confidence Claims:**
- The long-range to short-range similarity evolution reflects genuine hierarchical learning progression
- Head specialization represents functional complementarity rather than coincidental patterns
- Context similarity analysis provides actionable insights beyond traditional attention weight analysis

**Low Confidence Claims:**
- The methodology generalizes to other transformer architectures beyond BERT
- Observed patterns directly indicate optimal learning or superior architectural design
- Context similarity matrices capture all meaningful aspects of transformer information processing

## Next Checks
1. **Cross-Architecture Validation**: Apply the context similarity analysis to GPT-2 encoder layers, RoBERTa, and modern transformer variants. Document whether sentence-separator attention and similarity range patterns persist or differ systematically across architectures.

2. **Downstream Task Correlation**: Measure how context similarity patterns in specific layers and heads correlate with performance on benchmark NLP tasks (GLUE, SQuAD). Identify whether heads showing strong specialization or particular similarity distributions consistently contribute to better task performance.

3. **Ablation and Intervention Study**: Systematically remove or modify specific attention heads in BERT-12 and observe changes in context similarity patterns and task performance. This would test whether the observed specialization patterns are functionally necessary or could be redistributed without performance degradation.