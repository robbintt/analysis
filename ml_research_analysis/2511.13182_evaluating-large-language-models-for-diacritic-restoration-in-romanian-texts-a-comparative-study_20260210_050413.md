---
ver: rpa2
title: 'Evaluating Large Language Models for Diacritic Restoration in Romanian Texts:
  A Comparative Study'
arxiv_id: '2511.13182'
source_url: https://arxiv.org/abs/2511.13182
tags:
- restoration
- diacritics
- llms
- diacritic
- romanian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the performance of large language models (LLMs)
  in restoring diacritics in Romanian texts. Using a comprehensive corpus, models
  including OpenAI's GPT-3.5, GPT-4, GPT-4o, Google's Gemini 1.0 Pro, Meta's Llama
  2 and Llama 3, MistralAI's Mixtral 8x7B Instruct, airoboros 70B, and OpenLLM-Ro's
  RoLlama 2 7B were tested under multiple prompt templates ranging from zero-shot
  to complex multi-shot instructions.
---

# Evaluating Large Language Models for Diacritic Restoration in Romanian Texts: A Comparative Study

## Quick Facts
- arXiv ID: 2511.13182
- Source URL: https://arxiv.org/abs/2511.13182
- Reference count: 0
- This study evaluates LLMs' performance in Romanian diacritic restoration, finding GPT-4o achieves highest accuracy across multiple prompt templates.

## Executive Summary
This comparative study systematically evaluates large language models' capabilities in restoring diacritics in Romanian texts. The research tests multiple state-of-the-art models including OpenAI's GPT series, Google's Gemini, Meta's Llama family, and others using various prompt engineering approaches from zero-shot to complex multi-shot instructions. Results demonstrate that GPT-4o consistently achieves the highest diacritic restoration accuracy, significantly outperforming baseline approaches. The study reveals substantial performance variations across models, with some like Meta's Llama family showing inconsistent results. These findings underscore the importance of model selection and prompt design in NLP applications for diacritic-rich languages, while highlighting promising directions for future improvements in this specialized task.

## Method Summary
The study employs a comprehensive evaluation framework testing multiple large language models on Romanian diacritic restoration tasks. Models tested include OpenAI's GPT-3.5, GPT-4, GPT-4o, Google's Gemini 1.0 Pro, Meta's Llama 2 and Llama 3, MistralAI's Mixtral 8x7B Instruct, airoboros 70B, and OpenLLM-Ro's RoLlama 2 7B. Each model is evaluated using multiple prompt templates ranging from simple zero-shot instructions to complex multi-shot approaches with varying levels of guidance. A comprehensive Romanian text corpus serves as the evaluation dataset, allowing for systematic comparison of restoration accuracy across different models and prompting strategies.

## Key Results
- GPT-4o achieves the highest diacritic restoration accuracy among all evaluated models
- Models exhibit substantial performance variability, with some showing inconsistent results across different prompt templates
- Performance consistently exceeds a neutral echo baseline across all models, though with varying degrees of improvement
- Prompt engineering significantly impacts restoration accuracy, with complex multi-shot instructions generally outperforming simpler approaches

## Why This Works (Mechanism)
The effectiveness of large language models in diacritic restoration stems from their extensive training on multilingual text corpora, which includes exposure to diacritic patterns in various languages. Models with larger parameter counts and more diverse training data tend to capture the statistical relationships between context and diacritic usage more effectively. The success of complex multi-shot prompts suggests that LLMs can leverage few-shot examples to better understand the specific task requirements and apply learned patterns to new instances. The architecture's attention mechanisms allow models to consider broader contextual information when determining appropriate diacritic placement, which is particularly important in morphologically rich languages like Romanian where context heavily influences diacritic usage.

## Foundational Learning
- Diacritic restoration fundamentals: Why needed - understanding the linguistic challenge of restoring missing diacritics in text; Quick check - ability to identify common diacritic patterns in Romanian
- Prompt engineering techniques: Why needed - optimizing model performance through effective instruction design; Quick check - familiarity with zero-shot, one-shot, and few-shot prompting strategies
- Evaluation metrics for NLP tasks: Why needed - quantifying model performance in restoration accuracy; Quick check - knowledge of precision, recall, and F1-score applications
- Multilingual model training: Why needed - understanding how LLMs develop cross-linguistic capabilities; Quick check - awareness of tokenization and vocabulary considerations in multilingual models
- Romanian morphology and syntax: Why needed - grasping language-specific factors affecting diacritic placement; Quick check - recognition of Romanian grammatical cases and their diacritic implications

## Architecture Onboarding

Component map:
Input text -> Tokenizer -> Transformer layers -> Attention mechanisms -> Output layer -> Diacritic-restored text

Critical path:
Text preprocessing -> Token embedding -> Multi-head attention processing -> Feed-forward layers -> Output projection -> Diacritic prediction

Design tradeoffs:
- Model size vs. computational efficiency: Larger models generally perform better but require more resources
- Prompt complexity vs. response consistency: More detailed prompts may improve accuracy but can reduce output reliability
- Training data diversity vs. task specialization: Broader training may improve generalization but could dilute task-specific performance

Failure signatures:
- Inconsistent diacritic restoration across similar contexts
- Over-reliance on statistical patterns rather than semantic understanding
- Difficulty handling rare words or domain-specific terminology
- Performance degradation with longer input sequences

Three first experiments:
1. Compare zero-shot vs. one-shot vs. few-shot performance for each model
2. Test model performance on different Romanian text domains (news, literature, technical)
3. Evaluate impact of input length on restoration accuracy

## Open Questions the Paper Calls Out
None

## Limitations
- Limited evaluation to a select number of LLMs, potentially missing insights from other models
- Potential gaps in prompt engineering exploration that could affect performance
- Uncertainty about corpus representativeness across all Romanian writing styles and domains
- Lack of detailed baseline comparison methodology and performance metrics

## Confidence
High confidence:
- Comparative performance ranking of evaluated models
- General trend of GPT-4o outperforming other models

Medium confidence:
- Specific numerical accuracy values reported for each model
- Impact of model architecture and training data on performance

Low confidence:
- Effectiveness of prompt design across all evaluated models
- Potential for further improvements in less performant models

## Next Checks
1. Expand model evaluation to include newer versions and models from additional providers
2. Apply successful models and prompts to diacritic restoration in other languages for cross-linguistic validation
3. Conduct controlled fine-tuning experiments on Romanian diacritic-rich corpora to assess performance improvement potential