---
ver: rpa2
title: 'SimpleQA Verified: A Reliable Factuality Benchmark to Measure Parametric Knowledge'
arxiv_id: '2509.07968'
source_url: https://arxiv.org/abs/2509.07968
tags:
- answer
- simpleqa
- benchmark
- predicted
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "SimpleQA Verified is a rigorously curated benchmark of 1,000 short-form\
  \ questions designed to evaluate Large Language Models\u2019 parametric factuality.\
  \ It addresses shortcomings in OpenAI\u2019s SimpleQA, including incorrect labels,\
  \ topic biases, and redundancy, through a multi-stage process of de-duplication,\
  \ topic balancing, source reconciliation, and metadata enrichment."
---

# SimpleQA Verified: A Reliable Factuality Benchmark to Measure Parametric Knowledge

## Quick Facts
- arXiv ID: 2509.07968
- Source URL: https://arxiv.org/abs/2509.07968
- Authors: Lukas Haas; Gal Yona; Giovanni D'Antonio; Sasha Goldshtein; Dipanjan Das
- Reference count: 7
- Primary result: Gemini 2.5 Pro achieves 55.6 F1-score on 1,000-question benchmark

## Executive Summary
SimpleQA Verified addresses fundamental flaws in OpenAI's SimpleQA benchmark through a rigorous multi-stage cleaning process. The benchmark removes question redundancy, corrects incorrect labels, balances topical distribution, and filters out ambiguous or impossible questions. Using an improved autorater with explicit numeric answer ranges, the dataset provides a higher-fidelity tool for measuring genuine progress in factual recall and mitigating hallucinations. On this refined dataset, Gemini 2.5 Pro achieves state-of-the-art performance with a 55.6 F1-score, demonstrating the benchmark's ability to differentiate between frontier models' parametric knowledge capabilities.

## Method Summary
The benchmark starts with 4,326 SimpleQA questions and applies five filtering steps: unique URL filtering, semantic and lexical deduplication, robots.txt compliance filtering, topic balancing, and source reconciliation. The cleaned dataset of 1,000 questions is evaluated using GPT-4.1 as an autorater with modified prompts that include explicit acceptable ranges for numeric answers. Models are queried without external tools, and responses are graded as CORRECT, INCORRECT, or NOT_ATTEMPTED. The primary metric is F1-score, calculated as the harmonic mean of overall correct rate and correct rate given attempted answers.

## Key Results
- Gemini 2.5 Pro achieves 55.6 F1-score, outperforming other frontier models
- Benchmark successfully filters out incorrect SimpleQA labels, removing ~3.3% of questions with wrong ground truths
- Topic distribution balanced across 7 categories: Arts & Entertainment, Geography, History, Science, Sports, Technology, and Other
- Numeric answer evaluation improved through explicit acceptable ranges reducing formatting penalties

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Reducing question redundancy via semantic and lexical deduplication lowers the risk of models overfitting to specific question templates rather than acquiring general knowledge.
- **Mechanism:** By removing questions with high cosine similarity (using Gemini Embeddings and TF-IDF), the benchmark forces evaluation across a wider distribution of phrasings. This isolates the model's ability to retrieve facts from parameters rather than recognizing pattern artifacts from training data.
- **Core assumption:** High similarity scores correlate with "gimmie" questions that inflate performance metrics without reflecting true capability.
- **Evidence anchors:** [abstract] "addresses... question redundancy" and [section 2.2] "SimpleQA contains many highly similar questions... result of misaligned rating incentives."
- **Break condition:** If the deduplication threshold is too aggressive, it might remove legitimate distinct facts that happen to share similar linguistic structures.

### Mechanism 2
- **Claim:** Replacing rigid exact-match grading with explicit acceptable ranges for numeric answers improves the signal-to-noise ratio of the evaluation metric.
- **Mechanism:** The autorater prompt is modified to include specific error margins rather than relying on subjective interpretation of "significant figures." This allows the model to be graded on factual accuracy rather than formatting precision.
- **Core assumption:** A numeric answer within the defined tolerance range demonstrates successful parametric recall, even if not the exact ground truth integer.
- **Evidence anchors:** [abstract] "improved autorater with explicit numeric answer ranges" and [section 3.2] "remove the generic instruction... and instead explicitly specify the range... as part of the ground truth."
- **Break condition:** If the error taxonomy misclassifies a quantity requiring exact precision as one allowing a margin of error, the benchmark will falsely reward incorrect factual recall.

### Mechanism 3
- **Claim:** Rigorous source reconciliation filters out "impossible" questions where conflicting sources render the ground truth ambiguous, preventing models from being unfairly penalized for refusal.
- **Mechanism:** The authors use an ensemble of search-augmented models to identify questions where sources disagree. By removing or correcting these entries, they ensure that a "Correct" grade is actually achievable via parametric knowledge.
- **Core assumption:** Search-augmented models effectively approximate the availability and conflict state of knowledge on the open web.
- **Evidence anchors:** [abstract] "addresses... incorrect labels" and [section 2.5] "We remove questions... which are clearly ambiguous or where sources point to more than one distinct answer."
- **Break condition:** If the ensemble fails to retrieve a key source that contradicts the others, ambiguous questions may remain in the dataset.

## Foundational Learning

- **Concept: Parametric Factuality**
  - **Why needed here:** The benchmark explicitly isolates this from RAG (Retrieval Augmented Generation). You must understand that this measures what is "stored" in the model weights, not what it can look up.
  - **Quick check question:** If a model answers correctly using a tool/search, does it pass SimpleQA Verified? (Answer: No, tools are disabled; it tests internal knowledge).

- **Concept: Benchmark Saturation & Headroom**
  - **Why needed here:** The paper explains that older benchmarks became useless because models solved them. Understanding "hill-climbing" explains why the authors had to remove easy questions after cleaning the data.
  - **Quick check question:** Why did the authors adversarially select questions that frontier models got wrong after fixing the dataset labels?

- **Concept: F1-Score (Harmonic Mean)**
  - **Why needed here:** The paper uses a specific F1 score of "Overall Correct" and "Correct given Attempted." This balances raw accuracy against the model's tendency to hallucinate versus abstain.
  - **Quick check question:** If a model answers 1 question correctly and skips 999, what happens to the "Correct given Attempted" metric? (Answer: It is 100%, but the harmonic mean F1 would be low due to low "Overall Correct").

## Architecture Onboarding

- **Component map:** SimpleQA raw dataset (4,326 prompts) -> (1) Unique URL filter -> (2) Semantic/TF-IDF Deduper -> (3) Robots.txt filter -> (4) Topic Balancer -> (5) Source Reconciler -> 1,000 verified prompts
- **Critical path:** The **Adversarial Selection** is the most critical step. Without this, simply fixing "noisy" labels would make the benchmark too easy (inflating scores). The system depends on filtering for questions that *all* frontier models currently fail to maintain difficulty.
- **Design tradeoffs:**
  - **Size vs. Quality:** Reduced dataset by ~77% (4,326 to 1,000). Gains: higher fidelity/lower noise. Loss: Broader topic coverage and statistical significance for niche sub-domains.
  - **Specificity vs. Robustness:** Allowing numeric ranges improves robustness against minor rounding errors but introduces a small risk of accepting factually incorrect "ballpark" guesses.
- **Failure signatures:**
  - **High "Not Attempted" Rate:** Indicates a model is overly conservative, potentially refusing due to ambiguity detection or alignment constraints rather than lack of knowledge.
  - **High Accuracy / Low F1:** Indicates the model is accurate when it answers but refuses too often.
- **First 3 experiments:**
  1. **Baseline Validation:** Run your model on *original* SimpleQA vs. SimpleQA Verified to measure the "noise penalty" (how many errors were due to bad labels vs. model hallucination).
  2. **Ablation on Autorater:** Grade a subset of numeric answers using the old prompt (exact match) vs. the new prompt (ranges) to quantify the "formatting penalty" in your current evaluation stack.
  3. **Topic Skew Analysis:** Evaluate performance on the "Science" vs. "Geography" splits specifically to check if your model's parametric knowledge is unevenly distributed across the rebalanced topics.

## Open Questions the Paper Calls Out

- **Open Question 1:** To what extent does the reliance on GPT-4.1 as the automatic grader introduce a systematic bias against non-OpenAI models or specific answer phrasings?
- **Open Question 2:** How does the removal of web sources utilizing AI-specific crawling restrictions alter the topical distribution and temporal freshness of the benchmark?
- **Open Question 3:** Does the adversarial selection of "hard" questions result in a dataset that measures "tail" knowledge rather than general parametric reliability?

## Limitations
- The benchmark's quality hinges on the reliability of the multi-stage cleaning pipeline, which lacks quantitative error rates for each step
- The source reconciliation step relies on ensemble search-augmented models whose accuracy is not benchmarked against ground truth
- The adversarial selection process for maintaining difficulty is described qualitatively without specifying how many questions were removed for being "too easy"

## Confidence

- **High Confidence:** The benchmark successfully reduces question redundancy and improves topic balance. The removal of incorrect labels is strongly supported by the identification of specific SimpleQA errors.
- **Medium Confidence:** The improved autorater with explicit numeric ranges provides better evaluation signal. While the mechanism is well-described, the impact on final scores depends on the consistency of range assignments.
- **Medium Confidence:** The benchmark isolates parametric knowledge effectively. The cleaning process removes ambiguous and redundant questions, but the residual noise level and its impact on model differentiation is not quantified.
- **Low Confidence:** The benchmark's ability to maintain difficulty through adversarial selection. The paper describes the process but doesn't provide data on how many questions were removed for being too easy.

## Next Checks

1. **Autorater Consistency Validation:** Take 50 numeric questions and grade them using both the original autorater (exact match) and the improved autorater (with explicit ranges). Calculate the agreement rate and analyze systematic differences.

2. **Pipeline Error Rate Analysis:** For each cleaning step, manually review a stratified sample of 20 questions that were removed. Calculate precision and recall against human judgment to establish error rates for the entire pipeline.

3. **Difficulty Maintenance Verification:** Take the 100 questions removed during adversarial selection and evaluate them on a current frontier model. Calculate the correct rate to verify these questions are indeed too easy and confirm the remaining 1,000 questions provide sufficient headroom.