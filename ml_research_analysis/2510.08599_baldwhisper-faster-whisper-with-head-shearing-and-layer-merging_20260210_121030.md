---
ver: rpa2
title: 'BaldWhisper: Faster Whisper with Head Shearing and Layer Merging'
arxiv_id: '2510.08599'
source_url: https://arxiv.org/abs/2510.08599
tags:
- decoder
- layers
- merging
- whisper
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'BaldWhisper compresses Whisper for Bambara speech recognition
  using two-stage pruning: (1) layer merging via weighted averaging of adjacent decoder
  layers, and (2) low-rank embedding decomposition with feature distillation. This
  reduces model size by 48% and accelerates inference 2.15x on M1 MacBook Air while
  maintaining 90% of base performance, using only 32 hours of supervised Bambara data.'
---

# BaldWhisper: Faster Whisper with Head Shearing and Layer Merging

## Quick Facts
- arXiv ID: 2510.08599
- Source URL: https://arxiv.org/abs/2510.08599
- Reference count: 0
- Reduces Whisper model size by 48% while maintaining 90% of base performance

## Executive Summary
BaldWhisper presents a two-stage compression pipeline for efficient Bambara speech recognition using the Whisper architecture. The approach combines layer merging through weighted averaging of adjacent decoder layers with low-rank embedding decomposition enhanced by feature distillation. This results in a 48% reduction in model size and 2.15x inference speedup on M1 MacBook Air while retaining 90% of the base model's performance. The method is specifically validated on 32 hours of supervised Bambara data, demonstrating practical applicability for low-resource language scenarios.

## Method Summary
BaldWhisper employs a two-stage pruning strategy to compress Whisper models for low-resource languages. The first stage uses layer merging via weighted averaging (α, β parameters) of adjacent decoder layers, exploiting the observation that consecutive layers produce similar outputs. The second stage applies low-rank embedding decomposition to multilingual embeddings, followed by feature distillation to preserve performance. This approach is validated on Bambara speech recognition with 32 hours of supervised data, achieving significant compression while maintaining model accuracy.

## Key Results
- 48% reduction in model size compared to Whisper base
- 2.15x faster inference on M1 MacBook Air
- Maintains 90% of base model performance on Bambara speech recognition
- Requires only 32 hours of supervised Bambara data

## Why This Works (Mechanism)
The compression works by exploiting structural redundancies in the Whisper architecture. Layer merging targets adjacent decoder layers that produce similar outputs, allowing them to be combined without significant information loss. The low-rank decomposition reduces the dimensionality of multilingual embeddings while feature distillation preserves critical information during this compression. Together, these techniques maintain essential speech recognition capabilities while dramatically reducing computational requirements, making deployment feasible on resource-constrained devices.

## Foundational Learning

**Whisper Architecture** - Why needed: Understanding the base model structure is essential for knowing where compression can occur. Quick check: Whisper has 6 decoder layers in base variant.

**Layer Merging** - Why needed: This is the primary compression technique that reduces decoder depth. Quick check: Adjacent layers are merged using weighted average with parameters α and β.

**Low-Rank Decomposition** - Why needed: Reduces embedding matrix dimensionality while preserving key features. Quick check: Applied to multilingual embeddings after layer merging.

**Feature Distillation** - Why needed: Maintains performance during compression by transferring knowledge. Quick check: Used during low-rank decomposition to preserve model accuracy.

**Code-Switching** - Why needed: Bambara data contains French/English switching, affecting model design. Quick check: Model must handle multilingual input within single utterances.

**Parameter Sharing** - Why needed: Enables compression by identifying redundant parameters. Quick check: Shared weights across merged layers reduce total parameter count.

## Architecture Onboarding

**Component Map**: Input Audio -> Encoder -> Layer Merging (Decoder) -> Low-Rank Decomposition -> Feature Distillation -> Output Text

**Critical Path**: Audio preprocessing → Encoder feature extraction → Layer merging in decoder → Embedding decomposition → Text generation

**Design Tradeoffs**: Compression vs. accuracy (48% size reduction with 90% performance retention), computational efficiency vs. model complexity, language-specific optimization vs. multilingual generalization

**Failure Signatures**: Performance degradation when layer similarity assumption breaks, accuracy loss when low-rank decomposition removes critical information, instability when feature distillation fails to preserve knowledge

**First Experiments**: 1) Verify layer similarity patterns in decoder, 2) Test weighted averaging parameters on small dataset, 3) Validate low-rank decomposition preserves embedding semantics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would per-layer specialization of the merging parameters (α, β) yield better performance than using shared weights across all merged layer pairs?
- Basis in paper: [explicit] The conclusion states: "Several levers remain to further improve speed and performance, such as specializing α, β per layer and searching for their optimal values."
- Why unresolved: The authors used shared hyperparameters across all layer pairs, constrained by computational resources and time for hyperparameter search.
- What evidence would resolve it: Experiments comparing shared vs. per-layer α, β values, measuring WER on Bambara and computational overhead of the search.

### Open Question 2
- Question: Do alternative merging functions beyond simple weighted averaging improve compression-to-performance tradeoffs?
- Basis in paper: [explicit] The conclusion notes: "the merging function we used is a simple weighted average, but many new merging methods [6, 13] may be beneficial."
- Why unresolved: The authors chose the simplest approach based on the observation that adjacent layers have similar activations; exploring complex merging strategies was beyond scope.
- What evidence would resolve it: Comparative experiments with TIES-merging, Model Soups, or manifold alignment-based merging applied to Whisper decoder layers.

### Open Question 3
- Question: Does BaldWhisper generalize to other low-resource languages with different linguistic characteristics and code-switching patterns?
- Basis in paper: [inferred] The method was validated only on Bambara (32h), a Niger-Congo language with French/English code-switching. No experiments on other language families or code-switching scenarios were conducted.
- Why unresolved: The low-rank embedding decomposition assumes language-specific redundancy in multilingual embeddings; this assumption may not hold equally across languages.
- What evidence would resolve it: Application of the same compression pipeline to diverse low-resource languages (e.g., Wolof, Yoruba, Lao) with similar data constraints, reporting WER retention and speedup.

### Open Question 4
- Question: Does the approach scale effectively to larger Whisper variants (small, medium, large) while maintaining the 90% performance retention?
- Basis in paper: [inferred] All experiments used Whisper-base (73M parameters). Larger models have deeper decoders and different embedding-to-parameter ratios, which may affect merging and decomposition efficacy.
- Why unresolved: The activation similarity patterns observed in the 6-layer decoder of Whisper-base may not generalize to 24+ layer decoders in larger variants.
- What evidence would resolve it: Compression experiments on Whisper-small and Whisper-medium using the same two-stage approach, comparing parameter reduction rates and WER retention.

## Limitations

- Single language validation limits generalizability claims to other low-resource languages
- Small dataset size (32 hours) raises questions about robustness across diverse acoustic conditions
- Performance degradation metric lacks specificity about evaluation metrics beyond accuracy
- Layer merging assumption may not hold across different language families
- Low-rank decomposition may introduce language-specific biases affecting multilingual scalability

## Confidence

- **High confidence** in technical implementation of two-stage pruning pipeline
- **Medium confidence** in 2.15x speedup claim (hardware-specific to M1 MacBook Air)
- **Medium confidence** in 48% model size reduction
- **Low confidence** in 90% performance retention without knowing specific metrics

## Next Checks

1. Cross-language validation: Apply BaldWhisper to at least 3-5 additional low-resource languages from different language families
2. Larger dataset evaluation: Test compressed model on 100+ hours of data to verify performance stability
3. Ablation study: Conduct controlled experiments isolating impact of layer merging versus low-rank decomposition