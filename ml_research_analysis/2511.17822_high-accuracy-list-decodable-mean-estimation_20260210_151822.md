---
ver: rpa2
title: High-Accuracy List-Decodable Mean Estimation
arxiv_id: '2511.17822'
source_url: https://arxiv.org/abs/2511.17822
tags:
- will
- list
- proof
- learning
- theorem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of high-accuracy list-decodable
  learning, where the goal is to output a list of candidate solutions from which at
  least one has very small error compared to the ground truth, rather than just non-trivial
  error. The authors focus on the canonical setting of list-decodable mean estimation
  for identity-covariance Gaussians.
---

# High-Accuracy List-Decodable Mean Estimation

## Quick Facts
- arXiv ID: 2511.17822
- Source URL: https://arxiv.org/abs/2511.17822
- Reference count: 40
- Primary result: Establishes existence of finite lists for high-accuracy list-decodable mean estimation, then provides an efficient algorithm achieving this bound.

## Executive Summary
This paper addresses the fundamental problem of high-accuracy list-decodable learning for identity-covariance Gaussians. While prior work could only guarantee non-trivial error, this work demonstrates that with a list of size at most exp(O(log²(1/α)/ε²)), one can achieve ℓ₂ error at most ε. The authors provide both an information-theoretic proof of existence using Gaussian isoperimetry and a two-step algorithmic framework that first filters data to find a low-dimensional subspace, then exhaustively searches within it.

## Method Summary
The method consists of two main steps: First, an iterative filtering algorithm using high-degree Hermite polynomials identifies a low-dimensional subspace containing all candidate means by removing outliers based on their influence on high-order moments. Second, an exhaustive search over a net in this subspace finds moment-matching candidates, with deduplication ensuring the final list is ε-separated. The algorithm requires n = d^O(log L) + exp(exp(O(log L))) samples and runs in time d^O(log L) + exp(exp(O(log L))), where L is the list size.

## Key Results
- Establishes existence of list of size at most exp(O(log²(1/α)/ε²)) containing a candidate within ε of true mean
- Achieves ε = 0.01 error in regimes where both α and ε are small constants
- Provides first algorithm with both theoretical guarantees and polynomial-time complexity
- Demonstrates that high-accuracy list-decodable learning is possible beyond trivial error bounds

## Why This Works (Mechanism)

### Mechanism 1: Geometric Identifiability via Gaussian Isoperimetry
The authors use a geometric packing argument with Voronoi cells around candidate means. They prove that if there are too many ε-separated consistent means, the sum of probabilities Pr_{X~N(μ_i,I)}[X∈R_i] would exceed 1, violating mixture distribution properties. This relies on Gaussian isoperimetry to relate set volume to its "fattened" expansion. The core assumption is identity-covariance Gaussian inliers. Break condition: if data deviates significantly from Gaussian isoperimetric properties, the volume-expansion argument fails.

### Mechanism 2: Dimension Reduction via Hermite Polynomial Filtering
The algorithm iteratively filters samples based on high-degree Hermite polynomial tensor norms. Outliers distort high-order moments, so removing samples correlated with high-norm tensors preserves inliers while crushing outlier mass. This projects the problem into a low-dimensional subspace containing the true mean without requiring Sum-of-Squares hierarchies. Core assumption: sufficient samples n ≥ d^O(log L) to estimate high-order moments. Break condition: insufficient samples cause noisy empirical tensors that fail to identify the subspace.

### Mechanism 3: Fooling Intersections of Halfspaces
The algorithm searches for candidates that match low-order moments, ensuring the list isn't infinite by proving moment-matching candidates induce similar probability measures on Voronoi cells (intersections of halfspaces) as the true Gaussian. Core assumption: specific relations between list size L and moment-matching degree k. Break condition: insufficient moment degrees k for target list size L causes generalization error too high to prevent list explosion.

## Foundational Learning

- **List-Decodable Learning**: Problem setup where standard robust statistics (assuming >50% inliers) fail, requiring list output when α < 0.5. Quick check: If α = 0.01, why can't we just output the median of the data?

- **Hermite Polynomials (Probabilist's)**: Serve as "features" or "fingerprints" for Gaussian distribution in filtering step. Quick check: What is expectation of k-th Hermite polynomial tensor H_k(X) when X ~ N(μ,I)? (Answer: μ^⊗k).

- **Gaussian Isoperimetry**: Provides theoretical "volume" control proving small list size exists. Explains why Gaussian noise is "concentrated" and how "fattening" a set increases volume predictably. Quick check: Why is measure of β-fattened set A_β related to log(1/μ(A)) for Gaussians?

## Architecture Onboarding

- **Component map**: Rough Estimator -> Subspace Filter -> Net Searcher -> Sieve
- **Critical path**: The Subspace Filter is the algorithmic novelty. If this fails to remove outliers efficiently or projects μ out of the subspace, the subsequent exhaustive search will fail.
- **Design tradeoffs**: Accuracy vs. List Size (small ε improvements require massive L increases); Runtime vs. Sample Complexity (polynomial in d but large exponent depending on α, ε)
- **Failure signatures**: Inlier Leakage (filter too aggressive, removes valid inliers); Outlier Camouflage (outliers match low-order Hermite moments, pollute subspace)
- **First 3 experiments**: 1) Visualize the Filter on 2D synthetic data with α=0.2 inliers; 2) List Size Scaling - vary ε and measure resulting list size; 3) Stress Test Subspace - inject outliers with specific covariance structures

## Open Questions the Paper Calls Out

### Open Question 1
Can high-accuracy list-decodable mean estimation be extended to distributions with unknown covariance or bounded moments rather than just identity-covariance Gaussians? The theoretical results and algorithmic tools are explicitly restricted to identity-covariance Gaussians. Unresolved because identifiability arguments and filtering algorithms rely heavily on spectral properties of identity-covariance Gaussian distributions.

### Open Question 2
Is there an algorithm that achieves high-accuracy list-decodable estimation in time strictly polynomial in the list size L, removing the exp(exp(O(log L))) term? Theorem 1.4 establishes an algorithm with time complexity d^O(log L) + exp exp(O(log L)), which contains a double-exponential term. Unresolved because current algorithm relies on exhaustive net search driving the double-exponential complexity.

### Open Question 3
Can the high-accuracy framework be generalized to other list-decodable learning problems, such as linear regression or covariance estimation? Related work lists list-decodable linear regression, covariance estimation, and sparse mean estimation as areas with prior "low-accuracy" results. Unresolved because identifiability proof leverages specific geometry of Gaussian means (Voronoi cells) differing from parameter spaces of regression or covariance matrices.

## Limitations
- Requires identity-covariance Gaussian inliers; extension to other distributions remains open
- Sample complexity involves d^O(log L) term, which may be prohibitive in practice
- Constants in theoretical bounds (C, c) are generic and require empirical tuning

## Confidence

**High Confidence**: Geometric identifiability argument via Gaussian isoperimetry is mathematically rigorous and list size bound is well-established; core algorithmic framework (filter-then-search) is sound.

**Medium Confidence**: Hermite polynomial filtering step is theoretically justified but may face practical challenges; dimension reduction is theoretically guaranteed but practical performance uncertain.

**Low Confidence**: Exhaustive search step's efficiency in practice given potentially large subspace dimension is questionable; "fooling intersections of halfspaces" argument may not hold tightly with finite samples.

## Next Checks

1. **Scaling Experiment**: Systematically vary α and ε on synthetic data and measure actual list size produced, comparing against theoretical bound exp(O(log²(1/α)/ε²)).

2. **Robustness Test**: Evaluate algorithm's performance when inlier distribution deviates slightly from Gaussian (e.g., adding small skew or kurtosis). Measure how this affects list size and accuracy.

3. **Constant Sensitivity Analysis**: Perform ablation studies on filter thresholds (γ, λ) to determine impact on success rate and runtime, and attempt to derive practical guidelines for their selection.