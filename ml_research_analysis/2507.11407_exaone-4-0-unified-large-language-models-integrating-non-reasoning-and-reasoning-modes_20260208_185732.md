---
ver: rpa2
title: 'EXAONE 4.0: Unified Large Language Models Integrating Non-reasoning and Reasoning
  Modes'
arxiv_id: '2507.11407'
source_url: https://arxiv.org/abs/2507.11407
tags:
- exaone
- reasoning
- https
- performance
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: EXAONE 4.0 introduces unified large language models integrating
  non-reasoning and reasoning modes. The model adopts a hybrid attention mechanism
  combining global and local attention in a 3:1 ratio, extending context length to
  128K tokens.
---

# EXAONE 4.0: Unified Large Language Models Integrating Non-reasoning and Reasoning Modes

## Quick Facts
- arXiv ID: 2507.11407
- Source URL: https://arxiv.org/abs/2507.11407
- Reference count: 40
- Unified models integrating non-reasoning and reasoning modes with agentic tool use

## Executive Summary
EXAONE 4.0 introduces unified large language models that integrate both non-reasoning (fast response) and reasoning (deep chain-of-thought) modes within a single architecture. The models employ a hybrid attention mechanism combining global and local attention in a 3:1 ratio to extend context length to 128K tokens while maintaining computational efficiency. The system supports multilingual capabilities (English, Korean, Spanish) and includes agentic tool use functionalities, demonstrating superior performance compared to open-weight models in mathematical and coding domains while maintaining strong instruction-following capabilities.

## Method Summary
The EXAONE 4.0 architecture is a Transformer with hybrid attention (3:1 local:global ratio, 4K sliding window), QK-Reorder-LN normalization, and GQA heads. Pretraining uses 14T tokens (32B) with progressive context extension (4K→32K→128K) validated via NIAH testing. Post-training involves unified SFT mixing non-reasoning and reasoning data at 1.5:1 token ratio, RL with AGAPO algorithm (modified PPO without clipping, asymmetric sampling, group+global advantages), and two-stage preference learning (SimPER: Stage 1 correctness+conciseness, Stage 2 preference+language consistency). Model configurations include 32B (d_model=5120, 64 layers) and 1.2B (d_model=2048, 30 layers) with 128K and 64K max context respectively.

## Key Results
- Superior performance compared to open-weight models in its class, achieving competitive results against frontier models
- Extraordinary performance in mathematical and coding domains (MMLU-Pro, GPQA-Diamond, AIME 2025, LiveCodeBench)
- Agentic tool use performance reaches levels comparable to larger competing models
- Strong instruction-following capabilities and commendable performance in long-context tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Integrating local and global attention reduces computational cost for long sequences while maintaining performance.
- **Mechanism:** Hybrid attention uses local sliding window attention (4K window) in 3:1 ratio with global attention layers. Local attention limits computation to fixed window while periodic global layers maintain full context access, preventing long-range dependency loss.
- **Core assumption:** Small proportion of global attention sufficient to maintain global context understanding while most context can be processed locally.
- **Evidence anchors:**
  - [abstract] "The model adopts a hybrid attention mechanism combining global and local attention in a 3:1 ratio, extending context length to 128K tokens."
  - [Page 2, Section 2.1] "...the EXAONE 4.0 model employs a hybrid attention mechanism that combines local attention (sliding window attention type) with global attention in a 3:1 ratio..."
- **Break condition:** If global context retrieval degrades significantly for information between global layers, or if 3:1 ratio insufficient for complex multi-hop reasoning over long documents.

### Mechanism 2
- **Claim:** Unifying non-reasoning and reasoning modes in single model allows efficient task-adaptive inference.
- **Mechanism:** Model trained on combined non-reasoning (diverse tasks) and reasoning (math, code) data with 1.5:1 token ratio. At inference, user selects mode controlling generation style (CoT vs long CoT) and computational budget.
- **Core assumption:** Model can learn and switch between two distinct cognitive styles based on training data distribution and inference-time control.
- **Evidence anchors:**
  - [abstract] "...integrates a Non-reasoning mode and a Reasoning mode..."
  - [Page 5, Section 2.4.1] "Rather than fine-tuning the two modes sequentially, we combine both modes and train them together. The ratio between the two modes is determined by the amount of Reasoning mode data... Through ablation studies, we set the token ratio of REASONING to NON-REASONING data to 1.5:1."
- **Break condition:** If model conflates modes (enters deep reasoning for simple queries) or fails to maintain distinct performance profiles between modes.

### Mechanism 3
- **Claim:** New reinforcement learning algorithm (AGAPO) enhances reasoning by preserving exploratory tokens and leveraging negative feedback.
- **Mechanism:** AGAPO modifies GRPO by removing clipped objective (preventing low-probability "fork" tokens from being dropped), using asymmetric sampling (including all-incorrect samples for negative reinforcement), and applying two-stage global advantage normalization.
- **Core assumption:** Crucial reasoning steps often come from low-probability tokens, and learning from negative examples as important as positive ones.
- **Evidence anchors:**
  - [Page 6, Section 2.4.2] "AGAPO removes the clipping from PPO and instead uses a standard policy gradient loss... allowing for more substantial policy updates... AGAPO utilizes an asymmetric sampling method that does not discard samples where all responses are incorrect..."
  - [Page 11, Section 3.4] "EXAONE 4.0 models demonstrate extraordinary performance in Math/Coding benchmarks."
- **Break condition:** If removing clip leads to training instability or if advantage from all-incorrect samples doesn't provide useful learning signal for avoiding errors.

## Foundational Learning

- **Concept:** Sliding Window Attention
  - **Why needed here:** Critical for understanding hybrid architecture's efficiency gains and implications for context processing.
  - **Quick check question:** How does limiting attention to fixed window size reduce memory and compute complexity compared to global attention?

- **Concept:** Reinforcement Learning from Verifiable Rewards (RLVR)
  - **Why needed here:** AGAPO algorithm builds upon this paradigm. Understanding how rule-based verification provides reward signal for RL training is crucial for model's reasoning improvements.
  - **Quick check question:** How does verifiable reward differ from learned reward model in context of training language model?

- **Concept:** Preference Learning (DPO/SimPO)
  - **Why needed here:** Final post-training stage used to align model with human preferences and integrate two modes, crucial for understanding final model's behavior.
  - **Quick check question:** In preference learning, how does model use "chosen" and "rejected" response pairs to adjust policy without explicit reward model?

## Architecture Onboarding

- **Component map:** Hybrid Attention (3 Local:1 Global) -> QK-Reorder-LN -> GQA -> Unified SFT -> AGAPO RL -> Two-stage Preference Learner (SimPER)
- **Critical path:** Most critical path is understanding inference-time behavior controlled by unified training. Trace how prompt enters model and how selecting REASONING vs NON-REASONING mode alters generation logic (temperature, presence penalty, max tokens) and which trained parameters are more active.
- **Design tradeoffs:**
  - **Hybrid Attention (3:1):** Trades some potential long-range dependency precision for ~3-4x reduction in attention computation and memory, enabling 128K context. *Assumption:* Paper implies tradeoff is favorable, but validating requires benchmarks like RULER or HELMET.
  - **Unified Mode Training:** Trades simplicity of separate models for flexibility of one, but requires careful data balancing (1.5:1 ratio) to prevent mode collapse. Tradeoff is managing potential interference between two learning objectives.
  - **QK-Reorder-LN:** Trades increased computation (extra LayerNorm operations) for more stable and performant training in deep networks by controlling output variance.
- **Failure signatures:**
  - **Mode Collapse:** If non-reasoning queries trigger long, deliberative CoT, unified training has failed.
  - **Long-Context Forgetting:** Poor performance on "needle in a haystack" tasks may indicate hybrid attention ratio or sliding window size insufficient for global context retention.
  - **RL Instability:** AGAPO's removal of PPO clip could lead to large, destabilizing policy updates if KL penalties not carefully managed.
- **First 3 experiments:**
  1. **Ablation on Attention Ratio:** Retrain or fine-tune with 2:1 or 4:1 local-to-global attention ratio and evaluate on long-context benchmarks (RULER, HELMET) to quantify performance/efficiency tradeoff curve.
  2. **SFT Data Ratio Ablation:** Systematically vary REASONING:NON-REASONING token ratio (e.g., 1:1, 2:1) during unified SFT and measure impact on both mode-specific benchmark scores and mode-adherence accuracy.
  3. **Inference Mode Control Profiling:** Run benchmark set (e.g., MMLU-Pro, AIME) in both modes and profile key metrics: average tokens generated, latency, and accuracy. This creates performance profile to understand practical tradeoffs of each mode for users.

## Open Questions the Paper Calls Out

- **Question:** How does continued expansion of multilingual capabilities impact model's ability to maintain specific 1.5:1 reasoning-to-non-reasoning token balance required for stable mode switching?
  - **Basis in paper:** [explicit] Conclusion states, "As part of our future work, we aim to continuously strengthen usability by gradually expanding the supported languages."
  - **Why unresolved:** Paper notes maintaining specific token ratio (1.5:1) critical to preventing model from defaulting to Reasoning mode. Unclear if new languages will disrupt this delicate balance or require different ratios.
  - **What evidence would resolve it:** Ablation studies on unified mode training pipeline showing performance retention in English and Korean after adding fourth or fifth language.

## Limitations

- Training data sources, curation pipelines, and domain proportions remain proprietary, limiting independent verification
- Training hyperparameters across all stages (learning rates, batch sizes, KL coefficients, training steps) not disclosed
- Multilingual capabilities claimed but not extensively benchmarked across all three languages with comparable rigor

## Confidence

- **High Confidence:** Hybrid attention mechanism (3:1 local:global ratio with 4K sliding window) and its role in enabling 128K context length is well-specified and verifiable through architectural implementation.
- **Medium Confidence:** Unified training approach with 1.5:1 token ratio for mode integration is clearly described, but effectiveness depends heavily on undisclosed data characteristics.
- **Medium Confidence:** AGAPO RL algorithm modifications (removing PPO clipping, asymmetric sampling) are detailed, but impact on reasoning performance requires access to actual RL datasets and reward verification systems.
- **Low Confidence:** Multilingual capabilities (English, Korean, Spanish) are claimed but not extensively benchmarked across all three languages with comparable rigor.

## Next Checks

1. **Hybrid Attention Ablation:** Implement 3:1 local:global attention ratio and systematically vary it (2:1, 4:1) while measuring performance on long-context benchmarks (RULER, HELMET) and computational efficiency metrics to validate claimed tradeoff.

2. **Mode Separation Validation:** Design controlled experiments where same prompts evaluated in both reasoning and non-reasoning modes, measuring latency, token count, and accuracy to verify model maintains distinct cognitive styles as claimed.

3. **Long-Context Retention Test:** Create synthetic long-document tasks requiring information retrieval from non-adjacent sections (beyond 4K sliding window) to test whether 3:1 hybrid ratio with periodic global attention adequately preserves long-range dependencies.