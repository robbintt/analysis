---
ver: rpa2
title: 'Explanation Bias is a Product: Revealing the Hidden Lexical and Position Preferences
  in Post-Hoc Feature Attribution'
arxiv_id: '2512.11108'
source_url: https://arxiv.org/abs/2512.11108
tags:
- bias-agg
- bias-attr
- bias
- position
- lexical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a framework to evaluate lexical and position\
  \ biases in feature attribution methods for language models. The authors propose\
  \ three metrics\u2014Bias-cons (inter-seed consistency), Bias-agg (inter-model comparison),\
  \ and Bias-attr (inter-method comparison)\u2014to quantify different sources of\
  \ bias."
---

# Explanation Bias is a Product: Revealing the Hidden Lexical and Position Preferences in Post-Hoc Feature Attribution

## Quick Facts
- arXiv ID: 2512.11108
- Source URL: https://arxiv.org/abs/2512.11108
- Reference count: 40
- Key outcome: Introduces a framework to evaluate lexical and position biases in feature attribution methods using controlled experiments on artificial and natural language data.

## Executive Summary
This paper introduces a framework to evaluate lexical and position biases in feature attribution methods for language models. The authors propose three metrics—Bias-cons (inter-seed consistency), Bias-agg (inter-model comparison), and Bias-attr (inter-method comparison)—to quantify different sources of bias. Experiments are conducted on two BERT-based models using artificial datasets with controlled lexical/position information and a semi-controlled natural language task for causal relation detection.

Key findings include: (1) Lexical and position biases are not necessarily correlated—models high in one type may be low in the other; (2) ModernBERT does not consistently exhibit lower bias than traditional BERT; (3) Methods producing more divergent explanations (higher disagreement) tend to be more biased overall; (4) Classical faithfulness metrics (sufficiency and comprehensiveness) fail to detect bias in models that do not learn the task, suggesting limitations in their applicability.

## Method Summary
The authors evaluate lexical and position biases in feature attribution methods using three artificial datasets (noun-det-period, period-comma, unique-punctuation) with random labels and one semi-controlled natural dataset (causal relation detection). Two transformer models are fine-tuned: BERT-base and ModernBERT-base. Six attribution methods (PartSHAP, LIME, VanGrad, Grad×I, IntGrad, IntGrad×I) are applied to generate top-1 attributions. Bias is quantified using three Jensen-Shannon divergence-based metrics: Bias-cons (inter-seed consistency), Bias-agg (vs. uniform baseline), and Bias-attr (inter-method comparison). Sufficiency and comprehensiveness are used as classical faithfulness checks.

## Key Results
- Lexical and position biases are structurally unbalanced—models scoring high on one type score low on the other.
- ModernBERT does not consistently exhibit lower bias than traditional BERT; the tradeoff between lexical and position bias persists.
- Methods producing more divergent explanations (higher disagreement) tend to be more biased overall.
- Classical faithfulness metrics fail to detect bias in models that do not learn the task, highlighting their limitations.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Attribution methods that produce more divergent explanations from peers tend to be more biased overall.
- Mechanism: The Bias-attr metric computes pairwise Jensen-Shannon (JS) distance between a target method's attribution distribution and all other methods. High divergence correlates with high Bias-agg (distance from uniform baseline), suggesting outlier explanations are less trustworthy.
- Core assumption: A consensus among multiple attribution methods approximates ground truth; divergence signals method-specific bias rather than model-level behavior.
- Evidence anchors:
  - [abstract] "methods producing anomalous explanations are more likely to be biased themselves"
  - [section 3.5] "methods with a stronger bias compared to one another (Bias-attr) are also more biased overall compared to a uniform baseline (Bias-agg)"
  - [corpus] Related work on explanation disagreement (Krishna et al., 2022; Neely et al., 2022) supports method inconsistency as a known problem, though corpus papers focus on bias mitigation rather than divergence-utility relationships.
- Break condition: If multiple methods share the same systematic bias (e.g., all prefer early positions), consensus would not indicate faithfulness; the mechanism assumes bias sources are method-specific, not shared.

### Mechanism 2
- Claim: Lexical bias and position bias are structurally unbalanced—optimizing for one does not reduce the other.
- Mechanism: Bias-agg measurements across datasets show models scoring low on position bias often score high on lexical bias (and vice versa). BERT exhibits lower lexical bias but higher position bias than ModernBERT on artificial data; patterns invert across bias types.
- Core assumption: These two bias types stem from distinct architectural or pre-training factors rather than a single underlying cause.
- Evidence anchors:
  - [abstract] "lexical and position biases are structurally unbalanced...models that score high on one type score low on the other"
  - [section 3.5] "proneness to position bias does not seem to be related to lexical bias"
  - [corpus] Weak direct evidence; corpus papers address bias in explanations but do not examine the position/lexical tradeoff specifically.
- Break condition: If RoPE (ModernBERT's positional encoding) simultaneously reduces both bias types, the tradeoff would disappear; current evidence is limited to two model architectures.

### Mechanism 3
- Claim: Classical faithfulness metrics (sufficiency, comprehensiveness) fail to detect bias when models do not learn the task.
- Mechanism: On artificial datasets with random labels, models achieve chance-level F1 (0.33–0.55). Sufficiency and comprehensiveness both return ~0, correctly reflecting no learned signal but failing to flag biased attributions (e.g., VanGrad's strong position preference).
- Core assumption: Faithfulness metrics require meaningful model predictions to function; they measure prediction changes, not attribution distribution properties.
- Evidence anchors:
  - [section 3.4] "suff and cmp are not suitable to detect lexical and position bias in attribution methods for models that failed to learn the task (well)"
  - [section 3.5, Table 2] Both metrics return ~0 for all methods, including highly biased VanGrad
  - [corpus] Heyen et al. (2024) finding—cited in the paper—shows comprehensiveness can be unreliable even for trained models; corpus papers do not address this specific failure mode.
- Break condition: If sufficiency/comprehensiveness are reformulated to measure distributional properties rather than prediction deltas, they might detect bias in untrained models.

## Foundational Learning

- Concept: **Jensen-Shannon Divergence**
  - Why needed here: Core metric for all three bias measurements (Bias-cons, Bias-agg, Bias-attr); quantifies distance between probability distributions with bounded [0,1] output.
  - Quick check question: Given two attribution distributions P=[0.7, 0.2, 0.1] (biased toward first position) and Q=[0.33, 0.33, 0.34] (uniform), which has higher JS distance from uniform?

- Concept: **Feature Attribution Methods (Gradient-based vs. Perturbation-based)**
  - Why needed here: The paper compares six methods across two families; understanding their mechanics explains why VanGrad (gradient-based) shows stronger bias than LIME (perturbation-based).
  - Quick check question: Would you expect a perturbation-based method (masking tokens) to show more or less position bias than a gradient-based method that computes ∂output/∂input?

- Concept: **Positional Embeddings (Learned vs. RoPE)**
  - Why needed here: BERT uses learned absolute position embeddings; ModernBERT uses Rotary Positional Embeddings. The paper speculates RoPE may reduce position bias by encoding relative positions.
  - Quick check question: If RoPE encodes position via rotation in embedding space rather than absolute vectors, why might this reduce attention to specific token indices?

## Architecture Onboarding

- Component map:
  - **Data pipeline**: Three artificial datasets (noun-det-period, period-comma, unique-punctuation) + one natural (causal relation detection); all use top-1 attribution selection
  - **Model layer**: BERT-base (110M, learned position embeddings) and ModernBERT-base (149M, RoPE, larger pre-training corpus)
  - **Attribution layer**: Six methods via Ferret framework—PartSHAP, LIME, VanGrad, Grad×I, IntGrad, IntGrad×I
  - **Evaluation layer**: Three JS-based metrics (consistency, aggregate, inter-method) + two faithfulness metrics (sufficiency, comprehensiveness)

- Critical path:
  1. Fine-tune model on dataset (5 epochs, batch size 8, LR 5e-6)
  2. Generate top-1 attributions for each input token (excluding [CLS]/[SEP])
  3. Build frequency distributions over positions (0–19) and lexical elements
  4. Compute JS distances against baseline (uniform) and peer methods
  5. Correlate Bias-attr (divergence from peers) with Bias-agg (divergence from baseline)

- Design tradeoffs:
  - **Top-k=1 vs. top-k>1**: Paper uses k=1 for fair comparison with causal task (single causal signal), but acknowledges weaker signals in top-5 would increase disagreement and potentially amplify observable bias
  - **Artificial vs. natural evaluation**: Artificial data isolates bias from task confounds; natural data tests ecological validity but requires different baselines (token distributions are non-uniform)
  - **Seed count**: 10 random seeds provide consistency estimates; fewer seeds reduce Bias-cons reliability

- Failure signatures:
  - **High Bias-attr + High Bias-agg**: Method is both divergent from peers and far from uniform—strong candidate for rejection (e.g., VanGrad in 5/6 artificial scenarios)
  - **Near-zero sufficiency/comprehensiveness on low-F1 model**: Metrics correctly indicate no learned signal but provide no signal about attribution quality
  - **Position bias toward last tokens in BERT**: Consistent pattern across artificial and causal tasks (Figures 4, 5–7), potentially linked to learned absolute embeddings

- First 3 experiments:
  1. **Replicate Bias-attr/Bias-agg correlation on your target model/method**: Compute both metrics; if high Bias-attr methods also show high Bias-agg, the divergence signal is actionable for your setting.
  2. **Test lexical vs. position bias tradeoff**: Measure both Bias-agg scores for your model; if one is low, do not assume the other is also low—interventions may need to target each separately.
  3. **Sanity check with sufficiency/comprehensiveness on untrained model**: If your model achieves <55% F1, expect faithfulness metrics near zero; use Bias-agg instead to detect attribution pathologies.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does considering attribution tokens beyond the strongest signal (e.g., top-5 vs. top-1) amplify or mitigate the observed disagreement and bias among feature attribution methods?
- Basis in paper: [explicit] The authors state in the conclusion: "Another direction is to explore the attribution preference for contextual tokens beyond the strongest signal (e.g. top-5\top-1)."
- Why unresolved: The study restricted its analysis strictly to top-1 attributions to ensure comparability across tasks.
- What evidence would resolve it: Replicating the bias framework experiments using $k>1$ to observe if disagreement increases among lower-ranked contextual tokens.

### Open Question 2
- Question: How reliably do classical faithfulness metrics (sufficiency and comprehensiveness) reflect explanation quality in models that have only partially learned a task?
- Basis in paper: [explicit] The authors note that current metrics failed on unlearned tasks and "encourage further research on models that only partially learn a task."
- Why unresolved: The experiments only covered two extremes: models with chance-level performance (artificial data) and models with near-perfect performance (causal data).
- What evidence would resolve it: Experiments on datasets designed to induce partial learning (e.g., intermediate F1-scores) to test metric behavior.

### Open Question 3
- Question: Do specific architectural components, such as Rotary Positional Embeddings (RoPE), drive the trade-off where a model exhibits lower position bias but higher lexical bias?
- Basis in paper: [inferred] The paper speculates that RoPE might reduce position bias, but results show ModernBERT (using RoPE) scores high on lexical bias while BERT scores high on position bias, suggesting a structural trade-off.
- Why unresolved: The study compared only two model types, making it impossible to isolate whether the RoPE architecture specifically caused the inverse relationship between bias types.
- What evidence would resolve it: An ablation study comparing models with identical backbones but swapped positional embedding mechanisms.

### Open Question 4
- Question: To what extent do the observed correlations between high explanation disagreement and high bias generalize to decoder-only Large Language Models (LLMs)?
- Basis in paper: [inferred] The limitations section notes the risk that results may not generalize to other XAI methods or architectures outside the tested BERT-based encoders.
- Why unresolved: The experiments were confined to encoder-only transformers (BERT and ModernBERT) utilizing specific gradient and perturbation methods.
- What evidence would resolve it: Applying the Bias-cons, Bias-agg, and Bias-attr framework to decoder-only models (e.g., Llama, GPT) on similar controlled tasks.

## Limitations

- The tradeoff between lexical and position bias is observed only across two model architectures (BERT and ModernBERT). While statistically consistent in the paper's experiments, the generality of this structural imbalance to other transformer variants or larger models remains untested.
- The mechanism linking explanation divergence to bias assumes that method-level disagreements are independent; if multiple methods share the same bias source (e.g., positional encoding), the consensus-based assumption fails.
- The failure of classical faithfulness metrics is demonstrated only in the context of untrained models on artificial data; the behavior under partially learned or adversarial tasks is unclear.
- Attribution method hyperparameters (e.g., perturbation counts for LIME/SHAP) are not specified, which could influence bias magnitude.

## Confidence

- **High confidence**: The Bias-attr/Bias-agg correlation (divergence signals bias) is directly computed and visually clear in results; the pattern is robust across datasets.
- **Medium confidence**: The lexical vs. position bias structural imbalance is observed but based on only two models; the proposed mechanism is plausible but not exhaustively tested.
- **Medium confidence**: The failure of sufficiency/comprehensiveness on untrained models is directly demonstrated; however, the scope of conditions under which this occurs is narrow.

## Next Checks

1. **Replicate the lexical/position bias tradeoff on a third model** (e.g., RoBERTa or DeBERTa) to test whether the imbalance generalizes beyond BERT vs. ModernBERT.
2. **Test faithfulness metrics on a partially trained model** (F1 between 0.5 and 0.9) to determine if the failure mode is binary (untrained) or gradual.
3. **Vary attribution method hyperparameters** (e.g., increase SHAP samples, change LIME kernel width) to assess whether observed bias levels are method-stable or hyperparameter-sensitive.