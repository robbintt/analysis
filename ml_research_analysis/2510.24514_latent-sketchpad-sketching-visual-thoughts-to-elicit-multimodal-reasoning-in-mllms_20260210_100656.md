---
ver: rpa2
title: 'Latent Sketchpad: Sketching Visual Thoughts to Elicit Multimodal Reasoning
  in MLLMs'
arxiv_id: '2510.24514'
source_url: https://arxiv.org/abs/2510.24514
tags:
- visual
- reasoning
- latent
- sketchpad
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Latent Sketchpad addresses the challenge of multimodal reasoning
  in MLLMs, which struggle with complex tasks requiring visual planning and imagination.
  Inspired by human sketching for problem-solving, it introduces a framework that
  equips MLLMs with an internal visual scratchpad, enabling them to interleave textual
  reasoning with the generation of visual latents.
---

# Latent Sketchpad: Sketching Visual Thoughts to Elicit Multimodal Reasoning in MLLMs

## Quick Facts
- arXiv ID: 2510.24514
- Source URL: https://arxiv.org/abs/2510.24514
- Reference count: 40
- Primary result: Latent Sketchpad enables MLLMs to interleave textual reasoning with visual latent generation, achieving comparable or superior maze navigation performance while providing interpretable visual traces.

## Executive Summary
Latent Sketchpad addresses the challenge of multimodal reasoning in large language models by introducing an internal visual scratchpad mechanism. Inspired by human sketching during problem-solving, it equips MLLMs with the ability to generate context-aware visual latents alongside textual reasoning traces. The framework uses a Context-Aware Vision Head that autoregressively produces visual representations by attending to both global and local context, and a pretrained Sketch Decoder that renders these latents into interpretable sketches. Experiments on a new MazePlanning dataset demonstrate that this approach delivers reasoning performance comparable to or better than backbone models while providing valuable visual interpretability.

## Method Summary
Latent Sketchpad introduces a plug-and-play Vision Head that enables autoregressive visual latent generation within MLLMs. The Vision Head uses cross-attention to retrieve visual cues from previously generated segments while causal self-attention ensures coherence within current images. Visual latents are projected back into the language embedding space for continued generation. A Sketch Decoder pretrained on Quick, Draw! maps these latents to pixel-space sketches via a VAE latent space. The framework employs Latent Reconstruction Augmentation to improve robustness by introducing appearance variability while preserving spatial semantics. Training uses L1 regression loss with frozen backbone parameters, and connector adaptation is critical for maintaining spatial understanding.

## Key Results
- Achieves Success Rate of 72.20% on 5×5 mazes with Gemma3-12B, comparable to or better than text-only baselines
- Provides interpretable visual reasoning traces that enhance OOD generalization, particularly for Gemma3
- Visual Success Rate of 75.60% demonstrates effective path planning through generated sketches
- Ablation studies confirm importance of frozen connector adaptation and latent reconstruction augmentation

## Why This Works (Mechanism)

### Mechanism 1: Context-Aware Visual Latent Generation
The Vision Head enables autoregressive generation of visual latents by attending to both global context (all preceding images) and local context (partial latents within current image), maintaining visual coherence across reasoning steps. Cross-attention retrieves visual cues from previously generated segments while causal self-attention ensures coherence within the current image. The resulting context-enriched latent is projected back into the language embedding space for continued generation. Core assumption: Visual reasoning benefits from explicit short-term and long-term visual memory, similar to how humans maintain mental imagery during problem-solving.

### Mechanism 2: Frozen-Backbone Visual Head Training
The Vision Head can be trained independently via regression loss while keeping all MLLM parameters frozen, preserving original reasoning capacity while adding visual generation. A latent-level regression loss (L1 distance between predicted and target visual features) supervises the Vision Head without altering the pretrained backbone's weights. Core assumption: Pretrained MLLM visual features contain sufficient structure to be repurposed for generation without full model fine-tuning.

### Mechanism 3: Latent Reconstruction Augmentation for Robustness
Repeatedly reconstructing input visual thoughts through the Sketch Decoder before encoding introduces appearance variability while preserving spatial semantics, improving robustness. Each training image undergoes k∈[0,3] reconstruction rounds, generating semantically equivalent but pixel-perturbed views. This encourages focus on stable spatial structures rather than surface details. Core assumption: Visual reasoning depends more on spatial layout than pixel-perfect fidelity.

## Foundational Learning

- **Autoregressive token generation with mixed modalities**: The framework interleaves text and visual latent generation within a single autoregressive loop. Understanding how next-token prediction extends to continuous latents is essential.
  - Quick check: Can you explain how a model predicts the next token given hidden states, and how this extends to predicting continuous vectors instead of discrete tokens?

- **Vision Transformer (ViT) feature extraction and connectors**: MLLMs encode images through ViT-based encoders and project features via connectors into the LLM embedding space. The Vision Head operates on these representations.
  - Quick check: Given an image, what does a ViT output (shape, semantics), and how does a connector transform it for the LLM?

- **VAE latent spaces and reconstruction**: The Sketch Decoder maps visual latents into a VAE latent space for pixel-space rendering. Understanding VAE encoder/decoder dynamics is necessary for debugging visualization quality.
  - Quick check: What is the relationship between a VAE's encoder posterior q(z|x) and the decoder's reconstruction, and why might aligning ViT features to this space require a learnable adapter?

## Architecture Onboarding

- **Component map**: MLLM Backbone -> Context-Aware Vision Head -> Sketch Decoder -> Pixel-space sketch
- **Critical path**: Input image encoded by vision encoder → visual tokens → connector → LLM embedding space → during generation, <start_of_image> triggers Vision Head → produces visual latents autoregressively → latents projected back to embedding space → optionally decoded to sketches via Sketch Decoder
- **Design tradeoffs**: L1 vs. cosine loss for regression (L1 outperforms); latent vs. discrete visual tokens (continuous latents avoid quantization artifacts); frozen vs. adapted connector (freezing severely impairs spatial understanding)
- **Failure signatures**: Spatial hallucinations (generated paths cut through walls or teleport); layout drift (maze structure changes mid-reasoning); OOD degradation (performance drops sharply on 6×6 mazes)
- **First 3 experiments**: 1) Reproduce Vision Head training on small MazePlanning subset with frozen backbone; 2) Ablate global vs. local context and measure Success Rate degradation; 3) Test Sketch Decoder zero-shot reconstruction on held-out Quick, Draw! categories

## Open Questions the Paper Calls Out
1. Can Latent Sketchpad generalize to broader spatial reasoning and visual planning tasks beyond maze navigation?
2. What is the minimal visual fidelity required for sketches to effectively support reasoning, and does higher fidelity yield diminishing returns?
3. Why does Qwen2.5-VL fail to benefit from Latent Sketchpad in OOD settings while Gemma3 shows improved robustness?
4. To what extent does the plug-and-play Vision Head preserve reasoning capabilities when transferred across MLLM families without fine-tuning?

## Limitations
- Dependency on frozen connector adaptation is critical - failure to unfreeze the connector during downstream fine-tuning causes severe spatial confusion
- OOD generalization is limited, particularly for Qwen2.5-VL which has 4× larger visual features and requires more training data
- Sketch Decoder's AlignerNet architecture details are not fully specified, and exact procedure for extracting target visual latents remains ambiguous

## Confidence
- **High Confidence**: Core mechanism of Context-Aware Vision Head generation and autoregressive integration with text reasoning
- **Medium Confidence**: Latent Reconstruction Augmentation improves robustness as claimed
- **Low Confidence**: Sketch Decoder's generalization capabilities to unseen sketch domains and exact architectural details of AlignerNet

## Next Checks
1. Cross-dataset generalization test: Evaluate Sketch Decoder on held-out Quick, Draw! categories not seen during training
2. Connector adaptation ablation: Systematically test fine-tuning with frozen vs. unfrozen connectors across multiple maze sizes
3. Latent space probing: Analyze distribution of generated visual latents using t-SNE to verify they capture meaningful spatial relationships