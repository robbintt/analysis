---
ver: rpa2
title: 'RoRecomp: Enhancing Reasoning Efficiency via Rollout Response Recomposition
  in Reinforcement Learning'
arxiv_id: '2509.25958'
source_url: https://arxiv.org/abs/2509.25958
tags:
- rorecomp
- reasoning
- length
- arxiv
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of excessive verbosity in reinforcement
  learning with verifiable rewards (RLVR), where outcome-only rewards lead to unnecessarily
  long reasoning processes and inefficient exploration trajectories. The core method,
  Rollout Response Recomposition (RoRecomp), strategically recomposes training data
  by separating responses into priority batches (short-correct and long-incorrect
  responses) and compensation batches (remaining responses from a replay buffer) to
  provide clearer optimization signals for efficient reasoning.
---

# RoRecomp: Enhancing Reasoning Efficiency via Rollout Response Recomposition in Reinforcement Learning

## Quick Facts
- arXiv ID: 2509.25958
- Source URL: https://arxiv.org/abs/2509.25958
- Reference count: 40
- The paper reduces reasoning length by 27.7% in zero RL training while maintaining accuracy

## Executive Summary
RoRecomp addresses excessive verbosity in reinforcement learning with verifiable rewards by strategically recomposing training data. The method separates responses into priority batches (short-correct and long-incorrect responses) and compensation batches (remaining responses from replay buffer) to provide clearer optimization signals for efficient reasoning. Experimental results demonstrate substantial efficiency gains across three settings: zero RL, agentic RL, and thinking compression.

## Method Summary
RoRecomp re-composes rollout responses into priority batches (top-α shortest correct + top-α longest incorrect responses) and compensation batches (remaining responses from FIFO replay buffer). The method uses a cosine decay schedule for compensation batch probability (p_comp = max(0.2, (1+cos(π·t/T_max))/2)) to balance efficiency gains with stability. The approach is designed to fix high-variance baseline estimation in GRPO by amplifying positive advantages for concise reasoning and reinforcing negative advantages for verbose errors.

## Key Results
- Reduces reasoning length by 27.7% in zero RL training while maintaining accuracy
- Cuts tool calls by 46.8% while improving accuracy in agentic RL
- Achieves up to 52.5% length reduction in thinking compression with minimal performance impact

## Why This Works (Mechanism)

### Mechanism 1: Contrastive Gradient Concentration via Priority Batching
By filtering out ambiguous "middle-length" responses, RoRecomp reduces variance in advantage estimation. It creates a biased estimator that amplifies positive advantages for concise reasoning and reinforces negative advantages for verbose errors. The primary obstacle to concise reasoning in RLVR is high-variance gradient signals caused by mixed response lengths in small rollout groups.

### Mechanism 2: Stability Preservation via Replay Regularization
Exclusively training on prioritized efficiency data leads to model collapse; stability requires interleaving these priority batches with a "compensation batch" drawn from a replay buffer. This re-introduces the standard response distribution (intermediate lengths) to ensure the model maintains general reasoning capabilities while the priority batches push for efficiency.

### Mechanism 3: Implicit Curriculum via Dynamic Schedule
Gradually reducing the frequency of compensation batches allows the model to converge to a stable efficiency optimum better than a fixed schedule. A cosine decay schedule for the compensation probability ($p_{comp}$) implements curriculum learning, starting with stability-focused updates and shifting toward efficiency-focused updates as the reward stabilizes.

## Foundational Learning

- **GRPO (Group Relative Policy Optimization)**: Used as the baseline method; RoRecomp fixes the high-variance baseline estimation inherent in GRPO, which uses group mean rewards instead of a critic.
  - *Quick check question*: How does using the group mean as a baseline introduce noise when response lengths vary significantly within a single rollout group?

- **Reward Sparsity & Credit Assignment**: The paper addresses outcome-only rewards (0 or 1). Understanding why intermediate steps don't get explicit credit is crucial to seeing why the "data distribution" must be manipulated to guide the model.
  - *Quick check question*: Why does an outcome-based reward of 1.0 for a 1000-token response and a 100-token response result in the same raw reward, but different optimization signals after RoRecomp?

- **Experience Replay**: The "Compensation Batch" relies on a replay buffer.
  - *Quick check question*: In RoRecomp, what specific subset of data is pushed to the replay buffer, and why is it excluded from the priority batch?

## Architecture Onboarding

- **Component map**: Rollout Engine -> Verifier -> RoRecomp Selector -> Buffer -> Batch Composer -> Optimizer
- **Critical path**: The logic in Step 3 (RoRecomp Selector). The system must strictly separate the top α shortest correct and top α longest incorrect responses. If this separation logic fails and "long-correct" responses enter the priority batch, the efficiency signal dilutes immediately.
- **Design tradeoffs**:
  - Selection Ratio (α): High α (e.g., 0.9) includes more data (stable but less compression); Low α (e.g., 0.5) maximizes compression but risks instability
  - Compensation Schedule: A fixed high rate prevents collapse but limits length reduction; a fast decay maximizes reduction but risks "forgetting" complex reasoning patterns
- **Failure signatures**:
  - Runaway Verbosity: The model continues to increase length → Priority batch construction is likely including "long-correct" samples
  - Sudden Accuracy Collapse: Length drops near zero, accuracy tanks → Compensation batch probability decayed too fast or buffer size is too small
- **First 3 experiments**:
  1. Baseline Noise Profile: Run standard GRPO on a math benchmark; plot response length vs. accuracy to confirm the "length drift" problem exists
  2. Ablation on α: Sweep α ∈ {0.5, 0.7, 0.8, 0.9} on a small dataset to find the "edge of collapse" for your specific base model
  3. Buffer Validity: Disable the compensation batch entirely; confirm that while length drops drastically, accuracy becomes unstable

## Open Questions the Paper Calls Out
None

## Limitations
- The method assumes "middle-length" responses represent noise rather than necessary exploration paths, which may not hold for all reasoning domains
- The GRPO normalization modification referenced from [7] is critical but not fully specified in the paper
- The compensation batch schedule requires careful tuning per model/curriculum and can lead to stability collapse if aggressive

## Confidence
- **High Confidence**: The core efficiency gains (27.7% length reduction in zero RL, 46.8% tool call reduction in agentic RL) are well-supported by ablation studies
- **Medium Confidence**: The mechanism explaining how priority batching concentrates gradient signals is theoretically sound but relies on empirical assumptions
- **Medium Confidence**: The stability preservation claim through compensation batches is supported by ablation showing accuracy drops from 48% to 42%

## Next Checks
1. **GRPO Normalization Implementation**: Verify the exact modification from reference [7] that "mitigates inherent length bias" - this appears to be a critical prerequisite
2. **Ablation on Compensation Schedule**: Systematically sweep the cosine decay parameters to identify the stability-efficiency frontier for your specific model and task
3. **Response Distribution Analysis**: Before applying RoRecomp, analyze the baseline GRPO response length distribution to confirm the "length drift" problem exists in your setup