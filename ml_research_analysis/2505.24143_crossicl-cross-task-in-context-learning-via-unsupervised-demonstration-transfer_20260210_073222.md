---
ver: rpa2
title: 'CrossICL: Cross-Task In-Context Learning via Unsupervised Demonstration Transfer'
arxiv_id: '2505.24143'
source_url: https://arxiv.org/abs/2505.24143
tags:
- task
- answer
- source
- query
- final
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'CrossICL introduces a new paradigm of unsupervised cross-task
  transfer for In-Context Learning, enabling effective use of existing source task
  demonstrations for new target tasks without manual effort. It mitigates cross-task
  interference through a two-stage alignment strategy: Minimum Gap Selection identifies
  demonstrations most similar to the target query, followed by Progressive Task Adaptation
  that transforms and refines these demonstrations for alignment.'
---

# CrossICL: Cross-Task In-Context Learning via Unsupervised Demonstration Transfer

## Quick Facts
- arXiv ID: 2505.24143
- Source URL: https://arxiv.org/abs/2505.24143
- Reference count: 26
- Primary result: CrossICL achieves up to 7.6% improvement in ROUGE-L scores on average across 875 NLP tasks and six LLMs

## Executive Summary
CrossICL introduces a novel unsupervised cross-task transfer approach for In-Context Learning that enables effective use of existing source task demonstrations for new target tasks without manual intervention. The method addresses the critical challenge of cross-task interference by employing a two-stage alignment strategy that identifies and transforms relevant demonstrations. Through Minimum Gap Selection and Progressive Task Adaptation, CrossICL systematically reduces interference while maintaining the utility of source demonstrations. The approach demonstrates consistent performance improvements over zero-shot and Self-ICL baselines across a diverse range of NLP tasks and large language models.

## Method Summary
CrossICL employs a two-stage alignment strategy to transfer demonstrations from source tasks to target tasks without manual supervision. The first stage, Minimum Gap Selection, uses unsupervised similarity metrics to identify source task demonstrations most similar to the target query, mitigating initial cross-task interference. The second stage, Progressive Task Adaptation, iteratively transforms and refines these selected demonstrations to better align with the target task's requirements. This approach enables effective cross-task transfer by systematically reducing semantic and structural mismatches between source demonstrations and target queries while preserving the valuable learning signals embedded in the source demonstrations.

## Key Results
- CrossICL achieves up to 7.6% improvement in ROUGE-L scores on average across 875 NLP tasks
- Consistently outperforms zero-shot and Self-ICL baselines across six different LLMs including GPT-4o
- Performance improves with up to 10 demonstrations before declining due to information overload

## Why This Works (Mechanism)
CrossICL's effectiveness stems from its systematic approach to handling cross-task interference through task-aware demonstration selection and adaptation. The Minimum Gap Selection stage identifies demonstrations with minimal semantic and structural differences from the target query, reducing initial interference. The Progressive Task Adaptation stage then iteratively refines these demonstrations through transformation operations that align them more closely with the target task's requirements. This two-stage process creates a pathway for knowledge transfer that preserves useful information from source tasks while minimizing harmful interference, enabling LLMs to benefit from demonstrations even when they come from different but related tasks.

## Foundational Learning
- In-Context Learning (ICL): Understanding how LLMs learn from demonstrations without parameter updates is crucial for recognizing why demonstration quality and relevance matter so significantly for performance
- Cross-task Interference: Knowledge of how demonstrations from different tasks can conflict or interfere with target task learning explains the core challenge CrossICL addresses
- Task Similarity Metrics: Understanding how to measure semantic and structural similarity between tasks is essential for grasping the Minimum Gap Selection mechanism
- Demonstration Transformation: Familiarity with how demonstrations can be modified or adapted while preserving their instructional value helps explain the Progressive Task Adaptation stage
- ROUGE-L Evaluation: Understanding this text generation evaluation metric is important for interpreting the reported performance improvements
- Information Overload: Knowledge of how excessive demonstrations can degrade performance explains the observed performance degradation beyond 10 demonstrations

## Architecture Onboarding
Component map: Query -> Minimum Gap Selection -> Progressive Task Adaptation -> LLM Input
Critical path: Source Task Demonstrations → Similarity Scoring → Demonstration Selection → Demonstration Transformation → Prompt Construction → LLM Inference
Design tradeoffs: CrossICL trades computational overhead for improved performance by adding selection and adaptation stages, versus simpler but less effective zero-shot approaches
Failure signatures: Poor Minimum Gap Selection can propagate errors through adaptation; excessive demonstrations can cause information overload; task similarity metrics may miss subtle semantic differences
First experiments: 1) Test with varying numbers of source task demonstrations to find optimal range; 2) Compare performance across different unsupervised similarity metrics; 3) Evaluate adaptation effectiveness with different transformation strategies

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on unsupervised similarity metrics that may not fully capture task relevance, particularly for tasks with subtle semantic differences
- Effectiveness depends heavily on the quality of the Minimum Gap Selection step, with poor initial selection potentially propagating errors
- Evaluation focuses primarily on ROUGE-L scores for text generation tasks, which may not generalize to other NLP tasks or evaluation metrics
- Performance degradation beyond 10 demonstrations suggests a narrow optimal range that may not scale well to more complex tasks

## Confidence
High confidence: CrossICL's consistent improvement over baselines across 875 tasks and six LLMs demonstrates robust empirical performance that is well-supported by the experimental results.

Medium confidence: The effectiveness of the two-stage alignment strategy is supported by the results, but the paper could provide more detailed analysis of why this specific approach outperforms alternatives. The characterization of seven types of cross-task interference is based on analysis but could benefit from more rigorous validation.

Low confidence: The scalability of CrossICL to tasks beyond the evaluated scope and the generalization to evaluation metrics beyond ROUGE-L remain uncertain. The paper's claims about effectiveness with less similar source tasks need broader validation.

## Next Checks
1. Test CrossICL on non-generation NLP tasks (classification, question answering) using task-specific metrics to evaluate generalizability beyond text generation
2. Conduct ablation studies on the Minimum Gap Selection and Progressive Task Adaptation stages separately to quantify their individual contributions to performance gains
3. Evaluate CrossICL's performance with demonstrations from multiple source tasks simultaneously to test whether the method can effectively combine diverse task demonstrations