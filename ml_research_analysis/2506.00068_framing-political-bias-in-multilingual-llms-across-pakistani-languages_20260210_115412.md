---
ver: rpa2
title: Framing Political Bias in Multilingual LLMs Across Pakistani Languages
arxiv_id: '2506.00068'
source_url: https://arxiv.org/abs/2506.00068
tags:
- political
- bias
- language
- stance
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study presents the first large-scale evaluation of political
  bias in multilingual large language models (LLMs) across five Pakistani languages:
  Urdu, Punjabi, Sindhi, Pashto, and Balochi. The authors develop a culturally adapted
  Political Compass Test (PCT) and combine it with a multi-level framing analysis
  to assess ideological stance and stylistic framing.'
---

# Framing Political Bias in Multilingual LLMs Across Pakistani Languages

## Quick Facts
- arXiv ID: 2506.00068
- Source URL: https://arxiv.org/abs/2506.00068
- Reference count: 40
- Primary result: First large-scale evaluation of political bias in multilingual LLMs across five Pakistani languages using culturally adapted Political Compass Test and multi-level framing analysis

## Executive Summary
This study presents the first large-scale evaluation of political bias in multilingual large language models (LLMs) across five Pakistani languages: Urdu, Punjabi, Sindhi, Pashto, and Balochi. The authors developed a culturally adapted Political Compass Test (PCT) and combined it with a multi-level framing analysis to assess both ideological stance and stylistic framing across 11 socio-political topics relevant to Pakistani society. The evaluation covered 13 state-of-the-art LLMs and revealed that while models predominantly exhibit liberal-left orientations in English, they display more authoritarian framing in regional languages, demonstrating language-conditioned ideological shifts.

The research highlights that model-specific bias patterns persist across languages, suggesting that multilingual LLMs may not simply mirror training data distributions but encode distinct ideological patterns that vary by language. These findings underscore the critical need for culturally grounded, multilingual bias auditing frameworks to ensure fair and equitable AI deployment in linguistically diverse, low-resource settings.

## Method Summary
The authors developed a culturally adapted Political Compass Test (PCT) framework for Pakistani contexts, evaluating 13 state-of-the-art LLMs across five Pakistani languages (Urdu, Punjabi, Sindhi, Pashto, and Balochi) on 11 socio-political topics. The methodology combined ideological stance assessment with multi-level framing analysis to examine both what positions models take and how they express them stylistically. The evaluation systematically compared model responses across languages to identify language-conditioned ideological shifts and persistent model-specific bias patterns.

## Key Results
- LLMs predominantly exhibit liberal-left orientations when responding in English
- Models display more authoritarian framing when responding in regional Pakistani languages
- Model-specific bias patterns persist across languages, indicating language-conditioned ideological shifts

## Why This Works (Mechanism)
The mechanism underlying this research works because political bias in LLMs emerges from the interaction between training data composition and language-specific cultural contexts. When models process prompts in different languages, they draw upon distinct subsets of their training data that reflect varying cultural perspectives, historical narratives, and sociopolitical norms. This creates systematic differences in how models frame political issues across languages, revealing that multilingual models do not simply translate their English biases but develop language-specific ideological patterns based on the cultural and linguistic characteristics of their training data.

## Foundational Learning
- Political Compass Test (PCT): A framework for measuring political orientation along two dimensions (economic left-right and social libertarian-authoritarian). Why needed: Provides standardized metric for comparing ideological positions across languages and models. Quick check: Validate that adapted PCT items maintain equivalent meaning across all five Pakistani languages.

- Multi-level framing analysis: Examination of both content (what positions are taken) and style (how positions are expressed). Why needed: Captures not just ideological stance but also rhetorical patterns that may vary culturally. Quick check: Ensure framing categories are culturally appropriate and mutually exclusive across all languages.

- Language-conditioned bias: The phenomenon where model responses vary systematically based on the language of input. Why needed: Reveals that multilingual models develop distinct ideological patterns per language rather than simple translation of biases. Quick check: Compare responses to identical prompts across languages to isolate true linguistic effects from translation artifacts.

- Low-resource language considerations: Recognition that some Pakistani languages have limited digital representation in training data. Why needed: Acknowledges that data scarcity may amplify or distort bias measurements. Quick check: Analyze correlation between data availability and observed bias strength for each language.

- Cultural grounding in bias auditing: Adapting bias assessment frameworks to specific cultural contexts rather than applying Western-centric measures. Why needed: Ensures relevance and validity of bias measurements in non-Western settings. Quick check: Validate cultural appropriateness through local expert review of all test items and framing categories.

## Architecture Onboarding

Component map: Political Compass Test Framework -> Multi-level Framing Analysis -> LLM Evaluation Pipeline -> Bias Pattern Identification

Critical path: Culturally adapted PCT items → Language-specific prompt generation → Model response collection → Framing analysis → Ideological orientation mapping

Design tradeoffs: The study prioritized cultural relevance over standardization by adapting Western-centric bias measures for Pakistani contexts, potentially limiting cross-cultural comparability but improving local validity. The choice to evaluate multiple languages simultaneously provided rich comparative insights but increased complexity in ensuring consistent measurement across linguistic boundaries.

Failure signatures: If observed language-conditioned shifts reflect translation artifacts rather than genuine ideological differences, or if data scarcity in low-resource languages artificially amplifies bias signals. The study may also conflate model capability differences across languages with true ideological variation.

First experiments:
1. Conduct parallel evaluation of identical prompts across all five languages to establish baseline comparability
2. Test model responses to politically neutral control prompts in each language to identify language-specific model capabilities
3. Perform ablation study removing cultural adaptation elements to quantify their impact on bias measurements

## Open Questions the Paper Calls Out
None

## Limitations
- Cultural adaptation of PCT may introduce validity concerns across languages
- Data quality and quantity differences across languages may influence observed bias patterns
- Focus on 11 topics may not capture full spectrum of politically sensitive issues in Pakistani society

## Confidence

| Claim | Confidence |
|-------|------------|
| LLMs exhibit liberal-left orientations in English and authoritarian framing in regional languages | High |
| Model-specific bias patterns persist across languages | Medium |
| Culturally grounded multilingual bias auditing frameworks are needed | Medium |

## Next Checks
1. Conduct cross-cultural validation study using same PCT framework with other non-Western language communities to assess generalizability of language-conditioned ideological shifts

2. Perform systematic error analysis comparing responses to identical prompts across languages to distinguish genuine ideological shifts from translation artifacts or data quality differences

3. Implement longitudinal study tracking model responses to same political prompts over time to distinguish persistent ideological patterns from temporal variations in bias expression