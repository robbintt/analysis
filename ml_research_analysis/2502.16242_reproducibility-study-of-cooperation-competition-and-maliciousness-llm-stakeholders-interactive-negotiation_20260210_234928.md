---
ver: rpa2
title: 'Reproducibility Study of Cooperation, Competition, and Maliciousness: LLM-Stakeholders
  Interactive Negotiation'
arxiv_id: '2502.16242'
source_url: https://arxiv.org/abs/2502.16242
tags:
- game
- your
- deal
- negotiation
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This reproducibility study evaluates the "Cooperation, Competition,
  and Maliciousness: LLM-Stakeholders Interactive Negotiation" benchmark using various
  open-weight models (1.5B-70B parameters) and GPT-4o Mini. The study confirms that
  larger models generally perform better, with open-weight models slightly trailing
  proprietary models.'
---

# Reproducibility Study of Cooperation, Competition, and Maliciousness: LLM-Stakeholders Interactive Negotiation

## Quick Facts
- arXiv ID: 2502.16242
- Source URL: https://arxiv.org/abs/2502.16242
- Reference count: 33
- One-line primary result: Study confirms benchmark primarily measures rule-based inference rather than true negotiation skills, with single-agent baselines performing comparably to multi-agent negotiations

## Executive Summary
This reproducibility study evaluates the Cooperation, Competition, and Maliciousness (CCN) benchmark across various open-weight models (1.5B-70B parameters) and GPT-4o Mini. The findings reveal that larger models generally outperform smaller ones, though open-weight models slightly trail proprietary models. A significant discovery is that single-agent baselines, which use only game rules without communication, achieve comparable results to multi-agent negotiations, fundamentally challenging the benchmark's premise of measuring communication and negotiation skills.

The study introduces new metrics for structural information leakage and inequality assessment, uncovering that smaller models struggle with format adherence and experience substantial information leakage due to formatting errors. These findings question the benchmark's effectiveness in truly measuring negotiation capabilities, suggesting it may primarily assess a model's ability to infer acceptable deals based on game rules rather than genuine negotiation proficiency.

## Method Summary
The study replicates the original CCN benchmark experiments using a diverse set of models including GPT-4o Mini and open-weight models ranging from 1.5B to 70B parameters. The methodology involves testing both multi-agent negotiation scenarios and single-agent baselines that rely solely on game rules without communication. New metrics were introduced to evaluate structural information leakage and inequality in negotiation outcomes. The experiments compare performance across different model sizes and types, analyzing format adherence, information leakage, and the gap between open-weight and proprietary models.

## Key Results
- Single-agent baselines using only game rules achieve comparable performance to multi-agent negotiations
- Open-weight models trail proprietary models but show size-dependent performance improvements
- Smaller models exhibit significant information leakage and format adherence issues

## Why This Works (Mechanism)

## Foundational Learning
1. **Negotiation benchmark mechanics** - Understanding how CCN evaluates cooperation, competition, and maliciousness through structured interactions
   - Why needed: Provides context for interpreting model performance and benchmark validity
   - Quick check: Verify benchmark rules and scoring criteria are correctly understood

2. **Information leakage detection** - Methods to identify when models inadvertently reveal structural information during negotiations
   - Why needed: Critical for assessing model security and adherence to negotiation protocols
   - Quick check: Test detection methods on known information leakage scenarios

3. **Format adherence evaluation** - Techniques to measure whether models follow prescribed communication formats
   - Why needed: Essential for determining if performance differences stem from rule compliance or negotiation skill
   - Quick check: Validate format checking against controlled test cases

4. **Single-agent vs multi-agent comparison** - Framework for evaluating whether communication adds value beyond rule-based inference
   - Why needed: Central to questioning the benchmark's fundamental premise
   - Quick check: Compare baseline performance against multi-agent scenarios

## Architecture Onboarding
**Component Map:** Input Game Rules -> Model Processing -> Negotiation Output -> Performance Evaluation -> Structural Analysis
**Critical Path:** Game rules and constraints are fed to models, which generate negotiation responses, followed by performance scoring and analysis of information leakage and format adherence
**Design Tradeoffs:** Larger models offer better performance but at higher computational cost; single-agent baselines simplify evaluation but may not capture true negotiation dynamics
**Failure Signatures:** Information leakage manifests as format violations; poor format adherence indicates smaller models struggle with protocol compliance; comparable single-agent and multi-agent performance suggests rule inference dominates over communication
**First Experiments:** 1) Test single-agent baseline across multiple negotiation scenarios, 2) Measure information leakage in models with known vulnerabilities, 3) Compare format adherence across model sizes

## Open Questions the Paper Calls Out
None

## Limitations
- Study focuses exclusively on CCN benchmark, limiting generalizability to other negotiation frameworks
- Evaluation relies on a single benchmark, potentially introducing domain-specific biases
- Performance gaps between open-weight and proprietary models may not hold across different architectures

## Confidence
- Model performance comparisons: Medium confidence
- Single-agent baseline findings: Medium confidence
- Benchmark effectiveness critique: Medium confidence

## Next Checks
1. Replicate single-agent baseline experiment across multiple negotiation frameworks to verify consistent comparable performance
2. Conduct ablation studies on structural information leakage and inequality metrics across diverse negotiation scenarios
3. Test additional open-weight models beyond those evaluated to determine consistency of performance gaps across model families and parameter scales