---
ver: rpa2
title: A Lightweight Medical Image Classification Framework via Self-Supervised Contrastive
  Learning and Quantum-Enhanced Feature Modeling
arxiv_id: '2601.16608'
source_url: https://arxiv.org/abs/2601.16608
tags:
- medical
- learning
- quantum
- feature
- self-supervised
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study proposes a lightweight hybrid learning framework for
  medical image classification that integrates self-supervised contrastive learning
  with quantum-enhanced feature modeling. By combining a MobileNetV2 backbone, self-supervised
  pretraining on unlabeled data, and a parameterized quantum circuit as a feature
  enhancement module, the framework achieves strong performance on a coronary artery
  stenosis detection dataset.
---

# A Lightweight Medical Image Classification Framework via Self-Supervised Contrastive Learning and Quantum-Enhanced Feature Modeling

## Quick Facts
- arXiv ID: 2601.16608
- Source URL: https://arxiv.org/abs/2601.16608
- Reference count: 16
- Primary result: Hybrid framework achieves 0.8083 Accuracy, 0.9381 AUC, and 0.8053 F1 score for coronary artery stenosis detection using ~2-3M parameters.

## Executive Summary
This study proposes a lightweight hybrid learning framework for medical image classification that integrates self-supervised contrastive learning with quantum-enhanced feature modeling. By combining a MobileNetV2 backbone, self-supervised pretraining on unlabeled data, and a parameterized quantum circuit as a feature enhancement module, the framework achieves strong performance on a coronary artery stenosis detection dataset. Experimental results show that the model attains 0.8083 Accuracy, 0.9381 AUC, and 0.8053 F1 score while maintaining only approximately 2-3 million parameters and low computational cost. Compared to baseline models such as ResNet18, MobileNetV2, and EfficientNet-B0, the proposed method consistently outperforms in key metrics and offers improved generalization in small-sample and resource-constrained scenarios.

## Method Summary
The framework employs a two-stage training paradigm: first, a MobileNetV2 backbone is pretrained using SimCLR-style self-supervised contrastive learning on unlabeled angiography images to learn robust visual representations. Second, the pretrained backbone is fine-tuned with a quantum-enhanced feature module consisting of a parameterized quantum circuit (PQC) that processes compressed classical features via angle encoding and variational quantum gates, followed by residual fusion with classical features. The final classification head is a fully connected layer. The quantum module is implemented via classical simulation, with feature compression mapping to a small number of qubits to maintain efficiency.

## Key Results
- Achieved 0.8083 Accuracy, 0.9381 AUC, and 0.8053 F1 score on coronary artery stenosis detection task
- Outperformed baseline models (ResNet18, MobileNetV2, EfficientNet-B0) across all key metrics
- Maintained lightweight architecture with only ~2-3 million parameters suitable for resource-constrained deployment
- Demonstrated effectiveness of quantum-enhanced features in improving classification performance over classical-only approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The two-stage training paradigm (SSL pretraining followed by supervised fine-tuning) appears to mitigate feature scarcity in small-sample medical datasets.
- Mechanism: By applying a SimCLR-style contrastive objective on unlabeled images, the backbone encoder ($f_\theta$) learns to group augmented views of the same instance (positive pairs) while separating different instances (negative pairs). This enforces geometric consistency in the latent space before task-specific tuning.
- Core assumption: The augmentation pipeline preserves anatomical semantic integrity; aggressive augmentation does not destroy the pathological features required for downstream diagnosis.
- Evidence anchors:
  - [abstract] "MobileNetV2 is employed... pretrained using a SimCLR-style self-supervised paradigm on unlabeled images."
  - [section 2.2.1] "positive pairs preserve semantic consistency at the anatomical level... controlled perturbations... suppress shortcut learning."
  - [corpus] Related work "Quantum Machine Learning via Contrastive Training" (FMR 0.65) supports the general efficacy of contrastive objectives in QML, validating the learning paradigm.
- Break condition: If augmentations distort the lesion geometry (e.g., excessive cropping or rotation in angiography), the contrastive loss may enforce invariance to diagnostically critical features, degrading downstream performance.

### Mechanism 2
- Claim: The Parameterized Quantum Circuit (PQC) may enhance feature discriminability by modeling high-order correlations via quantum entanglement in a high-dimensional Hilbert space.
- Mechanism: Classical features are compressed and mapped to quantum states via angle encoding. The variational circuit ($U(\Theta)$) applies entangling gates, theoretically allowing the model to capture complex non-linear interactions that classical linear layers might miss. The expectation values of Pauli-Z operators are then measured to retrieve enhanced features.
- Core assumption: The classical-to-quantum data embedding (Eq. 10) preserves sufficient information density despite the drastic dimensionality reduction required to fit current qubit limitations ($u_i \in \mathbb{R}^Q$).
- Evidence anchors:
  - [abstract] "A lightweight parameterized quantum circuit (PQC) is embedded as a quantum feature enhancement module."
  - [section 2.3.2] "Through quantum entanglement, this variational structure introduces non-classical correlations... enabling expressive modeling."
  - [corpus] "Quanvolutional Neural Networks for Pneumonia Detection" suggests quantum-assisted feature extraction can be effective for medical imaging, though specific "entanglement" mechanisms vary by implementation.
- Break condition: If the number of qubits ($Q$) is too small relative to the input feature complexity, the compression ($W_q$) creates an information bottleneck, rendering the quantum evolution superficial.

### Mechanism 3
- Claim: The residual fusion strategy stabilizes the hybrid training dynamics by allowing the model to control the contribution of the quantum module.
- Mechanism: Instead of replacing classical features, the quantum-processed features are added to the classical vector via a learnable scalar $\alpha$ ($\tilde{h}_i = h_i + \alpha \cdot W_r q_i$). This allows the gradient descent to "trust" the quantum module only when it provides a signal superior to the classical residual.
- Core assumption: The quantum simulator provides stable gradients; "barren plateaus" (vanishing gradients) in the quantum layers do not stall the classical backbone's training.
- Evidence anchors:
  - [section 2.3.2] "A residual fusion strategy is employed... effectively mitigates training instability caused by the quantum module during early optimization stages."
  - [abstract] "...forming a hybrid classical–quantum architecture... fine-tuned on limited labeled data."
  - [corpus] Corpus evidence for specific "residual fusion" in hybrid quantum medical models is limited; general hybrid integration is noted in "Generative Diffusion Augmentation," but the specific residual stabilization mechanism relies primarily on the paper's internal claims.
- Break condition: If the quantum simulator introduces significant noise or gradient variance, the learnable scalar $\alpha$ may collapse to zero, effectively turning the model into a purely classical MobileNetV2 to minimize loss.

## Foundational Learning

- Concept: **Contrastive Self-Supervised Learning (SimCLR Framework)**
  - Why needed here: The paper relies on SSL to decouple representation learning from the scarcity of manual annotations. Understanding how "positive" and "negative" pairs construct the latent space is essential for diagnosing representation failures.
  - Quick check question: Can you explain why increasing the temperature parameter $\tau$ in the contrastive loss might change how the model treats hard negative samples?

- Concept: **Variational Quantum Circuits (VQC) & Angle Encoding**
  - Why needed here: The "quantum" enhancement is not a magic box but a specific mathematical transformation. Understanding how classical data rotates qubits and how measurement collapses the state back to classical data is critical for debugging the feature enhancement module.
  - Quick check question: If you input a feature vector of all zeros into the angle encoding (Eq. 10), what is the initial state of the qubits before the variational layer?

- Concept: **Residual Connections**
  - Why needed here: The integration of the quantum module uses a residual add ($h_i + \Delta q$). Understanding gradient flow through additive identity mappings is necessary to appreciate why the authors claim this stabilizes the hybrid training.
  - Quick check question: If the weights $W_r$ are initialized to zero, what is the initial effective output of the quantum residual branch?

## Architecture Onboarding

- Component map: Image → MobileNetV2 Features → Compression → **Quantum Circuit (PQC)** → Residual Fusion → Classification Head
- Critical path: Image → MobileNetV2 Features → Compression → **Quantum Circuit (PQC)** → Residual Fusion → Classification Head
  *Note: During Stage 1 (SSL), the PQC and Classifier are bypassed/unused; training focuses on the Backbone + Projection Head.*
- Design tradeoffs:
  - **Qubit Count ($Q$) vs Information Loss**: The paper uses a "lightweight" PQC, implying few qubits. Reducing features to $Q$ dimensions risks losing spatial texture info but gains non-linear modeling power.
  - **Simulation vs Hardware**: The PQC is likely simulated classically. This trades off the "quantum speedup" for feasibility, meaning the PQC layer is actually computationally expensive compared to standard layers during training.
- Failure signatures:
  - **Collapsed AUC (approx 0.5)**: Suggests the augmentation strategy in SSL destroyed semantic content, or the quantum layer introduced noise that the fusion mechanism couldn't filter.
  - **High Specificity / Low Sensitivity**: Suggests the model is "confused" by the quantum feature injection and defaulting to predicting the majority class (negative).
  - **Divergent Loss**: Likely caused by the quantum simulator gradients or a learning rate mismatch between the classical optimizer and the quantum parameter shifts.
- First 3 experiments:
  1. **Baseline Validation**: Train MobileNetV2 *from scratch* (supervised only) on the labeled dataset to establish the "data scarcity" penalty.
  2. **SSL Ablation**: Train MobileNetV2 with SSL pretraining but *without* the quantum module to isolate the contribution of the contrastive learning.
  3. **Hybrid Integration**: Train the full model (SSL + Quantum) and monitor the fusion weight $\alpha$ to verify if the optimizer actually utilizes the quantum features.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the proposed framework maintain its efficacy when applied to multi-class or multi-label medical imaging tasks, as opposed to the binary classification task evaluated in this study?
- Basis in paper: [explicit] The authors state that "this study focuses primarily on binary classification tasks, and the effectiveness of the proposed framework in multi-class or more complex medical imaging scenarios remains to be systematically investigated."
- Why unresolved: The current evaluation is restricted to binary coronary artery stenosis detection, leaving the model's behavior in more complex, multi-category diagnostic scenarios unknown.
- What evidence would resolve it: Successful experimental results on datasets with multiple disease categories or grading scales (e.g., multi-class tumor grading) using the same hybrid architecture.

### Open Question 2
- Question: How does the performance and stability of the parameterized quantum circuit (PQC) module change when deployed on actual Noisy Intermediate-Scale Quantum (NISQ) hardware compared to the classical simulator used?
- Basis in paper: [explicit] The limitations section notes that "the quantum-enhanced module is implemented using classical simulation, and its scale and complexity are constrained by current computational resources."
- Why unresolved: Idealized classical simulations do not account for quantum noise, decoherence, or gate errors inherent in current physical quantum processors, which could degrade the feature enhancement capabilities.
- What evidence would resolve it: A comparative study showing convergence curves and classification metrics when the PQC is executed on real quantum hardware versus a noise-free simulator.

### Open Question 3
- Question: Can the model generalize effectively across multi-center, multi-vendor datasets where imaging protocols and data distributions differ significantly?
- Basis in paper: [explicit] The authors acknowledge that "the dataset used in this work is derived from a relatively limited source" and that "further validation across multi-center, multi-vendor datasets is necessary."
- Why unresolved: The model was validated on a specific CAG dataset (LRSE-Net), and it is unclear if the self-supervised pretraining sufficiently captures domain-invariant features to handle distribution shifts in external clinical environments.
- What evidence would resolve it: Evaluation of the pre-trained model on independent cohorts from different hospitals without fine-tuning, or with minimal domain adaptation.

## Limitations
- Quantum circuit parameters (qubit count, depth, gate structure) remain unspecified, making reproducibility difficult
- No ablation study isolates the contribution of the PQC module from the SSL pretraining benefits
- Validation limited to single-domain coronary angiography dataset, limiting generalizability claims
- Classical simulation of quantum circuits may not reflect true quantum advantage or hardware constraints

## Confidence
- **High Confidence**: MobileNetV2 backbone design, residual fusion strategy, reported metrics on held-out test set, and SSL pretraining benefits
- **Medium Confidence**: Integration of PQC as a feature enhancer (mechanistically plausible but unablated), generalizability to other medical domains, and robustness under data scarcity
- **Low Confidence**: Quantum-specific advantages over classical nonlinear layers, actual quantum entanglement contribution, and hardware-agnostic simulation fidelity

## Next Checks
1. **Quantum Ablation Test**: Remove the PQC module and retrain with only classical MobileNetV2 + residual fusion (α disabled) to quantify the quantum contribution
2. **Quantum Circuit Transparency**: Report the exact number of qubits Q, circuit depth L, gate structure, and measure wall-clock time per epoch for the PQC simulation
3. **Augmentation Robustness Scan**: Systematically vary rotation range, crop ratio, and intensity jitter; plot AUC/F1 vs augmentation strength to verify the SSL robustness claim