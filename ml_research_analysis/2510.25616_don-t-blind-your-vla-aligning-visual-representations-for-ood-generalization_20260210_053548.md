---
ver: rpa2
title: 'Don''t Blind Your VLA: Aligning Visual Representations for OOD Generalization'
arxiv_id: '2510.25616'
source_url: https://arxiv.org/abs/2510.25616
tags:
- arxiv
- visual
- alignment
- representations
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of visual representation degradation
  in Vision-Language-Action (VLA) models during fine-tuning for robotic tasks. It
  shows that standard supervised fine-tuning causes these models to lose the strong
  visual-language grounding inherited from their pretrained Vision-Language Model
  (VLM) base, leading to degraded attention maps, collapsed representations, and reduced
  generalization.
---

# Don't Blind Your VLA: Aligning Visual Representations for OOD Generalization

## Quick Facts
- arXiv ID: 2510.25616
- Source URL: https://arxiv.org/abs/2510.25616
- Reference count: 40
- Key outcome: Visual Representation Alignment method improves OOD generalization in VLAs by up to 10% relative gains

## Executive Summary
This paper addresses the problem of visual representation degradation in Vision-Language-Action (VLA) models during fine-tuning for robotic tasks. Standard supervised fine-tuning causes VLAs to lose the strong visual-language grounding inherited from their pretrained Vision-Language Model base, leading to collapsed representations and reduced generalization. The authors propose a lightweight Visual Representation Alignment method that anchors the VLA's visual features to those of a frozen, pretrained vision teacher during fine-tuning. This approach, inspired by the Platonic Representation Hypothesis, encourages the model to preserve semantically consistent visual representations while adapting to action tasks.

## Method Summary
The authors introduce Visual Representation Alignment as a simple yet effective method to prevent VLA models from "forgetting" their strong visual-language grounding during fine-tuning. The approach involves maintaining a frozen vision teacher (typically a pretrained VLM) and adding a KL divergence loss between the visual encoder's output distribution and that of the teacher. This alignment loss is combined with the standard task-specific loss during fine-tuning, encouraging the VLA to preserve the rich semantic structure of its visual representations while learning action-oriented behaviors. The method is particularly motivated by the observation that VLAs, despite their strong pretraining, often fail to generalize to OOD data after task-specific fine-tuning, as their attention maps and visual representations become less semantically meaningful.

## Key Results
- Visual Representation Alignment improves OOD generalization on the Simpler benchmark by up to 10% relative gains over naive fine-tuning
- The method consistently outperforms freezing the visual encoder and other baselines across different evaluation settings
- Linear probing on ImageNet-100 demonstrates that aligned models maintain better representational quality after fine-tuning

## Why This Works (Mechanism)
The method works by preventing catastrophic forgetting of visual-language grounding during fine-tuning. When VLAs are adapted to robotic tasks, their visual representations tend to collapse toward task-specific features, losing the rich semantic structure from pretraining. By aligning visual features with a frozen teacher, the model maintains its ability to produce semantically meaningful representations while still adapting to action-specific objectives. This dual objective creates a regularization effect that preserves generalization capabilities without sacrificing task performance.

## Foundational Learning
- Vision-Language-Action models (VLAs): Multimodal models combining visual perception, language understanding, and action prediction for robotics. Why needed: These are the target models for the proposed method. Quick check: Verify the model can process image inputs and output action predictions.
- Platonic Representation Hypothesis: The theory that different modalities converge toward similar representations when trained on large amounts of data. Why needed: Provides theoretical motivation for why visual alignment should work. Quick check: Examine attention map similarity between modalities before and after alignment.
- KL divergence alignment loss: A regularization technique that minimizes the distributional difference between teacher and student representations. Why needed: The core mechanism for preserving visual representations. Quick check: Monitor the alignment loss during training to ensure it's contributing meaningfully.

## Architecture Onboarding

Component Map:
Pretrained VLM (teacher) -> Visual Encoder (student) -> Task-specific layers -> Action output

Critical Path:
Visual input → Teacher frozen encoder → KL alignment loss + Task loss → Updated VLA parameters

Design Tradeoffs:
- Alignment strength vs. task adaptation speed: Higher alignment preserves representations better but may slow task learning
- Teacher quality vs. computational overhead: Better teachers provide stronger guidance but increase memory requirements
- Alignment scope vs. flexibility: Full alignment preserves more but may restrict necessary task-specific modifications

Failure Signatures:
- Alignment loss dominates training (model doesn't learn task)
- Task loss plateaus while alignment loss decreases (model ignores task)
- Both losses decrease but validation performance doesn't improve (poor hyperparameter tuning)

First Experiments:
1. Ablation: Train with only task loss vs. task loss + alignment loss to measure impact
2. Visualization: Compare attention maps before and after alignment to verify preservation of semantic structure
3. Hyperparameter sweep: Test different alignment loss weights to find optimal balance

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Evaluation limited to a single VLA architecture (PiSA) and one robotic benchmark (Simpler)
- Unclear effectiveness for long-horizon, multi-stage robotic tasks versus simple single-step actions
- Performance with vision teachers whose training distribution significantly differs from fine-tuning data is not examined

## Confidence
- Claims about improving OOD generalization: High confidence
- Claims about preserving visual-language grounding: Medium confidence
- Claims about negligible computational overhead: Medium confidence

## Next Checks
1. Test the alignment method across multiple VLA architectures (e.g., PaLM-E, RT-2) to verify architecture-agnostic benefits
2. Evaluate performance degradation when the vision teacher's training distribution significantly differs from the fine-tuning data
3. Conduct ablation studies varying the alignment loss weight to identify optimal trade-offs between task performance and representation preservation