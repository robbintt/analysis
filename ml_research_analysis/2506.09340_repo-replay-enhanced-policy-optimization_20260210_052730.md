---
ver: rpa2
title: 'RePO: Replay-Enhanced Policy Optimization'
arxiv_id: '2506.09340'
source_url: https://arxiv.org/abs/2506.09340
tags:
- repo
- grpo
- policy
- samples
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Replay-Enhanced Policy Optimization (RePO),
  an extension of Group Relative Policy Optimization (GRPO) that integrates off-policy
  samples from a replay buffer into the policy optimization process. RePO addresses
  the limitations of GRPO, which relies solely on on-policy samples and can suffer
  from high computational costs and reduced data efficiency, particularly when all
  samples receive identical rewards.
---

# RePO: Replay-Enhanced Policy Optimization

## Quick Facts
- arXiv ID: 2506.09340
- Source URL: https://arxiv.org/abs/2506.09340
- Authors: Siheng Li; Zhanhui Zhou; Wai Lam; Chao Yang; Chaochao Lu
- Reference count: 13
- Primary result: Achieves 18.4-point average performance gain for Qwen2.5-Math-1.5B over GRPO

## Executive Summary
RePO extends GRPO by integrating off-policy samples from a replay buffer into the policy optimization process. While GRPO suffers from high computational costs and reduced data efficiency due to its reliance on on-policy samples, RePO addresses these limitations by retrieving historical outputs using diverse strategies (recency-based, reward-oriented, variance-driven). The approach maintains GRPO's group-relative advantage estimation while adding an off-policy loss term weighted by importance sampling ratios. Experiments demonstrate significant performance improvements across five LLMs on seven mathematical reasoning benchmarks.

## Method Summary
RePO modifies GRPO's on-policy-only optimization by storing generated outputs and their probabilities in a replay buffer, then retrieving historical samples using one of three strategies: recency-based (most recent outputs), reward-oriented (highest-reward outputs), or variance-driven (low discriminability scenarios). The method estimates advantages separately for on-policy and off-policy samples to prevent interference, and updates the policy using a combined objective that includes both on-policy and off-policy losses weighted by importance sampling ratios. This approach increases effective optimization steps while maintaining computational efficiency.

## Key Results
- Achieves 18.4-point average performance gain for Qwen2.5-Math-1.5B compared to GRPO
- Raises effective optimization steps by 48% for Qwen3-1.7B with 15% computational overhead increase
- Demonstrates 4.1-point improvement for Qwen3-1.7B model

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** RePO increases the frequency of effective optimization steps by introducing reward variance through off-policy samples.
- **Mechanism:** When all on-policy outputs receive identical rewards, GRPO's group-relative advantage becomes zero, preventing learning. RePO retrieves off-policy samples with different rewards, ensuring non-zero advantages even when current samples are uniform.
- **Core assumption:** The replay buffer contains diverse historical outputs with differing reward profiles.
- **Evidence anchors:** RePO increases effective-step percentage from 31.2% to 46.1% (relative +47.8%) compared to GRPO; variance-driven strategy targets low reward discriminability scenarios.
- **Break condition:** If buffer contains only samples with identical rewards or retrieval fails to access divergent rewards.

### Mechanism 2
- **Claim:** Separate advantage estimation for on-policy and off-policy samples reduces interference compared to pooling them.
- **Mechanism:** Mixing samples normalizes advantages against a single mean/std derived from heterogeneous groups, distorting the signal. RePO calculates advantages independently for each group, aligning baselines with specific generation contexts.
- **Core assumption:** On-policy and off-policy sample distributions differ significantly enough that shared baseline introduces noise.
- **Evidence anchors:** "Split" strategy consistently outperforms "Mixed" (43.6 vs 27.2 average accuracy with 8 samples).
- **Break condition:** If replay buffer is extremely small or retrieval limited to most recent step.

### Mechanism 3
- **Claim:** Off-policy loss safely reuses historical data by implicitly down-weighting samples unlikely to be generated by current policy.
- **Mechanism:** Gradient scales with importance sampling ratio πθ/πθ_off. If current policy has moved significantly from behavior policy, ratio becomes small, naturally dampening gradient contribution from outdated samples.
- **Core assumption:** KL divergence between current and behavior policies is effectively managed by clipping parameter and natural decay of ratio.
- **Evidence anchors:** Derivation shows off-policy gradient ∝ r_off^i,t · A_off^i,t; allows optimization based on diverse samples without advanced external models.
- **Break condition:** If KL divergence grows too large, variance of importance sampling ratio may destabilize training or gradient signal may vanish.

## Foundational Learning

- **Concept: Group Relative Policy Optimization (GRPO)**
  - **Why needed here:** RePO is an extension of GRPO. You must understand that GRPO replaces the value function with a "group baseline"—the mean reward of multiple outputs for the same prompt.
  - **Quick check question:** If I sample 4 outputs and they all get reward of 1.0, what is the calculated advantage in standard GRPO? (Answer: 0, because mean=1.0, std=0).

- **Concept: On-Policy vs. Off-Policy RL**
  - **Why needed here:** RePO bridges these two regimes. Understanding that "on-policy" requires data from current policy (expensive/discardable) while "off-policy" allows learning from old data (efficient/reusable) is central to RePO's value.
  - **Quick check question:** Why does pure on-policy RL struggle with data efficiency? (Answer: Because once policy updates, old sampled data is technically invalid/discarded).

- **Concept: Importance Sampling (The Ratio r_t)**
  - **Why needed here:** This is the mathematical tool that makes off-policy learning possible. RePO relies on ratio πθ/πθ_off to correct probability mismatch when learning from old data.
  - **Quick check question:** If πθ (current) assigns 1% probability to an action, but πθ_off (old) assigned 90%, what happens to gradient weight in RePO? (Answer: It is down-weighted significantly by ratio 0.01/0.90 ≈ 0.011).

## Architecture Onboarding

- **Component map:** Policy Model πθ -> Replay Buffer B -> Advantage Estimator -> Replay Strategy S -> Policy Update

- **Critical path:**
  1. Sample prompt q
  2. Generate Gon on-policy outputs; store them and log-probs in Buffer B
  3. Retrieve Goff outputs from B based on strategy S
  4. Compute rewards for all outputs
  5. **Split Advantage:** Compute Aon using only on-policy rewards; compute Aoff using only off-policy rewards
  6. Update πθ using combined objective JRePO

- **Design tradeoffs:**
  - **Recency vs. Reward Retrieval:** Recency-based aligns better with current policy (lower variance roff), suitable for base models. Reward-oriented aggressively reinforces high-reward behaviors, better for instruct models.
  - **Split vs. Mixed Estimation:** Split is empirically superior but requires managing two normalization statistics. Mixed simplifies code but introduces interference.
  - **Buffer Size vs. Staleness:** Larger buffer increases data diversity but risks high KL divergence; relies on importance ratio to down-weight bad gradients.

- **Failure signatures:**
  - **Zero Gradient (Replay Failure):** If buffer returns samples identical to on-policy samples (or all have same reward), advantage remains zero.
  - **Training Collapse (High KL):** If Recency strategy not used and policy changes rapidly, roff may explode or vanish. Log-prob storage in buffer is critical.

- **First 3 experiments:**
  1. **Sanity Check (Advantage Split):** Run RePO on small dataset (100 prompts) using "Mixed" vs. "Split" advantage estimation. Verify "Split" prevents variance collapse.
  2. **Strategy Ablation:** Implement buffer and test Recency-based vs. Reward-oriented retrieval on Qwen base vs. instruct model to reproduce differential benefit finding.
  3. **Effective Steps Analysis:** Measure percentage of steps where gradient norm is non-zero. Compare GRPO vs. RePO on sparse reward task (e.g., Math) to validate "48% increase in effective steps" claim.

## Open Questions the Paper Calls Out
None

## Limitations
- Primary limitation is dependency on replay buffer quality; effectiveness hinges on maintaining diverse buffer with samples spanning multiple reward distributions
- Computational overhead increase of 15% may be prohibitive for resource-constrained deployment scenarios
- Claim about increasing effective optimization steps by 48% requires validation across different model architectures and task domains

## Confidence

**Major Limitations:**
- Dependency on replay buffer quality
- 15% computational overhead
- Limited generalizability of effective steps claim

**Confidence Levels:**
- **High Confidence:** 18.4-point average performance gains for Qwen2.5-Math-1.5B are well-supported by experimental methodology
- **Medium Confidence:** 48% increase in effective optimization steps claim requires further validation across different architectures
- **Medium Confidence:** Instruct models benefit more from reward-oriented replay strategies, but underlying reasons need exploration

## Next Checks

1. **Buffer Diversity Analysis:** Conduct experiments measuring reward distribution diversity in replay buffer over training epochs to empirically validate mechanism's dependency on diverse reward profiles.

2. **KL Divergence Monitoring:** Implement continuous monitoring of KL divergence between current and behavior policies during training to verify importance sampling ratios remain stable and "stale data" protection functions as claimed.

3. **Computational Overhead Scaling:** Test RePO's performance and computational overhead scaling across different batch sizes and model sizes to establish practical limits of 15% overhead claim.