---
ver: rpa2
title: 'Assessing the Quality of Denoising Diffusion Models in Wasserstein Distance:
  Noisy Score and Optimal Bounds'
arxiv_id: '2506.09681'
source_url: https://arxiv.org/abs/2506.09681
tags:
- score
- distribution
- noise
- lemma
- assumption
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies denoising diffusion probabilistic models (DDPMs)
  when the score function is evaluated with constant-variance noise. Empirically,
  the authors show that DDPMs are robust to such noisy score evaluations on standard
  datasets like CIFAR-10 and CelebA-HQ.
---

# Assessing the Quality of Denoising Diffusion Models in Wasserstein Distance: Noisy Score and Optimal Bounds

## Quick Facts
- arXiv ID: 2506.09681
- Source URL: https://arxiv.org/abs/2506.09681
- Authors: Vahan Arsenyan; Elen Vardanyan; Arnak Dalalyan
- Reference count: 40
- One-line primary result: This paper studies denoising diffusion probabilistic models (DDPMs) when the score function is evaluated with constant-variance noise.

## Executive Summary
This paper investigates denoising diffusion probabilistic models (DDPMs) under the practical scenario where the score function is evaluated with constant-variance noise. The authors demonstrate both empirically and theoretically that DDPMs are robust to such noisy score evaluations, achieving optimal convergence rates comparable to the Gaussian case. The theoretical analysis provides finite-sample guarantees in Wasserstein-2 distance, leveraging smoothness of the score function to obtain tighter bounds than previous works.

## Method Summary
The authors employ a theoretical framework to analyze DDPMs when the score function is evaluated with constant-variance noise. They establish finite-sample guarantees in Wasserstein-2 distance by exploiting the smoothness of the score function, which allows for tighter error bounds than previous approaches. The analysis covers a broad class of distributions, including compactly supported semi-log-concave measures. Empirical validation is performed on standard datasets (CIFAR-10 and CelebA-HQ) to demonstrate robustness to noisy score evaluations.

## Key Results
- DDPMs are empirically robust to constant-variance noise in score evaluation on standard datasets like CIFAR-10 and CelebA-HQ.
- Theoretical analysis provides finite-sample guarantees in Wasserstein-2 distance with convergence rates of order √(D/ε).
- Achieves convergence rates matching the optimal Gaussian case under weaker assumptions about the target distribution.

## Why This Works (Mechanism)
The robustness of DDPMs to noisy score evaluations stems from the diffusion process's inherent smoothing properties and the continuity of the score function. The theoretical framework leverages the smoothness of the score function to bound the error introduced by noise, showing that this error diminishes as the number of queries increases. The optimal convergence rates are achieved by carefully controlling the trade-off between discretization error and score noise.

## Foundational Learning
1. **Semi-log-concave measures**: Why needed - To ensure well-behaved target distributions with tractable score functions. Quick check - Verify the target distribution satisfies the log-concavity condition.
2. **Wasserstein-2 distance**: Why needed - To quantify the quality of generated samples in terms of optimal transport. Quick check - Confirm the distance metric is appropriate for the application.
3. **Score matching**: Why needed - To learn the gradient of the log-density without requiring normalized densities. Quick check - Ensure the score function is well-defined and can be estimated.
4. **Diffusion process**: Why needed - To gradually transform a simple distribution into the target distribution. Quick check - Verify the forward and reverse processes are correctly implemented.
5. **Finite-sample bounds**: Why needed - To provide theoretical guarantees on performance with limited computational resources. Quick check - Confirm the bounds are non-asymptotic and depend on relevant problem parameters.
6. **Smoothness of score function**: Why needed - To control the error introduced by noisy score evaluations. Quick check - Verify the score function satisfies the required smoothness conditions.

## Architecture Onboarding

**Component Map**
- Score network -> Noise injection -> Diffusion process -> Reverse process -> Sample generation

**Critical Path**
1. Train score network to approximate the gradient of log-density
2. Evaluate score function with constant-variance noise
3. Apply denoising diffusion process to generate samples
4. Measure sample quality using Wasserstein-2 distance

**Design Tradeoffs**
- Balance between discretization error and score noise in the diffusion process
- Choice of noise level for score evaluation: too high degrades performance, too low may not capture robustness
- Computational cost vs. sample quality: more queries reduce noise impact but increase runtime

**Failure Signatures**
- Poor sample quality despite low score matching loss (indicates mismatch between training and evaluation noise)
- Instability in the reverse diffusion process (suggests issues with score function estimation)
- Wasserstein-2 distance not decreasing with more queries (potential problems with theoretical bounds)

**First 3 Experiments**
1. Train a DDPM on CIFAR-10 with varying levels of noise in score evaluation; measure sample quality and convergence rates.
2. Verify the smoothness of the score function on a simple semi-log-concave distribution; compare theoretical bounds with empirical performance.
3. Extend the analysis to higher-resolution images (e.g., CelebA-HQ 256x256) and assess the impact of score noise on perceptual quality metrics.

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical analysis relies on strong assumptions about target distribution (semi-log-concavity, smoothness of score function) that may not hold for complex real-world data.
- Empirical results are limited to a small set of standard datasets (CIFAR-10, CelebA-HQ) without exploring more challenging domains or out-of-distribution data.
- Theoretical bounds are asymptotic in nature and may not accurately reflect practical performance with finite computational resources.

## Confidence
- **High confidence**: Theoretical convergence rates and their comparison to optimal Gaussian case; empirical robustness to noisy score evaluations on tested datasets.
- **Medium confidence**: Generalization of results to broader classes of distributions and real-world data distributions; practical relevance of theoretical bounds for finite sample sizes.
- **Low confidence**: Impact of score noise on downstream tasks beyond sample generation; scalability of results to larger, more complex datasets and architectures.

## Next Checks
1. Test the robustness to noisy score evaluations on more diverse and challenging datasets, including out-of-distribution samples and higher-resolution images.
2. Conduct ablation studies to isolate the impact of score function smoothness and semi-log-concavity assumptions on theoretical bounds.
3. Compare the Wasserstein-2 based evaluation with alternative metrics (e.g., FID, precision/recall) to assess practical relevance of the theoretical guarantees.