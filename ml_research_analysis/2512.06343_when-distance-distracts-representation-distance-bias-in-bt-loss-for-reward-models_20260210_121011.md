---
ver: rpa2
title: 'When Distance Distracts: Representation Distance Bias in BT-Loss for Reward
  Models'
arxiv_id: '2512.06343'
source_url: https://arxiv.org/abs/2512.06343
tags:
- reward
- distance
- representation
- normbt
- pairs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work analyzes the Bradley-Terry (BT) loss for reward modeling
  and identifies a key limitation: the gradient magnitude depends jointly on prediction
  error and representation distance between response pairs. This coupling causes small-distance
  pairs to receive weak updates even when misranked, while large-distance pairs receive
  disproportionately strong updates, leading to biased learning signals.'
---

# When Distance Distracts: Representation Distance Bias in BT-Loss for Reward Models

## Quick Facts
- **arXiv ID:** 2512.06343
- **Source URL:** https://arxiv.org/abs/2512.06343
- **Reference count:** 40
- **Primary result:** Representation distance bias in BT loss causes small-distance pairs to receive weak updates; NORMBT normalization improves RewardBench accuracy by 5% on Reasoning category.

## Executive Summary
This paper identifies a key limitation in Bradley-Terry (BT) loss for reward modeling: gradient magnitude depends jointly on prediction error and representation distance between response pairs. This coupling causes small-distance pairs to receive weak updates even when misranked, while large-distance pairs receive disproportionately strong updates, leading to biased learning signals. The authors propose NORMBT, a lightweight per-pair normalization scheme that scales gradient contributions inversely with representation distance using the final-layer embedding difference as a proxy. Experiments across multiple LLM backbones and datasets show consistent improvements over BT baselines, with notable gains on fine-grained preference pairs.

## Method Summary
The authors analyze BT loss for reward modeling and identify a representation distance bias where gradient magnitude depends on both prediction error and embedding distance between response pairs. To address this, they propose NORMBT, which normalizes per-pair gradients by the L2 distance of last-token embeddings (proxy for representation distance) using an exponential moving average (EMA) of batch-wise mean distances for stability. NORMBT is a drop-in modification to BT loss with negligible computational overhead. The method is evaluated on reward modeling tasks using preference datasets, with models trained for one epoch using SGD with momentum and evaluated on RewardBench accuracy and Best-of-N selection performance.

## Key Results
- NORMBT achieves over 5% accuracy gains on the Reasoning category of RewardBench, which contains numerous fine-grained pairs
- Consistent improvements observed across multiple LLM backbones (gemma-2b-it, Llama-3.2-3B-Instruct) and datasets
- Outperforms margin-based and label-smoothing variants of BT loss
- Improves downstream Best-of-N selection performance compared to BT baseline

## Why This Works (Mechanism)
BT loss gradients scale with both prediction error and representation distance between responses. Small-distance pairs receive weak updates even when misranked, while large-distance pairs get disproportionately strong updates. NORMBT corrects this by normalizing gradient contributions inversely with representation distance, ensuring updates are driven primarily by prediction error rather than distance. The EMA-based scale stabilization prevents embedding scale drift during training.

## Foundational Learning
- **Bradley-Terry loss:** Pairwise preference ranking loss that compares chosen vs rejected responses - needed for understanding baseline method and its limitations
- **Representation distance bias:** The coupling between gradient magnitude and embedding distance that causes learning signal imbalance - key insight driving the proposed solution
- **Exponential moving average (EMA):** Technique for tracking running statistics (mean distance) with decay parameter β - provides stability for the normalization scale
- **Last-token embedding as proxy:** Using final-layer last-token representations to approximate full response distance - computationally efficient choice validated by ablation studies

## Architecture Onboarding
- **Component map:** Preference dataset -> Reward model (backbone + linear head) -> NORMBT loss -> EMA tracker -> Gradients
- **Critical path:** Forward pass through backbone, extract last-token embeddings, compute L2 distance, apply NORMBT weighting to BT loss, backward pass
- **Design tradeoffs:** Simple inverse distance scaling vs learned scaling functions; last-token vs average-pooled embeddings; EMA vs fixed distance scale
- **Failure signatures:** Performance degradation without EMA (embedding scales drift); using wrong embedding type (avg-pool underperforms); over-smoothing with wrong β value
- **First experiments:** 1) Load preference dataset and implement basic BT loss; 2) Add NORMBT normalization with EMA tracking; 3) Train for 1 epoch and evaluate on RewardBench accuracy

## Open Questions the Paper Calls Out
The paper identifies several directions for future research:
- Whether improvements in reward model accuracy and Best-of-N performance translate to significant gains in full reinforcement learning policy optimization (e.g., PPO)
- Whether more intricate scaling strategies can mitigate the slight performance trade-off observed in large-distance regions
- How NORMBT interacts with other training techniques that alter gradient dynamics, such as hidden state regularization

## Limitations
- The method focuses on pairwise preference learning in reward modeling and may not generalize to other preference learning scenarios without modification
- Slight performance trade-off remains in larger-distance regions, suggesting room for improved scaling strategies
- While experiments cover multiple datasets and model families, broader evaluation across different preference learning tasks would strengthen conclusions

## Confidence
- **High Confidence:** The empirical observation that BT loss gradients correlate with representation distance and negatively impact learning for small-distance pairs
- **Medium Confidence:** The claim that NORMBT's normalization strategy is optimal, as alternative approaches were not exhaustively explored
- **Medium Confidence:** The generalization of findings to other preference learning scenarios, given evaluation focus on specific datasets and architectures

## Next Checks
1. **Hyperparameter Sensitivity Analysis:** Systematically vary EMA decay rate β and numerical stability constant ε to identify optimal values and assess robustness
2. **Alternative Normalization Schemes:** Compare NORMBT against other distance-based normalization approaches, such as using mean distance across entire training set or learned scaling factors
3. **Cross-Domain Evaluation:** Test NORMBT on preference learning tasks outside standard reward modeling setup to evaluate broader applicability