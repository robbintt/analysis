---
ver: rpa2
title: Enhancing Control Policy Smoothness by Aligning Actions with Predictions from
  Preceding States
arxiv_id: '2601.18479'
source_url: https://arxiv.org/abs/2601.18479
tags:
- action
- asap
- lipschitz
- state
- states
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of high-frequency action oscillations
  in deep reinforcement learning policies, which can cause wear and safety issues
  in real-world deployments. The authors propose ASAP (Action Smoothing by Aligning
  Actions with Predictions from Preceding States), a novel loss-based method that
  defines transition-induced similar states as the distribution of next states from
  the same previous state, better capturing system dynamics than synthetic alternatives.
---

# Enhancing Control Policy Smoothness by Aligning Actions with Predictions from Preceding States

## Quick Facts
- **arXiv ID**: 2601.18479
- **Source URL**: https://arxiv.org/abs/2601.18479
- **Reference count**: 7
- **Primary result**: ASAP reduces action oscillations by up to 89.5% while maintaining or improving policy performance in RL benchmarks.

## Executive Summary
This paper addresses the problem of high-frequency action oscillations in deep reinforcement learning policies, which can cause wear and safety issues in real-world deployments. The authors propose ASAP (Action Smoothing by Aligning Actions with Predictions from Preceding States), a novel loss-based method that defines transition-induced similar states as the distribution of next states from the same previous state, better capturing system dynamics than synthetic alternatives. ASAP combines spatial regularization that aligns current actions with predicted actions from previous states, and temporal regularization that penalizes second-order action differences to suppress high-frequency oscillations. Experiments in Gymnasium and Isaac-Lab environments demonstrate that ASAP significantly reduces action oscillations while maintaining or improving policy performance compared to existing methods.

## Method Summary
ASAP introduces a spatial regularization term that aligns the current action with a predicted action from the preceding state, and a temporal regularization term that penalizes second-order differences in actions. The method defines "similar states" as the distribution of next states reachable from a previous state, creating a principled basis for spatial action consistency grounded in actual system dynamics. A predictor network is trained to output the expected action given the previous state, and the policy is trained to minimize the difference between its current action and this prediction. The temporal loss suppresses high-frequency oscillations by penalizing the second derivative of the action sequence, allowing the policy to function as a learnable low-pass filter.

## Key Results
- **89.5% improvement** in smoothness score on Hopper-v3 with ASAP compared to baseline SAC.
- **Significantly reduced oscillations** in action sequences across multiple tasks while maintaining or improving performance metrics.
- **Improved precision** in the Franka Reach task, reducing mean end-effector error from 0.90cm to 0.62cm.
- **Compatible with architectural approaches** like LipsNet, providing additive benefits when combined.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: States reachable from the same preceding state form a locally bounded neighborhood, providing a principled basis for spatial action consistency.
- **Mechanism**: The method defines "similar states" as the distribution of next states $P(\cdot|s_{t-1})$ reachable from a previous state $s_{t-1}$. By assuming transition dynamics are Lipschitz continuous with respect to bounded noise, the distance between any two reachable states is bounded by $2K_{\xi}\sigma_{\xi}$. This creates a valid region for enforcing a local Lipschitz constraint on the policy, grounding smoothness in actual system dynamics rather than heuristics.
- **Core assumption**: The transition function is $C^1$ (continuously differentiable) and thus locally Lipschitz continuous, and system noise is bounded.
- **Evidence anchors**:
  - [abstract] "transition-induced similar state is defined as the distribution of next states transitioned from the previous state."
  - [section: Method] Lemma 1 proves that under Assumptions 1-2, similar states form a spatially bounded neighborhood.
  - [corpus] General consensus in control theory (e.g., robust control references in paper) supports bounded noise assumptions, though specific corpus papers focus on different smoothing mechanisms like Q-gradient stabilization.
- **Break condition**: In environments with very high observation or transition noise, the bound $2K_{\xi}\sigma_{\xi}$ may become too large, causing the "similar state" neighborhood to lose local relevance, potentially degrading performance.

### Mechanism 2
- **Claim**: Aligning the current action with a predicted expected action from the preceding state reduces spatial oscillation.
- **Mechanism**: A "Prediction Head" ($\pi_P$) is trained to output the expected action given $s_{t-1}$. The loss function includes a spatial term $L_S$ that minimizes the difference between the current action $\pi(s_t)$ and the prediction $\pi_P(s_{t-1})$ (treated as a target). This forces the policy to output consistent actions across the distribution of states reachable from $s_{t-1}$.
- **Core assumption**: The predictor network can converge to an expectation of the policy output over the transition distribution faster than the policy changes.
- **Evidence anchors**:
  - [section: Method] Eq. 14 defines $L_S = ||\pi_\phi(s_t) - \text{stopgrad}(\pi_P(s_{t-1}))||^2_2$.
  - [section: Method] "This mechanism propagates the local Lipschitz property of the transition function... enforcing the local Lipschitz constraint on the actor."
  - [corpus] Weak direct evidence; related papers (e.g., HybridVLA, Diffusion Policy) handle temporal consistency via generative modeling rather than explicit alignment losses.
- **Break condition**: If the "moving target" problem is severe (e.g., in on-policy methods like PPO with few parallel environments), the predictor may fail to track the policy distribution, leading to unstable gradients.

### Mechanism 3
- **Claim**: Penalizing second-order action differences suppresses high-frequency oscillations while allowing lower-frequency maneuvers.
- **Mechanism**: The method adopts a temporal loss $L_T$ that penalizes the second-order difference of actions ($a_{t+1} - 2a_t + a_{t-1}$). Unlike first-order smoothing (velocity), this second-order approach (jerk/acceleration constraint) allows the policy to change direction more fluidly without incurring heavy penalties for steady-state changes, effectively acting as a learnable low-pass filter.
- **Core assumption**: High-frequency control signals manifest as rapid reversals (high second-order derivatives) that are distinct from necessary control dynamics.
- **Evidence anchors**:
  - [abstract] "penalizing second-order differences to suppress high-frequency oscillations."
  - [section: Method] Eq. 16 defines the temporal loss and notes it "enables more flexible action changes while maintaining stability."
  - [corpus] Consistent with broader RL smoothness literature (e.g., Grad-CAPS), which seeks to regularize derivatives to minimize hardware wear.
- **Break condition**: Environments requiring "bang-bang" control or extremely rapid, high-frequency responses may see performance drops if the regularization weight $\lambda_T$ is too high.

## Foundational Learning

- **Concept**: **Lipschitz Continuity**
  - **Why needed here**: The entire theoretical justification for ASAP rests on bounding the Lipschitz constant of the composite function $f \circ T$. You must understand that a lower Lipschitz constant implies the output (action) changes proportionally less than the input (state change), which is the definition of smoothness used here.
  - **Quick check question**: If a function has a Lipschitz constant of 0, what does its output look like? (Answer: Constant/flat).

- **Concept**: **Actor-Critic Methods (PPO/SAC)**
  - **Why needed here**: ASAP is implemented as an auxiliary loss on top of standard PPO (on-policy) and SAC (off-policy) algorithms. Understanding the distinction is vital for the "Training Method" section, particularly how the "moving target" issue affects on-policy learning differently.
  - **Quick check question**: In which algorithm (PPO or SAC) would you expect the "moving target" problem for the predictor to be more severe given fixed batch sizes?

- **Concept**: **Transition Dynamics & Noise Models**
  - **Why needed here**: ASAP defines similar states via the transition function $s_t = T(s_{t-1}, a_{t-1}, \xi)$. Understanding that $\xi$ represents bounded noise is critical for verifying the "Spatially Bounded Neighborhood" claim.
  - **Quick check question**: Does the ASAP method require learning a model of the environment dynamics $T$, or does it use the existing samples from the replay buffer/rollout?

## Architecture Onboarding

- **Component map**: Actor Network -> Shared backbone MLP -> Head A (Action) & Head B (Prediction)
- **Critical path**: The implementation requires access to tuples of $(s_{t-1}, s_t, a_t, a_{t+1})$ (or similar sequences) to compute the spatial and temporal losses. Data collection must store these predecessor states.
- **Design tradeoffs**:
  - **Responsiveness vs. Smoothness**: Increasing $\lambda_S$ reduces oscillation but may clip rapid maneuvers needed for unstable dynamics (e.g., balancing tasks).
  - **Predictor Capacity**: A weak predictor fails to provide a useful target; an overly complex predictor might overfit specific transitions.
- **Failure signatures**:
  - **High Variance Returns**: Observed in the "Lift-Cube" task (Table 4), indicating that smoothing constraints may inhibit exploration in some seeds.
  - **Lagging Response**: If $\lambda_T$ is too high, the agent may react too slowly to sudden environmental changes.
- **First 3 experiments**:
  1.  **Spatial Term Validation (Table 1)**: Train a standard SAC agent on Hopper. Freeze the policy and train *only* the Prediction Head. Then fine-tune the policy with $L_S$ enabled to confirm smoothness improves without reward degradation.
  2.  **Ablation on Temporal Term (Table 7)**: Run PPO on Walker with (a) $L_T$ only, (b) $L_S$ only, and (c) $L_S + L_T$ to quantify the interaction between the Grad-CAPS temporal term and the new ASAP spatial term.
  3.  **Isaac-Lab Reach Task (Table 4)**: Deploy on the Franka Reach task. This is the "stress test" for precision. Measure the mean end-effector error to verify that smoothness aids precision (as claimed: 0.90cm â†’ 0.62cm).

## Open Questions the Paper Calls Out
None

## Limitations
- The spatial regularization relies on bounded noise assumptions, which may not hold in highly stochastic environments.
- The predictor-target architecture introduces additional hyperparameters that require tuning, and performance can be sensitive to these values.
- In tasks requiring rapid, high-frequency responses, the temporal regularization may impose excessive smoothness constraints.
- The "moving target" problem for the predictor in on-policy methods could lead to instability with small batch sizes.

## Confidence

- **High confidence**: The core mechanism of aligning actions with predictions from preceding states (Mechanism 2) is well-supported by the ablation results showing significant improvements in smoothness metrics (up to 89.5% reduction in oscillation) without substantial performance degradation.
- **Medium confidence**: The theoretical justification for transition-induced similar states as a bounded neighborhood (Mechanism 1) is sound under stated assumptions, but the practical robustness to environments with high or unbounded noise is uncertain.
- **Medium confidence**: The second-order temporal regularization (Mechanism 3) effectively suppresses high-frequency oscillations as demonstrated empirically, though the optimal balance with policy responsiveness requires careful tuning.

## Next Checks

1. **Noise Sensitivity Analysis**: Systematically vary the noise level in transition dynamics (e.g., increase $\sigma_\xi$ in the simulator) and measure how the smoothness improvements degrade, testing the bound $2K_\xi\sigma_\xi$.
2. **Predictor Stability Test**: Implement a "delayed predictor update" scheme (e.g., update the predictor only every N policy updates) in PPO to mitigate the moving target problem and measure its impact on training stability and final performance.
3. **Generalization to Unstable Dynamics**: Apply ASAP to a classic unstable control task (e.g., inverted pendulum or balancing cart-pole) and measure if the smoothing constraints prevent the policy from learning the rapid, corrective actions necessary for stabilization.