---
ver: rpa2
title: 'BuildingGym: An open-source toolbox for AI-based building energy management
  using reinforcement learning'
arxiv_id: '2509.11922'
source_url: https://arxiv.org/abs/2509.11922
tags:
- control
- energy
- building
- buildinggym
- cooling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: BuildingGym is an open-source framework that integrates EnergyPlus
  with reinforcement learning to enable flexible, real-time control of building energy
  systems. It addresses the lack of a unified environment for training RL algorithms
  across various building control problems.
---

# BuildingGym: An open-source toolbox for AI-based building energy management using reinforcement learning

## Quick Facts
- arXiv ID: 2509.11922
- Source URL: https://arxiv.org/abs/2509.11922
- Reference count: 18
- Primary result: Open-source framework integrating EnergyPlus with RL for building energy management

## Executive Summary
BuildingGym is an open-source framework that integrates EnergyPlus with reinforcement learning to enable flexible, real-time control of building energy systems. It addresses the lack of a unified environment for training RL algorithms across various building control problems. Key features include acceptance of external signals for grid-interactive applications, support for both system- and room-level control, and easy configuration of RL algorithms and building models. The framework supports on-policy (e.g., PPO, A2C) and off-policy (e.g., DQN, TD3) RL methods. Evaluation on cooling load management tasks showed strong performance, with TD3 achieving median control errors of 0.5% for constant reduction and 1% for dynamic targets. BuildingGym bridges the gap between building managers and AI specialists, offering a standardized platform for developing and testing AI-driven building energy management strategies.

## Method Summary
BuildingGym integrates EnergyPlus simulation with reinforcement learning environments to create a unified framework for building energy management. The system allows users to configure building models, RL algorithms, and control scenarios through a standardized interface. It supports both on-policy methods (PPO, A2C) and off-policy methods (DQN, TD3) and can handle external signals for grid-interactive applications. The framework operates at both system and room levels, enabling flexible deployment scenarios. Building models are specified through EnergyPlus IDF files, while RL configurations are managed through standardized parameter files. The integration provides real-time simulation feedback to the RL agent, enabling iterative learning and optimization of control strategies.

## Key Results
- TD3 algorithm achieved median control errors of 0.5% for constant cooling load reduction
- TD3 algorithm achieved median control errors of 1% for dynamic cooling load target tracking
- Framework successfully demonstrated both on-policy (PPO, A2C) and off-policy (DQN, TD3) RL method compatibility

## Why This Works (Mechanism)
BuildingGym works by creating a standardized interface between EnergyPlus building simulations and reinforcement learning algorithms. The framework translates building control problems into RL environments where agents learn optimal control policies through interaction with the simulated building. By supporting both on-policy and off-policy methods, the framework accommodates different learning approaches suitable for various control scenarios. The integration of external signals enables grid-interactive applications, while the modular design allows easy swapping of building models and RL algorithms without extensive reconfiguration.

## Foundational Learning
- EnergyPlus simulation fundamentals: Understanding EnergyPlus is crucial for building model configuration and interpreting simulation results
- RL algorithm types and their applications: Knowing the differences between on-policy and off-policy methods helps in selecting appropriate algorithms
- Building energy system dynamics: Understanding how HVAC systems respond to control inputs is essential for effective RL training
- Simulation-to-real-world transfer: Knowledge of how simulation results translate to real building performance is critical for practical deployment
- Grid-interactive building concepts: Understanding external signals and their impact on building control is important for advanced applications

## Architecture Onboarding

Component Map:
EnergyPlus (IDF files) -> BuildingGym Environment -> RL Algorithm (PPO/A2C/DQN/TD3) -> Control Actions -> EnergyPlus Simulation -> Reward Signal -> RL Agent

Critical Path:
1. Load building model (IDF) and RL configuration
2. Initialize simulation and RL environment
3. Execute control action and receive state feedback
4. Calculate reward and update RL policy
5. Repeat until convergence or maximum episodes

Design Tradeoffs:
- Simulation accuracy vs. computational efficiency: Higher fidelity simulations provide better training but require more computational resources
- RL algorithm complexity vs. training time: More sophisticated algorithms may achieve better results but require longer training periods
- Model granularity vs. generalizability: Detailed room-level control offers precision but may not transfer well to different building configurations

Failure Signatures:
- Unstable training curves indicating poor reward shaping or learning rate issues
- Simulation convergence failures suggesting incompatible building model configurations
- Suboptimal control performance indicating insufficient exploration or inappropriate algorithm selection

First Experiments:
1. Simple temperature setpoint control using PPO algorithm on a basic building model
2. Constant cooling load reduction using TD3 algorithm with system-level control
3. Dynamic load tracking with external signal integration using A2C algorithm

## Open Questions the Paper Calls Out
None

## Limitations
- Long-term generalizability of RL models to real-world building systems remains uncertain
- Framework effectiveness for non-cooling energy management tasks (heating, lighting) not tested
- Impact of external signals under varying grid conditions not fully explored

## Confidence
- Confidence in framework's ability to support diverse RL algorithms and building models: **High**
- Confidence in evaluation results for cooling load management: **Medium**
- Confidence in framework's scalability and real-world applicability: **Low**

## Next Checks
1. Test BuildingGym on additional energy management tasks (e.g., heating, lighting) to assess its versatility
2. Validate the framework's performance with real-world building data to evaluate its practical applicability
3. Conduct long-term simulations to assess the robustness of RL models under varying operational and environmental conditions