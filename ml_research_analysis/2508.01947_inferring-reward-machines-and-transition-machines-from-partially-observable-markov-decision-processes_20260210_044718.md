---
ver: rpa2
title: Inferring Reward Machines and Transition Machines from Partially Observable
  Markov Decision Processes
arxiv_id: '2508.01947'
source_url: https://arxiv.org/abs/2508.01947
tags:
- state
- reward
- learning
- states
- observation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of learning policies in Partially
  Observable Markov Decision Processes (POMDPs) where non-Markovian observations make
  standard reinforcement learning difficult. The authors identify that POMDPs exhibit
  two types of non-Markovian behavior: transition dependencies and reward dependencies,
  which current approaches like Reward Machines (RMs) only partially address.'
---

# Inferring Reward Machines and Transition Machines from Partially Observable Markov Decision Processes

## Quick Facts
- arXiv ID: 2508.01947
- Source URL: https://arxiv.org/abs/2508.01947
- Reference count: 40
- Primary result: Achieves up to three orders of magnitude speedup over baselines on POMDP automata inference

## Executive Summary
This paper addresses the challenge of learning policies in Partially Observable Markov Decision Processes (POMDPs) where non-Markovian observations make standard reinforcement learning difficult. The authors identify that POMDPs exhibit two types of non-Markovian behavior: transition dependencies and reward dependencies, which current approaches like Reward Machines (RMs) only partially address. To tackle this, they introduce Transition Machines (TMs) to explicitly model transition dependencies, complementing RMs, and propose a unified framework called Dual Behavior Mealy Machine (DBMM) that subsumes both.

## Method Summary
The authors develop DB-RPNI, an efficient passive automata learning algorithm that infers DBMMs without costly reductions to ILP or HMM problems. The pipeline involves preprocessing traces to remove redundant inputs, inferring a Transition Machine to model history-based transitions, augmenting observations with TM states, and then inferring a Reward Machine on the augmented traces. This approach enables standard Q-learning agents to learn optimal policies in environments where the Markov property is broken due to partial observability.

## Key Results
- Achieves speedups of up to three orders of magnitude over state-of-the-art baselines on benchmark environments
- Successfully infers automata on a 25Ã—25 grid-world that enable Q-learning agents to learn optimal policies
- Demonstrates computational efficiency and scalability while producing compact, interpretable automata
- Ablation studies show observation supplementation prevents reward machine state explosion in low-data settings

## Why This Works (Mechanism)

### Mechanism 1: Dual-Dependency Decoupling
The framework introduces Transition Machines (TMs) to explicitly model history-based transition dynamics, separate from Reward Machines (RMs) which model reward logic. By decoupling these, the system avoids the "unnatural problem formulations" required when forcing all dependencies into a single RM structure.

### Mechanism 2: Direct Inference via DB-RPNI
The DB-RPNI algorithm adapts the classical state-merging RPNI technique, constructing a Prefix Tree Transducer from traces and iteratively merging states based on local compatibility. This runs in $O(|U| \cdot |L| \cdot T \cdot F)$ time under structure completeness, versus the exponential or cubic worst-cases of baselines.

### Mechanism 3: Observation Supplementation
Once a TM is inferred, the algorithm creates an augmented observation $o' = (o, q_{TM})$. This mapping restores the Markov property regarding transitions, ensuring that the RM only needs to model reward dependencies, preventing "Transition-Reward Interference" where a transition failure looks like a non-Markovian reward event.

## Foundational Learning

- **Concept: Reward Machines (RMs)**
  - Why needed: The paper positions its contribution (Transition Machines) as a necessary complement to RMs. Understanding that RMs are finite-state automata defining state-dependent reward functions is prerequisite.
  - Quick check question: Can you explain why a standard Markov Decision Process (MDP) does not require a Reward Machine?

- **Concept: The Markov Property in POMDPs**
  - Why needed: The core problem is the breakdown of the Markov property (history independence) in partial observability. The paper aims to "restore" this property.
  - Quick check question: In a grid world where a key is invisible until picked up, why does the observation "Empty Room" fail the Markov property?

- **Concept: Passive Automata Learning (RPNI)**
  - Why needed: The proposed DB-RPNI algorithm is an adaptation of the RPNI (Regular Positive and Negative Inference) algorithm.
  - Quick check question: What is the primary trade-off between "active learning" (querying an oracle) and "passive learning" (fixed dataset) in this context?

## Architecture Onboarding

- **Component map:** Preprocessor -> TM Learner (DB-RPNI) -> Observation Supplementor -> RM Learner (DB-RPNI) -> Agent
- **Critical path:** The TM Learner -> Observation Supplementor -> RM Learner sequence is strict. You cannot infer the minimal RM correctly without first resolving transition dependencies using the TM.
- **Design tradeoffs:**
  - Efficiency vs. Data Density: The system is highly efficient ($O(|U| \cdot |L| \cdot T \cdot F)$) but relies on *structure completeness*. If data is scarce, the state-merging logic may fail or over-generalize.
  - Det-POMDP Limitation: The current architecture is restricted to deterministic POMDPs (Det-POMDPs) and does not natively handle stochastic transitions.
- **Failure signatures:**
  - State Explosion: If the inferred RM has an unexpectedly high state count (e.g., >100 states for a simple task), check that **Observation Supplement** is enabled and the TM is correct.
  - Timeout during Inference: Check that **Trivial $\beta$-Input Removal** is active; the paper notes this causes >15x slowdowns if disabled.
  - Non-convergence in Policy Learning: If the Q-learning agent fails to converge, the inferred automata may not be "resolvent" (likely due to insufficient training traces).
- **First 3 experiments:**
  1. Run the DB-RPNI pipeline on the 3x3 grid environment described in the paper. Verify that inference time is <2 seconds and compare the inferred automata structure against the ground truth.
  2. Run the full pipeline on the 25x25 grid with "Low Data" (1,000 traces), then re-run with Observation Supplement disabled. Confirm the RM state count increases significantly (e.g., from ~7 to ~200+).
  3. Integrate the inferred TM and RM into a standard Q-learning agent. Train on the 25x25 grid and plot the reward curve to verify convergence to the optimal policy, ensuring the Markov property was effectively restored.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the DBMM framework and inference algorithm be extended to stochastic environments?
  - Basis in paper: The Conclusion explicitly states the method is restricted to deterministic POMDPs and identifies extending it to stochastic environments using probabilistic automata as a direction for future work.
  - Why unresolved: The current DBMM definition and DB-RPNI algorithm rely on deterministic transition and output functions, which cannot model the probabilistic dynamics inherent in general POMDPs.
  - What evidence would resolve it: A formulation of probabilistic DBMMs and a modified inference algorithm capable of learning from stochastic traces while providing theoretical convergence guarantees.

- **Open Question 2:** How can the inference method be adapted to handle incomplete or noisy trace data?
  - Basis in paper: The Conclusion highlights that the algorithm relies on high-quality data and explicitly calls for future work on "inferring from incomplete data" to improve practical applicability.
  - Why unresolved: The DB-RPNI algorithm requires strict local compatibility for state merging; missing transitions or noise would violate these determinism assumptions, likely leading to failed or incorrect inference.
  - What evidence would resolve it: A robust version of the algorithm that successfully infers correct automata from datasets lacking full structure completeness or containing observation errors.

- **Open Question 3:** Can an active learning approach be utilized to satisfy the "structure completeness" condition efficiently?
  - Basis in paper: The theoretical guarantees (Theorem 1) depend on the "structure completeness" of the sample set. While the authors focus on the passive setting, actively ensuring this property could improve efficiency.
  - Why unresolved: The current method is passive and assumes a fixed dataset; there is no mechanism to actively query the environment to resolve ambiguities or conflicts that prevent state merging.
  - What evidence would resolve it: An active learning variant that generates targeted queries to distinguish ambiguous states, achieving minimality with lower sample complexity than random exploration.

## Limitations
- The approach is restricted to deterministic POMDPs (Det-POMDPs) and does not handle stochastic transitions
- Relies heavily on "structure completeness" of trace datasets, which may not hold in real-world scenarios with sparse or noisy data
- The decoupling assumption for transition and reward dependencies may fail in environments with complex, inseparable non-Markovian behavior

## Confidence
- **High Confidence**: The computational efficiency gains (up to three orders of magnitude) and scalability to large grid-worlds are well-supported by experimental results
- **Medium Confidence**: The claim that decoupling transition and reward dependencies produces more interpretable automata is supported by examples but could benefit from more diverse environment testing
- **Low Confidence**: The method's performance on stochastic POMDPs or environments with highly intertwined transition-reward dependencies is unknown

## Next Checks
1. **Robustness to Incomplete Data**: Test the DB-RPNI algorithm on trace datasets that are intentionally incomplete or noisy to quantify the failure threshold and identify warning signs of incorrect automata inference
2. **Stochastic POMDP Extension**: Modify the DBMM framework to handle stochastic transitions and evaluate performance on a benchmark stochastic POMDP environment, documenting any architectural changes required
3. **Dependency Coupling Analysis**: Design a synthetic POMDP where transition and reward dependencies are fundamentally inseparable, and test whether the TM-RM decoupling approach fails or produces suboptimal automata, documenting the specific failure mode