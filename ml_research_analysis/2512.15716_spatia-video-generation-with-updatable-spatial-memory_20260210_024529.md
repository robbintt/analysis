---
ver: rpa2
title: 'Spatia: Video Generation with Updatable Spatial Memory'
arxiv_id: '2512.15716'
source_url: https://arxiv.org/abs/2512.15716
tags:
- arxiv
- video
- generation
- scene
- spatial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Spatia addresses the challenge of long-horizon video generation
  with spatial and temporal consistency by introducing an explicit 3D scene point
  cloud as persistent spatial memory. The method iteratively generates video clips
  conditioned on this spatial memory and updates it using visual SLAM, enabling dynamic-static
  disentanglement while preserving geometric coherence.
---

# Spatia: Video Generation with Updatable Spatial Memory

## Quick Facts
- arXiv ID: 2512.15716
- Source URL: https://arxiv.org/abs/2512.15716
- Reference count: 40
- Primary result: Achieves 69.73 WorldScore average and 19.38 PSNR in closed-loop video generation with explicit 3D spatial memory

## Executive Summary
Spatia introduces an explicit 3D scene point cloud as persistent spatial memory for long-horizon video generation, addressing the challenge of maintaining spatial and temporal consistency. The method iteratively generates video clips conditioned on this spatial memory and updates it using visual SLAM, enabling dynamic-static disentanglement while preserving geometric coherence. This approach outperforms both static scene models and foundation video generation models in visual quality metrics while supporting explicit camera control and 3D-aware interactive editing through direct manipulation of the 3D spatial memory.

## Method Summary
The core innovation lies in using an explicit 3D scene point cloud as persistent spatial memory that stores scene geometry and appearance across time steps. During generation, Spatia first predicts the next frame conditioned on this spatial memory, then updates the memory using visual SLAM from the generated frame to the previous memory. This iterative process allows the model to maintain spatial consistency across long video sequences while handling dynamic elements through the updatable memory mechanism. The explicit 3D representation enables camera control and interactive editing capabilities that are not available in traditional video generation approaches.

## Key Results
- Achieves 69.73 WorldScore average, outperforming foundation video generation models
- 19.38 PSNR in closed-loop generation demonstrates strong spatial consistency preservation
- Enables explicit camera control and 3D-aware interactive editing through direct manipulation of spatial memory

## Why This Works (Mechanism)
The method works by maintaining persistent spatial information through an explicit 3D point cloud that serves as a memory buffer across generation steps. Visual SLAM updates this memory after each frame generation, ensuring geometric consistency while allowing dynamic elements to be handled through the iterative update process. The explicit 3D representation decouples spatial reasoning from temporal generation, enabling better control over both geometry and motion compared to implicit approaches.

## Foundational Learning
- **3D scene representation**: Stores geometry and appearance as point clouds for persistent memory across time steps. Needed for maintaining spatial consistency in long videos. Quick check: Verify point cloud density and coverage across generated frames.
- **Visual SLAM integration**: Updates spatial memory using camera pose estimation between consecutive frames. Required for accurate geometric alignment. Quick check: Measure SLAM accuracy on generated frames versus ground truth.
- **Dynamic-static disentanglement**: Separates moving objects from static scene geometry in the memory representation. Essential for handling motion while preserving background consistency. Quick check: Analyze static element preservation across long sequences.

## Architecture Onboarding

Component map: Video Generation Model -> Visual SLAM Module -> 3D Spatial Memory -> Next Frame Prediction

Critical path: The generation loop follows the sequence: (1) Condition frame generation on 3D spatial memory, (2) Update memory using visual SLAM from generated frame, (3) Repeat for next frame. This creates a feedback loop where spatial consistency is maintained through explicit geometric tracking.

Design tradeoffs: The explicit 3D memory approach trades computational complexity for geometric accuracy compared to implicit methods. While providing superior spatial consistency and camera control, it requires additional processing for SLAM updates and point cloud maintenance, potentially limiting real-time applications.

Failure signatures: Performance degradation in textureless regions due to SLAM accuracy loss, accumulation of geometric inconsistencies over very long sequences, and challenges with highly dynamic scenes featuring significant occlusions or non-rigid deformations.

First experiments:
1. Generate a short video sequence (5-10 frames) with known camera trajectory to verify spatial memory consistency
2. Test SLAM update accuracy by comparing generated frame geometry against ground truth camera poses
3. Evaluate 3D-aware editing capabilities by modifying specific points in the spatial memory and observing frame generation changes

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Performance uncertainty in highly dynamic scenes with significant occlusions or non-rigid deformations
- Potential failure modes in textureless regions or scenes with ambiguous geometric features
- Limited validation of long-term consistency in extended duration sequences

## Confidence
- Visual quality and quantitative metrics: High
- Spatial consistency claims: Medium (based on controlled experimental conditions)
- Long-horizon generation robustness: Low (limited testing on extended sequences)
- 3D-aware editing generalizability: Medium (demonstrated on limited scenarios)

## Next Checks
1. **Extended duration testing**: Evaluate closed-loop generation on sequences exceeding 10 seconds to identify potential accumulation of inconsistencies or memory degradation over time.

2. **Cross-domain robustness**: Test the framework on scenes with varying levels of texture detail, dynamic range, and object complexity to assess performance boundaries.

3. **Comparative perceptual study**: Conduct human evaluation studies comparing Spatia against baseline methods across multiple dimensions including temporal coherence, spatial consistency, and visual fidelity to complement quantitative metrics.