---
ver: rpa2
title: Evaluating the Role of Large Language Models in Legal Practice in India
arxiv_id: '2508.09713'
source_url: https://arxiv.org/abs/2508.09713
tags:
- legal
- human
- llms
- tasks
- were
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study evaluates the performance of large language models
  (LLMs) in Indian legal tasks. Using a survey experiment with 50 advanced law students,
  outputs from five LLMs (GPT-4, Claude 3, ChatGPT 3.5, Gemini, Llama 2) and a junior
  lawyer were compared across five tasks: issue spotting, legal drafting, advice,
  research, and reasoning.'
---

# Evaluating the Role of Large Language Models in Legal Practice in India

## Quick Facts
- arXiv ID: 2508.09713
- Source URL: https://arxiv.org/abs/2508.09713
- Reference count: 6
- LLMs generally performed as well as or better than junior lawyers in most legal tasks

## Executive Summary
This study evaluates the performance of large language models (LLMs) in Indian legal tasks through a survey experiment with 50 advanced law students. Five LLMs (GPT-4, Claude 3, ChatGPT 3.5, Gemini, Llama 2) and a junior lawyer were compared across five tasks: issue spotting, legal drafting, advice, research, and reasoning. The study found that LLMs performed as well as or better than the human expert in most tasks, particularly excelling in drafting and reasoning, but struggled significantly with legal research due to hallucinations. Claude 3 and GPT-4 emerged as top performers, while Llama 2 performed poorly. The research concludes that while LLMs can augment certain legal tasks, human expertise remains essential for nuanced reasoning and accurate legal research.

## Method Summary
The study employed a survey experiment design using 50 advanced law students as participants. Five LLMs (GPT-4, Claude 3, ChatGPT 3.5, Gemini, Llama 2) and outputs from a junior lawyer were compared across five legal tasks: issue spotting, legal drafting, advice, research, and reasoning. Participants evaluated the outputs using standardized criteria, with performance measured against accuracy, completeness, and quality metrics. The experimental design aimed to simulate real-world legal practice scenarios while maintaining controlled evaluation conditions.

## Key Results
- LLMs performed as well as or better than junior lawyers in legal drafting and reasoning tasks
- Claude 3 and GPT-4 emerged as top-performing models across multiple tasks
- LLMs struggled significantly with legal research due to hallucinations and accuracy issues
- Llama 2 consistently underperformed compared to other models tested

## Why This Works (Mechanism)
The study demonstrates that LLMs can effectively process and generate legal text due to their training on diverse legal corpora and ability to recognize patterns in legal reasoning. The models' performance in drafting and reasoning tasks suggests they can identify relevant legal principles and apply them appropriately. However, their limitations in research tasks highlight the challenge of maintaining factual accuracy when retrieving and synthesizing information from legal databases, where hallucinations can lead to fabricated case citations or incorrect legal precedents.

## Foundational Learning

**Legal Task Classification**
- Why needed: Different legal tasks require distinct cognitive processes and expertise
- Quick check: Can the model distinguish between advisory and drafting tasks?

**Legal Research Methodology**
- Why needed: Understanding how legal professionals locate and verify precedents
- Quick check: Does the model cite authentic legal sources?

**Hallucination Detection**
- Why needed: LLMs may generate plausible but incorrect legal information
- Quick check: Can the model distinguish between actual cases and fabricated ones?

## Architecture Onboarding

**Component Map**
User Input -> LLM Processing -> Task-Specific Output -> Evaluation Criteria -> Performance Assessment

**Critical Path**
Task Selection → LLM Selection → Input Generation → Output Production → Evaluation → Performance Analysis

**Design Tradeoffs**
- Model size vs. response accuracy: Larger models generally performed better but required more resources
- Task specificity vs. generalization: Specialized legal models vs. general-purpose LLMs
- Speed vs. accuracy: Faster responses sometimes sacrificed detail and precision

**Failure Signatures**
- Legal research hallucinations: Fabricated case citations or non-existent legal precedents
- Overgeneralization: Applying broad legal principles inappropriately to specific cases
- Inconsistent reasoning: Contradictory legal arguments within the same response

**3 First Experiments**
1. Test each LLM on a standardized set of authentic Indian legal cases to measure hallucination rates
2. Compare response times and accuracy across different model sizes for identical legal queries
3. Evaluate the models' ability to cite and reference actual Indian legal statutes and precedents

## Open Questions the Paper Calls Out
None

## Limitations
- Small sample size of 50 participants limits statistical power
- Law students used instead of practicing attorneys may not reflect professional expertise
- Narrow focus on five specific legal tasks may limit generalizability
- No evaluation of real-world implementation challenges or ethical considerations

## Confidence

**High Confidence**
- LLMs' superior performance in legal drafting and reasoning tasks

**Medium Confidence**
- LLMs' comparative performance in issue spotting and legal advice

**Low Confidence**
- Generalizability of findings to all legal domains and professional practice

## Next Checks
1. Replicate the study with practicing attorneys across different legal specializations and seniority levels to validate performance comparisons
2. Expand testing to include a broader range of legal tasks and LLM models, including newer versions and specialized legal AI tools
3. Conduct a longitudinal study tracking LLM performance over time and their integration into actual legal workflows, including cost-benefit analysis and client satisfaction metrics