---
ver: rpa2
title: Relative Positioning Based Code Chunking Method For Rich Context Retrieval
  In Repository Level Code Completion Task With Code Language Model
arxiv_id: '2510.08610'
source_url: https://arxiv.org/abs/2510.08610
tags:
- code
- chunks
- context
- completion
- similar
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of effective context collection
  for repository-level code completion using large language models. The proposed method
  preprocesses code repositories into smaller chunks and retrieves relevant chunks
  based on syntactic and semantic similarity, enhanced with relative positioning (next
  chunks for prefix context, previous chunks for suffix context).
---

# Relative Positioning Based Code Chunking Method For Rich Context Retrieval In Repository Level Code Completion Task With Code Language Model

## Quick Facts
- arXiv ID: 2510.08610
- Source URL: https://arxiv.org/abs/2510.08610
- Authors: Imranur Rahman; Md Rayhanur Rahman
- Reference count: 17
- Achieved chrF score of 0.660 for Kotlin and 0.636 for Python in repository-level code completion

## Executive Summary
This paper addresses the challenge of effective context collection for repository-level code completion using large language models. The proposed method preprocesses code repositories into smaller, syntactically coherent chunks and retrieves relevant chunks based on syntactic and semantic similarity. The approach is enhanced with relative positioning - using next chunks for prefix context and previous chunks for suffix context - to provide coherent contextual information to the LLM. Evaluated on Kotlin and Python code completion tasks, the method demonstrated significant improvements, achieving third place in the Kotlin competition and fourth place in the Python competition.

## Method Summary
The proposed method tackles repository-level code completion by first preprocessing code repositories into smaller chunks that preserve syntactic and semantic coherence. These chunks are then indexed and retrieved based on similarity metrics that consider both syntactic structure and semantic meaning. The key innovation is the use of relative positioning, where chunks preceding the target code provide prefix context while subsequent chunks provide suffix context. This positioning strategy helps maintain the logical flow of code during completion tasks. The system processes the retrieved chunks and feeds them to a code language model for generating contextually appropriate completions.

## Key Results
- Achieved chrF score of 0.660 for Kotlin code completion (third place in competition)
- Achieved chrF score of 0.636 for Python code completion (fourth place in competition)
- Demonstrated that code chunking and relative positioning significantly enhance code completion performance compared to baseline approaches

## Why This Works (Mechanism)
The method works by addressing a fundamental challenge in repository-level code completion: how to provide relevant, contextual information to the language model without overwhelming it with irrelevant code. By preprocessing repositories into smaller, semantically coherent chunks, the system can more precisely retrieve context that is directly relevant to the code being completed. The relative positioning strategy ensures that the retrieved chunks maintain logical continuity with the target code, providing the LLM with a more coherent understanding of the code's context. This combination of precise retrieval and contextual coherence enables the model to generate more accurate completions that align with the existing code structure and patterns.

## Foundational Learning
- Code Chunking: Breaking code into smaller, syntactically coherent units - needed to enable precise context retrieval and reduce noise from irrelevant code segments
- Relative Positioning: Using positional relationships (previous/next chunks) for context retrieval - needed to maintain logical flow and coherence in the completion task
- Semantic Similarity Metrics: Measuring code similarity beyond syntactic structure - needed to capture functional equivalence and intent across different code implementations
- chrF Score: Character-level n-gram F-score for code evaluation - needed to measure completion accuracy at the character level, capturing both precision and recall

## Architecture Onboarding

**Component Map**
Repository Preprocessor -> Chunk Indexer -> Similarity Retriever -> Context Assembler -> Code Language Model -> Completion Generator

**Critical Path**
Code preprocessing and chunking → Chunk indexing and storage → Query processing and similarity matching → Context assembly with relative positioning → Language model inference → Completion generation and evaluation

**Design Tradeoffs**
- Smaller chunks provide more precise context but may lose broader semantic relationships
- Larger chunks preserve more context but reduce retrieval precision and increase computational overhead
- Relative positioning adds coherence but may introduce positional bias in certain code patterns
- Character-level evaluation (chrF) captures fine-grained accuracy but may miss semantic correctness

**Failure Signatures**
- Low retrieval precision: chunks contain irrelevant code, reducing completion quality
- Position misalignment: retrieved chunks don't logically connect to target code
- Semantic mismatch: syntactically similar but functionally different code is retrieved
- Chunk boundary issues: important contextual information split across chunks

**3 First Experiments**
1. Test retrieval precision with different chunk sizes (50, 100, 200 lines) on sample repositories
2. Evaluate impact of relative positioning by comparing with absolute positioning baseline
3. Measure chrF score sensitivity to different similarity threshold values

## Open Questions the Paper Calls Out
None

## Limitations
- Narrow evaluation scope focused only on Kotlin and Python, limiting generalizability to other languages
- chrF scores may not fully capture semantic correctness or functional equivalence of generated code
- Chunking methodology may not optimally handle large monolithic functions or highly interdependent code segments
- Computational efficiency and memory requirements for large-scale preprocessing and retrieval are not addressed

## Confidence
High: The core methodology of code chunking combined with relative positioning for context retrieval is sound and well-implemented, with robust empirical results within tested languages.

Medium: Generalizability of the approach to other programming languages and code structures beyond Kotlin and Python.

Medium: Long-term sustainability in real-world development environments considering computational overhead and integration challenges.

## Next Checks
1. Cross-language validation: Evaluate the method on at least three additional programming languages (Java, JavaScript, C++) to assess generalizability across different language paradigms.

2. Chunk size sensitivity analysis: Systematically vary chunk sizes and analyze impact on retrieval accuracy and code completion quality to identify optimal chunk boundaries.

3. Computational overhead benchmarking: Measure preprocessing time, memory usage, and inference latency for repositories of varying sizes (small: <100 files, medium: 100-1000 files, large: >1000 files) to quantify practical deployment constraints.