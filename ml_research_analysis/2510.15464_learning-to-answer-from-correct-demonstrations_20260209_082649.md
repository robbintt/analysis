---
ver: rpa2
title: Learning to Answer from Correct Demonstrations
arxiv_id: '2510.15464'
source_url: https://arxiv.org/abs/2510.15464
tags:
- class
- learning
- policy
- theorem
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses learning to answer questions from correct demonstrations
  in contextual bandits, where multiple correct answers may exist. Prior approaches
  based on maximum likelihood estimation (MLE) require low-complexity policy classes,
  which is a strong assumption.
---

# Learning to Answer from Correct Demonstrations

## Quick Facts
- arXiv ID: 2510.15464
- Source URL: https://arxiv.org/abs/2510.15464
- Reference count: 40
- Primary result: Achieves logarithmic sample complexity in cardinality of reward model class for learning to answer from demonstrations with multiple correct answers

## Executive Summary
This paper addresses the problem of learning to answer questions from demonstrations in contextual bandits, where multiple correct answers may exist. The key insight is that traditional maximum likelihood estimation (MLE) approaches fail in this setting because they require modeling the demonstrator's policy distribution, which can be complex even when the set of correct answers is simple. Instead, the authors propose relying on a low-cardinality reward model class that specifies which answers are correct, achieving optimal logarithmic sample complexity through a novel online learning algorithm.

## Method Summary
The approach relies on maintaining a weight distribution over a finite set of possible reward models (defining which answers are correct). In each round, the algorithm predicts the answer with the highest total weight from models that deem it correct. When a demonstration is received, the algorithm up-weights models that supported the demonstration but would have rejected the algorithm's own prediction. This creates an adaptive mechanism that identifies which models best predict correctness. The method extends to general bounded rewards and pass@k objectives, and handles suboptimal demonstrators through softer update rules.

## Key Results
- Proves that MLE can fail to generalize even with infinite data when multiple correct answers exist
- Achieves optimal logarithmic sample complexity in the cardinality of the reward model class
- Demonstrates extension to suboptimal demonstrators and general bounded rewards
- Shows applicability to pass@k objectives beyond simple correctness

## Why This Works (Mechanism)

### Mechanism 1: Adaptive Weighting over Reward Hypotheses
The algorithm maintains a weight for every possible reward model in the class. When a prediction is made and a demonstration is received, it actively up-weights models that supported the demonstration but would have rejected the algorithm's prediction, effectively amplifying the signal of models that "know better."

### Mechanism 2: Decoupling Reward Maximization from Distribution Matching
By optimizing for high reward (getting any correct answer) rather than distributional similarity, the approach bypasses MLE's failure mode. It treats success as generating any single correct answer, ignoring the specific distribution the demonstrator used to select among correct options.

### Mechanism 3: Mistake-Unaware Online-to-Batch Conversion
The learner updates its state based on the discrepancy between what it would have done and what was demonstrated, without direct feedback on its own errors. The online-to-batch conversion then averages policies to produce a final stochastic policy with bounded expected error.

## Foundational Learning

- **Contextual Bandits:**
  - Why needed: Frames question-answering as a single-step decision problem where the prompt is context and answer is action
  - Quick check: Can you explain why the paper treats the entire answer generation as a single action rather than a sequence of token actions?

- **Realizability and Hypothesis Classes:**
  - Why needed: The entire argument hinges on "Model Class Assumption" vs. "Policy Class Assumption"
  - Quick check: What is the difference between a Policy Class (Î ) and a Reward/Model Class (S) in this context?

- **Generalization in Offline Learning:**
  - Why needed: Addresses why standard MLE fails to generalize to unseen contexts when multiple answers exist
  - Quick check: Why does MLE fail to generalize on unseen contexts in the "Multiple Correct Answers" setup?

## Architecture Onboarding

- **Component map:** Model Class S -> Weight Vector w -> Predictor -> Answer
- **Critical path:**
  1. Initialize uniform weights over S
  2. Forward Pass: Calculate weighted support for every possible answer
  3. Update Pass: Zero out weights for models rejecting demonstration, double weights for models accepting demonstration but rejecting prediction
  4. Batch Policy Extraction: Average prediction rules from all training steps

- **Design tradeoffs:**
  - Sample Efficiency vs. Runtime: Optimal sample complexity but computationally expensive for large S
  - Reward Clarity vs. Policy Modeling: Excels with clear validity criteria but fails for soft stylistic nuances

- **Failure signatures:**
  - Collapse to 0 weights with noisy demonstrators
  - Memorization on finite small context spaces
  - Action Space Explosion for continuous generation

- **First 3 experiments:**
  1. Reproduce Theorem 1: Show MLE fails on synthetic dataset with two correct answers per context
  2. Verify Logarithmic Scaling: Plot sample size vs |S| for synthetic model classes
  3. Pass@k Implementation: Implement Algorithm 3 for code generation and measure pass rate improvement

## Open Questions the Paper Calls Out

- Can sample complexity guarantees be extended to infinite continuous classes like transformers?
- Can we obtain computationally tractable methods for learning from correct demonstrations?
- Is optimal O(log|S|) sample complexity achievable with proper or deterministic policies?
- How do these results extend to general Markov Decision Processes (MDPs)?

## Limitations

- Assumes finite, known model class which may not hold in practice
- Computational complexity scales with |S|, limiting scalability
- Requires access to correct demonstrations, making it sensitive to demonstrator quality
- Extension to continuous or parametric model classes remains open

## Confidence

- **High Confidence:** Theoretical analysis of mistake-bound algorithm and logarithmic sample complexity
- **Medium Confidence:** Claim that MLE fails in multi-answer settings (valid but may be somewhat artificial)
- **Medium Confidence:** Extension to suboptimal demonstrators (weaker theoretical guarantees)

## Next Checks

1. **Empirical MLE Failure Test:** Construct synthetic dataset matching Theorem 1 setup and measure whether standard MLE fails while proposed learner succeeds
2. **Logarithmic Scaling Verification:** Run algorithm with varying synthetic model classes and confirm logarithmic scaling empirically
3. **Suboptimal Demonstrator Test:** Implement softer update rules from Section 6 and measure performance degradation with noisy demonstrations