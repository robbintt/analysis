---
ver: rpa2
title: Evaluating and Achieving Controllable Code Completion in Code LLM
arxiv_id: '2601.15879'
source_url: https://arxiv.org/abs/2601.15879
tags:
- code
- completion
- qwen2
- arxiv
- implementation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces C3-Bench, the first instruction-guided benchmark
  for evaluating controllable code completion in large language models. The benchmark
  addresses limitations in existing evaluations by testing both functional correctness
  and instruction-following capabilities through 2,195 carefully designed tasks.
---

# Evaluating and Achieving Controllable Code Completion in Code LLM

## Quick Facts
- arXiv ID: 2601.15879
- Source URL: https://arxiv.org/abs/2601.15879
- Reference count: 40
- Primary result: Introduces C3-Bench, the first instruction-guided benchmark for controllable code completion, revealing significant instruction-following gaps between open-source and proprietary models.

## Executive Summary
This paper addresses a critical gap in code completion evaluation by introducing C3-Bench, the first benchmark specifically designed to test both functional correctness and instruction-following capabilities in large language models. The benchmark reveals that while models can generate working code, they often fail to follow specific implementation instructions, particularly open-source models. To address this, the authors develop a data synthesis pipeline that generates high-quality instruction-completion pairs from existing code, resulting in Qwen2.5-Coder-C3 which achieves state-of-the-art performance on the benchmark while maintaining competitive results on conventional benchmarks.

## Method Summary
The study introduces C3-Bench with 2,195 Python tasks, featuring Implementation-Control Completion (ICC) and Scale-Control Completion (SCC) tasks. Training data synthesis uses a two-phase pipeline: seed generation with Claude3.5-Sonnet for 1,000 high-quality pairs, followed by automated large-scale synthesis using Qwen2.5-Coder-32B-Instruct as both generator and validator. The fine-tuning process uses Qwen2.5-Coder-1.5B/32B with Adam optimizer, 64× A100-80GB GPUs, and 4K token truncation. Evaluation employs Pass@1 (functional correctness via unit tests), Instruction-Following Rate (IF - LLM-judged adherence), and Edit Similarity metrics.

## Key Results
- C3-Bench reveals substantial instruction-following gaps between open-source and proprietary models, with top models achieving only 47.6% IF on ICC tasks
- Qwen2.5-Coder-C3 achieves state-of-the-art performance on C3-Bench through simple data synthesis
- Removing instructions from prompts leads to significant IF degradation while Pass@1 remains largely unchanged
- Advanced LLMs including Gemini, DeepSeek-V3, and GPT-4o series struggle with scale-control tasks despite success on implementation-control tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Instruction-guided benchmarks expose capability gaps that conventional functional-correctness-only benchmarks miss
- Mechanism: Traditional benchmarks evaluate whether code executes correctly; C3-Bench adds a second constraint—whether the implementation approach matches fine-grained instructions. This dual evaluation reveals that models can generate working code while failing to follow specific requirements
- Core assumption: Models may overfit to pass unit tests without learning to follow implementation guidance, which is common in real-world IDE usage
- Evidence anchors: [abstract] "reveal substantial gaps in instruction-following capabilities between open-source and advanced proprietary models during code completion tasks"; [section 4.3] "removing instructions from query prompts leads to significant degradation in IF while Pass@1 remain largely unchanged"

### Mechanism 2
- Claim: Synthesized instruction-completion pairs from existing code can bootstrap instruction-following capabilities
- Mechanism: A two-phase pipeline—(1) seed generation with Claude3.5-Sonnet for 1,000 high-quality pairs, (2) automated large-scale synthesis using Qwen2.5-Coder-32B-Instruct as both generator and validator. Middle code extraction via AST manipulation creates diverse completion scenarios
- Core assumption: The synthesizing model's instruction-following capability transfers to the fine-tuned model without propagating systematic errors
- Evidence anchors: [section 3.1] "Using the seed examples as few-shot demonstrations, we employ Qwen2.5-Coder-32B-Instruct for automated middle code extraction and instruction generation"; [section 4.3] "Qwen2.5-Coder-32B-C3 achieves state-of-the-art performance on C3-Bench, demonstrating substantial improvements"

### Mechanism 3
- Claim: LLM-based semantic validation can reliably assess instruction adherence when functional correctness is already established
- Mechanism: After filtering for Pass@1 (functional correctness), Claude3.5-Sonnet judges whether implementations match instruction-specified approaches. This achieves 98% agreement with human experts across 10 assessment rounds, enabling scalable evaluation
- Core assumption: LLM judges accurately capture semantic equivalence between implementation strategies without being fooled by superficial syntactic differences
- Evidence anchors: [section 2.4] "achieving 98% agreement with senior Python developers across 10 independent assessment rounds"; [figure 8] Judgment prompt explicitly requires checking instruction adherence AND ground truth alignment

## Foundational Learning

- Concept: Fill-In-the-Middle (FIM) task format
  - Why needed here: C3-Bench builds on code completion tasks where models must generate middle code given prefix and suffix context; understanding FIM special tokens (e.g., `<|fim_prefix|>`, `<|fim_suffix|>`) is essential for correctly formatting inputs
  - Quick check question: Given prefix `def add(a, b):` and suffix `return result`, can you construct the FIM prompt format for a model supporting special tokens?

- Concept: Abstract Syntax Tree (AST) manipulation
  - Why needed here: The benchmark construction extracts logically complete code blocks via AST parsing using tree-sitter; understanding node types and masking operations is critical for reproducing or extending the dataset
  - Quick check question: What AST node type would represent a `for` loop body, and how would masking this node differ from masking 3-5 consecutive lines?

- Concept: Instruction-Following Rate (IF) vs. Pass@1 metrics
  - Why needed here: These are the two core evaluation axes; IF only applies to functionally correct completions, creating a hierarchical evaluation where correctness is prerequisite to controllability assessment
  - Quick check question: If a model achieves 60% Pass@1 but only 20% IF, what does this indicate about its practical utility?

## Architecture Onboarding

- Component map:
  C3-Bench Construction: AST parser → middle code extraction → equivalent implementation generation (via GPT-4o/Claude/DeepSeek/Qwen) → unit test validation → instruction generation (manual for ICC, Claude-generated for SCC)
  Training Data Synthesis: GitHub code → seed pairs (Claude3.5-Sonnet, 1K samples) → large-scale synthesis (Qwen2.5-Coder-32B-Instruct, 200K samples) → decontamination
  Evaluation Pipeline: ICC tasks → Pass@1 via unit tests → IF via Claude3.5-Sonnet judge; SCC tasks → AST-based structural verification + line count matching

- Critical path:
  1. AST-based middle code extraction determines task complexity
  2. Unit test filtering ensures ground-truth validity (only passing implementations retained)
  3. LLM judge consistency determines IF metric reliability
  4. Training data quality (not quantity) drives Qwen2.5-Coder-C3 improvement

- Design tradeoffs:
  - Manual vs. automated instruction generation: ICC uses expert-crafted instructions for precision; SCC uses Claude for scalability
  - Judge model selection: Claude3.5-Sonnet for accuracy vs. Qwen2.5-32B-Instruct for cost-efficiency
  - ICC vs. SCC focus: ICC reveals algorithmic instruction-following; SCC reveals fine-grained scope control—models good at one may struggle with the other (see Figure 12)

- Failure signatures:
  - High Pass@1 + low IF: Model generates working code but ignores instructions (common in open-source models)
  - High ES + low Pass@1: Model produces syntactically similar but functionally incorrect code
  - SCC failure with ICC success: Model struggles with scope constraints despite following implementation guidance
  - Base model capability ceiling: Qwen2.5-Coder-C3's ICC Pass@1 limited by base model's functional correctness (see Section 4.3)

- First 3 experiments:
  1. **Baseline benchmark run**: Evaluate your target model on C3-Bench using ChatML format (Figure 4), report Pass@1 and IF separately for ICC/SCC to identify which instruction type is problematic
  2. **Ablation study**: Run the same model with instructions removed from prompts (per Section 4.4 methodology); expect IF degradation with minimal Pass@1 change to confirm the benchmark measures instruction-following
  3. **Cross-benchmark correlation**: Compare your model's C3-Bench ranking against its performance on CrossCodeEval, RepoEval, and ExecRepoBench; low correlation indicates instruction-following is a distinct capability from standard completion

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does instruction-following capability for code completion generalize to multi-language scenarios and repository-level tasks requiring extended context?
- Basis in paper: [explicit] The Conclusion states, "While C3-Bench currently focuses on in-file Python tasks, future work should explore multi-language scenarios and repository-level tasks with extended context"
- Why unresolved: The current benchmark and experiments are strictly limited to in-file Python tasks, leaving the transferability of these instruction-following abilities to other languages or larger code contexts untested
- What evidence would resolve it: Construction of a multi-lingual C3-Bench extension and evaluation of models to see if instruction-following gaps persist across languages and repository-level contexts

### Open Question 2
- Question: Can instruction-tuning methods be developed that improve instruction adherence (IF) without being strictly bottlenecked by the base model's functional correctness (Pass@1) capabilities?
- Basis in paper: [explicit] The authors note that "ICC performance remains limited by the base model's capabilities, particularly in achieving high Pass@1 rates" and identify this as an important direction for future research
- Why unresolved: While Qwen2.5-Coder-C3 improved instruction following, its performance was still capped by the underlying model's ability to generate functionally correct code (Pass@1), suggesting the two capabilities are tightly coupled
- What evidence would resolve it: A decoupling of performance metrics where a model demonstrates significantly improved instruction adherence on correct solutions even when the base rate of correctness remains constant

### Open Question 3
- Question: What specific training objective modifications are required to improve performance on Scale-Control Completion (SCC) tasks, which currently challenge even advanced proprietary models?
- Basis in paper: [inferred] The paper notes that "Advanced LLMs including Gemini, DeepSeek-V3, and GPT-4o series struggle with scale-control tasks, indicating potential limitations in their training objectives"
- Why unresolved: The paper identifies the failure mode (struggling to adhere to line-span or block constraints) but does not propose a training solution that successfully closes this gap for the top-tier proprietary models
- What evidence would resolve it: Analysis of training data distributions regarding code length/granularity and the demonstration of a training regime that significantly boosts SCC scores for models that currently fail these tasks

## Limitations

- The benchmark's focus on Python limits generalizability to other programming languages where instruction-following patterns may differ significantly
- LLM-based semantic validation introduces potential systematic biases that could inflate instruction-following rate measurements
- The evaluation focuses on controlled benchmark conditions, lacking evidence of performance retention under varying context lengths typical in actual development environments

## Confidence

- **High confidence**: The benchmark construction methodology (AST-based middle code extraction, unit test filtering) and the core finding that open-source models lag significantly behind proprietary models in instruction-following capabilities
- **Medium confidence**: The data synthesis pipeline's effectiveness in improving instruction-following, as the lack of ablation studies creates uncertainty about alternative approaches
- **Low confidence**: The scalability and transferability of instruction-following capability improvements to real-world IDE scenarios

## Next Checks

1. **Judge bias analysis**: Conduct cross-validation using multiple judge models (Claude3.5-Sonnet, GPT-4, Qwen2.5-32B-Instruct) on the same subset of C3-Bench tasks to quantify systematic bias in instruction-following rate measurements and establish confidence intervals

2. **Error propagation study**: Implement a controlled experiment where synthesized data with known error rates is used for fine-tuning, then measure how these errors propagate through instruction-following capabilities to establish error bounds in the synthesis-to-transfer mechanism

3. **Generalization stress test**: Evaluate Qwen2.5-Coder-C3 on C3-Bench tasks with systematically varied context lengths (50%, 75%, 125% of standard) and instruction specificity levels to assess robustness of instruction-following capabilities under realistic IDE conditions