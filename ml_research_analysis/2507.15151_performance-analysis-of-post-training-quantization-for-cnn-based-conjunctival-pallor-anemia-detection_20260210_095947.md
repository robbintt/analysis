---
ver: rpa2
title: Performance Analysis of Post-Training Quantization for CNN-based Conjunctival
  Pallor Anemia Detection
arxiv_id: '2507.15151'
source_url: https://arxiv.org/abs/2507.15151
tags:
- quantization
- anemia
- accuracy
- precision
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the use of deep learning models, specifically
  a fine-tuned MobileNet architecture, for detecting anemia through conjunctival pallor
  analysis using the CP-AnemiC dataset of 710 pediatric images. The model achieved
  high performance with 93.13% accuracy, 93.74% precision, and 94.28% F1 score.
---

# Performance Analysis of Post-Training Quantization for CNN-based Conjunctival Pallor Anemia Detection

## Quick Facts
- arXiv ID: 2507.15151
- Source URL: https://arxiv.org/abs/2507.15151
- Authors: Sebastian A. Cruz Romero; Wilfredo E. Lugo Beauchamp
- Reference count: 18
- Primary result: MobileNet-based anemia detection achieves 93.13% accuracy with FP16 quantization maintaining 92.50% accuracy while halving model size

## Executive Summary
This study investigates deep learning models for detecting anemia through conjunctival pallor analysis using the CP-AnemiC dataset of 710 pediatric images. A fine-tuned MobileNet architecture achieved high performance with 93.13% accuracy, 93.74% precision, and 94.28% F1 score. The research focuses on optimizing for edge deployment through post-training quantization at FP32, FP16, INT8, and INT4 bit-widths. FP16 quantization maintained strong accuracy (92.50%) with reduced model size and faster inference, while INT8 and INT4 led to significant performance degradation, suggesting that standard quantization techniques may not be suitable for this medical diagnostic application.

## Method Summary
The study uses a MobileNetV2 backbone pretrained on ImageNet, fine-tuned for binary classification of anemia from conjunctival pallor images. The CP-AnemiC dataset contains 710 pediatric images (424 anemic, 286 non-anemic based on WHO Hb < 11 g/dL threshold). The model was trained with 5-fold cross-validation using Adam optimizer (lr=10⁻⁴), Binary Cross-Entropy loss, and batch size 32. Post-training quantization was applied via PyTorch→ONNX→TensorRT conversion, testing FP16, INT8 (MaxCalibrator), and INT4 (AWQ) bit-widths. Evaluation metrics included accuracy, precision, recall, F1 score, AUC, model size, and inference latency.

## Key Results
- MobileNet fine-tuning achieved 93.13% accuracy, 93.74% precision, and 94.28% F1 score on CP-AnemiC dataset
- FP16 quantization maintained 92.50% accuracy while reducing model size from 9.13 MB to 4.61 MB and inference time from 48.6 ms to 37.4 ms
- INT8 quantization severely degraded performance to 71.25% accuracy with 91.9 ms latency (2× FP32)
- INT4 quantization collapsed to 43.13% accuracy with F1 score of 0.0196, indicating complete model failure

## Why This Works (Mechanism)

### Mechanism 1: Conjunctival Tissue Transparency Enables Hemoglobin-Based Classification
The conjunctiva's minimal intervening tissue layers and direct vascular access allow surface coloration to correlate with blood hemoglobin concentration. A CNN trained on pixel patterns can learn this mapping when provided with binary labels derived from the WHO threshold (Hb < 11 g/dL). Image capture conditions (lighting, camera quality) must not introduce systematic bias that outweighs the hemoglobin signal.

### Mechanism 2: FP16 Quantization Retains Sufficient Precision for Feature Representations
FP16 truncates mantissa bits but preserves sufficient dynamic range for neural network weights and activations. The rounding error introduced is below the noise threshold of the learned feature representations. The MobileNet feature space has sufficient margin between anemic/non-anemic clusters that small perturbations do not cross decision boundaries.

### Mechanism 3: Integer Quantization at Low Bit-Widths Destroys Discriminative Information
Integer quantization maps continuous values to discrete bins using scale factors. At low bit-widths, the quantization step size exceeds the variance of discriminative features, collapsing distinct activations to identical integer values. The observed INT8 degradation (71.25% accuracy) suggests per-tensor calibration may be insufficient for this architecture.

## Foundational Learning

- **Post-Training Quantization (PTQ) vs. Quantization-Aware Training (QAT)**: The paper uses PTQ, which applies quantization after training without re-training. Understanding this distinction explains why INT8 failed—PTQ has no mechanism to adapt weights to quantization noise.
  - Quick check: If you observe 20% accuracy drop after INT8 PTQ, what is the first remediation strategy you should try?

- **Calibration and Scale Factor Computation**: The paper uses MaxCalibrator (maximum absolute value) for INT8 scale factors. This is sensitive to outliers and may produce suboptimal bin allocation for non-uniform weight distributions.
  - Quick check: Why might a per-channel quantization strategy outperform per-tensor quantization for convolutional layers?

- **MobileNet Architecture Constraints**: MobileNet uses depthwise separable convolutions with fewer parameters, making it more sensitive to quantization errors in individual layers compared to wider architectures.
  - Quick check: Which layers in MobileNet would you expect to be most vulnerable to quantization error—early feature extractors or late classifiers?

## Architecture Onboarding

- **Component map**: Input Image (conjunctival ROI) -> MobileNet Backbone (ImageNet pretrained) -> Modified FC Layer (binary output) -> Sigmoid Activation -> Anemic/Non-anemic Classification
- **Critical path**: The ONNX export must preserve numerical equivalence to PyTorch. Any operator mapping mismatch will compound during quantization. Verify with FP32 ONNX inference before attempting quantized versions.
- **Design tradeoffs**:
  | Bit-width | Accuracy | Model Size | Latency | Use Case |
  |-----------|----------|------------|---------|----------|
  | FP32      | 93.13%   | 9.13 MB    | 48.6 ms | Development/validation |
  | FP16      | 92.50%   | 4.61 MB    | 37.4 ms | Edge deployment (recommended) |
  | INT8      | 71.25%   | 9.24 MB    | 91.9 ms | Not viable for this task |
  | INT4      | 43.13%   | 17.75 MB   | 49.5 ms | Do not use |
- **Failure signatures**:
  - INT4 collapse: F1 score of 0.0196 indicates the model is predicting predominantly one class. Check confusion matrix for class imbalance exploitation.
  - INT8 latency spike: 91.9 ms latency (2× FP32) suggests dequantization overhead between mixed-precision layers. Verify all layers are actually running in INT8.
  - Accuracy cliff between FP16 and INT8: The 21% drop suggests the feature distributions are poorly matched to integer bins. Investigate activation histograms.
- **First 3 experiments**:
  1. Establish FP32 ONNX baseline: Export to ONNX, run inference, verify accuracy matches PyTorch within 0.1%. If not, debug operator conversions before proceeding.
  2. INT8 with entropy calibration: Replace MaxCalibrator with entropy-based calibration (e.g., TensorRT's EntropyCalibrator2) using 100-500 representative images. Compare accuracy recovery.
  3. Per-layer sensitivity analysis: Quantize one layer at a time to INT8 while keeping others at FP32. Identify which layers cause the largest accuracy drops—these are candidates to keep at FP16 in a mixed-precision configuration.

## Open Questions the Paper Calls Out

- **Question**: How do quantized models perform regarding inference latency and power consumption on actual edge devices like the NVIDIA Jetson?
  - Basis: The authors state that future work will study the impact on edge devices (Jetson Xavier NX and TX2 NX) to exploit their small form factor.
  - Why unresolved: Current benchmarks were performed on a high-performance desktop GPU (RTX 4090), not the target resource-constrained hardware.
  - Evidence: Latency and energy consumption metrics collected directly from the specified Jetson hardware.

- **Question**: Can Quantization-Aware Training (QAT) successfully maintain accuracy for INT8 representations where Post-Training Quantization (PTQ) failed?
  - Basis: The study found INT8 PTQ caused severe accuracy degradation (71.25%), despite the literature suggesting QAT can help models adapt to lower precision.
  - Why unresolved: The experiment was limited to PTQ; QAT was discussed theoretically but not empirically tested in this specific diagnostic context.
  - Evidence: A comparative study training MobileNet with QAT enabled for INT8 versus the current PTQ baseline.

- **Question**: Does a mixed-precision strategy selectively optimizing layers prevent the performance collapse observed in uniform INT4 quantization?
  - Basis: The authors propose using TensorRT to select optimal precision per layer to enhance speed while maintaining diagnostic integrity.
  - Why unresolved: The preliminary results showed uniform INT4 quantization failed completely (43.13% accuracy); mixed-precision has not yet been applied.
  - Evidence: Performance metrics from a model utilizing layer-wise mixed-precision configurations (e.g., FP16/INT8 mix).

## Limitations

- **Dataset generalizability**: CP-AnemiC contains 710 pediatric images from a specific population. Performance may degrade in adult populations or different ethnic groups with varying conjunctival pigmentation patterns.
- **Calibration methodology**: INT8 uses MaxCalibrator (per-channel weights, per-tensor activations) without specifying calibration dataset size or sampling strategy. This affects quantization scale factor quality and explains the observed 22% accuracy drop.
- **Implementation specifics**: Exact ONNX export configuration, normalization values, and TensorRT engine optimization parameters are unspecified, potentially affecting quantization fidelity.

## Confidence

- **High confidence**: FP32 and FP16 quantization results (93.13% and 92.50% accuracy respectively), as these are well-established quantization targets with minimal information loss.
- **Medium confidence**: INT8 and INT4 results (71.25% and 43.13% accuracy), given the unexpected performance degradation and model size increase for INT8, suggesting potential implementation or configuration issues.
- **Medium confidence**: The mechanism linking conjunctival pallor to hemoglobin levels, though physiological plausibility is supported by WHO guidelines for conjunctival examination.

## Next Checks

1. **Baseline ONNX verification**: Export the trained FP32 model to ONNX and verify inference accuracy matches PyTorch within 0.1% before attempting quantization. This isolates quantization issues from conversion errors.
2. **INT8 calibration optimization**: Replace MaxCalibrator with TensorRT's EntropyCalibrator2 using 100-500 representative images. Compare accuracy improvement and identify if calibration data quality explains the 22% drop.
3. **Mixed-precision analysis**: Quantize individual MobileNet layers to INT8 while keeping others at FP16/FP32. Identify which layers cause the largest accuracy degradation to inform targeted quantization strategies.