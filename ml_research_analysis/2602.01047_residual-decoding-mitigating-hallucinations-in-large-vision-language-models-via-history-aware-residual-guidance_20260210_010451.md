---
ver: rpa2
title: 'Residual Decoding: Mitigating Hallucinations in Large Vision-Language Models
  via History-Aware Residual Guidance'
arxiv_id: '2602.01047'
source_url: https://arxiv.org/abs/2602.01047
tags:
- decoding
- language
- image
- large
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Residual Decoding (ResDec) addresses hallucinations in Large Vision-Language
  Models (LVLMs) by leveraging historical token distributions during decoding. The
  method analyzes the evolution of token logits over time, identifying stable semantic
  regions using Jensen-Shannon Divergence (JSD).
---

# Residual Decoding: Mitigating Hallucinations in Large Vision-Language Models via History-Aware Residual Guidance

## Quick Facts
- **arXiv ID**: 2602.01047
- **Source URL**: https://arxiv.org/abs/2602.01047
- **Reference count**: 40
- **Primary result**: ResDec improves POPE accuracy by 7.84% and reduces object hallucinations by 26.44% on CHAIR without additional training

## Executive Summary
Residual Decoding (ResDec) addresses hallucinations in Large Vision-Language Models by analyzing the evolution of token logits during autoregressive decoding. The method identifies stable semantic regions using Jensen-Shannon Divergence (JSD) and aggregates logits from these regions to form a residual guidance signal. This signal is then fused with current decoding logits to correct language prior biases. Experiments demonstrate state-of-the-art performance across 11 hallucination benchmarks, with significant improvements in accuracy and hallucination reduction while maintaining strong performance on comprehensive multimodal tasks.

## Method Summary
ResDec analyzes token logit evolution during decoding by computing Jensen-Shannon Divergence between consecutive token distributions. It identifies a "Semantic Anchoring Phase" where logits stabilize, aggregates logits from this phase using confidence weights, and fuses them with current logits through a weighted sum. The method operates without additional training or inference cost by leveraging existing decoding trajectories. Implementation requires only tracking historical logits and computing JSD, making it applicable to any autoregressive LVLM architecture.

## Key Results
- ResDec improves POPE accuracy by 7.84% and F1 by 8.01% compared to baseline models
- Reduces object hallucinations by 26.44% on CHAIR benchmark while maintaining generation quality
- Achieves state-of-the-art performance across 11 hallucination benchmarks without additional training cost

## Why This Works (Mechanism)
The method exploits the observation that token logits stabilize during the "Semantic Anchoring Phase" of decoding, which corresponds to hallucination-free generation. By aggregating logits from this stable region, ResDec captures reliable semantic information that corrects the language prior biases introduced in earlier decoding steps. The JSD-based detection ensures only confident, stable token distributions contribute to the residual guidance, preventing amplification of hallucinated content.

## Foundational Learning
- **Jensen-Shannon Divergence (JSD)**: Measures similarity between probability distributions; needed to detect stable semantic regions in token evolution; quick check: verify JSD values decrease monotonically in Semantic Anchoring Phase
- **Autoregressive Decoding**: Sequential token generation where each step depends on previous outputs; needed to track logit evolution over time; quick check: confirm logits converge as generation progresses
- **Language Prior Bias**: Tendency of LVLMs to generate fluent but factually incorrect content; needed to understand hallucination sources; quick check: measure frequency of object hallucinations in baseline models
- **Semantic Anchoring**: Stable semantic representation in later decoding steps; needed to identify reliable guidance signals; quick check: validate stable regions contain correct object references
- **Residual Guidance**: Auxiliary correction signal added to current predictions; needed to understand correction mechanism; quick check: verify guidance improves accuracy without degrading fluency
- **Weighted Aggregation**: Combining multiple logit distributions with confidence scores; needed to form stable guidance signal; quick check: test different aggregation functions (mean vs weighted mean)

## Architecture Onboarding

**Component Map**: Input Image -> Vision Encoder -> LVLM Backbone -> Autoregressive Decoder -> Token Logits -> JSD Analysis -> Residual Aggregation -> Guided Decoding

**Critical Path**: During decoding, ResDec tracks historical logits, computes JSD between consecutive distributions, identifies Semantic Anchoring Phase, aggregates stable logits, and fuses them with current logits for output.

**Design Tradeoffs**: Static vs dynamic fusion weight (α) balances correction strength against fluency preservation; fixed historical window (W) trades memory efficiency against retrieval of relevant context; JSD threshold selection affects sensitivity to semantic stability.

**Failure Signatures**: Over-correction when α is too high causes grammatical errors and reduced fluency; insufficient correction when α is too low fails to mitigate hallucinations; incorrect JSD threshold misidentifies stable regions leading to either noise amplification or missed correction opportunities.

**First Experiments**: 1) Measure JSD evolution curves across different LVLM architectures to validate U-shaped pattern universality; 2) Test varying α values (0.25, 0.5, 0.75) on POPE to find optimal balance; 3) Compare residual aggregation window sizes (8, 16, 32) on CHAIR to determine optimal context length.

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Theoretical justification for JSD-based stability detection lacks mathematical derivation from transformer dynamics
- Fixed historical window may not capture relevant context in extremely long-form generation or multi-turn dialogue
- Static fusion weight may not optimally balance correction across different token categories (function vs content words)

## Confidence
- Effectiveness on benchmark datasets: Medium
- Theoretical foundation: Low
- Generalizability to real-world scenarios: Medium
- Computational efficiency claims: Medium
- Robustness to out-of-distribution tasks: Low

## Next Checks
1. Test ResDec on out-of-distribution visual questions and long-form generation tasks to assess robustness beyond benchmark datasets
2. Conduct ablation studies varying the JSD threshold and aggregation window size to determine optimal hyperparameters across different model architectures
3. Measure the actual computational overhead and latency impact during inference across different hardware configurations to validate the "no additional cost" claim in practical deployments