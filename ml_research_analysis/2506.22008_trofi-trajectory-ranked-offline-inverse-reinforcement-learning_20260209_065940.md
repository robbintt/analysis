---
ver: rpa2
title: 'TROFI: Trajectory-Ranked Offline Inverse Reinforcement Learning'
arxiv_id: '2506.22008'
source_url: https://arxiv.org/abs/2506.22008
tags:
- reward
- learning
- trofi
- function
- offline
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TROFI, a method for offline reinforcement
  learning without a pre-defined reward function. TROFI learns a reward model from
  human preferences using trajectory ranking, then labels the dataset and trains a
  policy with TD3+BC.
---

# TROFI: Trajectory-Ranked Offline Inverse Reinforcement Learning

## Quick Facts
- **arXiv ID**: 2506.22008
- **Source URL**: https://arxiv.org/abs/2506.22008
- **Reference count**: 13
- **Primary result**: TROFI learns reward models from ranked trajectories, then trains TD3+BC policies on relabeled datasets, achieving state-of-the-art performance on D4RL MuJoCo tasks without pre-defined rewards.

## Executive Summary
TROFI introduces a novel offline IRL approach that learns reward functions from human trajectory rankings rather than requiring optimal demonstrations or ground-truth rewards. The method trains a T-REX reward model on ranked trajectory pairs, then uses the learned rewards to label the full dataset and train a policy with TD3+BC. Experiments show TROFI outperforms state-of-the-art offline IRL methods on D4RL benchmarks and achieves comparable performance to using ground truth rewards, while requiring only a small percentage of ranked trajectories.

## Method Summary
TROFI operates in two phases: first, it learns a state-based reward function from a subset of ranked trajectories using T-REX's pairwise ranking loss. The reward model is trained to assign higher cumulative rewards to higher-ranked trajectories. Second, this learned reward function labels the entire offline dataset, which is then used to train a TD3+BC policy with behavioral cloning regularization. The approach is specifically designed for offline RL settings where only a fixed dataset is available, avoiding the need for pre-defined reward functions or online interaction.

## Key Results
- TROFI achieves state-of-the-art performance on D4RL MuJoCo tasks, outperforming BCQ, CQL, and TD3+BC baselines
- Even with 5-10% of trajectories ranked, TROFI performs comparably to using full rankings, suggesting minimal preference supervision is needed
- TROFI rewards can achieve higher Pearson correlation with value estimates than ground-truth rewards, explaining performance gains
- In a 3D game environment, TROFI achieves similar performance without requiring reward engineering

## Why This Works (Mechanism)

### Mechanism 1
Ranked trajectory comparisons enable reward inference without optimal demonstrations or ground-truth labels. T-REX learns a state-based reward function by enforcing that cumulative rewards over higher-ranked trajectories exceed those of lower-ranked ones via a pairwise ranking loss. This transforms qualitative preferences into a dense reward signal. Rankings must reflect underlying reward preferences consistently for this to work.

### Mechanism 2
A learned reward that is "easy to learn" improves value function alignment with future discounted returns. In offline RL, the fixed action distribution limits value function generalization. A reward function with simpler structure (e.g., smoother gradients) allows the value function to better correlate with actual discounted returns. TROFI rewards can have higher Pearson correlation with value estimates than ground-truth rewards.

### Mechanism 3
Behavioral cloning regularization stabilizes offline policy learning from labeled data. TD3+BC adds a BC term to the policy objective, constraining the policy to stay close to dataset actions while still optimizing Q-values. The dataset must contain sufficiently diverse actions near optimal behavior for this to work.

## Foundational Learning

- **Concept**: Offline RL fundamentals (distributional shift, value overestimation)
  - Why needed here: TROFI inherits TD3+BC's conservative learning; understanding why offline RL requires regularization prevents misinterpreting results.
  - Quick check question: Can you explain why online RL algorithms fail when applied directly to offline data?

- **Concept**: Preference-based reward learning
  - Why needed here: T-REX's ranking loss is the core of TROFI's reward inference; understanding preference elicitation helps design ranking protocols.
  - Quick check question: How does a pairwise ranking loss differ from direct reward regression?

- **Concept**: Value function approximation and TD learning
  - Why needed here: The paper's core insight links reward structure to value function learnability; requires understanding TD error minimization.
  - Quick check question: What does the Bellman equation optimize, and how does reward scale affect learning dynamics?

## Architecture Onboarding

- **Component map**: Ranked subset M → Reward model training → Full dataset labeling → TD3+BC policy training → Evaluation
- **Critical path**: The reward model must be trained before dataset labeling, which must complete before policy training can begin.
- **Design tradeoffs**: 
  - Ranking percentage (|M|): Paper shows 5-10% often sufficient; more rankings increase supervision cost with diminishing returns
  - Reward model complexity: Overly complex models may overfit rankings; simple models may fail to capture nuance
  - Policy optimizer choice: TD3+BC works well for MuJoCo; IQL better for Adroit
- **Failure signatures**: Low value-reward correlation (check with Pearson analysis), policy performance collapses with random/constant rewards, high variance across seeds
- **First 3 experiments**:
  1. Run TROFI with 100% rankings on hopper-medium-v2; compare normalized score against BC and GT baselines
  2. Test TROFI-100%, TROFI-10%, TROFI-5% on one medium and one expert dataset; verify Table 2 trends
  3. For a single environment, plot discounted returns vs value estimates for both TROFI and GT rewards

## Open Questions the Paper Calls Out

### Open Question 1
Why does using a smaller subset of ranked trajectories often result in better policy performance than ranking the entire dataset? The authors empirically observe this but leave the explanation for future work. An analysis of the reward model's loss landscape as a function of ranking set size would help resolve this.

### Open Question 2
Can TROFI maintain its performance when trajectory rankings are provided by actual humans rather than automated oracles? The study relies on an "oracle" for ranking. Experimental results comparing human vs automated rankings would establish robustness to ranking noise.

### Open Question 3
Does the hypothesis that "easy-to-learn" reward functions improve value function alignment generalize to environments beyond the MuJoCo tasks analyzed? The finding is derived primarily from MuJoCo locomotion tasks. Evaluation in diverse domains would test generalizability.

### Open Question 4
What are the theoretical properties defining an "easy-to-learn" reward function in offline RL? The paper provides empirical evidence but lacks a formal definition. A formal analysis quantifying "learnability" of reward functions would be valuable.

## Limitations
- The core claim about learned rewards outperforming ground-truth rewards may not generalize beyond offline settings where value function estimation is bottlenecked
- The 3D game experiment lacks quantitative comparison details, limiting assessment of practical applicability
- The ablation on ranking percentage assumes trajectory quality remains constant across different ranking fractions

## Confidence

- **High confidence** in offline IRL pipeline efficacy: TROFI consistently outperforms state-of-the-art offline IRL baselines across multiple D4RL datasets
- **Medium confidence** in reward-over-performance claim: Table 3 shows TROFI rewards achieving higher value correlation than GT rewards, but the effect is environment-specific
- **Medium confidence** in minimal ranking sufficiency: The 5-10% ranking claim is demonstrated but lacks systematic analysis across preference quality levels

## Next Checks

1. **Reward quality vs policy performance correlation**: Systematically measure Pearson correlation between learned reward values and discounted returns across environments, then correlate these with final policy performance to validate the claimed mechanism.

2. **Preference noise robustness**: Repeat TROFI experiments with artificially corrupted rankings (swapping random pairs) to determine the breaking point where preference quality affects performance.

3. **Cross-dataset generalization**: Train TROFI on one dataset type (e.g., medium-expert) and evaluate policy transfer to another (e.g., medium-replay) to test whether learned rewards capture transferable preferences.