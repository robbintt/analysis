---
ver: rpa2
title: 'AdaPM: a Partial Momentum Algorithm for LLM Training'
arxiv_id: '2510.09103'
source_url: https://arxiv.org/abs/2510.09103
tags:
- momentum
- uni00000013
- adapm
- uni00000011
- low-rank
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes AdaPM, a memory-efficient optimizer for large
  language model training that achieves over 90% reduction in momentum memory while
  maintaining performance comparable to AdamW. The key insight is that different transformer
  blocks have varying momentum requirements: embedding and attention output blocks
  need no momentum, query/key/MLP blocks need low-rank momentum with bias correction,
  and value blocks require full momentum.'
---

# AdaPM: a Partial Momentum Algorithm for LLM Training

## Quick Facts
- arXiv ID: 2510.09103
- Source URL: https://arxiv.org/abs/2510.09103
- Reference count: 34
- Primary result: Achieves >90% momentum memory reduction while maintaining AdamW-level performance in LLM training

## Executive Summary
AdaPM introduces a memory-efficient optimizer for large language model training that partitions momentum requirements across different transformer blocks. By recognizing that embedding and attention output blocks can operate without momentum, query/key/MLP blocks need only low-rank momentum with bias correction, and value blocks require full momentum, AdaPM achieves over 90% reduction in momentum memory. Combined with Adam-mini for second-order statistics compression, the method reaches up to 95% memory savings in optimizer states and reduces GPU hours by over 30% for GPT-2 1.5B pretraining.

## Method Summary
AdaPM implements a non-uniform momentum design that assigns different momentum strategies to transformer blocks based on their gradient characteristics. For embedding and attention output blocks, momentum is disabled entirely. Query, key, and MLP blocks use low-rank momentum with bias correction, where momentum is approximated as the product of small matrices L and R. Value blocks retain full momentum. The bias correction mechanism compensates for information loss in low-rank approximation by rescaling the residual. When combined with Adam-mini for compressing second-order statistics, AdaPM achieves substantial memory savings while maintaining convergence comparable to AdamW.

## Key Results
- Achieves over 90% reduction in momentum memory while matching AdamW performance
- Reduces GPU hours by over 30% for GPT-2 1.5B pretraining
- Combined with Adam-mini, achieves up to 95% memory savings in optimizer states
- Shows 1.96x slower convergence when bias correction is disabled

## Why This Works (Mechanism)

### Mechanism 1: Non-Uniform Momentum Partitioning
Different transformer blocks have heterogeneous optimization requirements, allowing selective momentum reduction without degrading performance. The authors empirically determine that Embedding and Attention Output blocks have sparse gradients where momentum is redundant, Value layers require full momentum, and Query, Key, and MLP blocks function well with low-rank momentum. This partitioning is validated through ablation studies showing that removing momentum from appropriate blocks does not harm convergence.

### Mechanism 2: Debiased Low-Rank Momentum Estimator
Standard low-rank approximation introduces bias that slows convergence, which is corrected by rescaling the approximation residual. Instead of using the low-rank projection directly, AdaPM calculates the residual (difference between current momentum estimate and projection) and adds a correction term $\frac{\beta_1}{1-\beta_1} r_t$ to the update. This correction eliminates bias under the assumption that residuals are approximately stationary over short time windows.

### Mechanism 3: Orthogonal State Compression Synergy
Compressing first-order momentum (AdaPM) is orthogonal to compressing second-order variance (Adam-mini), enabling additive memory savings. By attacking optimizer state memory from both sides—reducing both $m_t$ and $v_t$ footprints—the combined approach achieves significantly greater compression than either method alone, with minimal interference between the approximation errors.

## Foundational Learning

- **Concept: Adam Optimizer States ($m_t$ and $v_t$)**
  - Why needed here: Understanding what AdaPM compresses (the $m_t$ momentum term) versus what Adam-mini compresses (the $v_t$ variance term) is essential for grasping the orthogonal compression approach.
  - Quick check question: Can you explain why Adam requires 2x model size in extra memory (excluding gradients)?

- **Concept: Low-Rank Matrix Approximation (SVD/Factorization)**
  - Why needed here: The core efficiency gain comes from storing momentum as $L \times R$ (small matrices) instead of a full matrix $M$. Understanding the rank $r$ tradeoff is essential.
  - Quick check question: If a weight matrix is $4096 \times 4096$ and we use rank $r=5\%$, what are the dimensions of the factors $L$ and $R$?

- **Concept: Gradient Sparsity in Transformers**
  - Why needed here: The paper justifies removing momentum entirely for Embedding layers due to sparse gradients. Understanding this phenomenon explains why "No Momentum" is a viable mode.
  - Quick check question: In a large embedding matrix, why might the gradient for the majority of rows be zero at any single step?

## Architecture Onboarding

- **Component map:** Partition Manager -> Factorizer -> Residual Corrector -> Weight Update
- **Critical path:** 1) Standard gradient calculation $\nabla f(W_t)$ 2) Routing gradients to appropriate handler based on block type 3) For "Low-Rank" blocks: Run Factorizer → Calculate Residual → Apply Correction 4) Update weights using modified momentum term
- **Design tradeoffs:** Rank ($r$) controls memory vs. information retention (paper recommends $r=5\%$). Update frequency ($T$) balances factorization compute overhead against memory efficiency ($T=100$ found efficient). The approach trades compute for memory capacity.
- **Failure signatures:** Slow convergence indicates disabled bias correction or rank set too low (<2%). Divergence suggests Value blocks mistakenly set to Low-Rank or None. Excessive overhead occurs if update frequency is too low (e.g., every step).
- **First 3 experiments:** 1) Sanity Check: Train GPT-2 124M with AdaPM vs AdamW, verify validation loss curves match. 2) Ablation: Run GPT-2 124M with AdaPM but set correction term to 0, confirm 1.96x slowdown. 3) Memory Profiling: Integrate AdaPM with Adam-mini, monitor peak memory usage for ~94% state reduction.

## Open Questions the Paper Calls Out
- Can the non-uniform momentum partition strategy be extended to other prevalent architectures like diffusion models?
- Can the bias-correction technique be applied to low-rank activation estimation to further reduce training memory?
- What are the precise theoretical mechanisms determining why Value layers require full momentum while Query and Key layers do not?

## Limitations
- Limited architecture generality: The non-uniform momentum design is derived from transformer-specific gradient patterns and lacks evidence for applicability to CNNs, RNNs, or multimodal architectures.
- Assumption-dependent correctness: The debiased estimator relies on stationary residual assumption, which may break down during rapid learning rate changes or highly non-stationary data distributions.
- Hyperparameter sensitivity: The paper identifies critical thresholds but lacks systematic sensitivity analysis across the full hyperparameter space.

## Confidence
- **High confidence:** Memory reduction claims (90%+ momentum memory savings) are directly measurable and well-supported by experimental results.
- **Medium confidence:** Convergence performance claims (matching or exceeding AdamW) are supported by experiments on two model families but limited to specific architectures and datasets.
- **Low confidence:** Theoretical foundations of bias correction mechanism (stationary residual assumption) are plausible but not rigorously proven.

## Next Checks
1. **Architecture generalization test:** Implement AdaPM for training a ResNet-50 or LSTM on standard benchmarks to test whether transformer-derived partitioning rules remain effective for other architectures.
2. **Stationary residual validation:** Design controlled experiment with abrupt learning rate changes to measure how quickly bias correction adapts and quantify performance degradation during non-stationary periods.
3. **Hyperparameter sweep:** Systematically vary rank (1%, 3%, 5%, 10%) and update frequency (50, 100, 500, 1000) across multiple model scales to identify optimal memory vs. convergence speed tradeoffs.