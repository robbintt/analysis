---
ver: rpa2
title: 'Falcon: A Comprehensive Chinese Text-to-SQL Benchmark for Enterprise-Grade
  Evaluation'
arxiv_id: '2510.24762'
source_url: https://arxiv.org/abs/2510.24762
tags:
- https
- datasets
- chinese
- kaggle
- enterprise
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Falcon is a Chinese text-to-SQL benchmark designed for enterprise\
  \ environments, featuring 600 questions across 28 databases with MaxCompute/Hive\
  \ dialect support. It emphasizes multi-table reasoning (77% of questions require\
  \ joins across \u22654 tables) and captures Chinese-specific semantics like business\
  \ jargon, ellipsis, and numeric expressions."
---

# Falcon: A Comprehensive Chinese Text-to-SQL Benchmark for Enterprise-Grade Evaluation

## Quick Facts
- arXiv ID: 2510.24762
- Source URL: https://arxiv.org/abs/2510.24762
- Reference count: 40
- Key outcome: Current SOTA models achieve at most 50% execution accuracy on this benchmark

## Executive Summary
Falcon introduces a Chinese text-to-SQL benchmark designed to evaluate enterprise-grade systems, featuring 600 questions across 28 databases with MaxCompute/Hive dialect support. The benchmark emphasizes multi-table reasoning, with 77% of questions requiring joins across four or more tables, and captures Chinese-specific semantics including business jargon, ellipsis, and numeric expressions. Experiments show that even state-of-the-art models like DeepSeek achieve only 50% execution accuracy, with major errors stemming from schema linking in wide, denormalized schemas and mapping colloquial Chinese to precise SQL operators.

## Method Summary
Falcon is constructed from 500 public Kaggle datasets and 100 enterprise-inspired synthetic questions, with dual annotation for SQL features and Chinese linguistic phenomena. The evaluation pipeline includes schema-aware SQL comparison using content-based result hashing, with normalization for type canonicalization and timezone standardization. Models are evaluated using exact result accuracy (ExAcc) with MaxCompute/Hive dialect compliance, requiring CTEs for queries with three or more joins and disallowing non-deterministic functions.

## Key Results
- Current SOTA models (GPT-4o, DeepSeek) achieve at most 50% execution accuracy on Falcon
- Single-table queries achieve 60.95% ExAcc, while queries with ≥4 tables collapse to 21.43%
- Major error sources include schema linking failures in wide joins and mapping colloquial Chinese to exact SQL operators

## Why This Works (Mechanism)

### Mechanism 1: Schema Width Scaling as Accuracy Predictor
Execution accuracy decays monotonically as join width increases, with a sharp inflection point at ≥4 tables. Wider joins expose more semantically adjacent columns and require fragile key-threading through multiple hub tables, causing models to confuse near-synonymous attributes during column selection.

### Mechanism 2: Chinese Ellipsis and Business Jargon → Operator Misalignment
Concise, colloquial Chinese queries omit comparison operators, time windows, and aggregation intent, causing models to select incorrect SQL predicates. Chinese analysts frequently use elliptical phrasing and business terminology that require implicit recovery, which current models fail to perform.

### Mechanism 3: Content-Based Result Comparison → Robust Evaluation
Schema-aware, content-based SQL comparison reduces false negatives from cosmetic syntactic variations. The comparator normalizes results, generates MD5 hashes per column, and checks multiset coverage before row-level comparison, tolerating projection reorder and superfluous columns while enforcing semantic equivalence.

## Foundational Learning

- **Concept: MaxCompute/Hive SQL Dialect**
  - Why needed here: Falcon requires dialect-specific syntax (date functions, string collation, non-deterministic function restrictions)
  - Quick check question: Can you identify three syntax differences between standard SQL and MaxCompute/Hive for date arithmetic?

- **Concept: Schema Linking in Denormalized Enterprise Schemas**
  - Why needed here: Enterprise schemas contain hundreds of tables with ambiguous column names and implicit FK relations
  - Quick check question: Given a schema with orders.order_date and periods.period_date, which column should a query about "daily sales" reference?

- **Concept: Chinese Linguistic Phenomena in NLP**
  - Why needed here: Ellipsis resolution, business terminology mapping, and numeric expression variants drive operator misalignment errors
  - Quick check question: How would you expand "GMV top-10" into a complete SQL predicate with explicit aggregation and ordering?

## Architecture Onboarding

- **Component map:**
  - Benchmark corpus: 600 questions (500 public Kaggle + 100 enterprise-synthetic) across 28 databases
  - Dual annotation layer: SQL-computation features + Chinese semantics
  - Evaluation pipeline: Dialect linter → Execution validator → Content-based comparator → ExAcc scoring

- **Critical path:**
  1. Schema linking: Map Chinese question terms to correct tables/columns across wide, denormalized schemas
  2. Join construction: Propagate keys through ≥4-table chains without confusing semantically adjacent fields
  3. Operator alignment: Recover implicit aggregations, time windows, and group-by keys from elliptical Chinese phrasing
  4. Dialect compliance: Emit MaxCompute/Hive-compatible SQL avoiding non-deterministic constructs

- **Design tradeoffs:**
  - Hybrid sourcing balances domain diversity with enterprise realism but may introduce distribution shift
  - Content-based comparison tolerates syntactic variations but cannot detect semantically distinct queries with identical result hashes
  - Single-turn evaluation simplifies benchmarking but under-represents real multi-turn analytics workflows

- **Failure signatures:**
  - ExAcc drops sharply for queries with ≥4-table joins (21.43% vs. 60.95% for single-table)
  - High error concentration in schema linking and colloquial-to-operator mapping
  - Models frequently select wrong columns among near-synonyms in wide joins

- **First 3 experiments:**
  1. Baseline join-width scaling: Evaluate a single model stratified by join width to confirm accuracy decay curve
  2. Ellipsis-resolution preprocessing: Add Chinese preprocessor to expand common elliptical patterns and measure ExAcc delta
  3. Schema-encoding ablation: Test relation-aware schema encoding vs. flat DDL prompts on the ≥4-table subset

## Open Questions the Paper Calls Out

### Open Question 1
Can integrating relation-aware schema encoding mechanisms specifically mitigate the high error rates (78.57%) observed in queries requiring joins across four or more tables?
- Basis in paper: The conclusion advocates for "relation-aware schema encoding" to address persistent challenges, while Section 6.3 identifies wide joins (≥4 tables) as the primary source of errors
- Why unresolved: The paper evaluates existing SOTA models using standard prompts but does not implement or test the proposed relation-aware architectural improvements
- What evidence would resolve it: A comparative study on Falcon showing that models augmented with relation-aware encoding significantly outperform baseline LLMs on the multi-table subset

### Open Question 2
Does a dedicated preprocessing module for resolving Chinese ellipsis and business jargon significantly improve the semantic mapping accuracy of text-to-SQL models?
- Basis in paper: The conclusion explicitly proposes "lightweight preprocessing for Chinese ellipsis/coreference/business shorthand" as a necessary advancement
- Why unresolved: The error analysis identifies "mapping concise, colloquial Chinese" as a major failure mode, but the paper does not experiment with the suggested preprocessing solution
- What evidence would resolve it: Ablation results on queries annotated with "ellipsis" or "business terminology" demonstrating improved Exact Result Accuracy after applying such preprocessing

### Open Question 3
To what extent does dialect-aware constrained decoding improve the generation of valid, executable MaxCompute/Hive SQL compared to unconstrained generation?
- Basis in paper: The authors list "dialect-aware constrained decoding" as a key area for future advocacy
- Why unresolved: The current evaluation focuses on execution accuracy of standard generative models, without isolating gains specifically from enforcing dialect validity during decoding
- What evidence would resolve it: Evaluation of a constrained decoding agent showing reduction in syntax errors or "dialect non-compliance" relative to standard models

## Limitations
- Synthetic enterprise component (100 questions) may not fully capture real-world enterprise complexity
- Content-based comparator has not been externally validated and may have edge cases
- Single-turn evaluation does not reflect the multi-turn nature of typical enterprise analytics workflows

## Confidence
- **High Confidence**: The documented performance gap between single-table (60.95%) and multi-table (≥4 tables: 21.43%) queries
- **Medium Confidence**: The mechanism linking Chinese ellipsis and business jargon to operator misalignment
- **Medium Confidence**: The schema-width scaling effect as the primary driver of accuracy decay

## Next Checks
1. Comparator Validation: Construct test cases where semantically distinct queries produce identical results and semantically equivalent queries have syntactic variations
2. Synthetic Data Realism: Compare enterprise-inspired synthetic questions against actual enterprise query logs
3. Cross-Dialect Generalization: Evaluate model performance on Falcon using different SQL dialects to determine whether accuracy patterns are dialect-specific