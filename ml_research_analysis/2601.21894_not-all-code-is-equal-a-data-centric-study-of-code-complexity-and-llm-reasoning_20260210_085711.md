---
ver: rpa2
title: 'Not All Code Is Equal: A Data-Centric Study of Code Complexity and LLM Reasoning'
arxiv_id: '2601.21894'
source_url: https://arxiv.org/abs/2601.21894
tags:
- code
- complexity
- reasoning
- language
- fine-tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates whether structural properties of code
  used during fine-tuning influence the reasoning abilities of large language models
  (LLMs). The authors construct two complexity-controlled datasets: one varying solution
  structure for identical problems, and another varying problem difficulty, using
  cyclomatic complexity and logical lines of code as metrics.'
---

# Not All Code Is Equal: A Data-Centric Study of Code Complexity and LLM Reasoning

## Quick Facts
- arXiv ID: 2601.21894
- Source URL: https://arxiv.org/abs/2601.21894
- Authors: Lukas Twist; Shu Yang; Hanqi Yan; Jingzhi Gong; Di Wang; Helen Yannakoudakis; Jie M. Zhang
- Reference count: 36
- Primary result: Reasoning gains from code fine-tuning are non-uniform and non-monotonic, peaking at intermediate complexity levels

## Executive Summary
This paper investigates whether structural properties of code used during fine-tuning influence the reasoning abilities of large language models. The authors construct two complexity-controlled datasets: one varying solution structure for identical problems, and another varying problem difficulty, using cyclomatic complexity and logical lines of code as metrics. Across multiple model families and reasoning benchmarks, they find that reasoning gains from code fine-tuning are non-uniform and non-monotonic, peaking at intermediate complexity levels and degrading at both low and high extremes. Control datasets mixing all complexity levels are rarely optimal; in 83% of experiments, restricting fine-tuning to a specific complexity range yields better reasoning performance. These results challenge the assumption that greater code diversity is inherently beneficial and highlight the importance of carefully selecting structurally appropriate code for improving LLM reasoning.

## Method Summary
The study constructs two complexity-controlled datasets (CODENET and INSTRUCT) from existing code corpora, filtering to Python/JavaScript/Java samples and splitting each into 12 bins based on cyclomatic complexity (CC) and logical lines of code (LLOC). Each bin contains 8,087 samples. Models are fine-tuned using LoRA adapters (r=16, α=16) on all linear layers for 2 epochs with AdamW optimizer (lr=2×10⁻⁵, cosine schedule). Evaluation uses greedy decoding on 6 reasoning benchmarks (GSM8K, MATH401, MATH500, GPQA, BBEH-MINI, HLE) with standardized answer extraction via math-verify library. Results are compared against a natural language baseline (ShareGPT) and mixed-control datasets.

## Key Results
- Reasoning performance shows non-monotonic relationship with code complexity, typically peaking at intermediate CC levels (~10)
- Restricting fine-tuning to specific complexity ranges outperforms mixed-control datasets in 83% of experiments
- Problem-driven complexity (harder problems) transfers more reliably to reasoning than solution-driven complexity (complex implementations of simple problems)
- Optimal complexity thresholds are model-specific, with different model families showing distinct patterns

## Why This Works (Mechanism)

### Mechanism 1
Structural code complexity functions as an implicit chain-of-thought scaffold during fine-tuning. Code with richer control flow exposes models to explicit multi-step decomposition patterns, where cyclomatic complexity captures independent execution paths that mirror reasoning traces. Models internalize control-flow structures as reusable reasoning patterns that transfer to non-code tasks.

### Mechanism 2
Intermediate complexity levels optimize reasoning because they balance signal richness with learnability. Very simple code lacks sufficient branching structure for useful reasoning patterns, while very complex code introduces brittle control flow that obscures the reasoning signal. Peak performance at intermediate CC (~10) reflects a sweet spot where structure is rich enough to demonstrate decomposition but not so complex that learning destabilizes.

### Mechanism 3
Problem-driven complexity transfers better than solution-driven complexity because task difficulty aligns with reasoning demands. When complexity arises from intrinsically harder problems, models encounter code that must solve more challenging tasks, which more closely mirrors the demands of downstream reasoning benchmarks. Solution-driven complexity varies code style without changing task difficulty, providing weaker transfer signal.

## Foundational Learning

- **Cyclomatic Complexity (CC)**: Primary metric for quantifying control-flow complexity by counting independent execution paths through code. Essential for interpreting why CC-based splits show smoother performance patterns than LLOC. Quick check: Given a function with 3 if-statements in sequence (non-nested), what is its cyclomatic complexity?

- **Non-monotonic optimization landscape**: Central finding that reasoning performance does not improve linearly with code complexity, with intermediate values outperforming extremes due to signal-to-noise tradeoffs. Quick check: Why might both CC=3 and CC=50 code underperform compared to CC=10 code for the same model?

- **Transfer learning from code to reasoning**: Hypothesis that code structure transfers to general reasoning by inducing reusable computational patterns similar to chain-of-thought, without explicit reasoning annotations. Quick check: What must be true about a model's internal representations for code-based fine-tuning to improve performance on GSM8K math problems?

## Architecture Onboarding

- **Component map**: Radon (Python) -> escomplex (JavaScript) -> PMD (Java) -> compute CC/LLOC -> split into bins -> LoRA fine-tuning -> evaluation with math-verify

- **Critical path**: 
  1. Metric calculation must use per-function CC aggregated by maximum (not mean) to reflect dominant complexity
  2. Dataset splits must be balanced across languages to avoid language confounds
  3. Natural language baseline must run in parallel to isolate code-specific effects

- **Design tradeoffs**: 
  - Small dataset (8K samples) enables controlled comparison but may underrepresent effects at scale
  - Two metrics (CC + LLOC) add interpretability but CC alone may be sufficient for structural signal
  - Open-weight models ensure reproducibility but findings may not transfer to proprietary architectures

- **Failure signatures**:
  - U-shaped curves (Mistral-7B pattern): May indicate model architecture interacts differently with complexity
  - Near-perfect negative correlations (Llama on some benchmarks): High complexity actively degrades reasoning
  - CTRL outperforming all splits: Complexity range may be wrong for this model

- **First 3 experiments**:
  1. Replicate Qwen2.5-7B on INSTRUCT CC splits with GSM8K evaluation to verify LOW-to-MID peak exists
  2. Run ablation with only Python samples to isolate language effects against multilingual baseline
  3. Fine-tune on synthetic code where CC is artificially controlled to disentangle CC from semantic difficulty

## Open Questions the Paper Calls Out

### Open Question 1
Do the observed complexity effects generalize to larger fine-tuning datasets and continued pretraining regimes? The authors study fine-tuning on relatively small datasets (8,087 samples per split) to enable controlled comparisons, but effects may differ at larger scales. Evidence would require replicating experiments with datasets 10-100× larger or during continued pretraining.

### Open Question 2
Why does Mistral-7B exhibit a U-shaped performance curve, benefiting most from either very simple or very complex code? The paper documents this anomalous behavior but does not investigate whether it stems from architectural differences, training data composition, or optimization dynamics. Evidence would require ablation studies comparing attention patterns and internal representations across complexity levels.

### Open Question 3
Can hybrid complexity metrics combining control flow, data flow, and semantic patterns better predict reasoning gains than cyclomatic complexity alone? The paper suggests more expressive measures may better capture aspects of code that support reasoning, but only CC and LLOC were tested. Evidence would require constructing fine-tuning splits using additional metrics and comparing downstream reasoning performance.

### Open Question 4
Can optimal complexity ranges for a given model be predicted a priori without empirical search? The paper demonstrates that optimal complexity is model-specific but requires running multiple fine-tuning experiments to identify. Evidence would require developing and validating a predictive model based on model architecture or pretraining history.

## Limitations
- Small fine-tuning dataset (8,087 samples per split) and short training duration (2 epochs) may not capture effects at scale
- Open-weight models (3B-14B) limit generalizability to proprietary frontier models
- Exclusive focus on Python/JavaScript/Java excludes potentially informative languages like C++ or Rust
- Answer extraction pipeline relies on \boxed{} formatting which may not work uniformly across all model generations

## Confidence

- **High Confidence**: Non-monotonic relationship between code complexity and reasoning performance, and finding that complexity-controlled fine-tuning outperforms mixed-control datasets in 83% of experiments
- **Medium Confidence**: Mechanism explaining intermediate complexity optimization and claim that problem-driven complexity transfers better than solution-driven complexity
- **Low Confidence**: Generalizability to proprietary models and optimal complexity thresholds for different model families

## Next Checks

1. **Scale validation**: Replicate the INSTRUCT CC splits with Qwen2.5-72B on the full CodeNet dataset (millions of samples) to test whether the non-monotonic complexity curve persists at scale

2. **Architecture ablation**: Fine-tune Llama-3.1-8B with and without LoRA (full fine-tuning) on the same complexity splits to determine whether adapter architecture influences the optimal complexity range

3. **Synthetic control**: Create artificially controlled code where cyclomatic complexity is manipulated independently of semantic content and test whether pure structural complexity without task difficulty provides reasoning transfer