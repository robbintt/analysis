---
ver: rpa2
title: Self Voice Conversion as an Attack against Neural Audio Watermarking
arxiv_id: '2601.20432'
source_url: https://arxiv.org/abs/2601.20432
tags:
- watermarking
- speech
- audio
- speaker
- self
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates self voice conversion (VC) as a universal,\
  \ content-preserving attack against audio watermarking systems. The authors evaluate\
  \ whether self VC\u2014where speech is remapped to the same identity while altering\
  \ acoustic characteristics\u2014can effectively disrupt embedded watermarks in state-of-the-art\
  \ watermarking approaches."
---

# Self Voice Conversion as an Attack against Neural Audio Watermarking

## Quick Facts
- **arXiv ID**: 2601.20432
- **Source URL**: https://arxiv.org/abs/2601.20432
- **Reference count**: 40
- **Primary result**: Self voice conversion consistently disrupts watermark extraction accuracy to near-random guessing while preserving speaker identity and content

## Executive Summary
This paper investigates self voice conversion (VC) as a universal, content-preserving attack against audio watermarking systems. The authors evaluate whether self VC—where speech is remapped to the same identity while altering acoustic characteristics—can effectively disrupt embedded watermarks in state-of-the-art watermarking approaches. They conduct experiments on five representative watermarking methods, comparing their performance under no attack, baseline vocoder attacks, and the proposed self VC attack using kNN-VC and RVC models. The results show that self VC consistently drives bitwise watermark extraction accuracy close to random guessing (near 0.5 error) across all tested methods, while preserving speaker identity, linguistic content, and perceptual quality. This demonstrates that self VC is a severe, realistic threat to modern audio watermarking systems.

## Method Summary
The study proposes self voice conversion as an attack strategy where speech audio is converted to the same speaker identity but with modified acoustic characteristics. The attack uses two VC models: kNN-VC (k-Nearest Neighbors Voice Conversion) and RVC (ResNet-based Voice Conversion). These models transform input speech while maintaining the original speaker identity but altering the voice characteristics. The converted audio is then processed by five representative watermarking methods to evaluate extraction accuracy. The authors compare this attack against baseline vocoder attacks and measure performance across multiple metrics including bitwise accuracy, speaker verification scores, character error rates, and perceptual quality measures (PESQ and ViSQOL).

## Key Results
- Self VC consistently drives bitwise watermark extraction accuracy to near-random guessing levels (approximately 0.5 error rate) across all five tested watermarking methods
- Speaker identity is preserved with speaker verification scores comparable to clean audio (SV score > 0.7 for all methods)
- Linguistic content is maintained with character error rates similar to baseline vocoder attacks (CER < 25%)
- Perceptual quality remains high with PESQ and ViSQOL scores comparable to or better than baseline attacks

## Why This Works (Mechanism)
Self voice conversion disrupts watermarking by altering the acoustic features that watermarking systems rely on for extraction, while maintaining the same speaker identity. The conversion process changes the spectral and temporal characteristics of the speech signal in ways that interfere with the watermark embedding and extraction mechanisms, which typically depend on subtle acoustic patterns. Since the attack preserves speaker identity, it avoids detection as a different speaker while still destroying the watermark information through acoustic modifications that are perceptually acceptable.

## Foundational Learning
**Voice Conversion**: Technology that transforms speech from one speaker to sound like another speaker while preserving linguistic content - needed to understand how speech characteristics can be modified without changing content; quick check: verify that converted speech maintains same words but different voice quality
**Audio Watermarking**: Technique for embedding imperceptible data into audio signals for copyright protection or authentication - needed to understand what's being attacked; quick check: confirm watermark is inaudible and robust to common distortions
**Neural Vocoders**: Deep learning models that generate high-quality speech waveforms from acoustic features - needed as baseline attack comparison; quick check: ensure vocoder quality is comparable to VC models
**Speaker Verification**: System for confirming whether two audio samples belong to the same speaker - needed to verify identity preservation; quick check: verify SV scores remain high after attack
**Perceptual Quality Metrics**: Objective measures (PESQ, ViSQOL) that predict human perception of audio quality - needed to ensure attacks are realistic; quick check: confirm quality degradation is minimal

## Architecture Onboarding

**Component Map**: Source Audio -> VC Model (kNN-VC/RVC) -> Converted Audio -> Watermarking Method -> Extraction Result

**Critical Path**: The attack pipeline flows from source audio through the VC model to the converted audio, which is then processed by the watermarking method. The critical vulnerability lies in the watermarking method's inability to extract embedded data from converted audio despite preserving speaker identity.

**Design Tradeoffs**: The attack balances between effectiveness (disrupting watermark extraction) and stealth (preserving speaker identity and perceptual quality). More aggressive VC modifications would likely destroy watermarks more effectively but risk changing speaker identity or introducing perceptible artifacts.

**Failure Signatures**: The primary failure mode is the watermarking system incorrectly extracting the embedded data, resulting in bitwise accuracy near 0.5 (random guessing). Secondary failures include potential speaker verification score degradation or perceptual quality reduction, though the study shows these are minimal.

**First Experiments**:
1. Test self VC attack on a simple time-domain watermarking method not included in the original five
2. Apply the attack under realistic conditions including background noise and compression
3. Evaluate whether adaptive watermarking embedding strategies can resist self VC attacks

## Open Questions the Paper Calls Out
None

## Limitations
- The study is limited to specific VC models (kNN-VC and RVC) and may not represent all possible VC approaches
- The attack assumes access to clean source audio and VC models, which may not be realistic in all adversarial scenarios
- While perceptual quality is assessed via standard metrics, subjective listening tests across diverse audio conditions were not reported
- The scope is limited to five representative watermarking methods, which may not cover the full diversity of watermarking approaches

## Confidence
- **High confidence**: Self VC consistently degrades watermark extraction accuracy across all tested methods
- **Medium confidence**: Self VC preserves speaker identity and linguistic content as claimed
- **Medium confidence**: Self VC represents a realistic threat model given current VC accessibility

## Next Checks
1. Test self VC attack performance against additional watermarking methods not included in the original study, particularly those using different embedding domains (e.g., time-domain vs. transform-domain approaches)
2. Evaluate attack robustness under real-world conditions including background noise, compression, and transmission artifacts
3. Assess whether watermarking methods can be made resilient to self VC through defensive signal processing or adaptive embedding strategies