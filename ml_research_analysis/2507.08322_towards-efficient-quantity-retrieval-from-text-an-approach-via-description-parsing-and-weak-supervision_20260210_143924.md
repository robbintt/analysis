---
ver: rpa2
title: Towards Efficient Quantity Retrieval from Text:An Approach via Description
  Parsing and Weak Supervision
arxiv_id: '2507.08322'
source_url: https://arxiv.org/abs/2507.08322
tags:
- quantity
- description
- parsing
- retrieval
- quantities
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel quantity retrieval task aimed at
  extracting quantitative facts from unstructured documents using natural language
  queries. The key challenge lies in understanding quantity semantics within context,
  where textual similarity does not always reflect semantic relevance.
---

# Towards Efficient Quantity Retrieval from Text:An Approach via Description Parsing and Weak Supervision

## Quick Facts
- arXiv ID: 2507.08322
- Source URL: https://arxiv.org/abs/2507.08322
- Reference count: 26
- One-line primary result: A novel quantity retrieval framework using description parsing and weak supervision achieves 64.66% Exist@1 accuracy on financial annual reports.

## Executive Summary
This paper introduces a novel quantity retrieval task aimed at extracting quantitative facts from unstructured documents using natural language queries. The key challenge lies in understanding quantity semantics within context, where textual similarity does not always reflect semantic relevance. The authors propose a two-stage framework: first, converting documents into structured (description, quantity) pairs via a description parsing model trained on manually annotated data; second, enhancing retrieval with weak supervision based on quantity co-occurrence to generate paraphrase datasets. A ranking model trained on this data improves semantic matching. Evaluated on financial annual reports, the approach achieves a top-1 retrieval accuracy of 64.66%, significantly outperforming baseline methods (30.98% for BM25 on sentences). This demonstrates the effectiveness of semantic understanding and weak supervision in retrieving long-tail quantitative facts buried in unstructured text.

## Method Summary
The method involves a two-stage framework for quantity retrieval. First, a description parsing model (BERT + Bi-LSTM) converts unstructured text into structured (description, quantity) pairs by extracting discontinuous text segments using BIEO tagging. Second, weak supervision is applied by identifying quantity values that coincide across different descriptions, assuming these represent paraphrases of the same fact. This generates a training dataset for a Siamese BERT ranking model, which is fine-tuned using contrastive loss to improve semantic matching between queries and descriptions.

## Key Results
- The proposed approach achieves a top-1 retrieval accuracy (Exist@1) of 64.66% on financial annual reports.
- Description-level parsing (Cq-BM25) significantly outperforms sentence-level retrieval (Cs-BM25), with 58.02% vs 30.98% Exist@1 accuracy.
- The weak supervision approach (Cq-BERT-WS) improves retrieval accuracy by 6% absolute over Cq-BM25, demonstrating the effectiveness of learned semantic matching.

## Why This Works (Mechanism)

### Mechanism 1: Description Parsing for Semantic Isolation
- **Claim:** Isolating the textual description of a quantity from its surrounding sentence significantly improves retrieval signal-to-noise ratio compared to sentence-level retrieval.
- **Mechanism:** A sequence labeling model (BERT + Bi-LSTM) extracts discontinuous text segments (e.g., subject, time, indicator) to form a concise description. This filters out irrelevant context (connectives, other quantities) that dilute semantic matching in standard keyword search (BM25).
- **Core assumption:** The sentence structure allows for the extraction of a "fully and concisely" descriptive span, and the BIEO tagging scheme effectively captures discontinuous segments.
- **Evidence anchors:**
  - [abstract]: "We introduce a framework based on description parsing that converts text into structured (description, quantity) pairs..."
  - [section 5.4]: Shows Cq-BM25 (description-level) achieving 58.02% Exist@1 vs 30.98% for Cs-BM25 (sentence-level).
  - [corpus]: Neighbor papers like "Context-Efficient Retrieval" support the general principle of factual decomposition for better context usage, though specific quantity-parsing evidence is absent.
- **Break condition:** The mechanism degrades if the source text is grammatically malformed or if the quantity's "description" requires document-level context (e.g., references to previous paragraphs), which the sentence-bound model cannot capture.

### Mechanism 2: Value-Coincidence Weak Supervision
- **Claim:** Quantity values serve as a high-precision heuristic for linking paraphrased descriptions, enabling the automated construction of a semantic ranking dataset without manual labeling.
- **Mechanism:** The system identifies pairs of records $(d_i, q_i)$ and $(d_j, q_j)$ where $q_i = q_j$ (same value) and $d_i, d_j$ share textual similarity (via BM25). It assumes these pairs represent the same fact, treating them as positive training examples for a paraphrase detection model.
- **Core assumption:** Assumption: $P(F|D,Q) \approx 1$â€”the probability of two textually similar descriptions sharing the exact same numeric value by chance is negligible in a large corpus.
- **Evidence anchors:**
  - [section 3.2]: Provides the probabilistic derivation that $P(D|Q, \bar{F}) \to 0$ when vocabulary size is large.
  - [section 5.4]: Cq-BERT-WS (weak supervision) improves Exist@1 by 6% absolute over Cq-BM25, confirming the learned semantic matching works.
  - [corpus]: Weak or missing. Neighbors discuss retrieval but do not validate the specific "value-coincidence" heuristic for paraphrase mining.
- **Break condition:** Fails if the corpus contains many "common" numbers (e.g., years like "2021" or rounded percentages like "10%") used in different contexts, potentially generating false positive paraphrase pairs.

### Mechanism 3: Dense Retrieval Fine-Tuning
- **Claim:** Fine-tuning a Siamese network on the weakly supervised paraphrase data enables the model to bridge the lexical gap between user queries and document descriptions.
- **Mechanism:** The ranking model uses contrastive loss to maximize cosine similarity between embeddings of value-matched pairs while pushing apart "confusing" descriptions (retrieved by BM25 but with different values). This aligns the vector space with semantic intent rather than just keyword overlap.
- **Core assumption:** The pre-trained language model (BERT) possesses sufficient underlying knowledge to generalize from the noisy weak labels to unseen queries.
- **Evidence anchors:**
  - [section 3.3]: Describes the Siamese Network and contrastive loss setup.
  - [section 5.4]: Cq-BERT-WS significantly outperforms Cq-BERT-P (pre-trained on external data), indicating the domain-specific weak supervision is the critical factor.
  - [corpus]: General dense retrieval concepts are supported by neighbors (e.g., "CRED-SQL"), but specific contrastive learning on quantity pairs is specific to this paper.
- **Break condition:** Fails if the query uses terminology completely absent from the corpus (OOV problem), or if the "confusing" negatives sampled by BM25 are actually semantically correct but just have different values (false negatives).

## Foundational Learning

- **Concept: Sequence Labeling (BIEO Tagging)**
  - **Why needed here:** The Description Parsing Model relies on this to identify discontinuous text spans (the description) within a sentence.
  - **Quick check question:** How does the model handle a sentence with two quantities that share some descriptive context but differ in others? (Answer: It processes them one at a time using specific [START]/[END] markers).

- **Concept: Weak Supervision / Distant Supervision**
  - **Why needed here:** Essential for understanding how the system generates training data automatically using the "Value-Coincidence" heuristic.
  - **Quick check question:** What is the primary risk when using quantity matching as a proxy for semantic equivalence? (Answer: False positives from coincidental number matches).

- **Concept: Siamese Networks & Contrastive Loss**
  - **Why needed here:** Forms the architecture of the Ranking Model used to compare query embeddings against description embeddings.
  - **Quick check question:** Why is contrastive loss preferred here over standard Cross-Entropy? (Answer: It specifically optimizes the distance metric between pairs, pulling positive pairs closer and pushing negative pairs apart).

## Architecture Onboarding

- **Component map:** Parser (Offline) -> Weak Supervision Generator (Offline) -> Ranking Model (Offline/Online) -> Retrieval Service (Online)

- **Critical path:** The quality of the **Description Parser** is the upstream bottleneck. If the parser fails to correctly identify the text span (e.g., missing the "Time" or "Subject"), the Ranking Model cannot recover the error because the relevant text is simply missing from the index.

- **Design tradeoffs:**
  - **Sentence-level vs. Description-level:** The paper trades the simplicity of indexing whole sentences for the precision of indexing parsed descriptions. This sacrifices recall (if parsing errors occur) for higher precision.
  - **Manual vs. Weak Supervision:** They trade the cost of massive manual annotation for the noise of weak supervision. The evidence suggests this is a net positive for the retrieval task.

- **Failure signatures:**
  - **Parser Drift:** If the parser starts hallucinating segments or missing discontinuous spans (e.g., "Apple... revenue"), retrieval fails.
  - **Numeric Coincidence Noise:** If the weak supervision step generates pairs like (Revenue: 2021, Year: 2021) because the value "2021" matches, the ranking model learns false associations.
  - **Domain Shift:** The ranking model is fine-tuned on financial reports; it may fail on casual queries or different domains (e.g., scientific papers).

- **First 3 experiments:**
  1. **Parser Unit Test:** Run the parser on 50 manually checked sentences with multiple quantities to verify BIEO tagging accuracy on discontinuous spans.
  2. **Weak Supervision Audit:** Sample 20 pairs from the generated "paraphrase" dataset to verify if the "Value-Coincidence" assumption holds (i.e., are they actually paraphrases?).
  3. **End-to-End Baseline:** Compare Cq-BM25 (keywords on parsed descriptions) vs. Cs-BM25 (keywords on sentences) on a held-out set to validate the "Parsing" contribution before enabling the neural ranker.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can Large Language Models (LLMs) replace BERT-based architectures to enhance the accuracy of description parsing and overall retrieval performance?
- Basis in paper: [explicit] Section 7 states that "quantity understanding could benefit from the advances of large language models (LLMs)" and suggests replacing the current BERT-based method.
- Why unresolved: The authors currently utilize a BERT-BiLSTM architecture; they identify the integration of LLMs as a specific avenue for future work but do not experiment with it.
- What evidence would resolve it: A comparative evaluation of the current parsing model against an LLM-based approach on the 42,130-sentence dataset, measuring F1 scores and retrieval accuracy.

### Open Question 2
- Question: Does document-level information extraction significantly improve performance when descriptive factors span across sentence boundaries?
- Basis in paper: [explicit] Section 7 notes that "descriptive factors may span sentences or even paragraphs, requiring document-level information extraction," which the current sentence-level model does not address.
- Why unresolved: The current framework treats description parsing as a sequence labeling task strictly within the scope of a single sentence.
- What evidence would resolve it: Implementation of a cross-sentence extraction model and a subsequent comparison of retrieval recall rates for quantities requiring context from surrounding paragraphs.

### Open Question 3
- Question: Can the robustness of the paraphrase ranking model be improved by integrating methods for learning from noisy labels and resolving abbreviations?
- Basis in paper: [explicit] Section 7 lists "abbreviation resolution and learning from noisy weak labels" as specific future work to improve the ranking model's training on weakly supervised data.
- Why unresolved: The current weak supervision relies on strict value-coincidence, which generates noise (e.g., distinct facts sharing a value) that standard contrastive loss may not fully handle.
- What evidence would resolve it: Ablation studies testing noise-robust loss functions or abbreviation normalization modules within the ranking model training pipeline.

## Limitations
- The approach relies on manually annotated data for the description parser, which may not generalize beyond financial reports.
- The weak supervision mechanism assumes low probability of value coincidence across unrelated descriptions, but this assumption's validity depends heavily on corpus domain and value distribution.
- The parsing approach is constrained by sentence boundaries, potentially missing document-level context crucial for certain quantity descriptions.

## Confidence
- **High Confidence:** The effectiveness of description-level parsing over sentence-level retrieval (supported by Exist@1 metrics showing 58.02% vs 30.98%).
- **Medium Confidence:** The value-coincidence weak supervision approach, as it's theoretically sound but lacks extensive validation in the corpus.
- **Medium Confidence:** The dense retrieval fine-tuning, as improvements over pre-trained models are shown but the contrastive loss setup's optimality isn't extensively tested.

## Next Checks
1. **Parser Generalization Test:** Evaluate the description parser on a held-out financial report not seen during training to measure cross-document generalization of BIEO tagging.
2. **Weak Supervision Quality Audit:** Sample 50 pairs from the automatically generated paraphrase dataset and manually verify if shared values truly indicate semantic equivalence.
3. **Numeric Coincidence Analysis:** Calculate the frequency of identical quantity values appearing in unrelated contexts within the corpus to quantify false positive risk in the weak supervision step.