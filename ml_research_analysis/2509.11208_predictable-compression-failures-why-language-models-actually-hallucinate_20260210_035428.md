---
ver: rpa2
title: 'Predictable Compression Failures: Why Language Models Actually Hallucinate'
arxiv_id: '2509.11208'
source_url: https://arxiv.org/abs/2509.11208
tags:
- information
- permutation
- nats
- answer
- theorem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper resolves a fundamental paradox in large language models:
  they achieve near-Bayesian statistical performance yet systematically violate permutation
  invariance on exchangeable data. The authors show that transformers minimize expected
  conditional description length over permutations rather than the permutation-invariant
  description length, making them "Bayesian in expectation, not in realization." The
  core theoretical contribution is the Expectation-level Decompression Law (EDFL),
  which quantifies precisely how much information is required to lift an event''s
  probability from prior to posterior mass.'
---

# Predictable Compression Failures: Why Language Models Actually Hallucinate

## Quick Facts
- arXiv ID: 2509.11208
- Source URL: https://arxiv.org/abs/2509.11208
- Authors: Leon Chlon; Ahmed Karim; Maggie Chlon
- Reference count: 30
- Primary result: Transformers minimize expected conditional description length over permutations rather than permutation-invariant description length, making hallucinations quantifiable through information insufficiency.

## Executive Summary
This paper resolves a fundamental paradox in large language models by showing that while transformers achieve near-Bayesian statistical performance, they systematically violate permutation invariance on exchangeable data. The authors demonstrate that transformers minimize expected conditional description length over permutations rather than the permutation-invariant description length, making them "Bayesian in expectation, not in realization." The core theoretical contribution is the Expectation-level Decompression Law (EDFL), which quantifies precisely how much information is required to lift an event's probability from prior to posterior mass. This transforms hallucination from an unpredictable failure mode into a quantifiable consequence of information insufficiency.

## Method Summary
The paper develops an information-theoretic framework analyzing transformer behavior through the lens of minimum description length and Bayesian inference. The authors derive that transformers minimize expected conditional description length over permutations rather than permutation-invariant description length, creating systematic information insufficiency. They introduce the Expectation-level Decompression Law (EDFL) to quantify the information gap between prior and posterior distributions. Empirical validation uses synthetic permutation experiments across different transformer models, measuring permutation dispersion, mixture effects, and dose-response relationships. The framework is then operationalized through three planners: Bits-to-Trust, Risk-of-Hallucination, and Information Sufficiency Ratio.

## Key Results
- Permutation dispersion follows a logarithmic relationship: likelihood/accuracy improves with permutation count, following +b ln n with b ≈ 0.377 (Qwen2-7B) and b ≈ 0.147 (Llama-3.1-8B)
- Permutation mixtures improve ground-truth likelihood and accuracy by providing additional context
- Randomized dose-response experiments show hallucinations drop by approximately 0.13 per additional nat of information
- Pre-specified audit with Information Sufficiency Ratio = 1.0 achieves near-0% hallucinations through calibrated refusal at 24% abstention

## Why This Works (Mechanism)
The paper shows that transformers operate under a fundamental mismatch: they optimize for expected performance over data permutations while users expect permutation-invariant behavior. This creates systematic information insufficiency that manifests as hallucinations. The Expectation-level Decompression Law quantifies exactly how much additional information is needed to transform uncertain priors into reliable posteriors, making hallucination predictable and controllable through information-theoretic measures rather than ad-hoc heuristics.

## Foundational Learning
- **Permutation Invariance**: Statistical properties that should remain constant under data reordering. Needed to understand why transformers fail on seemingly simple exchangeable data. Quick check: Test model performance on shuffled input sequences versus original order.
- **Minimum Description Length (MDL)**: Principle that the best model minimizes the total description length of data plus model. Needed to connect compression efficiency with probabilistic inference. Quick check: Compare description lengths across different model architectures on same dataset.
- **Information Sufficiency Ratio (ISR)**: Ratio of available information to required information for reliable inference. Needed to quantify when models should abstain versus answer. Quick check: Measure ISR thresholds for different accuracy targets.
- **Conditional Description Length**: Information required to describe data given a model. Needed to understand transformer's optimization objective. Quick check: Calculate conditional description lengths for different input permutations.

## Architecture Onboarding
- **Component Map**: Input tokens → Transformer layers → Self-attention → Feed-forward networks → Output distribution
- **Critical Path**: Token embedding → Multi-head attention → Layer normalization → Position-wise feed-forward → Final softmax
- **Design Tradeoffs**: Depth vs width for information retention, attention mechanism for permutation sensitivity, training objectives for statistical consistency
- **Failure Signatures**: Systematic permutation sensitivity, context-dependent reliability, information insufficiency patterns
- **First Experiments**: 1) Measure permutation dispersion across different model sizes, 2) Test EDFL predictions on naturalistic datasets, 3) Implement ISR-based abstention mechanisms

## Open Questions the Paper Calls Out
None

## Limitations
- Empirical validation relies heavily on synthetic permutation experiments that may not capture real-world hallucination patterns
- Model-specific permutation dispersion parameters (b ≈ 0.377 vs 0.147) suggest the framework may not be universally applicable
- Quantitative claims about exact bit requirements for reliability targets may be overly precise given real-world complexity

## Confidence
High: The mathematical framework connecting information theory, Bayesian inference, and permutation invariance is internally consistent and provides valuable conceptual insights into transformer behavior.
Medium: The empirical demonstration that permutation mixtures improve performance and that information sufficiency correlates with hallucination reduction is convincing within the experimental setup, but generalizability to broader contexts needs verification.
Low: The quantitative claims about exact bit requirements for reliability targets and the specific relationship between information sufficiency ratio and abstention rates may be overly precise given the complexity of real-world language generation.

## Next Checks
1. Test the EDFL predictions on naturalistic datasets with known ground truth (e.g., question-answering datasets, code generation tasks) to verify whether the information-theoretic framework generalizes beyond synthetic permutations.
2. Conduct ablation studies varying model size, architecture (decoder-only vs encoder-decoder), and training objectives to determine whether the permutation dispersion relationship holds across different transformer variants and whether the b parameter correlates with model properties.
3. Implement the operational planners (Bits-to-Trust, Risk-of-Hallucination, Information Sufficiency Ratio) in deployed systems and measure their practical utility in reducing harmful hallucinations while maintaining acceptable coverage, particularly in safety-critical applications.