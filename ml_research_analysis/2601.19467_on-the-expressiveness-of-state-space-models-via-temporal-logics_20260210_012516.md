---
ver: rpa2
title: On the Expressiveness of State Space Models via Temporal Logics
arxiv_id: '2601.19467'
source_url: https://arxiv.org/abs/2601.19467
tags:
- diagonal
- languages
- which
- time-invariant
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper establishes a formal hierarchy of expressive power\
  \ for State Space Models (SSMs) by mapping different SSM variants to fragments of\
  \ linear temporal logic over finite traces (LTLf). The authors prove that diagonal\
  \ SSMs with fixed precision capture exactly the star-free languages (PLTLf), while\
  \ diagonal SSMs with logarithmic precision can recognize counting languages (PLTLf[\u2190\
  \u2212]) including non-regular languages."
---

# On the Expressivity of State Space Models via Temporal Logics

## Quick Facts
- arXiv ID: 2601.19467
- Source URL: https://arxiv.org/abs/2601.19467
- Reference count: 39
- Primary result: SSMs form a formal hierarchy matching fragments of LTLf logic, from star-free to counting languages

## Executive Summary
This paper establishes a formal hierarchy of expressive power for State Space Models (SSMs) by mapping different SSM variants to fragments of linear temporal logic over finite traces (LTLf). The authors prove that diagonal SSMs with fixed precision capture exactly the star-free languages (PLTLf), while diagonal SSMs with logarithmic precision can recognize counting languages (PLTLf[←−#]) including non-regular languages. Time-invariant SSMs are shown to compute modular predicates, recognizing languages in UN-PLTLf[MOD] (regular languages in AC0), and mixed SSMs combine these capabilities.

The theoretical framework provides systematic translations between logical formulas and SSM architectures, demonstrating tight non-expressibility bounds and revealing fundamental architectural limitations. These results provide theoretical justification for the expressive differences observed between SSM variants and situate them within the broader transformer expressiveness landscape, showing structural alignment with unique hard-attention transformers.

## Method Summary
The paper employs a formal logic-based approach to analyze SSM expressivity. The methodology involves establishing bisimulations between SSM computations and temporal logic satisfaction, proving that certain SSM variants can be translated to specific LTLf fragments and vice versa. The authors define SSM architectures with varying constraints (diagonal vs. full matrices, time-invariant vs. time-varying, fixed vs. logarithmic precision) and systematically characterize their computational power through formal reductions to known logical fragments. Expressivity hierarchies are established through mutual simulation arguments and non-expressibility bounds are proven via language-theoretic arguments.

## Key Results
- Diagonal SSMs with fixed precision capture exactly star-free languages (PLTLf), but cannot recognize regular languages like (aa)*
- Diagonal SSMs with logarithmic precision can recognize counting languages including non-regular languages through counting quantifiers
- Time-invariant SSMs compute modular predicates, recognizing UN-PLTLf[MOD] languages (regular languages in AC0)
- Mixed SSM architectures combine capabilities of both diagonal and time-invariant variants
- SSMs structurally align with unique hard-attention transformers in their expressive power

## Why This Works (Mechanism)

The mechanism works because SSMs can be viewed as finite-state transducers with specific structural constraints. By fixing the matrix structure (diagonal vs. full) and parameter variability (time-invariant vs. time-varying), the paper constrains the SSM's computational path through its state space in ways that directly correspond to logical expressivity fragments. The diagonal constraint limits state interactions, while time-invariance creates periodic behavior that maps to modular arithmetic in logic. The precision bounds determine whether the SSM can count or only track bounded patterns.

## Foundational Learning

Linear Temporal Logic over Finite Traces (LTLf):
- Why needed: Provides the logical framework for characterizing SSM expressivity
- Quick check: Can verify that (a U b) means "a holds until b becomes true"

State Space Models (SSMs):
- Why needed: Core computational model being analyzed
- Quick check: Can express x(t+1) = Ax(t) + Bu(t) as the basic SSM recurrence

Star-free languages:
- Why needed: The minimal fragment of regular languages that diagonal fixed-precision SSMs can capture
- Quick check: Can verify that (ab)* is star-free while (aa)* is not

Counting languages and modular predicates:
- Why needed: Higher expressivity classes achieved by SSM variants with different architectures
- Quick check: Can verify that "number of a's mod 3 = 0" is a modular predicate

## Architecture Onboarding

Component map: SSM parameters -> State transitions -> Output computation -> Language recognition
Critical path: Input sequence → SSM state evolution → Final state → Output classification
Design tradeoffs: Diagonal vs. full matrices (expressivity vs. efficiency), time-invariant vs. time-varying (simplicity vs. power), fixed vs. logarithmic precision (bounded vs. unbounded counting)
Failure signatures: Fixed-precision SSMs fail on counting tasks; diagonal SSMs fail on nested dependencies; time-invariant SSMs fail on non-periodic patterns
First experiments:
1. Test diagonal fixed-precision SSM on star-free vs. non-star-free languages
2. Test time-invariant SSM on modular arithmetic recognition tasks
3. Test diagonal logarithmic-precision SSM on counting languages with nested dependencies

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions in the provided content.

## Limitations

The main uncertainty lies in the computational complexity of the translations between SSM architectures and their corresponding logical fragments. While the paper establishes expressivity results, the polynomial-time vs. exponential-time nature of these conversions is not explicitly characterized. Additionally, the assumption of bounded precision and finite traces may not fully capture real-world SSM behavior, particularly in continuous-time settings.

## Confidence

High confidence: The expressivity hierarchy results relating diagonal SSMs to star-free languages and time-invariant SSMs to modular predicates are well-established through rigorous formal proofs.

Medium confidence: The characterization of diagonal SSMs with logarithmic precision capturing counting languages is supported by theoretical arguments but relies on specific architectural assumptions that may not hold universally.

Low confidence: The structural alignment claims between SSMs and unique hard-attention transformers, while theoretically interesting, lack extensive empirical validation and depend on idealized transformer assumptions.

## Next Checks

1. Implement and benchmark the theoretical translations between SSM architectures and their logical representations to measure practical computational complexity and verify polynomial-time bounds.

2. Conduct empirical studies comparing the actual output distributions of different SSM variants against their theoretically characterized language classes using synthetic sequences with known patterns.

3. Extend the formal analysis to continuous-time SSMs and compare expressivity against the finite-trace LTL framework, particularly for models like S4 that use discretization.