---
ver: rpa2
title: 'Benchmarking LLMs for Political Science: A United Nations Perspective'
arxiv_id: '2502.14122'
source_url: https://arxiv.org/abs/2502.14122
tags:
- task
- draft
- voting
- united
- resolution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces UNBench, the first comprehensive benchmark
  for evaluating Large Language Models (LLMs) in political science using real United
  Nations Security Council (UNSC) data from 1994-2024. UNBench includes four interconnected
  tasks spanning the UN decision-making process: co-penholder judgment, representative
  voting simulation, draft adoption prediction, and representative statement generation.'
---

# Benchmarking LLMs for Political Science: A United Nations Perspective

## Quick Facts
- arXiv ID: 2502.14122
- Source URL: https://arxiv.org/abs/2502.14122
- Reference count: 27
- Introduces UNBench, the first comprehensive benchmark for evaluating LLMs in political science using real UN Security Council data

## Executive Summary
This paper introduces UNBench, the first comprehensive benchmark for evaluating Large Language Models (LLMs) in political science using real United Nations Security Council (UNSC) data from 1994-2024. UNBench includes four interconnected tasks spanning the UN decision-making process: co-penholder judgment, representative voting simulation, draft adoption prediction, and representative statement generation. Experiments with multiple models show that GPT-4o and DeepSeek-V3 outperform smaller models and traditional classifiers, achieving balanced accuracy scores of 0.823 and 0.724 respectively in voting simulation, while also demonstrating strong performance in draft adoption prediction (0.677 and 0.668 balanced accuracy). The benchmark reveals both the potential and limitations of current LLMs in complex political reasoning and diplomatic language generation, with smaller models struggling on tasks requiring nuanced geopolitical understanding. The work establishes a foundation for future research on AI applications in international governance.

## Method Summary
UNBench evaluates LLMs on four UNSC political science tasks using real Security Council records from 1994-2024. The dataset includes draft resolutions, voting records, and diplomatic speeches parsed from PDFs using LlamaParse. Tasks are mapped to UN decision stages: co-penholder judgment (multi-choice classification), representative voting simulation (3-class classification), draft adoption prediction (binary classification), and representative statement generation (text generation). Time-based train/test splits preserve temporal validity. Models are evaluated using accuracy, balanced accuracy, PR AUC, F1, ROUGE, and Sentence-BERT similarity. GPT-4o and DeepSeek-V3 are tested via API, while smaller models run on GPUs with temperature=0.0 for reproducibility.

## Key Results
- GPT-4o and DeepSeek-V3 significantly outperform smaller models and traditional classifiers across all tasks
- GPT-4o achieves 0.823 balanced accuracy in voting simulation, while DeepSeek-V3 reaches 0.724
- Smaller models struggle with nuanced geopolitical reasoning, with Llama-3.2-1B showing severe performance degradation
- Model performance drops significantly on post-2023 data, revealing temporal generalization limitations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Role-based prompting enables country-specific political simulation by activating latent geopolitical knowledge.
- Mechanism: When prompted with national identity (e.g., "You are representing France"), LLMs retrieve and synthesize training-era information about that country's diplomatic history, alliances, and voting patterns to generate contextually appropriate responses.
- Core assumption: Models encode sufficient historical diplomatic behavior in their parameters during pretraining.
- Evidence anchors: [section] "The LLM is prompted to assume the role of the author country c_a, given the text of r_i, and asked to choose exactly one co-penholder" (Task 1 formalization). [corpus] "Uncovering Political Bias in LLMs using Parliamentary Voting Records" examines similar role-based political behavior in LLMs.
- Break condition: Model lacks sufficient training data on specific countries; recency cutoff means post-training geopolitical shifts (e.g., conflicts after Oct 2023) degrade performance.

### Mechanism 2
- Claim: Model scale strongly correlates with capacity for nuanced geopolitical reasoning.
- Mechanism: Larger parameter count enables richer representations of complex diplomatic relationships, historical precedents, and multilateral dynamics—allowing synthesis across more contextual variables when predicting outcomes or generating statements.
- Core assumption: Scale provides both knowledge breadth and reasoning depth for political inference.
- Evidence anchors: [abstract] "Experiments with multiple models show that GPT-4o and DeepSeek-V3 outperform smaller models and traditional classifiers." [section] "GPT-4o (0.726) and DeepSeek-V3 (0.695) dominate [Task 1]... Smaller LLMs (e.g., Llama-3.2-1B: 0.581) lag significantly, while traditional models (BERT: 0.011) fail entirely."
- Break condition: Smaller models cannot overcome representational bottlenecks; architectural innovations may not substitute for scale in this domain.

### Mechanism 3
- Claim: Contextual metadata (authorship, dates) enhances political prediction by providing diplomatic signaling cues.
- Mechanism: Adding resolution author names and dates to prompts gives models additional geopolitical context—author identity signals likely coalition alignment and veto risks; dates ground predictions in temporal political climates.
- Core assumption: Models can integrate structured metadata with textual content for improved inference.
- Evidence anchors: [section] "Adding author metadata improves balanced accuracy (from 0.677 to 0.684) and G-Mean (from 0.729 to 0.753)" (Task 2 results). [section] "Adding date or author metadata boosts both recall and G-Mean, with the strongest performance under the date-enhanced setting" (Task 3).
- Break condition: Metadata introduces noise if model cannot correlate it with relevant historical patterns; conflicting signals may degrade performance.

## Foundational Learning

- Concept: **UN Security Council procedural mechanics**
  - Why needed here: Understanding veto power (5 permanent members), 15-member voting, and adoption thresholds (9+ affirmative votes, no veto) is essential for interpreting task constraints.
  - Quick check question: If 12 members vote "In Favour" but Russia votes "Against," is the resolution adopted?

- Concept: **Balanced accuracy for imbalanced classification**
  - Why needed here: UN voting data is highly imbalanced (e.g., Task 2: 17,020 "In Favour" vs. 16 "Against"); standard accuracy would be misleading.
  - Quick check question: Why would 99% accuracy be uninformative if 98% of votes are "In Favour"?

- Concept: **Temporal generalization in political AI**
  - Why needed here: Models trained on historical data must handle emerging geopolitical events; the paper shows performance drops on post-2023 unseen data.
  - Quick check question: What happens to GPT-4o's voting simulation accuracy when evaluated on resolutions drafted after its training cutoff?

## Architecture Onboarding

- Component map: UNBench comprises 4 tasks mapped to 3 stages: (1) Drafting → Task 1 (co-penholder judgment, multi-choice); (2) Voting → Task 2 (per-country vote simulation, 3-class) + Task 3 (adoption prediction, binary); (3) Discussing → Task 4 (statement generation, open-ended). Dataset D links draft resolutions, voting records, and meeting transcripts via resolution IDs and meeting record IDs.

- Critical path: Data pipeline (PDF parsing → LlamaParse → text extraction) → Task-specific prompt construction → Model inference (classification via prompts, generation with temperature=0) → Evaluation (accuracy/F1 for Tasks 1-3; ROUGE/cosine similarity for Task 4).

- Design tradeoffs: Multi-choice format (Task 1) enables objective evaluation but constrains realism; time-based train/test split preserves temporal validity but limits training data; temperature=0 ensures reproducibility but reduces stylistic diversity in generated statements.

- Failure signatures:
  - Small models collapsing to majority-class predictions (Task 2: Llama-3.2-1B F1 = 0.326 despite 0.898 accuracy).
  - ROUGE scores near zero for statement generation (all models <0.21) indicating lexical mismatch with ground truth.
  - Performance degradation on non-WEOG vs. WEOG resolutions suggesting geopolitical bias.

- First 3 experiments:
  1. Baseline: Run GPT-4o and one small model (Llama-3.2-3B) on all 4 tasks with default prompts; verify balanced accuracy and ROUGE match reported values.
  2. Ablation: Remove author/date metadata from Task 2 and Task 3 prompts; quantify accuracy drop to isolate metadata contribution.
  3. Temporal split: Evaluate GPT-4o on pre-2023 vs. post-2023 test subsets; measure generalization gap (reported: Task 2 Bal. ACC drops from 0.675 to 0.528).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do multi-agent LLM frameworks compare to single-agent prompting in simulating complex UNSC voting dynamics?
- Basis in paper: [explicit] The authors explicitly suggest future work should study "more realistic multi-agent simulations where LLMs act as countries" (Section 7).
- Why unresolved: The current benchmark evaluates models in isolation using single-agent role-playing, ignoring the interactive negotiation phase of diplomacy.
- What evidence would resolve it: A comparative study measuring voting prediction accuracy in a multi-turn agent negotiation setup versus the current static baseline.

### Open Question 2
- Question: Does the integration of structured, domain-specific knowledge bases (richer country profiles) significantly enhance strategic consistency in simulation tasks?
- Basis in paper: [explicit] The authors propose "Constructing richer country profiles to enhance grounding and strategic consistency" as a primary future direction (Section 7).
- Why unresolved: Current prompting relies on the model's internal parametric knowledge, which smaller models fail to leverage effectively for nuanced geopolitical reasoning.
- What evidence would resolve it: Experiments comparing baseline models against RAG-enhanced models provided with detailed, historical geopolitical data for each country.

### Open Question 3
- Question: Can retrieval mechanisms or specific fine-tuning mitigate the performance degradation observed in LLMs when predicting outcomes for unseen geopolitical events?
- Basis in paper: [inferred] Appendix C.1 shows GPT-4o's balanced accuracy drops from 0.675 to 0.528 on unseen post-2023 data, indicating a temporal generalization gap.
- Why unresolved: The paper identifies this temporal drift as a challenge but does not test methods to bridge this performance gap for emerging events.
- What evidence would resolve it: Evaluating time-sensitive tasks using models augmented with real-time news retrieval or continuous learning on the post-2023 test split.

## Limitations

- Role-based prompting assumes LLMs encode sufficient historical diplomatic behavior, which may not hold for recent geopolitical events or less-documented countries
- The 99.4% class imbalance in Task 2 (In Favour vs Against votes) creates significant evaluation challenges where simple baselines could achieve high accuracy
- Near-zero ROUGE scores for statement generation suggest evaluation metrics may be mismatched with the task or that generated statements have fundamentally different structures from ground truth diplomatic language

## Confidence

- **High Confidence**: Model scale correlation with performance (GPT-4o and DeepSeek-V3 consistently outperform smaller models across all tasks), basic dataset construction methodology (time-based splits, balanced accuracy usage), and the fundamental framework of mapping UN decision-making to LLM tasks.
- **Medium Confidence**: The specific contribution of metadata augmentation (author/date information) to prediction accuracy, the exact impact of choice count on Task 1 performance, and the interpretation of ROUGE scores for diplomatic statement evaluation.
- **Low Confidence**: The mechanism by which role-based prompting activates geopolitical knowledge, the generalizability of findings to non-Western political contexts (given WEOG vs non-WEOG performance differences), and whether current evaluation metrics adequately capture diplomatic language quality.

## Next Checks

1. **Temporal Robustness Test**: Evaluate all models on progressively newer time slices (e.g., 2015-2020, 2020-2023, post-2023) to quantify how performance degrades with temporal distance from training data, particularly for GPT-4o where balanced accuracy drops from 0.675 to 0.528.

2. **Metadata Ablation Study**: Systematically remove author and date metadata from Task 2 and Task 3 prompts, then measure the exact performance degradation (reported as improvement from 0.677 to 0.684 with metadata in Task 2) to isolate metadata's contribution from model capabilities.

3. **Small Model Failure Analysis**: Examine the failure patterns of Llama-3.2-1B across all tasks, particularly why it achieves 0.581 accuracy in Task 1 with 5 choices but only 0.326 F1 in Task 2 despite 0.898 accuracy, to understand whether performance reflects genuine reasoning or majority-class collapse.