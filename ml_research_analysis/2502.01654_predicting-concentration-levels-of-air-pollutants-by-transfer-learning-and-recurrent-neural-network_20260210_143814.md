---
ver: rpa2
title: Predicting concentration levels of air pollutants by transfer learning and
  recurrent neural network
arxiv_id: '2502.01654'
source_url: https://arxiv.org/abs/2502.01654
tags:
- data
- neural
- learning
- transfer
- lstm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study applied LSTM recurrent neural networks to forecast air
  pollutant concentrations in Macau, using meteorological and air quality data. Transfer
  learning was employed to enhance prediction accuracy for stations with limited data
  by pre-training models on related datasets from other stations.
---

# Predicting concentration levels of air pollutants by transfer learning and recurrent neural network

## Quick Facts
- arXiv ID: 2502.01654
- Source URL: https://arxiv.org/abs/2502.01654
- Reference count: 33
- This study applied LSTM recurrent neural networks to forecast air pollutant concentrations in Macau, using meteorological and air quality data. Transfer learning was employed to enhance prediction accuracy for stations with limited data by pre-training models on related datasets from other stations.

## Executive Summary
This study addresses air quality prediction challenges in Macau by applying Long Short-Term Memory (LSTM) recurrent neural networks with transfer learning. The approach leverages data from multiple air quality monitoring stations (AQMS) and meteorological stations to predict pollutant concentrations, particularly focusing on stations with limited historical data. Transfer learning is implemented by pre-training LSTM models on data-rich stations and transferring learned weights to improve predictions at data-scarce stations. The method demonstrates improved prediction accuracy and reduced training time compared to randomly initialized networks.

## Method Summary
The approach combines time series data from four AQMSs and one AWS, using a 6-day sliding window to predict next-day pollutant concentrations. Data preprocessing includes min-max normalization and supervised learning conversion. Transfer learning is implemented by pre-training LSTM networks on source domains (data-rich stations/pollutants) and transferring weights to target domains. Two transfer modes are tested: trainable (fine-tuning all layers) and untrainable (freezing pre-trained layers). The framework is evaluated across four monitoring stations in Macau for multiple pollutants including PM2.5, PM10, NO2, NO, and CO.

## Key Results
- Transfer learning improved prediction accuracy for all target stations compared to randomly initialized networks
- Pre-trained networks achieved lower Best MSE values with fewer training epochs across all station-pollutant combinations
- Cross-pollutant transfer (PM10 → PM2.5) was particularly effective due to PM2.5's limited historical data
- Trainable transfer mode generally achieved better final accuracy than untrainable mode

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Pre-trained LSTM weights from data-rich stations provide better initial states for data-scarce stations, reducing training epochs and improving final prediction accuracy.
- **Mechanism:** LSTM RNNs trained on source domain (station with more observations) have their learned weight matrices transferred as initialization for target domain networks, rather than random initialization.
- **Core assumption:** Nearby monitoring stations within Macau share sufficiently similar atmospheric dispersion patterns and pollutant-meteorology relationships for weight reuse to be meaningful.
- **Evidence anchors:**
  - [abstract] "LSTM RNNs initialized with transfer learning methods have higher prediction accuracy; it incurred shorter training time than randomly initialized recurrent neural networks."
  - [section 5.1, Table 2] Pre-trained networks consistently achieved lower Best MSE (e.g., 0.003596 vs 0.007266 for Roadside Macau PM2.5) with fewer epochs (92 vs 400).
  - [corpus] Corpus evidence is weak—neighbor papers focus on Transformers and Prophet comparisons rather than weight-transfer LSTM approaches.
- **Break condition:** When source and target domains have fundamentally different pollutant source profiles (e.g., industrial vs. traffic-dominated zones) or when meteorological regimes diverge significantly.

### Mechanism 2
- **Claim:** A 6-day sliding window of historical observations provides sufficient temporal context for next-day pollutant concentration prediction.
- **Mechanism:** Time series data is converted to supervised learning format where each training sample contains features from 6 prior days (meteorological + pollutant data from multiple stations) to predict the target pollutant value at day 7.
- **Core assumption:** Pollutant concentration dynamics exhibit temporal autocorrelation within a ~1-week window, and earlier observations have diminishing predictive value.
- **Evidence anchors:**
  - [section 4] "each row of the dataset used data 6 days ago from each station" with format (x1, x2, x3, x4, x5, x6, y7).
  - [section 4, Figure 4] Visual depiction of the sliding window conversion from timestamps to training pairs.
  - [corpus] No direct corpus evidence for the 6-day window specifically—this appears to be an empirical choice.
- **Break condition:** When sudden emission events (wildfires, industrial accidents) occur that fall outside historical patterns, or when prediction horizon extends beyond next-day.

### Mechanism 3
- **Claim:** Cross-pollutant transfer (PM10 → PM2.5) improves prediction for pollutants with limited historical records.
- **Mechanism:** LSTM weights learned from predicting one pollutant (PM10, which has longer measurement history since before 2012) serve as initialization for predicting a related pollutant (PM2.5, which only has data from July 2012).
- **Core assumption:** Particulate matter pollutants share underlying atmospheric dispersion dynamics and similar relationships with meteorological variables.
- **Evidence anchors:**
  - [section 3] "All AQMSs in Macau have officially begun to measure the concentration of PM2.5 in July 2012. Therefore, the amount of PM2.5 observed data for each station is relatively smaller."
  - [section 5.1, Table 2] PM10 pre-trained models from multiple source stations improved PM2.5 prediction across all target stations.
  - [corpus] No corpus evidence specifically validating cross-pollutant transfer efficacy.
- **Break condition:** When pollutants have fundamentally different formation mechanisms (e.g., primary emissions vs. secondary photochemical formation like O3).

## Foundational Learning

- **Concept: LSTM cell state vs. hidden state**
  - Why needed: Understanding what information is being transferred when weights are copied between domains.
  - Quick check question: If you freeze the forget gate weights during fine-tuning, what behavior is preserved from the source domain?

- **Concept: Transfer learning taxonomy (inductive vs. transductive)**
  - Why needed: This paper uses inductive transfer where source and target have different but related tasks.
  - Quick check question: In this paper, is the source-to-target mapping homogeneous (same feature space) or heterogeneous? What changes between domains?

- **Concept: Min-max normalization for time series**
  - Why needed: The paper rescales all features to [0,1] range before training (formulas 2-3 in section 4).
  - Quick check question: If you transfer weights from a normalized source domain to an unnormalized target domain, what happens to the learned representations?

## Architecture Onboarding

- **Component map:**
  - Raw time series data from 4 AQMSs + 1 AWS -> Min-max normalization -> 6-day sliding window conversion -> LSTM layers -> Single scalar output (next-day concentration)

- **Critical path:**
  1. Data alignment across stations (handle missing data, align timestamps)
  2. Min-max normalization per feature
  3. Sliding window conversion
  4. Train source domain LSTM to convergence
  5. Copy weights to target domain network
  6. Fine-tune on target domain data

- **Design tradeoffs:**
  - **Trainable vs. untrainable pre-trained layers:** Table 3 shows trainable generally achieves lower final MSE, but untrainable converges faster with lower initial MSE in some cases.
  - **Source station selection:** Multiple source stations tested; best source varies by target (no single best source).
  - **Assumption:** The paper does not report computational cost comparisons in detail.

- **Failure signatures:**
  - High MSE on outlier/high-concentration days (section 5.1: "high concentration of air pollutants are outliers and RNNs were not designed to deliberately cope with outliers")
  - Poor transfer when source-target station characteristics diverge (urban roadside vs. ambient background)
  - Overfitting on validation data when target domain has very few samples

- **First 3 experiments:**
  1. **Baseline replication:** Train randomly initialized LSTM for PM2.5 at one station; record MSE, epochs to convergence, and initial loss.
  2. **Single-source transfer:** Pre-train on PM10 at a data-rich station; transfer weights to PM2.5 prediction at a data-scarce station; compare convergence speed and final accuracy.
  3. **Ablation on layer freezing:** Test trainable vs. untrainable configurations for transferred layers; measure tradeoff between initial state quality and final accuracy.

## Open Questions the Paper Calls Out

- **Can the proposed transfer learning framework maintain high accuracy when forecasting air pollution multiple days ahead or on an hourly basis?**
  - Basis in paper: [explicit] The conclusion states future work should involve predicting "next several days ahead" or using "hourly-observed data" rather than just the next day.
  - Why unresolved: The current study only evaluated predictions for the following day (t+1) using daily aggregates.
  - What evidence would resolve it: Experimental results comparing MSE for multi-step forecasts (t+n) and hourly predictions against single-step daily baselines.

- **Does incorporating imbalanced dataset processing methods improve the prediction accuracy for high-concentration air pollution events?**
  - Basis in paper: [explicit] The authors note that serious air pollution events are "relatively rare" (outliers) and suggest using "imbalanced dataset processing methods" in future work.
  - Why unresolved: The current RNN implementation was not designed to handle outliers, resulting in lower accuracy during high-pollution periods.
  - What evidence would resolve it: A comparative analysis of prediction errors on high-concentration test sets using techniques like SMOTE versus the standard LSTM approach.

- **Does the inclusion of vehicle traffic data significantly enhance the prediction accuracy for roadside air quality monitoring stations?**
  - Basis in paper: [explicit] The conclusion suggests that "vehicle traffic data" should be attempted as an additional input feature, particularly for roadside stations.
  - Why unresolved: The current model relies solely on meteorological and historical pollutant data, potentially omitting a key causal variable for roadside emissions.
  - What evidence would resolve it: Ablation studies showing performance differences between models trained with and without traffic volume features for roadside stations.

## Limitations

- The paper lacks specification of LSTM architecture details (layer sizes, activation functions) and training hyperparameters (optimizer, learning rate, batch size)
- Transfer learning benefits are demonstrated but without comprehensive ablation studies showing optimal source-target combinations
- Cross-pollutant transfer efficacy lacks validation through direct comparison with same-pollutant transfer scenarios
- The 6-day sliding window choice is empirical without sensitivity analysis for different temporal contexts

## Confidence

- **High confidence:** Transfer learning reduces training time and improves final MSE compared to random initialization (supported by Table 2 showing consistent improvements across all station-pollutant combinations)
- **Medium confidence:** Cross-pollutant transfer is beneficial (supported by results but lacking comparative ablation studies)
- **Medium confidence:** 6-day sliding window provides optimal temporal context (empirical choice without sensitivity analysis)

## Next Checks

1. Conduct ablation study comparing same-pollutant vs. cross-pollutant transfer to quantify the benefit of PM10→PM2.5 transfer
2. Test sensitivity to sliding window size (3-day, 6-day, 9-day) to determine optimal temporal context for Macau air quality prediction
3. Replicate experiments with different LSTM architectures (varying layer depth and width) to establish robustness of transfer learning benefits