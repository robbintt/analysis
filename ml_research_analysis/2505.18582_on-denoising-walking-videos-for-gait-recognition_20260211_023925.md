---
ver: rpa2
title: On Denoising Walking Videos for Gait Recognition
arxiv_id: '2505.18582'
source_url: https://arxiv.org/abs/2505.18582
tags:
- gait
- recognition
- feature
- diffusion
- denoising
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of gait recognition in the presence
  of identity-irrelevant cues like clothing texture and color. It proposes DenoisingGait,
  a novel method that combines knowledge-driven and geometry-driven denoising to enhance
  gait feature extraction from RGB videos.
---

# On Denoising Walking Videos for Gait Recognition

## Quick Facts
- arXiv ID: 2505.18582
- Source URL: https://arxiv.org/abs/2505.18582
- Reference count: 40
- Key outcome: Proposes DenoisingGait, combining diffusion-based and geometry-driven denoising to achieve state-of-the-art gait recognition performance across multiple datasets and conditions.

## Executive Summary
This paper addresses the challenge of gait recognition in the presence of identity-irrelevant cues like clothing texture and color. It proposes DenoisingGait, a novel method that combines knowledge-driven and geometry-driven denoising to enhance gait feature extraction from RGB videos. The knowledge-driven component leverages diffusion models to filter out gait-irrelevant cues, while the geometry-driven component uses a feature matching module to condense multi-channel diffusion features into a two-dimensional direction vector, producing a novel gait representation called Gait Feature Field. Experiments on CCPG, CASIA-B*, and SUSTech1K datasets demonstrate that DenoisingGait achieves state-of-the-art performance in most cases for both within- and cross-domain evaluations.

## Method Summary
DenoisingGait processes RGB walking videos through a two-stage denoising pipeline. First, a knowledge-driven component applies one-step denoising from Stable Diffusion (t=700) to filter out identity-irrelevant cues while preserving gait structure. Second, a geometry-driven component uses feature matching to convert the multi-channel diffusion features into 2D direction vectors, creating static and dynamic Gait Feature Fields. A texture suppression mechanism randomly zeros high-magnitude vectors during training to further enhance invariance to clothing. The resulting features are processed by a GaitBase backbone trained with triplet and cross-entropy losses.

## Key Results
- Achieves state-of-the-art performance across CCPG, CASIA-B*, and SUSTech1K datasets
- Timestep t=700 in diffusion denoising shows peak performance, improving accuracy by 5.3% over baseline
- Texture suppression improves cloth-changing (CL) accuracy by +3.0% over background removal alone
- Cross-frame matching alone underperforms, confirming both static and dynamic features are essential

## Why This Works (Mechanism)

### Mechanism 1: Knowledge-Driven Denoising via Diffusion Timestep Selection
- Diffusion timesteps control the balance between shape preservation and texture filtering, with t=700 optimally capturing gait structure while suppressing identity-irrelevant details.

### Mechanism 2: Geometry-Driven Feature Matching Condenses Multi-Channel Features to Direction Vectors
- Within-frame and cross-frame matching convert multi-channel diffusion features into 2D direction vectors, creating flow-like Gait Feature Fields that preserve both static appearance and dynamic motion.

### Mechanism 3: Texture Suppression via Random Zero-Padding of High-Magnitude Vectors
- High-magnitude vectors in static Gait Feature Field correlate with texture intensity; randomly zeroing them during training forces the model to learn texture-invariant representations.

## Foundational Learning

- **Diffusion Model Forward/Reverse Process**
  - Why needed here: Understanding how latent diffusion encodes/decodes images and why timesteps control granularity.
  - Quick check question: Can you explain why t=700 preserves shape but filters texture, referencing Eq. 1-3?

- **Optical Flow and Dense Correspondence**
  - Why needed here: Gait Feature Field is "flow-like"; understanding spatial/temporal matching helps grasp within-frame vs. cross-frame distinction.
  - Quick check question: How does cross-frame matching (Δl > 0) differ from traditional optical flow computation?

- **Metric Learning (Triplet + Cross-Entropy)**
  - Why needed here: DenoisingGait is trained with both losses; understanding why both are needed for recognition.
  - Quick check question: Why would pure classification loss fail for cross-domain gait recognition?

## Architecture Onboarding

- **Component map**: RGB Frames → SD 1.5 Encoder E → One-Step Denoise (t=700) → Feature Matching (E^Q, E^K, AoD) → Silhouette Mask → Static/Dynamic Gait Feature Fields → GaitBase Backbone → Triplet + CE Losses

- **Critical path**: Timestep selection → Feature Matching design → Texture suppression threshold (m=0.5). If any fails, downstream GaitBase receives corrupted inputs.

- **Design tradeoffs**:
  - Higher t: More denoising but risk losing discriminative structure
  - Larger neighborhood (Δh, Δw): Smoother direction fields but may blur fine motion
  - Higher texture suppression probability p: More invariance but potential signal loss

- **Failure signatures**:
  - Flat accuracy curve across timesteps → diffusion features not discriminative
  - Cross-frame matching alone underperforms → static appearance features are essential
  - Night-condition failure → low-quality silhouettes break background removal

- **First 3 experiments**:
  1. Timestep sweep: Run inference with t ∈ {100, 300, 500, 700, 900} on CCPG validation; verify non-monotonic accuracy peak
  2. Ablate Feature Matching: Disable within-frame, cross-frame, and both; measure delta on CL subset
  3. Texture suppression sanity check: Visualize ||G_static||₂ maps alongside raw RGB; confirm magnitude correlates with visible texture regions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the dependency on pre-extracted silhouettes be relaxed to maintain robustness in scenarios where background segmentation fails, such as night conditions?
- Basis in paper: Table 3 and Section 5 discuss performance degradation in night (NT) scenarios, specifically attributing it to "low-quality silhouettes" which are a prerequisite for the geometry-driven feature matching module.
- Why unresolved: The current method explicitly uses silhouettes for background removal (Eq. 4) to ensure feature matching focuses on the foreground. If the silhouette is noisy or missing, the "geometry-driven denoising" cannot effectively condense features.
- What evidence would resolve it: A modification of DenoisingGait that operates on raw RGB frames without silhouette masks while maintaining high accuracy on the SUSTech1K "Night" subset.

### Open Question 2
- Question: Can the computational and storage overhead of large generative models be reduced to facilitate practical, real-time gait recognition without significant accuracy loss?
- Basis in paper: Section 5 explicitly lists the high computational costs, storage requirements, and training data demands of large vision models as a challenge, noting that "effectively utilizing... these diverse models... remains an unexplored issue."
- Why unresolved: The paper demonstrates the efficacy of using a heavy Stable Diffusion (SD 1.5) backbone but does not propose or test a lightweight variant suitable for edge deployment or real-time processing.
- What evidence would resolve it: Experiments demonstrating a distilled or pruned version of the diffusion model that retains the "knowledge-driven denoising" capability at a fraction of the inference latency and memory footprint.

### Open Question 3
- Question: Is the optimal denoising timestep ($t=700$) universal across different diffusion model architectures, or is it a hyperparameter specific to the SD 1.5 backbone used?
- Basis in paper: Section 3.1 empirically determines $t=700$ as the peak performance timestep for the specific SD 1.5 model. It assumes this timestep captures "overall shape" but does not validate if this holds for other diffusion backbones.
- Why unresolved: The granularity of semantic vs. texture information varies by model architecture. Assuming a fixed timestep for "knowledge-driven denoising" may lead to suboptimal filtering if the underlying generative model is changed.
- What evidence would resolve it: A study evaluating the performance curve across timesteps using different pre-trained diffusion models (e.g., SDXL or different U-Net configurations) to see if the optimal $t$ shifts.

## Limitations
- The diffusion model's timestep-dependent feature quality is assumed but not directly visualized
- The texture suppression mechanism relies on an assumed correlation between vector magnitude and texture intensity
- The quality of silhouette masks is critical but the background removal method is unspecified
- The ablation study on texture suppression is limited to one dataset and condition

## Confidence

- **High**: The overall framework combining diffusion denoising with feature matching is well-defined and yields state-of-the-art results on multiple datasets
- **Medium**: The specific claims about the mechanism (e.g., why t=700 is optimal, how magnitude proxies texture) are supported by internal experiments but lack external validation
- **Low**: The texture suppression's effectiveness is demonstrated in one condition but lacks broader empirical support

## Next Checks

1. **Timestep sweep validation**: Replicate the non-monotonic accuracy curve across timesteps (t ∈ {100, 300, 500, 700, 900}) on a held-out validation set to confirm the t=700 peak is not dataset-specific

2. **Texture-magnitude correlation**: Visualize and statistically test the correlation between Gait Feature Field vector magnitudes and texture intensity in the original RGB frames to validate the suppression mechanism's foundation

3. **Silhouette quality sensitivity**: Evaluate performance with varying silhouette quality (e.g., using different segmentation models) to quantify the dependency of the geometry-driven component on mask accuracy