---
ver: rpa2
title: 'Balancing Accuracy and Efficiency: CNN Fusion Models for Diabetic Retinopathy
  Screening'
arxiv_id: '2512.21861'
source_url: https://arxiv.org/abs/2512.21861
tags:
- fusion
- diabetic
- accuracy
- retinopathy
- screening
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigated whether feature-level fusion of complementary
  CNN backbones could improve binary diabetic retinopathy (DR) screening on globally
  sourced fundus images. A dataset of 11,156 images from five public sources (APTOS,
  EyePACS, IDRiD, Messidor, ODIR) was used to train and evaluate three pretrained
  models (ResNet50, EfficientNet-B0, DenseNet121) and their pairwise and tri-fusion
  variants.
---

# Balancing Accuracy and Efficiency: CNN Fusion Models for Diabetic Retinopathy Screening

## Quick Facts
- **arXiv ID**: 2512.21861
- **Source URL**: https://arxiv.org/abs/2512.21861
- **Reference count**: 21
- **Primary result**: EfficientNet-B0 + DenseNet121 fusion achieves 82.89% accuracy and balanced F1-scores (83.60%/82.60%) for binary diabetic retinopathy screening on multi-source fundus images.

## Executive Summary
This study investigates whether concatenating penultimate-layer features from complementary CNN backbones improves binary diabetic retinopathy classification. Using 11,156 fundus images from five public datasets, the authors train ResNet50, EfficientNet-B0, DenseNet121, and their fusion variants. Fusion models consistently outperform single backbones, with the EfficientNet-B0 + DenseNet121 pair achieving the best accuracy-latency trade-off. The approach demonstrates that lightweight feature fusion can enhance generalization across heterogeneous datasets while maintaining computational efficiency suitable for scalable screening workflows.

## Method Summary
The method trains pretrained CNN backbones (ResNet50, EfficientNet-B0, DenseNet121) on binary diabetic retinopathy classification using 11,156 pooled fundus images from five public sources. Each backbone is truncated at its penultimate layer, and features are extracted and concatenated for fusion models. A classifier head (512-dense + ReLU + Dropout) processes the fused representation. Models are trained with BCEWithLogitsLoss, Adam optimizer (lr=0.001, weight_decay=1e-4), ReduceLROnPlateau scheduler, and early stopping. Five independent runs evaluate accuracy, F1-scores, and inference latency across batch sizes.

## Key Results
- EfficientNet-B0 + DenseNet121 fusion achieves 82.89% accuracy with balanced F1-scores (83.60% normal, 82.60% diabetic).
- EfficientNet-B0 is the fastest model at 1.16 ms/image (batch size 1000).
- Tri-fusion (all three backbones) incurs ~3× parameter increase (45.06M) with only marginal accuracy gain (82.33%).
- Pairwise fusion offers superior accuracy-latency trade-off compared to tri-fusion.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Concatenating penultimate-layer features from architecturally diverse CNN backbones improves binary DR classification by capturing complementary visual patterns that single models miss.
- **Mechanism**: Each backbone encodes different inductive biases—ResNet50's skip connections enable deep hierarchical features, EfficientNet-B0's compound scaling balances efficiency, DenseNet121's dense connectivity maximizes feature reuse. When concatenated, the fused representation combines distinct "views" of the same fundus image.
- **Core assumption**: Different backbones extract meaningfully non-redundant information; their errors are at least partially uncorrelated.
- **Evidence anchors**:
  - [abstract]: "fusion consistently outperforms single backbones"
  - [Page 1]: "Fusion models, which combine the complementary strengths of multiple architectures, offer an effective solution"
- **Break condition**: If fusion models perform ≤ best single backbone across multiple runs, the complementarity assumption fails.

### Mechanism 2
- **Claim**: Training on pooled multi-source data from geographically and technically diverse datasets improves robustness to real-world acquisition variation.
- **Mechanism**: Five datasets span India (APTOS, IDRiD), USA (EyePACS), France (Messidor), and China (ODIR), with varying image quality, resolutions, and annotation protocols. This heterogeneity forces the model to learn features invariant to device-specific artifacts and population differences.
- **Core assumption**: The test set distribution reflects the heterogeneity seen during training; future deployment data resembles this mixture.
- **Evidence anchors**:
  - [abstract]: "lightweight feature fusion can enhance generalization across heterogeneous datasets"
  - [Page 5]: "training on diverse sources for improved robustness to variation in acquisition protocols, resolutions, and populations"
- **Break condition**: If per-source performance varies drastically (e.g., >10% accuracy gap), generalization is uneven.

### Mechanism 3
- **Claim**: Pairwise fusion offers a superior accuracy-latency trade-off compared to tri-fusion, which incurs ~3× parameter increase without proportional accuracy gains.
- **Mechanism**: Eff+Den fusion (15.29M params) achieves 82.89% accuracy; tri-fusion (45.06M params) achieves 82.33%—slightly lower. The third backbone adds redundancy and computational cost without improving the fused representation's discriminative power.
- **Core assumption**: Inference latency constraints matter in deployment; the ~2.3% accuracy gain from fusion justifies ~3× parameter increase over single EfficientNet-B0.
- **Evidence anchors**:
  - [Page 4]: "tri-fusion model...incurs a substantially higher computational cost"
  - [Page 5, Table III]: Parameter counts—Eff+Den (15.29M) vs Tri-Fusion (45.06M)
- **Break condition**: If tri-fusion consistently outperforms pairwise with acceptable latency, the pairwise-optimal claim fails.

## Foundational Learning

- **Concept: Feature-level fusion vs logit-level ensemble**
  - **Why needed here**: The paper extracts features from the penultimate layer (before classification head) and concatenates them, rather than averaging final predictions. This matters because fused features can learn new interactions that logit averaging cannot.
  - **Quick check question**: If you averaged the outputs of two trained CNNs instead of concatenating their features, would you expect the same accuracy? Why or why not?

- **Concept: Backbone inductive biases**
  - **Why needed here**: Understanding why ResNet50 underperforms (78.31%) while EfficientNet-B0 leads (80.61%) informs fusion partner selection. Not all pairs are equally complementary.
  - **Quick check question**: What architectural property of DenseNet121 might make it a good fusion partner for EfficientNet-B0?

- **Concept: Stratified splitting across multi-source datasets**
  - **Why needed here**: With 5 sources and class imbalance, naive random splitting could leak source-specific patterns. Stratified sampling preserves class balance within each split.
  - **Quick check question**: If all EyePACS images landed in the training set and all APTOS in test, what failure mode would you expect?

## Architecture Onboarding

- **Component map**: Input (3×224×224 fundus image) → parallel backbone encoders (ResNet50, EfficientNet-B0, DenseNet121) → truncate at penultimate layer → concatenate features → linear projection → 512-dense + ReLU + Dropout(0.5) → 1-dimension output → sigmoid → probability

- **Critical path**:
  1. Pretrained backbone weights (ImageNet) → feature extraction quality
  2. Dimensionality reduction → prevents classifier overfitting on huge concatenated vectors
  3. Dropout(0.5) → regularization for fusion head
  4. BCEWithLogitsLoss → numerically stable for binary classification

- **Design tradeoffs**:
  - More backbones = higher accuracy ceiling but multiplicative latency/params
  - Dimensionality reduction d: too small loses information, too large risks overfitting
  - DenseNet121 is ~2× larger than EfficientNet-B0; asymmetric fusion partners may bias learned representations

- **Failure signatures**:
  - Fusion accuracy ≤ best single backbone → features are redundant, not complementary
  - Training loss diverges across runs → learning rate too high for fusion head; try lower LR or warmup
  - High variance across 5 runs (±1.33% for ResNet50) → increase regularization or dataset size

- **First 3 experiments**:
  1. Replicate single backbone baselines (ResNet50, EfficientNet-B0, DenseNet121) on 70/15/15 split to verify reported accuracies within ±2%
  2. Implement Eff+Den fusion with identical training hyperparameters (LR=0.001, Adam, weight decay 1e-4); expect ~82-83% accuracy
  3. Ablation: replace feature-level fusion with logit averaging; compare accuracy to validate that concatenation provides benefit beyond ensemble averaging

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the proposed fusion framework perform when extended from binary classification to multi-grade diabetic retinopathy (DR) severity assessment?
- **Basis in paper**: [explicit] The conclusion explicitly states that future work will "extend the framework to multi-grade DR severity."
- **Why unresolved**: The current study simplified the task to binary classification (normal vs. diabetic), collapsing all severity levels into a single positive class.
- **What evidence would resolve it**: Evaluation results of the Eff+Den fusion model on a 5-class severity task (e.g., using the full IDRiD or APTOS grading schemes) compared against single-backbone baselines.

### Open Question 2
- **Question**: Can a tiered "model-aware triage" system optimize the accuracy-latency trade-off more effectively than a uniform fusion deployment?
- **Basis in paper**: [explicit] The authors propose future integration of "model-aware triage (fast first-pass models backed by fusion-based confirmers)."
- **Why unresolved**: The paper evaluates architectures in isolation or full fusion but does not test cascaded workflows where a lightweight model filters easy cases before invoking the fusion model.
- **What evidence would resolve it**: A comparative workflow analysis showing total system latency and error rates when EfficientNet-B0 is used as a gatekeeper for the Eff+Den fusion model.

### Open Question 3
- **Question**: Would hybrid or logit-level fusion strategies offer a better balance of computational cost and accuracy than the feature-level concatenation approach used?
- **Basis in paper**: [inferred] The methodology defines feature-level, logit, and hybrid fusion strategies, but the experimental results focus exclusively on feature-level fusion.
- **Why unresolved**: It is unclear if the higher computational cost of feature concatenation (fully connected layers on high-dimensional vectors) is necessary, or if simpler logit averaging could achieve similar gains with lower latency.
- **What evidence would resolve it**: Benchmarks comparing the accuracy and inference speed of the proposed feature fusion against logit-averaged and hybrid fusion variants on the same test set.

## Limitations
- **Dataset heterogeneity impact**: Per-source validation performance is not reported, leaving uncertainty about whether fusion generalizes uniformly across all populations.
- **Hyperparameter sensitivity**: Key fusion dimensions (projection layer size) and batch size are unspecified, making exact reproduction difficult.
- **Generalizability beyond binary DR**: Extension to multi-grade DR severity remains unproven and is only proposed for future work.

## Confidence
- **High confidence**: Feature-level fusion consistently outperforms single backbones in accuracy and F1-scores across five independent runs.
- **Medium confidence**: Pairwise fusion offers the best accuracy-latency trade-off; tri-fusion shows marginally lower accuracy with higher computational cost, but small performance gaps warrant further validation.
- **Low confidence**: The claim that training on five diverse datasets inherently improves robustness is plausible but not empirically validated via per-source or cross-dataset transfer tests.

## Next Checks
1. **Per-source performance analysis**: Evaluate fusion and single backbone models on each dataset independently to quantify generalization consistency and identify potential dataset-specific biases.
2. **Hyperparameter ablation study**: Systematically vary the fusion projection dimension and batch size to determine their impact on accuracy, latency, and stability.
3. **Cross-dataset validation**: Train models on a subset of datasets (e.g., EyePACS + Messidor) and test on held-out datasets (e.g., APTOS + ODIR) to directly measure robustness to domain shift.