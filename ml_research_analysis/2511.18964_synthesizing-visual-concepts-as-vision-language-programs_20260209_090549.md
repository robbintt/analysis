---
ver: rpa2
title: Synthesizing Visual Concepts as Vision-Language Programs
arxiv_id: '2511.18964'
source_url: https://arxiv.org/abs/2511.18964
tags:
- objects
- reasoning
- image
- task
- object
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Vision-Language Programs (VLP) combine VLMs with program synthesis
  to enable interpretable visual reasoning by extracting structured descriptions from
  images and compiling them into executable neuro-symbolic programs. This approach
  addresses the inconsistency and illogical outputs of direct VLM prompting on systematic
  reasoning tasks.
---

# Synthesizing Visual Concepts as Vision-Language Programs

## Quick Facts
- arXiv ID: 2511.18964
- Source URL: https://arxiv.org/abs/2511.18964
- Reference count: 40
- Primary result: VLP outperforms direct VLM prompting on systematic visual reasoning tasks by up to 26.1% accuracy

## Executive Summary
Vision-Language Programs (VLP) addresses the inconsistency and illogical outputs of direct Vision-Language Model (VLM) prompting on systematic reasoning tasks by decoupling perception from reasoning. Instead of embedding reasoning inside the VLM, VLP leverages the model to produce structured visual descriptions that are compiled into neuro-symbolic programs. This approach enables interpretable visual reasoning by extracting structured descriptions from images and compiling them into executable neuro-symbolic programs.

The method combines VLMs with program synthesis to enable interpretable visual reasoning through a training-free pipeline. VLP shows consistent performance gains across multiple datasets including Bongard-HOI, COCOLogic, and CLEVR-Hans3, with improvements up to 13.5% on Bongard tasks and 26.1% on CLEVR-Hans3, especially on complex logical reasoning. The modular structure allows human-in-the-loop refinement to mitigate shortcuts and provides transparency in decision-making.

## Method Summary
VLP is a training-free pipeline that synthesizes visual concepts into executable programs. It first grounds symbols (objects, properties, actions) from few-shot examples using VLM queries, then constructs a probabilistic context-free grammar (PCFG) from a domain-specific language (DSL) augmented with these symbols. A heap search algorithm explores syntactically valid programs, ranking them by accuracy on support examples and PCFG probability. The highest-ranked program executes on query images via VLM function calls and symbolic operations, returning binary predictions.

## Key Results
- VLP outperforms direct VLM prompting on systematic reasoning tasks by up to 13.5% on Bongard tasks
- VLP achieves 26.1% improvement on CLEVR-Hans3, particularly for complex logical reasoning
- VLP is more efficient than reasoning models and scales well with additional examples
- Human-in-the-loop refinement can improve performance by removing problematic symbols (e.g., 13.3% improvement on CLEVR-Hans3)

## Why This Works (Mechanism)

### Mechanism 1
Decoupling perception from reasoning reduces inconsistent outputs in systematic visual reasoning tasks. VLMs first extract structured symbolic representations (objects, properties, actions) via VLM functions like `get_objects` and `get_actions`. These representations become inputs to symbolic reasoning functions, preventing the VLM from generating task-constraint-violating rules directly. This works because VLMs are better at perceptual extraction than at end-to-end logical rule induction. If VLM perceptual extraction fails to capture task-relevant symbols (e.g., omitting "size" property in CLEVR-Hans3), downstream reasoning cannot recover.

### Mechanism 2
Probabilistic program synthesis with occurrence-based weighting induces more accurate rules than direct VLM prompting. A PCFG defines the search space of syntactically valid programs. Symbols occurring more frequently in positive examples receive higher production probabilities, biasing search toward discriminative concepts. Programs are ranked by accuracy on few-shot examples, then by PCFG probability. This works because symbol frequency in positive vs. negative examples correlates with rule relevance. If spurious correlations exist in few-shot examples (e.g., color shortcuts in CLEVR-Hans3), weighting amplifies incorrect signals.

### Mechanism 3
Modular program structure enables interpretable debugging and shortcut mitigation. Programs are human-readable compositions of VLM functions, symbolic functions, and operators. Users can inspect which symbols/functions are used, identify shortcuts (e.g., "red" color shortcut in CLEVR-Hans3), and modify the DSL to add missing functions or remove problematic symbols. This works because humans can correctly identify which program components represent shortcuts vs. valid reasoning. If the underlying VLM consistently misses perceptual features, DSL edits cannot compensate without re-grounding symbols.

## Foundational Learning

- **Domain-Specific Languages (DSLs) for Neuro-Symbolic Systems**: VLP's entire architecture relies on defining type-constrained primitives (VLM functions, symbolic functions, operators) that compose into executable programs. Quick check: Can you explain why a DSL must define both syntax (how expressions combine) and semantics (what functions compute)?

- **Probabilistic Context-Free Grammars (PCFGs) for Program Synthesis**: The PCFG determines which programs are syntactically valid and assigns probabilities that guide search; understanding this is essential for modifying the search bias. Quick check: How would you modify production rule probabilities to prefer simpler programs over complex ones?

- **Inductive Logic Programming / Few-Shot Concept Learning**: VLP's task formulation—inducing a binary classification rule from positive/negative examples—directly maps to ILP formulations. Quick check: What happens to rule induction when few-shot examples contain label noise or annotation errors?

## Architecture Onboarding

- **Component map**: Symbol Grounding Module -> DSL → PCFG Constructor -> Program Synthesis Engine -> Execution Runtime
- **Critical path**: 1) Symbol grounding quality (Section D.2 shows ~60-90% hit ratios for ground-truth symbols depending on dataset/model) 2) VLM function output format (Section D.1: Kimi has 13% parse failures; InternVL3 near 100% success) 3) Program search budget (10-second budget in experiments; max depth 4-6)
- **Design tradeoffs**: Sampling vs. greedy decoding during symbol grounding and synthesis (Supplement C.3 shows mixed results); precomputing VLM function outputs reduces synthesis time but requires storage; symbol count per type (10 objects/10 properties/3-10 actions) balances search space vs. completeness
- **Failure signatures**: VLM formatting errors (Kimi: 13% failure on COCOLogic); incomplete symbol grounding (missing "size" property prevents expressing correct rules); shortcut learning (programs latch onto spurious correlations like color)
- **First 3 experiments**: 1) Reproduce Table 1 on Bongard-HOI with InternVL3-8B to verify ~17% improvement 2) Ablate occurrence-based weighting on COCOLogic to measure ~1-2% drop 3) Human-in-the-loop DSL edit by removing a ground-truth-relevant symbol to observe failure and recovery

## Open Questions the Paper Calls Out

### Open Question 1
Can VLP be extended to handle object-centric concepts requiring explicit spatial relations between objects? The current DSL operates on object-property lists without structured positional information, preventing reasoning about spatial configurations (e.g., "left of," "adjacent to"). This remains unresolved because the current architecture lacks explicit object representations. Demonstration of VLP successfully learning and applying spatial relation rules on datasets like CLEVR would resolve this.

### Open Question 2
Can multi-model program synthesis combining different VLMs or integrating classical algorithms improve VLP's robustness? Current VLP relies on a single VLM for all perception functions, inheriting that model's biases and failure modes. This remains unresolved as no comparative experiments exist. Evidence would come from showing improved performance or reduced failure rates when combining multiple VLMs or hybridizing with classical vision algorithms.

### Open Question 3
How can the computational cost of symbol grounding be reduced while maintaining perceptual quality? Current batch grounding may miss relevant symbols, but per-symbol querying is computationally expensive. No optimal strategy is identified. Ablation studies comparing grounding strategies with metrics for both computational cost and downstream program accuracy would resolve this.

## Limitations
- VLP depends on a specific "Heap Search" algorithm [5] for efficient program synthesis; without this implementation, standard enumeration may fail to reproduce results
- Results show dramatic performance differences based on VLM capability (InternVL3-8B vs. Kimi), suggesting a quality threshold requirement
- VLP may still be vulnerable to shortcuts when they are statistically stronger in few-shot examples

## Confidence
- **High**: VLP outperforms direct prompting on systematic reasoning tasks (supported by multiple datasets and statistical significance)
- **Medium**: VLP scales better than reasoning models (evidence shows efficiency gains, but reasoning model baselines vary)
- **Medium**: Human-in-the-loop refinement effectively mitigates shortcuts (demonstrated on specific cases but not systematically evaluated)

## Next Checks
1. Implement Heap Search Verification: Reproduce the Bongard-HOI experiment using both the specified Heap Search and a standard enumeration approach. Compare success rates and execution times to verify the claimed efficiency advantage.

2. VLM Quality Threshold Analysis: Systematically test VLP across VLMs with varying parse success rates (e.g., 70%, 80%, 90%, 95%). Document the minimum quality threshold where VLP consistently outperforms direct prompting.

3. Shortcut Resistance Test: Design synthetic few-shot examples where a spurious correlation (e.g., "red objects") perfectly predicts labels in support but fails on query. Compare VLP's resistance to direct prompting's shortcut vulnerability across multiple spurious features.