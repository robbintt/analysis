---
ver: rpa2
title: 'Beyond Single Embeddings: Capturing Diverse Targets with Multi-Query Retrieval'
arxiv_id: '2511.02770'
source_url: https://arxiv.org/abs/2511.02770
tags:
- query
- target
- document
- embeddings
- documents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Existing text retrievers generate a single query embedding, which
  limits their ability to capture diverse target documents when the relevant set is
  multimodal. To address this, we propose an autoregressive multi-embedding retriever
  (AMER) that generates multiple query vectors per input, each used to retrieve a
  ranked document list, which are then aggregated.
---

# Beyond Single Embeddings: Capturing Diverse Targets with Multi-Query Retrieval

## Quick Facts
- arXiv ID: 2511.02770
- Source URL: https://arxiv.org/abs/2511.02770
- Reference count: 29
- Single-query models achieve ~20% MRECALL on synthetic diverse targets; AMER achieves 100%

## Executive Summary
Existing text retrievers generate a single query embedding, which limits their ability to capture diverse target documents when the relevant set is multimodal. To address this, we propose an autoregressive multi-embedding retriever (AMER) that generates multiple query vectors per input, each used to retrieve a ranked document list, which are then aggregated. On synthetic data with diverse targets, AMER achieves perfect performance (100% MRECALL) versus ~20% for single-query models, validating its ability to model multiple target distributions. On real-world multi-answer retrieval datasets (AmbigQA, QAMPARI), AMER yields modest overall gains (4% and 21% relative improvement, respectively) but larger gains (up to 144%) on subsets where target documents are less similar, indicating its strength in capturing diversity.

## Method Summary
AMER generates multiple query embeddings autoregressively for each input query, with each embedding used to retrieve a ranked document list. The model uses a frozen document encoder (Inf-Retriever-v1-1.5B) and trains a query encoder (Llama-3.2-1B/3B/8B-Instruct or Qwen3-4B-Instruct) with LoRA. During training, Hungarian matching aligns predicted embeddings with ground truth document embeddings to handle the unordered nature of targets. At inference, ranked lists from each query embedding are merged using round-robin aggregation. The model is trained with InfoNCE loss and scheduled sampling, where the probability of using predicted embeddings increases linearly to 0.8 during training.

## Key Results
- On synthetic data with 5 diverse targets, AMER achieves 100% MRECALL vs ~20% for single-query models
- On AmbigQA, AMER achieves 4% relative improvement in MRECALL
- On QAMPARI, AMER achieves 21% relative improvement in MRECALL, with up to 144% gains on low-similarity subsets

## Why This Works (Mechanism)

### Mechanism 1
Single query embeddings cannot capture multimodal target distributions; performance degrades as target documents become less similar. Bi-encoder retrievers trained with contrastive loss push the single query embedding toward target document embeddings. When targets are far apart in embedding space, no single vector can simultaneously be close to all targets, forcing the model toward a compromise that approximates none well.

### Mechanism 2
Autoregressive generation of multiple query embeddings allows the model to condition each embedding on previous predictions, enabling diverse coverage of target space. The model produces the first embedding after seeing the query text, then conditions subsequent embeddings on the previous ground truth (training) or predicted (inference) embedding. This sequential dependency lets later embeddings specialize toward remaining uncovered targets rather than redundantly covering the same region.

### Mechanism 3
Hungarian matching between predicted embeddings and target embeddings during training enables learning without enforcing arbitrary orderings. Since target document sets are unordered, the loss function finds the optimal assignment between m predicted embeddings and m target embeddings that minimizes total contrastive loss. This prevents the model from wasting capacity learning spurious ordering patterns.

## Foundational Learning

- **Concept: Contrastive Learning (InfoNCE Loss)**
  - Why needed here: AMER trains with InfoNCE to maximize similarity between predicted query embeddings and positive document embeddings while pushing away in-batch negatives. Understanding this objective is essential for debugging training dynamics.
  - Quick check question: Given a batch with 8 queries each having 3 positive documents, how many negative documents does each query embedding see during loss computation?

- **Concept: Autoregressive Language Models**
  - Why needed here: AMER repurposes an autoregressive LM to output embedding sequences instead of token sequences. The sequential conditioning mechanism is borrowed from standard LM training but applied to continuous outputs.
  - Quick check question: How does teacher forcing differ from scheduled sampling in this embedding generation context?

- **Concept: Bi-Encoder Retrieval Architecture**
  - Why needed here: The paper assumes separate query and document encoders with similarity computed via dot product or cosine. AMER modifies only the query encoder while freezing the document encoder.
  - Quick check question: What are the computational tradeoffs of freezing the document encoder versus jointly training both encoders?

## Architecture Onboarding

- **Component map**: Query text -> tokenizer -> LLM hidden states -> output projection -> m_pred embeddings -> each retrieves top-k -> round-robin merge -> final ranked list

- **Critical path**: Query text → tokenizer → LLM hidden states → output projection → m_pred embeddings → each retrieves top-k → round-robin merge → final ranked list

- **Design tradeoffs**: Fixed m_pred vs. variable (paper uses fixed 2 for AmbigQA, 5 for QAMPARI); frozen document encoder (faster iteration but limits end-to-end optimization); random negatives vs. hard negatives (paper uses random for controlled comparison)

- **Failure signatures**: Low output diversity (if pairwise similarity of generated embeddings is too high, check via L2/cosine distance); synthetic success but real-world failure (indicates target diversity gap); scheduled sampling ablation underperforms (suggests train-inference mismatch is critical)

- **First 3 experiments**:
  1. Reproduce synthetic experiment (Section 5) with linear and MLP transformations to validate multi-embedding capacity; target 100% MRECALL vs. single-query ~20%
  2. Ablate scheduled sampling (Table 3) to confirm it outperforms always-predicted conditioning; expect 3-15% relative degradation without it
  3. Evaluate on low-similarity subset of QAMPARI to verify larger gains (target 100%+ relative improvement) versus whole-set gains (target ~20%)

## Open Questions the Paper Calls Out

### Open Question 1
Can the model be improved to dynamically determine the optimal number of query embeddings to generate per input, rather than relying on a fixed hyperparameter? The authors state in the Future Work section that research could explore "learning to flexibly decide the number of query vectors to predict."

### Open Question 2
What are the most effective methods for aggregating multiple ranked lists from different query embeddings into a final ranking? The conclusion explicitly identifies the need for "more effectively aggregating across different outputs from different queries."

### Open Question 3
Does incorporating hard negative mining and large-scale unsupervised pre-training significantly improve AMER's performance on real-world tasks? The authors note that developing better training data, specifically "mining hard negatives and training on a much larger scale of unsupervised data," is necessary to scale the approach.

### Open Question 4
Does freezing the document encoder prevent the model from fully optimizing the embedding space for multi-query retrieval? While the fixed encoder simplifies development, the multi-query vectors are forced to fit into an existing single-query-optimized embedding space, potentially limiting the expressiveness of the diverse targets.

## Limitations
- Modest real-world gains (4-21% relative improvement) despite perfect synthetic performance suggest optimization or dataset challenges
- Requires knowing number of target modes (m_pred) in advance, limiting open-ended applications
- Frozen document encoder restricts end-to-end optimization potential

## Confidence
- **High confidence**: Synthetic experiment results showing single-query models struggle with diverse targets while AMER achieves perfect performance
- **Medium confidence**: Real-world performance gains, as modest overall improvements could reflect dataset characteristics
- **Medium confidence**: Scheduled sampling importance, though ablation shows degradation, the magnitude suggests it's helpful but not critical

## Next Checks
1. **Diagnostic embedding analysis**: After reproducing synthetic results, compute pairwise cosine similarity between generated query embeddings and compare to target document diversity. Verify that AMER produces more diverse embeddings than single-query models when targets are multimodal.

2. **Ablation on scheduled sampling**: Implement and compare "Always Predicted" vs. scheduled sampling variants on both synthetic and real data. Measure performance degradation and check if the gap widens on high-diversity subsets.

3. **End-to-end optimization test**: Fine-tune both query and document encoders jointly on QAMPARI for 10-20 epochs. Compare performance to frozen document encoder baseline to quantify the optimization limitation.