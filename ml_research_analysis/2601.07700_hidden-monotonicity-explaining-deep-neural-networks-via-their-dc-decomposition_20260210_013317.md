---
ver: rpa2
title: 'Hidden Monotonicity: Explaining Deep Neural Networks via their DC Decomposition'
arxiv_id: '2601.07700'
source_url: https://arxiv.org/abs/2601.07700
tags:
- networks
- layer
- splitcam
- network
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method to decompose any ReLU network into
  two monotone and convex subnetworks, thereby enabling more interpretable saliency
  explanations. The key innovation lies in stabilizing both forward and backward passes
  through affine transformations, which overcomes numerical instabilities arising
  from the non-negative weight matrices in the decomposed networks.
---

# Hidden Monotonicity: Explaining Deep Neural Networks via their DC Decomposition

## Quick Facts
- arXiv ID: 2601.07700
- Source URL: https://arxiv.org/abs/2601.07700
- Authors: Jakob Paul Zimmermann; Georg Loho
- Reference count: 40
- Primary result: Decomposes ReLU networks into two monotone convex subnetworks to stabilize and improve saliency explanations

## Executive Summary
This paper introduces a method to decompose ReLU networks into differences of two monotone convex networks, enabling more stable and interpretable saliency explanations. The key innovation is stabilizing both forward and backward passes through affine transformations, addressing numerical instabilities from non-negative weight matrices. The authors develop new attribution methods (SplitCAM, SplitLRP, SplitGrad) by adapting existing XAI techniques to operate on the decomposed networks. Experiments demonstrate state-of-the-art performance on Quantus metrics across VGG16 and ResNet18 on ImageNet-S, outperforming classical baselines. The paper also shows that directly training models as differences of monotone networks yields self-explainable properties with distinct roles for the two streams.

## Method Summary
The method decomposes any ReLU network into two monotone and convex subnetworks by applying affine transformations to stabilize both forward and backward passes. This decomposition overcomes numerical instabilities arising from the non-negative weight matrices inherent in the decomposed networks. The authors adapt existing XAI techniques (CAM, LRP, Grad) to operate on these decomposed networks, creating new attribution methods: SplitCAM, SplitLRP, and SplitGrad. The approach enables more interpretable saliency explanations by leveraging the monotonic properties of the decomposed networks.

## Key Results
- State-of-the-art performance on Quantus metrics for faithfulness, localization, and robustness
- Outperforms classical attribution baselines on VGG16 and ResNet18 across ImageNet-S
- Demonstrates that decomposed networks capture distinct semantic roles (present vs. missing features)
- Shows self-explainable properties from direct training of monotone network differences

## Why This Works (Mechanism)
The decomposition into monotone convex subnetworks creates inherently interpretable components where each subnetwork captures specific aspects of the input-output relationship. The stability mechanisms ensure reliable gradient computations during backpropagation, which is critical for accurate attribution. The affine transformations prevent numerical instabilities that would otherwise arise from the non-negative weight constraints in the decomposed networks.

## Foundational Learning
- **DC Decomposition**: Breaking down a function into the difference of two convex functions, needed to create interpretable network components; quick check: verify convexity of each component
- **ReLU Network Properties**: Understanding how ReLU activations affect network monotonicity; quick check: confirm all weights remain non-negative in decomposed networks
- **Attribution Methods**: CAM, LRP, and Grad techniques for generating saliency maps; quick check: compare attribution quality with and without decomposition
- **Quantus Metrics**: Standardized evaluation framework for XAI methods; quick check: ensure all three metrics (faithfulness, localization, robustness) are computed
- **Numerical Stability**: Techniques for preventing overflow/underflow in deep networks; quick check: monitor gradient norms during training

## Architecture Onboarding

**Component Map**: Input -> ReLU Network -> DC Decomposition -> Two Monotone Networks -> Attribution Methods -> Saliency Maps

**Critical Path**: The forward pass through the decomposed networks must remain numerically stable, followed by accurate gradient computation in the backward pass. The attribution methods then aggregate these gradients into interpretable saliency maps.

**Design Tradeoffs**: The decomposition adds computational overhead but enables more interpretable explanations. The affine transformations improve stability but may introduce approximation errors. Direct training of monotone differences versus post-hoc decomposition represents another tradeoff in implementation complexity.

**Failure Signatures**: Numerical instabilities manifest as exploding or vanishing gradients during backpropagation. Poor attribution quality may indicate inadequate stabilization of the decomposed networks. The two streams failing to capture distinct semantic roles suggests the decomposition is not properly aligned with the network's decision-making process.

**First Experiments**: 1) Verify DC decomposition correctness on a simple network with known properties. 2) Test attribution quality on a small dataset with ground truth explanations. 3) Benchmark computational overhead against baseline attribution methods on a single network architecture.

## Open Questions the Paper Calls Out
None

## Limitations
- Method's reliance on ReLU activation limits applicability to networks using other activation functions
- Computational overhead from stability mechanisms not thoroughly characterized
- Generalization beyond image classification domains remains unexplored
- Self-explainable properties from direct training lack extensive empirical validation

## Confidence

**High**: Mathematical formulation and stability improvements are well-grounded and clearly demonstrated. Experimental results showing improved Quantus metrics are convincing within tested scope.

**Medium**: Distinct semantic roles captured by two subnetworks is intuitively appealing but based on limited qualitative examples. Generalization of attribution adaptations remains unexplored.

**Low**: Self-explainable properties from direct training lack extensive empirical validation with only preliminary results presented.

## Next Checks
1. Benchmark computational overhead of stability mechanisms against baseline attribution methods across different network depths and batch sizes
2. Test decomposition and attribution methods on non-image domains (tabular data, time series) to assess generalizability
3. Conduct ablation studies to isolate contribution of each stability component to overall attribution quality