---
ver: rpa2
title: Planned Event Forecasting using Future Mentions and Related Entity Extraction
  in News Articles
arxiv_id: '2511.07879'
source_url: https://arxiv.org/abs/2511.07879
tags:
- related
- entities
- event
- entity
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work develops a method to forecast planned civil unrest events
  by analyzing future mentions and related entities in news articles. It addresses
  the challenge of identifying not just any entities in a news article, but those
  specifically involved in the event (termed "Related Entities").
---

# Planned Event Forecasting using Future Mentions and Related Entity Extraction in News Articles

## Quick Facts
- arXiv ID: 2511.07879
- Source URL: https://arxiv.org/abs/2511.07879
- Reference count: 2
- One-line primary result: Forecasting planned civil unrest events by identifying future mentions and extracting related entities from news articles with 85% precision for relevant article identification and 64.3% precision for related entity extraction

## Executive Summary
This paper presents a novel approach for forecasting planned civil unrest events by analyzing news articles for future mentions and extracting entities specifically involved in those events. The system addresses the challenge of identifying not just any entities mentioned in an article, but those actually participating in or organizing planned protests, rallies, and demonstrations. Using a combination of word2vec phrase learning, LDA topic modeling, NER, and relation extraction with spatial locality patterns, the method achieves 85% precision, 69.5% recall, and 76.6% F-measure for identifying relevant articles, and 64.3% precision, 63% recall, and 87% accuracy for extracting related entities (date, location, person, organization).

## Method Summary
The method processes news articles through a sequential pipeline: RSS feeds are fetched and preprocessed (tokenized, lemmatized, stop words removed), then filtered using word2vec phrase learning with seed words "PROTEST" and "DEMONSTRATION" to find related protest terminology. A two-stage filtering approach applies LDA topic modeling to further refine the article set by identifying protest-specific topics. Named Entity Recognition using Stanford CRF extracts entity attributes, and relation extraction builds triplets to identify spatial locality patterns. The novel "Related Entity Extraction" method uses a window-based approach around identified relations to capture entities actually involved in events. Time normalization standardizes future dates for forecasting purposes.

## Key Results
- 85% precision, 69.5% recall, and 76.6% F-measure for identifying relevant protest-related articles
- 64.3% precision, 63% recall, and 87% accuracy for extracting related entities (Date, Location, Person, Organization)
- Successfully captures culturally-specific protest terminology (e.g., "bandh," "dharna") through word2vec phrase learning
- Two-stage filtering (phrase + LDA) effectively reduces noise compared to single-stage approaches

## Why This Works (Mechanism)

### Mechanism 1: Contextual Phrase Learning
Word2Vec embeddings trained on the target corpus identify words sharing similar context with seed words (PROTEST, DEMONSTRATION), capturing domain-specific and local-language terminology that synonym-based approaches miss. This learns terms like "bandh," "dharna," and "agitation" that are culturally specific but semantically aligned with protest concepts.

### Mechanism 2: Spatial Locality in Entity Extraction
Related entities co-occur in spatial proximity within text, enabling window-based extraction. Relation extraction builds triplets (entity1, verb_phrase, entity2), and when a pattern match identifies one related entity, a fixed window around that relation captures other involved entities (person, organization, location).

### Mechanism 3: Two-Stage Filtering
Two-stage filtering (phrase-based + LDA) reduces noise better than either method alone. Phrase filtering creates a denser subset of protest-relevant articles, then LDA identifies latent topics within this subset, allowing document-topic probability thresholds to exclude remaining noise (e.g., "surgical strike").

## Foundational Learning

- **Word2Vec Distributional Semantics**
  - Why needed here: To expand seed keywords into a broader, corpus-specific vocabulary without manual synonym curation
  - Quick check question: Can you explain why "bandh" would appear similar to "protest" in embedding space even if they share no words?

- **Latent Dirichlet Allocation (LDA)**
  - Why needed here: To model documents as mixtures of topics and filter based on topic probability, removing phrase-matched false positives
  - Quick check question: What happens to topic coherence if you apply LDA directly to a corpus where 99% of documents are irrelevant to your target topic?

- **Named Entity Recognition (NER) with CRF**
  - Why needed here: To extract structured attributes (Person, Organization, Location, Date) from unstructured text for event forecasting
  - Quick check question: Why might a CRF-based NER tag "March" as a date in one context and a location in another?

## Architecture Onboarding

- **Component map**: RSS Fetcher -> Preprocessor -> Phrase Filter (Word2Vec) -> Topic Filter (LDA) -> NER Tagger (Stanford CRF) -> Relation Extractor -> Related Entity Extractor (Window-based) -> Time Normalizer

- **Critical path**: Phrase learning quality -> LDA topic separation -> Relation triplet accuracy -> Window size tuning for entity extraction. The pipeline is sequential; errors propagate.

- **Design tradeoffs**:
  - Precision vs. Recall: Phrase filtering + LDA optimizes precision (85%) at some recall cost (69.5%)
  - Simplicity vs. Robustness: Authors explicitly ignore co-reference resolution and entity disambiguation, accepting noise for architectural simplicity
  - Geographic independence vs. local optimization: Word2Vec trained on local corpus captures regional terms but may not transfer

- **Failure signatures**:
  - High false positives on military/metaphorical uses of "strike," "march"
  - Missing entities when article structure scatters related mentions
  - Duplicate event reports not deduplicated
  - Relative dates ("next Wednesday") may fail normalization if publication date missing

- **First 3 experiments**:
  1. **Phrase learning validation**: Run Word2Vec on sample corpus with seed words; manually inspect top-20 similar words. Are local terms and synonyms captured without drift?
  2. **LDA topic inspection**: Apply LDA to phrase-filtered corpus; examine top words per topic. Is there a coherent "protest" topic separable from general politics?
  3. **Window size ablation**: Vary window size (1-5 relation triplets) for Related Entity Extraction; measure precision/recall on manually tagged relation triplets to find optimal locality threshold.

## Open Questions the Paper Calls Out

1. **Can sentiment analysis techniques reliably quantify the intensity of planned social unrest events to assist authorities?**
   - Basis in paper: Authors state that "one can apply sentiment analysis techniques to define the intensity of the event, which can help authorities to plan according to that."
   - Why unresolved: The current system extracts structural attributes (who, when, where) but does not assess the potential severity or emotional volatility of the event.

2. **To what extent can text summarization methods accurately extract the specific purpose or cause of a planned protest?**
   - Basis in paper: Authors propose "purpose of that event could also be forecasted by using text summarization techniques" as future work.
   - Why unresolved: The current implementation focuses on entity extraction but lacks a module to synthesize the underlying narrative or reason for the unrest from the article text.

3. **How does the integration of coreference resolution and entity disambiguation impact the accuracy of the Related Entity Extraction model?**
   - Basis in paper: Authors note they "ignored other issues like... entity co-referencing and entity disambiguation" and suggest these could be "incorporated later for further improvement."
   - Why unresolved: The current model relies on spatial locality (windowing) and may fail to link entities mentioned across different sentences or distinguish between entities with similar names.

## Limitations
- Evaluation based solely on a single corpus of Indian news articles from January to March 2017, with no external validation on independent datasets or different geographic regions
- Spatial locality assumption for related entity extraction may not generalize beyond Indian news writing conventions
- Authors explicitly acknowledge ignoring co-reference resolution and entity disambiguation, which likely introduces noise in extracted entities

## Confidence
- **High Confidence**: The basic pipeline architecture is well-specified and technically sound. The reported precision for relevant article identification (85%) is credible given the two-stage filtering approach.
- **Medium Confidence**: The specific performance metrics (69.5% recall for article identification, 64.3% precision for related entity extraction) are likely accurate for the tested corpus but may not generalize. The choice of word2vec similarity threshold (0.68) and LDA parameters appears reasonable but lacks sensitivity analysis.
- **Low Confidence**: Claims about geographic independence and generalizability are unsupported - the entire evaluation is based on a single language and region. The assertion that this method works for "any planned social unrest event" lacks empirical validation beyond the tested case.

## Next Checks
1. **Cross-dataset validation**: Apply the complete pipeline to news articles from a different country/language (e.g., US or UK news sources) and compare performance metrics. This would test the geographic independence claim and reveal whether the word2vec phrase learning captures universal protest-related terminology or is India-specific.

2. **Temporal generalization test**: Evaluate the system on articles from different time periods (pre-2017 and post-2017) to assess whether the LDA topic model remains effective as protest discourse evolves. This would identify whether the topic separation is stable over time or requires periodic retraining.

3. **Error analysis of related entity extraction**: Manually annotate a sample of extracted entities to quantify false positives (entities not actually involved in the event) versus false negatives (missed entities). This would validate the spatial locality assumption and identify whether the window-based approach or relation pattern matching is the primary source of extraction errors.