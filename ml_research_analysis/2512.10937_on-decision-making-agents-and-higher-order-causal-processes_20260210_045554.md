---
ver: rpa2
title: On Decision-Making Agents and Higher-Order Causal Processes
arxiv_id: '2512.10937'
source_url: https://arxiv.org/abs/2512.10937
tags:
- process
- functions
- function
- unique
- quantum
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work establishes a mathematical correspondence between deterministic
  agent-state policies in POMDPs and process functions, the classical-deterministic
  limit of higher-order quantum operations. The core method involves defining equivalence
  classes of agent-state policies and showing they are in bijection with process functions
  via a fixed-point criterion.
---

# On Decision-Making Agents and Higher-Order Causal Processes

## Quick Facts
- arXiv ID: 2512.10937
- Source URL: https://arxiv.org/abs/2512.10937
- Authors: Matt Wilson
- Reference count: 40
- This work establishes a mathematical correspondence between deterministic agent-state policies in POMDPs and process functions, demonstrating quantum-like computational advantages in classical multi-agent settings.

## Executive Summary
This paper establishes a mathematical correspondence between deterministic agent-state policies in POMDPs and process functions, the classical-deterministic limit of higher-order quantum operations. The core method involves defining equivalence classes of agent-state policies and showing they are in bijection with process functions via a fixed-point criterion. This correspondence allows translating multi-agent policy evaluation into higher-order function evaluation, revealing that observation-independent decentralized POMDPs naturally model multi-input process functions. The primary result is a strict separation theorem showing that certain POMDPs (based on the majority-GYNI game) can achieve strictly higher finite-horizon rewards with indefinite causal structures than with definite causal orders, demonstrating quantum-like computational advantages in classical multi-agent settings.

## Method Summary
The paper constructs a correspondence between deterministic POMDP agent-state policies and process functions through a fixed-point criterion. Agent-to-process function translation uses w[π,U](m,o) := (U(m,π(m),o), π(m)), while process-to-agent translation uses π_w := w_I and U_w(m,a,o) := w_F(m,o). The framework introduces multi-input process functions and contraction operations for evaluating policies on product spaces. The Lugano process demonstrates an indefinite causal order implementation that achieves perfect reward in the majority-GYNI game, while definite causal orders are provably bounded at 3/4 of maximum reward.

## Key Results
- Established bijection between deterministic agent-state policies and process functions via fixed-point criterion
- Demonstrated that observation-independent dec-POMDPs naturally model multi-input process functions
- Proved strict reward separation: Lugano process achieves perfect reward while definite causal orders are bounded at 3/4 maximum in majority-GYNI game

## Why This Works (Mechanism)
The correspondence works because both agent-state policies and process functions can be characterized by fixed-point equations over their respective state spaces. Agent policies π : M → A × M and process functions w : M → Ω × M both describe deterministic state transitions conditioned on inputs. The bijection arises from mapping agent memory updates to process function outputs and vice versa. Multi-input process functions naturally emerge from observation-independent dec-POMDPs because agents can share observations without causal constraints, enabling indefinite causal structures through the contraction operation.

## Foundational Learning

1. **Agent-State Policies (π: M → A × M)**
   - Why needed: Characterizes deterministic agent behavior with memory
   - Quick check: Verify π maps memory to action-output and next memory state

2. **Process Functions (w: M → Ω × M)**
   - Why needed: Classical limit of higher-order quantum operations
   - Quick check: Confirm w maintains fixed-point structure under composition

3. **Fixed-Point Criterion**
   - Why needed: Establishes bijection between policies and process functions
   - Quick check: Verify w satisfies w_F = π and w_I = π_F for some policy π

4. **Contraction Operation**
   - Why needed: Evaluates multi-input process functions on product spaces
   - Quick check: Confirm oi = gi(w_Ii(p,⃗o)) has unique solution

5. **Definite Causal Order**
   - Why needed: Characterizes when causal structure is fixed
   - Quick check: Verify factor independence and recursive contraction properties

## Architecture Onboarding

**Component Map:**
Majority-GYNI Game → POMDP Policies → Process Functions → Fixed-Point Evaluation → Reward Calculation

**Critical Path:**
Game specification → Policy enumeration → Process function translation → Fixed-point evaluation → Reward comparison

**Design Tradeoffs:**
- Deterministic vs stochastic policies: Paper focuses on deterministic for mathematical tractability
- Observation independence: Enables multi-input process function modeling but restricts game class
- Memory initialization: Fixed initial state simplifies analysis but may limit generality

**Failure Signatures:**
- Incorrect fixed-point evaluation yields wrong reward bounds
- Misidentified definite causal orders due to incomplete contraction checks
- Q function evaluation errors in cyclic indexing

**First Experiments:**
1. Implement majority-GYNI game and verify observation-independence property
2. Enumerate all definite causal order policies and compute reward bounds
3. Implement Lugano process and verify perfect reward achievement

## Open Questions the Paper Calls Out
None

## Limitations
- Focus on deterministic policies excludes stochastic decision-making scenarios
- Assumption of fixed initial memory states may not hold in practical applications
- Computational complexity grows exponentially with number of agents

## Confidence
- Correspondence theorem (High): Mathematical bijection is rigorously proven
- Reward separation result (Medium): Concrete example established but broader applicability needs testing
- Multi-input process function framework (Medium): Well-defined mathematically but computational aspects need validation

## Next Checks
1. Implement contraction algorithm for multi-input process functions and verify it correctly identifies definite causal orders across various examples
2. Test correspondence with stochastic POMDP policies to understand limitations
3. Apply framework to alternative multi-agent games beyond majority-GYNI to assess generalizability of computational advantage