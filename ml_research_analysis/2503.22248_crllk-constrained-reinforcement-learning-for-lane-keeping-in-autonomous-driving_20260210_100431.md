---
ver: rpa2
title: 'CRLLK: Constrained Reinforcement Learning for Lane Keeping in Autonomous Driving'
arxiv_id: '2503.22248'
source_url: https://arxiv.org/abs/2503.22248
tags:
- lane
- learning
- autonomous
- driving
- reinforcement
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of lane keeping in autonomous
  driving by formulating it as a constrained reinforcement learning (RL) problem.
  The core idea is to dynamically learn weight coefficients for multiple objectives
  (travel distance, lane deviation, collision avoidance) instead of using fixed scenario-specific
  tuning.
---

# CRLLK: Constrained Reinforcement Learning for Lane Keeping in Autonomous Driving

## Quick Facts
- arXiv ID: 2503.22248
- Source URL: https://arxiv.org/abs/2503.22248
- Reference count: 27
- Primary result: Dynamic Lagrangian weight learning achieves 69.0 travel reward vs 46.6 for PPO, with lower lane deviation (0.66 vs 0.99) and collision costs (0.17 vs 0.38) in simulation

## Executive Summary
This work addresses lane keeping in autonomous driving by formulating it as a constrained reinforcement learning problem. The core innovation is using adaptive Lagrangian multipliers to dynamically learn weight coefficients for multiple objectives (travel distance, lane deviation, collision avoidance) instead of manual tuning. The approach employs a one-timescale framework that updates policy and constraints simultaneously, validated through simulation and real-world Duckiebot demonstrations across multiple scenarios.

## Method Summary
The method formulates lane keeping as a constrained RL problem: maximize travel distance subject to lane deviation and collision cost constraints. It uses Lagrangian relaxation to convert this into an unconstrained problem, with adaptive multipliers λ₁ and λ₂ that penalize constraint violations. The one-timescale framework updates policy parameters θ and multipliers simultaneously in each training iteration. The modified reward combines the original travel reward with weighted constraint penalties, and multipliers are updated based on whether constraint thresholds are exceeded. Training uses PPO with the modified reward, and the system automatically learns the trade-offs between objectives.

## Key Results
- Simulation on small loop: CRLLK achieves J_R = 69.0 vs PPO's 46.6, with J_clane = 0.66 vs 0.99 and J_ccoll = 0.17 vs 0.38
- Real-world Duckiebot validation shows smooth navigation across multiple scenarios despite simulation-to-reality differences
- Dynamic weight learning eliminates extensive parameter tuning while improving both efficiency and reliability

## Why This Works (Mechanism)

### Mechanism 1
Lagrangian relaxation dynamically learns objective weights instead of requiring manual tuning. The constrained optimization is converted to an unconstrained problem via Lagrangian multipliers λ₁ and λ₂ that act as adaptive penalty weights—when a constraint is violated, λ increases, penalizing that cost more heavily in the modified reward. When the constraint is satisfied, λ decreases. Core assumption: the cost constraints are soft enough that a saddle-point solution exists where both policy and multipliers converge.

### Mechanism 2
One-timescale simultaneous updates enable practical training that tracks episodic collision costs. Unlike RCPO (which uses two-timescale updates with truncated samples), CRLLK updates θ and (λ₁, λ₂) in the same iteration. This allows collision costs—measured across multiple episodes—to directly influence multiplier updates without requiring expensive inner-loop optimization. Core assumption: simultaneous updates converge despite the non-stationary reward function caused by changing λ values.

### Mechanism 3
Constraint threshold values (α₁, α₂) encode task specification without requiring weight tuning. The operator specifies acceptable lane deviation (α₁ in decimeters) and collision tolerance (α₂). The system automatically finds the corresponding weights that satisfy these constraints while maximizing reward. Tighter constraints (lower α₁) produce more cautious policies with more corrective steering. Core assumption: the specified thresholds are achievable within the environment's dynamics.

## Foundational Learning

- **Lagrangian Duality for Constrained Optimization**: Why needed: The entire method rests on converting "maximize reward subject to constraints" into "maximize reward minus penalty-weighted constraint violations." Quick check: If λ₁ = 0 after training, what does this imply about the lane deviation constraint?

- **Proximal Policy Optimization (PPO)**: Why needed: CRLLK builds on PPO; the clipped objective and value function baseline are inherited. The modified reward is fed into standard PPO machinery. Quick check: Why does PPO use a clipped surrogate objective rather than direct policy gradient?

- **Sim-to-Real Transfer in Robotics**: Why needed: The paper trains in simulation and deploys on physical Duckietown. Understanding domain gaps helps diagnose real-world failures. Quick check: What sim-to-real gaps might cause a policy trained on simulated lane markings to fail on physical roads?

## Architecture Onboarding

- **Component map**:
  [Camera Input] → [Perception Module] → [State s_t] → [Policy Network π_θ(a|s)] → [Action Output: discrete {L, S, R} or continuous steering]
  [Training Loop]: Collect trajectories → Compute rewards and costs → Update λ via gradient descent → Compute modified reward → Update θ via PPO

- **Critical path**: (1) Define accurate cost functions c_lane and c_coll. (2) Set constraint thresholds α₁, α₂ based on operational requirements. (3) Initialize λ values (small positive). (4) Monitor λ convergence alongside policy performance.

- **Design tradeoffs**: Tighter constraints (lower α₁) → safer but slower driving. Discrete vs. continuous action space: CRLLK-D shows more stable results (69.0 ± 0.6) than CRLLK-C (62.4 ± 22.2). One-timescale vs. two-timescale: Faster training but potentially less stable convergence guarantees.

- **Failure signatures**: λ values oscillating wildly indicates learning rates may be too high. λ → 0 early suggests constraints are too loose. λ → ∞ indicates infeasible thresholds. Policy ignores lane markings in real-world suggests sim-to-real perception gap.

- **First 3 experiments**:
  1. Implement baseline PPO with fixed reward weights on small loop scenario to match reported metrics (J_R ≈ 46.6, J_clane ≈ 0.99).
  2. Train CRLLK with α₁ ∈ {0.3, 0.5, 0.75, 1.0} while holding α₂ = 0.02. Plot final J_R vs. J_clane to verify constraint-performance trade-off.
  3. Test η₁ ∈ {1e-5, 1e-4, 1e-3} to find stable λ learning rate range. Monitor λ₁ trajectories to identify oscillation vs. smooth convergence.

## Open Questions the Paper Calls Out

### Open Question 1
Is manual selection of constraint thresholds (α) fundamentally easier or more intuitive than manual reward weight tuning for non-expert users? While the method automates weight learning, it shifts the burden to defining acceptable cost limits, which may still require domain expertise. A user study comparing the time and effort required for humans to tune α values versus reward weights would resolve this.

### Open Question 2
Does the one-timescale update mechanism maintain stability and convergence guarantees when transferred to full-scale vehicles with complex, high-speed dynamics? One-timescale updates are often theoretically less stable than two-timescale approaches; performance might degrade in high-fidelity simulations or real-world highway driving. Empirical evaluation in CARLA or on a full-sized test vehicle would resolve this.

### Open Question 3
Can the constrained formulation adapt quickly enough to non-stationary collision risks in dense, dynamic traffic environments? The Lagrangian multipliers adapt based on cost violations; it is unclear if this feedback loop is fast enough to adjust the policy for rapidly changing collision probabilities typical in urban traffic. Testing in a dense multi-agent environment with moving obstacles and pedestrians would resolve this.

## Limitations
- The perception pipeline for extracting lane deviation from camera images is not detailed, making faithful reproduction challenging
- Theoretical convergence guarantees for the specific combination of PPO and adaptive multipliers are not provided
- Results are primarily on simple loop scenarios; performance on complex urban environments with intersections and varying traffic remains untested

## Confidence

- **High Confidence**: Lagrangian relaxation mechanism for dynamic weight learning is well-established in constrained optimization theory
- **Medium Confidence**: One-timescale update approach trades theoretical convergence guarantees for practical training efficiency
- **Medium Confidence**: Real-world demonstration on Duckiebot validates the approach but uses a simplified platform

## Next Checks

1. **Constraint Feasibility Analysis**: Systematically vary α₁ from 0.3 to 1.0 while monitoring λ₁ convergence and final J_clane. Verify that tighter constraints produce higher λ₁ values and that all tested α₁ values remain feasible (λ₁ converges to finite values).

2. **Sim-to-Real Perception Gap**: Implement domain randomization in simulation (varying lighting, road textures, camera noise) and measure performance degradation when transferring to physical Duckietown. Quantify the perception accuracy needed for successful transfer.

3. **Multi-Scenario Generalization**: Test the trained policy from the small loop scenario on the U-turn and long loop scenarios without retraining. Measure J_R, J_clane, and J_ccoll across all three scenarios to evaluate policy robustness to different track geometries.