---
ver: rpa2
title: Generative Click-through Rate Prediction with Applications to Search Advertising
arxiv_id: '2507.11246'
source_url: https://arxiv.org/abs/2507.11246
tags:
- user
- prediction
- generative
- behavior
- item
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GenCTR, a novel generative approach for enhancing
  click-through rate (CTR) prediction in search advertising. Unlike traditional discriminative
  models that directly predict CTR from user and item features, GenCTR leverages generative
  pre-training on user behavior sequences to capture richer user-item interactions.
---

# Generative Click-through Rate Prediction with Applications to Search Advertising

## Quick Facts
- arXiv ID: 2507.11246
- Source URL: https://arxiv.org/abs/2507.11246
- Reference count: 2
- Primary result: Generative pre-training on user behavior sequences improves CTR prediction by up to 0.04 AUC and achieves 1.32% CTR uplift in live deployment

## Executive Summary
This paper introduces GenCTR, a generative approach for enhancing click-through rate prediction in search advertising. The method leverages generative pre-training on user behavior sequences to capture richer user-item interactions, employing a two-stage training process: first pre-training a conditional generative model on next-item prediction, then fine-tuning within a discriminative CTR prediction framework through parameter sharing and model integration. The approach is evaluated on a newly collected dataset (GCTR) from a large e-commerce platform, demonstrating significant improvements across three backbone models in both offline and online metrics.

## Method Summary
GenCTR uses a two-stage training process: first, a conditional generative model pre-trains on next-item prediction using user behavior sequences and item category information; second, this pre-trained model is fine-tuned within a discriminative CTR prediction framework through parameter sharing and model integration. The architecture employs a conditional self-attention decoder with causal masking, where the query comes from target category while key/value come from preceding items. Category-conditioned negative sampling forces the model to distinguish among items within the same category.

## Key Results
- Up to 0.04 AUC improvement over baseline models (DNN, DCN V2, DCN V2 & TA)
- 1.32% CTR and 1.66% RPM uplift in online A/B testing
- Category-Conditioned Negative Sampling consistently outperforms Random Negative Sampling
- Parameter sharing alone yields 0.56% CTR uplift without increasing inference complexity

## Why This Works (Mechanism)

### Mechanism 1
Conditional generative pre-training transfers item representations that improve CTR discrimination beyond random initialization. The model learns to predict next-item embeddings given side information (category). By conditioning the query on target category while masking the item itself, the decoder must learn fine-grained item distinctions within categories—precisely the discrimination needed for CTR ranking. Evidence: Category-Conditioned Negative Sampling consistently outperforms Random Negative Sampling in ablation tests.

### Mechanism 2
Parameter sharing alone yields measurable CTR gains without increasing inference complexity. Pre-trained item and side-info embeddings replace random embeddings in the backbone CTR model. The backbone benefits from representations refined on larger behavioral data without any architectural changes. Evidence: Parameter sharing alone led to 0.56% CTR uplift in production deployment.

### Mechanism 3
Full model integration (feeding generative decoder output as additional features) provides incremental gains over parameter sharing alone. The decoder output captures sequential interaction patterns and latent context beyond static embeddings. This becomes an auxiliary input to the CTR model, enriching the feature space. Evidence: Model inheritance outperforms parameter sharing alone in offline experiments, with additional 1.32% CTR uplift in online testing.

## Foundational Learning

- **Self-Attention Decoder with Causal Masking**: The pre-training stage uses a decoder-only transformer where each position can only attend to earlier items. Understanding this is essential to see why the query must come from side info rather than the target item itself. Quick check: If you unmasked the target item during pre-training, what would happen to the learning signal?

- **Negative Sampling for Contrastive Learning**: Category-conditioned negative sampling forces the model to distinguish among items within the same category, not just across categories. This is central to why CS outperforms RS. Quick check: Why would random negatives be "too easy" for a model that already knows the category?

- **Transfer Learning via Parameter Initialization vs. Feature Concatenation**: GenCTR uses both strategies—parameter sharing (initialization) and model integration (feature concatenation). Distinguishing these helps interpret the ablation results. Quick check: Which approach adds inference-time compute? Which adds training-time complexity only?

## Architecture Onboarding

- **Component map**: User Behavior Sequence + Side Info → Conditional Self-Attention Decoder (pre-trained, frozen or fine-tuned) → Decoder Output g(s,c) → Concatenate with CTR features → Backbone CTR Model (DNN/DCN V2/DCN V2&TA) → CTR Prediction

- **Critical path**: 1) Pre-train decoder on user behavior sequences with category-conditioned negatives (3 epochs) 2) Initialize backbone embeddings from pre-trained embeddings (parameter sharing) 3) Connect decoder output as auxiliary input (model integration) 4) Fine-tune end-to-end on CTR labels (1 epoch)

- **Design tradeoffs**: Decoder depth uses single-layer for efficiency; deeper decoders may capture more complex patterns but increase latency. Freezing vs. fine-tuning decoder: paper fine-tunes; freezing would reduce training cost but may lose adaptation. Behavior sequence length truncated to 200; longer sequences may help but hit memory/compute limits.

- **Failure signatures**: Random negative sampling outperforms conditioned sampling suggests category granularity is wrong. Model integration hurts performance indicates decoder may be overfitting to pre-training objective. AUC improves but online CTR doesn't reflects offline metrics not capturing position bias. Training instability after integration suggests learning rate may need separate scheduling.

- **First 3 experiments**: 1) Parameter sharing baseline: Replace backbone embeddings with pre-trained embeddings, measure AUC/LogLoss delta. 2) Negative sampling ablation: Compare CS vs. RS on your own data distribution. 3) Decoder depth sweep: Test 1-layer vs. 2-layer decoder, measure both AUC gain and inference latency.

## Open Questions the Paper Calls Out

### Open Question 1
How does GenCTR perform when scaling to lifelong user behavior sequences (e.g., >1,000 interactions) compared to existing long-sequence models? The paper limits experiments to "users' last 200 clicks" while Related Work explicitly cites SIM, ETA, and TWIN as methods that successfully incorporate "lifelong sequential behavior data". The computational complexity of the self-attention decoder (O(N^2)) may prohibit application to long sequences without the approximation techniques used in the cited baselines.

### Open Question 2
Does a bidirectional pre-training objective (similar to BERT4Rec) offer superior feature extraction for CTR prediction compared to the proposed unidirectional decoder? The authors acknowledge that BERT4Rec's bidirectional training provides a "more comprehensive understanding of the context" yet they implement a "decoder-only" architecture. The paper does not ablate the directionality of the transformer.

### Open Question 3
Is the effectiveness of Category-Conditioned Negative Sampling (CS) robust against data noise, such as ambiguous or overlapping category taxonomies? The paper claims CS outperforms random sampling by focusing on "item differences within categories", relying on the implicit assumption that category mappings are distinct and accurate. In real-world e-commerce, items often belong to multiple categories or ill-defined taxonomies.

## Limitations
- Proprietary dataset evaluation limits independent verification of claimed improvements
- Does not explore sensitivity to key hyperparameters like embedding dimension, sequence length, or decoder depth
- Cannot independently verify online A/B testing results without access to same traffic conditions

## Confidence

- **High Confidence**: Architectural design (conditional decoder pre-training, parameter sharing, model integration) is internally consistent and well-specified. Ablation studies on negative sampling strategies are methodologically sound.
- **Medium Confidence**: Offline performance improvements (AUC/LogLoss) are likely reproducible on similar datasets, but magnitude depends on dataset characteristics not fully characterized.
- **Low Confidence**: Online A/B testing results (1.32% CTR, 1.66% RPM uplift) cannot be independently verified without access to same traffic conditions and competitive landscape.

## Next Checks

1. **Dataset Independence Test**: Implement GenCTR on a public sequential recommendation dataset (e.g., Amazon, Taobao) to verify if similar AUC improvements are achievable without proprietary data.

2. **Latency Impact Measurement**: Instrument the deployed system to measure end-to-end inference latency with vs. without decoder integration, quantifying the compute cost of the claimed improvements.

3. **Transferability Analysis**: Test whether pre-trained embeddings from one product category transfer to another category with limited data, assessing the generality of the learned representations.