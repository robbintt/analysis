---
ver: rpa2
title: Enhancing Spoken Discourse Modeling in Language Models Using Gestural Cues
arxiv_id: '2503.03474'
source_url: https://arxiv.org/abs/2503.03474
tags:
- gesture
- language
- discourse
- gestures
- spoken
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes incorporating gestural cues into language models
  to improve spoken discourse modeling. The method encodes 3D human motion sequences
  into discrete gesture tokens using VQ-VAE, aligns these with text embeddings through
  feature alignment, and fine-tunes the model on text infilling tasks for discourse
  connectives, quantifiers, and stance markers.
---

# Enhancing Spoken Discourse Modeling in Language Models Using Gestural Cues

## Quick Facts
- **arXiv ID**: 2503.03474
- **Source URL**: https://arxiv.org/abs/2503.03474
- **Reference count**: 33
- **Primary result**: Incorporating gestures into language models improves discourse marker prediction accuracy by 4.8% average F1 score

## Executive Summary
This paper proposes a method to enhance spoken discourse modeling in language models by incorporating gestural cues. The approach encodes 3D human motion sequences into discrete gesture tokens using a VQ-VAE, aligns these with text embeddings through feature alignment, and fine-tunes the model on text infilling tasks for discourse connectives, quantifiers, and stance markers. The method shows that gestures improve marker prediction accuracy, particularly for underrepresented markers, demonstrating that non-verbal cues contain linguistically meaningful structure that complements textual information.

## Method Summary
The approach consists of three stages: (1) training a VQ-VAE to tokenize 3D upper-body motion sequences into discrete gesture tokens, (2) aligning these gesture embeddings with text embeddings through an MLP projector using a joint masked prediction objective, and (3) fine-tuning LoRA adapters on the task of predicting masked discourse markers. The model is trained on the BEAT2 dataset with paired text and motion data, using 30% masking during feature alignment and evaluating performance on three marker types (discourse connectives, quantifiers, and stance markers) through text infilling tasks.

## Key Results
- Gesture-enhanced models achieve an average F1 score improvement of 4.8% across all three tasks compared to text-only baseline
- Error analysis reveals gestures are particularly helpful for disambiguating underrepresented markers
- Feature alignment is critical, with removal causing significant performance drops (e.g., F1 dropping from 51.1→46.7 for discourse connectives)
- Improvements are especially notable for specific markers like "after" (+17 correct), "but" (+6), "few" (+22), "some" (+23), and "must" (+23)

## Why This Works (Mechanism)

### Mechanism 1: VQ-VAE Gesture Tokenization Preserves Motion Semantics
The VQ-VAE encoder divides gesture sequences into chunks, processes them through time-aware transformers, and quantizes latents against a learned codebook (K=512). The reconstruction objective forces the codebook embeddings to encode motion semantics rather than just spatial positions, capturing fine-grained motion details that correlate with discourse function.

### Mechanism 2: Feature Alignment via Joint Masked Prediction
An MLP projector maps 256-dim gesture embeddings to 768-dim text embedding space. Training uses combined Masked Gesture Prediction + Masked Language Modeling with 30% masking, freezing the LM throughout. This enables the model to learn cross-modal correspondences by projecting gesture embeddings into the LM's input space.

### Mechanism 3: Gesture Disambiguates Underrepresented Markers
Specific gesture forms co-occur with discourse functions—raised index finger for contrastive connectives, palm-down for assertive stance, lateral motion for temporal relations. These gestures provide classification signal when textual context is ambiguous or training data is sparse, particularly helping markers with limited training examples.

## Foundational Learning

- **Concept: VQ-VAE (Vector Quantized Variational Autoencoder)**
  - Why needed here: Converts continuous 3D motion sequences into discrete tokens with a learned codebook
  - Quick check question: Given a latent vector z and codebook with K entries, can you write the operation that produces the quantized output z_q?

- **Concept: Masked Language Modeling Objective**
  - Why needed here: Both feature alignment and downstream fine-tuning use MLM-style objectives
  - Quick check question: Why does 30% masking yield lower validation loss than 10% or 80% in Table 4?

- **Concept: LoRA (Low-Rank Adaptation)**
  - Why needed here: The paper freezes the pre-trained LM and only trains adapter layers
  - Quick check question: If LoRA rank r=128 and the hidden dimension is 768, how many trainable parameters does one LoRA layer add?

## Architecture Onboarding

- **Component map**: VQ-VAE Gesture Tokenizer (13 upper-body joints → chunked sequences → transformer encoder → codebook quantization → transformer decoder) -> Feature Alignment (MLP projector maps 256→768 dim) -> Pre-trained LM (RoBERTa-base, frozen during alignment) -> LoRA Adapters (r=128, α=256)

- **Critical path**: 1) Train VQ-VAE on motion reconstruction (57 epochs) → verify reconstruction quality visually 2) Extract gesture tokens from paired text-motion data using Whisper timestamps 3) Train MLP projector with joint MLM+MGP loss (20 epochs, early stop) 4) Fine-tune LoRA adapters for each task

- **Design tradeoffs**: K=512 codebook size balances variety vs. alignment difficulty; upper-body only reduces computation but loses numerical gesture information; 30% masking provides optimal learning signal

- **Failure signatures**: Text-only baseline outperforms gesture model → check alignment loss convergence; high variance across seeds → insufficient data for alignment; systematic confusion between similar markers → missing modality or inherently ambiguous

- **First 3 experiments**: 1) Reproduce text-only baseline on BEAT2 with RoBERTa-base + LoRA 2) Train gesture tokenizer in isolation and visualize reconstruction quality 3) Run adversarial evaluation (random embeddings, positional-only) to verify gesture embeddings contribute meaningful signal

## Open Questions the Paper Calls Out

- **Question**: Can gestural cues effectively resolve implicit coherence relations where meaning is conveyed solely through motion rather than explicit text?
- **Question**: How does the gesture alignment framework perform when applied to decoder-based language models trained for next-token prediction?
- **Question**: Does incorporating finger joint kinematics significantly improve the disambiguation of semantically similar classes, such as numerical quantifiers?
- **Question**: Is the proposed framework robust to the lower accuracy of 2D video estimation compared to the 3D motion capture data used in this study?

## Limitations

- The approach relies on gesture-discourse correlations learned from a specific dataset (BEAT2 with 25 speakers), limiting generalizability across populations
- Minimal validation that discrete tokens capture linguistically meaningful gesture semantics versus arbitrary motion patterns
- Performance improvements measured on BEAT2's specific marker sets may not generalize to other domains or languages

## Confidence

- **High Confidence**: Technical implementation details (VQ-VAE architecture, feature alignment procedure, LoRA fine-tuning) are well-specified and follow established practices
- **Medium Confidence**: Claim that gestures improve discourse marker prediction is supported by experimental results, but generalizability remains untested
- **Low Confidence**: Core assumption that gesture motion patterns contain systematically correlated discourse information that generalizes across speakers is not directly validated

## Next Checks

1. **Cross-Speaker Transfer Test**: Train the gesture tokenizer and feature alignment on data from a subset of speakers, then evaluate discourse marker prediction on held-out speakers to validate generalization
2. **Adversarial Gesture Embedding Evaluation**: Replace learned gesture embeddings with random vectors during inference while keeping model architecture unchanged to establish that gesture embeddings contribute meaningful signal
3. **Gesture Semantic Clustering Analysis**: Apply unsupervised clustering to learned gesture embeddings and compare cluster assignments to discourse marker semantics to validate that motion patterns capture discourse-relevant structure