---
ver: rpa2
title: Improving Brain-to-Image Reconstruction via Fine-Grained Text Bridging
arxiv_id: '2505.22150'
source_url: https://arxiv.org/abs/2505.22150
tags:
- text
- reconstruction
- image
- semantic
- fine-grained
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Fine-grained Brain-to-Image reconstruction
  (FgB2I), which addresses the problem of insufficient semantic details in brain-to-image
  reconstruction by employing fine-grained text descriptions as a bridge. The core
  method uses large vision-language models to generate detailed captions for visual
  stimuli and trains a brain-to-text model using three reward metrics (object accuracy,
  text-image semantic similarity, and image-image semantic similarity) guided by reinforcement
  learning.
---

# Improving Brain-to-Image Reconstruction via Fine-Grained Text Bridging

## Quick Facts
- arXiv ID: 2505.22150
- Source URL: https://arxiv.org/abs/2505.22150
- Reference count: 15
- Primary result: FgB2I improves reconstruction accuracy across multiple metrics when applied to LDM, BrainDiffuser, and MindEye methods

## Executive Summary
This paper addresses the problem of insufficient semantic details in brain-to-image reconstruction by employing fine-grained text descriptions as a bridge between fMRI signals and visual stimuli. The proposed method, Fine-grained Brain-to-Image reconstruction (FgB2I), uses large vision-language models to generate detailed captions for visual stimuli and trains a brain-to-text model using reinforcement learning guided by three reward metrics. The approach integrates these fine-grained text descriptions with existing reconstruction methods to improve reconstruction quality, with the largest improvements observed in LDM which relies solely on text guidance.

## Method Summary
The FgB2I method operates in three stages: (1) Detail enhancement using LLaVA-1.5-7b to generate fine-grained captions from visual stimuli, (2) Brain-to-text model training with subject-specific linear layers, a 32-layer Transformer, and GPT2-Base decoder using cross-entropy loss followed by reinforcement learning with three rewards (object accuracy, text-image semantic similarity, and image-image semantic similarity), and (3) Semantic fusion where decoded text embeddings are combined with existing method features via CLIP embedding fusion to guide the diffusion prior. The approach is evaluated on the NSD dataset across subjects 1, 2, 5, and 7.

## Key Results
- FgB2I improves CLIP accuracy from 77.0% to 79.4% when replacing original captions with detail-enhanced captions
- The approach achieves improvements across multiple metrics (SSIM, Alexnet, InceptionV3, CLIP) when applied to LDM, BrainDiffuser, and MindEye methods
- Visual examples demonstrate correction of semantic inconsistencies and addition of missing details like railings that were omitted in original reconstructions

## Why This Works (Mechanism)

### Mechanism 1
Enriching semantic ground truth with fine-grained captions improves the upper bound of reconstruction quality. Large Vision-Language Models generate detailed descriptions of training images, providing richer targets for the brain-to-text model to capture objects and attributes missing from original simple captions.

### Mechanism 2
Reinforcement Learning with non-differentiable rewards aligns decoded text with visual semantics better than cross-entropy loss alone. The REINFORCE algorithm optimizes text generation using three specific reward metrics: Object Accuracy (Jaccard similarity of nouns), Text-Image Semantic Similarity (CLIP score), and Image-Image Semantic Similarity.

### Mechanism 3
Fusing decoded text embeddings with direct brain-to-image features provides complementary semantic guidance. The approach augments existing pipelines by decoding text from fMRI, encoding it via CLIP, and fusing this "text semantic embedding" with the original method's "high-level features" to guide the diffusion prior.

## Foundational Learning

- **Concept: Prefix Tuning/Conditioning**
  - Why needed: Maps variable-length fMRI signals to a fixed-length "prefix" that conditions a frozen Large Language Model (GPT2)
  - Quick check: How does the model handle the discrepancy between a continuous 3D brain volume and the discrete input required by a text generator?

- **Concept: Policy Gradient (REINFORCE)**
  - Why needed: Optimizes the text decoder using rewards that cannot be differentiated through standard backpropagation
  - Quick check: Why use REINFORCE instead of standard Supervised Fine-Tuning (SFT) for the text generation stage?

- **Concept: CLIP Semantic Space**
  - Why needed: Reconstruction quality is evaluated and guided by CLIP embeddings, requiring understanding of how text and images are aligned in a shared vector space
  - Quick check: What does a high CLIP score specifically indicate about the reconstruction relative to the ground truth?

## Architecture Onboarding

- **Component map:** fMRI Signal -> Linear Projection -> Transformer Prefix -> GPT2 Text Generation -> CLIP Text Encoder -> Weighted Average Fusion -> Diffusion Prior
- **Critical path:** fMRI Signal -> Linear Projection -> Transformer Prefix -> GPT2 Text Generation -> CLIP Text Encoder -> Weighted Average Fusion -> Diffusion Prior
- **Design tradeoffs:** Uses subject-specific linear layers for input but a unified Transformer/LLM core to balance personalized brain mapping with general semantic knowledge
- **Failure signatures:** Semantic Hallucination (generated text describes objects not present), Low-Level Divergence (high semantic scores but low pixel similarity)
- **First 3 experiments:** (1) Caption Validation: Verify LVLM detail-enhancement by measuring overlap between generated captions and human labels, (2) Reward Ablation: Train brain-to-text model with only one reward active at a time, (3) Pipeline Integration: Apply fused embeddings to LDM baseline first to debug semantic fusion

## Open Questions the Paper Calls Out

### Open Question 1
How can the detail enhancement stage be modified to effectively filter or correct object hallucinations produced by Large Vision-Language Models (LVLMs) to prevent semantic errors in downstream reconstruction? The paper notes that LVLMs inevitably hallucinate, producing inaccuracies that can sometimes limit the effectiveness of detail enhancement in image captions.

### Open Question 2
Can the granularity of neural signal decoding be improved beyond text-based bridging to capture high-frequency visual details that linguistic descriptions currently miss? The authors acknowledge the limitations of current fMRI data and the challenges in decoding neural signals, which inform future work to improve the granularity of signal decoding.

### Open Question 3
Does the introduction of fine-grained text embeddings create semantic conflicts or redundancy when combined with direct image embeddings in dual-branch reconstruction methods like MindEye? The paper observes that improvements were smaller for BrainDiffuser and MindEye than for LDM, hypothesizing that the combination of text and image semantic conditions makes the impact of improved text control more limited.

## Limitations
- The approach relies heavily on the quality of LVLM-generated captions, which may introduce hallucinated objects not present in original stimuli
- The effectiveness depends on the assumption that semantic details recoverable through text are also encoded in fMRI signals
- Fixed reward weights (α=β=γ=0.01) were not extensively tuned, and different weights might yield better results

## Confidence
- **High confidence**: The core mechanism of using fine-grained text descriptions as semantic targets is well-supported by ablation studies
- **Medium confidence**: The RL training approach with three reward metrics effectively aligns decoded text with visual semantics
- **Medium confidence**: The semantic fusion approach successfully augments existing reconstruction methods

## Next Checks
1. **Caption Hallucination Analysis**: Quantify overlap between LVLM-generated captions and ground truth object labels using Jaccard similarity to measure false positive rates
2. **Reward Ablation Study**: Systematically train the brain-to-text model with individual reward components active to isolate contributions
3. **Cross-Subject Generalization**: Test the fine-grained text bridging approach on subjects not included in the training data to evaluate generalizability