---
ver: rpa2
title: 'LLM Access Shield: Domain-Specific LLM Framework for Privacy Policy Compliance'
arxiv_id: '2505.17145'
source_url: https://arxiv.org/abs/2505.17145
tags:
- privacy
- sensitive
- data
- arxiv
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces LLM Access Shield, a framework for domain-specific
  privacy policy compliance in large language model interactions. The framework enforces
  policy compliance through three key innovations: LLM-based policy enforcement for
  domain-specific sensitive data detection, dynamic policy customization during user-LLM
  interactions, and format-preserving encryption for anonymizing sensitive information
  while maintaining contextual integrity.'
---

# LLM Access Shield: Domain-Specific LLM Framework for Privacy Policy Compliance

## Quick Facts
- arXiv ID: 2505.17145
- Source URL: https://arxiv.org/abs/2505.17145
- Authors: Yu Wang; Cailing Cai; Zhihua Xiao; Peifung E. Lam
- Reference count: 40
- Primary result: Domain-specific LLM framework achieves 0.935 accuracy on privacy classification vs 0.468 baseline

## Executive Summary
This paper introduces LLM Access Shield, a framework for domain-specific privacy policy compliance in large language model interactions. The framework enforces policy compliance through three key innovations: LLM-based policy enforcement for domain-specific sensitive data detection, dynamic policy customization during user-LLM interactions, and format-preserving encryption for anonymizing sensitive information while maintaining contextual integrity. The core method uses a domain-specific LLM trained via supervised and reinforcement fine-tuning with an "analyze-then-decide" reasoning paradigm to detect privacy risks. Format-preserving encryption protects sensitive entities identified by DLMS. Experimental results show DLMS models significantly outperform baseline Llama-3.2-3B-Instruct across safety classification, category detection, and privacy hiding rate.

## Method Summary
The framework uses supervised fine-tuning (SFT) on a specialized taxonomy of 6 sensitive data categories to create a high-accuracy privacy detection model, and reinforcement fine-tuning (RFT) with an "analyze-then-decide" paradigm for generalization to unseen policies. Format-preserving encryption (FPE) anonymizes detected sensitive entities while preserving prompt structure. The system operates as a proxy between users and external LLM services, intercepting and analyzing prompts, applying FPE to sensitive data, forwarding encrypted prompts to external LLMs, and decrypting responses before returning them to users.

## Key Results
- DLMS-SFT achieves 0.935 Accuracy vs 0.468 for baseline Llama-3.2-3B-Instruct on safety classification
- DLMS-SFT achieves 0.696 Subset Accuracy vs 0.303 for baseline on category detection
- DLMS-SFT achieves 0.839 Privacy Hiding Rate vs 0.064 for baseline on sensitive entity protection
- DLMS-RFT-CL achieves 0.862 Accuracy vs 0.564 for DLMS-SFT on non-taxonomy privacy policies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Supervised Fine-Tuning (SFT) on a specialized taxonomy enables the base LLM to detect and classify privacy-sensitive entities with significantly higher accuracy than an unmodified baseline.
- Mechanism: The DLMS-SFT model is fine-tuned on 2,311 curated samples labeled with a 6-category taxonomy (email, personal ID, phone, fax, bank account, monetary value). This process adjusts the model's weights to map specific textual patterns to structured outputs including safety labels, violated category codes, and exact sensitive entities.
- Core assumption: The taxonomy used for training accurately reflects the types of sensitive data the model will encounter in deployment, and high-quality labeled data can be generated for these categories.
- Evidence anchors:
  - [abstract] "The core method uses a domain-specific LLM (DLMS) trained via supervised fine-tuning... to detect privacy risks."
  - [Page 9, Table 5] "DLMS-SFT achieves 0.935 Accuracy vs 0.468 for the baseline Llama-3.2-3B-Instruct" and "Privacy Hiding Rate: 0.839 vs 0.064."
- Break condition: If the input prompt contains a type of sensitive information not defined in the training taxonomy (e.g., a new category like "biometric data"), the SFT model will likely fail to detect it (low generalization).

### Mechanism 2
- Claim: Reinforcement Fine-Tuning (RFT) with an "analyze-then-decide" paradigm improves the model's ability to generalize to unseen, non-taxonomy privacy policies without retraining.
- Mechanism: RFT uses a rule-based reward model to train a base model from scratch to generate reasoning traces within `<analyze>` tags before outputting a final decision in `<answer>` tags. The reward model scores the output based on format, safety status, category codes, and entity extraction. This process incentivizes the model to learn a general reasoning process for privacy compliance rather than pattern-matching a fixed taxonomy.
- Core assumption: The complex reasoning capability required for policy analysis can be elicited and shaped purely by the defined rule-based reward signals, and this learned skill transfers to novel policy descriptions provided in the prompt.
- Evidence anchors:
  - [Page 10, Table 7] "DLMS-RFT-CL (stage 1 only) achieves 0.862 Accuracy on non-taxonomy privacy policies vs 0.564 for DLMS-SFT (1 epoch, FPFT)."
  - [Page 6, Section 5.1] "We propose an analyze-then-decide inference framework... uses reinforcement learning (RL), without SFT, to post-train base models for reasoning capabilities."
- Break condition: If the reasoning process becomes overly complex or the reward function is poorly designed, the model may suffer from "reward hacking" or "catastrophic forgetting," degrading performance on specific sub-tasks like entity extraction.

### Mechanism 3
- Claim: Format-Preserving Encryption (FPE) enables the anonymization of detected sensitive entities while maintaining the semantic context and utility of the original prompt for the downstream LLM.
- Mechanism: When DLMS flags a prompt as "unsafe," the Sensitive Data Anonymizer (SDA) module uses an FPE algorithm (specifically FF3-1) to encrypt each sensitive entity. FPE transforms the plaintext into a ciphertext of the same format (length, character classes). This allows the encrypted prompt to be sent to an external LLM service, which can still process it semantically. The response from the LLM, which may contain the encrypted entities, is then decrypted by the SDA before being returned to the user.
- Core assumption: The downstream LLM's performance is primarily dependent on the semantic structure and format of the input, and replacing sensitive values with syntactically identical but semantically meaningless ciphertexts does not significantly degrade task performance.
- Evidence anchors:
  - [Page 10, Table 8] Shows a direct comparison where a phone number "+86 13945093743" is encrypted to "+92 43651064790", preserving its format, unlike AES which produces an opaque string.
  - [Page 10, Section 9.1] "FPE produces ciphertexts that maintain the same format—length, character classes, and delimiters—as the original input... making FPE particularly suitable for LLM-based applications."
- Break condition: The mechanism's utility preservation is challenged by the "No Free Lunch Theorem" mentioned in the paper; stronger privacy (hiding data) inherently risks reduced utility. If the downstream task requires the actual value of the data (e.g., "sum these two monetary values"), the anonymized prompt will fail.

## Foundational Learning

- Concept: Supervised Fine-Tuning (SFT)
  - Why needed here: This is the foundational technique used to create the specialized DLMS model. Understanding SFT is critical to grasp how a general-purpose Llama-3.2-3B-Instruct is transformed into a domain-specific tool for privacy detection.
  - Quick check question: Can you explain how the labeled dataset (with 'safe'/'unsafe' labels and category codes) is used to adjust the weights of the base LLM during the SFT process?

- Concept: Reinforcement Fine-Tuning (RFT) and Reward Modeling
  - Why needed here: RFT is the paper's proposed solution for generalization beyond a fixed taxonomy. It introduces a more advanced training paradigm than SFT. Grasping how the rule-based reward model guides the model's reasoning process is essential to understanding the framework's adaptability and its "analyze-then-decide" capability.
  - Quick check question: How does the rule-based reward function (R_total) combine scores for format, safety status, category codes, and entity extraction to shape the model's final output policy?

- Concept: Format-Preserving Encryption (FPE)
  - Why needed here: FPE is the key cryptographic primitive that enables the "utility-preserving anonymization" claimed by the framework. It is the final step in the protection pipeline, allowing sensitive data to be processed by external LLMs without exposure. Understanding its trade-offs is crucial for evaluating the framework's practicality.
  - Quick check question: Why is format preservation (maintaining length and character classes) critical when the encrypted prompt is sent to a general-purpose LLM like GPT-4, and how does this differ from using standard AES encryption?

## Architecture Onboarding

- Component Map:
  User Interface -> Proxy/Integrator -> DLMS (SFT/RFT) -> SDA -> External LLM Service

- Critical Path:
  1. User Prompt: "Summarize contract... contact: alice@example.com"
  2. **DLMS Analysis:** Model analyzes prompt. Output: `unsafe, T1, alice@example.com`
  3. **SDA Encryption:** Encrypt entity `alice@example.com`. Encrypted Prompt: "Summarize contract... contact: Xb2@Kp9.net"
  4. **External LLM:** Processes encrypted prompt, generates response.
  5. **SDA Decryption:** Detects and decrypts `Xb2@Kp9.net` back to `alice@example.com` in the final response shown to the user.

- Design Tradeoffs:
  - **SFT vs. RFT:** SFT offers higher accuracy (0.935 vs 0.910) and entity hiding (0.839 vs 0.555) on *known* taxonomies but is brittle to new policies. RFT with "analyze-then-decide" offers superior generalization to *unseen* policies (0.862 vs 0.564 accuracy) but is more complex to train and tune.
  - **Privacy vs. Utility:** The FPE mechanism trades absolute security (since format is preserved) for maintained utility in downstream LLM tasks. Stronger encryption (like AES) would destroy utility.
  - **Rule-Based Reward Tuning:** A conservative reward design (heavy penalties for overprediction) reduces false positives but increases false negatives, leading to a lower Privacy Hiding Rate.

- Failure Signatures:
  - Low Generalization (SFT Model): A prompt with a sensitive entity not in the 6-category taxonomy (e.g., a passport number) is classified as "safe."
  - Catastrophic Forgetting (RFT Model): The model performs well on complex reasoning but degrades in basic safety classification as training progresses through curriculum learning stages.
  - Utility Loss: The downstream LLM produces a nonsensical or failed response because the FPE-encrypted entities broke its semantic understanding, despite format preservation.

- First 3 Experiments:
  1. **Baseline SFT Evaluation:** Fine-tune the base Llama-3.2-3B-Instruct model on the provided 2,311-sample dataset using the SFT recipe. Evaluate its accuracy and Privacy Hiding Rate on the 1,542-sample test set to verify the core claim (Accuracy ≈ 0.935).
  2. **RFT Generalization Test:** Take the trained RFT model and evaluate it on the four non-taxonomy privacy policies (POL01-POL04) using few-shot prompting. Compare its accuracy to the SFT model to quantify the generalization benefit (target: Accuracy ≈ 0.86).
  3. **End-to-End FPE Utility Test:** Construct 20 test prompts with sensitive entities. Run them through the full DLMS + SDA pipeline. Send the encrypted prompts to a live LLM (e.g., GPT-4o mini) and manually grade the quality of the responses to confirm that task utility is preserved.

## Open Questions the Paper Calls Out

- How can response-filtering mechanisms be developed to prevent unauthorized disclosure of sensitive internal information when private LLMs access privileged organizational knowledge?
- How do Process Reward Models (PRMs) compare to rule-based reward approaches for enforcing stepwise compliance in complex privacy scenarios?
- Can a fine-grained reward function be designed to mitigate conservative over-cautiousness and improve recall in entity-level extraction tasks?

## Limitations
- The framework relies on synthetic data generation for training, with unspecified templates and LLM prompts for creating the 2,311 training samples
- FPE implementation details (key generation, tweak values, radix settings per entity type) are not fully specified
- End-to-end utility preservation in real-world LLM interactions is only partially demonstrated through controlled examples

## Confidence
- **High Confidence:** The SFT model's superior performance on taxonomy-based tasks (Accuracy: 0.935 vs 0.468) is well-supported by experimental results
- **Medium Confidence:** The RFT model's generalization claims (Accuracy: 0.862 vs 0.564 on non-taxonomy policies) are supported but depend on assumptions about reward function effectiveness
- **Low Confidence:** The end-to-end utility preservation in real-world LLM interactions is only partially demonstrated through controlled examples

## Next Checks
1. **Data Generation Validation:** Replicate the synthetic dataset creation process using the described methodology and verify that the generated samples accurately represent the 6 sensitive data categories
2. **RFT Reward Function Tuning:** Conduct ablation studies on the rule-based reward components to quantify their individual contributions and identify potential reward hacking scenarios
3. **Live LLM Integration Test:** Deploy the full pipeline (DLMS + SDA + external LLM) on a live LLM API with diverse, real-world prompts to measure actual utility preservation and identify failure modes not captured in controlled experiments