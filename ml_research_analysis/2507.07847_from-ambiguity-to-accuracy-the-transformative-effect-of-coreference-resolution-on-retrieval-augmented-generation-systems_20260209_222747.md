---
ver: rpa2
title: 'From Ambiguity to Accuracy: The Transformative Effect of Coreference Resolution
  on Retrieval-Augmented Generation systems'
arxiv_id: '2507.07847'
source_url: https://arxiv.org/abs/2507.07847
tags:
- coreference
- resolution
- retrieval
- document
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examines how coreference resolution impacts retrieval-augmented
  generation (RAG) systems, focusing on document retrieval and question-answering
  performance. By replacing ambiguous pronouns with explicit antecedents, coreference
  resolution improves semantic clarity in document representations.
---

# From Ambiguity to Accuracy: The Transformative Effect of Coreference Resolution on Retrieval-Augmented Generation systems

## Quick Facts
- **arXiv ID**: 2507.07847
- **Source URL**: https://arxiv.org/abs/2507.07847
- **Reference count**: 9
- **Key result**: Coreference resolution improves RAG retrieval accuracy (nDCG@1 gains up to 0.012) and helps smaller LLMs achieve performance comparable to larger models

## Executive Summary
This study examines how coreference resolution impacts retrieval-augmented generation (RAG) systems, focusing on document retrieval and question-answering performance. By replacing ambiguous pronouns with explicit antecedents, coreference resolution improves semantic clarity in document representations. Experiments across four datasets show that embedding models using mean pooling demonstrate the most significant retrieval improvements after coreference resolution, with gains of 0.012 in nDCG@1 scores. For question answering tasks, smaller language models benefit more from coreference resolution, with some achieving performance comparable to larger models when given resolved documents.

## Method Summary
The researchers apply LLM-based coreference resolution (using GPT-4o-mini or Qwen2.5-7B-Instruct) to replace pronouns with explicit antecedents in documents before embedding and retrieval. They evaluate across four datasets (BELEBELE, SQuAD2.0, BoolQ, NanoSCIDOCS) using various embedding models with different pooling strategies and QA models of different sizes. The pipeline compares performance between original and coreference-resolved documents on retrieval metrics (nDCG@k) and QA metrics (log likelihood accuracy, F1-score).

## Key Results
- Mean pooling embedding models show the strongest retrieval improvements after coreference resolution (0.012 nDCG@1 gains for LLM2Vec)
- Smaller language models benefit more from resolved documents than larger models in QA tasks
- Qwen2.5-7B-Instruct provides a 77% cost reduction versus GPT-4o-mini with only modest quality trade-offs
- Coreference resolution increases document length by approximately 4.8% (average pronoun count reduced from 2.7 to 1.4 per document)

## Why This Works (Mechanism)

### Mechanism 1: Semantic Explicitness Improves Embedding Alignment
Coreference resolution transforms documents by replacing pronouns with explicit antecedents, increasing token-level semantic overlap between queries and documents. This improves embedding alignment because models no longer need to infer hidden entity relationships. The resolved document contains more explicit lexical markers that align with query terms.

### Mechanism 2: Mean Pooling Amplifies Token-Level Semantic Gains
Mean pooling aggregates embeddings across all tokens with equal weight. When pronouns (semantically weak) are replaced with full noun phrases (semantically rich), the averaged document embedding shifts more substantially toward the true semantic content. [CLS] and last-token pooling compress information into a single token representation, which may already capture global semantics but is less sensitive to token-level improvements.

### Mechanism 3: Reduced Ambiguity Compensates for Limited Model Capacity
Smaller models have reduced inherent capacity for resolving referential ambiguity during in-context learning. Explicit antecedents reduce the inference burden, allowing limited parameter space to focus on reasoning rather than reference tracking. Larger models already possess sufficient capacity to handle ambiguity internally, yielding diminishing returns.

## Foundational Learning

- **Coreference Resolution**
  - *Why needed*: The paper's central intervention requires understanding what coreference resolution does—identifying expressions referring to the same entity and replacing pronouns with explicit nouns
  - *Quick check*: Given "Alice ran because she was late," what is the coreference resolution output?

- **Pooling Strategies in Embedding Models**
  - *Why needed*: Retrieval improvements depend critically on pooling method. Mean pooling averages all token embeddings; [CLS]/last-token pooling uses a single representative token
  - *Quick check*: Why would mean pooling be more sensitive to replacing a pronoun with a noun phrase?

- **In-Context Learning Limits in Smaller LLMs**
  - *Why needed*: The differential benefit for smaller models hinges on their reduced capacity to track references across context windows
  - *Quick check*: If a 3B model and a 70B model read the same ambiguous document, which one benefits more from pre-resolved coreferences, and why?

## Architecture Onboarding

- **Component map**: Coreference Resolution Module -> Embedding Model (with pooling) -> Retrieval System -> Generator LLM
- **Critical path**: Pre-process corpus documents through coreference resolution before embedding; apply mean pooling embedding models for maximum retrieval gain; route retrieved resolved documents to smaller generator models for largest QA uplift
- **Design tradeoffs**:
  - Resolution quality vs. cost: GPT-4o-mini provides higher-quality resolution but at API cost; Qwen2.5-7B-Instruct offers a 77% cheaper alternative
  - Document length vs. retrieval precision: Resolution increases word count (Table 9: +4.8% for Belebele), potentially affecting chunking strategies
  - Generative flexibility vs. clarity: Explicit references may constrain natural-sounding outputs
- **Failure signatures**:
  - Retrieval gains absent: Check pooling strategy—if using [CLS] or last-token, gains will be minimal
  - QA gains absent: Verify model size; larger models (>8B) show smaller improvements
  - Degraded performance: Coreference resolution errors propagating; audit resolution quality on sample
- **First 3 experiments**:
  1. Baseline retrieval comparison: Run retrieval with original vs. resolved documents on a held-out subset using mean pooling model; measure nDCG@1 difference
  2. Pooling sensitivity test: Compare mean pooling vs. [CLS] pooling on the same resolved corpus; quantify gap in retrieval improvement
  3. Model size QA experiment: Run QA task with resolved documents on a 3B and 7B model pair; compute relative performance gain differential

## Open Questions the Paper Calls Out

**Open Question 1**: What is the optimal trade-off between coreferential clarity (via resolution) and the generative flexibility or naturalness of language model outputs in RAG systems? The authors note that explicit references can constrain generative flexibility and call for balancing clarity with generative versatility.

**Open Question 2**: How robust are the observed coreference resolution benefits across specialized or highly technical domains (e.g., legal, medical, scientific texts)? The paper acknowledges that the approach may not fully capture complexities of specialized or highly technical text.

**Open Question 3**: To what extent do biases introduced by GPT-4o-mini-based coreference resolution affect downstream RAG fairness and reliability? The Ethics Statement acknowledges potential bias amplification and calls for regular audits of training data.

## Limitations

- **Resolution Quality Dependence**: The study relies entirely on LLM-based coreference resolution, creating a potential single point of failure where improvements may be artifacts of resolution quality rather than coreference elimination per se.
- **Pooling Strategy Specificity**: Findings may be pooling-dependent rather than universally applicable across embedding architectures, as mean pooling shows the strongest improvements.
- **Model Size Effects Ambiguity**: The claim that smaller models benefit more conflates model capacity with resolution quality, making the underlying mechanism unclear.

## Confidence

- **High Confidence**: Retrieval improvements from coreference resolution are real and measurable (nDCG@1 gains of 0.012). The mechanism of semantic explicitness improving embedding alignment is well-supported.
- **Medium Confidence**: Mean pooling shows stronger gains than other pooling strategies, but the underlying mechanism explanation is incomplete. The model size differential effect is observed but not fully explained.
- **Low Confidence**: The assertion that coreference resolution universally improves RAG systems across all architectures is overstated, given dependencies on specific pooling strategies and resolution model quality.

## Next Checks

1. **Pooling Strategy Robustness Test**: Run the complete experiment pipeline using mean pooling, [CLS] pooling, and last-token pooling on the same resolved corpus. Quantify the gap in retrieval improvement and test whether the effect holds across different embedding models.

2. **Resolution Quality Control**: Implement a human-annotated subset of resolved documents to measure resolution accuracy. Compare retrieval/QA performance against both original documents and LLM-resolved documents to isolate the effect of resolution quality versus coreference elimination.

3. **Chunking Strategy Impact**: Since coreference resolution increases document length (4.8% in Belebele), test whether the observed improvements persist when documents are chunked into standard sizes (e.g., 512 tokens). This would validate whether the benefits survive practical RAG deployment constraints.