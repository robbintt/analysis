---
ver: rpa2
title: Training-Free Synthetic Data Generation with Dual IP-Adapter Guidance
arxiv_id: '2509.22635'
source_url: https://arxiv.org/abs/2509.22635
tags:
- dipsy
- image
- images
- guidance
- diffusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DIPSY, a training-free approach for few-shot
  image classification that leverages dual IP-Adapter guidance for synthetic data
  generation. Unlike existing methods that require model fine-tuning or external captioning,
  DIPSY generates discriminative synthetic images using only few-shot examples through
  a novel extension of classifier-free guidance that independently controls positive
  and negative image conditioning.
---

# Training-Free Synthetic Data Generation with Dual IP-Adapter Guidance

## Quick Facts
- **arXiv ID**: 2509.22635
- **Source URL**: https://arxiv.org/abs/2509.22635
- **Reference count**: 40
- **Primary result**: DIPSY achieves state-of-the-art or comparable performance on ten benchmark datasets for few-shot image classification without requiring model fine-tuning or external captioning tools.

## Executive Summary
This paper introduces DIPSY, a training-free approach for few-shot image classification that generates synthetic data using dual IP-Adapter guidance. Unlike existing methods that require model fine-tuning or external captioning, DIPSY creates discriminative synthetic images using only few-shot examples through an extended classifier-free guidance mechanism. The method employs a class similarity-based sampling strategy to select effective negative prompts. Experiments across ten benchmark datasets show DIPSY achieves state-of-the-art or comparable performance to methods requiring training or external tools, particularly excelling at fine-grained classification tasks.

## Method Summary
DIPSY generates synthetic data for few-shot image classification using Stable Diffusion 1.5 with IP-Adapter and an extended classifier-free guidance mechanism. The method combines text conditioning with positive and negative image conditioning through dual IP-Adapter guidance, allowing independent control of both positive and negative image influences. Negative prompts are sampled from classes similar to the target class using CLIP similarity. The approach generates 200 synthetic images per class with 50 denoising steps, then fine-tunes a CLIP ViT-B/16 classifier using LoRA with a weighted loss combining real and synthetic data. The entire pipeline operates without training the generative model or requiring external captioning tools.

## Key Results
- Achieves state-of-the-art or comparable performance on ten benchmark datasets for few-shot image classification
- Outperforms methods requiring training or external tools, particularly on fine-grained classification tasks
- Eliminates the need for external captioning tools or generative model adaptation while maintaining competitive accuracy

## Why This Works (Mechanism)
DIPSY works by extending classifier-free guidance to independently control positive and negative image conditioning in the diffusion process. By using dual IP-Adapter guidance, the method can precisely control both what features to emphasize (positive conditioning) and what features to suppress (negative conditioning) during generation. The negative sampling strategy, which selects prompts from classes similar to the target class, ensures that the generated images are discriminative while remaining semantically related to the target class. This approach allows DIPSY to generate high-quality synthetic data without requiring external captioning or generative model fine-tuning.

## Foundational Learning
- **Dual IP-Adapter Guidance**: Independent control of positive and negative image conditioning in diffusion models. Why needed: Enables precise feature control during generation without external tools. Quick check: Verify that generated images show both target class features and suppressed negative class features.
- **Classifier-Free Guidance Extension**: Mathematical framework for combining multiple conditioning signals in diffusion models. Why needed: Provides the theoretical foundation for dual guidance. Quick check: Confirm that guidance equation correctly implements the weighted combination of all conditioning signals.
- **CLIP Similarity-Based Sampling**: Using CLIP features to identify semantically similar classes for negative prompt selection. Why needed: Ensures negative prompts are relevant but distinct from target classes. Quick check: Verify that sampled negative classes have high similarity but belong to different ground truth categories.

## Architecture Onboarding

**Component Map**: Few-shot images → CLIP feature extraction → Similarity matrix → Negative sampling → Dual IP-Adapter guidance → Synthetic image generation → LoRA fine-tuning → Classification

**Critical Path**: The most time-consuming operations are the 4 U-Net forward passes per denoising step (unconditional, text-only, text+pos_image, text+pos+neg_image) and the CLIP feature extraction for similarity computation.

**Design Tradeoffs**: 
- **Dual Guidance vs. Single Guidance**: Dual guidance provides better control but requires 4x more computation per step
- **Similarity-Based vs. Random Negative Sampling**: Similarity-based sampling ensures relevance but requires upfront CLIP feature computation
- **Training-Free vs. Fine-Tuning**: Avoids generative model adaptation but requires classifier fine-tuning for optimal performance

**Failure Signatures**:
- Poor discriminative quality: Generated images resemble negative prompt classes too much (reduce $w_{im-}$ or check sign)
- High computational cost: Generation takes approximately 4x longer than standard SD inference
- Low accuracy: May indicate improper guidance weight tuning or insufficient negative sampling diversity

**Three First Experiments**:
1. Implement dual guidance noise prediction with 4 U-Net passes per step and verify computational overhead
2. Test negative sampling pipeline with CLIP ViT-B/16 on a small dataset to ensure proper similarity-based selection
3. Generate synthetic images with varying guidance weights to find optimal balance between positive and negative conditioning

## Open Questions the Paper Calls Out
None

## Limitations
- Computational overhead of dual guidance (4 U-Net passes per denoising step) is significant but not empirically characterized
- Classifier fine-tuning is still required, making the "training-free" claim somewhat misleading
- Guidance weights and classifier fine-tuning hyperparameters are tuned per dataset but not published, hindering exact reproduction

## Confidence
- **High confidence**: The dual IP-Adapter guidance mechanism is technically sound and well-described mathematically
- **Medium confidence**: Performance comparisons are presented with proper statistical analysis, but lack of published hyperparameter values makes exact reproduction difficult
- **Low confidence**: Computational efficiency claims are difficult to assess without knowing actual guidance weight values and their impact on generation time

## Next Checks
1. Implement the dual guidance noise prediction equation and verify it requires exactly 4 U-Net forward passes per denoising step
2. Reproduce the negative sampling pipeline using CLIP ViT-B/16 similarity scores to select negative prompts from similar classes
3. Generate synthetic images using the described pipeline with 50 denoising steps and 200 images per class, then fine-tune CLIP ViT-B/16 with LoRA (rank 16) to verify classification accuracy claims