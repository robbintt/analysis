---
ver: rpa2
title: 'CliniBench: A Clinical Outcome Prediction Benchmark for Generative and Encoder-Based
  Language Models'
arxiv_id: '2509.26136'
source_url: https://arxiv.org/abs/2509.26136
tags:
- generative
- performance
- zero-shot
- clinical
- codes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces CliniBench, the first benchmark for comparing
  encoder-based classifiers and generative LLMs on discharge diagnosis prediction
  from admission notes in MIMIC-IV. It evaluates 12 generative LLMs and 3 encoder-based
  classifiers, demonstrating that encoder-based classifiers consistently outperform
  generative models.
---

# CliniBench: A Clinical Outcome Prediction Benchmark for Generative and Encoder-Based Language Models

## Quick Facts
- arXiv ID: 2509.26136
- Source URL: https://arxiv.org/abs/2509.26136
- Reference count: 40
- Primary result: Encoder-based classifiers achieve 80%+ main diagnosis accuracy vs 50-60% for generative LLMs on MIMIC-IV discharge prediction

## Executive Summary
CliniBench is the first benchmark comparing encoder-based classifiers and generative language models for predicting discharge diagnoses from hospital admission notes in MIMIC-IV. The study evaluates 12 generative LLMs and 3 encoder-based classifiers, demonstrating that fine-tuned encoders consistently outperform zero-shot generative models in diagnosis prediction accuracy. The benchmark introduces retrieval augmentation and chain-of-thought prompting strategies, showing that these techniques notably improve generative model performance, especially for rare diagnoses. While generative models offer flexibility through zero-shot inference and dynamic label spaces, encoder-based models remain superior for diagnosis prediction accuracy, highlighting the need for further improvements in retrieval strategies and domain-specific adaptations.

## Method Summary
The benchmark uses MIMIC-IV hospital admission notes with stratified 70/10/20 splits across ICD-9/ICD-10 × HOSP/ICU partitions. Encoder models (BioMedBERT-110M, GatorTronS-345M, M2-BERT-80M) are fine-tuned with Bayesian hyperparameter optimization and class-wise threshold tuning. Generative models (12 variants including Llama 3.1, Mistral, Qwen2) use guided decoding for JSON schema compliance and NeuML-PubMedBERT for text-to-ICD mapping. The study evaluates zero-shot, few-shot (1-5 demonstrations), and chain-of-thought prompting strategies with BM25 or fine-tuned retriever for similar patient examples.

## Key Results
- Encoder-based classifiers achieve >80% main diagnosis accuracy versus 50-60% for generative LLMs
- Retrieval augmentation with similar patient examples improves generative LLM performance, particularly for rare diagnoses
- Instruction tuning boosts generative model performance more than increasing model size, with non-instruction fine-tuned models failing to generate valid JSON

## Why This Works (Mechanism)

### Mechanism 1
Encoder-based classifiers achieve higher diagnosis accuracy than zero-shot generative LLMs by optimizing directly on a fixed label space. Fine-tuned encoders learn a deterministic mapping from clinical tokens to a static set of ICD codes without suffering from hallucinations or output formatting errors. Performance degrades significantly if input sequence length exceeds the model's context window (typically 512 tokens).

### Mechanism 2
Retrieval augmentation bridges the performance gap for generative LLMs by grounding predictions in similar clinical cases. Providing the LLM with retrieved examples of similar patients allows in-context learning, conditioning the generative process on specific patterns found in training data. Performance degrades if retrieval quality is poor, with random sampling causing distractive demonstrations.

### Mechanism 3
Instruction tuning is more effective than parameter scaling for ensuring output schema compliance in generative models. Instruction-tuned models are optimized to follow JSON formatting constraints, whereas base models often fail to produce parsable structures or default to generating unrelated text. Even with instruction tuning, generative models may produce low variance outputs, limiting recall.

## Foundational Learning

- **Concept: Extreme Multi-Label Classification (XMLC)**
  - Why needed: Diagnosis prediction involves assigning multiple ICD codes from thousands to a single admission note
  - Quick check: Why does the benchmark use "Fixed-size top-n" evaluation (n=20) rather than raw accuracy? (Answer: To standardize comparison between probability-ranking encoders and list-generating LLMs)

- **Concept: In-Context Learning (ICL) vs. Fine-Tuning**
  - Why needed: Understanding trade-off between updating model weights (Encoders/Fine-tuning) vs. updating prompt context (LLMs/ICL)
  - Quick check: Why might ICL be preferred in privacy-constrained hospital environments? (Answer: It avoids sharing data for weight updates, allowing use of pre-trained models with local retrieval)

- **Concept: Hallucination vs. Unrelated Output**
  - Why needed: Distinguishing between factually incorrect medical claims and outputs that are valid text but wrong for the specific task
  - Quick check: Why is "unrelated text" a specific failure mode for non-instruction-tuned LLMs in this benchmark?

## Architecture Onboarding

- **Component map:** MIMIC-IV admission notes → ICD codes → Data split → Encoder pipeline (fine-tune → threshold tuning → top-20 prediction) OR Generative pipeline (prompt engineering → guided decoding → text-to-ICD mapping)
- **Critical path:** The Text-to-Class Mapping component for generative models, as LLMs generate text descriptions rather than codes
- **Design tradeoffs:** Encoders offer high accuracy (80%+) and low inference cost but fail on long contexts (>850 tokens) and require retraining for new codes; Generative models have lower accuracy (50-60%) and high inference cost but handle long contexts and dynamic label spaces via RAG
- **Failure signatures:** Low variance outputs (repetitive synonyms), schema drift (invalid JSON), context overflow (encoder performance drop on long sequences)
- **First 3 experiments:** 1) Baseline comparison: Qwen2-72B-Instruct zero-shot vs. fine-tuned BioMedBERT on HOSP split (target: ~30% accuracy gap); 2) Retrieval ablation: BM25 retriever feeding 5 demonstrations to LLM, verify MAP improvement for rare codes; 3) Context length stress test: Feed notes of varying lengths (200, 600, 1000 tokens) into both architectures to confirm encoder cliff and LLM stability

## Open Questions the Paper Calls Out

- **Open Question 1:** Can incorporating multi-modal clinical data (e.g., laboratory results, imaging) into retrieval strategies improve few-shot diagnosis prediction performance? The paper suggests future work should augment retrieval with additional clinical information beyond admission notes, but current retrieval relies solely on text-based admission notes.

- **Open Question 2:** Does domain-specific adaptation using clinical guidelines or differential diagnosis datasets enable effective Chain-of-Thought reasoning for LLMs? Zero-shot CoT currently degrades performance, indicating general reasoning capabilities don't transfer directly to clinical logic.

- **Open Question 3:** To what extent can LLM-based verification mechanisms or probabilistic sampling strategies mitigate low-variance outputs in clinical text generation? The paper proposes adding verification mechanisms to filter predictions and reduce duplicates, but this remains untested.

## Limitations
- Benchmark focuses exclusively on MIMIC-IV data, limiting external validity to other healthcare systems
- Encoder context window constraint (512 tokens) excludes longer clinical narratives with potential diagnostic information
- Retrieval augmentation's computational overhead and potential biases in the retrieval system are not fully addressed
- Text-to-ICD mapping component for LLMs introduces additional transformation layers that may compound prediction errors

## Confidence

**High Confidence:** Encoder-based classifiers consistently outperform generative models in diagnosis prediction accuracy (Sections 5.1, Abstract). The performance gap is well-documented with specific metrics and supported by clear theoretical reasoning about fixed label spaces versus generative output.

**Medium Confidence:** Retrieval augmentation and chain-of-thought prompting meaningfully improve generative LLM performance, particularly for rare diagnoses (Sections 5.3, Abstract). While improvements are statistically significant, the magnitude varies considerably across models and demonstration counts.

**Low Confidence:** Instruction tuning is more effective than parameter scaling for ensuring output schema compliance (Section 5.1). The study provides compelling evidence of format compliance differences but lacks head-to-head comparisons of instruction-tuned versus scaled models beyond JSON generation.

## Next Checks

1. **External Validation:** Evaluate the same encoder and generative models on a non-MIMIC dataset (e.g., eICU or another healthcare system's discharge data) to assess generalizability and identify domain-specific failure modes.

2. **Retrieval Quality Impact:** Systematically vary the quality of retrieved demonstrations (high-similarity vs. random vs. adversarial) to quantify the upper bound of retrieval augmentation benefits and identify when poor retrieval actively harms LLM performance.

3. **Clinical Utility Assessment:** Design a study where domain experts evaluate the clinical usefulness of top-20 predictions from both architectures, distinguishing between technically correct but clinically irrelevant predictions and genuinely actionable diagnostic suggestions.