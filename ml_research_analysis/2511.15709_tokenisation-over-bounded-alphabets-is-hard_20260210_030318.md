---
ver: rpa2
title: Tokenisation over Bounded Alphabets is Hard
arxiv_id: '2511.15709'
source_url: https://arxiv.org/abs/2511.15709
tags:
- which
- tokenisation
- tokeniser
- problem
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Tokenisation is the process of splitting character strings into
  subwords, a critical step in natural language processing pipelines. While prior
  work has shown that finding optimal tokenisers is NP-complete, these results assumed
  unbounded alphabets, which is unrealistic in practice.
---

# Tokenisation over Bounded Alphabets is Hard

## Quick Facts
- arXiv ID: 2511.15709
- Source URL: https://arxiv.org/abs/2511.15709
- Reference count: 40
- Primary result: Both direct and bottom-up tokenisation variants remain NP-complete and admit no PTAS even over binary and unary alphabets

## Executive Summary
This paper proves that tokenisation is computationally intractable even when severely restricted to small alphabets. While prior work showed NP-completeness for unbounded alphabets, this paper demonstrates that the hardness persists for binary and even unary alphabets, making it a fundamental computational barrier rather than an artifact of large character sets. The authors establish that tokenisation variants - both selecting merge operations (bottom-up) and selecting vocabularies (direct) - cannot be approximated efficiently (no PTAS unless P=NP) for binary alphabets, and remain NP-complete for unary alphabets. These results definitively show that popular heuristic algorithms like BPE and UnigramLM are necessary rather than incidental solutions.

## Method Summary
The authors use polynomial-time reductions from well-established NP-complete problems to prove computational hardness. For binary alphabets, they reduce from 3-OCC-MAX2SAT (a variant with bounded clause occurrence) to show both NP-completeness and approximation hardness for direct tokenisation, then extend this to bottom-up tokenisation. For unary alphabets, they reduce from vertex cover to prove NP-completeness of direct tokenisation. The binary reduction uses carefully constructed binary strings encoding truth assignments, while the unary reduction uses unique string-length encodings based on polynomial functions. The approximation hardness proof relies on gap-preserving reductions from Berman & Karpinski's results on 3-OCC-MAX2SAT with specific approximation bounds.

## Key Results
- Tokenisation over binary alphabets is NP-complete and admits no PTAS unless P=NP
- Tokenisation over unary alphabets is NP-complete for direct tokenisation
- Hardness results hold for both bottom-up (merge sequence selection) and direct (vocabulary selection) variants
- The computational intractability is fundamental, not an artifact of large alphabets
- Standard heuristic algorithms like BPE and UnigramLM are provably necessary

## Why This Works (Mechanism)
The hardness proofs work by encoding combinatorial optimization problems into tokenisation instances such that optimal tokenisation solutions directly correspond to solutions of the original problems. For binary alphabets, the reduction encodes truth assignments using distinct binary strings of specific lengths, where merge operations correspond to satisfying clause constraints. The gap-preserving reduction amplifies the difficulty by creating instances where distinguishing good from optimal solutions requires solving the underlying NP-complete problem. For unary alphabets, the reduction encodes graph structures through unique string lengths, where vocabulary selection corresponds to vertex cover selection. The key insight is that even with severely restricted alphabets, the combinatorial explosion of possible tokenisations preserves the computational complexity of the underlying problems.

## Foundational Learning

**NP-completeness** - The class of decision problems that are both in NP and NP-hard; needed because the paper proves tokenisation variants are NP-complete, establishing they're at least as hard as any problem in NP. Quick check: Can the problem's solutions be verified in polynomial time and can any NP problem be reduced to it?

**Gap-preserving reduction** - A reduction that maintains approximation ratios between problems; needed to prove no PTAS exists by showing that a PTAS for tokenisation would imply a PTAS for 3-OCC-MAX2SAT. Quick check: Does the reduction preserve the gap between optimal and approximate solutions within constant factors?

**Vertex cover** - An NP-complete problem where the goal is to find the smallest set of vertices that touch all edges in a graph; needed as the source problem for proving unary tokenisation hardness. Quick check: Can you verify that every edge in the graph has at least one endpoint in the selected set?

**3-OCC-MAX2SAT** - A SAT variant where each variable appears in at most 3 clauses and the goal is to maximize satisfied clauses; needed because its known approximation hardness provides the gap bounds for binary tokenisation. Quick check: Are all clauses 2-literal clauses and does each variable appear in ≤3 clauses?

**String-length encoding** - A technique using unique string lengths to represent combinatorial structures; needed in the unary reduction to encode graphs and ensure unique token combinations. Quick check: Do all generated string lengths remain distinct and avoid accidental two-token matches?

## Architecture Onboarding

**Component Map**
3-OCC-MAX2SAT/Vertex Cover -> Reduction Construction -> Tokenisation Instance -> Compression Objective

**Critical Path**
Problem instance → Reduction mapping → String generation → Objective function evaluation → Hardness proof verification

**Design Tradeoffs**
The reduction design trades off alphabet size constraints against problem complexity preservation. Binary reductions use exponential string length encoding (0²ʲ vs 0²ʲ⁻¹) to create distinct merge patterns, while unary reductions use polynomial string length encoding (j+j²N+j³N²) to ensure uniqueness. These choices enable hardness preservation while respecting alphabet constraints.

**Failure Signatures**
- Incorrect string length uniqueness causing accidental token matches
- Insufficient gap amplification failing to prove approximation hardness
- Missing constraint satisfaction in backward direction of reductions
- Overflow in polynomial string length calculations for large instances

**First Experiments**
1. Implement binary reduction R1 with small 3-OCC-MAX2SAT instances (n=3, I=6048) and verify forward/backward correctness conditions
2. Test unary reduction R3 with small vertex cover instances and verify all string lengths remain unique
3. Compute compression objective values for both satisfiable and unsatisfiable SAT instances to confirm objective gap

## Open Questions the Paper Calls Out

**Open Question 1**: Does there exist any polynomial-time constant-factor approximation algorithm for binary tokenisation optimization? The authors prove no PTAS exists but suggest constant-factor approximation remains possible.

**Open Question 2**: Is the decision problem for standard unary bottom-up tokenisation NP-hard? The paper proves hardness for direct tokenisation and unary OPE but leaves standard bottom-up unary unanalyzed.

**Open Question 3**: Is the unary optimal pair encoding (OPE) decision problem strongly NP-complete? The authors only prove weak NP-hardness, relying on binary-encoded numbers rather than unary representation.

**Open Question 4**: Are other tokenisation objectives, such as likelihood maximization, NP-hard over bounded alphabets? The paper focuses on compression objectives, leaving other metrics unexplored.

## Limitations

- Gap-preserving reduction parameters depend on external Berman & Karpinski papers (1998, 1999) not fully restated in this work
- Unary direct tokenisation reduction requires careful verification of string-length uniqueness to prevent accidental two-token matches
- The proof sketches in the main text may omit subtle technical details requiring verification against complete appendix proofs
- No hardness result established for standard unary bottom-up tokenisation variant

## Confidence

- **High confidence** in NP-completeness results for both binary and unary alphabets across both tokenisation variants
- **Medium confidence** in approximation hardness results (no PTAS) for binary direct tokenisation, dependent on external gap parameters
- **Medium confidence** in technical details of unary direct tokenisation reduction, particularly regarding unique string-length encodings

## Next Checks

1. Verify the gap-preserving reduction parameters by locating and checking the exact bounds from Berman & Karpinski (1998, 1999) papers to confirm the claimed approximation ratio of at least 1.000002 for binary direct tokenisation.

2. Implement and test the unary direct tokenisation reduction with concrete values for J, I, and N to verify that all generated string lengths are unique and that no two-token combinations accidentally equal another target string length.

3. Cross-validate the forward and backward correctness conditions for the binary direct tokenisation reduction by constructing small test instances of 3-OCC-MAX2SAT (with 2016n clauses as specified) and verifying that the reduction correctly captures satisfiability through the tokenisation objective values.