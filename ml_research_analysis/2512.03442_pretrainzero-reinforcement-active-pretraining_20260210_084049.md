---
ver: rpa2
title: 'PretrainZero: Reinforcement Active Pretraining'
arxiv_id: '2512.03442'
source_url: https://arxiv.org/abs/2512.03442
tags:
- pretraining
- learning
- rlpt
- arxiv
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PretrainZero, the first reinforcement learning
  pre-training method designed to operate directly on general pretraining corpora
  like Wikipedia, extending reinforcement learning from domain-specific post-training
  to the pretraining stage. Unlike prior approaches that rely on synthetic datasets
  or question-answer pairs, PretrainZero learns a unified reasoning policy to actively
  identify informative and verifiable content within the pretraining corpus through
  a novel reinforcement active learning framework.
---

# PretrainZero: Reinforcement Active Pretraining

## Quick Facts
- arXiv ID: 2512.03442
- Source URL: https://arxiv.org/abs/2512.03442
- Reference count: 12
- PretrainZero improves MMLU-Pro by 8.43, SuperGPQA by 5.96, and math average benchmarks by 10.60 during reinforcement pretraining

## Executive Summary
PretrainZero introduces the first reinforcement learning pre-training method that operates directly on general pretraining corpora like Wikipedia, extending RL from post-training to pretraining stage. The method learns a unified reasoning policy to actively identify informative and verifiable content within the corpus through a novel reinforcement active learning framework. Unlike prior approaches requiring synthetic datasets or question-answer pairs, PretrainZero works effectively on noisy real-world data without requiring verifiable labels, pretrained reward models, or supervised fine-tuning.

The pretrained models serve as reasoning foundation models, maintaining improvements (2.35, 3.04, and 2.81) after general RLVR post-training, demonstrating that active reinforcement pretraining effectively addresses the verification data-wall in general reasoning tasks. Evaluated on Qwen3-4B-Base, PretrainZero achieves substantial improvements across multiple reasoning and math benchmarks while establishing a new paradigm for integrating reinforcement learning into the pretraining pipeline.

## Method Summary
PretrainZero introduces a reinforcement active pretraining framework that learns to identify informative and verifiable content within general pretraining corpora. The method employs a novel on-policy mask generation task as an auxiliary objective, allowing the model to anticipate what information should be learned actively. This is coupled with masked-span prediction tasks optimized via a min-max bilevel reinforcement learning objective. The framework enables effective training under low-information-density, noisy real-world data without requiring verifiable labels, pretrained reward models, or supervised fine-tuning. The approach learns a unified reasoning policy that can be transferred to post-training reinforcement learning, where it maintains significant performance improvements.

## Key Results
- Improves MMLU-Pro by 8.43 points during reinforcement pretraining
- Improves SuperGPQA by 5.96 points during reinforcement pretraining
- Improves math average benchmarks by 10.60 points during reinforcement pretraining
- Maintains improvements (2.35, 3.04, and 2.81) after general RLVR post-training

## Why This Works (Mechanism)
The method works by introducing active content selection during pretraining, where the model learns to identify and prioritize verifiable, information-dense passages within the pretraining corpus. The on-policy mask generation task serves as an auxiliary objective that trains the model to anticipate which information will be most valuable to learn, creating a self-supervised signal for active learning. The min-max bilevel reinforcement learning objective optimizes both the policy for content selection and the masked-span prediction tasks simultaneously, enabling the model to develop reasoning capabilities during the pretraining phase rather than requiring extensive post-training fine-tuning.

## Foundational Learning
- **Bilevel Optimization**: Two-level optimization framework needed to simultaneously optimize content selection policy and masked prediction tasks. Quick check: Verify the inner and outer optimization loops converge properly.
- **Reinforcement Learning from Pretraining Corpora**: Applying RL directly to unsupervised data without reward models. Quick check: Confirm the reward signal remains stable without external supervision.
- **Active Learning Framework**: Dynamic content selection based on model-generated signals. Quick check: Validate that the selection policy improves over time.
- **Mask Generation as Auxiliary Task**: Self-supervised signal for identifying valuable information. Quick check: Ensure mask generation aligns with actual learning objectives.
- **Policy-based Reasoning**: Learning unified reasoning policies rather than task-specific fine-tuning. Quick check: Test policy transfer to different reasoning tasks.

## Architecture Onboarding

**Component Map**: Token Stream -> Mask Generation Policy -> Content Selection -> Masked-Span Prediction -> Bilevel RL Objective -> Updated Policy

**Critical Path**: The reinforcement learning loop forms the critical path, where the mask generation policy selects content, the model predicts masked spans, and the bilevel optimization updates the policy based on performance. This loop must run efficiently to maintain pretraining throughput.

**Design Tradeoffs**: The method trades computational overhead from reinforcement learning against the benefit of learning reasoning capabilities during pretraining. The auxiliary mask generation task adds complexity but provides crucial self-supervision. The approach sacrifices some pretraining efficiency for long-term reasoning performance gains.

**Failure Signatures**: Training may diverge if the mask generation policy becomes too aggressive or conservative. The bilevel optimization could fail to converge if the inner and outer objectives are misaligned. Poor content selection may result in learning from low-quality or noisy passages, degrading overall model performance.

**3 First Experiments**:
1. Validate mask generation policy learns to identify high-quality content by measuring content selection accuracy on held-out Wikipedia passages
2. Test bilevel optimization convergence by monitoring inner and outer loop losses during training
3. Evaluate reasoning capability transfer by testing the pretrained model on simple reasoning tasks before post-training

## Open Questions the Paper Calls Out
None

## Limitations
- Generalization across model scales remains untested beyond the 4B parameter model
- Data distribution assumptions about Wikipedia containing sufficient verifiable information may not hold for all pretraining corpora
- Evaluation scope is limited to reasoning and math benchmarks without comprehensive testing on knowledge-intensive or safety-critical applications

## Confidence

**High confidence**: The core technical contribution (reinforcement active pretraining framework with mask generation task) is novel and well-described. The mathematical formulation of the bilevel optimization objective is sound.

**Medium confidence**: Empirical improvements on reasoning benchmarks are substantial and well-documented. However, the ablation studies could be more comprehensive, and the comparison to existing pretraining methods is limited.

**Low confidence**: Claims about "addressing the verification data-wall" and serving as "reasoning foundation models" are aspirational but not fully validated across diverse downstream applications.

## Next Checks

1. **Scale generalization study**: Evaluate PretrainZero on models ranging from 1B to 70B parameters to verify whether improvements scale consistently and identify potential degradation points.

2. **Cross-corpora robustness**: Test the method on diverse pretraining corpora (Books, CommonCrawl, ArXiv) to assess robustness to different data distributions and noise levels beyond Wikipedia.

3. **Long-term retention analysis**: Conduct multi-task retention studies after 1-3 months to verify that the active reasoning capabilities persist through continued training and fine-tuning on varied downstream tasks.