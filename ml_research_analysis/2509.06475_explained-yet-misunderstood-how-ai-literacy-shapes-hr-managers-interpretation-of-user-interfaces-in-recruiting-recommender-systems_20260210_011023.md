---
ver: rpa2
title: 'Explained, yet misunderstood: How AI Literacy shapes HR Managers'' interpretation
  of User Interfaces in Recruiting Recommender Systems'
arxiv_id: '2509.06475'
source_url: https://arxiv.org/abs/2509.06475
tags:
- literacy
- users
- explanations
- systems
- understanding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examines how AI literacy influences HR managers' interpretation
  of explainable AI (XAI) elements in recruitment recommender systems. An experiment
  with 410 German HR managers compared baseline dashboards to versions enriched with
  three XAI styles (important features, counterfactuals, and model criteria).
---

# Explained, yet misunderstood: How AI Literacy shapes HR Managers' interpretation of User Interfaces in Recruiting Recommender Systems

## Quick Facts
- arXiv ID: 2509.06475
- Source URL: https://arxiv.org/abs/2509.06475
- Authors: Yannick Kalff; Katharina Simbeck
- Reference count: 40
- Primary result: XAI elements improved subjective perceptions of helpfulness and trust among HR managers with moderate/high AI literacy but did not increase objective understanding and may reduce accurate interpretation for low-literacy users.

## Executive Summary
This study examines how AI literacy influences HR managers' interpretation of explainable AI (XAI) elements in recruitment recommender systems. An experiment with 410 German HR managers compared baseline dashboards to versions enriched with three XAI styles (important features, counterfactuals, and model criteria). Results show that while XAI elements improved subjective perceptions of helpfulness and trust among users with moderate or high AI literacy, they did not increase objective understanding. In fact, complex explanations may reduce accurate understanding. Only overlays of important features significantly aided interpretations for high-literacy users. This highlights the need for tailored explanation strategies and targeted AI literacy training in HRM.

## Method Summary
The study recruited 410 German HR managers and measured their AI literacy using the SNAIL scale (15 items across three dimensions: Technical Understanding, Critical Appraisal, and Practical Application). Participants were classified into low, medium, and high literacy groups using tertile splits. An online experiment using Moqups-created mockups tested a baseline dashboard against three XAI-enhanced variants (Important Features, Counterfactuals, Model Criteria). Random assignment was used, and analysis included paired-sample Wilcoxon tests for within-group comparisons and Kruskal-Wallis tests for group validation.

## Key Results
- XAI features improved subjective perceptions of trustworthiness and usefulness among users with moderate or high AI literacy
- Complex explanations (counterfactuals and model criteria) reduced objective understanding, especially among low-literacy users
- Only overlays of important features significantly aided interpretations for high-literacy users

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Explainable AI (XAI) features improve subjective perceptions of trust and usefulness only when users possess at least moderate AI literacy.
- **Mechanism:** Users with moderate or high AI literacy likely possess the cognitive schemas necessary to map abstract explanation widgets onto the system's decision logic. Low-literacy users lack these schemas, rendering the additional information uninterpretable, so it fails to improve their perception of the interface quality.
- **Core assumption:** The SNAIL scale accurately captures the cognitive capacity required to process XAI, and subjective ratings reflect genuine interpretative capability rather than social desirability.
- **Evidence anchors:**
  - [abstract] "While XAI features improved subjective perceptions... among users with moderate or high AI literacy... dashboards used in practice do not explain AI results."
  - [section] Section 4.1 notes that benefits were "concentrated among users with at least moderate scores," while low-literacy users saw no significant subjective gain.
  - [corpus] Weak direct support; "Beyond Technocratic XAI" emphasizes context-dependent design but does not validate this specific literacy threshold.
- **Break condition:** If users have low AI literacy, adding XAI widgets does not increase perceived helpfulness or trust.

### Mechanism 2
- **Claim:** Complex explanation styles (counterfactuals and model criteria) reduce objective understanding (accurate interpretation) for low- and medium-literacy users.
- **Mechanism:** These formats impose higher cognitive load by requiring users to process hypothetical scenarios ("what if") or abstract global rules rather than direct attribute mapping. This "information overload" distracts users from the raw data, leading to errors in factual interpretation.
- **Core assumption:** The decline in quiz scores is caused by the cognitive complexity of the explanation format rather than poor UI layout or time constraints.
- **Evidence anchors:**
  - [abstract] "...complex explanations sometimes reduced accurate understanding, especially among low-literacy users."
  - [section] Section 4.2 shows model criteria overlays significantly reduced interpretation scores across all groups ($p < .05$), and counterfactuals significantly hurt low-literacy performance ($p = .031$).
  - [corpus] "When concept-based XAI is imprecise" suggests users struggle with distinctions in concept-based explanations, supporting the difficulty of complex XAI.
- **Break condition:** If the explanation requires processing hypotheticals or abstract rules, low-literacy users will perform worse than with a baseline interface.

### Mechanism 3
- **Claim:** High self-assessed AI literacy correlates with improved subjective trust but can mask lower objective understanding (overconfidence).
- **Mechanism:** High-literacy users may rely on heuristics or top-down processing, assuming they understand the system's logic based on their confidence. This prevents the detailed scrutiny required for accurate factual interpretation, creating an "illusion of explanatory depth."
- **Core assumption:** The objective quiz questions accurately measure the necessary level of understanding for responsible use.
- **Evidence anchors:**
  - [section] Section 4.2 notes the high-literacy group "performed worst in comparison to all other groups" in the baseline condition, and Section 5 explicitly identifies an "overconfidence effect among high-literacy users."
  - [abstract] "XAI features improved subjective perceptions... [but] did not increase their objective understanding."
  - [corpus] No direct corpus evidence for this specific overconfidence mechanism in HR contexts.
- **Break condition:** If high-literacy users trust the XAI output without verifying specific data points, their objective accuracy drops.

## Foundational Learning

- **Concept:** **Feature Importance (Local vs. Global)**
  - **Why needed here:** The study distinguishes between "Important Features" (local/instance-based contributors) and "Model Criteria" (global rules). Understanding this distinction is required to predict why one method succeeded (local attributes) while the other failed (abstract global lists) for non-experts.
  - **Quick check question:** Does a "Model Criteria" list explain why *this specific candidate* was rejected, or just the general rules of the system?

- **Concept:** **Cognitive Load Theory**
  - **Why needed here:** The failure of counterfactuals and model criteria is attributed to the mental effort required to process negation or abstraction. Engineers must understand that adding information (XAI) consumes limited working memory, potentially degrading task performance.
  - **Quick check question:** Does adding a "What if?" scenario reduce the user's available mental resources to verify the actual candidate's skills?

- **Concept:** **The "Gulf of Evaluation"**
  - **Why needed here:** This HCI concept describes the gap between the system's state and the user's understanding of it. The study measures this "gulf" by comparing the system's provided info against the user's objective quiz score.
  - **Quick check question:** Does the XAI bridge the gulf (user understands the ranking) or widen it (user is confused by the explanation style)?

## Architecture Onboarding

- **Component map:**
  - Input: Candidate Profile (Skills, Salary, Experience)
  - Core Logic: Black-box Ranking Algorithm
  - XAI Layer: Three distinct modules (Feature Importance, Counterfactual Generator, Criteria Summarizer)
  - Interface: Dashboard displaying Rank + Name + XAI Overlay
  - User State: AI Literacy Level (Low/Medium/High) -> determines XAI efficacy

- **Critical path:**
  1. User views Baseline Ranking (Rank/Name/Match)
  2. System renders XAI Overlay (e.g., displays "Expected Salary" as an important feature)
  3. User integrates XAI into mental model (Success: High Literacy + Feature Importance; Failure: Low Literacy + Counterfactuals)
  4. User outputs decision (Subjective Trust Rating + Objective Quiz Answer)

- **Design tradeoffs:**
  - Feature Importance vs. Counterfactuals: Feature importance provides direct, additive context ("Salary matters") which aids high-literacy users. Counterfactuals provide subtractive/hypothetical context ("If salary were lower...") which confuses low-literacy users.
  - Transparency vs. Performance: Providing global model criteria increases "perceived" transparency but statistically degrades "objective" understanding.

- **Failure signatures:**
  - High Trust / Low Accuracy: High-literacy user rates dashboard "Very Useful" but fails factual questions about the candidate (Overconfidence)
  - Interpretation Collapse: Low-literacy user scores drop significantly when "Model Criteria" are displayed (Cognitive Overload)

- **First 3 experiments:**
  1. **Adaptive XAI:** Implement a UI toggle or literacy pre-test that defaults low-literacy users to "Feature Importance" only, hiding "Counterfactuals" to prevent score degradation.
  2. **Simplified Feature Overlays:** Test if "Important Features" works for low-literacy users if the language is simplified (e.g., "Why matched" vs. "Feature Importance").
  3. **Calibration Feedback:** Test a "quiz-feedback" loop where high-literacy users are shown their objective errors immediately to reduce overconfidence and calibrate their trust.

## Open Questions the Paper Calls Out
None

## Limitations
- The study's AI literacy measure (SNAIL scale) may not capture domain-specific recruitment knowledge that could confound results.
- Objective understanding was assessed through only five factual statements, which may not fully capture the complexity of decision-making in real recruitment scenarios.
- The experimental context used simplified mockups rather than live systems, potentially reducing ecological validity.

## Confidence
- **High confidence**: The finding that XAI features improved subjective perceptions among moderate/high-literacy users but not low-literacy users.
- **Medium confidence**: The claim that complex explanations reduced objective understanding.
- **Low confidence**: The overconfidence effect among high-literacy users.

## Next Checks
1. **Replication with domain expertise control**: Replicate the study while controlling for years of recruitment experience to isolate AI literacy effects from domain knowledge.
2. **Expanded objective measure**: Develop and validate a more comprehensive objective understanding assessment that better reflects real recruitment decision complexity.
3. **Real-system validation**: Test the same XAI approaches in an operational recruitment system with actual hiring data to assess ecological validity of the findings.