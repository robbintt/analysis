---
ver: rpa2
title: Neural Approximators for Low-Thrust Trajectory Transfer Cost and Reachability
arxiv_id: '2508.02911'
source_url: https://arxiv.org/abs/2508.02911
tags:
- trajectory
- problem
- control
- time
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents neural network models to predict low-thrust
  trajectory transfer costs and reachability. The models are trained on a large dataset
  (100 million samples) generated using a homotopy ray method that focuses on mission-design-oriented
  trajectories with low fuel consumption.
---

# Neural Approximators for Low-Thrust Trajectory Transfer Cost and Reachability

## Quick Facts
- **arXiv ID:** 2508.02911
- **Source URL:** https://arxiv.org/abs/2508.02911
- **Reference count:** 40
- **Primary result:** Neural network models predict low-thrust trajectory costs with 0.78% relative error and reachability with 0.63% relative error, generalizing across arbitrary orbital geometries and central bodies.

## Executive Summary
This paper presents neural network models to predict low-thrust trajectory transfer costs and reachability. The models are trained on a large dataset (100 million samples) generated using a homotopy ray method that focuses on mission-design-oriented trajectories with low fuel consumption. The data are transformed into a self-similar space to enable generalization across arbitrary semi-major axes, inclinations, and central bodies. The resulting neural network achieves a relative error of 0.78% for velocity increment prediction and 0.63% for minimum transfer time estimation. The models have been validated on third-party datasets, multi-flyby mission design problems, and mission analysis scenarios, demonstrating strong generalization capability, predictive accuracy, and computational efficiency. The implementations are publicly available in C++, Python, and MATLAB.

## Method Summary
The method involves transforming physical orbital states into a self-similar space through rotation and non-dimensionalization, then using Lambert solution outputs as features for a 9-layer MLP with 128 neurons per layer. The training data consists of 100 million samples generated via a homotopy ray method that densely samples low-fuel trajectories. Two separate models are trained: one for predicting fuel-optimal delta-v and another for time-optimal transfer time. The architecture uses AdamW optimizer with OneCycleLR scheduler and achieves inference times comparable to Lambert solvers.

## Key Results
- Relative error of 0.78% for delta-v prediction and 0.63% for minimum transfer time estimation
- Generalization across arbitrary semi-major axes, inclinations, and central bodies
- Inference time of approximately 1ms per prediction, matching Lambert solver speed
- Validated on third-party datasets and multi-flyby mission design problems

## Why This Works (Mechanism)

### Mechanism 1: Dimensionality Reduction via Self-Similarity
The model generalizes across arbitrary central bodies and orbital geometries by mapping physical states into a canonical "self-similar" space. The architecture exploits the rotational invariance of the two-body problem by rotating initial/terminal states to align with a fixed reference frame and non-dimensionalizing units. This transformation effectively reduces problem dimensionality by three without loss of physical fidelity.

### Mechanism 2: Mission-Oriented Data Densification (Homotopy Ray)
The model achieves high accuracy because training data is densely sampled in the "region of interest" (low fuel/time) rather than uniformly distributed. The Homotopy Ray Method starts from a zero-fuel Keplerian orbit and "walks" along a perturbation vector until the trajectory becomes infeasible, creating a dataset saturated with high-value, low-cost solutions needed for preliminary design.

### Mechanism 3: Lambert-Based Feature Embedding
Using Lambert (impulsive) solution outputs as inputs stabilizes training and reduces function approximation complexity. Low-thrust trajectories are continuous "versions" of impulsive Lambert arcs, so feeding Lambert delta-v and transfer angles as inputs allows the network to learn the residual correction rather than the absolute trajectory physics from scratch.

## Foundational Learning

- **Concept: Two-Body Problem & Orbital Elements**
  - **Why needed here:** To understand the "Ground Truth" generation. The paper uses Modified Equinoctial Elements (MEE) to avoid singularities during the indirect optimization process.
  - **Quick check question:** Can you explain why true anomaly (ν) is problematic for circular orbits, requiring MEE?

- **Concept: Lambert's Problem**
  - **Why needed here:** This is the "Baseline Approximator." The network inputs rely on Lambert solutions; understanding this helps interpret why the network inputs are structured as angles and delta-v.
  - **Quick check question:** What does a Lambert solver output given two positions and a time-of-flight?

- **Concept: Pontryagin's Minimum Principle**
  - **Why needed here:** The dataset is generated using an indirect method based on this principle. Understanding costates helps in debugging the data generation pipeline.
  - **Quick check question:** In optimal control, what does the costate vector represent physically?

## Architecture Onboarding

- **Component map:** Physical states → Self-Similar Transformation → Lambert Features → 9-layer MLP → Delta-v or Transfer Time output
- **Critical path:** The Self-Similar Transformation (Algorithm 2). If the rotation logic (aligning r1 to x, r2 to xy) or normalization is implemented incorrectly, the model will see inconsistent data and fail to converge.
- **Design tradeoffs:**
  - **Inference Speed vs. Accuracy:** Authors chose 9 layers (approx. 1ms inference) to match Lambert solver speed, rather than using deeper transformers which would be slower.
  - **Data Scale vs. Training Time:** They prioritized a massive 100M dataset (3 days generation) over architecture complexity to maximize the "Scaling Law" benefits.
- **Failure signatures:**
  - **High Relative Error on Third-Party Data:** If test data is sampled uniformly (not mission-oriented), the model might seem to underperform compared to baselines trained on that specific distribution.
  - **Infeasible Predictions:** The model predicts a scalar cost; it does not guarantee the existence of a feasible control law.
- **First 3 experiments:**
  1. **Unit Test Pre-processing:** Verify the rotation logic (Algorithm 2) by checking if rotated states preserve relative distances and if r1 aligns with [1, 0, 0].
  2. **Baseline Ablation:** Train a tiny model (1k samples) using raw Cartesian coordinates vs. Lambert features to confirm the performance gain of the feature engineering.
  3. **Inference Integration:** Wrap the C++/Python inference call inside a standard Lambert loop to verify it meets the real-time requirement for a Porkchop plot generation.

## Open Questions the Paper Calls Out

- **Open Question 1:** How can the current framework be extended to accurately approximate multi-revolution low-thrust transfers?
  - **Basis in paper:** [explicit] The paper states, "the current model supports only single-revolution transfers; extending it to multi-revolution cases remains a key direction for future work."
  - **Why unresolved:** The current input features, data generation via homotopy ray methods, and training datasets are specifically constrained to single-revolution geometries.
  - **What evidence would resolve it:** Successful training and validation of a model on a dataset containing long-duration, multi-revolution trajectories with comparable relative error rates.

- **Open Question 2:** Can the self-similar space transformation be generalized to include non-Keplerian perturbation forces, such as J2 or third-body gravity?
  - **Basis in paper:** [inferred] The dimensionality reduction relies on the rotational and dimensional invariance of a "central gravitational field," explicitly assuming strict two-body dynamics.
  - **Why unresolved:** Perturbations break the geometric symmetries (rotational invariance) used to reduce the input dimensionality from 17 to 11 variables.
  - **What evidence would resolve it:** Derivation of new normalization techniques or input features that capture perturbed dynamics while maintaining the network's generalization capability.

- **Open Question 3:** Can the trade-off between model size (accuracy) and inference time be further optimized for complex optimization frameworks?
  - **Basis in paper:** [inferred] The authors capped model size to keep inference time comparable to a Lambert solver, noting that larger models become a "bottleneck" despite the "Scaling Law" suggesting accuracy improves with size.
  - **Why unresolved:** The conflict between the desire for higher accuracy (via larger models) and the strict computational budget required for preliminary mission design remains unaddressed.
  - **What evidence would resolve it:** Demonstration of a network architecture or distillation method that achieves lower error rates than the current baseline without increasing inference latency.

## Limitations

- The current model only supports single-revolution transfers, limiting applicability to multi-revolution spiral trajectories
- Performance depends heavily on the quality and distribution of training data, requiring careful implementation of the homotopy ray method
- The self-similarity transformation assumes two-body dynamics, potentially limiting accuracy in perturbed environments

## Confidence

- **High Confidence:** The dimensional analysis and self-similarity transformation mechanisms are well-supported by the paper's theoretical framework and empirical results
- **Medium Confidence:** The Homotopy Ray Method's effectiveness in creating mission-oriented data is plausible but relies on assumed implementation details not fully disclosed
- **Low Confidence:** The generalizability of the Lambert-based feature embedding to extreme non-conic trajectories (e.g., very low thrust spirals) is not empirically validated

## Next Checks

1. **Dataset Distribution Analysis:** Compare the distribution of your generated dataset with the paper's mission-oriented region to ensure alignment with the Homotopy Ray Method's sampling strategy
2. **Third-Party Dataset Generalization:** Validate the model's performance on a publicly available third-party dataset (e.g., NASA's GMAT test cases) to assess real-world generalization
3. **Extreme Trajectory Stress Test:** Evaluate the model's accuracy on trajectories with very low thrust or non-Keplerian perturbations to identify potential breakdown conditions of the Lambert feature embedding