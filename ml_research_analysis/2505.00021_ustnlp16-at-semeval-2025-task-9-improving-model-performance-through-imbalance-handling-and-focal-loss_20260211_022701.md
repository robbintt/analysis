---
ver: rpa2
title: 'Ustnlp16 at SemEval-2025 Task 9: Improving Model Performance through Imbalance
  Handling and Focal Loss'
arxiv_id: '2505.00021'
source_url: https://arxiv.org/abs/2505.00021
tags:
- loss
- data
- focal
- classification
- class
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of severe class imbalance in
  food hazard detection using short, unstructured text data. The proposed solution
  integrates Easy Data Augmentation (EDA) with random oversampling and focal loss
  to improve model performance.
---

# Ustnlp16 at SemEval-2025 Task 9: Improving Model Performance through Imbalance Handling and Focal Loss

## Quick Facts
- arXiv ID: 2505.00021
- Source URL: https://arxiv.org/abs/2505.00021
- Reference count: 2
- Primary result: BERT and RoBERTa models with Easy Data Augmentation (EDA) and focal loss achieved 0.86 accuracy for hazard detection and 0.55 accuracy for product detection in food hazard classification.

## Executive Summary
This study addresses severe class imbalance in food hazard detection from short, unstructured text data. The authors propose a combination of Easy Data Augmentation (EDA), random oversampling, and focal loss to improve model performance on the SemEval-2025 Task 9 dataset. BERT and RoBERTa models were fine-tuned with these techniques, achieving significant improvements over baseline models. EDA alone yielded the highest accuracy (0.55) and F1-weighted score (0.52) for product detection, while the combination of focal loss and EDA achieved an accuracy of 0.86 for hazard detection. The results demonstrate that EDA is particularly effective for mitigating class imbalance in this domain.

## Method Summary
The method combines three key techniques: Easy Data Augmentation (EDA) applied before tokenization to create synthetic minority samples, random oversampling applied after tokenization to balance class distribution, and focal loss with parameters α=1 and γ=2 to emphasize hard-to-classify examples. The pipeline processes food recall reports through preprocessing (stopword removal, lemmatization, IQR outlier filtering), applies EDA operations (synonym replacement, random insertion, swap, deletion with 50% probability each), tokenizes using WordPiece, balances classes through oversampling, and trains BERT-base or RoBERTa-base models with focal loss. The approach uses AdamW optimizer with learning rate 5e-5, batch size 32, and cosine warmup schedule.

## Key Results
- EDA alone achieved the highest accuracy (0.55) and F1-weighted score (0.52) for product detection
- Focal loss combined with EDA achieved accuracy of 0.86 for hazard detection
- Baseline BERT performance was 0.22 accuracy and 0.03 F1-macro for product classification
- Excessive oversampling (rate 1.0) degraded performance to 0.29 accuracy, indicating overfitting

## Why This Works (Mechanism)

### Mechanism 1: Easy Data Augmentation (EDA)
EDA mitigates class imbalance by generating lexically diverse synthetic samples for minority classes through four operations: synonym replacement, random insertion, random swap, and random deletion. Each operation has a 50% probability of application. This increases minority-class coverage without collecting new data, though excessive augmentation (rate ≥1.0) shows diminishing returns due to potential synthetic noise corrupting semantics.

### Mechanism 2: Focal Loss
Focal loss shifts model attention toward hard-to-classify examples by down-weighting easy samples. The loss function FL(pt) = −αt(1 − pt)^γ log(pt) reduces contribution from well-classified examples through the (1-pt)^γ term. With γ=2, easy examples contribute ~100x less loss than hard ones. Focal loss benefits from augmented diversity, as it alone without EDA underperformed EDA alone.

### Mechanism 3: Sequential Pipeline Ordering
The pipeline applies EDA before tokenization to preserve subword information while creating synthetic samples, then applies oversampling after tokenization to balance class distribution without duplicating identical token sequences. This ordering captures augmented vocabulary through WordPiece tokenization while ensuring minority classes reach target representation without amplifying duplicate samples.

## Foundational Learning

- **Class imbalance in classification**: Understanding how severe long-tail distributions affect model performance; quick check: Would accuracy alone be reliable with a 100:1 majority-to-minority ratio?
- **Focal loss formulation**: Understanding α and γ parameters for tuning loss functions; quick check: If pt=0.9 and γ=2, what is the relative weight compared to pt=0.5?
- **Transformer fine-tuning with imbalance strategies**: Knowing when to apply oversampling relative to tokenization and augmentation; quick check: At what stage should oversampling occur relative to tokenization and augmentation?

## Architecture Onboarding

- **Component map**: Raw text → Cleaning → EDA → Tokenization → Oversampling → Model forward pass → Focal loss → Backprop
- **Critical path**: Input text flows through preprocessing, augmentation, tokenization, balancing, and model training with focal loss backpropagation
- **Design tradeoffs**: EDA alone performed best for product detection; focal loss + EDA better for hazard detection. Sample rate 0.1-0.2 outperformed 1.0. BERT and RoBERTa showed comparable performance.
- **Failure signatures**: Oversampling at r=1.0 collapsed accuracy to 0.29. Focal loss + oversampling without EDA underperformed. Baseline F1-macro of 0.03 indicates severe minority class failure.
- **First 3 experiments**: 1) Replicate baseline BERT on product classification (accuracy ~0.22). 2) Add EDA at sample rate 0.2 (accuracy ~0.55). 3) Add focal loss + EDA on hazard classification (accuracy ~0.86).

## Open Questions the Paper Calls Out

**Open Question 1**: To what extent does synthetic noise from EDA degrade semantic preservation and contribute to misclassification in minority classes? The paper notes augmented samples may not preserve semantic meaning but didn't quantify semantic drift rates or their impact on specific misclassification errors.

**Open Question 2**: Can domain adaptation techniques enhance the model's ability to generalize to food safety datasets with different reporting styles or hazard categories? The current experiments were confined to one dataset, leaving generalizability to domain shifts unverified.

**Open Question 3**: Would contrastive learning or cost-sensitive training outperform focal loss and oversampling in closing the performance gap between majority and minority classes? The paper hypothesizes other methods might help but provides no data on their relative efficacy.

## Limitations

- The reported performance gains are specific to the SemEval-2025 Task 9 dataset and may not generalize to other imbalanced classification tasks
- The paper uses fixed focal loss parameters (α=1, γ=2) without exploring the full hyperparameter space or conducting sensitivity analysis
- The lack of per-class precision/recall breakdowns and confusion matrices makes it difficult to assess minority class performance despite high weighted F1 scores

## Confidence

**High confidence**: EDA alone effectiveness for product detection (accuracy 0.55 vs baseline 0.22) is well-supported by Table 3 results and consistent across multiple sample rates.

**Medium confidence**: Focal loss + EDA superiority for hazard detection (accuracy 0.86) is supported by Table 4, but lacks ablation studies showing focal loss alone would achieve similar results.

**Low confidence**: Claims about pipeline ordering (EDA before tokenization, oversampling after) improving performance lack direct empirical support and were asserted without comparing alternative arrangements.

## Next Checks

**Check 1**: Conduct ablation studies on focal loss hyperparameters (α and γ) across the full range (α∈[0.1, 1.0], γ∈[1, 3]) while keeping EDA constant to determine if reported parameters are optimal.

**Check 2**: Test the methodology on at least two additional imbalanced text classification datasets (e.g., medical diagnosis reports, financial fraud detection) to assess generalizability and compare against standard class-weighted cross-entropy baselines.

**Check 3**: Perform detailed per-class analysis by generating confusion matrices and calculating precision/recall for each minority class to reveal whether improvements come from better minority class detection or majority class optimization inflating overall metrics.