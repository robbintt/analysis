---
ver: rpa2
title: 'A self-supervised learning approach for denoising autoregressive models with
  additive noise: finite and infinite variance cases'
arxiv_id: '2508.12970'
source_url: https://arxiv.org/abs/2508.12970
tags:
- noise
- denoising
- time
- series
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the problem of denoising autoregressive (AR)\
  \ time series corrupted by additive noise, particularly in cases where both the\
  \ AR process and the noise have infinite variance (e.g., \u03B1-stable distributions).\
  \ The proposed method, Stable-N2N, is a self-supervised learning approach inspired\
  \ by the Noise2Noise method from computer vision, which avoids requiring knowledge\
  \ of the noise distribution or access to clean data."
---

# A self-supervised learning approach for denoising autoregressive models with additive noise: finite and infinite variance cases

## Quick Facts
- arXiv ID: 2508.12970
- Source URL: https://arxiv.org/abs/2508.12970
- Reference count: 40
- Primary result: Proposes Stable-N2N, a self-supervised deep learning method for denoising AR processes corrupted by additive noise of both finite and infinite variance

## Executive Summary
This paper introduces Stable-N2N, a self-supervised learning approach for denoising autoregressive time series corrupted by additive noise, particularly effective when both the AR process and noise have infinite variance (e.g., α-stable distributions). Inspired by the Noise2Noise method from computer vision, the approach avoids requiring knowledge of the noise distribution or access to clean data. The method learns stationary properties of the underlying pure AR process by mapping one noise-corrupted random vector to another from the same process using a feedforward neural network with a multi-input multi-output (MIMO) approach. The denoising quality is evaluated by applying a modified Yule-Walker method based on fractional lower-order covariance to the denoised data and measuring the mean absolute error between estimated and true AR parameters.

## Method Summary
The Stable-N2N framework is built on a self-supervised learning paradigm that adapts the Noise2Noise approach to time series denoising. The core innovation is a MIMO FNN that takes as input multiple lagged vectors from the noise-corrupted AR process and predicts multiple future vectors, effectively learning the underlying AR dynamics without requiring clean data. The training objective minimizes the expected difference between predictions and noisy observations, leveraging the stationarity assumption to treat one noisy observation as a target for another. For evaluation, the denoised series undergoes modified Yule-Walker estimation using fractional lower-order covariance (FLOC) to handle infinite-variance noise, with performance measured by mean absolute error of AR parameter recovery and forecast accuracy.

## Key Results
- Stable-N2N outperforms baseline methods in MAE of estimated AR parameters, particularly for strong impulsive noise scenarios
- The method demonstrates superior denoising performance compared to several baselines in Monte Carlo simulations on both synthetic and semi-synthetic datasets
- Stable-N2N shows effectiveness in forecasting the pure AR signal from noise-corrupted data, with accuracy improvements over competing approaches

## Why This Works (Mechanism)
The method exploits the stationarity of AR processes by recognizing that two different observations from the same stationary process share the same underlying dynamics, despite both being corrupted by noise. By training a neural network to map one noisy observation to another, the model implicitly learns to denoise without explicit knowledge of the noise distribution. The MIMO architecture allows the network to capture temporal dependencies across multiple lags simultaneously, while the FLOC-based Yule-Walker estimation provides a robust parameter recovery method for infinite-variance scenarios.

## Foundational Learning

**Stationary AR processes** - Why needed: The entire framework relies on the assumption that the underlying AR process has a unique stationary solution, which ensures consistent statistical properties across observations. Quick check: Verify that the AR polynomial has all roots outside the unit circle.

**Fractional Lower-Order Covariance (FLOC)** - Why needed: Traditional covariance-based methods fail for infinite-variance distributions; FLOC provides a robust alternative for parameter estimation in α-stable noise environments. Quick check: Confirm that the selected fractional order β is appropriate for the noise characteristics.

**Noise2Noise principle** - Why needed: This self-supervised learning paradigm enables training without clean data by treating one noisy observation as a target for another, circumventing the need for noise distribution knowledge. Quick check: Ensure sufficient data diversity to capture the full range of noise realizations.

## Architecture Onboarding

**Component map**: Input Lagged Vectors -> MIMO FNN -> Output Predicted Vectors -> FLOC-YW Estimation -> AR Parameters

**Critical path**: The MIMO FNN training loop represents the critical computational path, where the network learns to map noisy observations to each other. The quality of this learned mapping directly determines the effectiveness of downstream AR parameter estimation.

**Design tradeoffs**: The choice of FNN over RNN/CNN architectures prioritizes simplicity and computational efficiency but may limit the ability to capture long-range temporal dependencies. The MIMO approach trades increased parameter count for the ability to denoise multiple time points simultaneously.

**Failure signatures**: Poor denoising performance manifests as high MAE in AR parameter estimation, particularly when the noise is highly impulsive or the AR process deviates significantly from stationarity. Network overfitting can occur if the training data lacks sufficient diversity in noise realizations.

**First experiments**: 1) Validate MIMO FNN convergence on synthetic AR data with known parameters, 2) Compare FLOC-YW estimation accuracy against traditional methods under infinite-variance noise, 3) Test sensitivity to model order specification by varying p systematically.

## Open Questions the Paper Calls Out

**Open Question 1**: How can the Stable-N2N framework be adapted to identify the correct autoregressive model order p when it is not known a priori? The authors note they construct forecasts "assuming that the model order p is known" and state, "We leave the detailed investigation of this aspect for future work."

**Open Question 2**: Can the proposed self-supervised denoising method be extended to handle non-stationary autoregressive time series without significant loss of efficacy? The authors acknowledge, "the proposed method... may face limitations when the underlying data are non-stationary."

**Open Question 3**: How does the performance of Stable-N2N change when implemented with alternative deep learning architectures, such as Recurrent Neural Networks (RNNs) or Convolutional Neural Networks (CNNs), rather than Feedforward Neural Networks (FNNs)? The authors state, "In this study, we do not analyze the performance of denoising methods based on different deep learning architectures."

## Limitations
- The method assumes stationarity in the AR process, which may not hold for many real-world time series
- While showing superior performance in simulations, effectiveness on real-world data with complex noise structures remains to be thoroughly validated
- Computational complexity of the MIMO approach with deep neural networks may pose challenges for very long time series or resource-constrained applications

## Confidence

**High confidence**: The mathematical formulation of the Stable-N2N method and its theoretical connection to the underlying AR process.

**Medium confidence**: The comparative performance against baseline methods in simulation studies, as these results depend on the specific simulation setups and parameter choices.

**Low confidence**: The generalizability of the results to real-world applications with unknown noise distributions and non-stationary processes.

## Next Checks
1. Apply the Stable-N2N method to real-world time series datasets from various domains (e.g., finance, neuroscience, environmental monitoring) to assess its practical effectiveness.
2. Conduct sensitivity analysis to evaluate the method's robustness to violations of the stationarity assumption and the presence of outliers or regime shifts in the underlying AR process.
3. Investigate the scalability of the method for very long time series and compare its computational efficiency with other denoising approaches.