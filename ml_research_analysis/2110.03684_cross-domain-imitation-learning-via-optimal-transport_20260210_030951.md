---
ver: rpa2
title: Cross-Domain Imitation Learning via Optimal Transport
arxiv_id: '2110.03684'
source_url: https://arxiv.org/abs/2110.03684
tags:
- learning
- expert
- agent
- optimal
- domain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Gromov-Wasserstein Imitation Learning (GWIL),
  a method for cross-domain imitation learning that addresses the challenge of transferring
  expert demonstrations from one domain to train an agent in a different domain with
  potentially different state-action spaces and dynamics. The core idea is to use
  the Gromov-Wasserstein distance to align and compare states between different agent
  spaces without requiring proxy tasks or explicit mapping between domains.
---

# Cross-Domain Imitation Learning via Optimal Transport

## Quick Facts
- arXiv ID: 2110.03684
- Source URL: https://arxiv.org/abs/2110.03684
- Authors: Arnaud Fickinger; Samuel Cohen; Stuart Russell; Brandon Amos
- Reference count: 12
- Key outcome: Introduces GWIL, a method for cross-domain imitation learning that uses Gromov-Wasserstein distance to align states between different domains without requiring proxy tasks or explicit mapping, enabling transfer from expert demonstrations in one domain to train an agent in a different domain with potentially different state-action spaces and dynamics.

## Executive Summary
This paper introduces Gromov-Wasserstein Imitation Learning (GWIL), a method for cross-domain imitation learning that addresses the challenge of transferring expert demonstrations from one domain to train an agent in a different domain with potentially different state-action spaces and dynamics. The core idea is to use the Gromov-Wasserstein distance to align and compare states between different agent spaces without requiring proxy tasks or explicit mapping between domains. The method formulates imitation learning as an optimal transport problem, where the Gromov-Wasserstein distance serves as a proxy reward for training the imitation agent. Theoretical analysis characterizes scenarios where GWIL preserves optimality, showing that optimal policies can be recovered up to isometries. Experiments demonstrate effectiveness in non-trivial continuous control domains, including rigid transformations, different state-action spaces (e.g., pendulum to cartpole), and significantly different morphologies (e.g., cheetah to walker). The results show that GWIL can recover optimal behaviors without external rewards, achieving competitive performance compared to baseline methods while offering good scalability.

## Method Summary
GWIL formulates imitation learning as an optimal transport problem using the Gromov-Wasserstein distance to align occupancy measures between expert and agent domains. The method computes a pseudo-reward based on how well the agent's state-action relationships match the expert's, as determined by the optimal coupling found through the Gromov-Wasserstein solver. This pseudo-reward is then used with standard RL algorithms (specifically SAC) to train the agent policy. The approach bypasses the need for gradients through unknown transition dynamics by using the GW distance as a proxy reward signal.

## Key Results
- Successfully transfers policies across different state-action spaces (pendulum to cartpole) without external rewards
- Achieves morphological transfer from cheetah to walker, recovering walking behaviors despite structural differences
- Demonstrates optimality preservation under isometry conditions in maze navigation tasks with rigid transformations
- Shows competitive performance compared to baseline methods while avoiding the need for explicit domain mappings

## Why This Works (Mechanism)

### Mechanism 1: Relational Structure Matching via Gromov-Wasserstein Distance
- Claim: Minimizing the Gromov-Wasserstein (GW) distance between occupancy measures aligns policies across domains with different state-action dimensions by matching relational structure rather than absolute coordinates.
- Mechanism: The GW distance compares two metric measure spaces by minimizing the discrepancy between intra-space pairwise distances. Given expert occupancy ρ^π_E over (S_E × A_E, d_E) and agent occupancy ρ^π_A over (S_A × A_A, d_A), GW finds a coupling u* that minimizes Σ|d_E(z_E, z'_E) - d_A(z_A, z'_A)|² u_{z_E,z_A} u*_{z'_E,z'_A}. This aligns states based on their relative positions, not their raw coordinates.
- Core assumption: The expert and agent domains share latent isomorphic structure—that is, there exists a distance-preserving bijection between the support of their occupancy measures.
- Evidence anchors:
  - [abstract]: "GWIL... uses the Gromov-Wasserstein distance to align and compare states between the different spaces of the agents."
  - [section 4.1, Definition 2]: Defines isomorphic policies via isometry ϕ satisfying d_E((s_E, a_E), (s'_E, a'_E)) = d_A(ϕ(s_E, a_E), ϕ(s'_E, a'_E)) and measure pushforward ρ^π_A = ϕ♯(ρ^π_E).
  - [corpus]: Limited direct evidence on GWIL specifically; related work (arXiv:2502.02867, 2509.04089) discusses Gromov-Wasserstein for heterogeneous alignment, supporting the general approach but not validating GWIL's optimality claims.
- Break condition: If the expert and agent domains have no isometric correspondence (e.g., fundamentally different task structures, incompatible morphologies with no relational analogy), GW distance minimization will not recover optimal behavior.

### Mechanism 2: Pseudo-Reward Proxy for RL-Based Optimization
- Claim: The GW distance induces a dense pseudo-reward signal that can be optimized via standard RL, bypassing the need for gradients through unknown transition dynamics.
- Mechanism: Given a coupling u* minimizing the GW objective, the pseudo-reward for agent state-action pair (s_A, a_A) is r^GW(z_A) = -1/ρ^π(z_A) Σ|d_E(z_E, z'_E) - d_A(z_A, z'_A)|² u*_{z_E,z_A} u*_{z'_E,z'_A}. This assigns higher reward when the agent's local pairwise distances match the expert's, measured through the optimal coupling.
- Core assumption: The empirical trajectory sufficiently approximates the true occupancy measure; the coupling u* remains meaningful when computed on finite samples.
- Evidence anchors:
  - [section 4.2, Definition 3]: Formal definition of r^GW.
  - [section 4.2, Proposition 2]: Proves that if π_A minimizes GW(π_E, π_A), then π_A is optimal for reward r^GW.
  - [appendix A, Figure 6]: Shows proxy reward curves converging consistently across seeds in maze, cartpole/pendulum, and walker/cheetah tasks.
  - [corpus]: No direct corpus validation; assumption of uniform occupancy approximation is inherited from prior IL work (Dadashi et al., 2020).
- Break condition: If trajectories are too short to capture the true occupancy, or if the coupling u* is poorly estimated (e.g., due to noise, insufficient samples), the pseudo-reward becomes unreliable.

### Mechanism 3: Optimality Recovery Under Isometry Conditions
- Claim: When the agent domain is an isometric transformation of the expert domain, minimizing GW recovers an optimal policy (up to isometry) without access to the transformation.
- Mechanism: Theorem 1 establishes that if there exist isometries ϕ: S_E → S_A and ψ: A_E → A_A preserving rewards, transition dynamics, and initial state distributions, then the policy minimizing GW distance is isomorphic to an optimal policy in the agent domain.
- Core assumption: Strong structural correspondence—rewards, dynamics, and initial distributions must all respect the isometry (conditions 5-7 in Theorem 1).
- Evidence anchors:
  - [section 4.1, Theorem 1]: Full theoretical characterization with proof sketch.
  - [section 5.1, Figure 3]: Empirical validation in maze task with reflection transformation; GWIL recovers optimal policy.
  - [section 5.2-5.3]: Shows effectiveness beyond strict isometry (pendulum→cartpole, cheetah→walker), though optimality is not guaranteed in these cases.
  - [corpus]: No corpus papers validate Theorem 1's conditions empirically.
- Break condition: If any of conditions 5-7 fail (e.g., reward structure changes, dynamics are not isometrically related), the recovered policy may be suboptimal or fail entirely. Empirical results (Figure 5) show seed-dependent recovery of forward/backward walking, illustrating this limitation.

## Foundational Learning

- Concept: **Optimal Transport and Wasserstein Distance**
  - Why needed here: GWIL extends Wasserstein IL (which requires same domain) to Gromov-Wasserstein (which handles different domains). Understanding OT basics clarifies why matching occupancy measures is the core IL objective.
  - Quick check question: Can you explain why the Wasserstein distance between distributions requires a shared metric space, and why Gromov-Wasserstein relaxes this requirement?

- Concept: **Occupancy Measures in RL**
  - Why needed here: The paper defines policy similarity via occupancy measures ρ^π(s,a) = π(a|s) Σ γ^t P(s_t=s|π). All theoretical results operate on these distributions, not on trajectories directly.
  - Quick check question: Given a policy and MDP, can you compute the discounted state-action occupancy measure? What does it mean for two policies to have the same occupancy measure?

- Concept: **Isometries and Metric Space Structure**
  - Why needed here: Theorem 1's optimality guarantee hinges on isometric relationships between domains. Understanding isometries (rotations, translations, reflections) clarifies what transformations GWIL can handle provably.
  - Quick check question: What properties must a function ϕ satisfy to be an isometry between metric spaces (X, d_X) and (Y, d_Y)?

## Architecture Onboarding

- Component map:
  1. Expert trajectory buffer -> Distance matrix precomputation -> Gromov-Wasserstein solver -> Pseudo-reward calculator -> RL optimizer (SAC)
  2. Agent trajectory collector -> Distance matrix computation -> Gromov-Wasserstein solver -> Pseudo-reward calculator -> RL optimizer (SAC)

- Critical path:
  1. Precompute expert pairwise distances once (offline)
  2. For each iteration: collect agent trajectory → compute agent pairwise distances → solve for coupling θ* → compute pseudo-rewards → SAC update
  3. Bottleneck: GW solver complexity is O(T_E² × T_A²) where T_E, T_A are trajectory lengths

- Design tradeoffs:
  - **Trajectory length**: Longer trajectories better approximate occupancy but increase computational cost quadratically
  - **Metric choice**: Euclidean metric used in experiments; task-specific metrics could improve alignment but require domain knowledge
  - **Entropy regularization**: Not used in paper, but could improve scalability (mentioned in Appendix C)
  - **Single vs. multiple demonstrations**: Paper uses single expert trajectory; aggregating multiple demonstrations could stabilize occupancy estimates

- Failure signatures:
  - Agent policy collapses to repetitive behavior: Check if pseudo-reward is uniform (coupling θ* may be degenerate)
  - Slow convergence or oscillation: Distance matrices may be poorly scaled; normalize distances
  - Suboptimal policy recovery: Check if isometry conditions are violated (Theorem 1 does not guarantee optimality in non-isometric cases)
  - Mode collapse in morphological transfer: Agent may converge to different element of isometry class (e.g., backward walking); try multiple seeds

- First 3 experiments:
  1. **Sanity check—rigid transformation**: Implement PointMass Maze with reflection; verify GWIL recovers optimal path without external reward. This directly tests Theorem 1.
  2. **Ablation—trajectory length**: Run pendulum→cartpole transfer with T ∈ {50, 100, 200, 500}. Plot success rate vs. T to identify minimum viable trajectory length.
  3. **Cross-morphology baseline**: Run cheetah→walker transfer (5 seeds). Report forward/backward walking frequency and compare to random policy baseline. Document that optimality is not guaranteed per Remark 1.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the Gromov-Wasserstein distance be adapted to incorporate the MDP's temporal structure for better alignment?
- Basis in paper: [explicit] The conclusion states that the Gromov-Wasserstein distance "ignores the temporal information and ordering present within the trajectories" and suggests exploring metrics aware of temporal structures.
- Why unresolved: The current method treats trajectories as static sets of state-action pairs (occupancy measures), discarding the sequential nature of the transitions which might be critical for complex tasks.
- What evidence would resolve it: A modified GWIL method using temporal metrics (e.g., dynamic time warping integration) that demonstrates improved performance on tasks where timing is crucial compared to the static baseline.

### Open Question 2
- Question: Can GWIL scale to high-dimensional environments and transfer the structure of many complex demonstrations?
- Basis in paper: [explicit] The conclusion explicitly lists "Scaling to more complex environments and agents towards the goal of transferring the structure of many high-dimensional demonstrations" as a future direction.
- Why unresolved: The experiments were limited to continuous control tasks with relatively low dimensionality (maze, pendulum, cartpole, cheetah/walker).
- What evidence would resolve it: Successful application of GWIL to high-dimensional domains (e.g., pixel-based control or complex manipulation) using multiple demonstrations, showing it retains efficiency advantages over adversarial methods.

### Open Question 3
- Question: How does the standard Gromov-Wasserstein distance compare to other optimal transport distances with global invariances?
- Basis in paper: [explicit] The authors ask "How GW compares to other optimal transport distances that work apply between two metric MDPs... that have more flexibility over how the spaces are connected."
- Why unresolved: The paper focuses specifically on the Gromov-Wasserstein formulation but acknowledges other OT distances exist that might offer different trade-offs regarding coupling flexibility and invariances.
- What evidence would resolve it: A comparative study benchmarking GWIL against methods using alternative optimal transport distances (e.g., Co-optimal transport) on the same cross-domain tasks.

### Open Question 4
- Question: How does the choice of distance metric ($d_E, d_A$) affect the success of imitation in non-isomorphic domains?
- Basis in paper: [inferred] The theoretical guarantees (Theorem 1) rely on the existence of isometries between domains based on the provided metrics, yet experiments rely on standard Euclidean metrics without ablation.
- Why unresolved: If the chosen metric does not reflect the functional similarities between the expert and agent domains, the "isomorphism" recovered by GWIL might be structurally correct but semantically meaningless.
- What evidence would resolve it: Experiments varying the distance metrics (e.g., Euclidean vs. learned metrics) and measuring the correlation between metric alignment and task performance.

## Limitations
- Theoretical guarantees rely on strong isometry assumptions that are rarely met in practical transfer scenarios
- Computational complexity scales quadratically with trajectory length, limiting applicability to long-horizon tasks
- Method requires computing pairwise distance matrices, which becomes prohibitive for high-dimensional state-action spaces

## Confidence

- **High confidence**: The mechanism of using GW distance as a proxy reward for RL optimization is well-established and the experimental results demonstrate consistent learning curves across tasks.
- **Medium confidence**: The optimality recovery under isometry conditions is theoretically sound, but the practical applicability depends on whether real-world tasks satisfy these strict conditions.
- **Low confidence**: The performance in cross-morphology scenarios without isometry guarantees is promising but lacks theoretical backing for optimality.

## Next Checks

1. **Isometry verification experiment**: Systematically test GWIL across controlled transformations (rotation, reflection, scaling) of the maze environment to quantify how well the method recovers optimal policies as isometry conditions are progressively relaxed.

2. **Scalability analysis**: Evaluate GWIL performance on tasks with increasing state-action dimensionality and trajectory length to empirically determine the practical limits of the quadratic scaling, identifying the maximum viable trajectory length for different dimensionalities.

3. **Multi-seed morphology study**: Conduct extensive testing of cheetah→walker transfer with 20+ seeds to characterize the distribution of recovered behaviors (forward vs. backward walking) and investigate whether intervention strategies can bias toward desired behaviors.