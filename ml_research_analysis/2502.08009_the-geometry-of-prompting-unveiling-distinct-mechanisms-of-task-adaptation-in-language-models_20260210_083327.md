---
ver: rpa2
title: 'The Geometry of Prompting: Unveiling Distinct Mechanisms of Task Adaptation
  in Language Models'
arxiv_id: '2502.08009'
source_url: https://arxiv.org/abs/2502.08009
tags:
- capacity
- manifold
- task
- sentence
- examples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates how different prompting methods\u2014\
  demonstrations, instructions, and soft prompts\u2014affect the geometry of internal\
  \ representations in large language models during text classification tasks. Using\
  \ a framework grounded in statistical physics, the authors analyze manifold capacity,\
  \ which quantifies the separability of category manifolds in the model\u2019s embedding\
  \ space."
---

# The Geometry of Prompting: Unveiling Distinct Mechanisms of Task Adaptation in Language Models

## Quick Facts
- **arXiv ID:** 2502.08009
- **Source URL:** https://arxiv.org/abs/2502.08009
- **Reference count:** 40
- **Primary result:** Different prompting methods (demonstrations, instructions, soft prompts) reshape LLM internal representations through distinct geometric mechanisms, with demonstrations improving intermediate feature separability while instructions primarily affect final outputs.

## Executive Summary
This study investigates how different prompting methods affect the geometry of internal representations in large language models during text classification tasks. Using a framework grounded in statistical physics, the authors analyze manifold capacity, which quantifies the separability of category manifolds in the model's embedding space. The results reveal that while demonstrations and instructions achieve similar performance, they operate through distinct representational mechanisms. Demonstrations reshape intermediate representations, improving feature separability, while instructions primarily influence the final output stage. Soft prompts, optimized through gradient descent, affect later layers and exhibit unique trade-offs in representational geometry. The study highlights the critical role of label semantics and input distribution examples in task adaptation, providing insights into the internal dynamics of in-context learning and laying the groundwork for representation-aware prompting strategies.

## Method Summary
The study analyzes how instruction prompts, demonstration examples, and soft prompts affect internal representation geometry in decoder-only LLMs for text classification. The authors extract residual stream activations at all layers, computing sentence embeddings (mean-pooled input token activations) and last-token embeddings. They measure geometric properties including manifold capacity, participation ratio, and radius across three prompt types on synthetic multi-task datasets and control datasets (AG News, TREC). Soft prompts are trained for 30 epochs using Adam optimization. The analysis framework builds on statistical physics concepts to quantify how prompts reshape the representational geometry of category manifolds.

## Key Results
- Demonstrations and instructions achieve similar accuracy but operate through distinct mechanisms: demonstrations reshape intermediate representations while instructions primarily affect the final output stage
- Soft prompts exhibit unique trade-offs in representational geometry and predominantly affect later layers
- Manifold capacity analysis reveals that demonstrations significantly improve feature separability in intermediate layers
- The geometry of internal representations is strongly influenced by label semantics and input distribution examples

## Why This Works (Mechanism)
The study reveals that different prompting methods leverage distinct mechanisms for task adaptation in LLMs. Demonstrations work by providing concrete examples that reshape the model's intermediate representations, effectively "teaching" the model how to transform input features into separable manifolds for different categories. Instructions, conversely, operate primarily at the output stage by conditioning the model's final predictions without substantially altering the underlying feature representations. Soft prompts represent a third mechanism, where gradient-optimized continuous vectors modify the model's internal states in a more targeted fashion, particularly affecting later layers. This geometric perspective explains why demonstrations are more effective for tasks requiring complex input transformations, while instructions suffice for simpler mapping tasks.

## Foundational Learning
- **Manifold Capacity (Î±):** A measure from statistical physics quantifying the maximum number of linearly separable classes that can exist in a high-dimensional space. Why needed: Central metric for analyzing how prompts reshape representational geometry. Quick check: Verify that MC decreases as task difficulty increases or when prompts are removed.
- **Participation Ratio:** A measure of manifold dimension that quantifies the effective dimensionality of data clusters in embedding space. Why needed: Complements capacity by revealing the intrinsic dimensionality of learned representations. Quick check: Compare PR values across layers to identify where dimensionality reduction occurs.
- **Residual Stream:** The continuous vector space through which information flows between transformer layers. Why needed: The primary substrate where prompt effects manifest geometrically. Quick check: Track how activation norms change across layers for different prompt types.
- **Sentence Embeddings vs Last-token Embeddings:** Two distinct ways of extracting representations from LLM outputs. Why needed: Enables layer-by-layer analysis of where prompt effects occur. Quick check: Confirm that sentence embeddings capture input processing while last-token embeddings reflect output conditioning.
- **In-Context Learning:** The ability of LLMs to perform tasks based on examples or instructions in the prompt without weight updates. Why needed: The phenomenon being analyzed geometrically. Quick check: Compare performance and geometry between in-context and fine-tuned approaches.

## Architecture Onboarding

**Component Map:** Input Text -> Token Embeddings -> Transformer Layers -> Residual Stream -> Prompt Processing -> Output Distribution

**Critical Path:** The residual stream activations at each layer represent the critical path for geometric analysis, as this is where prompt effects manifest most clearly in the representational space.

**Design Tradeoffs:** The study trades computational efficiency for geometric insight by extracting activations at all layers rather than just final outputs. This comprehensive approach reveals layer-specific effects but requires substantial storage and processing. The choice of mean-pooled sentence embeddings versus last-token representations represents another tradeoff between capturing input processing versus output conditioning.

**Failure Signatures:** If prompt effects are not observed in intermediate layers despite good performance, this suggests the model is relying primarily on shallow pattern matching or output conditioning rather than meaningful representation transformation. Conversely, if intermediate layers show strong prompt effects but performance is poor, the model may be creating separable manifolds that don't align with the true task structure.

**First Experiments:**
1. Compare manifold capacity across layers for zero-shot versus few-shot prompts to identify where demonstrations have the strongest geometric impact
2. Analyze participation ratio changes when adding demonstrations to identify dimensionality effects
3. Track manifold radius evolution across layers to understand how prompt types affect the compactness of learned representations

## Open Questions the Paper Calls Out
None

## Limitations
- The specific computational implementation of Manifold Capacity requires external code or re-derivation, creating a potential reproducibility barrier
- Results are limited to decoder-only models (Llama3.1-8b, Gemma2-2b), leaving questions about generalization to other architectures
- The focus on text classification tasks means findings may not extend to generation, reasoning, or other complex NLP tasks

## Confidence
- **High confidence:** The geometric trends showing demonstrations reshape intermediate representations while instructions primarily affect the final stage are robust across datasets and model scales
- **Medium confidence:** The specific numerical values of manifold capacity and participation ratio, given uncertainties in the MC computation methodology
- **Medium confidence:** The soft prompt training dynamics and their layer-specific effects, as these depend on both the optimization process and the specific initialization scheme described

## Next Checks
1. Implement and validate the Manifold Capacity calculation by implementing the critical dimension finding algorithm from Chou et al. (2024) and testing it on synthetic manifolds with known separability properties before applying to prompt analysis

2. Replicate the synthetic dataset generation by using the provided templates to generate a new dataset with Claude 3.5 or another model, then compare geometric patterns and performance metrics to ensure robustness across dataset instantiations

3. Test additional model architectures by running the same prompting and geometric analysis on an encoder-decoder model (e.g., T5) or a different decoder-only model family to assess whether the demonstrated geometric patterns generalize beyond the Llama3.1/Gemma2 scope