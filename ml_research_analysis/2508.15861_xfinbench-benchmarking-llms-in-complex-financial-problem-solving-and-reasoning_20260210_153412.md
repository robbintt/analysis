---
ver: rpa2
title: 'XFinBench: Benchmarking LLMs in Complex Financial Problem Solving and Reasoning'
arxiv_id: '2508.15861'
source_url: https://arxiv.org/abs/2508.15861
tags:
- question
- answer
- financial
- task
- latexit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces XFinBench, a benchmark with 4,235 examples
  designed to evaluate large language models'' (LLMs) ability in solving complex,
  knowledge-intensive financial problems across diverse graduate-level finance topics
  with multi-modal context. The benchmark identifies five core capabilities: terminology
  understanding, temporal reasoning, future forecasting, scenario planning, and numerical
  modelling.'
---

# XFinBench: Benchmarking LLMs in Complex Financial Problem Solving and Reasoning

## Quick Facts
- arXiv ID: 2508.15861
- Source URL: https://arxiv.org/abs/2508.15861
- Reference count: 40
- Key outcome: o1 achieves 67.3% accuracy, 12.5% behind human experts, with significant gaps in temporal reasoning and scenario planning.

## Executive Summary
This paper introduces XFinBench, a comprehensive benchmark with 4,235 examples designed to evaluate large language models' (LLMs) capabilities in solving complex, knowledge-intensive financial problems across diverse graduate-level finance topics with multi-modal context. The benchmark identifies five core capabilities: terminology understanding, temporal reasoning, future forecasting, scenario planning, and numerical modeling. Experiments on 18 leading models reveal that while o1 achieves the best performance among text-only models with 67.3% accuracy, it still lags significantly behind human experts (12.5% gap), especially in temporal reasoning and scenario planning. The authors also construct a knowledge bank with 3,032 finance terms, finding that relevant knowledge only brings consistent accuracy improvements to smaller open-source models. Error analysis reveals that rounding errors during calculation and blindness to position and intersection of curves in images are primary issues affecting performance in calculation and visual-context questions, respectively.

## Method Summary
XFinBench evaluates LLMs on three tasks: Statement Judging (True/False), Multi-choice QA, and Financial Calculation, covering five capabilities including temporal reasoning and scenario planning. The benchmark uses a Generate-then-verify framework with GPT-4o for initial question generation followed by human verification. Chain-of-Thought (CoT) prompting is used for all tasks, with Program-of-Thought (PoT) additionally employed for Financial Calculation where models generate Python code. Knowledge augmentation is tested using BM25 and Ada Embedding retrievers against a Knowledge Bank of 3,032 finance terms. The dataset includes 1,000 validation examples and 3,235 test examples with accuracy metrics including Exact Match and AccERR@5 (within 0.5% tolerance for calculations).

## Key Results
- o1 achieves 67.3% accuracy, the highest among text-only models but still 12.5% behind human experts
- Knowledge augmentation consistently improves smaller open-source models (e.g., Llama-3.1-8B) but often degrades performance for larger models (GPT-4o, Llama-3.1-405B)
- PoT prompting eliminates rounding errors but suffers from low execution rates, particularly affecting open-source models
- Visual-context questions show "blindness" to curve intersections, with models correctly identifying labels but failing to reason about spatial relationships
- Rounding errors and visual blindness are identified as primary failure modes in calculation and visual-context questions respectively

## Why This Works (Mechanism)

### Mechanism 1: Knowledge-Scaling Asymmetry in Augmentation
Smaller models suffer from a density deficit in internal parameterized knowledge. Retrieval-Augmented Generation (RAG) using a "Knowledge Bank" bridges this gap by offloading terminology and formula lookup. Larger models, already saturated with parametric knowledge, are prone to "Over Thinking" or "Over Reliance" when forced to process external context, disrupting their internal reasoning chains. The mechanism breaks if the retriever has low precision or if the prompt format causes the model to hallucinate contradictions between retrieved context and internal weights.

### Mechanism 2: Reasoning-Execution Trade-off in Program-of-Thought (PoT)
PoT shifts the burden from semantic calculation (prone to rounding errors) to syntactic code generation. If the model generates syntactically invalid Python or logic errors, execution fails, resulting in zero credit. CoT allows partial credit or approximate answers even with arithmetic flaws, making it more robust for models with high semantic reasoning but lower coding precision. The mechanism fails if the execution environment handles errors gracefully or if the model's code generation error rate approaches zero.

### Mechanism 3: Visual-Spatial Blindness in Multimodal Reasoning
Current vision encoders can identify labels ("Supply", "Demand") but fail to spatially bind these concepts to the relative position of curve intersections required for scenario planning. The model sees the "text" in the image but hallucinates the spatial relationship. The mechanism implies that simply scaling vision resolution won't fix the error; it requires improved spatial-reasoning architectures or specific pre-training on curve-intersection dynamics.

## Foundational Learning

- **Concept: Program-of-Thought (PoT) vs. Chain-of-Thought (CoT)**
  - Why needed: The paper explicitly compares these two prompting strategies for financial calculations.
  - Quick check: If a model has 95% reasoning accuracy but a 50% syntax error rate in Python, which prompting strategy (CoT or PoT) will yield a higher benchmark score?

- **Concept: Knowledge Augmentation (RAG)**
  - Why needed: The authors constructed a "Knowledge Bank" of 3,032 finance terms to test augmentation effects.
  - Quick check: Why would providing a GPT-4o class model with the exact correct definition of "American call option" potentially reduce its accuracy on a calculation problem?

- **Concept: Visual-Spatial Reasoning in MLLMs**
  - Why needed: XFinBench tests reasoning about curve positions and intersections, not just label recognition.
  - Quick check: Describe the difference between identifying a "Supply Curve" label in an image and identifying "the intersection of the Supply Curve and the shifted Demand Curve."

## Architecture Onboarding

- **Component map**: Textbook PDFs -> OCR -> GPT-4o (Generation) -> Human Verification (Validation) -> Dataset
- **Critical path**: The "Generate-then-verify" framework (Section 3.2) is the most brittle step, relying on GPT-4o to convert open-ended textbook questions into multiple-choice or specific calculation problems.
- **Design tradeoffs**: CoT vs. PoT tradeoff between logical robustness and arithmetic precision; Oracle vs. Dense Retrieval tradeoff between theoretical maximum performance and realistic system performance.
- **Failure signatures**: Rounding Error (Calculation) - Model logic is correct but intermediate float precision degrades final answer; Blindness (Visual) - Model describes labels but fabricates spatial relationships; Over-Thinking (Large Models) - Model hallucinates complexities beyond scope of provided knowledge.
- **First 3 experiments**: 
  1. Reproduce the Model Scaling Law by running benchmark on Llama-3.1-8B and Llama-3.1-405B with and without Oracle Knowledge Bank.
  2. Run Financial Calculation task using PoT on target model and plot "Execution Rate" vs. "Accuracy" to determine if execution rate < 80% requires fallback to CoT.
  3. Isolate Multi-choice question answering task with images and run "Caption-only" baseline to quantify performance loss due to visual processing vs. reasoning.

## Open Questions the Paper Calls Out

### Open Question 1
Why does knowledge augmentation via retrieval consistently improve performance for smaller open-source models (e.g., Llama-3.1-8B) but often result in performance degradation or negligible gains for larger proprietary models (e.g., GPT-4o, o1)? The paper identifies error types (over-thinking/over-reliance) but does not offer a mechanism for why large models are more susceptible to these failure modes during retrieval-augmented generation in finance.

### Open Question 2
How can the specific "blindness" to curve intersections in financial diagrams be mitigated in Multimodal LLMs? While the error is categorized, the paper does not propose architectural or training interventions to fix the specific inability to detect intersections, which is critical for supply-demand or time-series analysis.

### Open Question 3
How can the execution failure rate in Program-of-Thought (PoT) prompting be reduced to effectively address rounding errors in financial calculations? The paper highlights a trade-off: CoT is robust but prone to rounding errors, while PoT solves rounding errors but fails to execute.

## Limitations
- Knowledge augmentation asymmetry is based on limited model comparisons without ablation studies isolating reasoning depth vs. knowledge interference.
- Visual-spatial reasoning claims are primarily supported by qualitative error examples rather than quantitative spatial reasoning metrics.
- Generalizability of financial domain limitations to other knowledge-intensive domains remains extrapolative.

## Confidence

- **High Confidence**: Dataset construction methodology, overall benchmarking approach (3 tasks, 5 capabilities), and core finding that o1 achieves 67.3% accuracy while humans score ~80%.
- **Medium Confidence**: Error analysis patterns and performance trends across model families are consistent with presented results but require replication to confirm.
- **Low Confidence**: Theoretical mechanisms explaining why knowledge augmentation helps small models more and why visual-spatial reasoning fails lack direct experimental validation.

## Next Checks

1. **Knowledge Augmentation Ablation**: Replicate knowledge augmentation experiment with wider range of model sizes and explicitly measure "reasoning depth" by analyzing length and complexity of model responses with/without context to test over-thinking hypothesis.

2. **Visual Reasoning Control**: Conduct controlled experiment where financial charts are pre-processed into structured text descriptions of curves and their intersections. Compare model performance on image-based vs. text-based chart representations to isolate visual processing bottleneck.

3. **Cross-Domain Generalization**: Apply XFinBench evaluation methodology (CoT vs. PoT, Knowledge Augmentation) to a non-financial domain with similar characteristics (e.g., graduate-level physics or medical diagnosis) to test if observed model limitations are domain-specific or general LLM reasoning bottlenecks.