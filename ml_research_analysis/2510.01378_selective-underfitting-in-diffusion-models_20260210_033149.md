---
ver: rpa2
title: Selective Underfitting in Diffusion Models
arxiv_id: '2510.01378'
source_url: https://arxiv.org/abs/2510.01378
tags:
- score
- training
- diffusion
- region
- supervision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Diffusion models are trained to approximate a score function but\
  \ this leads to a paradox: perfect learning would just reproduce training data,\
  \ not create new samples. The authors argue that models solve this by selectively\
  \ underfitting\u2014accurately learning the score only in a restricted \"supervision\
  \ region\" (near training data) and extrapolating differently elsewhere."
---

# Selective Underfitting in Diffusion Models

## Quick Facts
- arXiv ID: 2510.01378
- Source URL: https://arxiv.org/abs/2510.01378
- Reference count: 40
- One-line primary result: Models generalize by selectively underfitting—accurately learning scores near training data while extrapolating differently elsewhere.

## Executive Summary
Diffusion models face a paradox: perfect learning of the score function would just reproduce training data rather than create novel samples. This paper resolves this by showing models selectively underfit—accurately learning the score only within a restricted "supervision region" near training data while extrapolating differently elsewhere. This geometric separation between supervision and extrapolation regions enables generalization. The authors formalize this region, demonstrate through experiments that larger models improve fit inside this region but underfit more outside, and show that the size of this region affects memorization versus generalization.

## Method Summary
The core method involves training diffusion models with modified objectives that explicitly control the supervision region versus extrapolation region. The key technical contribution is defining the supervision region as thin shells around training data where the noisy distribution concentrates (Proposition 3.2). Training uses importance sampling with distinct score and region distributions (Eq 7), allowing controlled variation of supervision coverage. Models are evaluated using SiT transformers (S/B/L/XL) on ImageNet 256×256, with metrics including FID, supervision loss, and memorization ratio. The empirical score computation uses FAISS GPU indexing for efficiency. ODE sampling with dopri5 provides high-quality inference for evaluation.

## Key Results
- Larger models show improved supervision loss (better fit to empirical score in supervision regions) but increased deviation in extrapolation regions
- Varying supervision region size reveals a trade-off: larger regions lead to memorization while smaller regions enable generalization
- REPA improves extrapolation efficiency (shifts FID-supervision loss curve downward) without changing supervision loss
- The "freedom of extrapolation" principle shows that generalization depends on having sufficient region to define the score function outside training data

## Why This Works (Mechanism)

### Mechanism 1: Supervision Region Concentration
Diffusion models are effectively supervised to fit the empirical score only within thin spherical shells around training data rather than globally. In high dimensions, the noisy training distribution concentrates probability mass on these shells (Proposition 3.2). This creates a restricted supervision region where the model learns the empirical score accurately.

### Mechanism 2: Freedom of Extrapolation
Generalization depends on the model having freedom to define the score function outside the supervision region. If the supervision region is kept small, the model has vast extrapolation regions where it can deviate from the empirical score, enabling novel sample generation. Enlarging the supervision region forces the model to fit the empirical score in more places, leading to memorization.

### Mechanism 3: Decomposed Scaling Law (PAT)
Generative performance (FID) is determined by a product of supervision efficiency (fitting the shells) and extrapolation efficiency (how that fit translates to FID), formalized as FID = f_extrapolation(L_supervision). Architectural choices or training recipes can be analyzed by plotting FID vs. supervision loss, with downward shifts indicating improved extrapolation efficiency.

## Foundational Learning

- **Concept:** Empirical Score Function (s*)
  - Why needed here: This is the ground truth target the model is trained on. The thesis relies on measuring deviation of learned score from this empirical score inside vs. outside supervision region.
  - Quick check question: Can you write out the analytic formula for the empirical score s*(z_t, t) for a finite dataset {x^(i)} (Eq 2)?

- **Concept:** Denoising Score Matching (DSM)
  - Why needed here: The standard training objective. The paper argues DSM supervises locally on p̂_t rather than globally.
  - Quick check question: In standard DSM, what distribution are the noisy inputs z_t sampled from, and why does that create the "supervision region"?

- **Concept:** High-Dimensional Concentration of Measure
  - Why needed here: Justifies why the supervision region is "thin shells." Without this, training distribution would cover space, invalidating selective argument.
  - Quick check question: In d-dimensions, why does the volume of a Gaussian concentrate on a thin shell of radius σ√d rather than the center?

## Architecture Onboarding

- **Component map:** Empirical Score Oracle -> Region Classifier -> Score Network (s_θ)
- **Critical path:**
  1. Validation Pipeline: Implement logic to compute r* and split evaluation metrics (Score Error) by "Supervised" vs. "Extrapolated" regions
  2. Training Abstraction: Modify training loop to allow distinct D_score and D_region as in Experiment 4.3
  3. PAT Integration: Add projection head or regularization term to align representations

- **Design tradeoffs:**
  - U-Nets vs. Transformers: U-Nets have better extrapolation efficiency but worse supervision efficiency; Transformers fit easily but extrapolate "wastefully"
  - Supervision Region Size: Small regions → Better generalization but risk underfitting; Large regions → Memorization

- **Failure signatures:**
  - Memorization without Generalization: Check if validation r* ≈ 1 everywhere or if supervision loss is near zero but FID is poor
  - Failed Extrapolation: Supervision loss drops but FID plateaus (indicates poor extrapolation efficiency)

- **First 3 experiments:**
  1. Verify Inference Distribution: Train model, sample inference trajectories, plot histogram of r* compared to training distribution r* ≈ 1
  2. Selective Underfitting Check: Compute ||s_θ - s*||² separately for r* ≈ 1 and r* > 1, verify larger models decrease error in supervision region but increase deviation in extrapolation region
  3. Ablate Freedom of Extrapolation: Run Experiment 4.3 varying |D_region| while fixing |D_score|, plot Memorization Ratio vs. |D_region|/|D_score|

## Open Questions the Paper Calls Out

- **Open Question 1:** What theoretical mechanisms specifically shape the score function within the extrapolation region? The paper establishes extrapolation happens but doesn't derive how the model determines the specific vector field in this unsupervised region.

- **Open Question 2:** How can we design training algorithms that optimally balance supervision efficiency and extrapolation efficiency? The paper demonstrates the trade-off but doesn't formulate a method to find optimal equilibrium between the two.

- **Open Question 3:** Does the selective underfitting framework generalize to other generative paradigms, such as GANs or Energy-Based Models? The concept is derived specifically from diffusion's Gaussian noise corruption and is untested on non-diffusion models.

## Limitations

- The geometric shell concentration argument relies critically on high-dimensional concentration of measure, which is mathematically derived rather than empirically validated across varying dimensions
- The memorization-generalization trade-off demonstration uses a synthetic setup with fixed score and 100K data points, requiring further validation on real-world models and datasets
- The PAT decomposition framework is primarily validated on existing REPA results rather than through controlled ablation studies that isolate architectural vs. training recipe effects

## Confidence

- **High Confidence:** The empirical observation that larger models show improved supervision loss but degraded extrapolation efficiency (Master 4.1)
- **Medium Confidence:** The selective underfitting mechanism and freedom of extrapolation principle, though the geometric concentration assumption is mathematically derived rather than empirically validated
- **Low Confidence:** The PAT framework as a complete unifying theory, as its predictive power for novel architecture/training recipe design remains largely theoretical

## Next Checks

1. **Concentration Verification:** Systematically measure the empirical distribution of noisy training samples z_t across different dimensions d and noise levels σ_t to verify the shell concentration hypothesis rather than assuming it holds.

2. **Real-World Memorization Test:** Apply the controlled supervision region experiment (4.3) to a full-scale ImageNet diffusion model, varying the supervision coverage while monitoring both memorization ratio and FID to validate the generalization trade-off holds in practical settings.

3. **PAT Predictive Validation:** Use the decomposed scaling framework to predict performance of a novel architecture or training modification, then validate whether changes in supervision efficiency vs. extrapolation efficiency actually predict changes in FID as the theory suggests.