---
ver: rpa2
title: Can Smaller LLMs do better? Unlocking Cross-Domain Potential through Parameter-Efficient
  Fine-Tuning for Text Summarization
arxiv_id: '2509.01314'
source_url: https://arxiv.org/abs/2509.01314
tags:
- domain
- datasets
- adapters
- dataset
- domains
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Can Smaller LLMs do better? Unlocking Cross-Domain Potential through Parameter-Efficient Fine-Tuning for Text Summarization

## Quick Facts
- **arXiv ID:** 2509.01314
- **Source URL:** https://arxiv.org/abs/2509.01314
- **Reference count:** 18
- **Primary result:** Smaller LLMs with PEFT adapters can outperform much larger models on domain-specific summarization

## Executive Summary
This paper investigates whether parameter-efficient fine-tuning (PEFT) adapters can enable smaller language models (8B parameters) to match or exceed the performance of much larger models (70B parameters) on domain-specific text summarization. The authors explore both within-domain adaptation (training on same domain) and cross-domain transfer (training on high-resource domains to help low-resource domains). Through systematic benchmarking across six PEFT methods and fourteen datasets, they demonstrate that specialized adapter modules can capture domain-specific patterns effectively, with single adapters often outperforming combinations.

## Method Summary
The method involves training PEFT adapters (AdaLoRA, (IA)³, LoHA, LoKr, LoRA, OFT) on various summarization datasets using a frozen Llama-3-8B-Instruct base model. Each adapter is trained with rank=64, alpha=8, learning rate=5e-4, cosine decay, and BF16 precision for 5 epochs. The adapters are then evaluated using a Borda Count ranking across five metrics (ROUGE, BERTScore, BLEU, METEOR, FActScore) on both training datasets and holdout validation sets from four domains (Medical, Legal, News, Scientific). The key innovation is leveraging high-resource datasets to create adapters that can transfer to low-resource domains with similar linguistic characteristics.

## Key Results
- Within-domain adapters enable 8B Llama-3 to outperform 70B Llama-3-Instruct zero-shot on domain-specific summarization (ROUGE improvements of 0.02-0.05)
- Cross-domain adapters trained on linguistically similar high-resource datasets can generalize to low-resource domains, with performance degrading when KL divergence exceeds ~15
- Single-dataset adapters consistently outperform combinations of multiple adapters, suggesting specialization is more effective than ensemble approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PEFT adapters trained on high-resource datasets can generalize to low-resource domains through linguistic commonality transfer.
- Mechanism: Adapter layers (trained on <1% of parameters) capture domain-specific vocabulary patterns, document structures, and summarization styles. When deployed on low-resource domains with similar linguistic characteristics (e.g., medical→scientific, legal→news), these patterns partially transfer, improving generation quality over zero-shot baselines.
- Core assumption: Target low-resource domains share sufficient vocabulary overlap or structural similarity with the high-resource training domain.
- Evidence anchors:
  - [abstract] "We leverage parameter-efficient fine-tuning techniques (PEFTs) on high-resource datasets to address these challenges to improve performance on unseen low-resource domains."
  - [section 5.2] "Adapters trained on datasets exhibiting higher similarity with the holdout validation sets achieve superior results... In the Medical domain, adapters such as CORD19-OFT and MSLR-(IA)³ demonstrate strong alignment with the Medical domain evaluation set."
  - [corpus] Limited direct corpus support for this specific mechanism; neighbor papers focus on cross-domain recommendation (arXiv:2503.07761) and low-resource language adaptation (arXiv:2502.10140) but not summarization-specific transfer.
- Break condition: If KL divergence between training and target domains exceeds ~15 (see Table 5), or vocabulary/contextual overlap drops below ~15%, performance degrades significantly (e.g., CNN/DM→Medical: KL=13.62, overlap=8.89%).

### Mechanism 2
- Claim: Within-domain PEFT adapters enable smaller LLMs (8B parameters) to outperform much larger models (70B parameters) on domain-specific summarization.
- Mechanism: Domain-specific adapters specialize the model's attention and output distributions toward domain-appropriate vocabulary and summary structures, compensating for the smaller model's reduced general capacity. The paper shows ROUGE improvements of 0.02-0.05 over Llama-3-70B-Instruct zero-shot.
- Core assumption: Labeled in-domain training data exists (at least 1000 samples as per experimental setup).
- Evidence anchors:
  - [abstract] "Our experiments show that for low-resource domains, inference using Within-Domain Adapters can achieve better performance than Few-Shot as well as a much larger Llama-3-70B-Instruct."
  - [section 5.3, Table 6] Scientific domain: SciTLDR-AdaLoRA achieves ROUGE 0.2746 vs. Llama3-70B 0.2311; Medical: CORD19-OFT 0.2856 vs. 70B 0.2527; Legal: MultiLex-LoKr 0.2411 vs. 70B 0.2158.
  - [corpus] No direct corpus validation for this specific claim; neighbor papers do not compare PEFT-adapted small models against larger model baselines.
- Break condition: When factual consistency (FActScore) is the priority metric, larger models may still outperform. Table 6 shows Llama3-70B achieves FActScore 0.9797 vs. Newsroom-LoKr 0.8909 in News domain.

### Mechanism 3
- Claim: Single-dataset adapters outperform combinations of multiple adapters, even within the same domain.
- Mechanism: Merging adapters introduces interference in the learned parameter updates, reducing specialization. The paper's Borda Count rankings show top-3 adapters for each domain are always individual adapters, not combinations.
- Core assumption: A well-chosen single dataset sufficiently represents the target domain's characteristics.
- Evidence anchors:
  - [section 5.1] "Our findings reveal that, across all domains, the top-3 ranked adapters are individual (single) adapters rather than combinations... individual adapters often capture domain-specific patterns more effectively, likely due to reduced interference and greater specialization."
  - [section 5.2, Table 4] Scientific domain: Single adapter SciTLDR-AdaLoRA (ROUGE 0.2746) outperforms combined Arxiv-LoKr + SciTLDR-AdaLoRA (ROUGE 0.1698).
  - [corpus] Weak corpus support; neighbor "EigenLoRAx" (arXiv:2502.04700) discusses adapter recycling but not combination performance.
- Break condition: Adding more than 2 adapters consistently degraded performance across all tested domains.

## Foundational Learning

- Concept: PEFT (Parameter-Efficient Fine-Tuning) methods
  - Why needed here: The paper assumes familiarity with how LoRA, AdaLoRA, (IA)³, etc., modify model weights differently. Without this, you cannot interpret why AdaLoRA works well for Scientific/News but LoKr excels on ArXiv/BillSum.
  - Quick check question: Can you explain why AdaLoRA with rank=64, alpha=8 might capture different patterns than OFT with rank=64?

- Concept: Domain adaptation vs. transfer learning
  - Why needed here: The core hypothesis relies on understanding how linguistic commonalities enable cross-domain transfer. You need to distinguish between within-domain (same domain, different dataset) and cross-domain (different domain entirely) scenarios.
  - Quick check question: If you train on PubMed (medical journals) and test on legal documents, would you expect better performance than training on CNN/DM (news)? Why or why not based on Table 5's similarity metrics?

- Concept: Evaluation metrics for summarization (ROUGE, BERTScore, FActScore)
  - Why needed here: The paper uses Borda Count across 5 metrics to rank adapters. Understanding trade-offs (e.g., ROUGE vs. FActScore) is critical for interpreting Table 4 and Table 6 results.
  - Quick check question: If an adapter improves ROUGE but decreases FActScore, what does this tell you about the quality of generated summaries?

## Architecture Onboarding

- Component map: Llama-3-8B-Instruct base model -> PEFT adapters (AdaLoRA, (IA)³, LoHA, LoKr, LoRA, OFT) -> Adapter storage (individual modules per dataset) -> Inference pipeline (load base model → attach adapter(s) → generate with 256 max tokens)

- Critical path:
  1. Select high-resource training dataset from target domain (or linguistically similar domain)
  2. Benchmark PEFT methods on validation split (use 1000 train/500 val samples minimum)
  3. Rank PEFTs using Borda Count across ROUGE, BERTScore, BLEU, METEOR, FActScore
  4. Select top-1 single adapter (not combination) as domain representative
  5. Evaluate on holdout validation data dated after base model's training cutoff

- Design tradeoffs:
  - Within-domain vs. cross-domain: Within-domain adapters outperform (Table 4), but cross-domain is viable fallback when no in-domain data exists
  - Adapter combination: Paper shows combinations degrade performance; prefer single best adapter
  - Metric prioritization: ROUGE/BERTScore favor adapted models; FActScore sometimes favors larger zero-shot models (Table 6, News domain)
  - PEFT selection: AdaLoRA generalizes best across domains, but dataset-specific winners vary (Table 3: LoKr for ArXiv, OFT for Multi-News)

- Failure signatures:
  - KL divergence >15 between training and target data → expect degraded cross-domain performance
  - Combining 3+ adapters → "diminishing returns or even degradation in scores" (Section 5.2)
  - Low vocabulary overlap (<20%) → cross-domain adapters underperform within-domain by 0.03-0.08 ROUGE
  - High perplexity during training ≠ poor performance: (IA)³ showed higher perplexity but strong post-training evaluation (Appendix C)

- First 3 experiments:
  1. Replicate single-dataset adapter benchmark on your domain: Train all 6 PEFTs on 1000 samples, evaluate on held-out test set, rank via Borda Count. Expect ~4-8 hour training on A100 80GB per PEFT.
  2. Test within-domain vs. cross-domain transfer: If targeting medical summarization with limited data, train on CORD19/PubMed (within-domain) vs. ArXiv (cross-domain). Measure ROUGE gap; expect ~0.02-0.05 difference based on Table 4.
  3. Validate single-adapter superiority over combinations: Merge top-2 adapters from your domain (e.g., via weight averaging or sequential loading if framework supports), compare against best single adapter. Expect single adapter to match or exceed combination performance.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can adapters be best selected, weighted, or merged to maximize performance in complex domain transfer scenarios?
- **Basis in paper:** [explicit] The conclusion states that "further research is needed to understand how to best select, weight, or merge adapters for optimal performance across increasingly complex domain transfer scenarios."
- **Why unresolved:** The authors observed that combining more than two adapters often led to performance degradation or diminishing returns, failing to identify a consistent successful strategy for complex blending.
- **What evidence would resolve it:** A study evaluating specific adapter merging algorithms (e.g., linear interpolation, task arithmetic) or dynamic weighting mechanisms on diverse cross-domain datasets.

### Open Question 2
- **Question:** Does the efficacy of cross-domain PEFT adapters generalize to tasks beyond text summarization?
- **Basis in paper:** [explicit] The limitations section notes that the experiments were "limited to a single... task" and suggests future work could explore "tasks beyond summarization."
- **Why unresolved:** The benchmark and methodology were exclusively tailored to summarization; it remains unknown if the linguistic commonalities leveraged for summarization apply equally to tasks like question answering or reasoning.
- **What evidence would resolve it:** Applying the same Within-Domain and Cross-Domain PEFT methodology to distinct tasks such as question answering or text classification.

### Open Question 3
- **Question:** Does the performance of cross-domain adapters remain consistent across different LLM architectures?
- **Basis in paper:** [explicit] The authors explicitly list as a limitation that "storage limitations confined our evaluation to a single Large Language Model (LLM)" and suggest exploring "a broader range of model architectures."
- **Why unresolved:** The results are specific to Llama-3-8B-Instruct; the impact of different attention mechanisms or parameter scales on the cross-domain adapter transfer was not tested.
- **What evidence would resolve it:** Replicating the specific PEFT benchmark (AdaLoRA, LoKr, etc.) on alternative base models such as Mistral, Gemma, or larger Llama variants.

### Open Question 4
- **Question:** How effective is high-resource proxy domain adaptation for truly underserved or marginalized linguistic contexts?
- **Basis in paper:** [explicit] The ethical statement acknowledges that the chosen domains "may not comprehensively represent marginalized or linguistically diverse communities" and calls for investigation into contexts where domain boundaries may not align.
- **Why unresolved:** The study assumes the existence of a similar high-resource dataset to act as a proxy, which is an assumption that may fail for distinct, low-resource linguistic communities.
- **What evidence would resolve it:** Evaluating adapter performance on datasets from marginalized dialects or low-resource languages that lack a clearly related high-resource training set.

## Limitations
- **Unclear adapter merging mechanism**: The paper claims single adapters outperform combinations but doesn't specify the exact merging algorithm used, which could fundamentally affect results.
- **Limited cross-domain generalization**: While transfer works for medical→scientific and legal→news pairs, performance degrades sharply when KL divergence exceeds 15, limiting applicability to arbitrary domain pairs.
- **Factual consistency trade-offs**: Larger models sometimes outperform on FActScore despite PEFT adapters winning on ROUGE/BERTScore, suggesting potential quality trade-offs not fully explored.

## Confidence
- **High Confidence**: Within-domain adapter performance (Table 4 results are robust and well-validated with clear baselines)
- **Medium Confidence**: Cross-domain transfer mechanism (supported by similarity metrics but limited to 4 test domains)
- **Medium Confidence**: Single-adapter superiority (rankings are clear but merging mechanism unspecified)
- **Low Confidence**: Generalizability to arbitrary domain pairs (only tested on specific medical/legal/news/scientific combinations)

## Next Checks
1. **Test Cross-Domain Generalization**: Apply adapters trained on medical (CORD19) to a new domain (e.g., financial news) and measure performance degradation relative to KL divergence between training and target domains.

2. **Implement Alternative Adapter Merging**: Test different adapter combination methods (weight averaging, task arithmetic, sequential application) to verify if single adapters truly outperform or if merging algorithm choice is critical.

3. **Validate Factual Consistency Trade-offs**: Generate summaries using top PEFT adapters and Llama-3-70B zero-shot, then conduct human evaluation specifically focused on factual accuracy vs. fluency to quantify the ROUGE/FActScore trade-off.