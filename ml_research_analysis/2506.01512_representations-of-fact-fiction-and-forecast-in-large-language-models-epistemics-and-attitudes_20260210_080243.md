---
ver: rpa2
title: 'Representations of Fact, Fiction and Forecast in Large Language Models: Epistemics
  and Attitudes'
arxiv_id: '2506.01512'
source_url: https://arxiv.org/abs/2506.01512
tags:
- peanut
- llms
- slot
- number
- parameters
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper evaluates the linguistic knowledge of epistemic modality
  in large language models (LLMs) by designing controlled stories to test their ability
  to generate appropriate epistemic expressions. The experiments focus on two main
  types of epistemic expressions: modal auxiliaries (may/might vs must/have to) and
  attitude verbs (know, believe, doubt).'
---

# Representations of Fact, Fiction and Forecast in Large Language Models: Epistemics and Attitudes

## Quick Facts
- **arXiv ID:** 2506.01512
- **Source URL:** https://arxiv.org/abs/2506.01512
- **Authors:** Meng Li; Michael Vrazitulis; David Schlangen
- **Reference count:** 40
- **Primary result:** LLMs show limited and non-robust performance in generating appropriate epistemic expressions, with medium-parameter models outperforming small-parameter ones

## Executive Summary
This paper evaluates how well large language models handle epistemic modality - the linguistic expression of uncertainty and knowledge states - through controlled story generation tasks. The study focuses on two key types of epistemic expressions: modal auxiliaries (may/might vs must/have to) and attitude verbs (know, believe, doubt). Results reveal that LLMs struggle with reliably expressing uncertainty, particularly with possibility modals and belief statements, though medium-parameter models (70-72B) perform significantly better than smaller ones (7-8B).

The findings suggest that current LLMs lack robust semantic knowledge of epistemic modality, making their expressions of uncertainty unreliable in many contexts. This limitation has important implications for applications requiring accurate handling of knowledge states, predictions, and degrees of certainty. The study concludes that enriching LLMs' semantic representations of epistemic modality is necessary for building more reliable uncertainty-aware systems.

## Method Summary
The researchers designed controlled stories to test LLMs' ability to generate appropriate epistemic expressions in different contexts. They focused on two main types of epistemic expressions: modal auxiliaries (may/might vs must/have to) and attitude verbs (know, believe, doubt). The experiments used two LLMs (GPT-4 and Llama2-7B/70B) and evaluated their performance across different parameter sizes. Stories were crafted to elicit specific epistemic expressions based on the certainty of information presented, allowing researchers to test whether models could appropriately distinguish between factual knowledge, beliefs, and possibilities.

## Key Results
- LLMs' performance in generating epistemic expressions is limited and not robust across different contexts
- Medium-parameter models (70-72B) significantly outperform small-parameter models (7-8B) in epistemic expression tasks
- LLMs perform better with necessity modals (must/have to) than possibility modals (may/might)
- Models are better at reporting facts than beliefs under different degrees of epistemic certainty

## Why This Works (Mechanism)
The study reveals that LLMs' handling of epistemic modality is fundamentally limited by their semantic representations of uncertainty. The models appear to have learned statistical patterns for epistemic expressions but lack deep understanding of the semantic relationships between different knowledge states. This manifests in their inability to consistently map story contexts to appropriate epistemic markers, particularly when distinguishing between varying degrees of certainty or when expressing possibility versus necessity.

## Foundational Learning
**Epistemic modality** - Linguistic expressions that convey speaker's attitude toward the truth of a proposition. Needed to understand how uncertainty is linguistically encoded. Quick check: Can identify examples of epistemic vs deontic modality.

**Modal auxiliaries** - Verbs like may, might, must, have to that express possibility or necessity. Needed to analyze model performance with different types of epistemic expressions. Quick check: Can distinguish between epistemic and root meanings of modals.

**Attitude verbs** - Verbs like know, believe, doubt that express mental states and degrees of certainty. Needed to evaluate how models handle different knowledge states. Quick check: Can categorize attitude verbs by epistemic strength.

**Semantic competence** - The ability to understand and generate meaning-appropriate linguistic expressions. Needed to assess whether models truly understand epistemic concepts. Quick check: Can explain difference between semantic and syntactic competence.

**Epistemic certainty** - The degree of confidence or knowledge about a proposition's truth. Needed to evaluate model performance across different certainty levels. Quick check: Can rank expressions by epistemic strength.

## Architecture Onboarding

**Component Map:** Input Story → Context Analysis → Epistemic Expression Generation → Output Text

**Critical Path:** The critical path involves the model's ability to correctly interpret the epistemic context from the story and generate semantically appropriate expressions. This requires accurate semantic representation of both the input context and the target epistemic expressions.

**Design Tradeoffs:** The study highlights a tradeoff between model size and epistemic competence - larger models show better performance but still exhibit significant limitations. This suggests that simply scaling models may not be sufficient to achieve robust epistemic understanding.

**Failure Signatures:** Models consistently underperform with possibility modals compared to necessity modals, and struggle more with expressing beliefs than facts. Failures often involve inappropriate epistemic strength (too certain or too uncertain) or incorrect choice of epistemic expression type.

**Three First Experiments:**
1. Test same epistemic expressions with additional LLM families to verify if performance patterns are consistent across architectures
2. Evaluate models on naturally occurring text with epistemic expressions rather than controlled stories
3. Apply targeted fine-tuning on epistemic modality and measure performance improvements

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Limited to only two LLMs (GPT-4 and Llama2-7B/70B), restricting generalizability
- Relies exclusively on synthetic controlled stories rather than natural language contexts
- Performance varies significantly across model sizes and expression types, suggesting complex underlying factors

## Confidence
**Primary finding confidence: Medium**
- Limited model diversity reduces confidence in broad conclusions
- Artificial test conditions may not reflect real-world performance
- Performance patterns appear robust for tested models but may not generalize

## Next Checks
1. **Multi-model validation**: Replicate experiments across at least five additional LLM families to assess consistency of observed performance patterns across different architectures.

2. **Real-world corpus analysis**: Test the same epistemic expressions in naturally occurring text corpora to determine if limitations persist in ecological valid contexts.

3. **Fine-tuning intervention study**: Apply targeted fine-tuning on epistemic modality expressions and measure whether performance improvements occur, helping determine if limitations are fundamental or addressable.