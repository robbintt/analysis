---
ver: rpa2
title: Conformal Language Model Reasoning with Coherent Factuality
arxiv_id: '2505.17126'
source_url: https://arxiv.org/abs/2505.17126
tags:
- factuality
- claims
- claim
- graph
- output
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new method to ensure factuality in language
  model reasoning outputs by enforcing "coherent factuality" - requiring each step
  of reasoning to be deducible from prior steps and ground truth. The authors apply
  split conformal prediction to filter subgraphs in a deducibility graph representation
  of reasoning chains, rather than filtering individual claims independently.
---

# Conformal Language Model Reasoning with Coherent Factuality

## Quick Facts
- arXiv ID: 2505.17126
- Source URL: https://arxiv.org/abs/2505.17126
- Reference count: 40
- Primary result: Achieves 90% factuality while retaining 80% of claims on MATH and FELM datasets using coherent factuality filtering

## Executive Summary
This paper introduces a method for ensuring factuality in language model reasoning outputs by enforcing "coherent factuality" - requiring each step of reasoning to be deducible from prior steps and ground truth. The authors apply split conformal prediction to filter subgraphs in a deducibility graph representation of reasoning chains, rather than filtering individual claims independently. Their approach achieves 90% factuality while retaining 80% of claims on MATH and FELM datasets. The method produces more "legible" outputs that are easier for humans to verify, with lower false positive and false negative rates compared to baseline methods.

## Method Summary
The authors construct deducibility graphs where nodes represent claims and edges represent whether one claim can be logically derived from another. They then apply split conformal prediction to these graphs, filtering subgraphs rather than individual claims. This ensures that each retained claim is deducible from previous claims and ground truth, maintaining coherent factuality. The method uses two scoring functions - one that ignores graph structure and another that weights claims by their number of descendants. The conformal prediction framework provides theoretical guarantees that the retained subgraph will maintain factuality at the desired level.

## Key Results
- Achieves 90% factuality while retaining 80% of claims on MATH and FELM datasets
- Subgraph-based conformal prediction maintains theoretical guarantees better than post-hoc filtering of independently-calibrated outputs
- Coherently filtered outputs show lower false positive and false negative rates compared to baseline methods
- Method produces more "legible" outputs that are easier for humans to verify

## Why This Works (Mechanism)
The approach works by leveraging the inherent structure of reasoning chains through deducibility graphs. By enforcing that each claim must be deducible from prior claims and ground truth, the method prevents hallucinations and ensures logical consistency throughout the reasoning process. The conformal prediction framework provides statistical guarantees about the factuality of the retained subgraph, addressing the key limitation of post-hoc filtering methods that treat claims independently. The graph structure allows for more nuanced scoring and filtering decisions that preserve logical coherence.

## Foundational Learning
- **Deducibility Graphs**: Represent logical dependencies between claims in reasoning chains; needed to capture the structure of coherent reasoning
  - Quick check: Verify graph construction maintains acyclicity and correctly represents logical dependencies
- **Split Conformal Prediction**: Statistical method for prediction sets with guaranteed coverage; needed to provide theoretical guarantees on factuality
  - Quick check: Validate calibration using holdout sets and confirm coverage matches target level
- **Coherent Factuality**: Definition requiring each claim to be deducible from prior claims and ground truth; needed to ensure logical consistency
  - Quick check: Test that all retained claims satisfy the deducibility constraint in practice

## Architecture Onboarding

### Component Map
Ground Truth -> LLM Reasoning Chain -> Claim Extraction -> Deducibility Graph Generation -> Conformal Prediction Filtering -> Filtered Reasoning Chain

### Critical Path
Claim Extraction → Deducibility Graph Generation → Subgraph Scoring → Conformal Prediction Filtering

### Design Tradeoffs
- Graph structure vs. computational complexity: More sophisticated graph construction improves accuracy but increases computation
- Scoring function expressiveness vs. interpretability: Complex scoring functions may improve retention but reduce transparency
- Factuality guarantees vs. claim retention: Stricter factuality requirements reduce the number of retained claims

### Failure Signatures
- Cyclic deducibility graphs indicate incorrect graph construction
- Low claim retention suggests overly strict factuality requirements or poor scoring functions
- Calibration failure indicates issues with the conformal prediction implementation

### First Experiments
1. Verify deducibility graph construction on simple reasoning chains with known dependencies
2. Test conformal prediction calibration on a held-out set before applying to full reasoning chains
3. Compare claim retention rates using different scoring functions on a validation set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can more sophisticated subgraph scoring functions that better approximate the "true" underlying risk function further increase claim retention rates while maintaining conformal guarantees?
- Basis in paper: Authors state "Improvements may include scoring subsets beyond those considered by our algorithm based and accounting for additional graph structure in node heuristic measures."
- Why unresolved: Only two simple scoring functions (graph-independent and descendant-weighting) were tested; the search space of potential scoring functions remains largely unexplored
- What evidence would resolve it: Systematic evaluation of alternative scoring functions (e.g., ancestor-weighted, attention-based graph neural approaches) showing improved claim retention at matched factuality levels

### Open Question 2
- Question: Can coherent factuality guarantees be extended to code generation, where compilation provides a well-defined notion of substantiation?
- Basis in paper: Authors identify "code generation is a natural domain, as compilation is both an easy and well defined notion of coherent substantiation, and correct final outputs clearly indicate correctness."
- Why unresolved: The paper only evaluated mathematical reasoning (MATH) and verbal reasoning (FELM) tasks; code generation has distinct graph structures (dependency graphs) that may require different formalization
- What evidence would resolve it: Demonstration of calibrated coherent factuality on code generation benchmarks with dependency-graph-based filtering

### Open Question 3
- Question: Do human evaluators confirm that coherently filtered outputs are more legible (easier to verify) than baseline-filtered outputs?
- Basis in paper: Authors "defer human studies of output legibility to future works" and used GPT-4o and Llama as proxy judges for legibility
- Why unresolved: LLM-as-judge evaluation may not correlate with human judgments of verifiability; the human cognitive burden of verifying reasoning chains was not directly measured
- What evidence would resolve it: Controlled human subject study comparing verification time and accuracy between coherent and baseline filtering methods

### Open Question 4
- Question: How can graph generation be improved for longer reasoning chains where GPT-4o currently struggles?
- Basis in paper: Authors note "GPT-4o struggled with longer reasoning outputs containing many claims, raising concerns about practicality for multi-step problems."
- Why unresolved: The scalability of LLM-generated deducibility graphs to complex, multi-step reasoning remains a practical bottleneck; open-source models (Llama) produced cyclic graphs 60% of the time
- What evidence would resolve it: Development of specialized graph generation methods that maintain accuracy (satisfying Definition 4) as claim count increases beyond current experimental ranges

## Limitations
- Performance on reasoning domains beyond MATH and FELM remains untested
- Computational overhead of constructing and analyzing deducibility graphs for large reasoning chains is not fully characterized
- Theoretical guarantees depend on assumptions about deducibility graph quality and factuality classifier performance

## Confidence

- **High confidence**: The core methodology of using split conformal prediction on deducibility subgraphs is sound and well-explained. The empirical results on the tested datasets are clearly presented and reproducible.
- **Medium confidence**: The claim that subgraph-based conformal prediction maintains theoretical guarantees better than independent claim filtering is supported but could benefit from more rigorous theoretical analysis and broader empirical validation.
- **Medium confidence**: The assertion that the method produces more "legible" outputs is based on the authors' interpretation and would benefit from user studies or more objective readability metrics.

## Next Checks
1. Test the method on additional reasoning datasets beyond MATH and FELM, particularly those with different reasoning structures and domain knowledge requirements.
2. Conduct a comprehensive ablation study comparing different graph construction methods and their impact on factuality preservation and claim retention.
3. Perform runtime analysis and scalability testing on longer reasoning chains to understand computational requirements and potential bottlenecks.