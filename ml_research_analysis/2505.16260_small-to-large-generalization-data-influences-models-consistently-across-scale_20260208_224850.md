---
ver: rpa2
title: 'Small-to-Large Generalization: Data Influences Models Consistently Across
  Scale'
arxiv_id: '2505.16260'
source_url: https://arxiv.org/abs/2505.16260
tags:
- uni00000156
- uni000001ef
- uni00000231
- training
- uni0000008a
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Small-to-Large Generalization: Data Influences Models Consistently
  Across Scale Alaa Khaddaj; Logan Engstrom; Aleksander Madry We investigate whether
  changes in training data distribution affect small- and large-scale models similarly.
  We train language models of different sizes on 10 diverse training data distributions
  and measure their behavior on 6 downstream tasks.'
---

# Small-to-Large Generalization: Data Influences Models Consistently Across Scale

## Quick Facts
- **arXiv ID**: 2505.16260
- **Source URL**: https://arxiv.org/abs/2505.16260
- **Reference count**: 40
- **Primary result**: Small- and large-scale model predictions correlate well across data changes, enabling data attribution with orders-of-magnitude smaller proxy models

## Executive Summary
This paper investigates whether changes in training data distribution affect small- and large-scale models similarly. Through extensive experiments training language models of different sizes on 10 diverse training data distributions, the authors demonstrate that small- and large-scale model predictions generally correlate well across data changes, even when proxy models are trained with 370× less compute. They identify specific test distributions and small proxy scales where correlation breaks down, particularly for knowledge-intensive tasks like SQuAD and TriviaQA.

The findings have significant practical implications: orders-of-magnitude smaller proxy models can effectively approximate large-scale model behavior for data attribution in vision and dataset selection in language modeling, though performance degrades at very small scales. The paper establishes that the relative improvement from a data source is monotonic and roughly linear across compute scales, rather than emerging only at scale.

## Method Summary
The authors train GPT-2 style language models (40M–760M parameters) on diverse training distributions and evaluate their behavior on downstream tasks. They measure correlations between small and large model losses across data distributions, use TRAK (a linearized attribution method) to compute data influence scores, and apply DSDM (Dataset Selection via Data Attribution) to filter training data. For vision experiments, they train ResNet variants on CIFAR and ImageNet. The methodology emphasizes Chinchilla-optimal training (matching token-to-parameter ratios) to ensure fair comparisons across scales, and uses multiple checkpoints to reduce noise in attribution scores.

## Key Results
- Small- and large-scale language model predictions correlate highly (R² > 0.9) across most data distribution changes
- TRAK attribution from small proxy models yields similar effectiveness as large reference models (LDS decreases by at most 10%)
- DSDM-selected datasets improve large-model performance regardless of proxy model size
- Correlation breaks down for specific knowledge-intensive tasks (SQuAD, TriviaQA) when using very small proxies (<40M parameters)
- The 370× compute gap represents a practical limit beyond which attribution quality degrades

## Why This Works (Mechanism)

### Mechanism 1: Scale-Invariant Loss Correlation
The paper demonstrates that loss landscapes across different data distributions scale linearly, implying models of different capacities learn similar feature representations when trained on the same data. This preserves the relative "difficulty" or "utility" of specific data points across scales. The relative improvement from a data source is monotonic and roughly linear across compute scales.

### Mechanism 2: Linear Datamodel Transfer (TRAK)
TRAK linearizes model output using gradients, and because small and large models have correlated feature spaces, the projected gradients (influence directions) align sufficiently for the linear approximation to transfer. Random projection preserves the gradient relationships necessary for influence estimation across scales.

### Mechanism 3: Proxy-Guided Data Selection (DSDM)
By ranking data based on its influence on a target task (computed cheaply via TRAK), one selects a subset that maximizes the linear surrogate of the loss. The ranking of data utility is preserved across scale, so the "optimal" subset for the proxy is also optimal (or near-optimal) for the reference model.

## Foundational Learning

- **Concept: Linear Datamodeling Score (LDS)**
  - Why needed: Primary metric to quantify how well data attribution scores from a proxy model predict the actual behavior of the large model
  - Quick check: If a proxy model has a high LDS correlation with a reference model, does it guarantee higher accuracy, or just correlated loss changes? (Answer: Correlated loss changes)

- **Concept: Chinchilla Scaling Laws**
  - Why needed: Defines "scale" not just by parameter count, but by compute (params × tokens). Understanding the 370× compute gap requires understanding the token-to-parameter ratio
  - Quick check: Why does a 40M parameter model represent a 370× compute gap relative to a 760M model in this paper? (Answer: Because it is trained Chinchilla-optimally on fewer tokens)

- **Concept: Random Projection (Johnson-Lindenstrauss)**
  - Why needed: TRAK relies on projecting high-dimensional gradients into a smaller subspace to make influence calculation tractable
  - Quick check: How does reducing the dimensionality of gradients preserve the ability to attribute data influence? (Answer: By preserving approximate distances/angles between gradient vectors)

## Architecture Onboarding

- **Component map**: Proxy Model -> TRAK Attributor -> DSDM Selector -> Reference Model
- **Critical path**:
  1. Train proxy models on the candidate data pool
  2. Compute TRAK scores mapping training data → target task performance
  3. Select training subset based on highest aggregated influence
  4. Retrain reference model on selected subset
- **Design tradeoffs**:
  - Smaller proxies reduce compute cost exponentially but increase risk of correlation breakdown
  - Lower projection dimensions speed up TRAK but may lose fine-grained influence signal
- **Failure signatures**:
  - Correlation degrades significantly on knowledge-heavy tasks like SQuAD and TriviaQA with very small proxies
  - Attribution quality degrades if proxy compute is <0.27% of reference compute
- **First 3 experiments**:
  1. Train a 760M model and a 57M proxy on 3 distinct data distributions and plot losses to verify R² correlation matches Figure 1
  2. Calculate TRAK attribution for both proxy and reference and measure Linear Datamodeling Score (LDS)
  3. Compare DSDM selection using the proxy vs. random selection on the reference model and verify downstream accuracy gains

## Open Questions the Paper Calls Out

1. Will the correlation between small proxy and large reference models persist as data attribution estimators improve? Current noise in attribution scores may be masking the true scaling relationship.

2. Why does the correlation between proxy and reference models degrade for specific knowledge-retrieval tasks like SQuAD and TriviaQA? The paper establishes this occurs but does not explain the underlying cause.

3. Does the observed generalization hold for training regimes that deviate from Chinchilla-optimal compute budgets? The methodology restricts proxies to Chinchilla-optimal ratios, leaving this question open.

## Limitations

- Correlation breaks down for specific knowledge-intensive tasks (SQuAD, TriviaQA) with very small proxies (<40M parameters)
- The 370× compute gap represents a practical limit beyond which attribution quality degrades significantly
- The effectiveness of small-to-large generalization across different model architectures remains unexplored

## Confidence

- **High confidence**: Small and large models show high correlation (R² > 0.9) across data changes for generic language tasks
- **Medium confidence**: Linear datamodel transfer mechanism via TRAK linearization is empirically validated but relies on strong assumptions
- **Low confidence**: Task-specific failure modes for knowledge-intensive tasks and the exact boundaries of correlation breakdown

## Next Checks

1. Systematically measure correlation breakdown across the full range of downstream tasks to identify precise conditions where small-to-large generalization fails

2. Test whether observed correlations hold when comparing different model architectures (e.g., transformer vs. RNN) at different scales

3. Evaluate TRAK attribution quality across varying projection dimensions and layer selections to establish sensitivity bounds for the linearization approach