---
ver: rpa2
title: Vector Arithmetic in Concept and Token Subspaces
arxiv_id: '2511.18162'
source_url: https://arxiv.org/abs/2511.18162
tags:
- word
- token
- heads
- concept
- random
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates whether vector arithmetic works in LLM\
  \ hidden states by identifying attention heads that separate semantic and surface-level\
  \ information. The authors build on prior work showing two types of heads\u2014\
  concept induction (copying meanings) and token induction (copying exact tokens)\u2014\
  and construct \"lenses\" that transform hidden states using these heads' attention\
  \ weights."
---

# Vector Arithmetic in Concept and Token Subspaces

## Quick Facts
- arXiv ID: 2511.18162
- Source URL: https://arxiv.org/abs/2511.18162
- Reference count: 12
- Key outcome: Concept lens transformations improve semantic vector arithmetic accuracy from 47% to 80% compared to raw hidden states

## Executive Summary
This paper investigates whether vector arithmetic works in LLM hidden states by identifying attention heads that separate semantic and surface-level information. The authors build on prior work showing two types of heads—concept induction (copying meanings) and token induction (copying exact tokens)—and construct "lenses" that transform hidden states using these heads' attention weights. They then test whether parallelogram arithmetic (e.g., king - man + woman ≈ queen) works better in these transformed spaces than in raw hidden states. The primary finding is that using concept heads dramatically improves semantic arithmetic accuracy (80% vs 47% for raw states), while token heads improve surface-level tasks like morphological changes.

## Method Summary
The authors construct "lenses" by summing the OV matrices of selected attention heads, where OV matrices (O·V) determine how heads affect the residual stream. They identify two head types from prior work: concept induction heads that copy semantic meanings and token induction heads that copy exact tokens. By multiplying hidden states with these lens matrices, they transform activations into subspaces where semantic relationships become more linear. They evaluate this approach using parallelogram arithmetic on analogy tasks (capital-country, family relations, etc.) and measure nearest-neighbor accuracy in the transformed spaces.

## Key Results
- Concept lens transformations achieve 80% nearest-neighbor accuracy on semantic tasks vs 47% for raw hidden states
- Token lenses perform better than concept lenses for surface-level tasks with word variations
- The transformation matrices are empirically full-rank but can be low-rank approximated (r=256) without losing accuracy
- Different layers show varying accuracy patterns, with middle layers (ℓ=16-20) generally performing best

## Why This Works (Mechanism)

### Mechanism 1: OV Matrix as Task-Specific Information Extractor
The product of value and output projection matrices from attention heads extracts specific information types from hidden states. Each attention head's OV matrix (O·V) acts as a learned projection that isolates the subspace of activations relevant to that head's function. Concept induction heads project onto semantic subspaces; token induction heads project onto surface-form subspaces. This works because heads identified as "concept" or "token" induction types reliably separate these information types across different inputs.

### Mechanism 2: Subspace-Specific Vector Arithmetic
Parallelogram arithmetic only works effectively when performed in an appropriate subspace, not in raw hidden states. Raw hidden states contain mixed semantic, syntactic, and surface information causing interference. The lens transformation L projects onto a subspace where the target relationship (e.g., capital-country) is linearly represented and other information is suppressed. This works because semantic and surface relationships are approximately linear in their respective subspaces.

### Mechanism 3: Effective Low-Rank Structure Despite Full-Rank Matrices
The summed OV matrices are full-rank empirically but can be low-rank approximated without performance loss. Although summing k=80 head matrices yields full-rank transformations, the meaningful structure occupies a lower-dimensional subspace. SVD truncation to rank r=256 preserves task-relevant information because task-relevant information concentrates in the top singular vectors.

## Foundational Learning

- **Attention Head OV Circuit**: Why needed here: Understanding how O·V matrices mediate what information heads write to the residual stream is essential for grasping why these transformations work. Quick check question: Given a head with value matrix V (m×d) and output matrix O (d×m), what is the maximum rank of O·V and why?

- **Word2Vec Parallelogram Arithmetic**: Why needed here: The paper's evaluation directly tests whether Mikolov-style analogies hold in transformed spaces. Quick check question: If "king - man + woman ≈ queen" works in some embedding space, what does this imply about how gender is represented?

- **Low-Rank Approximation via SVD**: Why needed here: The paper uses SVD truncation to demonstrate that full-rank transformations have low-dimensional effective structure. Quick check question: When you zero out singular values below threshold r, how does this affect the transformation's behavior?

## Architecture Onboarding

- **Component map**: Input word → Model forward pass → Hidden state at layer ℓ → Multiply by lens L (sum of top-k OV matrices) → Transformed vector in subspace → Arithmetic operations (a - b + b') → Nearest neighbor search for result

- **Critical path**: Extracting and transforming hidden states is straightforward; the non-obvious step is identifying which heads belong to Ck (concept) or Tk (token) sets. This requires running the head classification from Feucht et al. 2025 first.

- **Design tradeoffs**: Higher k (more heads) includes more information but may introduce noise; layer selection matters—different layers show different accuracy patterns (Figure 1 uses ℓ=16-20); prefix choice affects results (Appendix A shows some tasks degrade without context).

- **Failure signatures**: Nearest neighbor returns one of the operands (common failure mode noted in §3.2); tasks with cyclic relationships (opposite) fundamentally incompatible with parallelogram structure; many-to-one mappings (capitalize-first-letter) may not be representable.

- **First 3 experiments**: 1) Replicate on a single task: Choose capital-common-countries, extract Llama-2-7b hidden states at layer 20, apply concept lens with k=80, compute nearest-neighbor accuracy. Expected: ~80%. 2) Ablate lens type: Run the same task with token lens instead of concept lens. Expected: significantly lower accuracy for semantic tasks. 3) Test rank sensitivity: Apply SVD truncation to concept lens, sweeping r from 64 to 4096. Expected: flat performance until very low ranks.

## Open Questions the Paper Calls Out
None

## Limitations
- The findings rest on the assumption that the dual-route head classification (concept vs token induction) from Feucht et al. (2025) generalizes reliably across different linguistic contexts and model scales
- The arithmetic approach has fundamental limitations for certain relationship types, particularly cyclic relationships like "opposite"
- The paper focuses on a single model architecture (Llama-2-7b), leaving open questions about whether these mechanisms transfer to other architectures or larger models

## Confidence
**High Confidence**: The core empirical finding that concept lens transformation improves semantic arithmetic accuracy (80% vs 47%) is well-supported by the experimental design and controls. The low-rank approximation result showing maintained performance down to rank 256 is also robustly demonstrated with clear quantitative evidence.

**Medium Confidence**: The claim that OV matrix products act as information extractors relies on the prior dual-route model work. While the current results align with this interpretation, the mechanism description depends on accepting the original head classifications.

**Low Confidence**: The paper's claims about the general applicability of this approach to arbitrary semantic tasks are overstated. The method's failure modes and boundary conditions are not thoroughly explored, and the assumption that semantic relationships are generally linear in appropriate subspaces remains largely untested beyond the specific analogy tasks evaluated.

## Next Checks
1. **Cross-Architecture Validation**: Test the concept and token lens approach on multiple model architectures (e.g., Mistral, GPT-2, Pythia) to verify whether the dual-route head behavior generalizes beyond Llama-2.

2. **Head Classification Ablation**: Randomly shuffle the head-to-type assignments while preserving the number of concept and token heads in each lens to test whether specific head identities matter or if any heads of the right type suffice.

3. **Zero-Shot Generalization**: Evaluate the transformed vectors on novel semantic relationships not seen during development or not following standard analogy patterns to test whether the subspace transformation genuinely captures abstract semantic relationships.