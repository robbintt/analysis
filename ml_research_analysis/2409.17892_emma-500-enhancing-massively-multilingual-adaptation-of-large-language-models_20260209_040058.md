---
ver: rpa2
title: 'EMMA-500: Enhancing Massively Multilingual Adaptation of Large Language Models'
arxiv_id: '2409.17892'
source_url: https://arxiv.org/abs/2409.17892
tags:
- latn
- llama
- languages
- language
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces EMMA-500, a massively multilingual language
  model built by continuing training the Llama 2 7B model on a large corpus of 546
  languages. A diverse dataset called MaLA was compiled, including texts from code,
  books, scientific papers, and instructions, to enhance low-resource language coverage.
---

# EMMA-500: Enhancing Massively Multilingual Adaptation of Large Language Models

## Quick Facts
- arXiv ID: 2409.17892
- Source URL: https://arxiv.org/abs/2409.17892
- Reference count: 40
- Primary result: EMMA-500 achieves strong multilingual performance through continued training on 546 languages, showing improvements particularly for underrepresented languages.

## Executive Summary
EMMA-500 is a massively multilingual language model created by continuing training the Llama 2 7B model on a large corpus covering 546 languages. The authors compiled a diverse dataset called MaLA, including texts from code, books, scientific papers, and instructions, to enhance low-resource language coverage. Through careful data mixing that upsamples low-resource languages while preserving high-resource capabilities, the model was evaluated across nine tasks and fifteen benchmarks, demonstrating strong performance in translation, commonsense reasoning, and text classification. EMMA-500 achieved the best or competitive results compared to many decoder-only models and showed particular improvements for underrepresented languages.

## Method Summary
EMMA-500 was created through continual pre-training of Llama 2 7B using the GPT-NeoX framework on the MaLA corpus, which covers 939 languages with 546 used for training. The training employed a causal language modeling objective with a carefully balanced data mixing strategy that upsamples low-resource languages and downsamples high-resource content, combined with replay of code and instruction data to mitigate catastrophic forgetting. The model was trained for 200 billion tokens over 12,000 steps using 256 A100 GPUs with global batch size 4096 and sequence length 4096, employing AdamW optimization with cosine decay learning rate scheduling.

## Key Results
- EMMA-500 achieved best or competitive results compared to many decoder-only models on multilingual benchmarks
- The model demonstrated significant improvements particularly for underrepresented languages
- Strong performance was observed in translation, commonsense reasoning, and text classification tasks
- The approach showed effective cross-lingual transfer capabilities beyond direct training data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Continual pre-training expands language capacity without architectural changes.
- Mechanism: Exposure to new multilingual data during continued training causes the model's existing representations to adapt to new linguistic patterns, building on its pre-trained English-centric knowledge base.
- Core assumption: The base model's representations are sufficiently flexible to accommodate new languages without vocabulary extension.
- Evidence anchors:
  - [abstract] "continual pre-training in expanding large language models' language capacity, particularly for underrepresented languages"
  - [section 3.2] "We employ continual training using the causal language modelling objective for the decoder-only Llama model"
  - [corpus] Related work on bilingual translation data for adaptation supports this mechanism.
- Break condition: If target languages require fundamentally different tokenization or have orthographic systems poorly represented in the base vocabulary, performance gains may be limited.

### Mechanism 2
- Claim: Carefully balanced data mixing mitigates catastrophic forgetting while enabling new language acquisition.
- Mechanism: Upsampling low-resource language content and downsampling high-resource content, combined with replay of code and instruction data, maintains previously learned capabilities while allocating training capacity to new languages.
- Core assumption: The model can simultaneously retain existing knowledge and acquire new linguistic patterns when exposed to appropriately balanced data.
- Evidence anchors:
  - [section 3.1] "We downsample texts in high-resource languages and upsample text in low-resource languages using different sample rates"
  - [section 3.1] "mitigates the forgetting in model training" through inclusion of "books and scientific papers in high-resource languages, code, and instruction data"
  - [corpus] Limited direct evidence on optimal mixing ratios; this remains an empirical tuning question.
- Break condition: If replay data is insufficient or sampling ratios are poorly calibrated, the model may lose proficiency in high-resource languages or code generation.

### Mechanism 3
- Claim: Cross-lingual transfer enables performance gains in low-resource languages beyond direct training data.
- Mechanism: Representations learned for high-resource languages encode linguistic structures that partially transfer to related or typologically similar low-resource languages during shared training.
- Core assumption: Languages share transferable structural patterns that the model can abstract during multilingual pre-training.
- Evidence anchors:
  - [abstract] "demonstrating significant gains in cross-lingual transfer, task generalization, and language adaptability"
  - [section 4.3-4.6] Consistent performance improvements across language resource groups, with particularly strong gains in translation (Eng-X direction) for low-resource languages
  - [corpus] Cross-lingual transfer mechanisms remain incompletely understood; related work suggests language family and morphology play roles.
- Break condition: Transfer is weaker for languages with limited typological similarity to well-represented languages in training data.

## Foundational Learning

- **Continual Pre-training vs. Fine-tuning**
  - Why needed here: EMMA-500 uses continued unsupervised training on new data rather than task-specific supervised adaptation.
  - Quick check question: Can you explain why the authors use continued language modeling rather than instruction tuning for language adaptation?

- **Catastrophic Forgetting in LLMs**
  - Why needed here: The data mixing strategy explicitly addresses the risk of losing previously learned capabilities.
  - Quick check question: Why does including code and high-resource English data in the mix help prevent forgetting?

- **Multilingual Tokenization Challenges**
  - Why needed here: The MaLA corpus handles 939 languages with varying scripts; tokenization efficiency affects training effectiveness.
  - Quick check question: Why might whitespace-based token counting be problematic for languages like Chinese or Thai?

## Architecture Onboarding

- **Component map:**
  Base Model -> MaLA Corpus -> Data Mixing -> GPT-NeoX Training -> EMMA-500

- **Critical path:**
  1. Data preparation: Harmonize language codes (ISO 639-3), recognize writing systems (ISO 15924), clean and deduplicate
  2. Mixing: Apply sampling rates by resource tier (Table 2), combine mono/inst/code/curated sources
  3. Training: 256 A100 GPUs, batch size 4096, sequence length 4096, cosine LR with warmup

- **Design tradeoffs:**
  - No vocabulary extension: Simpler implementation but may limit efficiency for scripts poorly represented in Llama 2 tokenizer
  - 7B parameter scale: Enables comparison with many Llama 2 derivatives but limits absolute performance vs. larger models
  - Resource-tier sampling: Balances coverage vs. forgetting but requires manual calibration

- **Failure signatures:**
  - Quasi-random results on challenging benchmarks (MGSM, ARC) indicate 7B scale limitations
  - High Self-BLEU on open-ended generation suggests reduced output diversity
  - Performance gaps vs. Llama 3/Gemma 2 reflect base model vintage

- **First 3 experiments:**
  1. Ablate the instruction data component: Train without xp3x/Aya to isolate its contribution to cross-lingual transfer
  2. Test vocabulary extension: Compare against a variant with expanded tokenizer for under-represented scripts
  3. Scale study: Replicate the data mix with Llama 3.1 8B base to quantify architectural vs. data contributions

## Open Questions the Paper Calls Out

- How does the specific composition and sampling rate of the data mix (code, instructions, curated data) affect the trade-off between retaining high-resource capabilities and acquiring low-resource proficiency?
- Does training a model from scratch on the MaLA corpus outperform the continual pre-training (CPT) approach used for EMMA-500?
- To what extent does multilingual instruction tuning improve EMMA-500's performance on task generalization and natural interactions?

## Limitations

- Limited detail on quality filtering mechanisms for low-resource languages in the MaLA corpus
- Evaluation focuses heavily on translation and commonsense reasoning, with limited analysis of generative capabilities
- Cross-lingual transfer claims lack controlled experiments varying language family distances

## Confidence

**High Confidence Claims:**
- EMMA-500 demonstrates improved multilingual performance compared to Llama 2 base model across the evaluated tasks
- The continual pre-training approach successfully expands language coverage without catastrophic forgetting when using the proposed data mixing strategy
- The model shows competitive performance against decoder-only models of similar scale on multilingual benchmarks

**Medium Confidence Claims:**
- EMMA-500 achieves best or competitive results compared to other decoder-only models on most tasks
- The improvements are particularly significant for underrepresented languages
- The cross-lingual transfer capabilities enable generalization beyond direct training data

**Low Confidence Claims:**
- The specific contribution of each data component (mono vs. instruction vs. code) to overall performance
- The scalability of the approach to larger model sizes or different architectural families
- The long-term stability of the learned representations after continued pre-training

## Next Checks

1. **Controlled Cross-Lingual Transfer Experiment:** Train parallel models where low-resource languages are grouped by typological similarity to high-resource languages (e.g., Bantu languages paired with Swahili, Turkic languages with Turkish) versus random pairing. Measure performance differences to quantify the actual contribution of structural transfer versus simple data exposure.

2. **Vocabulary Extension Impact Study:** Implement a variant of EMMA-500 with an expanded tokenizer that better represents under-represented scripts. Compare tokenization efficiency metrics (tokens per character/word) and downstream task performance against the original to quantify the cost of not modifying the vocabulary.

3. **Longitudinal Capability Retention Test:** After training EMMA-500, evaluate the model on high-resource language tasks and code generation benchmarks at regular intervals during extended pre-training (additional 100B-500B tokens). Track performance degradation patterns to validate the catastrophic forgetting mitigation claims and identify optimal replay data ratios.