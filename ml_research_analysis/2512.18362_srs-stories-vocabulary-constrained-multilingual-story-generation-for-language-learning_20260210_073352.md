---
ver: rpa2
title: 'SRS-Stories: Vocabulary-constrained multilingual story generation for language
  learning'
arxiv_id: '2512.18362'
source_url: https://arxiv.org/abs/2512.18362
tags:
- words
- story
- rewrite
- language
- stories
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SRS-Stories, a system that uses large language
  models to generate personalized stories for language learners, using only vocabulary
  they know. The approach combines spaced repetition systems with story generation,
  teaching new words in context while reviewing previously learned vocabulary.
---

# SRS-Stories: Vocabulary-constrained multilingual story generation for language learning

## Quick Facts
- arXiv ID: 2512.18362
- Source URL: https://arxiv.org/abs/2512.18362
- Reference count: 19
- Generates personalized language learning stories using only known vocabulary plus target words

## Executive Summary
SRS-Stories is a system that generates personalized stories for language learners using large language models (LLMs) constrained to specific vocabulary sets. The system integrates with spaced repetition systems (SRS) to create stories that reinforce known vocabulary while introducing new words in context. Three prompting strategies and three rewriting methods were evaluated across English, Chinese, and Polish at different proficiency levels. Results show the proposed methods produce more grammatical, coherent, and interesting stories than baseline constrained beam search, with better integration of target vocabulary.

## Method Summary
The system combines SRS vocabulary tracking with LLM story generation. Given known vocabulary V and target words L, it generates stories where all words come from V∪L. Three prompting strategies were tested: Simple (direct generation), Planning (title→outline→story), and Examples First (generate example sentences→story). Three rewriting methods iteratively replace out-of-vocabulary words: base Rewrite (5 iterations), Rewrite Highlighted (marks unknown words), and Synonyms then Rewrite. Stories are generated with Llama 3.1 70B Instruct and evaluated with Qwen2.5 72B, with human validation for English.

## Key Results
- Proposed methods achieve higher grammaticality, coherence, and interestingness than constrained beam search baseline
- Iterative rewriting with vocabulary verification reduces OOV rate to ~0.6% while maintaining story quality
- Planning prompting improves coherence but Examples First better integrates target vocabulary in Chinese
- Simple prompting with basic rewriting yields best performance in grammatical correctness and vocabulary coverage

## Why This Works (Mechanism)

### Mechanism 1
Multi-step prompting strategies improve target vocabulary integration and story quality compared to direct constrained decoding. Decomposing story generation into planning stages (idea → outline → story) or pre-generating example sentences allows the LLM to reason about word usage contexts before narrative construction, reducing the cognitive load of simultaneous vocabulary and coherence constraints.

### Mechanism 2
Iterative rewriting with external verification more reliably enforces lexical constraints than single-pass generation. A non-LLM verifier identifies out-of-vocabulary words via lemmatization and vocabulary lookup; the LLM then rewrites flagged words over multiple iterations (up to 5), progressively reducing OOV rate while preserving narrative structure.

### Mechanism 3
SRS-driven vocabulary selection enables personalized learning material generation without manual curation. The SRS engine tracks known vocabulary V and schedules reviews based on forgetting curves; the story generator receives V, review words, and new words L, producing context that reinforces memory through spaced, meaningful exposure rather than isolated flashcards.

## Foundational Learning

- **Spaced Repetition Systems (SRS)**: Needed to understand how vocabulary scheduling drives personalized story generation. Quick check: Given a word reviewed 7 days ago with a 14-day half-life, should the SRS schedule it today?

- **Lexically Constrained Decoding**: Needed to understand why traditional constrained beam search fails (incoherence, grammatical errors) and motivates the LLM-prompting approach. Quick check: Why does masking tokens at each generation step (No Bad Words) produce less fluent text than unconstrained generation with post-hoc rewriting?

- **LLM Prompting Strategies (Chain-of-Thought, Decomposition)**: Needed to understand how decomposition principles (Planning vs. Examples First) affect story quality. Quick check: For a 10-word vocabulary list with unrelated terms, would Planning or Examples First likely produce better coherence?

## Architecture Onboarding

- **Component map**: SRS Engine → (V, L, review words) → LLM Story Generator → Draft Story → Non-LLM Constraint Verifier → Rewriter (up to 5 iterations) → Final Story → User

- **Critical path**: Vocabulary selection (SRS) → Story generation (LLM + prompt strategy) → Constraint verification (tokenizer + lemmatizer + vocabulary lookup) → Iterative rewriting → Output. The verifier is language-specific (Morfeusz for Polish, NLTK for English, character-matching for Chinese).

- **Design tradeoffs**: Simple Prompting vs. Planning (faster but potentially lower quality vs. slower with better coherence); Rewrite vs. Rewrite Highlighted vs. Synonyms (basic Rewrite yields lowest OOV but may reduce interestingness); Temperature=0 ensures reproducibility but limits creative diversity.

- **Failure signatures**: High OOV rate (>5%) indicates verifier-rewriter mismatch; low #|L|≥1 (<80%) means target vocabulary not integrated; incoherent story with Planning indicates outline-story drift.

- **First 3 experiments**: 1) Replicate English B1 results with Simple Prompting + Rewrite; measure OOV rate and #L to verify baseline implementation. 2) A/B test Planning vs. Examples First for Chinese HSK4; confirm Examples First improves grammaticality but reduces coherence. 3) Stress test rewriting loop: generate stories at A1 level and plot OOV vs. iteration count to validate 5-iteration plateau claim.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several are implicit from the discussion and limitations.

## Limitations
- Experimental scope limited to three languages (English, Chinese, Polish) at discrete CEFR levels
- Vocabulary lists may not capture actual learner knowledge or mixed language exposure patterns
- Evaluation relies heavily on LLM-based scoring with partial human validation (r≈0.5 correlation for English)

## Confidence
- **High confidence**: Iterative rewriting with vocabulary verification significantly reduces out-of-vocabulary words while maintaining story quality
- **Medium confidence**: Relative performance differences between prompting strategies across languages and proficiency levels
- **Low confidence**: Pedagogical effectiveness for actual vocabulary acquisition (requires longitudinal studies)

## Next Checks
1. Test Polish lemmatization pipeline with broader inflected forms to verify OOV reduction generalizes beyond sampled vocabulary
2. Conduct full human evaluations across all three languages and proficiency levels to confirm LLM scoring patterns
3. Design controlled experiment measuring vocabulary retention after reading stories vs. traditional SRS flashcards over multiple sessions