---
ver: rpa2
title: Characterizing the Effects of Translation on Intertextuality using Multilingual
  Embedding Spaces
arxiv_id: '2501.10731'
source_url: https://arxiv.org/abs/2501.10731
tags:
- intertextuality
- translation
- translations
- human
- greek
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a method for characterizing intertextuality
  in Biblical texts and their translations using multilingual embedding spaces. The
  authors compute intertextuality ratios by measuring cosine similarity between verse
  embeddings, distinguishing references within and across testaments.
---

# Characterizing the Effects of Translation on Intertextuality using Multilingual Embedding Spaces

## Quick Facts
- arXiv ID: 2501.10731
- Source URL: https://arxiv.org/abs/2501.10731
- Reference count: 11
- Human translations amplify intertextuality (ratios 1.33-1.66) compared to machine translations (ratios 1.02-1.60)

## Executive Summary
This paper introduces a method for characterizing how translation affects intertextuality in Biblical texts by measuring cosine similarity between verse embeddings in multilingual spaces. The authors develop a metric that quantifies intertextuality preservation by comparing similarity of known cross-references to baseline similarity from same-chapter random pairs. Using 2,183 ground-truth cross-references, they compare human translations (English, Finnish, Turkish, Swedish, Marathi) against machine translations generated by Aya23. Results show human translations consistently exhibit higher intertextuality ratios than machine translations, with English showing 1.66 for human versus 1.32 for machine translations.

## Method Summary
The method computes intertextuality ratios by measuring cosine similarity between verse embeddings from a multilingual embedding model. For each known intertextual pair, the similarity is divided by the mean similarity of randomly paired verses from the same chapter. This ratio quantifies how much more similar intertextual verses are compared to topical but non-intertextual verse pairs. The authors compute 95% confidence intervals via bootstrapping (10,000 iterations) to enable cross-translation comparisons. They evaluate the approach on the Bible using human translations from the JHU Bible Corpus and machine translations from Aya-23 8B, comparing intertextuality ratios across five languages.

## Key Results
- Human translations consistently show higher intertextuality ratios (1.33-1.66) than machine translations (1.02-1.60) across all five languages tested
- Human translators systematically amplify intertextuality between testaments more than within testaments
- Translation quality varies significantly by language pair, with English and Turkish showing higher COMET scores (61.2-72.6 and 65.4-68.2 respectively) compared to Marathi (26.5-29.8)
- English human translation achieves the highest ratio (1.66) while machine translations for Marathi show the lowest (1.02)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cosine similarity of multilingual embeddings captures intertextual relationships at corpus level.
- Mechanism: Embeddings from a multilingual model encode semantic content; intertextual verse pairs exhibit higher cosine similarity than randomly paired verses from the same chapter. The ratio of mean intertextual similarity to mean baseline similarity quantifies preservation.
- Core assumption: Embedding similarity correlates with human-judged intertextuality; same-chapter random pairs provide appropriate baselines.
- Evidence anchors:
  - [abstract] "We provide a metric to characterize intertextuality at the corpus level"
  - [section 2] "Our intertextuality measure is simply the cosine similarity of a pair of verse embeddings from a multilingual embedding model"
  - [corpus] Related work (Burns et al., 2021) uses embeddings for Latin intertextuality; no direct corpus validation of this specific metric beyond the paper's benchmark.
- Break condition: If embeddings fail to distinguish topical similarity from genuine intertextuality, or if chapter-level random baselines are too easy/too hard, ratio interpretations become unreliable.

### Mechanism 2
- Claim: Human translators systematically amplify intertextuality relative to machine translations.
- Mechanism: Human translators make lexical choices that reinforce known intertextual connections (e.g., rendering different source words as the same target word), while machine translation produces lexically faithful but rhetorically neutral output.
- Core assumption: Amplification reflects intentional or culturally-mediated translator behavior, not random variation.
- Evidence anchors:
  - [abstract] "human translations consistently amplify intertextuality (ratios 1.33-1.66) compared to machine translations (ratios 1.02-1.60)"
  - [section 5, Table 5] Hebrews 8:12–Isaiah 43:25 example: human translation uses "sin" in both verses where Greek source differs; similarity increases from 0.332 to 0.656
  - [corpus] Related work on machine translation quality exists, but no corpus evidence directly addresses intentional intertextuality amplification by human translators.
- Break condition: If machine translation quality degrades significantly for low-resource languages (Marathi: COMET 26.5-29.8), lower intertextuality ratios may reflect translation errors rather than neutral fidelity.

### Mechanism 3
- Claim: Bootstrapped confidence intervals enable cross-translation comparison of intertextuality ratios.
- Mechanism: Resampling verse pairs with replacement (10,000 iterations) produces empirical distributions of ratios; non-overlapping 95% CIs support claims of meaningful differences.
- Core assumption: Verse-pair similarities are approximately independent; resampling preserves distributional structure.
- Evidence anchors:
  - [section 2] "we compute the 95% confidence interval via bootstrapping...10,000 times"
  - [section 4, Table 4] CIs reported for all ratios (e.g., English human: 1.66 ± 0.21)
  - [corpus] Weak/missing: no corpus validation of bootstrap assumptions for intertextuality metrics.
- Break condition: If verse-pair dependencies exist (e.g., multiple pairs from same book), CIs may be artificially narrow.

## Foundational Learning

- **Multilingual embedding alignment**
  - Why needed here: The method depends on embeddings from different languages occupying a shared semantic space where cosine similarity is comparable across languages.
  - Quick check question: Can you explain why a multilingual model (rather than separate monolingual models) is necessary for this cross-lingual comparison?

- **Intertextuality as a literary phenomenon**
  - Why needed here: The metric operationalizes a complex literary concept; understanding what intertextuality is (quotation, echo, allusion) clarifies what the metric should capture.
  - Quick check question: What is the difference between explicit quotation and semantic resemblance as forms of intertextuality?

- **Ratio-based normalization**
  - Why needed here: Raw similarity scores are not comparable across languages/embedding spaces; dividing by a within-corpus baseline normalizes for language-specific density.
  - Quick check question: Why use same-chapter random pairs rather than fully random pairs as the baseline?

## Architecture Onboarding

- **Component map**: TAHOT (Hebrew OT) -> TAGNT (Greek NT) -> Septuagint (Greek OT) -> JHU Bible Corpus (human translations) -> Aya-23 8B (machine translations) -> Multilingual embedding model -> Intertextuality ratio computation

- **Critical path**: 1. Align all texts to English versification (preprocessing) 2. Generate embeddings for all verses in all languages 3. For each known intertextual pair, compute cosine similarity 4. Generate baseline similarities via same-chapter random swapping 5. Compute ratio and bootstrap CI

- **Design tradeoffs**:
  - Verse-level granularity enables alignment and comparison, but may miss narrative-episode-level intertextuality (noted as future work)
  - Vote threshold = 50 reduces noise but may exclude legitimate weak references
  - Aya-23 8B is practical for limited compute; larger models might yield different machine translation baselines
  - Same-chapter baseline controls for topic but may be conservative

- **Failure signatures**:
  - Ratio < 1: Suggests metric failure or mislabeled references
  - Very wide CIs: Insufficient reference pairs or high variance in similarities
  - Machine > human ratios: May indicate human translation source language mismatch (many human translations derive from English, not original manuscripts)
  - Low COMET + low ratio: Translation quality confounds intertextuality measurement

- **First 3 experiments**:
  1. Reproduce benchmark validation: Compute intertextuality ratio on Burns et al. Latin corpus; verify ~1.55 ratio with CI.
  2. Ablate baseline strategy: Compare same-chapter vs. same-book vs. fully-random baselines; assess sensitivity.
  3. Controlled translation comparison: For one language pair, generate machine translations from both original manuscripts AND from English; quantify how source language affects intertextuality ratios.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does narrative episode-level analysis compare to verse-level analysis for characterizing intertextuality preservation in translation?
- Basis in paper: [explicit] "Finally, we leave to future work exploring larger narrative contexts by examining narrative episodes instead of verse-level intertextuality."
- Why unresolved: Current method operates at verse-level granularity, which may miss intertextual connections spanning multiple verses or requiring broader narrative context.
- What evidence would resolve it: Apply intertextuality metrics to episode-level segments; compare ratios with verse-level results; identify cases where analyses diverge.

### Open Question 2
- Question: To what extent does improved verse alignment across parallel Bible corpora affect intertextuality measurements?
- Basis in paper: [explicit] "We plan to address the persistent issue of misalignment in parallel Bible corpora... by applying the alignment methodology proposed by (Craig et al., 2023)."
- Why unresolved: Misalignment introduces noise into similarity calculations, potentially affecting ratio accuracy across languages and testaments.
- What evidence would resolve it: Apply alignment methods to create better-aligned corpora; compare intertextuality ratios before and after correction.

### Open Question 3
- Question: How does intertextuality preservation differ when human translators work directly from source manuscripts versus through intermediary translations?
- Basis in paper: [inferred] The Limitations section notes "most translations present in the JHUBC were not translated directly from ancient manuscripts but instead work from English translations," making direct comparisons with machine translations problematic.
- Why unresolved: Translation chains may systematically affect whether intertextuality is preserved, amplified, or diminished compared to direct translation.
- What evidence would resolve it: Compare intertextuality ratios for human translations known to be direct from sources versus those through intermediaries; analyze machine translations with and without intermediate steps.

### Open Question 4
- Question: Can this method be refined to discover novel instances of intertextuality, rather than only characterizing known references?
- Basis in paper: [explicit] "Note that this method relies upon having access to ground-truth references... and would likely be too crude a method to discover novel instances of intertextuality without extensive threshold tuning."
- Why unresolved: Current metric requires pre-identified intertextual pairs; discovery would require threshold calibration and validation against expert annotation.
- What evidence would resolve it: Develop and validate threshold-tuning approaches; test discovered pairs against scholar-annotated gold standards.

## Limitations

- The paper's intertextuality metric lacks corpus validation for whether cosine similarity meaningfully captures human judgments of intertextuality
- Human translations derived from English (rather than original manuscripts) for many languages create a confounding factor that may artificially inflate intertextuality ratios
- The method's verse-level granularity excludes narrative-episode-level intertextuality, which could represent significant patterns
- Translation quality disparity for low-resource languages (Marathi: COMET 26.5-29.8) may confound intertextuality measurements

## Confidence

- **High confidence**: Cross-translation comparison methodology (bootstrap CI approach is standard and well-documented)
- **Medium confidence**: Human vs. machine intertextuality amplification findings (supported by examples but limited by confounding factors)
- **Low confidence**: Absolute intertextuality ratios across languages (confounded by translation quality variations and source language effects)

## Next Checks

1. **Benchmark Validation**: Compute intertextuality ratios on Burns et al. (2021) Latin corpus to verify the expected ~1.55 ratio with appropriate confidence intervals, confirming the metric's validity in a non-Biblical domain.

2. **Source Language Confounding**: For one target language with available translations from both original manuscripts and English, generate machine translations from both sources and measure how the choice of source text affects intertextuality ratios between human and machine translations.

3. **Baseline Sensitivity Analysis**: Systematically compare same-chapter, same-book, and fully-random baseline strategies across multiple languages to determine how sensitive the ratio metric is to baseline selection and whether this explains cross-language variations.