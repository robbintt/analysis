---
ver: rpa2
title: Benchmarking and Improving LVLMs on Event Extraction from Multimedia Documents
arxiv_id: '2509.12876'
source_url: https://arxiv.org/abs/2509.12876
tags:
- event
- lvlms
- extraction
- pages
- qwen2
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents the first systematic evaluation of Large Vision-Language\
  \ Models (LVLMs) on the Multimedia Event Extraction (M\xB2E\xB2) task, covering\
  \ text-only, image-only, and cross-media subtasks. The study benchmarks models like\
  \ DeepSeek-VL2 and Qwen-VL series under both few-shot prompting and fine-tuning\
  \ settings, using the M\xB2E\xB2 dataset with 1,105 text-only, 188 image-only, and\
  \ 395 cross-media event mentions across 8 event types and 18 argument roles."
---

# Benchmarking and Improving LVLMs on Event Extraction from Multimedia Documents

## Quick Facts
- arXiv ID: 2509.12876
- Source URL: https://arxiv.org/abs/2509.12876
- Authors: Fuyu Xing; Zimu Wang; Wei Wang; Haiyang Zhang
- Reference count: 18
- Primary result: First systematic LVLM evaluation on multimedia event extraction, showing cross-modal synergy and LoRA fine-tuning substantially improve performance

## Executive Summary
This paper presents the first systematic evaluation of Large Vision-Language Models (LVLMs) on the Multimedia Event Extraction (M²E²) task, covering text-only, image-only, and cross-media subtasks. The study benchmarks models like DeepSeek-VL2 and Qwen-VL series under both few-shot prompting and fine-tuning settings, using the M²E² dataset with 1,105 text-only, 188 image-only, and 395 cross-media event mentions across 8 event types and 18 argument roles. Key findings include: (1) Few-shot LVLMs excel in visual tasks but lag in textual tasks; (2) Fine-tuning with LoRA significantly boosts performance; (3) LVLMs achieve best results in cross-modal settings, leveraging modality synergy.

## Method Summary
The study evaluates LVLMs on six M²E² subtasks (text-only/image-only/cross-media Event Detection and Event Argument Extraction) using few-shot prompting and LoRA fine-tuning. Models tested include DeepSeek-VL2, Qwen2-VL-7B, Qwen2.5-VL-7B, and Qwen2.5-VL-72B. Few-shot evaluation uses 4 contrastive demonstrations per prompt with temperature=0. Fine-tuning employs LoRA with learning rate 1×10⁻⁴ and batch size 3, trained on merged M²E² training split (70%) and ACE 2005 filtered to 8 event types. Evaluation uses strict matching for text (exact trigger + type) and IoU ≥0.5 for visual bounding boxes.

## Key Results
- Few-shot LVLMs excel in visual tasks (Qwen2.5-VL-72B: F1 71.8% for visual ED) but lag in textual tasks
- Fine-tuning with LoRA significantly boosts performance (Qwen2-VL-7B: +36.6% F1 for ED, +14.4% for EAE)
- Cross-modal settings yield best results (Qwen2.5-VL-7B: F1 82.5% for multimedia ED) due to modality synergy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-modal synergy improves event extraction when both text and image modalities are available
- Mechanism: Visual context enriches and disambiguates textual information while grounding events
- Core assumption: Complementary information across modalities provides disambiguation signals
- Evidence anchors: Abstract states LVLMs exhibit strong synergy in cross-modal settings; section 4.5 shows models that struggled with text-only ED improve significantly with images
- Break condition: Contradictory information across modalities or safety filters blocking sensitive content

### Mechanism 2
- Claim: Parameter-efficient fine-tuning with LoRA substantially closes performance gap
- Mechanism: LoRA adapts model weights to task-specific patterns without full fine-tuning
- Core assumption: M²E² task structure is learnable through low-rank weight updates
- Evidence anchors: Abstract states LoRA substantially enhances performance; section 4.5 shows Qwen2-VL-7B achieves 36.6% F1 increase for ED with LoRA
- Break condition: Insufficient training data or LoRA rank too low to capture task complexity

### Mechanism 3
- Claim: Pre-training scale determines out-of-the-box visual capability
- Mechanism: Large-scale vision-language pre-training encodes visual event knowledge that transfers directly to visual EE
- Core assumption: Visual event understanding is more pattern-matching driven than textual EE
- Evidence anchors: Abstract states few-shot LVLMs perform better on visual tasks; section 4.5 shows Qwen2.5-VL-72B achieves F1 71.8% for visual ED
- Break condition: Visual events require precise localization and model lacks coordinate-aware pre-training

## Foundational Learning

- Concept: Event Extraction Structure (trigger, arguments, roles)
  - Why needed here: M²E² defines events as (trigger τ, arguments α, roles ρ) with grounding in text/image/both
  - Quick check question: Can you distinguish between an event trigger and an event argument in the sentence "Two Chicago police officers take a man into custody"?

- Concept: Modality-Specific Grounding (text-only, image-only, cross-media)
  - Why needed here: The benchmark evaluates six subtasks based on where event information appears
  - Quick check question: For a multimedia document about a protest, if the text mentions "arrest" but the image shows a vehicle, which modality grounds the Conflict:Attack event?

- Concept: LoRA (Low-Rank Adaptation)
  - Why needed here: The paper's main improvement comes from LoRA fine-tuning
  - Quick check question: Why might LoRA be preferred over full fine-tuning when adapting a 7B LVLM to a task with only ~700 training examples?

## Architecture Onboarding

- Component map:
  Input layer (multimodal document) -> Prompt construction (instruction, demonstrations, options, answer placeholder) -> LVLM backbone (DeepSeek-VL2, Qwen-VL series) -> LoRA adapters (optional) -> Structured event prediction output -> Evaluation metrics

- Critical path:
  1. Dataset preparation: Merge M²E² training split (70%) with filtered ACE 2005 for target event types
  2. Prompt design: 4 contrastive demonstrations (positive + negative), candidate options, task-specific instruction
  3. Training (if fine-tuning): LoRA with lr=1×10⁻⁴, batch size 3, temperature=0 for inference
  4. Evaluation: Separate metrics for text-only, image-only, cross-media ED and EAE

- Design tradeoffs:
  - Few-shot vs. fine-tuning: Few-shot requires no training data but lags textually (F1 20.6% vs 52.5% with LoRA)
  - Model scale: 72B models excel at visual few-shot (F1 71.8%) but are costly; 7B + LoRA matches performance (F1 65.4%) with lower inference cost
  - Modality coverage: Cross-media yields best results (F1 65.4-82.5%) but requires both modalities annotated

- Failure signatures:
  - Semantic confusion: Confusing Contact:Meet with Contact:Phone-Write
  - Span overextension: Predicting "Iraqi government forces" instead of "forces"
  - Spurious arguments: Labeling "Uganda" as place argument when not relevant
  - Localization errors: Predicting visually prominent objects that aren't event arguments
  - Safety filter blocking: Refusing to process Conflict:Attack or Life:Die content

- First 3 experiments:
  1. Establish few-shot baseline: Run Qwen2.5-VL-7B and 72B on all six M²E² subtasks with 4-shot prompts
  2. LoRA fine-tuning ablation: Fine-tune Qwen2.5-VL-7B with LoRA on M²E² training split alone vs. M²E² + ACE 2005
  3. Cross-modal grounding analysis: Evaluate multimedia ED with image-only, text-only, and both modalities

## Open Questions the Paper Calls Out

- Can cross-modal synergy and fine-tuning improvements generalize to domains outside news articles (e.g., social media, scientific reports)?
- How can internal safety filters be modified to distinguish between harmful content generation and legitimate analysis of sensitive events?
- What specific architectural components are required to resolve persistent challenges in cross-modal grounding and bounding box localization?
- What is the root cause of performance disparity where few-shot LVLMs excel at visual tasks but lag behind PLMs in textual tasks?

## Limitations

- Dataset scale constraints: M²E² contains only 395 cross-media event mentions, limiting LoRA fine-tuning generalization
- Methodological opacity: Critical implementation details like LoRA hyperparameters and complete prompt templates are missing
- Safety filter sensitivity: Internal safety filters block processing of certain event types, artificially reducing recall scores
- Limited model diversity: Evaluation restricted to Qwen and DeepSeek model families only

## Confidence

**High Confidence**: Few-shot LVLMs perform better on visual than textual tasks; cross-modal settings yield superior performance; LoRA fine-tuning substantially improves performance

**Medium Confidence**: Proposed few-shot prompting achieves state-of-the-art results; Qwen2.5-VL-72B's visual superiority is due to larger scale; auxiliary ACE 2005 training is beneficial

**Low Confidence**: Model scaling laws; safety filter impact quantification; generalization to other LVLM families

## Next Checks

1. **Safety Filter Impact Analysis**: Systematically measure recall degradation by event type when safety filters are enabled versus disabled

2. **LoRA Hyperparameter Sensitivity**: Conduct controlled experiments varying LoRA rank and learning rates to identify optimal configurations

3. **Cross-LVLM Generalization**: Evaluate the same few-shot and fine-tuning protocols on OpenAI GPT-4V, Google Gemini, and Anthropic Claude models