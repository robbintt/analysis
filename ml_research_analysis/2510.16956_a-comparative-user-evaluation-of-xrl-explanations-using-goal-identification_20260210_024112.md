---
ver: rpa2
title: A Comparative User Evaluation of XRL Explanations using Goal Identification
arxiv_id: '2510.16956'
source_url: https://arxiv.org/abs/2510.16956
tags:
- explanation
- goal
- users
- agent
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel evaluation methodology for explainable
  reinforcement learning (XRL) called "goal identification," where users predict an
  agent's underlying objective from explanations of its decision-making. The approach
  trains multiple agents with different reward functions in Ms.
---

# A Comparative User Evaluation of XRL Explanations using Goal Identification

## Quick Facts
- arXiv ID: 2510.16956
- Source URL: https://arxiv.org/abs/2510.16956
- Reference count: 20
- Primary result: Only one XRL explanation mechanism (Dataset Similarity Explanation) achieved significantly better-than-random accuracy (53.0%) in helping users identify agent goals

## Executive Summary
This paper introduces a novel evaluation methodology for explainable reinforcement learning (XRL) called "goal identification," where users predict an agent's underlying objective from explanations of its decision-making. The approach trains multiple agents with different reward functions in Ms. Pacman, then tests whether explanations help users distinguish between these goals. A user study with 100 participants evaluated four XRL explanation algorithms across four agent goals. Results showed that only one explanation mechanism (Dataset Similarity Explanation) achieved significantly better-than-random accuracy (53.0%), while others performed near chance levels (22.5-34.9%). Critically, users' self-reported confidence, ease of identification, and understanding showed weak or no correlation with actual accuracy, and users were often overconfident in their predictions. This highlights a significant gap between subjective user preferences and objective explanatory effectiveness in XRL, suggesting that current explanations may not adequately support real-world debugging tasks.

## Method Summary
The paper proposes a user-centered evaluation framework for XRL algorithms where participants must identify an agent's goal from provided explanations. Four DQN agents with distinct reward functions (Eat Dots, Eat Energy Pills and Ghosts, Survive, Lose a Life) were trained in Ms. Pacman for 10 million steps. Four explanation algorithms were implemented: Dataset Similarity Explanation (DSE), TRD Summarisation, SARFA (saliency maps), and Optimal Action Description (OAD). A user study with 100 participants evaluated these explanations across 20 observations, measuring accuracy in goal identification along with self-reported confidence and ease of understanding via Likert scales.

## Key Results
- Dataset Similarity Explanation achieved 53.0% accuracy, significantly better than random chance
- TRD Summarisation, SARFA, and OAD all performed near chance levels (22.5-34.9%)
- User self-reported confidence and ease showed weak or no correlation with actual accuracy (rho=-0.0181 and 0.1614)
- Users were significantly overconfident in their predictions, particularly for OAD and SARFA explanations

## Why This Works (Mechanism)
The goal identification methodology works by creating a controlled environment where explanation effectiveness can be objectively measured through binary classification tasks. By training agents with distinct, interpretable reward functions and presenting users with specific observations and explanations, the study isolates the explanatory capability of different XRL algorithms from confounding factors like policy performance or environment complexity.

## Foundational Learning
- **Goal identification methodology**: A framework for evaluating XRL by having users predict agent objectives from explanations; needed to create standardized, objective evaluation metrics for XRL algorithms
- **Dataset Similarity Explanation (DSE)**: An explanation method that compares current states to training data distributions; quick check: verify it identifies relevant state features that distinguish between reward functions
- **SARFA (State-Action Relevance Factor Analysis)**: A saliency-based explanation technique highlighting important features; quick check: confirm saliency maps align with human-identified relevant features
- **Optimal Action Description (OAD)**: A method explaining why an agent chose a particular action; quick check: validate that explanations consistently reference goal-relevant features
- **User confidence calibration**: The relationship between subjective confidence and actual performance; quick check: measure correlation between Likert ratings and accuracy

## Architecture Onboarding

**Component Map**: Ms. Pacman environment -> DQN agents with reward wrappers -> Explanation generators -> User interface with observations

**Critical Path**: Training agents -> Generating explanations for specific observations -> User study interface -> Accuracy and confidence measurement

**Design Tradeoffs**: 
- Single environment (Ms. Pacman) ensures controlled comparison but limits generalizability
- Binary classification task simplifies evaluation but may not capture complex goal relationships
- Self-reported metrics provide subjective insights but can be unreliable

**Failure Signatures**: 
- User accuracy near chance level indicates explanation failure
- Negative correlation between confidence and accuracy suggests explanation confusion
- Consistent misclassification patterns reveal systematic misunderstanding

**Three First Experiments**:
1. Test a single explanation algorithm on a simple environment to verify implementation correctness
2. Validate that users can distinguish goals without explanations (baseline performance)
3. Compare explanation effectiveness on a different RL environment to test generalizability

## Open Questions the Paper Calls Out
**Open Question 1**: To what extent does the misalignment between human-perceived feature importance and neural network feature importance drive the failure of saliency-based explanations (like SARFA) in goal identification? The authors note users consistently misidentified goals when viewing saliency maps, but the exact mechanism—whether due to user misunderstanding or fundamental disconnect between human logic and network gradients—remains unverified.

**Open Question 2**: Does a user sampling bias toward expecting rational agent behavior negatively impact the accuracy of Optimal Action Description (OAD) explanations? The study showed users disproportionately predicted "Survival" and "Eat Dots" over irrational goals like "Lose a Life," but didn't isolate whether this was explanation failure or psychological prior.

**Open Question 3**: Does increasing the optimality of the underlying agent's policy (e.g., via Rainbow DQN) improve user accuracy in goal identification compared to standard DQN agents? The authors reflect that agents occasionally took sub-optimal actions from a human perspective, raising questions about whether low accuracy was caused by explanation methods or erratic behavior.

## Limitations
- Artificial task design may not generalize to real-world scenarios with complex, partially unknown reward structures
- 100-participant sample size may not capture sufficient demographic diversity for generalizability
- Single game environment (Ms. Pacman) constrains external validity across different RL domains

## Confidence
- **High Confidence**: The empirical finding that user-reported confidence and ease measures poorly correlate with actual goal identification accuracy (rho=-0.0181 and 0.1614)
- **Medium Confidence**: The conclusion that current XRL explanations are inadequate for real-world debugging tasks, extrapolating beyond the specific evaluation methodology
- **Medium Confidence**: The comparative performance ranking of the four explanation algorithms is reliable for this specific task and implementation

## Next Checks
1. **External Validation**: Replicate the study using a different RL environment (e.g., LunarLander or Atari Breakout) to test whether weak explanatory effectiveness generalizes beyond Ms. Pacman.

2. **Component Analysis**: Conduct ablation studies on the top-performing explanation algorithm (DSE) to isolate which specific components (similarity metrics, visualization techniques, or textual framing) drive its relative effectiveness.

3. **Expert User Study**: Repeat the evaluation with RL practitioners or AI researchers rather than general users to determine whether domain expertise improves explanation effectiveness or whether fundamental limitations are technical rather than educational.