---
ver: rpa2
title: 'LaTIM: Measuring Latent Token-to-Token Interactions in Mamba Models'
arxiv_id: '2502.15612'
source_url: https://arxiv.org/abs/2502.15612
tags:
- latim
- fdcvcu
- mamba
- attention
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LaTIM, a novel token-level interpretability
  method for Mamba models that addresses the lack of interpretability tools in state
  space models. LaTIM reformulates Mamba's computation to enable fine-grained analysis
  of token-to-token interactions, adapting attention-based techniques like ALTI to
  the Mamba architecture.
---

# LaTIM: Measuring Latent Token-to-Token Interactions in Mamba Models

## Quick Facts
- **arXiv ID**: 2502.15612
- **Source URL**: https://arxiv.org/abs/2502.15612
- **Reference count**: 40
- **Primary result**: Introduces LaTIM, a token-level interpretability method for Mamba models achieving 0.88 AUC/AP (Mamba-1) and 0.98 AUC/AP (Mamba-2) on copying tasks, with clearer visualizations than existing methods

## Executive Summary
This paper addresses the interpretability gap in State Space Models (SSMs) like Mamba by introducing LaTIM, a novel token-level attribution method. LaTIM reformulates Mamba's sequential recurrence as a matrix multiplication to reveal hidden attention-like interactions between tokens, then decomposes these interactions through an additive approximation of non-linear activations. The method is evaluated across three diverse tasks and demonstrates superior performance compared to existing Mamba attribution techniques, while also revealing fundamental limitations in Mamba's selective processing mechanisms.

## Method Summary
LaTIM interprets Mamba models by mathematically unrolling the recurrent state space computation into a matrix multiplication, revealing "hidden" attention maps between tokens. The method approximates the non-linear SiLU activation as an additive function, enabling decomposition of the output into contributions from previous tokens. LaTIM can be applied exactly (by retraining with identity activation for 0 error) or approximately (for off-the-shelf use with slight error). The approach adapts attention-based interpretability techniques to Mamba's architecture, using either $\ell_2$ norm aggregation or ALTI-based logit attribution to produce interpretable heatmaps.

## Key Results
- Achieves AUC of 0.88 and AP of 0.41 for Mamba-1, and AUC of 0.98 and AP of 0.86 for Mamba-2 on synthetic copying tasks
- Improves translation alignment with AER of 0.46 for Mamba-1 and 0.49 for Mamba-2, outperforming Mamba-Attention and MambaLRP
- Provides clearer visualizations of token interactions, revealing Mamba's selective processing and decay mechanisms
- Exact LaTIM version (Identity activation) achieves zero decomposition error while maintaining task performance

## Why This Works (Mechanism)

### Mechanism 1: Recurrence as Attention Matrices
Mamba's sequential recurrence can be reformulated as a linear matrix multiplication, revealing hidden attention maps. The SSM recurrence $H_i = A_i H_{i-1} + B_i X_i$ is unrolled to show the output $\Upsilon = MX$, where the lower-triangular matrix $M$ represents interaction strengths between tokens derived from cumulative products of discrete parameters.

### Mechanism 2: Additive Decomposition of Non-Linearities
Token contributions can be isolated despite SiLU non-linearity by approximating the activation function as additive. The output $y_i$ is expressed as a sum of contributions $T_i(x_j)$ from previous tokens, allowing the use of norm-based attribution metrics while treating the non-linear activation as a sum of additive contributions.

### Mechanism 3: Input-Dependent Selection via Decay Analysis
LaTIM reveals that Mamba's selective processing manifests as decay of focus on repeated or distant tokens. By visualizing contribution scores $C_{i,j}$, the method shows Mamba maintains high attention on recent or initial tokens but struggles to sustain focus on repeated distractors or mid-sequence keys.

## Foundational Learning

- **State Space Models (SSMs)**: Understanding how SSMs compress history into state matrices is crucial for grasping the matrix unrolling mechanism. *Quick check*: Can you explain how discretization parameters $A$ and $B$ control memory vs. new input retention?
- **Layer-wise Relevance Propagation (LRP)**: The paper uses norm-based attribution metrics and contrasts itself against gradient-based methods. *Quick check*: Why might raw attention weights be considered "unfaithful" without value-weighted normalization?
- **Taylor Expansion / Approximation Theory**: Section 3.3 discusses approximating SiLU activation using Taylor expansions. *Quick check*: Why does SiLU prevent direct linear decomposition of token contributions?

## Architecture Onboarding

- **Component map**: Input -> Conv1D -> SiLU (Activation) -> SSM (Recurrence) -> Gating -> Output
- **Critical path**: Unroll recurrence into matrix multiplication, apply additive function to handle SiLU, project contributions through gating and output projection
- **Design tradeoffs**: Exact vs. Approximate strategies (Identity activation for 0 error vs. SiLU for off-the-shelf use), $\ell_2$ norms vs. ALTI-Logit for metric choice
- **Failure signatures**: Approximation drift in deeper layers, multi-key bias where model focuses on first key regardless of query, memory overflow from storing NxN interaction matrix
- **First 3 experiments**:
  1. Sanity check on copying task: Train Mamba on `<source> <SEP> copy` and verify diagonal heatmap pattern
  2. Decomposition error test: Compare original Mamba output vs. LaTIM-decomposed reconstruction using MSE
  3. AER evaluation: Fine-tune Mamba on MT task and compute Alignment Error Rate using LaTIM vs. Mamba-Attention

## Open Questions the Paper Calls Out
- Does LaTIM lead to measurable improvements in Mamba model robustness and trustworthiness?
- How can LaTIM be adapted for hybrid architectures combining SSMs with attention mechanisms?
- What architectural modifications are required to mitigate the "decaying attention" problem in multi-key retrieval?

## Limitations
- Theoretical error bounds on approximation are unclear, particularly for deeper models
- Method's effectiveness for applications beyond copying, translation, and retrieval hasn't been demonstrated
- Matrix unrolling approach becomes computationally prohibitive for long sequences

## Confidence
- **High Confidence**: Experimental validation with multiple metrics shows LaTIM produces interpretable heatmaps and outperforms existing techniques
- **Medium Confidence**: Theoretical derivation with empirical support for mathematical reformulation and additive decomposition
- **Low Confidence**: Limited validation for applications beyond the three evaluated tasks and scalability concerns

## Next Checks
1. Conduct error propagation analysis across different activation intensities and model depths to establish theoretical error bounds
2. Apply LaTIM to at least two additional Mamba applications (e.g., long-form generation, structured prediction) to validate broader applicability
3. Benchmark LaTIM's performance and memory usage on sequences of increasing length (128, 512, 1024, 2048 tokens) to establish practical limits