---
ver: rpa2
title: 'GeoEvolve: Automating Geospatial Model Discovery via Multi-Agent Large Language
  Models'
arxiv_id: '2509.21593'
source_url: https://arxiv.org/abs/2509.21593
tags:
- geospatial
- spatial
- geoevolve
- knowledge
- kriging
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GeoEvolve, a multi-agent LLM framework that
  automates geospatial model discovery by integrating evolutionary search with domain
  knowledge via a geospatial knowledge retriever. GeoEvolve operates with an inner
  loop that generates and mutates candidate algorithms, and an outer agentic controller
  that evaluates results and guides the search using structured geospatial knowledge.
---

# GeoEvolve: Automating Geospatial Model Discovery via Multi-Agent Large Language Models

## Quick Facts
- arXiv ID: 2509.21593
- Source URL: https://arxiv.org/abs/2509.21593
- Authors: Peng Luo; Xiayin Lou; Yu Zheng; Zhuo Zheng; Stefano Ermon
- Reference count: 22
- Primary result: GeoEvolve improves kriging RMSE by 13-21% and reduces conformal prediction interval scores by 17% through knowledge-guided evolutionary search

## Executive Summary
This paper introduces GeoEvolve, a multi-agent LLM framework that automates geospatial model discovery by integrating evolutionary search with domain knowledge via a geospatial knowledge retriever. GeoEvolve operates with an inner loop that generates and mutates candidate algorithms, and an outer agentic controller that evaluates results and guides the search using structured geospatial knowledge. Evaluated on spatial interpolation and uncertainty quantification, GeoEvolve significantly outperforms both baseline and knowledge-augmented variants. Ablation studies confirm that domain-guided retrieval is essential for stable, high-quality evolution.

## Method Summary
GeoEvolve is a multi-agent LLM framework that automates geospatial model discovery through nested evolutionary loops. The system uses an inner loop (OpenEvolve) for code mutation and selection, and an outer loop that analyzes results, retrieves geospatial knowledge via RAG, and guides subsequent evolution. The framework employs a geospatial knowledge retriever (GeoKnowRAG) to inject domain theory into the search process, steering algorithm evolution toward theoretically valid improvements. Experiments use Australia soil trace-element data for kriging and King County housing data for spatial uncertainty quantification.

## Key Results
- Improves kriging RMSE by 13-21% compared to baseline algorithms
- Reduces conformal prediction interval scores by 17% for uncertainty quantification
- Ablation studies confirm domain-guided retrieval is essential for stable, high-quality evolution
- Demonstrates knowledge-augmented search outperforms both original algorithms and OpenEvolve alone

## Why This Works (Mechanism)

### Mechanism 1: Knowledge-Guided Search Space Reduction
Injecting domain-specific geospatial theory via RAG constrains the evolutionary search space, steering code mutation toward theoretically valid improvements. The framework uses a Code Analyzer to identify structural weaknesses and queries GeoKnowRAG for specific geospatial priors, which inform a Prompt Generator that constrains the Code Evolver to mutate along physically meaningful dimensions.

### Mechanism 2: Nested Evolutionary Loops (Global vs. Local Optimization)
Decoupling search into fast inner-loop mutations and slow outer-loop strategic guidance prevents "forgetting" successful strategies while allowing exploration. The system runs OpenEvolve for 10 steps (inner loop), then pauses for outer-loop analysis of global elites and injection of high-level theoretical guidance.

### Mechanism 3: Semantic Feedback for Query Formulation
Using an LLM to semantically analyze code enables precise knowledge queries, creating a tighter feedback loop than scalar reward signals alone. The Evolved Code Analyzer examines code structure and generates specific search queries for the knowledge base, directing retrieval toward missing theoretical components.

## Foundational Learning

- **Concept: Spatial Autocorrelation & Variograms**
  - Why needed: Kriging relies on the idea that nearby things are more similar (autocorrelation). Understanding variograms (modeling how similarity decays with distance) is essential for comprehending algorithmic improvements like Matérn family and binning.
  - Quick check: Can you explain why "ordinary kriging" assumes a stationary mean and how a variogram models spatial dependency?

- **Concept: Evolutionary Search / Genetic Algorithms**
  - Why needed: The framework is built on OpenEvolve, which uses mutation and selection. Understanding that the system generates populations of code snippets, selects the "fittest" (lowest RMSE), and mutates them to find optimal algorithms is crucial.
  - Quick check: In the context of this paper, what acts as the "fitness function" for the evolutionary algorithm?

- **Concept: Retrieval-Augmented Generation (RAG)**
  - Why needed: GeoKnowRAG is the core differentiator. Understanding how chunking text, creating embeddings, and performing vector search allow an LLM to access external knowledge not present in its training data is essential.
  - Quick check: How does the GeoKnowRAG module use "Reciprocal Rank Fusion" to improve the quality of retrieved geospatial knowledge?

## Architecture Onboarding

- **Component map**: Code Evolver -> Evolved Code Analyzer -> GeoKnowRAG -> Geo-informed Prompt Generator
- **Critical path**: The outer loop sequence is the bottleneck and value driver: Evaluation -> Analysis -> Retrieval -> Prompting -> Evolution
- **Design tradeoffs**: Latency vs. Stability (nested loops offer higher stability but are slower); Generality vs. Specificity (manually curated knowledge base ensures relevance but limits breadth)
- **Failure signatures**: Knowledge Drift (evolving algorithms that fit data but violate theory), RAG Mismatch (retrieving generic instead of domain-specific knowledge), Syntax Decay (theoretically sound but syntactically invalid code)
- **First 3 experiments**:
  1. Run GeoEvolve vs. GeoEvolve without GeoKnowRAG on simple interpolation to confirm 10-15% performance difference
  2. Remove key concepts (e.g., Matérn covariance) from RAG database and observe discovery failure
  3. Vary inner-to-outer loop ratios (5x20 vs 20x5) to determine optimal theoretical intervention frequency

## Open Questions the Paper Calls Out
- How does GeoEvolve performance vary across different foundation models (e.g., GPT-4, Claude, Gemini, open-source LLMs)?
- How does GeoEvolve performance scale with the size and comprehensiveness of the geospatial knowledge base?
- Can GeoEvolve successfully discover improved algorithms for geospatial tasks beyond spatial interpolation and uncertainty quantification?

## Limitations
- Knowledge base generalizability is limited by the manually curated corpus of 141 documents
- Critical prompt templates for code analyzer and prompt generator are not fully specified
- Computational overhead of nested loops (100 iterations) vs. single-pass approaches is not characterized

## Confidence
- **High Confidence**: Ablation study results showing GeoKnowRAG's importance (13-21% RMSE improvement)
- **Medium Confidence**: Nested loop architecture's effectiveness demonstrated but needs sensitivity analysis
- **Low Confidence**: Claims about semantic feedback enabling precise knowledge queries assume code analyzer's domain expertise without sufficient validation

## Next Checks
1. **Knowledge Base Generalization**: Remove specific geospatial concepts from the RAG database and measure impact on discovery performance
2. **Prompt Template Verification**: Implement and test exact prompt templates using synthetic code failures to validate semantic understanding
3. **Loop Architecture Sensitivity**: Conduct experiments varying inner-to-outer loop ratio across multiple geospatial problems to determine optimal intervention frequency