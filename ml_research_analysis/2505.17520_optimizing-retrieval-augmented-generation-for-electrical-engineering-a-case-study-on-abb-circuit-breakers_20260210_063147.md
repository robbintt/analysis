---
ver: rpa2
title: 'Optimizing Retrieval-Augmented Generation for Electrical Engineering: A Case
  Study on ABB Circuit Breakers'
arxiv_id: '2505.17520'
source_url: https://arxiv.org/abs/2505.17520
tags:
- chunking
- retrieval
- context
- page
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study evaluates Retrieval-Augmented Generation (RAG) systems
  for ABB circuit breaker documentation, addressing challenges in accuracy and contextual
  relevance for engineering applications. Three RAG pipelines (OpenAI GPT4o, Cohere,
  Anthropic Claude) were tested using domain-specific datasets and three chunking
  methods (basic, paragraph-per-page, by-title).
---

# Optimizing Retrieval-Augmented Generation for Electrical Engineering: A Case Study on ABB Circuit Breakers

## Quick Facts
- **arXiv ID**: 2505.17520
- **Source URL**: https://arxiv.org/abs/2505.17520
- **Reference count**: 16
- **Primary result**: RAGAS evaluation shows variability across chunking methods, with Claude's paragraph-per-page chunking achieving highest faithfulness (0.8556), while Cohere's paragraph-per-page method excelled in context recall (0.7705).

## Executive Summary
This study evaluates Retrieval-Augmented Generation (RAG) systems for technical documentation on ABB SACE Emax 2 circuit breakers. Three RAG pipelines (OpenAI GPT4o, Cohere, Anthropic Claude) were tested across three chunking methods using domain-specific datasets. The research reveals that while certain configurations achieve high precision, limitations persist in factual faithfulness and completeness. The study demonstrates that how documents are chunked directly impacts retrieval accuracy and contextual relevance, with paragraph-per-page chunking generally performing best for engineering applications.

## Method Summary
The research evaluated three RAG pipelines (OpenAI GPT4o, Cohere, Anthropic Claude) using 12 ABB Emax 2 PDF documents converted to text with 537,708 tokens total. Three chunking methods were implemented via Unstructured's partition_pdf: Basic (max_chars=1000), Paragraph-per-Page (max_chars=1500, multipage_sections=False), and By-Title (max_chars=4000). Each pipeline used Chroma vector store with different embeddings (Cohere, OpenAI Ada, Voyage), top-10 retrieval via cosine similarity, and LLMs with 4k context and temperature=0. Evaluation used 31 manually curated QA pairs with RAGAS metrics measuring Faithfulness, Context Recall, Context Precision, and Answer Relevancy.

## Key Results
- Claude's paragraph-per-page chunking achieved the highest faithfulness score (0.8556)
- Cohere's paragraph-per-page method excelled in context recall (0.7705)
- Despite high precision in some configurations, systems exhibited limitations in factual faithfulness and completeness, particularly retrieving critical technical details
- Basic chunking method produced incorrect resistance values (220Ω or 390Ω instead of 120Ω) despite high context precision scores

## Why This Works (Mechanism)

### Mechanism 1
Page-boundary-aware chunking preserves contextual integrity better than arbitrary character segmentation for semi-structured engineering PDFs. By enforcing page boundaries and grouping by paragraphs, this method maintains the spatial relationship between text and associated images/tables, preventing context fragmentation where critical details are severed from their instructional headers.

### Mechanism 2
High RAGAS precision scores do not guarantee factual correctness in high-stakes engineering tasks. Standard retrieval metrics measure presence of relevant context, not correctness of generated synthesis. A model can retrieve relevant documents (high precision) but still hallucinate specific values if the specific token is ambiguous or if the model relies on prior training data over retrieved context.

### Mechanism 3
Larger context windows (4000 tokens) combined with low temperature (0) reduce hallucination frequency compared to smaller windows. Increasing the context window allows the system to include more retrieved chunks without truncation, providing the LLM with the full evidence chain. Setting temperature to 0 forces the model to select the highest probability token based strictly on this context, reducing randomness.

## Foundational Learning

- **Concept**: Chunking Granularity (Semantics vs. Structure)
  - **Why needed here**: How you split a PDF directly impacts whether the RAG system retrieves the correct schematic or a hallucinated value
  - **Quick check question**: Does splitting a document by arbitrary character count risk separating a component's name from its safety rating?

- **Concept**: Context Precision vs. Faithfulness (RAGAS Metrics)
  - **Why needed here**: One must understand that "Precision" measures "did we get the right documents?" while "Faithfulness" measures "did the answer stick to those documents?"
  - **Quick check question**: If a system retrieves the correct manual (100% Precision) but invents a safety threshold, does it pass the "Faithfulness" test?

- **Concept**: Temperature in High-Stakes Generation
  - **Why needed here**: The paper mandates Temperature=0 to ensure deterministic outputs, a critical safety requirement in electrical engineering where "creative" answers are dangerous
  - **Quick check question**: Why would a "creative" (Temperature > 0) response be hazardous when configuring a circuit breaker?

## Architecture Onboarding

- **Component map**: PDF Ingestion (Unstructured partition_pdf) -> Chunking (Basic/Para-Page/By-Title) -> Enrichment (GPT4o-mini, Temp=0) -> Storage (Chroma Vector Store + Embeddings) -> Retrieval (Cosine Similarity, Top 10) -> Generation (LLMs with 4k context, Temp=0)

- **Critical path**: The partition_pdf parameters (e.g., max_characters, new_after_n_chars) determine data fidelity. If this step creates orphaned text, the embedding step creates misleading vectors, and the LLM cannot recover the context.

- **Design tradeoffs**:
  - By-Title Chunking: Best for Context Precision (1.0), but highest risk of "I don't know" responses
  - Paragraph-per-Page: Best balance of Recall and Faithfulness
  - Basic Chunking: Fastest, but produces the most dangerous hallucinations

- **Failure signatures**:
  - "Silent Hallucination": Confidently stating wrong values (e.g., 390Ω vs 120Ω)
  - "Refusal to Answer": Generating "I don't know" despite information existing in corpus

- **First 3 experiments**:
  1. Reproduce the "Paragraph-per-Page" baseline with multipage_sections=False and max_characters=1500
  2. Stress Test Faithfulness by querying for specific numerical values using different chunking methods
  3. Implement iterative query refinement to resolve "I don't know" failures observed in GPT-4o pipeline

## Open Questions the Paper Calls Out

### Open Question 1
Does integrating domain-specific Knowledge Graphs (KGs) with vector search significantly reduce hallucinations in RAG responses for technical electrical engineering queries? The authors propose KGs could model relationships between entities to enable fact verification and improve retrieval accuracy, but the current study relied solely on vector databases without structured graph layers.

### Open Question 2
To what extent does multimodal data integration (schematics and tables) improve the completeness of answers where text-only retrieval currently fails? The paper identifies "Addressing Data Gaps" as critical and suggests using Vision Transformers and table parsing for multimodal RAG, as PDF-to-text conversion may lose visual context required for specific tasks.

### Open Question 3
Does the "Paragraph-per-Page" chunking method retain its superiority regarding Context Recall when the dataset is scaled to include multiple circuit breaker models? The authors note that focusing on a single circuit breaker was a key limitation and that scaling introduces the risk of compounding inaccuracies.

## Limitations
- The 31 evaluation questions are not fully disclosed, limiting reproducibility and independent validation
- The study does not report how often numerical hallucinations occur across all queries, only highlighting isolated examples
- No ablation study isolates the impact of context window size versus temperature on hallucination rates

## Confidence

- **High confidence**: Page-boundary-aware chunking improves contextual integrity over arbitrary segmentation for semi-structured PDFs
- **Medium confidence**: Larger context windows (4000 tokens) reduce truncation-related failures
- **Low confidence**: Specific numerical hallucinations (220Ω, 390Ω vs 120Ω) are isolated incidents; the study does not quantify their frequency or root cause

## Next Checks
1. Reproduce the "Paragraph-per-Page" baseline with multipage_sections=False and max_characters=1500
2. Stress Test Faithfulness by querying for specific numerical values using different chunking methods
3. Implement iterative query refinement to resolve "I don't know" failures observed in GPT-4o pipeline