---
ver: rpa2
title: 'From Sublinear to Linear: Fast Convergence in Deep Networks via Locally Polyak-Lojasiewicz
  Regions'
arxiv_id: '2507.21429'
source_url: https://arxiv.org/abs/2507.21429
tags:
- local
- convergence
- loss
- networks
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper resolves a key theoretical puzzle in deep learning by
  explaining why gradient descent often exhibits near-exponential convergence despite
  operating on highly non-convex loss landscapes. The authors bridge the gap between
  sublinear convergence guarantees in locally quasi-convex regions (LQCRs) and the
  rapid training dynamics observed in practice.
---

# From Sublinear to Linear: Fast Convergence in Deep Networks via Locally Polyak-Lojasiewicz Regions

## Quick Facts
- **arXiv ID**: 2507.21429
- **Source URL**: https://arxiv.org/abs/2507.21429
- **Reference count**: 8
- **Primary result**: Shows that regions satisfying local quasi-convexity (LQCRs) also satisfy local Polyak-Lojasiewicz (PL) conditions under mild NTK stability assumptions, enabling linear convergence of gradient descent

## Executive Summary
This paper resolves a key theoretical puzzle in deep learning by explaining why gradient descent often exhibits near-exponential convergence despite operating on highly non-convex loss landscapes. The authors bridge the gap between sublinear convergence guarantees in locally quasi-convex regions (LQCRs) and the rapid training dynamics observed in practice. The core insight is that under a mild local Neural Tangent Kernel (NTK) stability assumption, LQCRs also satisfy a local Polyak-Lojasiewicz (PL) condition, establishing the existence of Locally Polyak-Łojasiewicz Regions (LPLRs) where squared gradient norm lower-bounds the suboptimality gap, enabling linear convergence of gradient descent within these regions.

## Method Summary
The authors prove that when the Neural Tangent Kernel remains positive definite within a local region (NTK stability), the squared gradient norm lower-bounds the suboptimality gap, inducing a local PL condition. This creates LPLRs where gradient descent achieves linear convergence. The theory is validated empirically using a 5-layer fully-connected network on MNIST showing PL-like scaling and linear-rate loss decay, and a ResNet-style CNN trained with mini-batch SGD on CIFAR-10 exhibiting similar signatures. The results demonstrate that LPLR structure emerges robustly in practical deep learning scenarios, providing a theoretical explanation for fast optimization in standard architectures without architectural modifications.

## Key Results
- NTK stability within a local region implies a local PL condition, establishing LPLRs where gradient descent achieves linear convergence
- Empirical validation shows PL-like scaling and linear-rate loss decay in both fully-connected networks (MNIST) and ResNet-style CNNs (CIFAR-10)
- Width ablation demonstrates monotonic improvement in final loss as network width increases, supporting the width-PL relationship

## Why This Works (Mechanism)

### Mechanism 1: NTK Eigenvalue Lower-Bounds Suboptimality Gap
- Claim: When the Neural Tangent Kernel remains positive definite within a local region, the squared gradient norm lower-bounds the suboptimality gap, inducing a local PL condition.
- Mechanism: For empirical loss L(θ) = (1/2n)Σ(f_θ(x_i) - y_i)², the gradient norm satisfies ||∇L(θ)||² ≥ (λ_min/n)||f_θ - y||² ≥ 2λ_min(L(θ) - L*_R), where λ_min is the minimum eigenvalue of the NTK in region R. This yields PL constant μ = λ_min.
- Core assumption: The NTK remains positive definite with λ_min(Θ_θ) ≥ λ_min > 0 throughout the optimization region.
- Break condition: If the NTK becomes singular (λ_min → 0) or the iterates exit the stable region, the PL bound fails and convergence may revert to sublinear or stall.

### Mechanism 2: Linear Contraction from PL Condition
- Claim: Once a local PL condition holds, gradient descent achieves linear convergence with rate (1 - ηλ_min)^t.
- Mechanism: L-smoothness gives L(θ^(t+1)) ≤ L(θ^t) - (1/2L)||∇L||². Substituting the PL inequality ||∇L||² ≥ 2λ_min(L - L*_R) yields L^(t+1) - L*_R ≤ (1 - λ_min/L)(L^t - L*_R). For step size η ≤ 1/L, the rate becomes (1 - ηλ_min)^t.
- Core assumption: Loss function is L-smooth; iterates remain within the LPLR throughout training.
- Break condition: If iterates leave the LPLR, or if learning rate exceeds 1/L, the contraction guarantee fails.

### Mechanism 3: Width-Dependent Region Formation
- Claim: Sufficient over-parameterization promotes favorable local geometry where LPLRs are more likely to form and be reached.
- Mechanism: Wider networks have better-conditioned NTKs at initialization and greater stability during training, making λ_min larger and the LPLR radius wider. This increases both the probability of entering an LPLR and the achievable convergence rate.
- Core assumption: Width is a primary driver of over-parameterization effects on local geometry.
- Break condition: Assumption is not formally proven here; relationship may depend on depth, data complexity, and architecture beyond simple width scaling.

## Foundational Learning

- Concept: Polyak-Łojasiewicz (PL) Inequality
  - Why needed here: The PL condition (½||∇f||² ≥ μ(f - f*)) is the core sufficient condition for linear convergence on non-convex functions. Without understanding PL, the main theorem connecting NTK stability to fast rates is opaque.
  - Quick check question: For a function with PL constant μ = 0.1, loss gap Δ = 1.0, and step size η = 0.5, what is the loss gap after one gradient step? (Answer: Δ' ≤ (1 - 0.5×0.1)×1.0 = 0.95)

- Concept: Neural Tangent Kernel (NTK)
  - Why needed here: The NTK Θ_θ = J_θJ_θ^T governs training dynamics in the linearized regime. The minimum eigenvalue λ_min directly becomes the PL constant in this theory.
  - Quick check question: If the NTK has λ_min = 0 for some θ, what does this imply about the gradient at that point? (Answer: There exists a residual direction f_θ - y that produces zero gradient, creating a critical point that is not a global minimum)

- Concept: L-Smoothness
  - Why needed here: L-smoothness (||∇f(x) - ∇f(y)|| ≤ L||x - y||) bounds the descent lemma and determines the maximum stable learning rate η ≤ 1/L for convergence guarantees.
  - Quick check question: Why does the proof require η ≤ 1/L rather than allowing any step size? (Answer: The descent lemma L(x+) ≤ L(x) - (η - Lη²/2)||∇L||² requires η ≤ 2/L for descent; tighter η ≤ 1/L ensures the linear rate factor (1 - ηλ_min) remains positive)

## Architecture Onboarding

- Component map: Network width (m) -> λ_min stability -> LPLR radius -> convergence rate -> final loss

- Critical path:
  1. Choose width sufficient to ensure λ_min > 0 at initialization (empirically: wider networks achieve lower loss in ablation)
  2. Initialize with variance scaling appropriate to depth (enhanced init uses g(l,D) = (1 - |D/2 - l|/D)² factor)
  3. Set learning rate η ≤ 1/L (unknown L in practice; start small and monitor for linear decay on log scale)
  4. Monitor suboptimality gap on semi-log plot; linear decay indicates LPLR dynamics
  5. If loss plateaus above target, increase width or adjust initialization

- Design tradeoffs:
  - **Width vs. compute**: Larger m improves λ_min and LPLR radius but increases per-iteration cost
  - **Learning rate vs. stability**: Higher η speeds convergence if in LPLR but risks exiting the stable region
  - **Full-batch vs. SGD**: Theory proven for full-batch; SGD shows qualitative LPLR signatures (slope ~0.29 vs. ideal 1.0) but formal guarantees pending

- Failure signatures:
  - **Non-linear log-loss curve**: Indicates iterates may not be in an LPLR; consider width increase or different initialization
  - **Early stagnation**: Gradient norm decays but loss gap remains large; suggests λ_min ≈ 0 (poor conditioning)
  - **Slope << 1 in log-log gradient/loss plot**: Weak PL coupling; SGD noise or architecture may be degrading the LPLR structure
  - **Sharp early transient followed by slow decay**: Initialization may place iterates outside LPLR initially

- First 3 experiments:
  1. **Baseline LPLR verification**: Train a 5-layer MLP with width 2048 on MNIST binary subset with full-batch GD, η = 0.001. Plot suboptimality gap (L - L_final) on log scale. Confirm approximately linear decay region exists.
  2. **Width ablation**: Repeat with widths {512, 1024, 2048, 4096}, measuring final loss and fitting the log-log slope of ||∇L||² vs. (L - L*_R). Document how slope approaches 1.0 as width increases.
  3. **Initialization comparison**: Compare standard He init vs. enhanced depth-aware init. Measure early transient severity (loss in first 20 epochs) and long-run convergence rate. Report which init enters the linear-decay regime faster.

## Open Questions the Paper Calls Out
None

## Limitations
- The theory assumes full-batch gradient descent and does not fully account for SGD noise effects, though empirical evidence suggests LPLR structure persists with mini-batches
- The width-PL relationship is demonstrated empirically but lacks rigorous proof of causality
- The L-smoothness constant L is typically unknown in practice, making the precise learning rate bound η ≤ 1/L difficult to verify

## Confidence

**High Confidence**: The derivation connecting NTK positive definiteness to local PL conditions is mathematically sound and the linear convergence rate within LPLRs follows directly from standard PL theory combined with L-smoothness. The empirical signatures (linear decay in log-loss plots, PL-like scaling of ||∇L||² vs. loss gap) are clearly observable and reproducible.

**Medium Confidence**: The width ablation results show correlation between over-parameterization and improved LPLR characteristics, but the causal mechanism linking width to PL behavior requires more rigorous theoretical justification. The mini-batch SGD results are qualitative rather than quantitative, and the deviation from ideal linear rates (observed slope ~0.29 vs. ideal 1.0) needs systematic characterization.

**Low Confidence**: The claim that LPLR structure "robustly" emerges across practical scenarios is based on limited empirical validation. The relationship between depth, initialization schemes, and LPLR formation is not thoroughly characterized. The theory does not address scenarios where iterates exit LPLRs during training.

## Next Checks

1. **SGD Noise Characterization**: Systematically measure the deviation from ideal linear convergence in mini-batch SGD across varying batch sizes, and quantify the relationship between batch size, gradient noise, and observed convergence rates.

2. **Initialization Sensitivity Analysis**: Conduct a comprehensive study varying initialization schemes, depths, and network architectures to determine which factors most strongly influence the probability of starting within or quickly reaching an LPLR.

3. **LPLR Lifetime and Stability**: Track the evolution of λ_min(Θ_θ) throughout training to characterize how long iterates remain in regions with stable positive NTK eigenvalues, and identify failure modes where λ_min → 0.