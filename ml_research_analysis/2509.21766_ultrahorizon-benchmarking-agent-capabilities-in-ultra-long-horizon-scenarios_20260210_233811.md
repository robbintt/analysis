---
ver: rpa2
title: 'UltraHorizon: Benchmarking Agent Capabilities in Ultra Long-Horizon Scenarios'
arxiv_id: '2509.21766'
source_url: https://arxiv.org/abs/2509.21766
tags:
- tool
- offspring
- call
- rule
- cross
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "UltraHorizon is a benchmark for evaluating autonomous agents in\
  \ long-horizon, partially observable environments. It uses exploration as a unifying\
  \ task across three environments\u2014Mystery Grid, Sequence Exploration, and Alien\
  \ Genetics Laboratory\u2014requiring sustained reasoning, planning, memory management,\
  \ and tool use."
---

# UltraHorizon: Benchmarking Agent Capabilities in Ultra Long-Horizon Scenarios

## Quick Facts
- arXiv ID: 2509.21766
- Source URL: https://arxiv.org/abs/2509.21766
- Reference count: 40
- Primary result: Long-horizon autonomous agents consistently underperform humans on sustained exploration and reasoning tasks requiring hypothesis formation and memory management.

## Executive Summary
UltraHorizon introduces a benchmark for evaluating autonomous agents in long-horizon, partially observable environments. It uses exploration as a unifying task across three environments—Mystery Grid, Sequence Exploration, and Alien Genetics Laboratory—requiring sustained reasoning, planning, memory management, and tool use. Trajectories average 200k+ tokens and 400+ tool calls in the heaviest setting, and 35k+ tokens with 60+ tool calls in standard configurations. Experiments with state-of-the-art LLMs show consistent underperformance compared to human participants, revealing a persistent capability gap. Analysis of agent trajectories identifies eight error types rooted in in-context locking and foundational capability gaps. Simple scaling fails due to context overload; a lightweight context-refresh strategy improves performance. UltraHorizon highlights that long-horizon exploration demands advances beyond simple scaling, pointing toward principled memory integration, adaptive reasoning, and robust exploration strategies.

## Method Summary
UltraHorizon evaluates autonomous agents on three partially observable rule-discovery environments using tool-based interaction. Agents form hypotheses about hidden rules, conduct experiments, and submit final answers via a commit mechanism. Performance is scored by an LLM-as-Judge (DeepSeek-R1) with 20 points per correctly identified rule. Five LLMs were tested under fixed and free step settings. A Context Refresh with Notes Recall (CRNR) strategy mitigates context overload by clearing dialogue history near context limits while preserving system prompts and agent-maintained notes. Failure modes were categorized through analysis of 1,500+ agent trajectories, revealing patterns of in-context locking and memory-related errors.

## Key Results
- State-of-the-art LLMs consistently underperform humans on ultra-long-horizon exploration tasks requiring sustained reasoning and memory management
- Performance degrades monotonically with horizon length (rule count), not intrinsic rule complexity, indicating context and memory as primary bottlenecks
- CRNR context management strategy improves performance over naive scaling but absolute scores remain low
- Eight distinct failure types identified, with 23.2% of errors stemming from premature convergence to early hypotheses
- Token entropy analysis shows declining exploration diversity through trajectories, confirming in-context locking effects

## Why This Works (Mechanism)

### Mechanism 1: Context Refresh with Notes Recall (CRNR)
- **Claim:** A lightweight context management strategy can partially mitigate performance collapse at scale, though it does not solve underlying reasoning deficits.
- **Mechanism:** When accumulated interaction history approaches context limits, clear all prior turns except the system prompt. The agent then reconstructs necessary knowledge from self-maintained notes, functioning as an externalized memory. This prevents the confusion from excessively long contexts without requiring model retraining.
- **Core assumption:** Agents can reliably store and retrieve key information in notes.
- **Evidence anchors:** Section 4.4.2 and Figure 5 show CRNR consistently outperforming naive scaling across all three environments, though absolute scores remain low.

### Mechanism 2: In-Context Locking as Primary Failure Root
- **Claim:** Agent failures in long-horizon tasks are driven by a process-induced rigidity where initial patterns and assumptions persist despite contradicting evidence.
- **Mechanism:** Agents form early hypotheses or action patterns, then lack mechanisms for dynamic self-correction or strategic pivoting. Entropy analysis shows token diversity declining through trajectories, indicating narrowing exploration.
- **Core assumption:** Entropy decline correlates with behavioral rigidity.
- **Evidence anchors:** Section 5.2 identifies in-context locking as a primary failure origin, with Figure 6 showing consistent downward entropy trends across environments.

### Mechanism 3: Horizon Length as Dominant Bottleneck (Not Task Intrinsic Difficulty)
- **Claim:** Agent performance degrades primarily due to extended interaction horizons requiring sustained reasoning and memory, not the intrinsic reasoning complexity of individual rules.
- **Mechanism:** Ablation on Mystery Grid shows performance drops monotonically as hidden rule count increases from 1 to 5, even with unlimited exploration steps.
- **Core assumption:** Rule count proxies for horizon length.
- **Evidence anchors:** Section 4.3 and Table 2 demonstrate inverse relationship between horizon level and normalized score, alongside increasing tool calls.

## Foundational Learning

- **Concept:** Partially observable rule discovery
  - **Why needed here:** All three environments require agents to infer hidden, deterministic rules through controlled experimentation. Agents cannot succeed by pattern matching alone; they must form hypotheses, design experiments, observe outcomes, and iteratively refine models.
  - **Quick check question:** Given an environment where stepping on tile A sometimes gives +2 points and sometimes -1, what experiment would determine the trigger condition?

- **Concept:** Multi-tool orchestration under uncertainty
  - **Why needed here:** Agents must sequence tools (move, reset, query, conduct_cross, note_tool, Python interpreter) strategically. Misaligned tool usage is identified as a failure mode (11.0% of errors). Effective agents must select tools based on current hypotheses, not habits.
  - **Quick check question:** In Alien Genetics, after observing high lethality in one cross, what tool sequence would isolate which genetic factor causes lethality?

- **Concept:** Context and memory management at extreme scale
  - **Why needed here:** Trajectories reach 200k+ tokens and 400+ tool calls in heaviest settings. The paper shows naive scaling fails and identifies memory issues as 10% of errors. Agents must decide what to store, what to forget, and when to retrieve.
  - **Quick check question:** If you have 50 steps of exploration history and can only retain 10 key observations, what principles determine which to preserve?

## Architecture Onboarding

- **Component map:**
  UltraHorizon Benchmark -> Environments (Mystery Grid, Sequence Exploration, Alien Genetics Laboratory) -> Agent Interface (Tool layer, Commit mechanism) -> Evaluation (LLM-as-Judge, Score@k metric) -> Experimental Settings (Fixed steps, Free steps)

- **Critical path:**
  1. Select environment and step setting (fixed vs free)
  2. Initialize agent with system prompt and tool definitions
  3. Run agent-environment loop until step limit or commit
  4. Extract final answer from commit tool call
  5. Score via LLM-as-Judge against ground truth rules
  6. Optional: Apply CRNR at context-window thresholds

- **Design tradeoffs:**
  - Fixed vs free steps: Fixed enables fair comparison but may artificially constrain capable agents; free reveals exploration calibration but risks premature termination or resource waste
  - Context management: Naive scaling fails above optimal step counts; CRNR adds engineering complexity but improves performance
  - Evaluation method: LLM-as-Judge enables automated scoring but introduces scorer bias; Score@k mitigates stochasticity but requires multiple trials

- **Failure signatures:** From Figure 7 and Table 7
  - Premature Convergence (23.2%): Early hypothesis lock without sufficient testing
  - Repetitive Looping (15.6%): Same action sequence without progress
  - Error Propagation (13.4%): Mistakes compound without correction
  - Environment Mis-modeling (13.4%): Persistent prediction-observation gaps
  - Misaligned Tool Usage (11.0%): Wrong tool selection or overuse
  - Incoherent Planning (10.0%): Contradictory or missing prerequisites
  - Memory Issues (10.0%): Self-contradiction or forgotten constraints
  - Uncontrolled Experiments (3.6%): Multiple variables changed simultaneously

- **First 3 experiments:**
  1. **Baseline run:** Deploy target LLM on Mystery Grid with fixed 50 steps, no CRNR. Collect trajectory, compute score, and identify top 2 failure modes from error taxonomy.
  2. **Horizon ablation:** Run same model on Mystery Grid with hidden rule counts n=1, 3, 5 (unlimited steps). Plot normalized score vs n. Confirm monotonic decline pattern.
  3. **CRNR comparison:** Implement context refresh at 75% context-window capacity with notes recall. Run on Alien Genetics (heaviest setting). Compare against naive scaling on: (a) final score, (b) token efficiency, (c) error type distribution.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can more sophisticated memory architectures substantially outperform the lightweight CRNR strategy in long-horizon exploration tasks?
- **Basis in paper:** The paper introduces CRNR as "a simple yet effective scaling strategy" and states that progress "will require advances beyond simple scaling, pointing toward principled memory integration."
- **Why unresolved:** CRNR demonstrates improvement, but the paper only tests this lightweight approach; no comparison against structured memory systems is conducted.
- **What evidence would resolve it:** Systematic experiments comparing CRNR against more complex memory mechanisms on UltraHorizon's three environments, measuring both final scores and trajectory efficiency.

### Open Question 2
- **Question:** What mechanisms can effectively detect and correct "in-context locking" during agent execution?
- **Basis in paper:** The paper identifies "in-context locking" as a primary failure origin where "agents become locked on initial patterns, assumptions, or habits, and lack mechanisms for dynamic adjustment, exploration, or self-reflection," and entropy analysis confirms this effect.
- **Why unresolved:** The paper characterizes the problem and shows declining entropy as evidence, but proposes no intervention strategy to detect or break out of locked states mid-trajectory.
- **What evidence would resolve it:** Development of a self-monitoring component that tracks entropy/action diversity in real-time, with triggered interventions showing measurable score improvements compared to baseline agents.

### Open Question 3
- **Question:** How can agents learn to calibrate exploration depth autonomously without fixed step constraints?
- **Basis in paper:** Free-step experiments reveal that "current LLMs may lack intrinsic mechanisms to calibrate the depth of exploration for partially observable, rule-discovery tasks," with some models submitting prematurely and others over-exploring.
- **Why unresolved:** The paper demonstrates the calibration problem through divergent performance patterns but does not investigate whether meta-learning, uncertainty estimation, or self-termination heuristics can address it.
- **What evidence would resolve it:** Experiments where agents trained or prompted with explicit stopping criteria achieve consistent performance across free-step settings without human-specified budgets.

## Limitations
- Performance evaluation relies on LLM-as-Judge, introducing scorer-dependent variability not fully characterized
- Hidden rule pools are unspecified, preventing exact reproduction of environments
- CRNR's effectiveness is demonstrated but the mechanism by which note-recall avoids locking is not proven
- Results may not generalize beyond synthetic rule-discovery tasks to real-world long-horizon scenarios

## Confidence
- **High confidence:** Empirical observations of performance degradation with horizon length, failure mode frequencies, and CRNR's quantitative improvement over naive scaling
- **Medium confidence:** The claim that in-context locking is the primary failure mechanism, and that the benchmark reveals a fundamental capability gap rather than an evaluation artifact
- **Low confidence:** The assertion that these results generalize to all long-horizon tasks, given the synthetic and rule-discovery-specific nature of the environments

## Next Checks
1. **Cross-scorer validation:** Repeat scoring on 10% of submissions with a different LLM-as-judge (e.g., GPT-4o). Measure inter-scorer reliability and adjust performance rankings if discrepancies exceed 10%.

2. **Controlled horizon ablation:** In Mystery Grid, test fixed-rule-count conditions (n=3) with step budgets {25, 50, 75, 100}. Confirm that performance peaks then declines, isolating context overload from rule complexity.

3. **Mechanism stress test:** Implement a variant of CRNR that forces note review every 10 steps regardless of context usage. Compare error type distribution to baseline CRNR to determine whether periodic reflection or context management drives improvements.