---
ver: rpa2
title: 'TCM-3CEval: A Triaxial Benchmark for Assessing Responses from Large Language
  Models in Traditional Chinese Medicine'
arxiv_id: '2503.07041'
source_url: https://arxiv.org/abs/2503.07041
tags:
- medicine
- chinese
- clinical
- traditional
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study introduces TCM-3CEval, a comprehensive benchmark designed
  to evaluate large language models (LLMs) in Traditional Chinese Medicine (TCM).
  The benchmark addresses the gap in standardized evaluation tools for TCM by assessing
  models across three critical dimensions: Core Knowledge mastery, Classical Literacy,
  and Clinical Decision-Making.'
---

# TCM-3CEval: A Triaxial Benchmark for Assessing Responses from Large Language Models in Traditional Chinese Medicine

## Quick Facts
- arXiv ID: 2503.07041
- Source URL: https://arxiv.org/abs/2503.07041
- Reference count: 40
- Primary result: Introduces a comprehensive benchmark to evaluate LLMs in Traditional Chinese Medicine across Core Knowledge, Classical Literacy, and Clinical Decision-Making

## Executive Summary
TCM-3CEval addresses the critical need for standardized evaluation of large language models in Traditional Chinese Medicine by providing a triaxial benchmark that assesses models across foundational theory, classical text comprehension, and clinical reasoning. The benchmark reveals that models with specialized TCM training significantly outperform general-purpose models, particularly in classical text interpretation and clinical reasoning tasks. However, all models demonstrate limitations in specialized subdomains like Meridian & Acupoint theory, highlighting gaps between current AI capabilities and clinical needs. This framework sets a new standard for evaluating AI in culturally grounded medical domains and offers insights for optimizing LLMs in TCM.

## Method Summary
TCM-3CEval evaluates LLMs using 450 multiple-choice questions across three dimensions: TCM-Exam (150 questions on basic theory, diagnostics, meridians, materia medica, and formulas), TCM-LitQA (150 questions on classical texts including Huangdi Neijing and Shang Han Lun), and TCM-MRCD (150 clinical case questions across surgery, internal medicine, gynecology, and pediatrics). The benchmark employs a robustness check where options are shuffled multiple times and only consistent correct answers count. Models tested include DeepSeek, GPT-4o, o1-mini, Llama3, Claude, InternLM2.5, and PULSE, with inference-only evaluation using consistent prompt formatting.

## Key Results
- Specialized TCM-trained models like DeepSeek outperform general-purpose models by up to 50 percentage points in classical text interpretation
- All models show significant limitations in Meridian & Acupoint theory and Various TCM Schools subdomains
- Chinese linguistic and cultural priors enable better comprehension of polysemous TCM terminology and context-dependent meanings
- Hierarchical dependency exists where Core Knowledge mastery predicts Clinical Decision-Making performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Domain-specialized TCM training substantially outperforms general-purpose models
- Mechanism: Dual-phase training architecture with broad Chinese corpus pre-training followed by targeted domain-adaptive fine-tuning using classical TCM literature establishes linguistic and conceptual foundations
- Core assumption: TCM's unique epistemological framework requires explicit exposure during training
- Evidence anchors: DeepSeek's superior classical text interpretation performance (50 percentage points higher than general-purpose models)
- Break condition: If general-purpose models achieve comparable scores without TCM-specific training

### Mechanism 2
- Claim: Cultural-linguistic alignment enables comprehension of polysemous TCM terminology
- Mechanism: Chinese linguistic priors provide semantic grounding for interpreting metaphorical and context-dependent terminology
- Core assumption: TCM terminology cannot be adequately translated without cultural-linguistic embedding
- Evidence anchors: International general models exhibited critical errors in cultural context interpretation
- Break condition: If translation-based approaches perform equally well

### Mechanism 3
- Claim: Hierarchical competency assessment mirrors TCM physician training
- Mechanism: Progressive evaluation where foundational theory enables classical comprehension, which supports clinical reasoning
- Core assumption: TCM expertise follows sequential dependency similar to human training
- Evidence anchors: All models showed lower accuracy in Meridians & Acupoints compared to other aspects
- Break condition: If models excel at clinical decision-making without classical literacy

## Foundational Learning

- Concept: **Syndrome Differentiation (Bian Zheng)**
  - Why needed: TCM diagnosis relies on holistic pattern recognition rather than linear diagnostic reasoning
  - Quick check: Can you explain why a TCM practitioner might treat two patients with the same Western diagnosis differently?

- Concept: **TCM Classical Text Lineage**
  - Why needed: Benchmark assesses comprehension of canonical texts that remain clinically authoritative
  - Quick check: Why would a model need to understand historical medical schools to answer clinical questions?

- Concept: **Meridian & Acupoint Theory**
  - Why needed: This subdomain showed weakest performance across all models
  - Quick check: What makes meridian theory harder to encode than herb property memorization?

## Architecture Onboarding

- Component map: TCM-Exam (Basic Theory, Diagnostics, Meridians & Acupoints, Materia Medica, Formula Science) -> TCM-LitQA (Classical Texts) -> TCM-MRCD (Clinical Cases)

- Critical path:
  1. Establish baseline on TCM-Exam (foundational knowledge)
  2. Evaluate TCM-LitQA to identify classical comprehension gaps
  3. Test TCM-MRCD for clinical reasoning capability
  4. Analyze per-subdomain performance to pinpoint training deficits

- Design tradeoffs:
  - Breadth vs. depth: 450 questions across 15 subdomains provides coverage but limited granularity
  - Literature vs. real-world cases: Test sets derived from textbooks/cases, not electronic medical records
  - Accuracy vs. robustness: Shuffled-option evaluation reduces gaming but increases computational cost

- Failure signatures:
  - Meridian & Acupoint underperformance: Indicates lack of spatial-relational knowledge representation
  - Various TCM Schools weakness: Suggests insufficient historical-contextual training data
  - TCM Surgery low accuracy: May reflect less training data for procedural vs. medicinal knowledge
  - Polysemous terminology errors: Flags inadequate context-sensitive disambiguation

- First 3 experiments:
  1. Baseline comparison: Evaluate your model against DeepSeek, PULSE, and GPT-4o across all three datasets
  2. Ablation by dimension: Test whether Core Knowledge scores predict Clinical Decision-Making performance
  3. Cultural-linguistic probe: Compare performance on Chinese-script vs. translated classical text questions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would LLM performance on TCM-3CEval correlate with performance on real-world clinical electronic medical records?
- Basis: Study limitations include test sets primarily derived from literature cases
- Why unresolved: Textbook cases may not reflect complexity and ambiguity in actual clinical practice
- What evidence would resolve it: Comparative study evaluating same LLMs on TCM-3CEval and real clinical cases

### Open Question 2
- Question: Can integrating structured TCM knowledge graphs with LLMs improve performance in specialized subdomains?
- Basis: Current models lack integration of structured knowledge graphs capturing complex interrelationships
- Why unresolved: Corpus-based training may create factual knowledge without relational reasoning
- What evidence would resolve it: Ablation study comparing LLMs with and without knowledge graph integration

### Open Question 3
- Question: How can concept disambiguation mechanisms be designed for polysemous TCM terminology?
- Basis: Polysemous TCM terminology remains challenging for model comprehension
- Why unresolved: TCM terms carry multiple context-dependent meanings rooted in different classical sources
- What evidence would resolve it: Development of disambiguation module using contextual embeddings and expert-annotated sense inventories

### Open Question 4
- Question: What multi-modal evaluation methods could assess LLM capabilities in TCM diagnostic techniques?
- Basis: Current TCM-3CEval is text-only but clinical TCM relies on physical examination findings
- Why unresolved: Cannot fully capture tongue and pulse diagnosis in text descriptions
- What evidence would resolve it: Multi-modal benchmark including standardized tongue images and pulse waveform data

## Limitations
- Dataset accessibility: The 450-question benchmark requires access through Medbench's TCM track, not publicly available
- Implementation details: Lacks exact prompt templates, generation configurations, and shuffle iteration counts
- Ecological validity: Relies on textbook-derived test sets rather than real-world clinical data

## Confidence

- **High confidence**: Hierarchical competency framework is theoretically sound and aligns with TCM training pathways; specialized TCM-trained models outperform general-purpose models
- **Medium confidence**: Mechanism linking Chinese linguistic priors to improved classical text interpretation requires further validation; robustness check methodology is sound but implementation details unclear
- **Low confidence**: Exact magnitude of performance gaps cannot be independently verified without dataset and evaluation code access

## Next Checks
1. Obtain and independently validate the TCM-3CEval dataset to confirm question quality, answer accuracy, and distribution across subdomains
2. Replicate the option-shuffling robustness check with multiple shuffle iterations to verify methodology meaningfully reduces gaming
3. Test multilingual models on both Chinese-script and translated versions of classical text questions to quantify contribution of linguistic alignment versus conceptual understanding