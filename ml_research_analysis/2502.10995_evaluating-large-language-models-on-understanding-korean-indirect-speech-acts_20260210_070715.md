---
ver: rpa2
title: Evaluating Large language models on Understanding Korean indirect Speech acts
arxiv_id: '2502.10995'
source_url: https://arxiv.org/abs/2502.10995
tags:
- speech
- llms
- acts
- indirect
- utterance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluates the pragmatic competence of large language\
  \ models (LLMs) in understanding Korean indirect speech acts, focusing on three\
  \ types of indirect speech acts: directive, commissive, and expressive. A dataset\
  \ of 240 items was constructed, each containing a context and utterance, with two\
  \ scenarios per utterance\u2014one performing a direct speech act and the other\
  \ an indirect speech act."
---

# Evaluating Large language models on Understanding Korean indirect Speech acts

## Quick Facts
- arXiv ID: 2502.10995
- Source URL: https://arxiv.org/abs/2502.10995
- Authors: Youngeun Koo; Jiwoo Lee; Dojun Park; Seohyun Park; Sungeun Lee
- Reference count: 7
- Primary result: No LLM reached human-level performance in understanding Korean indirect speech acts; Claude3-Opus achieved highest scores (71.94% MCQ, 65% OEQ)

## Executive Summary
This study evaluates large language models' (LLMs) pragmatic competence in understanding Korean indirect speech acts across three categories: directive, commissive, and expressive. The research constructs a dataset of 240 items with context-utterance pairs, each appearing in both direct and indirect scenarios. Through multiple-choice and open-ended evaluation formats, the study reveals that while proprietary models like Claude3-Opus outperform others, no model reaches human-level performance, particularly struggling with indirect speech acts compared to direct ones.

## Method Summary
The study employs two experimental setups: Multiple-Choice Questions (MCQ) for automatic evaluation and Open-Ended Questions (OEQ) for human expert assessment. A 240-item Korean dataset was constructed, with each item containing context and utterance, appearing in two scenarios - one performing a direct speech act and the other an indirect speech act. Twelve models were evaluated (8 proprietary APIs, 4 open-source weights) using zero-shot inference. Human baseline was established with 52 native Korean speakers. Evaluation metrics included MCQ accuracy (averaged over 3 trials) and OEQ human evaluation on a 1-5 scale.

## Key Results
- Claude3-Opus achieved highest performance at 71.94% MCQ accuracy and 65% OEQ score
- All models performed significantly worse on indirect speech acts compared to direct ones
- Proprietary models consistently outperformed open-source models across both evaluation formats
- Most models showed literal-meaning anchoring, defaulting to direct interpretations when literal meaning was plausible

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Context integration enables indirect speech act comprehension when surface meaning diverges from speaker intent
- Mechanism: Models must bind utterance to conversational context, then infer illocutionary force rather than locutionary content, suppressing literal interpretation when context signals indirectness
- Core assumption: Context representations are sufficiently rich to override default semantic parsing
- Evidence anchors: Table 1 demonstrates how "The elevator is out of order" shifts from suggestion to complaint based on speaker-listener relationship; limited direct evidence in neighbor papers
- Break condition: Ambiguous context cues or independently plausible literal meanings cause models to default to literal interpretation

### Mechanism 2
- Claim: Larger parameter counts better capture subtle contextual nuances required for pragmatic inference
- Mechanism: Scale increases capacity to encode context-dependent patterns and idiomatic expressions signaling indirect intent
- Core assumption: Training data contains sufficient examples of indirect speech acts for scale to matter
- Evidence anchors: Table 6 shows Claude3-Opus (71.94), GPT-4 (71.39), Mistral-large (69.58) outperforming Llama3-8B (49.44), Qwen-14B (40.14); proprietary models exhibited relatively higher performance
- Break condition: Scale alone fails when fine-tuning optimizes for benchmark formats over genuine comprehension

### Mechanism 3
- Claim: Indirect speech act comprehension correlates with but differs from conversational implicature understanding
- Mechanism: Both require context-driven inference beyond literal meaning, but speech acts have type-specific felicity conditions whereas implicatures share Grice's cooperative principle
- Core assumption: Correlation reflects shared inference processes, not identical mechanisms
- Evidence anchors: Table 10 shows 0.67 correlation between speech acts and implicature comprehension; 0.73 correlation specifically between indirect speech acts and implicatures
- Break condition: Assumption that correlation implies shared substrate is unproven; could reflect common training data artifacts

## Foundational Learning

- Concept: **Speech Act Theory (Austin/Searle)**
  - Why needed here: The evaluation framework categorizes utterances by illocutionary force (representative, directive, commissive, expressive, declarative). Understanding this taxonomy is prerequisite to interpreting the paper's three test types (ReDi, ReCo, ReEx).
  - Quick check question: Given "The ice is thin" spoken to children approaching a frozen pond, is this a representative or directive speech act?

- Concept: **High-context vs low-context communication**
  - Why needed here: Korean is characterized as a high-context language where shared background reduces explicitness. This amplifies indirect speech act frequency and evaluation difficulty.
  - Quick check question: In a high-context culture, would "It's cold in here" more likely be interpreted as information or a request?

- Concept: **MCQ vs OEQ evaluation paradigms**
  - Why needed here: The paper uses both formats to distinguish recognition (selecting correct option) from generation (producing explanation). OEQ reveals overfitting patterns (e.g., Solar generating MCQ-style options for open-ended prompts).
  - Quick check question: If a model achieves 90% MCQ accuracy but generates incoherent OEQ explanations, what does this suggest about its pragmatic competence?

## Architecture Onboarding

- Component map: Context encoder -> Utterance parser -> Intention classifier -> Conflict resolver
- Critical path: 1. Encode context → 2. Parse utterance literal meaning → 3. Detect context-meaning mismatch → 4. Infer illocutionary force → 5. Classify speech act type
- Design tradeoffs:
  - MCQ evaluation: Faster, automated, but masks explanation quality; models may select correctly without true comprehension
  - OEQ evaluation: Reveals reasoning, but requires human scoring (ICC=0.715 inter-rater reliability noted)
  - Direct vs indirect pair design: Controls for utterance confounds, but doubles annotation cost
- Failure signatures:
  - Literal-meaning anchoring: Model selects direct speech act when indirect is correct (dominant error type per Figure 2)
  - Benchmark overfitting: Solar generates "(A), (B), (C)" options even in OEQ format
  - Language misalignment: Llama3 responded in English to Korean prompts (77% of responses)
  - Random option selection: Indicates fundamental comprehension failure vs indirect-direct confusion
- First 3 experiments:
  1. Replicate MCQ evaluation on your target model with the 240-item dataset (40 pairs × 3 types × 2 scenarios); measure direct vs indirect accuracy gap
  2. Run OEQ task on failure cases from experiment 1; manually score 1-5 scale for intent vs meaning comprehension
  3. Ablate context length (full context vs minimal context) to measure context dependency; expect larger degradation for indirect speech acts

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the observed performance patterns in Korean speech act comprehension generalize to languages with different levels of context-dependency, such as English or German?
- Basis in paper: The conclusion states, "Our future research aims to develop data sets in other languages, such as English, Chinese or German, to validate the robustness of the data sets and to compare the speech act comprehension capabilities of LLMs across different languages."
- Why unresolved: The current study is restricted to Korean (a high-context language); it is unknown if the specific difficulties with indirect speech acts (particularly directives) persist in lower-context linguistic environments.
- What evidence would resolve it: A replication of the evaluation framework using translated or culturally adapted datasets in English and German, followed by a cross-lingual comparison of model errors.

### Open Question 2
- Question: How does the understanding of indirect speech acts differ when LLMs are engaged in multi-turn conversational prompting compared to the single-turn question-answering formats used in this study?
- Basis in paper: The authors propose to "explore alternative experimental settings... having them to generate next utterance considering the given multi-turn conversation as input prompt ('Multi-turn Prompting')."
- Why unresolved: The current methodology relies on isolated context-utterance pairs (MCQ and OEQ), which may not fully reflect the dynamic nature of intent inference in ongoing dialogue.
- What evidence would resolve it: Performance metrics derived from a new experimental setup where LLMs must generate appropriate subsequent utterances in a multi-turn dialogue rather than selecting or explaining an answer option.

### Open Question 3
- Question: Can the specific deficiency in understanding Directive indirect speech acts (ReDi) be mitigated by reducing LLMs' reliance on literal meaning through targeted fine-tuning?
- Basis in paper: The authors note that LLMs struggle most with ReDi because they rely on literal meaning when it is "sufficiently interpretable," ignoring the indirect request implied by the context.
- Why unresolved: While the paper identifies the "literal bias" as the cause for failure in directive contexts, it does not test training interventions designed to force models to weigh contextual evidence over semantic surface forms.
- What evidence would resolve it: A comparative study showing that models fine-tuned on pragmatic inference tasks (specifically involving factual statements used as requests) outperform standard models on the ReDi subset.

## Limitations
- Dataset construction relies on manually crafted scenarios, introducing potential bias in context-utterance pairing
- MCQ format may artificially inflate performance by allowing pattern recognition rather than genuine pragmatic understanding
- Study focuses exclusively on Korean language, limiting generalizability to other high-context languages

## Confidence
**High Confidence**: Proprietary models (Claude3-Opus, GPT-4) outperforming open-source models on indirect speech act comprehension is well-supported by empirical results across both MCQ and OEQ formats.
**Medium Confidence**: Correlation between speech act comprehension and conversational implicature understanding (r=0.67 overall, r=0.73 for indirect acts) suggests related mechanisms, but causation remains unproven.
**Low Confidence**: Claim that "context integration enables indirect speech act comprehension when surface meaning diverges" lacks direct mechanistic evidence; observation of necessity doesn't demonstrate the binding mechanism.

## Next Checks
1. **Ablation study on context dependency**: Systematically reduce context information (full context → minimal context) for indirect speech act items and measure performance degradation. Expect significantly larger drops for indirect vs direct acts, confirming context's role in pragmatic inference.
2. **Cross-linguistic replication**: Test the same evaluation framework on another high-context language (e.g., Japanese or Mandarin) to determine if observed patterns generalize beyond Korean. Compare performance consistency across languages with different indirect speech act conventions.
3. **Fine-grained error analysis**: For models that fail indirect speech act comprehension, categorize errors into: (a) literal interpretation bias, (b) random selection, (c) benchmark overfitting, and (d) cultural/contextual misalignment. This will reveal whether failures stem from fundamental comprehension deficits or evaluation artifacts.