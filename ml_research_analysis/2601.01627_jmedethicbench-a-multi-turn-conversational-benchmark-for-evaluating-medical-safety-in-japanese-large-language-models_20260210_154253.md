---
ver: rpa2
title: 'JMedEthicBench: A Multi-Turn Conversational Benchmark for Evaluating Medical
  Safety in Japanese Large Language Models'
arxiv_id: '2601.01627'
source_url: https://arxiv.org/abs/2601.01627
tags:
- safety
- medical
- multi-turn
- across
- japanese
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces JMedEthicBench, the first multi-turn conversational
  benchmark for evaluating medical safety of LLMs in Japanese healthcare. The benchmark
  is grounded in 67 guidelines from the Japan Medical Association and contains over
  50,000 adversarial conversations generated using seven automatically discovered
  jailbreak strategies.
---

# JMedEthicBench: A Multi-Turn Conversational Benchmark for Evaluating Medical Safety in Japanese Large Language Models

## Quick Facts
- arXiv ID: 2601.01627
- Source URL: https://arxiv.org/abs/2601.01627
- Reference count: 19
- Multi-turn adversarial benchmark reveals medical models are more vulnerable than commercial models, with safety degrading across conversation turns

## Executive Summary
JMedEthicBench introduces the first multi-turn conversational benchmark for evaluating medical safety of large language models in Japanese healthcare contexts. Grounded in 67 Japan Medical Association guidelines, the benchmark generates over 50,000 adversarial conversations using seven automatically discovered jailbreak strategies. Evaluation of 27 models reveals that commercial models maintain robust safety while medical-specialized models exhibit increased vulnerability, with safety scores declining significantly across conversation turns (median: 9.5 to 5.0, p < 0.001). Cross-lingual evaluation demonstrates that medical model vulnerabilities persist across languages, suggesting inherent alignment limitations rather than language-specific factors.

## Method Summary
The benchmark constructs harmful questions from 67 Japan Medical Association medical ethics guidelines, validates them through multi-model refusal testing, and generates multi-turn adversarial conversations using an adapted AutoDAN-Turbo framework. The evaluation protocol employs dual-LLM scoring (DeepSeek-R1 + GPT-4o-mini) on a 1-10 scale across three conversation turns. The methodology includes automated jailbreak strategy discovery through adversarial multi-agent optimization, producing seven distinct attack patterns that progressively erode model safety through conversational context accumulation.

## Key Results
- Commercial models (e.g., GPT-4o-mini, Claude-3.5-Sonnet) maintain robust safety with median scores of 9.5-10.0
- Medical-specialized models show significantly lower safety (median scores 5.0-7.5) and greater vulnerability to multi-turn attacks
- Safety scores decline significantly across conversation turns (median: 9.5 to 5.0, p < 0.001)
- Cross-lingual jailbreak strategy effectiveness indicates alignment vulnerabilities persist across languages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-turn conversations progressively erode safety boundaries, creating a distinct threat surface from single-turn attacks.
- Mechanism: Adversarial prompts distributed across multiple turns exploit conversational context accumulation—the model's hidden state incorporates prior benign-seeming exchanges, lowering refusal thresholds for harmful requests that would trigger rejection in isolation.
- Core assumption: Safety mechanisms trained primarily on single-turn refusal patterns do not generalize to sequential pressure.
- Evidence anchors:
  - [abstract] "safety scores decline significantly across conversation turns (median: 9.5 to 5.0, p < 0.001)"
  - [Section 5.2] "The median safety score decreases from approximately 9.5 at Turn 0 to 6.0 at Turn 1 and 5.0 at Turn 2"
  - [corpus] MedKGEval paper similarly addresses "complexity of multi-turn doctor-patient interactions" as an open challenge, corroborating that multi-turn evaluation is non-trivial.
- Break condition: If models were explicitly trained with multi-turn adversarial examples during alignment, the turn-by-turn degradation would likely flatten.

### Mechanism 2
- Claim: Domain-specific medical fine-tuning can inadvertently weaken general safety alignment, creating a safety-capability inversion.
- Mechanism: Medical LLMs undergo continued pre-training or fine-tuning on specialized corpora that emphasize helpfulness and clinical detail, potentially causing catastrophic forgetting of refusal behaviors learned during base-model alignment.
- Core assumption: Safety and domain knowledge are not inherently at odds—degradation stems from training procedure, not domain.
- Evidence anchors:
  - [abstract] "domain-specific fine-tuning may accidentally weaken safety mechanisms"
  - [Section 5.1] "medical-specialized models demonstrate lower safety scores than general-purpose models of comparable size"
  - [corpus] "Balancing Safety and Helpfulness in Healthcare AI Assistants" explicitly addresses this trade-off as a deployment barrier.
- Break condition: If safety constraints are explicitly preserved or re-aligned during domain adaptation (e.g., via constrained fine-tuning or safety-aware training), this inversion should attenuate.

### Mechanism 3
- Claim: Jailbreak strategy effectiveness transfers across models and languages, indicating vulnerabilities in shared alignment methodology rather than model-specific or language-specific flaws.
- Mechanism: Automatically discovered strategies (e.g., fictional framing, authority simulation) exploit common patterns in how aligned models handle context—models conditioned to be helpful in academic or creative contexts can be manipulated to extend that helpfulness to harmful requests.
- Core assumption: Alignment training across the field shares structural similarities (RLHF patterns, refusal trigger phrases) that create common attack surfaces.
- Evidence anchors:
  - [abstract] "medical model vulnerabilities persist across languages, suggesting inherent alignment limitations rather than language-specific factors"
  - [Section 3.2] "seven distinct jailbreak strategies characterized by unique framing mechanisms, narrative structures, or authority appeals"
  - [corpus] Weak direct evidence—corpus papers do not systematically test cross-lingual jailbreak transfer; this remains underexplored in the literature.
- Break condition: If alignment training incorporated explicit counter-examples for each discovered strategy pattern, transfer effectiveness would diminish.

## Foundational Learning

- **Multi-turn adversarial dynamics**: Understanding how conversation history conditions model behavior differently than isolated prompts.
  - Why needed here: The benchmark's core finding depends on recognizing that multi-turn erosion is mechanistically distinct from single-turn attack success.
  - Quick check question: Can you explain why a model might refuse a harmful request in turn 1 but comply in turn 3?

- **Safety-helpfulness trade-offs in domain adaptation**: Recognizing that specialized training can overwrite prior alignment.
  - Why needed here: Medical models' unexpected vulnerability is interpretable only if you understand how fine-tuning can degrade safety.
  - Quick check question: Why might a medical-specialized model be *less safe* than its general-purpose base model?

- **Automated red-teaming and strategy discovery**: Grasping how adversarial patterns can be systematically generated and generalized.
  - Why needed here: The benchmark's 7 jailbreak strategies were not hand-crafted but discovered via multi-agent optimization.
  - Quick check question: What advantage does automated strategy discovery have over manually designed attack prompts?

## Architecture Onboarding

- **Component map**: 67 JMA guidelines → 3,350 candidate harmful questions → 1,935 validated questions → 7 jailbreak strategies → ~54,180 multi-turn conversations → dual-LLM scoring
- **Critical path**: The validation filter (Section 3.1) is the quality gate—without ensuring baseline single-turn harmfulness, multi-turn results confound attack sophistication with weak seed prompts.
- **Design tradeoffs**:
  - Dual-scorer protocol increases reliability but adds cost and potential scorer disagreement.
  - 3-turn limit covers 74.31% of natural conclusions but may miss longer-horizon degradation patterns.
  - LLM-based scoring scales well but may miss subtle harms human evaluators would catch (acknowledged in Limitations).
- **Failure signatures**:
  - Medical models clustering in lower-left quadrant (low safety, low helpfulness) suggests catastrophic forgetting rather than a deliberate trade-off.
  - Safety declining from Turn 0→Turn 2 with large effect sizes indicates multi-turn evaluation is not redundant with single-turn.
- **First 3 experiments**:
  1. **Baseline validation**: Run single-turn versions of the 2,345 test prompts against your model to establish whether safety exists to be eroded.
  2. **Turn-by-turn analysis**: Score each turn independently to identify where in the conversation your model's refusal collapses—early (turn 1) vs. late (turn 3) degradation suggests different intervention points.
  3. **Strategy ablation**: Test each of the 7 strategies in isolation to determine which framing patterns your model is most vulnerable to, then prioritize counter-training data accordingly.

## Open Questions the Paper Calls Out

1. **Question**: Does the observed safety score degradation persist or plateau in conversations extending beyond the current 3-turn limit?
   - Basis in paper: [explicit] Page 5 states, "We acknowledge that longer conversations (5–7 turns) were not evaluated, and future work should investigate whether degradation trends differ for extended multi-turn interactions..."
   - Why unresolved: The current experimental setup restricted evaluation to 3 turns to manage costs and ensure comparability, leaving the long-term trajectory of safety erosion unexplored.
   - What evidence would resolve it: Evaluation results from the same benchmark extended to 5–7 turns, showing the statistical trend of safety scores over longer sequences.

2. **Question**: How does the dual-LLM automated scoring protocol compare to human medical expert evaluations in detecting subtle safety failures?
   - Basis in paper: [explicit] Page 8 notes, "LLM-based scoring, while scalable, may introduce biases or miss subtle safety failures that human evaluators would detect."
   - Why unresolved: The study relied entirely on LLM-based judges (DeepSeek-R1 and GPT-4o-mini) for scoring, prioritizing scale over human validation.
   - What evidence would resolve it: A correlation analysis between the automated safety scores and blinded assessments by medical professionals on a shared subset of responses.

3. **Question**: Do the cross-lingual safety vulnerabilities identified in medical-specialized models persist across the full set of 27 evaluated models?
   - Basis in paper: [explicit] Page 8 states, "...comprehensive evaluation across all 27 models in both languages remains for future work," as the cross-lingual analysis was limited to six medical models.
   - Why unresolved: While medical models showed language-independent vulnerabilities, it remains unconfirmed whether general open-source and commercial models exhibit similar cross-lingual consistency or if they possess language-specific defense mechanisms.
   - What evidence would resolve it: Safety performance data for the remaining 21 general and commercial models evaluated on an English-translated version of the benchmark.

## Limitations
- LLM-based scoring introduces potential scorer bias and hallucination risks that may not align with human judgment
- 3-turn conversation limit may miss longer-horizon degradation patterns beyond the majority of natural conclusions
- Benchmark focuses specifically on Japanese healthcare guidelines, limiting cross-cultural generalizability

## Confidence
- **High Confidence**: Safety scores decline significantly across conversation turns (median: 9.5 to 5.0, p < 0.001)
- **Medium Confidence**: Domain-specific fine-tuning weakens safety mechanisms, though confounding factors may contribute
- **Low Confidence**: Cross-lingual transferability conclusion requires more extensive multilingual validation

## Next Checks
1. **Human validation study**: Recruit medical professionals to independently score a stratified random sample of 100 conversations from the test set to validate LLM-based scoring reliability and identify potential hallucination or bias in automated evaluation.

2. **Longer conversation analysis**: Extend the evaluation protocol to 5-7 turns for a subset of conversations to determine whether safety degradation plateaus, continues, or exhibits non-linear patterns beyond the current 3-turn limit.

3. **Cross-cultural generalization test**: Adapt the benchmark framework to evaluate models on medical safety guidelines from at least two other healthcare systems (e.g., US, EU) to test whether the observed commercial-vs-medical model safety gap and jailbreak transferability hold across different regulatory and cultural contexts.