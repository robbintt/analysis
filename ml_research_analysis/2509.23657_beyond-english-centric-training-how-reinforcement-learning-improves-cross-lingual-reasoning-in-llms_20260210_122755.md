---
ver: rpa2
title: 'Beyond English-Centric Training: How Reinforcement Learning Improves Cross-Lingual
  Reasoning in LLMs'
arxiv_id: '2509.23657'
source_url: https://arxiv.org/abs/2509.23657
tags:
- reasoning
- language
- arxiv
- performance
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically compares reinforcement learning (RL)
  and supervised fine-tuning (SFT) for improving cross-lingual reasoning in large
  language models. Using Qwen2.5-3B-Base and translated reasoning datasets across
  10 languages, experiments reveal that RL achieves significantly higher accuracy
  and stronger cross-lingual generalization than SFT on diverse multilingual reasoning
  benchmarks.
---

# Beyond English-Centric Training: How Reinforcement Learning Improves Cross-Lingual Reasoning in LLMs

## Quick Facts
- arXiv ID: 2509.23657
- Source URL: https://arxiv.org/abs/2509.23657
- Authors: Shulin Huang; Yiran Ding; Junshu Pan; Yue Zhang
- Reference count: 35
- Primary result: RL-trained models on non-English data outperform RL on English data for cross-lingual reasoning, challenging English-centric training assumptions.

## Executive Summary
This study systematically compares reinforcement learning (RL) and supervised fine-tuning (SFT) for improving cross-lingual reasoning in large language models. Using Qwen2.5-3B-Base and translated reasoning datasets across 10 languages, experiments reveal that RL achieves significantly higher accuracy and stronger cross-lingual generalization than SFT on diverse multilingual reasoning benchmarks. Notably, RL trained on non-English data outperforms RL trained on English data, a phenomenon not observed with SFT. Mechanistic analyses indicate that RL's linguistic flexibility during reasoning, sampling-driven exploration, and minimal semantic shift contribute to its superior generalization. These findings demonstrate that RL enables more robust and equitable multilingual reasoning, challenging the assumption that English-centric training is optimal.

## Method Summary
The paper translates English reasoning datasets (GSM8K, LUFFY) into 10 languages using Qwen3-30B-A3B, then trains Qwen2.5-3B-Base models using both SFT (LlamaFactory, lr=2e-5) and RL (verl platform with GRPO, lr=1e-6). Models are trained on each language individually and evaluated on all languages across five multilingual reasoning benchmarks. The key innovation is comparing RL vs. SFT for cross-lingual transfer, with particular attention to whether training language affects generalization.

## Key Results
- RL-trained models achieve significantly higher cross-lingual accuracy than SFT-trained models across all evaluation benchmarks
- RL trained on non-English data (e.g., German) generalizes better to other languages than RL trained on English data
- This "non-English advantage" is unique to RL and not observed with SFT training
- Language consistency interventions (prompting or rewards) reduce RL performance, suggesting flexibility aids generalization

## Why This Works (Mechanism)

### Mechanism 1: Linguistic Flexibility Enables Reasoning Transfer
RL optimizes for correctness rather than linguistic imitation, allowing models to recruit reasoning circuits from any language—typically English from pre-training—regardless of prompt language. Evidence shows RL achieves high accuracy with 0% language consistency, and enforcing consistency drops accuracy from 61.4% to 52.0%. This suggests flexibility, not consistency, drives generalization.

### Mechanism 2: Sampling-Driven Exploration Over Imitation
RL's on-policy sampling explores diverse reasoning trajectories, expanding the solution space beyond memorized demonstrations. Accuracy progression (Base→SFT→RFT→RL) shows sampling alone provides ~20pt gain; online RL adds ~5pt more. This exploration discovers robust strategies that pure imitation cannot access.

### Mechanism 3: Minimal Semantic Shift Preserves Pre-trained Reasoning
RL induces smaller shifts in hidden-state representations compared to SFT, preserving cross-lingual structures learned during pre-training. PCA analysis shows RL has the most concentrated distribution around origin, correlating with highest cross-lingual accuracy. This preservation maintains pre-training's multilingual alignment.

## Foundational Learning

- **Concept: Group Relative Policy Optimization (GRPO)**
  - Why needed: The paper uses GRPO as its RL algorithm. Understanding how it estimates advantages from group samples (not a separate value model) is essential for interpreting the sampling mechanism.
  - Quick check: How does GRPO compute the advantage for a given rollout within a group of samples from the same prompt?

- **Concept: Cross-lingual Transfer vs. Multilingual Training**
  - Why needed: The paper's core claim is about transfer (training on language A, generalizing to B), not joint multilingual training. Distinguishing these clarifies why non-English training data can outperform English data.
  - Quick check: If a model is trained on Chinese math problems and evaluated on Bengali, what must be true about its internal representations for transfer to occur?

- **Concept: Language Consistency Reward**
  - Why needed: Section 4.1 uses this as an intervention to test the linguistic flexibility hypothesis. Understanding its formulation (`r_overall = 0.5 * r_acc + 0.5 * r_consistency`) is critical for reproducing ablations.
  - Quick check: What trade-off does weighting accuracy and consistency equally introduce, and how might a different weight change the observed effect?

## Architecture Onboarding

- **Component map**: Qwen2.5-3B-Base -> SFT (LlamaFactory) or RL (GRPO) -> Translated GSM8K+LUFFY -> MGSM/MMath500/MAIME2024/MMLU-ProX-Lite/MGPQA-D benchmarks

- **Critical path**: 1) Translate training data using Qwen3-30B-A3B with model-based verification; 2) Run SFT baseline for each training language; 3) Run RL training for each training language using same data; 4) Evaluate all models on all test languages; 5) Compute Avg and Gen scores; 6) Ablate language flexibility with prompts and consistency rewards

- **Design tradeoffs**: Full-parameter tuning vs. PEFT (paper uses full for thoroughness); Temperature=1.0 for maximal exploration (higher variance); KL penalty=0.001 prevents drift but may constrain learning

- **Failure signatures**: RL language collapse (check reward design for biases); SFT outperforms RL on low-resource languages (base model lacks pre-training support); Consistency reward improves accuracy (flexibility hypothesis may not hold)

- **First 3 experiments**: 1) Reproduce SFT vs. RL gap on German training, English/Bengali test; 2) Ablate RL sampling temperature (0.5, 1.0, 1.5) to test exploration; 3) Add language-consistency reward and measure performance drop across training languages

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several implications arise from the findings. The non-English advantage in RL suggests fundamental differences in how language-specific training data interacts with RL's optimization dynamics, which warrants further investigation into the relationship between training language, pre-training language dominance, and transfer mechanisms.

## Limitations
- Study is limited to small-scale models (3B parameters), leaving scalability to larger models unknown
- Translation process is model-verified but not human-verified, potentially introducing artifacts
- Direct comparison of RL(En) vs. RL(De) is not performed, making it unclear whether the advantage stems from English vs. non-English training data or RL method itself
- Language-consistency intervention may introduce confounds beyond testing flexibility hypothesis

## Confidence
- **High confidence**: RL outperforms SFT on cross-lingual reasoning tasks when trained on non-English data; RL shows less semantic shift in hidden representations than SFT
- **Medium confidence**: RL's linguistic flexibility directly contributes to generalization gains; sampling-driven exploration explains performance gap between RFT and RL
- **Low confidence**: RL trained on non-English data outperforms RL trained on English data due to linguistic flexibility; observed representational compactness directly causes generalization advantage

## Next Checks
1. **Ablation of training language vs. method**: Train RL models on both English and non-English data, then compare cross-lingual generalization within the RL framework to isolate the effect of training language from the RL algorithm itself.

2. **Statistical testing of semantic shift**: Apply multivariate hypothesis tests (e.g., MANOVA) to the PCA-reduced hidden-state distributions to determine whether RL's minimal shift is statistically significant and correlated with generalization.

3. **Human validation of translations**: Sample and manually verify a subset of translated training examples to quantify translation noise and assess whether it could explain differential SFT vs. RL performance.