---
ver: rpa2
title: 'Beyond Sliding Windows: Learning to Manage Memory in Non-Markovian Environments'
arxiv_id: '2512.19154'
source_url: https://arxiv.org/abs/2512.19154
tags:
- memory
- learning
- stacking
- length
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Adaptive Stacking, a memory management technique
  for reinforcement learning agents in partially observable environments. The method
  allows agents to learn which observations to retain in a bounded memory stack, rather
  than passively keeping the most recent observations.
---

# Beyond Sliding Windows: Learning to Manage Memory in Non-Markovian Environments

## Quick Facts
- **arXiv ID:** 2512.19154
- **Source URL:** https://arxiv.org/abs/2512.19154
- **Reference count:** 40
- **Primary result:** Adaptive Stacking allows agents to learn which observations to retain in a bounded memory stack, achieving performance matching oracle memory agents while using far less memory.

## Executive Summary
This paper introduces Adaptive Stacking, a memory management technique for reinforcement learning agents in partially observable environments. The method allows agents to learn which observations to retain in a bounded memory stack, rather than passively keeping the most recent observations. The authors prove convergence guarantees for agents using Adaptive Stacking under both unbiased optimization and TD-based learning, even with significantly smaller memory than required for full observability. Experiments on memory-intensive tasks demonstrate that Adaptive Stacking matches the performance of oracle memory agents while using far less memory, and substantially outperforms naive baselines under tight memory budgets. The approach offers a promising path toward scalable, memory-efficient RL in large, partially observable environments.

## Method Summary
Adaptive Stacking is a memory management technique where an agent maintains a fixed-size stack of observations and learns to actively select which observation to remove before pushing a new one. At each timestep, the agent outputs two actions: an environment action and a memory action (an index indicating which observation to pop from the stack). The stack update rule is: pop the selected observation, then push the new observation. This transforms memory management from a passive FIFO process into an active decision problem aligned with the reward signal. The approach is theoretically grounded with convergence guarantees for TD learning under value-consistency assumptions, and empirically validated on tasks like T-Maze and XorMaze where traditional methods require much larger memory buffers.

## Key Results
- Adaptive Stacking achieves 100% success on long-horizon mazes (length 10^6) trained only on length 16, while oracle Frame Stacking fails due to overfitting
- The method matches oracle memory agent performance while using significantly less memory (k=2 vs k*=16 in T-Maze)
- Memory Regret metric shows the agent effectively learns to retain reward-predictive observations and discard irrelevant ones
- Theoretical convergence guarantees proven for both unbiased optimization and TD-based learning under value-consistency assumptions

## Why This Works (Mechanism)

### Mechanism 1: Selective Retention via Learned Gating
The agent learns to maintain optimal performance with a memory stack significantly smaller than the environment's Markov order by actively selecting which observations to retain. The policy outputs a "memory action" at every timestep, selecting which observation to pop from the stack before pushing the new one. This transforms memory management from a passive FIFO process into an active decision problem aligned with the reward signal. The core assumption is that environment causal dependencies are sparse relative to time; only a small subset of observations are actually necessary for future rewards.

### Mechanism 2: Convergence via Value-Consistency
Theoretical convergence is guaranteed for Temporal Difference learning under Adaptive Stacking, provided the compression induces a "value-consistent" state abstraction. By definition, value-consistency ensures that different latent histories compressed into the same memory stack state share the same expected return. This preserves the partial ordering of value functions required for standard TD convergence proofs. The core assumption is that the state abstraction (stack content) must be sufficient to distinguish between histories with different optimal values.

### Mechanism 3: Horizon Generalization via Abstraction
Learning to retain only relevant cues allows the agent to generalize to horizons much longer than those seen during training. Because the agent learns which observation to keep (the goal cue) rather than simply keeping the most recent ones, the agent state space remains compact regardless of the temporal distance between the cue and the reward. The core assumption is that the semantic relevance of observations remains stable across different horizon lengths.

## Foundational Learning

- **Concept:** Markov Order vs. Sufficient Memory (k* vs κ)
  - **Why needed here:** The paper fundamentally relies on the distinction between the memory needed to make the environment strictly Markov (k*) and the minimal memory needed for optimal action (κ). Without this, Adaptive Stacking offers no advantage.
  - **Quick check question:** Can you identify an environment where κ < k*?

- **Concept:** Partially Observable Markov Decision Processes (POMDPs)
  - **Why needed here:** The theoretical grounding treats the Adaptive Stacking process as a POMDP where the "agent state" (the stack) is an incomplete summary of the true "environment state" (the full history).
  - **Quick check question:** Why does a finite stack induce a POMDP even if the underlying environment is fully observable given the full history?

- **Concept:** State Abstraction
  - **Why needed here:** Adaptive Stacking is framed as a form of state abstraction, specifically "value-equivalent abstraction." Understanding this is crucial for diagnosing convergence failures.
  - **Quick check question:** What specific property must a state abstraction satisfy to guarantee convergence in TD-learning?

## Architecture Onboarding

- **Component map:** Policy Network -> Environment Action + Memory Action -> Stack Update -> Next State
- **Critical path:** The gradient flow through the memory action decision. The agent must learn that removing a specific observation now affects the value estimation at a much later timestep (long-term credit assignment).
- **Design tradeoffs:**
  - Action Space Expansion: Adding k memory actions increases the policy output space. For large k, this adds complexity compared to implicit retention in RNNs.
  - Determinism: The stack is a discrete, deterministic container. This is explicit and interpretable but non-differentiable regarding the stack content (requires RL signals, not backprop through structure).
- **Failure signatures:**
  - VC Violation: Performance collapses in dense-reward environments where history matters but is not captured in the small stack.
  - Catastrophic Forgetting: The agent learns to pop the most recent item (mimicking Frame Stacking) because the long-term reward signal is too sparse to reinforce the retention of early cues.
- **First 3 experiments:**
  1. Implement the TMaze environment with L=16 and k=2. Verify if the agent learns to keep the "color" observation and discard "corridor" observations.
  2. Run AS with k values varying from 1 to k*. Plot the "Memory Regret" metric to see if the agent fails to learn when k < κ.
  3. Introduce a dense reward signal that depends on the count of steps taken (violating VC). Compare AS performance against an Oracle FS to confirm the theoretical break condition.

## Open Questions the Paper Calls Out

### Open Question 1
How can Adaptive Stacking be extended to handle environments that violate the Value-Consistency assumption, such as those with unobservable reward machines? The paper explicitly identifies environments with unobservable reward machines as counter-examples where the primary theoretical assumption (Value-Consistency) fails. The authors state this distinction is useful for "future work aiming to blend Adaptive Stacking with automaton inference." This remains unresolved because the primary theoretical guarantees for TD-learning convergence rely on the Value-Consistency assumption, which does not hold in these complex temporal logic tasks.

### Open Question 2
Can convergence guarantees be maintained for Adaptive Stacking in continuing "big worlds" where unbiased Monte Carlo rollouts are computationally intractable? Section 4.2 notes that while unbiased estimates simplify convergence, they are unwieldy in big worlds. The authors state, "it will eventually be necessary to use truncated returns with bias inserted from bootstrapped value estimates to tackle the challenging futuristic environments." This is unresolved because the primary convergence theorem relies on unbiased optimization, and the theoretical results for continuing settings rely on mixing times or regenerative sampling which may not scale to "big worlds."

### Open Question 3
How does Adaptive Stacking scale to multi-agent settings where memory must model the changing behaviors of other agents? Appendix A discusses the "small agent in big world" problem, noting that the environment often contains other agents. The authors suggest the Adaptive Stack can be viewed as an "attempt to model their changing behaviour," relating the approach to Active Equilibria. This remains unresolved because the paper's formulation and experiments are restricted to single-agent NMDPs/POMDPs and do not address the non-stationarity introduced by learning opponents or teammates.

## Limitations

- The theoretical analysis relies heavily on the value-consistency assumption, which is not empirically verified across all tested environments
- Experiments focus on discrete, structured environments with visually distinct observation signals; performance in continuous control tasks with high-dimensional observations is unknown
- The approach adds action space complexity proportional to stack size, which may become prohibitive for very large memory budgets

## Confidence

- **High Confidence**: The core mechanism of selective retention via learned gating is well-supported by experimental results, particularly the T-Maze and XorMaze tasks
- **Medium Confidence**: The theoretical convergence proofs are sound within their stated assumptions, but practical conditions under which value-consistency holds require further validation
- **Medium Confidence**: The horizon generalization claims are supported by the long-horizon maze experiment, but the sample size is limited and specific enabling conditions are not fully characterized

## Next Checks

1. **Value-Consistency Stress Test**: Implement a dense-reward variant of the T-Maze where the reward depends on the number of steps taken (violating value-consistency). Compare Adaptive Stacking performance against oracle Frame Stacking to empirically test the theoretical break condition.

2. **Continuous Observation Test**: Adapt the XorMaze environment to use continuous observation vectors (e.g., pixel observations) rather than discrete colors. Train Adaptive Stacking and compare its memory efficiency and performance against an LSTM baseline.

3. **Overfitting Analysis**: Systematically vary the training horizon length (L) for the T-Maze and measure both training and test performance. Quantify the degree of overfitting by comparing returns on seen vs. unseen horizon lengths, and analyze whether the memory regret metric correlates with generalization performance.