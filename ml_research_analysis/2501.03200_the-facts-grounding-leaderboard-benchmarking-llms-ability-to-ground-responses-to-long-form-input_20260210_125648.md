---
ver: rpa2
title: 'The FACTS Grounding Leaderboard: Benchmarking LLMs'' Ability to Ground Responses
  to Long-Form Input'
arxiv_id: '2501.03200'
source_url: https://arxiv.org/abs/2501.03200
tags:
- response
- context
- arxiv
- responses
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work introduces FACTS Grounding, a benchmark and leaderboard\
  \ evaluating LLMs\u2019 ability to generate factually accurate, long-form responses\
  \ grounded in a provided context document (up to 32k tokens) given a user request.\
  \ The evaluation uses automated judge models to assess factuality and instruction-following\
  \ eligibility, with results aggregated to mitigate bias."
---

# The FACTS Grounding Leaderboard: Benchmarking LLMs' Ability to Ground Responses to Long-Form Input

## Quick Facts
- arXiv ID: 2501.03200
- Source URL: https://arxiv.org/abs/2501.03200
- Reference count: 22
- Top-performing model: Gemini 2.0 Flash Experimental achieves 83.6% average factuality score

## Executive Summary
This paper introduces FACTS Grounding, a benchmark and leaderboard evaluating LLMs' ability to generate factually accurate, long-form responses grounded in provided context documents. The benchmark uses automated judge models to assess factuality and instruction-following eligibility, with results aggregated to mitigate bias. Models are ranked on their factuality score, which penalizes responses that fail to address the user request or deviate from the context. The benchmark contains 1,719 prompts across diverse domains and tasks.

## Method Summary
The benchmark evaluates LLMs on their ability to ground responses in context documents up to 32k tokens. The evaluation process involves two phases: first, an eligibility check using three judge models (Gemini 1.5 Pro, GPT-4o, Claude 3.5 Sonnet) to filter responses that fail to address user requests; second, factuality scoring on eligible responses using the best-performing prompt templates for each judge. Final scores average across all three judges, with ineligible responses treated as inaccurate. The benchmark includes 1,719 prompts across five domains with mean context length of 2.5k tokens.

## Key Results
- Gemini 2.0 Flash Experimental achieves the highest average factuality score of 83.6%
- Multi-judge aggregation reduces self-preference bias (+3.23% inflation observed when judge matches evaluated model)
- Disqualifying ineligible responses leads to 1-5% reduction in final factuality scores
- Different prompt templates show 15+ point Macro-F1 differences, validating template sensitivity

## Why This Works (Mechanism)

### Mechanism 1: Multi-Judge Aggregation Reduces Evaluator Bias
Aggregating verdicts from multiple judge models mitigates systematic scoring bias inherent in single-model evaluation. Three independent LLM judges evaluate each response using the best-performing prompt template, with final factuality score averaging all three judges' scores. This addresses documented self-preference bias where models rate their own outputs higher than others.

### Mechanism 2: Two-Phase Filtering Prevents Metric Gaming
Disqualifying responses that fail to address user requests prevents trivial solutions from achieving high factuality scores. Phase 1 filters "ineligible" responses via consensus (all three judges must agree on ineligibility). Phase 2 evaluates grounding only on eligible responses, with ineligible responses treated as inaccurate in final score.

### Mechanism 3: Sentence-Level Verification with Strict Entailment Standards
Requiring "straightforward, indisputable evidence" for "supported" labels prevents superficial matches from inflating scores. Judges decompose responses into sentences and classify each as "supported," "unsupported," "contradictory," or "no_rad" (no attribution required). Only explicit textual entailment counts as supported.

## Foundational Learning

- **Grounding / Faithfulness**: Why needed - The benchmark measures whether responses are "fully grounded in the provided context document." Quick check - Given context "The meeting is at 3pm Tuesday," is the response "The meeting is tomorrow" grounded?

- **LLM-as-a-Judge Evaluation**: Why needed - The benchmark relies entirely on automated LLM judges for scoring. Quick check - Why might GPT-4o rate a GPT-4o-generated response more favorably than a Claude-generated response?

- **Data Contamination in Benchmarks**: Why needed - Context documents may appear in pre-training corpora. Quick check - If a model memorized a document during pretraining, would that help or hurt its ability to ground responses exclusively to that document?

## Architecture Onboarding

- **Component map**: Input: (system_instruction, context_document [≤32k tokens], user_request) → Response Generation → Phase 1: Eligibility Check (3 judges, consensus required) → Phase 2: Factuality Scoring (3 judges, best template) → Aggregation: Average of 3 judges → Final Factuality Score

- **Critical path**: Judge prompt template selection is the highest-leverage decision point—Macro-F1 varies from ~50 to ~71 across templates for the same judge model.

- **Design tradeoffs**: Consensus vs. majority voting for eligibility (consensus is more permissive but reduces false positives); strict vs. lenient entailment (strict reduces false positives but may penalize valid reasoning); single vs. multi-judge (multi-judge increases cost ~3x but addresses self-preference bias).

- **Failure signatures**: Models producing very short responses that are technically grounded but unhelpful; judges classifying valid inferences as "unsupported" due to strict template wording; score gaming via responses that are mostly "no_rad" sentences.

- **First 3 experiments**: 1) Replicate judge prompt evaluation on a sample to validate Macro-F1 rankings; 2) Test eligibility threshold sensitivity with ablation comparing consensus vs. majority-vote disqualification; 3) Analyze disagreement patterns to identify systematic biases by domain, response length, and task type.

## Open Questions the Paper Calls Out

The paper raises several open questions about the relationship between grounding performance and other model capabilities, the robustness of automated evaluation against gaming, and the generalizability of grounding skills to complex reasoning tasks. These questions remain unresolved and warrant further investigation.

## Limitations

- Reliance on LLM-as-a-judge evaluation introduces systematic self-preference bias, though multi-judge aggregation mitigates this
- Strict entailment standards may penalize valid inferential reasoning requiring synthesis across multiple context sentences
- Eligibility filtering mechanism may create a "sweet spot" where models learn to produce responses just above the threshold while avoiding substantive claims

## Confidence

- **High Confidence**: Multi-judge aggregation reduces evaluation bias (supported by corpus evidence of LLM narcissism and +3.23% self-preference inflation)
- **Medium Confidence**: Two-phase filtering effectively prevents metric gaming (supported by 1-5% score reduction but weak direct evidence)
- **Medium Confidence**: Sentence-level verification with strict entailment provides robust factuality assessment (supported by prompt sensitivity but potential over-penalization)

## Next Checks

1. Replicate judge prompt template evaluation on a sample to verify Macro-F1 scores match reported ranges (Claude: ~71, Gemini/GPT-4o: ~65)

2. Conduct ablation study comparing consensus-based disqualification against majority-vote disqualification to assess impact on final scores and selection effects

3. Systematically analyze examples where judges disagree most frequently, categorizing by domain, response length, task type, and sentence complexity to identify systematic biases