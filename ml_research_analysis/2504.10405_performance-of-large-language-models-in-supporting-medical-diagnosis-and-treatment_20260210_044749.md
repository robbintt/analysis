---
ver: rpa2
title: Performance of Large Language Models in Supporting Medical Diagnosis and Treatment
arxiv_id: '2504.10405'
source_url: https://arxiv.org/abs/2504.10405
tags:
- performance
- medical
- available
- llms
- accessed
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the performance of 21 contemporary Large Language
  Models (LLMs) on the 2024 Portuguese National Exam for medical specialty access
  (PNA), a standardized medical knowledge assessment. Models were tested using a strict
  pass@1 methodology without prompt engineering to provide a conservative estimate
  of their generalized medical knowledge and reasoning capabilities.
---

# Performance of Large Language Models in Supporting Medical Diagnosis and Treatment

## Quick Facts
- arXiv ID: 2504.10405
- Source URL: https://arxiv.org/abs/2504.10405
- Reference count: 40
- Key outcome: Several LLMs (e.g., Gemini 2.5 Pro, DeepSeek R1) exceeded human median scores on the 2024 Portuguese National Medical Exam, highlighting their potential as clinical assistive tools.

## Executive Summary
This study evaluates 21 contemporary Large Language Models on the 2024 Portuguese National Medical Exam (PNA), a rigorous 150-question multiple-choice assessment for medical specialty access. Using a strict pass@1 methodology and no prompt engineering, the study provides a conservative estimate of models' generalized medical knowledge and reasoning capabilities. Several LLMs, including both open- and closed-source models, achieved accuracy scores exceeding human benchmarks. The findings suggest that LLMs could serve as valuable assistive tools in clinical settings, particularly when considering cost-effectiveness and reasoning methodologies like Chain-of-Thought prompting.

## Method Summary
The study tested 21 LLMs on the 2024 PNA exam, a standardized Portuguese-language medical knowledge assessment. Questions were submitted in batches of 10 via Python scripts, with models providing only their first answer (pass@1). No prompt engineering or model fine-tuning was employed. A novel composite scoring metric balanced accuracy with cost-efficiency and potential data contamination risk. Performance was compared against human benchmarks including median scores, 95th percentile, and top student performance.

## Key Results
- Several LLMs, including Gemini 2.5 Pro and DeepSeek R1, achieved accuracy scores exceeding human student benchmarks.
- The composite scoring metric highlighted models that balanced high accuracy with cost-effectiveness and lower contamination risk.
- Performance varied significantly across models, with some open-source models performing comparably to or better than closed-source counterparts.

## Why This Works (Mechanism)
The study demonstrates that LLMs possess substantial generalized medical knowledge that can be effectively assessed through standardized multiple-choice examinations. By using a pass@1 methodology without prompt engineering, the research provides a conservative estimate of models' baseline medical reasoning capabilities. The composite scoring approach balances traditional accuracy metrics with practical considerations like cost and data contamination risk, offering a more holistic evaluation framework for medical LLM deployment.

## Foundational Learning
- **Pass@k methodology**: A evaluation approach where k answers are generated and the best is selected. Why needed: To understand different answer selection strategies. Quick check: Compare pass@1 vs pass@5 results.
- **Chain-of-Thought prompting**: A technique where models are encouraged to show their reasoning process. Why needed: To assess potential performance improvements with reasoning guidance. Quick check: Apply CoT to top-performing models and measure accuracy gains.
- **Data contamination risk**: The possibility that models were trained on exam data. Why needed: To evaluate the validity of results and generalization. Quick check: Compare model performance on in-distribution vs out-of-distribution medical questions.

## Architecture Onboarding
- **Component map**: Exam questions (Portuguese) -> LLM API/Inference -> Answer extraction -> Score calculation -> Composite scoring
- **Critical path**: Question batching (10 per prompt) -> Model inference (pass@1) -> Answer parsing -> Accuracy calculation -> Composite score computation
- **Design tradeoffs**: Strict pass@1 methodology ensures conservative estimates but may underestimate true capabilities; no prompt engineering maintains generalizability but misses potential performance gains.
- **Failure signatures**: Incorrect answers may stem from factual hallucinations, logical reasoning errors, or insufficient medical knowledge; performance drops may indicate context window limitations or tokenization issues.
- **3 first experiments**: 1) Replicate with prompt engineering to assess performance ceiling. 2) Test on a different medical knowledge assessment for generalizability. 3) Analyze error types qualitatively to understand failure modes.

## Open Questions the Paper Calls Out
1. Can LLMs maintain superior performance over medical students in a live, blinded exam setting where data contamination is strictly eliminated? (Proposed as PNA 2025 LLM-Student Showdown)
2. To what extent does high accuracy on standardized multiple-choice exams translate to efficacy in real-time, interactive clinical diagnosis? (Calls for Clinical Vignette Evaluation)
3. What is the specific distribution of reasoning errors versus hallucinations in top-tier medical LLMs? (Admits lack of systematic qualitative error analysis)

## Limitations
- The strict pass@1 methodology without prompt engineering may underestimate models' practical clinical reasoning capabilities.
- The study uses a single Portuguese-language exam, limiting generalizability to other languages or healthcare systems.
- The composite scoring metric incorporates subjective elements such as contamination risk estimates and estimated pricing.

## Confidence
- **High confidence**: Several LLMs (e.g., Gemini 2.5 Pro, DeepSeek R1) outperformed human median scores on this specific exam.
- **Medium confidence**: LLMs can serve as assistive clinical tools, depending on real-world validation beyond standardized tests.
- **Low confidence**: Cost-effectiveness comparisons, given reliance on estimated token prices and absence of actual usage data.

## Next Checks
1. Replicate the experiment using the same 2024 PNA dataset with prompt engineering (e.g., few-shot examples, explicit CoT instructions) to assess performance uplift.
2. Evaluate the same models on an independent, multilingual medical knowledge assessment to test cross-language generalizability.
3. Conduct a head-to-head comparison of model performance on the PNA exam under both pass@1 and pass@k (k>1) conditions to quantify the impact of answer selection strategy.