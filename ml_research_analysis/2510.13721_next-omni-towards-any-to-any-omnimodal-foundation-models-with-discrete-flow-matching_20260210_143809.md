---
ver: rpa2
title: 'NExT-OMNI: Towards Any-to-Any Omnimodal Foundation Models with Discrete Flow
  Matching'
arxiv_id: '2510.13721'
source_url: https://arxiv.org/abs/2510.13721
tags:
- arxiv
- generation
- understanding
- multimodal
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: NExT-OMNI addresses the challenge of achieving unified multimodal
  understanding, generation, and retrieval across text, images, video, and audio within
  a single framework. The model leverages discrete flow matching (DFM) combined with
  reconstruction-enhanced unified representations to enable any-to-any cross-modal
  generation and interaction while supporting cross-modal retrieval tasks.
---

# NExT-OMNI: Towards Any-to-Any Omnimodal Foundation Models with Discrete Flow Matching

## Quick Facts
- arXiv ID: 2510.13721
- Source URL: https://arxiv.org/abs/2510.13721
- Reference count: 40
- Key outcome: NExT-OMNI addresses the challenge of achieving unified multimodal understanding, generation, and retrieval across text, images, video, and audio within a single framework

## Executive Summary
NExT-OMNI presents an omnimodal foundation model that unifies understanding, generation, and retrieval across text, images, video, and audio using discrete flow matching (DFM) instead of traditional autoregressive architectures. The model leverages reconstruction-enhanced unified representations to enable any-to-any cross-modal generation and interaction while supporting cross-modal retrieval tasks. By replacing sequential autoregressive decoding with parallel DFM denoising, NExT-OMNI achieves faster inference and improved multimodal feature fusion, demonstrating competitive performance on standard benchmarks and outperforming prior unified models in multi-turn multimodal interaction and cross-modal retrieval tasks.

## Method Summary
NExT-OMNI employs a three-stage progressive training pipeline: (1) PT for modality alignment and unified representation learning, (2) CPT for extended context and video understanding, and (3) SFT for instruction following. The core innovation is discrete flow matching, which replaces autoregressive token-by-token generation with parallel denoising of entire sequences. The model uses a unified encoder (initialized from CLIP-ViT-Large and Whisper-Turbo) combined with a Qwen2.5-7B backbone, trained with reconstruction losses and semantic alignment objectives. Dynamic length generation with adaptive caching provides 1.2× inference speedup compared to autoregressive approaches. The unified representation design enables both generation and retrieval tasks from a single encoder, eliminating the need for separate understanding and generation modules.

## Key Results
- Achieves 1.2× inference speedup compared to autoregressive architectures through dynamic length generation with adaptive caching
- Demonstrates competitive performance on OmniBench, WorldSense, AV-Odyssey, OpenING, and Spoken QA benchmarks
- Outperforms prior unified models in multi-turn multimodal interaction and cross-modal retrieval tasks (InfoSeek, OVEN)
- Enables any-to-any generation and retrieval across text, images, video, and audio from a single unified encoder

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Discrete flow matching enables parallel bidirectional information integration, improving multimodal fusion over sequential autoregressive decoding
- Mechanism: DFM samples a noisy sequence x_t at time t, then iteratively denoises the entire sequence in parallel rather than token-by-token. Metric-induced probability paths use token embedding distances to guide transitions toward target tokens
- Core assumption: Parallel denoising with bidirectional attention better aggregates cross-modal context than causal-masked autoregression
- Evidence anchors:
  - [abstract] "By leveraging metric-induced probability paths and kinetic optimal velocities, NExT-OMNI natively supports any-to-any understanding and generation with enhanced response efficiency"
  - [section] "these models begin with completely corrupted sequences and iteratively denoise entire sequences in parallel, enabling richer bidirectional information integration"
  - [corpus] FlowBind and AR-Omni papers explore similar any-to-any generation; corpus shows emerging interest but limited comparative validation of DFM vs AR specifically
- Break condition: If sequential dependencies are critical (e.g., long-form coherent narratives), DFM's parallel approach may introduce coherence artifacts

### Mechanism 2
- Claim: Reconstruction-enhanced unified representations preserve both semantic alignment and low-level detail, enabling cross-modal retrieval and generation from a single encoder
- Mechanism: During encoder warmup, joint training combines (i) VQVAE reconstruction loss for low-level detail and (ii) semantic alignment loss (CLIP-style contrastive for vision, caption generation for audio). These losses are reused during DFM training to prevent over-focusing on high-level semantics
- Core assumption: A single encoder can serve both understanding (semantic) and generation (detail) tasks without decoupled architectures
- Evidence anchors:
  - [abstract] "reconstruction-enhanced unified representations to enable any-to-any cross-modal generation and interaction while supporting cross-modal retrieval tasks"
  - [section] Equation (3): L_overall = λ1·Lce + λ2·LV_rec + λ3·LA_rec, showing reconstruction terms explicitly constrain DFM training
  - [corpus] Weak corpus evidence; related papers don't emphasize reconstruction feedback as explicitly
- Break condition: If understanding and generation require fundamentally different granularity, unified representations may underperform specialized decoupled encoders on either task

### Mechanism 3
- Claim: Dynamic length generation with adaptive caching accelerates inference while maintaining text generation quality
- Mechanism: Responses are padded to block-size multiples during training. At inference, the model dynamically adjusts generation length based on <EOS> confidence, then performs multi-step denoising. Caching minimizes recomputation for instruction tokens and selectively updates response features based on cosine similarity
- Core assumption: Simple tokens can be determined in fewer denoising steps, and most features change minimally across steps
- Evidence anchors:
  - [abstract] "achieves faster inference and improved multimodal feature fusion"
  - [section] "This vanilla adaptive cache implementation, combined with the parallel decoding advantages of NExT-OMNI's DFM architecture, achieves a 1.2× inference response speed improvement compared to AR architectures"
  - [corpus] No direct corpus comparison for this caching strategy
- Break condition: If response content is highly unpredictable or requires precise token-by-token refinement, dynamic length may truncate or pad inappropriately

## Foundational Learning

- Concept: **Flow Matching on Discrete Tokens**
  - Why needed here: DFM extends continuous flow matching to discrete token spaces using probability paths that transition from source noise to target tokens. Requires understanding of Markov chain sampling and the Euler solver for CTMC simulation
  - Quick check question: Can you explain how metric-induced probability paths differ from standard interpolation for discrete tokens?

- Concept: **Vector Quantized Variational Autoencoders (VQVAE)**
  - Why needed here: Used for unified representation learning—continuous encoder outputs are mapped to discrete codebook entries, enabling both reconstruction and semantic alignment. Multi-codebook quantization (MCQ) balances detail vs. predictability
  - Quick check question: How does increasing the number of sub-codebooks affect reconstruction quality versus generation difficulty?

- Concept: **Bidirectional vs Causal Attention**
  - Why needed here: DFM leverages bidirectional attention to aggregate context from all positions simultaneously, unlike AR's causal masking. This is central to the claimed performance gains in retrieval and fusion
  - Quick check question: Why might bidirectional attention degrade performance on tasks requiring strict left-to-right generation?

## Architecture Onboarding

- Component map:
  - Modality Encoders (CLIP-ViT-Large, Whisper-Turbo) -> VQVAE quantization -> Unified encoder warmup -> DFM backbone (Qwen2.5-7B) -> Modality heads (AR-style) -> Any-to-any generation/retrieval

- Critical path:
  1. Encoder warmup (reconstruction + semantic alignment on 70M image-text pairs, 102K hours audio)
  2. DFM training with interleaved multimodal data, interleaved batch training by modality
  3. Inference with dynamic length generation + adaptive caching

- Design tradeoffs:
  - Unified encoder vs decoupled: Single encoder simplifies architecture and enables retrieval, but may sacrifice task-specific optimization
  - MCQ codebook count: More sub-codebooks improve reconstruction but make multi-index prediction harder
  - Block size for dynamic generation: Larger blocks reduce padding waste but may over-generate

- Failure signatures:
  - Degraded text generation quality -> check if reconstruction loss terms are weighted too heavily
  - Poor cross-modal retrieval -> verify unified representation is being extracted from <EOS> token, not intermediate layers
  - Slow inference despite caching -> ensure instruction tokens are cached and only response features are adaptively updated

- First 3 experiments:
  1. Ablate reconstruction loss (set λ2, λ3 to 0) to validate its contribution to generation and retrieval performance
  2. Compare unified encoder vs dual encoder (understanding-focused + generation-focused) on InfoSeek/OVEN retrieval benchmarks
  3. Profile inference latency with and without adaptive caching across different response lengths to verify claimed 1.2× speedup

## Open Questions the Paper Calls Out

- How does the discrete flow matching (DFM) paradigm scale with model parameters beyond the 7B scale and training tokens exceeding 2T? [explicit] Appendix C (Limitation) states the authors only conducted training at the 7B scale with 2T tokens due to resource constraints, and the "full potential has not been demonstrated" without larger foundation model support

- Can the unified representation and DFM architecture be effectively adapted for embodied AI tasks, specifically action trajectory generation? [explicit] Section 4 (Conclusion) explicitly lists extending NExT-OMNI to "action trajectory generation in vision-language-action models" as a future direction

- Does the "reconstruction-enhanced unified representation" fully mitigate the performance conflicts between generation and understanding tasks compared to decoupled architectures? [inferred] Table 5 shows a performance drop in understanding (VQAv2) when moving from "DFM Decoupled" to "DFM Unified" before applying dynamic strategies, and Section 3.6 discusses the trade-offs between unified and decoupled designs

## Limitations

- Reliance on proprietary training data (100K hours audio, internal video) prevents exact replication and raises questions about generalizability
- Reconstruction-enhanced unified representations lack comparative validation against specialized decoupled encoders on retrieval benchmarks
- Claimed 1.2× inference speedup from adaptive caching lacks detailed hyperparameter specifications that could affect real-world performance
- DFM architecture's superiority over autoregressive models lacks ablation studies isolating the contribution of parallel bidirectional denoising

## Confidence

**High Confidence**: Claims about DFM's parallel denoising mechanism and unified representation design are well-supported by equations and implementation details. The three-stage training pipeline with progressive context length increases is clearly specified.

**Medium Confidence**: Performance claims on standard benchmarks are reasonable but rely on proprietary data composition. The 1.2× inference speedup is plausible given the described caching mechanism but lacks detailed implementation specifications.

**Low Confidence**: The assertion that unified representations enable effective cross-modal retrieval without specialized architectures is not empirically validated against decoupled approaches. The exact contribution of reconstruction loss terms to generation quality versus semantic understanding remains unclear.

## Next Checks

1. **Reconstruct vs. Specialized**: Compare NExT-OMNI's unified encoder against a decoupled architecture (separate understanding and generation encoders) on InfoSeek and OVEN cross-modal retrieval benchmarks to quantify the performance tradeoff of the unified design

2. **Cache Sensitivity Analysis**: Systematically vary the adaptive caching threshold parameters and measure the actual inference speedup across different response lengths and modalities to verify the claimed 1.2× improvement and identify optimal settings

3. **Reconstruction Ablation Study**: Train NExT-OMNI with reconstruction loss terms (λ₂, λ₃) set to zero and evaluate the impact on both generation quality (image/text fidelity) and retrieval performance to isolate the contribution of the reconstruction-enhanced unified representations