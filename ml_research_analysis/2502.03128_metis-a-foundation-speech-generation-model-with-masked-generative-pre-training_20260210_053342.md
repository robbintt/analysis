---
ver: rpa2
title: 'Metis: A Foundation Speech Generation Model with Masked Generative Pre-training'
arxiv_id: '2502.03128'
source_url: https://arxiv.org/abs/2502.03128
tags:
- speech
- arxiv
- generation
- wang
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces Metis, a foundation model for unified speech
  generation. Metis uses a two-stage approach: pre-training on 300K hours of unlabeled
  speech data with masked generative modeling on SSL tokens, followed by fine-tuning
  on specific tasks with task-specific conditions.'
---

# Metis: A Foundation Speech Generation Model with Masked Generative Pre-training
## Quick Facts
- arXiv ID: 2502.03128
- Source URL: https://arxiv.org/abs/2502.03128
- Authors: Yuancheng Wang; Jiachen Zheng; Junan Zhang; Xueyao Zhang; Huan Liao; Zhizheng Wu
- Reference count: 40
- Primary result: Foundation model for unified speech generation outperforming task-specific systems with <20M parameters and 300× less data

## Executive Summary
Metis introduces a foundation model for unified speech generation that achieves state-of-the-art performance across five speech generation tasks through a two-stage approach. The model uses masked generative pre-training on 300K hours of unlabeled speech data, followed by fine-tuning on specific tasks with task-specific conditions. What makes Metis particularly noteworthy is its dual discrete speech representation system that combines SSL tokens for semantic/prosodic information with acoustic tokens for waveform reconstruction.

The model demonstrates remarkable efficiency, outperforming task-specific or multi-task systems even with fewer than 20M trainable parameters and requiring 300 times less training data. Metis supports multimodal conditional inputs and shows strong performance in multi-task fine-tuning scenarios, positioning it as a promising foundation model for speech generation applications.

## Method Summary
Metis employs a two-stage training approach: pre-training on 300K hours of unlabeled speech data using masked generative modeling on SSL tokens, followed by fine-tuning on specific tasks with task-specific conditions. The model utilizes two discrete speech representations - SSL tokens for semantic and prosodic information, and acoustic tokens for waveform reconstruction. This dual representation allows the model to capture both high-level speech characteristics and fine-grained acoustic details. The pre-training stage focuses on learning general speech representations, while fine-tuning adapts these representations to specific generation tasks such as text-to-speech, voice conversion, speaker extraction, speech enhancement, and lip-to-speech synthesis.

## Key Results
- Achieves state-of-the-art performance across five speech generation tasks with <20M trainable parameters
- Requires 300 times less training data compared to traditional task-specific systems
- Demonstrates strong multi-task fine-tuning capabilities and supports multimodal conditional inputs
- Outperforms existing task-specific or multi-task systems even with minimal fine-tuning requirements

## Why This Works (Mechanism)
The dual discrete representation system is the key mechanism that enables Metis's performance. By separating semantic/prosodic information (SSL tokens) from acoustic details (acoustic tokens), the model can learn richer, more structured representations of speech. The masked generative pre-training forces the model to learn robust representations by predicting missing tokens, similar to how BERT works for text but adapted for speech. This approach allows the model to capture both the content and style of speech in a disentangled manner, making it easier to adapt to different tasks during fine-tuning.

## Foundational Learning
- **Masked generative pre-training**: Why needed - to learn robust speech representations from unlabeled data; Quick check - compare pre-trained vs randomly initialized models on downstream tasks
- **Dual discrete representations**: Why needed - to separate high-level semantic information from low-level acoustic details; Quick check - ablation study removing one token type
- **Task-specific fine-tuning**: Why needed - to adapt general representations to specific speech generation tasks; Quick check - measure performance drop when skipping fine-tuning
- **SSL token extraction**: Why needed - to capture semantic and prosodic information in discrete form; Quick check - visualize token distributions across different speakers/content
- **Acoustic token reconstruction**: Why needed - to ensure high-quality waveform generation; Quick check - measure reconstruction quality metrics (e.g., PESQ, STOI)
- **Multimodal conditioning**: Why needed - to support various input types (text, audio, video); Quick check - test performance with different input modalities

## Architecture Onboarding
**Component map**: Speech input -> SSL tokenizer -> Masked reconstruction module -> Acoustic tokenizer -> Waveform reconstruction -> Output speech
**Critical path**: Masked generative pre-training (SSL tokens) -> Dual representation learning -> Task-specific fine-tuning (with conditions) -> Multi-task adaptation
**Design tradeoffs**: The dual token system adds complexity but enables better disentanglement of speech attributes. Pre-training requires massive unlabeled data but reduces fine-tuning needs. The model sacrifices some task-specific optimization for generalization capability.
**Failure signatures**: Poor performance on out-of-domain speech (accents, noise conditions), degradation when SSL tokens are corrupted, suboptimal results when fine-tuning data is limited or mismatched with pre-training distribution.
**First experiments**: 1) Test pre-trained model on held-out fine-tuning tasks without adaptation, 2) Conduct ablation study removing acoustic tokens, 3) Evaluate model robustness on noisy or accented speech samples

## Open Questions the Paper Calls Out
None identified in the provided information.

## Limitations
- Heavy reliance on MOS scores, which are subjective and may not capture all quality aspects
- Limited ablation studies on the importance of the dual token representation system
- Insufficient details about pre-training data quality, domain coverage, and potential biases
- Unexplored generalizability to out-of-distribution speech conditions and languages beyond training data

## Confidence
**High**: The core technical approach of masked generative pre-training with dual discrete representations is well-articulated and demonstrates clear empirical advantages
**Medium**: The "foundation model" claims are partially supported but need more rigorous testing across diverse speech domains and languages
**Low**: Scalability claims (300× less data, <