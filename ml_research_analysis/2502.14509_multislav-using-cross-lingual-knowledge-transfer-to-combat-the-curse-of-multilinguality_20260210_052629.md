---
ver: rpa2
title: 'MultiSlav: Using Cross-Lingual Knowledge Transfer to Combat the Curse of Multilinguality'
arxiv_id: '2502.14509'
source_url: https://arxiv.org/abs/2502.14509
tags:
- language
- translation
- directions
- languages
- slavic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates multilingual neural machine translation (NMT)
  for Slavic languages, testing whether extending the data regime through cross-lingual
  knowledge transfer improves performance. The authors trained several model variants,
  including bi-directional, pivot-based, and many-to-many multilingual models, on
  parallel data for Czech, Polish, Slovak, Slovene, and English.
---

# MultiSlav: Using Cross-Lingual Knowledge Transfer to Combat the Curse of Multilinguality

## Quick Facts
- arXiv ID: 2502.14509
- Source URL: https://arxiv.org/abs/2502.14509
- Reference count: 19
- Primary result: Multilingual models outperform bilingual baselines in Slavic languages, achieving 89.2 COMET average across 20 translation directions

## Executive Summary
This study evaluates multilingual neural machine translation for Slavic languages, testing whether extending the data regime through cross-lingual knowledge transfer improves performance. The authors trained several model variants, including bi-directional, pivot-based, and many-to-many multilingual models, on parallel data for Czech, Polish, Slovak, Slovene, and English. They found that multilingual models outperform baselines even in zero-shot translation scenarios, demonstrating effective cross-lingual transfer. Adding English to the Slavic language set increased overall translation quality without causing the curse of multilinguality. The MultiSlav+ENG model achieved the best average COMET score of 89.2 across all 20 translation directions. The results show that training on closely related languages can yield state-of-the-art translation quality, especially for low-resource pairs. Models are released under an open license.

## Method Summary
The authors trained transformer-based NMT models using MarianNMT on parallel corpora from the MTData library. They compared bi-directional bilingual models, pivot-based models, and many-to-many multilingual models. Models used 6-layer encoder-decoder transformers with 1024 hidden dimensions and 16 attention heads. SentencePiece unigram tokenization was used with vocabularies sized 32k-80k depending on language count. Language tokens (e.g., >>ces<<) were prepended to indicate target language. Training used Adam optimizer with inverse sqrt learning rate decay, and evaluation was performed on FLORES-101 dev set using COMET as the primary metric.

## Key Results
- MultiSlav model achieved 89.2 COMET average across 20 translation directions
- Zero-shot translation (SLK→SLV) achieved 90.1 COMET without direct training data
- Adding English to Slavic languages (MultiSlav+ENG) improved 19/20 directions without degrading Slavic-only translation quality
- Many2Many multilingual models outperformed pivot-based approaches by 0.4-0.6 COMET points

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multilingual training within a closely related language family enables cross-lingual knowledge transfer that improves translation quality, particularly for low-resource directions.
- Mechanism: Shared linguistic features (morphological richness, lexical overlap, syntactic similarity) among Slavic languages create overlapping representations in the embedding space. When the model learns translation patterns from Czech↔Polish (63M examples), this knowledge generalizes to Slovak↔Slovene (18M examples) through shared latent representations.
- Core assumption: Languages from the same family written in the same script share transferable representations.
- Evidence anchors:
  - [abstract] "we prove cross-lingual benefits even in 0-shot translation regime for low-resource languages"
  - [section 6] Zero-shot ablation: models trained excluding SLK→SLV data still outperformed baselines (90.1 vs 89.4 COMET)
  - [corpus] ATLAS paper confirms scaling laws for multilingual transfer; limited direct corpus evidence for Slavic-specific transfer mechanisms
- Break condition: Adding languages from different families or scripts may introduce negative transfer; adding English showed neutral-to-positive effects but may not generalize to non-Latin scripts.

### Mechanism 2
- Claim: Many2Many multilingual models outperform pivot-based approaches by eliminating error accumulation and utilizing all available bitext directly.
- Mechanism: Pivot translation (A→pivot→B) compounds errors through two sequential models and ignores direct A↔B parallel data. Many2Many models learn direct mappings between all language pairs simultaneously, allowing the optimizer to leverage both direct and indirect signal paths.
- Core assumption: Direct parameter sharing across all translation directions is more effective than chaining specialized models.
- Evidence anchors:
  - [section 5.3] Pivot models showed "+0.1 to +0.2 COMET" improvement over baseline; Many2Many showed "+0.6 COMET"
  - [section 5.3] "accumulating errors in each pass and lack of direct training bitext" explains pivot underperformance
  - [corpus] Group then Scale paper suggests mixture-of-experts can mitigate language competition; not directly tested here
- Break condition: Pivot models may be preferable when memory constraints prevent loading a single large multilingual model, or when specific directions require independent fine-tuning.

### Mechanism 3
- Claim: Adding a high-resource pivot language (English) to a language-family-specific multilingual model provides additional training signal without degrading intra-family translation quality.
- Mechanism: English provides 3x more parallel data (Table 1: 578M total vs 185M Slavic-only). The shared encoder-decoder parameters learn general translation competence from English pairs while Slavic-specific patterns are preserved through the Slavic bitext.
- Core assumption: Cross-family language addition does not cause capacity saturation or representation interference.
- Evidence anchors:
  - [section 7] "We did not observe any drawbacks that would be potentially brought by 'The Curse of Multilinguality'"
  - [section 5.4] MultiSlav+ENG improved COMET in 19/20 directions; Slavic directions showed no degradation vs Slavic-only model
  - [corpus] ATLAS paper explicitly models capacity saturation thresholds; not validated for this specific architecture
- Break condition: Morphologically rich languages may lose grammatical information when translating through morphologically simpler pivots (English); authors note this risk but do not quantify it.

## Foundational Learning

- Concept: **Many2Many Neural Machine Translation**
  - Why needed here: This is the core architecture enabling single-model multilingual translation across all directions simultaneously.
  - Quick check question: Can you explain why a single encoder-decoder model can translate between any pair of its trained languages, including directions it wasn't explicitly trained on?

- Concept: **Language Tokens (Target-Side Hinting)**
  - Why needed here: Required to disambiguate output language when multiple target languages share the same source; implemented as `>>xxx<<` prefixes.
  - Quick check question: Why might source-side language tokens be redundant when translating from a language-specific source?

- Concept: **SentencePiece Unigram Tokenization**
  - Why needed here: Vocabulary construction must balance coverage across languages while maintaining reasonable vocabulary sizes (16k-80k tokens tested).
  - Quick check question: Why would equal sampling for tokenizer training outperform proportional sampling when corpus sizes are imbalanced?

## Architecture Onboarding

- Component map:
  - Data preparation -> Filtering (FastText LID, length ratios, character whitelists) -> Tokenizer training (equal sampling, 32k-80k vocab) -> Model training (MarianNMT, 6+6 layer transformer) -> Inference (target language token prepended)

- Critical path:
  1. Data preparation: Filter via FastText LID, length ratios, character whitelists (Appendix C)
  2. Tokenizer training: Sample ~40M sentences with equal language weighting
  3. Model training: Adam optimizer, 8000-step warmup, inverse sqrt decay, early stopping after 20 validations without improvement
  4. Inference: Prepend target language token to source sentence

- Design tradeoffs:
  - Vocabulary size: Larger vocabularies (80k) marginally help but increase memory; 16k per language is sufficient
  - Sampling strategy: Equal sampling prevents English dominance but may underrepresent rare Slavic patterns
  - Pivot vs Many2Many: Pivot models are 2x smaller (separate Many2One + One2Many) but sacrifice quality

- Failure signatures:
  - Training instability when increasing layers beyond 6 (authors tested 10, found unstable)
  - LLM "safety refusals" blocking translation of controversial content (0.12-0.55% of test set)
  - Morphological information loss when pivoting through English for gender/number agreement

- First 3 experiments:
  1. Replicate baseline: Train Czech↔Polish bi-directional model on filtered data; target ~89.4 COMET baseline
  2. Zero-shot validation: Train MultiSlav excluding Slovak↔Slovene; verify >baseline performance on held-out direction
  3. Ablate English: Compare MultiSlav vs MultiSlav+ENG on Slavic-only test directions; expect ≤0.1 COMET difference

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does cross-lingual knowledge transfer manifest effectively within a language family when the languages use different scripts (e.g., Latin vs. Cyrillic)?
- Basis in paper: [explicit] Section 8 (Future Work) asks if transfer is "inherent to in-language-family scenarios despite different alphabets."
- Why unresolved: The study restricted its scope to Latin-script Slavic languages (Czech, Polish, Slovak, Slovene).
- What evidence would resolve it: Extending the MultiSlav model to include Cyrillic-script Slavic languages (e.g., Russian, Ukrainian) and comparing transfer rates against same-script pairs.

### Open Question 2
- Question: What is the impact of including geographically co-located languages from different families (e.g., Romanian, Hungarian) on the transfer performance within the Slavic family?
- Basis in paper: [explicit] Section 8 notes the study did not account for counter-examples involving languages outside the single family but within the same region.
- Why unresolved: The authors focused solely on the Slavic family to establish a baseline for transfer, excluding external inputs.
- What evidence would resolve it: Training a multilingual model with Slavic and non-Slavic regional neighbors to see if transfer improves or degrades compared to the family-only model.

### Open Question 3
- Question: Why does training a multilingual model without the reverse translation direction (e.g., training SLK→SLV without SLV→SLK) result in better performance for the trained direction?
- Basis in paper: [explicit] Section 6.1 observes this counter-intuitive result in zero-shot ablations and calls for future research to determine if it is a common occurrence.
- Why unresolved: The authors noted the phenomenon but stated a deeper analysis falls outside the scope of the current study.
- What evidence would resolve it: A comprehensive ablation study across multiple language pairs analyzing representation sharing and capacity allocation when directions are removed.

## Limitations
- Findings limited to Latin-script Slavic languages; unclear if transfer benefits generalize to different scripts
- No quantification of morphological information loss when using English as pivot for gender/number agreement
- Scalability to 10+ languages untested; potential curse of multilinguality not fully explored

## Confidence
- High Confidence: Multilingual models outperform bilingual baselines and pivot approaches (consistent COMET improvements)
- Medium Confidence: Adding English does not cause curse of multilinguality (supported by data but mechanism unclear)
- Low Confidence: Generalizability to other language families or script systems (authors acknowledge but provide no empirical evidence)

## Next Checks
1. **Cross-family validation**: Test the Many2Many approach on a multilingual model including both Slavic languages and languages from a different family (e.g., Romance or Germanic) to assess whether the cross-lingual transfer benefits persist across language families with different morphological and syntactic properties.

2. **Script diversity validation**: Evaluate model performance when including languages written in different scripts (e.g., adding Russian Cyrillic or Greek) to determine whether the reported benefits are specific to Latin-script languages or generalize to multilingual settings with script diversity.

3. **Capacity scaling validation**: Systematically vary model capacity (number of layers, embedding dimensions) and measure the onset of capacity saturation or negative transfer when adding more languages, directly testing the curse of multilinguality hypothesis across different model sizes.