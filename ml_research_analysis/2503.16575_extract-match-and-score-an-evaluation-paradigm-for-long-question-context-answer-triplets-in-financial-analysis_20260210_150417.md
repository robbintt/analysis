---
ver: rpa2
title: 'Extract, Match, and Score: An Evaluation Paradigm for Long Question-context-answer
  Triplets in Financial Analysis'
arxiv_id: '2503.16575'
source_url: https://arxiv.org/abs/2503.16575
tags:
- answer
- points
- saliency
- financial
- long-form
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of evaluating long-form question-context-answer
  triplets in financial analysis, where traditional metrics like BLEU and ROUGE fall
  short due to the complexity and length of the texts. The authors propose an Extract,
  Match, and Score (EMS) evaluation paradigm, which decomposes long-form answers into
  salient points, aligns them with reference points, and assigns fine-grained similarity
  scores.
---

# Extract, Match, and Score: An Evaluation Paradigm for Long Question-context-answer Triplets in Financial Analysis

## Quick Facts
- arXiv ID: 2503.16575
- Source URL: https://arxiv.org/abs/2503.16575
- Reference count: 25
- Primary result: Proposed EMS evaluation paradigm decomposes long-form answers into saliency points, aligns them with reference points, and assigns fine-grained similarity scores, demonstrating superior ability to capture nuanced quality differences compared to traditional metrics in financial QA.

## Executive Summary
This paper addresses the challenge of evaluating long-form question-context-answer triplets in financial analysis, where traditional metrics like BLEU and ROUGE fall short due to the complexity and length of the texts. The authors propose an Extract, Match, and Score (EMS) evaluation paradigm, which decomposes long-form answers into salient points, aligns them with reference points, and assigns fine-grained similarity scores. EMS demonstrates superior ability to capture nuanced differences in answer quality compared to baselines like RAGChecker, BLEU, and BERTScore. On a financial QA dataset built from earnings call transcripts, EMS-based metrics show clear improvements in evaluation sensitivity, especially when assessing large language models of varying sizes.

## Method Summary
The EMS evaluation paradigm consists of three stages: (1) Extract saliency points from reference and candidate answers using an LLM with instructions to retain details, remove summaries, and allow duplicates; (2) Match each reference point to the most semantically similar candidate point using an LLM with few-shot in-context learning; (3) Score matched pairs on a 0-1 continuous scale based on completeness, accuracy, and detail level using either an LLM or conventional metrics. The framework computes EMS-Recall, EMS-Precision, and EMS-F1 by aggregating point-level scores. The authors validate their approach on a dataset of 5 questions from Q3 2024 earnings call transcripts of the 10 largest S&P 500 companies, comparing model outputs from Llama3.2 variants (1B, 11B, 90B) against LLM-generated reference answers.

## Key Results
- EMS metrics show clear scaling trend across 1B→11B→90B models (F1: 0.21→0.35→0.40), while BLEU/ROUGE/BERTScore show no consistent pattern
- EMS-Recall and EMS-Precision better differentiate model quality than traditional metrics on long-form financial QA
- LLM-based scoring provides more sensitive evaluation than conventional metrics like ROUGE and BERTScore

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing long-form answers into saliency points enables more sensitive quality assessment than holistic comparison.
- Mechanism: Long-form text is split into discrete claims (each ≤2 sentences) that preserve numerical details and repeated content, creating evaluation units where semantic alignment is tractable for both LLM and conventional scorers.
- Core assumption: Answer quality can be approximated by aggregating point-level quality scores; summaries/introductions are redundant for evaluation purposes.
- Evidence anchors:
  - [abstract]: "decomposes long-form answers into salient points, aligns them with reference points, and assigns fine-grained similarity scores"
  - [Section 2.1]: "saliency points are not brief summaries... should maintain all the details from the original long-form answer"
  - [corpus]: EVA-Score (arXiv:2407.04969) uses similar extraction-validation paradigm for long-form summarization, suggesting decomposition is a convergent approach.

### Mechanism 2
- Claim: Separate matching and scoring stages yield finer-grained evaluation than binary hit-or-miss approaches.
- Mechanism: Matching identifies candidate-reference point pairs via semantic comparison; scoring assigns continuous [0,1] values based on completeness, accuracy, and detail level—allowing partial credit for imperfect matches.
- Core assumption: LLMs can reliably perform pairwise semantic matching and granular scoring when tasks are decomposed.
- Evidence anchors:
  - [Section 2.3]: "scoring stage differs by assigning a soft score (continuous from 0 to 1) to each matched pair... more useful than a binary score"
  - [Section 3.3, Table 1]: EMS shows clear scaling trend across 1B→11B→90B models (F1: 0.21→0.35→0.40), while BLEU/ROUGE/BERTScore show no consistent pattern.

### Mechanism 3
- Claim: EMS metrics (Recall/Precision/F1 over saliency points) better differentiate model quality scaling than n-gram or embedding similarity metrics.
- Mechanism: Aggregating point-level soft scores into EMS-Recall (reference coverage) and EMS-Precision (answer relevance) captures semantic completeness and faithfulness, which correlate with model capability.
- Core assumption: Larger LLMs produce higher-quality long-form answers—a premise used as ground truth for metric validation.
- Evidence anchors:
  - [Section 3.3]: "Based on the well-established understanding that larger models offer superior performance, our proposed EMS evaluation is more effective"
  - [Section 2.4, Eq. 6-9]: Formal definitions show how soft scores aggregate to corpus-level metrics.

## Foundational Learning

- Concept: **Long-form evaluation failure modes of conventional metrics**
  - Why needed here: BLEU/ROUGE rely on n-gram overlap, which becomes sparse and uninformative for multi-paragraph answers with paraphrasing and structural variation.
  - Quick check question: If a candidate answer paraphrases all reference facts correctly but shares few n-grams, would ROUGE-F1 be low despite high quality?

- Concept: **LLM-as-judge decomposition strategies**
  - Why needed here: EMS uses separate LLM calls for extraction, matching, and scoring—reducing task complexity per call compared to end-to-end evaluation.
  - Quick check question: Why might three simple LLM calls outperform one complex evaluation prompt?

- Concept: **Soft vs. binary evaluation signals for optimization**
  - Why needed here: Continuous scores enable gradient-like feedback for prompt/model iteration; binary signals only indicate pass/fail without directionality.
  - Quick check question: If a candidate matches 7/10 reference points partially (0.6 each) vs. 6/10 perfectly (1.0 each), which better reflects utility?

## Architecture Onboarding

- Component map:
  - Extractor LLM -> Matcher LLM -> Scorer (LLM or conventional) -> Aggregator

- Critical path: Reference + Candidate → Extract (both) → Match (ref points to answer points) → Score (matched pairs) → Aggregate metrics

- Design tradeoffs:
  - LLM scorer vs. conventional scorer: LLM provides semantic nuance but higher cost/latency; ROUGE/BERTScore faster but less sensitive to meaning.
  - Retain vs. deduplicate repetitions: Retaining catches redundancy issues; deduplicating simplifies matching.
  - Prompt specificity: Financial-domain prompts emphasize numerical precision; generic prompts may miss domain nuances.

- Failure signatures:
  - **Matching collapse**: Many reference points return -1 (no match) despite semantic overlap → check prompt examples, candidate point quality
  - **Score compression**: Most scores cluster near 0.5 → scorer lacks calibration; review in-context examples
  - **Extraction verbosity/over-splitting**: Points exceed 2 sentences or fragment facts → refine extraction prompt constraints

- First 3 experiments:
  1. **Baseline sanity check**: Run EMS on answers from models with known quality gap (e.g., 1B vs. 90B); confirm metrics show expected ordering.
  2. **Scorer ablation**: Compare EMS(LLM) vs. EMS(ROUGE) vs. EMS(BERTScore) on same answer set; quantify sensitivity-cost tradeoff.
  3. **Extraction prompt iteration**: Vary detail-retention instructions; measure impact on score variance and correlation with human judgment (if available).

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the evaluation accuracy of the EMS paradigm compare when using gold-standard answers curated by human financial professionals versus the LLM-generated silver answers used in this study?
  - Basis in paper: [explicit] The authors state in the Limitations section that "future studies should recruit financial professionals to develop the golden answer and validate the effectiveness of the proposed EMS evaluation."
  - Why unresolved: The current study relies on state-of-the-art LLMs to generate reference answers, which may introduce inherent biases or hallucinations that a human expert would not make, potentially skewing the evaluation baseline.
  - What evidence would resolve it: A study constructing a new dataset with human-verified reference answers and comparing the correlation of EMS scores against human judgment on this dataset versus the LLM-generated one.

- **Open Question 2**: Can the EMS framework be effectively adapted to assess long-form answers that require complex mathematical reasoning or quantitative derivation?
  - Basis in paper: [explicit] The Limitations section notes that the study "did not dive into the mathematical reasoning, which is crucial in financial NLP," focusing primarily on qualitative inference from transcripts.
  - Why unresolved: The current saliency point extraction is designed for semantic claims; it is unclear if the "Match" and "Score" stages can sufficiently handle numerical errors or correct derivations without specific numerical-logic modules.
  - What evidence would resolve it: Applying the EMS pipeline to financial datasets requiring numerical reasoning (e.g., FinQA) and evaluating its ability to penalize calculation errors compared to semantic mismatches.

- **Open Question 3**: How can the EMS framework be extended to systematically evaluate logical consistency and factuality in addition to semantic similarity?
  - Basis in paper: [explicit] The authors explicitly list in the Limitations that "future work will develop a more systematic and multifaceted approach to consider additional factors such as logical consistency and factuality."
  - Why unresolved: The current scoring mechanism focuses on the alignment of details (Completeness, Accuracy relative to reference, Level of Detail) but lacks an explicit module to detect hallucinations or internal logical contradictions within the generated answer.
  - What evidence would resolve it: An extension of the scoring stage to include a factuality verification step (e.g., checking claims against the provided context) and testing this extension on datasets known for containing hallucinations.

## Limitations
- The evaluation dataset is small (5 questions across 10 companies), limiting generalizability
- Reliance on LLM-based components introduces potential variability in scoring consistency across versions/prompts
- Financial-domain specificity of prompts may not transfer to other domains
- Paper doesn't report inter-annotator agreement or human baseline comparisons for point-level scoring

## Confidence
- **High confidence**: The decomposition mechanism (Mechanism 1) and the formal structure of EMS metrics (Equations 6-9) are well-specified and reproducible. The baseline comparison showing EMS sensitivity to model scaling is clearly demonstrated.
- **Medium confidence**: The soft scoring approach (Mechanism 2) improves granularity over binary scoring, but the specific 0-1 calibration and its stability across domains needs more validation.
- **Medium confidence**: The claim that EMS better captures model quality scaling (Mechanism 3) is supported by the presented results, but the small dataset size limits the strength of this conclusion.

## Next Checks
1. **Robustness across domains**: Apply EMS to non-financial long-form QA datasets (e.g., healthcare or legal domains) to test whether the decomposition-matching-scoring approach generalizes beyond earnings call transcripts.

2. **Human evaluation correlation**: Conduct a small-scale human judgment study where annotators rate answer quality and compare these ratings against EMS scores to validate that the framework captures human notions of quality, not just model size trends.

3. **Ablation on point granularity**: Systematically vary the saliency point extraction parameters (e.g., max 1 sentence vs. max 3 sentences per point) and measure impact on EMS metric sensitivity and stability to identify optimal granularity for different answer types.