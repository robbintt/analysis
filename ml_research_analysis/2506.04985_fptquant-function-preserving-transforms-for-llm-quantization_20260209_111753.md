---
ver: rpa2
title: 'FPTQuant: Function-Preserving Transforms for LLM Quantization'
arxiv_id: '2506.04985'
source_url: https://arxiv.org/abs/2506.04985
tags:
- quantization
- fptquant
- arxiv
- transforms
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FPTQuant introduces four novel function-preserving transforms to
  enable aggressive LLM quantization by addressing outlier-induced performance degradation.
  The transforms include pre-RoPE scaling for queries/keys, per-head invertible scaling
  for values, per-channel scaling within MLP blocks, and dynamic residual normalization.
---

# FPTQuant: Function-Preserving Transforms for LLM Quantization

## Quick Facts
- **arXiv ID**: 2506.04985
- **Source URL**: https://arxiv.org/abs/2506.04985
- **Reference count**: 40
- **Primary result**: Achieves up to 3.9× speedup over FP16 with static INT4 quantization while maintaining competitive accuracy

## Executive Summary
FPTQuant introduces four novel function-preserving transforms to enable aggressive LLM quantization by addressing outlier-induced performance degradation. The transforms include pre-RoPE scaling for queries/keys, per-head invertible scaling for values, per-channel scaling within MLP blocks, and dynamic residual normalization. These transforms leverage transformer equivariances to maintain model function while reshaping activation distributions for better quantization. FPTQuant achieves up to 3.9× speedup over FP16 with static INT4 quantization, outperforming most prior methods in accuracy-speed tradeoff.

## Method Summary
FPTQuant applies four function-preserving transforms (FPTs) to reshape activation distributions for quantization: (1) Pre-RoPE scaled 2×2 rotations per key head merged into Wq/Wk, (2) per-head invertible matrices Tv merged into Wv/Wo, (3) per-channel scaling Tu merged into Wu/Wd, and (4) dynamic residual normalization Sn from RMSNorm. The method uses a two-stage optimization: local L4 norm minimization to set transforms, then end-to-end student-teacher training with JSD loss to optimize quantized student outputs against FP16 teacher. Mergeable transforms have zero inference cost while online transforms (S_n, T_d) provide per-token adaptivity at minimal overhead.

## Key Results
- Achieves up to 3.9× speedup over FP16 with static INT4 quantization
- Outperforms most prior methods in accuracy-speed tradeoff
- Shows competitive or better perplexity and zero-shot reasoning accuracy compared to baselines
- Up to 29% faster than FlatQuant while maintaining similar accuracy
- Requires no custom kernels and adds negligible inference overhead

## Why This Works (Mechanism)

### Mechanism 1: Pre-RoPE Equivariance Enables Mergeable Query/Key Transforms
RoPE applies block-diagonal 2×2 rotations per position. Scaled 2×2 block-diagonal rotations T_k applied pre-RoPE commute with RoPE because rotations sharing the same axes commute. Since T̄_k·T_k^T = I, the attention dot product is preserved. The transforms can be merged into projection weights. If positional encoding is additive (e.g., ALiBi) or uses full-rank mixing, commutation fails.

### Mechanism 2: Value-Head Independence Permits Full Invertible Transforms
The attention BMM sums over sequence dimension per sample, head, and token. Feature dimension d is independent of this reduction, so per-head invertible transforms Tv commute with BMM. For grouped-query attention, Tv^(h) is repeated across m query heads sharing one value head. If W_o has a bias depending on head dimension, or if cross-head mixing is introduced, per-head independence is violated.

### Mechanism 3: Residual Normalization Reduces Token-Scale Variance
Standard transformers leave residuals unnormalized, so tokens with large norms dominate when added to sub-block outputs. FPTQuant computes Sn = 1/||X_n||_RMS from residual X_n, scales both residual and sub-block output. This ensures Z̃_n = X̃_n + Ỹ_n has unit RMS. The scale factors are computed recursively via Sn = S_{n-1} ⊘ ||Z̃_{n-1}||_RMS. If the LM head does not start with normalization, the final rescaling cannot be absorbed.

## Foundational Learning

- **Concept: Rotary Position Embedding (RoPE)**
  - Why needed: Pre-RoPE transform Tk must commute with RoPE's block-diagonal rotation matrices
  - Quick check: Given a RoPE embedding f(x, i) = x·R_Θ,i where R_Θ,i is block-diagonal with 2×2 rotation blocks, why does a full-rank rotation matrix M not generally satisfy M·R_Θ,i = R_Theta,i·M?

- **Concept: Quantization Range-Precision Tradeoff**
  - Why needed: Outliers force a choice between clipping (losing outlier information) or expanding range (losing precision on dense regions)
  - Quick check: If activations have mean 0, std 1, but 0.1% of values are at magnitude 100, what quantization grid minimizes MSE for 4-bit integers, and what is the precision loss for the 99.9% of inliers?

- **Concept: Matrix Equivariance and Commutation**
  - Why needed: FPTQuant exploits operations that commute to merge transforms into weights
  - Quick check: For diagonal M, prove (A⊙B)·M = (A·M) ⊙ (B·M). For non-diagonal M, give a 2×2 counterexample.

## Architecture Onboarding

- **Component map**: Tk, T̄k → Wq, Wk (pre-RoPE); Tv, T̄v → Wv, Wo; Tu, T̄u → Wu, Wd; Sn → residuals, attention softmax outputs, SwiGLU outputs; Tr → merged into all linear weights; Td → online Hadamard at down-projection input

- **Critical path**: 1. Initialize all transforms; 2. Local optimization: Minimize Lp norm of merged weights sequentially; 3. Set quantization grid via L3 minimization on calibration data; 4. End-to-end student-teacher training: Minimize JSD between quantized and FP outputs; 5. Merge all mergeable transforms into weights; deploy with only online ops

- **Design tradeoffs**: Mergeable vs online (zero cost vs per-token adaptivity); Per-head vs shared (more params = better outlier shaping but risk of overfitting); Static vs dynamic (hardware support vs adaptivity)

- **Failure signatures**: Tk applied post-RoPE or with non-block-diagonal structure → attention scores change; Tv not correctly repeated for grouped-query attention → shape mismatch; Sn not applied to both residual and sub-block output → output divergence; End-to-end training with next-token loss instead of student-teacher → overfitting; Forgetting to absorb final Sn into LM head's RMSNorm → extra division op

- **First 3 experiments**: 1. Single-quantizer ablation: Replicate table 8 on your target model to identify most problematic activation locations; 2. Transform isolation test: Apply only Tv with W4A4KV4 on small model (e.g., Llama-3.2-3B); 3. Speed-accuracy sweep: Measure prefill latency for static INT4 on single transformer block across batch sizes 1 and 16

## Open Questions the Paper Calls Out

- **Generalization to non-Llama architectures**: It cannot be guaranteed that insights and gains equally translate to all LLMs, as evaluation was limited to Llama generations. The reliance on RoPE-specific properties may not hold for models using ALiBi or learned embeddings.

- **Automated FPT selection**: Choosing which FPTs to use is largely dependent on model, quantization setting, and resource constraints. The paper lacks an automated mechanism to search the combinatorial space of transform placements.

- **Sub-4-bit quantization limits**: FPTQuant favors mergeable transforms for speed but slightly underperforms slower baselines in aggressive INT4 settings. It's unclear if this accuracy gap widens significantly at INT2/INT3 where outlier handling requires more capacity.

## Limitations

- **Parameter scaling**: Per-head invertible transforms have H×d² parameters, scaling quadratically with hidden dimension and becoming prohibitive for very large models.

- **Dynamic quantization constraints**: Dynamic residual normalization requires runtime RMSNorm computation and isn't supported on many hardware accelerators.

- **Block-wise Hadamard grouping**: For non-power-of-2 dimensions, the grouping strategy isn't fully specified, potentially affecting transform expressivity.

## Confidence

- **High confidence**: Pre-RoPE scaling mechanism and its commutation properties are well-proven mathematically with explicit theorems and proofs.
- **Medium confidence**: Per-head invertible transforms show strong empirical results, but exact parameterization isn't fully specified.
- **Low confidence**: Dynamic residual normalization is the most novel component with the most assumptions about commutation properties.

## Next Checks

1. **Parameter efficiency test**: Implement per-head invertible transforms on a large model (d≥4096) and measure actual parameter count vs baseline to determine practical limits.

2. **Static alternative for S_n**: Develop a static approximation of dynamic residual normalization that can be merged into weights and test if it provides most of the benefit while maintaining hardware compatibility.

3. **Calibration sensitivity analysis**: Systematically vary the number and quality of calibration sequences and measure impact on both perplexity and zero-shot reasoning accuracy, including domain-shifted calibration data.