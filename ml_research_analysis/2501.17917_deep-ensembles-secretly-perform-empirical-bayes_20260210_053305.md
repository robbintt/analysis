---
ver: rpa2
title: Deep Ensembles Secretly Perform Empirical Bayes
arxiv_id: '2501.17917'
source_url: https://arxiv.org/abs/2501.17917
tags:
- ensembles
- deep
- learning
- posterior
- prior
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper shows that deep ensembles perform exact Bayesian averaging
  with a posterior obtained via an implicitly learned data-dependent prior. The authors
  establish that deep ensembles can be interpreted as an empirical Bayes procedure,
  where the prior is learned from the data through maximum marginal likelihood.
---

# Deep Ensembles Secretly Perform Empirical Bayes

## Quick Facts
- arXiv ID: 2501.17917
- Source URL: https://arxiv.org/abs/2501.17917
- Authors: Gabriel Loaiza-Ganem; Valentin Villecroze; Yixin Wang
- Reference count: 7
- Key outcome: Deep ensembles perform exact Bayesian averaging with an implicitly learned data-dependent prior

## Executive Summary
This paper reveals that deep ensembles, a popular ensemble-based approach for uncertainty quantification in deep learning, can be interpreted as an empirical Bayes procedure. The authors demonstrate that deep ensembles perform exact Bayesian posterior averaging, where the prior is implicitly learned from the data through maximum marginal likelihood. This interpretation bridges the gap between deep ensembles and Bayesian neural networks, showing they are more closely related than previously thought. The analysis reveals that the implicitly learned prior is a mixture of point masses, which helps explain observed phenomena about ensembles.

## Method Summary
The paper establishes a theoretical framework showing that deep ensembles perform exact Bayesian averaging with a posterior obtained via an implicitly learned data-dependent prior. The authors analyze the maximum marginal likelihood procedure used in deep ensembles and demonstrate how it implicitly learns a prior from the data. This prior is shown to be a mixture of point masses, which explains the ensemble's ability to capture uncertainty. The framework connects deep ensembles to empirical Bayes methods and provides a principled justification for their strong empirical performance.

## Key Results
- Deep ensembles perform exact Bayesian averaging with a posterior obtained via an implicitly learned data-dependent prior
- The implicitly learned prior is given by a mixture of point masses
- Deep ensembles are the only Bayesian neural networks that perform exact posterior averaging
- The empirical Bayes interpretation provides a principled justification for deep ensembles' strong empirical performance

## Why This Works (Mechanism)
Deep ensembles work by implicitly learning a data-dependent prior through maximum marginal likelihood optimization. When training multiple networks with different random initializations, the optimization process implicitly performs Bayesian model selection, identifying the prior that best explains the observed data. The ensemble then performs exact Bayesian averaging over this learned prior, providing principled uncertainty estimates. This mechanism explains why deep ensembles often outperform other Bayesian approaches - they are effectively performing exact Bayesian inference with a learned prior rather than relying on approximate methods.

## Foundational Learning
- **Bayesian Neural Networks**: Why needed - to understand the connection between deep ensembles and Bayesian methods. Quick check - can you explain the difference between exact and approximate Bayesian inference?
- **Empirical Bayes**: Why needed - to understand how priors can be learned from data rather than specified a priori. Quick check - what is the relationship between marginal likelihood and prior learning?
- **Maximum Marginal Likelihood**: Why needed - to understand the optimization objective used in deep ensembles. Quick check - how does maximizing marginal likelihood relate to model selection?
- **Mixture Models**: Why needed - to understand the structure of the implicitly learned prior. Quick check - what are the advantages of using mixture models for prior representation?

## Architecture Onboarding
**Component Map:**
Data -> Maximum Marginal Likelihood Optimization -> Implicit Prior Learning -> Ensemble Training -> Bayesian Averaging

**Critical Path:**
The critical path involves data-driven prior learning through marginal likelihood optimization, followed by ensemble training and Bayesian averaging. The prior learning step is crucial as it determines the quality of the final uncertainty estimates.

**Design Tradeoffs:**
- Computational cost vs. uncertainty quality (more ensemble members = better uncertainty but higher cost)
- Prior flexibility vs. overfitting risk (more complex priors can better fit data but may overfit)
- Exact Bayesian inference vs. computational feasibility (deep ensembles provide exact inference while remaining practical)

**Failure Signatures:**
- Poor uncertainty calibration when data is limited or unrepresentative
- Overconfident predictions when the implicit prior is too narrow
- Underestimated uncertainty when ensemble members are too similar

**First Experiments:**
1. Train deep ensembles on synthetic data with known ground truth to verify exact Bayesian averaging
2. Compare uncertainty estimates from deep ensembles against ground truth Bayesian inference on simple problems
3. Analyze the learned prior structure by examining the distribution of ensemble parameters

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical analysis makes simplifying assumptions that may not hold in practice
- Exact Bayesian interpretation relies on specific conditions that could break down with common implementation choices like weight decay or batch normalization
- Analysis focuses primarily on regression tasks, leaving generalization to classification unclear
- The claim that deep ensembles are the "only Bayesian neural networks" performing exact posterior averaging is particularly strong and may be vulnerable to edge cases

## Confidence
- High confidence in the empirical Bayes interpretation and its mathematical foundation
- Medium confidence in the exact posterior averaging claim for practical deep ensembles
- Medium confidence in the mixture-of-point-masses characterization of the learned prior
- Low confidence in full generalizability across all deep ensemble implementations

## Next Checks
1. Test the theoretical framework against practical deep ensemble implementations with common regularization techniques like weight decay and dropout
2. Validate the exact posterior averaging claim across different architectures, particularly transformers and convolutional networks
3. Extend the analysis to classification tasks and compare with alternative Bayesian neural network approaches like variational inference and Markov Chain Monte Carlo methods