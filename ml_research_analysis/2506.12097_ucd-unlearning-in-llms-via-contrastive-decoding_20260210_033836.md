---
ver: rpa2
title: 'UCD: Unlearning in LLMs via Contrastive Decoding'
arxiv_id: '2506.12097'
source_url: https://arxiv.org/abs/2506.12097
tags:
- unlearning
- forget
- arxiv
- utility
- tofu
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces UCD, an inference-time unlearning algorithm\
  \ for large language models that uses contrastive decoding to remove specific information\
  \ while preserving overall performance. The method leverages two smaller auxiliary\
  \ models\u2014one trained on the forget set and one on the retain set\u2014to guide\
  \ the outputs of the original model by adjusting token logits based on their difference."
---

# UCD: Unlearning in LLMs via Contrastive Decoding

## Quick Facts
- arXiv ID: 2506.12097
- Source URL: https://arxiv.org/abs/2506.12097
- Authors: Vinith M. Suriyakumar; Ayush Sekhari; Ashia Wilson
- Reference count: 40
- Primary result: UCD achieves state-of-the-art tradeoff between forget quality and model utility using inference-time contrastive decoding with auxiliary models

## Executive Summary
UCD introduces an inference-time unlearning algorithm that removes specific information from large language models while preserving overall performance. The method uses two smaller auxiliary models—one trained on the forget set and one on the retain set—to guide the outputs of the original model by adjusting token logits based on their difference. Evaluated on TOFU and MUSE benchmarks, UCD significantly improves the tradeoff between forget quality and model utility, achieving results indistinguishable from retraining on forget tasks while maintaining or improving utility. Notably, UCD scales efficiently to large models like Llama2-70B, requiring minimal compute compared to existing methods.

## Method Summary
UCD applies contrastive decoding to unlearning by computing the difference between log-probabilities from two auxiliary models: one trained on the forget set (A_corr) and one trained on the retain set (A_clean). This difference (ΔA) captures which tokens should be suppressed or boosted. The reference model's logits are then adjusted by subtracting α·ΔA from the reference model's logits, reducing the probability of forget-associated tokens while boosting retain-associated tokens. The method operates at inference time without modifying model weights, requiring only fine-tuning of small auxiliary models and careful selection of the α parameter.

## Key Results
- UCD achieves indistinguishability from retrained models on TOFU benchmark forget tasks while improving utility on retain sets
- On Llama2-70B, UCD closely approximates forget performance of retrained models using only 13B auxiliary models
- UCD outperforms state-of-the-art unlearning methods (DI, LAS, OPC) across all tested configurations and benchmarks
- The method scales efficiently, requiring minimal compute compared to existing weight-based unlearning approaches

## Why This Works (Mechanism)

### Mechanism 1: Contrastive Signal Captures Forget-Specific Token Preferences
The logit difference between forget-tuned and retain-tuned auxiliary models encodes which tokens the corrupted model should suppress or boost. When ΔA is large and positive, the forget-tuned model prefers token y more strongly than the retain-tuned model—indicating y is associated with information to unlearn. The update subtracts α·ΔA from the reference model's logits, reducing the probability of forget-associated tokens. Conversely, negative ΔA boosts retain-associated tokens.

### Mechanism 2: Inference-Time Logit Modification Avoids Weight Optimization Pathologies
Modifying logits at inference time circumvents optimization challenges inherent to weight-based unlearning (local minima, over-unlearning, catastrophic forgetting). UCD operates in token-space, not parameter-space. No gradients are computed through the reference model. This eliminates the need for careful learning rate scheduling, multi-step optimization, or balancing multiple loss terms.

### Mechanism 3: Cross-Scale Transfer via Proportional Preference Assumption
Small auxiliary models can guide large reference models because preference directions (not magnitudes) transfer across scales. Under the proportional assumption, if log A_corr - log A_clean = m(log P_corr - log P_clean) for some constant m, then choosing α = 1/m exactly recovers P_clean. In practice, approximate proportionality yields approximate alignment, allowing 7B auxiliary models to guide 70B reference models.

## Foundational Learning

- **Concept: Contrastive Decoding**
  - Why needed: UCD directly applies contrastive decoding (Li et al., 2023) to unlearning. Without understanding the base technique, the adaptation is opaque.
  - Quick check: Given two models' log probabilities for a token, how would you compute a contrastive adjusted logit?

- **Concept: Log-Likelihood Ratio / Log-Odds**
  - Why needed: The core operation is computing differences in log-probabilities (log-odds differences). This is fundamental to interpreting ΔA.
  - Quick check: If model A assigns log-prob -2.0 to a token and model B assigns -3.0, what is ΔA and which model prefers the token more?

- **Concept: Fine-tuning vs. Inference-Time Intervention**
  - Why needed: UCD fundamentally differs from prior unlearning by avoiding weight updates. Understanding this distinction clarifies tradeoffs (no retraining cost, but inference overhead).
  - Quick check: What are the computational implications of an inference-time method versus a fine-tuning method for a model serving 1000 queries/day?

## Architecture Onboarding

- **Component map:**
  Reference Model (P_corr) -> Logit Combination -> Softmax -> Token Sampling
  Forget Auxiliary (A_corr) → ↑
  Retain Auxiliary (A_clean) → α × ΔA subtraction

- **Critical path:**
  1. Verify availability of clean pretraining data (without D_forget) or acceptable approximate clean model
  2. Fine-tune A_corr on D_forget (typically 1-3 epochs, standard LR)
  3. Fine-tune A_clean on D_retain (same hyperparameters for comparability)
  4. Validate tokenization match between all three models
  5. Tune α on a held-out validation split measuring forget quality vs. utility tradeoff
  6. Deploy inference wrapper that computes adjusted logits per token

- **Design tradeoffs:**
  - UCD vs. UCS: Use UCD when clean auxiliary models are available (allows both suppression and boosting). Use UCS (clips negative ΔA) when bootstrapping from approximate clean models (more conservative, prevents over-correction).
  - Auxiliary model size: Smaller auxiliaries (7B for 70B reference) reduce compute but may weaken preference signal. Paper shows 13B auxiliaries improve utility vs. 7B on 70B reference.
  - α selection: Higher α (0.5-1.0) improves forget quality but risks utility loss; lower α (0.01-0.1) is more conservative.

- **Failure signatures:**
  - Over-unlearning: Model utility drops significantly on retain set → α too high or D_forget/D_retain not separable
  - Under-unlearning: Forget quality remains high → α too low or auxiliary models undertrained
  - Tokenization mismatch: Garbled outputs or degraded performance → verify identical tokenizers
  - OOM on inference: Running three models simultaneously → batch size reduction or model parallelism across auxiliaries

- **First 3 experiments:**
  1. Validate auxiliary model training: Confirm A_corr achieves high loss on D_retain and A_clean achieves high loss on D_forget (sanity check that they learned distinct distributions).
  2. Alpha sweep on validation split: For α ∈ {0.01, 0.1, 0.5, 1.0}, plot forget quality vs. model utility. Identify Pareto frontier.
  3. Ablate auxiliary model size: Compare using same-family 7B vs. 13B auxiliaries for a 13B reference model. Measure impact on forget-utility tradeoff to validate cross-scale transfer hypothesis.

## Open Questions the Paper Calls Out

None

## Limitations

- **Token-level generalization uncertainty:** The assumption that token-level preference patterns transfer proportionally from small auxiliary models to large reference models remains theoretically unproven.
- **Representational vs. output-level unlearning:** UCD operates at the output distribution level rather than modifying internal representations, which may be insufficient when downstream systems query model representations directly.
- **Resource overhead considerations:** Although UCD avoids retraining costs, it requires running three models simultaneously during inference, which could be substantial for production systems with high query volumes.

## Confidence

- **High confidence:** The core mechanism of contrastive decoding applied to unlearning is well-established, and UCD's empirical results on TOFU and MUSE benchmarks are robust and reproducible.
- **Medium confidence:** The cross-scale transfer assumption showing that small auxiliary models can guide large reference models is empirically validated but lacks theoretical guarantees.
- **Low confidence:** The long-term implications of inference-time unlearning on model representations remain unknown without analysis of whether forget-associated concepts persist in hidden states.

## Next Checks

1. **Representation-level analysis:** Apply probing techniques to compare hidden state representations of UCD-processed outputs versus the retrained model. Specifically, check if forget-associated concepts remain detectable in intermediate layers despite being suppressed at the output level.

2. **Cross-domain transferability test:** Evaluate UCD on datasets and model architectures outside the TOFU/MUSE/Llama2 family (e.g., GPT-3.5/4, FLAN-T5, or code models). Measure whether the cross-scale transfer assumption holds when moving beyond the tested configuration.

3. **Computational overhead quantification:** Benchmark UCD's inference-time cost (latency, memory, throughput) against weight-based unlearning methods at scale (10K+ queries). Compare total cost of ownership including auxiliary model storage and serving infrastructure.