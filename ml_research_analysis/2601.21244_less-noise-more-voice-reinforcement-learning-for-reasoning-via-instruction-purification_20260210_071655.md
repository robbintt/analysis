---
ver: rpa2
title: 'Less Noise, More Voice: Reinforcement Learning for Reasoning via Instruction
  Purification'
arxiv_id: '2601.21244'
source_url: https://arxiv.org/abs/2601.21244
tags:
- arxiv
- grpo
- training
- interference
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses reinforcement learning with verifiable rewards
  (RLVR) in large language models (LLMs) for reasoning tasks, focusing on the challenge
  of inefficient exploration due to sparse rewards and interference tokens in prompts.
  The proposed Less Noise Sampling Framework (LENS) identifies and removes interference
  tokens that hinder successful rollouts, then transfers successful samples from purified
  prompts to guide policy optimization on the original noisy prompts.
---

# Less Noise, More Voice: Reinforcement Learning for Reasoning via Instruction Purification

## Quick Facts
- arXiv ID: 2601.21244
- Source URL: https://arxiv.org/abs/2601.21244
- Reference count: 21
- Key outcome: LENS outperforms standard GRPO by 3.88% on average and achieves over 1.6× speedup in convergence across seven math reasoning benchmarks.

## Executive Summary
The paper addresses reinforcement learning with verifiable rewards (RLVR) in large language models (LLMs) for reasoning tasks, focusing on the challenge of inefficient exploration due to sparse rewards and interference tokens in prompts. The proposed Less Noise Sampling Framework (LENS) identifies and removes interference tokens that hinder successful rollouts, then transfers successful samples from purified prompts to guide policy optimization on the original noisy prompts. This approach enables the model to learn to ignore interference and improve reasoning robustness. Experimental results demonstrate that LENS outperforms standard GRPO by 3.88% on average and achieves over 1.6× speedup in convergence across seven math reasoning benchmarks, while also outperforming both scaling exploration and prompt filtering baselines.

## Method Summary
LENS introduces a framework that identifies interference tokens in prompts by measuring the KL divergence between the current policy and a reference model. The framework then creates purified prompts by removing the most interfering tokens, generates successful rollouts from these cleaned prompts, and uses them to guide policy optimization on the original noisy prompts through a calibrated rollout policy optimization (CRPO) mechanism. The key insight is that many exploration failures stem from a small subset of tokens rather than the inherent difficulty of the reasoning task.

## Key Results
- LENS outperforms standard GRPO by 3.88% on average across seven math reasoning benchmarks
- Achieves over 1.6× speedup in convergence compared to baseline methods
- Identifies that fewer than 5% of tokens cause the majority of exploration failures
- Demonstrates effectiveness on models ranging from 3B to 8B parameters

## Why This Works (Mechanism)

### Mechanism 1: Token-Level Interference Identification
The paper identifies interference tokens using an Interference Score defined as the absolute difference between log-probabilities of the current policy and reference model. Tokens with high divergence are flagged as sources of noise that destabilize rollouts. This works because deviations from the reference model on input prompts primarily indicate noise rather than superior reasoning strategies.

### Mechanism 2: Signal Extraction via Prompt Purification
By removing identified interference tokens (typically top 1-5%), the framework increases the probability of generating successful reasoning chains for previously failing prompts. The assumption is that interference tokens are semantically redundant or distracting, and their removal preserves core problem logic while eliminating exploration bottlenecks.

### Mechanism 3: Cross-Distribution Policy Calibration (CRPO)
CRPO trains the model on original noisy prompts while using successful rollouts from purified prompts as supervision targets. The optimization applies loss to the original prompt but uses purified rollout rewards, facilitated by importance sampling ratios. This enables learning robust reasoning that handles interference while benefiting from cleaner examples.

## Foundational Learning

- **Concept: Reinforcement Learning with Verifiable Rewards (RLVR)**
  - Why needed here: LENS specifically addresses exploration bottlenecks in RLVR where sparse binary rewards lead to zero-variance gradients
  - Quick check question: In a group of 8 rollouts where all are incorrect, what is the advantage value, and how does LENS attempt to fix this?

- **Concept: KL Divergence in Policy Optimization**
  - Why needed here: The interference detection mechanism relies on measuring divergence between current policy and frozen reference model
  - Quick check question: Why might a high KL divergence on a prompt token suggest it is "noisy" or causing "over-optimization"?

- **Concept: Importance Sampling**
  - Why needed here: CRPO uses rollouts from modified prompts to update the policy on original prompts, requiring probability ratio correction
  - Quick check question: Why is the ratio $\rho(y; \theta) = \pi_\theta(y|x_i) / \pi_{\text{old}}(y|x_{\text{roll}(y)})$ necessary in the objective function?

## Architecture Onboarding

- **Component map:** Interference Scorer -> Purifier -> Rollout Buffer -> CRPO Selector -> Optimizer
- **Critical path:** The Purification Benefit Check acts as a safety rail, only activating transfer if purified prompt yields higher success rate than original
- **Design tradeoffs:** Deletion ratio γ typically 1-5%, with weaker models (3B) requiring higher thresholds than stronger models (7B/8B)
- **Failure signatures:** Semantic drift from removing critical constraints, collapse to filtered training distribution, computational overhead of 27-62% per step
- **First 3 experiments:** 1) Replicate zero-reward ratio analysis comparing LENS to vanilla GRPO, 2) Visualize pruned tokens for 20 random samples to verify human intuition of "noise," 3) Run threshold sensitivity sweep on γ∈{0.01, 0.03, 0.05}

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the LENS framework maintain its efficiency and performance advantages when scaled to Large Language Models significantly larger than 8B parameters (e.g., 32B or 70B)?
- Basis in paper: The authors explicitly state in the Limitations section that "Evaluating the performance and scalability of our method on larger-scale models (e.g., 32B or 70B) remains an avenue for future research" due to computational resource constraints.
- Why unresolved: All experimental results were conducted on models with 3B to 8B parameters; it is unknown if the "Interference Score" behaves similarly or if computational overhead remains acceptable for larger models.
- What evidence would resolve it: Empirical results comparing LENS against GRPO baselines on 32B or 70B models across standard math reasoning benchmarks.

### Open Question 2
- Question: Can the Interference Token Identification method be effectively adapted for tasks involving multi-dimensional scoring or non-binary rewards?
- Basis in paper: The paper notes in the Limitations section that the method "has been validated primarily in tasks with binary rewards" and that "Its applicability to more complex environments... requires further investigation."
- Why unresolved: The current framework relies on binary success/failure signals to identify tokens causing rollouts to fail; complex reward landscapes may obscure the correlation between specific tokens and reward outcomes.
- What evidence would resolve it: Successful application of LENS to RLHF tasks or code generation with partial credit scoring, demonstrating improved sample efficiency over standard methods.

### Open Question 3
- Question: Can LENS be successfully integrated with other GRPO variants (such as those optimizing rollout frequency or reward shaping) to yield compound benefits?
- Basis in paper: The authors list "Algorithmic Integration" as a limitation, stating: "We have not yet explored its integration with other GRPO-based variants... potentially enhancing exploration capabilities and training stability."
- Why unresolved: LENS was tested as a modification to vanilla GRPO; its interaction with orthogonal improvements like dynamic rollout allocation or advantage reshaping is unknown.
- What evidence would resolve it: Experiments combining the LENS purification mechanism with algorithms like DAPO or GRESO to measure convergence speed and final accuracy.

## Limitations

- **Semantic validity uncertainty**: While the Benefit Check filters cases where purification destroys problem semantics, there's no guarantee remaining pruned prompts are semantically equivalent to originals
- **Reference model dependence**: The Interference Score relies on comparing to a frozen reference model that may be poorly calibrated or biased
- **Generalization concerns**: Risk of models learning to depend on cleaner distributions rather than truly handling diverse noise patterns during inference
- **Computational overhead**: Introduces 27-62% per-step overhead due to additional rollouts and interference scoring

## Confidence

- **High Confidence**: Interference token identification using KL divergence is well-founded and empirically supported by heavy-tailed score distribution and improved success rates
- **Medium Confidence**: Transferring successful rollouts from purified to original prompts via importance sampling is theoretically sound but practically dependent on semantic similarity
- **Low Confidence**: 1.6× speedup claim is based on comparison with GRPO but lacks ablation studies isolating LENS component effects or total compute budget analysis

## Next Checks

1. **Semantic Equivalence Validation**: For 50 random pruned prompts, conduct human evaluation to verify semantic content preservation after removing top-γ% interference tokens, flagging cases where critical constraints are inadvertently removed.

2. **Robustness to Unseen Noise Patterns**: Create a benchmark set of prompts with artificial interference (redundant instructions, swapped numerical values) not seen during training, evaluating whether LENS-trained models maintain performance improvements over GRPO on these out-of-distribution noisy prompts.

3. **Component Ablation for Efficiency**: Run ablations where (a) interference scorer is replaced with random token deletion, (b) Benefit Check is disabled, and (c) importance sampling correction is removed, comparing both final performance and convergence speed to quantify contribution of each component.