---
ver: rpa2
title: 'DecompSR: A dataset for decomposed analyses of compositional multihop spatial
  reasoning'
arxiv_id: '2511.02627'
source_url: https://arxiv.org/abs/2511.02627
tags:
- reasoning
- right
- left
- role
- content
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DecompSR, a dataset and generation framework
  designed to probe compositional multihop spatial reasoning in large language models.
  It enables systematic control over parameters such as reasoning depth (k), linguistic
  variation, entity names, noise, and story ordering to independently evaluate productivity,
  systematicity, substitutivity, and overgeneralization.
---

# DecompSR: A dataset for decomposed analyses of compositional multihop spatial reasoning

## Quick Facts
- arXiv ID: 2511.02627
- Source URL: https://arxiv.org/abs/2511.02627
- Reference count: 40
- Large language models struggle with compositional multihop spatial reasoning, showing sharp accuracy drops as reasoning depth increases

## Executive Summary
This paper introduces DecompSR, a dataset and generation framework designed to systematically probe compositional multihop spatial reasoning in large language models. The dataset enables independent control over parameters like reasoning depth, linguistic variation, entity naming, noise, and story ordering to isolate specific compositional deficits. With over 5 million verified samples, DecompSR provides a rigorous tool for diagnosing productivity, systematicity, substitutivity, and overgeneralization in model reasoning abilities.

## Method Summary
The DecompSR dataset is generated through a random-walk construction on a 2D grid, ensuring each story requires exactly k relational inference steps with no loops. Each sample consists of a story (s), question (q), and answer (a) triple. The generation framework allows systematic variation of linguistic templates, entity names, noise injection, and sentence ordering. Dataset correctness is verified using a symbolic solver (clingo) that translates natural language to Answer Set Programming and derives deterministic answers. Models are evaluated using 0-shot and 5-shot In-Context Learning with specific prompt templates, measuring accuracy across different compositional dimensions.

## Key Results
- Models show robust performance to linguistic variation and entity naming but accuracy drops sharply as reasoning depth (k) increases
- Near-chance performance on nonsense language beyond k=1 indicates fundamental systematicity limitations
- Models struggle with productivity, showing poor scaling of reasoning accuracy with increasing complexity

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Independent parameter variation isolates specific compositional deficits in LLMs.
- **Mechanism:** The generation framework produces ⟨s, q, a⟩ triples where story complexity (k), language, entity names, noise, and ordering are orthogonal control variables. By holding all but one constant, performance differences can be attributed to the varied factor.
- **Core assumption:** LLMs process these factors somewhat independently; confounding is minimal.
- **Evidence anchors:**
  - [abstract] "independently vary several aspects of compositionality, namely: productivity... substitutivity... overgeneralisation... systematicity"
  - [Section 3] "Our framework allows for systematically varying elements of the ⟨s, q, a⟩ triple through several precisely controlled parameters, each designed to target different aspects of reasoning"
  - [corpus] Related work (Compositional-ARC, CryptoX) similarly uses controlled compositional splits but in visual and cryptographic domains; cross-domain consistency suggests the approach generalizes.
- **Break condition:** If parameters interact non-linearly (e.g., noise effects depend on k), attributing failure to a single factor becomes invalid.

### Mechanism 2
- **Claim:** Random-walk construction ensures k equals the true minimal inference chain length.
- **Mechanism:** A random walk on a 2D grid visits k unique nodes without revisits. Each step yields one relational sentence. Since no loops exist, the minimal path between start and end nodes is exactly k, forcing models to chain all k relations.
- **Core assumption:** Models cannot exploit shortcuts like pattern matching on surface cues when the structure enforces full traversal.
- **Evidence anchors:**
  - [Section 3] "This guarantees that the k-hop reasoning depth specified for s is exactly equal to the minimal inferential steps needed to link the entities in q and obtain a"
  - [Section 2.2] Notes prior datasets had loops that "obscure true reasoning depth"
  - [corpus] Weak direct evidence; StepGame benchmark (cited) uses similar k-hop structure but the loop-free guarantee is novel here.
- **Break condition:** If models learn to guess based on statistical regularities in the template phrasing rather than reasoning, k is no longer a measure of reasoning depth.

### Mechanism 3
- **Claim:** ASP-based symbolic verification guarantees dataset correctness.
- **Mechanism:** An oracle translates natural language stories to Answer Set Programming facts. A predefined spatial reasoning knowledge module is appended, and the clingo solver derives the answer deterministically. This reverse-engineers generation to confirm logical soundness.
- **Core assumption:** The oracle translation is faithful to the natural language semantics.
- **Evidence anchors:**
  - [Section 3] "This oracle-based ASP approach effectively reverse-engineers the DecompSR generation process, confirming that each problem instance is logically sound and has a unique, deducible answer"
  - [Table 3] Oracle+ASP achieves 1.00 accuracy across all k values, confirming dataset validity
  - [corpus] No direct corpus evidence for this specific ASP approach in reasoning benchmarks.
- **Break condition:** If the oracle's natural language parser fails on edge cases, some samples may be incorrectly validated.

## Foundational Learning

- **Compositional Generalization Framework (Hupkes et al. 2020)**
  - Why needed: The paper directly builds on this taxonomy (productivity, systematicity, substitutivity, overgeneralization) to design experiments.
  - Quick check: Can you explain why testing productivity (scaling complexity) differs from testing systematicity (novel combinations of known primitives)?

- **Spatial Relation Representation**
  - Why needed: Understanding how cardinal/intercardinal directions map to grid coordinates is essential for interpreting the task structure.
  - Quick check: Given "A is upper-left of B" and "B is right of C," what is A's relation to C?

- **Answer Set Programming Basics**
  - Why needed: The verification pipeline and the LLM+ASP experiment require understanding declarative logic programming and stable model semantics.
  - Quick check: What does it mean for a logic program to have a "stable model"?

## Architecture Onboarding

- **Component map:** Grid generator -> Random walk (k steps) -> Template instantiator (language, entity names) -> Noise/shuffle injector -> ASP translator -> clingo solver (verification)
- **Critical path:** 1. Define k and random seed 2. Generate loop-free walk 3. Apply language template and entity naming scheme 4. Optionally inject distractors or shuffle sentence order 5. Verify via ASP solver before releasing sample
- **Design tradeoffs:** Larger k -> more diagnostic for productivity but higher inference cost and near-chance model performance (k≥20); Shuffled stories -> better probe order-invariance but may increase task difficulty beyond compositional limits; Nonsense language -> pure systematicity test but unfamiliar to pre-trained models, risking floor effects
- **Failure signatures:** Sharp accuracy drop from k=1 to k=2 (observed in Table 4) suggests single-hop pattern matching, not chaining; Near-guess performance on nonsense language at k≥2 (Table 1) indicates lack of systematic binding; Accuracy degradation with shuffled or noisy stories (Figure 3) reveals overgeneralization from ordered/clean training priors
- **First 3 experiments:** 1. Run 5-shot baseline on k=1,2,5,10 with default settings to establish your model's productivity profile; 2. Test substitutivity by swapping symbolic node names for nonce names at fixed k to measure entity-level generalization; 3. Introduce distractors at k=5 to assess noise robustness; compare shuffled vs. ordered stories to diagnose order-dependent heuristics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the collapse in accuracy when using nonsense vocabulary (nonce words) stem from a failure of the specific in-context familiarisation method or an inherent lack of systematicity in LLMs?
- Basis in paper: [explicit] Section 4.2 shows models collapse to guess rates for $k \ge 2$ with nonce words.
- Why unresolved: The paper tests specific prompt engineering but does not isolate whether the familiarisation strategy was insufficient or if the models fundamentally cannot map novel symbols to spatial relations.
- What evidence would resolve it: Ablation studies varying familiarisation strategies (e.g., direct definition vs. examples) or fine-tuning experiments on the nonce vocabulary.

### Open Question 2
- Question: Is the observed degradation in reasoning accuracy at higher reasoning depths ($k$) primarily caused by the model's inability to reliably translate natural language into an internal formal representation?
- Basis in paper: [inferred] Section 4.6 shows translation accuracy drops at high $k$ and posits that "If models can't reliably translate stories, then they are perhaps unlikely to accurately reason about stories."
- Why unresolved: The paper provides correlational evidence (translation fails, reasoning fails) but does not establish a causal link between the translation bottleneck and the final reasoning error.
- What evidence would resolve it: Experiments intervening on the translation step (e.g., providing ground-truth intermediate representations) to see if reasoning accuracy recovers.

### Open Question 3
- Question: Can specific architectural modifications or training regimes enable LLMs to maintain robust spatial reasoning accuracy as the reasoning depth ($k$) scales beyond 10 hops?
- Basis in paper: [explicit] The text states, "should one wish to... develop a model particularly apt at productivity, the DecompSR dataset allows one to generate stories for arbitrary $k$."
- Why unresolved: Current models approach guess rates at high $k$; the paper identifies the gap but does not propose a solution for scaling productivity.
- What evidence would resolve it: Training or fine-tuning models specifically on DecompSR data with increasing $k$ values and measuring performance retention.

## Limitations
- Performance degradation may reflect training data priors rather than pure reasoning deficits, as models trained on ordered/clean text struggle with shuffled/noisy variants
- The symbolic solver verification depends on the fidelity of the natural language parser, creating potential blind spots for edge cases
- The framework primarily tests spatial reasoning and may not generalize to other compositional domains

## Confidence
- **High**: The dataset construction methodology and ASP verification pipeline are well-documented and demonstrably correct (oracle achieves 1.00 accuracy)
- **Medium**: The experimental findings about productivity and systematicity limitations, as these could be influenced by training data distribution effects not controlled for
- **Medium**: The claims about independent parameter variation isolating specific deficits, as parameter interactions are acknowledged but not thoroughly explored

## Next Checks
1. Test whether fine-tuning on shuffled/noisy stories improves performance on held-out compositional splits, distinguishing true reasoning deficits from distributional bias
2. Evaluate the same models on CryptoX and Compositional-ARC to determine if spatial reasoning deficits generalize across compositional domains
3. Analyze model attention patterns during k-hop reasoning to verify whether failures stem from inability to chain relations versus surface-level pattern matching