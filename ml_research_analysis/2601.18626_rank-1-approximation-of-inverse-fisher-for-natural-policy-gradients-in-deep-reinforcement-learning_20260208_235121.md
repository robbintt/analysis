---
ver: rpa2
title: Rank-1 Approximation of Inverse Fisher for Natural Policy Gradients in Deep
  Reinforcement Learning
arxiv_id: '2601.18626'
source_url: https://arxiv.org/abs/2601.18626
tags:
- policy
- learning
- gradient
- smac
- natural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents a scalable approach to computing natural gradients\
  \ in deep reinforcement learning by leveraging a rank-1 approximation of the inverse\
  \ Fisher Information Matrix (FIM) using the Sherman-Morrison formula. The method\
  \ avoids the prohibitive computational cost of explicit FIM inversion, achieving\
  \ O(d) complexity instead of O(d\xB3), while maintaining convergence guarantees\
  \ under certain conditions."
---

# Rank-1 Approximation of Inverse Fisher for Natural Policy Gradients in Deep Reinforcement Learning

## Quick Facts
- arXiv ID: 2601.18626
- Source URL: https://arxiv.org/abs/2601.18626
- Authors: Yingxiao Huo; Satya Prakash Dash; Radu Stoican; Samuel Kaski; Mingfei Sun
- Reference count: 40
- One-line primary result: Sherman-Morrison-based rank-1 Fisher approximation achieves O(d) complexity and competitive performance on MuJoCo tasks

## Executive Summary
This paper presents SMAC (SM-ActorCritic), a scalable natural gradient method for deep reinforcement learning that approximates the inverse Fisher Information Matrix using a rank-1 decomposition via the Sherman-Morrison formula. The approach reduces computational complexity from O(d³) to O(d) while maintaining convergence guarantees for log-linear function approximators. Experiments on classic control and MuJoCo environments demonstrate competitive performance against standard actor-critic methods and trust-region baselines, with particular strength in continuous control tasks.

## Method Summary
SMAC uses a rank-1 approximation of the empirical Fisher Information Matrix computed as Ẑk = λI + ∇θ log π(a|s)∇θ log π(a|s)^T, where λ is a damping coefficient. The Sherman-Morrison formula provides a closed-form inverse: (λI + gg^T)^(-1) = (1/λ)I - gg^T / (λ² + λ‖g‖²), enabling O(d) computation instead of O(d³) matrix inversion. The method processes batches of 1000 samples, averaging gradients before applying the update, and uses GAE for advantage estimation. Theoretical analysis proves global convergence for log-linear policies, while empirical results show competitive performance on MuJoCo tasks.

## Key Results
- Achieves O(d) computational complexity versus O(d³) for exact Fisher inversion
- Reduces optimization time by ~80% using batch-mean gradient aggregation (B=1000)
- Outperforms standard actor-critic methods and achieves competitive performance against trust-region baselines on HalfCheetah, Hopper, and Swimmer
- Demonstrates faster convergence and superior sample efficiency in several environments
- Shows improved performance and stability across multiple seeds in complex continuous control tasks

## Why This Works (Mechanism)

### Mechanism 1: Sherman-Morrison Decomposition for O(d) Inverse Computation
The Sherman-Morrison formula transforms rank-1 matrix inversion into three vector operations. For damped empirical Fisher Ẑk = λI + gg^T, the inverse becomes (1/λ)I - gg^T / (λ² + λ‖g‖²). This reduces complexity from O(d³) to O(d) while maintaining invertibility when λ > 0. Break condition: Numerical instability occurs if λ is too small and gradient norms become very large.

### Mechanism 2: Single-Sample Empirical Fisher as Curvature Proxy
The method uses Ẑk = λI + g_k g_k^T where g_k = ∇θ log π(a|s)A(s,a) is a single-sample policy gradient. The λI term regularizes the rank-1 estimate, ensuring positive definiteness. Break condition: Single-sample curvature estimates may misguide updates in high-variance environments like Pendulum.

### Mechanism 3: Batch-Mean Aggregation for Compute-Performance Tradeoff
Averaging log-probability gradients over 1000 samples reduces optimization time by ~80% while preserving performance. The rank-1 structure is maintained with the mean gradient direction. Break condition: If batch gradient norms become very small, preconditioning effect diminishes and updates approach vanilla SGD with 1/λ scaling.

## Foundational Learning

- Concept: **Natural Gradient Descent**
  - Why needed here: Amari's insight that standard gradients don't account for parameter space geometry motivates the entire approach
  - Quick check question: If changing θ₁ by 0.1 changes the policy distribution much more than changing θ₂ by 0.1, which direction should a natural gradient favor relative to standard gradient?

- Concept: **Fisher Information Matrix**
  - Why needed here: The FIM measures curvature of the KL-divergence surface; understanding its O(d²) storage and O(d³) inversion costs motivates the approximation strategy
  - Quick check question: Why does the FIM have O(d²) storage and O(d³) inversion costs for a policy with d parameters?

- Concept: **Sherman-Morrison Formula**
  - Why needed here: This is the core mathematical tool enabling the O(d) complexity reduction
  - Quick check question: If A is diagonal (λI) and you know A^(-1) = (1/λ)I, how many operations are needed to compute (A + uv^T)^(-1)x for a vector x?

## Architecture Onboarding

- Component map:
  Policy Network (π_θ) -> log π(a|s) -> ∇_θ log π -> g_k (policy gradient with advantage)
  -> Ẑk = λI + ∇logπ·∇logπ^T -> θ_update via Sherman-Morrison
  Value Network (V_φ) <- MSE loss <- GAE <- Advantage estimates
  -> Adam update (standard)

- Critical path:
  1. Environment rollout collects T transitions
  2. GAE computes advantage estimates A_t
  3. For each sample: compute ∇θ log π(a|s), apply equation 5 for natural gradient
  4. Critic updated via standard Adam on MSE loss
  5. Hyperparameter sensitivity is highest for λ (damping) and η (step size)

- Design tradeoffs:
  | Choice | Benefit | Cost |
  |--------|---------|------|
  | Single-sample vs batch-mean | More frequent curvature updates | ~5x more computation time |
  | Small λ | Stronger preconditioning | Numerical instability |
  | Large λ | Stable, approaches vanilla SGD | Weak preconditioning |
  | Large batch B | Lower compute overhead | May smooth out useful curvature |

- Failure signatures:
  - Exploding variance mid-training: λ may be too small; increase damping
  - Convergence to sub-optimal policy: rank-1 approximation may be insufficient
  - Updates barely moving: λ is likely too large; 1/λ scaling dominates
  - NaN/Inf in update: gradient norm exploding; check advantage function scale

- First 3 experiments:
  1. **Damping sweep on HalfCheetah**: Grid search λ ∈ {0.01, 0.1, 1.0, 10.0} with fixed η to establish sensitivity baseline
  2. **Single-sample vs batch-mean ablation**: Reproduce Figure 1 results on Reacher or Ant to validate implementation
  3. **Gradient norm monitoring**: Log ‖∇logπ‖² at each update alongside returns to correlate stability with performance

## Open Questions the Paper Calls Out

- **Can global convergence guarantees be extended beyond log-linear function approximators to general deep neural network policies?**
  - Basis: Theorem 1 establishes convergence "when using log-linear function approximators" but experiments use deep networks
  - Why unresolved: Proof relies on specific properties of log-linear policies that may not hold for deep networks
  - Evidence needed: Convergence proof for neural network policies or analysis of bounded approximation error scaling

- **How does SMAC compare to Kronecker-factored approximation (K-FAC) methods?**
  - Basis: K-FAC is cited as "widely adopted" but experiments omit this baseline
  - Why unresolved: Computational-accuracy trade-off between rank-1 and Kronecker-factorized approximations remains uncharacterized
  - Evidence needed: Head-to-head benchmarks comparing wall-clock time, sample efficiency, and final returns

- **Why does SMAC underperform in Pendulum and Walker, and can these failure modes be predicted?**
  - Basis: "In Pendulum, our method fails to outperform AC-CG... indicating that our proposed approximation of the FIM can struggle"
  - Why unresolved: Authors note failure but provide no characterization of when rank-1 approximations are insufficient
  - Evidence needed: Analysis correlating environment properties with relative performance gap

## Limitations

- The rank-1 approximation cannot capture full matrix curvature, leading to performance degradation in certain environments
- Empirical Fisher may deviate significantly from true Fisher, especially in high-variance settings
- Damping coefficient selection appears highly problem-dependent, requiring careful tuning

## Confidence

- **Theoretical Convergence (High):** Sherman-Morrison decomposition and O(d) complexity are mathematically rigorous
- **Empirical Performance (Medium):** Results on HalfCheetah, Hopper, and Swimmer are compelling, but underperformance in Pendulum and high variance in Walker2d limit generalization claims
- **Computational Claims (High):** O(d) complexity and ~80% time reduction are directly verifiable from the formula application

## Next Checks

1. **Robustness sweep across 10+ diverse environments:** Test SMAC on Atari, DM Control Suite, and Procgen to assess generalization beyond MuJoCo and classic control
2. **Adaptive damping implementation:** Implement scheme to increase λ when update norms exceed threshold to test stability without manual tuning
3. **Hybrid curvature estimation:** Combine rank-1 updates with periodic full Fisher-vector products (every K updates) to test whether occasional full curvature computation improves performance in difficult environments