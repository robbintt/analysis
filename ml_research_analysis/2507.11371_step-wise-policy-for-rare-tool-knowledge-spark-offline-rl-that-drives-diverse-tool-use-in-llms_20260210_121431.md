---
ver: rpa2
title: 'Step-wise Policy for Rare-tool Knowledge (SPaRK): Offline RL that Drives Diverse
  Tool Use in LLMs'
arxiv_id: '2507.11371'
source_url: https://arxiv.org/abs/2507.11371
tags:
- tool
- exploration
- reasoning
- learning
- tools
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SPaRK addresses the problem of limited exploration in tool usage
  by large language models, which often rely on high-temperature sampling rather than
  systematic exploration of available tools. The method introduces a dual-objective
  reward system that optimizes for both answer quality and tool diversity through
  offline Proximal Policy Optimization (PPO) on synthetically generated trajectories.
---

# Step-wise Policy for Rare-tool Knowledge (SPaRK): Offline RL that Drives Diverse Tool Use in LLMs

## Quick Facts
- **arXiv ID:** 2507.11371
- **Source URL:** https://arxiv.org/abs/2507.11371
- **Reference count:** 5
- **Primary result:** SPaRK achieves 40.8% accuracy on MMLU-Pro, representing a 7.8 percentage point improvement over standard PPO and nearly doubling the performance of supervised fine-tuning at 26.2%.

## Executive Summary
SPaRK addresses the problem of limited exploration in tool usage by large language models, which often rely on high-temperature sampling rather than systematic exploration of available tools. The method introduces a dual-objective reward system that optimizes for both answer quality and tool diversity through offline Proximal Policy Optimization (PPO) on synthetically generated trajectories. SPaRK uniquely employs a rarity-first exploitation strategy where the model selects the least-used tool among viable options, encouraging systematic exploration. The primary result shows that SPaRK achieves 40.8% accuracy on MMLU-Pro, representing a 7.8 percentage point improvement over standard PPO and nearly doubling the performance of supervised fine-tuning at 26.2%. The approach demonstrates significantly higher entropy in tool selection, suggesting that algorithmic exploration through explicit tool diversity can enhance reasoning capabilities without sacrificing accuracy.

## Method Summary
SPaRK trains a Llama-3.1 8B model through offline PPO on synthetically generated trajectories to improve multi-step reasoning with tool usage. The method employs a rarity-first exploitation strategy where a GPT-4o judge scores candidate actions (8 tools + CoT) on a 0-10 scale, and the policy selects the lowest-scoring viable tool above a quality threshold. The dual-objective reward combines tool diversity (gap between best and chosen scores) with answer quality (binary process_ok flag). The model uses LoRA adapters for efficient fine-tuning and achieves improved accuracy while maintaining higher tool selection entropy compared to standard PPO approaches.

## Key Results
- SPaRK achieves 40.8% accuracy on MMLU-Pro test set, improving 7.8 percentage points over standard PPO
- Tool selection entropy is significantly higher in SPaRK compared to baseline PPO, indicating more diverse tool usage
- Performance nearly doubles that of supervised fine-tuning (26.2%) on the same task
- The rarity-first exploitation strategy successfully encourages exploration while maintaining answer quality

## Why This Works (Mechanism)

### Mechanism 1: Rarity-First Exploitation via Threshold-Gated Selection
Selecting the lowest-scoring viable tool forces systematic exploration while maintaining answer quality. A GPT-4o judge scores 9 candidate actions on a 0-10 scale, and among actions scoring above a quality threshold (6.0), the policy selects the minimum-scoring option. This explicitly counters the default behavior of always selecting the highest-scoring action. If the threshold is set too low (5.0), trajectories contain incorrect reasoning steps, degrading final accuracy.

### Mechanism 2: Dual-Objective Composite Reward
The per-step reward combines tool diversity and answer quality: R_t = ρ × (Best Score − Chosen Score) − (1 − ρ) × (1 − process_ok). The first term rewards choosing rare tools, while the second term penalizes incoherent reasoning steps. The hyperparameter ρ controls the exploration-exploitation tradeoff. If ρ is too high, the model over-explores and selects poor-quality tools; if too low, it collapses to the highest-scoring tool.

### Mechanism 3: Offline PPO with Step-wise KL-Regularized Policy Updates
Synthetic trajectories are generated offline via GPT-4o-judged rollouts, and the PPO objective is augmented with a squared KL penalty. The critic estimates state values, and advantages are computed as R_t − V_old(s_t). LoRA adapters enable efficient fine-tuning of the 8B-parameter actor and critic networks. If KL divergence exceeds the target threshold (0.2) or the synthetic data lacks coverage, the policy may overfit to spurious patterns.

## Foundational Learning

- **Concept: Markov Decision Processes (MDPs) for Language Reasoning**
  - Why needed here: SPaRK frames multi-step tool-augmented reasoning as an MDP where each step is a (state, action, reward, next_state) tuple. Understanding state representations, action spaces, and reward design is essential.
  - Quick check question: Can you sketch the MDP tuple for a single reasoning step in SPaRK, including what constitutes the state and action?

- **Concept: Proximal Policy Optimization (PPO) Mechanics**
  - Why needed here: The core training loop uses PPO's clipped surrogate objective with KL regularization. Understanding the clipping mechanism (ε = 0.2), advantage estimation, and why offline PPO differs from online rollouts is critical.
  - Quick check question: Why does PPO use a clipped surrogate objective, and what role does the KL penalty play in SPaRK's implementation?

- **Concept: Exploration vs. Exploitation in Policy Learning**
  - Why needed here: SPaRK's central innovation is explicitly rewarding exploration alongside exploitation. Understanding how the dual-objective reward decouples and recombines these goals is foundational.
  - Quick check question: How does SPaRK's "minimum-score-above-threshold" rule differ from standard ε-greedy exploration or temperature-based sampling?

## Architecture Onboarding

- **Component map:** Synthetic Data Generator -> Actor Network (LoRA-adapted Llama-3.1 8B) -> Critic Network (LoRA-adapted with MLP value head) -> PPO Trainer (clipped surrogate + KL penalty) -> Reward Model (composite function)

- **Critical path:** Generate synthetic trajectories → filter by process_ok and correctness → initialize actor and critic with LoRA adapters → for each batch compute log-probabilities, value estimates, advantages → update actor (PPO loss) and critic (MSE loss) → monitor KL divergence → evaluate on held-out MMLU-Pro test set

- **Design tradeoffs:** Threshold (6.0) vs. exploration breadth; ρ parameter controls diversity vs. quality balance; offline vs. online PPO (offline is simpler but cannot adapt dynamically); LoRA rank (8) vs. full fine-tuning (efficient but may limit capacity)

- **Failure signatures:** KL divergence explosion indicates policy drift; CoT dominance if threshold is too high; low accuracy despite high diversity suggests ρ is too high or threshold too low; critic underfitting if MSE loss on value estimates remains high

- **First 3 experiments:** 1) Reproduce baseline ladder: train baseline → SFT → PPO (no diversity) → SPaRK; verify accuracy progression 22.4% → 26.2% → 33.0% → 40.8%. 2) Threshold ablation: rerun synthetic data generation with thresholds 5.0, 6.0, 7.0; measure trajectory correctness and final accuracy. 3) Tool usage entropy analysis: compute entropy of tool selection distributions for SPaRK vs. PPO (no diversity); confirm SPaRK has significantly higher entropy.

## Open Questions the Paper Calls Out
- Does the performance gain stem primarily from the learned exploration policy or merely from the availability of tools? The authors note the "comparison lacks a critical baseline: the base model augmented with tool access but without specialized training."
- Can online reinforcement learning yield further improvements over the offline approach used in this study? The discussion states that "online PPO training... could yield further improvements by allowing the policy to adapt dynamically."
- How sensitive is the SPaRK framework to the specific judge model used for scoring candidate actions? The authors list the need to "investigate the impact of different judge models beyond GPT-4o" as a priority for future work.

## Limitations
- The exact composition of the 8 tool set and their API specifications remains unspecified, creating ambiguity in reproducing the MDP state-action space.
- The ρ hyperparameter value in the dual-objective reward is not reported, making it unclear how the diversity-quality tradeoff was tuned.
- The process_ok binary classifier prompt is also unspecified, potentially affecting reward signal quality.

## Confidence
- **High confidence** in the overall methodology and reported accuracy improvement (40.8% on MMLU-Pro, +7.8% over baseline PPO)
- **Medium confidence** in the mechanism of rarity-first selection, as the threshold selection (6.0) appears empirically determined without systematic ablation
- **Medium confidence** in the claim that dual-objective reward improves generalization, as the diversity-accuracy correlation is demonstrated but causal pathways remain untested

## Next Checks
1. Implement a systematic ablation study varying the reward threshold (5.0, 6.0, 7.0) and ρ parameter to identify optimal settings and sensitivity
2. Measure tool selection entropy distributions across training epochs to confirm that the policy maintains high diversity without collapsing to default tools
3. Test the policy's robustness to distribution shift by evaluating on held-out MMLU-Pro categories not seen during training, comparing SPaRK's generalization against PPO baseline