---
ver: rpa2
title: 'FedPref: Federated Learning Across Heterogeneous Multi-objective Preferences'
arxiv_id: '2501.13604'
source_url: https://arxiv.org/abs/2501.13604
tags:
- clients
- algorithm
- learning
- federated
- fedpref
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FedPref, a federated learning algorithm designed
  for heterogeneous multi-objective preferences. The core challenge is that clients
  have multiple objectives with different preference weights, leading to potentially
  conflicting models.
---

# FedPref: Federated Learning Across Heterogeneous Multi-objective Preferences

## Quick Facts
- **arXiv ID:** 2501.13604
- **Source URL:** https://arxiv.org/abs/2501.13604
- **Reference count:** 40
- **Primary result:** Federated learning algorithm for heterogeneous multi-objective preferences using recursive clustering and weighted aggregation based on modified cosine similarity

## Executive Summary
FedPref addresses the challenge of federated learning when clients have different multi-objective preferences, which can lead to conflicting model updates. The algorithm combines recursive clustering and weighted aggregation to group clients with similar preferences and dynamically adapt aggregation weights. Experiments across five diverse environments demonstrate FedPref outperforms baselines like FedAvg, FedProx, CFL, and MaTFL in terms of average client reward while achieving high solution diversity and convergence.

## Method Summary
FedPref implements a federated learning framework where clients train on scalarized multi-objective reinforcement learning tasks. The server performs weighted aggregation based on a modified cosine similarity metric that uses a `topR` operator to select the most impactful weights per layer. When cluster-mean model updates fall below a threshold, spectral clustering splits the cluster. The algorithm maintains cluster assignments and mean models, computing pairwise similarities to create personalized models for each client through weighted averaging of peer models.

## Key Results
- Outperforms baselines (FedAvg, FedProx, CFL, MaTFL) in average client reward across five environments
- Achieves high solution diversity and convergence as measured by hypervolume and sparsity metrics
- Demonstrates robustness across different preference distributions and local model architectures
- Shows sensitivity to hyperparameter tuning, particularly `topR` ratio and minimum similarity thresholds

## Why This Works (Mechanism)

### Mechanism 1: Similarity-Based Preference Alignment
If clients have similar objective preferences, their model updates will move in similar directions, allowing for safe aggregation. FedPref calculates modified cosine similarity between client model updates and the cluster mean, using a `topR` operator to select only the top R fraction of weights (by absolute value) per layer before computing similarity. This filtering attempts to mitigate the "curse of dimensionality" in large models. The core assumption is that the direction of the gradient update is a reliable proxy for the client's underlying preference vector, and the "most important" weights (largest absolute values) best capture this direction.

### Mechanism 2: Recursive Bisection for Conflict Resolution
When a cluster's mean model stops improving (converges), it implies that the clients within the cluster have conflicting gradients, requiring a split. The server monitors the magnitude of change in the cluster-mean model. If this change falls below a threshold ε, the algorithm triggers spectral clustering to bipartition the cluster based on the similarity matrix. The core assumption is that stagnation of the cluster mean is caused by gradient conflict rather than noise or local optima convergence.

### Mechanism 3: Personalized Weighted Aggregation
Aggregating models based on pairwise similarity creates personalized models that outperform a single global "compromise" model. Instead of a uniform average (FedAvg), clients receive a weighted average of peer models. The weight of peer j in client i's update is proportional to their similarity, clipped by a minimum threshold. The core assumption is that positive similarity implies helpful knowledge transfer, while low/negative similarity implies harmful interference.

## Foundational Learning

- **Concept: Multi-Objective Scalarization**
  - Why needed here: FedPref clients do not train on a vector of objectives directly; they convert them to a single loss function using preference weights. Understanding this explains why gradients diverge in the first place.
  - Quick check question: If Client A has weights [0.9, 0.1] and Client B has [0.1, 0.9], will they minimize the same loss landscape?

- **Concept: Curse of Dimensionality in Cosine Similarity**
  - Why needed here: The authors explicitly modify standard cosine similarity using `topR` because high-dimensional vectors (neural network weights) tend to appear orthogonal or dissimilar by chance.
  - Quick check question: Why does comparing every single weight in a large layer make it hard to distinguish "similar" models from "random" models?

- **Concept: Spectral Clustering**
  - Why needed here: This is the specific algorithm used to split clusters. It relies on graph theory (eigenvalues of the similarity matrix) rather than distance centroids like K-Means.
  - Quick check question: Why might spectral clustering be better for splitting a "crescent-shaped" distribution of model updates than K-Means?

## Architecture Onboarding

- **Component map:** Client Update (Scalarized loss → Local SGD → Model Update Δθ) → Server State (Cluster assignments C, Cluster-Mean models θ̄C) → Similarity Engine (topR modified cosine similarity matrix S) → Trigger Logic (Monitors ||Δθ̄C|| against threshold ε) → Aggregator (Personalized models θi^new = Σŵij θj)

- **Critical path:** The convergence check (Trigger Logic) is the critical decision point. If it fails to fire, conflicting clients remain together, degrading performance. If it fires too often, clients are isolated, losing federated benefits.

- **Design tradeoffs:**
  - `topR` Ratio (R): Lower values (e.g., 0.2) sparsify the comparison, reducing noise but risking ignoring relevant gradient dimensions. The paper found 0.2–0.8 optimal depending on the environment.
  - Min Similarity Bound (smin): The paper suggests values < 0 (e.g., -0.4) are often better than 0, indicating that even "dissimilar" clients can collaborate beneficially within a cluster.

- **Failure signatures:**
  - Premature Convergence: Clusters split into singletons (individual clients) immediately.
  - Preference Collapse: All clients converge to the same model despite different preferences (likely `topR` is set too high or similarity metric is failing).
  - Stagnation: Cluster mean oscillates or fails to improve (threshold ε is too tight/loose).

- **First 3 experiments:**
  1. Metric Validation: Run FedPref on a toy environment (e.g., Deep-Sea Treasure) with R=1.0 (standard) vs. R=0.2 (sparse) to visualize the impact of the modified similarity metric on clustering quality.
  2. Ablation on `s_min`: Test the "Min Similarity Bound" parameter. The paper notes that negative thresholds often outperform zero; verify this by sweeping smin ∈ [-1.0, 0.4] on the MO-Lunar Lander environment.
  3. Stress Test: Train on the Deterministic Minecart environment (which the paper identifies as difficult due to sparse rewards) to identify if the clustering logic creates sub-optimal splits due to lack of gradient signal.

## Open Questions the Paper Calls Out
None

## Limitations
- The modified cosine similarity metric's robustness under extreme preference divergence is not fully validated
- The recursive clustering mechanism may trigger false positives in sparse reward environments
- Performance heavily depends on environment-specific hyperparameter tuning
- The paper does not explore edge cases where preferences are perfectly orthogonal

## Confidence
- **High confidence:** FedPref outperforms baselines in average client reward across all five tested environments; the algorithm's core structure (clustering + weighted aggregation) is well-defined.
- **Medium confidence:** The mechanism linking preference weights to gradient directions through the `topR` filtering is theoretically sound but not empirically validated for extreme cases.
- **Low confidence:** The claim that negative minimum similarity thresholds are universally better than zero requires more systematic validation across diverse preference distributions.

## Next Checks
1. Test FedPref on an environment with perfectly orthogonal preference weights (e.g., [1,0] vs [0,1]) to validate whether the `topR` metric can distinguish truly conflicting gradients.
2. Run ablation studies on the clustering threshold ε across environments to determine if the stagnation-based split criterion reliably separates conflicting clients.
3. Implement a variant where clients use different local model architectures to verify if the similarity metric remains meaningful across architectural heterogeneity.