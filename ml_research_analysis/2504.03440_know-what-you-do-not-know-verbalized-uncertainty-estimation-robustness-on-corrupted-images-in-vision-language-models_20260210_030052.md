---
ver: rpa2
title: 'Know What You do Not Know: Verbalized Uncertainty Estimation Robustness on
  Corrupted Images in Vision-Language Models'
arxiv_id: '2504.03440'
source_url: https://arxiv.org/abs/2504.03440
tags:
- confidence
- severity
- frequency
- noise
- claude
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examined verbalized uncertainty estimation in vision-language
  models under image corruption. Three state-of-the-art VLMs (GPT-4V, Gemini Pro Vision,
  and Claude 3 Opus) were tested on corrupted images using Gaussian noise, defocus
  blur, and JPEG compression at five severity levels.
---

# Keep What You do Not Know: Verbalized Uncertainty Estimation Robustness on Corrupted Images in Vision-Language Models

## Quick Facts
- arXiv ID: 2504.03440
- Source URL: https://arxiv.org/abs/2504.03440
- Reference count: 40
- Primary result: VLMs show overconfidence and poor calibration on corrupted images, with confidence scores frequently in the [80, 100] range despite reduced accuracy

## Executive Summary
This study examines verbalized uncertainty estimation in vision-language models when processing corrupted images. The researchers tested GPT-4V, Gemini Pro Vision, and Claude 3 Opus on visual question-answering tasks using Gaussian noise, defocus blur, and JPEG compression at five severity levels. All models showed decreased accuracy but maintained high confidence scores as corruption severity increased, resulting in significant miscalibration. GPT-4V demonstrated the best calibration performance with higher refusal rates to answer difficult questions. The findings indicate that current VLMs struggle with uncertainty estimation, particularly under image corruption, which undermines their reliability for high-stakes applications.

## Method Summary
The study evaluated three VLMs (GPT-4V, Gemini Pro Vision, and Claude 3 Opus) on corrupted images using standardized prompts requesting confidence scores alongside answers. The evaluation used three datasets: 36 easy VQA images, 29 hard VQA images from the JUS dataset, and 13 counting images. Images were corrupted using Gaussian noise, defocus blur, and JPEG compression at five severity levels each. Confidence scores were extracted from model responses and compared against manually verified accuracy to calculate Expected Calibration Error (ECE). The study focused on verbalized uncertainty as the only accessible method for proprietary VLMs.

## Key Results
- All three VLMs showed decreased accuracy but maintained high confidence scores as image corruption severity increased
- GPT-4V achieved the best calibration performance with higher refusal rates to answer difficult questions
- Models were particularly miscalibrated when asked to provide 95% confidence intervals for counting tasks
- Confidence scores frequently clustered in the [80, 100] range regardless of actual accuracy
- Increased corruption severity led to increased miscalibration, with ECE scores showing linear growth (R² > 0.5)

## Why This Works (Mechanism)

### Mechanism 1
Verbalized confidence scores serve as proxies for model uncertainty when internal token probabilities are inaccessible. The model outputs a numerical confidence estimate alongside its answer, externalizing its internal uncertainty representation into natural language. This is necessary because proprietary VLMs like GPT-4V don't expose token probabilities via API. The verbalized confidence bears systematic relationship to true epistemic uncertainty, though this relationship can be broken if RLHF rewards confident-sounding outputs.

### Mechanism 2
Image corruption severity degrades calibration by reducing accuracy faster than confidence. Corruptions obscure visual features, reducing the model's ability to extract task-relevant information. However, the confidence-generation mechanism does not receive commensurate signal degradation, producing overconfident wrong answers. This decoupling occurs because the vision encoder's degradation is not propagated to the language model's confidence estimation.

### Mechanism 3
Refusal to answer correlates with improved calibration by filtering unverifiable inputs. When models recognize extreme corruption, refusing to answer prevents confident incorrect outputs. This selective abstention improves aggregate calibration metrics by removing low-confidence/low-accuracy samples from evaluation. Refusal is triggered by uncertainty rather than unrelated factors, though this assumption can fail if corruption is misclassified as explicit content.

## Foundational Learning

- **Expected Calibration Error (ECE)**: Primary metric for quantifying miscalibration; measures weighted gap between confidence and accuracy across bins. Quick check: If a model outputs 80% confidence on average but only 60% of answers are correct, what is the approximate ECE?

- **Verbalized vs. Sampling-Based Uncertainty**: Proprietary VLMs expose only text outputs; understanding this distinction clarifies why verbalized methods are used despite limitations. Quick check: Why can't we directly access token probabilities from GPT-4V via API?

- **RLHF and Overconfidence**: Hypothesized cause of systematic overconfidence; RLHF rewards confident-sounding outputs, biasing verbalized estimates upward. Quick check: How might fine-tuning with human preferences affect a model's willingness to express uncertainty?

## Architecture Onboarding

- **Component map**: Images → corruption functions (Gaussian noise, defocus blur, JPEG compression) → 5 severity levels → VLM API calls (GPT-4V, Gemini, Claude) with standardized prompts → manual answer verification → accuracy/confidence extraction → ECE computation per bin

- **Critical path**: 1) Select dataset (VQA v2 for easy, JUS for hard/counting) 2) Apply corruption at severity 0-5 3) Query model with prompt template 4) Extract confidence from response (regex on "X%" or "[low, high]" intervals) 5) Manually verify correctness 6) Compute ECE, plot calibration curves

- **Design tradeoffs**: Small dataset (36 images easy, 29 hard, 13 counting) enables manual verification but limits statistical power; open-ended questions better reflect real usage but complicate automated evaluation; verbalized uncertainty is accessible but may be less reliable than sampling-based methods

- **Failure signatures**: Models outputting 95-100% confidence on clearly wrong answers → severe overconfidence; refusal rate spikes at severity 4-5 → model detecting impossibility; Gemini safety refusals on distorted images → corruption misclassified as explicit content

- **First 3 experiments**: 1) Replicate easy VQA with Gaussian noise only; verify ECE increases linearly with severity (R² > 0.5 expected) 2) Test a single image across all 15 corruption types from Michaelis et al. to identify corruption-specific calibration patterns 3) Compare verbalized confidence vs. consistency-based uncertainty (sample same query 5 times, measure answer variance) on a 10-image subset

## Open Questions the Paper Calls Out

- **Open Question 1**: Can advanced prompting strategies like chain-of-thought reasoning or top-k sampling improve the calibration of verbalized uncertainty in VLMs on corrupted images? The authors note they only tested "vanilla" prompting and different strategies could yield different results.

- **Open Question 2**: Can temperature scaling effectively mitigate the overconfidence of RLHF-tuned VLMs when processing corrupted inputs? The paper explicitly asks if this overconfidence could be treated with temperature scaling as in Kadavath et al. (2022).

- **Open Question 3**: How does VLM calibration degrade under weather-based or digital corruption types not tested in this study? The paper notes that Michaelis et al. (2019) defines 15 corruption types, but only three were tested, excluding corruptions like fog, frost, or snow.

## Limitations

- The evaluation dataset is relatively small (36 easy, 29 hard, 13 counting images), which may limit statistical power and generalizability across tasks
- Reliance on manual answer verification introduces potential human error and makes large-scale validation impractical
- Verbalized uncertainty may systematically differ from sampling-based uncertainty measures, potentially biasing calibration estimates
- The study does not investigate whether training-time corruption augmentations could improve calibration robustness

## Confidence

- **High Confidence**: The core finding that VLMs exhibit overconfidence under image corruption is well-supported by ECE degradation patterns and confidence-accuracy misalignment across all three models
- **Medium Confidence**: The claim that Gemini misclassifies corrupted images as explicit content causing spurious refusals is based on qualitative observations but lacks systematic quantification
- **Medium Confidence**: The suggestion that RLHF contributes to overconfidence is plausible but not directly tested; the mechanism remains speculative without ablation studies

## Next Checks

1. **Statistical Power Validation**: Replicate the experiment with a larger image set (minimum 100 easy questions) to verify that ECE degradation patterns hold with tighter confidence intervals

2. **Mechanism Isolation**: Test a subset of images with and without RLHF fine-tuning (where accessible) to determine if overconfidence persists in pre-RLHF models

3. **Corruption-Specific Analysis**: Examine whether certain corruption types (e.g., JPEG vs. Gaussian noise) show qualitatively different calibration patterns, potentially revealing architecture-specific failure modes