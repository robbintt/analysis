---
ver: rpa2
title: Knowledge Grafting of Large Language Models
arxiv_id: '2505.18502'
source_url: https://arxiv.org/abs/2505.18502
tags:
- arxiv
- knowledge
- learning
- fusion
- compression
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GraftLLM, a novel framework for cross-capability
  transfer in large language models (LLMs). The core idea is to store specialized
  knowledge from source models as compact, modular SkillPacks, which can be efficiently
  integrated into a target model.
---

# Knowledge Grafting of Large Language Models

## Quick Facts
- **arXiv ID:** 2505.18502
- **Source URL:** https://arxiv.org/abs/2505.18502
- **Reference count:** 40
- **Key outcome:** Introduces GraftLLM, a framework for cross-capability transfer in LLMs using modular SkillPacks to preserve target model capabilities while enabling efficient knowledge integration.

## Executive Summary
This paper presents GraftLLM, a novel framework designed to address the challenge of cross-capability transfer in large language models. Traditional approaches often struggle with parameter conflicts and loss of general capabilities when integrating specialized knowledge. GraftLLM overcomes these limitations by introducing SkillPacksâ€”compact, modular representations of specialized knowledge that can be efficiently integrated into a target model. The framework enables forget-free continual learning and model fusion while maintaining the target model's general capabilities. Experiments demonstrate significant performance improvements on benchmarks like MT-Bench and AlpacaEval 2.0, positioning GraftLLM as a scalable solution for heterogeneous model integration.

## Method Summary
GraftLLM introduces a modular approach to knowledge transfer by encapsulating specialized knowledge from source models into SkillPacks, which are compact, reusable units of expertise. These SkillPacks are designed to integrate seamlessly into target models without overwriting their general capabilities. The framework addresses common challenges such as parameter conflicts and catastrophic forgetting by preserving the target model's original structure while augmenting it with specialized knowledge. The integration process is optimized for efficiency, enabling both continual learning and model fusion. Experiments validate that GraftLLM outperforms existing methods in preserving general capabilities while achieving superior performance in knowledge transfer tasks.

## Key Results
- GraftLLM achieves significant performance gains on MT-Bench and AlpacaEval 2.0 benchmarks while maintaining parameter efficiency.
- The framework enables forget-free continual learning and effective model fusion without overwriting the target model's general capabilities.
- Experiments demonstrate that GraftLLM outperforms existing techniques in knowledge transfer, integration, and parameter efficiency.

## Why This Works (Mechanism)
GraftLLM's effectiveness stems from its modular SkillPack architecture, which isolates specialized knowledge from source models and integrates it into target models without disrupting their general capabilities. By encapsulating knowledge into compact, reusable units, the framework minimizes parameter conflicts and preserves the target model's original structure. This approach enables efficient cross-capability transfer, forget-free continual learning, and scalable model fusion. The modular design also reduces computational overhead during integration, making the framework both practical and efficient for real-world applications.

## Foundational Learning
- **SkillPacks:** Compact, modular representations of specialized knowledge. *Why needed:* To isolate and transfer expertise without disrupting the target model's general capabilities. *Quick check:* Verify that SkillPacks are smaller than the full source model and retain task-specific performance.
- **Cross-capability transfer:** The ability to integrate specialized knowledge into a model with different capabilities. *Why needed:* To enable knowledge sharing across heterogeneous models without overwriting general skills. *Quick check:* Test integration of a math-specialized SkillPack into a general-purpose model.
- **Forget-free continual learning:** Incremental updates without catastrophic forgetting. *Why needed:* To maintain existing knowledge while acquiring new skills over time. *Quick check:* Evaluate model performance on previous tasks after multiple SkillPack integrations.
- **Parameter conflict resolution:** Mechanisms to prevent overwriting of target model parameters. *Why needed:* To preserve the target model's general capabilities during knowledge integration. *Quick check:* Measure performance degradation on general tasks after SkillPack integration.

## Architecture Onboarding

**Component Map:** Source Model -> SkillPack Extraction -> SkillPack Storage -> Target Model Integration

**Critical Path:** The core workflow involves extracting specialized knowledge from a source model, encoding it into a SkillPack, storing it efficiently, and integrating it into the target model while preserving its general capabilities.

**Design Tradeoffs:** The modular SkillPack approach sacrifices some granularity in knowledge transfer for improved efficiency and reduced parameter conflicts. While this limits the ability to transfer highly interconnected knowledge, it enables scalable and practical integration across diverse models.

**Failure Signatures:** Potential failures include incomplete knowledge transfer if SkillPacks are too compact, integration conflicts if SkillPack structure mismatches target model architecture, and performance degradation if SkillPack extraction misses critical dependencies.

**3 First Experiments:**
1. Extract a SkillPack from a math-specialized model and integrate it into a general-purpose model, measuring performance on both math and general tasks.
2. Perform sequential SkillPack integrations to test forget-free continual learning capabilities.
3. Compare parameter efficiency and memory overhead of GraftLLM against baseline knowledge transfer methods.

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation scope is limited to general language benchmarks (MT-Bench, AlpacaEval 2.0), with unclear performance on highly specialized or domain-specific tasks.
- Parameter efficiency claims lack detailed ablation studies, memory overhead analysis, and computational cost comparisons.
- Long-term stability of continual learning is not thoroughly examined, with no analysis of performance degradation over extended periods or multiple incremental updates.
- The mechanism for SkillPack extraction, structuring, and optimization is not fully transparent, raising questions about generalizability and robustness.

## Confidence

**High confidence** in the core framework's design and its distinction from existing methods, as the modular SkillPack concept and integration strategy are clearly articulated and logically sound.

**Medium confidence** in the empirical results, given that performance improvements are demonstrated on established benchmarks, but the evaluation lacks depth in specialized domains and long-term stability analysis.

**Low confidence** in the parameter efficiency and scalability claims due to the absence of detailed ablation studies, memory overhead analysis, and cost-benefit comparisons with existing techniques.

## Next Checks
1. Conduct extensive ablation studies to quantify the memory overhead of SkillPacks and compare the computational costs of GraftLLM against baseline methods during both training and inference.
2. Evaluate GraftLLM on a diverse set of domain-specific benchmarks to assess its effectiveness in transferring specialized knowledge beyond general language capabilities.
3. Perform long-term continual learning experiments with repeated incremental updates to measure the framework's ability to prevent catastrophic forgetting over extended periods.