---
ver: rpa2
title: On the Feasibility of Hijacking MLLMs' Decision Chain via One Perturbation
arxiv_id: '2511.20002'
source_url: https://arxiv.org/abs/2511.20002
tags:
- perturbation
- semantic
- targets
- adversarial
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that a single universal perturbation can
  hijack multiple stateless decisions of multimodal large language models by acting
  as a "semantic router," directing inputs with distinct visual semantics toward predefined
  textual targets. The authors introduce the Semantic-Aware Universal Perturbation
  (SAUP) and develop the Semantic-Oriented (SORT) optimization algorithm, which leverages
  normalized space optimization and a semantic separation strategy to overcome geometric
  constraints in the latent space.
---

# On the Feasibility of Hijacking MLLMs' Decision Chain via One Perturbation

## Quick Facts
- **arXiv ID:** 2511.20002
- **Source URL:** https://arxiv.org/abs/2511.20002
- **Reference count:** 39
- **Key outcome:** A single universal perturbation can hijack multiple stateless decisions of multimodal large language models by acting as a "semantic router," directing inputs with distinct visual semantics toward predefined textual targets.

## Executive Summary
This paper demonstrates that a single universal perturbation can hijack multiple stateless decisions of multimodal large language models by acting as a "semantic router," directing inputs with distinct visual semantics toward predefined textual targets. The authors introduce the Semantic-Aware Universal Perturbation (SAUP) and develop the Semantic-Oriented (SORT) optimization algorithm, which leverages normalized space optimization and a semantic separation strategy to overcome geometric constraints in the latent space. To evaluate this threat, they create the RIST dataset with fine-grained semantic annotations for autonomous driving and robotics scenarios. Extensive experiments on three representative MLLMs (Llava, Qwen, and InternVL3) show that a single adversarial frame can achieve attack success rates of 93%, 77%, 61%, and 66% for controlling 2, 3, 4, and 5 distinct targets respectively, with performance varying based on semantic granularity and perturbation constraints. The work reveals a fundamental vulnerability in MLLMs' decision chains, showing that cascading errors can be triggered through a single perturbation that exploits semantic differences across inputs.

## Method Summary
The Semantic-Aware Universal Perturbation (SAUP) is crafted using the Semantic-Oriented (SORT) optimization algorithm. SORT employs Normalized Space Optimization (NSO) to optimize perturbations in unconstrained space, then maps them back to pixel space via an inverse transformation function. The algorithm also uses Semantic Separation Optimization (SSO) with a margin-based loss to ensure distinct semantic classes are routed to their respective targets. Training involves collecting images per semantic class (10-50 images per class), assigning distinct target text to each class, and optimizing a perturbation frame or corner region to maximize attack success rate while minimizing visual impact. The perturbation is evaluated across multiple MLLMs and semantic categories to demonstrate universal effectiveness.

## Key Results
- Single perturbation achieves 93% ASR for controlling 2 targets, 77% for 3 targets, 61% for 4 targets, and 66% for 5 targets on Qwen model
- Removing Normalized Space Optimization causes ASR to plummet by an average of 49.5% across test sets
- Perturbation region size is critical: frame (7,056 pixels) supports up to 5 targets while corner (1,600 pixels) fails beyond 7-8 targets
- RIST dataset enables evaluation with fine-grained semantic annotations for autonomous driving and robotics scenarios

## Why This Works (Mechanism)

### Mechanism 1: Dominant Shift in Latent Space
A single perturbation δ can shift all input embeddings away from their original manifolds toward a distant adversarial subspace, where decision boundaries are densely concentrated. The perturbation features become dominant over input content, establishing a new "anchor point" in latent space that overwhelms original semantic content.

### Mechanism 2: Semantic Deflection via Jacobian Amplification
The local Jacobian Jδ at the perturbation point extracts and amplifies subtle semantic differences between inputs, deflecting each toward distinct targets. Using first-order Taylor expansion, z(c) ≈ ϕ(δ) + Jδ·x(c), where Jδ serves as a semantic-sensitive linear projection that quantifies amplification capacity through its spectral norm.

### Mechanism 3: Normalized Space Optimization Enables Gradient Stability
Optimizing perturbations in normalized (unconstrained) space rather than bounded pixel space prevents gradient plateauing and enables convergence on complex multi-target mappings. By defining trainable variable Δ in normalized space and recovering δ via inverse transformation, the optimizer avoids boundary artifacts that would otherwise hinder finding feasible multi-target perturbations.

## Foundational Learning

- **Concept: Universal Adversarial Perturbations (UAPs)**
  - Why needed here: SAUP extends UAP from "many-to-one" (all inputs → single wrong label) to "many-to-many" (semantic routing). Understanding UAP's image-agnostic property is prerequisite.
  - Quick check question: Can you explain why a perturbation trained on ImageNet might transfer to unseen images from a different distribution?

- **Concept: First-Order Taylor Expansion for Adversarial Analysis**
  - Why needed here: The paper's theoretical framework relies on linearizing the vision encoder around δ to decompose effects into shift + deflection.
  - Quick check question: What does the Jacobian matrix Jδ = ∂ϕ(x)/∂x|_{x=δ} represent geometrically?

- **Concept: Stateless vs. Stateful Decision Systems**
  - Why needed here: The threat model specifically targets stateless MLLM deployments (each decision independent). This differs from chain-of-thought or conversational attacks.
  - Quick check question: Why would a stateless system be more vulnerable to single-perturbation attacks than a system maintaining conversation history?

## Architecture Onboarding

- **Component map:** Input Images (x^c per class) → Vision Encoder ϕ → Latent Embeddings z^c → Perturbation δ → Perturbed Embeddings z^c + δ → Decoder D → Target Text (t^c per class)

- **Critical path:**
  1. Collect training images per semantic class (10-50 images per class)
  2. Assign distinct target text t^(c) to each class
  3. Initialize Δ = 0 in normalized space
  4. For each iteration: sample batch → construct x_adv = Ψ(x) + Δ → compute L_total = L_CE + λ·L_Margin → update Δ → clip to bounds
  5. Recover δ = Ψ⁻¹(Δ) for deployment
  6. Evaluate on held-out test set

- **Design tradeoffs:**
  - Frame vs. Corner: Frame (7,056 pixels) achieves higher multi-target capacity; Corner (1,600 pixels) is less conspicuous but fails beyond 6-7 targets
  - Number of targets: ASR degrades with target count (93% at 2 targets → 66% at 5 targets for Qwen)
  - Training set size: 10 images → overfitting (10% test ASR); 50 images → generalization (95% test ASR)
  - Target sequence length: Controlling 15-word sequences achieves 78% ASR (Qwen), indicating autoregressive error propagation aids attack

- **Failure signatures:**
  - Loss plateauing early in training → missing NSO (optimizing in pixel space)
  - High train ASR, near-zero test ASR → insufficient training diversity
  - Rapid ASR collapse beyond 7-8 targets → perturbation region too small
  - Attack fails on specific prompts → prompt-specific overfitting

- **First 3 experiments:**
  1. Reproduce 2-target attack on ImageNet subset: Use 50 train / 20 test images per class, frame constraint (6-pixel width), verify ASR >85% on Qwen or Intern. This validates NSO + SSO implementation.
  2. Ablate NSO vs. SSO: Run default SORT, w/o NSO, w/o SSO variants. Expect w/o NSO to fail catastrophically (near 0% ASR), w/o SSO to show modest degradation (~6% drop). Confirms optimization mechanics.
  3. Test capacity limits: Increment targets from 2 to 9 on ImageNet. Plot ASR decay curve. Compare frame vs. corner constraints. Expect corner to collapse to 0% around 8 targets; frame maintains ~30-40%. Validates information capacity hypothesis.

## Open Questions the Paper Calls Out

### Open Question 1
Can Semantic-Aware Universal Perturbations (SAUPs) maintain high attack success rates under physical world conditions, such as variations in brightness, perspective, and printing noise? The paper evaluates attacks in digital white-box settings but acknowledges the need to develop robust SAUPs resilient to physical conditions like brightness changes for future work.

### Open Question 2
Does the "information capacity" of a perturbation strictly determine the upper bound of distinct semantic targets, or can the theoretical "Required Expansion Ratio" be optimized to support arbitrarily complex routing? While the paper observes ASR collapse at 8 targets for corner perturbations due to limited pixel count, the precise relationship between pixel budget and theoretical geometric bounds remains open.

### Open Question 3
Why do advanced MLLMs with full-parameter supervised fine-tuning (SFT) exhibit higher susceptibility to SAUPs than simpler projection-aligned models? The paper notes that Llava (linear projection alignment) is significantly more robust to prompt variations compared to Qwen and Intern (SFT models), speculating that deeper alignment may amplify the influence of adversarial perturbations, but this requires further investigation.

## Limitations
- First-order Taylor expansion analysis assumes small perturbations and local linearity, potentially missing higher-order interactions in high-dimensional latent spaces
- RIST dataset, while carefully constructed, remains relatively small (15-13 trajectories) and may not capture full real-world complexity
- Perturbation region constraints (frame vs. corner) were chosen somewhat arbitrarily without systematic evaluation of optimal tradeoffs
- Claims of "universal" attacks require stronger validation across more diverse MLLMs beyond the three tested models

## Confidence
- **Mechanism Claims (Medium):** Empirically supported but relies on first-order approximations without rigorous higher-order analysis
- **Attack Effectiveness (High):** ASR results are clearly reported with controlled ablation studies showing robust degradation patterns
- **Generalizability (Medium):** Results transfer across three distinct MLLMs but sample size of tested models is limited
- **Optimization Innovation (Medium):** Novel NSO approach is demonstrably effective but lacks precedent in corpus and comparative analysis

## Next Checks
1. **Multi-Model Transferability:** Test SAUP perturbations across 5-7 diverse MLLMs (including different sizes and architectures) to quantify universality claims and identify model-specific vulnerabilities
2. **Defense Robustness Analysis:** Evaluate SAUP against standard defenses including adversarial training, input preprocessing (denoising, JPEG compression), and detection-based filtering to quantify performance degradation
3. **Semantic Granularity Scaling:** Systematically vary semantic class granularity while measuring ASR, perturbation magnitude, and visual imperceptibility to identify theoretical limits of semantic routing capacity for given perturbation region sizes