---
ver: rpa2
title: 'Artificial Impressions: Evaluating Large Language Model Behavior Through the
  Lens of Trait Impressions'
arxiv_id: '2510.08915'
source_url: https://arxiv.org/abs/2510.08915
tags:
- warmth
- competence
- impressions
- prompts
- llama-3
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LLMs learn to associate linguistic patterns with warmth and competence
  impressions of users, mirroring human perception patterns. This study introduces
  artificial impressions as measurable patterns in LLM internal representations that
  reflect these trait-based judgments.
---

# Artificial Impressions: Evaluating Large Language Model Behavior Through the Lens of Trait Impressions

## Quick Facts
- arXiv ID: 2510.08915
- Source URL: https://arxiv.org/abs/2510.08915
- Authors: Nicholas Deas; Kathleen McKeown
- Reference count: 40
- LLMs learn to associate linguistic patterns with warmth and competence impressions of users, mirroring human perception patterns.

## Executive Summary
This study introduces "artificial impressions" as measurable patterns in LLM internal representations that reflect trait-based judgments about prompt authors. The researchers developed a methodology using linear probes on LLM hidden states to measure warmth and competence impressions according to the two-dimensional Stereotype Content Model (SCM). They found that artificial impressions are more reliably measurable from representations than through direct prompting, and these impressions predict downstream behaviors including response quality and hedging frequency. The study also revealed potential biases, with models showing more negative competence and warmth impressions for African American Language texts compared to White Mainstream English.

## Method Summary
The researchers generated synthetic prompts using LLMs conditioned on trait specifications (warmth/competence adjectives), extracted hidden state activations from MLP layers, and trained linear probes to predict trait impressions. They validated probe performance on held-out synthetic data, then applied the probes to real-world conversation corpora (LMSysChat-1M) and dialect comparison datasets (TwitterAAE and counterparts). Response quality was evaluated using a Llama-3.1 405B judge, and hedging behavior was measured through keyword detection. The study compared probe performance against bag-of-words baselines and tested behavioral predictions using regression analysis.

## Key Results
- Artificial impressions (warmth/competence) are linearly decodable from LLM hidden states with 75-90 F1 for warmth and 75-85 F1 for competence
- Warmth and competence impressions predict LLM response quality and hedging behaviors, with competence being a stronger predictor of hedging
- Models exhibit more negative competence and warmth impressions for African American Language texts compared to White Mainstream English

## Why This Works (Mechanism)

### Mechanism 1: Linear Decodability of Trait Impressions from Hidden States
- During pretraining, models learn statistical associations between linguistic patterns and trait-relevant concepts that concentrate in intermediate MLP layers, enabling linear classification
- Core assumption: Linguistic-trait associations learned during pretraining are sufficiently consistent to permit linear separation
- Evidence: F1 scores exceed BOW baseline at most model depths, with peak performance at mid-layers

### Mechanism 2: Warmth-Competence Dimensional Separability
- Warmth and competence are encoded as distinct, weakly-correlated dimensions rather than a unified "favorability" construct
- Core assumption: SCM's two-dimensional structure maps onto how LLMs encode trait information
- Evidence: Probes achieve different performance profiles and predictions align only 28-40% of the time with weak negative correlations

### Mechanism 3: Impression-Behavior Linkage Through Shared Representations
- Encoded impressions predict downstream behaviors including response quality and hedging frequency
- Core assumption: Representations encoding impressions are causally connected to generation
- Evidence: Lower competence predictions correlate with increased hedging (coefficients: -0.69 to -1.18) and higher warmth/competence predict better quality scores

## Foundational Learning

- **Stereotype Content Model (SCM)**
  - Why needed: Provides the two-dimensional framework (warmth + competence) that structures all probe design and interpretation
  - Quick check: Why treat warmth and competence as separate dimensions rather than combining into a single "positivity" score?

- **Linear Probing of Neural Representations**
  - Why needed: Core methodologyâ€”understanding what linear probes reveal about representation structure is essential for interpreting results
  - Quick check: If a probe achieves 85% F1, what can you conclude about whether the model "uses" this information during generation?

- **Language Attitudes and Sociolinguistic Variation**
  - Why needed: Critical for interpreting AAL vs. WME findings and understanding how dialect features trigger stereotype-consistent impressions
  - Quick check: Why might identical semantic content expressed in different dialects yield different impression predictions?

## Architecture Onboarding

- **Component map**: Synthetic data generation -> Hidden state extraction -> Probe training -> Behavioral validation
- **Critical path**: 1) Generate ground-truth synthetic prompts with known trait labels 2) Extract and probe hidden states across layers 3) Validate on held-out synthetic data 4) Test behavioral predictions on real data
- **Design tradeoffs**: Synthetic vs. real training data (labels vs. distribution match); MLP vs. residual stream activations (stability vs. comprehensiveness); single vs. paired trait specifications (simplicity vs. diversity)
- **Failure signatures**: Probe accuracy near random (representations may not encode trait); BOW outperforms probe (surface-level task); high warmth-competence correlation (poor discriminant validity); no behavioral correlation (overfitting)
- **First 3 experiments**: 1) Replicate probing on different model family to test generalizability 2) Cross-domain validation: train on synthetic, test on human-annotated real prompts 3) Layer ablation: systematically vary which layer's activations are probed

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do artificial impressions evolve across multi-turn dialogues compared to the single-turn prompts analyzed?
- Basis: The authors note that person perception research documents how impressions change over interactions, and they leave such investigations to future work
- Why unresolved: Current methodology strictly focuses on initial messages in English conversations
- What evidence would resolve it: A longitudinal study applying linear probes to hidden states across multiple conversation turns

### Open Question 2
- Question: To what extent do alternative psychological frameworks, such as the Power-Benevolence model, capture LLM stereotype content differently than SCM?
- Basis: The Discussion section notes that relaxing assumptions and exploring alternative models of stereotype content are promising directions
- Why unresolved: The study relied solely on the two-dimensional SCM for simplicity and established precedent
- What evidence would resolve it: Training probes based on Power-Benevolence model dimensions and comparing predictive power to SCM probes

### Open Question 3
- Question: Which specific pretraining data distributions or post-training alignment techniques are responsible for the formation of artificial impressions?
- Basis: The Limitations section notes that while findings are consistent across models, "it is still unknown what factors in the pretraining process or data lead to the phenomena we identify"
- Why unresolved: The study evaluated model outputs and internal states but did not perform ablation studies on training corpora or stages
- What evidence would resolve it: An ablation study comparing impression probe performance on models trained on filtered vs. unfiltered datasets

## Limitations

- Synthetic training data may not fully capture real-world prompt distributions, limiting probe generalizability
- Single large judge model for quality scoring introduces potential bias and scalability concerns
- Focus on Western psychological framework (SCM) may not generalize to other cultural contexts or impression formation models

## Confidence

- **High confidence**: Linear decodability of warmth and competence from hidden states; probe performance exceeding baselines (75-90 F1 for warmth, 75-85 F1 for competence); connection between competence impressions and hedging behavior
- **Medium confidence**: Behavioral predictions (quality scores, hedging frequency); AAL/WME dialect comparison findings; the specific two-dimensional SCM structure mapping to LLM representations
- **Low confidence**: Causal claims about impression representations influencing generation; generalizability across model families and cultural contexts; stability of artificial impressions under different pretraining corpora

## Next Checks

1. **Cross-cultural validation**: Apply the probe framework to prompts in different languages and cultural contexts to test whether the warmth-competence dimensional structure holds universally or reflects Western social cognition patterns

2. **Temporal stability analysis**: Train probes on synthetic data from multiple time points during model pretraining to determine when and how artificial impressions emerge, distinguishing between early statistical learning versus later alignment effects

3. **Counterfactual generation test**: Use the trained probes to steer generation toward specific warmth/competence targets and measure whether the resulting text changes human-annotated trait impressions, establishing causal rather than correlational relationships