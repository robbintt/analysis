---
ver: rpa2
title: 'Dep-Search: Learning Dependency-Aware Reasoning Traces with Persistent Memory'
arxiv_id: '2601.18771'
source_url: https://arxiv.org/abs/2601.18771
tags:
- memory
- reasoning
- retrieve
- information
- result
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Dep-Search integrates dependency-aware question decomposition,
  persistent memory management, and GRPO-based reinforcement learning to enable large
  language models to perform structured multi-hop reasoning. The framework uses explicit
  control tokens for decomposition, retrieval, memory access, and summarization, while
  automatically storing and reusing extracted facts to avoid redundant searches.
---

# Dep-Search: Learning Dependency-Aware Reasoning Traces with Persistent Memory

## Quick Facts
- arXiv ID: 2601.18771
- Source URL: https://arxiv.org/abs/2601.18771
- Reference count: 40
- Dep-Search achieves 39.29 and 49.77 average scores on Qwen2.5-3B-Instruct and Qwen2.5-7B-Instruct models respectively, with up to 16.6% relative improvement on multi-hop reasoning tasks.

## Executive Summary
Dep-Search introduces a framework for structured multi-hop reasoning in large language models by combining dependency-aware question decomposition, persistent memory management, and reinforcement learning. The system uses explicit control tokens to decompose questions into dependency graphs, retrieve and store facts in an LRU memory buffer, and summarize intermediate results for reuse. Trained via Group-Relative Policy Optimization (GRPO), the model learns efficient reasoning traces that avoid redundant searches and improve answer accuracy across seven diverse QA datasets.

## Method Summary
Dep-Search uses a policy LLM that generates control tokens (<Decompose>, <Retrieve>, <Memory>, <Conclusion>) to structure reasoning. Questions are decomposed into dependency DAGs using QDMR-style syntax, then processed through a loop of retrieval (embedding + reranker), memory storage, and fact retrieval. Facts are stored in an LRU buffer (optimal capacity 15) and accessed via embedding similarity. The policy is trained via GRPO with group-relative advantages, optimizing for answer quality while penalizing excessive retrieval/decomposition. Training uses Qwen2.5-3B/7B models, Wikipedia 2018 corpus, and 3 epochs with K=4 trajectories per question.

## Key Results
- Achieves 39.29 and 49.77 average scores on Qwen2.5-3B-Instruct and Qwen2.5-7B-Instruct respectively
- Memory module contributes largest performance gains (5.25 points), followed by QDMR decomposition (3.32 points) and summarization (2.08 points)
- Optimal memory capacity found at 15 entries, with performance degrading beyond this threshold
- Relative improvements up to 16.6% on multi-hop reasoning tasks compared to strong baselines

## Why This Works (Mechanism)

### Mechanism 1: Dependency-Aware Decomposition (QDMR)
- **Claim:** Explicitly structuring sub-questions as a dependency graph (DAG) rather than a linear chain appears to reduce reasoning errors in multi-hop tasks.
- **Mechanism:** The model emits a `<Decompose>` token to break a query into sub-questions with explicit references (e.g., "Use (1) to find (2)"). This forces the resolution of prerequisite facts before dependent reasoning steps, preventing the model from hallucinating intermediate answers.
- **Core assumption:** The base LLM has sufficient capacity to learn the syntax of graph-structured decomposition and follow topological ordering during generation.
- **Evidence anchors:** [Abstract]: "decompose questions with dependency relationships... ensuring that prerequisite sub-questions are resolved before dependent ones." [Section 5.2]: Removing QDMR decomposition causes a drop of 3.32 points, particularly affecting multi-hop datasets.

### Mechanism 2: Persistent Memory with LRU Management
- **Claim:** Decoupling fact storage from the immediate context window via an explicit LRU buffer likely reduces redundant retrieval operations and context noise.
- **Mechanism:** Retrieved documents are summarized into fact sentences via a `<Conclusion>` token and stored in an external buffer. The model can then query this buffer using `<Memory>` tokens and embedding similarity, retrieving only relevant historical facts rather than re-searching.
- **Core assumption:** The summarization step reliably extracts truthful, atomic facts without introducing hallucinations that corrupt the memory buffer.
- **Evidence anchors:** [Section 5.2]: Ablation studies show the memory module causes the largest performance drop (-5.25 points) when removed. [Section 5.5]: Performance degrades beyond 15 entries due to noise.

### Mechanism 3: Group-Relative Policy Optimization (GRPO)
- **Claim:** Normalizing rewards against a group of sampled trajectories appears to stabilize RL training by mitigating the impact of varying question difficulty.
- **Mechanism:** Instead of relying on a critic model, GRPO samples $K$ trajectories per question and calculates advantage $A(\tau)$ as the difference between a trajectory's reward and the group mean. This allows the model to learn from "less bad" failures on hard questions and penalize "lucky" successes on easy ones.
- **Core assumption:** The group size ($K=4$ in experiments) is sufficient to approximate the expected value for a given prompt.
- **Evidence anchors:** [Section 3.4]: "Group-relative advantages... naturally handle varying question difficulty by comparing trajectories within the same group."

## Foundational Learning

- **Concept: Directed Acyclic Graphs (DAGs) in Reasoning**
  - **Why needed here:** Dep-Search replaces linear chains with DAGs. Understanding topological sorting is required to debug why the model chooses to solve sub-question B before A.
  - **Quick check question:** Can you sketch the dependency graph for "Who is the mayor of the birthplace of the winner of the 2024 MVP?"

- **Concept: Reinforcement Learning with Sparse Rewards**
  - **Why needed here:** The system learns when to search vs. recall from memory based on a final reward signal.
  - **Quick check question:** How does GRPO differ from PPO in terms of value function estimation?

- **Concept: Dense Retrieval & Reranking**
  - **Why needed here:** The `<Retrieve>` token triggers a two-stage search (embedding + reranker).
  - **Quick check question:** Why might a reranker be necessary after an initial dense retrieval step?

## Architecture Onboarding

- **Component map:** Question -> <Decompose> -> DAG of sub-questions -> <Retrieve>/<Memory>/<Conclusion> loop -> Answer
- **Critical path:**
  1.  **Input:** Question $Q$.
  2.  **Decompose:** Model generates DAG of sub-questions.
  3.  **Act:** Loop of `<Retrieve>` (get docs) $\to$ `<Conclusion>` (write to memory) $\to$ `<Memory>` (read facts).
  4.  **Optimize:** GRPO calculates group-relative advantage to update Policy weights.
- **Design tradeoffs:**
  - **Memory Capacity:** Paper finds 15 entries optimal; higher values introduce noise (Fig 4).
  - **Reward Thresholds:** Setting strict limits on retrieval counts ($k_1$) encourages efficiency but risks cutting off evidence gathering for complex questions.
- **Failure signatures:**
  - **Action Loops:** Model generates repeated `<Retrieve>` calls without progressing. (Fix: Increase penalty coefficient $\lambda_{ret}$).
  - **Memory Hallucination:** Model writes incorrect summaries to memory. (Fix: Verify the `Summarize()` prompt or restrict write access).
- **First 3 experiments:**
  1.  **Sanity Check:** Run inference on a held-out set with memory disabled vs. enabled to verify the ablation gap.
  2.  **Threshold Sweep:** Replicate Figure 2 on a small validation set to find optimal reward thresholds ($k_1, k_2$) for your specific model size.
  3.  **Capacity Test:** Vary memory buffer size (1, 10, 20) on a multi-hop dataset (e.g., 2WikiMHQA) to verify the "15-entry" efficiency hypothesis.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does Dep-Search generalize effectively to model architectures beyond Qwen2.5, particularly to models with different tokenization schemes or control token handling capabilities?
- **Basis in paper:** [explicit] The authors only evaluate on Qwen2.5-3B-Instruct and Qwen2.5-7B-Instruct, noting "These models provide a good balance between performance and computational efficiency, allowing us to evaluate Dep-Search's effectiveness across different model scales."
- **Why unresolved:** No experiments were conducted on other model families (e.g., LLaMA, Mistral, Gemma) to verify whether the control token framework transfers across architectures with different pretraining objectives.
- **What evidence would resolve it:** Comparative experiments on at least 2-3 additional model families, measuring both absolute performance and relative improvement over baselines.

### Open Question 2
- **Question:** Can more sophisticated memory management strategies (beyond fixed-capacity LRU eviction) improve knowledge retention and reasoning accuracy in longer multi-hop chains?
- **Basis in paper:** [explicit] The conclusion states future work should explore "dynamic memory management strategies" and the paper only implements "an LRU buffer with a fixed capacity."
- **Why unresolved:** The current memory module uses static LRU eviction; performance degrades beyond 15 entries (dropping from 42.3 to 40.8), suggesting the fixed-capacity approach may lose valuable knowledge in complex scenarios.
- **What evidence would resolve it:** Ablation studies comparing LRU against priority-based, importance-weighted, or learned memory retention policies across tasks requiring longer reasoning chains.

### Open Question 3
- **Question:** How does optimal memory capacity scale with question complexity, reasoning depth, and number of sub-questions in the decomposition?
- **Basis in paper:** [inferred] Memory capacity sensitivity analysis (Figure 4) shows optimal performance at 15 entries on 2WikiMHQA only, but different datasets trigger different retrieval frequencies (3.2 to 8.2 calls), suggesting capacity needs may vary.
- **Why unresolved:** The paper fixes Cmem=20 without analyzing whether optimal capacity correlates with task complexity metrics like number of hops, sub-question count, or retrieval frequency.
- **What evidence would resolve it:** Systematic experiments varying memory capacity across datasets with known reasoning depths (e.g., 2-hop vs. 4-hop questions in Musique), measuring both performance and reuse rates.

## Limitations

- **Dependency Graph Quality:** The paper does not systematically evaluate whether generated dependency graphs are structurally correct, raising questions about whether the approach truly captures multi-hop reasoning dependencies versus memorizing patterns.
- **Memory Reliability:** The external memory system relies on LLM summarization without verification, potentially allowing hallucinations to corrupt the reasoning process silently.
- **Hyperparameter Sensitivity:** The paper presents specific hyperparameters as optimal without systematic sensitivity analysis, limiting confidence in their generalizability across different model scales or task distributions.

## Confidence

**High Confidence (8-10/10):** The experimental results demonstrating consistent performance improvements across seven diverse QA datasets are well-supported by the data. The ablation studies clearly show the relative contributions of each component.

**Medium Confidence (5-7/10):** The theoretical justification for why dependency-aware decomposition improves reasoning is sound, but the empirical evidence is limited to relative performance gains rather than direct analysis of reasoning quality.

**Low Confidence (2-4/10):** Claims about the optimality of specific hyperparameters (memory capacity=15, learning rate=1e-5, etc.) lack systematic sensitivity analysis. The paper presents these as optimal values but does not demonstrate that small deviations would significantly impact performance.

## Next Checks

1. **Dependency Graph Structural Analysis:** Implement a systematic evaluation of the generated dependency graphs on a subset of questions, measuring topological correctness and dependency accuracy. Compare against ground-truth decomposition structures if available, or use human evaluation to assess whether the DAGs truly capture prerequisite relationships.

2. **Memory Buffer Content Verification:** Create a validation pipeline that samples memory entries and checks their factual accuracy against source documents. Measure hallucination rates and correlation between memory accuracy and downstream reasoning performance. Test whether corrupted memory entries systematically degrade performance.

3. **Hyperparameter Sensitivity Sweep:** Replicate the main experiments with systematic variation of key hyperparameters: memory capacity (5-30 entries), learning rate (0.5e-5 to 2e-5), and reward thresholds (k1, k2 varied by Â±50%). Quantify the stability of performance improvements across this parameter space to identify whether the reported settings are truly optimal or merely sufficient.