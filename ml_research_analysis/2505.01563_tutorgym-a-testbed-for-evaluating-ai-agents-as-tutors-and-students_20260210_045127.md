---
ver: rpa2
title: 'TutorGym: A Testbed for Evaluating AI Agents as Tutors and Students'
arxiv_id: '2505.01563'
source_url: https://arxiv.org/abs/2505.01563
tags:
- learning
- tutor
- tutorgym
- tutors
- tutoring
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TutorGym introduces a testbed for evaluating AI agents as tutors
  and students by interfacing them with existing intelligent tutoring systems (ITS)
  that have been validated in classroom studies. It enables comprehensive evaluation
  of AI tutoring capabilities through step-by-step problem-solving and solution recognition
  within real ITS interfaces, going beyond simple final answer generation.
---

# TutorGym: A Testbed for Evaluating AI Agents as Tutors and Students

## Quick Facts
- arXiv ID: 2505.01563
- Source URL: https://arxiv.org/abs/2505.01563
- Authors: Daniel Weitekamp; Momin N. Siddiqui; Christopher J. MacLellan
- Reference count: 40
- Primary result: Current LLMs perform poorly as tutors (52-70% accuracy at generating correct next-step actions) but can simulate human-like learning curves as students

## Executive Summary
TutorGym introduces a testbed for evaluating AI agents as tutors and students by interfacing them with existing intelligent tutoring systems (ITS) that have been validated in classroom studies. It enables comprehensive evaluation of AI tutoring capabilities through step-by-step problem-solving and solution recognition within real ITS interfaces, going beyond simple final answer generation. When evaluated, current large language models showed poor tutoring performance, with only 52-70% accuracy at generating correct next-step actions and none better than chance at identifying incorrect actions. However, when trained as students using in-context learning, LLMs demonstrated remarkably human-like learning curves, suggesting potential for future development of AI-based simulated learners.

## Method Summary
The testbed interfaces AI agents with three types of ITS backends (CTAT, Apprentice, OATutor) through a standard API exposing tutor state and evaluating actions. Agents are evaluated using "completeness profiles" that capture all correct actions for each reachable state, augmented with incorrect actions from real student data. For tutoring evaluation, LLMs are prompted to generate correct actions and identify incorrect ones. For simulated learning, agents accumulate state-action-reward tuples as in-context examples, with the prompt capped at 50k characters (approximately 15k tokens). The system uses 223 tutor domains across 732 CTAT problems, 30 Apprentice random generators, and 5161 OATutor problems.

## Key Results
- LLMs achieved only 52-70% accuracy at generating correct next-step actions as tutors
- No LLM performed better than chance at identifying incorrect actions
- LLMs trained as students via in-context learning demonstrated human-like learning curves
- Current commercial LLMs show significant limitations in tutoring capabilities despite strong performance on final-answer benchmarks

## Why This Works (Mechanism)

### Mechanism 1: ITS Interface Grounding for Stateful Evaluation
The testbed provides a standard API (`act(state)`, `check(state, action)`) that exposes each tutor's current state to an AI agent. The agent's output (an SAI: Selection-Action-Input tuple) is evaluated by the underlying ITS's own expert model via `check()`. This creates a feedback loop where the AI's step-by-step actions are validated against the precise, fine-grained logic of existing ITSs. Core assumption: The underlying ITS tutor models provide reliable ground truth for correct and incorrect actions.

### Mechanism 2: In-Context Learning as a Process Model for Simulated Learners
The `Trainer` class manages a sequence of problems where LLMs receive feedback (correct/incorrect) and worked examples when they fail. State-action-reward tuples are added to the LLM's prompt as in-context examples, accumulating a history of "experiences" that guide future predictions. The authors hypothesize this primarily provides procedural context (where to type, what order) rather than teaching new math skills.

### Mechanism 3: Completeness Profiles for Comprehensive Tutor Model Evaluation
TutorGym's `get_all_demos()` function generates collections of reachable ITS states with every correct next action, augmented with incorrect actions from real student data. The AI agent is evaluated on two tasks: generating a correct action for each state and classifying sets of correct and incorrect actions. This tests both generative and evaluative capabilities rather than just the "happy path."

## Foundational Learning

- **Concept: Intelligent Tutoring System (ITS) Components**
  - Why needed here: To understand what TutorGym is testing. The paper evaluates AI agents against the three core components of an ITS: the interface, the tutor (expert) model, and the student model.
  - Quick check question: Can you explain the difference between a tutor model (which evaluates student actions) and a student model (which tracks student mastery)?

- **Concept: Model Tracing vs. Final-Answer Evaluation**
  - Why needed here: This is the core motivation for TutorGym. The paper argues that evaluating an AI based solely on whether it gets the final answer right is insufficient for tutoring.
  - Quick check question: Why would an AI that gets 90% of final answers correct still be considered a poor tutor by TutorGym's standards?

- **Concept: In-Context Learning (ICL)**
  - Why needed here: This is the mechanism by which LLMs act as simulated learners. The engineer must understand that ICL is not a permanent weight update but a temporary state created by feeding examples into the prompt.
  - Quick check question: If you restart the TutorGym training process with the same LLM, will it remember what it "learned" in the previous run? Why or why not?

## Architecture Onboarding

- **Component map:** Tutor Backends (CTAT, Apprentice, OATutor) -> TutorGym Core API (ProblemState, SAI handling) -> Agent Wrappers (Symbolic, LLM, RL Wrapper) -> Trainer class (orchestration)

- **Critical path:**
  1. `Trainer` calls `tutor.get_state()`, passes `ProblemState` to `agent.act(state)`
  2. Agent returns an `SAI`
  3. `Trainer` calls `tutor.check(state, action)` to get a `reward`
  4. Feedback is passed to `agent.train(state, action, reward)`
  5. If incorrect, `Trainer` calls `tutor.get_demo(state)` for a worked example
  6. If correct, `Trainer` calls `tutor.apply(action)` to advance the tutor

- **Design tradeoffs:**
  - Completeness vs. Tractability: Full completeness profiles are expensive but rigorous; sampling strategies may be needed for large domains
  - LLM Context Window: Capping examples (e.g., at 50k characters) trades off trajectory length against example detail
  - API Cost vs. Local Inference: Commercial LLMs are accurate but costly ($730+ reported); local models are cheaper but slower and hardware-intensive

- **Failure signatures:**
  - Tutor Model Hallucination: LLM labels incorrect actions as "correct" or provides plausible but wrong hints
  - Procedural Drift: Simulated learner fails because context window fills with examples from a previous domain
  - State Space Explosion: RL agent fails to converge because `RL_Wrapper` creates intractably large state/action spaces

- **First 3 experiments:**
  1. Baseline Tutoring Evaluation: Run one LLM (e.g., GPT-4o) as a tutor on a single TutorGym domain using its completeness profile. Measure accuracy at both grading student actions and generating correct hints.
  2. Simulated Learner Curve Fitting: Run an LLM as a student on the same domain. Plot its error rate over successive problems and compare to human student data to validate the ICL mechanism.
  3. Context Window Ablation: Rerun the simulated learner experiment with a reduced context window (e.g., from 15k to 5k tokens). Observe how the learning curve degrades to test sensitivity to available context.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does in-context learning enable LLMs to learn new math skills, or does it merely provide information on interface location and answer entry order?
- Basis in paper: [explicit] The authors explicitly state, "We hypothesize that they primarily provide information on the location and order of answer entry, rather than enabling the LLM to 'learn' new math skills."
- Why unresolved: The study observed human-like learning curves but did not isolate the specific mechanism (skill acquisition vs. interface formatting) driving the improvement.
- What evidence would resolve it: Ablation studies comparing LLM performance on problems requiring novel conceptual skills versus problems requiring only novel interface formatting would clarify the learning mechanism.

### Open Question 2
- Question: Can in-context learning improve an LLM's ability to identify incorrect actions (grading), which is distinct from generating correct actions?
- Basis in paper: [explicit] The authors note that "These results suggest that in-context learning may improve LLM tutoring capabilities... However, we only evaluate whether the agent can generate a single correct action here, not its ability to grade complete action profiles."
- Why unresolved: Zero-shot LLMs performed at chance levels when identifying incorrect actions; it remains unknown if the in-context training method used for "student" agents improves this specific tutoring capability.
- What evidence would resolve it: Evaluating the "student" agents (trained via in-context learning) on the task of labeling correct and incorrect actions within the completeness profiles.

### Open Question 3
- Question: Can open-source LLMs function as effective simulated learners given current hardware and inference speed constraints?
- Basis in paper: [explicit] The authors omitted open-source models from the simulated learner evaluation because "inference speed was prohibitively slow on our available hardware," but state they "hope to investigate... in future work."
- Why unresolved: The cost and speed of commercial models were limiting factors, but the viability of local, open-source alternatives for this high-context task was not demonstrated.
- What evidence would resolve it: Benchmarking open-source models (e.g., Llama, DeepSeek) within the TutorGym Trainer class to measure learning curves relative to inference time and hardware requirements.

## Limitations

- The completeness profiles are sampled rather than truly complete for complex domains, potentially affecting evaluation rigor
- The evaluation relies on underlying ITS tutor models as ground truth, introducing potential circularity if these models have limitations
- Sample size of 223 tutor domains may not represent the full diversity of ITS domains found in real educational settings

## Confidence

- **High Confidence:** LLMs perform poorly as tutors (52-70% accuracy at generating correct next-step actions)
- **Medium Confidence:** LLMs demonstrate human-like learning curves as simulated learners
- **Medium Confidence:** In-context learning primarily captures procedural context rather than genuine skill transfer

## Next Checks

1. **Ground Truth Validation:** Independently verify the correctness of actions in completeness profiles by having human experts review a random sample of state-action pairs across different tutor domains.

2. **Domain Diversity Analysis:** Systematically analyze whether the 223 tutor domains span the full range of mathematical and conceptual complexity found in real educational settings, particularly focusing on domains outside mathematics.

3. **Context Window Sensitivity:** Conduct controlled experiments varying the context window size (e.g., 5k, 10k, 15k tokens) to quantify how context window limitations affect the simulated learning curves and identify the threshold where performance degradation becomes significant.