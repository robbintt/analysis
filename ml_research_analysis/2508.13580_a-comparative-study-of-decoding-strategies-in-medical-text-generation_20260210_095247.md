---
ver: rpa2
title: A Comparative Study of Decoding Strategies in Medical Text Generation
arxiv_id: '2508.13580'
source_url: https://arxiv.org/abs/2508.13580
tags:
- decoding
- medical
- strategies
- tasks
- rouge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically evaluates 11 decoding strategies across
  5 medical tasks using both general-purpose and medical LLMs of varying sizes. Deterministic
  methods like beam search outperform stochastic ones, achieving higher scores at
  the cost of increased inference time.
---

# A Comparative Study of Decoding Strategies in Medical Text Generation

## Quick Facts
- arXiv ID: 2508.13580
- Source URL: https://arxiv.org/abs/2508.13580
- Reference count: 31
- Primary result: Deterministic decoding strategies outperform stochastic ones in medical text generation, with beam search achieving highest scores but increased inference time.

## Executive Summary
This study systematically evaluates 11 decoding strategies across 5 medical tasks using both general-purpose and medical LLMs of varying sizes. Deterministic methods like beam search outperform stochastic ones, achieving higher scores at the cost of increased inference time. Larger models yield better performance but are not more robust to decoding strategy changes. Surprisingly, medical LLMs do not consistently outperform general models and are more sensitive to decoding choices. Evaluation metrics show varying agreement, with MAUVE being particularly sensitive to decoding strategy. These findings highlight the critical role of decoding strategy selection in medical applications, where it can sometimes matter as much as model choice.

## Method Summary
The study evaluates 11 decoding strategies across 5 open-ended medical tasks (translation, summarization, QA, dialogue, image captioning) using 9 LLMs (general-purpose, medical-specific, multimodal). The evaluation uses 100 samples per task from five datasets: UFAL Medical Corpus (German–English), PubMed summarization, medalpaca medical QA, Healthbench dialogues, and ROCOv2 radiology images. The study measures ROUGE (all tasks), BLEU (translation), BERTScore (all tasks), and MAUVE (dialogue, QA), along with inference time and sensitivity (coefficient of variation). Statistical analysis uses non-parametric tests (Friedman, Wilcoxon with Holm correction) on task-normalized scores.

## Key Results
- Beam search achieves the highest automated metric scores across all tasks but requires significantly more inference time
- Medical LLMs show significantly higher sensitivity to decoding strategy choice than general models (Levene test: p = 0.0004)
- Larger models consistently outperform smaller ones, but model size does not correlate with robustness to decoding strategy changes
- Deterministic strategies consistently outperform stochastic ones, with top-p, η, and top-k sampling performing worst

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Deterministic decoding strategies outperform stochastic methods in medical text generation accuracy
- Mechanism: Search-based methods like beam search maintain multiple candidate sequences and optimize globally rather than making locally greedy or random choices. By retaining the top-k most probable sequences and computing cumulative probabilities, the method approximates globally optimal outputs that are more likely to contain accurate medical terminology and factual consistency.
- Core assumption: Medical text prioritizes factual precision over linguistic diversity; the "correct" medical answer is generally singular rather than open-ended.
- Evidence anchors:
  - [abstract] "deterministic strategies generally outperform stochastic ones: beam search achieves the highest scores, while η and top-k sampling perform worst"
  - [section 4.1] "BS had the best mean rank, followed by CS and DBS; top-p, eta, and top-k performed the worst"
  - [corpus] Weak direct corpus support for medical domain specificity; DSVD paper mentions faithful generation challenges but not domain-specific decoding effects
- Break condition: Tasks requiring creative or diverse outputs (e.g., patient-facing explanations with varied phrasing) may benefit from stochastic methods despite lower ROUGE scores.

### Mechanism 2
- Claim: Medical-specialized LLMs exhibit greater sensitivity to decoding strategy choice than general-purpose models
- Mechanism: Fine-tuning on narrow medical corpora sharpens probability distributions for in-domain tokens but flattens them for out-of-domain content. This creates calibration gaps where the model assigns high confidence to medical terms but uncertain distributions elsewhere, making output quality more dependent on how the decoding method samples from these distributions.
- Core assumption: Domain fine-tuning narrows the effective vocabulary and creates sharper but less robust probability distributions.
- Evidence anchors:
  - [section 4.2] "medical LLMs are more sensitive to the decoding strategy... Levene: F(1,16) = 20.10, p = 0.0004"
  - [section 5] "fine-tuning has shown to make language models less calibrated... for out-of-domain questions, the probability distribution may be flatter"
  - [corpus] "The Geometry of Creative Variability" paper discusses calibration gaps in language models but does not specifically address domain fine-tuning effects
- Break condition: If fine-tuning data is sufficiently diverse or includes general-domain mixing, sensitivity may decrease.

### Mechanism 3
- Claim: Decoding strategy impact on output quality can exceed model choice
- Mechanism: The decoding strategy determines which regions of the learned probability space are explored. A suboptimal sampling method can consistently select lower-quality tokens even from a well-trained model, while an optimal search strategy can extract better outputs from a weaker model. This effect is amplified in precision-critical domains.
- Core assumption: Model quality and decoding quality are partially independent dimensions; good models can produce poor outputs under bad decoding.
- Evidence anchors:
  - [abstract] "the critical role of decoding strategy in medical text generation, sometimes outweighing model choice in impact"
  - [section 5] "the choice of decoding strategy should be regarded as an important design decision"
  - [corpus] Energy-Conscious LLM Decoding paper examines decoding impact on resources but does not directly address quality-vs-model comparisons
- Break condition: For extremely large capability gaps between models, model choice will dominate regardless of decoding.

## Foundational Learning

- Concept: Autoregressive token generation
  - Why needed here: All decoding strategies operate on the sequential prediction of tokens conditioned on prior context; understanding this foundation is necessary to compare how different strategies modify the base sampling process.
  - Quick check question: Can you explain why greedy decoding produces different outputs than beam search given the same model and prompt?

- Concept: Probability distribution calibration
  - Why needed here: The paper's findings about medical LLM sensitivity relate directly to how fine-tuning affects the sharpness and reliability of token probability distributions.
  - Quick check question: What would happen to output variance if a model's probability distribution became flatter (more uniform) across its vocabulary?

- Concept: Evaluation metric alignment
  - Why needed here: MAUVE's weak correlation with ROUGE/BERTScore and high sensitivity to decoding strategy means metric choice can determine which decoding method appears "best."
  - Quick check question: Why might a metric emphasizing diversity (MAUVE) rank decoding strategies differently than a metric emphasizing exact overlap (ROUGE)?

## Architecture Onboarding

- Component map: Logits → Softmax (with optional temperature scaling) → Strategy-specific token selection → Autoregressive continuation → Metric evaluation

- Critical path: The decoding layer sits between the model's probability distribution and the final output, making it a critical control point for output quality.

- Design tradeoffs:
  - Accuracy vs. latency: Beam search yields highest ROUGE but slowest inference (section 4.1 shows positive correlation τ = 0.4909 between time and quality)
  - Robustness vs. specialization: Medical models outperform on some tasks but are less robust to decoding changes (CV differences significant at p = 0.0004)
  - Metric stability vs. sensitivity: BERTScore most stable (CV ≈ 0.002), MAUVE most variable (CV ≈ 0.162)

- Failure signatures:
  - Stochastic methods producing factually incorrect medical content (top-k, η-sampling worst performers)
  - Medical models showing performance collapse when switched from greedy to sampling
  - MAUVE scores that contradict ROUGE/BERTScore trends, indicating decoding-sensitive artifact rather than genuine quality difference

- First 3 experiments:
  1. Replicate the beam search vs. top-p comparison on your specific medical task with your chosen model, measuring both ROUGE and inference time to establish the accuracy-latency tradeoff curve for your deployment constraints.
  2. Test your medical-specialized model across all 11 decoding strategies with fixed hyperparameters to measure sensitivity (coefficient of variation); if CV > 0.05, commit to strategy-specific tuning.
  3. Run parallel evaluation with ROUGE and MAUVE on a held-out sample; if rank correlation is weak (τ < 0.3), select primary metric based on task requirements (precision vs. diversity) and report both.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the observed performance advantages of deterministic strategies generalize to medical datasets significantly larger than 100 samples?
- Basis in paper: [explicit] The authors state, "Future work should examine whether these findings generalize to larger datasets, as our evaluation was limited in sample size."
- Why unresolved: Resource constraints limited the experimental scope to only 100 samples per task, which may not capture the full variance of clinical language or rare medical phenomena.
- Evidence would resolve it: Replicating the evaluation pipeline on full-scale benchmarks (e.g., the complete UFAL Medical Corpus) to see if beam search maintains its statistical superiority.

### Open Question 2
- Question: How does the inclusion of repetition and frequency penalties alter the performance ranking of deterministic versus stochastic decoding strategies?
- Basis in paper: [explicit] The authors list as a limitation "the lack of analysis on repetition and frequency penalties, despite their growing use in practice."
- Why unresolved: While deterministic methods scored higher, they are prone to repetitive outputs; penalties might mitigate this, potentially changing the trade-off between stochastic and deterministic methods.
- Evidence would resolve it: An ablation study applying repetition penalties across the 11 strategies to measure changes in ROUGE scores and output diversity.

### Open Question 3
- Question: Can a composite metric combining MAUVE with precision-oriented measures provide a more reliable evaluation for medical text generation?
- Basis in paper: [explicit] The discussion notes that "MAUVE alone may be insufficient; combining it with a precision-oriented metric could yield a more balanced evaluation, a direction future work should explore."
- Why unresolved: MAUVE showed weak agreement with BERTScore and high sensitivity to decoding, suggesting it assesses different qualities (diversity) than medical accuracy requires.
- Evidence would resolve it: Developing a weighted composite metric and validating its correlation with human clinician assessments of factual accuracy.

## Limitations

- The study uses only 100 samples per task, which may not capture the full variance of clinical language or rare medical phenomena.
- Evaluation relies primarily on automated metrics without human assessment of factual accuracy or clinical utility, which is critical for medical applications.
- The observed sensitivity of medical LLMs to decoding strategy could reflect insufficient fine-tuning or calibration rather than an inherent property of domain specialization.

## Confidence

**High Confidence**: The empirical observation that beam search achieves the highest automated metric scores across tasks. The statistical analysis using Friedman tests and Wilcoxon pairwise comparisons with Holm correction provides robust evidence for this ranking, supported by clear effect sizes and p-values.

**Medium Confidence**: The claim that medical LLMs are more sensitive to decoding strategy than general models. While the Levene test shows significant variance differences (p = 0.0004), this could be influenced by model size differences, training duration variations, or the specific datasets used.

**Low Confidence**: The assertion that decoding strategy choice can outweigh model selection in impact. This conclusion relies on relative comparisons within constrained model families and doesn't account for extreme capability differences between model scales.

## Next Checks

1. **Reproduce sensitivity analysis with controlled seeds**: Run the full evaluation pipeline across all 11 decoding strategies using fixed random seeds for test set sampling and decoding stochasticity. Compute coefficient of variation for each (model, task) pair and verify the reported Levene test results. Include 95% confidence intervals for all metric scores to quantify the stability of observed differences.

2. **Conduct human evaluation of clinical utility**: For a subset of medical QA and summarization outputs, have clinical experts rate factual accuracy, completeness, and potential harm on a 5-point scale. Compare expert rankings against automated metrics to identify discrepancies, particularly for MAUVE vs. ROUGE/BERTScore divergence. Focus on cases where decoding strategy choice creates clinically meaningful differences.

3. **Test calibration gap hypothesis**: For each medical LLM, compute expected calibration error (ECE) across different token probability ranges. Compare ECE distributions between general and medical models to determine if fine-tuning creates the predicted "flatter" distributions for out-of-domain content. Validate whether this calibration difference correlates with decoding strategy sensitivity.