---
ver: rpa2
title: 'CreativeVR: Diffusion-Prior-Guided Approach for Structure and Motion Restoration
  in Generative and Real Videos'
arxiv_id: '2512.12060'
source_url: https://arxiv.org/abs/2512.12060
tags:
- video
- restoration
- quality
- motion
- artifacts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CreativeVR introduces a diffusion-prior-guided framework for video
  restoration that addresses structural and motion artifacts in both AI-generated
  and real-world videos. Unlike traditional methods that stabilize artifacts rather
  than repair them, CreativeVR uses a deep adapter conditioned on degraded video latents,
  injected into alternating blocks of a frozen text-to-video diffusion transformer
  via a precision knob.
---

# CreativeVR: Diffusion-Prior-Guided Approach for Structure and Motion Restoration in Generative and Real Videos

## Quick Facts
- arXiv ID: 2512.12060
- Source URL: https://arxiv.org/abs/2512.12060
- Reference count: 40
- Primary result: Achieves up to 37% relative FIQA improvement on faces and strong gains in GPT-based perceptual metrics on AIGC videos while maintaining practical throughput.

## Executive Summary
CreativeVR introduces a diffusion-prior-guided framework for video restoration that addresses structural and motion artifacts in both AI-generated and real-world videos. Unlike traditional methods that stabilize artifacts rather than repair them, CreativeVR uses a deep adapter conditioned on degraded video latents, injected into alternating blocks of a frozen text-to-video diffusion transformer via a precision knob. This design enables smooth trade-offs between precise restoration for classical degradations and stronger corrective behavior for generative or real-world failures. A key novelty is a temporally coherent synthetic degradation module that composes realistic degradations during training to align the prior toward hard structural failures.

## Method Summary
CreativeVR employs a frozen Wan2.1-based DiT backbone (1.3B parameters) with a lightweight adapter DiT (half the blocks) that processes degraded video latents and injects features into alternating backbone blocks via residual modulation. The precision knob γ_ℓ controls the trade-off between fidelity-oriented restoration and prior-guided structural correction. Training uses a temporally coherent synthetic degradation curriculum that composes morphing, directional motion blur, grid-based warping, frame dropping, and spatiotemporal downsampling with smooth parameter evolution. Only adapter parameters are optimized using a noise-prediction diffusion loss, while the backbone remains frozen. Inference uses a CausVid-distilled student backbone for 4-step generation with γ_ℓ=0.4 for prior-guided correction or 1.0 for precise restoration.

## Key Results
- Achieves up to 37% relative FIQA improvement on faces in AIGC videos compared to baselines
- Strong gains in GPT-based perceptual metrics for aesthetic and multi-aspect video quality
- Maintains competitive full-reference scores (PSNR, SSIM, LPIPS, DISTS) on traditional restoration benchmarks
- Practical throughput of 13 FPS at 720p resolution on a single A100 GPU

## Why This Works (Mechanism)

### Mechanism 1: Residual Modulation via Precision Knob (γ_ℓ)
The scalar precision knob enables controllable trade-offs between fidelity-oriented restoration and prior-guided structural correction. A lightweight adapter DiT processes degraded-video tokens and injects features into alternating backbone blocks via residual modulation. Small γ_ℓ preserves the frozen T2V prior; larger γ_ℓ increases fidelity to degraded input while enabling stronger correction.

### Mechanism 2: Temporally Coherent Synthetic Degradation Curriculum
Training on composed, temporally smooth degradations aligns the diffusion prior toward realistic structural failures without introducing synthetic flicker. The degradation module composes temporal morphing, directional motion blur with smoothly varying parameters, grid-based spatial warping, stochastic frame dropping, and spatio-temporal downsampling. All temporal parameters evolve via low-frequency trajectories to avoid flicker.

### Mechanism 3: Frozen Backbone + Trainable Adapter Separation
Freezing the T2V DiT backbone preserves large-scale generative priors while the adapter learns degradation-specific conditioning. Only adapter parameters are optimized via noise-prediction loss, while the backbone remains frozen. At inference, a timestep-distilled student backbone can replace the teacher without retraining the adapter.

## Foundational Learning

- **Diffusion Denoising Process**: CreativeVR operates in latent space, predicting noise ε given noisy latent z_t, timestep t, and conditioning c. Why needed: The model uses standard diffusion denoising in video latent space. Quick check: Can you explain why the noise-prediction loss optimizes only the adapter while the backbone stays frozen?

- **Video VAE Latent Spaces**: Input videos are encoded to latents z = E(x), degraded latents ˜z = E(˜x) become conditioning tokens c = P(˜z). Why needed: The model operates entirely in compressed latent space. Quick check: What happens to temporal information when a video VAE compresses 49 frames into latents, and why does this matter for adapter conditioning?

- **Parameter-Efficient Fine-Tuning (Adapters/LoRA)**: The adapter adds ~50% of backbone blocks but trains only φ, not θ; residual injection follows adapter design patterns. Why needed: The model uses adapter-based conditioning for efficiency. Quick check: How does residual modulation differ from concatenation-based conditioning, and what trade-offs does it introduce?

## Architecture Onboarding

- **Component map**: Video → VAE Encoder → latent z (clean) / ˜z (degraded) → Patch Embedder → conditioning tokens c → Adapter DiT → features a_ℓ → Residual injection → Backbone DiT → Denoised latent ˆz → VAE Decoder → restored video ˆx

- **Critical path**: Training: clean x → synthetic degradation → ˜x → encode both → adapter conditions on ˜z → backbone denoises z_t → loss on ε prediction. Inference: degraded ˜x → encode → adapter conditions on ˜z → distilled backbone samples in 4 steps → decode

- **Design tradeoffs**: Precision knob γ_ℓ: Default 0.4 for prior-guided correction (AIGC), 1.0 for precise VR. Degradation strength: Strong yields best FIQA gains (+37% relative) but may reduce PSNR on pixel-faithful benchmarks. Adapter depth: Half the backbone blocks; adding more may improve correction but increases training cost.

- **Failure signatures**: Stabilized artifacts: Model sharpens textures but preserves warped geometry → γ_ℓ too small or degradation curriculum too weak. Identity drift: Corrected output changes semantics → γ_ℓ too low or prior too dominant. Temporal flicker: Frame-to-frame inconsistency → degradation parameters not smooth enough or adapter insufficiently conditioned on temporal context.

- **First 3 experiments**: 
  1. Precision knob sweep: Run inference on AIGC54 with γ_ℓ ∈ {0.1, 0.4, 0.7, 1.0}. Log FIQA scores and GPT-based perceptual scores.
  2. Degradation ablation: Train three adapter variants (Light, Medium, Strong) and evaluate on REDS30 (PSNR/SSIM) and AIGC54 (FIQA).
  3. Backbone compatibility: Train adapter on teacher backbone, then run inference with both teacher (50 steps) and CausVid student (4 steps). Compare quality drop and throughput gain.

## Open Questions the Paper Calls Out

### Open Question 1
Can the optimal setting for the precision knob (γ_ℓ) be estimated dynamically from the input video rather than being set manually? The paper introduces a scalar "precision knob" to trade off between fidelity and correction but relies on fixed values chosen based on the task type. A mechanism that predicts γ_ℓ per video or per-frame without manual tuning would be valuable.

### Open Question 2
Does the hand-crafted synthetic degradation module fully cover the distribution of structural failures in modern AIGC videos? While effective, the synthetic curriculum is an approximation; there is no analysis proving that these specific augmentations are sufficient to model the latent space errors of diverse generators like SORA2 or Veo3.

### Open Question 3
Does CreativeVR maintain temporal consistency when applied to videos significantly longer than the 49-frame training clips? The paper evaluates on 5-second clips and does not explicitly demonstrate or ablate performance on longer sequences where error accumulation or context drift might occur.

## Limitations
- The synthetic degradation module, while effective, may not fully capture all types of structural failures in modern AIGC generators
- The model's performance on videos longer than the 49-frame training clips is not explicitly validated
- Access to the Wan2.1 checkpoint and CausVid-distilled student weights is required for full reproduction

## Confidence
- Wan2.1 checkpoint access and architecture: Low
- CausVid-distilled student weights and recipe: Low
- Degradation preset hyperparameters (Light/Medium/Strong): Medium

## Next Checks
1. Verify that degradation parameters evolve smoothly over time using sinusoidal/Perlin-noise trajectories rather than abrupt changes
2. Test that adapter injection at alternating layers maintains temporal consistency across 49-frame sequences
3. Confirm that γ_ℓ=0.4 produces better FIQA scores on AIGC54 while γ_ℓ=1.0 produces better PSNR on REDS30