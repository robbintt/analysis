---
ver: rpa2
title: 'In-Context Curiosity: Distilling Exploration for Decision-Pretrained Transformers
  on Bandit Tasks'
arxiv_id: '2510.00347'
source_url: https://arxiv.org/abs/2510.00347
tags:
- test
- pretraining
- curiosity
- training
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of improving generalization in
  Decision-Pretrained Transformers (DPTs) when applied to out-of-distribution (OOD)
  reinforcement learning environments. DPTs, trained on offline trajectories via supervised
  learning, often fail to generalize beyond their pretraining data distribution, especially
  when the data is biased or lacks exploration.
---

# In-Context Curiosity: Distilling Exploration for Decision-Pretrained Transformers on Bandit Tasks

## Quick Facts
- arXiv ID: 2510.00347
- Source URL: https://arxiv.org/abs/2510.00347
- Authors: Huitao Yang; Guanting Chen
- Reference count: 20
- Primary result: PPT improves OOD generalization on bandits when pretrained on biased data

## Executive Summary
Decision-Pretrained Transformers (DPTs) excel at in-context learning from offline trajectories but struggle with out-of-distribution (OOD) generalization, particularly in exploration-heavy tasks like multi-armed bandits. This paper proposes in-context curiosity—a lightweight regularizer inspired by exploration—to improve DPT robustness. The Prediction-Powered Transformer (PPT) framework augments DPT with an auxiliary reward predictor, using prediction error as an intrinsic curiosity signal during pretraining. Experiments on Gaussian bandits show PPT mitigates performance degradation when test environments have higher reward variance, especially with biased training data.

## Method Summary
The method trains two transformer models: a policy model (π_θ) and a predictor model (q_φ). The predictor estimates expected rewards from partial trajectories, while the policy uses both historical actions/rewards and predictor outputs as context. During pretraining, the policy loss includes a curiosity bonus based on the predictor's squared error, encouraging exploration of uncertain arms. This creates a regularization effect that prevents overfitting to biased datasets. The framework is tested on Gaussian multi-armed bandits where test environments may have higher variance than training environments.

## Key Results
- PPT shows improved robustness compared to DPT when test environments have higher reward variance
- The advantage is most pronounced when pretraining data is biased or lacks exploration
- Moderate values of the curiosity coefficient λ (100-500) provide the best trade-off between stability and exploration

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Converting prediction error into an intrinsic curiosity signal may encourage broader exploration during offline training, mitigating the "memorization" of biased trajectories.
- **Mechanism:** An auxiliary predictor model ($q_\phi$) estimates expected rewards. The squared error between these predictions and ground-truth (or proxy) rewards defines a curiosity vector $E_j$. This vector weights the policy gradient, effectively rewarding the policy for prioritizing actions where the predictor is uncertain or wrong.
- **Core assumption:** High prediction error correlates with states/actions that are under-explored or informative, and "visiting" them in the training objective improves generalization.
- **Evidence anchors:**
  - [abstract] "...using prediction error as an intrinsic curiosity signal to encourage broader exploration during training."
  - [section 3] Defines curiosity as $curiosity(a) := \|ER(a) - \hat{\mu}(a)\|^2$ and integrates it into the loss.
  - [corpus] Neighbor "In-Context Learning for Pure Exploration" supports the link between active hypothesis testing and bandit exploration, though not this specific mechanism.
- **Break condition:** If the pretraining data is extremely sparse or the predictor converges too quickly (zero error), the curiosity signal vanishes, rendering the mechanism inert.

### Mechanism 2
- **Claim:** Augmenting the policy context with predicted reward estimates ($c_{1:j}$) allows the transformer to condition decisions on a latent "understanding" of the environment structure rather than just historical tokens.
- **Mechanism:** During both training and inference, the predictor's output is concatenated to the history $D_j$. This effectively acts as a learned value function query, providing the policy with a summary statistic of arm quality that purely observational history might lack (especially in biased data).
- **Core assumption:** The predictor generalizes faster or more robustly than the policy alone, and provides a useful inductive bias.
- **Evidence anchors:**
  - [section 3] "This modification allows the policy to condition not only on past actions and rewards, but also on predicted outcomes..."
  - [abstract] Notes PPT augments DPT with an auxiliary reward predictor.
  - [corpus] Weak direct evidence in neighbors for this specific concatenation strategy; primarily derived from paper text.
- **Break condition:** If the predictor is inaccurate (high bias), the policy receives misleading context, potentially degrading performance compared to a standard DPT.

### Mechanism 3
- **Claim:** A weighted curiosity bonus ($\lambda$) in the pretraining objective regularizes the policy against overfitting to the mode of the pretraining distribution.
- **Mechanism:** The loss function includes $-\lambda \cdot \langle E_j, \pi_\theta \rangle$. This mathematically penalizes the policy for ignoring high-error arms. By tuning $\lambda$, one controls the trade-off between imitating the dataset (NLL loss) and seeking uncertainty (curiosity bonus), preventing the "collapse" common in expert-biased datasets.
- **Core assumption:** The optimal balance between exploitation (dataset imitation) and exploration (curiosity) can be found via a static scalar $\lambda$.
- **Evidence anchors:**
  - [section 3] Equation 7 explicitly shows the weighted curiosity bonus term.
  - [section 4] "Moderate values improve robustness... Excessively large $\lambda$ values may destabilize training."
  - [corpus] Weak evidence; neighbors focus on algorithm distillation rather than this specific regularization technique.
- **Break condition:** If $\lambda$ is too high, the policy ignores rewards and explores randomly; if too low, it behaves like standard DPT.

## Foundational Learning

- **Concept: Decision-Pretrained Transformers (DPT)**
  - **Why needed here:** This is the base architecture the paper modifies. You must understand that DPTs are trained via supervised learning (next-token prediction) on offline RL trajectories, not via reward maximization.
  - **Quick check question:** Does a standard DPT maximize cumulative reward during pretraining? (Answer: No, it minimizes negative log-likelihood of actions in the dataset).

- **Concept: Distribution Shift in Offline RL**
  - **Why needed here:** The core problem is that DPTs fail when test environments (e.g., high variance) differ from training environments. The "tricky" dataset experiment specifically tests this fragility.
  - **Quick check question:** Why does a DPT trained on "expert-only" data fail when deployed in a noisy environment? (Answer: It memorized the expert policy without learning to explore or handle uncertainty).

- **Concept: Gaussian Multi-Armed Bandits**
  - **Why needed here:** This is the paper's experimental sandbox. Unlike deep RL, bandits isolate the exploration-exploitation trade-off without complex state dynamics.
  - **Quick check question:** In a Gaussian bandit, what does the variance $\sigma^2$ represent in the context of this paper? (Answer: The noise level of rewards, which creates the OOD challenge during testing).

## Architecture Onboarding

- **Component map:** Predictor Model ($q_\phi$) -> Policy Model ($\pi_\theta$) -> Input Context ($D_j$)
- **Critical path:**
  1. **Dataset Gen:** Create "Tricky" (biased) and "Ideal" (exploratory) datasets of bandit interactions.
  2. **Predictor Train:** Train $q_\phi$ to minimize MSE against true mean rewards (or proxies).
  3. **Policy Train:** Train $\pi_\theta$ using NLL + Curiosity Bonus (Equation 7). This is the crucial divergence from standard DPT.
  4. **OOD Eval:** Deploy on environments with variance $\sigma^2_{test} > \sigma^2_{train}$.
- **Design tradeoffs:**
  - **Compute:** PPT costs ~2x DPT training time (two models).
  - **Data:** Requires ground-truth mean rewards ($c^*$) for optimal performance, though empirical proxies work (Appendix A.3.2).
  - **Hyperparameter:** $\lambda$ (curiosity weight) is sensitive. Range [100, 500] suggested.
- **Failure signatures:**
  - **Collapse to Random:** $\lambda$ too high; policy under-exploits high-reward arms.
  - **DPT-like Failure:** $\lambda$ too low; no exploration benefit.
  - **OOD Washout:** In very high variance tests ($\sigma^2 > 0.5$), PPT's advantage diminishes (Figure 4).
- **First 3 experiments:**
  1. **Sanity Check (Ideal Data):** Run PPT vs. DPT on "Ideal" data. Verify PPT doesn't degrade performance significantly.
  2. **Stress Test (Tricky Data):** Run on "Tricky" (biased) data. Plot regret curves as $\sigma^2_{test}$ increases. You should see PPT's curve rise slower than DPT's.
  3. **Ablation on $\lambda$:** Sweep $\lambda$ (e.g., 0, 100, 500) on the Tricky dataset to find the "moderate value" that balances stability and exploration.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the in-context curiosity framework be adapted to handle uncertainty arising from state transitions in full reinforcement learning environments?
- **Basis in paper:** [explicit] The authors state, "A natural next step is to move beyond the bandit setting and adapt the framework to in-context reinforcement learning scenarios with state transitions."
- **Why unresolved:** The current prediction error metric works for bandits (reward prediction), but extending it to sequential tasks requires capturing uncertainty in environment dynamics, which the current architecture does not address.
- **What evidence would resolve it:** Successful application of the PPT framework to standard RL benchmarks (e.g., Atari or MuJoCo) where the model demonstrates robust exploration despite incomplete state-transition data.

### Open Question 2
- **Question:** Can principled strategies or adaptive schemes be developed to dynamically adjust the curiosity coefficient $\lambda$ during training?
- **Basis in paper:** [explicit] The authors note, "A particularly important improvement may lie in developing principled strategies for selecting the curiosity coefficient $\lambda$, or even devising adaptive schemes that adjust it dynamically during training."
- **Why unresolved:** Currently, $\lambda$ is a fixed hyperparameter; incorrect fixed values can lead to under-exploration or training instability.
- **What evidence would resolve it:** An algorithm that adjusts $\lambda$ based on live training metrics (e.g., prediction loss variance) and consistently outperforms the best fixed-$\lambda$ baseline.

### Open Question 3
- **Question:** Can the reliance on ground-truth reward information be fully replaced by empirical estimators without compromising the exploration capabilities of the model?
- **Basis in paper:** [explicit] The authors suggest, "It's possible to relax the reliance on ground-truth arm means by employing approximations such as empirical averages... systematic investigation of proxy contexts is left as future work."
- **Why unresolved:** Preliminary results using proxy contexts were promising but restricted to limited "ideal" and "tricky" datasets; robustness across diverse data distributions is unknown.
- **What evidence would resolve it:** Rigorous experiments demonstrating that models trained solely on trajectory-derived empirical averages achieve generalization performance statistically indistinguishable from those trained with oracle ground-truth rewards.

## Limitations
- Narrow experimental scope - only tested on stationary, tabular bandit problems
- Sensitivity to hyperparameters - the curiosity weight λ requires careful tuning
- Predictor dependency - performance critically depends on access to ground-truth reward means or reliable proxies

## Confidence
- Claims about in-context curiosity improving OOD generalization: **High confidence** for Gaussian bandit setting
- Claims about mechanism generalizing to broader RL environments: **Medium confidence**
- Claims about practical applicability without ground-truth rewards: **Low confidence**

## Next Checks
1. Test PPT on non-stationary bandits where reward distributions change over time
2. Evaluate performance when using only empirical reward estimates (no ground-truth means) across the full range of λ values
3. Compare against standard exploration strategies (e.g., Thompson sampling, UCB) in the bandit setting