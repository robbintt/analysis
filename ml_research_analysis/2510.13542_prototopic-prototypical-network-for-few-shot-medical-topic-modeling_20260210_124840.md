---
ver: rpa2
title: 'ProtoTopic: Prototypical Network for Few-Shot Medical Topic Modeling'
arxiv_id: '2510.13542'
source_url: https://arxiv.org/abs/2510.13542
tags:
- topic
- topics
- prototopic
- diversity
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ProtoTopic, a prototypical network-based
  framework for few-shot topic modeling on medical abstracts. The model addresses
  the challenge of generating coherent and diverse topics in domains with limited
  training data, such as healthcare.
---

# ProtoTopic: Prototypical Network for Few-Shot Medical Topic Modeling

## Quick Facts
- arXiv ID: 2510.13542
- Source URL: https://arxiv.org/abs/2510.13542
- Reference count: 40
- Key outcome: ProtoTopic achieves higher coherence (e.g., 0.5754 vs 0.5137) and diversity (e.g., 86.1% vs 49.6%) than LDA and BERTopic on PubMed200k RCT dataset.

## Executive Summary
ProtoTopic is a prototypical network-based framework for few-shot topic modeling on medical abstracts. The model addresses the challenge of generating coherent and diverse topics in domains with limited training data, such as healthcare. ProtoTopic leverages two text embedding models (PubMedBERT and all-MiniLM-L6-v2), K-means clustering to generate pseudo-labels, and a prototypical network to learn topic representations. The framework then uses class-based TF-IDF to extract representative keywords for each topic. Evaluated on the PubMed200k RCT dataset, ProtoTopic significantly outperforms two baseline models (LDA and BERTopic) in terms of topic coherence and diversity across 25, 50, and 100 topics. ProtoTopic achieves higher coherence scores (e.g., 0.5754 vs 0.5137 for BERTopic at 25 topics) and greater topic diversity (e.g., 86.1% vs 49.6% for BERTopic at 25 topics). Qualitative analysis confirms that ProtoTopic generates more specific and interpretable keywords compared to baselines. The results demonstrate ProtoTopic's effectiveness in producing high-quality, medically relevant topics even with limited data, highlighting the potential of prototypical networks for few-shot topic modeling in healthcare NLP.

## Method Summary
ProtoTopic uses medical abstracts as input, which are preprocessed by removing non-alphabet characters, lowercasing, tokenizing, and removing stopwords. The abstracts are embedded using either PubMedBERT (768-dim) or all-MiniLM-L6-v2 (384-dim). K-means clustering generates pseudo-labels by grouping embeddings into K clusters (where K equals the desired number of topics). A prototypical network is then trained using these pseudo-labels: prototypes are computed as mean embeddings per class, and query points are assigned to topics based on softmax probabilities over negative distances to prototypes. The network fine-tunes the last two layers of the embedding model during training. Finally, class-based TF-IDF extracts the top-25 keywords per topic for interpretability. The model is evaluated using C_v coherence and topic diversity metrics at 25, 50, and 100 topics.

## Key Results
- ProtoTopic achieves coherence scores of 0.5754 (PubMedBERT) and 0.5684 (all-MiniLM-L6-v2) at 25 topics, outperforming BERTopic's 0.5137.
- ProtoTopic shows higher topic diversity (86.1% for PubMedBERT, 85.7% for all-MiniLM-L6-v2) compared to BERTopic's 49.6% at 25 topics.
- ProtoTopic's keywords are more specific and interpretable than baseline models, with examples like "diabetic ketoacidosis" and "psychotic disorder" appearing in top keywords.
- ProtoTopic demonstrates robustness to varying numbers of topics, maintaining high performance across 25, 50, and 100 topics.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prototype-based distance metrics in embedding space enable few-shot topic discovery.
- Mechanism: Documents are embedded into vector space; prototypes are computed as class centroids (mean of support set embeddings). Query documents are assigned to topics based on distance to nearest prototype using softmax over negative distances.
- Core assumption: Documents sharing semantic content cluster near common prototypes in embedding space, and this geometric proximity meaningfully corresponds to topic similarity.
- Evidence anchors:
  - [abstract] "Prototypical networks are efficient, explainable models that make predictions by computing distances between input datapoints and a set of prototype representations, making them particularly effective in low-data or few-shot learning scenarios."
  - [section III.B] "We then determined the classes of the query set... by finding the closest prototype to each point embedding... This allowed us to define a probability that a query point belonged to a given class using the softmax of the distances."
  - [corpus] Weak/no direct corpus evidence for prototypical networks in topic modeling; related work uses prototypical networks for classification (ProSeNet, ProtoryNet) but not topic discovery.
- Break condition: If embedding space lacks semantic structure (e.g., poorly pretrained encoders), distance-to-prototype will not correlate with topic coherence.

### Mechanism 2
- Claim: K-means pseudo-labels bootstrap supervised learning in an otherwise unsupervised task.
- Mechanism: K-means clustering on embeddings generates pseudo-labels (cluster assignments). These pseudo-labels define support/query splits and provide a supervisory signal for prototypical network training.
- Core assumption: K-means captures a reasonable initial topic structure that can be refined through prototypical network training.
- Evidence anchors:
  - [abstract] Implied by framework description; not explicitly detailed.
  - [section III.B] "Following embedding generation, we applied K-means to the embeddings to cluster them into distinct groups... The output of this step is a set of centroids where each document is assigned to a single centroid... These centroids were used as pseudo-labels for training the proposed prototypical network."
  - [corpus] No corpus evidence for pseudo-label bootstrapping in topic modeling; common in semi-supervised learning but unvalidated for this specific application.
- Break condition: If K-means initialization is poor (e.g., inappropriate K, bad cluster geometry), pseudo-labels will misguide prototype learning.

### Mechanism 3
- Claim: Iterative fine-tuning of embedding models improves topic separation.
- Mechanism: During prototypical network training, the last two layers of PubMedBERT/all-MiniLM-L6-v2 are fine-tuned to minimize the negative log-probability of correct class assignments, iteratively improving embeddings for topic discrimination.
- Core assumption: The prototype-based loss provides a meaningful gradient signal that improves embeddings specifically for topic modeling (not just classification).
- Evidence anchors:
  - [abstract] Implied; not explicitly stated.
  - [section III.B] "During this process, the PubMedBERT/all-MiniLM-L6-v2 transformers were iteratively fine-tuned to improve the quality of text embeddings using the computed prototype representations from different steps."
  - [corpus] No corpus evidence; fine-tuning embeddings for topic modeling is not widely validated in related literature.
- Break condition: If fine-tuning overfits to pseudo-labels (which may be noisy), embeddings may degrade on held-out documents or novel topics.

## Foundational Learning

- Concept: **Prototypical Networks**
  - Why needed here: Core architecture; computes class prototypes as support set means and classifies via distance metrics.
  - Quick check question: Can you explain how a prototype is computed from a support set and how classification is performed for a query point?

- Concept: **Text Embeddings (Transformer-based)**
  - Why needed here: PubMedBERT and all-MiniLM-L6-v2 generate the vector representations used throughout the pipeline.
  - Quick check question: What is the difference between domain-specific embeddings (PubMedBERT) and general-purpose embeddings (all-MiniLM-L6-v2), and why might domain-specific embeddings help in medical text?

- Concept: **Topic Coherence and Diversity Metrics**
  - Why needed here: Primary evaluation metrics; C_v coherence measures semantic relatedness of topic keywords, diversity measures uniqueness across topics.
  - Quick check question: If a model achieves high coherence but low diversity, what does that indicate about the topics?

## Architecture Onboarding

- Component map:
  - **Input**: Medical abstracts (preprocessed: lowercase, remove non-alphabet, tokenize)
  - **Embedding Layer**: PubMedBERT (768-dim, domain-specific) OR all-MiniLM-L6-v2 (384-dim, general-purpose)
  - **Pseudo-label Generator**: K-means clustering on embeddings (K = number of topics)
  - **Prototypical Network**: Computes prototypes from support sets, assigns queries via distance
  - **Keyword Extractor**: Class-based TF-IDF (c-TF-IDF) extracts representative keywords per topic
  - **Output**: Topic assignments + top-N keywords per topic

- Critical path:
  1. Preprocess abstracts (remove non-alphabet, lowercase, tokenize)
  2. Generate embeddings via frozen transformer (last 2 layers unfrozen during training)
  3. Run K-means to generate pseudo-labels
  4. Train prototypical network in episodes (5 groups, 5 support, 5 query per episode; 50 episodes/epoch, 10 epochs)
  5. Assign final topic labels via trained prototypical network
  6. Extract keywords via c-TF-IDF

- Design tradeoffs:
  - PubMedBERT vs. all-MiniLM-L6-v2: Domain specificity vs. computational efficiency; PubMedBERT slightly better at low topic counts, comparable at higher counts.
  - Number of topics (K): Higher K increases coherence (topics more specific) but may decrease diversity (more keyword overlap). ProtoTopic starts with high diversity, which decreases as K increases.
  - Freezing vs. fine-tuning: Only last 2 layers fine-tuned to balance adaptation and stability.

- Failure signatures:
  - **Low coherence, low diversity**: Model failed to learn meaningful structure; check embedding quality, K-means initialization, or data preprocessing.
  - **High coherence, low diversity**: Topics too similar; likely K too low or embedding space collapse.
  - **Low coherence, high diversity**: Topics distinct but semantically incoherent; may indicate noisy pseudo-labels or overfitting to support sets.
  - **Training instability**: Loss not converging; check learning rate (0.00005 used), support/query split balance, or gradient issues in fine-tuning.

- First 3 experiments:
  1. **Baseline replication**: Run ProtoTopic (PubMedBERT) on PubMed200k RCT with K=25, 50, 100 topics. Measure C_v coherence and diversity. Compare to reported values (e.g., 0.5754 coherence, 86.1% diversity at K=25).
  2. **Embedding ablation**: Run ProtoTopic with all-MiniLM-L6-v2 vs. PubMedBERT at fixed K. Analyze coherence/diversity differences and qualitative keyword specificity.
  3. **Pseudo-label sensitivity**: Vary K-means initialization (random vs. k-means++) and number of clusters. Measure impact on final coherence/diversity to assess robustness to pseudo-label quality.

## Open Questions the Paper Calls Out

- **Open Question 1**: Does ProtoTopic's performance generalize to clinical validation, where experts assess topic utility?
  - Basis in paper: [explicit] The authors state that "the fidelity of the topics and topic keywords generated by the model could be evaluated through a user study by a clinician."
  - Why unresolved: Current evaluation relies solely on automated metrics ($C_V$ coherence) and the authors' own qualitative analysis, lacking external human validation.
  - What evidence would resolve it: Results from a user study where clinicians rate the interpretability and specificity of ProtoTopic keywords against baselines.

- **Open Question 2**: Can density-based clustering methods like HDBSCAN improve the generation of pseudo-labels compared to K-means?
  - Basis in paper: [explicit] The paper notes that "different clustering techniques other than K-means can also be explored," specifically citing BERTopic's use of HDBSCAN.
  - Why unresolved: The current framework relies exclusively on K-means clustering to generate the pseudo-labels required for training the prototypical network.
  - What evidence would resolve it: A comparative ablation study measuring coherence and diversity when K-means is replaced by HDBSCAN in the pipeline.

- **Open Question 3**: Would modifying the loss function to enforce tighter clusters improve topic separation?
  - Basis in paper: [explicit] The authors suggest that "additional loss terms are introduced [in other works] to ensure tight clusters and prototypes which are spaced out."
  - Why unresolved: ProtoTopic currently uses the standard loss function defined by Snell et al. [9], without terms specifically penalizing prototype overlap.
  - What evidence would resolve it: Performance metrics comparing the current standard loss against a modified loss function including inter-cluster separation penalties.

## Limitations
- Evaluation relies solely on automated metrics ($C_V$ coherence) and the authors' own qualitative analysis, lacking external human validation.
- The framework uses K-means clustering to generate pseudo-labels, which may be sensitive to initialization and may not capture complex cluster structures.
- The paper does not explore alternative loss functions that could enforce tighter clusters and better topic separation.

## Confidence
- High: ProtoTopic significantly outperforms baseline models (LDA and BERTopic) on PubMed200k RCT dataset in terms of coherence and diversity.
- Medium: The use of prototypical networks for few-shot topic modeling is novel but lacks extensive validation in related literature.
- Low: The impact of alternative clustering methods (e.g., HDBSCAN) and modified loss functions on performance is not explored.

## Next Checks
1. Run ProtoTopic with PubMedBERT on PubMed200k RCT at K=25, 50, 100 topics and verify coherence scores match reported values (0.5754, 0.5684, 0.5441).
2. Compare c-TF-IDF keyword extraction quality between ProtoTopic and BERTopic on a sample of topics to validate qualitative claims.
3. Test ProtoTopic with K-means++ initialization vs. random initialization to assess sensitivity to pseudo-label quality.