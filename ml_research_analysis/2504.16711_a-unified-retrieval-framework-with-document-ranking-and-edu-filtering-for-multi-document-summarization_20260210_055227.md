---
ver: rpa2
title: A Unified Retrieval Framework with Document Ranking and EDU Filtering for Multi-document
  Summarization
arxiv_id: '2504.16711'
source_url: https://arxiv.org/abs/2504.16711
tags:
- retrieval
- document
- ranking
- summarization
- edus
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a unified retrieval framework for multi-document
  summarization (MDS) that addresses context-length constraints by combining document
  ranking and EDU filtering. The method uses salient elementary discourse units (EDUs)
  as latent queries to guide document ranking, then filters out irrelevant EDUs instead
  of truncating tokens.
---

# A Unified Retrieval Framework with Document Ranking and EDU Filtering for Multi-document Summarization

## Quick Facts
- **arXiv ID:** 2504.16711
- **Source URL:** https://arxiv.org/abs/2504.16711
- **Reference count:** 40
- **Primary result:** Achieves up to +1.85 ROUGE-1 on Wikisum and +1.11 on WCEP-10 in PRIMERA by combining document ranking and EDU filtering

## Executive Summary
This paper introduces ReREF, a unified retrieval framework for multi-document summarization that overcomes context-length limitations by integrating document ranking and EDU (elementary discourse unit) filtering. The method uses salient EDUs as latent queries to guide document ranking, then filters out irrelevant EDUs rather than truncating tokens. An iterative Expectation-Maximization algorithm refines EDU salience scores and document relevance rankings. Evaluated across four MDS datasets with diverse summarizers, the framework consistently improves ROUGE scores and human-annotated informativeness and fluency. Ablation studies confirm both EDU filtering and document ranking are essential for performance.

## Method Summary
The ReREF framework addresses token-length constraints in multi-document summarization by using salient EDUs as latent queries for document ranking. The method employs an iterative Expectation-Maximization algorithm that alternates between refining EDU salience scores based on reference summary similarity and updating document relevance rankings. After ranking, irrelevant EDUs are filtered out rather than truncating tokens, preserving semantic coherence. The approach is evaluated across four MDS datasets with five different summarizer architectures in both fully supervised and few-shot settings, demonstrating consistent ROUGE score improvements over baselines including BM25 and DYLE.

## Key Results
- Achieves up to +1.85 ROUGE-1 improvement on Wikisum and +1.11 on WCEP-10 with PRIMERA summarizer
- Human evaluation shows significant gains in informativeness and fluency on WCEP-10
- Outperforms BM25 and DYLE in both query selection and ranking accuracy
- Ablation studies demonstrate both EDU filtering and document ranking components are essential for performance

## Why This Works (Mechanism)
The framework works by treating salient EDUs as latent queries that capture semantic content more precisely than document-level representations. By filtering irrelevant EDUs instead of truncating tokens, the method preserves the most informative discourse units while maintaining context length constraints. The iterative EM algorithm refines both the selection of salient EDUs and the ranking of relevant documents, creating a mutually reinforcing process that progressively improves retrieval quality. This approach addresses the fundamental challenge of context-length limitations in MDS while maintaining semantic coherence.

## Foundational Learning
- **Elementary Discourse Units (EDUs)**: Smallest semantic units in discourse parsing that preserve coherent meaning; needed for precise content selection rather than document-level truncation
- **Latent Query Generation**: Using EDU salience scores to create implicit search queries; needed because explicit queries may miss relevant content
- **Expectation-Maximization Algorithm**: Iterative optimization framework that alternates between estimating latent variables (EDU salience) and updating parameters (document rankings); needed for joint refinement
- **Cosine Similarity for Salience Scoring**: Measures semantic similarity between EDUs and reference summaries; needed to identify content relevant to the target summary
- **Document Ranking with EDU-based Queries**: Ranks documents based on their EDU relevance to the latent query; needed to prioritize semantically relevant documents
- **EDU Filtering vs Token Truncation**: Selectively removing irrelevant discourse units rather than cutting off text; needed to preserve semantic coherence while meeting length constraints

## Architecture Onboarding

**Component Map:** DMRST Parser -> EDU Segmentation -> Salience Scoring -> Document Ranking -> EDU Filtering -> Summarization

**Critical Path:** EDU Segmentation → Salience Scoring → Document Ranking → EDU Filtering

**Design Tradeoffs:** Iterative EM refinement provides better accuracy but increases computational cost versus single-pass alternatives; EDU filtering preserves semantics but requires reliable segmentation; joint learning of segmentation and retrieval could improve robustness but adds complexity

**Failure Signatures:** Poor segmentation quality propagates to retrieval errors; reference summary bias excludes valid content; computational overhead limits scalability; iterative refinement may converge slowly on noisy data

**First Experiments:**
1. Test EDU segmentation quality on noisy real-world documents to identify failure modes
2. Compare single-pass EDU filtering against iterative refinement to isolate EM contribution
3. Evaluate performance degradation when synthetic noise is introduced into EDU boundaries

## Open Questions the Paper Calls Out
### Open Question 1
- **Question:** How can the computational efficiency of the iterative EM algorithm and EDU segmentation be optimized for real-time, large-scale summarization scenarios?
- **Basis in paper:** The authors state in the conclusion, "In future work, we aim to enhance our framework’s efficiency when it is applied to large-scale, real-world multi-document summarization scenarios."
- **Why unresolved:** The proposed method relies on an external parser (DMRST) and iterative Expectation-Maximization steps, which are computationally more intensive than standard truncation or single-pass retrievers like BM25
- **What evidence would resolve it:** A study measuring latency and throughput on massive datasets, potentially introducing a non-iterative approximation or integrated segmentation to reduce runtime

### Open Question 2
- **Question:** To what extent does the accuracy of the external DMRST parser constrain the performance of ReREF, and could joint learning of segmentation and retrieval improve robustness?
- **Basis in paper:** Section 3.3 notes the reliance on the DMRST parser for EDU segmentation, treating it as a fixed preprocessing step rather than a learnable component, which risks propagating segmentation errors into the retrieval ranking
- **Why unresolved:** The paper assumes reliable segmentation, but in noisy real-world data, segmentation errors could break the semantic coherence of the "latent queries"
- **What evidence would resolve it:** An ablation study analyzing performance degradation when synthetic noise is introduced into EDU boundaries, or a comparison with an end-to-end differentiable segmentation approach

### Open Question 3
- **Question:** Does relying on reference-summary similarity for training data creation (EDU scoring) introduce a "coverage bias" that prevents the retrieval of salient information absent from the gold summary?
- **Basis in paper:** Section 4.3 describes creating the ground truth for EDU salience by calculating cosine similarity between EDUs and the *reference summary*
- **Why unresolved:** By definition, reference summaries may exclude relevant details due to length constraints; training the retriever to prioritize only content similar to the reference may penalize the retrieval of other valid, salient facts
- **What evidence would resolve it:** A human evaluation of the "relevance" of filtered-out EDUs that were dissimilar to the reference summary to determine if valid information is being discarded

## Limitations
- Computational overhead from iterative EM algorithm and external parser limits scalability for large-scale scenarios
- Reliance on reference-summary similarity for training may introduce coverage bias, excluding valid but non-overlapping content
- Limited human evaluation scope (single dataset, no inter-annotator agreement reported) reduces reliability of subjective assessments

## Confidence

**Confidence Labels:**
- Unified retrieval + EDU filtering improving ROUGE scores: **High**
- Gains in human evaluation (informativeness/fluency): **Medium** (due to limited scope and missing agreement metrics)
- Superiority over BM25 and DYLE: **Medium** (dependent on specific datasets and evaluation protocols)
- Generalizability across summarizer types: **Low** (narrow range of tested architectures)

## Next Checks
1. Replicate human evaluation on a second MDS dataset with explicit reporting of inter-annotator agreement
2. Compare the iterative refinement pipeline against a single-pass EDU filtering baseline to isolate the contribution of iteration
3. Benchmark runtime and query generation cost for large-scale or streaming document collections