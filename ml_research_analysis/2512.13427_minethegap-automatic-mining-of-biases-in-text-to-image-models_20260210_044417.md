---
ver: rpa2
title: 'MineTheGap: Automatic Mining of Biases in Text-to-Image Models'
arxiv_id: '2512.13427'
source_url: https://arxiv.org/abs/2512.13427
tags:
- prompts
- prompt
- bias
- images
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MineTheGap, a method for automatically discovering
  text prompts that induce biased outputs in text-to-image models. The approach uses
  a genetic algorithm driven by a novel bias score that measures the gap between generated
  images and LLM-generated textual variations.
---

# MineTheGap: Automatic Mining of Biases in Text-to-Image Models

## Quick Facts
- **arXiv ID**: 2512.13427
- **Source URL**: https://arxiv.org/abs/2512.13427
- **Reference count**: 40
- **Primary result**: MineTheGap automatically discovers text prompts that induce biased outputs in text-to-image models, validated on known biases with 0.72 Spearman correlation.

## Executive Summary
MineTheGap introduces a method for automatically discovering text prompts that induce biased outputs in text-to-image models. The approach uses a genetic algorithm driven by a novel bias score that measures the gap between generated images and LLM-generated textual variations. This score identifies biases by comparing the distribution of images to plausible interpretations of the prompt, without requiring predefined bias categories. The method is validated on a dataset with known biases, achieving a Spearman correlation of 0.72 with ground-truth occupational demographics. Across four TTI models, MineTheGap consistently uncovers prompts that elicit strong, model-specific biases, outperforming random sampling. User studies confirm that mined prompts are perceived as more biased than random ones. The approach is flexible, capable of targeting specific bias types, and highlights limitations in current TTI models' diversity and fairness.

## Method Summary
MineTheGap employs a genetic algorithm to iteratively refine a pool of text prompts that expose biases in text-to-image models. The core mechanism is a bias score that quantifies the gap between the distribution of generated images and LLM-generated text variations of the prompt. For each candidate prompt, the TTI model generates N images and the LLM generates N text variations. Both sets are embedded using CLIP, and their similarity matrix reveals whether the model under-explores the semantic space. The genetic loop selects top-scoring prompts, mutates them via LLM, injects random candidates, and repeats for a fixed number of iterations. Lower scores indicate higher bias. The method does not require predefined bias categories and can target specific bias types by design.

## Key Results
- MineTheGap achieves 0.72 Spearman correlation with ground-truth occupational demographics on a known bias dataset
- Mined prompts consistently elicit stronger, model-specific biases than random prompts across four TTI models (SD 1.4, 2.1, 3, FLUX.1 Schnell)
- User studies confirm mined prompts are perceived as more biased than random prompts
- The approach is flexible and can target specific bias types without predefined categories

## Why This Works (Mechanism)

### Mechanism 1: Genetic Algorithm Drives Search Toward High-Bias Prompts
The iterative selection of high-bias prompts produces populations with progressively lower bias scores. A population of b prompts is ranked by bias score; top-s prompts are mutated via LLM; random candidates are injected; the cycle repeats for fixed iterations. Lower scores indicate higher bias. The core assumption is that the prompt space contains exploitable structure such that mutations of biased prompts yield other biased prompts. Evidence includes the abstract stating the method "leverages a genetic algorithm to iteratively refine a pool of prompts, seeking for those that expose biases" and the section 3 description of the optimization loop. A break condition occurs if the bias score is noisy or non-stationary across seeds/models, causing convergence to stall.

### Mechanism 2: Bias Score Captures Semantic Distribution Gaps via CLIP Similarity
The method compares image embeddings to text-variation embeddings to identify prompts where the model under-explores the semantic space. The process generates N images from prompt p and N LLM text variations, embeds both via CLIP, constructs an NxN similarity matrix S, and computes missed visual concepts and least-aligned images scores using α-percentile formulations. The core assumption is that LLM-generated text variations approximate the "intended" human-expected distribution of interpretations. Evidence includes the abstract's description of "comparing the distribution of generated images to the distribution of LLM-generated text variations of the prompt" and the full derivation in section 4.1. A break condition occurs if CLIP embeddings do not distinguish semantically meaningful differences for certain concepts, or if LLM variations are themselves skewed.

### Mechanism 3: Random Injection Prevents Premature Convergence
Injecting r random LLM-generated prompts per iteration maintains diversity and avoids local optima. At each iteration, after selecting top-s prompts and generating m mutations each, remaining population slots (r = b - s×m) are filled with fresh random prompts from the LLM. The core assumption is that the random candidate distribution is sufficiently broad to occasionally seed new high-bias regions. Evidence includes section 3's explicit statement about injecting random prompts and the pipeline diagram in figure 3. A break condition occurs if the LLM's random-prompt distribution is biased toward certain semantic clusters, preventing true exploration.

## Foundational Learning

- **Concept: Genetic algorithms (selection, mutation, crossover alternatives, fitness functions)**
  - Why needed here: The core mining loop is a GA operating over discrete prompt space; understanding selection pressure and population dynamics is essential to debug convergence.
  - Quick check question: Can you explain why fitness-proportionate selection can prematurely converge, and how rank-based or tournament selection mitigates this?

- **Concept: Vision-language joint embeddings (CLIP/SigLIP) and cosine similarity**
  - Why needed here: The bias score is computed in a shared embedding space; misalignment between text and image embeddings can silently corrupt rankings.
  - Quick check question: If CLIP similarity saturates near 1.0 for all pairs, what happens to the bias score's discriminative power?

- **Concept: Text-to-image diffusion sampling and classifier-free guidance (CFG)**
  - Why needed here: CFG affects diversity vs. fidelity; the paper uses CFG variation to validate the bias score (higher CFG → more biased outputs).
  - Quick check question: What is the expected effect of increasing CFG on sample diversity and why does that matter for bias detection?

## Architecture Onboarding

- **Component map**: Initial population -> Selection -> Mutation -> Random injection -> Bias scoring -> Next population
- **Critical path**: TTI model → image batch → CLIP image embeddings; prompt → LLM variations → CLIP text embeddings → similarity matrix → bias score → selection → next population. Latency dominated by TTI generation and LLM calls.
- **Design tradeoffs**:
  - Population size (b) vs. compute: Larger populations find lower-loss prompts faster but increase per-iteration cost (Fig. S4)
  - α percentile: Lower α enforces stricter coverage but yields noisier scores; higher α is more permissive (Fig. S5)
  - LLM choice: Different LLMs produce similar mined-prompt distributions (Fig. S19–S20), but prompt diversity may vary
- **Failure signatures**:
  - Scores plateau early: Population converged; check if random injection is too small or mutation is too conservative
  - All prompts cluster semantically: LLM prior bias; inspect meta-prompts or switch LLM
  - Bias score uncorrelated with human judgment: Check CLIP embedding quality; consider alternative vision-language models
- **First 3 experiments**:
  1. Sanity check on color task (App. A): Run the provided synthetic red/blue/green optimization to verify GA convergence logic works in your environment
  2. Ablate α on held-out prompts: Compare α = 15, 25, 35 against user-study judgments to calibrate sensitivity; expect peak correlation near 25th percentile (Fig. S5)
  3. Cross-model validation: Mine prompts for SD 1.4, evaluate on SD 2.1/3 and FLUX; confirm each model scores lowest on its own mined prompts (Fig. 9)

## Open Questions the Paper Calls Out

- **Open Question 1**: To what extent does the inherent bias of the Large Language Model (LLM) used to generate text variations skew the "ground truth" distribution, potentially causing MineTheGap to miss biases where the LLM and the text-to-image model share the same blind spots?
  - Basis: The authors explicitly list as a limitation: "it relies on an LLM to approximate the target distribution, which itself might be biased."
  - Why unresolved: The method assumes the LLM provides a valid diverse distribution of interpretations. If the LLM is biased (e.g., always generating male variations for "doctor"), the metric will not penalize the TTI model for generating only male images.
  - What evidence would resolve it: A comparative analysis where MineTheGap is run using multiple different LLMs with varying known biases to generate variations, measuring the variance in the resulting bias scores and mined prompts.

- **Open Question 2**: How does the sensitivity of the vision-language embedding model (CLIP) limit the detection of specific semantic biases, such as spatial relationships or fine-grained textures?
  - Basis: The authors note that measuring the gap "using CLIP similarity" restricts the analysis "to features it is sensitive to."
  - Why unresolved: If CLIP embeddings fail to differentiate between subtle semantic concepts (e.g., "standing on" vs. "standing near"), the similarity matrix will indicate high alignment, resulting in a low bias score even if the visual concepts are semantically distinct.
  - What evidence would resolve it: A validation study focusing on "hard negatives" (concepts CLIP struggles to distinguish) to determine if the bias score correlates with human judgment in these specific failure modes.

- **Open Question 3**: Can the specific "missed visual concepts" identified by the mining process be effectively utilized as a feedback mechanism to fine-tune or debias TTI models without causing catastrophic forgetting or reducing image quality?
  - Basis: The conclusion states that the method "should lead to further understanding of how to train and use the model when aiming for results that are both fair and beneficial."
  - Why unresolved: While the paper successfully demonstrates detection and mining, it does not implement or validate a mitigation strategy using the discovered data.
  - What evidence would resolve it: Experiments showing that augmenting training data with the missed visual concepts leads to a measurable increase in the bias score (lower bias) and improved diversity in future generations.

## Limitations
- The bias score relies on LLM-generated text variations as a proxy for human expectations, which may be flawed if the LLM itself is biased
- The method is validated only on four specific TTI models (SD 1.4, 2.1, 3, FLUX.1 Schnell) and may not generalize to other architectures
- The fixed iteration budget (30) and population parameters are not justified by convergence analysis, raising questions about whether global optima are reached

## Confidence
- **Genetic algorithm iteratively discovers low-bias-score prompts**: Medium
- **Bias score identifies semantically meaningful gaps**: Medium
- **User study confirms human judgment aligns with scores**: High
- **Method is flexible and model-specific**: High

## Next Checks
1. Ablate α percentile: Systematically vary α (e.g., 15th, 25th, 35th percentiles) on a held-out prompt set and measure correlation with human bias ratings. Expect peak correlation near the 25th percentile used in the paper, but verify robustness.
2. Cross-model transfer: Mine prompts for SD 1.4, evaluate bias scores on SD 2.1, SD 3, and FLUX. Compare model-specificity (each model should score lowest on its own mined prompts) and generalizability.
3. LLM variation quality audit: Manually inspect a sample of LLM-generated text variations for semantic coherence and bias. If variations are systematically skewed (e.g., over-representing certain professions or genders), the bias score is measuring LLM bias, not TTI model bias.