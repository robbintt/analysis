---
ver: rpa2
title: Cross-Format Retrieval-Augmented Generation in XR with LLMs for Context-Aware
  Maintenance Assistance
arxiv_id: '2502.15604'
source_url: https://arxiv.org/abs/2502.15604
tags:
- system
- data
- information
- response
- maintenance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates a Retrieval-Augmented Generation (RAG) system
  that integrates large language models (LLMs) to support maintenance tasks by processing
  multi-format data sources (PDFs, CSVs, text files). Eight LLMs were tested on three
  query complexity scenarios, with performance measured by BLEU/METEOR accuracy scores,
  success rates, and response times.
---

# Cross-Format Retrieval-Augmented Generation in XR with LLMs for Context-Aware Maintenance Assistance

## Quick Facts
- arXiv ID: 2502.15604
- Source URL: https://arxiv.org/abs/2502.15604
- Reference count: 30
- Primary result: GPT-4 and GPT-4o-mini consistently outperform other LLMs in cross-format maintenance query resolution, achieving up to 93% success on complex multi-source queries.

## Executive Summary
This study presents a Retrieval-Augmented Generation (RAG) system that integrates large language models (LLMs) to support maintenance tasks by processing multi-format data sources (PDFs, CSVs, text files). Eight LLMs were tested on three query complexity scenarios, with performance measured by BLEU/METEOR accuracy scores, success rates, and response times. GPT-4 and GPT-4o-mini consistently outperformed other models, particularly in complex scenarios requiring cross-format data integration. The RAG system successfully delivered accurate responses in most cases, with success rates reaching 97-98% for simpler queries. However, challenges remain with complex queries and local-model deployment, as Llama models showed lower accuracy and higher hallucination rates. The results demonstrate the potential of RAG systems to optimize maintenance operations through efficient information retrieval and response generation across diverse data formats.

## Method Summary
The system evaluates eight LLMs (GPT-3.5, GPT-4, GPT-4o, GPT-4o-mini, Llama 2, Llama 3, Llama 3.1:8B, Llama 3.2:3B) on a RAG framework that processes PDFs (parsed with pypdf), CSVs (queried via DuckDB with LLM-generated SQL), and text files. The knowledge base summary is provided as JSON to the LLM, which generates subqueries mapped to specific knowledge bases via multi-path routing. Three evaluation scenarios test simple text queries (A), simple CSV queries (B), and complex multi-source queries (C), with 50 runs per model per scenario. Performance is measured by success rate, BLEU/METEOR scores, response time, and hallucination incidence.

## Key Results
- GPT-4o-mini achieved 93% success rate on complex multi-format queries, while Llama 2 scored only 2%
- Success rates reached 97-98% for simple queries but dropped significantly for complex cross-format scenarios
- Local models (Llama variants) showed higher hallucination rates (up to 8%) and lower accuracy compared to cloud-based GPT models
- Response times varied widely (3-31 seconds) depending on model and query complexity

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Cross-format retrieval enables unified querying across heterogeneous data sources (PDFs, CSVs, text files) without manual conversion.
- **Mechanism:** The system provides a JSON-formatted Knowledge Base Summary to the LLM, including column headers and sample rows for CSVs and parsed content for PDFs. The LLM then generates format-appropriate queries—SQL via DuckDB for structured data, semantic retrieval for unstructured text. This abstraction layer allows a single natural language query to trigger parallel retrievals from multiple sources.
- **Core assumption:** The LLM can reliably infer which data format and query syntax is appropriate given the knowledge base metadata and user intent.
- **Evidence anchors:** [abstract] "We assessed the performance of eight LLMs... particularly when addressing complex queries requiring multi-format data integration." [Section 3] "The current evaluation focuses on two primary file types: PDF... parsed using the pypdf Python library... CSV... employs the DuckDB Python library, which allows for SQL-like queries."

### Mechanism 2
- **Claim:** Multi-path routing decomposes complex queries into subqueries, enabling simultaneous retrieval from multiple knowledge bases.
- **Mechanism:** The LLM analyzes the input query and generates subqueries mapped to specific knowledge bases, outputting a structured JSON object. Each subquery is routed independently, and results are aggregated before response generation. This parallel execution reduces latency compared to sequential lookups and ensures comprehensive coverage for multi-part questions.
- **Core assumption:** The LLM can accurately partition complex queries into semantically coherent subqueries that align with available knowledge bases without losing contextual dependencies.
- **Evidence anchors:** [Section 3 - Multi-Path Retrieval] "The architecture utilizes LLMs to analyze input queries and generate subqueries corresponding to the available knowledge bases... The Multi-Path Retrieval module returns JSON files... submitted to the Cross-Format Retrieval module." [Section 4.3] "GPT-4o-mini scored the highest with a success rate of 93%... In contrast, the LLama models struggled significantly, with LLama 2 achieving a success rate of only 2%."

### Mechanism 3
- **Claim:** Retrieval grounding reduces hallucinations by constraining LLM outputs to retrieved evidence, though effectiveness varies by model capability.
- **Mechanism:** The RAG system retrieves relevant passages before generation, conditioning the LLM's output on this context rather than parametric knowledge alone. Models with stronger instruction-following and context-integration capabilities (GPT-4, GPT-4o-mini) achieve higher success rates and lower hallucination, while smaller local models (Llama variants) show higher rates of misalignment.
- **Core assumption:** Retrieved context is sufficiently relevant and complete to answer the query, and the model prioritizes this context over its pre-trained knowledge.
- **Evidence anchors:** [Section 4.3 - Results, Scenario C] "Llama2 and Llama3 models had a hallucination rate ranging up to 8%... Conversely, GPT-4o and GPT-4o-mini models generate verbose responses, resulting in a generally lower BLEU and METEOR score, despite containing the correct information." [Section 5] "These misalignments arise particularly in scenarios involving routine inquiries about common issues, as the model may attempt to draw from its generalized understanding rather than from the curated knowledge base."

## Foundational Learning

- **Concept: Retrieval-Augmented Generation (RAG)**
  - **Why needed here:** The entire system depends on grounding LLM responses in external knowledge bases rather than relying on parametric memory, which is critical for domain-specific maintenance procedures.
  - **Quick check question:** Can you explain why RAG reduces hallucination compared to pure LLM generation, and under what conditions it might still fail?

- **Concept: Structured vs. Unstructured Data Retrieval**
  - **Why needed here:** The system must handle both PDFs (unstructured text) and CSVs (structured tabular data), requiring different query strategies—semantic similarity vs. SQL-like filtering.
  - **Quick check question:** Given a user query asking for "all maintenance logs from January," which data format and retrieval method would be appropriate?

- **Concept: BLEU and METEOR Metrics**
  - **Why needed here:** The paper uses these metrics to quantify response accuracy; understanding their limitations (BLEU emphasizes n-gram overlap, METEOR captures semantic similarity) is essential for interpreting results.
  - **Quick check question:** Why might a response with a lower BLEU score still be considered correct by human evaluators?

## Architecture Onboarding

- **Component map:** Voice/image capture from XR device → Query Analysis → Multi-Path Router → Cross-Format Retrieval → Knowledge Base → Response Generator → XR display
- **Critical path:** Query → LLM subquery generation → JSON routing → Format-specific retrieval → Context aggregation → LLM response generation → XR display. Latency is dominated by LLM inference (3-31s depending on model) and retrieval complexity.
- **Design tradeoffs:**
  - **Cloud vs. Local Models:** GPT-4 offers superior accuracy (83-93% success on complex queries) but requires external data sharing; Llama models enable local deployment for data sovereignty but show significantly lower performance (2-12% success on complex queries).
  - **Verbosity vs. Precision:** GPT-4o models generate richer responses with extra relevant data, lowering BLEU scores but improving utility; stricter filtering could improve metric alignment at the cost of completeness.
  - **JSON as Interchange Format:** Universally parsable by LLMs, but adds serialization overhead and potential parsing errors for complex nested structures.
- **Failure signatures:**
  - **High hallucination rate (5-8%):** Indicates retrieval failure or model overriding context with parametric knowledge—common in Llama models on complex queries.
  - **Low BLEU with high METEOR:** Suggests correct semantic content but divergent phrasing—acceptable for utility, problematic for strict benchmarking.
  - **Partial answers:** Subquery decomposition may have missed a knowledge base or retrieval returned incomplete context.
  - **Timeouts on local models:** Resource constraints under high demand can cause response latency exceeding practical thresholds (>30s).
- **First 3 experiments:**
  1. **Baseline retrieval accuracy:** Run 50 queries per scenario (A/B/C) with GPT-4o-mini, measuring success rate, BLEU, METEOR, and latency. Confirm reproduction of paper's reported 93-98% success on simple queries, 93% on complex.
  2. **Failure mode analysis:** Manually inspect all incorrect/partial responses from Llama 3 on Scenario C to categorize failure types (retrieval miss, subquery fragmentation, hallucination). Use this to prioritize retrieval improvements.
  3. **Cross-format stress test:** Construct 20 queries requiring simultaneous PDF + CSV retrieval with conflicting or partial information. Measure how often models synthesize correctly vs. hallucinate or favor one source.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can an adaptive context-based filtering mechanism effectively prioritize specific operational knowledge bases over general LLM training data to reduce false positives?
- **Basis in paper:** [explicit] The Discussion section states that to mitigate misalignments where models rely on generalized understanding, "an adaptive context-based filtering mechanism is essential" to ensure the system prioritizes accurate retrieval.
- **Why unresolved:** Current state-of-the-art models often generate "false positives" by drawing from vast general training datasets rather than the curated, context-specific knowledge base, particularly for routine inquiries.
- **What evidence would resolve it:** A comparative study showing a statistically significant reduction in hallucination rates and false positives when the filtering mechanism is applied versus a baseline RAG system.

### Open Question 2
- **Question:** How can retrieval techniques be refined to maintain high accuracy and low latency when handling complex queries that require cross-format data integration?
- **Basis in paper:** [explicit] The Abstract notes that "Future research will focus on refining retrieval techniques for these models... particularly for intricate scenarios," a sentiment reiterated in the Conclusion regarding "refining the system's responsiveness."
- **Why unresolved:** The evaluation revealed a sharp decline in success rates for complex queries (Scenario C), especially for local models (e.g., Llama 2 dropping to 2% success), indicating current retrieval integration methods are insufficient for multi-source tasks.
- **What evidence would resolve it:** Demonstration of a modified RAG architecture where success rates for complex, cross-format queries (Scenario C) match or exceed the high benchmarks set for simple queries (Scenario A/B) across all tested models.

### Open Question 3
- **Question:** Can local-model deployments be optimized to match the response speed and comprehension accuracy of cloud-based models while maintaining data privacy?
- **Basis in paper:** [inferred] The Discussion highlights "Local-Model Concerns," noting that while local execution secures data, it results in "marked discrepancies in response accuracy and speed" and struggles with high-demand scenarios compared to cloud counterparts.
- **Why unresolved:** There is currently a forced trade-off between the data security of local execution and the superior performance of cloud-based APIs; the paper does not present a solution that achieves both.
- **What evidence would resolve it:** Benchmark results showing a locally deployed model achieving comparable BLEU/METEOR scores and sub-10-second response times in complex scenarios (similar to GPT-4o-mini) without external data transmission.

## Limitations
- Evaluation is limited to three predefined query scenarios with fixed runs, potentially missing real-world variability
- Heavy reliance on JSON metadata extraction assumes consistent and complete data in industrial environments
- Local model deployment shows significantly lower performance, raising scalability concerns for on-device XR applications
- Hallucination rates spike for complex queries in less capable models, indicating retrieval grounding effectiveness is model-dependent

## Confidence
- **High confidence:** Cross-format retrieval improves query accuracy over single-format systems; GPT-4 and GPT-4o-mini consistently outperform local models in complex multi-format queries.
- **Medium confidence:** RAG grounding effectively reduces hallucination for cloud models, but local models remain vulnerable; BLEU/METEOR metrics correlate imperfectly with real-world maintenance task success.
- **Low confidence:** The system's performance on highly ambiguous or adversarial queries is untested; long-term reliability under variable XR hardware and network conditions is unknown.

## Next Checks
1. **Cross-domain generalization test:** Apply the system to a different technical domain (e.g., medical equipment maintenance) using the same evaluation protocol to assess transfer of retrieval and generation capabilities.
2. **Ablation study on JSON metadata quality:** Systematically degrade knowledge base summaries (e.g., remove column headers, truncate samples) to quantify the impact on query decomposition accuracy and retrieval relevance.
3. **Real-time XR deployment pilot:** Integrate the RAG system into a live XR maintenance workflow, measuring end-to-end latency, user task completion time, and hallucination incidence under realistic operational constraints.