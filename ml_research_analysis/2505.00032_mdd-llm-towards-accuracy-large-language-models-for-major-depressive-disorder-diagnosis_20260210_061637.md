---
ver: rpa2
title: 'MDD-LLM: Towards Accuracy Large Language Models for Major Depressive Disorder
  Diagnosis'
arxiv_id: '2505.00032'
source_url: https://arxiv.org/abs/2505.00032
tags:
- data
- mdd-llm
- llms
- diagnosis
- proposed
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents MDD-LLM, a large language model-based framework
  for diagnosing major depressive disorder (MDD). The authors address the challenge
  of limited medical resources and complex diagnostic methods for MDD by fine-tuning
  LLMs on a large-scale UK Biobank dataset (274,348 records).
---

# MDD-LLM: Towards Accuracy Large Language Models for Major Depressive Disorder Diagnosis

## Quick Facts
- arXiv ID: 2505.00032
- Source URL: https://arxiv.org/abs/2505.00032
- Reference count: 0
- Primary result: 70B model achieves accuracy 0.8378 and AUC 0.8919 on UK Biobank MDD diagnosis task

## Executive Summary
This paper presents MDD-LLM, a framework that adapts large language models (LLMs) for Major Depressive Disorder (MDD) diagnosis using structured medical data. The approach transforms tabular patient records into natural language prompts, enabling LLMs to leverage their pre-trained medical knowledge for classification. Using the UK Biobank dataset of 274,348 records, the authors fine-tune Llama 3.1 models with LoRA, achieving state-of-the-art performance that significantly outperforms traditional machine learning methods while demonstrating robustness to missing data.

## Method Summary
The MDD-LLM framework converts structured medical data (15 features including demographics, clinical measures, and behavioral factors) into LLM-compatible prompts using a Text Template transformation method. The transformed data trains Llama 3.1 models (8B and 70B parameters) with LoRA fine-tuning (rank=8, alpha=16, learning rate 0.0003). The approach uses an 80/20 train/test split with 5-fold cross-validation on UK Biobank data. The model outputs binary predictions ("Yes"/"No" for MDD) with probability scores extracted from token likelihoods.

## Key Results
- 70B model achieves accuracy of 0.8378 and AUC of 0.8919 (95% CI: 0.8799 - 0.9040)
- Outperforms traditional ML methods (XGBoost, MLP) and deep learning approaches
- Demonstrates robustness to missing data with only slight performance decline when 60% of features are missing
- Provides interpretable explanations for predictions while maintaining high diagnostic accuracy

## Why This Works (Mechanism)

### Mechanism 1: Tabular-to-Prompt Transformation Activates Pre-trained Medical Knowledge
Converting structured medical data into natural language prompts allows LLMs to leverage pre-existing medical knowledge for diagnosis tasks. The transformation maps numerical/categorical features (age, BMI, sleep duration) into descriptive text sentences, activating reasoning pathways learned during pre-training to recognize patterns like "prolonged work conditions can increase likelihood of depression."

### Mechanism 2: LoRA Enables Parameter-Efficient Medical Domain Adaptation
Low-Rank Adaptation allows effective fine-tuning by updating a small parameter subset while preserving base model capabilities. LoRA injects trainable rank-decomposition matrices into attention layers, adapting the 70B parameter model for MDD diagnosis without full parameter updates (rank=8, alpha=16, learning rate 0.0003).

### Mechanism 3: Prior Knowledge Provides Missing Data Robustness
Pre-trained LLMs handle incomplete medical features better than traditional ML by leveraging encoded feature correlations. When features are missing, LLMs use learned correlations (e.g., BMI-depression links, work pressure-depression associations) to compensate, maintaining performance even with significant data incompleteness.

## Foundational Learning

- **Supervised Fine-Tuning (SFT) with Instruction-Response Format**: Understanding how to structure instruction-output pairs ("instruction": "Predict MDD?", "output": "Yes") for classification tasks. Why needed: Proper prompt formatting determines whether LLMs activate relevant medical reasoning pathways.
- **LoRA (Low-Rank Adaptation) Parameters**: Rank and alpha settings directly affect fine-tuning efficiency and model capacity. Why needed: These parameters control the trade-off between adaptation quality and computational resources.
- **Class Imbalance Metrics (AUC, F1, Sensitivity, Specificity)**: Dataset has 4.6% MDD positive rate; accuracy alone is misleading for evaluation. Why needed: Proper metrics are essential for evaluating minority class performance in imbalanced medical datasets.

## Architecture Onboarding

- **Component map**: UK Biobank Tabular Data → Tabular Transformation (Text/List/GPT Template) → Prompt Construction (instruction + input + expected output) → Tokenization (Llama 3.1 tokenizer) → Llama 3.1 Backbone (8B or 70B) + LoRA Adapters → Output: "Yes/No" + Probability Score
- **Critical path**: Text Template transformation quality directly determines whether the LLM activates relevant medical reasoning. Poorly formatted prompts cascade into degraded diagnosis accuracy.
- **Design tradeoffs**: LoRA vs QLoRA: LoRA trains faster (40 min vs 55 min) but uses 42% more memory (245 GB vs 172 GB); 8B vs 70B: 70B achieves higher accuracy (0.8378 vs 0.7904) at substantially higher infrastructure cost; Text vs GPT Template: Similar performance, but GPT Template incurs API costs.
- **Failure signatures**: Model outputs probability but no "Yes/No" → check prompt formatting alignment; Accuracy ~0.61 (baseline LLaMA3.1 performance) → transformation pipeline likely broken; GPU OOM during training → switch from LoRA to QLoRA.
- **First 3 experiments**: Validate transformation pipeline on 1K sample subset with List vs Text Template comparison before full training; Run LoRA rank ablation (rank 4/8/16) to verify rank=8 is optimal for this task; Replicate 60% missing feature test on held-out data to confirm robustness claims before deployment consideration.

## Open Questions the Paper Calls Out

- **Open Question 1**: Would replacing the general-purpose Llama 3.1 backbone with a medically specialized LLM significantly enhance diagnostic performance? Basis: Authors state further exploration is needed to assess impact of medical LLM replacement.
- **Open Question 2**: What specific mitigation strategies can effectively reduce hallucinations in the model's reasoning without degrading its predictive accuracy? Basis: Authors note hallucination in LLMs remains a challenge requiring attention.
- **Open Question 3**: Does the MDD-LLM framework maintain high accuracy and robustness when validated on external, multi-center cohorts outside the UK Biobank? Basis: Authors list collecting real-world and multi-center samples as a future objective.

## Limitations

- Data access barrier requiring formal UK Biobank application with specific ethics approvals
- Transformation template opacity due to missing Appendix S1 containing exact formatting rules
- Single dataset evaluation limiting generalizability to different populations and clinical settings

## Confidence

- **High Confidence**: LoRA fine-tuning effectiveness (strong quantitative evidence with memory/time comparisons), missing data robustness (direct experimental validation), traditional ML comparison (clear benchmark with specified architectures)
- **Medium Confidence**: Tabular-to-prompt transformation mechanism (conceptual explanation with ablation studies but lacks detailed implementation specifics), pre-trained medical knowledge activation (inferred from superior performance versus traditional ML), interpretability claims (method described but quality not systematically evaluated)
- **Low Confidence**: Generalization to clinical practice (no real-world deployment or clinical validation studies), feature importance stability (no feature ablation studies provided), cost-effectiveness claims (infrastructure requirements mentioned but no economic analysis)

## Next Checks

- **Check 1: Template Implementation Validation** - Recreate the Text Template transformation using only information in the paper and publicly available documentation. Generate 100 sample prompts and verify they follow the described format, comparing against any supplementary materials.
- **Check 2: Missing Data Robustness Replication** - Using the same UK Biobank dataset, systematically remove 0%, 20%, 40%, 60%, and 80% of features across different feature types. Measure performance degradation at each level and compare against reported results.
- **Check 3: Cross-Dataset Generalization Test** - Apply the fine-tuned model to an independent clinical dataset with similar features and MDD labels. Measure performance degradation and identify which features or patterns transfer well versus those requiring retraining.