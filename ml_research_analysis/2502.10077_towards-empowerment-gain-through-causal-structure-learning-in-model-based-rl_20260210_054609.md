---
ver: rpa2
title: Towards Empowerment Gain through Causal Structure Learning in Model-Based RL
arxiv_id: '2502.10077'
source_url: https://arxiv.org/abs/2502.10077
tags:
- uni00000013
- uni00000011
- uni00000014
- causal
- uni00000052
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes ECL, a framework that integrates empowerment
  with causal structure learning to improve controllability and learning efficiency
  in model-based reinforcement learning. The core idea is to use empowerment as an
  intrinsic motivation to guide exploration under learned causal structures, while
  simultaneously refining these structures through data collected during exploration.
---

# Towards Empowerment Gain through Causal Structure Learning in Model-Based RL

## Quick Facts
- **arXiv ID:** 2502.10077
- **Source URL:** https://arxiv.org/abs/2502.10077
- **Reference count:** 40
- **Primary result:** ECL integrates empowerment with causal structure learning to improve controllability and learning efficiency in model-based RL across 6 environments.

## Executive Summary
This paper introduces ECL, a framework that combines empowerment maximization with causal structure learning to improve exploration and generalization in model-based reinforcement learning. The core insight is that learning causal relationships between state variables and actions can reduce spurious correlations and improve controllability, while empowerment serves as an intrinsic motivation signal to guide exploration toward causally-relevant dimensions. ECL alternates between learning causal masks, optimizing empowerment-driven exploration policies, and training downstream task policies with curiosity-shaped rewards to prevent overfitting to incomplete causal models.

## Method Summary
ECL operates in three main steps: (1) learn dense dynamics and reward models from initial data, then discover a causal mask via constraint-based (CMI testing) or score-based (L1 regularization) methods; (2) optimize an empowerment policy to maximize the difference between empowerment under causal vs. dense models, alternating with causal mask refinement; (3) train a downstream task policy using CEM planning with a shaped reward that includes both task reward and a curiosity component based on prediction uncertainty differences between causal and dense models.

## Key Results
- ECL achieves higher sample efficiency and asymptotic performance than baseline causal MBRL methods across 6 environments
- Causal discovery accuracy reaches F1 > 0.97 and ROC AUC > 0.98 in chemical environments
- Curiosity reward prevents overfitting and improves out-of-distribution generalization
- ECL shows superior performance in both in-distribution and out-of-distribution settings

## Why This Works (Mechanism)

### Mechanism 1: Causal Mask Learning for State Abstraction
Learning causal structure reduces spurious correlations by masking irrelevant state dimensions during prediction. A binary adjacency matrix M is learned via conditional independence testing or sparse regularization, then applied via element-wise product to filter state-action inputs before prediction. This assumes the environment dynamics satisfy Markov and faithfulness conditions, enabling causal edges to be identified from conditional independence patterns.

### Mechanism 2: Empowerment Gain Maximization for Targeted Exploration
Maximizing the difference between empowerment under causal vs. dense dynamics models drives exploration toward causally-controllable state dimensions. The exploration policy maximizes E_ϕc(s|M) - E_ϕc(s), where empowerment measures mutual information between actions and next states. This assumes empowerment computed under correct causal structure better captures true controllability than dense model empowerment.

### Mechanism 3: Curiosity Reward for Causal Overfitting Mitigation
A curiosity-based intrinsic reward prevents downstream task policies from becoming over-conservative due to incomplete causal models. The shaped reward r = r_task + λ·r_cur includes r_cur computed as the difference between KL divergences of ground-truth dynamics from causal vs. dense models, encouraging visiting states where the causal model underfits but the dense model remains accurate.

## Foundational Learning

- **Concept: Structural Causal Models (SCMs) and Directed Acyclic Graphs (DAGs)**
  - Why needed here: The paper represents MDP dynamics as a causal graph G=(V,E) where nodes are state/action/reward variables and edges indicate causal influence. Understanding d-separation and conditional independence is essential for interpreting why the causal discovery methods work.
  - Quick check question: Given a DAG with edges s1_t → s2_{t+1} and a_t → s2_{t+1}, is s1_t conditionally independent of a_t given s2_{t+1}?

- **Concept: Empowerment as Mutual Information**
  - Why needed here: The core intrinsic motivation signal E = max_π I(s_{t+1}; a_t | s_t, M) measures the channel capacity from actions to future states. Computing/estimating mutual information gradients is central to implementing the exploration policy.
  - Quick check question: How does conditioning on causal mask M change the mutual information estimate compared to using all state dimensions?

- **Concept: Model-Based RL with Planning (e.g., CEM)**
  - Why needed here: The downstream task policy uses the Cross-Entropy Method for planning with the learned dynamics and reward models. Understanding trajectory sampling and elite selection is needed to debug task performance.
  - Quick check question: What happens to CEM planning if the learned dynamics model is systematically biased in certain regions of state space?

## Architecture Onboarding

- **Component map:**
  Dense Dynamics Model → Causal Mask Learner → Causal Dynamics Model → Reward Model → Empowerment Policy → Task Policy (CEM with Curiosity Reward)

- **Critical path:**
  1. Collect initial transitions with π_collect (diverse state-action coverage)
  2. Train dense dynamics model P_ϕc via L_dyn
  3. Learn causal mask M via L_c-dyn (constraint-based vs. score-based)
  4. Alternate: train π_e on empowerment gain, use collected transitions to refine M and reward model
  5. Train downstream π_θ via CEM with shaped reward

- **Design tradeoffs:**
  - Constraint-based vs. Score-based causal discovery: Constraint-based (ECL-Con) uses CMI thresholds; more interpretable but sensitive to threshold. Score-based (ECL-Sco) uses L1 regularization; smoother but requires tuning λ_M.
  - Empowerment horizon: Paper uses single-step empowerment; extending to multi-step would increase computational cost but may capture longer-range controllability.
  - Curiosity coefficient λ: High values encourage broader exploration; low values risk causal overfitting. Paper uses λ=1.

- **Failure signatures:**
  - Causal mask becomes all-zeros → empowerment gain collapses → no exploration progress
  - Reward model overfits to training distribution → poor generalization in OOD states
  - Curiosity reward dominates → policy ignores task reward, never converges
  - Dynamics prediction error grows with multi-step rollouts → CEM planning degrades

- **First 3 experiments:**
  1. Sanity check causal discovery: Run ECL-Con on Chemical Chain environment; verify learned mask matches ground truth (100% accuracy expected). If mismatch, debug CMI threshold.
  2. Ablate empowerment gain: Compare ECL with full empowerment gain objective vs. ECL with only causal model empowerment (no dense model subtraction). Expect lower sample efficiency.
  3. Stress-test curiosity reward: Vary λ ∈ {0, 0.5, 1, 2} on Chemical Full environment; plot episodic reward curves. Expect λ=1 to achieve best asymptotic performance; λ=0 to show overfitting; λ=2 to show slower convergence.

## Open Questions the Paper Calls Out

- **Disentangling Behavioral Dimensions:** ECL implicitly enhances controllability but does not explicitly tease apart different behavioral dimensions. The current method aggregates empowerment gains, which may fail to isolate specific skills necessary for complex manipulation tasks.

- **Non-stationary Environments:** The current framework does not account for changing dynamics where the underlying causal graph changes over time. The methodology assumes a static Factored MDP structure, limiting robustness in real-world scenarios where causal relationships shift during deployment.

- **Omitted Entropy Terms:** The empowerment maximization objective currently omits two entropy terms for simplification. It's unclear if this approximation compromises theoretical optimality or stability of the empowerment gain calculation.

- **Real-world Robotics Scaling:** The paper proposes leveraging pre-trained 3D or object-centric visual dynamics models to scale to real-world robotics, but current evaluations are restricted to simulation with specific distractors.

## Limitations
- Does not explicitly disentangle different behavioral dimensions or skills
- Cannot handle non-stationary environments where causal structure changes over time
- Omits entropy terms in empowerment objective for simplification, potentially affecting optimality
- Limited evaluation in real-world robotics scenarios

## Confidence
- **Empowerment-driven exploration mechanism:** Medium - theory and ablation support its role, but MI estimation details are underspecified
- **Causal mask discovery:** High - Table 1 shows near-perfect F1 and AUC scores in chemical domains
- **Downstream policy performance:** Medium - extensive experiments but results depend on both causal and empowerment components working together

## Next Checks
1. Verify causal mask learning on Chemical Chain with ground-truth comparison
2. Ablate empowerment gain by comparing full vs. simplified objectives
3. Sweep curiosity reward coefficient λ and measure overfitting vs. exploration trade-offs