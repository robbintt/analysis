---
ver: rpa2
title: A Framework for Benchmarking and Aligning Task-Planning Safety in LLM-Based
  Embodied Agents
arxiv_id: '2504.14650'
source_url: https://arxiv.org/abs/2504.14650
tags:
- safety
- embodied
- task
- arg1
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Safe-BeAl, a framework to systematically
  measure and mitigate safety hazards in LLM-based embodied agents. It includes SafePlan-Bench,
  a benchmark with 2,027 tasks across 8 hazard categories, and Safe-Align, a preference-optimization
  method that learns from error-prone actions to enhance safety.
---

# A Framework for Benchmarking and Aligning Task-Planning Safety in LLM-Based Embodied Agents

## Quick Facts
- arXiv ID: 2504.14650
- Source URL: https://arxiv.org/abs/2504.14650
- Reference count: 22
- Key outcome: Safe-BeAl framework improves safety by 8.55-15.22% over GPT-4-based agents while maintaining task success rates

## Executive Summary
This paper introduces Safe-BeAl, a framework for systematically measuring and mitigating safety hazards in LLM-based embodied agents. It includes SafePlan-Bench, a benchmark with 2,027 tasks across 8 hazard categories, and Safe-Align, a preference-optimization method that learns from error-prone actions to enhance safety. Experiments show Safe-BeAl significantly improves safety metrics while maintaining task success rates, addressing a critical gap in embodied AI safety evaluation.

## Method Summary
Safe-BeAl combines a comprehensive safety benchmark with a preference optimization method to enhance embodied agent safety. The framework formalizes safety through dual constraints (process-level and termination-level), generates diverse task scenarios using multi-agent role-playing, and aligns LLM behavior through atomic action preference optimization. The approach integrates with existing simulators like VirtualHome and provides a modular safety detector for identifying hazards across 8 categories.

## Key Results
- Safe-BeAl improves safety metrics by 8.55-15.22% compared to GPT-4-based agents
- Maintains task success rates with only 1-5% reduction while achieving significant safety improvements
- Baseline analysis reveals even GPT-4 exhibits unsafe behaviors in 6-7% of task executions
- Preference optimization effectively learns from error-prone actions to enhance safety

## Why This Works (Mechanism)

### Mechanism 1: Dual Safety Constraint Formalization
- Defining safety through two complementary constraints (process-level and termination-level) provides more complete hazard coverage than single-level approaches.
- Process Safety Constraints validate each atomic action against immediate hazards, while Termination Safety Constraints check the final environmental state for cumulative hazards. The safety verification function IsSafe(A, S) returns true only when both constraint types are satisfied.
- Assumption: Physical-world hazards can be decomposed into action-level and state-level violations that are detectable through rule-based mappings.
- Evidence: Framework encompasses safety hazards across 8 categories; formal specification of cproc(ai, si-1) and cterm(sn) constraints with IsSafe verification function.
- Break condition: If hazards emerge from complex action-state interactions that cannot be captured by independent constraint checks, or if the rule-based mappings miss novel hazard patterns not present in training data.

### Mechanism 2: Atomic Action Preference Optimization with Differential Weighting
- Treating each atomic action as an optimization unit while down-weighting identical prefix actions enables more precise learning of error-prone decision points.
- Safe-Align modifies the reward function to assign weight μ < 1 to identical action prefixes between safe/unsafe pairs, focusing optimization on the first divergent action onwards. This is combined with a target reward margin γ to ensure minimum separation between preferred and dispreferred trajectories.
- Assumption: Unsafe behaviors emerge from specific decision points rather than being distributed uniformly across action sequences, and these decision points can be identified through pair-wise comparison.
- Evidence: "learns from error-prone actions to enhance safety"; formal reward function with μ weighting for identical actions and β scaling.
- Break condition: If unsafe behaviors emerge from subtle interactions across multiple actions rather than identifiable decision points, or if the assumption that initial actions are typically safe does not hold for complex tasks.

### Mechanism 3: Hazard-Category-Guided Benchmark Construction
- Starting with expert-defined hazard categories and using multi-agent role-playing to generate diverse task scenarios produces more comprehensive safety coverage than bottom-up hazard discovery.
- Eight hazard categories are defined based on safety literature. Hazard seeds are collected by running existing agents and manually identifying failures. Multi-Agent Acting then uses role-specific prompts to generate diverse tasks via LLM dialogue, filtered by Rouge-L similarity to ensure novelty.
- Assumption: The eight hazard categories are comprehensive for household embodied tasks, and multi-agent role-playing produces task diversity comparable to real-world hazard distribution.
- Evidence: "2,027 daily tasks...across 8 distinct hazard categories"; detailed description of hazard category definition, seed construction, and Multi-Agent Acting with similarity filtering.
- Break condition: If the eight categories miss significant hazard types, or if LLM-generated tasks do not reflect real-world hazard frequency distributions.

## Foundational Learning

- **Preference Optimization (DPO/SimPO)**
  - Why needed here: Safe-Align builds directly on Direct Preference Optimization and Simple Preference Optimization, modifying their reward formulations.
  - Quick check question: Can you explain why DPO requires a reference model during training and how SimPO eliminates this requirement?

- **Embodied Task Planning Formalization**
  - Why needed here: The paper formalizes planning as D = ⟨S, O, P, A, T⟩ with transition models.
  - Quick check question: What is the difference between high-level task planning and low-level action execution in embodied AI systems?

- **Safety Constraint Specification**
  - Why needed here: The distinction between process safety (per-action) and termination safety (final state) is central.
  - Quick check question: For a "wash dishes" task, identify one process safety violation and one termination safety violation.

## Architecture Onboarding

- **Component map:**
  ```
  SafePlan-Bench -> SafeRisks Dataset -> Safety Detector
  ├── Hazard Categories (8) -> ActivityPrograms filtering -> Action→Hazard mappings
  ├── SafeRisks Dataset (2,027) -> Adversarial prompt generation -> State→Hazard mappings
  │   ├── Hazard Seeds -> Executability check -> VirtualHome integration
  │   └── Multi-Agent Acting
  └── Safety Detector
  Safe-Align
  ├── Training Data Construction
  │   ├── ActivityPrograms filtering
  │   ├── Adversarial prompt generation
  │   └── Executability check
  └── Preference Optimization
      ├── SFT phase (LoRA)
      └── Safe-Align phase (modified DPO)
  ```

- **Critical path:**
  1. Define hazard categories relevant to your deployment domain
  2. Build safety detector with action-state→hazard mappings
  3. Generate preference pairs by running baseline agents, identifying failures, and constructing safe/unsafe pairs
  4. Apply Safe-Align with μ ≈ 0.75 and validate on held-out safety benchmarks

- **Design tradeoffs:**
  - Safety vs task success: 1-5% SuccR reduction for 8-15% SafeR improvement—acceptable for safety-critical domains
  - Rule-based vs learned detectors: Paper uses rule-based for reliability; learned detectors may generalize better but inherit LLM biases
  - Hazard category granularity: 8 categories cover household tasks; industrial/medical domains need domain-specific categories

- **Failure signatures:**
  - High SafeR but low SuccR: Over-conservative alignment, model refusing valid actions
  - High SuccR but low SafeR on SafeRisks: Task-focused training without safety alignment
  - High termination violations, low process violations: Model handles immediate actions but fails at long-horizon state tracking

- **First 3 experiments:**
  1. Replicate safety evaluation on VirtualHome with the 8-category detector to establish baseline SafeR for your chosen LLM backbone
  2. Construct a small preference dataset (100-200 pairs) for your target domain and validate that Safe-Align improves SafeR without catastrophic SuccR drops
  3. Test generalization by evaluating on out-of-distribution tasks to assess whether safety knowledge transfers across hazard types

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the integration of multimodal sensory inputs impact the performance of Safe-Align compared to the current text-only SafeRisks dataset?
- Basis in paper: The Limitations section states the SafeRisks dataset focuses on a single modality, but real-world agents will receive multimodal information.
- Why unresolved: Current data generation and alignment pipeline relies on text-based LLM generation, lacking the complexity of visual grounding required for real-world sensory data.
- What evidence would resolve it: A multimodal extension of SafePlan-Bench and experiments showing Safe-Align's effectiveness when agents process visual scenes alongside natural language instructions.

### Open Question 2
- Question: Can more sophisticated error localization methods be developed to identify safety hazards in novel action sequences without strict dependence on ground-truth script comparisons?
- Basis in paper: The authors note that localizing errors within embodied agent trajectories is a complex issue, and the current approach provides only a preliminary solution based on deviation from ground truth.
- Why unresolved: The current approach detects errors primarily when the generated sequence deviates from the ground truth, potentially failing to detect novel, executable plans that are semantically unsafe but not present in the reference data.
- What evidence would resolve it: An updated alignment method capable of identifying unsafe atomic actions in "hallucinated" but executable plans without a ground-truth reference, evaluated against human safety judgments.

### Open Question 3
- Question: To what extent does the safety alignment provided by Safe-BeAl remain robust under adversarial attacks or specifically malicious instructions?
- Basis in paper: The analysis reveals unsafe behaviors "even in the absence of adversarial inputs or malicious intent," implying the framework's performance under attack is unexplored.
- Why unresolved: The training data construction uses "specially designed adversarial prompts" to generate unsafe samples for alignment, but does not test the model's robustness against external adversarial attacks trying to force unsafe actions.
- What evidence would resolve it: Evaluation metrics for Safe-Align models when subjected to standard adversarial attack benchmarks or maliciously crafted task instructions.

### Open Question 4
- Question: How effectively does the safety knowledge learned via Safe-Align transfer to diverse simulation environments or physical robotics platforms outside the VirtualHome simulator?
- Basis in paper: The paper integrates a "modular and extensible safety detector" specifically into the VirtualHome simulator to evaluate safety, leaving transferability to other environments an open issue.
- Why unresolved: The safety constraints and object-property mappings are defined specifically for the VirtualHome state representation, and it is unclear if the learned "physical-world safety knowledge" generalizes to different physics engines or real-world noise.
- What evidence would resolve it: Zero-shot or fine-tuned evaluation of a Safe-Align model in a distinct embodied AI simulator or physical robot demonstrations.

## Limitations

- Generalization beyond household domains remains untested; the 8 hazard categories are tailored to domestic environments
- Real-world applicability gap exists between controlled simulation and physical robot deployment with sensor noise and actuation delays
- Preference data quality dependency is critical, with current approach potentially missing subtle safety violations or introducing selection bias

## Confidence

- **High confidence**: The dual constraint formalization (process + termination safety) and its implementation are well-grounded and internally consistent
- **Medium confidence**: The atomic action preference optimization mechanism shows promising results but relies on assumptions about error distribution that may not hold universally
- **Medium confidence**: The benchmark construction methodology produces diverse tasks, but the extent to which Multi-Agent Acting generates realistic hazard distributions versus artificial scenarios remains uncertain

## Next Checks

1. **Cross-domain transfer validation**: Apply Safe-BeAl to a non-domestic domain (e.g., warehouse or laboratory setting) with domain-specific hazard categories to test framework adaptability and identify necessary modifications.

2. **Physical robot deployment test**: Implement the safety detector and Safe-Align method on a physical robot platform with real-world sensors to identify simulation-to-reality gaps and measure performance degradation.

3. **Long-horizon planning stress test**: Design evaluation tasks requiring 20+ sequential actions to stress-test whether the safety constraints maintain effectiveness over extended planning horizons and whether the preference optimization generalizes beyond short sequences.