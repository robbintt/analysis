---
ver: rpa2
title: 'ComplexFuncBench: Exploring Multi-Step and Constrained Function Calling under
  Long-Context Scenario'
arxiv_id: '2501.10132'
source_url: https://arxiv.org/abs/2501.10132
tags:
- function
- calling
- response
- query
- call
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ComplexFuncBench is a benchmark for evaluating complex function
  calling in large language models (LLMs) under long-context scenarios. It features
  multi-step and constrained function calling requiring long-parameter filing, parameter
  value reasoning, and 128k context length.
---

# ComplexFuncBench: Exploring Multi-Step and Constrained Function Calling under Long-Context Scenario

## Quick Facts
- arXiv ID: 2501.10132
- Source URL: https://arxiv.org/abs/2501.10132
- Reference count: 40
- Key outcome: New benchmark reveals state-of-the-art LLMs struggle with complex function calling under long-context scenarios, with parameter value errors dominating failure modes.

## Executive Summary
ComplexFuncBench introduces a benchmark for evaluating complex function calling in large language models under long-context scenarios. The benchmark focuses on multi-step and constrained function calling requiring long-parameter filing, parameter value reasoning, and 128k context length extraction. Using an automatic evaluation framework called ComplexEval, experiments show that state-of-the-art LLMs struggle significantly with parameter value errors, constrained reasoning, and long-context extraction, with success rates ranging from 9.8% to 61.0% across different models.

## Method Summary
ComplexFuncBench consists of 1,000 samples across five real-world domains requiring multi-step function calls to 43 real-time Booking.com APIs. The benchmark includes 600 single-domain and 400 cross-domain samples with an average of 3.26 steps and 5.07 calls per task. An automatic evaluation framework, ComplexEval, was developed using multi-dimensional matching (rule-based, response-based, and LLM-based) to quantitatively assess complex function calling. The framework includes format checking, Hungarian mapping for call pairing, and response evaluation using GPT-4o to score completeness and correctness.

## Key Results
- Claude-3.5-Sonnet achieved 61.0% success rate while Qwen2.5-72B only reached 9.8% on the benchmark
- Parameter value errors dominated failure modes, accounting for 78.8% of Qwen2.5-72B's errors
- Models averaged 1-3 extra steps beyond the shortest path, with 19-21% stopping prematurely before completing all required calls
- Filter and slug parameters showed the highest error rates (>40%) due to difficulty understanding parameter descriptions and inter-parameter relationships

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-dimensional matching improves evaluation accuracy for semantically equivalent function calls.
- Mechanism: ComplexEval combines rule-based exact matching, response-based matching (comparing API responses), and LLM-based matching (handling synonyms like "NY" vs "New York") to determine equivalence without requiring identical parameter strings.
- Core assumption: LLMs can reliably judge semantic equivalence of parameter expressions when provided with function descriptions and conversation history.
- Evidence anchors:
  - [abstract] "we propose an automatic framework, ComplexEval, for quantitatively evaluating complex function calling tasks"
  - [Section 3.1] "Multi-Dimensional Matching... allows for different expressions of parameter values"
  - [corpus] Related work (LongFuncEval, DICE-BENCH) similarly addresses long-context function calling evaluation but uses different matching strategies.
- Break condition: Fails when LLM-based matching produces inconsistent equivalence judgments across semantically similar calls.

### Mechanism 2
- Claim: Disambiguation during annotation creates single valid function calling paths, enabling consistent benchmark evaluation.
- Mechanism: Annotators remove overlapping API responses that could lead to multiple valid subsequent calls (e.g., keeping only one location ID for a city), ensuring each sample has one unambiguous ground-truth path.
- Core assumption: Real-world API responses can be pruned without losing evaluation validity for the targeted capabilities.
- Evidence anchors:
  - [Section 2.1.2] "disambiguate the calling parameter values, we will delete API responses that may cause parameter ambiguity"
  - [Figure 2(c)] Visual example showing removal of ambiguous location IDs
  - [corpus] Weak corpus evidence on disambiguation specifically; most related benchmarks focus on task coverage rather than path uniqueness.
- Break condition: Fails when pruned responses no longer reflect realistic API behavior, potentially overfitting models to simplified scenarios.

### Mechanism 3
- Claim: Error-specific API feedback enables evaluation of self-correction capabilities in function calling.
- Mechanism: Format errors return specific error messages (e.g., "parameter should be string, got float"), while invalid calls return general errors, allowing models to attempt correction based on feedback specificity.
- Core assumption: Models can parse error messages and adjust subsequent calls appropriately.
- Evidence anchors:
  - [Section 3.1] "This approach allows the model to undergo a trial-and-error process, and we can evaluate the model's self-correction ability"
  - [Figure 3] Shows error response flow with specific vs. general error messages
  - [corpus] "On the Robustness of Agentic Function Calling" examines error recovery but under different conditions.
- Break condition: Fails if models cannot interpret error messages or if error messages leak ground-truth information.

## Foundational Learning

- Concept: **Parameter value reasoning from implicit constraints**
  - Why needed here: The benchmark requires inferring parameter values from constraints like "day before Christmas Eve 2024" or "1 hour after landing" rather than explicit values. This was the dominant error source (78.8% value_error rate for Qwen2.5-72B).
  - Quick check question: Given "book a taxi to Times Square 1 hour after landing" and a flight arriving at 13:00, can you compute the correct pick_up_time parameter?

- Concept: **Multi-step function call dependencies**
  - Why needed here: Tasks require 3.26 average steps with 5.07 average calls, where later calls depend on earlier API responses (e.g., getting a flight token before calling Get_Flight_Details). Models averaged 1-3 extra steps beyond the shortest path.
  - Quick check question: If Search_Flights returns tokens d7699 and d7740, which token should be passed to Get_Flight_Details to find baggage allowances for the cheapest option?

- Concept: **Long-context parameter extraction**
  - Why needed here: Parameters like `legs`, `filter`, and `token` can approach 600 characters, extracted from 128k context. Error analysis shows 38.1% failure on legs parameters and 40%+ failure on filter parameters.
  - Quick check question: From a 100KB API response containing multiple flight options with embedded tokens, can you locate and correctly copy a 200-character token string?

## Architecture Onboarding

- Component map:
  Function Set (43 APIs) -> ComplexFuncBench Dataset (1,000 samples) -> ComplexEval Framework -> Error Classification

- Critical path:
  1. Load query + function list → Model generates function calls
  2. Format validation → Return specific error or proceed
  3. Hungarian mapping pairs predicted calls to golden calls via embedding similarity
  4. Multi-dimensional matching (rule → response → LLM) determines equivalence
  5. Feed API response or error message → Loop until final response
  6. GPT-4o evaluates response completeness (0-2) and correctness (0-2)

- Design tradeoffs:
  - **Disambiguation vs. realism**: Pruning API responses ensures single valid paths but may not reflect real API variability
  - **LLM-based matching vs. consistency**: GPT-4o matching handles synonyms but introduces evaluation variance
  - **Shortest path annotation vs. alternative valid paths**: Only shortest path is labeled correct, potentially penalizing valid but longer solutions

- Failure signatures:
  - **value_error dominant (78.8%)**: Model extracts wrong values from context or misreasons about constraints
  - **stop_early (19-21%)**: Model generates response before completing all required calls
  - **filter/slug parameters**: 40%+ error rates suggest difficulty understanding parameter descriptions and inter-parameter relationships

- First 3 experiments:
  1. Baseline evaluation: Run target model on ComplexFuncBench with greedy sampling, max_tokens=2048, tool_choice="auto" to establish Success Rate and Call Accuracy per domain
  2. Error stratification: Classify failures by error type and parameter category (date, location, filter, legs, token, slug) to identify specific weakness patterns
  3. Ablation on matching method: Compare rule-only vs. response-based vs. full LLM-based matching to quantify evaluation sensitivity to matching criteria

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can LLMs be optimized to reduce parameter value errors, which constitute the majority of failures in complex function calling tasks?
- Basis in paper: [explicit] Section 4.3.1 notes that `value_error` accounts for 78.8% of errors in Qwen2.5-72B and is the primary failure mode across all tested models, citing challenges in constrained reasoning and long-context extraction.
- Why unresolved: The paper identifies the specific error types (e.g., inferring `filter` and `legs` parameters) but does not propose architectural or training solutions to fix the reasoning deficits.
- What evidence would resolve it: A modification to the training pipeline or model architecture that demonstrably lowers the `value_error` rate on the ComplexFuncBench dataset.

### Open Question 2
- Question: Can complex function calling evaluation frameworks be adapted to handle ambiguous API responses with multiple valid paths?
- Basis in paper: [inferred] Section 2.1.2 explicitly states that ambiguous API responses were manually removed during the "Disambiguation" phase to ensure a single valid path, acknowledging that real-world scenarios often contain the very ambiguities the benchmark excludes.
- Why unresolved: The current evaluation metric (Success Rate) relies on a single annotated "shortest path," making it unable to quantitatively assess models that might validly choose alternative API outcomes.
- What evidence would resolve it: An extension of ComplexEval that supports probabilistic or multi-path validation without requiring manual disambiguation of the API responses.

### Open Question 3
- Question: What mechanisms are required to prevent models from terminating function calls prematurely ("stop_early")?
- Basis in paper: [explicit] Section 4.3.1 identifies "stop_early" as a significant error mode, with Claude-3.5-Sonnet and GPT-4o stopping prematurely in roughly 20% of cases before gathering all required information.
- Why unresolved: The authors report the frequency of the issue but do not determine if the cause is a failure in planning, context window saturation, or misinterpreting the "completeness" of intermediate API data.
- What evidence would resolve it: Analysis of attention weights or a specialized reward model that penalizes final responses generated before all query constraints are satisfied by API data.

## Limitations
- The disambiguation approach may create an artificial evaluation environment that doesn't fully reflect real-world API behavior where multiple valid responses exist
- The LLM-based matching (using GPT-4o) for parameter value equivalence introduces potential evaluation inconsistency across different judges
- The benchmark's controlled environment with pruned API responses may overfit models to simplified scenarios rather than realistic usage patterns

## Confidence
- **High Confidence**: The benchmark successfully identifies that current state-of-the-art models struggle with complex function calling tasks, particularly parameter value reasoning (78.8% value_error rate for Qwen2.5-72B) and constrained reasoning (e.g., "day before Christmas Eve" constraints).
- **Medium Confidence**: The specific error rates and performance gaps between models (Claude-3.5-Sonnet at 61.0% vs Qwen2.5-72B at 9.8%) are accurately measured within the benchmark's controlled environment, but may not generalize to all real-world scenarios.
- **Low Confidence**: The extent to which the benchmark's disambiguation approach affects model evaluation validity and whether the pruned API responses accurately represent realistic usage patterns.

## Next Checks
1. **Cross-Validation with Human Evaluation**: Have human annotators independently evaluate a subset of model outputs using the same success criteria to measure inter-annotator agreement and validate the consistency of the automated evaluation framework.

2. **Real-World API Integration Test**: Deploy models on actual Booking.com API endpoints without the benchmark's disambiguation pruning to measure performance degradation and identify whether the controlled environment overfits models to simplified scenarios.

3. **Error Recovery Capability Assessment**: Design a controlled experiment where models encounter specific error types (parameter format errors vs. invalid calls) and measure their ability to self-correct based on error feedback, validating the claimed benefit of the error-specific feedback mechanism.