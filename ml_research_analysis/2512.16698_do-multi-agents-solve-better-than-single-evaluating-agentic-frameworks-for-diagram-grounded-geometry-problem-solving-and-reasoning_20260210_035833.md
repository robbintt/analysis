---
ver: rpa2
title: Do Multi-Agents Solve Better Than Single? Evaluating Agentic Frameworks for
  Diagram-Grounded Geometry Problem Solving and Reasoning
arxiv_id: '2512.16698'
source_url: https://arxiv.org/abs/2512.16698
tags:
- line
- angle
- geometric
- point
- problem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically compares single-agent and multi-agent
  pipelines for diagram-grounded geometry problem solving across four benchmarks.
  Open-source models (Qwen-2.5-VL 7B/32B) consistently improve with multi-agent decomposition,
  gaining up to 9.4% on OlympiadBench, while closed-source models like Gemini-2.0-Flash
  perform better in single-agent mode on classic benchmarks, with only modest gains
  on newer We-Math dataset.
---

# Do Multi-Agents Solve Better Than Single? Evaluating Agentic Frameworks for Diagram-Grounded Geometry Problem Solving and Reasoning

## Quick Facts
- arXiv ID: 2512.16698
- Source URL: https://arxiv.org/abs/2512.16698
- Reference count: 40
- Multi-agent pipelines improve open-source model performance on geometry benchmarks by up to 9.4% accuracy.

## Executive Summary
This study systematically compares single-agent and multi-agent pipelines for diagram-grounded geometry problem solving. Using four benchmarks (Geometry3K, MathVerse, OlympiadBench, We-Math), the authors evaluate both open-source (Qwen-2.5-VL) and closed-source (Gemini-2.0-Flash, GPT-4o) models. Multi-agent decomposition consistently improves open-source model performance, achieving up to 9.4% accuracy gains on OlympiadBench, while closed-source models show modest improvements only on newer datasets. The research demonstrates that agentic decomposition is beneficial for open-source models but not universally optimal.

## Method Summary
The study employs zero-shot inference with frozen models, comparing single-agent end-to-end MLLM approaches against multi-agent pipelines (Interpreter→Solver). Four benchmarks are used: Geometry3K (601 test problems), MathVerse (788 mini-test), OlympiadBench, and We-Math. Accuracy is measured via multiple-choice and LLM-as-a-Judge (using Gemini-2.0-Flash) for free-form answers with tolerance ε. Multi-agent pipelines use an Interpreter (VLM) to generate symbolic predicates from images and questions, which a Solver (LLM) uses to produce final answers. Open-source models (Qwen-2.5-VL-7B/32B) are 4-bit quantized via unsloth.

## Key Results
- Open-source models (Qwen-2.5-VL) gain up to 9.4% accuracy on OlympiadBench using multi-agent decomposition.
- Closed-source models (Gemini-2.0-Flash) perform better in single-agent mode on classic benchmarks, with only modest gains on We-Math.
- Multi-agent pipelines achieve new state-of-the-art zero-shot results with fewer parameters compared to end-to-end approaches.

## Why This Works (Mechanism)
Multi-agent decomposition improves performance by breaking complex geometry problems into manageable subtasks: visual interpretation and symbolic reasoning. The Interpreter extracts structured predicates from diagrams, reducing the cognitive load on the Solver. This decomposition allows specialized handling of visual and logical components, leading to more accurate reasoning. Open-source models benefit more from this structure because they have limited inherent reasoning capacity compared to closed-source models, making the decomposition more impactful.

## Foundational Learning
- **Diagram-grounded geometry problem solving**: Understanding how to interpret geometric diagrams and extract relevant information. Needed because the core task involves visual-spatial reasoning combined with mathematical logic.
- **Multi-agent vs single-agent architectures**: Understanding the trade-offs between end-to-end processing and decomposed workflows. Needed to evaluate when decomposition is beneficial.
- **LLM-as-a-Judge evaluation**: Using language models to score free-form answers with tolerance ε. Needed for evaluating open-ended geometry solutions beyond multiple-choice.
- **Zero-shot inference**: Running models without fine-tuning on specific tasks. Needed because the study evaluates models in their pre-trained state.
- **VLM (Visual Language Model)**: Models that can process both images and text. Needed for the Interpreter component that extracts predicates from diagrams.
- **Symbolic predicate generation**: Converting visual information into structured, logical statements. Needed as the intermediate representation between visual interpretation and mathematical reasoning.

## Architecture Onboarding

**Component Map**: VLM (Interpreter) -> LLM (Solver) -> Answer

**Critical Path**: Image + Question → Interpreter generates predicates → Solver receives predicates + question → Solver produces answer → LLM-as-a-Judge evaluates answer

**Design Tradeoffs**: Single-agent offers simplicity and end-to-end coherence but may struggle with complex visual reasoning. Multi-agent provides specialized handling of visual and logical components but introduces potential misalignment between components. Open-source models benefit more from decomposition due to limited inherent reasoning capacity.

**Failure Signatures**: Weak Interpreter generates noisy or over-constraining predicates, degrading Solver accuracy. Closed-source models may underperform in multi-agent mode on classic benchmarks due to misaligned decomposition. Over-constraining predicates can lead to incorrect local constraints (e.g., misread angles/labels).

**First 3 Experiments**:
1. Run Qwen-2.5-VL-7B (4-bit) end-to-end on Geometry3K and MathVerse to establish single-agent baseline accuracy.
2. Implement multi-agent pipeline with Gemini-2.0-Flash as Interpreter and Phi-4 as Solver on same benchmarks using provided prompt templates.
3. Compare accuracy between single-agent and multi-agent approaches, inspecting predicate quality and example outputs.

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- The specific numerical tolerance ε for LLM-as-a-Judge evaluation is not disclosed, affecting reproducibility of free-form answer scoring.
- Sampling parameters (temperature, top_p) for Pass@3 attempts are not detailed, making exact replication difficult.
- Hardware specifications and runtime constraints are omitted, which could influence model performance.
- The study does not fully explore the impact of weaker Interpreters on downstream Solver performance.

## Confidence

**High Confidence**: Multi-agent decomposition consistently improves open-source model performance on newer benchmarks (OlympiadBench, We-Math) with clear numerical improvements up to 9.4% accuracy gains.

**Medium Confidence**: Closed-source models perform better in single-agent mode on classic benchmarks, though multi-agent gains are only modest (up to 1.9%) and not universal.

**Low Confidence**: Claims about achieving state-of-the-art zero-shot results with fewer parameters are limited by lack of direct comparisons to other SOTA methods and omission of hardware/runtime details.

## Next Checks

1. Replicate the multi-agent pipeline on Geometry3K and MathVerse using provided prompt templates, comparing Qwen-2.5-VL-7B (4-bit) single-agent vs multi-agent (Gemini-2.0-Flash as Interpreter, Phi-4 as Solver) accuracy.

2. Test the impact of Interpreter quality by running multi-agent pipeline with both strong (Qwen-2.5-VL-32B) and weak (Qwen-2.5-VL-7B) Interpreters on same benchmarks, comparing predicate quality and downstream Solver accuracy.

3. Validate the LLM-as-a-Judge tolerance by experimenting with different ε values for free-form answer scoring to determine sensitivity and assess whether reported results hold across a range of tolerances.