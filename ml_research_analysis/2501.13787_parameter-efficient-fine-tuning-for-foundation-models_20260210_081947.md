---
ver: rpa2
title: Parameter-Efficient Fine-Tuning for Foundation Models
arxiv_id: '2501.13787'
source_url: https://arxiv.org/abs/2501.13787
tags:
- arxiv
- peft
- tuning
- prompt
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey provides a comprehensive overview of Parameter-Efficient
  Fine-Tuning (PEFT) techniques for Foundation Models (FMs), addressing the critical
  challenge of adapting large-scale pre-trained models to downstream tasks while minimizing
  computational costs. The core method idea revolves around selective parameter tuning,
  where only a small subset of model parameters is updated during fine-tuning, rather
  than the entire model.
---

# Parameter-Efficient Fine-Tuning for Foundation Models

## Quick Facts
- **arXiv ID:** 2501.13787
- **Source URL:** https://arxiv.org/abs/2501.13787
- **Reference count:** 40
- **Primary result:** PEFT methods achieve performance comparable to full fine-tuning while reducing trainable parameters by over 99%, often improving task-specific performance by 0.1%-0.5%.

## Executive Summary
This survey provides a comprehensive overview of Parameter-Efficient Fine-Tuning (PEFT) techniques for Foundation Models (FMs), addressing the critical challenge of adapting large-scale pre-trained models to downstream tasks while minimizing computational costs. The core method idea revolves around selective parameter tuning, where only a small subset of model parameters is updated during fine-tuning, rather than the entire model. This approach includes techniques such as adapter layers, prompt tuning, and low-rank adaptations like LoRA. The survey systematically categorizes these methods and evaluates their effectiveness across various FM types, including Large Language Models (LLMs), Vision Foundation Models (VFMs), Vision-Language Models (VLMs), Multi-Modal Foundation Models (MFMs), and Visual Content Generation Models (VGMs).

## Method Summary
The survey describes PEFT as a methodology for adapting large pre-trained FMs to downstream tasks by updating only a small subset of parameters rather than the entire model. The core technique exemplified is LoRA (Low-Rank Adaptation), where weight updates are represented as low-rank matrices $A$ and $B$, and the forward pass becomes $y = W_0 x + B A x$. The base model weights are frozen, and only the rank-decomposition matrices are trained. The survey categorizes PEFT methods into five groups: (1) Selective PEFT (e.g., BitFit, Freeze Layers), (2) Reparameterization PEFT (e.g., LoRA), (3) Additive PEFT (e.g., adapters), (4) Prompt PEFT (e.g., Prefix Tuning), and (5) Hybrid PEFT (combining multiple approaches).

## Key Results
- PEFT methods can reduce trainable parameters by over 99% while maintaining or improving task-specific performance
- LoRA reduces GPT-3 parameters from 175 billion to 4.7 million while improving performance by 0.1% to 0.5%
- Performance gains show diminishing returns beyond a certain threshold of trainable parameters
- LLMs and VFMs dominate current PEFT research, while MFMs remain underexplored

## Why This Works (Mechanism)

### Mechanism 1: Intrinsic Dimensionality Exploitation
- **Claim:** Adapting Foundation Models (FMs) to downstream tasks does not require modifying the full parameter space; rather, the necessary adjustments exist within a low-dimensional subspace.
- **Mechanism:** Methods like LoRA operate on the assumption that weight updates during adaptation have low intrinsic rank, decomposing updates into low-rank matrices to reduce trainable parameters while retaining capacity to learn task-specific features.
- **Core assumption:** Pre-training of FMs acts as knowledge compression where specific tasks correspond to unique intrinsic dimensions within the model's subspace.
- **Evidence anchors:** [Section III.D.1] states natural language tasks can be tackled with surprisingly small numbers of parameters, and larger models tend to have lower intrinsic dimensions.
- **Break condition:** Performance degrades sharply if chosen rank is too low for complex tasks requiring high-rank feature updates, or if target task lies outside pre-trained knowledge manifold.

### Mechanism 2: Selective Criticality
- **Claim:** Not all parameters in an FM are equally important for a specific downstream task; tuning a critical subset is sufficient.
- **Mechanism:** Selective PEFT (e.g., BitFit, Freeze Layers) freezes majority of backbone and updates only specific subsets, such as bias terms or final layers.
- **Core assumption:** The fundamental assumption is that certain parameters are particularly important for specific tasks.
- **Evidence anchors:** [Section III.A] notes BitFit adjusts part of the bias terms, achieving performance comparable to full fine-tuning in specific scenarios.
- **Break condition:** Interference with features essential for other tasks (if multi-tasking) or insufficient capacity if frozen backbone lacks domain-specific knowledge required for new task.

### Mechanism 3: Virtual Prompt Distribution Alignment
- **Claim:** Continuous soft prompts can shift input distribution of downstream tasks to better match FM's pre-training data distribution.
- **Mechanism:** Prompt PEFT (e.g., Prefix Tuning, P-Tuning) optimizes continuous embedding vectors concatenated to input, guiding model toward generating desired output by filling gap between downstream data and original pre-training corpus.
- **Core assumption:** FM's capabilities are sufficient for task, provided input is framed correctly via optimized embeddings.
- **Evidence anchors:** [Section III.C.1] states goal is to align input distribution with original training data.
- **Break condition:** Optimization instability or hallucinations if prompt length is too long relative to input, or if model lacks foundational knowledge to be triggered by prompt.

## Foundational Learning

- **Concept:** Transformer Attention Mechanisms (Query, Key, Value)
  - **Why needed here:** Most PEFT methods are explicitly inserted into Attention blocks or FFN layers. Understanding how Q, K, V projections work is required to know where to inject trainable parameters.
  - **Quick check question:** Can you explain how concatenating a "prefix" to the Key and Value matrices in the Attention layer influences the model's context processing?

- **Concept:** Matrix Rank and Low-Rank Decomposition
  - **Why needed here:** The Reparameterization category (LoRA) relies entirely on mathematical property that large weight matrix update can be decomposed into two smaller matrices to reduce dimensionality.
  - **Quick check question:** If a weight matrix W is d × d, and you decompose it into A (d × r) and B (r × d) where r << d, what is the reduction ratio in trainable parameters?

- **Concept:** Catastrophic Forgetting vs. Plasticity
  - **Why needed here:** Survey emphasizes PEFT preserves pre-trained knowledge better than full fine-tuning. This trade-off between learning new tasks (plasticity) and retaining old knowledge is core motivation for freezing backbone parameters.
  - **Quick check question:** Why does freezing the backbone weights of a Foundation Model help mitigate catastrophic forgetting when adapting to a new domain?

## Architecture Onboarding

- **Component map:**
  - Pre-trained FM backbone (frozen) -> PEFT Modules (Adapters/LoRA/Prompts) -> Task-specific Head -> Output

- **Critical path:**
  1. Select a pre-trained FM backbone (e.g., LLaMA-7B)
  2. Inject chosen PEFT config (e.g., LoRA(rank=8)), wrapping target layers
  3. Freeze all backbone parameters; ensure only PEFT modules (and optionally Head) have requires_grad=True
  4. Train with backpropagation; gradients flow through frozen backbone to update only small PEFT parameters

- **Design tradeoffs:**
  - LoRA: Best for deployment (merges weights, zero inference latency), but requires hyperparameter tuning (Rank r, Alpha α)
  - Adapters: Modular and easy to swap, but adds inference latency due to extra sequential computation
  - Soft Prompts: Does not change model weights (great for multi-tenant serving), but consumes input context window and can be harder to optimize

- **Failure signatures:**
  - Underfitting: Trainable parameter count is too low (e.g., Rank r is too small) for task complexity
  - Hyper-sensitivity: Loss diverges immediately; standard full fine-tuning learning rates are often too high for PEFT
  - Structure Mismatch: Applying visual adapter design to text-only model without architectural adjustments

- **First 3 experiments:**
  1. Establish Baseline: Run LoRA on standard benchmark (e.g., GLUE/SuperGLUE) with paper's recommended Rank (r=8) and compare against Linear Probe to verify backbone contribution
  2. Latency Profiling: Measure inference time (ms/token) for Adapters vs. LoRA (merged) vs. Frozen to quantify inference overhead tradeoff
  3. Ablation on Position: Apply LoRA only to Wq (Query) vs. only to Wv (Value) vs. both. Determine optimal injection point for specific architecture

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the field establish a unified, comprehensive benchmark to ensure fair comparisons of PEFT methods across different evaluation datasets and task setups?
- Basis in paper: [explicit] Authors explicitly state in Section VII (Discussion) that there is a notable lack of comprehensive benchmarks for PEFT, leading to inconsistent performance assessment standards.
- Why unresolved: Current studies use varied evaluation datasets and setups, making it difficult for users to evaluate strengths and weaknesses of different methods objectively.
- What evidence would resolve it: Establishment of standardized baseline framework accepted by community that facilitates consistent evaluation across various PEFT techniques.

### Open Question 2
- Question: How can PEFT be effectively adapted for continual learning environments to handle dynamic data flows without interfering with or overwriting previously learned knowledge?
- Basis in paper: [explicit] Paper identifies "Continual PEFT" as future direction, noting that adapting PEFT to sequence of tasks may cause model to interfere with or overwrite learned knowledge.
- Why unresolved: Standard PEFT methods focus on static task adaptation, whereas real-world applications often require systems that learn continuously while retaining memory of past tasks.
- What evidence would resolve it: PEFT framework capable of learning new tasks sequentially while maintaining performance stability on earlier tasks, demonstrating robustness in dynamic environments.

### Open Question 3
- Question: What are the precise scaling laws regarding performance relative to number of trainable parameters in PEFT methods like LoRA and adapters?
- Basis in paper: [explicit] Authors list "Scaling Laws of PEFT" as future direction, noting current efforts show diminishing returns beyond certain threshold and asking how performance scales with parameter count.
- Why unresolved: Understanding optimal range for parameter selection is crucial for optimizing efficiency, yet specific scaling behaviors for various PEFT techniques remain undefined.
- What evidence would resolve it: Empirical studies that map performance metrics against specific increases or decreases in trainable parameters, identifying optimal efficiency-performance trade-off points.

## Limitations

- Limited Multi-Modal Foundation Model Coverage: MFMs are underexplored in current PEFT research, suggesting effectiveness of existing techniques on this domain is not fully validated
- Lack of Unified Benchmarking Framework: No standardized experimental setup for evaluating PEFT methods, making direct comparisons challenging
- Hyperparameter Sensitivity: Effectiveness highly dependent on hyperparameters like rank and learning rates, with no specific guidelines provided

## Confidence

- **High Confidence:** The fundamental principle that PEFT methods reduce trainable parameters by >99% while maintaining performance is well-supported by multiple studies
- **Medium Confidence:** The claim that intrinsic dimensionality exploitation is primary mechanism behind PEFT's success is plausible but lacks direct mathematical proof
- **Low Confidence:** The assertion that specific parameters are "critical" for certain tasks is presented without detailed empirical evidence or saliency metrics

## Next Checks

1. Benchmark PEFT Methods on a Unified Dataset: Conduct controlled experiment using standard benchmark (e.g., GLUE or SuperGLUE) to compare LoRA, adapters, and prompt tuning under identical conditions
2. Hyperparameter Sensitivity Analysis: Perform ablation study on LoRA's rank and learning rate settings to determine their impact on performance
3. Explore Multi-Modal PEFT Techniques: Apply existing PEFT methods to a Multi-Modal Foundation Model (e.g., CLIP or Flamingo) and evaluate their effectiveness