---
ver: rpa2
title: List-Decodable Regression via Expander Sketching
arxiv_id: '2511.22524'
source_url: https://arxiv.org/abs/2511.22524
tags:
- fraction
- buckets
- lemma
- inlier
- constant
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an expander-sketching framework for list-decodable
  linear regression that achieves near-optimal sample complexity, list size, and input-sparsity
  runtime. The method uses sparse signed adjacency matrices derived from lossless
  expanders to create "lightly contaminated" synthetic batches, each containing a
  constant fraction of inliers and only a bounded number of outliers.
---

# List-Decodable Regression via Expander Sketching

## Quick Facts
- arXiv ID: 2511.22524
- Source URL: https://arxiv.org/abs/2511.22524
- Reference count: 21
- Key outcome: Achieves near-optimal sample complexity, list size, and input-sparsity runtime for list-decodable regression using expander sketching

## Executive Summary
This paper introduces an expander-sketching framework for list-decodable linear regression that achieves near-optimal sample complexity, list size, and input-sparsity runtime. The method uses sparse signed adjacency matrices derived from lossless expanders to create "lightly contaminated" synthetic batches, each containing a constant fraction of inliers and only a bounded number of outliers. Local normal-equation statistics within these buckets are aggregated robustly via median-of-means or geometric median estimators, producing accurate global moment estimates. A short spectral filtering phase removes adversarial contributions along dominant directions. The resulting algorithm outputs a list of size O(1/α) with one candidate within statistical radius of the true parameter, runs in time O(nnz(X) + d³/α), and avoids both SoS optimization and explicit batch structure assumptions. Experiments on synthetic and real datasets show the method remains robust under heavy contamination and outlier magnitude, often outperforming standard and robust regression baselines.

## Method Summary
The algorithm constructs r independent bipartite expander graphs with n left vertices (data points) and B right vertices (buckets). Each data point is hashed into d_L buckets using the sparse adjacency matrix from the expander. Local second-order statistics (H_{t,b} = A^t A) are computed for each bucket and aggregated via median-of-means to produce global moment estimates. Ridge regression is then solved on these moments to obtain candidate parameters. If the residual covariance indicates remaining adversarial energy, a spectral filtering phase iteratively removes buckets with high Rayleigh quotients along dominant directions. This process is repeated R = O(1/α) times to generate a list of candidate parameters, with the guarantee that at least one candidate is within statistical radius of the true parameter.

## Key Results
- Achieves optimal error rate of O(1/√α) with list size O(1/α)
- Runs in input-sparsity time O(nnz(X) + d³/α), independent of batch size
- Maintains robustness under heavy contamination (up to 1-α fraction outliers) and outlier magnitude
- Outperforms standard and robust regression baselines on both synthetic and real datasets
- Requires only oblivious adversaries and noiseless inlier model

## Why This Works (Mechanism)

### Mechanism 1: Combinatorial Isolation via Synthetic Batching
The bipartite expander graph structure ensures that a constant fraction of buckets become "lightly contaminated" - containing mostly inliers and few outliers. The unique neighbor property and collision bounds limit how many outliers can collide into these good buckets, capping contamination at a constant level C₀. This isolation property mimics batched data structure even when input is i.i.d. with arbitrary outliers.

### Mechanism 2: Dimension Reduction via Median-of-Means on Sketched Statistics
When buckets are lightly contaminated, aggregating their local second-order statistics via geometric median or MoM recovers global moments within required operator norm error. Since a γ-fraction of buckets are good (dominated by inliers), coordinate-wise median filters out arbitrary corruption from bad buckets, effectively reducing to robust mean estimation over bucket statistics.

### Mechanism 3: Spectral Filtering for Residual Pruning
The algorithm computes robust residual covariance and identifies directions of maximum variance likely aligned with residual adversary noise. By scoring buckets on Rayleigh quotients along the top eigenvector and removing top ρ fraction, it targets specific buckets contributing to spectral anomalies. This exponentially faster pruning reduces adversarial energy while preserving inlier information.

## Foundational Learning

- **List-Decodable Learning**: Needed because standard robust regression assumes >50% inliers, but list-decodable setting (α ≤ 0.5) requires returning a list of candidates rather than single parameter. Quick check: Why does algorithm output list of size O(1/α) rather than single vector?

- **Lossless Expander Graphs**: Structural engine replacing batch assumption. Understanding expansion is necessary to grasp how random sparse sketch guarantees isolation properties. Quick check: How does "unique neighbor" property in bipartite graph help limit number of outliers in specific bucket?

- **Input-Sparsity Time (nnz)**: Paper emphasizes efficiency (tilde{O}(nnz(X))). Contrasts with SoS methods scaling polynomially with dimension and batch size. Quick check: Why does using sparse sketching matrix (constant non-zeros per column) ensure runtime scales with number of non-zeros in data matrix?

## Architecture Onboarding

- **Component map**: Random Bipartite Graph Generator → Sparse Hashing Matrices S_t → Local moment computer (H_{t,b}) → MoM Aggregator (hat{Sigma}, hat{g}) → Ridge Regression solver → Residual Covariance Estimator → Top-k Eigen-solver → Bucket Pruner → Seed Looper → Clustering module

- **Critical path**: 1) Sketch Construction: Generating r independent expander graphs. 2) Robust Aggregation: Converting bucket stats to global moments (where robustness is gained). 3) Spectral Filtering: Iterative cleanup (only runs if residual energy is high). 4) Seeding: Repeating pipeline R times to ensure at least one seed succeeds.

- **Design tradeoffs**: Bucket count (B): Higher B improves isolation but increases sketching overhead and variance in local bucket estimates. Paper suggests B asymp d/(α log d). Left degree (d_L): Higher degree improves unique neighbor guarantees but increases sketch density. Paper uses constant d_L = O(1). List size (R): To guarantee success, R prop 1/α. Increasing R improves probability of finding good seed but linearly increases compute time.

- **Failure signatures**: High Test MSE despite low training loss suggests spectral filter is removing good buckets or adversary compromised sketch randomness. Runtime explosion if filtering loop (T) doesn't converge or B set too high relative to n. Memory overflow storing full moment tensors H_{t,b} for all buckets; optimization requires processing blocks or streaming.

- **First 3 experiments**: 1) Vary Inlier Fraction (α): Sweep α ∈ [0.1, 0.5] on synthetic data to verify O(1/α) error scaling and list size stability. 2) Scalability Test: Fix d and α, scale n (and thus nnz(X)) to verify linear runtime scaling against baseline like SoS. 3) Hyperparameter Sensitivity (d_L and B): Test if lowering graph degree d_L breaks "light contamination" property, validating expander isolation mechanism.

## Open Questions the Paper Calls Out
None

## Limitations
- Sketch randomness assumption relies on oblivious adversaries; adaptive adversaries could potentially probe sketch structure and evade light contamination property
- Spectral filter convergence depends on adversary's spectral energy profile; may require many iterations or fail if inlier/outlier spectra overlap significantly
- Constant factor dependencies in runtime and sample complexity are stated but not empirically studied; practical performance may degrade with conservative parameter settings

## Confidence

**High Confidence**:
- Correctness of theoretical analysis for expander isolation property (Lemma 2)
- MoM aggregation error bounds (Lemma 8)
- Overall framework structure and asymptotic runtime (Theorem 1)

**Medium Confidence**:
- Spectral filtering potential argument (Lemma 10)
- List size guarantee (Theorem 1)
- Practical effectiveness against adaptive adversaries

**Low Confidence**:
- Precise constant factors in runtime and sample complexity
- Behavior under overlapping inlier/outlier spectra
- Performance with high-dimensional sparse data where nnz(X) scaling matters

## Next Checks

1. **Adaptive Adversary Test**: Implement adversary that can probe sketch structure through small validation set and design outliers to maximize contamination in good buckets. Compare algorithm's performance against oblivious and adaptive settings.

2. **Spectral Profile Analysis**: Generate data where inlier covariance has eigenvalues overlapping with adversary's residual energy spectrum. Measure spectral filtering iterations required and whether algorithm still achieves claimed error rate.

3. **Constant Factor Sweep**: Empirically vary d_L (left degree) and B (bucket count) across range of values for fixed n, d, α. Plot trade-off between error, runtime, and memory usage to identify practical sweet spot and validate paper's asymptotic claims.