---
ver: rpa2
title: 'Mario at EXIST 2025: A Simple Gateway to Effective Multilingual Sexism Detection'
arxiv_id: '2507.10996'
source_url: https://arxiv.org/abs/2507.10996
tags:
- detection
- sexism
- content
- training
- lora
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses text-based sexism detection in English and
  Spanish tweets through hierarchical Low-Rank Adaptation (LoRA) of Llama 3.1 8B.
  The method introduces conditional adapter routing that explicitly models label dependencies
  across three hierarchically structured subtasks: binary sexism identification, source
  intention detection, and multilabel sexism categorization.'
---

# Mario at EXIST 2025: A Simple Gateway to Effective Multilingual Sexism Detection

## Quick Facts
- arXiv ID: 2507.10996
- Source URL: https://arxiv.org/abs/2507.10996
- Reference count: 40
- Primary result: Hierarchical LoRA achieves competitive performance with 1.67% trainable parameters, 75% training time reduction, and 98% storage saving on EXIST 2025 sexism detection task

## Executive Summary
This paper addresses text-based sexism detection in English and Spanish tweets through hierarchical Low-Rank Adaptation (LoRA) of Llama 3.1 8B. The method introduces conditional adapter routing that explicitly models label dependencies across three hierarchically structured subtasks: binary sexism identification, source intention detection, and multilabel sexism categorization. The approach applies LoRA adaptation to all linear transformations rather than only attention layers, achieving strong performance with minimal preprocessing. The multilingual training strategy leverages Llama 3.1's native bilingual capabilities, achieving 1.7-2.4% F1 improvements through cross-lingual transfer.

## Method Summary
The approach uses LoRA adaptation to all linear transformations (attention and FFN layers) with rank 16 and alpha 16. Three task-specific adapter sets handle the hierarchical structure: binary detection, intention detection, and multilabel categorization. Conditional routing activates appropriate adapters based on parent predictions, with a hierarchy loss penalizing invalid transitions. Joint bilingual training on English and Spanish data exploits cross-lingual transfer. The model uses 4-bit QLoRA quantization, Flash Attention v2, and gradient checkpointing with constant learning rate and warmup.

## Key Results
- Achieves ICM-Hard: 0.6774 for binary classification, 0.4991 for intention detection, 0.6519 for multilabel categorization
- Uses only 1.67% trainable parameters compared to full fine-tuning
- Reduces training time by 75% and model storage by 98%
- Joint bilingual training yields 1.7-2.4% F1 improvements over separate language models

## Why This Works (Mechanism)

### Mechanism 1
Applying LoRA to all linear transformations—not just attention layers—increases model capacity to capture task-specific patterns in hierarchical sexism detection. The decomposition W = W₀ + BA is applied to q_proj, k_proj, v_proj, o_proj (attention), gate_proj, up_proj, down_proj (FFN), and lm_head. This broader coverage allows adaptation signals to propagate through both attention-based context modeling and feed-forward transformation pathways. Core assumption: Task-specific patterns for nuanced sexism categorization require adaptation beyond attention mechanisms alone; FFN layers contribute meaningfully to semantic differentiation.

### Mechanism 2
Hierarchical label-aware LoRA routing with soft parent-child constraints improves structured multi-task classification by explicitly modeling label dependencies. Separate LoRA adapters Δ^(ℓ) per hierarchy level are activated conditionally. During inference, prediction ŷ^(ℓ-1) determines which adapter routes the next level. A hierarchy loss ℒ_hierarchy penalizes high-confidence child predictions when parent predicts NOT_SEXIST. Core assumption: Hierarchical tasks (binary → intention → categorization) have exploitable label dependencies; invalid transitions (e.g., NOT_SEXIST → specific sexism type) should be suppressed.

### Mechanism 3
Joint bilingual training leverages cross-lingual transfer, improving performance over separate language-specific models. Single model trained on pooled English-Spanish data exploits Llama 3.1's native bilingual representations. Shared semantic patterns of sexism transfer bidirectionally. Core assumption: Sexism manifests through language-shared semantic patterns; Llama 3.1's pretraining provides sufficient cross-lingual alignment for transfer without explicit alignment modules.

## Foundational Learning

- **Low-Rank Adaptation (LoRA)**: Why needed here: The entire method hinges on parameter-efficient fine-tuning. Understanding rank, alpha, target modules, and the W = W₀ + BA decomposition is prerequisite to modifying configuration. Quick check question: Given rank r=16 and input dimension d=4096, how many trainable parameters does a single LoRA module add to one weight matrix?

- **Hierarchical Multi-Task Learning**: Why needed here: The three subtasks have structured dependencies; the routing mechanism and hierarchy loss assume understanding of parent-child label constraints. Quick check question: If a binary classifier predicts NOT_SEXIST, what should the hierarchical constraint do to child-level predictions?

- **Cross-Lingual Transfer in Pre-Trained LLMs**: Why needed here: Joint bilingual training assumes Llama 3.1's representations support transfer. Knowing why this works (or fails) informs whether to extend to other language pairs. Quick check question: What property of a pre-trained model enables zero-shot or few-shot transfer across languages without parallel data?

## Architecture Onboarding

- **Component map**: Input → Llama 3.1 frozen backbone → LoRA-adapted forward pass → Level 1 adapter → binary prediction → Conditional routing → Level 2/3 adapter activation → Loss computation (task + hierarchy constraint)

- **Critical path**: Input → Llama 3.1 frozen backbone → LoRA-adapted forward pass → Level 1 adapter → binary prediction → Conditional routing → Level 2/3 adapter activation → Loss computation (task + hierarchy constraint) → Backprop through LoRA parameters only

- **Design tradeoffs**: Rank 16 vs. higher: +0.3% F1 at 4x parameters (Table 5)—marginal gains, steep cost. Joint vs. separate training: +1.7-2.4% F1 but requires bilingual data; single model simpler to deploy. All-linear vs. attention-only: More parameters but stronger task adaptation (claimed, not directly ablated in paper)

- **Failure signatures**: Error propagation: Wrong binary prediction cascades to incorrect routing at Levels 2-3. Cross-lingual interference: If languages conflict, joint model underperforms both monolingual baselines. Over-constraint: Excessive λ in ℒ_hierarchy may suppress valid edge cases

- **First 3 experiments**: 1. Ablate target modules: Train LoRA on attention-only vs. all-linear; compare F1 on validation set to isolate FFN contribution. 2. Monolingual baseline comparison: Train separate English-only and Spanish-only adapters; verify reported +1.7-2.4% joint training gains. 3. Hierarchy constraint sensitivity: Vary λ (0, 0.1, 0.5, 1.0) and measure impact on invalid transition rate vs. overall F1

## Open Questions the Paper Calls Out

### Open Question 1
How can annotator demographic information (e.g., gender, age, ethnicity) be effectively encoded into the LoRA adaptation process to capture the subjective nature of sexism perception? The conclusion states that incorporating demographic information "presents an opportunity for improvement" and suggests exploring "persona-specific adapters" or "demographic-aware attention mechanisms." This remains unresolved because the current methodology intentionally discarded rich annotator demographics in favor of gold standard labels for efficient, direct optimization.

### Open Question 2
Can integrating temporal engagement forecasting with this hierarchical detection framework enable proactive identification of viral sexist content? The conclusion suggests that combining the detection approach with "temporal engagement forecasting" could allow for "proactive identification of potentially viral sexist content within the critical first minutes of posting." This is unresolved because the current system focuses solely on text classification without incorporating the temporal dynamics or early engagement signals required for virality prediction.

### Open Question 3
Does the observed superior performance on Spanish data compared to English stem from inherent linguistic patterns of sexism or imbalances in the training data distribution? The results section notes Spanish consistently outperforms English (e.g., +0.26 ICM-Hard in categorization) but lists the cause as "likely reflect[ing] both differences in training data distribution and linguistic characteristics" without isolating the variables. This remains unresolved because the paper does not provide a detailed analysis of the label distributions or linguistic features specific to the Spanish subset that might explain the significant performance gap.

### Open Question 4
Does applying LoRA to all linear transformations (versus only attention layers) provide consistent benefits for hierarchical tasks in domains other than sexism detection? The methodology asserts that targeting all linear layers "enhancing the model's capacity" based on internal experiments, but the paper only validates this specific configuration on the EXIST 2025 sexism task. It remains unclear if the computational cost of adapting MLP layers is justified generally, or if this architectural choice is overly specific to the nuances of social media sexism classification.

## Limitations

- The claimed advantages of full-linear LoRA adaptation versus attention-only adaptation lack direct ablation validation in the paper
- The 1.7-2.4% F1 improvements from joint bilingual training are not robust to different language pairs or dataset sizes as the baselines are not fully detailed
- The effectiveness of hierarchical routing depends on strong label dependencies that are assumed rather than empirically tested in isolation

## Confidence

- **High Confidence**: The parameter efficiency of LoRA (1.67% trainable parameters, 75% training time reduction, 98% storage saving) is a direct, verifiable outcome of the training setup and is well-supported by the reported numbers
- **Medium Confidence**: The claim that joint bilingual training yields 1.7-2.4% F1 improvements is supported by reported numbers, but the robustness of these gains to language pair and dataset size is not established
- **Medium Confidence**: The assertion that full-linear LoRA adaptation (vs. attention-only) improves task-specific performance is logically plausible and consistent with the broader LoRA literature, but lacks direct ablation in the paper
- **Low Confidence**: The exact contribution of the ℒ_hierarchy soft constraint to performance is not isolated, and the risk of error propagation in hierarchical routing is not quantified, so the net benefit is uncertain

## Next Checks

1. **Ablate LoRA Target Modules**: Train and evaluate LoRA adapters targeting only attention layers vs. all linear layers (attention + FFN). Measure F1 on the validation set to directly assess the contribution of FFN adaptation to task performance. This will clarify if the added parameter cost is justified.

2. **Monolingual Baseline Comparison**: Train and evaluate separate English-only and Spanish-only LoRA adapters under identical conditions. Compare their F1 scores to the joint bilingual model to verify the robustness and generalizability of the reported 1.7-2.4% gains from cross-lingual transfer.

3. **Hierarchy Constraint Sensitivity**: Systematically vary the hierarchy loss weight λ (e.g., 0, 0.1, 0.5, 1.0) during training. For each setting, measure both the overall F1 and the rate of invalid parent-child label transitions. This will quantify the trade-off between enforcing label dependencies and preserving valid edge cases, and determine if the chosen λ is optimal or if the results are sensitive to its value.