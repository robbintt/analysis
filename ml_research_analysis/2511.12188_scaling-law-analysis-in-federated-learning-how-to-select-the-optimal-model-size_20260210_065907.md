---
ver: rpa2
title: 'Scaling Law Analysis in Federated Learning: How to Select the Optimal Model
  Size?'
arxiv_id: '2511.12188'
source_url: https://arxiv.org/abs/2511.12188
tags:
- training
- size
- optimal
- data
- federated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies how federated learning affects the optimal model
  size for large-scale training. It derives a PAC-Bayes generalization bound for federated
  SGD and uses it to analytically determine the optimal model size under convex assumptions.
---

# Scaling Law Analysis in Federated Learning: How to Select the Optimal Model Size?

## Quick Facts
- arXiv ID: 2511.12188
- Source URL: https://arxiv.org/abs/2511.12188
- Reference count: 40
- Primary result: Federated learning scaling law shows optimal model size follows negative power law with client count (R² > 0.88)

## Executive Summary
This paper investigates how federated learning affects optimal model size selection in large-scale training. The authors derive a PAC-Bayes generalization bound for federated SGD and use it to analytically determine optimal model size under convex assumptions. The key finding is that in federated settings, the optimal model size scales according to a negative power law with the number of clients - larger client counts require smaller models for the same computational budget. Empirical validation on Vision Transformers and ResNets across CIFAR-100 and ImageNet demonstrates strong agreement with theoretical predictions.

## Method Summary
The authors derive a PAC-Bayes generalization bound for federated SGD to establish a theoretical foundation for optimal model size selection. They analytically solve for the optimal model size under convex assumptions, then validate their predictions empirically. The experimental validation involves training Vision Transformers and ResNets on CIFAR-100 and ImageNet datasets under federated learning conditions, measuring test accuracy as a function of model size and client count. The results are compared against the theoretical scaling law predictions to assess agreement.

## Key Results
- Federated learning optimal model size follows negative power law with client count
- Empirical validation shows strong agreement (R² > 0.88) with theoretical predictions
- Larger client counts consistently require smaller models for optimal performance
- Results validated across multiple architectures (Vision Transformers, ResNets) and datasets (CIFAR-100, ImageNet)

## Why This Works (Mechanism)
The mechanism works because federated learning introduces additional noise and variance from aggregating updates across multiple clients with potentially heterogeneous data distributions. This aggregation noise effectively reduces the benefit of larger model capacity, as the signal-to-noise ratio decreases with more clients. The PAC-Bayes bound captures this relationship by showing that generalization error increases with client count, which can be compensated by reducing model size. The negative power law relationship emerges from the mathematical structure of how client count affects the bound's terms.

## Foundational Learning

**PAC-Bayes bounds** - Theoretical framework for generalization analysis that bounds the expected risk of a randomized predictor relative to a prior distribution. Needed to establish rigorous generalization guarantees in federated learning settings.

**Federated SGD dynamics** - Understanding how local gradient updates from multiple clients are aggregated and how this affects convergence and generalization. Critical for modeling the noise introduced by client heterogeneity.

**Power law scaling** - Mathematical relationship where one quantity varies as a power of another. Essential for characterizing how optimal model size scales with system parameters like client count.

## Architecture Onboarding

**Component map**: PAC-Bayes bound derivation -> Convex optimization solution -> Empirical validation -> Scaling law formulation

**Critical path**: Theoretical derivation → Convex solution → Experimental setup → Result analysis → Scaling law verification

**Design tradeoffs**: The convex assumption enables analytical solutions but may not fully capture deep neural network behavior. The paper balances theoretical rigor with empirical validation to bridge this gap.

**Failure signatures**: Deviations from negative power law scaling would indicate either breakdown of the convex assumption, non-IID client data effects not captured in analysis, or implementation issues in federated training setup.

**First experiments**: 1) Verify optimal model size prediction on held-out dataset architecture, 2) Test scaling relationship across different client count ranges, 3) Evaluate impact of client data heterogeneity on scaling law predictions

## Open Questions the Paper Calls Out
None

## Limitations
- Convex assumptions may not hold for deep neural networks commonly used in practice
- Empirical validation covers limited range of model sizes and client counts
- Results may not generalize to non-IID data distributions across clients
- Communication constraints and other practical federated learning challenges not considered

## Confidence
- High confidence: Larger client counts correlate with smaller optimal model sizes across experiments
- Medium confidence: Specific negative power law relationship and coefficients may be sensitive to experimental conditions
- Medium confidence: Practical guidance useful but may require recalibration for different federated learning scenarios

## Next Checks
1. Test scaling law predictions on additional model architectures beyond Vision Transformers and ResNets
2. Validate scaling relationship across wider range of client counts to verify asymptotic behavior
3. Evaluate theory under non-IID data distributions across clients to assess real-world applicability