---
ver: rpa2
title: Automating Structural Engineering Workflows with Large Language Model Agents
arxiv_id: '2510.11004'
source_url: https://arxiv.org/abs/2510.11004
tags:
- structural
- engineering
- system
- agents
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "MASSE automates end-to-end structural engineering tasks using\
  \ a multi-agent LLM framework with specialized Analyst, Engineer, and Management\
  \ teams. Each agent handles a distinct workflow stage\u2014data extraction, analysis,\
  \ design, and safety verification\u2014communicating via structured JSON to ensure\
  \ clarity and traceability."
---

# Automating Structural Engineering Workflows with Large Language Model Agents

## Quick Facts
- **arXiv ID:** 2510.11004
- **Source URL:** https://arxiv.org/abs/2510.11004
- **Reference count:** 40
- **Key outcome:** MASSE automates end-to-end structural engineering tasks using a multi-agent LLM framework with specialized Analyst, Engineer, and Management teams. Each agent handles a distinct workflow stage—data extraction, analysis, design, and safety verification—communicating via structured JSON to ensure clarity and traceability. Evaluated on 100 real-world racking system cases, MASSE reduced expert workload from ~2 hours to ~2 minutes with accuracy scores above 85% across benchmarks, and reasoning models (e.g., o4-mini) achieved the most consistent performance. The approach generalizes to other verbalizable, tool-centered engineering domains.

## Executive Summary
MASSE introduces a multi-agent LLM framework that automates complex structural engineering workflows by decomposing tasks into specialized teams. The system integrates deterministic tools (FEM solvers, RAG) with structured JSON communication to achieve high accuracy and efficiency. Evaluated on 100 racking system cases, MASSE reduced expert workload from ~2 hours to ~2 minutes while maintaining accuracy scores above 85%. The approach demonstrates potential for generalization to other engineering domains where tasks are verbalizable and tool-mediated.

## Method Summary
MASSE employs a multi-agent LLM framework with three specialized teams: Analyst (data extraction), Engineer (analysis and design), and Management (compliance verification). Agents communicate via structured JSON to minimize ambiguity and preserve state across workflow stages. The system integrates deterministic tools including OpenSeesPy for FEM analysis and FAISS-indexed RAG for code retrieval. MASSE was evaluated on 100 racking system cases using benchmarks SAAB, SDAB, LAB, and MASEB, comparing performance against single-agent baselines and ablation variants.

## Key Results
- Reduced expert workload from ~2 hours to ~2 minutes while maintaining >85% accuracy across benchmarks
- Reasoning models (o4-mini) achieved highest consistency with SAAB 96.6, though at increased computational cost
- Structured JSON communication critical for performance, with ablation showing significant accuracy drops when removed
- Multi-agent decomposition eliminated single-agent failure modes (100% failure rate in monolithic approach)

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Task Decomposition
Partitioning long-horizon engineering workflows into specialized, role-based agents (Analyst, Engineer, Manager) mitigates the cascading error rates observed in monolithic single-agent systems. The system decouples distinct cognitive domains: data extraction (Analyst), physical simulation (Engineer), and compliance verification (Management). By isolating the context window and error propagation paths for each stage, a failure in one module (e.g., data extraction) does not immediately corrupt the procedural logic of another (e.g., FEM analysis).

### Mechanism 2: Structured Artifact Communication
Enforcing JSON-based communication protocols between agents, rather than verbose natural language, preserves state and reduces ambiguity in multi-turn reasoning. Agents read from and write to a "Structured Memory" (JSON schemas), forcing the LLM to generate parsable, typed outputs (e.g., `loads_lbs`, `nodes`). This effectively filters out hallucinations that do not conform to the schema.

### Mechanism 3: Deterministic Tool Augmentation
Offloading mathematical computation and code retrieval to external deterministic tools (Python solvers, RAG) anchors LLM reasoning to ground truth. The LLM acts as an orchestrator rather than a calculator. When the "Design Engineer" or "Model Engineer" needs to perform analysis, the system executes Python scripts (OpenSeesPy) rather than asking the LLM to predict the result.

## Foundational Learning

- **Concept: Finite Element Analysis (FEA) Orchestration**
  - **Why needed here:** The core value prop is automating structural simulation. You must understand that the agents are generating *input files* (JSON/Python) for solvers like OpenSees, not doing the physics themselves.
  - **Quick check question:** Can you identify which agent generates the node coordinates and which agent executes the `run_complete_opensees_analysis` function?

- **Concept: Structured Memory vs. Context Window**
  - **Why needed here:** The system relies on a persistent memory store (JSON) to pass data between teams. Without this, the "Engineer" would lose the specific load values extracted by the "Analyst" once the conversation history grows too long.
  - **Quick check question:** If the "Loading Analyst" updates a weight value in memory, does the "Safety Manager" see the update immediately, or does it require a specific function call to refresh context?

- **Concept: Retrieval-Augmented Generation (RAG) for Codes**
  - **Why needed here:** Structural engineering depends on adherence to building codes (e.g., seismic parameters). The system uses RAG to fetch specific values (e.g., `Sa_0.2`) rather than relying on the LLM's internal (and likely outdated) training data.
  - **Quick check question:** What is the risk if the Seismic Analyst retrieves `Sa_0.2` for the wrong city because the vector search retrieves a semantically similar but geographically incorrect document chunk?

## Architecture Onboarding

- **Component map:** ProjectManager (Decomposition) → SeismicAnalyst (RAG Lookup) → StructuralAnalyst (Model Gen) → DesignEngineer (Capacity Calc) → ModelEngineer (FEM Runner) → VerificationEngineer (Code Check) → SafetyManager (Final Decision)
- **Critical path:** Problem Description → ProjectManager (Split) → SeismicAnalyst (Get Site Data) → StructuralAnalyst (Make Model) → ModelEngineer (Run FEM) → SafetyManager (Verify)
- **Design tradeoffs:** Reasoning vs. Speed: o4-mini achieved peak performance (SAAB 96.6) but with heaviest computational cost and runtime; GPT-4o offers balance. Agent Rounds: Increasing communication rounds (1 to 4) improves score (~40 to ~90) but linearly increases runtime (20s to 70s).
- **Failure signatures:** Single-Agent Collapse: High failure rate (100% in paper) with errors distributed across Logic, Dependency, and Formatting. JSON Serialization Error: If agent returns markdown text instead of pure JSON, StructuredMemory update fails. Tool Invocation Failure: If OpenSees crashes, system may hang unless exception handling is explicitly programmed.
- **First 3 experiments:**
  1. Run the Ablation on Memory: Disable shared memory and force agents to pass data via chat history only. Observe drop in LAB and SAAB scores.
  2. Stress Test the Seismic RAG: Provide problem description with location not in vector database (or typo). Verify if SeismicAnalyst raises error or hallucinates values.
  3. Solver Sensitivity: Modify problem description to include unstable structure (e.g., mechanism). Check if ModelEngineer detects convergence failure from OpenSees.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can MASSE's multi-agent architecture generalize effectively to other verbalizable, tool-centered engineering domains beyond structural engineering?
- **Basis in paper:** The authors state: "we hypothesize that such agent-driven workflows can generalize beyond structural engineering to other domains where tasks are verbalizable, procedural, and tool-mediated" and "The approach generalizes to other verbalizable, tool-centered engineering domains."
- **Why unresolved:** The system was evaluated only on racking system cases; no empirical validation in other engineering domains (e.g., mechanical, electrical, geotechnical) has been conducted.
- **What evidence would resolve it:** Successful deployment and benchmarking of MASSE-style agents in at least one other engineering discipline with comparable accuracy and efficiency gains.

### Open Question 2
- **Question:** How can real-time feedback be integrated to support adaptive self-improvement of the multi-agent system?
- **Basis in paper:** The conclusion states: "future work will emphasize... integration of real-time feedback to support adaptive self-improvement."
- **Why unresolved:** The current system is training-free and does not incorporate mechanisms for learning from errors or user corrections during operation.
- **What evidence would resolve it:** Demonstrated improvement in system performance over time when deployed with human-in-the-loop feedback mechanisms.

### Open Question 3
- **Question:** How can the system achieve accuracy levels sufficient for professional certification in structural engineering?
- **Basis in paper:** While MASSE achieves >85% accuracy, structural engineering requires near-perfect reliability for safety-critical decisions. The paper acknowledges "even small misjudgments can produce catastrophic outcomes."
- **Why unresolved:** The gap between 85-95% benchmark accuracy and the professional standard for certified structural analysis remains unaddressed.
- **What evidence would resolve it:** Systematic failure mode analysis and accuracy improvements demonstrating >99% reliability on safety-critical subtasks, validated by professional engineers.

## Limitations

- **Task Scope:** Evaluation focuses on specific structural domain with standardized problem formats; generalization to other engineering domains remains untested
- **Performance Scaling:** Runtime reduction assumes stable LLM API performance; real-world deployment with concurrent requests could impact throughput and cost
- **Safety Verification:** Final compliance decisions based on structured outputs, but edge cases where agents disagree or tool outputs are ambiguous are not fully addressed

## Confidence

- **High Confidence:** Structured JSON communication reducing error propagation is well-supported by ablation studies
- **Medium Confidence:** Runtime and accuracy benchmarks are internally consistent but rely on specific LLM APIs and hardware configurations
- **Low Confidence:** Safety and robustness claims for real-world deployment are inferred from controlled experiments but lack validation under adversarial inputs

## Next Checks

1. **Domain Transfer Test:** Apply MASSE to a non-racking structural problem (e.g., 2D frame analysis) and measure accuracy degradation to validate generality beyond training domain
2. **Error Propagation Audit:** Intentionally corrupt output of one agent and trace how error propagates through workflow to identify weak points in error-handling chain
3. **Tool Failure Simulation:** Simulate OpenSees crashes or RAG retrieval failures and verify if system gracefully degrades (e.g., retries, fallbacks) or propagates failure to final verdict