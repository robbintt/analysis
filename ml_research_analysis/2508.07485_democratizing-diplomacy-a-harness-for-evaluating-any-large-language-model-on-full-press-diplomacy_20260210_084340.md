---
ver: rpa2
title: 'Democratizing Diplomacy: A Harness for Evaluating Any Large Language Model
  on Full-Press Diplomacy'
arxiv_id: '2508.07485'
source_url: https://arxiv.org/abs/2508.07485
tags:
- strategic
- game
- figure
- diplomacy
- kimi-k2
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We present a framework for evaluating any off-the-shelf large language
  model on full-press Diplomacy without fine-tuning or specialized training. Using
  data-driven prompt optimization, even 24B parameter models can reliably complete
  games by playing aggressive strategies and maintaining coherent diplomatic relationships.
---

# Democratizing Diplomacy

## Quick Facts
- **arXiv ID**: 2508.07485
- **Source URL**: https://arxiv.org/abs/2508.07485
- **Reference count**: 40
- **Primary result**: Framework evaluates any off-the-shelf LLM on full-press Diplomacy without fine-tuning

## Executive Summary
This paper introduces a framework for evaluating large language models on full-press Diplomacy using data-driven prompt optimization without specialized training. The system enables any off-the-shelf model to complete games by playing aggressive strategies and maintaining coherent diplomatic relationships. Benchmarking 16 contemporary models reveals that larger models consistently outperform smaller ones while all achieve competent gameplay. The framework democratizes strategic reasoning evaluation by demonstrating that complex behaviors like betrayal timing and coalition building emerge naturally from general-purpose language models.

## Method Summary
The framework employs a data-driven prompt optimization methodology that allows evaluation of any off-the-shelf large language model on full-press Diplomacy without fine-tuning or specialized training. The system uses Critical State Analysis to enable efficient experimentation by replaying key game moments rather than running complete games. This approach makes it possible to test multiple models and strategies efficiently while maintaining the complexity of diplomatic gameplay. The framework captures how models exhibit distinct strategic personalities and can adapt their playstyle based on opponent strength.

## Key Results
- 24B parameter models can reliably complete full-press Diplomacy games using aggressive strategies
- Larger models consistently outperform smaller models in the 16-model benchmark
- Critical State Analysis methodology enables efficient experimentation by replaying key game moments
- Models exhibit distinct strategic personalities and adapt playstyle based on opponent strength

## Why This Works (Mechanism)
The framework works by leveraging the natural language understanding capabilities of large language models to interpret diplomatic contexts and generate appropriate responses. Data-driven prompt optimization tailors the model's instructions to the specific demands of Diplomacy gameplay, enabling coherent strategy formation and diplomatic relationship management. The Critical State Analysis approach focuses computational resources on pivotal decision points rather than entire game sequences, making evaluation more tractable while preserving strategic complexity.

## Foundational Learning

**Prompt Engineering**: Essential for guiding model behavior without fine-tuning. Models require carefully crafted prompts to understand game state and generate appropriate diplomatic responses. Quick check: Test prompt variations across models to identify optimal formulations.

**Strategic Reasoning**: Models must balance short-term tactical decisions with long-term strategic planning. Success depends on maintaining coherent narratives across multiple turns. Quick check: Verify models can track alliance commitments over extended gameplay.

**Diplomatic Communication**: Effective gameplay requires modeling trust, deception, and coalition dynamics. Models must generate persuasive messages while interpreting opponents' intentions. Quick check: Assess message coherence and strategic alignment across diplomatic exchanges.

## Architecture Onboarding

**Component Map**: Data Collection -> Prompt Optimization -> Game Simulation -> Critical State Analysis -> Performance Evaluation

**Critical Path**: Prompt Optimization -> Game Simulation -> Critical State Analysis
The system requires optimized prompts before gameplay, and Critical State Analysis depends on completed game simulations to identify key moments.

**Design Tradeoffs**: 
- Off-the-shelf vs. fine-tuned models: Democratization achieved but performance ceiling constrained
- Full games vs. Critical State Analysis: Complete strategic context vs. experimental efficiency
- General vs. domain-specific prompts: Broader applicability vs. optimal performance

**Failure Signatures**:
- Inconsistent diplomatic messaging indicating poor context tracking
- Tactical decisions that undermine long-term strategic positioning
- Failure to recognize betrayal opportunities or threats

**First Experiments**:
1. Run baseline evaluation of smallest and largest models to establish performance range
2. Test prompt variations on a single model to optimize diplomatic communication
3. Implement Critical State Analysis on completed games to identify key decision points

## Open Questions the Paper Calls Out
None

## Limitations
- Off-the-shelf model constraint limits performance compared to specialized agents
- Data-driven prompt optimization may miss domain-specific nuances
- Critical State Analysis could miss emergent patterns only visible in full game trajectories

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Models reliably complete games and exhibit distinct strategic personalities | High |
| Larger models consistently outperform smaller ones | High |
| Complex behaviors emerge naturally from general-purpose models | Medium |
| 24B parameter models achieve "reliable" gameplay completion | Medium |

## Next Checks
1. Conduct ablation studies comparing prompt-optimized off-the-shelf models against minimally fine-tuned versions to quantify performance gap
2. Implement multi-seed replication of key experiments to establish statistical significance
3. Develop cross-linguistic evaluation to test whether strategic reasoning patterns transfer across language models trained on different linguistic corpora