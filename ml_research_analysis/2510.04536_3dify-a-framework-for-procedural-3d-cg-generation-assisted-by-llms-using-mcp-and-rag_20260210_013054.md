---
ver: rpa2
title: '3Dify: a Framework for Procedural 3D-CG Generation Assisted by LLMs Using
  MCP and RAG'
arxiv_id: '2510.04536'
source_url: https://arxiv.org/abs/2510.04536
tags:
- dify
- generation
- d-cg
- tools
- procedural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 3Dify is a procedural 3D computer graphics generation framework
  that enables users to create 3D models using only natural language instructions.
  The framework leverages Large Language Models (LLMs) and integrates state-of-the-art
  technologies such as the Model Context Protocol (MCP) and Retrieval-Augmented Generation
  (RAG).
---

# 3Dify: a Framework for Procedural 3D-CG Generation Assisted by LLMs Using MCP and RAG

## Quick Facts
- arXiv ID: 2510.04536
- Source URL: https://arxiv.org/abs/2510.04536
- Authors: Shun-ichiro Hayashi; Daichi Mukunoki; Tetsuya Hoshino; Satoshi Ohshima; Takahiro Katagiri
- Reference count: 20
- One-line primary result: Enables 3D model creation from natural language using LLM-controlled DCC tools via MCP and RAG

## Executive Summary
3Dify is a procedural 3D computer graphics generation framework that enables users to create 3D models using only natural language instructions. The framework leverages Large Language Models (LLMs) and integrates state-of-the-art technologies such as the Model Context Protocol (MCP) and Retrieval-Augmented Generation (RAG). Built upon the open-source Dify platform, 3Dify automates operations of Digital Content Creation (DCC) tools like Blender, Unreal Engine, and Unity, either through MCP or Computer-Using Agent (CUA) methods.

## Method Summary
3Dify is built on the Dify platform and uses three specialized LLM agents (Visualizer, Planner, Manager) to control DCC tools through MCP or CUA methods. The framework generates 2D pre-visualization images for user selection, extracts procedural parameters through RAG-enhanced LLMs, and executes operations on DCC tools via Python scripts or GUI automation. The system supports local LLM deployment for improved security and cost efficiency.

## Key Results
- Successfully generated a desktop gaming PC model from a single natural language prompt
- Demonstrated efficient integration of MCP protocol for DCC tool automation
- Showed flexible framework that supports both local and cloud LLM deployments
- Proved effective use of image-selection feedback loops for refining procedural generation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can control DCC tools through MCP by generating tool calls that execute API commands
- Mechanism: The Manager LLM receives procedural instructions from the Planner LLM and invokes MCP tools, which translate to Python scripts executed in the DCC tool's console. The "code" tool in Blender MCP allows arbitrary Python execution, bypassing per-function MCP tool implementations.
- Core assumption: The LLM can generate syntactically and semantically correct API calls for the target DCC tool without visual feedback during 3D generation.
- Evidence anchors: [section V]: "a large proportion of the calls among the available MCP tools were to the 'code' tool. This tool executes Blender operation scripts written in Python, which are generated by the Manager LLM."

### Mechanism 2
- Claim: Multi-agent role separation improves task decomposition for procedural 3D generation
- Mechanism: Three specialized LLMs (Visualizer → Planner → Manager) form a pipeline where each handles a distinct abstraction level: Visualizer generates 2D pre-viz candidates, Planner extracts procedural parameters and variability scope from selected images, and Manager executes operations on the DCC tool.
- Core assumption: Role specialization reduces cognitive load on individual LLMs and prevents context confusion better than a single monolithic agent.
- Evidence anchors: [section IV-A]: Explicit role definitions for Visualizer, Planner, and Manager LLMs with distinct responsibilities

### Mechanism 3
- Claim: Image-selection feedback loops enable LLMs to learn user preferences and refine procedural parameters
- Mechanism: Users select preferred images from 2D pre-viz candidates; the LLM infers variable patterns from selections (e.g., style, proportions, color schemes) and applies them to subsequent generations.
- Core assumption: 2D visual preferences map meaningfully to 3D procedural parameters, and the LLM can perform this mapping without explicit parameter exposure.
- Evidence anchors: [section III-A, Step 2]: "an LLM presents multiple 2D (not 3D) image candidates as pre-visualization (pre-viz) images. The user then selects several images that are closest to the intended result"

## Foundational Learning

- Concept: **Model Context Protocol (MCP) architecture**
  - Why needed here: MCP is the primary interface through which 3Dify communicates with DCC tools. Understanding client-server separation, tool definitions, and resource exposure is essential for extending MCP support to new tools.
  - Quick check question: Can you explain how an MCP server exposes a "tool" vs. a "resource," and why 3Dify's MCP server template uses both?

- Concept: **Procedural generation node graphs and parameterization**
  - Why needed here: The Planner LLM must extract procedural parameters from visual inputs. Without understanding what parameters control (e.g., roof height dependency on wall height), you cannot design effective RAG content or validate LLM outputs.
  - Quick check question: Given a procedural house generator with wall_height and roof_pitch parameters, how would changing wall_height affect roof geometry if they have a dependency relationship?

- Concept: **Conversation state management in multi-turn LLM workflows**
  - Why needed here: 3Dify's Chatflow relies on conversation variables for agent coordination and loop control. Misunderstanding state persistence will cause silent failures where agents lose context mid-workflow.
  - Quick check question: In Dify's Chatflow, what happens to intermediate LLM outputs when control returns to the start node, and how must you handle data you need in subsequent turns?

## Architecture Onboarding

- Component map: User Prompt -> Visualizer LLM -> User Selection -> Planner LLM -> Manager LLM -> MCP Client -> DCC Tool MCP Server -> 3D Model
- Critical path:
  1. User prompt → Visualizer LLM generates 2D pre-viz candidates
  2. User selects preferred images → LLM extracts variable patterns
  3. Planner LLM analyzes selections, outputs procedural parameters and instructions
  4. Manager LLM receives instructions, invokes MCP tools or CUA
  5. MCP server executes Python/API calls in DCC tool → 3D model generated
  6. Optional refinement loop returns to step 1 or 3 based on user feedback
- Design tradeoffs:
  - **MCP vs. CUA**: MCP is more reliable but requires per-function server implementation; CUA accesses all GUI features but has higher misoperation risk and requires attempt limits
  - **Local vs. cloud LLMs**: Local deployment reduces API costs and improves data security but limits model selection and requires GPU infrastructure
  - **RAG knowledge scope**: Tool-specific documentation improves coverage but increases maintenance; procedural knowledge sharing across tools improves reusability but may introduce tool-incompatible patterns
- Failure signatures:
  - **Spatial coherence loss**: Objects misaligned or protruding after multi-step operations (documented in PC case movement failure) — suggests context overflow or insufficient coordinate tracking
  - **MCP tool bloat**: Context window exhaustion as MCP tool count grows, since each tool includes metadata — consider dynamic tool loading or CUA fallback
  - **Chatflow state loss**: Agents repeating or skipping stages — check that conversation variables are written in the green "state update" zone before returning to start node
- First 3 experiments:
  1. **Minimal MCP validation**: Install Blender MCP server, configure Dify MCP client, prompt "Create a red cube at origin" — verify MCP tool invocation and Python script execution in Blender console logs
  2. **Feedback loop convergence test**: Use Visualizer LLM to generate 4 pre-viz images of a simple object (e.g., chair), select 2, observe whether next iteration reflects selection patterns — document number of iterations to user satisfaction
  3. **Multi-agent state stress test**: Design a workflow requiring Planner → Manager handoff with 5+ intermediate variables; intentionally omit one conversation variable write and confirm state loss manifests as expected

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the framework maintain spatial coherence and object relationships during iterative modifications of complex 3D scenes?
- Basis in paper: [explicit] In Section V, the authors note that an instruction to "move the entire PC upward" failed, inferring that "the LLM may have struggled to fully track the positional relationships among dozens of objects" as tasks accumulated.
- Why unresolved: The current implementation relies on the Manager LLM's context, which appears to lose track of coordinate states in multi-step procedural generation.
- What evidence would resolve it: Successful execution of iterative positional adjustments on multi-component assemblies without manual intervention or coordinate drift.

### Open Question 2
- Question: Does incorporating visual feedback into the 3D generation loop improve the accuracy of the Manager LLM compared to text-only command execution?
- Basis in paper: [explicit] Section V states that "the LLM did not incorporate any visual information in the 3D-CG generation process" during the demonstration, relying solely on text-based MCP tool calls.
- Why unresolved: While the framework supports a 2D pre-viz feedback loop, the actual 3D construction phase is currently "blind," potentially causing logic errors that visual self-correction could solve.
- What evidence would resolve it: Comparative experiments showing fewer structural errors when visual context (screenshots) is provided to the Manager LLM during the build phase.

### Open Question 3
- Question: What is the reliability trade-off between MCP-based control and Computer-Using Agent (CUA) methods for operating DCC tools?
- Basis in paper: [inferred] Section IV-E2 notes that CUA carries a "higher risk of misoperation" than MCP, yet the demonstration in Section V utilized only MCP.
- Why unresolved: The paper claims CUA allows extensibility to features lacking APIs, but it does not demonstrate this capability or quantify the failure rates associated with GUI automation versus direct API control.
- What evidence would resolve it: A user study or benchmark measuring task completion success rates and error frequencies for identical generation tasks performed via MCP vs. CUA.

## Limitations
- Spatial coherence maintenance across multi-step operations remains poorly validated with documented failures in maintaining object relationships
- Transferability of 2D visual preferences to 3D procedural parameters lacks empirical validation
- MCP tool context window limitations could cause failures as tool ecosystems grow

## Confidence
- **High Confidence**: The multi-agent role separation architecture and MCP-based DCC tool control mechanisms are technically sound and well-documented through implementation details.
- **Medium Confidence**: The image-selection feedback loop for preference learning is conceptually valid but lacks empirical validation of its effectiveness in transferring 2D visual preferences to 3D procedural parameters.
- **Low Confidence**: Spatial coherence maintenance across complex multi-step operations is the most significant limitation, with only anecdotal evidence of failure rather than systematic evaluation.

## Next Checks
1. **Spatial Coherence Stress Test**: Systematically measure object alignment accuracy across 10+ consecutive edit operations on complex multi-component models, comparing MCP vs CUA approaches and documenting failure rates.
2. **Feedback Loop Convergence Analysis**: Conduct a controlled user study measuring how many iterations are required for the feedback loop to converge to user preferences across different object types, quantifying diversity and convergence quality.
3. **MCP Context Window Scalability Test**: Measure LLM token usage and error rates as MCP tool count increases from 10 to 50+ tools, testing dynamic tool loading strategies and CUA fallback reliability under context pressure.