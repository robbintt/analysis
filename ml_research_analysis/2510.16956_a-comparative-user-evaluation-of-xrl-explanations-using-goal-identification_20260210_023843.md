---
ver: rpa2
title: A Comparative User Evaluation of XRL Explanations using Goal Identification
arxiv_id: '2510.16956'
source_url: https://arxiv.org/abs/2510.16956
tags:
- explanation
- goal
- users
- agent
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel evaluation methodology for explainable
  reinforcement learning (XRL) called "goal identification," where users predict an
  agent's underlying objective from explanations of its decision-making. The approach
  trains multiple agents with different reward functions in Ms.
---

# A Comparative User Evaluation of XRL Explanations using Goal Identification

## Quick Facts
- **arXiv ID:** 2510.16956
- **Source URL:** https://arxiv.org/abs/2510.16956
- **Reference count:** 20
- **Key outcome:** User study shows only one XRL explanation mechanism achieved significantly better-than-random accuracy (53.0%), while others performed near chance levels (22.5-34.9%)

## Executive Summary
This paper introduces a novel evaluation methodology for explainable reinforcement learning (XRL) called "goal identification," where users predict an agent's underlying objective from explanations of its decision-making. The approach trains multiple agents with different reward functions in Ms. Pacman, then tests whether explanations help users distinguish between these goals. A user study with 100 participants evaluated four XRL explanation algorithms across four agent goals. Results showed that only one explanation mechanism (Dataset Similarity Explanation) achieved significantly better-than-random accuracy (53.0%), while others performed near chance levels (22.5-34.9%). Critically, users' self-reported confidence, ease of identification, and understanding showed weak or no correlation with actual accuracy, and users were often overconfident in their predictions. This highlights a significant gap between subjective user preferences and objective explanatory effectiveness in XRL, suggesting that current explanations may not adequately support real-world debugging tasks.

## Method Summary
The researchers developed a goal identification evaluation framework where users must identify an agent's objective from its explanations. They trained multiple reinforcement learning agents in Ms. Pacman with different reward functions targeting specific goals (eating pills, fruits, ghosts, or mixing objectives). Four XRL explanation algorithms were implemented: Reward Decomposition, Dataset Similarity Explanation, Attentive Guidance, and a baseline that simply shows the next action. In a user study with 100 participants, each user evaluated 16 episodes (4 agents × 4 episodes each), predicting the agent's goal after viewing explanations. Performance was measured as the percentage of correctly identified goals, and participants also rated their confidence, ease of identification, and understanding after each episode.

## Key Results
- Only the Dataset Similarity Explanation algorithm achieved significantly better-than-random accuracy (53.0%)
- Three other explanation algorithms performed near chance levels (22.5-34.9%)
- Self-reported measures of confidence, ease of identification, and understanding showed weak or no correlation with actual accuracy
- Users were often overconfident in their predictions, highlighting a disconnect between subjective experience and objective performance

## Why This Works (Mechanism)
The goal identification methodology works by creating a controlled experimental environment where the ground truth about agent objectives is known. By training agents with distinct reward functions, the researchers can objectively measure whether explanations help users identify these predetermined goals. The approach leverages the fundamental property of reinforcement learning where an agent's behavior directly reflects its reward structure, making it possible to create clear, measurable objectives for evaluation.

## Foundational Learning
- **Reinforcement Learning Fundamentals** (why needed: to understand how agents learn from rewards and make decisions; quick check: can you explain the relationship between reward functions and agent behavior?)
- **Explainable AI Concepts** (why needed: to grasp different explanation mechanisms and their purposes; quick check: can you describe at least three types of XRL explanations?)
- **Human-Computer Interaction Principles** (why needed: to understand how users interact with and interpret AI explanations; quick check: can you explain the difference between objective performance and subjective user experience?)
- **Experimental Design Methodology** (why needed: to understand how to create controlled studies that isolate specific variables; quick check: can you identify the independent and dependent variables in this study?)

## Architecture Onboarding

**Component Map:** User Interface -> XRL Explanation Generator -> RL Agent -> Game Environment -> Reward Function

**Critical Path:** User views explanation → User makes goal prediction → System records accuracy → User rates confidence/ease/understanding

**Design Tradeoffs:** The study prioritized controlled experimental conditions over ecological validity by using a single game environment and simplified goal identification task. This tradeoff allowed for clear measurement of explanation effectiveness but may limit generalizability to real-world applications.

**Failure Signatures:** Poor explanation performance (near-chance accuracy) indicates the explanation mechanism fails to convey meaningful information about agent objectives. Weak correlation between subjective ratings and accuracy suggests users cannot reliably self-assess explanation quality.

**First 3 Experiments to Run:**
1. Replicate the study with different RL environments to test generalizability
2. Test explanation algorithms with varying levels of complexity and detail
3. Conduct A/B testing with different explanation presentation formats

## Open Questions the Paper Calls Out
None

## Limitations
- Constrained experimental design with only 100 participants and single game environment limits generalizability
- Use of specific set of four XRL algorithms and four agent goals may not capture full diversity of real-world applications
- Binary "goal identification" task represents a simplified view of how users typically interact with XRL explanations in practice

## Confidence
- **High:** Gap between subjective preferences and objective explanatory effectiveness is well-supported
- **Medium:** Comparative performance of different XRL algorithms shows clear ranking but small effect sizes
- **Low:** Generalizability to other RL domains and real-world applications

## Next Checks
1. Replicate the study across multiple RL environments and tasks to assess generalizability of the goal identification methodology
2. Conduct a larger-scale user study with diverse participant backgrounds to validate the disconnect between subjective preferences and objective performance
3. Test the impact of varying explanation presentation formats and interaction methods on user performance and confidence calibration