---
ver: rpa2
title: Unsupervised Learning for Class Distribution Mismatch
arxiv_id: '2505.06948'
source_url: https://arxiv.org/abs/2505.06948
tags:
- class
- classes
- instances
- negative
- known
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes UCDM, an unsupervised approach for class distribution
  mismatch (CDM) problems that avoids labeled data entirely. The method generates
  positive and negative training pairs using diffusion models: positives are created
  by adding noise to real images and conditioning on class prompts, while negatives
  are generated by erasing semantic classes from real images via conditional DDIM
  inversion followed by unconditional denoising.'
---

# Unsupervised Learning for Class Distribution Mismatch

## Quick Facts
- **arXiv ID:** 2505.06948
- **Source URL:** https://arxiv.org/abs/2505.06948
- **Reference count:** 40
- **Primary result:** UCDM achieves 35.1%, 63.7%, and 72.5% higher accuracy on known, unknown, and new classes respectively compared to OpenMatch on Tiny-ImageNet with 60% class mismatch.

## Executive Summary
This paper addresses the class distribution mismatch (CDM) problem by proposing UCDM, an unsupervised method that trains classifiers without any labeled data. UCDM uses diffusion models to generate synthetic positive and negative training pairs: positives are created by adding noise to real images and conditioning on class prompts, while negatives are generated by erasing semantic classes from real images via conditional DDIM inversion followed by unconditional denoising. A confidence-based pseudo-labeling mechanism iteratively incorporates real images into training. UCDM outperforms semi-supervised baselines significantly on Tiny-ImageNet with 60% mismatch and demonstrates strong performance on CIFAR-10/100, achieving competitive results in both closed-set and open-set tasks without any ground truth labels.

## Method Summary
UCDM solves CDM by training a classifier using only unlabeled data and a list of known class names. The method generates synthetic training pairs using a frozen Stable Diffusion 2.0 model: for each seed image and known class, it creates a positive instance by adding noise and conditioning on the class prompt, and a negative instance by conditionally inverting the image to erase the class semantic while preserving visual structure. These synthetic pairs are used to train a WideResNet-28-2 classifier with a contrastive loss that maximizes probability for positives and minimizes it for negatives. A confidence-based pseudo-labeling mechanism iteratively labels high-confidence real data and incorporates it into training to reduce domain shift. The approach achieves strong performance in both closed-set (known classes) and open-set (unknown/new classes) scenarios.

## Key Results
- Achieves 35.1%, 63.7%, and 72.5% higher accuracy on known, unknown, and new classes respectively compared to OpenMatch on Tiny-ImageNet with 60% mismatch
- Outperforms semi-supervised baselines that use 40 labels per class
- Demonstrates competitive performance on CIFAR-10/100 datasets
- Effective across mismatch proportions from 20% to 75% unknown classes
- Shows strong balance between closed-set and open-set performance

## Why This Works (Mechanism)

### Mechanism 1: Semantic Erasure via Conditional Inversion
The paper utilizes Conditional DDIM Inversion to remove specific semantic attributes while preserving visual structure, creating synthetic "negative" samples for unknown classes. The model uses a conditional guide (class prompt) to push the latent representation away from the target class's probability manifold, theoretically minimizing the likelihood p(y|x) (Theorem 3.1). An unconditional reverse process then reconstructs the image, recovering visual fidelity without the semantic label. The core assumption is that the gradient direction effectively isolates and removes class semantics without destroying the image's latent manifold.

### Mechanism 2: Contrastive Generalization from Synthetic Pairs
Training on synthetic positive-negative pairs (where positives contain the class and negatives lack it) allows the model to define boundaries for "known" vs. "other" without ground-truth labels. A seed image is transformed into positive (class added) and negative (class erased) instances. The classifier is trained to maximize probability for positives and minimize it for negatives, forcing the feature encoder to learn discriminative features for known classes that are robust to visual variations in seed images. The core assumption is that features learned from diffusion-generated samples transfer effectively to real-world test distributions.

### Mechanism 3: Confidence-Based Real Data Grounding
Synthetic data alone creates a domain gap; iterative labeling of high-confidence real data stabilizes the classifier and aligns it with the target distribution. The model assigns pseudo-labels to real unlabeled data based on a dual-confidence metric (known-probability and other-probability). These high-confidence real samples are incorporated into training sets, iteratively reducing the distribution shift between synthetic training manifold and real test manifold. The core assumption is that initial predictions on real data are sufficiently accurate to generate clean pseudo-labels before confirmation bias sets in.

## Foundational Learning

- **Concept: Diffusion Probabilistic Models (DDPM/DDIM)**
  - Why needed here: The core engine of UCDM is Stable Diffusion. Understanding the difference between forward (adding noise) and reverse (denoising) processes, and specifically how DDIM inversion works, is required to understand how "erasure" is possible.
  - Quick check question: Can you explain how a deterministic DDIM inversion differs from a stochastic diffusion reverse process?

- **Concept: Open-Set Recognition (OSR)**
  - Why needed here: This is the problem formulation. You must understand that the goal is not just classifying K classes, but detecting inputs that do not belong to any of the K classes (classifying them as "other").
  - Quick check question: How does the softmax output of a standard classifier fail in an open-set scenario?

- **Concept: Class Distribution Mismatch (CDM)**
  - Why needed here: The paper distinguishes CDM from standard OSR. In CDM, the training unlabeled data contains classes ("unknown") that are not in the labeled set (or in this unsupervised case, not in the predefined "known" list).
  - Quick check question: If the unlabeled training set only contained "known" classes, would UCDM still be necessary?

## Architecture Onboarding

- **Component map:** Input Dataset + Known Class Names -> Stable Diffusion 2.0 (Frozen) -> Conditional Inversion & Reverse module -> Generated $D_P, D_N$ -> WideResNet-28-2 Backbone + K Binary Classifiers + K-way Classifier -> Confidence Labeler -> Updates Real Data Buffer

- **Critical path:** The Negative Instance Generation (Section 3.4). If the conditional inversion step does not successfully strip the semantic class while preserving structure, the "other" class detector will fail to generalize.

- **Design tradeoffs:**
  - Frozen vs. Fine-tuned Diffusion: The authors freeze Stable Diffusion 2.0 to save compute and prevent overfitting to the specific unlabeled set, trading off potential domain alignment for generalization.
  - Inference Cost: High overhead due to generating diffusion samples for every seed image before training starts.
  - Threshold Sensitivity: The confidence threshold $\delta$ is critical; the paper suggests $\delta > 0.95$ (Section 4.3).

- **Failure signatures:**
  - Domain Drift: Generated images look "cartoonish" compared to real seed images. Likely cause: Random noise injection ($\sigma_t$) is too high in the positive pipeline (should be 1.0, see Appendix B.3).
  - Collapse to "Other": Model predicts everything as "Other". Likely cause: Negative instances are too visually distinct from positive instances (erasure removed too much visual info), or confidence threshold is too low.

- **First 3 experiments:**
  1. Visual Sanity Check: Run the generation pipeline (Algorithm 1) on 10 seed images. Visually verify that "Negative" instances look like the seed but lack the specified class semantic (e.g., a "dog" is removed, but the background remains).
  2. Ablate Generation Source: Train the classifier using only random noise generated images vs. the proposed seed-based images to quantify the domain shift penalty (compare with Table 5 results).
  3. Mismatch Proportion Sweep: Replicate the 20% to 75% mismatch experiment on CIFAR-10 to ensure the "Balance Score" metric holds as unknown classes dominate the unlabeled set.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can integrating Large Language Models (LLMs) to automate prompt enrichment overcome the limitation of fixed prompt variability and improve positive instance diversity?
- Basis in paper: The Conclusion explicitly states, "Limited prompt variability restricts positive instance diversity in UCDM. Integrating a large language model could improve this."
- Why unresolved: The current implementation relies on a fixed template ("A photo of a [CLASS]"), which restricts the semantic richness and variety of the generated training data.
- What evidence would resolve it: Experiments comparing classifier performance when using LLM-generated diverse prompts versus the fixed template, specifically measuring the variance in generated images and resulting accuracy.

### Open Question 2
- Question: Does the conditional DDIM inversion mechanism for negative instance generation scale effectively to high-resolution imagery without introducing visual artifacts?
- Basis in paper: The experiments are restricted to low-resolution (32x32) datasets like CIFAR and Tiny-ImageNet, despite utilizing a Stable Diffusion backbone typically trained on higher resolutions.
- Why unresolved: The theoretical assumption that the approximation error ($\delta_t$) is negligible is empirically validated only on simple, low-resolution images; it is unclear if semantic erasure remains clean and artifact-free on high-fidelity data.
- What evidence would resolve it: Evaluation on high-resolution benchmarks (e.g., ImageNet-1K at 256x256 or higher) to assess the visual fidelity of erased instances and the corresponding stability of the contrastive loss.

### Open Question 3
- Question: How does UCDM perform when the predefined "known" classes are semantically distant from the frozen diffusion model's pre-training distribution?
- Basis in paper: The method relies on a frozen Stable Diffusion 2.0 model to synthesize and erase classes, implicitly assuming the model has sufficient prior knowledge of the target concepts.
- Why unresolved: If the target task involves niche domains (e.g., specialized manufacturing defects or rare biological species) not well-represented in the diffusion model's training set, the generation of accurate positive-negative pairs may fail.
- What evidence would resolve it: Experiments on domain-specific datasets (e.g., medical imaging or satellite data) where the class concepts are out-of-distribution for the pre-trained diffusion backbone.

## Limitations
- Generation Quality Dependence: Performance fundamentally tied to diffusion model's semantic understanding; poor representation of certain classes leads to failed negative generation
- Hyperparameter Sensitivity: Critical confidence threshold and noise level parameters lack systematic sensitivity analysis across datasets
- BN Statistics Degradation: Batch normalization statistics corruption noted as "crucial" but lacks quantitative impact analysis or standardized fix

## Confidence
- **High Confidence:** The core mechanism of conditional inversion for negative generation (Theorem 3.1 is mathematically sound); empirical superiority on Tiny-ImageNet with 60% mismatch (35.1%, 63.7%, 72.5% gains over OpenMatch)
- **Medium Confidence:** The effectiveness of iterative confidence-based labeling loop for stabilizing training; lack of ablation for necessity vs. static pseudo-labels
- **Medium Confidence:** The closed-set vs. open-set performance balance; competitive closed-set performance but no deep analysis of potential trade-offs

## Next Checks
1. **Ablate the Confidence Threshold:** Re-run CIFAR-10 experiments with $\delta \in \{0.90, 0.95, 0.98, 0.995\}$ to quantify sensitivity of pseudo-label quality to this hyperparameter
2. **Test on a Known-Diffusion Model:** Swap Stable Diffusion 2.0 for Stable Diffusion 1.4 (or another variant) and re-run the Tiny-ImageNet 60% mismatch experiment to test robustness of generation quality to underlying model
3. **Visualize BN Statistics:** Log the running mean and variance of BatchNorm layers in the WideResNet during training on synthetic vs. real data to confirm if reported "degradation" is observable and quantifiable