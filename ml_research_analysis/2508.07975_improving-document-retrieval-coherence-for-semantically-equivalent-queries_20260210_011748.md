---
ver: rpa2
title: Improving Document Retrieval Coherence for Semantically Equivalent Queries
arxiv_id: '2508.07975'
source_url: https://arxiv.org/abs/2508.07975
tags:
- query
- queries
- retrieval
- coherence
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a coherence ranking loss for dense retrieval
  models that improves both retrieval accuracy and ranking consistency across semantically
  equivalent queries. The proposed loss extends the Multiple Negative Ranking objective
  by penalizing embedding misalignment between lexical variations of the same query
  and enforcing consistent similarity margins between queries and documents.
---

# Improving Document Retrieval Coherence for Semantically Equivalent Queries

## Quick Facts
- arXiv ID: 2508.07975
- Source URL: https://arxiv.org/abs/2508.07975
- Authors: Stefano Campese; Alessandro Moschitti; Ivano Lauriola
- Reference count: 35
- Primary result: +0.6% to +1.8% NDCG improvement with +15% to +29% RBO coherence gains

## Executive Summary
This paper introduces a coherence ranking loss (CR) for dense retrieval models that simultaneously improves retrieval accuracy and ranking consistency across semantically equivalent queries. The method extends the Multiple Negative Ranking objective by penalizing embedding misalignment between lexical variations of the same query and enforcing consistent similarity margins between queries and documents. Experiments across MS-MARCO, Natural Questions, BEIR, and TREC DL benchmarks demonstrate that models trained with CR loss achieve both better coherence (measured by RBO) and modest relevance gains, with the effect generalizing across different PLMs including MPNet, MiniLM, and ModernBERT.

## Method Summary
The approach combines three components in a unified loss function: Query Embedding Alignment (QEA) penalizes distance between embeddings of semantically equivalent queries via MSE; Similarity Margin Consistency (SMC) ensures ranking margins computed using equivalent queries match; and standard Multi-Negative Ranking (MNR) maintains query-document discrimination. The method generates 10 lexical variations per query using Phi-3 LLM, mines hard negatives per query, and jointly optimizes all three loss components during training. The CR loss is shown to improve both coherence (RBO@5 increases from 0.46 to 0.60 on MS-MARCO) and relevance metrics (NDCG@10 improves by +0.47 on MS-MARCO, +1.69 on NQ) compared to standard fine-tuning baselines.

## Key Results
- RBO@5 improves by +15% absolute on MS-MARCO (0.46→0.60) and +29% on Natural Questions (0.54→0.70)
- NDCG@10 gains of +0.47 on MS-MARCO and +1.69 on Natural Questions compared to fine-tuning baseline
- Significant improvements in retrieve-and-rerank scenarios: probability of retrieving same top-5 document increases from 0.63 to 0.81 on MS-MARCO
- Effectiveness generalizes across PLMs (MPNet, MiniLM, ModernBERT) and BEIR zero-shot transfer

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Aligning embeddings of semantically equivalent queries improves retrieval coherence without degrading relevance.
- Mechanism: The Query Embedding Alignment (QEA) component penalizes distance between embeddings of lexically different but semantically equivalent queries via Mean Squared Error, encouraging the encoder to map paraphrases to nearby points in embedding space.
- Core assumption: Equivalent queries should occupy similar regions in embedding space regardless of lexical variation.
- Evidence anchors:
  - [abstract] "The loss penalizes discrepancies between the top-k ranked documents retrieved for diverse but semantic equivalent queries."
  - [section 3.2] "QEA component simply tries to align the embeddings of lexically different queries by penalizing their differences measured through Mean Squared Error (MSE)."
  - [corpus] Related work (TCDE, Topic-Centric Dual Expansion) suggests semantic alignment between queries and documents is critical, supporting the alignment premise, though direct corpus evidence for QEA specifically is limited.
- Break condition: If equivalent queries have genuinely different information needs (faulty equivalence labeling), alignment will degrade relevance by forcing distinct intents together.

### Mechanism 2
- Claim: Enforcing consistent similarity margins between equivalent queries and document pairs produces more stable rankings.
- Mechanism: The Similarity Margin Consistency (SMC) component ensures that for any positive-negative document pair, the margin (similarity difference) computed using query q matches the margin computed using equivalent query q_i, anchoring ranking behavior across paraphrases.
- Core assumption: Ranking stability requires not just embedding proximity but consistent similarity scoring behavior.
- Evidence anchors:
  - [section 3.2] "SMC targets alignment in similarity scores... enforces equivalent queries to have the same similarities when compared to the same positive and negative documents."
  - [table 3] Ablation shows LCR (combined QEA + SMC) achieves RBO@5 of 0.34 vs. 0.20 (QEA alone) or 0.22 (SMC alone) on MS-MARCO, indicating synergy.
  - [corpus] GRADA work on adversarial document attacks suggests ranking stability is vulnerable to manipulation; margin consistency may provide robustness, though this is not directly tested in the paper.
- Break condition: If document embeddings are poorly separated (many near-duplicate documents), enforcing margin consistency may amplify noise rather than reduce variance.

### Mechanism 3
- Claim: Joint optimization of coherence and relevance via CR loss yields both improved rank stability and modest relevance gains.
- Mechanism: CR loss combines QEA, SMC, and standard Multi-Negative Ranking (MNR) in a single objective, allowing gradients from coherence constraints to regularize the encoder while MNR maintains query-document discrimination.
- Core assumption: Coherence and relevance are not inherently conflicting objectives for dense retrievers.
- Evidence anchors:
  - [section 4.1] "LCR shows better results on both datasets on all relevance metrics" (+0.47 NDCG@10 MS-MARCO, +1.69 NDCG@10 NQ vs. FT baseline).
  - [table 2] RBO@5 improves from 0.46 (FT) to 0.60 (LCR) on MS-MARCO; from 0.54 to 0.70 on NQ.
  - [corpus] No direct corpus evidence for joint coherence-relevance optimization; this appears to be a novel contribution of this work.
- Break condition: If training data contains high label noise or inconsistent query-document relevance judgments, joint optimization may amplify inconsistencies.

## Foundational Learning

- Concept: Dense Retrieval with Dual Encoders
  - Why needed here: The method operates on dense vector representations of queries and documents produced by separate encoders; understanding how similarity is computed is prerequisite.
  - Quick check question: Given query embedding q and document embeddings d1, d2, how would you rank documents by relevance?

- Concept: Multi-Negative Ranking (MNR) Loss
  - Why needed here: CR loss extends MNR; you must understand the base objective to see what CR adds.
  - Quick check question: In a batch of N query-document pairs, how many negative samples does MNR implicitly use per query?

- Concept: Rank-Biased Overlap (RBO)
  - Why needed here: The paper's primary coherence metric; measures similarity between ranked lists with top-weighted importance.
  - Quick check question: Two ranked lists share the same top-3 items in different orders; will RBO@5 be higher or lower than if they shared items at positions 4-5?

## Architecture Onboarding

- Component map:
  Query Encoder (PLM) -> Document Encoder (PLM) -> CR Loss Module (QEA, SMC, MNR) -> Training Data Pipeline (query clusters + hard negatives)

- Critical path:
  1. Generate equivalent query clusters using Phi-3 (or similar LLM) with the provided prompt.
  2. Mine hard negatives per query using standard DR techniques (e.g., BM25 + cross-encoder reranking).
  3. For each batch, sample query clusters; compute all three loss components; backpropagate jointly.
  4. Tune λ1, λ2 coefficients (paper tested {0, 0.2, 0.5, 0.8, 1}) on validation set.

- Design tradeoffs:
  - More equivalent queries per cluster (k=10 in paper) improves coherence signal but increases generation cost and potential noise.
  - Higher λ1/λ2 values enforce stronger coherence but may over-regularize and reduce discrimination on diverse queries.
  - Using STS-pretrained checkpoints (vs. raw PLMs) provides better initialization but requires additional pre-training effort.

- Failure signatures:
  - Relevance drops but coherence improves: λ values too high; reduce coherence weighting.
  - Coherence low on "complex" queries (top-1/50 score gap <0.1): Expected; paper shows RBO drops significantly in these cases (0.17→0.34 FT→LCR on MS-MARCO).
  - Generated equivalent queries not truly equivalent: Manual validation required; paper reports 100% accuracy on 100-sample check, but distribution shift may occur.

- First 3 experiments:
  1. Replicate FT vs. LCR comparison on MS-MARCO dev split with MPNet; target RBO@5 improvement from ~0.46 to >0.55.
  2. Ablate QEA vs. SMC components individually to verify synergy (expect Table 3 pattern: combined > either alone).
  3. Test generalization: Train LCR on MS-MARCO, evaluate zero-shot on 3 BEIR datasets; target +0.3-0.5 NDCG@10 average improvement.

## Open Questions the Paper Calls Out

- How does retrieval coherence optimization impact real-world applications utilizing web-scale indexes containing billions of documents?
- To what extent does retriever coherence influence the final accuracy of end-to-end Retrieval-Augmented Generation (RAG) pipelines?
- Why does the combination of data augmentation and Coherence Ranking loss fail to improve document relevance compared to the loss alone on large datasets?

## Limitations
- Current experiments limited to academic benchmarks (MS-MARCO, NQ) with relatively small document collections compared to web-scale indexes
- Manual validation of query equivalence quality limited to 100 samples, providing limited statistical confidence for entire dataset
- Performance on datasets with different query characteristics (long queries, conversational search) remains untested

## Confidence

- **High Confidence**: Coherence improvements on MS-MARCO and Natural Questions datasets, effectiveness of joint QEA+SMC optimization over individual components, and practical benefits for retrieve-and-rerank applications.
- **Medium Confidence**: Generalization across different PLMs (MPNet, MiniLM, ModernBERT), effectiveness on BEIR zero-shot transfer, and scalability to datasets with different query characteristics.
- **Low Confidence**: Performance on datasets with different query distributions (e.g., long queries, conversational search), effectiveness when query equivalence generation quality degrades, and behavior in low-resource settings with fewer training queries.

## Next Checks

1. **Dataset Transfer Validation**: Evaluate the method on at least two additional retrieval datasets with different characteristics (e.g., TREC CAR for long queries, BioASQ for domain-specific content) to assess generalization limits.
2. **Equivalence Quality Sensitivity**: Systematically vary the quality of generated equivalent queries (e.g., by using different LLMs or reducing generation budget) to quantify robustness to query equivalence labeling errors.
3. **Real-World Application Test**: Implement the trained model in a retrieve-and-rerank pipeline on a live search system, measuring both ranking consistency across query variations and user satisfaction metrics.