---
ver: rpa2
title: 'Infinite Sampling: Efficient and Stable Grouped RL Training for Large Language
  Models'
arxiv_id: '2506.22950'
source_url: https://arxiv.org/abs/2506.22950
tags:
- sampling
- group
- memory
- micro
- decoding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the memory bottleneck in group-based reinforcement
  learning for large language models (LLMs), specifically when generating multiple
  responses per prompt for Group Reward Policy Optimization (GRPO). The proposed Infinite
  Sampling framework enables efficient and stable GRPO training by decoupling group
  size from GPU memory usage through three key components: (1) micro sampling groups
  that decompose large groups into memory-feasible decoding rounds; (2) continuous
  sampling that interleaves token-level generation across micro groups to maximize
  slot utilization; and (3) a length-aware scheduler combining prefix-based length
  prediction with a two-stage strategy (global grouping via FPTAS and runtime refill
  via SJF).'
---

# Infinite Sampling: Efficient and Stable Grouped RL Training for Large Language Models

## Quick Facts
- arXiv ID: 2506.22950
- Source URL: https://arxiv.org/abs/2506.22950
- Authors: Liangyu Wang; Huanyi Xie; Xinhai Wang; Tianjin Huang; Mengdi Li; Di Wang
- Reference count: 40
- One-line primary result: Reduces peak memory usage by over 50% and improves throughput by over 25% during GRPO training while maintaining full-length completions.

## Executive Summary
This paper addresses the memory bottleneck in group-based reinforcement learning for large language models, specifically when generating multiple responses per prompt for Group Reward Policy Optimization (GRPO). The proposed Infinite Sampling framework enables efficient and stable GRPO training by decoupling group size from GPU memory usage through three key components: (1) micro sampling groups that decompose large groups into memory-feasible decoding rounds; (2) continuous sampling that interleaves token-level generation across micro groups to maximize slot utilization; and (3) a length-aware scheduler combining prefix-based length prediction with a two-stage strategy (global grouping via FPTAS and runtime refill via SJF). Experimental results demonstrate that Infinite Sampling maintains full-length completions while significantly reducing memory requirements and improving decoding efficiency.

## Method Summary
Infinite Sampling enables memory-efficient GRPO training by decomposing large sample groups (G=32) into smaller micro groups (g=4) that can be decoded sequentially within fixed GPU memory constraints. The framework implements three key innovations: micro sampling groups that reuse a shared prompt KV cache while maintaining individual response KV caches in a fixed-size pool; continuous sampling that interleaves token generation across micro groups to minimize idle compute slots; and a length-aware scheduler that uses prefix-based length prediction with FPTAS global grouping and SJF runtime refill to balance load and maximize throughput. The method maintains GRPO's requirement for group-normalized rewards while enabling training on standard GPU hardware without memory exhaustion.

## Key Results
- Reduces peak memory usage by over 50% compared to full-group decoding (e.g., from 21.55 GB to 10.64 GB on Qwen3-1.7B)
- Improves decoding throughput by over 25% compared to naive micro sampling group methods
- Maintains full-length completions (e.g., 186 tokens vs 21 tokens in Dynamic-Slot mode) while reducing decoding steps from 2467 to 1775
- Enables stable GRPO training on standard GPU hardware without memory overflow

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Decomposing large sample groups ($G$) into smaller micro groups ($g$) caps peak GPU memory usage, decoupling memory footprint from total group size.
- **Mechanism:** The framework allocates a fixed memory pool for KV caches based on the micro group size $g$ (e.g., 4 samples). Instead of storing KV tensors for all $G$ samples (e.g., 32) at once, it decodes micro groups sequentially, overwriting the memory pool after each group completes while retaining the shared prompt prefill cache.
- **Core assumption:** The hardware has sufficient memory to hold the model weights, gradients, optimizer states, and at least one micro group's KV cache plus the shared prompt cache.
- **Evidence anchors:**
  - [abstract] "...micro sampling groups that decompose large groups into memory-feasible rounds... reduce peak memory usage by over 50%."
  - [section 3.1] "This scheme offers a simple yet effective trade-off... we gain the ability to scale to arbitrarily large group sizes within fixed memory."
  - [corpus] Neighbor papers like "Down-Sampling Rollouts in LLM Reinforcement Learning" discuss compute/memory asymmetry in RL, but do not specifically address the KV cache pooling mechanism described here.
- **Break condition:** If the micro group size $g$ is set too high relative to available GPU memory, the system will OOM during the allocation of the pool.

### Mechanism 2
- **Claim:** Interleaving token generation across micro groups (Continuous Sampling) improves throughput by minimizing idle compute slots caused by variable sequence lengths.
- **Mechanism:** Unlike naive sequential decoding where a new group waits for the previous one to finish entirely, Continuous Sampling refills individual decoding slots as soon as a specific sequence finishes. This is enabled by the shared prompt KV cache, allowing new samples to attach to the prefill context dynamically without waiting for a batch barrier.
- **Core assumption:** The variance in sequence lengths is non-trivial; if all sequences were exactly the same length, the "bubble" reduction would be minimal.
- **Evidence anchors:**
  - [abstract] "Continuous sampling that interleaves generation across groups to improve utilization... improves throughput by over 25%."
  - [section 3.2] "Interleaved decoding of multiple micro groups, significantly reducing idle time and improving GPU utilization."
  - [