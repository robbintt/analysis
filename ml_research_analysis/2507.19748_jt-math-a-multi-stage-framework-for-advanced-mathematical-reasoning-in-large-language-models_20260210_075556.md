---
ver: rpa2
title: 'JT-Math: A Multi-Stage Framework for Advanced Mathematical Reasoning in Large
  Language Models'
arxiv_id: '2507.19748'
source_url: https://arxiv.org/abs/2507.19748
tags:
- data
- mathematical
- reasoning
- training
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces JT-Math-8B, a series of open-source large
  language models optimized for advanced mathematical reasoning through a multi-stage
  framework. The authors address the challenge of enhancing LLMs' ability to solve
  complex, multi-step mathematical problems requiring deep conceptual understanding.
---

# JT-Math: A Multi-Stage Framework for Advanced Mathematical Reasoning in Large Language Models

## Quick Facts
- **arXiv ID:** 2507.19748
- **Source URL:** https://arxiv.org/abs/2507.19748
- **Reference count:** 11
- **Primary result:** Introduces JT-Math-8B, achieving state-of-the-art performance among 8B models with 63.74 average score across benchmarks

## Executive Summary
JT-Math introduces a multi-stage framework for enhancing mathematical reasoning in large language models, addressing the challenge of solving complex, multi-step problems requiring deep conceptual understanding. The framework employs a systematic approach: pre-training on a high-quality 210B-token mathematical corpus, followed by supervised fine-tuning and curriculum-based reinforcement learning that progressively increases task difficulty and context length up to 32K tokens. The resulting JT-Math-8B-Instruct model surpasses both open-source and proprietary models of similar size on diverse mathematical benchmarks, while JT-Math-8B-Thinking demonstrates superior problem-solving capabilities for competition-level mathematics with an average score of 77.68.

## Method Summary
The JT-Math framework follows a three-stage training pipeline designed specifically for mathematical reasoning. It begins with pre-training on a curated 210B-token corpus combining high-quality mathematical content and code, using model-based filtering to ensure quality. This is followed by supervised fine-tuning with two distinct heads: one optimized for short chain-of-thought reasoning (8K context, 3e-6 LR) and another for long chain-of-thought reasoning (32K context, 8e-5 LR). The final stage employs curriculum-based reinforcement learning using GRPO, where the Thinking model progressively increases context length from 8K to 32K tokens while training on increasingly difficult problems. The entire framework is trained from a custom JT-Coder-8B base, though the paper suggests Qwen2.5-Coder-7B as a functional proxy.

## Key Results
- JT-Math-8B-Instruct achieves state-of-the-art performance among 8B models with an average score of 63.74 across diverse benchmarks
- JT-Math-8B-Thinking excels on reasoning tasks with an average score of 77.68, demonstrating superior problem-solving capabilities
- Outperforms both open-source models and commercial models like OpenAI's O1-mini and GPT-4o on competition-level mathematics
- Demonstrates effectiveness of curriculum-based reinforcement learning for mathematical reasoning

## Why This Works (Mechanism)
The framework's success stems from its staged approach that systematically builds mathematical reasoning capabilities. The high-quality pre-training corpus establishes a strong mathematical foundation, while the dual SFT heads allow the model to specialize in different reasoning styles. The curriculum-based RL is particularly crucial, as it allows the model to gradually adapt to longer contexts and more complex problems without suffering from instability. The binary reward signal in RL, while simple, provides clear optimization direction for answer correctness. The progressive context extension (8K→16K→32K) enables the model to develop sustained reasoning capabilities necessary for solving competition-level problems.

## Foundational Learning
- **Chain-of-Thought Reasoning:** Essential for breaking down complex mathematical problems into manageable steps; quick check: model can solve multi-step problems with coherent intermediate reasoning
- **Curriculum Learning:** Gradually increasing task difficulty prevents catastrophic forgetting and allows stable learning; quick check: performance improves monotonically across curriculum stages
- **Reinforcement Learning with Binary Rewards:** Provides clear optimization signal for correctness; quick check: KL divergence and entropy remain stable during RL training
- **Long-Context Processing (32K):** Enables sustained reasoning chains necessary for complex problems; quick check: model can utilize full context length without degradation
- **Model-Based Data Filtering:** Ensures high-quality training data by filtering low-quality samples; quick check: filtered dataset maintains mathematical rigor and correctness

## Architecture Onboarding

**Component Map:** Pre-training (210B tokens) -> SFT (Short CoT + Long CoT) -> RL (GRPO with curriculum)

**Critical Path:** Pre-training establishes mathematical foundation → SFT specializes reasoning styles → RL optimizes for correctness with progressive difficulty

**Design Tradeoffs:** Binary rewards simplify optimization but may miss intermediate reasoning quality; curriculum approach adds training complexity but enables stable long-context learning; dual SFT heads increase specialization but require more data and training resources

**Failure Signatures:** Entropy collapse during RL indicates reward hacking or insufficient exploration; long-context instability suggests curriculum progression too aggressive; poor generalization indicates pre-training corpus insufficient

**3 First Experiments:**
1. Verify curriculum progression stability by training RL with gradual context extension (8K→16K→32K) and monitoring KL divergence
2. Test model-based filtering implementation by comparing performance with and without the "0.9 quantile per 128-token interval" filtering
3. Validate long-context capability by evaluating reasoning quality at different context lengths (8K, 16K, 32K) on the same problem set

## Open Questions the Paper Calls Out
- **Multi-tool Mathematical Reasoning:** The framework's applicability to domains requiring external tool integration (like code execution) remains untested
- **Data-Scale Limitations:** Whether the 210B-token corpus creates inherent generalization limits compared to trillion-token models
- **Reward Function Design:** The binary reward may not capture intermediate reasoning quality, potentially leading to reward hacking

## Limitations
- Initialization base model (JT-Coder-8B-Base) is not publicly available, requiring approximation with similar models
- Proprietary benchmarks (CNMO, OlympiadBench) cannot be independently verified, limiting result validation
- Limited hyperparameter guidance for critical long-context curriculum training phase

## Confidence
- **High Confidence:** Multi-stage framework architecture and GRPO implementation methodology
- **Medium Confidence:** Data curation pipeline and quantitative results on public benchmarks
- **Low Confidence:** Proprietary benchmark performance claims and direct commercial model comparisons

## Next Checks
1. Reproduce the 32K-context RL training using curriculum scheduling (8K→16K→32K) and verify that KL divergence and entropy remain stable throughout training
2. Implement the "0.9 quantile per 128-token interval" filtering mechanism and test its impact on SFT data quality and downstream reasoning performance
3. Conduct ablation studies removing the long-context pre-training stage and the long-CoT RL stage to quantify their individual contributions to the reported performance gains