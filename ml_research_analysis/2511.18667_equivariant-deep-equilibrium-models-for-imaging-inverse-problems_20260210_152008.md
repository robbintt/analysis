---
ver: rpa2
title: Equivariant Deep Equilibrium Models for Imaging Inverse Problems
arxiv_id: '2511.18667'
source_url: https://arxiv.org/abs/2511.18667
tags:
- training
- deqs
- deep
- imaging
- trained
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of training deep equilibrium
  models (DEQs) for imaging inverse problems without requiring ground truth data,
  leveraging the equivariant imaging (EI) framework. The key contribution is demonstrating
  that modular implicit differentiation can simplify training DEQs with complex EI
  losses, enabling effective self-supervised learning.
---

# Equivariant Deep Equilibrium Models for Imaging Inverse Problems

## Quick Facts
- arXiv ID: 2511.18667
- Source URL: https://arxiv.org/abs/2511.18667
- Reference count: 0
- Primary result: Equivariant Deep Equilibrium Models (EI-DEQs) trained without ground truth achieve PSNR within 1.3 dB of supervised models for sparse-view CT and accelerated MRI reconstruction.

## Executive Summary
This paper addresses the challenge of training Deep Equilibrium Models (DEQs) for imaging inverse problems without requiring ground truth data, leveraging the Equivariant Imaging (EI) framework. The key contribution is demonstrating that modular implicit differentiation can simplify training DEQs with complex EI losses, enabling effective self-supervised learning. Experiments on sparse-view CT and accelerated MRI show that DEQs trained with implicit differentiation significantly outperform those trained with Jacobian-free backpropagation and other baseline methods. Moreover, DEQs trained with EI achieve performance close to supervised methods, with PSNR within 1.3 dB of supervised models. The study also finds that EI-trained DEQs are equivariant, suggesting they implicitly learn a proximal map corresponding to a symmetry-invariant prior, bridging the gap between learned reconstruction and explicit regularization.

## Method Summary
The method combines Deep Equilibrium Models with Equivariant Imaging for self-supervised training of imaging inverse problems. A DEQ finds the fixed point $x^* = F_\theta(x^*)$ where $F_\theta$ typically combines a denoiser $D_\theta$ with data consistency. Instead of standard backpropagation through the solver, modular implicit differentiation solves an auxiliary fixed-point equation for gradients, enabling training with complex, multi-pass EI losses. The EI loss enforces consistency between transformed reconstructions and reconstructions from transformed measurements, allowing the model to learn information in the null space of the measurement operator. The DEQ architecture uses a residual U-Net denoiser (pretrained on BSD500) with spectral and group normalization for contractiveness, trained with Anderson acceleration or Broyden's method for fixed-point solving.

## Key Results
- DEQs trained with implicit differentiation achieve significantly higher PSNR than those trained with Jacobian-free backpropagation (e.g., 31.34 vs 29.39 dB for MRI self-supervised)
- EI-trained DEQs achieve PSNR within 1.3 dB of supervised models while requiring no ground truth data
- EI-trained DEQ operators demonstrate high equivariance to the symmetry group, suggesting they approximate proximal maps of invariant priors
- Modular implicit differentiation simplifies training DEQs with complex EI losses compared to unrolling or approximate differentiation

## Why This Works (Mechanism)

### Mechanism 1: Modular Implicit Differentiation
Deep Equilibrium Models find fixed points $x^* = F_\theta(x^*)$ instead of using explicit layers. Standard backpropagation fails for infinite-depth networks, but implicit differentiation solves this by reducing gradient computation to a fixed-point equation. Proposition 1 shows the vector-Jacobian product can be computed by solving $\beta = \text{Fix}_\beta \{ \dots \}$, avoiding the need to unroll iterations or store computation graphs. This is critical for complex EI losses that involve multiple passes through the measurement operator.

### Mechanism 2: Equivariant Imaging for Null Space Learning
The EI framework enables learning reconstruction models without ground truth by exploiting signal symmetries. The EI loss enforces that a transformation $T_g$ applied to the reconstruction equals the reconstruction of the transformed measurement. Since the measurement operator $A$ is not equivariant to $G$, transformed measurements $A T_g x$ have different null spaces. Consistency across these varying null spaces forces the model to infer missing information based on symmetry priors, effectively learning to fill information deleted by the measurement operator.

### Mechanism 3: DEQs as Proximal Maps of Invariant Priors
Training a DEQ with the EI loss encourages the learned operator $F_\theta$ to approximate the proximal map of an invariant prior. The EI objective implicitly penalizes non-invariant reconstructions, and the resulting operator $F_\theta$ becomes equivariant to $G$. Mathematically, the proximal map of an invariant function is equivariant; thus, the trained DEQ behaves like a classical iterative algorithm regularized by a learned, symmetry-aware prior, bridging the gap between learned reconstruction and explicit regularization.

## Foundational Learning

- **Concept**: Deep Equilibrium Models (DEQs)
  - **Why needed here**: This is the core architecture that finds fixed points $x^* = F_\theta(x^*)$ directly, simulating infinite layers without storing intermediate activations.
  - **Quick check question**: Can you explain why standard backpropagation fails for an "infinite-depth" network and how implicit differentiation solves this memory issue?

- **Concept**: Implicit Differentiation
  - **Why needed here**: Approximate differentiation (JFB) fails for complex losses; implicit differentiation derives gradients at fixed points using the Implicit Function Theorem without unrolling.
  - **Quick check question**: In the gradient equation $\frac{\partial \ell}{\partial \theta} = \dots$, why is the term $(I - \frac{\partial F}{\partial x})$ inverted?

- **Concept**: Inverse Problems & Null Spaces
  - **Why needed here**: The motivation for EI comes from the "null space" problem—information that $A$ deletes. EI fills this gap using symmetries.
  - **Quick check question**: If a measurement operator $A$ is a subsampled Radon transform, what information is missing, and how does knowing that the image is rotation-invariant help recover it?

## Architecture Onboarding

- **Component map**: Input $y$ → Fixed-point solver finds $x^*$ satisfying $x^* = F_\theta(x^*, y)$ → EI loss computes consistency between $T_g \hat{x}$ and $f_\theta(A T_g \hat{x})$ → Modular implicit differentiation solves adjoint fixed-point equation for gradients

- **Critical path**:
  1. **Forward**: Input $y$ → Solver finds $x^*$ satisfying $x^* = F_\theta(x^*, y)$
  2. **Loss**: Compute Equivariant Imaging loss (rotating $x^*$, re-measuring, re-reconstructing)
  3. **Backward**: Use modular ID (Proposition 1). Instead of backproping through the solver, solve the adjoint fixed-point equation to get gradients w.r.t $\theta$

- **Design tradeoffs**:
  - **ID vs. JFB**: Implicit Differentiation (ID) is exact but requires solving a fixed-point equation for gradients (slower per step, complex implementation). Jacobian-Free Backpropagation (JFB) is faster/simpler but yields lower PSNR (e.g., 29.39 vs 31.34 for MRI Self-Supervised in Table 1).
  - **DEQ vs. Unrolling**: DEQ has constant memory but inference requires solving a system (slower). Unrolling is fast at inference but memory-heavy to train and limited in "depth."

- **Failure signatures**:
  - **Divergence**: If the spectral norm of $F_\theta$ is not managed (e.g., via spectral normalization), the fixed-point solver will not converge.
  - **Hallucination**: If the EI symmetry assumption is wrong for the data, the model will invent features to satisfy the symmetry, reducing PSNR.

- **First 3 experiments**:
  1. **Gradient Correctness**: Implement the modular ID hook (Fig. 2) and verify gradients against finite differences on a small scale to ensure the custom backward pass is correct.
  2. **Solver Convergence**: Test the forward solver (Broyden/Anderson) on the initialization $D_\theta$. If it doesn't converge in < 30 steps, check spectral normalization.
  3. **Ablation on $\alpha$**: Run a sweep on the equivariance weight $\alpha$ (paper uses 100 for CT, 1 for MRI). A value too low misses the symmetry; too high may violate data consistency.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can the implicit prior learned by the EI-trained DEQ operator be rigorously characterized mathematically?
- **Basis in paper**: The conclusion states that while evidence suggests the DEQ learns an invariant prior, "our evidence is not conclusive" and calls for "rigorous characterization" as a future direction.
- **Why unresolved**: The paper provides empirical evidence of equivariance (Table 3) but lacks a theoretical proof linking the specific EI loss landscape to a specific explicit regularizer.
- **What evidence would resolve it**: A theoretical derivation showing that the DEQ fixed-point operator corresponds to the proximal map of a specific, definable invariant function.

### Open Question 2
- **Question**: How can the training of self-supervised DEQs be improved to be more robust and computationally efficient?
- **Basis in paper**: The conclusion identifies "developing more robust and efficient methods for training DEQs, especially in self-supervised settings," as a "critical direction for future work."
- **Why unresolved**: While modular implicit differentiation (ID) simplifies implementation, ID is often computationally more expensive than Jacobian-free backpropagation (JFB); the paper shows ID outperforms JFB in quality but does not solve the efficiency trade-off.
- **What evidence would resolve it**: A new training algorithm or modification to the modular ID framework that reduces training time/memory requirements while maintaining the reconstruction quality demonstrated in the paper.

### Open Question 3
- **Question**: Is the reliance on a pre-trained denoiser for weight initialization strictly necessary for the convergence and performance of EI-DEQs?
- **Basis in paper**: Section 4.2.1 states that the learnable module's "initial weights are pretrained on the BSD500 dataset," but it does not analyze performance when training from scratch.
- **Why unresolved**: It is unclear if the self-supervised EI loss is sufficient to drive the network convergence from a random initialization or if the pre-training provides a critical starting basin of attraction.
- **What evidence would resolve it**: Ablation studies comparing the reconstruction quality and convergence speed of EI-DEQs initialized with pre-trained weights versus random initialization.

## Limitations

- The paper relies on a combination of architectural choices and training procedures that are not fully specified in the text, affecting reproducibility
- The claim that EI-trained DEQs approximate proximal maps of invariant priors is compelling but lacks rigorous theoretical establishment
- The computational cost of modular implicit differentiation (solving a fixed-point equation in the backward pass) is not quantified or compared to JFB

## Confidence

- **High Confidence**: The superiority of modular implicit differentiation over Jacobian-free backpropagation for complex losses is well-supported by direct PSNR comparisons (Table 1).
- **Medium Confidence**: The mechanism by which EI enables learning in the null space of $A$ is logically sound and grounded in prior work, but the specific claim about DEQs implicitly learning a proximal map is inferred from empirical observations rather than a formal proof.
- **Low Confidence**: The exact architecture and training protocol (optimizer, normalization details, solver parameters) are underspecified, making a faithful reproduction challenging without significant experimentation.

## Next Checks

1. **Gradient Correctness**: Implement the modular ID backward hook (Fig. 2) and verify gradients against finite differences on a small-scale problem to ensure the custom backward pass is correct.
2. **Solver Convergence**: Test the forward solver (Broyden/Anderson) on the initialization $D_\theta$. If it doesn't converge in < 30 steps, check spectral normalization.
3. **Ablation on $\alpha$**: Run a sweep on the equivariance weight $\alpha$ (paper uses 100 for CT, 1 for MRI). A value too low misses the symmetry; too high may violate data consistency.