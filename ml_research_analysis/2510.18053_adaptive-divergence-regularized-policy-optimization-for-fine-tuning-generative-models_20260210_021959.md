---
ver: rpa2
title: Adaptive Divergence Regularized Policy Optimization for Fine-tuning Generative
  Models
arxiv_id: '2510.18053'
source_url: https://arxiv.org/abs/2510.18053
tags:
- adrpo
- regularization
- fine-tuning
- reward
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the exploration-exploitation dilemma in RL\
  \ fine-tuning of generative models, where fixed regularization coefficients create\
  \ trade-offs between preserving model capabilities and optimizing reward. The authors\
  \ propose Adaptive Divergence Regularized Policy Optimization (ADRPO), which dynamically\
  \ adjusts regularization strength based on sample-specific advantage estimates\u2014\
  reducing regularization for high-advantage samples while strengthening it for poor\
  \ samples."
---

# Adaptive Divergence Regularized Policy Optimization for Fine-tuning Generative Models

## Quick Facts
- arXiv ID: 2510.18053
- Source URL: https://arxiv.org/abs/2510.18053
- Authors: Jiajun Fan; Tong Wei; Chaoran Cheng; Yuxin Chen; Ge Liu
- Reference count: 40
- Primary result: ADRPO with Wasserstein-2 regularization fine-tunes a 2B parameter SD3 model that outperforms substantially larger models (4.8B and 12B parameters) across multiple generative domains while maintaining diversity

## Executive Summary
This paper addresses the exploration-exploitation dilemma in reinforcement learning fine-tuning of generative models, where fixed regularization coefficients create trade-offs between preserving model capabilities and optimizing reward. The authors propose Adaptive Divergence Regularized Policy Optimization (ADRPO), which dynamically adjusts regularization strength based on sample-specific advantage estimates—reducing regularization for high-advantage samples while strengthening it for poor samples. Applied to text-to-image generation, ADRPO with Wasserstein-2 regularization fine-tunes a 2B parameter SD3 model that outperforms substantially larger models (4.8B and 12B parameters) across attribute binding, semantic consistency, artistic style transfer, and compositional control while maintaining diversity. The method generalizes to LLMs and multi-modal audio reasoning, enhancing GRPO with adaptive KL regularization.

## Method Summary
ADRPO modifies standard RL fine-tuning by introducing sample-adaptive regularization coefficients β_tot = β₀ - A, where A is the advantage estimate for each sample. The method maintains a baseline regularization coefficient β₀ but modulates it based on sample quality: high-advantage samples receive reduced penalties allowing aggressive updates, while poor samples receive strengthened penalties preserving reference model capabilities. For continuous distributions (flow matching), ADRPO-FM uses bidirectional gradient signaling where negative advantages actively push the model away from poor trajectories. The approach generalizes across generative paradigms by using appropriate divergence measures: KL divergence for discrete LLMs and Wasserstein-2 for continuous flow matching models.

## Key Results
- ADRPO fine-tunes 2B SD3 to outperform 4.8B and 12B parameter models on DrawBench and RAFT prompts across attribute binding, semantic consistency, artistic style transfer, and compositional control
- Maintains superior Pareto frontier in reward-diversity trade-offs compared to fixed-regularization baselines (ORW-CFM-W2, DPO)
- Demonstrates emergent ability to escape local optima through strategic exploration in LLM fine-tuning, with Qwen3 achieving 5× higher reward than GRPO
- Generalizes to multi-modal audio reasoning, enabling 7B model to outperform commercial models including Gemini 2.5 Pro and GPT-4o Audio on MMAU benchmark

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Sample-adaptive regularization enables simultaneous improvement in both reward optimization and diversity preservation.
- **Mechanism:** The adaptive coefficient β_tot = β₀ - A(x₁, c) modulates regularization strength inversely with advantage estimates. High-advantage samples receive reduced penalties, allowing aggressive policy updates toward promising regions; low-advantage samples receive strengthened penalties, anchoring the policy to the reference model and preserving capabilities.
- **Core assumption:** Advantage estimates reliably distinguish exploitable high-quality samples from uncertain/poor samples requiring conservative updates.
- **Evidence anchors:**
  - [abstract]: "reducing regularization for high-value samples while applying stronger regularization to poor samples"
  - [Section 3.2.2]: "This formulation creates an adaptive regularization coefficient β_tot = β₀ - A that adapts based on the quality of each sample"
  - [corpus]: RSPO paper investigates regularization in self-play alignment but lacks adaptive mechanism; corpus shows limited prior work on advantage-based adaptive regularization specifically
- **Break condition:** If advantage estimates become noisy or uncorrelated with true sample quality (e.g., unreliable reward models), adaptive regularization may amplify errors rather than correct them.

### Mechanism 2
- **Claim:** Bidirectional gradient signaling in flow matching actively suppresses poor generations rather than merely down-weighting them.
- **Mechanism:** For samples with positive advantage (A > 0), the objective A·|v_θ - u_t|² encourages velocity matching. For negative advantage (A < 0), sign inversion reverses gradients, actively pushing the model away from poor trajectories. Average samples (A ≈ 0) contribute minimally, focusing computation on informative examples.
- **Core assumption:** Advantage distribution is centered around zero with meaningful variance across samples.
- **Evidence anchors:**
  - [Section 3.3.2]: "For samples with negative advantage (A < 0), the sign inversion reverses the gradient direction, actively pushing the model away from poor generations"
  - [Section 3.3.2]: "creates a fundamentally different learning dynamic compared to reward-weighting approaches"
  - [corpus]: Flow Density Control (2511.22640) addresses fine-tuning but uses different optimization formulation; no direct corpus evidence for bidirectional gradient mechanism
- **Break condition:** If advantage clipping is too tight or batch normalization biases advantages, the bidirectional signal may be dampened or inverted.

### Mechanism 3
- **Claim:** Adaptive regularization enables emergent escape from local optima through strategic entropy modulation.
- **Mechanism:** Unlike fixed-regularization methods that maintain static exploration levels, ADRPO's sample-wise adaptation allows the policy to first exploit lower-entropy regions, then actively increase entropy when advantage signals indicate stagnation, before converging to high-reward solutions—a three-phase pattern resembling learned simulated annealing.
- **Core assumption:** The advantage signal captures when current policy regions are suboptimal, triggering increased exploration.
- **Evidence anchors:**
  - [Section 4.5]: "ADRPO exhibits a remarkable ability to first explore lower entropy regions, then actively increase entropy to escape local optima"
  - [Figure 4 description]: Qwen3 shows "5× higher reward than GRPO" with three-phase optimization trajectory
  - [corpus]: EPO (2509.22576) addresses exploration in LLM agents but through different mechanisms; POEM (2601.14705) adds evolutionary mutations to PPO for exploration
- **Break condition:** In domains where reward landscape has deceptive plateaus rather than clear local optima signals, the emergent exploration may trigger prematurely or insufficiently.

## Foundational Learning

- **Concept: Advantage Estimation (A = R - V)**
  - Why needed here: Core signal for adaptive regularization; determines whether to strengthen or weaken constraints per sample
  - Quick check question: Can you explain why subtracting a baseline V(c) from reward R produces a more stable training signal than using raw rewards?

- **Concept: Divergence Regularization (KL vs. W₂)**
  - Why needed here: Different generative paradigms require different divergence measures—KL for discrete LLMs, Wasserstein-2 for continuous flow matching models
  - Quick check question: Why is Wasserstein distance preferred over KL divergence for continuous distributions like image latents?

- **Concept: Flow Matching Velocity Fields**
  - Why needed here: Understanding how v_θ(x_t, t, c) parameterizes transformations between noise and data is essential for implementing the ADRPO-FM variant
  - Quick check question: How does the velocity field u_t = x₁ - x₀ define the straight-line interpolation path in flow matching?

## Architecture Onboarding

- **Component map:**
  - Advantage computation module: R(x₁, c) from reward model (CLIP Score for T2I, RM-Gemma for LLMs), V(c) as batch-averaged baseline
  - Adaptive regularization coefficient: β_tot = β₀ - A_clipped
  - Loss assembly: L_ADRPO = L_RL(θ) + (β₀ - A)·L_D(θ)
  - Advantage clipping: [A_min, A_max] bounds (typically ±β₀)

- **Critical path:**
  1. Sample generation from current policy π_θ (online sampling)
  2. Reward computation R(x, c)
  3. Advantage estimation A = R - V(c) with batch normalization
  4. Clipping to stable range
  5. Loss computation with adaptive β_tot
  6. Parameter update (LoRA recommended for large models)

- **Design tradeoffs:**
  - Wider clipping range (2×β₀): More aggressive exploitation, risk of instability on complex tasks
  - Narrower clipping range (0.5×β₀): More conservative, may limit reward gains
  - Online vs. offline sampling: Online enables exploration but risks collapse; ADRPO's adaptive mechanism specifically addresses this instability

- **Failure signatures:**
  - Diversity collapse: ClipDiversity dropping while reward plateaus indicates over-exploitation (A_max too high)
  - Training instability: Loss spikes suggest β_tot going negative frequently (A_max > β₀)
  - No improvement over baseline: Advantages near zero indicate reward model issues or batch size too small for stable V(c) estimation

- **First 3 experiments:**
  1. **Sanity check on small model:** Implement ADRPO-GRPO on Qwen2-0.5B with β₀=0.04, clipping ±0.04; verify entropy increases mid-training before convergence (reproduce Figure 4 pattern)
  2. **Hyperparameter sweep for clipping:** Test [0.5×β₀, 1×β₀, 2×β₀] clipping ranges; confirm robustness finding (variations <0.4%) on held-out prompts
  3. **Baseline comparison on target domain:** Run ADRPO-FM vs. ORW-CFM-W2 vs. DPO on SD3 with identical prompts; plot reward-diversity trajectory to verify superior Pareto frontier (reproduce Figure 3)

## Open Questions the Paper Calls Out

- **Question:** Will ADRPO maintain its effectiveness on foundation models with hundreds of billions of parameters, where online RL fine-tuning is known to be more susceptible to training collapse?
  - **Basis in paper:** [explicit] "An important avenue for future work is extending these findings to much larger foundation models...models with parameters in the hundreds of billions are known to be more susceptible to training collapse during online RL fine-tuning, potentially making adaptive regularization even more crucial at scale."
  - **Why unresolved:** Experiments were limited to moderate-scale models (SD3 2B, Qwen2-0.5B, Qwen3-0.6B) due to computational constraints.
  - **What evidence would resolve it:** ADRPO performance evaluation on models ≥70B parameters with training stability analysis.

- **Question:** Can more sophisticated value approximation methods improve advantage estimation beyond simple batch statistics, particularly for reward landscapes with high variance?
  - **Basis in paper:** [explicit] "Our current implementation of advantage estimation using batch statistics works well in practice but could be improved with more sophisticated value approximation methods, especially for complex reward landscapes with high variance."
  - **Why unresolved:** The paper uses batch-based normalization for computational efficiency, leaving advanced value function methods unexplored.
  - **What evidence would resolve it:** Comparison with learned critic networks or temporal difference-based advantage estimation across diverse reward landscapes.

- **Question:** What is the mechanistic explanation for ADRPO's emergent ability to escape local optima through strategic exploration patterns observed in LLM fine-tuning?
  - **Basis in paper:** [inferred] The paper documents emergent three-phase exploration patterns (explore lower entropy → increase entropy to escape → converge) but provides no theoretical or empirical mechanism for why this behavior emerges from advantage-guided regularization.
  - **Why unresolved:** The behavior is observed phenomenologically without causal analysis.
  - **What evidence would resolve it:** Controlled ablations tracking how advantage estimates modulate entropy dynamics, or theoretical analysis linking adaptive β to exploration incentives.

- **Question:** How sensitive is ADRPO to the baseline regularization coefficient β0 across different domains and model scales?
  - **Basis in paper:** [inferred] The paper demonstrates robustness to advantage clipping ranges (0.5× to 2× β0 in audio tasks with <0.4% variation) but systematically varies only Amin/Amax while keeping β0 fixed.
  - **Why unresolved:** β0 is set equal to baseline methods' fixed β for fair comparison, not independently optimized.
  - **What evidence would resolve it:** Sensitivity analysis sweeping β0 across multiple orders of magnitude in each domain (T2I, LLM, multi-modal).

## Limitations

- The core mechanism relies heavily on the assumption that advantage estimates are reliable and well-calibrated, with batch-averaged rewards as baseline potentially unstable for small batch sizes
- The W2 regularization formulation for flow matching velocity fields lacks detailed implementation specifications, particularly regarding latent space geometry and ODE solver settings
- Evaluation metrics for audio reasoning tasks (MMAU) are less established than visual/text domains, raising concerns about result reproducibility across different audio benchmarks

## Confidence

- **High confidence**: ADRPO's mechanism of adaptive regularization based on advantage estimates (Section 3.2.2), demonstrated superiority over fixed-regularization baselines on T2I and LLM tasks (Section 4), and Pareto improvement in reward-diversity trade-offs (Figure 3, Figure 5)
- **Medium confidence**: Bidirectional gradient signaling mechanism (Section 3.3.2) due to limited ablation studies, and emergent exploration behavior (Section 4.5) which relies on a single LLM experiment
- **Low confidence**: Exact implementation details for W2 regularization in flow matching, and generalizability to tasks with highly skewed reward distributions

## Next Checks

1. **Advantage estimation robustness**: Run ADRPO with varying batch sizes (8, 32, 128) on the same T2I task to quantify how baseline estimation stability affects adaptive regularization performance. Compare reward trajectories and final metrics.

2. **Reward landscape sensitivity**: Test ADRPO on a synthetic task with controlled reward landscapes (e.g., bimodal rewards with plateaus) to verify whether adaptive regularization genuinely enables escape from local optima versus simple exploitation of easier regions.

3. **Cross-task generalization**: Implement ADRPO for a domain outside the paper's scope (e.g., molecular generation or robot control) to test whether the same β₀=1 (T2I) or β₀=0.04 (LLM) hyperparameters generalize, or if task-specific tuning is required.