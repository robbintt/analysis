---
ver: rpa2
title: Encoder-Decoder Diffusion Language Models for Efficient Training and Inference
arxiv_id: '2510.22852'
source_url: https://arxiv.org/abs/2510.22852
tags:
- diffusion
- decoder
- encoder
- e2d2
- block
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces E2D2, an encoder-decoder architecture for
  discrete diffusion language models that achieves faster inference and training than
  decoder-only approaches. The key idea is to separate clean token representation
  (handled by an encoder) from iterative denoising (handled by a lightweight decoder),
  reducing computational cost by amortizing encoder calls.
---

# Encoder-Decoder Diffusion Language Models for Efficient Training and Inference

## Quick Facts
- arXiv ID: 2510.22852
- Source URL: https://arxiv.org/abs/2510.22852
- Reference count: 40
- Introduces E2D2, an encoder-decoder architecture for discrete diffusion language models achieving faster inference and training than decoder-only approaches

## Executive Summary
This work presents E2D2, an encoder-decoder architecture for discrete diffusion language models that improves both training and inference efficiency. The key innovation is separating clean token representation (handled by an encoder) from iterative denoising (handled by a lightweight decoder), which amortizes encoder calls and reduces computational cost. E2D2 demonstrates superior performance on summarization, translation, and mathematical reasoning tasks while achieving up to 3x faster inference than masked diffusion models and requiring 2x fewer FLOPs during training compared to block diffusion models of equal size.

## Method Summary
E2D2 introduces an encoder-decoder architecture for discrete diffusion language models that decouples clean token representation from iterative denoising. The encoder processes the original sequence once and generates a compressed representation, while the decoder performs iterative denoising steps using this representation. This architecture supports both block and full-sequence diffusion parameterizations and incorporates KV caching for faster inference. The approach amortizes encoder computation across multiple denoising steps, significantly reducing overall computational cost while maintaining or improving model quality.

## Key Results
- Achieves up to 3x faster inference than masked diffusion models with KV caching support
- Requires 2x fewer FLOPs during training compared to block diffusion models of equal size
- Outperforms decoder-only diffusion baselines on summarization, translation, and mathematical reasoning tasks

## Why This Works (Mechanism)
E2D2 works by separating the representation of clean tokens from the denoising process. The encoder creates a compact representation of the original sequence that is reused across multiple denoising steps, eliminating redundant computation. The lightweight decoder focuses solely on the iterative denoising task without needing to process the full context at each step. This separation allows for better computational amortization and more efficient use of model parameters, particularly during inference where the encoder is called only once while the decoder performs multiple iterative steps.

## Foundational Learning
- **Discrete Diffusion Models**: A generative modeling framework that denoises corrupted sequences step-by-step; needed for understanding the baseline approach being improved
- **Encoder-Decoder Architecture**: Separates encoding and decoding functions in sequence models; needed to understand how E2D2 differs from standard architectures
- **KV Caching**: A technique for efficient inference by reusing key-value pairs; needed to understand how E2D2 achieves faster inference
- **Amortized Computation**: Spreading computational cost across multiple operations; needed to grasp the efficiency gains in E2D2
- **Block vs Full-Sequence Diffusion**: Different approaches to corrupting sequences during training; needed to understand the various diffusion parameterizations supported
- **FLOPs Analysis**: Measurement of computational operations; needed to evaluate training efficiency claims

## Architecture Onboarding

**Component Map**
Encoder -> Representation Storage -> Decoder -> Output Sequence

**Critical Path**
Input sequence → Encoder → Compressed representation → Decoder (iterative denoising steps) → Final output sequence

**Design Tradeoffs**
- Encoder complexity vs decoder efficiency: A more powerful encoder provides better representations but increases upfront cost
- Compression level: Tighter compression reduces storage but may lose information needed for denoising
- Block vs full-sequence diffusion: Block diffusion is more efficient but may miss long-range dependencies

**Failure Signatures**
- Degraded performance on tasks requiring deep long-range context due to encoder amortization
- Suboptimal denoising when compressed representations lose critical information
- Inefficient scaling to multi-gigaword corpora where encoder amortization benefits diminish

**3 First Experiments**
1. Compare E2D2 with decoder-only diffusion models on standard language modeling benchmarks (LM1B, WikiText)
2. Evaluate inference speed with KV caching enabled vs disabled across different sequence lengths
3. Test training efficiency by measuring wall-clock time and memory usage on varying batch sizes

## Open Questions the Paper Calls Out
None

## Limitations
- Performance on tasks requiring deep long-range context may be limited due to encoder amortization potentially underrepresenting fine-grained token dependencies
- Training efficiency gains are based on FLOPs comparison rather than comprehensive end-to-end wall-clock validation across diverse hardware and batch sizes
- Experimental comparisons are primarily limited to smaller diffusion baselines, with limited testing on broader generative tasks beyond summarization, translation, and mathematical reasoning

## Confidence

**High**: Architectural innovations and efficiency arguments under controlled conditions are well-supported by the proposed design and theoretical analysis.

**Medium**: Training speed and throughput improvements, as the claims rely heavily on FLOPs comparison without extensive wall-clock validation across diverse hardware configurations.

**Low**: Assertions of superior performance on complex reasoning tasks, given the limited experimental scope and lack of testing on more challenging benchmarks.

## Next Checks
1. Evaluate E2D2 on long-context tasks (e.g., multi-document QA or book summarization) to test encoder amortization robustness.
2. Conduct wall-clock and memory usage comparisons on multiple GPU/TPU setups and with varying batch sizes to validate claimed training efficiency gains.
3. Test the approach on additional generative benchmarks (e.g., story completion, dialogue generation) to assess generalization across diverse language modeling tasks.