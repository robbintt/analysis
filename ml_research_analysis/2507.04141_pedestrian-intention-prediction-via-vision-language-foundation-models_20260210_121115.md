---
ver: rpa2
title: Pedestrian Intention Prediction via Vision-Language Foundation Models
arxiv_id: '2507.04141'
source_url: https://arxiv.org/abs/2507.04141
tags:
- pedestrian
- prediction
- prompt
- prompts
- intention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates using vision-language foundation models
  (VLFMs) to predict pedestrian crossing intentions in autonomous vehicles. It introduces
  hierarchical prompt templates incorporating visual frames, physical cues, and ego-vehicle
  dynamics to guide VLFMs effectively.
---

# Pedestrian Intention Prediction via Vision-Language Foundation Models

## Quick Facts
- arXiv ID: 2507.04141
- Source URL: https://arxiv.org/abs/2507.04141
- Authors: Mohsen Azarmi; Mahdi Rezaei; He Wang
- Reference count: 36
- Key outcome: Vision-language foundation models with hierarchical prompts and vehicle speed dynamics achieve up to 19.8% accuracy improvement in pedestrian crossing intention prediction, with automatic prompt engineering yielding additional 12.5% gains.

## Executive Summary
This study demonstrates that vision-language foundation models (VLFMs) can effectively predict pedestrian crossing intentions when guided by systematically refined hierarchical prompts that incorporate visual frames, physical cues, and vehicle dynamics. The approach introduces time-conscious vehicle speed representations and employs an Automatic Prompt Engineer (APE) framework to optimize prompts across three datasets. Experiments show VLFMs, particularly larger models like GPT-4V, outperform conventional vision-based models in accuracy, F1-score, and contextual understanding across all tested scenarios.

## Method Summary
The method employs hierarchical prompt templates that progressively condition VLFMs from role-based prompts through physical cue observations to vehicle dynamics, with an APE framework using Monte Carlo search to optimize prompt formulations. The approach integrates 16-frame video sequences (30fps) with bounding boxes and timestamps, injecting vehicle speed data in static, descriptive, or time-conscious formats. GPT-4V serves as the primary VLFM backend with LLaVA-Next and GPT-4 mini as alternatives, using a scoring function that balances execution accuracy and log probability to guide prompt optimization across 40 iterations with 365 validation samples.

## Key Results
- Hierarchical prompts incorporating vehicle speed and time-conscious elements improve prediction accuracy by up to 19.8% compared to baseline approaches
- APE-optimized prompts yield an additional 12.5% accuracy gain beyond manual prompt engineering
- Larger VLFMs (GPT-4V) consistently outperform smaller models (LLaVA-Next 3B) across all metrics, achieving 0.74-0.81 accuracy versus 0.65-0.72

## Why This Works (Mechanism)

### Mechanism 1
Hierarchical prompt templates improve VLFM prediction accuracy by progressively conditioning the model on task-relevant context. The approach builds from role-based prompts → physical cue observation → vehicle dynamics, with each layer adding behavioral and environmental context. This progressive structure reflects human reasoning patterns that build foundational knowledge before complex details.

### Mechanism 2
Time-conscious vehicle dynamics representation captures temporal dependencies that static representations miss. Speed variations over time (e.g., "speed increased from X to Y over Z seconds") provide the model with acceleration/deceleration cues that correlate with pedestrian decision-making in traffic negotiation scenarios.

### Mechanism 3
Automatic Prompt Engineering (APE) systematically discovers linguistically diverse, high-performing prompts that outperform manual design. Monte Carlo search iteratively generates candidates via perturbation (rephrasing, synonyms), scores using weighted accuracy + confidence, and retains top-K prompts across T iterations.

## Foundational Learning

- **Vision-Language Foundation Models (VLFMs)**: These models integrate visual frames with textual prompts to perform cross-modal reasoning. Understanding their input requirements and reasoning capabilities is essential for effective prompt design.
  - Quick check: Can you explain how a VLFM would process a 16-frame video sequence with an accompanying text prompt about pedestrian behavior?

- **Prompt Engineering for Multimodal Tasks**: The paper demonstrates that prompt structure (hierarchical, time-conscious, chain-of-thought) significantly impacts performance. Engineers must understand which linguistic patterns guide model attention effectively.
  - Quick check: Given the finding that "posture" and "orientation" keywords improve scores while "acting" and "behaving" degrade them, how would you redesign a vague prompt?

- **Temporal Dynamics in Pedestrian Behavior**: Vehicle speed changes over time provide causal cues for pedestrian decisions. Understanding the relationship between ego-vehicle dynamics and pedestrian intent is critical for feature integration.
  - Quick check: Why would a pedestrian's crossing decision differ when a vehicle maintains constant speed versus when it decelerates?

## Architecture Onboarding

- **Component map**: 16 annotated frames (30fps) with bounding boxes + textual prompt templates → Role (PR) → Physical Cues (PB) → Vehicle Dynamics (PDs/PDd/PDt) → APE Optimizer (Monte Carlo search with ChatGPT-4 for perturbation, GPT-4V for scoring) → VLFM Backends (GPT-4V, GPT-4 mini, LLaVA-Next) → Binary crossing intention + confidence scores

- **Critical path**: Frame annotation (bounding box + timestamp) → Template selection from hierarchy → Speed data injection (if available) → VLFM inference → Prediction extraction
  - Assumption: The paper assumes bounding box annotations are pre-computed; real deployment would require object detection upstream.

- **Design tradeoffs**:
  - Accuracy vs. Latency: Larger VLFMs (GPT-4V) achieve higher accuracy (0.74-0.81) but have higher inference costs than smaller models (LLaVA-Next 3B: 0.65-0.72 accuracy).
  - Prompt Complexity vs. Interpretability: Chain-of-thought prompts perform best but are harder to debug; simple direct questions are transparent but less accurate.
  - Dataset Coverage vs. Optimization Time: APE runs 40 iterations across 365 validation samples; scaling to more datasets increases optimization cost.

- **Failure signatures**:
  - Ambiguous prompts (e.g., "What do you think about behavior?") produce low confidence and erratic predictions.
  - Missing speed data on JAAD restricts experiments to descriptive templates (PDd), limiting temporal dynamics analysis.
  - Smaller models (3B parameters) struggle with JAAD's complex scenarios, suggesting capacity constraints.

- **First 3 experiments**:
  1. Ablate prompt hierarchy: Test PR-only, PR+PB, and PR+PB+PDt configurations to isolate each component's contribution on a held-out validation set.
  2. Speed representation comparison: Run PDs vs. PDd vs. PDt on PIE dataset (which has numeric speed data) to quantify time-conscious gains.
  3. Cross-model prompt transfer: Take top-5 APE-optimized prompts from GPT-4V and evaluate zero-shot on LLaVA-Next (7B) to assess prompt portability across architectures.

## Open Questions the Paper Calls Out
None

## Limitations
- The study relies on pre-computed bounding boxes and assumes high-quality timestamp alignment between frames and vehicle speed data.
- The APE framework's search space may not fully explore all linguistic variations, and the transferability of optimized prompts across VLFM architectures remains unproven.
- The absence of ablation studies for individual prompt components limits causal attribution of performance gains.

## Confidence
- **High confidence**: The core finding that hierarchical prompts improve VLFM performance, supported by systematic experiments across three datasets and clear accuracy improvements.
- **Medium confidence**: The specific mechanism by which time-conscious vehicle dynamics improve predictions, as the study isolates this effect but doesn't control for confounding temporal features.
- **Low confidence**: The general applicability of APE-optimized prompts across different VLFM architectures, given that cross-model testing was limited and showed mixed transferability.

## Next Checks
1. **Prompt portability test**: Take top-3 APE-optimized prompts from GPT-4V and evaluate zero-shot on LLaVA-Next (3B) and GPT-4 mini to quantify cross-model performance degradation and identify architecture-specific prompt requirements.
2. **Temporal confounding control**: Re-run experiments on PIE dataset with temporally shuffled frames (maintaining speed data) to isolate whether improvements come from vehicle dynamics or inherent video temporal patterns.
3. **Capacity constraint validation**: Systematically compare VLFM performance against model size (1.8T → 8B → 3B parameters) on JAAD's most complex scenarios to establish clear performance vs. capacity thresholds for pedestrian intention tasks.