---
ver: rpa2
title: 'PHEATPRUNER: Interpretable Data-centric Feature Selection for Multivariate
  Time Series Classification through Persistent Homology'
arxiv_id: '2504.18329'
source_url: https://arxiv.org/abs/2504.18329
tags:
- data
- features
- time
- persistent
- homology
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PHEATPRUNER integrates persistent homology and sheaf theory to
  tackle interpretability challenges in multivariate time series classification. By
  transforming correlation matrices into distance matrices and constructing Vietoris-Rips
  complexes, the method identifies persistent topological features and prunes up to
  45% of variables without sacrificing accuracy in models like Random Forest, CatBoost,
  XGBoost, and LightGBM.
---

# PHEATPRUNER: Interpretable Data-centric Feature Selection for Multivariate Time Series Classification through Persistent Homology

## Quick Facts
- arXiv ID: 2504.18329
- Source URL: https://arxiv.org/abs/2504.18329
- Reference count: 40
- Primary result: Prunes up to 45% of variables while maintaining or improving accuracy across multiple classifiers

## Executive Summary
PHEATPRUNER addresses interpretability challenges in multivariate time series classification by combining persistent homology and sheaf theory. The method transforms correlation matrices into distance matrices, constructs Vietoris-Rips complexes, and identifies persistent topological features to prune up to 45% of variables without sacrificing accuracy. Sheaf theory generates explanatory vectors that clarify each variable's contribution to predictions. Experiments on UEA Archive and mastitis detection datasets demonstrate maintained or improved model performance with significant dimensionality reduction, advancing both interpretability and decision-making in time series analysis.

## Method Summary
PHEATPRUNER operates by first computing the correlation matrix of variables in multivariate time series, then transforming it into a distance matrix using $D_{ij} = \sqrt{2(1 - C_{ij})}$. The method constructs Vietoris-Rips complexes and computes persistent homology to identify topological features. The optimal pruning threshold $\epsilon$ is determined by calculating the median death time across all persistence barcodes. Variables that remain isolated in the Vietoris-Rips complex at this threshold are pruned. Finally, sheaf-theoretic consistency filtration is applied to the remaining features, generating explanatory vectors that quantify local agreement between topologically connected variables. These enriched features are then used with tree-based classifiers including Random Forest, CatBoost, XGBoost, and LightGBM.

## Key Results
- Successfully pruned up to 45% of applied variables while maintaining or enhancing classification accuracy
- Demonstrated effectiveness across multiple classifiers including Random Forest, CatBoost, XGBoost, and LightGBM
- Maintained performance on UEA Archive datasets and improved mastitis detection accuracy
- Generated explanatory vectors through sheaf theory that clarify variable contributions to predictions

## Why This Works (Mechanism)

### Mechanism 1: Topological Pruning of Isolated Variables
The method identifies variables that lack topological connectivity by constructing Vietoris-Rips complexes from distance matrices. Variables that remain isolated (0-dimensional simplices) at the optimal scale $\epsilon$ are filtered out, as they do not participate in the data's persistent structural "shape." This assumes critical classification variables are involved in persistent topological structures rather than acting as isolated points.

### Mechanism 2: Median Death Time Threshold Selection
Persistent homology tracks the birth and death of topological features across scales. The median death time serves as a robust statistic for selecting the optimal pruning threshold $\epsilon$, filtering out short-lived (noisy) connections while retaining stable structures. This assumes the distribution of feature lifespans contains a stable "signal" region where relevant variables are connected.

### Mechanism 3: Sheaf-Theoretic Consistency Enrichment
After pruning, a sheaf structure is imposed on the simplicial complex, and consistency vectors are calculated based on the covariance of data assigned to connected vertices. These vectors quantify local agreement between variables and are appended to the dataset, providing explainability without degrading predictive performance. This assumes consistency between topologically connected variables carries discriminative information about the target class.

## Foundational Learning

- **Vietoris-Rips Complex**: Transforms numeric correlations into a topological "shape" where connectedness defines feature survival. Quick check: Given points A, B, and C, if dist(A,B) < $\epsilon$ and dist(B,C) < $\epsilon$, but dist(A,C) > $\epsilon$, does the Vietoris-Rips complex contain the triangle ABC? (Answer: No, only edges AB and BC).

- **Persistence Barcode/Diagram**: Tracks the birth and death of topological features across scales. The paper relies on "death times" from these barcodes to calculate the optimal pruning threshold. Quick check: In a persistence barcode, does a long bar represent a noisy feature or a robust topological feature? (Answer: Robust/persistent).

- **Sheaf Theory (Stalks and Restrictions)**: The explainability component uses "sheafification" to generate consistency vectors. A sheaf assigns data (stalks) to spaces and compares them via restriction maps. Quick check: What mathematical object does a "stalk" typically represent in this paper's context? (Answer: A vector space associated with a variable/vertex).

## Architecture Onboarding

- **Component map**: Input MTS -> Correlation Matrix -> Distance Matrix -> Persistence Barcodes -> Median Death Time -> $\epsilon_{optimal}$ -> Vietoris-Rips Complex -> Prune Isolated Nodes -> Sheaf Construction -> Consistency Vectors -> Classifier

- **Critical path**: The calculation of the Distance Matrix and the selection of $\epsilon_{optimal}$ are crucial. If the distance transform ($D_{ij} = \sqrt{2(1 - C_{ij})}$) is mishandled, the scale of $\epsilon$ changes, potentially invalidating the median-death heuristic.

- **Design tradeoffs**: 
  - Square Root vs. Linear Distance: Uses $\sqrt{2(1-C)}$ to better differentiate highly correlated items
  - Tree Models vs. Deep Learning: Targets tree-based models (RF, XGBoost) rather than deep learning models

- **Failure signatures**:
  - 0% Pruning: Occurs if the median death time is too high or correlations are universally weak
  - Accuracy Drop: Seen in the "ERing" dataset, suggesting topological structure didn't align with class discriminatory features

- **First 3 experiments**:
  1. Replicate Distance Transform: Verify impact of square root transform by plotting distance distributions using both linear and square root methods
  2. Sensitivity Analysis on $\epsilon$: Sweep $\epsilon$ values (25th to 75th percentile of death times) to observe pruning ratio stability
  3. Sheaf Ablation: Train classifier on pruned data with and without sheaf-derived consistency vectors to quantify accuracy/explainability gains

## Open Questions the Paper Calls Out

- Can PHEATPRUNER be effectively extended to handle multimodal datasets? The current method is validated primarily on unimodal multivariate time series and may require new fusion mechanisms for different physical units or sampling rates.

- Does incorporating neuro-symbolic methods into the PHEATPRUNER pipeline further enhance model accuracy and interpretability? The current paper focuses on classical machine learning models and TDA, with integration with neuro-symbolic logic unexplored.

- Is the "median death time" a theoretically optimal threshold for the Vietoris-Rips parameter $\epsilon$ across diverse topological structures? While effective for tested datasets, it's unclear if this statistical measure holds true for datasets with sparse topological features or high noise levels.

- Under what conditions do sheaf-generated explanatory vectors improve predictive performance rather than just providing interpretability? Current results suggest sheaf features are orthogonal to decision boundaries used by classifiers, leaving unclear if they're redundant or if specific non-linear models could leverage them for accuracy gains.

## Limitations

- The pruning strategy assumes topologically isolated variables are non-discriminative, which may fail in datasets where class separation depends on independent feature sets
- Sheaf-theoretic enrichment lacks external validation and may produce redundant features
- The median-death heuristic's optimality is based on internal experiments rather than comparative analysis with alternative threshold selection methods

## Confidence

- **High confidence** in the topological pruning mechanism's mathematical soundness
- **Medium confidence** in the median-death heuristic's practical optimality across diverse datasets
- **Medium confidence** in sheaf-based explainability vectors adding discriminative value
- **Low confidence** in generalizability to datasets with inherently independent discriminative features

## Next Checks

1. Conduct sensitivity analysis on the median-death threshold by comparing performance across multiple quantiles (25th, median, 75th percentiles) to assess robustness
2. Perform ablation studies to quantify the exact contribution of sheaf-derived consistency features versus raw pruned features
3. Test the method on synthetic datasets where class boundaries are explicitly defined by independent (uncorrelated) features to identify failure modes