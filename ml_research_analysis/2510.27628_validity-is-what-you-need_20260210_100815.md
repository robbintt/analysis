---
ver: rpa2
title: Validity Is What You Need
arxiv_id: '2510.27628'
source_url: https://arxiv.org/abs/2510.27628
tags:
- agentic
- arxiv
- systems
- which
- these
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a realist definition of Agentic AI as a software
  delivery mechanism that puts multi-step applications to work autonomously in complex
  enterprise settings. The authors argue that while foundation models like LLMs enable
  agentic systems, their success depends on validation by end users and stakeholders.
---

# Validity Is What You Need

## Quick Facts
- arXiv ID: 2510.27628
- Source URL: https://arxiv.org/abs/2510.27628
- Authors: Sebastian Benthall; Andrew Clark
- Reference count: 40
- Key outcome: Realist definition of Agentic AI as validated enterprise applications; success depends on validation, not foundation models.

## Executive Summary
This paper argues that Agentic AI should be understood as a validated software delivery mechanism rather than as an autonomous general intelligence. While foundation models like LLMs enable agentic systems, their success depends on rigorous validation by end users and stakeholders. The authors outline a multi-stage design process emphasizing mechanism design, objective definition, feedback analysis, and validation. The key insight is that with strong validation measures, simpler and more interpretable models can often replace LLMs, as they better handle core logic and domain-specific constraints.

## Method Summary
The paper proposes a 5-stage design process for Agentic AI systems: (1) model enterprise context as multi-agent sociotechnical system, (2) define objectives in terms of that system, (3) check for feedback loops and information leaks, (4) build the system, and (5) validate, verify, and train. The method emphasizes stakeholder alignment, operationalizing social goals into measurable tests, and creating guardrails against reward hacking. No specific datasets or training procedures are provided, as the focus is on the validation framework rather than model architecture.

## Key Results
- Rigorous validation may reduce dependency on large LLMs, enabling simpler, more interpretable models
- Agentic reliability degrades exponentially with multi-step workflows due to error compounding
- Success depends on mechanism design (stakeholder alignment) rather than model capability alone

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Rigorous application-level validation may reduce dependency on large, general-purpose Foundation Models (LLMs).
- **Mechanism:** As specific validation criteria (guardrails, test cases, domain-specific logic) are defined to satisfy the "principal" stakeholder, the solution space narrows. This allows "general intelligence" to be replaced by "specific intelligence" (smaller, interpretable models or deterministic code) that guarantees performance within those defined constraints.
- **Core assumption:** Enterprise value is derived from reliability and alignment with specific goals, not the breadth of general reasoning capabilities.
- **Evidence anchors:**
  - [abstract] "Ironically, with good validation measures in place, in many cases the foundation models can be replaced with much simpler, faster, and more interpretable models..."
  - [section 5] "If the capabilities of foundation models succeed in exciting the demand for agentic AI, we anticipate that skillful application of these and other well-understood technologies will ultimately be responsible for satisfying it."
  - [corpus] Weak direct evidence in this specific corpus; related works focus on agent infrastructure (e.g., TRAIL, Files-Are-All-You-Need) rather than model substitution.
- **Break condition:** If the enterprise task requires novel reasoning or handling of out-of-distribution data not anticipated during the validation design phase, simple models will fail, and general Foundation Models remain necessary.

### Mechanism 2
- **Claim:** Agentic system reliability degrades non-linearly as the number of steps increases.
- **Mechanism:** Agentic workflows rely on sequential steps. If the probability of success for a single step is less than 1 ($P < 1$), the cumulative probability for the chain is the product of individual probabilities ($P_{total} = P_1 \times P_2 \times \dots$). As steps are added, accuracy drops exponentially.
- **Core assumption:** Step outcomes are not perfectly corrected by subsequent steps (lack of self-correction mechanisms).
- **Evidence anchors:**
  - [section 3] "...if an agent has .9 accuracy on a single task... in a sequence of four tasks, that can compound to .9 x .9 x .9 x .9 = .66 accuracy."
  - [section 3] "We must distinguish between the core abilities of an LLM and its abilities when embedded in an agentic, multi-step context."
  - [corpus] "TRAIL" addresses trace reasoning, implying complexity in sequential steps, though it focuses on evaluation rather than the mathematical compounding of error.
- **Break condition:** If the system implements effective "healing" or branching logic where failure in one step does not catastrophically terminate or corrupt the workflow.

### Mechanism 3
- **Claim:** Agentic success is primarily a function of "mechanism design" (incentive/sensor alignment) rather than model capability.
- **Mechanism:** By modeling the enterprise context as a multi-agent sociotechnical system, designers translate social goals into operational tests. This aligns the "Agent" (software) with the "Principal" (stakeholder), preventing reward hacking or drift.
- **Core assumption:** The "principal's" goals can be explicitly operationalized into metrics and constraints (guardrails) at design time.
- **Evidence anchors:**
  - [abstract] "Agentic AI systems are primarily applications, not foundations, and so their success depends on validation by end users and principal stakeholders."
  - [section 4] "Building an agentic AI... is much like the economic practice of mechanism design... The frontier for innovation in Agentic AI is in the design and validation process."
  - [corpus] "Mind What You Ask For" warns of persuasion risks, indirectly supporting the need for robust mechanism design over trusting model intent.
- **Break condition:** If the enterprise environment is adversarial or opaque such that stated incentives do not match actual behavior, the validation logic will be gamed or bypassed.

## Foundational Learning

- **Concept: Mechanism Design (Economics)**
  - **Why needed here:** The paper frames Agentic AI not as a chatbot, but as an economic "agent" acting for a "principal." Understanding utility functions and incentive compatibility is required to design the validation layer.
  - **Quick check question:** Can you distinguish between the "Principal" and the "Agent" in a standard enterprise workflow?

- **Concept: Bellman Equation / MDPs**
  - **Why needed here:** The paper references this as the formal backbone of rational agency (optimizing value over time via states and actions).
  - **Quick check question:** How does a "discount factor" ($\beta$) influence an agent's decision to prioritize immediate vs. long-term rewards?

- **Concept: Information Asymmetry**
  - **Why needed here:** The paper argues a general model cannot know specific enterprise data ("information gap"). Understanding this gap explains why pretraining alone is insufficient for enterprise validity.
  - **Quick check question:** Why can't a pretrained LLM guarantee correct behavior in a proprietary enterprise context it has never seen?

## Architecture Onboarding

- **Component map:** Principal Stakeholder -> Validation/Governance Layer -> Agentic Core -> Sociotechnical Environment
- **Critical path:** Designing the **Validation Layer**. The paper argues this is the bottleneck, not the model size. Start by defining the "validity" criteria before selecting the model.
- **Design tradeoffs:** **LLM vs. SLM/Expert System.** LLMs offer flexibility but suffer from hallucination/security risks. SLMs/Deterministic systems offer reliability/interpretability but lack flexibility. Strong validation enables the latter.
- **Failure signatures:**
  - **Compounding Error:** High failure rate on multi-step chains (check intermediate step accuracy).
  - **Drift:** Model performance changes as underlying data or model versions shift (requires continuous auditing).
  - **Reward Hacking:** Agent satisfies the metric but fails the actual business goal (validation logic was too narrow).
- **First 3 experiments:**
  1. **Decomposition Accuracy Test:** Measure accuracy of a single LLM step vs. a 4-step chain to quantify the "error compounding" effect in your specific domain.
  2. **Substitution Test:** Try replacing the core LLM with a simple rule-based system or SLM in a well-defined task to see if "validity" is maintained at lower cost.
  3. **Adversarial Guardrails Test:** Intentionally inject "jailbreak" or "irrelevant" prompts into the agentic loop to verify if the validation layer catches them before they affect the enterprise environment.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can AI techniques be used to improve the multi-stage design and governance process for Agentic AI systems?
  - **Basis in paper:** [explicit] The authors "question how AI can improve on this design and governance process" after proposing a multi-stage design heuristic.
  - **Why unresolved:** While the paper outlines a design process involving stakeholder alignment and guardrails, it does not determine the extent to which AI can automate or enhance this meta-governance layer.
  - **What evidence would resolve it:** Demonstrated frameworks where AI successfully automates the translation of stakeholder objectives into operational constraints or guardrails.

- **Open Question 2:** Are general performance metrics for foundation models sufficient to validate specific enterprise Agentic AI systems?
  - **Basis in paper:** [explicit] The authors "question whether, in principle, any amount of demonstrated general performance for a large model can be sufficient for validating a specific enterprise end-user's Agentic AI system."
  - **Why unresolved:** An information gap exists between general pretraining data and the specific, proprietary sociotechnical contexts of enterprise environments.
  - **What evidence would resolve it:** Proof that general model capabilities fail to predict performance in specific enterprise contexts without external validation structures.

- **Open Question 3:** Do LLMs provide a viable foundation for agentic AI systems in real business use cases?
  - **Basis in paper:** [explicit] The paper states, "It remains to be seen if LLMs provide a viable foundation for agentic AI systems in real business use cases."
  - **Why unresolved:** LLMs suffer from inherent limitations such as security vulnerabilities (e.g., prompt injection), confidentiality risks, and context window limits that may be disqualifying in high-stakes settings.
  - **What evidence would resolve it:** Longitudinal studies of enterprise deployments showing LLM-based systems maintaining necessary security and reliability standards compared to interpretable alternatives.

- **Open Question 4:** How can robust application-level evaluations be developed for situated Agentic AI?
  - **Basis in paper:** [explicit] The conclusion identifies "application evaluations which verify and validate" as a necessary but currently "open research area."
  - **Why unresolved:** Current evaluation science focuses on foundation models (e.g., benchmarks) rather than the validity of the total application within a complex, multi-step environment.
  - **What evidence would resolve it:** The adoption of standardized end-to-end auditing frameworks specifically designed for multi-step agentic workflows.

## Limitations

- The paper's claims are primarily conceptual and theoretical, lacking empirical validation on specific datasets or enterprise use cases.
- No concrete validation metrics, thresholds, or testing protocols are specified beyond general categories.
- The assertion that simpler models can reliably replace LLMs in many enterprise contexts is speculative without concrete performance comparisons.

## Confidence

- **High Confidence:** The core premise that agentic AI systems require validation beyond model capabilities is well-supported by the literature on sociotechnical systems and mechanism design.
- **Medium Confidence:** The claim that error compounding in multi-step workflows is a significant reliability issue is plausible but lacks empirical data specific to the proposed validation approach.
- **Low Confidence:** The assertion that simpler models can reliably replace LLMs in many enterprise contexts, even with validation, is speculative without concrete performance comparisons.

## Next Checks

1. **Enterprise Case Study:** Implement the 5-stage design process in a specific enterprise domain (e.g., insurance claims processing) and measure the actual performance of LLM vs. simpler models under the proposed validation framework.
2. **Error Compounding Experiment:** Systematically measure accuracy degradation across multi-step workflows with varying step counts and validation strategies to quantify the compounding effect.
3. **Model Substitution Benchmark:** Create a standardized benchmark task where both LLM and validated simpler models are tested for equivalent business outcomes, measuring cost, reliability, and interpretability trade-offs.