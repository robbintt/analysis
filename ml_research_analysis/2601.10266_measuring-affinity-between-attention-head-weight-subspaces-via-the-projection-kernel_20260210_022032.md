---
ver: rpa2
title: Measuring Affinity between Attention-Head Weight Subspaces via the Projection
  Kernel
arxiv_id: '2601.10266'
source_url: https://arxiv.org/abs/2601.10266
tags:
- heads
- head
- layer
- scores
- weight
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of accurately measuring relationships
  between attention heads in Transformers, where existing metrics like the Composition
  Score (CS) are limited by normalization effects and scaling biases. The authors
  propose the Projection Kernel (PK), a principal-angle-based measure of subspace
  similarity, to quantify the overlap between attention-head weight subspaces.
---

# Measuring Affinity between Attention-Head Weight Subspaces via the Projection Kernel

## Quick Facts
- **arXiv ID:** 2601.10266
- **Source URL:** https://arxiv.org/abs/2601.10266
- **Reference count:** 40
- **Primary result:** Projection Kernel (PK) outperforms Composition Score (CS) in measuring attention-head affinity, showing clearer functional relationships and robustness to weight preprocessing.

## Executive Summary
This paper introduces the Projection Kernel (PK) as a new metric for quantifying the affinity between attention head weight subspaces in Transformers. Unlike the existing Composition Score (CS), PK is based on principal angles between subspaces and demonstrates superior performance in detecting known head-to-head interactions, particularly for functional head classes like Induction Heads and Identity Heads. Experiments on GPT2-small show PK's robustness to weight preprocessing and its ability to identify L4H7 as a key Identity Head hub. The method provides a framework for evaluating head relationships that is both mathematically principled and empirically effective.

## Method Summary
The Projection Kernel measures affinity between attention heads by computing the squared Frobenius norm of the product of orthonormal bases spanning the column spaces of weight matrices. For two weight matrices, QR decomposition generates orthonormal bases, and PK is calculated as the squared Frobenius norm of their product. The method is evaluated against CS using wiring diagrams, token projection, and head classification tasks on GPT2-small. The authors compare multiple weight pairings (OQ, OK, OV) and demonstrate PK's superiority through PR-AUC metrics and qualitative visualizations of head interactions.

## Key Results
- PK outperforms CS in head detection and classification tasks, achieving higher PR-AUC scores for identifying IOI-relevant heads
- Wiring diagrams using PK clearly show known head relationships (Induction Heads, Name Movers) while CS shows scattered connections
- PK identifies L4H7 as an Identity Head hub with high inlet/outlet scores, demonstrating functional significance
- PK is robust to weight preprocessing (LN folding), unlike CS which shows significant score changes
- Random orthogonal matrix baseline analysis shows PK distributions are distinguishable from theoretical expectations

## Why This Works (Mechanism)
PK works by measuring the principal angles between subspaces spanned by weight matrices, capturing the geometric overlap between attention head representations. The method leverages orthonormal bases to create a stable, scale-invariant measure of similarity that reflects the actual functional overlap between heads. By focusing on column spaces rather than individual vectors, PK captures the collective representational capacity of attention heads, making it more suitable for detecting functional relationships than point-wise metrics like CS.

## Foundational Learning
- **Principal Angles:** Measure the geometric relationship between subspaces; needed to quantify overlap between attention head representations. Quick check: Verify that principal angles range from 0 to π/2 for orthogonal bases.
- **Orthonormal Bases:** Used to create stable representations of weight subspaces; needed for consistent PK computation. Quick check: Confirm QR decomposition produces orthonormal columns with dot products near zero.
- **QR Decomposition:** Method for computing orthonormal bases from weight matrices; needed to implement PK. Quick check: Verify U^T U ≈ I for the computed basis U.
- **Frobenius Norm:** Measures the overall magnitude of matrix products; needed to aggregate principal angle information. Quick check: Confirm ||U^T U'||_F^2 ∈ [0, d_head] for orthonormal bases.
- **Subspace Span:** The geometric space covered by weight matrix columns; needed to define the objects being compared. Quick check: Verify that span(W) = span(U) for the QR basis U of W.
- **Weight Preprocessing:** Normalization techniques like LayerNorm folding; needed to understand PK's robustness. Quick check: Compare PK scores on original vs. preprocessed weights for stability.

## Architecture Onboarding

**Component Map:**
GPT2-small (12L, 12H) -> Attention Heads (WQ, WK, WV, WO matrices) -> Weight Subspaces -> Orthonormal Bases (QR) -> Projection Kernel Computation -> Affinity Scores

**Critical Path:**
Extract attention weights → Compute orthonormal bases via QR → Calculate PK between head pairings → Evaluate using wiring diagrams and PR-AUC metrics → Compare against CS and baselines

**Design Tradeoffs:**
PK trades computational simplicity for geometric accuracy, using orthonormal bases to create a stable similarity measure that is robust to scaling and preprocessing. The method assumes full-rank weight matrices, which may not hold in all transformer variants. PK focuses on column spaces rather than row spaces, representing a specific modeling choice that may miss certain relationships but captures functional representational overlap.

**Failure Signatures:**
- Row vs. column space confusion leads to incorrect similarity scores
- Improper normalization of PK values makes numerical comparison with CS difficult
- Mismatched weight preprocessing between PK and CS implementations causes divergent results
- Rank-deficient weight matrices violate the full-rank assumption underlying PK's geometric interpretation

**Three First Experiments:**
1. Compute PK for all 144 heads in GPT2-small using WQ, WK, WV, and WO matrices separately, verifying values are in [0, d_head]
2. Generate wiring diagrams for top 20 PK scores and compare against CS visualizations for functional head classes
3. Calculate PR-AUC for head detection using IOI annotations, comparing PK against CS and baseline metrics

## Open Questions the Paper Calls Out
- **Head Hub Specialization:** Is L4H7 uniquely specialized among Identity Heads, or do other Identity Heads perform similar hub functions? This requires comparative analysis of inlet/outlet scores across all Identity Heads.
- **Larger Model Generalization:** Can PK effectively quantify head relationships in larger models using RMSNorm and RoPE instead of LayerNorm and absolute embeddings? This requires testing on architectures like Llama or Mistral.
- **Baseline Distribution Accuracy:** Can an empirical baseline from observed weight matrices better test differences across pairing types than the current random orthogonal approximation? This requires developing and validating a new significance test framework.
- **Prompt-Specific Interpretation:** Can carefully designed input prompts improve the functional interpretation of heads compared to average corpus statistics? This requires task-specific prompt engineering and interpretation studies.

## Limitations
- Reliance on IOI dataset and annotations may limit generalizability to other tasks and models
- Theoretical random baseline assumptions may not fully capture attention head geometry
- Method requires full-rank weight matrices, which may not hold with certain regularization techniques
- Focus on column spaces represents a specific modeling choice that may miss certain head relationships

## Confidence
- **High Confidence:** Mathematical formulation of PK and its basic properties are sound and well-verified
- **Medium Confidence:** Interpretation of PK values in wiring diagrams and correspondence to functional head classes requires careful validation
- **Medium Confidence:** Superiority over CS in PR-AUC metrics is demonstrated but may vary with different task definitions

## Next Checks
1. Apply PK to other transformer architectures (OPT, LLaMA) and tasks beyond IOI to test generalizability of observed affinity patterns
2. Systematically compare PK computed from different weight components and basis generation methods to assess implementation sensitivity
3. Track PK score evolution during training to determine if identified head relationships emerge gradually or appear abruptly