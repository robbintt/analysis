---
ver: rpa2
title: Spherical Brownian Bridge Diffusion Models for Conditional Cortical Thickness
  Forecasting
arxiv_id: '2509.08442'
source_url: https://arxiv.org/abs/2509.08442
tags:
- cortical
- spherical
- sbdm
- bridge
- diffusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Spherical Brownian Bridge Diffusion Model
  (SBDM) for forecasting high-resolution, vertex-level cortical thickness trajectories.
  The method leverages a bidirectional Brownian bridge diffusion process to map between
  baseline cortical thickness and future changes, while incorporating tabular conditions
  such as demographics and diagnosis via a conditional spherical U-Net (CoS-UNet).
---

# Spherical Brownian Bridge Diffusion Models for Conditional Cortical Thickness Forecasting

## Quick Facts
- arXiv ID: 2509.08442
- Source URL: https://arxiv.org/abs/2509.08442
- Reference count: 34
- Primary result: SBDM achieves 0.092 mm MAE on CN subjects, outperforming existing methods.

## Executive Summary
This paper introduces the Spherical Brownian Bridge Diffusion Model (SBDM) for forecasting vertex-level cortical thickness trajectories. The method leverages a bidirectional Brownian bridge diffusion process to map between baseline cortical thickness and future changes, while incorporating tabular conditions such as demographics and diagnosis via a conditional spherical U-Net (CoS-UNet). Experiments on ADNI and OASIS datasets demonstrate that SBDM significantly reduces prediction errors compared to existing methods, achieving mean absolute errors as low as 0.092 mm on cognitively normal subjects.

## Method Summary
SBDM is a diffusion-based generative model that forecasts vertex-level cortical thickness (CTh) changes using a Brownian bridge process. The model takes baseline CTh maps, demographic information, diagnosis, and time interval as inputs, and generates predictions for future CTh changes. The CoS-UNet architecture incorporates spherical graph convolutions to handle the non-Euclidean geometry of cortical surfaces and cross-attention layers to integrate tabular conditions. The model is trained on longitudinal MRI data from ADNI and validated on both ADNI and OASIS datasets.

## Key Results
- SBDM achieves 0.092 mm MAE on cognitively normal subjects, significantly outperforming baselines
- Model generalizes well to external OASIS dataset
- Enables generation of both factual and counterfactual disease trajectories
- Reduces prediction errors compared to linear regression, spherical U-Net, and surface vision transformers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A bidirectional diffusion bridge preserves structural information more effectively than standard denoising diffusion for data-to-data translation.
- Mechanism: Standard diffusion models map from pure Gaussian noise to data, requiring the model to reconstruct an image's entire structure from scratch. The Brownian Bridge Diffusion Model (BBDM) instead maps stochastically between two structured endpointsâ€”a start point ($x_0$) and an end point ($x_B$). In SBDM, the baseline cortical thickness is "engraved" directly into the starting point of the generation process.
- Core assumption: The transformation from a baseline brain state to a future state can be modeled as a continuous, learnable trajectory that does not require the loss of all structural information.
- Evidence anchors:
  - [abstract] "...leverages a bidirectional Brownian bridge diffusion process to map between baseline cortical thickness and future changes..."
  - [section 3.1] "Unlike DDPM... we leverage the recently proposed Brownian bridge diffusion model (BBDM)... [which] stochastically maps between structured start- and endpoints, preserving data-specific characteristics..."
  - [corpus] The "Vision Bridge Transformer at Scale" paper supports this, contrasting bridge models that "directly model the trajectory between inputs and outputs" with traditional noise-to-data diffusion.

### Mechanism 2
- Claim: Spherical graph convolutions are required to capture spatially valid relationships on non-Euclidean cortical surfaces.
- Mechanism: Cortical surfaces are represented as triangular meshes (icospheres), which have no regular grid structure. Standard convolutions fail here because they assume a flat, Euclidean neighborhood. The CoS-UNet uses one-hop spherical graph convolutions that operate on the mesh's true geodesic neighbors.
- Core assumption: Cortical thinning patterns have spatially coherent structure on the manifold of the cortical surface, and this spatial information is critical for accurate forecasting.
- Evidence anchors:
  - [abstract] "...CoS-UNet integrates spherical convolutions... to handle the non-Euclidean geometry of cortical surfaces..."
  - [section 4.3] The ablation study shows that models not designed for spherical data (MLP, 1D U-Net) are "not competitive," yielding MAEs nearly three times higher than architectures using spherical processing.
  - [corpus] The "Spectral Diffusion Models on the Sphere" paper validates the difficulty of this problem, noting the "nontrivial geometric and stochastic issues that are absent in the Euclidean setting."

### Mechanism 3
- Claim: Dense cross-attention layers provide an effective mechanism for integrating low-dimensional, multi-modal tabular conditions into a high-dimensional generative process.
- Mechanism: The model must fuse high-resolution vertex data (thousands of points) with low-resolution tabular data (age, sex, diagnosis, time). SBDM embeds the tabular conditions and injects them into every stage of the U-Net encoder via cross-attention. This allows the model to learn a dynamic, non-linear mapping from demographics to specific regional effects on the cortex.
- Core assumption: The influence of demographic and clinical factors on cortical thickness change is spatially heterogeneous across the cortex.
- Evidence anchors:
  - [abstract] "...integrates spherical convolutions and cross-attention layers to... integrate multi-modal data."
  - [section 3.2] "...we inject [conditional embeddings] into every residual stage through a cross-attention layer, giving the network dense, stage-wise guidance."
  - [corpus] Direct evidence for cross-attention on tabular-spherical fusion is weak. The "Cortex-Grounded Diffusion Models" paper uses conditioning signals but does not specify the mechanism.

## Foundational Learning

- Concept: **Brownian Bridge Diffusion Process**
  - Why needed here: This is the core generative engine, fundamentally different from standard diffusion. It is the basis for how the model forecasts trajectories.
  - Quick check question: In a standard diffusion model, what is the starting point of the reverse generation process? In a Brownian bridge, what two pieces of information constrain the reverse process?

- Concept: **Spherical Signal Processing (Spherical Graph Convolutions)**
  - Why needed here: The paper's primary architectural contribution (CoS-UNet) is built on this. Without it, you cannot correctly process the input data or understand the model's design.
  - Quick check question: Why can't you apply a standard 2D convolution kernel directly onto a flattened map of the cerebral cortex? What is the "one-hop" neighborhood on an icosphere?

- Concept: **Cross-Attention for Multi-Modal Fusion**
  - Why needed here: This is the key mechanism for personalization, allowing the model to generate factual and counterfactual trajectories based on external variables.
  - Quick check question: In a transformer's cross-attention layer, what do the "query" and "key/value" inputs typically correspond to? How could you use this structure to make a U-Net's features dependent on a subject's age?

## Architecture Onboarding

- Component map:
  `tau_0 (baseline CTh) -> CoS-UNet (encoder with spherical conv + cross-attention) -> x_B (predicted change) -> tau_t = tau_0 + x_B`

- Critical path (Inference):
  The inference process (Algorithm 2) is where forecasting happens.
  1. Initialize: Set the first bridge point to the known baseline, `x_0 = tau_0`.
  2. Iterate: For `beta` from `0` to `B-1`, perform a bridge step:
      a. The core computation is the denoising network call: `f_theta(x_beta, beta, t, c)`. This is a full forward pass of the CoS-UNet.
      b. Use the network's output in the recursion `x_{beta+1} = ...` (Eq. 5) to get the next bridge point.
  3. Output: The final bridge point `x_B` is the predicted change `Delta_tau_t`. The future CTh is `tau_t = tau_0 + Delta_tau_t`.

- Design tradeoffs:
  - **Pixel Space vs. Latent Space:** SBDM operates directly on high-resolution CTh maps, improving interpretability and avoiding decoding errors but increasing computational cost compared to latent diffusion.
  - **Icosphere Resolution:** A higher subdivision level captures finer detail but increases vertex count quadratically, raising memory usage.
  - **Sampling Speed:** The paper uses a non-Markovian sampler to take larger steps, reducing generation time at the potential cost of sample quality.

- Failure signatures:
  - **High MAE in Precentral Gyrus:** A known difficulty, as this area has high anatomical variability. Consistent failure here suggests the spherical convolutions are struggling with specific geometric distortions.
  - **Unrealistic Trajectories:** Generated `Delta_tau_t` values that result in negative thickness indicate the model has not learned the physical constraints of the data.
  - **Condition Ignorance:** Factual and counterfactual trajectories being nearly identical implies the cross-attention mechanism has failed to integrate the tabular conditions.
  - **Spatial Noise:** "Dotted heterogeneities" in error maps suggest the denoising process is not smoothing predictions correctly.

- First 3 experiments:
  1. **Baseline Comparison:** Replicate the "Linear Regression" and "Spherical U-Net" baselines from Table 1 on a small data subset to validate the data pipeline and confirm the performance gap SBDM is designed to close.
  2. **Architecture Ablation:** Replace the spherical graph convolutions in CoS-UNet with standard 1D convolutions (treating vertices as a sequence). Measure the MAE increase to quantify the value of geometric inductive bias.
  3. **Conditioning Ablation:** Train a model with all conditions and a second model *without* the follow-up diagnosis `d_t`. Generate trajectories for the same subject with both models to verify that `d_t` has a measurable impact on the predicted atrophy pattern.

## Open Questions the Paper Calls Out

- **Question:** How can the biological plausibility of counterfactual disease trajectories be objectively evaluated in the absence of ground truth longitudinal data?
  - Basis in paper: [explicit] The authors state that "counterfactual scenarios generally lack a definitive ground truth, making it difficult to assess their realism."
  - Why unresolved: By definition, a counterfactual represents a progression that did not happen for a specific subject, meaning no empirical MRI data exists to serve as a direct target for validation.
  - What evidence would resolve it: Validation studies correlating generated counterfactual atrophy rates with known histopathological data or large-scale population statistics derived from subjects who actually underwent the target condition.

- **Question:** Can the SBDM framework be adapted to latent space diffusion to improve computational efficiency without sacrificing the high vertex-level resolution achieved by operating on raw cortical thickness maps?
  - Basis in paper: [inferred] The paper notes it operates "directly in the space of cortical thickness maps" to avoid "suboptimal decoding" and degradation of fine-grained outputs, implicitly highlighting the trade-off between direct optimization and the efficiency of latent representations.
  - Why unresolved: The authors prioritize resolution over the potential efficiency gains of latent spaces, leaving the challenge of designing a spherical autoencoder that preserves fine-grained geometric details unsolved.
  - What evidence would resolve it: A comparative study demonstrating that a latent spherical diffusion model can reconstruct vertex-level details with the same fidelity (MAE) as the direct approach while reducing training/inference time.

- **Question:** To what extent does the inclusion of additional biological biomarkers (e.g., amyloid/tau status) improve prediction accuracy for heterogeneous disease groups like AD and MCI?
  - Basis in paper: [inferred] The results indicate higher prediction errors for MCI and AD subjects compared to cognitively normal (CN) subjects, which the authors attribute to the "heterogeneity of atrophy patterns in Alzheimer's disease."
  - Why unresolved: The current model relies on demographics and diagnosis, which may be too coarse to capture the diverse biological pathways of neurodegeneration, limiting accuracy in non-normal cohorts.
  - What evidence would resolve it: Experiments adding biological biomarkers to the conditional variables $c$ to see if the performance gap (MAE difference) between CN and AD/MCI groups narrows.

## Limitations

- The cross-attention conditioning mechanism, while described, lacks direct empirical validation in the paper's ablation studies, leaving open the question of whether it provides a significant advantage over simpler conditioning strategies.
- The choice of spherical mesh resolution (icosphere level) is not specified, which could materially affect both performance and computational efficiency.
- External validation is limited to OASIS; results on other independent cohorts or clinical populations are unreported.

## Confidence

- **High confidence** in the core technical contributions: the Brownian bridge formulation and spherical convolution architecture are clearly defined and experimentally validated.
- **Medium confidence** in the conditioning mechanism's effectiveness, due to sparse ablation evidence.
- **Medium confidence** in generalization, given limited external dataset testing.

## Next Checks

1. Replicate the ablation study by replacing spherical graph convolutions with 1D convolutions to quantify the impact of non-Euclidean processing.
2. Isolate the contribution of the cross-attention conditioning by training a variant without it and comparing factual/counterfactual trajectory diversity.
3. Validate performance on an additional independent dataset (e.g., AIBL or local clinical cohort) to assess robustness beyond OASIS.