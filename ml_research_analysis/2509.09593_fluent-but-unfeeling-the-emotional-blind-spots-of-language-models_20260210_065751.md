---
ver: rpa2
title: 'Fluent but Unfeeling: The Emotional Blind Spots of Language Models'
arxiv_id: '2509.09593'
source_url: https://arxiv.org/abs/2509.09593
tags:
- emotion
- emotions
- language
- mask
- emotional
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study introduces EXPRESS, a benchmark dataset for fine-grained\
  \ emotion recognition that addresses limitations in existing benchmarks by using\
  \ self-disclosed emotions from Reddit posts with an average length of 259 words.\
  \ A novel evaluation framework decomposes emotions into 10 dimensions using Plutchik\u2019\
  s emotion theory to assess language models beyond predefined emotion categories."
---

# Fluent but Unfeeling: The Emotional Blind Spots of Language Models

## Quick Facts
- arXiv ID: 2509.09593
- Source URL: https://arxiv.org/abs/2509.09593
- Reference count: 40
- Primary result: EXPRESS benchmark reveals LLMs struggle with fine-grained emotion prediction, with CoT prompting degrading performance in this subjective task

## Executive Summary
This study introduces EXPRESS, a benchmark dataset for fine-grained emotion recognition that addresses limitations in existing benchmarks by using self-disclosed emotions from Reddit posts with an average length of 259 words. A novel evaluation framework decomposes emotions into 10 dimensions using Plutchik's emotion theory to assess language models beyond predefined emotion categories. Systematic testing of 14 models reveals that predicting emotions aligning with human self-disclosures remains challenging, with accuracy metrics (AccL, AccV, F1V) generally low, though performance improves with model size and few-shot prompting. Notably, CoT prompting degraded performance in this subjective task. Qualitative analysis showed that model-generated emotions sometimes matched theoretical definitions better than self-disclosures, but often missed contextual cues.

## Method Summary
The study creates EXPRESS, a dataset of 33,679 Reddit posts where emotion words are masked and replaced with [MASK] tokens. Posts are segmented to 512 tokens and collected via regex patterns matching "I feel + emotion" disclosures. The dataset uses self-disclosed emotions rather than annotator labels, assuming users are authoritative on their own emotional states. A novel evaluation framework maps predicted emotion words to 10-dimensional vectors using Plutchik's emotion theory via NRC Emotion Lexicon, enabling comparison beyond exact word matches. The study tests 14 language models across zero-shot, few-shot (random and nearest-neighbor), and chain-of-thought prompting settings, measuring lexical accuracy (exact word matches), vector accuracy (exact vector matches), and average vector F1 scores.

## Key Results
- Masked language models (MLMs) like RoBERTa perform comparably to much larger causal models (CLMs) despite having significantly fewer parameters
- Chain-of-thought prompting degrades performance in subjective emotion recognition tasks, with smaller models showing dramatic drops in valid response rates
- Nearest-neighbor few-shot prompting with semantically aligned examples improves performance compared to random few-shot examples
- Models frequently over-predict generic emotions (anxious, grateful, frustrated) while missing more specific emotional states present in self-disclosures

## Why This Works (Mechanism)

### Mechanism 1
Bidirectional attention architectures (MLMs) may outperform autoregressive models (CLMs) in emotion "fill-in-the-blank" tasks despite having significantly fewer parameters. MLMs like RoBERTa utilize context from both sides of a masked token simultaneously, potentially capturing the full narrative arc of a self-disclosure before predicting the emotion. CLMs, restricted to left-to-right context, must infer the emotional resolution before consuming the full explanatory context. The core assumption is that the self-disclosed emotion is often a summary or conclusion of the surrounding text, making bidirectional visibility advantageous for this specific task structure. Evidence shows MLMs are ranked among top seven models despite parameter disadvantages.

### Mechanism 2
In-context learning improves emotion prediction accuracy when few-shot examples are semantically aligned (nearest neighbors) rather than randomly selected. Providing examples with similar narrative structures or emotional contexts appears to prime the model's latent space, reducing the search space for the appropriate emotion label. Random examples may introduce noise or conflicting emotional schemas. The core assumption is that BERT-based embeddings effectively capture semantic similarity in emotional narratives, ensuring that "nearest" examples are actually relevant to the target situation. Evidence shows few-shot setting with nearest distanced examples further boosts models' performance with average improvements in AccV of 0.053.

### Mechanism 3
Chain-of-Thought (CoT) prompting degrades performance in fine-grained emotion recognition by encouraging reasoning that relies on priors rather than context. Explicit step-by-step reasoning prompts may cause models to verbalize stereotypical associations or general theories (priors) rather than attending to the specific, nuanced details of the user's self-disclosure. The core assumption is that subjective tasks like emotion recognition rely more on direct pattern matching ("system 1" thinking) than logical deduction ("system 2" thinking). Evidence shows CoT prompting degrades performance, with smaller models (e.g., Gemma-2-9B) dropping valid response rates from 99.9% to 76% when forced to use CoT.

## Foundational Learning

- **Concept: Plutchik's Wheel of Emotion**
  - Why needed here: The paper maps 251 raw emotion labels to 8 basic dimensions (e.g., joy, sadness, fear) plus sentiment. Understanding this decomposition is required to interpret "Vector Accuracy" (AccV) vs. "Lexical Accuracy" (AccL).
  - Quick check question: If a model predicts "furious" but the ground truth is "angry," would AccL or AccV capture this similarity?

- **Concept: Ecological Validity in Data Annotation**
  - Why needed here: The EXPRESS dataset relies on "self-disclosed" emotions from Reddit rather than external annotators. This assumes users are the authoritative source on their own internal state, a methodological stance distinct from expert-labeled datasets.
  - Quick check question: Why might an external expert label a post as "angry" when the user self-discloses "hurt," and which label does this paper prioritize?

- **Concept: Masked vs. Causal Language Modeling Objectives**
  - Why needed here: The paper demonstrates that RoBERTa (Masked) performs comparably to GPT-4o (Causal). Understanding the difference between predicting `[MASK]` tokens using bidirectional context vs. predicting the next token using past context explains this performance parity.
  - Quick check question: Does a causal model have access to the text that appears *after* the emotion word when making its prediction?

## Architecture Onboarding

- **Component map:**
  EXPRESS Dataset -> Masking Layer -> Inference Engine -> Decomposition Module -> Evaluation Layer
  (33,679 Reddit posts) -> (replace emotions with [MASK]) -> (14 language models) -> (10-dim vector mapping) -> (AccL, AccV, F1_V metrics)

- **Critical path:**
  1. Filtering: Regex matching for "I feel + [emotion]" patterns to ensure self-disclosure
  2. Inference: Pass masked text to model (Zero-shot, Few-shot, or CoT)
  3. Mapping: Convert model output string to 10-d vector via EmoLex (or crowdsourced extension)
  4. Scoring: Compare predicted vector against self-disclosed ground truth vector

- **Design tradeoffs:**
  - Segmentation: Capping context at 512 tokens ensures compatibility with models like RoBERTa but theoretically discards long-range dependencies
  - Label Granularity: Using 251 labels allows for high fidelity but results in low Lexical Accuracy (0.051â€“0.318), necessitating Vector Accuracy to show true capability

- **Failure signatures:**
  - Intensity Dampening: Models frequently predict "angry" instead of "furious" or "afraid" instead of "terrified"
  - Generic Over-prediction: Over-reliance on safe, high-frequency terms like "anxious" or "frustrated" when the context implies more specific states
  - CoT Format Collapse: Smaller models (e.g., Gemma-2-9B) drop response validity from 99.9% to 76% when forced to use CoT

- **First 3 experiments:**
  1. **Sanity Check (AccL vs AccV):** Run zero-shot inference on 100 random samples. Verify that AccV is significantly higher than AccL to confirm the model understands the "type" of emotion even if the specific word choice differs.
  2. **Retrieval Ablation:** Implement the "nearest neighbor" few-shot retrieval using BERT embeddings. Compare performance against random few-shot to validate the "in-context learning" gain.
  3. **CoT Verification:** Test CoT prompting on the subset of data where the model previously failed (mismatched vectors). Analyze reasoning traces to confirm if the model is "ignoring context" or simply hallucinating definitions.

## Open Questions the Paper Calls Out

- **Open Question 1:** In instances where LLMs miss contextual cues, are they failing to register the information or prioritizing "internal world modeling" over the provided text? The qualitative analysis identified that LLMs miss cues but could not determine the internal processing cause of these specific misalignments. Attention head analysis or ablation studies on specific context words would resolve this.

- **Open Question 2:** Can the qualitative rationales provided by domain experts be effectively utilized to fine-tune models for improved emotion recognition? The current study focused on zero-shot and few-shot prompting baselines rather than model training or alignment strategies. A comparative study showing performance gains after fine-tuning models on EXPRESS augmented with expert reasoning would resolve this.

- **Open Question 3:** How does the demographic bias of the EXPRESS dataset (predominantly male, young, Western) impact the generalizability of emotion recognition models to diverse global populations? The Limitations section identifies Reddit's demographic bias and explicitly calls for future work to "broaden the demographic coverage." Evaluation against datasets from other platforms or stratified demographic groups would measure performance gaps.

## Limitations

- The EXPRESS dataset's reliance on self-disclosed emotions from Reddit may introduce demographic and cultural biases not accounted for in the analysis
- The segmentation approach truncating posts to 512 tokens could theoretically discard important emotional context, though experiments showed no significant performance impact
- The bidirectional context advantage of MLMs may not translate to other emotion recognition tasks, particularly open-ended generation or dialogue systems where CLMs are architecturally necessary

## Confidence

- **High Confidence**: The observation that CoT prompting degrades performance in subjective emotion recognition tasks is well-supported by systematic testing across 14 models
- **Medium Confidence**: The claim that nearest-neighbor few-shot prompting improves performance assumes that BERT-based embeddings effectively capture semantic similarity in emotional narratives
- **Medium Confidence**: The finding that MLMs perform comparably to CLMs despite fewer parameters is robust for this specific mask-filling task, but generalizability to other emotion recognition architectures is not fully established

## Next Checks

1. **Cross-cultural validation**: Test the EXPRESS benchmark with self-disclosed emotions from non-Western Reddit communities or alternative platforms to assess whether the observed MLM advantages hold across cultural contexts where emotional expression patterns differ.

2. **Architectural ablation study**: Create a modified version of the benchmark where the emotion word is not masked but instead appears at the end of the text, forcing CLMs to rely on left-to-right context only, to isolate whether the MLM advantage is truly architectural or task-dependent.

3. **Human expert comparison**: Conduct a controlled study where trained emotion recognition experts label a subset of EXPRESS posts, comparing their annotations against both self-disclosures and model predictions to establish a "ground truth" reference point for evaluating model performance.