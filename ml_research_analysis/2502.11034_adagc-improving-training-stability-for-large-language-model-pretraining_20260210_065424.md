---
ver: rpa2
title: 'AdaGC: Improving Training Stability for Large Language Model Pretraining'
arxiv_id: '2502.11034'
source_url: https://arxiv.org/abs/2502.11034
tags:
- adagc
- adamw
- training
- loss
- gradient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses loss spikes in large language model pretraining,
  which cause training instability and degraded performance. The authors propose AdaGC,
  an adaptive gradient clipping method that applies parameter-specific thresholds
  based on exponential moving averages of gradient norms.
---

# AdaGC: Improving Training Stability for Large Language Model Pretraining

## Quick Facts
- arXiv ID: 2502.11034
- Source URL: https://arxiv.org/abs/2502.11034
- Reference count: 40
- Key outcome: Eliminates loss spikes in LLM pretraining, achieving 3.5% perplexity reduction on WikiText and 1.47% validation perplexity improvement on 13B models

## Executive Summary
This paper addresses the critical problem of loss spikes in large language model pretraining, which cause training instability and degraded performance. The authors propose AdaGC, an adaptive gradient clipping method that applies parameter-specific thresholds based on exponential moving averages of gradient norms. Unlike global clipping, AdaGC dynamically adjusts per-parameter thresholds to handle both temporal gradient norm decay and spatial parameter heterogeneity. Extensive experiments demonstrate complete loss spike elimination on Llama-2 7B/13B models while achieving significant perplexity improvements.

## Method Summary
AdaGC applies per-parameter gradient clipping thresholds based on exponential moving averages of historical gradient norms. The method computes a parameter-specific threshold as λrel times the EMA of previous gradient norms, clipping gradients only when they exceed this adaptive threshold. A warmup phase using global clipping for the first 100 steps prevents early instability from large initial gradients. The approach integrates seamlessly with existing optimizers like AdamW and Lion, requiring minimal additional memory for EMA state tracking while providing significant stability improvements over traditional global clipping approaches.

## Key Results
- Eliminates loss spikes entirely on Llama-2 7B/13B models, reducing perplexity by 3.5% on WikiText
- Achieves 1.47% validation perplexity improvement on 13B models with 25% faster convergence on CLIP ViT-Base
- Maintains O(1/√T) convergence rate comparable to Adam while outperforming StableAdamW
- Successfully integrates with both AdamW and Lion optimizers across multiple architectures

## Why This Works (Mechanism)

### Mechanism 1
Per-parameter gradient clipping thresholds, adaptive to each parameter's historical gradient norms, improve stability and performance over global clipping. AdaGC maintains an EMA of gradient norms for each parameter tensor and clips gradients for that specific tensor if their norm exceeds a threshold proportional to this EMA. This targets "outlier gradients" specific to that parameter, recognizing that different layers exhibit heterogeneous gradient behaviors and spike at different times/magnitudes.

### Mechanism 2
Smoothing the clipping threshold with an EMA of historical gradient norms allows the method to adapt to the "normal" behavior of a parameter over time. The EMA provides a smoothed baseline of what is a "normal" gradient magnitude for a parameter, automatically tightening as gradients decay while maintaining efficacy against spikes without manual retuning. This addresses the temporal decay in gradient norms during training.

### Mechanism 3
Theoretical convergence is preserved, with the method converging at the same rate as Adam with global clipping. The paper provides a theoretical proof showing that Adam with AdaGC maintains an O(1/√T) convergence rate under non-convex conditions, suggesting that the adaptive, local clipping does not fundamentally harm the optimization landscape in a way that slows convergence.

## Foundational Learning

- **Gradient Clipping**: Understanding what global clipping is and why it's used (preventing exploding gradients/loss spikes). Quick check: What is the update rule for standard gradient clipping, and what is its primary goal?

- **Exponential Moving Average (EMA)**: Core mathematical tool used to compute adaptive thresholds. Understanding its smoothing properties and how the β coefficient affects it. Quick check: If β is 0.9, approximately how much weight is given to the most recent value in the EMA calculation compared to the historical average?

- **Non-Convex Optimization & Convergence Rates**: To contextualize the theoretical contribution. The O(1/√T) rate is a standard metric for comparing optimizer efficiency. Quick check: Why is convergence rate an important metric for an optimization algorithm, and what does O(1/√T) signify about the speed of learning?

## Architecture Onboarding

- **Component map**: AdaGC Module -> Optimizer Wrapper (AdamW/Lion) -> Parameter Update
- **Critical path**: 1) Compute Gradients via backward pass, 2) Apply AdaGC Logic (compute per-tensor norms, update EMA state, clip if needed), 3) Optimizer Step consumes clipped gradients
- **Design tradeoffs**: Requires additional memory for EMA state per parameter tensor; per-parameter granularity is optimal but shard-wise can introduce inconsistency; minimal computational overhead
- **Failure signatures**: Loss spikes persist (λrel too high or Tstart insufficient), Training slows/Over-clipping (λrel too low or β too low), Instability at initialization (not using warmup phase)
- **First 3 experiments**: 1) Ablation on λrel (vary 1.01-1.10 on GPT-2 125M), 2) Ablation on Warmup (Tstart) comparing with/without global clipping warmup, 3) Comparison to Baselines (GlobalGC and AdamW no clipping) on WikiText perplexity

## Open Questions the Paper Calls Out

### Open Question 1
Does AdaGC maintain complete loss spike elimination and performance gains over full-scale pretraining durations (e.g., trillions of tokens) on models larger than 13B parameters? The paper only covers 36B tokens of training, while modern LLM pretraining requires trillions of tokens where gradient behaviors might diverge from early training dynamics.

### Open Question 2
Can the shard-wise adaptation strategy be refined to eliminate the 4.3% loss variance increase observed in Tensor Parallelism setups? The paper identifies a trade-off where distributed efficiency currently degrades training stability compared to the theoretical parameter-wise ideal.

### Open Question 3
How sensitive are the optimal hyperparameters (β and λrel) to extreme scale, given they were tuned primarily on the Llama-2 Tiny model? As model size increases, the dynamics of gradient norm decay may shift, potentially requiring different EMA momentum or relative thresholds.

## Limitations
- Source code not yet available for independent verification
- Theoretical analysis assumes bounded gradients which may not hold with severe loss spikes
- Requires additional memory for EMA state tracking per parameter tensor
- Ablation study focuses primarily on λrel while holding other hyperparameters constant

## Confidence

- **High Confidence**: The core mechanism of per-parameter adaptive clipping based on EMA of gradient norms is well-supported by both theoretical analysis and extensive empirical results across multiple architectures (Llama-2, CLIP).
- **Medium Confidence**: The claimed 3.5% perplexity reduction and 1.47% validation improvement depend on specific hyperparameter choices that may not generalize.
- **Low Confidence**: The assertion that this is the "first" adaptive clipping method for LLMs is questionable given related work like ZClip and AGGC.

## Next Checks

1. **Reproduce the ablation study**: Independently verify the sensitivity of AdaGC performance to λrel across multiple models and datasets, including testing values outside the reported 1.01-1.10 range to establish the full stability region.

2. **Memory overhead validation**: Measure and report the actual memory overhead of storing per-tensor EMA states for various model sizes, and validate that the per-parameter granularity is necessary rather than per-shard clipping.

3. **Convergence rate validation**: Design experiments to empirically test whether AdaGC maintains the claimed O(1/√T) convergence rate by tracking the training loss decay on a log-log scale and comparing against theoretical predictions.