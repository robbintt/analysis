---
ver: rpa2
title: LLMs Learn Constructions That Humans Do Not Know
arxiv_id: '2508.16837'
source_url: https://arxiv.org/abs/2508.16837
tags:
- constructions
- construction
- these
- which
- sentences
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that large language models (LLMs) learn
  syntactic constructions that do not actually exist for human speakers, creating
  false positive constructions through a confirmation bias in probing methods. The
  authors investigate this by creating 500 sentences representing five true clause-level
  constructions, then using both meta-linguistic prompts and contextual embeddings
  to test whether models hallucinate additional constructions.
---

# LLMs Learn Constructions That Humans Do Not Know

## Quick Facts
- arXiv ID: 2508.16837
- Source URL: https://arxiv.org/abs/2508.16837
- Reference count: 9
- Large language models learn syntactic constructions that do not exist for human speakers

## Executive Summary
This paper demonstrates that large language models learn syntactic constructions that do not actually exist for human speakers, creating false positive constructions through a confirmation bias in probing methods. The authors investigate this by creating 500 sentences representing five true clause-level constructions, then using both meta-linguistic prompts and contextual embeddings to test whether models hallucinate additional constructions. Results show high accuracy (91-94%) in clustering sentences into hallucinated constructions that are invisible to human linguists, regardless of whether probing uses explicit meta-linguistic prompts or implicit behavioral methods. The authors conclude that construction probing methods suffer from a fundamental confirmation bias, as incorrect hypotheses about syntactic structures would be validated by these methods.

## Method Summary
The authors created 500 sentences representing five true clause-level constructions, then used both meta-linguistic prompts and contextual embeddings to test whether models hallucinate additional constructions. They employed probing methods that either explicitly asked models to identify constructions or implicitly tested models' ability to cluster sentences based on learned patterns. The study tested both behavioral probing (asking models to explain their reasoning) and embedding-based clustering approaches to compare results across different methodological approaches.

## Key Results
- High accuracy (91-94%) in clustering sentences into hallucinated constructions
- Hallucinated constructions are invisible to human linguists
- Results hold regardless of whether probing uses explicit meta-linguistic prompts or implicit behavioral methods
- Construction probing methods show fundamental confirmation bias

## Why This Works (Mechanism)
The paper demonstrates that LLMs learn constructions through overly specific contextual patterns rather than genuine schematic generalizations. When models encounter similar but not identical patterns across different sentences, they create false positive constructions by over-generalizing from these patterns. This happens because LLMs rely heavily on distributional statistics and co-occurrence patterns in training data, leading them to identify patterns that may not represent true linguistic constructions but rather artifacts of the training corpus.

## Foundational Learning
- Construction Grammar: Framework for understanding how language users mentally represent linguistic patterns; needed to understand what constitutes a "construction" versus mere word combinations
- Probing methods: Techniques for testing what knowledge models have learned; needed to understand how the authors tested for constructions
- Syntactic constructions: Recurrent patterns of words that convey meaning; needed to grasp what the paper claims LLMs are learning incorrectly
- Confirmation bias in ML: Tendency for methods to validate whatever hypothesis is tested; needed to understand the core problem with current probing approaches
- Contextual embeddings: Vector representations that capture word meaning based on surrounding context; needed to understand one approach used for testing constructions

## Architecture Onboarding
Component map: Sentence corpus -> Probing method (meta-linguistic or embedding-based) -> Clustering algorithm -> Construction identification -> Validation against human knowledge

Critical path: The study focuses on the transition from sentence representations to construction identification, where the critical vulnerability occurs. Models map similar contextual patterns to the same construction label, creating false positives.

Design tradeoffs: The authors chose to test both explicit prompting and implicit embedding methods to show that confirmation bias exists across different probing approaches. They could have used more diverse construction types but limited themselves to five to maintain experimental control.

Failure signatures: The key failure mode is when clustering algorithms group sentences together based on superficial contextual similarity rather than genuine constructional similarity. This manifests as high accuracy scores for groupings that human linguists cannot identify as coherent constructions.

Three first experiments:
1. Test whether different clustering algorithms produce different false positive rates
2. Vary the number of sentences per construction to see if results scale
3. Compare results when using sentences from different domains (news, fiction, academic)

## Open Questions the Paper Calls Out
None

## Limitations
- The five constructions tested may not represent universally unknown patterns to all human speakers across all languages and dialects
- Results may be specific to BERT and similar transformer architectures rather than general to all LLMs
- The paper doesn't demonstrate whether these hallucinated constructions have any practical impact on downstream tasks

## Confidence
- Claim that the five constructions represent genuinely unknown human knowledge: Medium
- Claim that LLMs create "false positive constructions": Medium
- Claim that this creates fundamental problems for using LLMs to validate linguistic theory: Low

## Next Checks
1. Conduct systematic cross-linguistic validation with speakers of multiple languages/dialects to verify that the five constructions are truly unknown to humans
2. Test whether the hallucinated constructions emerge when using different LLM architectures (beyond BERT) and different training corpora to determine if this is a model-specific artifact
3. Perform a controlled study where human linguists are given the same probing prompts without knowing the experimental hypothesis to test whether humans would also "hallucinate" similar constructions given the same methodological constraints