---
ver: rpa2
title: 'Prompting Science Report 2: The Decreasing Value of Chain of Thought in Prompting'
arxiv_id: '2506.07142'
source_url: https://arxiv.org/abs/2506.07142
tags:
- step
- direct
- species
- answer
- average
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This report examines Chain-of-Thought (CoT) prompting effectiveness
  across various AI models using the GPQA Diamond benchmark. The study compares non-reasoning
  models (Claude 3.5 Sonnet, Gemini 2.0 Flash, GPT-4o-mini, GPT-4o, and Gemini Pro
  1.5) with reasoning models (o3-mini, o4-mini, and Gemini Flash 2.5) under three
  prompting conditions: direct answers, CoT prompting, and default unprompted behavior.'
---

# Prompting Science Report 2: The Decreasing Value of Chain of Thought in Prompting

## Quick Facts
- arXiv ID: 2506.07142
- Source URL: https://arxiv.org/abs/2506.07142
- Authors: Lennart Meincke; Ethan Mollick; Lilach Mollick; Dan Shapiro
- Reference count: 0
- Primary result: Chain-of-Thought prompting shows diminishing returns for modern AI models, particularly for dedicated reasoning models.

## Executive Summary
This study examines Chain-of-Thought (CoT) prompting effectiveness across AI models using the GPQA Diamond benchmark. For non-reasoning models, CoT showed modest but significant improvements in average performance (RD = 0.044-0.135, p < .001), particularly for models that don't naturally engage in step-by-step reasoning. However, CoT increased response time by 35-600% and introduced more variability, with some models showing significant declines in questions requiring 100% accuracy. For reasoning models, CoT yielded only marginal accuracy improvements (RD = 0.029-0.031, p = .003-.024) while significantly increasing response times by 20-80%. The findings suggest that while simple CoT prompts can boost average performance in non-reasoning models, the gains must be weighed against increased costs, time, and potential decreases in perfect accuracy.

## Method Summary
The study evaluated Chain-of-Thought prompting across eight AI models (five non-reasoning: Claude 3.5 Sonnet, GPT-4o, Gemini 2.0 Flash, GPT-4o-mini, Gemini Pro 1.5; three reasoning: o3-mini, o4-mini, Gemini Flash 2.5) using the GPQA Diamond benchmark (198 PhD-level multiple-choice questions). Three prompting conditions were tested: Direct answers, CoT prompting, and default unprompted behavior. Each model-question-condition combination ran 25 trials at temperature=0. Performance was measured across four metrics: 100% correct (25/25), 90% correct (23/25), 51% correct (13/25), and average rating. Statistical comparisons used paired bootstrap-permutation tests (5,000 replicates).

## Key Results
- CoT improved average accuracy for non-reasoning models (RD = 0.044-0.135, p < .001) but increased latency by 35-600%
- CoT introduced variability that harmed perfect-accuracy rates on easier questions for three of five non-reasoning models
- Reasoning models showed negligible accuracy gains from CoT (RD = 0.029-0.031, p = .003-.024) while becoming 20-80% slower
- Many modern non-reasoning models already perform implicit CoT reasoning by default, reducing the value of explicit CoT prompts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Many modern LLMs already perform implicit Chain-of-Thought reasoning by default, reducing the marginal value of explicit CoT prompts.
- Mechanism: When a model has been trained or fine-tuned to engage in step-by-step processing without explicit instruction, adding a "think step by step" prompt produces redundant reasoning traces with minimal accuracy gains. The model's default behavior already activates relevant reasoning pathways.
- Core assumption: The observed default reasoning behavior is functionally equivalent to explicit CoT, not merely superficial pattern-matching that looks like reasoning.
- Evidence anchors:
  - [abstract] "We also found that many recent models perform some form of CoT reasoning even if not asked; for these models, a request to perform CoT had little impact."
  - [page 3] "At the time of writing, most non-reasoning models have a tendency to produce a short reasoning trace before answering, similar to CoT."
  - [corpus] Related work ("How Chain-of-Thought Works?") suggests CoT may serve as a decoding constraint; if models already apply similar constraints internally, external prompting is redundant.
- Break condition: If default behavior changes (e.g., models are instructed to answer immediately without reasoning), explicit CoT prompts should regain effectiveness.

### Mechanism 2
- Claim: CoT prompting improves average accuracy on difficult questions but introduces variability that can reduce perfect-accuracy rates on easier questions.
- Mechanism: Generating additional reasoning tokens expands the solution space, which helps on complex problems where the model might otherwise converge prematurely. However, longer reasoning chains increase opportunities for intermediate errors that propagate to final answers on questions the model would answer correctly with direct retrieval.
- Core assumption: The variability introduced is meaningful (affecting final answers) rather than cosmetic variation in reasoning style.
- Evidence anchors:
  - [page 6] "In three of the five non-reasoning models, CoT introduced a higher likelihood of errors on 'easy' questions that the model would otherwise get right, harming performance on the '100% Correct' metric."
  - [page 4] Gemini Flash 2.0 showed RD = -0.131 for 100% correct (significant decline) while improving on average rating (RD = 0.135).
  - [corpus] Sparse autoencoding work on CoT faithfulness suggests generated thoughts may not reflect true internal reasoning—variability may stem from unfaithful reasoning traces.
- Break condition: If tasks have no "easy" questions (uniformly high difficulty), the tradeoff shifts toward CoT benefits; if tasks require 100% reliability, CoT may be contraindicated.

### Mechanism 3
- Claim: Dedicated reasoning models have built-in reasoning processes that make external CoT prompts largely redundant with substantial latency costs.
- Mechanism: Reasoning models (o3-mini, o4-mini, Flash 2.5) are architected to allocate compute to internal reasoning before output. External CoT prompts add a second reasoning layer without improving the first, doubling latency (20-80% increase) for negligible accuracy gains (RD = 0.029-0.031).
- Core assumption: The internal reasoning process in these models is sufficiently complete that external prompting cannot meaningfully augment it.
- Evidence anchors:
  - [page 5] "For reasoning models, Chain-of-Thought prompting yielded only marginal improvements in accuracy... CoT requests took between 20-80% (10-20 seconds) longer than direct requests."
  - [page 6] "For dedicated reasoning models, the added benefits of explicit CoT prompting appear negligible and may not justify the substantial increase in processing time."
  - [corpus] Weak direct evidence in corpus for reasoning model internals; related work focuses on standard LLMs. Assumption remains untested at mechanistic level.
- Break condition: If reasoning models are deployed on novel task distributions where their internal reasoning heuristics don't transfer, external CoT may provide scaffolding benefits.

## Foundational Learning

- **Concept: Risk Difference (RD) and statistical significance**
  - Why needed here: The paper reports effect sizes as RD (e.g., 0.135) with p-values; understanding that RD represents the absolute difference in success rates between conditions is essential for interpreting practical significance versus statistical significance.
  - Quick check question: If a model improves from 40% to 50% accuracy with CoT, what is the RD?

- **Concept: Temperature and sampling variability in LLMs**
  - Why needed here: The study uses temperature = 0 but still runs 25 trials per question; understanding that even deterministic settings can produce variability (via token sampling or API nondeterminism) explains why multiple trials are necessary.
  - Quick check question: Why might an LLM produce different answers to the same question even at temperature 0?

- **Concept: Implicit vs. explicit reasoning in model behavior**
  - Why needed here: The central finding hinges on models performing reasoning "by default"; distinguishing between genuine implicit reasoning and pattern-matching that resembles reasoning is critical for interpreting results.
  - Quick check question: How would you test whether a model's default "reasoning" is functionally equivalent to explicit CoT?

## Architecture Onboarding

- **Component map:**
  Model layer: Non-reasoning models (Claude 3.5 Sonnet, GPT-4o, Gemini 2.0 Flash, GPT-4o-mini, Gemini Pro 1.5) vs. Reasoning models (o3-mini, o4-mini, Gemini Flash 2.5)
  Prompt layer: Three conditions—Direct ("Answer directly without any explanation"), CoT ("Think step by step"), Default (no prompt suffix, no formatting constraint)
  Evaluation layer: Four metrics—Average rating, 51% correct (majority), 90% correct, 100% correct (strict)
  Benchmark: GPQA Diamond (198 PhD-level multiple-choice questions across biology, physics, chemistry)

- **Critical path:**
  1. Identify model type (reasoning vs. non-reasoning) → determines whether CoT is worth testing
  2. Determine task requirements (average performance vs. strict accuracy) → determines which metric to optimize
  3. Test Default behavior first → check if model already performs implicit CoT
  4. If needed, test CoT vs. Direct → measure RD and latency tradeoff
  5. Weigh accuracy gains against latency/cost increases

- **Design tradeoffs:**
  - CoT improves average performance but may harm perfect-accuracy requirements
  - CoT increases latency 35-600% (non-reasoning) or 20-80% (reasoning)
  - For reasoning models, explicit CoT is almost never worth the cost
  - For non-reasoning models, test whether Default already includes implicit CoT before adding explicit CoT

- **Failure signatures:**
  - Adding CoT to reasoning models → minimal accuracy gain, 2x latency
  - Forcing "answer directly" on non-reasoning models → suppresses helpful implicit reasoning
  - Using CoT on tasks requiring 100% accuracy → introduces harmful variability
  - Applying CoT to models that already perform default reasoning → redundant computation

- **First 3 experiments:**
  1. **Baseline check:** Run 25 trials of your target model on a sample task in Default mode (no prompt suffix). Inspect outputs for implicit reasoning traces. If present, CoT may be redundant.
  2. **Direct vs. CoT comparison:** For non-reasoning models only, run paired experiments (Direct vs. CoT) with 25 trials each. Measure RD for average rating and 100% correct. Calculate latency difference.
  3. **Threshold sensitivity analysis:** Determine your application's accuracy requirement (51%, 90%, or 100%). If you need 100% correct, test whether CoT harms your specific task distribution before deploying.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do highly customized, domain-specific Chain-of-Thought (CoT) prompts yield significant performance gains compared to the generic "think step by step" approach tested?
- Basis in paper: [explicit] The authors note in the Discussion that "Highly customized CoT prompts aimed at a specific problem may yield larger benefits, but generic approaches to CoT have more limited results."
- Why unresolved: The study only utilized a simple, generic CoT prompt across all questions to establish a baseline, rather than optimizing prompts for the specific PhD-level subject matter.
- What evidence would resolve it: A comparative study using the same benchmarks where one group uses generic CoT and the other uses prompts optimized by domain experts for each specific question type.

### Open Question 2
- Question: To what extent do these findings generalize to tasks outside of multiple-choice scientific reasoning, such as creative writing, coding, or logical extraction?
- Basis in paper: [explicit] The authors list "only one benchmark" as a limitation and state their goal is not to assess general performance but the specific impact on GPQA Diamond.
- Why unresolved: GPQA Diamond represents a narrow domain of "Google-proof" academic reasoning; it is unclear if the decreased value of CoT holds for tasks requiring long-form generation or different logical structures.
- What evidence would resolve it: Replicating the experimental conditions (Direct vs. CoT vs. Default) across diverse benchmark datasets like MMLU (broader topics), HumanEval (coding), or creative writing tasks.

### Open Question 3
- Question: What is the mechanistic cause of the performance decline in "100% Correct" accuracy observed in some non-reasoning models when using CoT?
- Basis in paper: [inferred] The paper notes that CoT can introduce variability and "trigger occasional errors in questions the model would otherwise get right," but does not isolate why the reasoning trace introduces these specific failures.
- Why unresolved: The study quantifies the error rate but does not analyze the semantic content of the reasoning traces to determine if the model is "distracting" itself or hallucinating during the steps.
- What evidence would resolve it: A qualitative error analysis comparing the reasoning traces of incorrect CoT answers against the internal states of correct direct answers to identify the point of failure.

## Limitations
- The study uses a single benchmark (GPQA Diamond) representing PhD-level scientific reasoning, limiting generalizability to other task domains
- The analysis assumes default reasoning behavior is functionally equivalent to explicit CoT without mechanistic validation
- The study doesn't examine highly customized or domain-specific CoT prompts that might yield larger benefits than generic approaches

## Confidence

- **High confidence**: CoT prompting provides negligible benefits for dedicated reasoning models (o3-mini, o4-mini, Flash 2.5) while substantially increasing latency (20-80% slower). The effect sizes (RD = 0.029-0.031) are small and statistically significant but not practically meaningful given the cost increase.
- **Medium confidence**: CoT improves average accuracy for non-reasoning models (RD = 0.044-0.135) but introduces variability that can harm perfect-accuracy requirements. This conclusion is well-supported by the data but assumes that the observed default reasoning behavior is functionally equivalent to explicit CoT.
- **Medium confidence**: Many modern non-reasoning models perform implicit CoT by default, reducing the marginal value of explicit prompts. While the evidence shows reasoning traces appear without prompting, the functional equivalence to explicit CoT remains an assumption rather than a demonstrated mechanism.

## Next Checks

1. **Mechanism validation test**: For models showing implicit CoT behavior, conduct ablation experiments removing specific reasoning tokens from outputs to test whether the final answer quality degrades. This would validate whether default reasoning is functionally essential versus merely stylistic.

2. **Generalization test**: Apply the same experimental design to a non-scientific benchmark (e.g., mathematical problem-solving or code generation) to verify whether CoT effectiveness patterns hold across different task domains and whether default reasoning behaviors generalize.

3. **Temperature sensitivity test**: Repeat key experiments at temperature=0.7 (typical for creative applications) with single trials to measure how variability in reasoning output affects accuracy gains versus losses, as real-world applications rarely use deterministic sampling.