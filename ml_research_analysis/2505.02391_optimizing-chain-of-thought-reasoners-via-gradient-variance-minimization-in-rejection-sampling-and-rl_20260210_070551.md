---
ver: rpa2
title: Optimizing Chain-of-Thought Reasoners via Gradient Variance Minimization in
  Rejection Sampling and RL
arxiv_id: '2505.02391'
source_url: https://arxiv.org/abs/2505.02391
tags:
- sample
- sampling
- arxiv
- which
- units
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of training large language models
  for chain-of-thought (CoT) reasoning by addressing the inefficiency in gradient
  estimation caused by uniform sampling strategies in existing methods like RAFT.
  The authors propose GVM-RAFT, a dynamic sample allocation strategy that minimizes
  stochastic gradient variance by adaptively distributing computational resources
  based on prompt-specific acceptance rates and gradient norms.
---

# Optimizing Chain-of-Thought Reasoners via Gradient Variance Minimization in Rejection Sampling and RL

## Quick Facts
- arXiv ID: 2505.02391
- Source URL: https://arxiv.org/abs/2505.02391
- Reference count: 40
- Primary result: Achieves 2-4× speedup in convergence with improved accuracy over vanilla RAFT through gradient variance minimization

## Executive Summary
This paper addresses the inefficiency in training large language models for chain-of-thought (CoT) reasoning by tackling the problem of gradient estimation variance in rejection sampling methods. The authors identify that uniform sampling strategies in existing approaches like RAFT lead to suboptimal resource allocation, particularly when dealing with prompts of varying difficulty. They propose GVM-RAFT, a dynamic sample allocation strategy that minimizes stochastic gradient variance by adaptively distributing computational resources based on prompt-specific acceptance rates and gradient norms.

The method is theoretically grounded and achieves significant practical improvements, demonstrating 2-4× speedup in convergence while maintaining or improving accuracy on mathematical reasoning tasks. The approach also generalizes to reinforcement learning algorithms like GRPO, making it broadly applicable to various CoT reasoning training paradigms. Experiments with Qwen models validate the effectiveness and efficiency of the approach across different reasoning scenarios.

## Method Summary
The authors propose GVM-RAFT, a dynamic sample allocation strategy that minimizes stochastic gradient variance in chain-of-thought reasoning training. The method adaptively distributes computational resources by estimating prompt-specific acceptance rates and gradient norms, then allocating more samples to difficult prompts where gradient variance is high. This approach is theoretically grounded in optimization principles that connect gradient variance to convergence speed. The method is designed to work with rejection sampling frameworks like RAFT and extends naturally to reinforcement learning algorithms such as GRPO. The key innovation is the dynamic allocation mechanism that replaces uniform sampling, leading to more efficient use of computational resources during training.

## Key Results
- Achieves 2-4× speedup in convergence compared to vanilla RAFT
- Improves accuracy on mathematical reasoning tasks while using fewer computational resources
- Successfully generalizes the approach to RL algorithms like GRPO
- Demonstrates effectiveness across different prompt difficulties through adaptive sample allocation

## Why This Works (Mechanism)
The method works by recognizing that not all prompts contribute equally to gradient estimation during training. Traditional uniform sampling wastes resources on easy prompts while undersampling difficult ones that contribute more to learning. By minimizing gradient variance through dynamic sample allocation, the method ensures that computational resources are focused where they provide the most learning signal. This is particularly important in chain-of-thought reasoning where prompt difficulty can vary significantly.

## Foundational Learning

**Gradient Variance in Stochastic Optimization**: Understanding how gradient variance affects convergence speed in stochastic optimization algorithms is crucial. High variance gradients lead to slower convergence and require more samples to achieve stable updates. Why needed: Forms the theoretical foundation for why minimizing gradient variance improves training efficiency. Quick check: Verify that reducing gradient variance consistently improves convergence rates across different optimization scenarios.

**Rejection Sampling in LLM Training**: Rejection sampling frameworks like RAFT use acceptance criteria to filter generated responses during training. Understanding how acceptance rates vary across prompts and how this affects gradient estimation is essential. Why needed: The method builds on existing rejection sampling approaches but improves their efficiency. Quick check: Analyze acceptance rate distributions across different prompt types to identify variance patterns.

**Chain-of-Thought Reasoning Patterns**: Understanding the structure and difficulty variations in CoT reasoning tasks helps explain why adaptive sampling is beneficial. Why needed: Different reasoning steps have different computational requirements and difficulty levels. Quick check: Categorize prompts by difficulty and verify that adaptive allocation targets the appropriate difficulty levels.

## Architecture Onboarding

**Component Map**: Prompt Generator -> Acceptance Checker -> Gradient Estimator -> Variance Minimizer -> Sample Allocator -> Model Trainer

**Critical Path**: The critical path involves generating responses, checking acceptance, estimating gradients, minimizing variance, and allocating samples. The variance minimization step is the key innovation that determines how samples are distributed across prompts.

**Design Tradeoffs**: The method trades off computational overhead from variance estimation against improved convergence speed. While variance estimation adds some overhead, the overall training efficiency improves due to better resource allocation. The approach also requires maintaining acceptance rate statistics and gradient norm estimates.

**Failure Signatures**: Potential failure modes include incorrect variance estimation leading to poor sample allocation, computational overhead from the allocation mechanism overwhelming the benefits, and poor generalization when the assumptions about prompt difficulty distributions don't hold.

**3 First Experiments**:
1. Compare convergence speed on simple mathematical reasoning tasks with varying difficulty distributions
2. Test the method's sensitivity to hyperparameters controlling the allocation strategy
3. Evaluate the overhead of variance estimation against the benefits in convergence speed

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Theoretical framework relies on assumptions about prompt difficulty distributions that may not hold across diverse domains
- Performance gains primarily demonstrated on mathematical reasoning tasks with Qwen models, limiting generalizability
- Does not extensively explore trade-offs between computational efficiency and sample quality in low-resource settings

## Confidence
**High**: The theoretical foundation connecting gradient variance to convergence speed is well-established in optimization literature
**Medium**: Empirical results showing improved accuracy and convergence speed on Qwen models for mathematical reasoning tasks
**Medium**: The extension to GRPO and other RL algorithms appears sound but is less thoroughly validated

## Next Checks
1. Test GVM-RAFT on diverse reasoning tasks beyond mathematics (e.g., commonsense reasoning, symbolic reasoning) to assess domain generalization
2. Evaluate performance across different model families and sizes to determine architecture dependence
3. Conduct ablation studies isolating the contribution of variance minimization from other factors like improved prompt selection