---
ver: rpa2
title: On the Reasoning Capacity of AI Models and How to Quantify It
arxiv_id: '2501.13833'
source_url: https://arxiv.org/abs/2501.13833
tags:
- reasoning
- strategy
- positional
- accuracy
- these
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a phenomenological framework for analyzing
  AI model reasoning capabilities beyond traditional accuracy metrics. The authors
  develop two complementary models: a Probabilistic Mixture Model (PMM) that decomposes
  responses into reasoning, memorization, and guessing components, and an Information-Theoretic
  Consistency (ITC) analysis that quantifies the relationship between model confidence
  and strategy selection.'
---

# On the Reasoning Capacity of AI Models and How to Quantify It

## Quick Facts
- arXiv ID: 2501.13833
- Source URL: https://arxiv.org/abs/2501.13833
- Authors: Santosh Kumar Radha; Oktay Goktas
- Reference count: 0
- Primary result: Introduces framework decomposing AI model responses into reasoning, memorization, and guessing components using positional bias analysis

## Executive Summary
This paper presents a phenomenological framework for analyzing AI model reasoning capabilities beyond traditional accuracy metrics. The authors develop a Probabilistic Mixture Model (PMM) and Information-Theoretic Consistency (ITC) analysis to decompose responses into reasoning, memorization, and guessing components. Through systematic experiments on positional bias in multiple-choice reasoning tasks using GPT-4o-mini on GPQA, the study reveals that current models often rely on sophisticated combinations of memorization and pattern matching rather than true logical deduction. The framework enables applications to specify reliability thresholds based on strategy distributions rather than aggregate performance metrics.

## Method Summary
The method involves systematically shuffling answer positions in multiple-choice questions and analyzing accuracy differentials to decompose model responses into three strategy components. For each question, the model is queried with correct answers systematically placed at different positions (A, B, C, D), generating position-specific accuracy data. The PMM then extracts strategy probabilities using linear constraints: PM = ΔA/(1-PO), PR = (Aom - PO)/(1-PO) - PM, and PG = 1 - PM - PR. The ITC analysis computes Shannon entropy from output probabilities and compares it to a theoretical entropy-accuracy frontier. Strategy distributions are visualized in ternary probability space, and phase space dynamics are analyzed across different randomization parameters.

## Key Results
- Current models exhibit significant limitations in genuine logical deduction, with apparent success often relying on sophisticated combinations of memorization and pattern matching
- Distinct cognitive strategy distributions exist across different question types, with stable attractors identified in strategy space
- Systematic deviations from optimal probability distributions reveal when models rely on learned heuristics vs logical deduction
- Position D exhibits systematically lower selection probability (~0.1) as an incorrect choice compared to other positions (~0.2-0.25), indicating architectural bias

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Model responses can be decomposed into three strategy components—reasoning (position-invariant), memorization (position-dependent), and guessing—by measuring accuracy differentials across shuffled answer positions.
- Mechanism: By systematically varying where correct answers appear and computing ΔA(q) = A_memorized_position - A_other_positions, memorization probability is isolated via PM(q) = ΔA(q)/(1 - P_O). Reasoning and guessing follow from constraint equations.
- Core assumption: Strategies combine linearly with perfect reasoning (PR = 1) and uniform guessing (PG = 1/|O|) under ideal conditions.
- Evidence anchors: [abstract]: "Probabilistic Mixture Model (PMM) that decomposes model responses into reasoning, memorization, and guessing components"; [section]: Eq. 17 derives PM(q) = ΔA(q)/(1-PO), validated against empirical accuracy with strong fit for α > 0.4
- Break condition: Non-linear strategy interactions dominate; partial understanding creates probabilistic reasoning outcomes (model residuals spike at α < 0.4)

### Mechanism 2
- Claim: Positional encoding from causal attention creates systematic decision biases that leak into reasoning, revealing when models rely on learned heuristics vs logical deduction.
- Mechanism: Position D exhibits ~0.1 selection probability when incorrect vs ~0.2-0.25 for other positions. This asymmetry persists across randomization, indicating architectural—not content-driven—bias channels.
- Core assumption: Position-dependent effects reflect fundamental attention mechanisms rather than dataset artifacts.
- Evidence anchors: [section]: "Position D exhibits markedly lower selection probability (~0.1) as an incorrect choice compared to other positions (~0.2-0.25)"; [section]: Fig. 5 shows position D's monotonic Δµ_D(θ) decline to -0.04, indicating systematic disadvantage
- Break condition: Architectures with true bidirectional attention or position-agnostic mechanisms

### Mechanism 3
- Claim: Model calibration can be assessed against a theoretical entropy-accuracy frontier; systematic under-dispersion indicates overconfidence from heuristic reliance.
- Mechanism: Theoretical frontier H_ideal(A) = -A·log₂A - (1-A)·log₂[(1-A)/(k-1)] defines optimal probability distribution. Empirical points fall 0.2-0.5 bits below this, revealing unwarranted determinism.
- Core assumption: Ideal calibration distributes remaining probability uniformly among incorrect options.
- Evidence anchors: [section]: Eq. 30 defines frontier; Fig. 10 shows all questions below this curve; [section]: "Questions exhibit entropy values substantially below H_ideal(α), with typical deviations of 0.2-0.5 bits"
- Break condition: Models use explicit uncertainty quantification or temperature scaling

## Foundational Learning

- Concept: Shannon Entropy for Discrete Distributions
  - Why needed here: ITC analysis quantifies prediction confidence via H(q) = -Σρᵢlog₂ρᵢ; interpreting 0-2 bit range is essential.
  - Quick check question: Given output probabilities [0.7, 0.2, 0.05, 0.05], what's the entropy? (Answer: ~1.36 bits, between confident and uniform)

- Concept: Probabilistic Mixture Models with Linear Constraints
  - Why needed here: PMM assumes P_correct = PM·P_M(o) + PR·P_R + PG·P_G with PM + PR + PG = 1; solving requires linear algebra intuition.
  - Quick check question: If ΔA = 0.35 and P_O = 0.25, what's PM? (Answer: 0.35/0.75 ≈ 0.47)

- Concept: Phase Space Dynamics and Attractors
  - Why needed here: Strategy evolution forms conservative dynamical systems with stable attractors (PR ≈ 0.4-0.5, PM ≈ 0.3-0.4) and vortical structures.
  - Quick check question: What does a vortex in strategy space indicate vs laminar flow? (Answer: Vortex = strategy cycling/unstable; laminar = conserved ratios)

## Architecture Onboarding

- Component map: Data Generator -> PMM Solver -> ITC Analyzer -> Phase Space Engine
- Critical path:
  1. Generate N trials per question (paper uses 100) with systematic position permutations
  2. Compute position-specific accuracies α_o(q, θ) for each position o ∈ {A,B,C,D}
  3. Extract strategy probabilities via PMM equations (17, 19, 20)
  4. Calculate entropy H(q) from output probabilities, compare to H_ideal(A)
  5. Map trajectories in strategy space as θ varies

- Design tradeoffs:
  - Trial count: More trials → better statistical power but higher API cost
  - Inclusive vs exclusive randomization: Reveals position-specific bias characteristics differently (Fig. 4a vs 4b)
  - Linear vs higher-order decomposition: Paper notes residuals spike at α < 0.4, suggesting missing cross-terms

- Failure signatures:
  - Negative strategy probabilities → linear assumption violated
  - High model deviation δα > 0.15 in low-accuracy regime → higher-order effects dominant
  - Position convergence never reaches P_O = 0.25 → incomplete randomization or dataset artifacts

- First 3 experiments:
  1. Baseline bias characterization: θ ∈ {0, 1} on 50 questions, compute PM distribution and identify high-memorization questions
  2. Strategy trajectory mapping: Track PM/PR/PG evolution across θ ∈ {0, 0.25, 0.5, 0.75, 1} to identify attractors and phase transitions
  3. Calibration audit: Plot all questions in entropy-accuracy space, compute mean deviation from frontier, correlate with strategy mix

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the stable attractors observed in strategy space represent fundamental computational constraints of transformer architectures, or are they artifacts of current training methodologies?
- Basis in paper: [explicit] Authors pose this question directly: "The existence of these stable attractors raises an intriguing question: Do these represent fundamental computational constraints of transformer architectures, or are they artifacts of current training methodologies?"
- Why unresolved: The phenomenological framework characterizes observable behavior but cannot distinguish between architectural constraints and training-derived patterns.
- What evidence would resolve it: Comparative studies across architectures (e.g., transformers vs. RNNs) and training paradigms, examining whether attractor locations shift or dissolve under different training conditions.

### Open Question 2
- Question: How do reasoning-enhancing techniques (Chain-of-Thought, Iteration-of-Thought) alter the decomposition of cognitive strategies?
- Basis in paper: [explicit] "Exploring the impact of such reasoning-enhancing techniques remains an avenue for future investigation."
- Why unresolved: The study employed direct answer elicitation without CoT/IoT prompts; whether these techniques genuinely increase PR or merely shift distributions remains unknown.
- What evidence would resolve it: Applying the PMM framework to model responses under CoT/IoT conditions and measuring changes in PR, PM, and PG distributions.

### Open Question 3
- Question: Does the phenomenological framework generalize across diverse model architectures, scales, and benchmark domains beyond GPT-4o-mini and GPQA?
- Basis in paper: [inferred] The methodology focuses on a single model-benchmark pair; while the framework is presented as general, empirical validation across architectures, model sizes, and task types is absent.
- Why unresolved: Position-dependent behaviors may manifest differently across architectures; the three-strategy decomposition may not capture domain-specific cognitive modes.
- What evidence would resolve it: Replication studies across open/closed models (Claude, LLaMA, Gemini), parameter scales, and diverse benchmarks (GSM8K, Big-Bench, domain-specific evaluations).

## Limitations

- The PMM decomposition assumes linear strategy combination, but model behavior exhibits significant nonlinearities at low accuracy levels (α < 0.4) where residuals spike and negative probabilities emerge.
- The framework's reliance on perfect reasoning (PR = 1) and uniform guessing (PG = 1/4) as ideal conditions creates systematic bias when these assumptions break.
- The study's single-model evaluation (GPT-4o-mini) limits generalizability across architectures, particularly for models with bidirectional attention or explicit uncertainty quantification mechanisms.

## Confidence

- **High Confidence**: Positional bias identification and its persistence across randomization (Mechanism 2). The empirical evidence from Fig. 5 and related work citations provides robust support for systematic position-dependent effects.
- **Medium Confidence**: PMM decomposition validity for α > 0.4. While the mathematical framework is sound and fits empirical data well above this threshold, the model deviation increases substantially below α = 0.4, indicating missing higher-order effects.
- **Low Confidence**: Calibration frontier framework and entropy comparisons. The theoretical foundation for ideal entropy-accuracy relationships lacks corpus support, and the assumption of uniform probability distribution among incorrect options may not hold for sophisticated models.

## Next Checks

1. **Cross-Architecture Validation**: Test the PMM decomposition and ITC analysis across diverse LLM architectures (bidirectional transformers, state-space models, models with explicit uncertainty quantification) to identify break conditions and refine the framework.

2. **Higher-Order Decomposition**: Extend the PMM to include second-order terms capturing strategy interactions, particularly for the low-accuracy regime where linear assumptions fail. Validate whether this improves residual reduction below α = 0.4.

3. **Real-World Application Testing**: Apply the reliability threshold framework to practical AI applications (medical diagnosis, legal reasoning) to evaluate whether strategy-based thresholds outperform traditional accuracy metrics in deployment scenarios.