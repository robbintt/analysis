---
ver: rpa2
title: How Good Are LLMs at Processing Tool Outputs?
arxiv_id: '2510.15955'
source_url: https://arxiv.org/abs/2510.15955
tags:
- answer
- response
- code
- json
- none
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the challenge of extracting information from
  complex JSON tool responses using large language models (LLMs). The authors formulate
  this as a question-answering task, create a dataset of real-world API responses
  and questions, and evaluate 15 open and closed weight models using multiple prompting
  approaches.
---

# How Good Are LLMs at Processing Tool Outputs?

## Quick Facts
- arXiv ID: 2510.15955
- Source URL: https://arxiv.org/abs/2510.15955
- Reference count: 40
- This paper studies extracting information from complex JSON tool responses using LLMs, finding that code generation outperforms answer generation for filtering/aggregation tasks while the optimal strategy depends on response length, question type, and prompting approach.

## Executive Summary
This study systematically evaluates how well LLMs can process structured JSON tool responses, formulating it as a question-answering task. The authors create a benchmark from real API responses and test 15 open and closed-weight models across different prompting strategies. Results show JSON processing remains challenging even for frontier models, with performance varying significantly based on response characteristics and the processing approach used.

## Method Summary
The authors created a benchmark of 1,298 QA pairs from 6 real-world REST API endpoints, evaluating 15 LLMs using zero-shot prompting across four control points: output type (answer vs. code), reasoning (CoT vs. none), schema inclusion, and response inclusion (full, reduced, or none). Evaluation used exact match, contains, and LLM-as-a-Judge metrics. The study tests whether code generation, schema inclusion, and CoT reasoning improve JSON processing performance.

## Key Results
- Code generation outperforms answer generation for filtering and aggregation tasks, while answer generation works better for extractive tasks
- Including JSON schema in prompts improves model performance, especially for code generation
- Chain-of-thought prompting improves performance on filtering and aggregation but not extractive tasks
- Performance varies significantly (3% to 50% differences) based on prompting strategy and response characteristics

## Why This Works (Mechanism)

### Mechanism 1
Code generation outperforms direct answer generation for tasks requiring reasoning over data (filtering, aggregation) because LLMs trained extensively on code can map JSON structures to programmatic operations more reliably than performing multi-step reasoning internally. Code provides an explicit, verifiable computational substrate whereas direct reasoning lacks intermediate checks. This works when the task is complex enough to benefit from explicit computation, but may reduce accuracy for simple extraction tasks due to unnecessary complexity overhead.

### Mechanism 2
Including JSON schema in prompts improves model performance by providing explicit structural metadata (types, required fields, enum values) that reduces ambiguity in how the model should traverse and interpret nested data structures. This is especially valuable for code generation where type assumptions affect correctness. The mechanism assumes models can effectively incorporate schema information to guide their parsing logic, but may mislead models if the schema is incomplete or contains errors.

### Mechanism 3
Chain-of-thought prompting improves performance on filtering and aggregation tasks by providing intermediate reasoning steps that help models track state across iterations and aggregations. Extractive tasks (single lookup) require no such decomposition, so CoT adds noise without benefit. The mechanism relies on models trained on reasoning traces being able to leverage explicit step-by-step instructions for multi-step operations, but may fail if models produce verbose or tangential reasoning that makes output parsing harder.

## Foundational Learning

- **JSON structure traversal and nested data access**: Needed to understand hierarchical JSON with varying nesting depths found in real API responses. Quick check: Given a JSON object with structure `{data: {result: [{id: 1, price: "$50"}, {id: 2, price: "$75"}]}}`, can you write pseudocode to find the average price after removing currency symbols?

- **Task type classification (extractive vs. filtering vs. aggregation)**: Critical because the paper shows optimal strategy depends on task type—routing queries to the right approach yields up to 50% performance improvement. Quick check: Classify "List all hotels with cleanliness rating > 8" and "What is the average price of available rooms?" into the three categories.

- **LLM context window limitations and position bias**: Essential for understanding why performance degrades with response length (7-91% drop across models) and exhibits recency bias (5-75% variance based on answer position). Quick check: If a JSON response is 80K tokens and the answer is at position 1 (beginning) vs. position 8 (end), what bias might you expect based on the paper's findings?

## Architecture Onboarding

- **Component map**: User Query → Query Classifier (extractive/filtering/aggregation) → Processing Router (code gen vs answer gen) → Prompt Builder (+schema? +CoT? +response?) → LLM → Output Parser → Result

- **Critical path**: Query classification → Processing approach selection → Schema inclusion. Misclassification at step 1 cascades to suboptimal strategy selection.

- **Design tradeoffs**:
  - Code gen: More accurate for complex tasks, but requires sandboxed execution environment; lower verbosity helps multi-turn context management
  - Answer gen: Simpler pipeline, better for extraction, but higher verbosity and formatting inconsistency risk
  - Full response inclusion: Best accuracy but may exceed context limits; reduced response is compromise; no response is worst
  - Dual pipeline: Higher accuracy but 2x engineering complexity

- **Failure signatures**:
  - JSON structure misinterpretation: Model picks correct key but wrong record (confusing nested arrivalAirport vs. legs[0].arrivalAirport)
  - Semantic ambiguity: Multiple similar keys (name, room_name, name_without_policy) cause wrong selection
  - Format misalignment: Model ignores formatting instructions, adds units or verbose text
  - Code execution failure: Wrong data type assumptions, incorrect JSON traversal paths

- **First 3 experiments**:
  1. **Baseline calibration**: Test your target model on the paper's dataset subset (start with 100 samples across all 3 task types) using both code and answer generation to establish your own accuracy baselines. Expected: code gen should win on filtering/aggregation, answer gen on extractive.
  2. **Schema impact A/B test**: Run same queries with/without schema inclusion. Measure delta in exact match accuracy. Expected: +3-12% improvement per paper findings.
  3. **Context length stress test**: Create artificially long JSON responses (20K, 50K, 80K tokens) and measure accuracy degradation curve for your model. Identify context threshold where performance becomes unacceptable. Expected: Linear degradation with recency bias for some models.

## Open Questions the Paper Calls Out

### Open Question 1
How do tool response processing approaches perform in end-to-end agentic workflows compared to the isolated QA setup? The authors acknowledge their experiments focus only on isolated tasks, but real agent systems involve tool selection, planning, and error recovery that may interact with processing strategies. A benchmark integrating processing approaches into complete multi-tool agentic pipelines would resolve this.

### Open Question 2
Can RAG-based or agentic approaches to JSON processing outperform direct answer or code generation? The limitations section notes other approaches such as RAG on the API response or an agentic approach to code generation were not systematically compared. A controlled study comparing these approaches across the same dataset and metrics would resolve this.

### Open Question 3
Can JSON responses be automatically simplified to improve processing accuracy without requiring ground truth knowledge? The oracle projection experiment showed 8-38 point improvements by simplifying JSON, but the authors note the projection algorithm assumes ground truth is known which is not realistic. Developing heuristics or learned models that predict relevant JSON paths from the query alone would resolve this.

## Limitations
- The dataset, while derived from real APIs, remains relatively small at 1,298 examples, limiting generalizability
- The zero-shot evaluation framework doesn't explore fine-tuning possibilities that might improve performance
- The study focuses on synchronous processing without addressing streaming or incremental processing of very large responses

## Confidence
- **High confidence**: Code generation superiority for filtering/aggregation tasks, schema inclusion benefits, and task-type-dependent strategy recommendations (consistent across all 15 models)
- **Medium confidence**: Chain-of-thought benefits for multi-step reasoning and recency bias findings (more variability across models and task types)
- **Low confidence**: Specific performance thresholds and exact accuracy numbers (depend heavily on dataset composition and evaluation pipeline implementation)

## Next Checks
1. **Cross-dataset validation**: Test the identified optimal strategies (code generation with schema inclusion) on a separate, independently collected JSON-QA dataset to verify robustness of task-type routing recommendations across different domains and JSON structures.

2. **Larger context window evaluation**: Systematically evaluate performance degradation curves for models with different context window sizes (7K, 32K, 128K) on progressively longer JSON responses to better understand the interaction between context limits and recency bias across model families.

3. **Execution environment impact study**: Compare accuracy between models that generate correct code versus those whose code executes successfully, isolating whether performance gaps stem from generation errors or execution environment limitations.