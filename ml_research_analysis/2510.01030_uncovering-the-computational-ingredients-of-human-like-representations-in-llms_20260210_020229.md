---
ver: rpa2
title: Uncovering the Computational Ingredients of Human-Like Representations in LLMs
arxiv_id: '2510.01030'
source_url: https://arxiv.org/abs/2510.01030
tags:
- alignment
- other
- food
- human
- animal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates how different computational ingredients of
  large language models (LLMs) affect their alignment with human conceptual representations.
  Using 77 diverse models and 128 object concepts from the THINGS dataset, the authors
  measure human-model alignment through triadic similarity judgments and ordinal embeddings.
---

# Uncovering the Computational Ingredients of Human-Like Representations in LLMs

## Quick Facts
- arXiv ID: 2510.01030
- Source URL: https://arxiv.org/abs/2510.01030
- Reference count: 40
- Key outcome: Instruction fine-tuning and larger architectural dimensionality most predictive of human conceptual alignment

## Executive Summary
This paper systematically evaluates how different computational ingredients of large language models affect their alignment with human conceptual representations. Using 77 diverse models and 128 object concepts from the THINGS dataset, the authors measure human-model alignment through triadic similarity judgments and ordinal embeddings. The study finds that instruction fine-tuning and larger MLP/embedding dimensions are most predictive of alignment, while model size, training data, and multimodal pretraining show limited or even negative effects. These results suggest focusing on post-training and architectural dimensionality to build LLMs with human-like conceptual representations.

## Method Summary
The authors measure human-model alignment by collecting 35,000 triadic similarity judgments per model using the THINGS dataset's 128 object concepts. For each model, they generate random triplets and query the model to identify which of two items is most similar to a third. Responses are converted into ordinal embeddings via crowd-kernel loss minimization, then aligned to human SPoSE embeddings through Procrustes transformation. Alignment is quantified using Procrustes R², with mixed-effects regression controlling for model family. Computational ingredients (instruction tuning, dimensionality, training data, multimodal pretraining) are systematically analyzed for their predictive power on alignment.

## Key Results
- Instruction fine-tuning shows the strongest positive effect on human alignment across model families
- Larger MLP and embedding dimensions consistently improve alignment
- Model size, training data, and multimodal pretraining have limited or negative effects on alignment
- No existing benchmark fully captures alignment variance, though MMLU and BigBenchHard correlate best

## Why This Works (Mechanism)
The paper demonstrates that post-training modifications (instruction tuning) and architectural choices (larger internal dimensions) have the strongest impact on aligning model representations with human conceptual structures. The mechanism appears to involve instruction tuning refining semantic representations to better match human patterns, while larger dimensions provide more expressive capacity for capturing nuanced human-like similarities. The lack of correlation with traditional benchmarks suggests that human conceptual alignment requires different optimization targets than task performance.

## Foundational Learning
- **Triangular similarity judgments**: Humans naturally think about similarity in comparative contexts; these judgments capture relative conceptual distances more effectively than absolute similarity ratings. Quick check: Verify judgment patterns match known human similarity hierarchies (e.g., living vs non-living things).
- **Ordinal embeddings**: These preserve only relative distances rather than absolute positions, making them robust to scaling differences between human and model representations. Quick check: Confirm embedding reconstruction error is low when compared to ground truth triplet data.
- **Procrustes alignment**: This statistical technique removes arbitrary rotation, scaling, and translation to measure true representational similarity. Quick check: Validate Procrustes R² values fall within expected ranges for aligned representations.
- **Mixed-effects regression**: This controls for model family-level effects when analyzing computational ingredients, preventing confounding between architecture choices. Quick check: Confirm random intercepts significantly improve model fit.
- **Crowd-kernel loss**: This specialized loss function optimizes embeddings specifically for ordinal data from triplet comparisons. Quick check: Monitor validation loss convergence during embedding fitting.

## Architecture Onboarding

### Component Map
Model -> Triplet Generator -> Inference Engine -> Response Parser -> Ordinal Embedding Fitter -> Procrustes Aligner -> Alignment Score

### Critical Path
The bottleneck is generating and processing 35,000 triplets per model, requiring efficient batching and parallelization. Inference speed varies dramatically between base and instruction-tuned models, with base models often requiring additional prompt engineering.

### Design Tradeoffs
Using 128 concepts provides good coverage while keeping triplet count manageable (C(128,3) = 341,376 possible triplets). The choice of 30D embeddings balances representational capacity against overfitting risk. Procrustes alignment assumes linear transformations suffice, potentially missing nonlinear representational differences.

### Failure Signatures
Models producing non-compliant responses (not just item names) create parsing errors. Base models often fail to understand instruction-style prompts without additional guidance. Convergence failures in crowd-kernel optimization suggest triplet count or learning rate adjustments needed.

### First 3 Experiments
1. Run small-scale validation with 3-5 models (mix of base/instruct, small/large) on 1,000 triplets to verify pipeline functionality
2. Compare greedy vs sampled decoding impact on alignment scores using a subset of models
3. Test ordinal embedding robustness by varying triplet count (10K vs 35K) and monitoring alignment stability

## Open Questions the Paper Calls Out
- What specific properties of training datasets (beyond token counts) enable semantic alignment with humans? The authors note they "could not assess the role of training dataset composition" and call for future work to "look beyond token counts to clarify which dataset properties enable semantic alignment."
- Why does multimodal pretraining show no benefit (or even a negative effect) on conceptual alignment with humans, contrary to expectations? The mixed-effects model showed multimodal pretraining predicted "reliably lower alignment" after accounting for other factors.
- How do models deploy semantic knowledge in open-ended reasoning tasks, and does this deployment correlate with triadic similarity judgments? The authors note "there may be other ways of probing semantic knowledge such as by observing how models deploy this knowledge in open-ended reasoning tasks."

## Limitations
- The analysis relies on correlational findings rather than causal evidence, making it unclear whether observed relationships reflect direct effects or confounding factors
- Focus on object concepts from THINGS represents a narrow slice of conceptual space, potentially limiting generalizability to abstract concepts or other semantic domains
- The human alignment metric, while derived from extensive human judgments, may not fully capture all aspects of "human-like" representations

## Confidence
**High Confidence**: Instruction fine-tuning improves human alignment (tested across multiple model families with consistent results, controlled for model size)
**Medium Confidence**: Benchmark performance does not predict human alignment (moderately supported but limited by specific benchmarks chosen)
**Low Confidence**: No existing benchmark fully captures alignment variance (tentative given limited number of benchmarks tested)

## Next Checks
1. Conduct ablation studies varying inference temperature and sampling strategies to determine sensitivity of alignment scores to these hyperparameters across different model families
2. Test alignment on a broader set of concepts including abstract and action-related concepts to evaluate generalizability beyond concrete objects
3. Design controlled experiments varying only single architectural parameters (e.g., MLP dimensions) while holding other factors constant to establish causal relationships between computational ingredients and human alignment