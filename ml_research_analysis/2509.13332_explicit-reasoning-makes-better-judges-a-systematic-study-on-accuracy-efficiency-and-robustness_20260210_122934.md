---
ver: rpa2
title: 'Explicit Reasoning Makes Better Judges: A Systematic Study on Accuracy, Efficiency,
  and Robustness'
arxiv_id: '2509.13332'
source_url: https://arxiv.org/abs/2509.13332
tags:
- assistant
- thinking
- rubric
- user
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper systematically compares "thinking" and "non-thinking"
  LLMs in the LLM-as-a-judge paradigm, evaluating Qwen 3 models (0.6B, 1.7B, 4B parameters)
  on accuracy, efficiency, and robustness. Results show thinking models achieve approximately
  10 percentage points higher accuracy than non-thinking counterparts with minimal
  computational overhead (under 2x), whereas augmentation strategies like 7-shot in-context
  learning incur over 8x FLOPs for smaller gains.
---

# Explicit Reasoning Makes Better Judges: A Systematic Study on Accuracy, Efficiency, and Robustness

## Quick Facts
- arXiv ID: 2509.13332
- Source URL: https://arxiv.org/abs/2509.13332
- Reference count: 40
- Qwen 3 models with explicit reasoning achieve ~10 percentage points higher accuracy than non-thinking counterparts with minimal computational overhead

## Executive Summary
This systematic study compares "thinking" (explicit reasoning traces before verdict) versus "non-thinking" LLM-as-a-judge paradigms across Qwen 3 models (0.6B, 1.7B, 4B parameters). Results demonstrate that thinking models achieve approximately 10 percentage points higher accuracy on pairwise comparison tasks with less than 2x FLOPs overhead, while augmentation strategies like 7-shot in-context learning require over 8x FLOPs for smaller gains. The benefits extend to robustness, with thinking models maintaining 6% higher consistency under various bias conditions including positional, bandwagon, identity, and diversity effects. A critical finding is the existence of a model capacity threshold below which thinking mode fails to improve accuracy, with the 0.6B model performing worse than random on difficult tasks.

## Method Summary
The study evaluates Qwen 3 models on pairwise comparison tasks using RewardBench (Chat, Chat Hard, Safety, Reasoning subsets) and M-RewardBench (20% sampled). Models are compared in thinking (explicit reasoning traces enabled) and non-thinking configurations across four augmentation strategies: in-context learning (3/5/7-shot), rubric-guided, reference-based, and n-best aggregation. FLOPs are estimated theoretically using a formula accounting for prefill and decode operations. Bias robustness is tested by injecting synthetic perturbations (positional, bandwagon, identity, diversity, verbosity, random distraction) into 1000 evaluation samples and measuring consistency across position-swapped orderings. Multilingual evaluation uses M-RewardBench covering 10 languages.

## Key Results
- Thinking models achieve ~10 percentage point accuracy improvement over non-thinking with <2x FLOPs overhead
- 7-shot ICL requires 4.5x more computational resources than thinking mode while delivering less than half the accuracy improvement
- Thinking models maintain 6% higher consistency under bias conditions across all tested categories
- Model capacity below a threshold (0.6B) fails on difficult tasks, performing worse than random
- Multilingual evaluation shows 8.88-point average score improvement with thinking mode

## Why This Works (Mechanism)

### Mechanism 1: Deliberative Decomposition Reduces Heuristic Reliance
Thinking models achieve higher accuracy by generating intermediate reasoning traces that force systematic comparison before verdict selection. The reasoning step decomposes evaluation into explicit criteria (helpfulness, accuracy, relevance), reducing reliance on surface-level heuristics that non-thinking models implicitly use. This is most pronounced in "Chat Hard" tasks requiring nuanced disambiguation.

### Mechanism 2: Output-Side Computation Scaling Is More Efficient Than Input-Side Context Expansion
Thinking mode achieves superior accuracy-cost trade-offs because generating additional output tokens (reasoning) costs less than processing expanded input contexts (few-shot examples). Few-shot ICL requires prefilling N additional example tokens, scaling prefill FLOPs linearly with context length. Thinking mode primarily increases decode FLOPs (generating reasoning), which is smaller for typical reasoning trace lengths.

### Mechanism 3: Explicit Justification Creates Bias Resistance
Thinking models maintain higher consistency under bias conditions because the reasoning step requires explicit justification that exposes and counters superficial influences. When forced to articulate reasoning, the model must engage with actual response quality rather than shortcuts like position, length, or identity markers. The reasoning step acts as a "deliberation buffer" that filters bias-triggering cues.

## Foundational Learning

- **Pairwise Comparison Evaluation**
  - Why needed here: The paper uses pairwise judging (selecting between two responses) as the core evaluation paradigm. Understanding this framing is essential to interpret accuracy metrics.
  - Quick check question: Can you explain why pairwise comparison differs from pointwise scoring, and why it's more sensitive to positional bias?

- **Transformer FLOPs Decomposition (Prefill vs. Decode)**
  - Why needed here: The efficiency argument hinges on understanding why expanding input context (ICL) costs more than generating additional output tokens (thinking).
  - Quick check question: Given a model with hidden size d and N layers, why does prefill FLOPs scale with L² while decode FLOPs scales with L×T?

- **Chain-of-Thought as Test-Time Compute**
  - Why needed here: The "thinking mode" is an implementation of Chain-of-Thought reasoning, and understanding this connection clarifies why it improves complex reasoning tasks.
  - Quick check question: How does test-time compute via CoT differ from training-time compute scaling in terms of flexibility and cost?

## Architecture Onboarding

- **Component map:**
  Judge Input Layer -> Augmentation Layer -> Thinking/Non-Thinking Router -> Reasoning Generator (if thinking) -> Verdict Extractor -> Consistency Checker

- **Critical path:**
  1. Baseline evaluation: Run non-thinking mode on target benchmark (e.g., RewardBench)
  2. Thinking evaluation: Enable thinking mode with same prompts; extract reasoning + verdict
  3. FLOPs comparison: Calculate theoretical FLOPs using provided formula (Appendix A)
  4. Bias robustness test: Apply bias-inducing modifications to inputs; measure consistency

- **Design tradeoffs:**
  - Safety tasks: Rubric-based prompting outperforms thinking (policy-driven criteria benefit from structured checklists over open reasoning)
  - Model scale: Models <1B parameters may fail to generate useful reasoning traces
  - Hybrid strategy: Use non-thinking for simple "Chat" tasks; escalate to thinking for "Chat Hard" and "Reasoning" categories

- **Failure signatures:**
  - Format errors: Model fails to output [[A]] or [[B]] verdict (Table 6 shows higher format error rates for thinking mode in some configurations)
  - Worse-than-random accuracy on difficult tasks indicates capacity insufficiency
  - Inconsistent verdicts under position swapping indicate persistent positional bias

- **First 3 experiments:**
  1. Replicate the baseline vs. thinking comparison on a 1.7B+ model using RewardBench Chat and Chat Hard subsets; verify ~10-point accuracy gap.
  2. Measure FLOPs directly (not just theoretical) for 3-shot ICL vs. thinking mode on identical hardware; confirm <2x vs. >4x difference.
  3. Inject verbosity bias (using the paper's verbosity prompt from Figure 30) and compare consistency scores; expect ~10-point improvement with thinking enabled.

## Open Questions the Paper Calls Out

- **Cross-Architecture Generalization**: Do the efficiency and robustness advantages of explicit reasoning in Qwen 3 models transfer to other model architectures or closed-source models?
- **Hybrid Approach Potential**: Can a hybrid approach combining structured rubrics with chain-of-thought reasoning enhance evaluation fidelity beyond what thinking or rubrics achieve alone?
- **Capability Threshold Dependency**: Does the "capability threshold" required for reliable judging depend more on parameter count or specific reasoning capabilities?

## Limitations

- Model capacity threshold is identified but not precisely characterized
- Augmentation strategy coverage is incomplete, leaving potential superior combinations untested
- Bias injection relies on synthetic perturbations that may not reflect real-world scenarios
- FLOPs estimation depends on theoretical calculations rather than measured computational costs

## Confidence

- **High Confidence**: Thinking mode achieves ~10 percentage point accuracy improvement; minimal computational overhead; improved robustness under synthetic bias
- **Medium Confidence**: Output-side computation scaling efficiency; capacity threshold hypothesis; multilingual evaluation benefits
- **Low Confidence**: Mechanism of bias resistance creation; generalizability to non-Qwen models; long-term stability of benefits

## Next Checks

1. **Cross-Model Validation**: Replicate the thinking vs. non-thinking comparison using alternative LLM architectures (e.g., Llama, Mistral) across multiple scales to verify the accuracy and efficiency findings aren't model-specific artifacts.

2. **Real-World Bias Testing**: Apply the thinking vs. non-thinking paradigm to natural datasets where bias emerges organically (e.g., social media content moderation, legal document review) rather than synthetic injection to validate robustness claims in practical settings.

3. **Scaling Law Analysis**: Systematically vary model parameters (0.6B → 1.7B → 4B → 7B+) and measure the breakpoint where thinking mode transitions from harmful to beneficial, characterizing the relationship between model capacity and reasoning effectiveness.