---
ver: rpa2
title: Uncertainty Quantification of Surrogate Models using Conformal Prediction
arxiv_id: '2408.09881'
source_url: https://arxiv.org/abs/2408.09881
tags:
- prediction
- coverage
- calibration
- data
- uncertainty
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper applies conformal prediction to provide statistically
  guaranteed uncertainty quantification for surrogate models across diverse scientific
  domains. The method provides marginal coverage regardless of model architecture,
  training regime, or output dimensionality up to 20 million dimensions, requiring
  only seconds to minutes of calibration on standard hardware.
---

# Uncertainty Quantification of Surrogate Models using Conformal Prediction

## Quick Facts
- arXiv ID: 2408.09881
- Source URL: https://arxiv.org/abs/2408.09881
- Authors: Vignesh Gopakumar; Ander Gray; Joel Oskarsson; Lorenzo Zanisi; Daniel Giles; Matt J. Kusner; Stanislas Pamela; Marc Peter Deisenroth
- Reference count: 40
- Primary result: Statistically guaranteed uncertainty quantification for surrogate models across diverse scientific domains with marginal coverage regardless of model architecture or output dimensionality up to 20 million dimensions.

## Executive Summary
This paper presents a conformal prediction framework for providing statistically guaranteed uncertainty quantification for surrogate models across diverse scientific domains. The method achieves marginal coverage regardless of model architecture, training regime, or output dimensionality up to 20 million dimensions, requiring only seconds to minutes of calibration on standard hardware. Through extensive experiments spanning PDEs, fusion diagnostics, and weather forecasting, the framework demonstrates near-perfect empirical coverage across different nonconformity scores while handling high-dimensional spatio-temporal outputs through cell-wise calibration.

## Method Summary
The framework implements inductive conformal prediction by splitting available data into training and calibration sets. It computes nonconformity scores on the calibration set, estimates a quantile that captures a fraction (1-Î±) of calibration errors, and applies this quantile as a buffer around test predictions. The approach handles high-dimensional outputs by performing cell-wise calibration independently for each spatial-temporal cell, circumventing the curse of dimensionality. Three nonconformity scores are explored: Absolute Error Residual (AER) for deterministic models, Standard Deviation (STD) for probabilistic models, and Conditional Quantile Regression (CQR) for quantile models.

## Key Results
- Achieves near-perfect empirical coverage (90%) across 9 different surrogate architectures including U-Nets, FNOs, and GNNs
- Scales to 20 million+ dimensional outputs while maintaining calibration times of seconds to minutes
- Demonstrates validity even for out-of-distribution predictions where models are deployed on physics regimes different from training data
- Maintains tensorial structure preservation while providing cell-wise calibrated uncertainty bounds

## Why This Works (Mechanism)

### Mechanism 1: Exchangeability-Driven Quantile Calibration
The framework provides statistically guaranteed marginal coverage if calibration data is exchangeable with test data. Inductive conformal prediction splits data into training and calibration sets, computes nonconformity scores on calibration data, and estimates a quantile that captures the target coverage fraction. This quantile is then applied as a buffer around test predictions. The core assumption is exchangeability between calibration and test distributions, which is weaker than strict i.i.d. assumptions. Coverage degrades when test regimes differ fundamentally from calibration distribution.

### Mechanism 2: Cell-wise Independence for Dimensionality Scaling
By applying calibration independently to each spatial-temporal cell, the framework circumvents the curse of dimensionality for millions of output dimensions. Rather than modeling complex joint distributions over entire output grids, each grid cell is treated as an independent scalar regression problem. This approach requires valid coverage at the cell level to be sufficient for downstream safety requirements. The method is insufficient when joint coverage of entire spatio-temporal fields is required.

### Mechanism 3: Nonconformity Score Selection
The choice of nonconformity score determines whether prediction intervals are constant or adaptive. AER uses a fixed quantile of past errors, resulting in constant-width error bars regardless of input. STD uses normalized errors scaled by predicted uncertainty, producing adaptive error bars that shrink where the model is confident

## Foundational Learning
The framework builds on the theoretical foundations of conformal prediction, which provides distribution-free uncertainty quantification guarantees. The method extends classical conformal prediction to handle high-dimensional surrogate model outputs through cell-wise calibration. The approach assumes exchangeability between calibration and test data, which is a weaker condition than i.i.d. assumptions typically required for statistical guarantees.

## Architecture Onboarding
The framework is compatible with various surrogate model architectures including U-Nets, Fourier Neural Operators, Graph Neural Networks, and other deep learning models. The method requires minimal architectural changes - it operates as a post-processing layer that wraps around existing models. The only requirement is that models can produce either deterministic predictions (for AER), uncertainty estimates (for STD), or quantile predictions (for CQR).

## Open Questions the Paper Calls Out
- How to extend the framework to provide joint coverage guarantees for entire spatio-temporal fields rather than just marginal cell-wise coverage
- The behavior of the method under covariate shift when calibration and test distributions differ
- Potential improvements in calibration efficiency for extremely high-dimensional outputs
- Extensions to non-exchangeable data scenarios

## Limitations
- Cell-wise calibration provides marginal coverage guarantees but not joint coverage for entire output fields
- Performance depends on the exchangeability assumption between calibration and test data
- The method requires a separate calibration set, which may be challenging to obtain in data-scarce domains
- Computational overhead for high-dimensional outputs, though still manageable at seconds to minutes

## Confidence
The experimental results demonstrate near-perfect empirical coverage across multiple architectures and problem domains. The theoretical foundations are well-established in the conformal prediction literature. The method has been validated on real-world scientific problems including PDEs, fusion diagnostics, and weather forecasting. However, the limitations section acknowledges that joint coverage guarantees and performance under covariate shift remain open questions.

## Next Checks
- Verify the implementation details of the three nonconformity scores across different architectures
- Examine the trade-off between calibration set size and coverage guarantees
- Investigate the computational scaling behavior for different dimensionalities and architectures
- Validate the framework on additional scientific domains and problem types
- Explore potential extensions to handle non-exchangeable data scenarios