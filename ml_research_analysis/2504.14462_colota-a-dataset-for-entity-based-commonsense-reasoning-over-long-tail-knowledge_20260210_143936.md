---
ver: rpa2
title: 'CoLoTa: A Dataset for Entity-based Commonsense Reasoning over Long-Tail Knowledge'
arxiv_id: '2504.14462'
source_url: https://arxiv.org/abs/2504.14462
tags:
- reasoning
- commonsense
- colota
- queries
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CoLoTa, a novel dataset designed to evaluate
  the commonsense reasoning capabilities of large language models (LLMs) on long-tail
  entities. The dataset consists of 3,300 queries derived from question answering
  and claim verification tasks, focusing on obscure entities from Wikidata.
---

# CoLoTa: A Dataset for Entity-based Commonsense Reasoning over Long-Tail Knowledge

## Quick Facts
- arXiv ID: 2504.14462
- Source URL: https://arxiv.org/abs/2504.14462
- Reference count: 40
- Primary result: CoLoTa reveals LLMs hallucinate more on obscure entities while maintaining high answer rates, creating a reliability gap

## Executive Summary
CoLoTa is a novel dataset designed to evaluate commonsense reasoning capabilities of large language models (LLMs) on long-tail entities—obscure knowledge that LLMs are unlikely to have seen during training. Unlike existing benchmarks that target popular entities, CoLoTa challenges LLMs by requiring reasoning over less familiar knowledge from Wikidata. The authors find that even advanced models like OpenAI-o1 struggle significantly with these queries, showing higher hallucination rates and reasoning errors. The dataset also serves as a benchmark for Knowledge Graph Question Answering (KGQA), highlighting the need for methods that incorporate both factual and commonsense knowledge.

## Method Summary
CoLoTa contains 3,300 queries derived from StrategyQA and CREAK, modified to target obscure Wikidata entities with few triples. Each query includes structured annotations: Wikidata QIDs, relevant KG subgraphs, natural-language inference rules (FOL-style), and decomposed reasoning steps. Evaluation uses Zero-shot and Few-shot Chain of Thought prompting across multiple LLMs (GPT-3.5, GPT-4o, OpenAI-o1, Gemini-1.5 Flash, Llama-3.3-70B) and KGQA methods (KB-Binder, KGR). Metrics include accuracy, answer rate, FActScore (factuality), and Reasoning Score (logical validity).

## Key Results
- OpenAI-o1 accuracy drops 0.15-0.42 across LLMs from original to CoLoTa queries
- Answer rates remain high (0.97 for OpenAI-o1) while accuracy falls (0.67 on CoLoTa)
- FActScore drops up to 0.40 and Reasoning Score drops up to 0.22 on CoLoTa queries
- KGQA methods achieve only 0.29-0.35 accuracy on CoLoTa

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Substituting popular entities with long-tail counterparts from Wikidata exposes and amplifies factual knowledge gaps in LLMs, leading to increased hallucinations when models attempt commonsense reasoning without adequate parametric knowledge.
- Mechanism: CoLoTa replaces well-known entities (e.g., "Barack Obama") with obscure entities having few Wikidata triples (e.g., "Liau Hiok-hian"). When LLMs lack training-data exposure to these entities, they cannot reliably retrieve the necessary factual predicates, yet still attempt to answer—resulting in hallucinated facts and degraded reasoning accuracy.
- Core assumption: The primary difficulty driver is entity popularity (triple count in Wikidata), not query structure; the original query logic remains valid after substitution.
- Evidence anchors: [abstract]: "even the most prominent LLMs, such as OpenAI-o1, suffer from high rates of reasoning errors and hallucinations on tasks requiring commonsense reasoning over obscure, long-tail entities."

### Mechanism 2
- Claim: Explicitly annotating queries with structured inference rules and KG subgraphs provides verifiable grounding that pure LLM reasoning lacks, enabling fine-grained diagnosis of factual vs. reasoning failures.
- Mechanism: Each CoLoTa entry includes Wikidata entity QIDs, relevant KG subgraphs, a natural-language inference rule (formalized as First-Order Logic), and decomposed reasoning steps. This scaffolding lets KGQA systems ground answers in external KG facts and allows separate evaluation of factuality (FActScore) and reasoning validity (Reasoning Score).
- Core assumption: All required facts exist in Wikidata and inference rules can be expressed as natural-language axioms; KGQA methods can retrieve and apply this structure.
- Evidence anchors: [section]: Section 3.1 defines the five constituents (query, QIDs, KG subgraph, inference rule, reasoning steps) and Eq. (1) formalizes inference rules.

### Mechanism 3
- Claim: Decoupling factual retrieval from commonsense inference enables isolation of hallucination sources—whether errors stem from missing knowledge or flawed logic chains.
- Mechanism: CoLoTa's dual-metric evaluation (FActScore for atomic fact support; Reasoning Score for logical step validity) separates knowledge gaps from reasoning failures. This allows diagnosing whether models hallucinate facts, commit logical errors, or both.
- Core assumption: Commonsense inference rules are relatively stable and domain-applicable; the key variable is the model's ability to retrieve correct facts and apply valid logic.
- Evidence anchors: [section]: Table 6 shows OpenAI-o1's FActScore drops by up to 0.40 on CoLoTa (factual hallucination increase) while Reasoning Score drops by up to 0.22 (logic errors increase).

## Foundational Learning

- **Concept: Long-tail knowledge distribution**
  - Why needed here: CoLoTa's central challenge is that LLMs perform well on popular entities abundant in training data but fail on obscure entities with sparse documentation.
  - Quick check question: Given two entities with 5,000 vs. 50 Wikidata triples, which would an LLM likely answer more accurately without external retrieval? Why?

- **Concept: Commonsense inference rules vs. factual triples**
  - Why needed here: CoLoTa explicitly separates world knowledge (KG triples: "Virginia Raggi → position held → mayor of Rome") from reasoning principles ("if two locations are in different countries, taxi travel is implausible").
  - Quick check question: For "Could someone in Toronto take a taxi to the Metropolitan Museum of Art?", identify one factual triple needed and one commonsense inference rule required.

- **Concept: Hallucination vs. refusal trade-off in evaluation**
  - Why needed here: High answer rates with low accuracy indicate models guess rather than admit ignorance; understanding this trade-off is critical for reliability assessment.
  - Quick check question: If an LLM maintains 95% answer rate but accuracy drops from 85% to 55% on long-tail entities, what does this suggest about its calibration?

## Architecture Onboarding

- **Component map:** Query processor -> KG retriever -> Inference annotator -> Evaluator
- **Critical path:**
  1. Filter StrategyQA/CREAK queries where required facts exist in Wikidata (inter-annotator verification)
  2. Generate SPARQL queries to find long-tail entity counterparts with similar properties but fewer triples
  3. Rewrite queries, annotate KG subgraphs, inference rules, and reasoning steps
  4. Benchmark LLMs (zero-shot/few-shot CoT) and KGQA methods (KB-Binder, KGR)

- **Design tradeoffs:**
  - Annotation depth vs. scalability: Detailed inference-rule and reasoning-step annotations enable fine-grained evaluation but limit dataset size to 3,300 queries
  - KG grounding vs. query diversity: Requiring all facts in Wikidata ensures KGQA compatibility but excludes queries needing knowledge outside Wikidata
  - Entity obscurity threshold: Overly obscure entities may make queries unanswerable even with retrieval; balancing challenge with fairness is non-trivial

- **Failure signatures:**
  - High answer rate (>90%) with low accuracy (<60%) on long-tail queries → hallucination on unknown entities
  - FActScore declining more than Reasoning Score → factual knowledge gaps dominate
  - Reasoning Score declining more than FActScore → logic chain failures dominate
  - KGQA methods returning "I don't know" frequently (answer rate <50%) → retrieval failures rather than reasoning errors

- **First 3 experiments:**
  1. Baseline degradation measurement: Run OpenAI-o1 and GPT-4o with zero-shot CoT on original vs. CoLoTa queries; quantify accuracy, answer rate, FActScore, and Reasoning Score drops
  2. KG grounding ablation: Provide KB-Binder or KGR with CoLoTa's pre-extracted Wikidata subgraphs; test whether KG access reduces FActScore gap vs. pure LLM baselines
  3. Error categorization: Sample 100 failures; manually classify as (a) missing factual knowledge, (b) incorrect inference rule application, or (c) logical step errors; correlate with metric patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can Knowledge Graph Question Answering (KGQA) methodologies be modified to successfully integrate commonsense reasoning with factual retrieval?
- Basis in paper: [explicit] The authors state that existing KGQA methods "perform abysmally" because they cannot leverage LLMs to perform commonsense reasoning, and propose the dataset to "pave the way for future KGQA research... to develop KGQA methods that incorporate factual and commonsense knowledge."
- Why unresolved: Current KGQA baselines (e.g., KB-Binder, KGR) rely on semantic parsing or claim retrofitting that focuses on factoid retrieval, lacking the architectural capacity to apply the abstract inference rules required by CoLoTa queries.
- What evidence would resolve it: A KGQA model that achieves significantly higher accuracy on CoLoTa than the current baselines (max 0.35) by explicitly retrieving Wikidata subgraphs and applying logical inference rules.

### Open Question 2
- Question: Can LLMs be trained or calibrated to reliably abstain from answering queries involving long-tail entities to minimize hallucinations?
- Basis in paper: [explicit] The authors observe that while accuracy drops significantly on CoLoTa, the "answer rate remains very close to the original queries," leading to hallucinations because models "still try to make a guess" rather than admitting ignorance.
- Why unresolved: Standard reinforcement learning or training objectives often penalize abstention less than error, or fail to distinguish between uncertainty due to ambiguity versus lack of parametric knowledge.
- What evidence would resolve it: A model that exhibits a lower answer rate proportional to its uncertainty on obscure entities, or a calibration method that aligns confidence scores with accuracy on the CoLoTa benchmark.

### Open Question 3
- Question: To what extent does the lack of parametric knowledge about an entity degrade the logical coherence of a model's reasoning process independent of factual correctness?
- Basis in paper: [inferred] The paper notes a considerable drop in the "Reasoning Score" (validity of logic steps) for OpenAI-o1 on CoLoTa compared to popular entities, suggesting that missing facts cause the reasoning mechanism itself to falter, not just the factual output.
- Why unresolved: It is unclear if this degradation is due to attention mechanisms failing without entity context, or if the models rely on memorized reasoning traces associated with popular entities that cannot be generalized.
- What evidence would resolve it: An ablation study providing models with external factual context (e.g., retrieved Wikidata triples) to see if logical validity (Reasoning Score) is restored even without parametric memorization.

## Limitations
- Exact prompt templates and few-shot exemplars used for LLM evaluation are not specified
- FActScore and Reasoning Score require manual verification, limiting automated validation
- Dataset size (3,300 queries) may not capture full diversity of long-tail reasoning scenarios

## Confidence
- **High Confidence:** The core finding that LLMs exhibit significant accuracy degradation on long-tail entities while maintaining high answer rates, indicating hallucination tendencies
- **Medium Confidence:** The mechanism that entity popularity (triple count) is the primary difficulty driver, as this assumes no confounding factors in query structure or complexity
- **Medium Confidence:** The KG grounding mechanism's effectiveness, as empirical KGQA results are not detailed in the available information

## Next Checks
1. Replicate the accuracy drop pattern (0.15-0.42 decline) using the exact CoLoTa dataset and standard few-shot CoT prompts
2. Conduct ablation study comparing LLM performance with and without the provided Wikidata subgraphs to quantify grounding benefit
3. Perform error analysis on 100+ failures to validate the FActScore vs. Reasoning Score decoupling mechanism and identify dominant failure modes