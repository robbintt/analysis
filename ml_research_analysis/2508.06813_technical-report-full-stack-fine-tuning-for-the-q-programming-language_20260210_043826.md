---
ver: rpa2
title: 'Technical Report: Full-Stack Fine-Tuning for the Q Programming Language'
arxiv_id: '2508.06813'
source_url: https://arxiv.org/abs/2508.06813
tags:
- evaluation
- training
- dataset
- pass
- test
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a full-stack fine-tuning pipeline for adapting
  large language models to the Q programming language, a niche language used in quantitative
  finance. The authors construct a LeetCode-style dataset, perform domain-adaptive
  pretraining, supervised fine-tuning, and reinforcement learning on Qwen-2.5 models
  across five parameter scales.
---

# Technical Report: Full-Stack Fine-Tuning for the Q Programming Language

## Quick Facts
- arXiv ID: 2508.06813
- Source URL: https://arxiv.org/abs/2508.06813
- Reference count: 35
- This paper presents a full-stack fine-tuning pipeline for adapting large language models to the Q programming language, a niche language used in quantitative finance. The authors construct a LeetCode-style dataset, perform domain-adaptive pretraining, supervised fine-tuning, and reinforcement learning on Qwen-2.5 models across five parameter scales. Their best model achieves 59% pass@1 accuracy on the Q benchmark, surpassing Claude Opus-4 by 29.5% and outperforming GPT-4.1 across all model sizes. The work provides a reproducible blueprint for specializing LLMs to low-resource domains with rigorous evaluation frameworks.

## Executive Summary
This paper addresses the challenge of adapting large language models to specialized programming languages with limited training data. The authors focus on Q, a niche language used in quantitative finance that lacks dedicated LLM support despite its importance in financial computing. They develop a comprehensive fine-tuning pipeline that combines domain-adaptive pretraining, supervised fine-tuning, and reinforcement learning to create specialized Q programming models. The work demonstrates that even with small datasets, systematic fine-tuning approaches can yield substantial performance gains over general-purpose models.

The research provides a reproducible framework for low-resource domain adaptation, showing that carefully constructed training data and multi-stage fine-tuning can bridge the performance gap for specialized languages. The authors validate their approach across multiple model scales, from 0.5B to 72B parameters, establishing a clear methodology for adapting LLMs to any specialized programming domain where data scarcity would otherwise limit performance.

## Method Summary
The authors construct a LeetCode-style dataset by extracting Q code from GitHub, creating a benchmark with 500 questions across five categories. They implement a three-stage fine-tuning pipeline: domain-adaptive pretraining (DAP) on 2.8K Q functions to adapt the model to Q syntax, supervised fine-tuning (SFT) on 1.6K function-question-answer triplets, and reinforcement learning with Verifiable Rewards (RLVR) using 640 curated examples. The pipeline is applied to Qwen-2.5 models across five scales (0.5B to 72B parameters), with training conducted on 8xA100 GPUs for larger models and 4xA100 for smaller ones. The evaluation framework uses a custom test harness that compiles and executes Q code to verify correctness, reporting pass@1 and pass@5 metrics.

## Key Results
- Best model achieves 59% pass@1 accuracy on Q benchmark, surpassing Claude Opus-4 by 29.5% and GPT-4.1 across all model sizes
- Full-stack fine-tuning pipeline yields 20-25% absolute improvement over general-purpose models on Q programming tasks
- Performance scales with model size, with 72B parameter model showing best results but smaller models (7B) offering strong performance with lower computational cost
- Reinforcement learning provides only marginal gains (1-3% improvement), suggesting supervised fine-tuning may be sufficient for this domain

## Why This Works (Mechanism)
The approach succeeds by systematically addressing the data scarcity challenge through a staged fine-tuning strategy. Domain-adaptive pretraining first acclimates general models to Q syntax patterns using available code functions, creating a foundation for more specialized learning. The supervised fine-tuning stage then teaches the model to map natural language questions to correct Q implementations using carefully curated examples. This progression allows the model to first learn the "language of Q" before learning "Q problem-solving," which is more effective than attempting to learn both simultaneously. The verifiable reward framework in reinforcement learning provides precise feedback for optimization, though its marginal impact suggests the supervised stage captures most of the learning value for this well-defined programming task.

## Foundational Learning

**Q Programming Language Syntax**: Q is a vector programming language used in quantitative finance, built on kdb+ database system. Understanding its array-based operations, temporal functions, and database query patterns is essential for evaluating model performance.

*Why needed*: The entire evaluation framework depends on correct compilation and execution of Q code, requiring familiarity with Q's unique syntax and semantics.

*Quick check*: Can you distinguish between Q's functional form and projection syntax when reviewing generated code?

**Domain-Adaptive Pretraining (DAP)**: A fine-tuning approach that adapts general-purpose models to specific domain characteristics using unlabeled or minimally labeled data before supervised training.

*Why needed*: Allows the model to first learn domain-specific patterns (Q syntax) without the complexity of task-specific learning, improving downstream performance.

*Quick check*: Verify that pretraining data covers the full range of Q syntax constructs you expect to see in the target domain.

**Reinforcement Learning with Verifiable Rewards (RLVR)**: A training paradigm where models are optimized using rewards derived from verifiable outcomes, such as correct code execution, rather than human preference labels.

*Why needed*: Provides precise, objective feedback for code generation tasks where correctness can be automatically determined through execution.

*Quick check*: Confirm that your reward function captures all relevant aspects of solution quality (correctness, efficiency, style) and that rewards are properly normalized.

## Architecture Onboarding

**Component Map**: GitHub repository extraction -> LeetCode-style dataset construction -> Domain-adaptive pretraining (DAP) -> Supervised fine-tuning (SFT) -> Reinforcement learning with Verifiable Rewards (RLVR) -> Evaluation framework

**Critical Path**: The most critical sequence is dataset construction → DAP → SFT, as these directly determine the model's ability to understand and generate correct Q code. The RLVR stage, while valuable, provides only incremental improvement.

**Design Tradeoffs**: The authors chose to exclude internet-dependent questions and "out of distribution" examples from evaluation, trading comprehensiveness for focused assessment of core Q programming capabilities. This creates a more controlled evaluation but may overestimate real-world performance. The decision to use Qwen-2.5 models rather than more established architectures represents a risk that paid off, though it may limit reproducibility for teams without access to these specific models.

**Failure Signatures**: Models may fail on questions requiring knowledge beyond the training distribution, particularly those involving external API calls or complex financial domain concepts not represented in the GitHub corpus. Compilation errors in generated code often indicate syntax adaptation failures from the DAP stage. Runtime errors suggest issues with logical reasoning or misunderstanding of Q's vector operations.

**3 First Experiments**:
1. Test the model on a held-out validation set from the same distribution as training data to establish baseline performance before full evaluation
2. Perform ablation by removing the DAP stage to quantify its contribution to overall performance gains
3. Evaluate model outputs using only compilation success rate (ignoring runtime correctness) to isolate syntax adaptation from logical reasoning capabilities

## Open Questions the Paper Calls Out
None

## Limitations
The evaluation is constrained by the small size of the Q benchmark dataset (500 questions), which limits statistical power and may not fully represent the complexity of real-world Q programming tasks. The authors acknowledge that questions requiring internet access or those deemed "out of distribution" were excluded from evaluation, potentially creating an overly optimistic performance picture. Additionally, the reinforcement learning component appears to provide only marginal gains (1-3% improvement), raising questions about its cost-effectiveness for this particular domain.

## Confidence

- **High confidence**: The baseline performance comparisons against GPT-4.1 and Claude Opus-4 are methodologically sound and the implementation details for data processing and fine-tuning are reproducible
- **Medium confidence**: The reported 59% pass@1 accuracy on the Q benchmark is valid within the constrained evaluation framework, but may not generalize to broader Q programming contexts
- **Medium confidence**: The architectural choices and hyperparameter selections are reasonable, though the marginal benefit of reinforcement learning suggests room for optimization

## Next Checks

1. Evaluate the model on an expanded Q programming benchmark that includes internet-dependent queries and real-world production code scenarios to test generalization beyond the curated dataset

2. Conduct ablation studies specifically isolating the contribution of domain-adaptive pretraining versus supervised fine-tuning to better understand which components drive performance improvements

3. Test the model's ability to handle multi-file Q projects and API integration scenarios that go beyond single-question evaluation, assessing practical deployment readiness