---
ver: rpa2
title: 'MOSAIC: Masked Objective with Selective Adaptation for In-domain Contrastive
  Learning'
arxiv_id: '2510.16797'
source_url: https://arxiv.org/abs/2510.16797
tags:
- contrastive
- domain
- training
- language
- adaptation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MOSAIC, a multi-stage framework for adapting
  large-scale text embedding models to specialized domains by jointly optimizing masked
  language modeling (MLM) and contrastive objectives. The approach addresses the challenge
  of transferring robust semantic discrimination from general-domain models to domain-specific
  contexts while preserving fine-grained token-level learning.
---

# MOSAIC: Masked Objective with Selective Adaptation for In-domain Contrastive Learning

## Quick Facts
- **arXiv ID:** 2510.16797
- **Source URL:** https://arxiv.org/abs/2510.16797
- **Reference count:** 30
- **Primary result:** Joint MLM + contrastive adaptation achieves up to +13.4% NDCG@10 on domain retrieval tasks

## Executive Summary
MOSAIC addresses the challenge of adapting large-scale text embedding models to specialized domains while preserving fine-grained token-level learning and robust semantic discrimination. The method introduces a multi-stage framework that jointly optimizes masked language modeling (restricted to domain-specific tokens) and contrastive objectives, enabling effective transfer from general-domain models to domain-specific contexts. Through comprehensive experiments on biomedical and Islamic domains, MOSAIC demonstrates substantial improvements over strong baselines, particularly in low-resource settings, while maintaining semantic quality through careful stage ordering and objective balancing.

## Method Summary
MOSAIC employs a three-stage pipeline to adapt pre-trained text embedding models to specialized domains. Stage 1 expands the tokenizer vocabulary with domain-specific tokens, initializing new embeddings as means of their subword components. Stage 2 jointly optimizes masked language modeling (restricted to newly added domain tokens with α=0.3 weighting) and contrastive learning with in-batch negatives. Stage 3 transitions to contrastive-only training to restore sentence-level discrimination. The framework uses nomic-embed-text-v1-unsup as the base model, processes biomedical data from PubMed 2025 snapshots (filtered via gte-base consistency checking), and applies consistency filtering to Islamic data from Tafseer Ibn Kathir. Training employs batch size 128, learning rate 0.0005, weight decay 0.01, and 1-5 epochs depending on the stage.

## Key Results
- **NDCG@10 gains:** Up to +13.4% improvement over strong baselines in biomedical retrieval
- **Cross-domain effectiveness:** Significant performance gains in both high-resource (biomedical) and low-resource (Islamic) domains
- **Ablation validation:** Stage ordering critical - contrastive-only → joint MLM+contrastive reduces BIOSSES STS from 88.1 to 70.5
- **Low-resource robustness:** Effective adaptation even with limited in-domain data (7,587 Islamic verse pairs)

## Why This Works (Mechanism)
MOSAIC succeeds by addressing the fundamental tension between token-level adaptation and sentence-level semantic preservation. The method recognizes that domain adaptation requires learning specialized terminology (handled by restricted MLM) while maintaining the embedding space's ability to discriminate between semantically similar and dissimilar sentences (handled by contrastive learning). By separating vocabulary expansion from joint optimization and carefully ordering the training stages, MOSAIC prevents the token-level learning from overwhelming the semantic discrimination capabilities. The selective MLM restriction ensures that the model learns domain-specific token representations without degrading its ability to capture broader semantic relationships.

## Foundational Learning
- **Masked Language Modeling (MLM):** Language model training objective that predicts masked tokens from context; needed to learn domain-specific terminology representations; quick check: verify masking rate and token restriction implementation
- **Contrastive Learning:** Training paradigm that pulls similar examples together and pushes dissimilar examples apart in embedding space; needed to maintain semantic discrimination capabilities; quick check: confirm in-batch negative sampling and loss computation
- **Vocabulary Expansion:** Process of adding domain-specific tokens to the model's tokenizer; needed to represent specialized terminology absent from the base model; quick check: verify new token initialization as subword means
- **Multi-stage Training:** Sequential optimization approach where different objectives are trained in specific orders; needed to balance competing learning signals; quick check: validate stage progression and objective switching
- **Consistency Filtering:** Data quality control using pre-trained models to filter training pairs; needed to ensure high-quality domain-specific training data; quick check: verify filtering thresholds and top-k settings
- **Joint Objective Optimization:** Simultaneous training with multiple loss functions; needed to balance token-level and sentence-level learning; quick check: monitor individual loss magnitudes during training

## Architecture Onboarding

**Component Map:** Tokenizer Expansion -> Joint MLM+Contrastive Training -> Contrastive-Only Training

**Critical Path:** Stage 1 (vocabulary expansion) → Stage 2 (joint optimization with α=0.3) → Stage 3 (contrastive recovery)

**Design Tradeoffs:** The method trades computational complexity (three-stage pipeline vs. single-stage adaptation) for improved performance and stability. The selective MLM restriction limits the model's exposure to domain tokens but prevents degradation of general semantic capabilities. Joint optimization requires careful balancing to prevent MLM from dominating contrastive learning.

**Failure Signatures:** MLM dominance manifests as poor sentence discrimination (low NDCG@10, high BIOSSES scores indicating poor STS performance). Incorrect stage ordering shows sharp performance drops (e.g., BIOSSES from 88.1 to 70.5). Inadequate vocabulary expansion results in high perplexity on domain-specific tokens.

**First Experiments:** 1) Evaluate Stage 1 checkpoint to quantify embedding quality degradation from naive vocabulary expansion. 2) Test MLM masking restricted to domain tokens vs. all tokens to validate the importance of selective masking. 3) Compare joint MLM+contrastive vs. contrastive-only first training to confirm stage ordering importance.

## Open Questions the Paper Calls Out
- **Cross-linguistic generalizability:** How does MOSAIC perform on morphologically rich languages or non-Latin scripts? The current study is limited to English, and tokenizer expansion may behave differently for agglutinative languages.
- **Boundary query performance:** Can the framework prevent performance degradation on queries bridging domain-specific terminology and general social context? Current optimization may isolate domain content from broader semantic context.
- **Dynamic α scaling:** Is the optimal balance coefficient between MLM and contrastive loss static, or should it scale relative to the size of the expanded domain vocabulary? The interaction between weighting and new token count remains unexplored.

## Limitations
- **Reproducibility gaps:** Critical details missing for gte-base consistency filtering parameters and MLM masking restriction implementation
- **Cross-domain validation:** Limited testing on non-English domains and morphologically complex languages
- **Data asymmetry:** High-resource biomedical experiments (20M pairs) vs. low-resource Islamic experiments (7,587 pairs) without detailed quality controls

## Confidence

**High confidence** in core methodology: Three-stage pipeline clearly described with substantial performance gains (up to 13.4% NDCG@10) and convincing ablation studies demonstrating stage ordering importance.

**Medium confidence** in reproducibility: Main experimental setup specified but critical details missing (filtering thresholds, masking implementation, stage-specific hyperparameters). Failure modes provide diagnostic guidance.

**Low confidence** in cross-domain generalization: Limited testing on non-English domains, small dataset for Islamic experiments, and no exploration of morphologically complex languages.

## Next Checks

1. **MLM domain token restriction verification:** Implement and test MLM loss restriction to newly added domain tokens only. Compare performance when masking all tokens versus only domain tokens to confirm selective masking importance.

2. **Stage ordering ablation:** Train models with contrastive-only first, then joint MLM+contrastive (reversed order) versus proposed Stage 2→Stage 3 sequence. Measure BIOSSES STS performance differences to validate stage ordering importance.

3. **Vocabulary expansion impact assessment:** Evaluate model performance immediately after Stage 1 vocabulary expansion (before joint training) to quantify embedding quality degradation that necessitates Stage 2 joint optimization.