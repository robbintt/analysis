---
ver: rpa2
title: Reducing Hallucinations in LLMs via Factuality-Aware Preference Learning
arxiv_id: '2601.03027'
source_url: https://arxiv.org/abs/2601.03027
tags:
- factuality
- preference
- factual
- f-dpo
- standard
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses hallucinations in large language models by\
  \ introducing F-DPO, a factuality-aware extension of Direct Preference Optimization.\
  \ F-DPO uses binary factuality labels to correct preference\u2013factuality misalignment\
  \ through label flipping and a factuality-conditioned margin, ensuring chosen responses\
  \ are never less factual than rejected ones."
---

# Reducing Hallucinations in LLMs via Factuality-Aware Preference Learning

## Quick Facts
- **arXiv ID**: 2601.03027
- **Source URL**: https://arxiv.org/abs/2601.03027
- **Reference count**: 23
- **One-line result**: F-DPO reduces hallucination rates by up to 80% while improving factuality scores without auxiliary reward models

## Executive Summary
This paper introduces F-DPO, a factuality-aware extension of Direct Preference Optimization that addresses hallucination problems in large language models. The method uses binary factuality labels to correct preference-factuality misalignment through label flipping and a factuality-conditioned margin, ensuring chosen responses are never less factual than rejected ones. F-DPO demonstrates consistent improvements across seven open-weight LLMs (1B-14B), reducing hallucination rates and improving factuality scores without requiring auxiliary reward models, token-level annotations, or multi-stage training.

## Method Summary
F-DPO extends standard DPO by incorporating factuality awareness through two key mechanisms: label flipping to correct misaligned preference pairs where the rejected response is more factual than the chosen one, and a factuality-conditioned margin that ensures the selected response maintains factual superiority. The method operates directly on preference data with binary factuality labels, eliminating the need for complex reward modeling or detailed annotation schemes. This approach is applied during preference optimization to align model behavior with factual correctness while preserving the benefits of preference-based fine-tuning.

## Key Results
- Qwen3-8B hallucination rate reduced from 0.424 to 0.084 (80% reduction)
- Factuality scores improved by 50% on average (5.26 to 7.90)
- TruthfulQA performance: +17% MC1 accuracy (0.500 to 0.585) and +49% MC2 accuracy (0.357 to 0.531) on Qwen2.5-14B

## Why This Works (Mechanism)
The core mechanism works by aligning preference optimization objectives with factual correctness. Standard DPO optimizes for user preferences but may inadvertently reinforce factually incorrect responses if they appear more desirable in preference pairs. F-DPO corrects this misalignment by identifying cases where rejected responses are actually more factual than chosen ones, flipping the labels to ensure factual superiority is maintained. The factuality-conditioned margin then reinforces this alignment during optimization, creating a training signal that prioritizes factual accuracy alongside preference satisfaction.

## Foundational Learning
- **Preference optimization fundamentals**: Understanding how models learn from pairwise comparisons rather than absolute labels, essential for implementing DPO-based methods.
- **Factuality assessment methods**: Binary labeling schemes for determining factual correctness, necessary for identifying misalignment in preference pairs.
- **Reward modeling alternatives**: Techniques that avoid auxiliary reward models while maintaining optimization quality, crucial for F-DPO's design.
- **Margin-based learning**: Methods for enforcing constraints during optimization, important for implementing the factuality-conditioned margin.
- **Label flipping strategies**: Approaches for correcting mislabeled or misaligned training data, central to F-DPO's correction mechanism.
- **Multi-task optimization**: Balancing competing objectives (preference satisfaction vs. factuality), relevant for understanding F-DPO's trade-offs.

## Architecture Onboarding
**Component Map**: Preference Data -> Factuality Labels -> Label Flipping -> F-DPO Optimization -> Factuality-Enhanced Model

**Critical Path**: The label flipping mechanism is critical as it ensures factual correctness is prioritized during preference optimization. Without this step, standard DPO would optimize for preferences regardless of factual accuracy.

**Design Tradeoffs**: F-DPO trades some preference optimization flexibility for factual accuracy. While standard DPO maximizes preference satisfaction, F-DPO may occasionally choose less preferred but more factual responses, potentially reducing overall preference scores but improving reliability.

**Failure Signatures**: Models may show reduced fluency or creativity if factuality constraints are too strict. Over-flipping labels could lead to training on noisy data, while under-flipping maintains preference-factuality misalignment.

**First Experiments**:
1. Apply F-DPO to a small model on a controlled dataset with known factuality issues to verify label flipping effectiveness.
2. Compare F-DPO vs standard DPO on preference satisfaction metrics to quantify the trade-off.
3. Test F-DPO on ambiguous factual scenarios to assess handling of borderline cases.

## Open Questions the Paper Calls Out
None

## Limitations
- Binary factuality labeling may oversimplify complex factual scenarios and struggle with ambiguous or context-dependent claims
- Evaluation relies heavily on factuality benchmarks like TruthfulQA, which may not capture real-world hallucination scenarios across diverse domains
- Generalization to proprietary or much larger models beyond the tested 1B-14B open-weight models remains unverified

## Confidence
- **High confidence**: Core methodology and technical soundness of F-DPO vs standard DPO, with well-documented empirical improvements
- **Medium confidence**: General applicability across model sizes is supported by seven tested models but lacks broader validation
- **Low confidence**: Long-term generalization to unseen domains and complex factual scenarios without further testing

## Next Checks
1. Test F-DPO on proprietary models (e.g., GPT-4, Claude) to verify performance beyond open-weight LLMs and assess scalability
2. Conduct ablation studies to quantify the impact of label flipping mechanism versus factuality-conditioned margin
3. Evaluate F-DPO on domain-specific factual benchmarks (medical, legal, technical) to assess robustness in specialized knowledge areas