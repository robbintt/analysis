---
ver: rpa2
title: Concentration bounds on response-based vector embeddings of black-box generative
  models
arxiv_id: '2511.08307'
source_url: https://arxiv.org/abs/2511.08307
tags:
- generative
- high
- probability
- sufficiently
- vector
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper establishes high probability concentration bounds on\
  \ sample vector embeddings of black-box generative models obtained through Data\
  \ Kernel Perspective Space (DKPS) embedding. The authors derive theoretical guarantees\
  \ showing that under appropriate regularity conditions, the estimation error of\
  \ population-level vector embeddings decreases as the number of sample responses\
  \ grows faster than n\xB3, where n is the number of generative models."
---

# Concentration bounds on response-based vector embeddings of black-box generative models

## Quick Facts
- arXiv ID: 2511.08307
- Source URL: https://arxiv.org/abs/2511.08307
- Reference count: 40
- This paper establishes high probability concentration bounds on sample vector embeddings of black-box generative models via Data Kernel Perspective Space (DKPS) embedding

## Executive Summary
This paper derives high probability concentration bounds for vector embeddings of black-box generative models obtained through Data Kernel Perspective Space (DKPS) embedding. The authors show that under appropriate regularity conditions, the estimation error between sample and population-level CMDS embeddings decreases as the number of sample responses grows faster than n³, where n is the number of generative models. The theoretical framework provides polynomial bounds on the estimation error that are empirically validated through both simulated and real large language model response experiments.

## Method Summary
The method involves embedding black-box generative models into a vector space by collecting r i.i.d. response replicates for each model-query pair, computing sample means, and applying classical multidimensional scaling (CMDS) to obtain vector embeddings. The theoretical analysis establishes concentration bounds on the estimation error between sample and population embeddings, showing that with high probability, the error is bounded by a polynomial function of (n³/r)^(1/2-δ) for any δ > 0. The approach relies on three key assumptions: bounded embedding functions, existence of a compact embedding space, and a fixed-rank dissimilarity matrix structure.

## Key Results
- Theoretical bound: Estimation error ||ψ̂W* - ψ||_{2,∞} is bounded by Poly_3((n³/r)^(1/2-δ)) with high probability
- Empirical validation: Actual estimation errors are several orders of magnitude smaller than theoretical upper bounds (~10^{-4} vs ~10^{-1})
- Coverage verification: Empirical coverage across 100 Monte Carlo samples shows ~70-80% satisfaction of theoretical bounds

## Why This Works (Mechanism)
The concentration bounds work because the estimation error between sample and population CMDS embeddings can be controlled through the spectral properties of the sample dissimilarity matrix B̂ and its deviation from the population matrix B. When the number of replicates r grows faster than n³, the entrywise deviations in the dissimilarity matrix become sufficiently small relative to the spectral gap between the d-th and (d+1)-th eigenvalues, enabling stable recovery of the population embedding via CMDS. The bounded embedding function assumption ensures the sample means concentrate around their expectations, while the fixed-rank assumption guarantees the existence of a spectral gap necessary for the stability of the CMDS procedure.

## Foundational Learning
- **Data Kernel Perspective Space (DKPS) embedding**: A method for representing black-box generative models as vectors based on their response distributions to user queries; needed because it provides a principled way to compare models without access to their internal parameters
- **Classical Multidimensional Scaling (CMDS)**: A dimensionality reduction technique that embeds dissimilarity matrices into low-dimensional Euclidean space; needed to convert pairwise dissimilarities between model response distributions into vector representations
- **Procrustes alignment**: A method for finding the optimal orthogonal transformation between two point sets; needed because CMDS embeddings are unique only up to orthogonal rotations
- **Spectral gap analysis**: The study of eigenvalue differences in matrices to determine stability of eigenvectors; needed to establish when sample eigenvectors converge to population eigenvectors
- **Entrywise concentration bounds**: Probabilistic bounds on individual matrix entries; needed to control the deviation between sample and population dissimilarity matrices

## Architecture Onboarding
**Component map:** Generate responses -> Compute sample means -> Calculate dissimilarity matrix -> Apply CMDS -> Obtain vector embeddings
**Critical path:** Response generation → Sample mean computation → CMDS eigendecomposition → Procrustes alignment
**Design tradeoffs:** Fixed rank assumption enables clean spectral analysis but may limit applicability; bounded embedding assumption simplifies concentration analysis but may not hold for all generative models; ω(n³) sample requirement provides strong guarantees but is computationally expensive
**Failure signatures:** Poor empirical coverage indicates either insufficient sample size, violation of bounded embedding assumption, or inadequate spectral gap in the dissimilarity matrix
**First experiments:** 1) Verify eigenvalue spectrum of population B matrix shows clear spectral gap; 2) Test coverage sensitivity to different r growth rates (n² vs n³); 3) Compare empirical error magnitude to theoretical bounds for different δ values

## Open Questions the Paper Calls Out
**Open Question 1:** Can concentration bounds be established for a sample size r growing slower than ω(n³)?
- Basis: The authors state that obtaining ω(n³) replicates is costly, making the investigation of bounds for slower rates of increase in r an important area of future work.
- Why unresolved: The current proofs for the spectral norm bounds in Corollary 1 and Theorem 2 rely specifically on the assumption that r grows faster than n³.
- What evidence would resolve it: A theoretical proof demonstrating convergence under a slower growth rate condition, such as r = ω(n²).

**Open Question 2:** Can the fixed-rank assumption (Assumption 1) be relaxed to accommodate embedding dimensions that grow with the number of models?
- Basis: The Discussion notes that Assumption 1 is made for convenient analysis but is not a strict necessity; the authors explicitly leave the relaxation of this assumption for generalized scenarios to future work.
- Why unresolved: A variable rank prevents the use of the current stability arguments regarding the compactness of the embedding space and comparability of vector components.
- What evidence would resolve it: Theoretical guarantees for the estimation error that hold even when the rank of the dissimilarity matrix B depends on n.

**Open Question 3:** Can the upper bounds on the estimation error be tightened to better match the empirical magnitude of the error?
- Basis: The Discussion highlights that the derived bounds (e.g., in Table 1 and Table 2) are "not sharp" and significantly exceed the actual observed errors, suggesting room for improvement.
- Why unresolved: The current polynomial bounds used in the analysis may be loose over-approximations of the actual tail behavior of the estimation error.
- What evidence would resolve it: Derivation of a tighter concentration inequality where the upper bound is of the same order of magnitude as the average estimation error observed in simulations.

## Limitations
- Theoretical bounds are orders of magnitude looser than empirical errors (actual ~10^{-4} vs bounds ~10^{-1}), limiting practical guidance
- Computational burden of r = ω(n³) requirement is prohibitive for large-scale applications, creating practical limitations
- Key implementation details remain underspecified: rank determination criterion and δ parameter selection for real data experiments

## Confidence
- High confidence in mathematical framework and theoretical derivation methodology
- Medium confidence in empirical validation due to unspecified implementation details
- Low confidence in practical applicability given computational constraints

## Next Checks
1. Reproduce simulation results for n ∈ {10,12,15,18,20} with 100 Monte Carlo iterations, explicitly reporting the eigenvalue-based rank determination criterion and δ parameter value used
2. Test sensitivity of empirical coverage to r growth rate by comparing results for r = n^{2.5}, n^{3.0}, and n^{3.5} to identify the minimal sufficient sample size
3. Validate the bounded embedding function assumption by empirically measuring the maximum norm of g(f_i(q_j)) across all model-query pairs in the real data experiments