---
ver: rpa2
title: Concentration bounds on response-based vector embeddings of black-box generative
  models
arxiv_id: '2511.08307'
source_url: https://arxiv.org/abs/2511.08307
tags:
- generative
- high
- probability
- sufficiently
- vector
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper establishes high probability concentration bounds on\
  \ sample vector embeddings of black-box generative models obtained through Data\
  \ Kernel Perspective Space (DKPS) embedding. The authors derive theoretical guarantees\
  \ showing that under appropriate regularity conditions, the estimation error of\
  \ population-level vector embeddings decreases as the number of sample responses\
  \ grows faster than n\xB3, where n is the number of generative models."
---

# Concentration bounds on response-based vector embeddings of black-box generative models

## Quick Facts
- arXiv ID: 2511.08307
- Source URL: https://arxiv.org/abs/2511.08307
- Authors: Aranyak Acharyya; Joshua Agterberg; Youngser Park; Carey E. Priebe
- Reference count: 40
- Key outcome: Theoretical bounds show estimation error decreases when sample responses grow faster than n³, with empirical validation showing actual errors are several orders of magnitude smaller than upper bounds.

## Executive Summary
This paper establishes high probability concentration bounds for Data Kernel Perspective Space (DKPS) embeddings of black-box generative models using response-based dissimilarity matrices. The authors derive theoretical guarantees showing that the estimation error of population-level vector embeddings decreases as the number of sample responses grows faster than n³, where n is the number of generative models. The framework connects sample concentration of dissimilarity matrices to spectral perturbation theory, providing rigorous guarantees for embedding stability. Numerical experiments using both simulated and real large language model responses demonstrate that these theoretical bounds are empirically satisfied, though the actual estimation errors are significantly smaller than the theoretical upper bounds.

## Method Summary
The method involves generating r response replicates for each model-query pair, computing sample mean embeddings, and constructing a dissimilarity matrix using Frobenius norms. Classical Multidimensional Scaling (CMDS) is then applied to obtain vector embeddings by eigendecomposition of the double-centered dissimilarity matrix. The theoretical analysis establishes concentration bounds on the estimation error of these embeddings, showing that under appropriate regularity conditions, the error scales as a polynomial function of (n³/r)^(1/2-δ) for any δ > 0. The approach requires careful control of model response variability and stable eigenvalue structure in the population dissimilarity matrix.

## Key Results
- Estimation error decreases as sample responses grow faster than n³
- High probability bounds show error scales as (n³/r)^(1/2-δ) for any δ > 0
- Empirical validation with real LLMs shows actual errors are orders of magnitude smaller than theoretical bounds
- Theoretical framework connects sample concentration to spectral perturbation theory

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The estimation error of the dissimilarity matrix $\hat{B}$ concentrates around the population matrix $B$ as sample size increases.
- **Mechanism:** By averaging $r$ response replicates for each model-query pair, sample means converge to population means. Error entries are bounded probabilistically using variance terms, and with bounded variance and large $r$, the Frobenius norm of the error matrix becomes small.
- **Core assumption:** Response distribution variability is uniformly bounded.
- **Evidence anchors:** Abstract states error decreases with sample responses; Theorem 1 establishes entrywise bounds dependent on variance terms.
- **Break condition:** If model responses have infinite or exploding variance, concentration fails.

### Mechanism 2
- **Claim:** Small perturbations in the dissimilarity matrix translate to small perturbations in the vector embeddings.
- **Mechanism:** Perturbation theory (Weyl's Inequality and Davis-Kahan Theorem) shows that small spectral norm perturbations in $\hat{B}$ result in small changes to leading eigenvectors (the embeddings), up to an orthogonal rotation.
- **Core assumption:** Rank $d$ is constant and eigenvalues are stable and non-vanishing.
- **Evidence anchors:** Section 4, Theorem 2 decomposes error using Davis-Kahan; Assumption 2 ensures well-conditioning.
- **Break condition:** If smallest non-zero eigenvalue approaches zero, error bound diverges.

### Mechanism 3
- **Claim:** Sample complexity requirement grows cubically with the number of models ($r = \omega(n^3)$).
- **Mechanism:** Spectral norm error bound scales as $(n^3/r)^{1/2-\delta}$. To drive error to zero as $n$ increases, $r$ must grow faster than $n^3$ to counteract the accumulation of entry-wise errors across the $n \times n$ matrix.
- **Core assumption:** $r$ grows strictly faster than $n^3$.
- **Evidence anchors:** Abstract states error decreases with sample responses growing faster than $n^3$; Corollary 1 and 2 formalize this requirement.
- **Break condition:** If $r$ grows slower than $n^3$, error bound grows indefinitely as $n \to \infty$.

## Foundational Learning

- **Concept: Classical Multidimensional Scaling (CMDS)**
  - **Why needed here:** This is the core algorithm transforming pairwise dissimilarities into Euclidean coordinates (vector embeddings). You cannot understand error propagation without knowing embeddings are eigenvectors of the double-centered dissimilarity matrix.
  - **Quick check question:** How does CMDS handle a distance matrix to produce lower-dimensional coordinates, and what role do eigenvalues play?

- **Concept: Spectral Norm and Perturbation Theory**
  - **Why needed here:** The paper measures error using the spectral norm. Understanding how matrix perturbations affect eigenvectors (Davis-Kahan) is the bridge between "data error" and "embedding error."
  - **Quick check question:** If you add a small noise matrix $E$ to a matrix $A$, does the leading eigenvector of $A+E$ necessarily stay close to the leading eigenvector of $A$?

- **Concept: Concentration Inequalities**
  - **Why needed here:** To move from "random responses" to "high probability bounds," one must understand how variance scales down with sample size $r$.
  - **Quick check question:** Why does the probability of the sample mean deviating from the true mean decrease as the number of i.i.d. samples increases?

## Architecture Onboarding

- **Component map:** Black-box Interface -> Embedding Function (g) -> Aggregator -> Spectral Solver (CMDS) -> Perspective Store
- **Critical path:** The sampling loop. Generating $r$ responses per query per model is the computational bottleneck, specifically requiring $r$ to scale with $n^3$ for theoretical guarantees.
- **Design tradeoffs:**
  - **Strictness vs. Cost:** Bounds are "not sharp" - good results may be achievable with $r \ll n^3$ empirically, but theoretical guarantees are lost.
  - **Temperature/Noise:** Lowering model "temperature" (variability) reduces variance terms, tightening bounds. High temperature requires higher $r$.
- **Failure signatures:**
  - **Eigenvalue Collapse:** If scree plot shows no clear "elbow" or smallest eigenvalue $\lambda_d$ is near zero, error bound explodes.
  - **Inconsistent Dimensions:** If rank $d$ of $\hat{B}$ fluctuates as $n$ grows, it violates Assumption 1.
- **First 3 experiments:**
  1. **Validate Scaling:** Generate synthetic binary responses with fixed $n=10, m=3$. Vary $r$. Plot estimation error vs $(n^3/r)$. Verify curve follows polynomial bound.
  2. **Stress Test Rank:** Generate data where true dimension $d$ is ambiguous. Run DKPS and observe if error bounds fail when enforcing fixed $d$ that doesn't match underlying manifold.
  3. **Real Model Calibration:** Pick 2 LLMs and 2 queries. Sample $R=1000$ responses for ground truth. Subsample $r=50$ responses. Check if bound from Theorem 2 is empirically satisfied.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can high-probability concentration bounds be established for a sample size growth rate $r$ that is slower than $r = \omega(n^3)$?
- **Basis in paper:** Section 6 states obtaining $r = \omega(n^3)$ replicates is costly in reality, and investigation of concentration bounds for slower rate of increase in $r$ will be of importance.
- **Why unresolved:** Current theoretical derivation relies on cubic growth rate to ensure spectral norm convergence, creating practical bottleneck for real-world applications.
- **What evidence would resolve it:** Derivation of bounds that hold with high probability for $r = O(n^\alpha)$ where $\alpha < 3$.

### Open Question 2
- **Question:** How can the theoretical upper bounds be sharpened to more closely approximate the empirically observed estimation errors?
- **Basis in paper:** Section 6 notes actual estimation errors are "several orders of magnitude smaller" than upper bounds, and authors state bounds are "not sharp, and hence there is a room for improvement."
- **Why unresolved:** Current polynomial bounds involve conservative constants and exponents derived from general spectral perturbation theorems.
- **What evidence would resolve it:** Refined analysis resulting in tighter polynomial coefficients or reduced exponents that align closely with experimental results.

### Open Question 3
- **Question:** Can the condition that rank of population dissimilarity matrix $B$ is constant (Assumption 1) be relaxed to allow embedding dimension $d$ to grow with $n$?
- **Basis in paper:** Section 4 recognizes Assumption 1 needs to be relaxed for more generalized scenarios, left to future work.
- **Why unresolved:** Fixed rank simplifies analysis by preventing vector dimension from growing, but may not hold in all settings.
- **What evidence would resolve it:** Derivation of concentration bounds in regime where rank $d$ is a function of $n$.

## Limitations

- The theoretical bounds are conservative, with the cubic scaling requirement being particularly stringent for practical applications.
- The framework depends critically on Assumptions 1 and 2 regarding eigenvalue stability and constant rank, which may not hold in all scenarios.
- The proof technique relies on bounded variance conditions that may be violated when dealing with generative models exhibiting heavy-tailed response distributions.

## Confidence

**High Confidence:** The mechanism connecting sample concentration to spectral error propagation is well-established through Davis-Kahan perturbation theory. Numerical experiments demonstrating bounds are empirically satisfied provide strong support.

**Medium Confidence:** The specific cubic scaling requirement appears to be a conservative theoretical artifact rather than practical necessity, as evidenced by empirical results showing good performance with smaller sample sizes.

**Low Confidence:** The uniform boundedness assumption on variance terms may be violated in practice, particularly when comparing generative models with very different response characteristics.

## Next Checks

1. **Sharpness Analysis:** Systematically vary the ratio $r/n^3$ in controlled simulations to empirically determine minimal sample complexity required for reliable embedding recovery, comparing this to theoretical bound.

2. **Assumption Sensitivity:** Test robustness of theoretical bounds under violations of Assumptions 1 and 2 by generating synthetic data where smallest eigenvalue $\lambda_d$ approaches zero or where rank $d$ fluctuates as $n$ grows.

3. **Variance Sensitivity:** Evaluate performance of DKPS embedding when response distributions have heavy tails or unbounded variance by comparing bounded and unbounded variance generative models under identical conditions.