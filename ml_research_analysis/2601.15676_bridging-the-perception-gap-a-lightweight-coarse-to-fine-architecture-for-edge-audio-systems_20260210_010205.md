---
ver: rpa2
title: 'Bridging the Perception Gap: A Lightweight Coarse-to-Fine Architecture for
  Edge Audio Systems'
arxiv_id: '2601.15676'
source_url: https://arxiv.org/abs/2601.15676
tags:
- edge
- perception
- audio
- cloud
- cofi-agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes CoFi-Agent, a hybrid edge-cloud architecture
  designed to address the tension between perception depth and computational efficiency
  in edge audio systems. The core idea is a coarse-to-fine workflow: a lightweight
  local Audio-LLM performs initial inference, and a cloud controller conditionally
  triggers targeted refinement via on-device tools like temporal re-listening and
  local ASR when uncertainty is detected.'
---

# Bridging the Perception Gap: A Lightweight Coarse-to-Fine Architecture for Edge Audio Systems

## Quick Facts
- arXiv ID: 2601.15676
- Source URL: https://arxiv.org/abs/2601.15676
- Reference count: 26
- Primary result: CoFi-Agent improves MMAR accuracy from 27.20% to 53.60% using adaptive edge-cloud collaboration

## Executive Summary
This paper proposes CoFi-Agent, a hybrid edge-cloud architecture designed to address the tension between perception depth and computational efficiency in edge audio systems. The core idea is a coarse-to-fine workflow: a lightweight local Audio-LLM performs initial inference, and a cloud controller conditionally triggers targeted refinement via on-device tools like temporal re-listening and local ASR when uncertainty is detected. This approach avoids always-on tool usage, keeping raw audio on-device and transmitting only compact evidence. On the MMAR benchmark, CoFi-Agent improves accuracy from 27.20% to 53.60% and achieves a better accuracy-latency trade-off than an always-on pipeline, demonstrating that adaptive, conditional edge-cloud collaboration can significantly enhance edge audio reasoning under practical constraints.

## Method Summary
CoFi-Agent implements a three-stage pipeline: (1) edge coarse perception using Qwen2-Audio-7B-Instruct (FP16) on-device, (2) cloud-based adaptive confidence gating that escalates ~62% of samples for refinement, and (3) on-device tool execution (Whisper-small ASR and temporal re-listening) followed by cloud evidence integration using GPT-4o. The system transmits only text transcripts and tool summaries rather than raw audio, preserving privacy while enabling cloud-based reasoning. The segment proposer uses energy-based segmentation with sliding windows to identify regions-of-interest for targeted re-listening. Hardware requirements include NVIDIA Quadro RTX 6000 (24GB VRAM) with models kept warm in memory.

## Key Results
- Accuracy improvement from 27.20% to 53.60% on MMAR benchmark
- Better accuracy-latency trade-off compared to always-on tool pipeline
- ASR inclusion provides +10.5% accuracy gain for speech-heavy queries
- Raw audio remains on-device; only compact evidence transmitted to cloud

## Why This Works (Mechanism)

### Mechanism 1
Conditional escalation via uncertainty-based gating improves accuracy-efficiency trade-offs compared to always-on investigation. A prompt-based classifier G evaluates coarse perception outputs for hedging language, missing evidence, and logical inconsistencies, triggering refinement only when uncertainty is detected. Core assumption: Uncertainty cues in model outputs correlate meaningfully with actual reasoning errors. Break condition: High false-positive rates on low-SNR non-speech clips or missed confident hallucinations degrade efficiency gains.

### Mechanism 2
Tool-augmented re-listening and ASR recover fine-grained evidence that single-pass models miss. After gating, a cloud planner selects ROI and focused sub-query, with edge executing temporal re-listening or local ASR and returning compact evidence. Core assumption: Segment proposer P can identify temporally relevant regions using energy-based segmentation and sliding windows. Break condition: Missed brief decisive events in long recordings (>1 min) cause re-listening to focus on irrelevant regions.

### Mechanism 3
Symbolic transmission (transcripts, summaries) preserves acoustic privacy while enabling cloud-based reasoning. Raw audio never leaves device; only text outputs from ASR and tool summaries transmit to cloud reasoner. Core assumption: Transcripts contain sufficient semantic information for reasoning, and on-device PII redaction is feasible. Break condition: ASR produces corrupted transcripts under extreme noise (SNR < 0dB), misleading cloud reasoner with inaccurate evidence.

## Foundational Learning

- **Single-pass vs. iterative inference in Audio-LLMs**: Why needed here - CoFi-Agent explicitly addresses single-pass encoding limitation that cannot "go back" to verify temporally localized cues. Quick check: Can you explain why a single-pass model fails on queries requiring event ordering?

- **Cascade/Early-Exit architectures**: Why needed here - Coarse-to-fine design follows cascade principles - lightweight checks exit early, expensive tools invoke only for hard cases. Quick check: What is the trade-off between exit threshold strictness and overall system latency?

- **Edge-cloud latency budgets and RTT**: Why needed here - System targets ~15-45ms network RTT; understanding where latency accumulates (Table 2) is critical for real deployments. Quick check: Which component in Table 2 dominates end-to-end latency, and can it be parallelized?

## Architecture Onboarding

- **Component map**: Edge (Qwen2-Audio-7B-Instruct, Whisper-small ASR, segment proposer P) -> Cloud (GPT-4o controller for gating + planning + final verdict) -> Evidence integration

- **Critical path**: 1. Stage 0: Edge perception (0.16s), 2. Cloud gating (0.60s), 3. On-device tools (1.85s conditional), 4. Cloud reasoning (6.81s for escalated cases)

- **Design tradeoffs**: Always-on tools provide higher coverage but inject noise into easy samples and increase latency; adaptive gating offers better accuracy-efficiency curve but depends on gating classifier quality; ASR inclusion adds +10.5% accuracy for speech-heavy queries

- **Failure signatures**: ASR hallucination in near-silence or extreme noise; missed short decisive events in long clips due to heuristic segmentation; knowledge-mismatch queries requiring external information; confident hallucinations bypassing gating

- **First 3 experiments**: 1. Replicate gating threshold sweep measuring accuracy and latency at different escalation rates (40%, 62%, 80%) on held-out MMAR subset; 2. Ablate segment proposer by replacing energy-based + sliding-window with random ROI selection; 3. Stress-test privacy boundaries measuring PII leakage in transcripts and evaluating on-device redaction overhead

## Open Questions the Paper Calls Out

- Can replacing the prompt-based cloud controller with a learnable, on-device gating mechanism effectively minimize decision overhead and latency? The conclusion states future work will focus on "minimizing decision overhead via learnable gating." The current cloud-based LLM (GPT-4o) adds approximately 0.60 seconds of latency per sample (Table 2).

- Does jointly training the segment proposer with the reasoning module improve detection of brief but decisive events in long recordings? The conclusion proposes "jointly training the segment proposer with the reasoning module," addressing identified failure mode where "heuristic segment proposer may skip brief but decisive events." Current heuristics fail on clips longer than one minute.

- Can CoFi-Agent's conditional edge-cloud collaboration paradigm be successfully extended to bandwidth-constrained Edge Video scenarios? The conclusion lists "extending this paradigm to bandwidth-constrained Edge Video scenarios" as future work. Video data has significantly higher bandwidth requirements and different temporal complexity compared to audio.

## Limitations
- Exact prompt templates for confidence gate classifier are unspecified, potentially impacting accuracy-efficiency trade-off
- Energy-based segmentation algorithm lacks detail, with unverified effectiveness on diverse audio conditions
- System relies on GPT-4o for cloud reasoning, introducing API dependency and potential privacy concerns

## Confidence

- **High Confidence**: Coarse-to-fine workflow structure and rationale for addressing single-pass model limitations
- **Medium Confidence**: Reported accuracy-latency trade-off improvements, pending replication of gating mechanism
- **Low Confidence**: Privacy preservation claims without empirical validation of on-device PII redaction under varied conditions

## Next Checks
1. Replicate the gating threshold sweep to measure accuracy and latency at different escalation rates (e.g., 40%, 62%, 80%) on a held-out MMAR subset
2. Ablate the segment proposer by replacing energy-based + sliding-window with random ROI selection to quantify the value of focused re-listening
3. Stress-test privacy boundaries by measuring PII leakage in transcripts under controlled conditions and evaluating on-device redaction overhead