---
ver: rpa2
title: Steel-LLM:From Scratch to Open Source -- A Personal Journey in Building a Chinese-Centric
  LLM
arxiv_id: '2502.06635'
source_url: https://arxiv.org/abs/2502.06635
tags:
- data
- zhang
- steel-llm
- training
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Steel-LLM is a 1-billion-parameter Chinese-centric language model
  developed from scratch with limited computational resources (8 GPUs), demonstrating
  that high-quality LLMs can be built efficiently while maintaining full transparency.
  The model employs innovative techniques including Soft Mixture of Experts (Soft
  MoE) and an enhanced Feed-Forward Network to optimize performance within resource
  constraints.
---

# Steel-LLM:From Scratch to Open Source -- A Personal Journey in Building a Chinese-Centric LLM

## Quick Facts
- arXiv ID: 2502.06635
- Source URL: https://arxiv.org/abs/2502.06635
- Reference count: 40
- A 1-billion-parameter Chinese-centric language model built from scratch with limited computational resources (8 GPUs)

## Executive Summary
Steel-LLM is a 1-billion-parameter Chinese-centric language model developed from scratch using only 8 GPUs, demonstrating that high-quality LLMs can be built efficiently while maintaining full transparency. The model employs innovative techniques including Soft Mixture of Experts (Soft MoE) and an enhanced Feed-Forward Network to optimize performance within resource constraints. Steel-LLM achieved competitive results on Chinese benchmarks, scoring 41.90% on CEVAL and 36.08% on CMMLU, outperforming some early models from larger institutions. The complete training pipeline, datasets, and intermediate checkpoints were released, providing practical guidance for small-scale LLM development.

## Method Summary
Steel-LLM was developed using a combination of innovative architectural modifications and strategic training approaches. The model employs a Soft Mixture of Experts (Soft MoE) instead of traditional Sparse MoE to ensure full parameter training on limited GPU resources. An enhanced Feed-Forward Network was implemented by extending the SwiGLU activation function to the second layer of the MLP, improving non-linear representational capabilities. The pretraining corpus consisted of 80% Chinese and 20% English data, with a total of 30 billion tokens, carefully curated from diverse sources including SkyPile, Wanjuan, and specialized datasets. Supervised fine-tuning was conducted using 700,000+ high-quality instruction-response pairs, balancing the need for comprehensive training with resource constraints.

## Key Results
- Achieved 41.90% on CEVAL and 36.08% on CMMLU Chinese benchmarks
- Outperformed early models from larger institutions on comparable benchmarks
- Successfully maintained multilingual capabilities with 80% Chinese and 20% English data composition

## Why This Works (Mechanism)
Steel-LLM works by optimizing resource efficiency through architectural innovations that allow comprehensive parameter training on limited hardware. The Soft MoE approach ensures all parameters are trained effectively, avoiding the expert imbalance issues common with Sparse MoE on limited GPU setups. The enhanced FFN with double-layer SwiGLU activation improves the model's ability to capture complex patterns while maintaining computational efficiency. The balanced multilingual pretraining data composition (80% Chinese, 20% English) enables stable performance across both languages without sacrificing quality in either domain.

## Foundational Learning

**Soft Mixture of Experts (Soft MoE)**
Why needed: Enables efficient parameter utilization on limited GPU resources by allowing all parameters to be trained rather than having sparse expert routing
Quick check: Verify that all model parameters contribute to gradients during training

**SwiGLU Activation Function**
Why needed: Provides improved non-linear representational capabilities compared to traditional activation functions while maintaining computational efficiency
Quick check: Compare activation patterns and gradient flow between standard and SwiGLU layers

**Supervised Fine-Tuning (SFT)**
Why needed: Refines the pretrained model for specific tasks using high-quality instruction-response pairs
Quick check: Validate that fine-tuning data covers diverse task types and difficulty levels

## Architecture Onboarding

**Component Map**
Raw Data -> Preprocessing -> Pretraining (Soft MoE + Enhanced FFN) -> Supervised Fine-Tuning -> Evaluation

**Critical Path**
The most critical components are the Soft MoE implementation and the enhanced FFN with double-layer SwiGLU activation, as these directly determine the model's efficiency and representational capabilities within the 1B parameter constraint.

**Design Tradeoffs**
The primary tradeoff was between using Sparse MoE (which offers inference efficiency but requires more GPUs for training) versus Soft MoE (which trains all parameters but may have higher inference costs). The team chose Soft MoE to ensure complete training on their 8-GPU setup.

**Failure Signatures**
Common failure modes include expert imbalance in MoE layers, vanishing gradients in deep architectures, and data quality issues during preprocessing. The team mitigated these through careful initialization, gradient monitoring, and rigorous data cleaning.

**First 3 Experiments**
1. Baseline training without Soft MoE to establish performance floor
2. Ablation study comparing single-layer vs. double-layer SwiGLU activation
3. Scaling study varying the amount of SFT data to identify optimal training volume

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Does applying the SwiGLU activation function to the second layer of the MLP yield significant performance improvements over standard single-layer activation in 1-billion-parameter models?
- Basis in paper: Section 3.2 states the authors "extended the application of the SwiGLU activation function to the second layer," but the ablation studies focus on training speed and fine-tuning data strategies rather than isolating the specific contribution of this architectural modification.
- Why unresolved: While the paper claims this "enhances the model's non-linear representational capabilities," it does not provide a controlled comparison against a baseline model using standard FFN configurations to quantify the exact gain.
- What evidence would resolve it: An ablation study comparing the final benchmark scores of Steel-LLM with and without the second-layer SwiGLU activation, keeping all other parameters constant.

### Open Question 2
- Question: How does Soft Mixture of Experts (Soft MoE) compare to Sparse MoE in terms of performance trade-offs when training small-scale LLMs under identical GPU memory constraints?
- Basis in paper: Section 3.1 explains the choice of Soft MoE over Sparse MoE was driven by the need to "fully train each parameter" and avoid expert imbalance on 8 GPUs. However, it leaves open whether this choice sacrificed the potential inference efficiency or performance ceilings associated with Sparse MoE.
- Why unresolved: The paper posits that Soft MoE is preferable for their resource constraints but does not validate if a Sparse MoE model could have achieved higher benchmark scores if implemented with the same parameter budget.
- What evidence would resolve it: A comparative analysis of training convergence speed and final evaluation metrics between a 1B Soft MoE model and a compute-matched 1B Sparse MoE model.

### Open Question 3
- Question: To what extent does the documented domain imbalance in the pretraining corpus impact the model's performance on underrepresented tasks?
- Basis in paper: Section 5 explicitly lists as a limitation: "our data preprocessing workflow still exhibits certain limitations, particularly the lack of balance in the proportion of texts from different domains within the pre-training corpus."
- Why unresolved: The paper identifies the skew in data sources but does not analyze how this skew correlates with the model's weaker performance in specific domains.
- What evidence would resolve it: A detailed breakdown of benchmark performance categorized by domain, correlated against the token count distribution of the pretraining data for those specific domains.

### Open Question 4
- Question: What is the saturation point for Supervised Fine-Tuning (SFT) data volume for a 1-billion-parameter model before diminishing returns set in?
- Basis in paper: Section 6.1 discusses the debate between "less is more" and "large amount," noting that for Steel-LLM, "augmenting the fine-tuning phase with additional data was necessary." However, it does not define the upper limit of this benefit for small models.
- Why unresolved: The experiments show that adding more data improved results, but they do not establish if the 700k+ entries used were sufficient to reach the model's capacity limit.
- What evidence would resolve it: A scaling curve analysis showing validation loss and benchmark accuracy as the SFT dataset size is incrementally increased from thousands to several million samples.

## Limitations
- Lack of independent benchmarking verification for claimed performance metrics
- Model's performance on downstream tasks beyond reported benchmarks remains untested
- Specific implementation details of Soft MoE and enhanced FFN modifications not fully disclosed

## Confidence
- High confidence in technical feasibility of building 1B parameter Chinese-centric LLM with limited computational resources
- Medium confidence in reported benchmark performance scores due to lack of independent verification
- Medium confidence in claimed multilingual capabilities based on balanced data composition approach

## Next Checks
1. Independent third-party benchmarking of Steel-LLM on Chinese and English tasks to verify the reported CEVAL (41.90%) and CMMLU (36.08%) scores
2. Comparative efficiency analysis measuring actual GPU hours and computational resources used versus theoretical estimates provided
3. Reproducibility study attempting to recreate the Soft MoE and enhanced FFN components using the open-source materials to verify claimed performance improvements