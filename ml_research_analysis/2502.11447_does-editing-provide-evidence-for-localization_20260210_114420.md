---
ver: rpa2
title: Does Editing Provide Evidence for Localization?
arxiv_id: '2502.11447'
source_url: https://arxiv.org/abs/2502.11447
tags:
- heads
- localization
- arxiv
- evidence
- optimal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether editing-based interventions in
  language models provide evidence for localization of specific behaviors to particular
  model components. The authors develop a method to find optimal localized edits by
  adapting LLM alignment techniques, specifically using LoRA-based finetuning with
  a preference learning objective.
---

# Does Editing Provide Evidence for Localization?

## Quick Facts
- arXiv ID: 2502.11447
- Source URL: https://arxiv.org/abs/2502.11447
- Reference count: 6
- This paper finds that editing-based interventions in language models provide little evidence for localization of behaviors to specific components.

## Executive Summary
This paper investigates whether editing-based interventions provide evidence for localization of specific behaviors in language models. Using a method that adapts LLM alignment techniques with LoRA-based finetuning and preference learning, the authors study truthfulness localization in a 7B parameter model. They find that while localized edits can effectively steer model behavior, optimal edits at randomly selected attention heads often perform as well as edits at heads identified by probing as being related to truthfulness. This challenges the validity of current interpretability approaches that rely on editing-based evidence for localization claims.

## Method Summary
The authors develop a method to find optimal localized edits by adapting LLM alignment techniques using LoRA-based finetuning with a preference learning objective. They apply this to study truthfulness localization in a 7B parameter model. The approach involves training logistic regression probes on head activations to identify heads related to truthfulness, then comparing interventions at these probing-identified heads versus randomly selected heads. They use ITI (Intervention on the Mean) as a baseline and implement head-localized rank-1 LoRA with reparameterization. The method is evaluated on the TruthfulQA dataset using paired preference data and multiple metrics including Info*Truth, KL divergence, and multiple-choice accuracy.

## Key Results
- Localized edits can effectively steer model behavior toward truthfulness
- Optimal edits at randomly selected attention heads often perform as well as edits at probing-identified heads
- This holds even when considering single heads or very constrained edits
- Success of editing-based interventions does not reliably indicate that target behavior is encoded in edited locations

## Why This Works (Mechanism)
The effectiveness of editing-based interventions in steering model behavior stems from the ability of LoRA-based finetuning to find directions in weight space that optimize the preference objective. The method works by projecting weight updates onto specific heads while maintaining the rest of the model unchanged. However, the mechanism by which these edits improve performance appears to be largely independent of the specific localization identified by probing, suggesting that the model's behavior is distributed across many components rather than localized to specific ones.

## Foundational Learning
- **LoRA (Low-Rank Adaptation)**: Parameter-efficient finetuning method that updates weights via low-rank decomposition. Needed for efficient localized editing without full model retraining. Quick check: Verify rank-1 decomposition correctly restricts updates to target heads.
- **Preference Learning with IPO**: Direct preference optimization using paired comparison data. Needed to steer model behavior toward desired outputs. Quick check: Confirm preference loss gradients properly backpropagate through LoRA parameters.
- **Attention Head Probing**: Using logistic regression to identify heads correlated with target behavior. Needed to establish baseline for localization claims. Quick check: Validate probing accuracy correlates with actual behavioral impact.
- **Intervention on the Mean (ITI)**: Baseline method that shifts activation distributions based on mean differences. Needed for comparison against optimal localized edits. Quick check: Verify ITI implementation matches original specification.

## Architecture Onboarding
- **Component map**: Model (Alpaca-7B) -> Attention Heads -> Probe Training -> LoRA Edit Training -> Evaluation
- **Critical path**: Head activations → probe training → edit optimization → behavioral evaluation
- **Design tradeoffs**: Full-model editing vs. head-localized editing (effectiveness vs. interpretability), probe-based selection vs. random selection (theoretically grounded vs. baseline comparison)
- **Failure signatures**: Random-head edits matching probing-identified edits suggests either probe ineffectiveness or distributed behavior encoding
- **3 first experiments**: 1) Train probes and verify they identify heads with higher truthfulness correlation than random, 2) Implement ITI baseline and confirm it improves truthfulness scores, 3) Run IPO with full model and verify it outperforms localized edits

## Open Questions the Paper Calls Out
None

## Limitations
- Underspecified implementation details for ITI baseline and probing training procedure
- Dependence on GPT-judge evaluation without full finetuning specification
- Results may reflect limitations in probing method rather than disproving localization

## Confidence
- **High confidence**: Localized LoRA edits can effectively steer model behavior toward truthfulness
- **Medium confidence**: Effectiveness of editing does not reliably indicate localization of truthfulness to edited locations
- **Low confidence**: Specific quantitative comparisons between ITI and IPO methods due to underspecified implementation details

## Next Checks
1. Reproduce the probing step with full hyperparameter specification to verify that probing-identified heads genuinely capture truthfulness better than chance
2. Implement the ITI mass-mean-shift intervention with explicit normalization and scaling parameters to confirm baseline performance
3. Run ablation studies with different random head selections and probe training seeds to establish whether random-head performance is robust or sensitive to initialization