---
ver: rpa2
title: 'GraphAllocBench: A Flexible Benchmark for Preference-Conditioned Multi-Objective
  Policy Learning'
arxiv_id: '2601.20753'
source_url: https://arxiv.org/abs/2601.20753
tags:
- objective
- problem
- pareto
- learning
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GraphAllocBench introduces a novel, flexible benchmark for Preference-Conditioned
  Policy Learning (PCPL) in Multi-Objective Reinforcement Learning (MORL), addressing
  the lack of realistic, scalable testbeds. It features CityPlannerEnv, a graph-based
  resource allocation environment inspired by city management, where agents balance
  competing objectives like congestion, growth, and sustainability by allocating resources
  across demands represented as bipartite graphs.
---

# GraphAllocBench: A Flexible Benchmark for Preference-Conditioned Multi-Objective Policy Learning

## Quick Facts
- arXiv ID: 2601.20753
- Source URL: https://arxiv.org/abs/2601.20753
- Reference count: 21
- Primary result: Introduces a novel benchmark with 100+ diverse problems for Preference-Conditioned Policy Learning (PCPL) in Multi-Objective Reinforcement Learning (MORL), featuring CityPlannerEnv and novel evaluation metrics.

## Executive Summary
GraphAllocBench introduces a novel, flexible benchmark for Preference-Conditioned Policy Learning (PCPL) in Multi-Objective Reinforcement Learning (MORL), addressing the lack of realistic, scalable testbeds. It features CityPlannerEnv, a graph-based resource allocation environment inspired by city management, where agents balance competing objectives like congestion, growth, and sustainability by allocating resources across demands represented as bipartite graphs. The benchmark includes 100+ diverse problems with varying objective functions, dependency structures, and preference conditions, along with two novel evaluation metrics: Proportion of Non-Dominated Solutions (PNDS) and Ordering Score (OS), complementing the standard hypervolume metric. Experiments show that a PCPL approach using Heterogeneous Graph Neural Networks (HGNNs) significantly outperforms Multi-Layer Perceptron (MLP) baselines, particularly on large-scale problems with 100 demands and resources.

## Method Summary
GraphAllocBench provides CityPlannerEnv, a bipartite graph-based resource allocation environment where agents allocate resources to demands while balancing competing objectives based on user preference weights. The benchmark includes 17 problems (0-6) with varying demands (2-100), resources, and objectives (2-5). Agents use PPO with Smooth Tchebycheff Scalarization, receiving concatenated allocation matrices and preference vectors as input. Two feature extractors are compared: MLP (128-dim hidden, SiLU, 2 layers) and HGNN with attention pooling. Preferences are sampled from flat Dirichlet distribution during training. The evaluation uses three metrics: Hypervolume ratio vs. ideal Pareto front, Proportion of Non-Dominated Solutions (PNDS), and Ordering Score (OS) via Spearman rank correlation.

## Key Results
- HGNN+AttentionPool achieves 30.2±2.4 HV×10⁶ vs MLP 5.5±0.9 on Problem 6c (100 demands)
- MLP achieves higher OS (0.84-0.88) than HGNN (0.65-0.75) on large problems, suggesting more stable preference mapping
- Problems 1c and 3b show degraded performance due to non-convex/discontinuous Pareto fronts
- Benchmark exposes limitations of existing MORL approaches and highlights potential of graph-based methods

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Preference conditioning enables a single policy to approximate the full Pareto front by adapting behavior at inference time without retraining.
- **Mechanism:** A preference vector $\mathbf{w} = [w_0, \ldots, w_{N-1}]$ is concatenated to observations and fed through the network. During training, preferences are sampled from a flat Dirichlet distribution; the scalarized reward $J(\mathbf{P}) \cdot \mathbf{w}$ (via Smooth Tchebycheff) creates a gradient signal that teaches the policy to map preference-to-behavior. At inference, arbitrary $\mathbf{w}$ produces corresponding trade-offs.
- **Core assumption:** The policy network has sufficient capacity to learn a smooth mapping from continuous preference space to discrete allocation decisions, and the scalarization function accurately reflects true trade-offs.
- **Evidence anchors:**
  - [abstract] "This enables a single model to flexibly adapt to arbitrary trade-offs at run-time by producing a policy on or near the Pareto front."
  - [Section 2.2] "PCPL methods use a continuous, user-defined preference as input to a single policy... allows us to learn a preference-conditioned policy."
  - [corpus] Related work (Pareto Set Learning for MORL, arXiv:2501.06773) confirms preference-conditioned approaches as the dominant paradigm for scalable MORL.
- **Break condition:** Non-convex or discontinuous Pareto fronts cause scalarization to fail; the paper notes Problems 1c and 3b show degraded performance on such fronts.

### Mechanism 2
- **Claim:** Heterogeneous Graph Neural Networks (HGNNs) outperform MLPs on large-scale allocation by explicitly encoding resource-demand dependency structure.
- **Mechanism:** The bipartite graph (resources $\leftrightarrow$ demands) is processed through stacked Heterogeneous Graph Attention layers, where each node type (Resource/Demand/Unallocated) has dedicated attention mechanisms. Preference-conditioned multi-head attention pooling aggregates node embeddings into a global representation that preserves structural information better than flattening.
- **Core assumption:** The dependency graph structure is informative for optimal allocation decisions; flattening destroys this signal.
- **Evidence anchors:**
  - [abstract] "a PCPL approach using Heterogeneous Graph Neural Networks (HGNNs) significantly outperforms Multi-Layer Perceptron (MLP) baselines, particularly on large-scale problems with 100 demands and resources."
  - [Table 2] HGNN+AttentionPool achieves 30.2±2.4 HV×10⁶ vs MLP 5.5±0.9 on Problem 6c (100 demands).
  - [corpus] Weak direct corpus evidence on HGNN specifically for MORL; related work focuses on standard GNNs for combinatorial optimization.
- **Break condition:** For small graphs (2-5 demands), MLP convergence is faster and more stable; HGNN overhead may not justify gains.

### Mechanism 3
- **Claim:** The Ordering Score (OS) metric captures preference-consistency that Hypervolume alone misses.
- **Mechanism:** OS computes Spearman rank correlation between preference weights $w_i$ and resulting objective values $J_i(\mathbf{P})$ across swept preferences. High OS indicates the agent respects input preferences (e.g., higher $w_i$ → higher $J_i$), even if solutions aren't Pareto-optimal.
- **Core assumption:** PCPL should be evaluated not just on proximity to Pareto front (HV) but on faithfulness to preference inputs.
- **Evidence anchors:**
  - [Section 4.3] "the ordering score becomes an important way to evaluate how well an agent follows the preference it is given."
  - [Table 2] MLP achieves higher OS (0.84-0.88) than HGNN (0.65-0.75) on Problems 6a-c, suggesting MLP finds more stable local solutions even with worse global performance.
  - [corpus] No prior corpus evidence; OS appears novel to this benchmark.
- **Break condition:** OS does not capture magnitude of objective values, only rank ordering; high OS with low HV indicates preference-aligned but suboptimal solutions.

## Foundational Learning

- **Concept: Pareto Optimality and Dominance**
  - Why needed here: The entire benchmark is designed around approximating Pareto fronts. Without understanding that a solution $x_1$ dominates $x_2$ iff $f_i(x_1) \geq f_i(x_2)$ for all objectives with strict inequality for at least one, you cannot interpret HV, PNDS, or the core problem formulation.
  - Quick check question: Given two solutions with objective vectors $(5, 3)$ and $(4, 4)$, does either dominate the other?

- **Concept: Scalarization Functions (Weighted Sum vs. Tchebycheff)**
  - Why needed here: The paper uses Smooth Tchebycheff specifically because weighted-sum scalarization cannot reach non-convex regions of the Pareto front. Understanding this limitation explains why Problems 1c and 3b are challenging.
  - Quick check question: Why would weighted-sum scalarization fail to find solutions on a concave portion of a Pareto front?

- **Concept: Graph Attention Networks and Pooling**
  - Why needed here: The HGNN architecture uses Graph Attention layers with preference-conditioned attention pooling. You need to understand how attention weights are computed over nodes and how pooling aggregates node-level representations into graph-level features for the policy head.
  - Quick check question: How does attention pooling differ from simple mean pooling, and why would conditioning attention on preferences improve preference-awareness?

## Architecture Onboarding

- **Component map:** Environment (CityPlannerEnv) -> Feature Extractor (MLP or HGNN) -> Policy/Value Heads -> PPO Training Loop -> Evaluation: HV + PNDS + OS metrics

- **Critical path:** The HGNN feature extractor is the key architectural decision. Start with the MLP baseline (Section 5, Figure 4) to establish baseline performance, then swap to HGNN for Problems 6a-c where graph scale makes structure exploitable.

- **Design tradeoffs:**
  | Choice | Pro | Con |
  |--------|-----|-----|
  | MLP feature extractor | Fast convergence, stable on small graphs | Quadratic parameter scaling, no structural bias |
  | HGNN + MeanMaxPool | Better HV on large graphs, fewer parameters | Lower OS (less stable preference mapping) |
  | HGNN + AttentionPool | Best HV, preference-aware pooling | More complex, slightly lower OS than MLP |

- **Failure signatures:**
  - Low PNDS + High HV: Agent occasionally finds good solutions but unreliably—check for local optima traps (Problem 2b).
  - High OS + Low HV: Agent respects preferences but solutions are far from Pareto front—likely insufficient exploration or suboptimal scalarization.
  - High variance across seeds (Problems 1a-c): Sharp reward discontinuities causing learning instability.

- **First 3 experiments:**
  1. **Reproduce MLP baseline on Problem 0:** Train PPO with MLP feature extractor on the simplest problem (2 demands, 2 objectives, convex Pareto front). Target: HV ratio >0.95, OS >0.95 within 500K steps.
  2. **Ablate scalarization on Problem 1c:** Compare weighted-sum vs. Smooth Tchebycheff scalarization on a non-convex Pareto front. Expect weighted-sum to miss concave regions.
  3. **Scale test MLP vs. HGNN on Problem 6a:** Train both architectures on 100-demand graphs. Verify HGNN achieves ~2-4× HV improvement per Table 2. Monitor training stability differences.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can environmental uncertainty, such as natural disasters, be integrated into GraphAllocBench to evaluate risk-aware decision-making?
- **Basis in paper:** [explicit] The Conclusion states future work will extend the benchmark with "environmental uncertainty – such as simulating natural disasters – to evaluate risk-aware decision-making."
- **Why unresolved:** The current environment models sequential allocation but does not incorporate stochastic environmental shocks or varying risk profiles.
- **What evidence would resolve it:** Performance analysis of policies under stochastic events using new risk-aware metrics.

### Open Question 2
- **Question:** Can HGNN architectures be optimized to achieve high Ordering Scores (preference alignment) while maintaining their Hypervolume advantage over MLPs?
- **Basis in paper:** [inferred] Section 6.2 notes that while HGNNs significantly outperform MLPs on Hypervolume for large graphs, MLPs consistently achieve better Ordering Scores.
- **Why unresolved:** It is unclear if the graph pooling methods dilute the specific preference signal or if the architecture requires specific modifications for alignment.
- **What evidence would resolve it:** An HGNN model that matches or exceeds MLP Ordering Scores without sacrificing Hypervolume performance.

### Open Question 3
- **Question:** How can scalarization methods be advanced to better approximate highly non-convex and discontinuous Pareto Fronts in PCPL?
- **Basis in paper:** [inferred] Section 5 states that even Smooth Tchebycheff Scalarization "still has limitations when the Pareto front is non-convex and discontinuous," leading to poor performance on Problems 1c and 3b.
- **Why unresolved:** Current gradient-based methods struggle to guide policies effectively when optimal solutions are separated by discontinuities.
- **What evidence would resolve it:** A new scalarization technique achieving significantly higher HV ratios on the benchmark's oscillatory or spiked objective functions.

## Limitations
- HGNN performance gains lack ablation studies on architectural choices (GAT, GCN, GIN variants).
- Ordering Score metric lacks comparative validation against alternative preference-fidelity metrics.
- Benchmark focuses exclusively on city planning scenarios without validation on other MORL problem classes.

## Confidence
- **High confidence:** PCPL framework with preference conditioning enables single-model Pareto front approximation.
- **Medium confidence:** HGNN outperforms MLP on large graphs due to structural bias.
- **Medium confidence:** OS metric captures preference-consistency beyond HV.

## Next Checks
1. **Ablation Study on Graph Architectures:** Compare HGNN with alternative GNN variants (GAT, GCN, GIN) and pooling strategies on Problems 6a-c to isolate whether performance gains stem from heterogeneous processing, attention mechanisms, or graph structure exploitation.
2. **Cross-Domain Validation:** Apply the same HGNN+PCPL framework to non-city-planning MORL benchmarks (e.g., resource collection from COMO or maGym) to test whether graph-based advantages transfer beyond allocation problems.
3. **Preference-Fidelity Analysis:** Design experiments where preferences are deliberately mismatched (e.g., weight 0.9 on an objective the agent cannot improve) to test whether OS captures genuine preference-following or optimization artifacts, and compare OS against alternative metrics like preference-weighted regret.