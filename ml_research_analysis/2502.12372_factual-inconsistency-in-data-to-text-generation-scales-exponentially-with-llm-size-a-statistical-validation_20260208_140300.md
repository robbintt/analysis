---
ver: rpa2
title: 'Factual Inconsistency in Data-to-Text Generation Scales Exponentially with
  LLM Size: A Statistical Validation'
arxiv_id: '2502.12372'
source_url: https://arxiv.org/abs/2502.12372
tags:
- scaling
- power
- exponential
- size
- family
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how factual inconsistency in data-to-text
  generation scales with large language model (LLM) size. Previous research focused
  on power law scaling of generalization error, but this work examines both power
  law and exponential scaling for factual inconsistency using a rigorous three-stage
  statistical validation framework (predictive performance estimation, goodness-of-fit
  assessment, and comparative analysis).
---

# Factual Inconsistency in Data-to-Text Generation Scales Exponentially with LLM Size: A Statistical Validation

## Quick Facts
- **arXiv ID**: 2502.12372
- **Source URL**: https://arxiv.org/abs/2502.12372
- **Reference count**: 40
- **Primary result**: Factual inconsistency in data-to-text generation follows exponential (not power law) scaling with LLM size, validated through a three-stage statistical framework

## Executive Summary
This paper investigates how factual inconsistency in data-to-text generation scales with large language model (LLM) size, challenging the widely assumed power law scaling. Through rigorous statistical validation across three LLM families (Pythia, OPT, BLOOM) and five datasets, the authors demonstrate that factual inconsistency decreases exponentially with model size rather than following the previously assumed power law relationship. The study introduces a three-stage statistical validation framework that prevents spurious scaling law conclusions and reveals important interactions between decoding strategies and dataset characteristics.

## Method Summary
The study fine-tunes three LLM families (Pythia, OPT, BLOOM) on five D2T datasets using QLoRA, then evaluates factual inconsistency using four automated metrics. Scaling relationships are modeled using both power law (Ax^α + B) and exponential (Ce^βx + D) forms, with parameters estimated via maximum likelihood using Huber loss. A three-stage validation framework ensures robust conclusions: Stage I tests predictive performance via 5-fold cross-validation, Stage II assesses goodness-of-fit using F-tests, and Stage III compares non-nested models using Vuong's likelihood-ratio test. Residual normality is verified via Shapiro-Wilk test before proceeding to goodness-of-fit and model comparison stages.

## Key Results
- Factual inconsistency decreases exponentially with LLM size, with decay rate β typically ranging from -1.8 to -0.6
- The three-stage statistical validation framework prevents false positive scaling law conclusions
- Deterministic decoding strategies (greedy, beam) show inverted scaling on closed-domain datasets (E2E, ViGGO), where inconsistency increases with model size

## Why This Works (Mechanism)

### Mechanism 1: Exponential Decay of Factual Inconsistency with Model Scale
The relationship between model size and factual inconsistency follows Ce^βx + D where β typically ranges from -1.8 to -0.6 (negative decay rate). This produces steep initial gains in factual consistency at smaller model sizes, then plateaus—contrasting with power law's shallower, more sustained improvement curve. The core assumption is that the four automated consistency metrics faithfully proxy human factual consistency judgments across all tested domains.

### Mechanism 2: Three-Stage Statistical Validation Prevents Overconfident Scaling Claims
A sequential validation pipeline (predictive performance → goodness-of-fit → model comparison) prevents spurious scaling law conclusions when data points are limited. Stage I (5-fold CV Huber loss) tests generalization to held-out data. Stage II (F-test at p<0.05) verifies the model explains variance beyond a trivial mean baseline. Stage III (Vuong's likelihood-ratio test at p<0.005) compares non-nested power law vs exponential models directly. The core assumption is that residuals from both scaling models follow normal distributions.

### Mechanism 3: Decoding Strategy Modulates Scaling Behavior on Closed-Domain Tasks
Stochastic decoding (nucleus, top-k) shows monotonic improvement in factual consistency as model size grows, but deterministic decoding (greedy, beam) can exhibit inverted scaling on closed-domain datasets. Greedy/beam search selects highest-probability tokens, potentially reinforcing spurious patterns memorized from training data. For highly structured closed-domain datasets (E2E, ViGGO), larger models may overfit to these patterns, degrading factual consistency.

## Foundational Learning

- **Concept: Power Law vs Exponential Scaling Forms**
  - Why needed here: Misidentifying the scaling form leads to incorrect extrapolation—power law (Ax^α + B) predicts gradual continued improvement at scale; exponential (Ce^βx + D) predicts rapid saturation.
  - Quick check question: If exponential scaling has β = -1.5, by what factor does inconsistency decrease when model size increases from 1B to 3B parameters?

- **Concept: Non-Nested Hypothesis Testing (Vuong's Test)**
  - Why needed here: Power law and exponential models are not nested (one cannot be derived from the other via parameter constraints), so standard likelihood-ratio tests fail. Vuong's test compares their explanatory power directly.
  - Quick check question: Why would comparing AIC/BIC alone be insufficient for selecting between power law and exponential fits?

- **Concept: Factual Consistency Evaluation Paradigms**
  - Why needed here: Each metric has distinct failure modes—ALIGN SCORE uses information alignment, QAFACTEVAL uses question-answering, SUMMA C-CONV uses NLI, UNIEVAL-FACT uses unified training. Understanding these clarifies when findings generalize.
  - Quick check question: What type of factual inconsistency would QAFACTEVAL miss that SUMMA C-CONV might catch?

## Architecture Onboarding

- **Component map**: Datasets (E2E, ViGGO, WebNLG, DART, WikiTableText) → Fine-tune LLMs (Pythia, OPT, BLOOM) via QLoRA → Generate text with different decoding strategies → Compute consistency scores (ALIGN SCORE, QAFACTEVAL, SUMMA C-CONV, UNIEVAL-FACT) → Fit scaling models (power law, exponential) → Validate via 3-stage framework (CV Huber loss → F-test → Vuong's test)

- **Critical path**: Collect inconsistency scores across model sizes → fit both scaling models → verify residual normality (Shapiro-Wilk) → run Stage I (CV Huber loss) → if passing, run Stage II (F-test) → if both models pass, run Stage III (Vuong) → report preferred model with β/α values and confidence intervals

- **Design tradeoffs**: Huber loss (δ=1) provides outlier robustness but may underweight genuinely extreme behavior; QLoRA trades full model adaptation for compute feasibility; stringent Stage III threshold (p<0.005) reduces false positives but may miss weak preferences

- **Failure signatures**: High Huber loss on held-out data (red values in tables) → model doesn't generalize; failed F-test despite low Huber loss (common in BLOOM) → good prediction but poor structural fit; positive β values → inconsistency increases with size (aberrant, seen with deterministic decoding on E2E/ViGGO); wide MOE bands → insufficient model checkpoints or high variance in consistency scores

- **First 3 experiments**:
  1. Replicate nucleus sampling results on Pythia-70M through Pythia-12B across DART and WebNLG using ALIGN SCORE, confirming β falls in [-1.8, -0.6] range.
  2. Add a fourth model family (e.g., LLaMA-3.2 1B/3B variants) to test whether exponential scaling generalizes beyond the tested architectures and training corpora.
  3. Run controlled ablation: compare Vuong test outcomes with vs. without Stage II filtering to quantify how often "good prediction, bad fit" cases would have caused incorrect model selection.

## Open Questions the Paper Calls Out

- **Do the observed exponential scaling laws hold for in-context learning or prompting-based strategies?**
  - The paper states "in-context learning and prompting strategies are not considered in this study, which we leave as future work." The study exclusively utilized parameter-efficient fine-tuning (QLoRA), leaving the behavior of frozen models using prompts unknown.

- **Do human evaluations of factual consistency align with the exponential scaling observed via automated metrics?**
  - The limitations section notes the "reliance on automated metrics without human evaluation" as a constraint, despite strong correlations with human judgment. There is a risk that the exponential scaling is an artifact of the specific automatic metrics rather than human perception of consistency.

- **Why does factual inconsistency increase with model size when using deterministic decoding strategies on closed-domain datasets?**
  - The appendix identifies "aberrant behavior" in the E2E and ViGGO datasets where inconsistency rises with size under greedy/beam search. The authors hypothesize deterministic decoding bias or overfitting but do not isolate the causal mechanism.

- **Is there a theoretical mechanism explaining why factual inconsistency follows exponential rather than power-law scaling?**
  - The limitations state the findings are "based on empirical observations" and lack "formal theoretical guarantee." Without a theoretical basis, it is difficult to predict if this trend is universal or specific to current model architectures.

## Limitations

- **Metric reliability across domains**: The core conclusion depends on four automated consistency metrics serving as valid proxies for human factual consistency judgments, which were not independently validated against human annotations in this work.

- **Architecture and corpus specificity**: The exponential scaling pattern was observed specifically for decoder-only LLM families trained on similar web-scale corpora, and may not generalize to other model families or training domains.

- **Limited model checkpoint sampling**: Each family has limited size granularity (5-8 checkpoints), potentially insufficient to capture the true scaling relationship, especially for the BLOOM family which showed elevated failure rates.

## Confidence

- **High confidence**: The three-stage statistical validation framework itself is sound and well-executed, successfully identifying cases where power law would be incorrectly selected based on predictive performance alone.

- **Medium confidence**: The exponential scaling finding for stochastic decoding across the three tested model families and five datasets, though dependent on metric validity and specific architectures tested.

- **Low confidence**: The deterministic decoding aberration on closed-domain datasets, where the empirical observation is clear but the proposed mechanism remains speculative.

## Next Checks

1. **Human evaluation validation**: Conduct human annotation study on a representative subset (e.g., 100 samples each from DART, E2E, and WikiTableText) using expert judges to rate factual consistency. Compare human scores against the four automated metrics to quantify correlation and identify systematic divergences.

2. **Architecture generalization test**: Extend the framework to a fourth model family with different architecture (e.g., LLaMA-3.2 variants: 1B, 3B, 8B, 70B) and different training corpus. Apply the same fine-tuning and consistency evaluation pipeline to test whether exponential scaling holds beyond decoder-only models trained on web data.

3. **Decoding strategy ablation**: Systematically vary decoding parameters (nucleus p=0.5,0.7,0.9; beam sizes 1,3,5,10; top-k k=20,50,100,640) across all model sizes for E2E dataset. Plot consistency vs. model size for each configuration to identify the precise conditions under which deterministic decoding shows inverted scaling.