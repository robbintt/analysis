---
ver: rpa2
title: 'IDEAlign: Comparing Large Language Models to Human Experts in Open-ended Interpretive
  Annotations'
arxiv_id: '2509.02855'
source_url: https://arxiv.org/abs/2509.02855
tags:
- similarity
- feedback
- data
- human
- expert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces IDEAlign, a scalable benchmarking paradigm
  for evaluating large language models (LLMs) on open-ended, interpretive annotation
  tasks. These tasks require expert-level judgments grounded in specific objectives,
  such as thematic analysis or generating feedback, where no single "correct" response
  exists.
---

# IDEAlign: Comparing Large Language Models to Human Experts in Open-ended Interpretive Annotations

## Quick Facts
- arXiv ID: 2509.02855
- Source URL: https://arxiv.org/abs/2509.02855
- Authors: Hyunji Nam; Lucia Langlois; James Malamut; Mei Tan; Dorottya Demszky
- Reference count: 40
- Primary result: IDEAlign improves LLM alignment with expert judgments by 9-30% over traditional lexical and embedding baselines

## Executive Summary
IDEAlign introduces a scalable benchmarking paradigm for evaluating large language models (LLMs) on open-ended, interpretive annotation tasks. These tasks require expert-level judgments grounded in specific objectives, such as thematic analysis or generating feedback, where no single "correct" response exists. By eliciting expert similarity judgments via a pick-the-odd-one-out triplet task and using the same format to prompt LLMs, IDEAlign enables direct comparison between human and model judgments within a shared protocol. Across two educational datasets, IDEAlign significantly outperforms traditional lexical and vector-based metrics, achieving 9-30% higher rank correlations with expert judgments.

## Method Summary
IDEAlign elicits expert similarity judgments via a pick-the-odd-one-out triplet task, avoiding the cognitive burden of absolute rating scales. By aggregating many triplet decisions, it generates calibrated pairwise similarity scores. The same triplet format is used to prompt LLMs, enabling direct comparison between human and model judgments within a shared protocol. Across two educational datasets—assessing student mathematical reasoning and providing essay feedback—traditional lexical and vector-based metrics (BLEU, embeddings, topic models) show poor alignment with expert judgments, often conflating style with substance. In contrast, prompting LLMs via IDEAlign significantly improves alignment with expert judgments, achieving 9-30% higher rank correlations than baselines.

## Key Results
- IDEAlign achieves 9-30% higher Spearman rank correlations with expert judgments compared to BLEU, embeddings, and topic modeling baselines
- Traditional lexical and vector-based metrics often conflate style with substance, showing poor alignment with expert judgments
- Triplet-based LLM prompting via IDEAlign demonstrates significant improvement in expert alignment for open-ended interpretive tasks

## Why This Works (Mechanism)
IDEAlign works by reframing the problem of measuring interpretive alignment from an absolute rating task to a relative similarity judgment. The pick-the-odd-one-out triplet task is cognitively simpler for experts than providing absolute scores, reducing cognitive burden and potential bias. By aggregating many triplet decisions, IDEAlign generates calibrated pairwise similarity scores that reflect expert consensus. Using the same triplet format to prompt LLMs creates a shared evaluation protocol, enabling direct comparison between human and model judgments. This approach is particularly effective for open-ended tasks where traditional metrics fail to capture nuanced interpretive judgments.

## Foundational Learning
- **Triplet judgment aggregation**: Experts provide pairwise similarity judgments through pick-the-odd-one-out tasks. Why needed: Simplifies complex interpretive judgments. Quick check: Validate aggregation formula produces stable similarity scores with sufficient triplet samples.
- **Expert consensus measurement**: Aggregate triplet decisions into pairwise similarity scores using the formula sim(A,B) = count(neither A nor B selected) / count(triplets containing A and B). Why needed: Quantifies expert agreement on similarity. Quick check: Bootstrap analysis confirms 300-600 triplets provide stable scores.
- **Cross-metric comparison**: Compare LLM similarity rankings against expert rankings using Spearman rank correlation. Why needed: Quantifies alignment between model and expert judgments. Quick check: Correlate rankings across different baseline metrics to establish reference points.

## Architecture Onboarding

**Component Map**: Expert triplets -> Similarity aggregation -> Expert rankings -> LLM prompts (triplet format) -> LLM rankings -> Spearman correlation comparison

**Critical Path**: Expert triplet judgments → Pairwise similarity scores → LLM triplet prompts → Similarity rankings → Rank correlation with experts

**Design Tradeoffs**: Triplet format reduces cognitive burden vs. potential information loss compared to absolute ratings; expert sampling prioritization improves coverage of under-represented pairs vs. uniform sampling simplicity

**Failure Signatures**: Embedding metrics inflate similarity due to style/length (20-85% false increases); topic modeling yields unstable correlations across hyperparameters (0.131-0.518 range)

**First Experiments**:
1. Collect 300-600 expert triplet judgments per dataset, validate aggregation stability via bootstrap
2. Run baseline metrics (BLEU, embeddings, topic models) and compute Spearman correlation with expert rankings
3. Prompt LLMs in triplet format and compare correlation improvements vs. baselines

## Open Questions the Paper Calls Out
None

## Limitations
- Results depend on specific datasets, expert pools, and prompt designs that may not generalize to other domains
- Aggregation formula for pairwise similarity from triplet data is sensitive to sampling imbalance
- Paper does not address potential bias from prompt phrasing or the effect of different triplet sampling strategies
- Embedding and topic model baselines are shown to conflate style with substance, but the extent of this bias is task- and dataset-specific

## Confidence
- Claim that IDEAlign achieves 9-30% higher rank correlation than baselines: **Medium** (strong in-sample results, limited external validation)
- Claim that embedding/topic models conflate style with substance: **High** (clearly demonstrated with specific interventions)
- Claim that triplet aggregation reliably reflects expert consensus: **Medium** (validated via bootstrap but not via external criteria)

## Next Checks
1. Replicate IDEAlign on a third, distinct domain (e.g., medical chart review or legal document summarization) with at least 300 expert triplets to test generalization of alignment improvements
2. Conduct ablation studies varying triplet sampling strategy (uniform vs. prioritized) and prompt phrasing to quantify sensitivity to design choices
3. Perform inter-rater reliability analysis on triplet judgments to bound the impact of expert disagreement on final similarity scores