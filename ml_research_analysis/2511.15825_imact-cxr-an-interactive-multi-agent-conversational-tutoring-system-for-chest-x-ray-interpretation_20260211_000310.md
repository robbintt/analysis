---
ver: rpa2
title: IMACT-CXR - An Interactive Multi-Agent Conversational Tutoring System for Chest
  X-Ray Interpretation
arxiv_id: '2511.15825'
source_url: https://arxiv.org/abs/2511.15825
tags:
- reasoning
- knowledge
- gaze
- mastery
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: IMACT-CXR is a multi-agent conversational tutor for chest X-ray
  interpretation that integrates spatial annotation, gaze analysis, knowledge retrieval,
  and vision-language reasoning within a single AutoGen workflow. The system validates
  learner bounding boxes, tracks eye gaze using a TensorFlow U-Net segmentation model,
  and maintains skill mastery estimates via Bayesian Knowledge Tracing to guide feedback
  delivery.
---

# IMACT-CXR - An Interactive Multi-Agent Conversational Tutoring System for Chest X-Ray Interpretation

## Quick Facts
- arXiv ID: 2511.15825
- Source URL: https://arxiv.org/abs/2511.15825
- Reference count: 0
- One-line primary result: Multi-agent CXR tutor with gaze analytics, mastery tracking, and reasoning guidance achieves 59% mean IoU and 71% diagnostic accuracy, outperforming text-based tutor and Radiology-GPT in preliminary evaluation.

## Executive Summary
IMACT-CXR is a multi-agent conversational tutoring system for chest X-ray interpretation that integrates spatial annotation, gaze analysis, knowledge retrieval, and vision-language reasoning within a single AutoGen workflow. The system validates learner bounding boxes, tracks eye gaze using a TensorFlow U-Net segmentation model, and maintains skill mastery estimates via Bayesian Knowledge Tracing to guide feedback delivery. Specialized agents retrieve PubMed evidence, suggest similar cases from REFLACX, and trigger NV-Reason-CXR-3B for image-grounded reasoning when mastery is low or explicitly requested.

## Method Summary
IMACT-CXR uses an AutoGen sequential workflow with Focus Gate, Assessment + Mastery (BKT), Routing, and specialized agents (KnowledgeBase, Reasoning, CaseSimilarity, SocraticTutor) culminating in FacultyStyleResponder. Learner inputs include bounding boxes, gaze samples, and free-text observations. The system validates spatial annotations via IoU threshold, maps gaze to lung lobes via U-Net segmentation, updates per-skill mastery via Bayesian Knowledge Tracing, and orchestrates agent escalation based on mastery thresholds. NV-Reason-CXR-3B runs locally for reasoning triggers; PubMed E-utilities retrieves evidence; REFLACX cases provide similarity suggestions.

## Key Results
- IMACT-CXR achieves 59% mean IoU for localization and 71% diagnostic accuracy on 20 cases
- Outperforms text-based tutor and Radiology-GPT baselines in preliminary evaluation
- Ablation studies confirm gaze analytics, mastery tracking, reasoning guidance, and knowledge retrieval each contribute meaningfully

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Mastery-tracking via Bayesian Knowledge Tracing (BKT) reduces redundant feedback and accelerates skill acquisition by triggering interventions only when posterior mastery falls below thresholds.
- **Mechanism:** Per-skill BKT parameters (p_init, plearn, pguess, pslip) update after each correctness observation Ct; posterior P(Lt|Ct) gates whether the system invokes PubMed retrieval, case similarity suggestions, or NV-Reason-CXR-3B reasoning. Low mastery (< 0.2 after ≥5 attempts) escalates to reasoning guidance; mastery < 0.3 after ≥3 attempts triggers knowledge reinforcement.
- **Core assumption:** Learners benefit from reduced feedback redundancy and mastery-contingent escalation; BKT parameters generalize across radiology skills.
- **Evidence anchors:** [abstract] "Bayesian Knowledge Tracing (BKT) maintains skill-specific mastery estimates that drive both knowledge reinforcement and case similarity retrieval"; [section 2.2.2] Equations 5-8 define BKT update; thresholds for triggering agents specified; [corpus] RadAgents (arXiv 2509.20490) uses agentic reasoning for CXR but lacks explicit mastery tracking—suggesting IMACT-CXR's BKT integration is a differentiating feature, though direct comparison data is absent.
- **Break condition:** If BKT parameters poorly fit actual learning curves (e.g., high slip rates misclassify mastery), interventions may trigger inappropriately; calibration with larger learner cohorts would be required.

### Mechanism 2
- **Claim:** Multi-modal input fusion (spatial annotations + gaze telemetry + free-text) enables tutor responses that emulate expert mentoring by evaluating all available learner evidence before feedback.
- **Mechanism:** Each turn, the orchestrator validates bounding-box IoU (threshold 0.6), maps gaze fixations to lung lobes via U-Net segmentation (computing coverage ratio, dwell time ratio, sequence score), and grades text responses—all before generating faculty-style feedback.
- **Core assumption:** Simultaneous multi-modal assessment provides richer diagnostic signal than text-only or image-only evaluation.
- **Evidence anchors:** [abstract] "The tutor simultaneously ingests learner bounding boxes, gaze samples, and free-text observations"; [section 2.1.1] Workflow stages: Focus Gate → Assessment + Mastery → Routing → Knowledge/Reasoning/Similarity → Faculty Response; [corpus] Eyes on the Image (arXiv 2508.13068) demonstrates gaze-guided learning for CXR, supporting the premise that gaze signals carry diagnostic value.
- **Break condition:** If gaze hardware is unreliable or bounding-box input is noisy, validation failures may prematurely terminate turns with only directional guidance, reducing learning opportunities.

### Mechanism 3
- **Claim:** Safety prompts and progressive disclosure preserve discovery-based learning by preventing premature ground-truth leakage while still enabling targeted coaching.
- **Mechanism:** Ground-truth findings are sanitized to generic categories (e.g., "12 mm size" → "size/measurement") before reaching LLM agents; only elements the student has mentioned or explicitly requested are discussed.
- **Core assumption:** Discovery-based learning is more effective than immediate answer revelation for radiology skill acquisition.
- **Evidence anchors:** [abstract] "Safety prompts prevent premature disclosure of ground-truth labels"; [section 2.3] Sanitization, progressive disclosure, and "no value echo" constraints detailed; [corpus] No direct corpus evidence on safety mechanisms in radiology tutoring; this appears to be an IMACT-CXR-specific contribution.
- **Break condition:** Over-sanitization could frustrate learners if hints remain too vague; threshold calibration needed.

## Foundational Learning

- **Concept: Bayesian Knowledge Tracing (BKT)**
  - Why needed here: Core to understanding how IMACT-CXR estimates skill mastery over time and decides when to escalate interventions.
  - Quick check question: Given p_init=0.2, plearn=0.15, and a correct response on the first attempt, what is the updated mastery P(L1)?

- **Concept: Intersection over Union (IoU)**
  - Why needed here: The Focus Gate validates learner bounding boxes against ground truth using IoU threshold 0.6 before proceeding.
  - Quick check question: A student box covers 40% of the ground-truth region and extends 30% beyond it—is IoU ≥ 0.6?

- **Concept: AutoGen Multi-Agent Orchestration**
  - Why needed here: IMACT-CXR is implemented as a sequential AutoGen workflow; understanding agent roles and routing is essential for extension.
  - Quick check question: In AutoGen, how does a function agent differ from a conversational agent, and which type handles the Focus Gate?

## Architecture Onboarding

- **Component map:** Student Input (bbox + gaze + text) → Focus Gate Agent (IoU validation) → Assessment Agent → BKT Module (mastery update) → Routing Agent (threshold-based decisions) → KnowledgeBaseAgent (PubMed) → ReasoningAgent (NV-Reason-CXR-3B) → CaseSimilarityAgent (REFLACX) → SocraticTutorAgent → FacultyStyleResponder → State Persistence → Next Turn

- **Critical path:** Focus Gate → Assessment + BKT → Routing → FacultyStyleResponder. If Focus Gate fails, the turn ends early with directional guidance only.

- **Design tradeoffs:**
  - Latency vs. completeness: Full turn with ReasoningAgent takes 66.5s vs. 24.1s without (Table 1); reasoning is triggered sparingly.
  - Proprietary vs. local models: Assessment/responder use GPT-4 API (external dependency); NV-Reason-CXR-3B runs locally (GPU-bound, ~42s).
  - Gaze hardware requirement: System assumes reliable eye-tracking; fallback behavior undefined.

- **Failure signatures:**
  - IoU gate repeatedly failing → learner stuck, no assessment reached
  - NV-Reason timeout (>60s) → degraded turn latency
  - PubMed API rate limits → fallback to LLM synthesis (may reduce evidence quality)
  - BKT mastery stuck at low values despite correct answers → possible p_slip misconfiguration

- **First 3 experiments:**
  1. **Latency profiling without ReasoningAgent:** Run 50 mock turns with only KnowledgeBaseAgent to confirm sub-30s response times; identify bottlenecks in GPT-4 calls.
  2. **BKT parameter sensitivity:** Vary p_slip and p_guess on simulated learner traces to verify mastery thresholds trigger appropriate agent escalations.
  3. **Gaze ablation replay:** Disable gaze analytics on recorded sessions from the preliminary evaluation to replicate the -Gaze ablation (expected ~6.8% localization drop per Table 4); validate metric calculations.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does IMACT-CXR produce statistically significant improvements in localization and diagnostic reasoning when evaluated across multiple participants with varying expertise levels?
- **Basis in paper:** [explicit] "Formal user studies with multiple participants are needed to establish statistical significance and generalizability."
- **Why unresolved:** Preliminary evaluation used only a single novice participant across 20 cases, precluding statistical inference.
- **What evidence would resolve it:** Randomized controlled study with 30+ participants stratified by training level, with power analysis and significance testing.

### Open Question 2
- **Question:** Do learning gains from IMACT-CXR persist over time, or do they decay without continued deliberate practice?
- **Basis in paper:** [explicit] Future work includes "conducting longitudinal studies to measure knowledge retention."
- **Why unresolved:** All evaluations measured immediate post-training performance with no delayed follow-up assessments.
- **What evidence would resolve it:** Retention testing at 1-week, 1-month, and 3-month intervals comparing IMACT-CXR trainees to baseline groups.

### Open Question 3
- **Question:** Can the NV-Reason-CXR-3B component be compressed to reduce the ~42-second inference latency while preserving tutoring effectiveness?
- **Basis in paper:** [explicit] Limitations include "the computational cost of NV-Reason when GPU resources are scarce (~42 s per call)" and future work includes "model compression for on-premise deployment."
- **Why unresolved:** No distilled or quantized variants have been evaluated against the full model.
- **What evidence would resolve it:** Ablation comparing compressed model variants on diagnostic accuracy and turns-to-mastery metrics.

### Open Question 4
- **Question:** How does IMACT-CXR compare to expert human radiologist tutors in learning efficiency and skill transfer?
- **Basis in paper:** [inferred] The paper compares only to text-based tutors and Radiology-GPT, noting human tutoring is "limited by faculty availability" but excludes human experts from the experimental comparison.
- **Why unresolved:** No human tutor baseline was included in the preliminary evaluation.
- **What evidence would resolve it:** Randomized trial with matched case sets comparing IMACT-CXR to faculty-led tutoring sessions.

## Limitations
- Single-participant preliminary evaluation restricts generalizability of reported performance improvements
- System assumes reliable eye-tracking hardware and sufficient GPU resources for NV-Reason-CXR-3B inference
- Critical implementation details (U-Net architecture, prompt templates, skill taxonomy) are unspecified

## Confidence
**High confidence**: The multi-modal fusion architecture (spatial + gaze + text) is technically sound and supported by the cited gaze analytics work. The AutoGen workflow structure and BKT implementation details are clearly specified.

**Medium confidence**: The reported performance improvements over baselines are plausible given the ablation study design, but the single-participant sample prevents definitive conclusions about efficacy.

**Low confidence**: The generalizability of BKT parameters across learners and the effectiveness of safety mechanisms for preserving discovery-based learning remain unvalidated.

## Next Checks
1. **Multi-learner efficacy trial**: Deploy IMACT-CXR with 15+ novice participants across 20 cases each, measuring mIoU, accuracy, and turns-to-mastery to establish statistical significance.

2. **BKT parameter calibration**: Run cross-validation on simulated learner traces with varying p_slip and p_guess values to identify optimal parameters for different learning rates.

3. **Gaze hardware dependency assessment**: Implement a fallback mode without gaze analytics and compare performance metrics to the full system to quantify gaze's contribution under hardware constraints.