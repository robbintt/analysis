---
ver: rpa2
title: 'Dancing with Deer: A Constructional Perspective on MWEs in the Era of LLMs'
arxiv_id: '2508.15977'
source_url: https://arxiv.org/abs/2508.15977
tags:
- mwes
- language
- meaning
- linguistic
- these
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Dancing with Deer: A Constructional Perspective on MWEs in the Era of LLMs

## Quick Facts
- arXiv ID: 2508.15977
- Source URL: https://arxiv.org/abs/2508.15977
- Reference count: 6
- Primary result: LLMs can generalize novel MWE meanings after single exposure but fail at reasoning over MWE combinations requiring cross-modal knowledge.

## Executive Summary
This paper evaluates Large Language Models' (LLMs) ability to handle Multiword Expressions (MWEs) through the lens of Construction Grammar (CxG). The authors present experiments showing that both LLMs and humans can generalize the meaning of novel MWEs from a single exposure when the task aligns with statistically typical patterns. However, only humans can reason over combinations of MWEs, as this requires comparison to a lifetime of stored constructional exemplars rich with cross-modal details. The paper argues that while LLMs acquire formal linguistic competence through text exposure alone, they lack functional competence that requires embodied, multi-sensory experience.

## Method Summary
The authors conducted experiments using 10 novel MWEs (e.g., "wink at Pringles," "dance with the deer") and 3 nascent MWEs ("got rizz," "skibidi toilet," "brat summer"). They tested GPT-4o and GPT-o1's ability to interpret these expressions in new contexts and reason over combinations of two or more MWEs. The methodology involved in-context learning where definitions and usage examples were provided in the prompt, followed by questions requiring interpretation or reasoning. Human annotators served as a baseline for comparison. The experiments measured success rates for single-MWE interpretation, MWE combination reasoning, and cross-modal MWE interpretation.

## Key Results
- GPT-4o successfully inferred meaning of all 10 novel MWEs (100% accuracy) when presented with definition, usage, and novel syntactic context.
- GPT-4o's performance dropped to 70% on reasoning questions combining two MWEs, while GPT-o1 performed worse at 60%.
- Both LLMs failed to provide gold-standard responses for nascent MWEs with strong cross-modal origins ("skibidi toilet," "got rizz").

## Why This Works (Mechanism)

### Mechanism 1: Constructional Templates for Partial Productivity
Modeling MWEs as constructional templates with fixed and flexible slots enables systematic representation of partially productive expressions. Rather than treating MWEs as frozen chunks or fully productive rules, constructional rolesets encode both substantive elements and schematic slots with semantic role assignments. This allows detection and interpretation of novel instantiations within constrained productivity.

### Mechanism 2: Single-Exposure Generalization via Statistical Priors
Both LLMs and humans can generalize novel MWE meanings from a single exposure when the task aligns with statistically typical patterns. LLMs leverage statistical priors from pretraining data about how MWEs typically behave syntactically and semantically, mirroring usage-based acquisition where frequency facilitates construction extension.

### Mechanism 3: Cross-Modal Association Gap in Functional Competence
The gap between human and LLM functional linguistic competence stems from humans storing cross-modal associations with each constructional exemplar, while LLMs store only textual context. Humans can reason over novel MWE combinations by comparing against a lifetime of rich exemplars containing sensory, social, and embodied details that text-trained LLMs cannot access.

## Foundational Learning

- **Construction Grammar (CxG) as unified theory**: CxG treats constructions (form-meaning pairings at any level) as providing the machinery to handle both idiomatic and compositional expressions uniformly. This explains why the paper critiques Generative approaches that separate syntax from semantics.
  - Quick check: Can you explain why treating idioms as "frozen chunks" in the lexicon creates problems for a theory that strictly separates syntax from semantics?

- **Usage-based acquisition and frequency effects**: Frequency of use entrenches constructions and facilitates extension while preventing overgeneralization. This explains both why LLMs can acquire formal competence from text exposure and why they lack functional competence.
  - Quick check: If a speaker encounters "kick the bucket" (meaning 'die') ten times and "kick the pail" zero times, what predictions does usage-based CxG make about their willingness to extend the idiom to "kick the pail"?

- **Formal vs. functional linguistic competence (refined)**: The paper refines Mahowald et al.'s (2024) distinction to diagnose LLM limitations. Formal competence = structural/lexical knowledge; functional competence = reasoning, social understanding, planning. The critical addition is recognizing that some functional tasks align with statistical priors (LLMs succeed) while others require cross-modal reasoning (LLMs fail).
  - Quick check: Would you classify the ability to infer that "the bank teller sent their LLM to work" implies disengagement as formal or functional competence? What about inferring whether someone should feel complimented or insulted by "Bro got that skibidi rizz"?

## Architecture Onboarding

- **Component map**: Input Text → [MWE Detection Module] → [Constructional Template Matching] → [Roleset Selection] → [Semantic Role Assignment] → Output: Meaning Representation (AMR/UMR graph)

- **Critical path**: 1) Identify candidate MWEs (cannot rely on fixed-string matching for partially productive MWEs) 2) Match against constructional templates (fixed elements + schematic slot patterns) 3) Select appropriate roleset (literal vs. idiomatic meaning) 4) Assign semantic roles to constituents filling schematic slots 5) For novel MWEs: provide definition and usage in-context; LLM generalizes via statistical priors

- **Design tradeoffs**:
  - **Symbolic vs. subsymbolic**: PropBank/UMR provide interpretable, scaffolding resources that work with small training sets (critical for low-resource/polysynthetic languages), but require manual annotation effort. Neural methods scale but lack explainability.
  - **Static lexicon vs. productive templates**: Listing all MWE variants in a lexicon overproliferates entries and misses generalizations. Constructional templates capture partial productivity but require defining slot constraints.
  - **Word-based vs. MMU-based**: For polysynthetic languages like Arapaho, traditional word-hood is irrelevant. Must define "meaningful morphosyntactic units" (MMUs) based on constructional slots, not orthography.

- **Failure signatures**: LLM succeeds at single-MWE generalization but fails at MWE combination reasoning (70% → indicates reliance on statistical priors rather than robust functional competence); LLM responses become brittle to prompt variations when task requires higher-level generalization; more advanced model (GPT-o1) performs worse than GPT-4o on reasoning task (60% vs. 70%); nascent MWEs with cross-modal origins produce non-gold-standard responses even when text definitions are provided.

- **First 3 experiments**:
  1. **Replicate novel MWE single-exposure test**: Create 10 novel MWEs with definitions and usage. Test whether LLM can interpret novel syntactic contexts. Compare to human annotators. Expect: LLM ≈ humans (~100% success) for formal competence tasks.
  2. **MWE combination reasoning test**: Create 10 questions requiring reasoning over pairs of novel MWEs (counterfactuals, combinations). Measure accuracy and consistency across prompt variations. Expect: humans > LLMs, LLMs show brittleness to prompt phrasing.
  3. **Cross-modal MWE interpretation test**: Select nascent MWEs with strong cross-modal origins. Provide text-only definitions. Test whether LLM can reason about social implications (compliment vs. insult). Compare to humans who know the cross-modal context. Expect: humans with cross-modal exposure > humans with text-only > LLMs.

## Open Questions the Paper Calls Out

### Open Question 1
How can cross-modal features be effectively encoded in NLP systems to bridge the gap between text-based LLM processing and human functional linguistic competence? The conclusion states, "We can begin to bridge the gap... by pushing forward research into how to encode cross-modal features." This remains unresolved because LLMs currently lack the "rich cross-modal associations" that humans use to reason over combinations of novel or nascent MWEs. Development of systems that can reason over combined novel MWEs with consistency comparable to human annotators would resolve this question.

### Open Question 2
Can incorporating symbolic constructional resources, such as AMR or UMR annotation schemas, scaffold LLMs to improve robustness and generalization for MWEs? Section 5.2.4 suggests, "One such method might be the incorporation of AMR or UMR annotation schema... scaffolding toward systems that are more robust and generalizable." This remains unresolved because current LLMs show brittleness and performance degradation when reasoning over novel MWE combinations, relying on statistical priors rather than robust generalization. Experiments demonstrating that LLMs augmented with constructional templates show improved performance and reduced prompt brittleness on functional linguistic tasks involving MWEs would resolve this question.

### Open Question 3
To what extent do constructional templates based on meaningful morphosyntactic units (MMUs) prevent the "faulty representations" caused by brute-force LLM methods in low-resource languages? The paper notes that brute-force NLP methods risk creating "faulty representations" causing "irreparable harm" to low-resource language communities, advocating for symbolic resources instead. This remains unresolved because while CxG offers a theoretical solution for polysynthetic languages like Arapaho, it remains unclear if this approach effectively scales to counteract the errors inherent in data-hungry LLMs for low-resource languages. A comparative evaluation of semantic representation accuracy in a low-resource polysynthetic language using CxG-based templates versus standard LLM training approaches would resolve this question.

## Limitations

- The core limitation is the absence of direct evaluation of the constructional template mechanism itself; related work suggests neural models develop sensitivity to MWE patterns but this paper does not empirically validate whether explicit templates improve MWE detection or interpretation.
- The cross-modal association gap mechanism is primarily theoretical (grounded in Bybee 2006) rather than experimentally demonstrated within this corpus.
- The paper does not experimentally compare human performance with and without cross-modal exposure, nor does it test whether providing cross-modal associations to LLMs improves performance.

## Confidence

- **High Confidence**: The empirical findings on LLM performance (100% on single-MWE interpretation, 70% on MWE combinations, failure on nascent MWEs) are directly observable and reproducible. The methodology for in-context generalization is clearly specified.
- **Medium Confidence**: The mechanism of constructional templates as the explanation for systematic partial productivity is well-supported by related work (PropBank rolesets) but lacks direct validation in this paper. The claim that MWEs exist on a continuum from fixed to productive is standard in CxG but not novel here.
- **Low Confidence**: The cross-modal association gap explanation for LLM limitations is primarily theoretical. While the paper provides examples (skibidi toilet requiring video context), it does not experimentally compare human performance with and without cross-modal exposure, nor does it test whether providing such associations to LLMs improves performance.

## Next Checks

1. **Constructional Template Validation**: Create a benchmark dataset of partially productive MWEs with varying degrees of schematicity. Compare a constructional template-based approach against pure neural detection on this dataset to measure whether explicit templates improve generalization to novel instantiations.

2. **Cross-Modal Augmentation Experiment**: Select nascent MWEs with strong cross-modal origins. Create text descriptions of the visual/social context (e.g., "skibidi toilet refers to a viral trend where people make toilet-like head movements to a song"). Test whether providing this context to LLMs improves their ability to reason about social implications compared to text-only definitions.

3. **Prompt Robustness Analysis**: Systematically vary the reasoning prompts for MWE combinations (change word order, add/removing context, different counterfactual framings). Measure how LLM performance varies with prompt changes to quantify brittleness and determine whether certain prompt formats consistently yield better reasoning.