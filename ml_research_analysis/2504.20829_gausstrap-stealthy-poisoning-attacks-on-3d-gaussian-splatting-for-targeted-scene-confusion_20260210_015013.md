---
ver: rpa2
title: 'GaussTrap: Stealthy Poisoning Attacks on 3D Gaussian Splatting for Targeted
  Scene Confusion'
arxiv_id: '2504.20829'
source_url: https://arxiv.org/abs/2504.20829
tags:
- attack
- viewpoints
- backdoor
- viewpoint
- ssim
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GaussTrap, the first backdoor attack targeting
  3D Gaussian Splatting (3DGS) models. It proposes a three-stage poisoning pipeline
  (attack, stabilization, normal training) to embed malicious views at specific viewpoints
  while preserving high-quality rendering elsewhere.
---

# GaussTrap: Stealthy Poisoning Attacks on 3D Gaussian Splatting for Targeted Scene Confusion

## Quick Facts
- arXiv ID: 2504.20829
- Source URL: https://arxiv.org/abs/2504.20829
- Authors: Jiaxin Hong, Sixu Chen, Shuoyang Sun, Hongyao Yu, Hao Fang, Yuqi Tan, Bin Chen, Shuhan Qi, Jiawei Li
- Reference count: 40
- Primary result: First backdoor attack on 3D Gaussian Splatting achieving PSNR > 36 dB at attack viewpoints while preserving clean-view quality

## Executive Summary
GaussTrap introduces the first backdoor attack targeting 3D Gaussian Splatting (3DGS) models through a three-stage poisoning pipeline. The method implants malicious viewpoints at specific camera poses while maintaining high-quality rendering elsewhere in the scene. By exploiting 3DGS's adaptive densification and introducing Viewpoint Ensemble Stabilization (VES), the attack achieves stealthy, viewpoint-consistent poisoned renderings that remain undetectable through normal quality metrics. Experiments demonstrate successful attacks on both synthetic and real-world datasets, highlighting significant security vulnerabilities in 3DGS pipelines.

## Method Summary
GaussTrap operates through a three-stage sequential optimization process: attack training to embed malicious views, stabilization to ensure smooth transitions, and normal training to recover clean-view quality. The method leverages 3DGS's adaptive densification mechanism, where high reconstruction error triggers Gaussian splitting/cloning to accommodate attack content. VES generates perturbed viewpoints around attack poses to prevent artifacts, while the multi-stage approach incrementally satisfies attack effectiveness, stability, and clean-view recovery objectives without catastrophic interference.

## Key Results
- Achieves PSNR > 36 dB and SSIM > 0.98 on attack viewpoints while maintaining quality on normal views
- VES reduces stabilization artifacts from PSNR ~17 to ~26, preventing black cloud artifacts
- Outperforms baseline IPA-NeRF+3DGS across all evaluation metrics
- Demonstrates scalability to multiple backdoors (5 tested) with predictable quality degradation

## Why This Works (Mechanism)

### Mechanism 1: Three-Stage Sequential Optimization for Stealthy Embedding
Sequentially optimizing for attack effectiveness, viewpoint stability, and clean-view quality produces a poisoned model that is both effective and stealthy. The attack stage minimizes loss between rendered images and malicious targets, stabilization stage regularizes neighboring viewpoints via VES to prevent artifacts, and normal stage recovers rendering quality for non-target views. Core assumption: three objectives can be incrementally satisfied without catastrophic interference. Evidence: [abstract] "three-stage pipeline (attack, stabilization, and normal training) to implant stealthy, viewpoint-consistent poisoned renderings." Break condition: If attack-stage optimization irreversibly modifies Gaussian positions/opacities such that normal-stage training cannot recover original scene geometry.

### Mechanism 2: Viewpoint Ensemble Stabilization (VES) for Viewpoint Consistency
VES samples and trains on angularly-perturbed viewpoints around the attack perspective to maintain rendering continuity and prevent visible artifacts. Given attack camera rotation R_w2c, generates stabilization poses via composed pitch and yaw offsets: R'_w2c = R_x(Δθ) · R_y(Δϕ) · R_w2c for all (Δθ, Δϕ) ∈ {-δ, 0, δ}² \ {(0,0)}. These are rendered using a clean 3DGS reference model and incorporated as ground-truth for stabilization training. Core assumption: A clean pre-trained 3DGS model is available to synthesize stabilization views. Evidence: [abstract] "Viewpoint Ensemble Stabilization (VES) to ensure smooth transitions between attack and normal views, enhancing stealth." Break condition: If the attack viewpoint lacks nearby training cameras, or if angular offsets produce significant occlusion changes due to scene geometry.

### Mechanism 3: Adaptive Gaussian Capacity Exploitation via Densification
3DGS's adaptive densification can be co-opted to allocate additional Gaussian primitives for representing attack content without destroying original scene structure. During attack training, high reconstruction error produces large gradients, triggering densification that increases local Gaussian density. This additional capacity allows the model to represent both original scene features and injected content. Core assumption: The Gaussian capacity budget is not exhausted; densification occurs in spatial regions relevant to the attack view. Evidence: [section 3.1] "when the gradient ∇g > τ_g, local details are improved by splitting or cloning." Break condition: If the attack image requires depth/occlusion patterns that fundamentally contradict the scene's 3D geometry.

## Foundational Learning

- **3D Gaussian Splatting Rendering Pipeline**: Understanding projection (Σ' = JWΣW^T J^T), depth sorting, and alpha blending (C = Σᵢ Tᵢαᵢcᵢ) is essential to see how viewpoint-specific manipulation is possible through parameter optimization. Quick check: Why can the same set of 3D Gaussians produce different 2D images from different camera poses, and how does this enable viewpoint-targeted backdoors?

- **Backdoor Attack Threat Model (Data Poisoning)**: The attack assumes users download pre-trained 3DGS models from untrusted repositories. Distinguishing this from test-time adversarial attacks clarifies why optimization happens during training. Quick check: In GaussTrap, the trigger is a specific viewpoint P_atk. How does this differ from traditional image-space triggers (e.g., pixel patches), and what are the implications for detection?

- **Image Quality Metrics (PSNR/SSIM/LPIPS)**: The paper reports PSNR > 36 dB and SSIM > 0.98 for attack views; you must interpret these to assess attack stealthiness and compare to baselines. Quick check: Why does the paper evaluate four viewpoint types (attack, stabilization, train, and test) separately rather than aggregating into a single metric?

## Architecture Onboarding

- **Component map**: Scene dataset S = {(V_t, P_t)}ᵢ → VES Generator (Algorithm 1) → stabilization poses P_stab and views V_stab → Three-Stage Trainer (2500 epochs, T_a:T_s:T_t = 25:5:5) → 3DGS Core (Gaussian set G, renderer R, densification threshold D) → Loss L = (1-λ)·L1 + λ·L_D-SSIM

- **Critical path**: Load clean scene data and pre-trained 3DGS → Run VES generator to create stabilization viewpoints → Render stabilization views using clean model as ground truth → For each epoch: execute attack phase → stabilization phase → normal phase → Each phase: sample pose → render → compute L1+SSIM loss → backprop → densify/prune

- **Design tradeoffs**: Densification level D: Higher values (100000) yield better attack quality but larger model size and memory. Stabilization angles: 13°-15° for synthetic (Blender), 5° for real-world (MipNeRF-360); larger angles cover more transition region but require more VES renders. Iteration ratio T_a:T_s:T_t: More attack iterations strengthen backdoor; more normal iterations improve clean-view recovery. Multi-backdoor count n: Adding backdoors degrades all metrics; expect ~1-2 PSNR drop per additional backdoor.

- **Failure signatures**: Black cloud/floaters in stabilization views: VES insufficient → increase stabilization angle range or iterations T_s. Attack view remains blurry: Densification too low → increase D; verify attack image resolution matches scene. Test view quality collapse: Attack overpowers normal training → reduce T_a or increase T_t. PSNR gap between Tables 1 and 3 (scene-specific): Certain scenes (Ship: PSNR 29.04 test) are harder; expect variability.

- **First 3 experiments**: 1) Single-backdoor reproduction on Blender "Chair": Use "Earth" attack image, D=100000, δ ∈ {13°, 15°}. Target metrics: attack PSNR ~38, test PSNR ~26, stabilization PSNR ~26. Run ablation: w/o VES (expect stabilization PSNR ~17 per Figure 3) vs w/ VES. 2) Densification sweep: Run with D ∈ {30000, 60000, 100000} on Blender scene. Plot attack PSNR and test PSNR vs D. Expected: monotonic improvement in attack quality, modest test-view gains. 3) Multi-backdoor scaling test: Inject n ∈ {2, 3, 5} backdoors with uniformly sampled attack viewpoints. Measure degradation trajectory. Verify Table 7 trend: 1→5 backdoors drops attack PSNR from 40.85 to 33.21.

## Open Questions the Paper Calls Out
None

## Limitations
- Novelty scope: Three-stage optimization framework and VES mechanism are specific to 3DGS's Gaussian-based representation and may not generalize to other 3D scene representation methods.
- Dataset bias: Evaluation relies on synthetic (Blender) and real-world (MipNeRF-360) datasets with specific characteristics. Performance on other scene types or larger-scale scenes remains untested.
- Computational cost: High densification levels (D=100,000) required for effective attacks significantly increase model size and training time, potentially limiting practical deployment.

## Confidence
- **High confidence**: Three-stage pipeline effectiveness (PSNR > 36 dB at attack views, VES reduces stabilization artifacts from PSNR ~17 to ~26), densification exploitation mechanism, and baseline comparison with IPA-NeRF+3DGS.
- **Medium confidence**: Multi-backdoor scalability (Table 7 shows degradation trend but limited to 5 backdoors), real-world dataset performance (MipNeRF-360 results show lower PSNR values, suggesting dataset-specific challenges).
- **Low confidence**: Defense implications (paper demonstrates vulnerability but doesn't evaluate potential defenses), attack transferability (whether poisoned models transfer across different 3DGS implementations).

## Next Checks
1. **Ablation on VES stability**: Run GaussTrap without VES stabilization on Blender "Chair" scene and measure black cloud artifact severity across multiple attack viewpoints.
2. **Densification capacity limit**: Test GaussTrap with progressively higher densification levels (D=200k, 500k, 1M) to determine when additional capacity no longer improves attack quality or when model size becomes prohibitive.
3. **Cross-dataset generalization**: Apply GaussTrap to a third, independent 3DGS dataset (e.g., forward-facing scenes from Tank and Temples) to validate performance consistency across different scene types.