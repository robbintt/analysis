---
ver: rpa2
title: High-Layer Attention Pruning with Rescaling
arxiv_id: '2507.01900'
source_url: https://arxiv.org/abs/2507.01900
tags:
- pruning
- attention
- layer
- learning
- layers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces HARP (High-layer Attention Rescaled Pruning),
  a training-free structured pruning method that selectively removes attention heads
  from the higher layers of large language models. The approach leverages the observation
  that attention heads in deeper layers are less important due to increased token
  representation similarity.
---

# High-Layer Attention Pruning with Rescaling

## Quick Facts
- arXiv ID: 2507.01900
- Source URL: https://arxiv.org/abs/2507.01900
- Reference count: 35
- Key outcome: HARP achieves 16.7% inference speedup for 65k-token sequences while maintaining performance through high-layer attention head pruning with adaptive rescaling

## Executive Summary
This paper introduces HARP (High-layer Attention Rescaled Pruning), a training-free structured pruning method that selectively removes attention heads from the higher layers of large language models. The approach leverages the observation that attention heads in deeper layers are less important due to increased token representation similarity. HARP prunes query and key parameters in these high layers, allowing the attention mechanism to be skipped entirely, and introduces an adaptive rescaling parameter to maintain representation scale after pruning. Comprehensive experiments on LLaMA3.1-8B, Mistral-7B-v0.3, Qwen2-7B, and Gemma2-9B demonstrate HARP's superiority over existing methods across 27 datasets, particularly in generation tasks.

## Method Summary
HARP works by first selecting the top-P layers for pruning (based on model size), then performing a top-down grid search over rescaling parameter α ∈ {0, 0.1, ..., 1.0} to minimize perplexity on a held-out corpus. The method removes all query and key projection parameters from selected layers, effectively bypassing the attention mechanism computation. In the residual block, it applies H = H + α·H·W_V·W_O where the attention output is now computed directly from values. The pruning is asymmetric—only high layers are pruned—based on theoretical analysis showing that deeper layers produce more similar token representations, making attention heads less discriminative.

## Key Results
- Achieves 16.7% inference speedup for 65,536-token sequences with 3.3% parameter reduction on LLaMA3.1-8B
- Outperforms existing methods across 27 datasets including TriviaQA, NaturalQuestions, GSM8K, and WinoGrande
- Layer position ablation shows Top-8 pruning achieves 60.39% average vs 32.72% for Bottom-8 pruning
- Rescaling ablation demonstrates 9× improvement (3.40% → 31.10%) when using searched α vs α=1.0

## Why This Works (Mechanism)

### Mechanism 1: High-Layer Token Representation Convergence
As depth increases, repeated attention aggregation and residual connections cause token embeddings to approach the over-smoothing subspace where all rows become identical. When Sim(H^(ℓ)) → 1, token representations become nearly parallel, yielding approximately uniform attention weights that reduce attention to simple averaging. This makes higher-layer attention heads less discriminative and more amenable to pruning.

### Mechanism 2: Representation Scale Compensation via Rescaling
Removing attention aggregation alters hidden state magnitude because attention generally changes Frobenius norm. The adaptive rescaling parameter α in the residual block calibrates output scale post-pruning, preventing performance collapse particularly in generation tasks where error propagation is severe.

### Mechanism 3: O(N²d) Complexity Reduction from Attention Bypass
Pruning Q/K parameters enables complete attention mechanism skip, eliminating the quadratic complexity bottleneck for long sequences. For N > d_ff, the O(N²d) self-attention complexity dominates, making this optimization particularly valuable for long-context applications.

## Foundational Learning

- Concept: Frobenius Norm and Representation Scale
  - Why needed here: The rescaling justification hinges on understanding how matrix norms capture representation magnitude and why averaging operations change them
  - Quick check question: Given attention matrix Â (row-normalized, non-uniform weights) and hidden states H, will ||ÂH||_F equal ||H||_F? Explain why not.

- Concept: Over-smoothing in Deep Networks
  - Why needed here: The theoretical foundation (distance to subspace M) is directly borrowed from GNN over-smoothing literature; understanding this explains the layer-asymmetric pruning rationale
  - Quick check question: In a 32-layer transformer, would you expect token representations at layer 4 or layer 28 to have higher pairwise cosine similarity? Why?

- Concept: Perplexity as Surrogate Objective
  - Why needed here: HARP uses perplexity to select rescaling parameters and tune baseline pruning ranges—understanding what perplexity measures is critical for interpreting these choices
  - Quick check question: If Model A achieves perplexity 12.16 and Model B achieves perplexity 28.94 on the same corpus, which model assigns higher probability to the test data? What does this imply about token prediction quality?

## Architecture Onboarding

- Component map:
```
Input → LayerNorm → [Attention SKIPPED: Q/K pruned, V projection → Output projection] 
       → Residual Add (with α rescaling) → FFN → LayerNorm → Residual Add → Output
```

- Critical path: The top-down α search (Algorithm 1) is the initialization bottleneck. For 8 layers with K=10 candidates, this requires 80 forward passes. Incorrect α values cause catastrophic performance drops (see: generation task dropping from 31.10% to 3.40% without proper search).

- Design tradeoffs:
  - Layer selection: Pruning higher layers preserves performance but limits total compression ratio; the paper achieves only 3.3% parameter reduction for LLaMA3.1-8B
  - Integration with FFN pruning: HARP alone prunes attention; combining with FLAP (for FFN) improves compression but adds complexity
  - Search cost vs. training cost: 80 forward passes is expensive for inference-only methods but negligible compared to any retraining approach

- Failure signatures:
  - Generation tasks fail catastrophically (GSM8K: 36.16% → 1.29%) without rescaling due to error propagation in long outputs
  - Discriminative tasks more robust (smaller vocabulary space for errors)
  - Pruning middle/low layers (Bottom-8 strategy) causes 50%+ performance degradation

- First 3 experiments:
  1. Layer position ablation: Compare Top-8, Middle-8, Bottom-8 pruning with α=1.0 on discriminative tasks. Expected: Top > Middle > Bottom, confirming layer-asymmetric importance.
  2. Rescaling necessity test: Run HARP on LLaMA3.1-8B with α=1.0 (no search) vs. searched α values on WikiText. Measure perplexity gap and correlate with downstream task performance.
  3. Long-context scaling: Profile inference time for sequence lengths [1K, 2K, 4K, 8K, 16K, 32K, 65K] comparing original vs. pruned model. Verify speedup increases with N (should be negligible at 1K, ~16% at 65K).

## Open Questions the Paper Calls Out

- **Question 1:** Can advanced optimization strategies (Bayesian optimization, differentiable relaxations) improve α selection efficiency and effectiveness over the greedy grid search?
  - Basis in paper: Page 5: "Designing more advanced search strategies (e.g., Bayesian optimization or differentiable relaxations) is an interesting open direction for future work."

- **Question 2:** Does combining HARP with quantization or KV-cache compression yield additive efficiency gains without performance degradation?
  - Basis in paper: Page 11 (Conclusion): "Future work includes extending HARP to larger and multimodal models, combining it with techniques such as quantization or KV-cache compression..."

- **Question 3:** Can task-aware or adaptive layer selection criteria outperform the fixed high-layer pruning strategy?
  - Basis in paper: Page 11: "...developing more task-aware pruning criteria to further improve robustness and efficiency." Also, Table 5 shows Hessian-8 and Similarity-8 underperform Top-8, suggesting room for better selection methods.

## Limitations

- The theoretical model assumes specific conditions on singular values and eigenvalues that may not hold universally across all transformer architectures
- The method achieves only 3.3% parameter reduction for LLaMA3.1-8B due to selective high-layer pruning
- Generation tasks show significant degradation when α search is skipped or poorly executed, suggesting the rescaling mechanism is brittle

## Confidence

**High Confidence:** The O(N²d) complexity reduction claim and the observation that attention heads in deeper layers contribute less to token discrimination are well-supported by both theoretical analysis and empirical ablation studies.

**Medium Confidence:** The rescaling mechanism's effectiveness is demonstrated but relies heavily on the grid search calibration. The choice of perplexity as the calibration metric assumes strong correlation between held-out corpus performance and downstream task generalization.

**Low Confidence:** The claim that "all existing methods cannot achieve training-free structured pruning of the attention mechanism" requires careful qualification—several methods achieve similar goals through different mechanisms.

## Next Checks

1. **Layer Position Robustness Test:** Systematically vary P (number of layers pruned) from 2 to 16 on LLaMA3.1-8B and measure performance degradation curves for both discriminative and generation tasks.

2. **Cross-Architecture Generalization:** Apply HARP to a transformer variant with different normalization placement (pre-norm vs. post-norm) or additional skip connections. Measure whether the layer-asymmetric pruning pattern holds.

3. **Long-context Error Analysis:** For sequence lengths [1K, 2K, 4K, 8K, 16K, 32K, 65K], track not just speedup but also attention pattern entropy and token prediction accuracy degradation over output length.