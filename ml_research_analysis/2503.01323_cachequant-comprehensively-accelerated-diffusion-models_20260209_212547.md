---
ver: rpa2
title: 'CacheQuant: Comprehensively Accelerated Diffusion Models'
arxiv_id: '2503.01323'
source_url: https://arxiv.org/abs/2503.01323
tags:
- diffusion
- quantization
- arxiv
- time
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CacheQuant, a training-free acceleration
  framework for diffusion models that jointly optimizes model caching and quantization
  techniques. The key challenge addressed is that independent optimization of these
  two methods leads to poor performance due to coupled and accumulated errors.
---

# CacheQuant: Comprehensively Accelerated Diffusion Models

## Quick Facts
- arXiv ID: 2503.01323
- Source URL: https://arxiv.org/abs/2503.01323
- Reference count: 40
- Key outcome: 5.18× speedup and 4× compression for Stable Diffusion on MS-COCO with only 0.02 loss in CLIP score

## Executive Summary
This paper introduces CacheQuant, a training-free acceleration framework that jointly optimizes model caching and quantization for diffusion models. The key insight is that independent optimization of these two methods leads to poor performance due to coupled and accumulated errors. CacheQuant employs Dynamic Programming Schedule (DPS) to determine optimal cache scheduling while minimizing errors, and Decoupled Error Correction (DEC) to further mitigate these errors. The method achieves significant acceleration while maintaining high-quality outputs, outperforming traditional acceleration methods across different diffusion model frameworks and hardware platforms.

## Method Summary
CacheQuant is a training-free acceleration framework that combines dynamic programming-based cache scheduling with decoupled error correction to jointly optimize model caching and quantization for diffusion models. The method first determines an optimal cache schedule using DPS that minimizes cumulative errors from both caching and quantization by treating cache scheduling as an ordered partitioning problem. It then applies DEC to separately correct cache-induced and quantization-induced errors through channel-wise affine corrections, which can be absorbed into quantized weights for inference efficiency. The approach is demonstrated across different diffusion model frameworks (UNet and DiT) and various hardware platforms.

## Key Results
- Achieves 5.18× speedup and 4× compression for Stable Diffusion on MS-COCO with only 0.02 loss in CLIP score
- Outperforms traditional acceleration methods including caching, distillation, pruning, and quantization
- Maintains high-quality outputs across different diffusion model frameworks (UNet and DiT) and various hardware platforms

## Why This Works (Mechanism)

### Mechanism 1: Dynamic Programming Schedule (DPS)
- Optimally groups timesteps for cache reuse to minimize cumulative error from both caching and quantization
- Frames cache scheduling as an ordered partitioning problem with DP recurrence: M(T,K) = min_{K≤s≤T} {M(s-1, K-1) + D(s,T)}
- Practical constraints limit group length to [N/2, 2N], reducing solution time from 4 hours to 8 minutes for LDM-250

### Mechanism 2: Decoupled Error Correction (DEC)
- Separately corrects cache-induced and quantization-induced errors rather than direct output correction
- Applies channel-wise affine corrections: X_g ≈ a_1·X_c + b_1 (input correction) and O_c ≈ a_2·O_cq + b_2 (output correction)
- Parameters solved via least squares per channel, with output correction absorbed into quantized weights

### Mechanism 3: Joint Optimization Necessity
- Caching and quantization optimizations are non-orthogonal; naive composition causes severe degradation
- Independent optimization yields FID drops of 0.76 (quantization) and 4.71 (caching), but simple integration causes 11.99 FID loss
- Joint optimization via DPS+DEC addresses both simultaneously

## Foundational Learning

- **Model Caching for Diffusion**: CacheQuant builds on DeepCache (UNet) and Δ-DiT (DiT) caching strategies. You must understand what features are cached (upsampling outputs vs block deviations) and why temporal redundancy exists.
  - Quick check: Can you explain why adjacent denoising steps have similar feature maps, and which blocks are typically cached vs recomputed?

- **Uniform Quantization (PTQ)**: CacheQuant uses post-training quantization with scale factor s and zero-point z: x̂ = clip(⌊x/s⌉ + z, 0, 2^b-1). Understanding weight vs activation quantization granularity is essential.
  - Quick check: Given a tensor with values [0.1, 0.5, 1.0, 2.0], can you compute 8-bit uniform quantization parameters and the quantized representation?

- **Dynamic Programming for Sequence Partitioning**: DPS solves ordered grouping via DP recurrence. Understanding boundary conditions M(t,1) and backtracking for schedule recovery is required to implement or modify the algorithm.
  - Quick check: How would you modify the DP if you wanted to add a penalty for groups shorter than N/2 steps?

## Architecture Onboarding

- **Component map**: DPS Scheduler -> Cache Manager -> Quantizer -> DEC Module -> Inference Engine
- **Critical path**: 1) Run calibration pass to collect feature maps X^g_t across all T steps, 2) Execute DPS (Algorithm 1) to determine optimal schedule, 3) Compute DEC correction parameters using cached-vs-ground-truth comparisons, 4) Apply quantization with absorbed DEC output corrections, 5) Deploy accelerated model with cache-aware inference loop
- **Design tradeoffs**: Higher cache frequency N → more speedup but more error accumulation; W8A8 is training-free while W4A8 requires reconstruction; tighter [N/2, 2N] bounds → faster optimization but potentially suboptimal schedules
- **Failure signatures**: FID/CLIP score suddenly drops >5 points (likely schedule too aggressive or DEC parameters corrupted), generated images have color shifts/channels (check quantization scale factors per channel), speedup lower than expected (verify cache is actually being reused)
- **First 3 experiments**:
  1. Reproduce LDM-4 ImageNet baseline: Run DeepCache N=5 and CacheQuant N=5 (W8A8) on 100 images. Expected: FID ~3.79 vs ~4.03 per Table 2.
  2. Ablate DEC components: Compare (baseline quantization), (+DPS only), (+DPS+DEC) following Table 5. Expected FID progression: 15.36 → 8.47 → 7.21.
  3. Profile real speedup on your hardware: Deploy on target GPU/CPU/ARM with N=5, W8A8. Measure end-to-end latency per image. Compare to theoretical Bops reduction.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can CacheQuant be refined to achieve training-free performance recovery at W4A8 precision without relying on reconstruction?
- Basis in paper: The "Limitations and future work" section states the method relies on reconstruction for W4A8 performance and explicitly aims to "improve its compatibility with W4A8 precision."
- Why unresolved: The proposed Decoupled Error Correction (DEC) is training-free but currently insufficient to handle the noise introduced by 4-bit weights without fine-tuning/reconstruction.
- Evidence: Successful application of CacheQuant at W4A8 precision on Stable Diffusion without reconstruction, maintaining FID/CLIP parity with W8A8 baselines.

### Open Question 2
- Question: How does the computational overhead of the Dynamic Programming Schedule (DPS) scale with significantly larger diffusion models or longer inference trajectories?
- Basis in paper: The paper notes DPS takes 8 minutes for LDM-4 (250 steps); this one-time overhead may become a bottleneck for massive models (e.g., video diffusion) with thousands of steps.
- Why unresolved: The complexity reduction (limiting group length to $2N$) is validated on standard image models, but the quadratic nature of dynamic programming may still be prohibitive for extreme-scale generative tasks.
- Evidence: Benchmarks of DPS execution time and memory usage on models exceeding 1B parameters or employing >1000 sampling steps.

### Open Question 3
- Question: Can the Decoupled Error Correction (DEC) mechanism effectively mitigate errors in non-uniform or mixed-precision quantization schemes?
- Basis in paper: The method is evaluated on uniform quantization; the linear correction model ($a, b$) used by DEC assumes specific error distributions that might not hold for non-linear quantization formats.
- Why unresolved: DEC separates cache and quantization errors linearly; complex, non-uniform quantization noise might violate the assumptions required for the least squares correction to be effective.
- Evidence: Experimental results applying CacheQuant to mixed-precision (e.g., different bit-widths for different layers) or logarithmic quantization without performance degradation.

## Limitations
- Joint optimization requires storing all T feature maps during calibration, creating a memory bottleneck for models with very long sampling schedules
- The L1 distance metric used in DPS may not perfectly correlate with perceptual quality degradation, potentially leading to suboptimal schedules
- Non-orthogonality assumption between caching and quantization, while empirically demonstrated, lacks theoretical justification for why affine corrections suffice across diverse model architectures

## Confidence

**High confidence**: The empirical demonstration that caching and quantization errors compound (Figure 3), the measurable acceleration metrics (5.18× speedup, 4× compression), and the quantitative improvements from DEC (0.91 FID improvement on LDM-ImageNet) are well-supported by experimental results.

**Medium confidence**: The necessity of joint optimization and the effectiveness of the DP-based scheduling approach are supported by ablation studies but rely on specific design choices (L1 metric, length constraints) that may not generalize optimally to all diffusion architectures.

**Low confidence**: The theoretical foundation for why channel-wise affine corrections adequately address the complex, coupled errors in diffusion models, and whether the method would scale to extremely long sampling schedules without memory constraints.

## Next Checks

1. **Test error accumulation**: Systematically measure FID degradation at each denoising step for CacheQuant vs baseline to quantify where DEC provides the most benefit and whether error accumulation remains bounded.

2. **Schedule sensitivity analysis**: Evaluate CacheQuant performance using random cache schedules versus DPS-optimized schedules across multiple random seeds to establish the statistical significance of the DP optimization benefit.

3. **Memory-accuracy tradeoff**: Measure actual memory usage during calibration (feature map storage) and explore whether approximate DPS (e.g., hierarchical clustering) can achieve comparable schedules with reduced memory requirements.