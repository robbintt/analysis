---
ver: rpa2
title: 'MoRE: Unlocking Scalability in Reinforcement Learning for Quadruped Vision-Language-Action
  Models'
arxiv_id: '2503.08007'
source_url: https://arxiv.org/abs/2503.08007
tags:
- data
- more
- tasks
- learning
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MoRE introduces a scalable reinforcement learning framework for
  quadruped VLA models by integrating a mixture-of-experts architecture with RL-based
  training objectives. The method fine-tunes a pre-trained MLLM backbone with LoRA
  experts to enable task-specific specialization while maintaining computational efficiency.
---

# MoRE: Unlocking Scalability in Reinforcement Learning for Quadruped Vision-Language-Action Models

## Quick Facts
- arXiv ID: 2503.08007
- Source URL: https://arxiv.org/abs/2503.08007
- Reference count: 40
- Primary result: MoRE achieves 60% average success rate across six skills, outperforming baselines by up to 20%

## Executive Summary
MoRE introduces a scalable reinforcement learning framework for quadruped vision-language-action models by integrating a mixture-of-experts architecture with RL-based training objectives. The method fine-tunes a pre-trained MLLM backbone with LoRA experts to enable task-specific specialization while maintaining computational efficiency. By training as a Q-function on mixed-quality data (expert and sub-optimal trajectories), MoRE leverages offline RL to extract robust policies from automatically collected data. Extensive experiments demonstrate superior performance across six skills with a 60% average success rate, outperforming baselines by up to 20%.

## Method Summary
MoRE combines a mixture-of-experts architecture with reinforcement learning to create a scalable framework for quadruped VLA models. The approach fine-tunes a pre-trained MLLM backbone using LoRA experts, enabling task-specific specialization while maintaining computational efficiency. The model is trained as a Q-function using mixed-quality data from both expert and sub-optimal trajectories, leveraging offline RL techniques to extract robust policies. This design allows MoRE to scale effectively across multiple tasks while maintaining strong performance through efficient parameter utilization and robust policy learning from diverse data sources.

## Key Results
- Achieves 60% average success rate across six skills
- Outperforms baselines by up to 20% improvement
- Demonstrates strong generalization in out-of-distribution scenarios
- Successfully deployed on Unitree Go2 robot in real-world settings

## Why This Works (Mechanism)
MoRE's effectiveness stems from its hybrid architecture that combines the generalization capabilities of pre-trained MLLMs with the task-specific adaptability of LoRA experts. The mixture-of-experts routing mechanism allows the model to dynamically select relevant experts for different tasks, while the Q-function training on mixed-quality data enables robust policy learning from diverse trajectories. The offline RL approach is particularly effective because it can extract useful patterns from both high-quality expert demonstrations and sub-optimal data, making the training process more data-efficient and scalable.

## Foundational Learning
- **Reinforcement Learning Q-function training**: Essential for learning optimal policies from interaction data; quick check: verify Q-values converge during training
- **Mixture-of-Experts architecture**: Enables task-specific specialization while sharing common knowledge; quick check: monitor expert activation patterns across different tasks
- **LoRA fine-tuning**: Provides parameter-efficient adaptation of pre-trained models; quick check: measure parameter count vs performance trade-off
- **Offline RL on mixed-quality data**: Allows learning from both expert and sub-optimal trajectories; quick check: analyze performance on expert vs non-expert test scenarios
- **Vision-Language-Action integration**: Combines perception, reasoning, and control in unified framework; quick check: validate cross-modal alignment in decision-making
- **Task-specific expert routing**: Dynamically selects relevant experts for different tasks; quick check: evaluate routing accuracy and consistency

## Architecture Onboarding

**Component Map**: Pre-trained MLLM Backbone -> LoRA Expert Modules -> Mixture-of-Experts Router -> Q-function Head -> Action Output

**Critical Path**: Perception (Vision/Language) → Expert Routing → Q-value Estimation → Action Selection → Robot Execution

**Design Tradeoffs**: MoRE trades increased model complexity (multiple experts) for improved task specialization and data efficiency. The use of LoRA experts balances parameter efficiency with task-specific performance. The mixture-of-experts approach adds routing overhead but enables better scalability across diverse tasks compared to monolithic architectures.

**Failure Signatures**: Potential failures include expert collapse (where one expert dominates all tasks), routing errors leading to inappropriate expert selection, and Q-function overestimation on out-of-distribution states. The model may also struggle with tasks requiring fine-grained coordination between multiple experts.

**First Experiments**:
1. Evaluate individual expert performance on single tasks before mixture training
2. Test expert routing accuracy across different task categories
3. Measure Q-function learning stability with varying ratios of expert vs sub-optimal data

## Open Questions the Paper Calls Out
None identified in the provided information.

## Limitations
- Evaluation limited to six skills, may not capture full generalization capabilities
- Real-world deployment only tested on Unitree Go2 platform
- Computational efficiency gains from LoRA not quantitatively detailed
- No safety considerations or failure mode analysis provided

## Confidence
- Scalability claims: High
- RL-based training effectiveness: High
- Real-world deployment capabilities: Medium
- Computational efficiency benefits: Low
- Safety and robustness: Low

## Next Checks
1. Conduct comprehensive ablation study to evaluate individual contributions of mixture-of-experts, LoRA experts, and RL-based Q-function training
2. Expand real-world evaluation to multiple quadruped platforms with different hardware specifications
3. Perform detailed computational efficiency analysis comparing LoRA experts with other model compression techniques under various resource constraints