---
ver: rpa2
title: Explainable, Multi-modal Wound Infection Classification from Images Augmented
  with Generated Captions
arxiv_id: '2502.20277'
source_url: https://arxiv.org/abs/2502.20277
tags:
- wound
- image
- infection
- images
- scarwid
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of accurately classifying infections
  in diabetic foot ulcers (DFUs) using only wound images, which is particularly difficult
  for novice nurses without access to detailed clinical data. To overcome this limitation,
  the authors propose SCARWID, a novel deep learning framework that combines wound
  images with synthetic textual descriptions generated by a fine-tuned Vision-Language
  Model (Wound-BLIP).
---

# Explainable, Multi-modal Wound Infection Classification from Images Augmented with Generated Captions

## Quick Facts
- arXiv ID: 2502.20277
- Source URL: https://arxiv.org/abs/2502.20277
- Reference count: 40
- Achieves 85.2% sensitivity, 77.7% specificity, and 81.4% accuracy in classifying diabetic foot ulcer infections using only images and generated captions.

## Executive Summary
This study introduces SCARWID, a novel deep learning framework that addresses the challenge of accurately classifying infections in diabetic foot ulcers (DFUs) using only wound images. The framework combines wound images with synthetic textual descriptions generated by a fine-tuned Vision-Language Model (Wound-BLIP) using a cross-attention mechanism. SCARWID achieves superior performance compared to state-of-the-art models, with 85.2% sensitivity, 77.7% specificity, and 81.4% accuracy. The model also provides interpretable results through visual attention maps and generated captions that align with clinical knowledge, making it particularly useful for novice nurses without access to detailed clinical data.

## Method Summary
SCARWID uses a multi-modal approach combining wound images with synthetic captions generated by Wound-BLIP, a Vision-Language Model fine-tuned on GPT-4o-generated descriptions. The framework employs a cross-attention mechanism to fuse image and text embeddings, then classifies infections by retrieving the top-k most similar labeled examples from a support database. To enhance training data diversity, synthetic wound images are generated using a latent diffusion model. The model is trained with triplet loss to learn embedding similarity, and at inference, it retrieves top-k neighbors from a labeled support set to determine the infection status through majority voting.

## Key Results
- SCARWID achieves 85.2% sensitivity, 77.7% specificity, and 81.4% accuracy in DFU infection classification.
- The cross-attention fusion of image and text embeddings outperforms single-modality approaches, with Image+Text achieving ~2% higher sensitivity than Image Only.
- Retrieval-based classification using triplet-learned embeddings shows robustness, with lower standard deviations in evaluation scores across cross-validation folds compared to probability-based models.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-attention fusion of image and text embeddings improves infection classification accuracy over single-modality approaches.
- Mechanism: The Image-Text Fusion module uses image embeddings as queries (Q) and text embeddings as keys (K) and values (V) in a cross-attention layer. This allows the model to attend to image regions most relevant to textual wound descriptors, creating a joint embedding that disambiguates visually similar wounds.
- Core assumption: The synthetic captions from Wound-BLIP contain discriminative information not fully captured by image features alone.
- Evidence anchors:
  - [abstract] "an Image-Text Fusion module that uses cross-attention to extract cross-modal embeddings from an image and its corresponding Wound-BLIP caption"
  - [section 4.5] "SCARWID (Image Only) achieves a sensitivity of 83.1%, about 2% lower than that of SCARWID (Image & Text) and a specificity score of 73.6% is 4% lower"
  - [corpus] Weak direct corpus support for cross-attention specifically; related work uses multimodal fusion but not identical architecture.
- Break condition: If generated captions contain hallucinations or are semantically uninformative, cross-attention may amplify noise rather than signal.

### Mechanism 2
- Claim: Label-guided GPT-4o caption generation followed by BLIP fine-tuning produces clinically meaningful wound descriptions that enhance model reasoning.
- Mechanism: GPT-4o receives ground-truth infection labels and is prompted to describe visual attributes influencing the diagnosis. These 1,000 image-text pairs fine-tune BLIP's text decoder via language modeling loss, enabling Wound-BLIP to generate consistent descriptions at inference without label access.
- Core assumption: GPT-4o's visual analysis, when conditioned on expert labels, produces captions that reflect clinically relevant features—even though GPT-4o performs poorly on image-only medical diagnosis without such guidance.
- Evidence anchors:
  - [abstract] "Wound-BLIP, a Vision-Language Model (VLM) fine-tuned on GPT-4o-generated descriptions to synthesize consistent captions from images"
  - [section 3.3.2] "we utilize GPT-4o with our label-guided prompting technique to generate textual descriptions... initially informing the model of the ground-truth infection label"
  - [corpus] Related work (Multimodal AI on Wound Images, arXiv:2501.13247) uses clinical notes with wound images, supporting multimodal augmentation but not synthetic captioning.
- Break condition: If GPT-4o generates systematic errors or "hallucinated" features not present in images, Wound-BLIP will inherit these biases.

### Mechanism 3
- Claim: Retrieval-based classification using triplet-learned embeddings improves robustness over probability-based classifiers for wound infection detection.
- Mechanism: The Image-Text Fusion module is trained with triplet loss to minimize distances between same-class pairs and maximize distances between different-class pairs. At inference, the query embedding retrieves top-k neighbors from a labeled support database (N=1,024), and the majority label among neighbors determines the prediction.
- Core assumption: Infection status is reflected in the geometric structure of the cross-modal embedding space, and similar wounds cluster by infection status.
- Evidence anchors:
  - [abstract] "Infection status is determined by retrieving the top-k similar items from a labeled support set"
  - [section 4.5] "SCARWID exhibits lower standard deviations in evaluation scores across 5 folds during cross-validation, highlighting its robustness, especially when compared to probability-based models"
  - [section 4.6.1] "It is observed that infected and uninfected wounds are separated into two distinct clusters"
  - [corpus] ConDiff (Busaranuvong et al., 2024) also uses distance-based classification, achieving 85.4% sensitivity—corroborating retrieval/similarity approaches for this task.
- Break condition: If the support database lacks coverage of wound presentation diversity, retrieval fails for out-of-distribution cases.

## Foundational Learning

- Concept: **Cross-Attention in Transformers**
  - Why needed here: The Image-Text Fusion module uses cross-attention to align image patches with textual tokens. You must understand Q/K/V mechanics to debug attention patterns.
  - Quick check question: Given an image embedding as Q and text embedding as K/V, which modality's information is being selectively aggregated?

- Concept: **Triplet Loss and Metric Learning**
  - Why needed here: SCARWID learns embedding similarity via triplet loss (anchor, positive, negative with margin α=1). Understanding margin effects is critical for tuning.
  - Quick check question: If triplet loss converges but retrieval accuracy is poor, what might be wrong with the sampling strategy?

- Concept: **Vision-Language Model Fine-Tuning (BLIP)**
  - Why needed here: Wound-BLIP is created by freezing the image encoder and fine-tuning only the text decoder. You need to understand which parameters to freeze/unfreeze.
  - Quick check question: Why might freezing the image encoder be preferable when fine-tuning on a small medical dataset (N=1,000)?

## Architecture Onboarding

- Component map:
  ```
  Input Image → Wound-BLIP (ViT Image Encoder + Text Decoder) → Generated Caption
                    ↓                                           ↓
              DeiT Image Encoder                          CLIP Text Encoder
                    ↓                                           ↓
                    └──────── Cross-Attention Layer ──────────┘
                                      ↓
                            Cross-Modal Embedding (256-d)
                                      ↓
                        Euclidean Distance → Top-k Retrieval
                                      ↓
                           Majority Vote → Prediction
  ```

- Critical path:
  1. Verify Wound-BLIP caption quality on held-out images before training fusion module
  2. Ensure triplet sampling balances infected/uninfected pairs
  3. Confirm support database (D_support) contains at most one image per subject to prevent data leakage

- Design tradeoffs:
  - **k=5 retrieval vs. larger k**: Smaller k is more sensitive to local embedding quality; larger k smooths predictions but may include noisy neighbors.
  - **Diffusion augmentation (2,400 images) vs. manual augmentation**: Synthetic images improve accuracy 2.5–4.5% but risk introducing artifacts not representative of real wounds.
  - **Text-only vs. Image-only vs. Fusion**: Fusion outperforms both (Table 4), but adds inference latency and dependency on caption quality.

- Failure signatures:
  - Captions with features not visually present (hallucination): Check Grad-CAM alignment between described features and highlighted regions.
  - High sensitivity but low specificity: Model over-predicts infection; may need to adjust class balance in triplet sampling or increase margin α.
  - Cluster overlap in UMAP visualization: Embedding space not separating classes; check if triplet loss converged or if encoder initialization is poor.

- First 3 experiments:
  1. **Baseline sanity check**: Run SCARWID (Image Only) and SCARWID (Text Only) on the same test fold to confirm both modalities contribute (expect Image Only ~78% accuracy, Text Only ~75%).
  2. **Caption quality audit**: Sample 50 generated captions and manually verify alignment with image content; report hallucination rate.
  3. **Support database ablation**: Reduce D_support from 1,024 to 256 samples and measure accuracy drop to quantify retrieval sensitivity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can hallucinations in vision-language models be effectively mitigated in medical contexts to prevent erroneous wound descriptions from affecting diagnosis?
- Basis in paper: [explicit] The authors identify "erroneous wound descriptions due to hallucination" as a primary drawback and state, "This underscores the need for future research on mitigating the effects of hallucination of vision-language models in medical contexts."
- Why unresolved: The current framework relies on synthetic captions which may contain misalignments between the generated text and the actual visual content, creating a risk of misdiagnosis if used without validation.
- What evidence would resolve it: A study comparing standard VLM outputs against hallucination-suppressed variants, measuring the frequency of factual errors in generated wound descriptions.

### Open Question 2
- Question: Can Retrieval-Augmented Generation (RAG) be successfully integrated into this framework to provide more accurate, evidence-based clinical reasoning?
- Basis in paper: [explicit] The authors list "applying Retrieval-Augmented Generation (RAG) for MLLMs" as a potential future research direction to enable "more accurate and evidence-based clinical reasoning... through sophisticated search mechanisms."
- Why unresolved: While SCARWID uses retrieval for classification, it does not currently use retrieved knowledge to explicitly generate a clinical rationale or reasoning chain for the specific diagnosis.
- What evidence would resolve it: An implementation of RAG within the SCARWID pipeline that demonstrates higher factual accuracy in generated explanations compared to the standard Wound-BLIP model.

### Open Question 3
- Question: To what extent does the inclusion of structured, step-by-step reasoning prompts improve the diagnostic accuracy of the caption generation process?
- Basis in paper: [explicit] The Future Work section suggests designing prompts that compel GPT-4o to deliver structured responses reflecting "Image Comprehension, Recall of Medical Knowledge, and Step-by-Step Reasoning."
- Why unresolved: The current label-guided prompting technique generates concise descriptions but lacks an explicit reasoning chain, which may limit the model's ability to capture complex wound attributes.
- What evidence would resolve it: A comparative evaluation of caption quality and downstream classification performance between standard label-guided prompts and the proposed structured reasoning prompts.

## Limitations
- Reliance on GPT-4o-generated captions introduces potential hallucination risks, which may affect cross-attention fusion performance.
- Diffusion model for synthetic image generation lacks detailed specifications, making exact reproduction challenging.
- Subject-wise data splitting is critical but not fully specified for the support database construction.

## Confidence
- **High Confidence**: Retrieval-based classification mechanism and cross-attention fusion architecture are well-supported by empirical results and ablation studies.
- **Medium Confidence**: Caption quality impact is plausible but relies on unaudited GPT-4o outputs; diffusion model contributions are inferred from performance gains without full architectural details.
- **Low Confidence**: Exact support database sampling strategy and synthetic image generation parameters are underspecified.

## Next Checks
1. Audit caption quality: Manually inspect 50-100 Wound-BLIP outputs for hallucination and feature alignment.
2. Verify subject-wise splits: Confirm no subject overlap between training and test folds, especially in support database construction.
3. Ablate synthetic images: Train SCARWID without diffusion augmentation to quantify its contribution to performance gains.