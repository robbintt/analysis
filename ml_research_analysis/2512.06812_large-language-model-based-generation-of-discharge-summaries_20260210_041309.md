---
ver: rpa2
title: Large Language Model-Based Generation of Discharge Summaries
arxiv_id: '2512.06812'
source_url: https://arxiv.org/abs/2512.06812
tags:
- discharge
- notes
- summary
- information
- summaries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated five large language models (Mistral, Llama
  2, GPT-3.5, GPT-4, and Gemini 1.5 Pro) for generating discharge summaries from clinical
  notes in the MIMIC-III dataset. Models were assessed using exact-match (BLEU, ROUGE),
  soft-overlap (BERTScore, BLEURT), and reference-free metrics (BLANC, SummaC), alongside
  a qualitative clinical evaluation.
---

# Large Language Model-Based Generation of Discharge Summaries

## Quick Facts
- arXiv ID: 2512.06812
- Source URL: https://arxiv.org/abs/2512.06812
- Reference count: 40
- Large language models, especially proprietary ones with one-shot prompting, can generate clinically useful discharge summaries from MIMIC-III clinical notes, though hallucinations and missing information remain challenges.

## Executive Summary
This study evaluated five large language models (Mistral, Llama 2, GPT-3.5, GPT-4, and Gemini 1.5 Pro) for generating discharge summaries from clinical notes in the MIMIC-III dataset. Models were assessed using exact-match (BLEU, ROUGE), soft-overlap (BERTScore, BLEURT), and reference-free metrics (BLANC, SummaC), alongside a qualitative clinical evaluation. Proprietary models, especially Gemini 1.5 Pro with one-shot prompting, outperformed open-source models, achieving the highest similarity to gold-standard summaries. Fine-tuned Mistral showed competitive results but lagged in performance. Human evaluation confirmed the practical utility of summaries from proprietary models, though hallucinations and missing information remained challenges. Open-source models transcribed source information better but struggled with synthesis. Overall, proprietary models are promising for automatic discharge summary generation if data privacy is ensured.

## Method Summary
The study filtered MIMIC-III clinical notes to admissions with ≤7,600 tokens (removing error-flagged notes and summary addendums), creating 20,357 training and 982 test admissions. Open-source models (Mistral-7B and Llama-2-7B) were fine-tuned using QLoRA with specified hyperparameters on RTX 3090 (24GB VRAM). Proprietary models (GPT-3.5, GPT-4, Gemini 1.5 Pro) were evaluated using zero-shot and one-shot prompting with structured output formats. All models generated summaries on the test set, which were evaluated using exact-match (BLEU, ROUGE), soft-overlap (BERTScore, BLEURT), reference-free (BLANC, SummaC), and human evaluation (completeness/correctness/conciseness) metrics.

## Key Results
- Proprietary models, particularly Gemini 1.5 Pro with one-shot prompting, achieved the highest similarity to gold-standard summaries
- Fine-tuned Mistral showed competitive results but remained inferior to proprietary models in synthesis capability
- Open-source models better transcribed source information but struggled with synthesizing comprehensive discharge summaries
- Human evaluation confirmed practical utility of proprietary model summaries despite hallucinations and missing information

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** One-shot prompting substantially improves discharge summary quality over zero-shot prompting for proprietary models.
- **Mechanism:** A single input-output example provides format scaffolding and domain-specific priors, reducing output drift and aligning the model to the target document structure.
- **Core assumption:** The example is representative of the target distribution and does not leak artifacts into the generation.
- **Evidence anchors:**
  - [abstract] Proprietary models, particularly Gemini with one-shot prompting, outperformed others, producing summaries with the highest similarity to the gold-standard.
  - [section] Table 2 shows Gemini one-shot achieved 30.90% ROUGE-1 vs. 23.33% zero-shot; GPT-4 one-shot improved from 20.97% to 22.30%.
  - [corpus] LCDS paper notes hallucination risks even with strong LLMs, suggesting prompting alone may not fully address reliability.
- **Break condition:** When the one-shot example contains information that leaks into generations (e.g., hallucinated "low-residue diet" noted in qualitative evaluation), performance may degrade.

### Mechanism 2
- **Claim:** QLoRA fine-tuning enables open-source models to better transcribe source information but does not significantly improve synthesis capability.
- **Mechanism:** Low-rank adaptation modifies a small subset of weights, allowing the model to specialize on clinical vocabulary and note structure without full retraining.
- **Core assumption:** The fine-tuning dataset is sufficiently representative and the low-rank constraint does not under-capacity the adaptation.
- **Evidence anchors:**
  - [abstract] Fine-tuned Mistral showed competitive results but lagged in performance.
  - [section] Table 4 shows Mistral fine-tuned achieved 31.13% ROUGE-1 overlap with notes vs. 31.19% baseline; Table 2 shows only 0.74% ROUGE-1 gain against gold summaries.
  - [corpus] Related work on hallucinations in open-source LLMs suggests fine-tuning may not resolve synthesis failures without architectural or data improvements.
- **Break condition:** When constrained generation length or lack of structured prompts prevents the model from producing comprehensive outputs.

### Mechanism 3
- **Claim:** Proprietary models with large context windows synthesize multi-document inputs more effectively than smaller open-source models.
- **Mechanism:** Larger context windows (128K–1M tokens) allow simultaneous processing of all admission notes, enabling cross-document reasoning rather than isolated extraction.
- **Core assumption:** The model's attention mechanism effectively utilizes information across the full context, not just local windows.
- **Evidence anchors:**
  - [abstract] Open-source models transcribed source information better but struggled with synthesis.
  - [section] Gemini 1.5 Pro (1M tokens) and GPT-4 (128K tokens) outperformed Mistral/Llama-2 (4K–8K tokens); Table 2 shows Gemini one-shot achieving the best overall scores.
  - [corpus] Abstract Meaning Representation for Hospital Discharge Summarization highlights that long-context handling is critical for discharge summaries but hallucination remains a bottleneck.
- **Break condition:** When critical information appears early in the context and is attenuated by attention mechanisms, or when rate limits and costs restrict practical deployment.

## Foundational Learning

- **Concept:** Exact-match vs. semantic similarity metrics
  - **Why needed here:** BLEU and ROUGE measure surface-level overlap, while BERTScore and BLEURT capture semantic equivalence. The paper reports low exact-match scores but higher semantic similarity, explaining why proprietary models perform better than raw n-gram metrics suggest.
  - **Quick check question:** If a model uses "myocardial infarction" where the reference says "heart attack," which metric would penalize this more?

- **Concept:** QLoRA (Quantized Low-Rank Adaptation)
  - **Why needed here:** The paper uses QLoRA to fine-tune 7B models on 24GB VRAM. Understanding 4-bit quantization and low-rank decomposition explains why training was feasible but improvements were marginal.
  - **Quick check question:** Why might a low-rank adapter struggle to capture complex document-level reasoning patterns?

- **Concept:** Reference-free evaluation (BLANC, SummaC)
  - **Why needed here:** Discharge summaries may omit information present in notes or include clinician-added context. Reference-free metrics evaluate summary quality against source documents, not just gold references.
  - **Quick check question:** When might BLANC scores be negative, and what does this indicate about the generated summary?

## Architecture Onboarding

- **Component map:** MIMIC-III preprocessing (filter notes, remove errors/addendums) -> Model layer (Mistral/Llama-2 fine-tuned with QLoRA or GPT-3.5/4/Gemini via API) -> Adaptation (fine-tuning or structured prompting) -> Evaluation (exact-match, soft-overlap, reference-free, human)

- **Critical path:** 1. Filter notes to fit smallest context window (7,600 tokens) 2. Design structured prompt with section headers 3. Select one-shot example from training split 4. Run inference, parse structured output 5. Evaluate per-section and document-level metrics

- **Design tradeoffs:**
  - Proprietary vs. open-source: Performance vs. privacy/compliance (GDPR, HIPAA)
  - One-shot vs. zero-shot: Quality gains vs. context consumption and potential example leakage
  - Structured output vs. free-form: Consistency vs. exact-match penalty (section order may differ from reference)

- **Failure signatures:**
  - Low TTR with low BLEU → repetitive/looping generation (Llama-2)
  - High note-overlap but low gold-similarity → transcription without synthesis (Mistral)
  - High BERTScore but low ROUGE → correct meaning, wrong phrasing/format
  - Qualitative: Hallucinated details (e.g., dietary recommendations not in source), missing discharge-specific info (follow-up instructions often absent from notes)

- **First 3 experiments:**
  1. Establish baseline with zero-shot prompting on GPT-4 and Gemini using structured output format; evaluate all metrics on 50-sample subset.
  2. Compare one-shot prompting with carefully selected example (avoiding specific clinical details that could leak); measure improvement delta per section.
  3. Fine-tune Mistral-7B with QLoRA on full training set; compare transcription (note-overlap) vs. synthesis (gold-overlap) scores to identify adaptation gaps.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can incorporating multimodal data, specifically voice recordings, resolve the issue of missing information currently found in text-only clinical notes?
  - **Basis in paper:** [explicit] The authors explicitly suggest that future work should address missing data by "introducing alternative methods... such as voice recordings" and exploring "multimodal systems."
  - **Why unresolved:** Current models rely solely on text notes which often omit critical discharge details determined only at the moment of writing.
  - **What evidence would resolve it:** A study comparing discharge summaries generated from text-only inputs versus text-plus-voice inputs on the completeness metric.

- **Open Question 2:** Can larger open-source models (e.g., Llama-3 70B) match the synthesis capabilities of proprietary models while ensuring data privacy?
  - **Basis in paper:** [explicit] The authors explicitly call for exploring "larger variants such as Llama-3 70B" to bridge the performance gap with proprietary systems.
  - **Why unresolved:** Smaller open-source models (7B) lagged in reasoning and synthesis, while proprietary models pose privacy risks.
  - **What evidence would resolve it:** Benchmarking Llama-3 70B against GPT-4/Gemini using the same MIMIC-III dataset and evaluation metrics.

- **Open Question 3:** How does the "subjective nature of importance" across different clinical specialties impact the reliability of automated summarization evaluation metrics?
  - **Basis in paper:** [inferred] The authors note that "What is critical in one care unit may not be relevant in another," implying that standard metrics may not capture this nuance.
  - **Why unresolved:** Quantitative metrics assume a single gold standard, but clinical relevance is highly subjective and varies by professional and unit.
  - **What evidence would resolve it:** A multi-center human evaluation correlating automated metric scores with clinician satisfaction across diverse hospital departments.

## Limitations

- The study's one-shot example selection process was not standardized, potentially introducing variability in results
- QLoRA fine-tuning showed only marginal improvements, suggesting either insufficient adaptation capacity or suboptimal hyperparameter configuration
- Reliance on structured output format may artificially suppress exact-match metrics even when content is semantically correct

## Confidence

- **High confidence:** The relative performance ranking between proprietary and open-source models is robust, supported by consistent results across multiple evaluation metrics and human assessment.
- **Medium confidence:** The specific performance numbers (e.g., exact ROUGE scores) may be sensitive to prompt variations and one-shot example selection, though the directional conclusions remain reliable.
- **Medium confidence:** The clinical relevance assessment through human evaluation is meaningful but limited by the small sample size (10 summaries per model) and potential rater bias.

## Next Checks

1. **Prompt standardization test:** Systematically vary the one-shot example across different clinical scenarios and document lengths to measure performance sensitivity and identify optimal selection criteria.
2. **Fine-tuning optimization study:** Experiment with different QLoRA rank parameters and adapter architectures to determine if the marginal gains observed represent a fundamental limitation or optimization opportunity.
3. **Hallucination audit:** Conduct detailed qualitative analysis of hallucinations across all models, particularly focusing on patterns in missing discharge-specific information versus hallucinated details, to inform targeted mitigation strategies.