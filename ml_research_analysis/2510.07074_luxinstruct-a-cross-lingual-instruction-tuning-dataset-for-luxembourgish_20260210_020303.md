---
ver: rpa2
title: 'LuxInstruct: A Cross-Lingual Instruction Tuning Dataset For Luxembourgish'
arxiv_id: '2510.07074'
source_url: https://arxiv.org/abs/2510.07074
tags:
- luxembourgish
- instruction
- language
- tuning
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LUXINSTRUCT, a cross-lingual instruction
  tuning dataset for Luxembourgish designed to overcome the scarcity of high-quality
  instruction data for low-resource languages. Instead of relying on machine translation,
  the dataset is constructed using aligned data from English, French, and German,
  preserving linguistic and cultural nuances.
---

# LuxInstruct: A Cross-Lingual Instruction Tuning Dataset For Luxembourgish

## Quick Facts
- arXiv ID: 2510.07074
- Source URL: https://arxiv.org/abs/2510.07074
- Reference count: 40
- Creates first high-quality instruction dataset for Luxembourgish without machine translation

## Executive Summary
LuxInstruct introduces a cross-lingual instruction tuning dataset for Luxembourgish, addressing the scarcity of high-quality instruction data for low-resource languages. The dataset leverages aligned data from English, French, and German sources to generate instruction-output pairs, avoiding machine translation artifacts. Empirical results demonstrate that cross-lingual instruction tuning improves representational alignment between Luxembourgish and other languages while enhancing the model's generative capabilities in Luxembourgish.

## Method Summary
The authors construct LuxInstruct using native Luxembourgish Wikipedia articles, RTL news articles, and an online dictionary, combined with aligned data from English, French, and German sources. They employ a reverse instruction generation approach where an LLM (GPT-4.1-mini) extracts spans from Luxembourgish text and writes corresponding English instructions. The dataset covers seven task types including open-ended generation, article-to-title, and word translation. Fine-tuning uses LoRA on attention projections with specific hyperparameters, and evaluation employs G-Eval using three LLM judges (GPT-5 mini, Gemini 2.5 Flash-Lite, DeepSeek-V3).

## Key Results
- Cross-lingual instruction tuning improves representational alignment between Luxembourgish and other languages (measured by CKA scores)
- Models trained with cross-lingual data show superior generative capabilities in Luxembourgish compared to monolingual tuning
- Cross-lingual approach outperforms traditional machine translation-based methods in preserving linguistic and cultural nuances

## Why This Works (Mechanism)

### Mechanism 1: Cross-Lingual Representation Alignment
Cross-lingual instruction tuning maps distinct language embedding spaces closer together by forcing semantic links between inputs in one language and targets in another. This enables knowledge transfer from high-resource to low-resource representations. The approach assumes pre-trained models possess shared multilingual representations that can be refined rather than requiring alignment from scratch. Evidence shows increased CKA scores for EN-LB and FR-LB configurations compared to monolingual LB-LB tuning.

### Mechanism 2: Native Text Output Preservation
Using native human-written text for outputs avoids "translationese" artifacts that degrade model performance. By sourcing outputs from native Wikipedia, news, and dictionary entries rather than machine translation, the dataset preserves idiomatic structures and cultural context. This prevents the model from learning statistical noise common in MT outputs. The approach assumes native text available online is of sufficient quality to serve as robust training signals.

### Mechanism 3: Cross-Lingual Transfer of Reasoning Patterns
Exposure to complex instructions in high-resource languages allows models to "borrow" syntactic complexity and reasoning patterns, applying them to the low-resource target language. This works because reasoning capabilities are somewhat language-agnostic at the latent level, enabling cross-lingual transfer of instruction-following behavior. Evidence shows English/French instructions generally yield higher G-Eval scores than monolingual Luxembourgish examples.

## Foundational Learning

- **Concept: Instruction Tuning**
  - Why needed: This is the core technique - fine-tuning models to follow specific task formats like "Summarize this" rather than learning language structure
  - Quick check: How does showing a model (Instruction, Output) pairs differ from just showing it raw text?

- **Concept: Cross-Lingual Transfer**
  - Why needed: The paper relies on the premise that a model can learn a task in English and perform it in Luxembourgish
  - Quick check: Why would training on "English Instruction -> Luxembourgish Output" help a model answer "Luxembourgish Instruction -> Luxembourgish Output"?

- **Concept: Centered Kernel Alignment (CKA)**
  - Why needed: The paper uses CKA to mathematically prove that the "mental representation" of concepts in the model is becoming similar across languages
  - Quick check: If CKA score increases between English and Luxembourgish representations after training, what does that imply about how the model stores the concept of "dog" in both languages?

## Architecture Onboarding

- **Component map:** Seed Data (Wikipedia, RTL News, LOD Dictionary) -> Generation Engine (GPT-4.1-mini) -> Filtering Layer (Heuristic filters) -> Training Layer (LoRA fine-tuning) -> Evaluation Layer (G-Eval framework)
- **Critical path:** The Reverse Instruction Generation process where the system takes Luxembourgish sentences, asks an LLM "What question could have generated this?", and creates English instruction pairs
- **Design tradeoffs:** The authors trade volume/cheapness (machine translation) for quality/cultural integrity (native/human data); they find using more distant languages (EN/FR) for instructions might align representations better than closely related ones (DE)
- **Failure signatures:** Low CKA scores indicate failed cross-lingual gap bridging; fluency drops suggest instruction logic misalignment; cultural hallucination indicates over-reliance on English parametric knowledge
- **First 3 experiments:**
  1. Fine-tune a base model on the EN-LB subset and measure CKA score shift against a baseline
  2. Train separate adapters for EN-LB, DE-LB, and FR-LB, comparing performance on standardized Luxembourgish instructions
  3. Randomly sample 50 generated pairs and manually verify if English instructions match Luxembourgish outputs

## Open Questions the Paper Calls Out
- How to address the restricted diversity limitation, as the dataset currently covers only seven task types
- Whether the observed cross-lingual alignment benefits generalize to other low-resource languages beyond Luxembourgish

## Limitations
- The dataset focuses exclusively on Luxembourgish, limiting generalization to other low-resource languages
- Evaluation relies on a small set of 50 manually curated test instructions, potentially affecting robustness
- Performance improvements depend heavily on base model's existing multilingual capabilities, with models lacking LB support showing limited gains

## Confidence
- **High Confidence:** Cross-lingual instruction tuning improves representational alignment across languages (strongly supported by CKA score increases)
- **Medium Confidence:** Cross-lingual tuning enhances generative capabilities in Luxembourgish (empirical support from G-Eval scores but depends on test instruction quality)
- **Medium Confidence:** Native text outputs avoid "translationese" artifacts (logically sound but not directly measured)

## Next Checks
1. Apply the LuxInstruct methodology to another low-resource language (e.g., Icelandic or Basque) to test cross-lingual alignment benefits transfer
2. Conduct human evaluation on 100 random instruction-output pairs to verify GPT-4.1-mini generation process consistency
3. Systematically test base models with varying Luxembourgish support levels to determine minimum language capacity required for effective cross-lingual instruction tuning