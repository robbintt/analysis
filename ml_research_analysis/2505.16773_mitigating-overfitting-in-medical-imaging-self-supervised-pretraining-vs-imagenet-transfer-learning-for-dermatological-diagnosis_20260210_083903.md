---
ver: rpa2
title: 'Mitigating Overfitting in Medical Imaging: Self-Supervised Pretraining vs.
  ImageNet Transfer Learning for Dermatological Diagnosis'
arxiv_id: '2505.16773'
source_url: https://arxiv.org/abs/2505.16773
tags:
- learning
- dermatological
- imagenet
- self-supervised
- overfitting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compares self-supervised pretraining (using a Variational
  Autoencoder trained from scratch on a proprietary dermatological dataset) with ImageNet
  transfer learning for dermatological image classification. While the ImageNet-pretrained
  model converges faster and achieves higher initial accuracy, it exhibits signs of
  overfitting and stagnation in validation performance.
---

# Mitigating Overfitting in Medical Imaging: Self-Supervised Pretraining vs. ImageNet Transfer Learning for Dermatological Diagnosis

## Quick Facts
- arXiv ID: 2505.16773
- Source URL: https://arxiv.org/abs/2505.16773
- Authors: Iván Matas; Carmen Serrano; Miguel Nogales; David Moreno; Lara Ferrándiz; Teresa Ojeda; Begoña Acha
- Reference count: 10
- Primary result: Self-supervised VAE pretraining on dermatological data reduces overfitting and improves generalization compared to ImageNet transfer learning.

## Executive Summary
This study compares self-supervised pretraining using a Variational Autoencoder (VAE) trained from scratch on dermatological data with ImageNet transfer learning for dermatological image classification. While the ImageNet-pretrained model converges faster and achieves higher initial accuracy, it exhibits signs of overfitting and stagnation in validation performance. The self-supervised model shows steady improvements without overfitting, achieving strong generalization. The VAE approach forces the model to learn clinically relevant features and uses KL-divergence regularization to create a smoother latent space, preventing overfitting on small medical datasets.

## Method Summary
The study employs a two-stage training approach. In Stage I, a ConvNeXt-Tiny encoder is trained from scratch as part of a VAE on unlabeled dermatological images using an ELBO loss function with KL warm-up. In Stage II, the pre-trained encoder is frozen and used with a simple MLP classifier head trained on labeled data using Focal Loss. This is compared against an ImageNet-pretrained ConvNeXt-Tiny backbone with the same classifier head. The key difference is that the self-supervised model learns domain-specific features, while the ImageNet model may suffer from negative transfer of non-medical features.

## Key Results
- Self-supervised model achieved final validation loss of 0.110 (-33.33% reduction) versus ImageNet model's 0.100 (-16.67%)
- Self-supervised model improved validation accuracy from 45% to 65% (+44.44%) with minimal overfitting gap
- ImageNet model showed 87% training accuracy (+50.00%) versus 75% validation (+19.05%) with increasing overfitting gap of +0.060

## Why This Works (Mechanism)

### Mechanism 1: Domain-Specific Feature Prioritization
Training a VAE from scratch on in-domain dermatological data forces the encoder to prioritize clinically relevant features, avoiding negative transfer of non-medical features from ImageNet. The reconstruction task shapes the latent space to represent salient features like texture and border irregularity.

### Mechanism 2: KL-Divergence Regularization in Latent Space
The KL-divergence term in the VAE's ELBO loss acts as a regularizer, creating a smoother latent space that prevents overfitting. This forces latent vectors to approximate a standard normal distribution, preventing the encoder from memorizing training examples.

### Mechanism 3: Domain Shift and Spurious Correlations
ImageNet pretraining accelerates convergence but can initialize the model with suboptimal feature detectors for medical imaging. This leads to faster overfitting on spurious correlations when fine-tuned on specialized datasets.

## Foundational Learning

- **Concept: Variational Autoencoders (VAEs) and the ELBO**
  - Why needed: The core method is training a VAE; understanding ELBO (loss function balancing reconstruction and regularization) is critical.
  - Quick check: If you increase the KL-divergence weight in ELBO, what happens to reconstructed image sharpness and overfitting tendency?

- **Concept: Transfer Learning vs. Training from Scratch**
  - Why needed: The entire study compares these approaches; understanding the trade-off between leveraging pre-learned features and domain-specific bias is essential.
  - Quick check: Why might ImageNet-trained models perform poorly or overfit on small medical imaging datasets?

- **Concept: Overfitting and the Train/Validation Gap**
  - Why needed: The central thesis is that one method mitigates overfitting better; reading learning curves and understanding metrics like "overfitting gap" (Train Acc - Val Acc) is fundamental.
  - Quick check: If training accuracy is 98% but validation accuracy is 70%, what does the 28% gap suggest about the model's learning?

## Architecture Onboarding

- **Component map:** ConvNeXt-Tiny Encoder -> Latent Space (256 dims) -> Custom Decoder (Stage I) or MLP Classifier Head (Stage II)

- **Critical path:**
  1. Stage I: Train full VAE (Encoder + Decoder) on unlabeled dermatological images for 300 epochs using ELBO loss
  2. Checkpoint: Save trained Encoder weights, discard Decoder
  3. Stage II: Initialize new Encoder with saved weights, freeze all layers
  4. Stage II: Attach classifier head, train only head (and fine-tune final encoder stage) on labeled data for 30-70 epochs using Focal Loss

- **Design tradeoffs:**
  - ConvNeXt-Tiny chosen for efficiency; larger models would overfit more on small datasets
  - Freezing encoder body (only fine-tuning final stage) is key regularization choice
  - Focal Loss handles class imbalance; Cross-Entropy would be dominated by common lesion types

- **Failure signatures:**
  - KL vanishing: If KL term goes to zero early, model learns unstructured latent space
  - Overfitting in Model B: Rapidly decreasing training loss with stagnating or increasing validation loss
  - Underfitting in Model A: Validation loss doesn't decrease long-term, suggesting low learning rate or insufficient model capacity

- **First 3 experiments:**
  1. Reproduce ImageNet transfer learning pipeline (Model B) to confirm high initial accuracy but significant overfitting gap
  2. Implement VAE pretraining with ConvNeXt-Tiny encoder and custom decoder; monitor ELBO loss and KL divergence
  3. Ablation study on classifier head: experiment with different architectures after VAE pretraining to optimize validation performance

## Open Questions the Paper Calls Out
None

## Limitations
- Results based on proprietary dataset of undisclosed size, limiting generalizability
- Only ConvNeXt-Tiny architecture evaluated; optimal model scale for self-supervised pretraining unknown
- No ablation studies on VAE hyperparameters (latent dimension, KL warm-up schedule) or classifier head architecture

## Confidence

- **High Confidence:** Empirical observation that self-supervised model shows lower final validation loss and smaller overfitting gap
- **Medium Confidence:** Causal claim that ImageNet pretraining leads to overfitting due to negative transfer of non-medical features
- **Low Confidence:** Specific mechanism that KL-divergence term is primary reason for superior generalization

## Next Checks

1. **Dataset Scaling Study:** Replicate experiment on datasets of varying sizes to confirm VAE pretraining consistently mitigates overfitting across scales

2. **Architecture Ablation:** Perform ablation study on VAE, varying latent dimension size and KL warm-up schedule to determine optimal configuration

3. **Extended Comparison:** Compare VAE pretraining to domain-specific pretraining (e.g., on public dermatology dataset) and training from scratch without pretraining