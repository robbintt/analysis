---
ver: rpa2
title: 'UNO: Unifying One-stage Video Scene Graph Generation via Object-Centric Visual
  Representation Learning'
arxiv_id: '2509.06165'
source_url: https://arxiv.org/abs/2509.06165
tags:
- object
- slots
- video
- vidsgg
- relation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: UNO is a unified, single-stage framework for video scene graph
  generation that jointly handles both box-level (DSGG) and pixel-level (PVSG) tasks.
  The core innovation is an object-centric representation based on extended slot attention
  that decomposes visual features into object and relation slots, enabling shared
  latent representations across both tasks.
---

# UNO: Unifying One-stage Video Scene Graph Generation via Object-Centric Visual Representation Learning

## Quick Facts
- **arXiv ID:** 2509.06165
- **Source URL:** https://arxiv.org/abs/2509.06165
- **Authors:** Huy Le; Nhat Chung; Tung Kieu; Jingkang Yang; Ngan Le
- **Reference count:** 40
- **Primary result:** Achieves 45.2% R@20 on DSGG and 9.44% R@20 on PVSG tasks

## Executive Summary
UNO presents a unified, single-stage framework for video scene graph generation that jointly handles both box-level (DSGG) and pixel-level (PVSG) tasks. The core innovation is an object-centric representation based on extended slot attention that decomposes visual features into object and relation slots, enabling shared latent representations across both tasks. To maintain temporal consistency, UNO introduces object temporal consistency learning without explicit tracking modules, and employs a dynamic triplet prediction module that links relation slots to corresponding object pairs while reducing redundancy. Experiments on Action Genome and PVSG benchmarks show UNO outperforms state-of-the-art methods, achieving 45.2% R@20 on DSGG and 9.44% R@20 on PVSG tasks.

## Method Summary
UNO is a unified, single-stage framework for video scene graph generation handling both box-level (DSGG) and pixel-level (PVSG) tasks. The method uses frozen visual encoders (ViT-S/14, ViT-B/14, or ResNet-50) and decomposes visual features into object and relation slots via extended slot attention. Object temporal consistency learning aligns slots across frames without explicit tracking, while a dynamic triplet prediction module maps relation slots to subject-object pairs. The framework is trained separately per task using Hungarian matching for assignment, optimizing combined losses including classification, localization, and temporal consistency objectives.

## Key Results
- Achieves 45.2% R@20 on DSGG (box-level scene graph generation)
- Achieves 9.44% R@20 on PVSG (pixel-level scene graph generation)
- Demonstrates improved efficiency through unified architecture compared to separate models for each task

## Why This Works (Mechanism)
The object-centric slot attention mechanism effectively decomposes complex visual scenes into distinct object and relation representations, allowing the model to capture both spatial and relational information in a unified manner. The temporal consistency learning ensures that object representations remain stable across video frames without requiring explicit tracking modules, reducing computational overhead while maintaining temporal coherence. The dynamic triplet prediction module efficiently maps relation slots to object pairs through learned reference embeddings, reducing redundancy compared to traditional pairwise approaches.

## Foundational Learning
- **Slot Attention:** Decomposes visual features into distinct object and relation representations - needed for separating object identities from their relationships; quick check: verify attention weights properly distribute across different visual regions
- **Hungarian Matching:** Assigns predictions to ground truth for training - needed for handling varying numbers of objects and relations; quick check: confirm matching cost computation correctly weights different loss components
- **Temporal Consistency Learning:** Aligns object representations across frames - needed for maintaining object identity without explicit tracking; quick check: monitor contrastive loss values between consecutive frames
- **Dynamic Triplet Prediction:** Maps relation slots to subject-object pairs - needed for efficient relation prediction without exhaustive pairwise computation; quick check: verify reference embeddings correctly identify corresponding object pairs
- **Unified Architecture:** Single model handling both DSGG and PVSG tasks - needed for efficiency and shared representation learning; quick check: compare parameter count and inference time against separate models

## Architecture Onboarding
**Component Map:** Visual Encoder -> Slot Attention -> Object Slots + Relation Slots -> Prediction Heads -> Losses (DSGG/PVSG)

**Critical Path:** Visual Encoder → Slot Attention → Object/Relation Slots → Dynamic Triplet Prediction → Relation Classification → Hungarian Matching → Loss Optimization

**Design Tradeoffs:** Unified architecture reduces model complexity but requires careful balancing of losses for different task objectives. Object-centric decomposition improves efficiency but may struggle with crowded scenes. Temporal consistency learning avoids explicit tracking but depends on reliable slot matching across frames.

**Failure Signatures:** Slot collapse (multiple slots binding to same region), temporal inconsistency (objects drifting across frames), relation-slot misalignment (incorrect subject-object pair mapping), and loss imbalance (one task dominating training).

**Three First Experiments:**
1. Verify slot attention properly decomposes features by visualizing attention maps and ensuring distinct slots capture different objects
2. Test temporal consistency loss by measuring slot similarity scores across consecutive frames with and without the contrastive loss
3. Validate dynamic triplet prediction by checking whether relation slots correctly map to ground truth subject-object pairs through nearest neighbor search

## Open Questions the Paper Calls Out
None

## Limitations
- Missing crucial implementation details including loss weights, optimizer settings, and training hyperparameters
- Object-centric slot attention effectiveness depends heavily on unspecified architectural choices
- Temporal consistency learning may struggle with occlusions or rapid motion without explicit tracking
- PVSG evaluation uses relatively small test set (100 images per vIOU), limiting statistical significance

## Confidence
- **High confidence**: Unified architecture combining slot attention with temporal consistency learning is technically sound and well-motivated
- **Medium confidence**: Experimental results are reproducible in principle but require missing hyperparameters for exact replication
- **Low confidence**: Scalability claims to other tasks are based on single demonstration without thorough analysis

## Next Checks
1. Reconstruct the temporal consistency loss implementation to verify correct computation of similarities between object slots across consecutive frames
2. Validate the dynamic triplet prediction module by testing reference embedding mechanism and stability during training
3. Replicate the slot attention ablation by implementing baseline without slot attention and measuring performance drop on both DSGG and PVSG tasks