---
ver: rpa2
title: The Effects of Demographic Instructions on LLM Personas
arxiv_id: '2505.11795'
source_url: https://arxiv.org/abs/2505.11795
tags:
- llms
- sexism
- sexist
- annotators
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explores whether large language models (LLMs) can accurately
  detect sexist content in tweets while accounting for demographic perspectives. Using
  the EXIST 2023 dataset with annotations from diverse annotators, the research evaluates
  five LLMs (GPT-3.5, GPT-4, GPT-4o, Mistral, and Qwen) for bias toward gender and
  age groups.
---

# The Effects of Demographic Instructions on LLM Personas

## Quick Facts
- **arXiv ID:** 2505.11795
- **Source URL:** https://arxiv.org/abs/2505.11795
- **Reference count:** 36
- **One-line primary result:** Persona prompting shows inconsistent effects on demographic alignment in LLM sexism detection.

## Executive Summary
This study investigates whether demographic persona instructions can mitigate bias in large language models when detecting sexist content in tweets. Using the EXIST 2023 dataset with annotations from diverse annotators, the research evaluates five LLMs (GPT-3.5, GPT-4, GPT-4o, Mistral, and Qwen) for bias toward gender and age groups. Results show LLMs exhibit higher agreement with female annotators than male, indicating gender-based bias. Attempts to mitigate this through demographic persona instructions yielded inconsistent results, with some models showing improved alignment with specific groups while others did not. The findings highlight the limitations of persona prompting as a bias mitigation strategy and underscore the need for further research into intersectional demographic factors and broader model testing.

## Method Summary
The study uses the EXIST 2023 dataset containing 7,958 tweets, each annotated by six annotators stratified by gender and age. Five LLMs are evaluated: GPT-3.5, GPT-4, GPT-4o, Mistral-22B, and Qwen-14B. The researchers employ Krippendorff's α to measure agreement between model outputs and human annotations across demographic groups. Baseline runs are conducted without demographic cues, followed by persona-conditioned runs with gender and age instructions inserted into prompts. Bootstrap confidence intervals (10,000 iterations) are computed for all results.

## Key Results
- All five tested LLMs show higher agreement with female annotators than male annotators (e.g., GPT-3.5: 0.415 vs 0.371).
- Persona prompting produces inconsistent effects: GPT-4F, GPT-4oF, and MistralF showed increased female alignment, while GPT-3.5F showed decreased alignment.
- The study cannot confirm persona prompting as a reliable bias mitigation strategy due to inconsistent results across models.

## Why This Works (Mechanism)

### Mechanism 1: Inherent Demographic Alignment in Base Models
Pre-training and RLHF processes may embed demographic patterns from training data that reflect stereotypical associations with content moderation sensitivity. The paper reports all five tested LLMs showed higher Krippendorff's α agreement with female annotators than male (e.g., GPT-3.5: 0.415 vs 0.371).

### Mechanism 2: Perspectivist Data Structure Enables Demographic Analysis
The EXIST 2023 dataset structure (6 annotators per tweet, stratified by gender × age) preserves disagreement patterns. Soft-labeling with probabilistic distributions captures annotator variance rather than forcing binary consensus.

### Mechanism 3: Persona Prompting Does NOT Reliably Shift Demographic Alignment
Instructing LLMs to adopt demographic personas produces inconsistent and unpredictable effects on output alignment. Some models (GPT-4F, MistralF) showed increased female alignment when given female persona; others (GPT-3.5F) showed decreased alignment.

## Foundational Learning

- **Concept: Krippendorff's α (Inter-rater Reliability)**
  - Why needed here: The paper uses this metric to quantify agreement between LLM outputs and human annotator groups. Values closer to 1.0 indicate stronger agreement; the paper reports α ranging from ~0.19 to ~0.48.
  - Quick check question: If a model has α = 0.40 with female annotators and α = 0.35 with male annotators, what does this tell you about its demographic bias?

- **Concept: Perspectivism in ML Annotation**
  - Why needed here: Traditional approaches aggregate annotations into single "gold standard" labels; perspectivism retains disagreement to model subjective tasks like sexism detection where no single ground truth exists.
  - Quick check question: Why might forcing consensus on "is this sexist?" produce a less useful dataset than preserving annotator disagreement?

- **Concept: Persona Prompting**
  - Why needed here: The intervention tested in this paper—adding demographic instructions to prompts to shift model behavior. Understanding what it is (and its limitations) is critical before attempting to use it.
  - Quick check question: Based on this paper's findings, should you recommend persona prompting as a reliable method to reduce demographic bias in production systems?

## Architecture Onboarding

- **Component map:**
  Input Layer: Raw tweets from EXIST 2023 dataset
  ↓
  Prompt Template: Baseline OR persona-enhanced (gender + age placeholders)
  ↓
  LLM Layer: 5 models tested (GPT-3.5, GPT-4, GPT-4o, Mistral-22B, Qwen-14B)
  ↓
  Output: Binary classification (YES/NO for sexism)
  ↓
  Evaluation: Krippendorff's α computed against human annotations stratified by gender × age

- **Critical path:**
  1. Obtain EXIST 2023 dataset with demographic annotation metadata
  2. Implement prompt template with optional demographic placeholders
  3. Generate LLM predictions for each tweet (baseline + each persona variant)
  4. Stratify human annotations by demographic group
  5. Compute Krippendorff's α between LLM outputs and each demographic subgroup
  6. Bootstrap confidence intervals (10,000 iterations per paper protocol)

- **Design tradeoffs:**
  - Binary classification vs. multi-category sexism taxonomy (paper notes future work on granular categories)
  - Single-factor personas (gender OR age) vs. intersectional (both simultaneously)—paper tested separately, not combined
  - Model selection: OpenAI models declining in agreement from GPT-3.5 → GPT-4 → GPT-4o suggests newer models may have different bias profiles

- **Failure signatures:**
  - Persona prompting reduces alignment instead of improving it (observed in GPT-3.5F)
  - Model agreement drops near chance (GPT-4o baseline α = 0.228 with females, 0.191 with males)
  - Confidence intervals overlap across conditions (not observed in paper; all CIs < 0.001)

- **First 3 experiments:**
  1. **Replicate baseline gender bias finding:** Run GPT-4o on 100 tweets from EXIST 2023 without persona instructions; compute α separately for male and female annotator labels. Confirm female alignment exceeds male.
  2. **Test intersectional persona prompting:** Combine gender + age in single prompt (e.g., "You are a female aged 46+")—paper notes this as future work. Compare to single-factor results.
  3. **Prompt sensitivity check:** Test 3 alternative phrasings of persona instructions on same model (GPT-4 recommended as middle performer). Measure variance in alignment shifts to assess robustness of the intervention.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does combining multiple demographic factors (intersectional personas) produce more consistent alignment with human annotators than single-factor personas?
- Basis in paper: [explicit] "While the current study addresses one-dimensional personas, we aim to explore demographic intersectionality in future work by examining how multiple factors interact and whether this results in more consistent alignment across LLMs."
- Why unresolved: The study only tested gender and age independently; humans possess multi-faceted identities that may interact in complex ways.
- What evidence would resolve it: Experiments with combined persona prompts (e.g., "female, age 23-45") compared against intersectional human annotator subgroups, measuring whether agreement improves consistently across models.

### Open Question 2
- Question: Why does persona prompting improve alignment in some LLM architectures but decrease alignment in others?
- Basis in paper: [inferred] Tables 2-3 show GPT-4, Mistral, and Qwen generally improved with age-based personas, while GPT-3.5 and GPT-4o showed inconsistent or negative effects. The authors state "incorporating gender-based personas in prompts should not be assumed to mitigate gender bias" but do not explain architectural causes.
- Why unresolved: The paper documents the inconsistency but does not investigate model architecture, training data, or instruction-tuning differences that might explain divergent behaviors.
- What evidence would resolve it: Controlled experiments across model variants with documented training differences, or mechanistic analysis of how persona tokens influence internal representations.

### Open Question 3
- Question: Do findings about demographic bias and persona prompting generalize beyond sexism detection to other subjective classification tasks?
- Basis in paper: [inferred] The study focuses exclusively on sexism detection using the EXIST 2023 dataset. The authors note that "relevance judgments" and other subjective tasks may have similar issues, but this remains untested.
- Why unresolved: Sexism detection may have unique properties; bias patterns and persona effectiveness could differ for hate speech, sentiment analysis, or relevance assessment tasks.
- What evidence would resolve it: Replication of the experimental design across multiple subjective tasks (e.g., hate speech detection, political bias classification) with diverse datasets containing demographic annotator information.

## Limitations

- The study tested only single-factor personas (gender OR age separately, not intersectional combinations) using static text prompts without iterative refinement or multi-shot examples.
- The inconsistent effects across models suggest the intervention's effectiveness is highly model-dependent and not reliably generalizable.
- The reliance on binary sexism classification limits understanding of nuanced bias patterns, and the soft-label agreement metric interpretation depends on how annotator disagreement is weighted.

## Confidence

- **High confidence:** The finding that all five tested LLMs show higher agreement with female annotators than male annotators (0.415 vs 0.371 for GPT-3.5 as example). This pattern is consistently observed across models and supported by the dataset's demographic annotation structure.
- **Medium confidence:** The claim that persona prompting is not a reliable bias mitigation strategy. While the directional effects are clear, the inconsistency across models and the limited prompt engineering approaches tested warrant caution in generalizing this conclusion.
- **Low confidence:** The specific numerical differences in alignment shifts when applying persona prompts, as these vary substantially by model and may depend on prompt phrasing not fully explored in the study.

## Next Checks

1. **Replicate the gender bias baseline:** Run the same five LLMs on a random sample of 100 tweets from EXIST 2023 without any persona instructions. Compute Krippendorff's α separately for male and female annotator groups to confirm the directional pattern (female > male) holds consistently.

2. **Test intersectional persona prompting:** Combine gender and age demographic instructions in single prompts (e.g., "You are a male aged 18-22") to evaluate whether this produces different effects than single-factor prompting. Compare alignment changes to the single-factor results reported in the paper.

3. **Prompt sensitivity analysis:** Implement three alternative phrasings of the demographic persona instructions (varying formality, explicitness, or framing) on GPT-4 to measure variance in alignment shifts. This will assess whether the persona prompt formulation itself explains the inconsistent effects observed across models.