---
ver: rpa2
title: 'ViCocktail: Automated Multi-Modal Data Collection for Vietnamese Audio-Visual
  Speech Recognition'
arxiv_id: '2506.04635'
source_url: https://arxiv.org/abs/2506.04635
tags:
- speech
- recognition
- audio
- attention
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a fully automated pipeline for generating audio-visual
  speech recognition (AVSR) datasets from raw video, demonstrating its effectiveness
  by creating the first Vietnamese AVSR dataset (ViCocktail) with 269 hours of training
  data. The pipeline extracts lip regions, audio, and transcriptions using automated
  tools, including an ASR model for Vietnamese text generation.
---

# ViCocktail: Automated Multi-Modal Data Collection for Vietnamese Audio-Visual Speech Recognition

## Quick Facts
- **arXiv ID**: 2506.04635
- **Source URL**: https://arxiv.org/abs/2506.04635
- **Reference count**: 0
- **Primary result**: First Vietnamese AVSR dataset (ViCocktail) with 269h training data, showing AVSR significantly outperforms audio-only models in noisy conditions (9.40% WER clean vs 18.39% WER in -5 dB cocktail-party noise, vs 70.01% for audio-only)

## Executive Summary
This paper presents ViCocktail, the first Vietnamese audio-visual speech recognition (AVSR) dataset, created through an automated pipeline that extracts lip regions, audio, and transcriptions from raw YouTube videos. The pipeline employs a cascade of quality filters including face detection, active speaker detection, audio-visual synchronization, and automatic transcription using a strong Vietnamese ASR model. Experiments demonstrate that AVSR models significantly outperform audio-only models in noisy environments, with the best AV-HuBERT model achieving 9.40% WER in clean conditions and only 18.39% WER in severe cocktail-party noise (-5 dB SNR), compared to 70.01% WER for audio-only models. The work highlights AVSR's robustness to noise and provides an open-source pipeline and dataset for expanding AVSR to low-resource languages.

## Method Summary
The method involves a fully automated pipeline that converts raw YouTube videos into clean AVSR training data without manual annotation. The pipeline stages include shot detection, face detection/tracking using S3FD, active speaker detection, SyncNet-based audio-visual synchronization checking, lip extraction using Dlib landmarks (mouth landmarks 48-67, 96×96 crops), and automatic transcription using a strong Vietnamese Wav2Vec2 ASR model. The resulting ViCocktail dataset contains 269 hours of training data and 2 hours of test data from approximately 2475 speakers. Two encoder architectures are evaluated: Conformer CTC/Attention (initialized from Auto-AVSR English checkpoint) and AV-HuBERT CTC/Attention (initialized from MuAViC checkpoints). Both models use weighted CTC + cross-entropy loss with a 6-layer decoder. Visual-only and audio-only baselines are also trained using the same encoder architectures.

## Key Results
- AVSR models significantly outperform audio-only models in noisy conditions, with AV1 achieving 18.39% WER in -5 dB cocktail-party noise versus 70.01% for A1
- English pre-trained AV-HuBERT (AV1: 9.40% WER clean) outperforms multilingual pre-training (AV2: 14.28% WER) and from-scratch training (AV3: 18.60% WER)
- Visual-only model V1 maintains 41.34% WER across all noise conditions, demonstrating stable but limited performance without audio input
- The automated pipeline successfully generates high-quality training data, though intermediate quality metrics are not reported

## Why This Works (Mechanism)

### Mechanism 1: Audio-Visual Fusion Provides Complementary Noise Robustness
- Claim: Combining audio and visual modalities enables graceful degradation under extreme noise, whereas audio-only models catastrophically fail.
- Mechanism: Visual features extracted from lip movements are unaffected by acoustic noise. When fused with audio features via MLP, the model can rely more heavily on the visual pathway when audio SNR drops, preserving recognition accuracy.
- Core assumption: The fusion mechanism can dynamically weight modalities based on signal quality, though the paper uses simple concatenation + MLP rather than explicit attention-based weighting.
- Evidence anchors:
  - [abstract] "achieving competitive performance with robust ASR in clean conditions and significantly outperforming them in noisy environments like cocktail parties"
  - [results, Table 2] AV1 achieves 9.40% WER (clean) → 18.39% WER (-5 dB, 2 interferers), whereas audio-only A1 degrades from 7.53% → 70.01% (8.3× increase)
  - [results, Table 2] Visual-only V1 maintains 41.34% WER regardless of noise level

### Mechanism 2: Cross-Lingual Transfer via Language-Agnostic Visual Features
- Claim: AVSR models pre-trained on high-resource languages (English, multilingual) can be efficiently adapted to low-resource languages with limited target-language data.
- Mechanism: Visual speech features (lip shapes, movements) are largely language-agnostic—phoneme visemes transfer across languages. Pre-trained audio-visual encoders capture universal temporal synchronization patterns that transfer even when acoustic phonology differs.
- Core assumption: Visual features transfer more readily than audio-only features across languages; the paper hypothesizes but does not isolate this empirically.
- Evidence anchors:
  - [introduction] "We hypothesize that AVSR can similarly benefit from this approach, as visual features may be more language-agnostic and transferable across languages"
  - [results, Table 2] English pre-trained AV-HuBERT (AV1: 9.40% clean) outperforms multilingual pre-training (AV2: 14.28%) and from-scratch (AV3: 18.60%)
  - [corpus] MoHAVE (arXiv:2502.10447) and related work confirm cross-lingual AVSR transfer is an active research direction, but comparative isolation of visual vs. acoustic transfer remains limited

### Mechanism 3: Automated Pipeline Enables Scalable Dataset Creation via Cascaded Filtering
- Claim: A multi-stage automated pipeline can convert raw web video into clean AVSR training data without manual annotation.
- Mechanism: Each stage acts as a quality filter: (1) face detection + tracking ensures continuous speaker presence; (2) Active Speaker Detection removes non-speaking segments; (3) SyncNet removes audio-visual desynchronization; (4) high-quality ASR generates pseudo-labels. The cascade progressively eliminates failure modes.
- Core assumption: The ASR model used for labeling must be sufficiently accurate that label noise does not overwhelm learning signal.
- Evidence anchors:
  - [methodology, Figure 2] Pipeline stages: Shot detection → Face detection/tracking → Active speaker detection → AV sync → Lip extraction → ASR transcription
  - [methodology] "We experimented with various open-source and proprietary ASR systems... a variant of the Wav2Vec2 model... demonstrated the best performance"
  - [corpus] Auto-AVSR (referenced in paper [20]) demonstrates similar automated labeling; corpus neighbors show active research in scalable AVSR data but limited systematic pipeline ablations

## Foundational Learning

- **Concept: CTC/Attention Hybrid Decoding**
  - Why needed here: Both encoder architectures use CTC/Attention decoders. Understanding why CTC (conditional independence) and Attention (sequence modeling) are combined helps interpret training dynamics.
  - Quick check question: Why does CTC alone struggle with lipreading, and how does adding cross-entropy attention loss help?

- **Concept: Self-Supervised Audio-Visual Pre-training (AV-HuBERT)**
  - Why needed here: AV-HuBERT outperforms Conformer trained from scratch. Understanding masked cluster prediction explains why self-supervision helps cross-lingual transfer.
  - Quick check question: What does AV-HuBERT predict during pre-training, and why might this be language-agnostic?

- **Concept: Active Speaker Detection and Audio-Visual Synchronization**
  - Why needed here: The pipeline's data quality hinges on these components. Without understanding ASD and SyncNet, you cannot debug pipeline failures.
  - Quick check question: If SyncNet detects a 200ms audio delay, what happens to the extracted training sample?

## Architecture Onboarding

- **Component map:**
```
Raw Video → Face Detection (S3FD) → Face Tracking (IOU)
         → Active Speaker Detection → SyncNet (AV sync check)
         → Lip Extraction (Dlib landmarks 48-67, 96×96 crop)
         → Audio (16kHz) → ASR (Wav2Vec2) → Pseudo-labels

Model Architecture:
  Visual: 3D-ResNet-18 (5×7×7 kernel) → Conformer/Transformer encoder
  Audio: 1D-ResNet-18 (Conformer) OR FFN projection (AV-HuBERT)
  Fusion: Concatenation + MLP
  Decoder: CTC head (linear) || Attention head (6-layer Transformer)
  Loss: weighted sum of CTC + Cross-Entropy
```

- **Critical path:**
  1. Data pipeline quality determines model ceiling—errors in ASD/SyncNet propagate directly.
  2. Encoder pre-training source (English vs. multilingual vs. scratch) has larger impact than architecture choice.
  3. Audio augmentation (interferers at -5 to 10 dB SNR) during training is essential for noise robustness.

- **Design tradeoffs:**
  - **Conformer vs. AV-HuBERT encoder:** Conformer is simpler, trained end-to-end. AV-HuBERT requires self-supervised pre-training but transfers better.
  - **English vs. multilingual pre-training:** Surprisingly, English-only outperforms multilingual for Vietnamese. Hypothesis: multilingual checkpoint may have learned interference patterns from 8 languages that don't align with Vietnamese phonology.
  - **Automated vs. manual labels:** Training uses ASR labels (noisy but scalable); test uses manual labels (clean but expensive).

- **Failure signatures:**
  - Audio-only model WER spikes >50% at low SNR → visual pathway not contributing (check lip extraction quality)
  - Visual-only model WER >60% → visual front-end failing (check face detection, landmark prediction)
  - From-scratch model doesn't converge → insufficient data (269h) for architecture capacity; must use pre-training
  - Sync confidence low across dataset → video source has poor A/V alignment or dubbing

- **First 3 experiments:**
  1. **Reproduce the pipeline on a small video subset (10 videos):** Verify face tracking continuity, ASD accuracy, and SyncNet confidence scores before scaling. Manually inspect 20 random extracted lip crops for alignment.
  2. **Ablate pre-training source:** Train AV-HuBERT with (a) English checkpoint, (b) multilingual checkpoint, (c) random initialization on a held-out validation split. Confirm English pre-training advantage reproduces.
  3. **Noise robustness stress test:** Evaluate AV1 and A1 on the full test set with all 9 (SNR, interferer) conditions. Plot WER curves to verify the 2× degradation (AV) vs. 8.3× degradation (audio-only) pattern at -5 dB SNR.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can dedicated self-supervised pre-training on a scaled-up Vietnamese AVSR dataset outperform the current adaptation of English pre-trained models?
- Basis in paper: [explicit] The authors state in the conclusion: "In the future, we plan to scale up data collection and explore self-supervised pre-training to develop more efficient models."
- Why unresolved: The current study relies on adapting off-the-shelf models (AV-HuBERT, Conformer) pre-trained on English or multilingual data, rather than training a dedicated self-supervised model on Vietnamese data.
- What evidence would resolve it: A comparison of WER between the current best adapted model and a model trained via self-supervision on an expanded ViCocktail dataset.

### Open Question 2
- Question: Why does adaptation from a monolingual English checkpoint yield better performance for Vietnamese than a multilingual checkpoint?
- Basis in paper: [inferred] The results show the English AV-HuBERT checkpoint achieves a 9.40% WER (clean), outperforming the multilingual checkpoint (14.28% WER), contradicting the initial hypothesis that visual features are "language-agnostic and transferable."
- Why unresolved: The paper reports this counter-intuitive finding but does not provide an analysis of whether this is due to the specific languages in the multilingual set, data quality, or model architecture limitations.
- What evidence would resolve it: An ablation study analyzing feature similarity across languages or testing the multilingual model's performance when Vietnamese is explicitly included in the pre-training mix.

### Open Question 3
- Question: To what extent is the automated pipeline dependent on the availability of high-quality ASR systems for the target language?
- Basis in paper: [inferred] The methodology relies on a strong Wav2Vec2 model (pre-trained on 13k hours) to generate transcripts. The authors claim the pipeline helps "under-resourced" languages, but the dependency on a massive ASR for labeling may contradict this utility for truly low-resource languages.
- Why unresolved: The paper does not analyze how noise in the ASR labeling (pseudo-labeling) affects the final AVSR model performance compared to human-annotated data.
- What evidence would resolve it: An experiment training the AVSR model using only the raw ASR labels versus manually corrected labels to quantify the performance gap caused by automated transcription errors.

## Limitations

- The automated pipeline's intermediate quality metrics are not reported, making it difficult to assess the true quality of the 269h training set
- The visual-only model achieves 41.34% WER even in clean conditions, suggesting that lipreading alone is far from sufficient
- The paper does not analyze how noise in the ASR labeling (pseudo-labeling) affects the final AVSR model performance compared to human-annotated data
- The pre-training experiments show English outperforms multilingual, but this is a single data point without ablation on transfer mechanisms

## Confidence

- **High confidence**: The experimental results showing AVSR outperforming audio-only models in noisy conditions are well-supported by the 9-condition test set evaluation.
- **Medium confidence**: The automated pipeline's effectiveness is demonstrated through the final dataset quality, but intermediate stage quality metrics are missing.
- **Medium confidence**: The cross-lingual transfer advantage of English pre-training is shown but not thoroughly explained or isolated from other factors.

## Next Checks

1. **Pipeline Quality Audit**: Run the full pipeline on a held-out validation subset and report intermediate metrics: ASD precision/recall, SyncNet confidence scores, and ASR word error rate on the generated labels.

2. **Transfer Mechanism Isolation**: Train visual-only and audio-only models with frozen encoders from English, multilingual, and scratch checkpoints to isolate which modality benefits most from pre-training.

3. **Noise Robustness Characterization**: Plot WER degradation curves for AV1, A1, and V1 across all 9 SNR/interferer conditions to verify the claimed 2× vs. 8.3× degradation factors and identify the exact SNR threshold where AVSR provides the most benefit.