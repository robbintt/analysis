---
ver: rpa2
title: 'Catalog-Native LLM: Speaking Item-ID Dialect with Less Entanglement for Recommendation'
arxiv_id: '2510.05125'
source_url: https://arxiv.org/abs/2510.05125
tags:
- recommendation
- item
- language
- text
- zhang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of integrating collaborative
  filtering with large language models for recommendation, where naive ID-text mixing
  often causes knowledge interference. The proposed Item-ID + Oral-language Mixture-of-Experts
  Language Model (IDIOMoE) treats item interactions as a distinct "dialect" by splitting
  each FFN into separate text and item experts, with static token-type routing to
  avoid interference.
---

# Catalog-Native LLM: Speaking Item-ID Dialect with Less Entanglement for Recommendation

## Quick Facts
- arXiv ID: 2510.05125
- Source URL: https://arxiv.org/abs/2510.05125
- Authors: Reza Shirkavand; Xiaokai Wei; Chen Wang; Zheng Hui; Heng Huang; Michelle Gong
- Reference count: 40
- One-line primary result: IDIOMoE achieves +27.1% NDCG@10, +16.6% HR@10, +31.2% MRR on an industrial dataset by using MoE to separate item and text processing

## Executive Summary
This paper introduces IDIOMoE, a Mixture-of-Experts architecture for LLM-based recommendation that addresses knowledge interference between item IDs and natural language by treating item interactions as a distinct "dialect." The model splits each feed-forward network into separate text and item experts with static token-type routing, preserving the base model's language understanding while improving recommendation performance. Extensive experiments on public and proprietary datasets demonstrate consistent improvements over both classic and LLM-based baselines.

## Method Summary
The paper addresses the challenge of integrating collaborative filtering with large language models for recommendation, where naive ID-text mixing often causes knowledge interference. The proposed Item-ID + Oral-language Mixture-of-Experts Language Model (IDIOMoE) treats item interactions as a distinct "dialect" by splitting each FFN into separate text and item experts, with static token-type routing to avoid interference. Experiments show IDIOMoE consistently outperforms both classic and LLM-based baselines across public and proprietary datasets—e.g., +27.1% NDCG@10, +16.6% HR@10, +31.2% MRR on an industrial dataset—while preserving the base model's language understanding. Ablations confirm that the gains come from expert specialization, not added parameters, and key-value memory analysis reveals cleaner item-text separation and higher category purity in the MoE model.

## Key Results
- +27.1% NDCG@10 improvement on industrial dataset
- +16.6% HR@10 improvement on industrial dataset
- +31.2% MRR improvement on industrial dataset
- Consistent outperformance over both classic and LLM-based baselines

## Why This Works (Mechanism)
The MoE architecture separates item and text processing through specialized experts, preventing knowledge interference that occurs when item IDs are naively mixed with natural language. Static token-type routing ensures each token type (item vs. text) is processed by its corresponding expert, maintaining the base model's language understanding while improving collaborative filtering performance.

## Foundational Learning
- **Knowledge Interference**: When item IDs are mixed with natural language, the model's representations become entangled, reducing performance in both domains. Understanding this helps identify why naive ID-text mixing fails.
- **Static Token-Type Routing**: Routing tokens based on their type (item vs. text) ensures appropriate expert specialization. Quick check: verify routing accuracy by inspecting expert activations per token type.
- **Feed-Forward Network Splitting**: Dividing FFN layers into separate text and item experts allows specialized processing. Quick check: compare parameter efficiency between shared vs. split FFN approaches.

## Architecture Onboarding

**Component Map**: User query -> Text experts + Item experts (MoE) -> Recommendation output

**Critical Path**: User query → Static token-type routing → Text/Item expert processing → Joint recommendation generation

**Design Tradeoffs**: Static routing provides stability but may limit adaptability to dynamic contexts; parameter efficiency is maintained through expert specialization rather than increased parameters.

**Failure Signatures**: Performance degradation when token-type routing is confused, or when expert specialization becomes too narrow, leading to loss of cross-domain understanding.

**3 First Experiments**:
1. Compare NDCG@10 between IDIOMoE and baseline models on public datasets
2. Analyze key-value memory separation between item and text experts
3. Conduct ablation study removing MoE components to measure performance impact

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation relies on offline metrics without user studies to confirm perceived quality improvements
- Static token-type routing may limit adaptability to dynamic user-item contexts
- Computational overhead of routing decisions and expert specialization not detailed
- Proprietary dataset results cannot be independently verified

## Confidence

| Claim | Confidence |
|-------|------------|
| Core technical contribution and public dataset results | High |
| Proprietary dataset results | Medium |
| Generalization across domains | Medium |

## Next Checks
1. Conduct online A/B testing to validate that offline metric improvements translate to user engagement gains
2. Measure and report inference latency and computational overhead introduced by the MoE routing mechanism
3. Test the model's performance on additional domains (e.g., music, news) to assess cross-domain robustness