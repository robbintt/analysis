---
ver: rpa2
title: 'What Patients Really Ask: Exploring the Effect of False Assumptions in Patient
  Information Seeking'
arxiv_id: '2601.15674'
source_url: https://arxiv.org/abs/2601.15674
tags:
- questions
- question
- incorrect
- type
- what
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Patients increasingly use LLMs to seek health information, but\
  \ benchmarks often focus on exam-style questions rather than real-world queries.\
  \ This work introduces a dataset of patient-generated medical questions collected\
  \ via Google\u2019s People Also Ask feature for 200 top U.S."
---

# What Patients Really Ask: Exploring the Effect of False Assumptions in Patient Information Seeking

## Quick Facts
- arXiv ID: 2601.15674
- Source URL: https://arxiv.org/abs/2601.15674
- Authors: Raymond Xiong; Furong Jia; Lionel Wong; Monica Agrawal
- Reference count: 22
- Patients increasingly use LLMs for health information, but benchmarks focus on exam-style questions rather than real-world queries

## Executive Summary
Patients frequently use LLMs to seek health information, but most benchmark datasets contain well-formed questions that don't reflect real-world medical information seeking. This work introduces a dataset of 4,012 patient-generated medical questions collected from Google's People Also Ask feature for 200 top U.S. medications, revealing that 23.6% contain incorrect assumptions or dangerous intent. While top models like GPT-5 achieve 91% accuracy at detecting these issues, most other LLMs fall below 50%, often providing non-committal or misleading answers. The analysis shows corrupted questions propagate through query histories, with prior incorrectness significantly increasing the likelihood of subsequent errors (p < 0.001).

## Method Summary
The authors collected medical questions using Google's People Also Ask feature for 200 top U.S. medications, performing randomized depth-first traversal to gather 4,012 unique questions. They classified questions using GPT-5 as an LLM judge into three categories: Type A (incorrect assumptions, 16.1%), Type B (dangerous intent, 7.5%), and Type C (benign, 76.4%). A high-confidence corrupted subset of 536 questions was identified by intersecting GPT-5 and GPT-5-mini labels. Ten LLMs generated responses without system prompts, which were then evaluated by GPT-5 judge for correctness in challenging/correction the false premises. Propagation effects were analyzed using logistic regression and two-sample Z-tests.

## Key Results
- 23.6% of real-world patient medical questions contain incorrect assumptions or dangerous intent
- Most LLMs achieve below 50% accuracy at detecting these issues, with only top models like GPT-5 reaching 91%
- Prior incorrect questions significantly increase the likelihood of subsequent incorrect questions in search trajectories (p < 0.001)
- Medical fine-tuned models (Meditron, OpenBioLLM, MedAlpaca) perform no better than general-purpose models at presupposition detection

## Why This Works (Mechanism)

### Mechanism 1: Query History Contamination Propagates Incorrectness
- Claim: Prior incorrect questions causally increase the probability that subsequent questions in a search trajectory will also contain false assumptions or dangerous intent.
- Mechanism: Users clicking through PAA questions encounter misleading premises, which reshape follow-up queries; the system then surfaces related questions that inherit and amplify the original incorrectness, creating a self-reinforcing error cascade.
- Core assumption: The PAA traversal reflects genuine user information-seeking behavior and the sequential dependencies approximate real-world query evolution.
- Evidence anchors:
  - [abstract]: "Analysis shows corrupted questions propagate through query histories, with prior incorrectness significantly increasing the likelihood of subsequent errors (p < 0.001)."
  - [section] Results §3.2: Logistic regression shows significant positive correlation between proportion of incorrect history and likelihood of incorrect current question (OR 95% CI = [1.68, 3.16], FDR-adjusted p = 3.413×10⁻⁷).
  - [corpus] MedRedFlag (arXiv:2601.09853) corroborates that real-world health questions embed false premises requiring redirection, but does not quantify propagation—this paper's propagation analysis appears novel.
- Break condition: If PAA questions reflect aggregated search behavior rather than individual user trajectories, the causal chain from prior to subsequent incorrectness may be weaker for single-user sessions.

### Mechanism 2: Benchmark-Reality Gap in LLM Presupposition Detection
- Claim: LLMs that perform well on medical exam-style benchmarks systematically fail to identify embedded false assumptions in naturalistic patient questions.
- Mechanism: Exam questions are designed to be unambiguous with single correct answers; real patient queries contain underspecified, ambiguous, or pragmatically flawed presuppositions that require recognizing *that* a question is ill-posed before answering *what* it asks.
- Core assumption: The gap reflects a genuine capability limitation rather than prompt engineering or evaluation methodology.
- Evidence anchors:
  - [abstract]: "LLMs struggle to detect these errors: top models like GPT-5 achieve 91% accuracy, but most others fall below 50%, often providing non-committal or misleading answers."
  - [section] Table 2: Open-source models (Meditron 70B, OpenBioLLM 70B, MedAlpaca 7B) achieve 16–51% accuracy on corrupted questions despite strong benchmark performance elsewhere.
  - [corpus] Cancer-Myth (arXiv:2504.11373) and Identifying and Answering Questions with False Assumptions (arXiv:2508.15139) similarly find LLMs struggle with false presuppositions, supporting generalizability.
- Break condition: If explicit prompting (as in Sieker et al., 2025, cited in the paper) substantially improves detection, the limitation may be prompt-sensitivity rather than fundamental capability.

### Mechanism 3: Medical-Specific Fine-Tuning Does Not Transfer to Presupposition Detection
- Claim: Domain-specific medical LLMs (Meditron, OpenBioLLM, MedAlpaca) do not outperform general-purpose models at identifying incorrect assumptions, suggesting this capability is not captured by medical knowledge fine-tuning.
- Mechanism: Medical fine-tuning optimizes for factual knowledge and clinical reasoning; detecting pragmatically ill-formed questions requires meta-linguistic reasoning about the question's premises, which is not explicitly trained.
- Core assumption: The evaluated medical LLMs are representative of domain-adapted training approaches.
- Evidence anchors:
  - [section] Table 2: Meditron 70B achieves 28% (Type A) and 18% (Type B); MedAlpaca 7B achieves 27% and 18%—both below LLaMA3 Instruct 8B (47%, 32%).
  - [corpus] Do Clinical Question Answering Systems Really Need Specialised Medical Fine Tuning? (arXiv:2601.12812) questions whether domain fine-tuning is necessary for clinical QA, but does not specifically address presupposition detection.
- Break condition: If medical LLMs were primarily fine-tuned on well-formed clinical text (notes, guidelines), they may have been explicitly shielded from malformed queries during training.

## Foundational Learning

- **Concept: Presupposition and pragmatic ill-formedness**
  - Why needed here: Understanding that a question can be grammatically valid but pragmatically unanswerable (e.g., "Why is metformin banned?" assumes a false premise) is essential to grasping why LLMs fail on this task.
  - Quick check question: If a user asks "What is the permanent cure for migraine?", what assumption is embedded, and why would answering directly be misleading?

- **Concept: Propagation in sequential search behavior**
  - Why needed here: The paper's core finding on error propagation relies on understanding how prior context shapes subsequent queries in a Markov-like dependency.
  - Quick check question: In the PAA traversal model, if question Q₃ follows Q₂ which follows Q₁, and Q₁ is incorrect, what statistical relationship does the paper test for?

- **Concept: LLM-as-a-judge evaluation**
  - Why needed here: The classification and evaluation pipeline uses GPT-5 as an automated judge; understanding the limitations of this approach (75% agreement for labeling, 85% for evaluation) is critical for interpreting results.
  - Quick check question: What validation step did the authors use to ensure the LLM judge aligned with human judgment?

## Architecture Onboarding

- **Component map:** Data collection (Google PAA scraper) -> randomized DFS traversal (max depth 10) -> 4,012 unique questions across 202 medication queries -> GPT-5 judge classification (Type A/B/C) -> high-confidence filtering (536 corrupted questions) -> 10 LLM response generation -> GPT-5 judge evaluation -> propagation analysis (logistic regression, Z-tests)

- **Critical path:**
  1. PAA question collection and trajectory logging
  2. Question classification with LLM judge + human validation subset
  3. Response generation from target LLMs (no system prompts)
  4. Response evaluation for premise correction
  5. Statistical analysis of propagation effects

- **Design tradeoffs:**
  - LLM-as-judge provides scalability but introduces annotation noise (75–85% human agreement); manual annotation would be more accurate but infeasible at 4,012 questions
  - PAA data reflects aggregated search behavior, not individual user sessions; this increases scale but may dilute causal claims about single-user propagation
  - No system prompts during testing simulates naive users but may understate model capabilities with careful prompting

- **Failure signatures:**
  - Non-committal responses: Model provides accurate but vague information without directly challenging the false premise
  - Premise acceptance: Model answers the question as posed (e.g., "The most addictive painkiller is oxycodone" without rejecting the superlative assumption)
  - Safety disclaimers as fig leaves: Model includes "consult a doctor" but does not correct the underlying misconception

- **First 3 experiments:**
  1. **Reproduce classification pipeline:** Run the provided GPT-5 classification prompt on a sample of questions, validate against human labels, confirm ~75% agreement
  2. **Test a new LLM:** Add a new model (e.g., a recent open-source release) to the evaluation pipeline, generate responses to the 536 high-confidence corrupted questions, and measure accuracy
  3. **Intervention experiment:** Test whether a simple system prompt ("Identify and correct any false assumptions before answering") improves premise detection, comparing against the no-prompt baseline to quantify prompt sensitivity

## Open Questions the Paper Calls Out

- **Open Question 1:** Do the propagation patterns of false assumptions observed in aggregated Google PAA data hold for individual patient search histories?
  - Basis in paper: [explicit] The Limitations section states the dataset reflects "aggregated search behavior instead of direct patient-authored narratives" and suggests future work investigate individual behavior.
  - Why unresolved: The PAA algorithm aggregates user paths, but it remains unclear if a single user's reasoning deteriorates in the same "branching" manner identified in the aggregated data.
  - What evidence would resolve it: A longitudinal study analyzing the query logs of individual users to track how misconceptions evolve during a single search session.

- **Open Question 2:** Can fine-tuning on corrupted question datasets enable LLMs to inherently detect false presuppositions without relying on prompt engineering?
  - Basis in paper: [inferred] The conclusion notes that relying on controlled prompts incurs costs and varies reliability, motivating research to "develop models that address these issues inherently."
  - Why unresolved: The paper establishes that vanilla models fail and prompted models succeed, but does not test if model weights can be adjusted to solve the problem without external prompting.
  - What evidence would resolve it: Benchmarking LLMs fine-tuned on the released dataset against baseline models using zero-shot, unengineered prompts on the corrupted questions.

- **Open Question 3:** Do the rates of incorrect assumptions and dangerous intent in medical queries generalize across non-English languages and non-US regions?
  - Basis in paper: [explicit] The Limitations section notes the analysis focused "primarily on English-language questions and on medications commonly prescribed in the United States."
  - Why unresolved: Medical misinformation and linguistic presupposition structures (e.g., "Why is X...") may manifest differently across languages and cultural healthcare contexts.
  - What evidence would resolve it: Replicating the PAA data collection and classification methodology using non-US search engines (e.g., Baidu, Yandex) and non-English queries.

## Limitations

- The LLM-as-a-judge evaluation methodology introduces annotation noise (75-85% human agreement) that could affect dataset construction and final performance metrics
- The causal claims about query history propagation are limited by the observational nature of PAA data, which reflects aggregated rather than individual user behavior
- The absence of system prompts during LLM evaluation may underestimate model capabilities, as careful prompting could substantially improve presupposition detection

## Confidence

- **High confidence**: Most LLMs struggle with presupposition detection (below 50% accuracy), well-supported by evaluation results and corroborated by related work
- **Medium confidence**: Propagation analysis showing prior incorrectness increases subsequent incorrectness probability (p < 0.001), statistically significant but limited by observational data structure
- **Medium confidence**: Medical fine-tuning does not improve presupposition detection, supported by comparison results though limited by small sample of medical models

## Next Checks

1. **Reproduce classification pipeline**: Run the provided GPT-5 classification prompt on a sample of questions and validate against human labels to confirm the reported ~75% agreement rate.

2. **Test prompt sensitivity**: Evaluate whether a simple system prompt ("Identify and correct any false assumptions before answering") improves LLM performance on corrupted questions compared to the no-prompt baseline.

3. **Validate propagation claims**: Replicate the statistical analysis using different time windows or alternative measures of query similarity to confirm the robustness of the observed propagation effects.