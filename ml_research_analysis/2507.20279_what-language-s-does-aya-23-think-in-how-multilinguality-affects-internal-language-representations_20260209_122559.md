---
ver: rpa2
title: What Language(s) Does Aya-23 Think In? How Multilinguality Affects Internal
  Language Representations
arxiv_id: '2507.20279'
source_url: https://arxiv.org/abs/2507.20279
tags:
- language
- layers
- multilingual
- code-mixed
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how multilingual language models internally
  process different languages, particularly focusing on balanced multilingual models
  like Aya-23 compared to predominantly English-trained models. The researchers use
  logit lens analysis and neuron specialization techniques to examine language-specific
  processing patterns across translation tasks, code-mixed inputs, and various language
  pairs.
---

# What Language(s) Does Aya-23 Think In? How Multilinguality Affects Internal Language Representations

## Quick Facts
- arXiv ID: 2507.20279
- Source URL: https://arxiv.org/abs/2507.20279
- Reference count: 4
- Primary result: Balanced multilingual training enables Aya-23 to activate typologically related languages during translation rather than relying on English pivoting

## Executive Summary
This paper investigates how multilingual language models internally process different languages by examining Aya-23-8B compared to predominantly English-trained models. Using logit lens analysis and neuron specialization techniques, the researchers analyze language-specific processing patterns across translation tasks, code-mixed inputs, and various language pairs. The study reveals that Aya-23 employs multilingual processing strategies, activating related languages during translation, and concentrates language-specific neurons in final layers for code-mixed inputs. Statistical analysis shows output languages influence internal representations more strongly than input languages across all models tested.

## Method Summary
The researchers analyzed three models: Aya-23-8B (balanced 23-language instruction-tuned), Llama 3.1-8B (~8% multilingual tokens), and Chinese-LLaMA-2-7B (LoRA-adapted Chinese specialist). They used logit lens analysis to trace language activation across transformer layers by projecting hidden states to vocabulary space, and identified neuron specialization via activation frequency thresholds. The study created controlled code-mixed datasets using WMT24++ parallel corpus with mixing ratios of 25%, 50%, and 75% across 10 language pairs. They computed IoU overlap matrices for neuron sharing and used Average Precision classification to identify code-mixing-specific neurons.

## Key Results
- Aya-23 activates typologically related languages during translation rather than using English as a pivot, showing intermediate-to-late layer activation of languages sharing features with the target
- Output languages exert stronger influence on internal representations than input languages across all models, with statistical analysis showing 12/13 languages showing significant output influence versus 7/13 for input influence
- Language-specific neurons for code-mixed inputs concentrate predominantly in final layers (27-31) in Aya-23, diverging from early-and-late layer distributions found in decoder-only models
- Base language characteristics drive neuron sharing more strongly than mixed-in languages, with French-based code-mixed inputs maintaining consistent neuron overlap regardless of mixing rate

## Why This Works (Mechanism)

### Mechanism 1: Distributed Multilingual Activation vs. Pivot-Language Processing
Balanced multilingual training enables models to activate typologically related languages during translation rather than defaulting to a single pivot language. During translation tasks, Aya-23-8B activates intermediate representations of languages sharing typological features with the target (e.g., Japanese tokens during English-to-Chinese translation), suggesting cross-linguistic feature sharing rather than sequential source→pivot→target processing.

### Mechanism 2: Output-Language Dominance in Internal Representations
The output/destination language exerts stronger influence on internal representations than the input/source language across translation tasks. Statistical analysis of AUC values from logit lens experiments shows output language presence produces significant changes in 12/13 languages vs. only 7/13 for input languages, suggesting generation constraints propagate backward through layers more strongly than input features propagate forward.

### Mechanism 3: Code-Mixing Neuron Concentration in Final Layers
Language-specific neurons for code-mixed inputs concentrate in final layers (27-31) in balanced multilingual models, diverging from early-and-late distributions found in predominantly monolingual decoder-only models. Using Average Precision classification on neuron activations, code-mixing-specific neurons show pronounced spikes in layer 31 across all language pairs, suggesting language selection occurs at generation time rather than throughout the network.

## Foundational Learning

- **Logit Lens Analysis**: Primary method for tracing language activation across transformer layers by projecting hidden states to vocabulary space. Why needed here: Enables layer-wise analysis of language preferences. Quick check: Can you explain how projecting intermediate hidden states through the unembedding matrix reveals layer-wise language preferences?

- **Neuron Specialization via Activation Frequency**: Identifies language-specific neurons by ranking activation contributions and computing IoU overlap between language pairs. Why needed here: Quantifies language-specific processing patterns. Quick check: How would you interpret high IoU overlap between French and Italian code-mixed neurons but low overlap between French and Chinese?

- **Code-Mixing Taxonomy and Script Effects**: Base language and script similarity systematically affect neuron sharing and translation quality. Why needed here: Explains why some language pairs show stronger neuron sharing than others. Quick check: Why would zh-ja pairs show moderate neuron overlap despite different scripts, while fr-en shows lower overlap despite shared script?

## Architecture Onboarding

- **Component map**: Aya-23-8B, Llama 3.1-8B, Chinese-LLaMA-2-7B -> Transformer MLP layers (0-31 for 8B models) -> FFN activations, residual stream pre-layerNorm, unembedding matrix for logit projection

- **Critical path**: 
  1. Prepare code-mixed dataset (WMT24++ + bilingual dictionaries at 25%/50%/75% mixing rates)
  2. Run inference capturing per-layer hidden states and neuron activations
  3. Apply logit lens (project hℓ through unembedding) and compute language probabilities per layer
  4. Identify specialized neurons via activation frequency threshold (k=90%) and compute IoU matrices
  5. Classify code-mixing neurons via Average Precision ranking across all language pairs

- **Design tradeoffs**: Rule-based code-mixing ensures controlled ratios but produces ungrammatical outputs; binary ReLU thresholding for neuron specialization loses activation magnitude information; excluding neurons shared by all languages isolates language-specific patterns but may miss universal multilingual features

- **Failure signatures**: 
  - If logit lens shows no language differentiation until final layer → model may lack multilingual capability
  - If neuron overlap is uniform across all pairs → threshold k may be too permissive
  - If French advantage disappears in Chinese-LLaMA → check tokenizer compatibility

- **First 3 experiments**:
  1. Replicate logit lens on single translation pair (en→zh) across all three models to verify English-pivot vs. multilingual activation patterns
  2. Run neuron specialization on fr-en code-mixed data at 50% mixing rate; compute layer-wise top-k neuron counts and verify concentration in layers 27-31 for Aya-23
  3. Compute IoU overlap matrix for 10 code-mixed pairs; verify French-based pairs show consistent overlap while Chinese-based pairs show mixing-rate-dependent degradation

## Open Questions the Paper Calls Out

### Open Question 1
Do internal multilingual processing patterns (specifically the lack of English pivoting) generalize to low-resource languages? The authors note their analysis focuses "primarily on high-resource languages, with limited investigation into how low-resource languages are processed internally."

### Open Question 2
How do code-mixing neurons differ functionally from general language-specific neurons in early layers? The limitations section states the findings likely characterize "code-mixing neurons" rather than pure "language neurons," as the classification task distinguished between code-mixed and non-code-mixed inputs.

### Open Question 3
To what extent does the grammaticality of code-mixed inputs influence neuron concentration in final layers? The authors acknowledge their code-mixed dataset uses a "rule-based word-to-word translation" which "fails to account for grammatical structure."

## Limitations

- Dataset composition relies on controlled code-mixed data that may not reflect natural code-switching patterns found in real-world multilingual communication
- Model scope limited to three models with unclear pretraining corpus composition, making it difficult to attribute patterns to architectural choices versus training data distribution
- Statistical significance analysis may be overly conservative with Bonferroni correction potentially masking genuine effects

## Confidence

**High Confidence**: Aya-23 employs multilingual processing strategies rather than English-pivot processing, supported by logit lens results showing activation of typologically related languages during translation.

**Medium Confidence**: Neuron concentration in final layers for code-mixed inputs is supported but requires careful interpretation due to differences in model architectures and training objectives.

**Low Confidence**: The specific mechanism linking code-mixing neuron concentration to generation-time language selection remains speculative, with the causal relationship not definitively established.

## Next Checks

**Check 1: Natural Code-Mixing Validation**
Re-run the neuron specialization analysis using naturally occurring code-mixed data from social media or conversational corpora rather than rule-based synthetic mixing. Compare neuron activation patterns and sharing metrics between synthetic and natural code-mixed inputs.

**Check 2: Cross-Architectural Generalization**
Test the logit lens and neuron specialization methodologies on additional multilingual models with varying architectural designs and training data compositions. Focus on models trained with different mixing strategies to determine whether observed patterns are model-specific or represent general principles.

**Check 3: Ablation on Output-Language Conditioning**
Conduct controlled experiments removing explicit output-language conditioning from the instruction tuning process while keeping other factors constant. Compare the strength of output-language dominance in internal representations between conditioned and unconditioned models.