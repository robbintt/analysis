---
ver: rpa2
title: 'MOMA-AC: A preference-driven actor-critic framework for continuous multi-objective
  multi-agent reinforcement learning'
arxiv_id: '2511.18181'
source_url: https://arxiv.org/abs/2511.18181
tags:
- uni00000013
- learning
- multi-agent
- agents
- multi-objective
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MOMA-AC, the first inner-loop actor-critic
  framework for continuous multi-objective multi-agent reinforcement learning (MOMARL).
  The framework uses preference-conditioned policies and centralised critics to enable
  scalable learning of diverse, Pareto-efficient solutions across agents.
---

# MOMA-AC: A preference-driven actor-critic framework for continuous multi-objective multi-agent reinforcement learning

## Quick Facts
- arXiv ID: 2511.18181
- Source URL: https://arxiv.org/abs/2511.18181
- Reference count: 40
- Introduces first inner-loop actor-critic framework for continuous MOMARL with preference-conditioned policies

## Executive Summary
MOMA-AC presents a novel inner-loop actor-critic framework for continuous multi-objective multi-agent reinforcement learning (MOMARL). The approach uses preference-conditioned policies and centralized critics to learn diverse, Pareto-efficient solutions across agents. By combining multi-headed actors with vector-valued critics, the framework addresses key coordination and utility estimation challenges in MOMARL. Experimental results on cooperative locomotion tasks demonstrate statistically significant improvements in expected utility and hypervolume compared to baseline methods.

## Method Summary
MOMA-AC introduces a preference-driven actor-critic framework for continuous MOMARL that learns policies conditioned on preference vectors. The method employs multi-headed actors where each head corresponds to a specific preference vector, enabling the agent to generate diverse policies. Centralized vector-valued critics estimate joint utilities across all objectives, while dual critics help mitigate overestimation bias. The framework operates in an inner-loop manner, allowing preference conditioning during training rather than post-hoc policy extraction. This design enables scalable learning of Pareto-efficient solutions while maintaining coordination across agents through shared utility estimation.

## Key Results
- Achieves statistically significant improvements in expected utility and hypervolume over baseline methods
- Demonstrates robust scalability with increasing agent count in cooperative locomotion tasks
- Outperforms independent and outer-loop training approaches in continuous MOMARL settings

## Why This Works (Mechanism)
The framework's effectiveness stems from its preference-conditioning mechanism that allows direct optimization toward specific utility regions during training. By using multi-headed actors, each specialized for different preference vectors, the agent can generate diverse policies that cover different regions of the Pareto front. The centralized vector-valued critics enable accurate estimation of joint utilities across all objectives and agents, facilitating better coordination. The dual critic architecture reduces overestimation bias that commonly affects multi-objective value estimation. The inner-loop training approach ensures that preference information is incorporated throughout learning rather than as a post-processing step, leading to more efficient convergence toward desired trade-offs.

## Foundational Learning
- **Multi-objective optimization**: Learning policies that balance competing objectives simultaneously; needed because real-world tasks often involve trade-offs between multiple goals
- **Actor-critic methods**: Policy-based approaches with value function critics; needed for stable learning in continuous action spaces
- **Preference conditioning**: Adapting policies based on user-specified preference vectors; needed to generate diverse policies for different utility regions
- **Centralized training with decentralized execution**: Joint utility estimation during training but individual policy execution; needed for coordination while maintaining scalability
- **Pareto efficiency**: Solutions where no objective can be improved without worsening another; needed as the optimality criterion for multi-objective problems

## Architecture Onboarding
**Component map**: State/Input -> Multi-headed Actor -> Action; State/Input -> Vector-valued Critics -> Joint Utility; Preference Vector -> Policy Conditioning
**Critical path**: Input state → Actor head selection (based on preference) → Action output; Input state → Joint critic → Utility estimation
**Design tradeoffs**: Multi-headed actors provide diversity but increase parameter count; centralized critics improve coordination but may limit scalability; inner-loop preference conditioning improves efficiency but requires preference specification
**Failure signatures**: Overestimation bias in critics leading to suboptimal policies; preference misalignment causing poor policy selection; coordination failure when agents have conflicting objectives
**3 first experiments**: 1) Single-agent, two-objective tasks to validate basic preference conditioning; 2) Two-agent cooperative tasks to test coordination; 3) Scalability test with increasing agent count

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions, though it acknowledges the need for further validation on competitive and mixed-motive settings, as well as exploration of adaptive preference learning mechanisms.

## Limitations
- Experimental validation limited to cooperative locomotion tasks, leaving unclear generalization to competitive or mixed-motive settings
- Assumes known and fixed preference vectors during training, which may not reflect real-world scenarios with evolving preferences
- Limited analysis of scalability with respect to state and action dimensionality
- Computational overhead of multi-headed actor architecture not thoroughly characterized

## Confidence
- Framework novelty and design: High
- Experimental results on tested tasks: Medium
- Scalability claims: Low
- Theoretical convergence guarantees: Medium

## Next Checks
1. Test MOMA-AC on competitive multi-agent environments (e.g., Predator-Prey or Soccer) to evaluate performance in non-cooperative settings
2. Implement adaptive preference learning where agent preferences evolve during training, measuring impact on convergence and policy quality
3. Conduct ablation studies varying the number of objectives (beyond the current 2-3) and state/action dimensions to characterize true scalability limits