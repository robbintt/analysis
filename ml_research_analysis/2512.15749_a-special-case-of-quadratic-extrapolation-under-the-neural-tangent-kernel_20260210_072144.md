---
ver: rpa2
title: A Special Case of Quadratic Extrapolation Under the Neural Tangent Kernel
arxiv_id: '2512.15749'
source_url: https://arxiv.org/abs/2512.15749
tags:
- derivative
- training
- origin
- arxiv
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies quadratic extrapolation behavior of neural networks
  under the neural tangent kernel (NTK) regime, specifically examining extrapolation
  at the origin. While previous work showed ReLU networks extrapolate linearly far
  from the origin, this paper identifies that at the origin, the network exhibits
  quadratic extrapolation.
---

# A Special Case of Quadratic Extrapolation Under the Neural Tangent Kernel

## Quick Facts
- arXiv ID: 2512.15749
- Source URL: https://arxiv.org/abs/2512.15749
- Authors: Abiel Kim
- Reference count: 40
- Key outcome: NTK-induced ReLU networks exhibit quadratic extrapolation at origin when trained on data infinitely far from origin, contrasting with linear extrapolation behavior far from origin.

## Executive Summary
This paper identifies a special case of quadratic extrapolation behavior in neural networks under the NTK regime, specifically at the origin. While previous work showed ReLU networks extrapolate linearly far from the origin, this paper proves that at the origin, the network exhibits quadratic extrapolation when trained on data pushed infinitely far away. The key insight is that the NTK's non-translation invariant feature map makes the origin a geometrically distinct evaluation point, allowing higher-order terms to dominate. The main theoretical contribution is Theorem 1, which provides a rigorous proof of this quadratic behavior under specific asymptotic conditions.

## Method Summary
The paper analyzes a two-layer ReLU MLP trained under NTK regime (infinite width, infinitesimal learning rate) on data translated infinitely far from origin. The method constructs a training set where all inputs are pushed to infinity while keeping evaluation at the origin, creating a special geometric configuration. The analysis uses NTK kernel regression framework, computing the Gram matrix and its pseudo-inverse under Tikhonov regularization. The proof relies on showing that certain NTK representation coefficients become constant with respect to bias components, leading to vanishing higher-order derivatives and resulting in quadratic extrapolation behavior.

## Key Results
- NTK networks exhibit quadratic (not linear) extrapolation at the origin when trained on data infinitely far from origin
- The quadratic behavior depends on orientation between feature directions and evaluation direction
- Second derivative is zero when feature directions are orthogonal to evaluation direction
- Higher-order derivatives (order ≥ 3) vanish under these conditions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The non-translation invariance of the NTK feature map creates distinct extrapolation regimes at the origin versus infinity.
- **Mechanism:** Unlike translation-invariant kernels, the NTK's infinite-dimensional feature map changes geometry relative to the coordinate origin. By treating the origin not as a "far away" point but as a unique intersection of feature directions, the network's linear extrapolation behavior (valid at infinity) breaks down, potentially allowing higher-order (quadratic) terms to dominate.
- **Core assumption:** The analysis assumes the evaluation point is strictly near the origin ($x_0 \approx 0$) while training data is effectively at infinity ($t \to \infty$).
- **Evidence anchors:**
  - [abstract] "...infinite-dimensional feature map... is not translationally invariant... study of an out-of-distribution evaluation point very far from the origin is not equivalent to the evaluation of a point very near the origin."
  - [section 1] "...the origin of the RKHS must be a distinct special case..."
  - [corpus] Related work (Xu et al.) confirms linear behavior at infinity, supporting the contrast.
- **Break condition:** If the NTK feature map were translation invariant, the origin would hold no special geometric status, and linear extrapolation would likely persist everywhere.

### Mechanism 2
- **Claim:** Pushing training data to infinity forces the ReLU indicators to become "input agnostic," collapsing the NTK Gram matrix into a constant low-rank structure.
- **Mechanism:** When training inputs $x_i$ are translated by a large magnitude $-t v_\phi$, the ReLU indicator $I(w^\top \hat{x}_\infty \ge 0)$ ceases to depend on the specific input $x_i$. It depends only on the feature direction $w$ and the translation direction $v_\phi$. This forces the NTK Gram matrix to asymptotically approach a constant matrix $J$ (all ones), regularized by a small $\delta$.
- **Core assumption:** The translation magnitude $t$ is sufficiently large to dominate the original data distribution $\phi$.
- **Evidence anchors:**
  - [section A.1] "...indicators for any training input become input agnostic... strictly depends on a feature direction $w$ and training direction $v_\phi$."
  - [section 3, Remark 1] Derives the pseudo-inverse as $\frac{1}{\delta}I - \frac{t^2\kappa}{\delta(n\kappa t^2+\delta)}J$.
- **Break condition:** If training data remains bounded or close to the origin, the Gram matrix retains data-dependent structure, and this specific quadratic closed-form solution likely does not hold.

### Mechanism 3
- **Claim:** The specific closed-form of the NTK representation coefficients ($\beta_{NTK}$) truncates higher-order derivatives, capping the extrapolation order at quadratic.
- **Mechanism:** The paper proves that for this specific asymptotic setup, the partial derivatives of the $\beta$ components with respect to the bias/features vanish for orders $z \ge 1$ (Lemma 2). Combined with the distributional derivative of the indicator (Dirac delta), the Taylor expansion of the predictor $f_{NTK}$ effectively loses terms of order 3 and higher, leaving a quadratic function.
- **Core assumption:** The predictor acts as a min-norm interpolator in the NTK RKHS.
- **Evidence anchors:**
  - [section 3] "...components of the NTK representation coefficient... are constant with respect to the bias component..."
  - [section A.4] "...it is not difficult to see that the third and all higher order derivatives are automatically 0."
  - [corpus] Corpus evidence for this specific "derivative vanishing" mechanism is weak; it appears unique to this preprint's theoretical setup.

## Foundational Learning

- **Concept:** Neural Tangent Kernel (NTK) & "Lazy Training"
  - **Why needed here:** The entire theoretical result relies on the equivalence between training an infinite-width network and kernel regression using the NTK. Without this, the "Gram matrix" and "RKHS" arguments fail.
  - **Quick check question:** How does the NTK regime explain the difference between feature learning and kernel regression in wide networks?

- **Concept:** Reproducing Kernel Hilbert Space (RKHS) & Feature Maps
  - **Why needed here:** The paper analyzes extrapolation by inspecting the explicit feature map $\phi(x)$ (derivatives of the network w.r.t parameters) rather than just the kernel $k(x, y)$. Understanding $\beta_{NTK}$ as a coordinate vector in this space is essential.
  - **Quick check question:** What is the relationship between the kernel function $K(x, x')$ and the inner product of feature maps $\langle \phi(x), \phi(x') \rangle$?

- **Concept:** Tikhonov Regularization & Pseudo-Inverses
  - **Why needed here:** The asymptotic Gram matrix is singular (rank 1). The paper uses Tikhonov regularization ($\Gamma = \delta I$) to derive a pseudo-inverse, which is critical to defining the learned coefficients $\beta$.
  - **Quick check question:** Why is regularization necessary to invert a singular kernel matrix, and how does $\delta \to 0$ approximate the pseudo-inverse?

## Architecture Onboarding

- **Component map:** Data Generator -> Kernel Engine -> Coefficient Solver -> Evaluator
- **Critical path:** The derivation of the asymptotic pseudo-inverse (Remark 1) is the linchpin. If the algebra reducing the Gram matrix to $(\delta I + t^2 \dots J)^{-1}$ is incorrect, the resulting $\beta$ values are wrong, and the quadratic proof collapses.
- **Design tradeoffs:**
  - **Theoretical Rigor vs. Realism:** The proof requires $t \to \infty$ and width $\to \infty$. In finite practical settings, this "quadratic" behavior may appear as slight curvature noise rather than a clean mathematical law.
  - **Origin vs. Infinity:** This architecture explicitly optimizes/code-switches for behavior *at the origin*. Standard architectures usually optimize for generalization within the data support.
- **Failure signatures:**
  - **Linear Output:** If the evaluation point is moved away from the origin (but still OOD), the mechanism predicts a reversion to linear extrapolation (per Xu et al.).
  - **Random Output:** If the training data is not sufficiently distant ($t$ is small), the "input agnostic" indicator assumption fails, and the quadratic regularity breaks down.
- **First 3 experiments:**
  1. Verify Quadratic Curvature: Train a wide 2-layer ReLU MLP on synthetic data placed at increasing distance $t$ from origin. Evaluate near $x=0$. Plot $f(x)$ vs $x$. *Expectation:* As $t$ increases, the plot near 0 should look increasingly quadratic (or linear if orthogonality conditions are met).
  2. Orientation Sensitivity (Theorem 1 Check): Vary the evaluation direction $v_0$ and training shift direction $v_\phi$. Test cases where $\langle w, v_0 \rangle \approx 0$ (orthogonal). *Expectation:* Quadratic term should vanish, resulting in linear extrapolation despite the origin setup.
  3. Comparison with Far-Field: Compare the origin-evaluation result against a standard OOD evaluation point far from both origin and training data. *Expectation:* Far-field should be linear; origin-field should be quadratic.

## Open Questions the Paper Calls Out
None

## Limitations
- The theoretical analysis relies on asymptotic limits (t → ∞ and width → ∞) that are not practically achievable, making the quadratic behavior potentially appear as subtle curvature in finite-width networks.
- The proof assumes a very specific experimental setup (evaluation strictly at origin with training data pushed infinitely far) that may not generalize to more realistic out-of-distribution scenarios.
- The NTK regime represents only one extreme of neural network behavior, missing feature learning dynamics that could produce different extrapolation patterns.

## Confidence
- **High Confidence:** The mathematical derivation of Theorem 1 and its supporting lemmas is rigorous within the stated asymptotic regime.
- **Medium Confidence:** The claim that this represents a "special case" distinct from previous work on linear extrapolation is valid, but practical significance remains unclear without empirical validation.
- **Low Confidence:** The practical implications for real-world neural networks operating far from the asymptotic regime are speculative.

## Next Checks
1. **Finite t convergence study:** Empirically measure how the second derivative at the origin evolves as t increases from finite values, quantifying the rate of convergence to the asymptotic quadratic behavior.
2. **Width dependency analysis:** Test networks with varying widths to establish the minimum width required to observe the predicted quadratic behavior, bridging the gap between theory and practice.
3. **Generalization test:** Evaluate whether similar special cases exist at other geometrically distinct points (e.g., along axes or at other feature map singularities) beyond just the origin.