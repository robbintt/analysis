---
ver: rpa2
title: Select or Project? Evaluating Lower-dimensional Vectors for LLM Training Data
  Explanations
arxiv_id: '2601.16651'
source_url: https://arxiv.org/abs/2601.16651
tags:
- gradient
- components
- full
- subset
- component
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how to efficiently compute training data
  influence for large language models by reducing the dimensionality of model gradients.
  The authors propose a retrieval-based benchmark to evaluate whether selecting a
  small, architecturally-informed subset of model components or projecting the full
  gradient into a lower-dimensional space better captures training data influence.
---

# Select or Project? Evaluating Lower-dimensional Vectors for LLM Training Data Explanations

## Quick Facts
- arXiv ID: 2601.16651
- Source URL: https://arxiv.org/abs/2601.16651
- Reference count: 28
- Key outcome: Greedy component selection outperforms full gradients and random projection for LLM training data influence estimation

## Executive Summary
This paper addresses the computational challenge of computing training data influence for large language models by investigating whether selecting a small subset of model components or projecting gradients into lower dimensions better captures this influence. The authors propose a retrieval-based benchmark where the goal is to identify original training examples among paraphrased or model-generated candidates using gradient similarity. They find that a greedy component selection approach, which identifies the most informative model components for retrieval, consistently outperforms both random projection and the full gradient in terms of accuracy while being significantly more computationally efficient.

## Method Summary
The method computes per-sample gradients w.r.t. all 113 component tensors in a 1.2B parameter transformer model (embedding layer plus 16 layers × 7 trainable components per layer). Rather than materializing full gradients, the approach pre-computes component-wise dot products between all sample pairs. A greedy forward selection algorithm iteratively adds components that maximize retrieval accuracy, reconstructing cosine similarity from the stored dot products. The method is compared against random projection using TRAK's CudaProjector, which preserves pairwise distances but not task-specific signal. The entire pipeline is designed to be 75× faster than naive gradient computation while maintaining or improving retrieval accuracy.

## Key Results
- Greedy component selection achieves 0.36 retrieval accuracy in model-generated settings, nearly double the full gradient's 0.218
- MLP gate and up projections consistently outperform attention K/V projections across all settings
- A small parameter subset (5-10% of total) outperforms the full gradient, proving the full gradient dilutes discriminative signal with noise
- Early and late layers provide the most informative signals, with middle layers consistently weaker

## Why This Works (Mechanism)

### Mechanism 1
Semantically similar training samples produce gradients with higher cosine similarity than unrelated samples. Gradients indicate the parameter-space direction that reduces loss for a specific sample. Paraphrased versions of training data alter lexical form while preserving semantics, producing gradient vectors that remain geometrically close to the original because the underlying "learning signal" is functionally similar.

### Mechanism 2
Architectural components vary in their instance-specific information density; MLP gate/up projections encode more discriminative signal than attention K/V projections. Different components perform different functional roles. MLP layers are thought to store and process factual associations and instance-level features, while K/V projections in attention manage token-level context that may be more distributed or subject to cancellation effects in deeper layers.

### Mechanism 3
Greedy selection of a small component subset outperforms the full gradient because the full gradient dilutes discriminative signal with noise from less-informative components. The full gradient concatenates all component gradients, including those with weak or conflicting signals. By greedily selecting components that maximize retrieval accuracy, the algorithm identifies a subset whose combined dot products yield cleaner discrimination, as cosine similarity can be reconstructed from component-wise dot products via summation.

## Foundational Learning

- **Gradient-based influence estimation**: Why needed here - The entire method depends on understanding how loss gradients at training time relate to model behavior at inference, and why comparing gradients enables identifying influential training examples. Quick check: Explain why the gradient of sample A at the final model checkpoint can indicate whether sample B influenced the model's response to A.

- **Cosine similarity for high-dimensional vector comparison**: Why needed here - The retrieval task relies on cosine similarity to compare gradient vectors; understanding why cosine (not Euclidean) is used is critical for interpreting the mechanism. Quick check: Why does cosine similarity normalize away gradient magnitude, and why is this important when comparing samples with varying loss values?

- **Random projection (Johnson-Lindenstrauss lemma)**: Why needed here - The baseline "Project" method uses random projection to preserve pairwise distances in lower-dimensional space; understanding its theoretical guarantees helps explain why it preserves geometry but not task-specific signal. Quick check: What does the JL lemma guarantee about distance preservation, and why doesn't this guarantee optimal performance on a discriminative retrieval task?

## Architecture Onboarding

- **Component map**: 16 transformer layers × 7 trainable components per layer = 112 components + 1 embedding layer = 113 total. Per layer: Q, K, V, O attention projections (4.2M params each) + MLP Gate, Up, Down projections (16.8M params each). Embedding layer: 103M params.

- **Critical path**: 1) Compute gradients w.r.t. all 113 component tensors for each sample; 2) Pre-compute and store component-wise dot products for all sample pairs; 3) Run greedy forward selection iteratively adding components that maximize retrieval accuracy; 4) At inference, reconstruct cosine similarity for selected subset by summing pre-computed dot products.

- **Design tradeoffs**: Selection vs. Projection: Selection is 75× faster (4 hours vs. 300+ hours with caching) and more accurate for retrieval, but may not preserve global gradient geometry required for other influence methods. Budget vs. accuracy: Non-monotonic relationship; adding more components beyond optimal subset introduces noise, degrading accuracy toward full-gradient baseline. Early vs. late layers: Early layers stable for paraphrased inputs; late MLP/Q layers useful for model-generated outputs; middle layers consistently weaker.

- **Failure signatures**: Paraphrasing LLM executes instructions instead of rephrasing → task shift → gradient mismatch. Relying on attention K/V projections alone → accuracy near random chance (0.198 vs. 0.2 random baseline). Using full gradient for model-generated setting → accuracy collapses to 0.218 (near random). Selecting components to maximize full-gradient alignment rather than retrieval accuracy → suboptimal retrieval despite high geometric fidelity.

- **First 3 experiments**: 1) Compute retrieval accuracy for each of the 113 components individually on the paraphrased dataset to confirm MLP gate/up and early/late layer patterns. 2) Run forward greedy selection with early stopping when accuracy plateaus, comparing against full-gradient and 5% random projection baselines. 3) Repeat experiments on model-generated completions to verify that selected subsets generalize to harder settings where full gradient fails catastrophically.

## Open Questions the Paper Calls Out

### Open Question 1
Do the superior results of greedy component selection generalize to larger model scales (e.g., 7B+ parameters) and diverse architectures? Basis: The authors acknowledge experiments were limited to a 1.2B model due to resource constraints and explicitly call for exploring "different model architectures, scales, and data domains." Why unresolved: Gradient signal distribution and computational trade-offs may shift significantly in larger models or those with different structural inductive biases. What evidence would resolve it: Replicating the retrieval benchmark on larger open-source models (e.g., Llama-3-8B) to compare the performance curves of selection versus projection.

### Open Question 2
Does the importance of specific layer components remain stable when tracking influence dynamically throughout the training process? Basis: The Limitations section states that future work should "extend analysis to dynamic attribution methods (that track influence throughout the training process...)." Why unresolved: The current study is static (final checkpoint); component sensitivity (e.g., early vs. late layers) might fluctuate across different training stages. What evidence would resolve it: Applying the greedy selection algorithm to gradients aggregated from multiple training checkpoints (e.g., TracIn-style) to determine if the optimal component subset changes over time.

### Open Question 3
Does the targeted selection strategy improve efficacy for downstream tasks like data pruning compared to full gradients? Basis: The authors suggest the method could support "data cleaning- or pruning methods," but only evaluate retrieval accuracy for explanation tasks. Why unresolved: High retrieval accuracy for identifying paraphrased data may not correlate with the ability to identify mislabeled or noisy samples that harm model generalization. What evidence would resolve it: Using the selected gradient subsets to identify and remove training examples (pruning) and measuring the impact on the final model's validation loss.

## Limitations
- Findings based on single 1.2B parameter transformer model may not generalize to larger or differently structured models
- Retrieval accuracy may not be a valid proxy for all types of influence estimation tasks
- Component selection patterns observed primarily on relatively simple paraphrase-based queries, with less clear advantage on more challenging model-generated completions

## Confidence
- **High confidence**: Retrieval accuracy advantage of greedy selection over full gradient and random projection (supported by quantitative results in Tables 1 and Figure 4)
- **Medium confidence**: Component selection patterns (MLP vs attention components) (based on single model architecture, limited corpus support)
- **Medium confidence**: Scalability claims (theoretical O(n²) complexity, but practical bottlenecks from gradient computation not fully explored)

## Next Checks
1. **Cross-architecture validation**: Test component selection patterns on larger models (e.g., 7B+ parameters) and different architectures (e.g., LLaMA, Mistral) to verify the MLP component informativeness findings generalize beyond AMD-OLMo-1B-SFT.

2. **Alternative explanation task validation**: Apply the selected component subsets to influence function estimation or counterfactual explanation tasks beyond retrieval to test whether retrieval-optimal components also perform well for other explanation objectives.

3. **Progressive training regime study**: Evaluate how component informativeness patterns change across training checkpoints (early vs. late training) to understand whether the observed patterns reflect final-layer specialization or persistent architectural properties.