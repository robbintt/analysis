---
ver: rpa2
title: Supervised Fine Tuning of Large Language Models for Domain Specific Knowledge
  Graph Construction:A Case Study on Hunan's Historical Celebrities
arxiv_id: '2511.17012'
source_url: https://arxiv.org/abs/2511.17012
tags:
- knowledge
- extraction
- fine-tuning
- historical
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study proposes a supervised fine-tuning approach to adapt
  large language models for domain-specific knowledge graph construction, focusing
  on Hunan's historical celebrities. By designing a fine-grained schema and constructing
  a domain-specific instruction-tuning dataset, the authors fine-tune four open-source
  LLMs (Qwen2.5-7B, Qwen3-8B, DeepSeek-R1-Distill-Qwen-7B, and Llama-3.1-8B-Instruct)
  using parameter-efficient LoRA.
---

# Supervised Fine Tuning of Large Language Models for Domain Specific Knowledge Graph Construction:A Case Study on Hunan's Historical Celebrities

## Quick Facts
- arXiv ID: 2511.17012
- Source URL: https://arxiv.org/abs/2511.17012
- Reference count: 30
- Key result: Qwen3-8B achieves 89.39 composite score after fine-tuning on 100 samples with 50 epochs

## Executive Summary
This study proposes a supervised fine-tuning approach to adapt large language models for domain-specific knowledge graph construction, focusing on Hunan's historical celebrities. By designing a fine-grained schema and constructing a domain-specific instruction-tuning dataset, the authors fine-tune four open-source LLMs (Qwen2.5-7B, Qwen3-8B, DeepSeek-R1-Distill-Qwen-7B, and Llama-3.1-8B-Instruct) using parameter-efficient LoRA. The experimental results demonstrate significant performance improvements across all models, with Qwen3-8B achieving the best performance. This research provides a cost-effective method for constructing historical character knowledge graphs in low-resource scenarios.

## Method Summary
The paper fine-tunes open-source LLMs using a schema-guided instruction template and LoRA parameter-efficient adaptation. The approach involves constructing a 14-field schema for historical celebrity entities, creating an instruction-tuning dataset in Alpaca format, and applying LoRA fine-tuning to four different models. The evaluation uses a multi-granularity metric combining exact matching for factual attributes and semantic similarity for narrative properties. The best results were achieved with Qwen3-8B using 100 training samples and 50 training epochs.

## Key Results
- Qwen3-8B achieved the highest composite score of 89.3866 after fine-tuning
- LoRA enables parameter-efficient adaptation with only 1% of trainable parameters
- Chain-of-thought reasoning degrades extraction performance (85.10 vs 89.39)
- 100 samples × 50 epochs configuration yields optimal results for this schema complexity

## Why This Works (Mechanism)

### Mechanism 1
Schema-constrained instruction tuning improves extraction fidelity by reducing output variance and aligning model generation with domain-specific semantic boundaries. A fine-grained schema is embedded into the instruction template, and during fine-tuning, the model learns to condition its output generation on these structural constraints rather than free-form generation. The JSON output format enforces schema compliance at the token level.

### Mechanism 2
LoRA enables domain knowledge injection while preserving base model capabilities through low-rank parameter updates that approximate full fine-tuning with 1% of trainable parameters. LoRA freezes pre-trained weights and trains only low-rank matrices where the update is computed as a product of these matrices, ensuring training starts from base model behavior.

### Mechanism 3
Multi-granularity evaluation combining exact matching and vector-space similarity better reflects real-world extraction quality than single-method metrics, particularly for narrative properties with multiple valid expressions. Structured properties use character-level exact matching, while narrative properties use GTE-large-zh embeddings to compute semantic similarity in 1024-dimensional space.

## Foundational Learning

- **Knowledge Graph Schema Design**
  - Why needed here: The entire extraction pipeline depends on a well-defined ontology specifying entity types, properties, and relations.
  - Quick check question: Can you explain the difference between a person-entity property (e.g., birthDate) and a person-person relation (e.g., hasSupervisor), and why each requires different extraction logic?

- **Low-Rank Adaptation (LoRA)**
  - Why needed here: The paper uses LoRA as its primary fine-tuning mechanism. Understanding how low-rank matrices approximate weight updates explains why the method is both efficient and effective for domain adaptation.
  - Quick check question: If a LoRA adapter has rank r=8 and the original weight matrix is 4096×4096, how many total trainable parameters does the adapter add, and what percentage is this of the original?

- **Instruction Tuning Formats**
  - Why needed here: The paper constructs datasets in Alpaca format and designs domain-specific prompt templates. Understanding instruction tuning explains how the model learns task-specific behavior from format conventions.
  - Quick check question: In the Alpaca format, what is the difference between the "instruction" and "input" fields, and why might the input field be left blank as in this paper's Figure 3?

## Architecture Onboarding

- **Component map:**
  1. Ontology Construction → Defines 14-property schema (entity types, attributes, relations)
  2. Data Pipeline → Multi-source collection → preprocessing → annotation
  3. Instruction Dataset → Alpaca-format samples combining schema-guided prompts with JSON outputs
  4. LoRA Fine-tuning → Applied to Qwen2.5-7B, Qwen3-8B, DeepSeek-R1-Distill-Qwen-7B, Llama-3.1-8B-Instruct
  5. Extraction Engine → Fine-tuned model receives text + schema prompt → outputs structured JSON
  6. Evaluation Layer → Multi-granularity scoring (exact match + GTE-large-zh semantic similarity)
  7. Knowledge Graph Storage → JSON-to-Neo4j transformation via Cypher queries

- **Critical path:**
  Schema definition quality → Instruction template design → Training data coverage → LoRA hyperparameters (rank, learning rate, epochs) → Evaluation metric weights. The paper identifies schema and template design as the highest-impact decisions.

- **Design tradeoffs:**
  - Sample size vs. epochs: 100 samples × 50 epochs achieves best score, but 50 samples × 50 epochs yields 87.31—suggesting diminishing returns above 100 samples
  - CoT vs. direct extraction: Enabling chain-of-thought degrades performance (85.10 vs. 89.39) while increasing compute
  - Model selection: Qwen3-8B outperforms Llama-3.1-8B-Instruct on Chinese cultural content, suggesting language-specific pre-training matters more than parameter count

- **Failure signatures:**
  - Model outputs JSON with missing fields (typically "aliases" or "family relations") → training data lacks examples with these properties populated
  - Hallucinated relationships not in source text → CoT enabled or model over-reasoning from pre-trained knowledge
  - Inconsistent entity names across outputs (e.g., "Zeng Guofan" vs. "曾国藩") → schema lacks normalization rules or evaluation doesn't penalize inconsistency
  - Performance plateaus before 30 epochs → learning rate too low or dataset too small for convergence

- **First 3 experiments:**
  1. Baseline replication: Fine-tune Qwen2.5-7B with LoRA (rank=8) on 100 samples × 30 epochs using the paper's exact prompt template. Measure exact-match accuracy on names/dates and semantic similarity on achievements. Expected: 85-87 composite score matching Table 5.
  2. Schema ablation: Remove the "social relations" and "family relations" fields from the schema, reducing output complexity. Retrain and compare extraction accuracy. If scores increase, the original schema may be too complex for the training data size.
  3. Cross-domain transfer: Apply the fine-tuned model to a different Chinese historical domain (e.g., Tang Dynasty poets) without additional training. Measure performance drop to assess domain specificity vs. general historical extraction capability.

## Open Questions the Paper Calls Out

- Can knowledge graphs constructed by fine-tuned LLMs be used to enhance the models' own semantic understanding and reasoning capabilities in a closed-loop system?
- How can the schema be extended to represent complex temporal structures such as event chains and evolving relationship chains?
- Can multi-modal information (images, scanned documents, artifacts) be integrated into the knowledge extraction pipeline for cultural heritage domains?
- How well does this fine-tuning approach transfer to other regional cultural domains with different historical contexts and linguistic characteristics?

## Limitations

- The evaluation methodology uses random weight combinations without clear justification for weight selection or validation of discriminatory power.
- Training data scale is limited (150 total samples, with only 100 used for the best model), raising questions about generalization to larger datasets or other historical domains.
- The semantic similarity component relies on GTE-large-zh embeddings, but domain-specific historical terminology coverage is not verified.

## Confidence

- **High Confidence:** The fundamental mechanism of LoRA fine-tuning and its parameter efficiency. The basic effectiveness of schema-constrained instruction tuning for structured output generation.
- **Medium Confidence:** The specific performance improvements (89.39 score for Qwen3-8B) and relative model rankings, though evaluation methodology introduces uncertainty.
- **Low Confidence:** The absolute quality of the knowledge graph outputs and the robustness of the multi-granularity evaluation metric.

## Next Checks

1. **Evaluation Methodology Validation:** Replicate the evaluation using alternative semantic similarity models and compare results to the GTE-large-zh baseline. Test whether weight variations significantly impact model rankings to assess metric stability.

2. **Cross-Domain Generalization Test:** Apply the fine-tuned Qwen3-8B model to a completely different Chinese historical domain without additional fine-tuning. Measure performance degradation to quantify domain specificity.

3. **Sample Size Scalability Study:** Systematically vary training sample sizes (25, 50, 100, 150) while keeping epochs constant at 50, and plot performance curves to reveal whether the current 100-sample configuration represents an optimal point.