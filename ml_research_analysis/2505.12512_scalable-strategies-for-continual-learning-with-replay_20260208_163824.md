---
ver: rpa2
title: Scalable Strategies for Continual Learning with Replay
arxiv_id: '2505.12512'
source_url: https://arxiv.org/abs/2505.12512
tags:
- learning
- replay
- task
- continual
- merging
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of scalable continual learning,
  particularly the inefficiency of replay methods which can double training costs.
  The authors propose three main strategies: 1) Applying low-rank adaptation (LoRA)
  for efficient and regularized fine-tuning, showing it excels in highly under-regularized
  regimes; 2) Introducing a consolidation phase that reduces replay samples during
  task learning then allocates saved samples to a post-task consolidation period,
  achieving up to 55% reduction in total replay samples needed for a given performance
  target; 3) Developing sequential merging, an adaptation of model merging techniques
  for continual learning that merges pre- and post-task weights sequentially, achieving
  performance comparable to exponential moving average baselines.'
---

# Scalable Strategies for Continual Learning with Replay

## Quick Facts
- arXiv ID: 2505.12512
- Source URL: https://arxiv.org/abs/2505.12512
- Reference count: 40
- Authors: Truman Hickok
- Primary result: Achieves up to 65% reduction in replay samples while maintaining baseline performance

## Executive Summary
This paper addresses the challenge of scalable continual learning by proposing three complementary strategies to improve the efficiency of replay-based methods. The authors demonstrate that replay methods can double training costs, and present solutions through low-rank adaptation (LoRA) for regularization, a consolidation phase for strategic sample redistribution, and sequential merging for parameter integration. The combined approach achieves up to 65% reduction in replay samples needed to match baseline performance, significantly improving the scalability of continual learning systems.

## Method Summary
The paper proposes three strategies for efficient continual learning with replay. First, LoRA is applied to provide inherent regularization against catastrophic forgetting, particularly effective in under-regularized regimes by constraining weight updates to low-rank subspaces. Second, a consolidation phase redistributes replay samples by reducing replay ratio during task learning and allocating saved samples to a post-task consolidation period, achieving up to 55% reduction in total replay samples. Third, sequential merging adapts model merging techniques by linearly combining pre- and post-task weights after each task, achieving performance comparable to exponential moving average baselines. The authors demonstrate that these strategies can be synergistically combined to maximize efficiency gains.

## Key Results
- LoRA excels in under-regularized regimes by preventing large weight deviations that could overwrite prior task solutions
- Consolidation achieves up to 55% reduction in total replay samples needed while maintaining performance
- Sequential merging achieves performance comparable to exponential moving average baselines
- Combined approach (consolidation + sequential merging + informed LoRA) reaches baseline performance with up to 65% fewer replay samples

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Low-rank adaptation (LoRA) provides inherent regularization against catastrophic forgetting in under-regularized regimes
- Mechanism: LoRA constrains weight updates to a low-rank subspace by decomposing ΔW into smaller matrices A and B, limiting the representational capacity for changes per task and preventing large weight deviations that could overwrite prior task solutions
- Core assumption: The change in model weights during adaptation can be effectively approximated by a low-rank decomposition with rank r ≪ min(d, k)
- Evidence anchors:
  - [abstract] "Applying low-rank adaptation (LoRA) for efficient and regularized fine-tuning, showing it excels in highly under-regularized regimes"
  - [section 4.1] "By restricting ΔW to a limited subspace, LoRA prevents large weight deviations that could overwrite prior task solutions, thereby serving as a built-in hedge against catastrophic forgetting"
  - [corpus] Weak direct evidence - corpus papers focus on merging and compression but don't validate LoRA in replay-based CL
- Break condition: LoRA underperforms when tasks are large (>10 classes per task), when replay ratio is high (>0.5), or when new task requirements lie outside the subspace spanned by low-rank adapters

### Mechanism 2
- Claim: Phasic redistribution of replay samples through consolidation improves sample efficiency by up to 55%
- Mechanism: Reducing replay ratio during task learning decreases computational burden, then a dedicated post-task consolidation phase reinstates degraded knowledge using saved replay samples on a balanced distribution
- Core assumption: Knowledge degradation during task learning can be effectively repaired through targeted post-hoc training, and the benefits of delayed, concentrated replay outweigh interleaved rehearsal
- Evidence anchors:
  - [abstract] "Introducing a consolidation phase that reduces replay samples during task learning then allocates saved samples to a post-task consolidation period, achieving up to 55% reduction in total replay samples needed"
  - [section 5.2] "Consolidation allows a model trained with up to 55% less total replay samples to match the 1.0 RR baseline"
  - [corpus] Weak evidence - corpus mentions sample compression for CL but doesn't address consolidation specifically
- Break condition: Breaks down when tasks are extremely small (≤2 classes per task) due to increased interference, or when consolidation step rate is poorly tuned to the degradation patterns

### Mechanism 3
- Claim: Sequential merging of pre- and post-task weights achieves comparable performance to exponential moving average baselines with greater efficiency and flexibility
- Mechanism: After each task, merge pre-task weights θ_(t-1) with post-task weights θ*_t using: θ_t = (1-α)θ_(t-1) + αθ*_t, allowing tunable integration without storing gradient-step checkpoints
- Core assumption: Task-specific parameter updates can be linearly combined while preserving performance, and interference between sequential tasks can be mitigated through weighted averaging
- Evidence anchors:
  - [abstract] "Developing sequential merging... achieving performance comparable to exponential moving average baselines"
  - [section 6.3] "Sequential merging is better suited to the continual learning setting than parallel merging and can match the strong EMA baseline"
  - [corpus] Strong support - "Modular Delta Merging" and "Holistic Approach to Continual Model Merging" validate merging for CL
- Break condition: Degrades when merging coefficient α is poorly calibrated to task similarity, or when task count becomes very large (>50 tasks) without adaptive tuning

## Foundational Learning

- Concept: Catastrophic Forgetting
  - Why needed here: The paper assumes baseline understanding of why neural networks forget previous tasks when learning new ones; all three mechanisms directly address this phenomenon
  - Quick check question: Can you explain why standard gradient descent on sequential tasks causes performance degradation on earlier tasks?

- Concept: Replay Ratio (RR)
  - Why needed here: Central to the consolidation strategy; defined as N_replay/N_task and directly controls the tradeoff between computational cost and retention
  - Quick check question: Given a fixed batch size of 64, what replay ratio would allocate 16 samples to current task data?

- Concept: Model Merging / Task Arithmetic
  - Why needed here: Sequential merging is adapted from this paradigm; assumes understanding that task-specific fine-tunes can be combined via parameter arithmetic
  - Quick check question: What is the "task vector" in task arithmetic, and how is it computed from a fine-tuned model?

## Architecture Onboarding

- Component map:
  Replay Buffer -> LoRA Module Factory -> Consolidation Scheduler -> Sequential Merger -> Loss Combiner

- Critical path:
  1. Initialize LoRA adapters for incoming task
  2. Train with reduced replay ratio (e.g., 0.25 vs. baseline 1.0)
  3. Merge LoRA into base weights
  4. Execute consolidation phase on balanced replay distribution
  5. Apply sequential merge with tuned α
  6. Proceed to next task

- Design tradeoffs:
  - LoRA rank vs. representational capacity: Lower rank (r=8) better for small tasks (1-6 classes), higher rank (r=32) for larger tasks (10-30 classes)
  - Replay ratio vs. consolidation step rate: Total Replay Percentage (TRP) should target 35-55% for efficiency gains
  - Sequential merging α vs. retention: Lower α prioritizes stability, higher α prioritizes plasticity
  - Assumption: The paper hypothesizes that consolidation can be delayed across multiple tasks (increasing effective task size) but acknowledges unproven scalability beyond tens of tasks

- Failure signatures:
  - LoRA: Sudden accuracy drop on new tasks when rank is too low for task complexity
  - Consolidation: Pre-training accuracy degrades despite consolidation; indicates need for targeted replay distribution (future work)
  - Sequential merging: Performance divergence from EMA baseline as task count increases; indicates need for adaptive α tuning
  - Combined: Total Replay Percentage drops below 35% with significant accuracy loss; suggests minimum replay threshold

- First 3 experiments:
  1. Baseline establishment: Run full fine-tuning with 1.0 replay ratio across 20 tasks (6 classes each) in CIL and CPT settings to establish reference performance
  2. Component isolation: Test each mechanism independently (LoRA only, consolidation only, sequential merging only) to characterize individual contribution and break conditions
  3. Combined system validation: Implement full pipeline (LoRA + consolidation + sequential merging) at 35% TRP and verify 65% replay reduction claim matches baseline accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do these scalable replay strategies transfer effectively to multimodal models?
- Basis in paper: [explicit] The authors hypothesize that the methods will "seamlessly transfer to multimodal models" despite limiting experiments to image classification.
- Why unresolved: The study restricted validation to image classification, which lacks the smooth boundaries present in multimodal data (e.g., CLIP).
- What evidence would resolve it: Empirical evaluation of the combined strategies on multimodal benchmarks.

### Open Question 2
- Question: How do these strategies perform at extreme scales (billions of parameters or hundreds of tasks)?
- Basis in paper: [explicit] The authors state that performance characteristics for "extremely large task sequences or model sizes" remain an open question.
- Why unresolved: Experiments were limited to tens of tasks and standard model sizes, whereas real-world deployments may involve much larger scales.
- What evidence would resolve it: Benchmarking the methods on large-scale models (e.g., billion-parameter ViTs) across hundreds of sequential tasks.

### Open Question 3
- Question: Does targeted sampling based on accuracy drift improve the consolidation phase?
- Basis in paper: [explicit] The authors use a balanced distribution as a "proof-of-concept" but suggest sampling from classes with accuracy drops could be explored in future work.
- Why unresolved: The current implementation uses a uniform distribution, leaving potential efficiency gains from adaptive, targeted sampling unverified.
- What evidence would resolve it: A comparison of uniform replay versus drift-aware sampling during the post-task consolidation period.

## Limitations

- Validation is primarily limited to vision tasks with a single architecture (OpenCLIP)
- Consolidation mechanism is not fully evaluated in multi-task scenarios where delayed replay might cause interference
- Sequential merging approach only compared to EMA baselines without exploring adaptive weight decay or more recent merging strategies
- Paper does not address potential replay buffer staleness or computational overhead of repeated consolidation phases

## Confidence

- **High** for LoRA regularization effects in under-regularized regimes
- **Medium** for consolidation strategy sample efficiency claims
- **Medium** for sequential merging performance relative to EMA

## Next Checks

1. Test the consolidation phase with delayed replay across multiple tasks (>10) to verify scalability limits
2. Evaluate LoRA rank sensitivity across diverse task sizes (1 to 30 classes) in a single experimental sweep
3. Benchmark sequential merging against adaptive EMA variants and recent task-aware merging methods