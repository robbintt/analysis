---
ver: rpa2
title: 'End-to-End Bangla AI for Solving Math Olympiad Problem Benchmark: Leveraging
  Large Language Model Using Integrated Approach'
arxiv_id: '2501.04425'
source_url: https://arxiv.org/abs/2501.04425
tags:
- bangla
- reasoning
- problems
- mathematical
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study presents a systematic approach to enhance large language
  models (LLMs) for solving Bangla math Olympiad problems. The methodology integrates
  fine-tuning with specialized datasets, Retrieval-Augmented Generation (RAG), and
  customized prompting strategies.
---

# End-to-End Bangla AI for Solving Math Olympiad Problem Benchmark: Leveraging Large Language Model Using Integrated Approach

## Quick Facts
- arXiv ID: 2501.04425
- Source URL: https://arxiv.org/abs/2501.04425
- Reference count: 16
- Best accuracy: 77/100 on Bangla math Olympiad problems

## Executive Summary
This study presents an integrated approach to enhance large language models for solving Bangla mathematics Olympiad problems. The methodology combines fine-tuning with specialized datasets, Retrieval-Augmented Generation (RAG), and customized prompting strategies. Models were evaluated on Bangla datasets, with Qwen2.5-32B-Instruct-AWQ achieving the highest accuracy of 77/100. The research demonstrates that multilingual reasoning, problem categorization, and tailored prompts significantly impact performance, while highlighting the effectiveness of larger model parameters and domain-specific fine-tuning.

## Method Summary
The approach employs a three-phase fine-tuning pipeline for Qwen2.5-7B-Instruct: first on 72K TIR dataset, then 250K CoT subset, and finally synthetic/augmented data. RAG uses keyword-based retrieval to surface similar problems as few-shot examples. TIR agents generate Python code iteratively in a REPL environment, with self-consistency voting aggregating results. The system processes Bangla problem statements while generating reasoning steps in English, leveraging stronger mathematical reasoning capabilities in English while maintaining input comprehension.

## Key Results
- Qwen2.5-32B-Instruct-AWQ achieved 77/100 accuracy without fine-tuning
- Fine-tuned Qwen2.5-7B-Instruct with TIR+CoT+RAG datasets reached 71/100 accuracy
- Cross-lingual reasoning (Bangla problem, English steps) improved accuracy by 4 points
- Larger model parameters and domain-specific fine-tuning showed significant performance gains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Tool-Integrated Reasoning (TIR) with self-consistency voting improves mathematical accuracy by enabling iterative code-based verification.
- Mechanism: Multiple TIR agents generate distinct solution paths as executable Python code in a REPL environment. Failed paths trigger re-evaluation; successful paths undergo majority voting to select the final answer.
- Core assumption: Mathematical reasoning can be decomposed into verifiable computational steps, and consensus across independent solution paths correlates with correctness.
- Evidence anchors: [abstract] "iterative reasoning improve the model's efficiency"; [section 2.6] "TIR agents generate python code iteratively until the correct solution is found... final answer is chosen by majority voting"; [corpus] SBSC paper (arXiv:2502.16666) supports multi-turn code generation for Olympiad problems
- Break condition: If Python execution environment unavailable, or if problems require non-computational proofs, mechanism degrades to standard inference.

### Mechanism 2
- Claim: Keyword-based RAG retrieval provides contextual scaffolding by surfacing structurally similar problems as few-shot examples.
- Mechanism: Keywords manually extracted from training data identify problem categories. Similarity search retrieves analogous problems with TIR solutions, which are injected into prompts as few-shot demonstrations.
- Core assumption: Problem-solving strategies transfer across problems sharing conceptual keywords and structural patterns.
- Evidence anchors: [abstract] "implementation of Retrieval-Augmented Generation (RAG)"; [section 2.3] "identified significant keywords... conducted similarity searches... incorporated as few-shot instances"; [corpus] DAGGER paper (arXiv:2601.06853) examines CoT behavior in Bangla math with context augmentation
- Break condition: If keyword extraction is imprecise or problem categories are novel, retrieval may surface irrelevant examples, potentially degrading performance.

### Mechanism 3
- Claim: Multilingual cross-reasoning (Bangla problem, English reasoning steps) enhances accuracy compared to monolingual Bangla processing.
- Mechanism: The model processes problem statements in Bangla but generates intermediate reasoning in English, leveraging stronger English mathematical reasoning capabilities while maintaining input comprehension.
- Core assumption: LLMs encode mathematical reasoning primarily in English representations; cross-language reasoning leverages this while preserving problem semantics.
- Evidence anchors: [section 4] "if the problem statement is in Bangla and the solution steps are described in English, there is an enhanced accuracy score"; [table 2] Shows GPT-4o TIR with Bangla problem + English reasoning achieves 67/100 vs 63/100 for Bangla/Bangla; [corpus] Weak direct evidence; neighboring papers don't specifically address cross-lingual reasoning
- Break condition: If translation introduces semantic drift or model lacks sufficient Bangla comprehension, mechanism fails.

## Foundational Learning

- Concept: **Retrieval-Augmented Generation (RAG)**
  - Why needed here: Core infrastructure for providing few-shot examples; requires understanding embedding-based similarity and retrieval indexing.
  - Quick check question: Can you explain how a retrieved document becomes part of an LLM's context window?

- Concept: **Tool-Integrated Reasoning (TIR)**
  - Why needed here: Agents must generate executable Python code; requires understanding code generation, REPL execution, and error handling loops.
  - Quick check question: How would you design a feedback loop where code execution errors inform subsequent LLM queries?

- Concept: **Self-Consistency Decoding**
  - Why needed here: Multiple solution paths require aggregation; majority voting over sampled outputs is the ensemble mechanism.
  - Quick check question: If 5 TIR agents produce answers [42, 42, 17, 42, 99], what is the self-consistency output?

## Architecture Onboarding

- Component map:
  Input Layer: Bangla problem text → keyword extraction module
  Retrieval Layer: Keyword-matched RAG search → few-shot example assembly
  Inference Layer: LLM (Qwen2.5-7B-Instruct or 32B-AWQ) + TIR agents
  Execution Layer: Python REPL sandbox for code verification
  Aggregation Layer: Self-consistency voting across agent outputs
  Fine-tuning Pipeline: TIR dataset → CoT dataset → Synthetic augmentation (staged training)

- Critical path:
  1. Problem categorization (manual keyword matching)
  2. Few-shot example retrieval via RAG
  3. Multi-agent TIR inference with tailored prompts
  4. Code execution and iterative refinement
  5. Majority voting for final answer

- Design tradeoffs:
  - Model size vs. compute: 32B-AWQ achieves 77% without fine-tuning but requires more inference resources; 7B fine-tuned reaches 71% with lower footprint.
  - RAG complexity vs. gain: Paper notes keyword-based RAG "does not improve up to satisfactory level" — suggests semantic embedding retrieval may be worth exploring.
  - Agent count vs. latency: Table 4 shows 10 agents with instructions outperform 42 without; diminishing returns above 10.

- Failure signatures:
  - Deepseek-Math-7B-Instruct achieves only 28/100 — model selection critical for Bangla
  - Qwen2.5-7B-Coder fails to generate code for Bangla problems — domain mismatch
  - Pure Bangla reasoning underperforms cross-lingual setup

- First 3 experiments:
  1. Baseline probe: Run Qwen2.5-7B-Instruct on 20 held-out BDMO problems with basic prompt; measure accuracy and identify failure categories.
  2. RAG ablation: Compare keyword-based retrieval vs. no retrieval on same problems; quantify few-shot impact.
  3. TIR agent sweep: Test 3, 5, and 10 agents with identical prompts; plot accuracy vs. latency tradeoff curve.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can advanced semantic or dense retrieval methods significantly outperform the keyword-based search used in this study for Bangla Math RAG?
- Basis in paper: [explicit] The authors state in the Conclusion that the keyword-based RAG implementation "does not improve up to satisfactory level" and explicitly list "optimizing retrieval models" as future work.
- Why unresolved: The current methodology relied on simple keyword matching, which may fail to capture the semantic nuances required for complex mathematical problem retrieval.
- What evidence would resolve it: A comparative evaluation of dense embedding-based retrievers versus keyword search on the same Bangla Math Olympiad benchmark.

### Open Question 2
- Question: Does applying the proposed fine-tuning and RAG pipeline to the Qwen2.5-32B model yield significant accuracy improvements over its base performance?
- Basis in paper: [explicit] The authors note that the 32B model achieved the highest score (77/100) without any fine-tuning or RAG, suggesting "This one might be improved too with help of our additional fine-tuning."
- Why unresolved: Resource or scope constraints prevented the authors from applying the full integrated approach (TIR, CoT, RAG) to the larger 32B parameter model.
- What evidence would resolve it: Fine-tuning the Qwen2.5-32B-Instruct model with the curated datasets and comparing the results against the 32B baseline and the fine-tuned 7B model.

### Open Question 3
- Question: To what extent does the performance bottleneck stem from the reliance on translated datasets versus native, human-curated Bangla mathematical data?
- Basis in paper: [explicit] The Conclusion hypothesizes that "a more curated and well organized dataset fine tuning may lead to more improvised and upgraded result," acknowledging that current data relied heavily on translation and synthetic generation.
- Why unresolved: The study utilized translated Numina datasets and synthetic GPT-4o data; the specific impact of translation noise or synthetic artifacts on reasoning capabilities was not isolated.
- What evidence would resolve it: Training separate models on native Bangla math datasets versus translated datasets and benchmarking the difference in error rates.

## Limitations
- Fine-tuning hyperparameters (learning rate, batch size, sequence length) are not disclosed, creating significant barriers to reproduction.
- RAG implementation details are sparse—keyword extraction methodology, embedding models, and retrieval thresholds are unspecified.
- TIR agent configuration lacks precision (exact agent count, iteration limits, temperature settings).

## Confidence
- High confidence: Model selection matters, multilingual reasoning improves accuracy, and fine-tuning with domain datasets demonstrably enhances performance.
- Medium confidence: TIR with self-consistency voting is effective, though error analysis on failures is limited; RAG's marginal benefit suggests implementation rather than concept may be limiting.
- Low confidence: The claim that keyword-based RAG is the optimal retrieval approach, given its noted underperformance and lack of comparison with semantic embedding methods.

## Next Checks
1. **Ablation study on fine-tuning stages**: Train Qwen2.5-7B-Instruct with only TIR dataset, only CoT dataset, and both datasets to quantify individual contributions to the 71% accuracy gain.
2. **RAG methodology comparison**: Implement both keyword-based and semantic embedding retrieval systems on the same 20-problem subset to measure relative effectiveness.
3. **Cross-lingual reasoning validation**: Systematically test Bangla problem/English reasoning vs. Bangla/Bangla setups across all problem categories to confirm the 4-point accuracy differential reported in Table 2.