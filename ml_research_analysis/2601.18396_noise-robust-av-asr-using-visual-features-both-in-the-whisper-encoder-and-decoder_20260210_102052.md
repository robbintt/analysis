---
ver: rpa2
title: Noise-Robust AV-ASR Using Visual Features Both in the Whisper Encoder and Decoder
arxiv_id: '2601.18396'
source_url: https://arxiv.org/abs/2601.18396
tags:
- whisper
- fusion
- noise
- visual
- dual-use
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses noise robustness in audiovisual automatic speech
  recognition (AV-ASR) by proposing a dual-use fusion method that incorporates visual
  features both in the Whisper encoder and decoder. The method uses visual features
  from AV-HuBERT to model audiovisual interactions in the encoder and weigh modalities
  in the decoder.
---

# Noise-Robust AV-ASR Using Visual Features Both in the Whisper Encoder and Decoder

## Quick Facts
- arXiv ID: 2601.18396
- Source URL: https://arxiv.org/abs/2601.18396
- Reference count: 0
- This work proposes dual-use fusion that incorporates visual features both in the Whisper encoder and decoder, significantly improving noise robustness in audiovisual automatic speech recognition.

## Executive Summary
This work addresses noise robustness in audiovisual automatic speech recognition (AV-ASR) by proposing a dual-use fusion method that incorporates visual features both in the Whisper encoder and decoder. The method uses visual features from AV-HuBERT to model audiovisual interactions in the encoder and weigh modalities in the decoder. The dual-use approach significantly improves noise robustness compared to existing methods, achieving 35% relative improvement (WER: 4.41% vs. 6.83%) over middle fusion on Whisper small and 57% relative improvement (WER: 4.07% vs. 9.53%) on Whisper medium in babble noise with 0dB SNR. Fine-tuned on 1929 hours of audiovisual data, the dual-use method using Whisper medium achieves 4.08% (MUSAN babble noise) and 4.43% (NoiseX babble noise) average WER across various SNRs, establishing a new state-of-the-art on the LRS3 AV-ASR benchmark.

## Method Summary
The method involves two-stage fine-tuning on 1929 hours of audiovisual data (LRS3, LRS2, Voxceleb2-EN). Visual features from the 24th block of AV-HuBERT are upsampled and projected to match Whisper's dimension, then multiplied by a zero-initialized scalar α and added to acoustic features in the encoder. Flamingo blocks with gated cross-attention are inserted before each Whisper decoder block to attend to visual features. Audio input uses 26-dim log-filterbank features at 100 fps, video uses grayscale 88×88 ROIs at 25 fps. Training includes MUSAN babble noise augmentation at 0dB SNR.

## Key Results
- Dual-use fusion achieves 35% relative improvement (WER: 4.41% vs. 6.83%) over middle fusion on Whisper small in babble noise
- Dual-use fusion achieves 57% relative improvement (WER: 4.07% vs. 9.53%) over middle fusion on Whisper medium in babble noise
- Dual-use method with Whisper medium achieves 4.08% (MUSAN babble) and 4.43% (NoiseX babble) average WER across SNRs, establishing new state-of-the-art on LRS3

## Why This Works (Mechanism)

### Mechanism 1
Adding visual features to the Whisper encoder with zero-initialized scaling enables audiovisual interaction modeling without destabilizing pre-trained representations. Visual features from AV-HuBERT are upsampled, projected to match Whisper's dimension, multiplied by a zero-initialized scalar α, then added to acoustic features. The zero-start allows gradual visual feature injection during training, preserving Whisper's pre-trained audio representations while learning cross-modal interactions through the encoder's attention layers.

### Mechanism 2
Flamingo blocks in the decoder enable dynamic modality weighting based on noise conditions and linguistic context. Gated cross-attention blocks inserted before each Whisper decoder block attend to visual features from AV-HuBERT. The gated mechanism (zero-initialized) allows the decoder to selectively amplify visual information when audio is unreliable, while suppressing it in clean conditions where audio dominates.

### Mechanism 3
Visual features from later AV-HuBERT encoder blocks provide more robust noise compensation than early layers. AV-HuBERT's 24-block encoder progressively abstracts visual representations. Later blocks capture higher-level linguistic-visual correlations (phoneme-to-viseme mappings) rather than low-level pixel patterns, providing more consistent noise-robust information.

## Foundational Learning

- **Transformer Encoder-Decoder Attention Patterns**: Understanding how Whisper's encoder representations flow to decoder is essential for tracing where visual features must integrate. Quick check: Can you sketch how cross-attention connects encoder outputs to decoder layers in a standard transformer?

- **Gated Multi-Modal Fusion (Flamingo-style)**: The Flamingo block's gated mechanism controls information flow; understanding gating dynamics explains why zero-initialization matters. Quick check: What happens to gradient flow if a gate is initialized to near-zero versus near-one?

- **Self-Supervised Visual Representation Learning (AV-HuBERT)**: AV-HuBERT provides the visual features; knowing it's trained via masked multimodal cluster prediction helps interpret what the 24th block encodes. Quick check: Why would a model trained on masked prediction learn noise-robust visual features?

## Architecture Onboarding

- **Component map**: AV-HuBERT large encoder (325M params) → grayscale video → visual features h^V (24 blocks, extract from 24th) → Whisper encoder → audio log-filterbank + (visual features × α) → audiovisual representations h^AV → Whisper decoder → previous tokens + h^AV → Flamingo blocks (attend to h^V) → decoder blocks → token probabilities

- **Critical path**: Visual feature quality (AV-HuBERT block selection) → encoder fusion stability (α scaling) → decoder modality weighing (Flamingo gates). Failure at any stage propagates.

- **Design tradeoffs**: Addition vs. concatenation fusion: Addition with zero-initialized α enables smooth training; concatenation disrupted pre-trained weights (Table 2: 12.15% vs. 5.28% WER on dev 0dB). Encoder vs. decoder fusion: Encoder-only (early) learns interactions but struggles in larger models; decoder-only (middle) exploits LM but lacks interactions; dual-use combines both. Model scale: Larger Whisper (medium) improves clean performance but requires more fine-tuning data to prevent early fusion degradation.

- **Failure signatures**: WER spikes in clean condition with early fusion on large models → gradient flow issue, reduce model depth or use dual-use. Visual features from early AV-HuBERT blocks → high WER in noise, switch to later blocks. Concatenation fusion → training instability, switch to addition with zero-initialized scalar α.

- **First 3 experiments**: 1) Replicate dual-use on Whisper tiny with LRS3 subset to validate encoder fusion (α should increase from 0 during training). 2) Ablate Flamingo blocks (encoder-only fusion) to isolate decoder contribution to noise robustness. 3) Visualize gate activations across SNR levels to confirm learned modality weighting (clean → audio-dominant, 0dB → visual-augmented).

## Open Questions the Paper Calls Out
The paper doesn't explicitly call out open questions in the text provided.

## Limitations
- The paper provides limited details on Flamingo block implementation, making exact replication challenging.
- Critical training hyperparameters like batch size, optimizer settings, and inference parameters are omitted.
- Evaluation is restricted to LRS3 dataset with limited noise conditions (babble noise only).
- Early fusion shows performance degradation with larger Whisper models, suggesting potential scalability issues.

## Confidence

**High confidence**: The dual-use fusion approach achieves state-of-the-art results on LRS3 benchmark with significant relative improvements over existing methods (35-57% WER reduction in babble noise).

**Medium confidence**: Visual features from later AV-HuBERT encoder blocks (24th) provide optimal noise-robust representations. While ablation results support this, the mechanism lacks theoretical justification and may be dataset-dependent.

**Low confidence**: The zero-initialized scaling parameter α ensures stable training by preventing early disruption of pre-trained audio representations. The paper doesn't provide empirical validation of α's trajectory during training or its impact on convergence.

## Next Checks
1. **Zero-initialization trajectory validation**: Track the scaling parameter α during training across different model sizes (tiny, small, medium). Verify that α increases from zero and that larger models show slower but stable growth, explaining why early fusion fails at scale.

2. **Modality weighting analysis**: Extract and visualize the Flamingo gate activations across different SNR conditions. Confirm that gates learn to suppress visual features in clean audio while amplifying them in noisy conditions, validating the context-dependent modality selection claim.

3. **Layer ablation study**: Systematically test visual feature extraction from AV-HuBERT blocks 0, 6, 12, 18, and 24 on a subset of the LRS3 dev set across multiple noise types. Characterize the trade-off between temporal resolution and linguistic abstraction to establish when later-block features become counterproductive.