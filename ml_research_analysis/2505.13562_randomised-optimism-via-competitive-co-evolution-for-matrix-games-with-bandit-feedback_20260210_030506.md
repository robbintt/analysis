---
ver: rpa2
title: Randomised Optimism via Competitive Co-Evolution for Matrix Games with Bandit
  Feedback
arxiv_id: '2505.13562'
source_url: https://arxiv.org/abs/2505.13562
tags:
- coebl
- regret
- matrix
- games
- iteration
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses learning in two-player zero-sum matrix games
  with bandit feedback, where the payoff matrix is unknown and players only observe
  noisy rewards. The core method, Competitive Co-evolutionary Bandit Learning (COEBL),
  integrates evolutionary algorithms with bandit learning to implement randomised
  optimism via variation operators.
---

# Randomised Optimism via Competitive Co-Evolution for Matrix Games with Bandit Feedback

## Quick Facts
- **arXiv ID:** 2505.13562
- **Source URL:** https://arxiv.org/abs/2505.13562
- **Reference count:** 40
- **Key outcome:** COEBL achieves sublinear regret and outperforms UCB, EXP3, and EXP3-IX in two-player zero-sum matrix games with bandit feedback.

## Executive Summary
This paper introduces Competitive Co-evolutionary Bandit Learning (COEBL), a method for learning in two-player zero-sum matrix games where the payoff matrix is unknown and players only receive noisy rewards. COEBL combines evolutionary algorithms with bandit learning by implementing randomised optimism through Gaussian mutation of estimated payoffs. Theoretical analysis proves sublinear regret, matching deterministic optimism methods, while empirical results on Rock-Paper-Scissors, DIAGONAL, and BIGGER NUMBER games show consistent improvement over classical bandit algorithms.

## Method Summary
COEBL addresses matrix games with bandit feedback by maintaining estimates of the payoff matrix and applying randomised optimism through Gaussian mutation operators. At each round, it constructs a perturbed estimated matrix by adding noise scaled to confidence bounds, solves a maximin optimization problem on this perturbed matrix to generate candidate policies, and updates only when the candidate improves the worst-case payoff. The method integrates evolutionary variation operators with bandit learning principles, using strict selection based on maximin fitness to stabilize learning while maintaining exploration.

## Key Results
- COEBL achieves sublinear regret, matching the performance of deterministic optimism-based methods like UCB
- In Rock-Paper-Scissors, COEBL converges to the Nash equilibrium (1/3, 1/3, 1/3)
- Across all benchmark games, COEBL consistently outperforms EXP3, EXP3-IX, and UCB in terms of regret and convergence to equilibrium

## Why This Works (Mechanism)

### Mechanism 1: Randomised Optimism via Variation Operators
Injecting noise scaled to confidence bounds enables exploration comparable to deterministic UCB methods. The algorithm perturbs the estimated payoff matrix with Gaussian noise added to the UCB term, ensuring exploration without rigidity while maintaining theoretical optimism. This requires sufficient mutation rate (c ≥ 8 theoretically, though c=2 works empirically in RPS).

### Mechanism 2: Selection via Maximin Fitness
A fitness-based selection mechanism stabilizes learning by only updating policies that strictly improve the worst-case payoff. After generating a candidate policy by solving the maximin problem on the perturbed matrix, COEBL compares fitness values and updates only if the candidate is strictly better, preventing oscillation due to random noise.

### Mechanism 3: Co-Evolutionary Best-Response Dynamics
The algorithm treats the opponent's strategy as part of the environment to be optimized against. By maintaining entry-wise counts and estimating the matrix, the agent simulates the opponent's best response to shape its own strategy, converging to Nash Equilibrium through self-play dynamics.

## Foundational Learning

- **Nash Equilibrium & Maximin Optimisation**: Understanding the difference between greedy and maximin policies is critical since the objective is minimizing potential loss against a rational opponent. *Quick check: Why does a deterministic greedy strategy fail in Rock-Paper-Scissors?*

- **Multi-Armed Bandits (UCB & EXP3)**: COEBL modifies the standard bandit setup, requiring understanding of the "Optimism in the Face of Uncertainty" principle. *Quick check: How does UCB regret typically scale with time T compared to EXP3 in stochastic settings?*

- **Evolutionary Algorithm Components**: The paper uses evolutionary computing terminology (variation operators, mutation, fitness) as perturbation and selection steps. *Quick check: In this context, does mutation change the policy directly or the estimated environment?*

## Architecture Onboarding

- **Component map:** Estimator -> Variation Operator (Mutate) -> Solver (LP) -> Selector
- **Critical path:** The computational bottleneck is the Solver, which runs full LP optimization at every round on the m × m matrix
- **Design tradeoffs:** Mutation rate c requires theoretical minimum of 8 but empirical RPS uses c=2; strict selection ensures stability but may slow adaptation in noisy environments
- **Failure signatures:** Linear regret if exploration fails (mutation too low); stalling if LP solutions don't pass selection gate
- **First 3 experiments:** 1) Implement COEBL on RPS against itself to verify convergence to (0.33, 0.33, 0.33); 2) Run ablation on c values in DIAGONAL to observe trade-off; 3) Pit COEBL vs UCB on BIGGER NUMBER to verify randomised optimism advantage

## Open Questions the Paper Calls Out

1. **Can sublinear regret bounds be proven for mutation constants c < 8?** The current analysis assumes c ≥ 8 due to technical constraints, though empirical RPS uses c=2 successfully.

2. **Can COEBL extend to general-sum games, multi-player settings, or Markov games?** The current framework is specialized for two-player zero-sum matrix games.

3. **Does performance hold under heavier-tailed noise distributions?** Current analysis assumes sub-Gaussian noise, but extending to sub-exponential noise is unexplored.

4. **How to guarantee convergence in games with exponentially large action spaces?** COEBL achieves sublinear regret but fails to converge to Nash equilibrium in high-dimensional instances like DIAGONAL (n ≥ 5).

## Limitations

- Theoretical requirement of mutation rate c ≥ 8 conflicts with empirical success at c=2, creating a gap between theory and practice
- Computational cost of solving full LP at every iteration (O(m³)) limits scalability to large action spaces
- While achieving sublinear regret, COEBL fails to converge to Nash equilibrium in high-dimensional games due to curse of dimensionality

## Confidence

- **High**: Core algorithmic framework and bandit regret analysis are sound and well-grounded in established literature
- **Medium**: Empirical results demonstrate advantage over baselines, though hyperparameter tuning appears critical and not fully explained
- **Low**: Theoretical guarantees require c ≥ 8 but experiments use c=2, creating disconnect between theory and practice

## Next Checks

1. Verify the LP solver formulation by testing on small games where the exact Nash equilibrium is known
2. Conduct ablation study on mutation rate c across all three benchmark games to map theoretical-practical gap
3. Implement computationally efficient approximation for larger action spaces to test scalability claims