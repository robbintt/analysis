---
ver: rpa2
title: Generative Retrieval with Few-shot Indexing
arxiv_id: '2408.02152'
source_url: https://arxiv.org/abs/2408.02152
tags:
- few-shot
- indexing
- retrieval
- document
- docids
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes Few-Shot GR, a novel generative retrieval framework
  that uses few-shot prompting instead of training-based indexing. Existing generative
  retrieval methods rely on training a model to map queries to document identifiers
  (docids), which is resource-intensive and limits adaptability to dynamic corpora.
---

# Generative Retrieval with Few-shot Indexing

## Quick Facts
- **arXiv ID:** 2408.02152
- **Source URL:** https://arxiv.org/abs/2408.02152
- **Reference count:** 38
- **Key outcome:** Proposes Few-Shot GR, a generative retrieval framework using few-shot prompting instead of training-based indexing, showing improved recall and MRR on standard benchmarks while being more efficient.

## Executive Summary
The paper introduces Few-Shot GR, a novel generative retrieval framework that replaces traditional training-based indexing with few-shot prompting using large language models (LLMs). This approach addresses the resource-intensive nature and inflexibility of existing generative retrieval methods that require training models to map queries to document identifiers. By leveraging LLMs for docid generation and retrieval, Few-Shot GR achieves state-of-the-art performance on Natural Questions and MS MARCO benchmarks while offering significant efficiency improvements in indexing.

## Method Summary
Few-Shot GR operates through a two-phase process: indexing and retrieval. During indexing, an LLM generates a docid bank by creating multiple document identifiers for each document using few-shot prompting, eliminating the need for training. For retrieval, the same or compatible LLM generates docids constrained to the docid bank, which are then mapped back to documents. The method employs one-to-many mapping, generating multiple docids per document to improve recall. This approach is particularly advantageous for dynamic corpora as it avoids catastrophic forgetting issues associated with training-based methods.

## Key Results
- Few-Shot GR outperforms state-of-the-art generative retrieval methods in recall and MRR on Natural Questions and MS MARCO benchmarks
- The method demonstrates significant indexing efficiency improvements compared to training-based approaches
- One-to-many mapping strategy is identified as a key factor contributing to improved retrieval performance

## Why This Works (Mechanism)
The approach works by leveraging the few-shot learning capabilities of LLMs to bypass the resource-intensive training required by traditional generative retrieval methods. By generating a docid bank during indexing and constraining retrieval to this bank, the system achieves efficient and accurate document retrieval without the need for model fine-tuning. The one-to-many mapping strategy further enhances performance by increasing the probability of matching generated docids to relevant documents.

## Foundational Learning
- **Generative Retrieval**: Retrieval methods that use models to directly generate document identifiers rather than traditional ranking approaches. Needed to understand the problem domain and why training-free alternatives are valuable.
- **Few-shot Prompting**: Technique where LLMs are given a small number of examples to learn a task without gradient-based training. Quick check: Can the LLM generate consistent docids across multiple runs with the same prompts?
- **Docid Bank**: A collection of document identifiers generated by the LLM during indexing. Quick check: Does the bank size scale linearly with document count and does it maintain retrieval quality?
- **One-to-Many Mapping**: Strategy of generating multiple identifiers per document to increase retrieval recall. Quick check: What is the optimal ratio of docids to documents for balancing performance and storage?

## Architecture Onboarding
**Component Map:** Documents → LLM (Indexing) → Docid Bank → LLM (Retrieval) → Generated Docids → Document Mapping → Retrieved Documents

**Critical Path:** Query → LLM (Retrieval) → Docid Generation → Docid Bank Lookup → Document Retrieval

**Design Tradeoffs:** 
- Few-shot prompting eliminates training overhead but relies heavily on LLM quality
- One-to-many mapping improves recall but increases storage requirements for the docid bank
- Dynamic corpus support without retraining vs. potential limitations in specialized domains

**Failure Signatures:** 
- Poor retrieval performance due to low-quality LLM docid generation
- Increased false positives from overly permissive one-to-many mapping
- Scalability issues with very large docid banks

**First Experiments:**
1. Verify docid generation consistency across multiple runs with identical prompts
2. Test retrieval performance with varying numbers of docids per document
3. Benchmark indexing and retrieval latency on progressively larger corpora

## Open Questions the Paper Calls Out
None

## Limitations
- Heavy reliance on LLM quality and appropriateness for specific domains
- Limited analysis of how different LLM choices affect performance across domains
- Lack of thorough investigation into optimal one-to-many mapping ratios for different corpus characteristics

## Confidence
- **High:** Core technical contribution of using few-shot prompting for docid generation
- **Medium:** Empirical results showing improved recall and MRR on standard benchmarks
- **Medium:** Efficiency claims regarding indexing improvements in practical deployment scenarios

## Next Checks
1. **Cross-domain robustness testing:** Evaluate Few-Shot GR on diverse corpora beyond Natural Questions and MS MARCO, including specialized domains like biomedical or legal documents, to assess generalizability.

2. **Dynamic corpus evaluation:** Conduct longitudinal studies measuring performance as corpora evolve over time, specifically testing for catastrophic forgetting and adaptation speed compared to training-based methods.

3. **Resource consumption analysis:** Perform comprehensive benchmarking of memory usage, indexing time, and retrieval latency across different corpus sizes to validate efficiency claims in practical deployment scenarios.