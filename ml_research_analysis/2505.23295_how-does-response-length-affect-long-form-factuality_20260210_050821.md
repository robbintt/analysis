---
ver: rpa2
title: How Does Response Length Affect Long-Form Factuality
arxiv_id: '2505.23295'
source_url: https://arxiv.org/abs/2505.23295
tags:
- factual
- length
- response
- facts
- factuality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether and how response length affects
  the factuality of long-form text generated by large language models (LLMs). Using
  a newly introduced automatic evaluation framework, Bi-level Atomic Fact Evaluation
  (BAFE), which combines Wikipedia and Google Search verification for improved accuracy
  and efficiency, we conduct controlled experiments on biography and long fact description
  tasks.
---

# How Does Response Length Affect Long-Form Factuality

## Quick Facts
- arXiv ID: 2505.23295
- Source URL: https://arxiv.org/abs/2505.23295
- Reference count: 40
- Longer responses exhibit significantly lower factual precision, confirming the existence of length bias.

## Executive Summary
This study investigates whether and how response length affects the factuality of long-form text generated by large language models (LLMs). Using a newly introduced automatic evaluation framework, Bi-level Atomic Fact Evaluation (BAFE), which combines Wikipedia and Google Search verification for improved accuracy and efficiency, we conduct controlled experiments on biography and long fact description tasks. Our results show that longer responses exhibit significantly lower factual precision, confirming the existence of length bias. To understand the underlying causes, we examine three hypotheses: error propagation, long context, and facts exhaustion. Through empirical analysis, we find that facts exhaustion—where the model gradually exhausts reliable knowledge and begins relying on speculative details—is the primary cause of factual degradation, rather than error propagation or long context. These findings highlight the need for improved long-form factuality evaluation metrics and mitigation strategies for facts exhaustion in LLM-generated text.

## Method Summary
The study introduces BAFE, a two-level verification framework combining Wikipedia and Google Search to evaluate factual precision in long-form text. Responses are decomposed into atomic facts, first verified against Wikipedia, then against Google Search if unsupported. The method is validated against human annotations and used to test length bias across controlled experiments. Main experiments use GPT-4o to generate responses at controlled lengths (100–500 words) for biography and long fact description tasks, measuring factual precision to confirm length bias and test underlying hypotheses.

## Key Results
- Longer responses exhibit significantly lower factual precision, confirming the existence of length bias.
- Facts exhaustion is the primary cause of factual degradation, not error propagation or long context.
- Multiple-topic generation maintains higher factual precision than single-topic generation of equal length.

## Why This Works (Mechanism)

### Mechanism 1: Facts Exhaustion Hypothesis
- **Claim:** Longer responses exhibit lower factual precision because models exhaust reliable knowledge first and then generate speculative details.
- **Mechanism:** The model prioritizes high-confidence knowledge when beginning generation. As output length increases under a single topic constraint, the model must include additional details with progressively lower confidence, increasing hallucination rates.
- **Core assumption:** The model has a finite, confidence-ranked store of factual knowledge per topic, which depletes as generation continues.
- **Evidence anchors:**
  - [abstract] "Our results reveal that facts exhaustion, where the model gradually exhausts more reliable knowledge, is the primary cause of factual degradation, rather than the other two hypotheses."
  - [section 4.3] "Multiple-topic setting consistently leads to higher factual precision... responses in multiple-topic settings consistently have higher factual precision than those in single-topic settings."
  - [corpus] Neighbors like FACTORY address long-form factuality evaluation, but do not directly test facts exhaustion vs. error propagation.

### Mechanism 2: Error Propagation (Rejected)
- **Claim:** Factual errors in early sentences propagate to later sentences, accumulating in longer responses.
- **Mechanism:** Errors in earlier portions of the response influence subsequent generation, creating a compounding effect.
- **Core assumption:** The autoregressive generation process means errors condition later outputs.
- **Evidence anchors:**
  - [abstract] "Our results reveal that facts exhaustion... is the primary cause of factual degradation, rather than the other two hypotheses."
  - [section 4.1] "Error propagation exists but is weak and short-term, with no significant accumulation effect in longer responses."
  - [corpus] FactCorrector focuses on post-hoc correction but does not validate propagation effects.

### Mechanism 3: Long Context (Rejected)
- **Claim:** Longer context length degrades factual precision because models struggle to maintain accuracy over extended sequences.
- **Mechanism:** As generated context grows, the model's conditioning on longer sequences reduces its ability to generate accurate facts.
- **Core assumption:** Context length directly affects the model's capacity to maintain factuality.
- **Evidence anchors:**
  - [abstract] "facts exhaustion... is the primary cause... rather than the other two hypotheses."
  - [section 4.2] "Long context is not the cause of factual degradation... factual precision for Topic B remains stable across all variations in context length."
  - [corpus] "Lost in the Middle" and other corpus papers show context-length effects in retrieval, but the current paper's experiments isolate context length from topic exhaustion.

## Foundational Learning

- **Concept: Atomic Fact Decomposition**
  - **Why needed here:** The evaluation framework (BAFE) and analysis rely on breaking long responses into atomic facts to measure factual precision precisely.
  - **Quick check question:** Can you explain how splitting a paragraph into atomic facts helps isolate factual errors?

- **Concept: Factual Precision Metric**
  - **Why needed here:** The paper uses this metric (percentage of supported facts among all atomic facts) to quantify length bias.
  - **Quick check question:** How does factual precision differ from recall in the context of factuality evaluation?

- **Concept: Autocorrelation Analysis**
  - **Why needed here:** Used to test the error propagation hypothesis by measuring correlation between errors at different positions.
  - **Quick check question:** What would a high autocorrelation coefficient at lag 5 imply about error propagation?

## Architecture Onboarding

- **Component map:**
  Response Decomposition Module -> First-Level Verification Module -> Second-Level Verification Module -> Aggregation & Metric Computation

- **Critical path:**
  1. Long-form response → atomic fact decomposition.
  2. Each fact → Wikipedia verification (first-level).
  3. Unsupported facts → self-contained revision → Google Search verification (second-level).
  4. All verification results → factual precision computation.

- **Design tradeoffs:**
  - Using Wikipedia first reduces cost but may miss facts covered elsewhere.
  - Single Google Search query per fact improves speed but may miss information from multiple queries.
  - Greedy decoding ensures deterministic decomposition but may reduce diversity in atomic fact extraction.

- **Failure signatures:**
  - High false negative rate: Many facts marked unsupported due to Wikipedia coverage gaps (mitigated by second-level).
  - Noisy search results: Google snippets may lack context, leading to misclassification.
  - Ambiguous references: Atomic facts without context may be unverifiable (addressed by self-contained revision).

- **First 3 experiments:**
  1. Validate BAFE vs. human annotations on 786 atomic facts; measure agreement and cost/time vs. FACTSCORE and SAFE.
  2. Generate responses at controlled lengths (100–500 words); measure factual precision to confirm length bias.
  3. Compare single-topic vs. multiple-topic generation at equal total length to test facts exhaustion.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can "facts exhaustion" be effectively mitigated during long-form generation?
- Basis in paper: [explicit] The Conclusion states, "Future work could explore methods to supplement or retrieve deeper factual knowledge in LLMs to improve long-form factual accuracy."
- Why unresolved: This study identifies facts exhaustion as the primary cause of length bias but does not propose or test specific mitigation strategies (such as retrieval-augmented generation) to address the depletion of reliable knowledge.
- What evidence would resolve it: A study demonstrating that specific interventions (e.g., dynamic retrieval) allow models to maintain high factual precision even as response length increases, effectively flattening the negative slope observed in the paper's results.

### Open Question 2
- Question: How can evaluation metrics better balance factual coverage and precision given the existence of length bias?
- Basis in paper: [explicit] The Conclusion notes, "Due to the existence of length bias, we encourage developing more comprehensive metrics that consider both factual coverage and precision."
- Why unresolved: Current metrics like FActScore penalize longer responses for lower precision, potentially discouraging the generation of comprehensive content. A framework that rewards the utility of additional correct information without overly penalizing the inevitable drop in precision is missing.
- What evidence would resolve it: The development and validation of a new metric where a long response with 90% precision but high information utility scores comparably or better than a short response with 99% precision but low utility.

### Open Question 3
- Question: Do the findings regarding facts exhaustion and error propagation generalize to open-source models?
- Basis in paper: [explicit] The Limitations section states, "Our experiments are primarily conducted on GPT-4o... Future work should explore whether our findings hold across a broader range of models, such as open-source LLMs."
- Why unresolved: The study relied on GPT-4o for its strong instruction-following capabilities. It is unclear if smaller or open-source models, which may have different attention mechanisms or knowledge storage, suffer from the same primary cause (exhaustion) or if error propagation/long context issues become more dominant.
- What evidence would resolve it: Replication of the controlled length bias experiments (specifically the single-topic vs. multi-topic analysis) on diverse open-source models (e.g., Llama, Mistral) to see if the "facts exhaustion" hypothesis remains the primary driver of factual degradation.

## Limitations

- The study focuses on English text and two specific tasks (biography and long fact description), which may not generalize to all domains or languages.
- BAFE's reliance on Wikipedia and Google Search may miss facts not covered in these sources, though the second-level verification partially mitigates this.
- The use of greedy decoding may affect the generalizability of results to other decoding strategies.

## Confidence

- **High confidence:** Length bias exists (longer responses have lower factual precision); facts exhaustion hypothesis is supported by multiple-topic comparison.
- **Medium confidence:** Rejection of error propagation and long context as primary causes; BAFE's effectiveness compared to human evaluation.
- **Lower confidence:** Generalizability across all LLM architectures, tasks, and languages; the exact nature of knowledge depletion during generation.

## Next Checks

1. Replicate the length bias experiment across diverse domains (e.g., technical documentation, creative writing) to assess generalizability.
2. Implement BAFE on a different LLM generation pipeline to verify consistency of findings across models.
3. Conduct ablation studies on BAFE components (Wikipedia vs. direct search, decomposition methods) to quantify their individual contributions to evaluation accuracy.