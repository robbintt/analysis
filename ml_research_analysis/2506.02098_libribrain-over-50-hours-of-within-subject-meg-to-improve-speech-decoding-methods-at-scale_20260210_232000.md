---
ver: rpa2
title: 'LibriBrain: Over 50 Hours of Within-Subject MEG to Improve Speech Decoding
  Methods at Scale'
arxiv_id: '2506.02098'
source_url: https://arxiv.org/abs/2506.02098
tags:
- data
- speech
- word
- dataset
- decoding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LibriBrain introduces the largest within-subject MEG dataset for
  speech decoding, containing over 50 hours of recordings from a single participant
  listening to naturalistic English audiobooks. This represents a 5-50x increase in
  depth compared to existing non-invasive speech datasets.
---

# LibriBrain: Over 50 Hours of Within-Subject MEG to Improve Speech Decoding Methods at Scale

## Quick Facts
- arXiv ID: 2506.02098
- Source URL: https://arxiv.org/abs/2506.02098
- Reference count: 40
- Primary result: Over 50 hours of within-subject MEG recordings from single participant listening to English audiobooks, 5-50x larger than existing speech MEG datasets

## Executive Summary
LibriBrain introduces the largest within-subject MEG dataset for speech decoding, containing over 50 hours of recordings from a single participant listening to naturalistic English audiobooks. This represents a 5-50x increase in depth compared to existing non-invasive speech datasets. The dataset includes detailed annotations for speech, words, and phonemes aligned with MEG recordings, along with standard data splits for reproducible machine learning experiments. Baseline experiments demonstrate substantial improvements in decoding performance as training data increases, validating the value of scaling within-subject datasets.

## Method Summary
The dataset was collected using a 306-channel MEG system while a single participant listened to 85 English audiobooks from the LibriVox project. The recordings were synchronized with audio transcripts and aligned at speech, word, and phoneme levels. Data preprocessing included denoising, epoch extraction, and time-frequency analysis. The dataset provides standardized train/validation/test splits and includes a Python library for integration with deep learning frameworks. Baseline experiments tested speech detection, phoneme classification, and word classification tasks using standard neural network architectures.

## Key Results
- Speech detection achieved F1 score of 0.899 with full training data
- Phoneme classification reached micro-F1 of 0.117
- Word classification achieved top-10 accuracy of 0.362
- Performance improved substantially as training data increased, validating dataset scaling benefits

## Why This Works (Mechanism)
The success of LibriBrain stems from the unprecedented scale of within-subject MEG recordings, which captures detailed neural patterns associated with speech processing. The naturalistic listening paradigm using real audiobooks provides ecologically valid stimulation compared to controlled experimental stimuli. The fine-grained temporal alignment between neural signals and speech content enables precise decoding of linguistic information at multiple levels (speech presence, phonemes, words).

## Foundational Learning
- **Within-subject MEG design**: Captures individual neural variability and enables personalized decoding models; verify by comparing across different participants
- **Naturalistic stimulus paradigm**: Uses real audiobooks rather than artificial stimuli to capture ecologically valid speech processing; check by comparing to controlled stimulus results
- **Multi-level annotation alignment**: Aligns neural data with speech, word, and phoneme boundaries for fine-grained decoding; validate by testing decoding accuracy at different linguistic levels
- **Scalable dataset architecture**: Enables progressive model training as more data becomes available; test by measuring performance gains with increasing training samples

## Architecture Onboarding

**Component Map:** Data Collection -> Preprocessing -> Annotation Alignment -> Dataset Split -> Baseline Model Training -> Performance Evaluation

**Critical Path:** The MEG recording quality and annotation accuracy are critical, as poor signal-to-noise ratio or misaligned annotations directly impact decoding performance. The dataset split strategy is also crucial for fair evaluation.

**Design Tradeoffs:** Single participant design enables deep within-subject analysis but limits generalizability; naturalistic stimuli provide ecological validity but introduce variability; multi-level annotations increase utility but require complex alignment.

**Failure Signatures:** Poor MEG signal quality manifests as reduced decoding accuracy across all tasks; annotation errors cause specific drops in phoneme/word classification; insufficient training data leads to overfitting on small subsets.

**First Experiments:**
1. Train speech detection model using 10% of available data
2. Evaluate phoneme classification performance across different frequency bands
3. Test word classification accuracy using different sequence lengths

## Open Questions the Paper Calls Out
None

## Limitations
- Single-participant design limits generalizability across individuals
- Focus on English audiobooks constrains applicability to other languages
- Standard architectures used rather than optimized models for this specific dataset
- Real-time decoding applications remain untested

## Confidence
- Dataset scale and characteristics: High
- Baseline performance improvements: Medium
- Generalizability to other populations: Low
- Real-world applicability: Low

## Next Checks
1. Test model performance across multiple participants to assess individual variability and generalizability
2. Evaluate decoding accuracy for non-English speech and different acoustic environments
3. Implement and benchmark real-time decoding pipelines to assess practical deployment feasibility