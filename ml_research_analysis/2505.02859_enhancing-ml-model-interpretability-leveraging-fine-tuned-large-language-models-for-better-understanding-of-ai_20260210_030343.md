---
ver: rpa2
title: 'Enhancing ML Model Interpretability: Leveraging Fine-Tuned Large Language
  Models for Better Understanding of AI'
arxiv_id: '2505.02859'
source_url: https://arxiv.org/abs/2505.02859
tags:
- evaluation
- artifact
- chatbot
- shap
- research
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel reference architecture for improving
  machine learning model interpretability by integrating fine-tuned large language
  models (LLMs) with explainable AI (XAI) techniques. The approach leverages a fine-tuned
  LLM to provide interactive, context-aware explanations of SHAP values, making complex
  model predictions more understandable to users.
---

# Enhancing ML Model Interpretability: Leveraging Fine-Tuned Large Language Models for Better Understanding of AI

## Quick Facts
- **arXiv ID:** 2505.02859
- **Source URL:** https://arxiv.org/abs/2505.02859
- **Reference count:** 7
- **Primary result:** A three-step fine-tuning approach (domain-specific, global explanation, human alignment) improves LLM ability to explain SHAP values in natural language, enhancing ML interpretability across expertise levels.

## Executive Summary
This paper presents a reference architecture that integrates fine-tuned large language models with explainable AI (XAI) techniques to enhance machine learning interpretability. The approach uses a three-step fine-tuning process to train LLMs to provide interactive, context-aware explanations of SHAP values, making complex model predictions more accessible to users. The solution was instantiated in a battery state-of-health prediction use case and evaluated through expert interviews and a user survey (n=61). Results demonstrate significant improvements in interpretability and clarity, particularly for users with less XAI expertise.

## Method Summary
The methodology employs Llama-2-13b-chat-hf as a base model, fine-tuned in three sequential steps using LoRA via Oobabooga text-generation-webui: (1) in-domain unsupervised fine-tuning on domain-specific documents, (2) global explanation unsupervised fine-tuning on SHAP analysis summaries, and (3) human alignment supervised fine-tuning on structured Q&A JSON prompts. The fine-tuned LLM is integrated with a Gradio chatbot interface to provide natural language explanations of SHAP values alongside visualizations. Evaluation metrics include perplexity for unsupervised steps, loss for supervised alignment, and user survey ratings for interpretability, clarity, and cognitive effort.

## Key Results
- The three-step fine-tuning approach significantly improved LLM performance in explaining SHAP values across all evaluated metrics
- User studies showed the solution significantly improved interpretability and clarity, especially for users with less XAI expertise
- The battery state-of-health use case demonstrated practical applicability of the reference architecture

## Why This Works (Mechanism)
The approach works by combining domain expertise, explanation quality, and human alignment through progressive fine-tuning. Domain-specific fine-tuning ensures the LLM understands technical terminology, global explanation fine-tuning teaches it to summarize SHAP values effectively, and human alignment fine-tuning ensures explanations match user expectations and communication patterns. This staged approach addresses the gap between raw SHAP values and actionable insights that users can understand and trust.

## Foundational Learning
1. **SHAP (SHapley Additive exPlanations)** - A game theory-based method for explaining individual predictions of any machine learning model; needed to understand the core XAI technique being explained
2. **LoRA (Low-Rank Adaptation)** - A parameter-efficient fine-tuning method that freezes original model weights and injects trainable low-rank matrices; needed for efficient LLM adaptation
3. **Alpaca format** - A structured JSON format for instruction-tuning datasets; needed for organizing human alignment training data
4. **Oobabooga text-generation-webui** - A user interface for running and fine-tuning LLMs locally; needed for the fine-tuning implementation
5. **Gradio** - An open-source platform for building ML web applications; needed for creating the interactive chatbot interface
6. **CatBoost** - A gradient boosting library that provides SHAP values out-of-the-box; needed for the battery SoH prediction model

Quick check: Verify you can generate SHAP values from your ML model and structure them for LLM explanation

## Architecture Onboarding

**Component map:** User -> Gradio Chatbot -> Fine-tuned LLM -> SHAP Values from ML Model

**Critical path:** Model prediction → SHAP value extraction → LLM explanation generation → User interface delivery

**Design tradeoffs:** The architecture prioritizes interpretability over raw performance, accepting the computational overhead of LLM fine-tuning and inference to achieve better user understanding. The three-step fine-tuning balances computational efficiency with explanation quality.

**Failure signatures:** Hallucinations on domain-specific terms indicate insufficient domain-specific fine-tuning; minimal perplexity improvement on global explanation step is expected and normal; high human alignment loss suggests misalignment with user communication preferences.

**First experiments:**
1. Test the fine-tuned LLM on held-out Q&A pairs to evaluate explanation quality
2. Compare user comprehension with and without LLM explanations in a controlled study
3. Evaluate hallucination rates on domain-specific terminology through automated testing

## Open Questions the Paper Calls Out
None

## Limitations
- The exact content of domain-specific training documents remains unspecified, limiting precise replication
- The specific JSON prompt structures for human alignment fine-tuning lack sufficient detail for faithful reproduction
- The evaluation relied on expert interviews and an online survey (n=61), which may not capture all edge cases or long-term model behavior

## Confidence
- **High confidence**: The reference architecture design and three-step fine-tuning methodology are clearly specified and technically sound
- **Medium confidence**: The performance improvements in interpretability and clarity are well-supported by user studies, though the specific contribution of each fine-tuning step could be more precisely quantified
- **Medium confidence**: The battery SoH use case demonstrates practical applicability, but generalizability to other domains requires further validation

## Next Checks
1. Conduct ablation studies to quantify the individual contribution of each fine-tuning step to overall interpretability improvements
2. Test the architecture with alternative XAI methods (beyond SHAP) to assess method-agnostic applicability
3. Evaluate long-term model performance and hallucination rates through extended user testing across multiple sessions