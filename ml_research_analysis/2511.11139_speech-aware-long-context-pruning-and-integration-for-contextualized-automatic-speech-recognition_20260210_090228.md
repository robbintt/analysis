---
ver: rpa2
title: Speech-Aware Long Context Pruning and Integration for Contextualized Automatic
  Speech Recognition
arxiv_id: '2511.11139'
source_url: https://arxiv.org/abs/2511.11139
tags:
- speech
- keywords
- contextual
- context
- recognition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of leveraging long-context information
  in contextualized automatic speech recognition (ASR), particularly in scenarios
  like conference presentations where extensive OCR-derived contextual keywords are
  available but contain significant noise. The core method, SAP2 (Speech-Aware Context
  Pruning with Speech-Driven Attention-based Pooling), is a two-stage framework that
  dynamically prunes irrelevant contextual keywords using a SpeechLLM and integrates
  them into ASR systems.
---

# Speech-Aware Long Context Pruning and Integration for Contextualized Automatic Speech Recognition

## Quick Facts
- arXiv ID: 2511.11139
- Source URL: https://arxiv.org/abs/2511.11139
- Reference count: 25
- Primary result: Achieves 7.71% WER on SlideSpeech and 1.12% WER on LibriSpeech using two-stage pruning and integration framework

## Executive Summary
This paper addresses the challenge of leveraging long-context information in contextualized automatic speech recognition (ASR), particularly in scenarios like conference presentations where extensive OCR-derived contextual keywords are available but contain significant noise. The core method, SAP2 (Speech-Aware Context Pruning with Speech-Driven Attention-based Pooling), is a two-stage framework that dynamically prunes irrelevant contextual keywords using a SpeechLLM and integrates them into ASR systems. The key innovation is the Speech-Driven Attention-based Pooling mechanism, which compresses extensive textual inputs into concise, speech-relevant context embeddings by weighting contextual keywords with speech attention scores. Experimental results demonstrate state-of-the-art performance on the SlideSpeech and LibriSpeech datasets.

## Method Summary
SAP2 is a two-stage contextualized ASR framework that first prunes long contextual keyword lists using a SpeechLLM, then integrates the pruned keywords for recognition. The pruning stage uses a speech-driven attention mechanism to compress text embeddings based on their relevance to speech content, producing a condensed context representation. The recognition stage performs ASR conditioned on this pruned context. The framework employs LoRA-based fine-tuning with a frozen speech encoder for efficient adaptation, updating only the LLM backbone and multimodal projector while retaining robust acoustic representations.

## Key Results
- Achieves 7.71% WER on SlideSpeech and 1.12% WER on LibriSpeech test-clean
- Reduces biased keyword error rates (B-WER) by 41.1% compared to non-contextual baselines on SlideSpeech
- Maintains consistent performance under extensive contextual input conditions (5-slide contexts)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Two-stage pruning-integration improves contextual ASR by explicitly separating keyword selection from recognition, reducing noise.
- Mechanism: A SpeechLLM first prunes OCR-derived keywords using a cross-modal attention pool (speech-driven), producing a compressed subset. A second model then performs ASR conditioned on this clean subset. This decoupling prevents noisy long-context from overwhelming the decoder.
- Core assumption: Relevant keywords are sparse in long context; explicit selection before decoding is more effective than joint selection-and-recognition.

### Mechanism 2
- Claim: Speech-Driven Attention-based Pooling compresses context embeddings while preserving speech-relevant information via cross-modal attention weights.
- Mechanism: The audio encoder produces speech embeddings hx. Attention scores between hx and text embeddings hz are computed; these scores weight tokens within fixed-size windows, and weighted averages are pooled to reduce sequence length. This keeps tokens relevant to speech and discards others.
- Core assumption: Cross-modal attention captures semantic alignment between speech content and textual context.

### Mechanism 3
- Claim: LoRA-based fine-tuning with a frozen speech encoder efficiently adapts a large SpeechLLM for contextual ASR without full retraining.
- Mechanism: Only the LLM backbone and multimodal projector are updated via low-rank adapters; the pre-trained speech encoder remains frozen. This retains robust acoustic representations while learning context-integration and pruning.
- Core assumption: Pre-trained speech encoder already generalizes well; only cross-modal integration and pruning need adaptation.

## Foundational Learning

- Concept: Cross-modal Attention
  - Why needed here: Enables the model to compute relevance scores between audio and text, which underpins both pruning and pooling.
  - Quick check question: Given audio features hx and text features hz, how would you compute a simple relevance score matrix (T×C)? (Answer: Dot-product of hx and hz transposed, optionally scaled and softmaxed.)

- Concept: Sequence Compression via Pooling
  - Why needed here: Reduces the length of textual context to fit within context windows while preserving information.
  - Quick check question: If you have a sequence of embeddings and you want to reduce it by a factor of 2, how could you use learned attention weights over windows of size 2? (Answer: Group into windows, compute weights (e.g., softmax over window), take weighted sum.)

- Concept: LoRA (Low-Rank Adaptation)
  - Why needed here: Efficiently fine-tunes large models by adding trainable low-rank matrices to weight matrices, reducing memory and compute.
  - Quick check question: If a weight matrix W is d×d, how many parameters does a LoRA adapter of rank r add (two matrices A and B)? (Answer: 2×d×r, as A is d×r and B is r×d.)

## Architecture Onboarding

- Component map:
  - Audio Encoder (frozen, pre-trained) → Speech Embeddings (hx)
  - Context Keywords (text) → LLM Embedding Layer → Context Embeddings (hz)
  - Speech-Driven Attention-based Pooling Module → Compressed Context Embeddings (ĥz)
  - Two-stage pipeline:
    - Stage 1 (Pruning Model): Takes hx, hz (full context), produces pruned keywords via the pooling mechanism and LLM generation
    - Stage 2 (Recognition Model): Takes hx, ĥz (pruned), generates transcription
  - LoRA adapters applied to multimodal projector and LLM backbone

- Critical path:
  1. Encode audio → hx
  2. Embed full context keywords → hz
  3. Apply attention pooling (compute cross-modal attention, pool windows) → compressed ĥz
  4. Stage 1 LLM generates pruned keyword list
  5. Stage 2 LLM conditions on audio and pruned keywords to output transcription

- Design tradeoffs:
  - Pooling window size: Larger → more compression but risk of information loss; smaller → better fidelity but less compression
  - Two-stage vs. joint (JPI): Two-stage separates objectives (cleaner) but adds inference overhead; joint reduces overhead but may suffer from objective interference
  - LoRA rank: Higher rank → more capacity but more parameters; lower rank → more efficient but may underfit

- Failure signatures:
  - High U-WER but low B-WER: Base ASR weak; consider updating audio encoder or using stronger base
  - High B-WER with long context: Pruning ineffective; check attention scores, pooling window, or stage 1 training
  - Slow inference: Context length still too long; increase pooling window size or check pruning efficiency

- First 3 experiments:
  1. Ablate pooling: Train with and without speech-driven attention-based pooling on 5-slide contexts; compare WER/B-WER and inference time
  2. Vary pooling window: Train with window sizes 2, 4, 8 on L95 5-slide; plot WER vs. window size to find sweet spot
  3. Compare pipelines: Train SAP2-TPI, SAP2-PC (prompt concatenation), and SAP2-JPI (joint) on same data; compare WER, B-WER, and F1-score of pruned keywords to validate two-stage separation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does utilizing semantically meaningful phrasal contexts instead of single-word units improve the SAP2 framework's ability to capture contextual relevance and reduce noise during the pruning stage?
- Basis in paper: The conclusion states, "Because of the limitations... we only use contexts splitted as single words, future directions will explore semantically meaningful phrasal contexts."
- Why unresolved: The current implementation splits contexts into single words (tokens), potentially losing semantic dependencies between words that could help distinguish relevant information from noise.
- What evidence would resolve it: Comparative experiments evaluating B-WER and WER on SlideSpeech using phrase-based context inputs versus the current single-word tokenization.

### Open Question 2
- Question: Can the integration of direct visual embeddings from slides, combined with OCR text, further improve contextualized ASR performance beyond the text-only approach?
- Basis in paper: The conclusion lists "multimodal fusion with visual embeddings" as a specific future direction for the visual aids utilized in conference scenarios.
- Why unresolved: The current method relies solely on OCR-derived text embeddings, ignoring the visual layout and non-textual graphical information present in slides.
- What evidence would resolve it: A comparison of standard SAP2 versus a multimodal variant that encodes slide images directly into the context integration mechanism.

### Open Question 3
- Question: Can an end-to-end differentiable training strategy effectively mitigate the interference issues observed in the Joint Pruning-Integration (JPI) ablation study?
- Basis in paper: In Section 3.5, the authors note that JPI underperformed TPI due to "interference between dual objectives," leaving the potential for a more tightly coupled optimization approach unexplored.
- Why unresolved: The paper only evaluated a sequential auto-regressive joint model (JPI), not mechanisms designed to optimize the pruning decision directly via the downstream ASR loss.
- What evidence would resolve it: Experiments comparing the two-stage TPI pipeline against a model where the pruning mechanism is trained using gradients propagated directly from the final speech recognition loss.

## Limitations

- Generalization beyond conference presentations: All experiments use SlideSpeech (conference presentations with OCR-derived context). Performance on other long-context domains (e.g., medical dictations, broadcast news with metadata) remains unknown.
- Pruning quality validation: While B-WER improvements are reported, the paper doesn't provide detailed analysis of pruning accuracy (precision/recall of selected keywords) or ablation studies showing how pruning quality correlates with downstream ASR performance.
- Computational overhead of two-stage approach: The paper reports strong results but doesn't provide comprehensive latency comparisons between two-stage and potential joint approaches, particularly for real-time applications.

## Confidence

**High Confidence**: Core methodology (two-stage pruning-integration framework, speech-driven attention-based pooling, LoRA fine-tuning) is well-specified and produces measurable performance improvements on benchmark datasets.

**Medium Confidence**: Claims about efficiency gains from freezing speech encoder and using LoRA adapters are supported by parameter counts but lack runtime performance comparisons.

**Low Confidence**: Claims about robustness to extensive contextual inputs (maintaining performance under various context lengths) are supported by limited experiments with 5-slide contexts and require validation on more diverse context lengths and structures.

## Next Checks

1. **Pruning quality ablation**: Measure and report precision, recall, and F1-score of the pruned keyword set compared to ground truth across different context lengths (1-slide, 3-slide, 5-slide, 10-slide). Correlate pruning quality metrics with downstream WER/B-WER improvements to establish causal relationship.

2. **Context structure robustness**: Evaluate SAP2 performance when context comes from non-consecutive sources (e.g., randomly sampled slides from throughout the presentation, external knowledge bases, or multimodal sources like images with captions). Compare against the current consecutive-slide clustering approach.

3. **Two-stage vs. joint efficiency**: Implement and evaluate a SAP2-JPI (Joint Pruning-Integration) variant that performs pruning and recognition in a single forward pass. Measure both WER/B-WER performance and inference latency to quantify the computational tradeoff of the two-stage approach.