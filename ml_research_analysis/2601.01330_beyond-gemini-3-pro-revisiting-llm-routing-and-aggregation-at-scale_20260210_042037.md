---
ver: rpa2
title: 'Beyond Gemini-3-Pro: Revisiting LLM Routing and Aggregation at Scale'
arxiv_id: '2601.01330'
source_url: https://arxiv.org/abs/2601.01330
tags:
- llms
- aggregation
- routing
- jisi
- chen
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the problem of surpassing state-of-the-art
  closed-source LLMs like Gemini-3-Pro through collaborative multi-agent systems.
  It identifies three key bottlenecks in current LLM routing and aggregation: limited
  query-based routing, static aggregation, and underutilization of routing-aggregation
  complementarity.'
---

# Beyond Gemini-3-Pro: Revisiting LLM Routing and Aggregation at Scale

## Quick Facts
- **arXiv ID**: 2601.01330
- **Source URL**: https://arxiv.org/abs/2601.01330
- **Reference count**: 18
- **Primary result**: Surpasses Gemini-3-Pro by 1.15% while reducing costs by 53.23% using a training-free multi-agent framework

## Executive Summary
This paper addresses the challenge of surpassing state-of-the-art closed-source LLMs like Gemini-3-Pro through collaborative multi-agent systems. The authors identify three key bottlenecks in current LLM routing and aggregation: limited query-based routing, static aggregation, and underutilization of routing-aggregation complementarity. They propose JiSi, a training-free framework that introduces query-response mixed routing, support-set-based aggregator selection, and an adaptive routing-aggregation switch. Evaluated on nine benchmarks with ten open-source LLMs, JiSi demonstrates that collective intelligence offers a scalable and efficient alternative path to achieving AGI-level performance.

## Method Summary
JiSi is a training-free framework that combines query-response mixed routing, support-set-based aggregator selection, and an adaptive routing-aggregation switch. The system uses an embedding bank containing precomputed query embeddings, response embeddings, token costs, and correctness labels for multiple LLMs. For each query, it retrieves a support set of similar queries, selects an appropriate aggregator based on capability vectors, routes to top candidates, and conditionally aggregates or returns the top response based on a quality threshold. The framework operates without training, relying on similarity-based retrieval and composite scoring mechanisms to achieve superior performance compared to individual models.

## Key Results
- Outperforms Gemini-3-Pro by 1.15% average accuracy across nine benchmarks
- Achieves 53.23% cost reduction compared to using Gemini-3-Pro
- Demonstrates effectiveness across diverse domains including reasoning, coding, math, and general knowledge
- Validates the scaling law of multi-agent collaboration with 10 open-source LLMs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Query-response mixed routing captures deep semantics and task difficulty beyond shallow text similarity.
- **Mechanism**: Projects both query and LLM-generated responses into embedding space, computing weighted composite scores combining query similarity, response embedding similarity, and reasoning cost similarity (token count as difficulty proxy). This distinguishes semantically similar but difficulty-disparate queries.
- **Core assumption**: LLM response length and embedding characteristics correlate meaningfully with query difficulty.
- **Evidence anchors**: Abstract states "Query-Response Mixed Routing capturing both semantic information and problem difficulty"; Section 3 demonstrates with example distinguishing "Prove every large even number is a sum of two primes" vs "two odds"; corpus work "Beyond Majority Voting" confirms treating model outputs uniformly ignores latent heterogeneity.
- **Break condition**: If response length no longer correlates with difficulty (e.g., models trained to emit terse answers for hard problems), the difficulty signal degrades.

### Mechanism 2
- **Claim**: Support-set-based aggregator selection dynamically balances domain expertise against aggregation ability.
- **Mechanism**: Retrieves support set of similar queries from embedding bank using cosine similarity, computes coarse-grained model scores from capability vectors, and selects aggregator with highest weighted score. Ensures aggregator has both task-relevant domain knowledge and proven synthesis skill.
- **Core assumption**: An LLM's aggregation proficiency correlates with performance on representative support set, and domain-specific strength is partially transferable via similarity retrieval.
- **Evidence anchors**: Abstract states "Support-Set-based Aggregator Selection jointly evaluating the aggregation and domain capacity of aggregators"; Section 3.3 defines support set selection via similarity threshold and aggregator selection from capability-weighted scores; corpus "Mixture of Thoughts" argues for aggregating what experts think beyond what they say.
- **Break condition**: If support set queries are unrepresentative of test distribution or capability vectors are stale, aggregator selection may misfire.

### Mechanism 3
- **Claim**: Adaptive routing-aggregation switch filters noise and improves efficiency by conditionally bypassing aggregation.
- **Mechanism**: Computes fine-grained prior scores for candidate responses, applies threshold filtering: if only one response exceeds threshold, returns it directly (pure routing); if multiple exceed, aggregates. Prevents low-quality responses from polluting aggregation context and saves compute when routing suffices.
- **Core assumption**: Prior scores derived from embedding bank reliably predict response quality; threshold can be set globally or per-domain.
- **Evidence anchors**: Abstract states "Adaptive Routing-Aggregation Switch dynamically leveraging the advantages of routing and aggregation"; Section 3.3 formalizes threshold-based filtering and fallback to top-1 routing; weak direct corpus evidence on adaptive switching specifically.
- **Break condition**: If threshold is poorly calibrated (too high → excessive fallback; too low → noise inclusion), performance degrades.

## Foundational Learning

- **Embedding-based similarity retrieval**: Core to constructing support sets and routing queries to models via cosine similarity in latent space. *Quick check*: Given a query embedding, can you compute its similarity to 100 stored embeddings and return the top-k?

- **Capability vectors / performance profiling**: The embedding bank stores per-LLM correctness vectors over benchmark queries to inform aggregator and model selection. *Quick check*: How would you represent an LLM's accuracy across 1,000 questions as a compact signal for routing?

- **Training-free vs. learned routing**: JiSi explicitly avoids training, relying on pre-built banks and similarity heuristics; understanding this constraint clarifies design choices. *Quick check*: What are the tradeoffs between training a router (e.g., RouterDC) versus using retrieval-based prior scores?

## Architecture Onboarding

- **Component map**: Embedding Bank -> Support-Set Selector -> Aggregator Selector -> Query-Response Router -> Adaptive Switch

- **Critical path**: 1) New query → embed → retrieve support set → select aggregator; 2) Route query to top-K candidate LLMs → collect responses + embeddings + costs; 3) Compute filtered fine-grained scores → apply threshold → aggregate or route

- **Design tradeoffs**: Larger embedding bank → better coverage but higher memory and preprocessing cost; Higher K → richer aggregation context but more API calls and latency; Higher threshold → cleaner aggregation inputs but more fallback to routing (potentially lower ceiling)

- **Failure signatures**: Degradation to single-model routing (check selection distribution entropy); Excessive aggregation cost (monitor frequency of multi-response aggregation vs. fallback); Stale capability vectors (if LLMs are updated but bank isn't, scores misalign)

- **First 3 experiments**:
  1. **Ablate response embeddings**: Run routing with only query embeddings vs. full query-response mixed; expect performance drop on difficulty-sensitive benchmarks (AIME, HLE)
  2. **Vary threshold t**: Sweep threshold (e.g., 0.6, 0.7, 0.8, 0.9) and measure performance vs. cost tradeoff; identify sweet spot
  3. **Scaling test**: Start with 5 LLMs in pool, incrementally add to 10, plot performance curve per domain (reasoning, coding, chat) to verify scaling behavior

## Open Questions the Paper Calls Out

- **Open Question 1**: Can the JiSi framework be effectively adapted for multi-turn, interactive agentic workflows without relying on the single-turn context packing currently required for tasks like SWE-bench? The paper acknowledges aggregation methods are hard to conduct multi-turn interaction and packs all context in a single query for SWE-bench, but the mechanism for maintaining context across multiple conversational turns remains undefined.

- **Open Question 2**: Does the observed "multi-agent collaboration scaling law" persist, or does performance saturate, when the candidate pool expands significantly (e.g., >50 or >100 LLMs) with higher variance in model quality? The paper demonstrates scaling from 5 to 10 LLMs but empirical evidence is limited to a small pool of 10 high-performance models.

- **Open Question 3**: How robust is the "Query-Response Mixed Router" to failures or biases in the underlying embedding model used to construct the bank? The framework relies entirely on `gte-Qwen2-7B-instruct` for its embedding bank, but the paper doesn't ablate the impact of this specific choice on routing accuracy.

## Limitations

- **Dataset representation uncertainty**: Heavy reliance on OpenRouterBench dataset without clear specification of how representative it is across the nine diverse benchmarks tested.

- **Inconsistent correctness evaluation**: Capability vectors constructed from correctness labels vary across benchmark types (different judge models used), introducing potential inconsistency in quality signals.

- **Single-turn context constraint**: Framework validated on single-turn responses; mechanism for maintaining context across multiple conversational turns remains undefined.

## Confidence

- **High Confidence**: Claim that JiSi outperforms Gemini-3-Pro by 1.15% on average across nine benchmarks, given explicit experimental setup and measurable cost reduction of 53.23%.
- **Medium Confidence**: Generalizability to other domains or larger LLM pools; while effectiveness with ten specific open-source models demonstrated, scaling behavior and performance with different model combinations remains untested.
- **Low Confidence**: Robustness of adaptive threshold mechanism; paper lacks sensitivity analysis showing how threshold tuning affects performance across different benchmark types.

## Next Checks

1. **Dataset Representation Test**: Evaluate JiSi on a held-out benchmark not used in training the embedding bank to assess true generalization versus memorization of benchmark-specific patterns.

2. **Threshold Sensitivity Analysis**: Systematically sweep the adaptive threshold parameter t across its plausible range (0.6-0.95) and measure performance variance to identify optimal values per benchmark type.

3. **Capability Vector Consistency Check**: Compare routing performance when using different judge models for correctness evaluation across benchmarks to quantify the impact of inconsistent quality signals on routing decisions.