---
ver: rpa2
title: Your Latent Reasoning is Secretly Policy Improvement Operator
arxiv_id: '2511.16886'
source_url: https://arxiv.org/abs/2511.16886
tags:
- policy
- improvement
- reasoning
- step
- latent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Deep Improvement Supervision (DIS), a training
  method for Tiny Recursive Models (TRMs) that provides step-wise intermediate targets
  during recursion. DIS uses a monotonic discrete corruption schedule of the ground-truth
  output, turning each recursion step into a supervised sub-goal.
---

# Your Latent Reasoning is Secretly Policy Improvement Operator

## Quick Facts
- arXiv ID: 2511.16886
- Source URL: https://arxiv.org/abs/2511.16886
- Authors: Arip Asadulaev; Rayan Banerjee; Fakhri Karray; Martin Takac
- Reference count: 17
- Primary result: DIS achieves 24% accuracy on ARC-AGI-1 with 0.8M parameters, outperforming most open-source LLMs without external knowledge

## Executive Summary
This paper introduces Deep Improvement Supervision (DIS), a training method for Tiny Recursive Models (TRMs) that provides intermediate supervision during recursive reasoning steps. The key innovation is using a monotonic discrete corruption schedule that progressively corrupts the ground-truth output, turning each recursion step into a supervised sub-goal. This addresses the dead compute problem in TRMs where intermediate steps may not contribute meaningfully to the final output. DIS achieves competitive performance on complex reasoning benchmarks including ARC-AGI 1 and 2 using a simpler architecture than traditional TRMs, while significantly reducing computational overhead through 18x fewer forward passes.

## Method Summary
DIS transforms the recursive reasoning process by providing step-wise intermediate targets through monotonic discrete corruption of ground-truth outputs. Each recursion step receives a progressively more corrupted version of the final answer, effectively creating a curriculum where the model learns to improve upon increasingly degraded solutions. This supervision scheme ensures that every recursive step contributes to the final output, eliminating "dead compute" steps. The method avoids training a halting step module by using a fixed recursion depth, reduces supervision steps by 3x, and requires 8x fewer latent reasoning steps compared to traditional TRM approaches while maintaining or improving performance on reasoning benchmarks.

## Key Results
- Achieves 24% accuracy on ARC-AGI-1 with only 0.8 million parameters
- Reduces total forward passes by 18x while maintaining competitive performance
- Outperforms most open-source LLM models on ARC-AGI-1 without external knowledge
- Requires 3x fewer supervision steps and 8x fewer latent reasoning steps compared to TRM

## Why This Works (Mechanism)
DIS works by converting the recursive reasoning process into a series of supervised improvement steps. The monotonic discrete corruption schedule provides a natural curriculum where each step learns to correct errors from the previous step, creating a policy improvement operator that gradually refines the solution. This approach ensures that every recursive computation contributes meaningfully to the final output, addressing the fundamental inefficiency of traditional TRMs where some steps may be redundant or counterproductive.

## Foundational Learning
- **Recursive reasoning**: Understanding how models can break down complex problems into sequential sub-steps; needed to grasp TRM architecture and why dead compute steps occur; quick check: can you explain how a recursive model differs from a standard feed-forward model?
- **Curriculum learning**: The monotonic corruption schedule creates a learning progression; needed to understand how DIS provides increasingly challenging targets; quick check: can you describe how the corruption schedule changes across recursion steps?
- **Policy improvement operators**: Each recursive step improves upon the previous solution; needed to understand the theoretical foundation of DIS; quick check: can you explain how each step functions as a policy improvement?
- **Supervision scheduling**: Step-wise intermediate targets during recursion; needed to understand how DIS differs from standard end-to-end supervision; quick check: can you contrast DIS supervision with traditional recursive model training?
- **ARC-AGI benchmarks**: Visual reasoning tasks requiring abstract pattern recognition; needed to contextualize the evaluation metrics; quick check: can you describe what makes ARC-AGI problems challenging for AI models?
- **Parameter efficiency**: Achieving high performance with small model sizes; needed to appreciate DIS's contribution to efficient reasoning; quick check: can you compare the parameter counts of DIS models to typical transformer baselines?

## Architecture Onboarding

**Component Map**
Input -> Recursive Cell -> Output Refinement -> Next Recursion State -> ... (repeated for fixed depth) -> Final Output

**Critical Path**
The critical path is the recursive chain where each step receives a corrupted ground truth target and must produce an output that improves upon it. The monotonic corruption schedule ensures that each step has a clear sub-goal, making the intermediate reasoning steps observable and trainable.

**Design Tradeoffs**
Fixed recursion depth vs. adaptive halting: DIS uses fixed depth to avoid training a halting module, trading potential efficiency for training simplicity and guaranteed computation. Simpler architecture vs. TRM complexity: DIS achieves competitive results with reduced architectural overhead, but may miss some benefits of more sophisticated TRM designs.

**Failure Signatures**
If DIS underperforms, failure modes include: (1) corruption schedule too aggressive, preventing meaningful learning at early steps; (2) recursion depth insufficient for complex problems; (3) the monotonic assumption breaks down for certain problem types where non-monotonic reasoning paths are optimal.

**3 First Experiments**
1. Vary corruption schedule parameters (initial corruption level, decay rate) to find optimal settings for different reasoning domains
2. Test fixed vs. adaptive recursion depth to quantify the efficiency tradeoff
3. Ablation study removing intermediate supervision to isolate the benefit of DIS's step-wise targets

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation relies on a limited set of reasoning benchmarks that may not fully capture general reasoning capabilities
- The 24% accuracy claim on ARC-AGI-1 requires careful interpretation given the challenging generalization requirements
- Monotonic corruption schedule may not generalize to problems requiring non-monotonic reasoning paths
- Claims about competitive performance relative to larger models need independent replication

## Confidence

- **High confidence**: Architectural description of DIS and TRMs is clear and reproducible; improvement in forward pass efficiency (18x reduction) is well-supported; comparison against open-source LLM baselines is appropriately scoped
- **Medium confidence**: Claims about competitive performance require independent replication, particularly ARC-AGI-2 results; monotonic corruption schedule's effectiveness needs validation on diverse problem domains
- **Low confidence**: Addressing dead compute steps may not generalize to all TRM architectures; the efficiency metrics (3x fewer supervision steps, 8x fewer reasoning steps) should be verified across diverse domains

## Next Checks
1. **Cross-domain generalization test**: Evaluate DIS-TRM on at least two reasoning domains not included in training set (symbolic reasoning, logical puzzles) to assess monotonic corruption schedule generalization
2. **Scaling behavior analysis**: Systematically evaluate performance scaling with model size (0.8M, 8M, 80M parameters) and recursion depth to identify bottlenecks
3. **Error analysis and failure modes**: Conduct detailed error analysis categorizing failure types on ARC-AGI-2 to determine whether DIS addresses correct problem (computation vs. reasoning strategy)