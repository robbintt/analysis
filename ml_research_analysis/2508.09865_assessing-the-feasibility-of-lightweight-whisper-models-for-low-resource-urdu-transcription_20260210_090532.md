---
ver: rpa2
title: Assessing the Feasibility of Lightweight Whisper Models for Low-Resource Urdu
  Transcription
arxiv_id: '2508.09865'
source_url: https://arxiv.org/abs/2508.09865
tags:
- urdu
- speech
- whisper
- error
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study benchmarks lightweight Whisper models (Tiny, Base, Small)
  for Urdu speech recognition in low-resource settings without fine-tuning. Ten native
  Urdu speakers provided 36 audio samples, transcribed using pre-trained models and
  evaluated with Word Error Rate (WER).
---

# Assessing the Feasibility of Lightweight Whisper Models for Low-Resource Urdu Transcription

## Quick Facts
- **arXiv ID**: 2508.09865
- **Source URL**: https://arxiv.org/abs/2508.09865
- **Reference count**: 19
- **Primary result**: Whisper-Small achieved lowest WER (33.68%) among pre-trained Whisper variants for Urdu ASR without fine-tuning

## Executive Summary
This study benchmarks three lightweight Whisper models (Tiny, Base, Small) for Urdu speech recognition in low-resource settings without any fine-tuning. Ten native Urdu speakers provided 36 audio samples that were transcribed using pre-trained models and evaluated with Word Error Rate (WER). Whisper-Small achieved the lowest WER (33.68%), outperforming Base (53.67%) and Tiny (67.08%). The qualitative analysis revealed distinct performance patterns across model sizes, with Tiny producing unintelligible outputs, Base showing moderate performance with lexical distortions, and Small preserving sentence structure despite phonetic and substitution errors. While Whisper-Small emerges as the most promising lightweight option, significant improvements are needed for practical deployment.

## Method Summary
The study evaluated pre-trained Whisper models (Tiny, Base, Small) on Urdu speech recognition without fine-tuning. Ten native Urdu speakers provided 36 audio samples covering diverse content. Each model transcribed the audio, and transcriptions were evaluated using Word Error Rate (WER) as the primary metric. The study included both quantitative WER analysis and qualitative assessment of transcription quality across different model sizes.

## Key Results
- Whisper-Small achieved the lowest WER (33.68%) among all tested models
- Whisper-Base showed moderate performance with WER of 53.67%
- Whisper-Tiny performed worst with WER of 67.08%, often producing unintelligible outputs
- Small model preserved sentence structure better than other variants despite phonetic errors

## Why This Works (Mechanism)
The study did not investigate the underlying mechanisms that explain why Whisper-Small performs better than other variants for Urdu transcription. The paper focuses on empirical benchmarking rather than theoretical analysis of model behavior or architectural advantages.

## Foundational Learning
The paper does not discuss foundational learning principles or theoretical underpinnings that might explain the observed performance differences between Whisper model variants. The research is purely empirical, comparing pre-trained model outputs without exploring the learning processes or linguistic knowledge acquisition.

## Architecture Onboarding
Component Map: Audio Input -> Preprocessing -> Whisper Encoder -> Decoder -> Text Output
Critical Path: Speech signal processing -> Feature extraction -> Transformer-based encoding -> Decoding -> Transcription
Design Tradeoffs: Model size vs. computational efficiency vs. transcription accuracy
Failure Signatures: Unintelligible outputs (Tiny), lexical distortions (Base), phonetic substitutions (Small)
First Experiments:
1. Test each model on single-word utterances to identify baseline recognition capabilities
2. Evaluate performance across different speaking rates and accents
3. Compare transcription accuracy for code-switched utterances

## Open Questions the Paper Calls Out
The paper does not explicitly identify specific open questions or future research directions. The limitations section suggests areas for improvement but does not frame them as explicit research questions to be addressed.

## Limitations
- Very small sample size (36 audio samples from 10 speakers) raises statistical robustness concerns
- Evaluation limited to pre-trained models without fine-tuning, restricting assessment of true potential
- WER metric alone may not capture quality for low-resource language with linguistic nuances
- No computational requirements or inference times reported for assessing "lightweight" nature

## Confidence
High: The relative ranking of Whisper model sizes (Tiny > Base > Small) in terms of WER performance is reliable based on experimental setup.

Medium: The claim that Whisper-Small is "most promising" for Urdu ASR is supported by results but may not hold in real-world conditions or with fine-tuning.

Medium: Qualitative observations about model behavior are based on limited samples and require validation with larger datasets.

## Next Checks
1. Replicate the study with a significantly larger and more diverse Urdu speech corpus (minimum 100+ speakers, varied acoustic conditions, multiple domains) to establish statistical significance.

2. Conduct fine-tuning experiments for the Whisper models using available Urdu speech data to assess performance gains and determine optimal model size for the trade-off between accuracy and computational efficiency.

3. Implement additional evaluation metrics beyond WER (e.g., semantic error rate, named entity recognition accuracy, code-switching handling) and compare Whisper models against language-specific ASR systems and other multilingual approaches.