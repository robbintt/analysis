---
ver: rpa2
title: 'The Assistant Axis: Situating and Stabilizing the Default Persona of Language
  Models'
arxiv_id: '2601.10387'
source_url: https://arxiv.org/abs/2601.10387
tags:
- assistant
- persona
- role
- response
- axis
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models can adopt different personas but usually
  default to a "helpful Assistant" identity shaped during post-training. This work
  investigates the space of possible personas by extracting activation directions
  corresponding to diverse character archetypes.
---

# The Assistant Axis: Situating and Stabilizing the Default Persona of Language Models

## Quick Facts
- arXiv ID: 2601.10387
- Source URL: https://arxiv.org/abs/2601.10387
- Reference count: 40
- Large language models default to a "helpful Assistant" persona shaped during post-training; this work identifies a measurable "Assistant Axis" in activation space that can be used to stabilize behavior.

## Executive Summary
This work investigates the space of possible personas in large language models by extracting activation directions corresponding to diverse character archetypes. The main variation in persona space is an "Assistant Axis" that measures how far the current persona is from the default Assistant. Steering toward this axis reinforces helpful behavior and reduces jailbreak susceptibility, while steering away increases role adoption and jailbreak success. The axis is also present in pre-trained models, primarily promoting helpful human archetypes. Capping activations along the Assistant Axis within a safe range stabilizes behavior in high-risk scenarios and reduces adversarial jailbreak success with minimal impact on capabilities.

## Method Summary
The researchers generated 275 diverse roles and extracted post-MLP residual stream activations from middle layers when models embodied these personas versus default Assistant behavior. Principal component analysis revealed PC1 consistently captured "similarity to Assistant" across models. They defined the Assistant Axis as the contrast vector between default Assistant activation and mean role activations. Steering was implemented by adding scaled vectors at middle layers during inference, while activation capping bounded the projection along the Assistant Axis using percentile thresholds calibrated from Assistant-like responses.

## Key Results
- The Assistant Axis captures the primary variation in persona space, with PC1 correlating >0.92 with Assistant-ness across all tested models
- Steering toward the Assistant direction reduced harmful responses to persona-based jailbreaks by ~60% without degrading capabilities
- Activation capping at the 25th percentile along the Assistant Axis stabilized behavior and reduced jailbreak success with minimal capability impact

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The "Assistant" persona corresponds to a linearly representable direction in model activation space, measurable as a contrast vector between default Assistant behavior and other personas.
- Mechanism: The researchers extracted role vectors by averaging post-MLP residual stream activations when models embody different character archetypes. Principal component analysis revealed that PC1 consistently captures "similarity to Assistant" across models (>0.92 correlation), with the default Assistant activation projecting onto one extreme and fantastical/mystical roles on the opposite end.
- Core assumption: Persona characteristics are encoded linearly rather than nonlinearly in activation space. The paper acknowledges this assumption may be incomplete.
- Evidence anchors:
  - [section 2.1.3]: "4-19 components were required to explain 70% of the variance across the different models"
  - [section 3.1]: "We defined an Assistant Axis as... subtracted the mean of all fully role-playing role vectors from the mean default Assistant activation"
  - [corpus]: Persona Vectors (Chen et al.) provides corroborating evidence that character traits can be governed by linear directions in activation space
- Break condition: If you cannot replicate the PC1 ≈ Assistant Axis alignment in your target model (cosine similarity <0.6), the contrast vector method may not generalize.

### Mechanism 2
- Claim: Steering activations along the Assistant Axis causally modulates persona adherence and jailbreak susceptibility.
- Mechanism: Adding scaled vectors at middle layers during inference shifts the model's position in persona space. Steering toward the Assistant direction reduces harmful responses to persona-based jailbreaks by ~60% (Llama 3.3 70B); steering away increases role susceptibility and induces mystical/theatrical speech patterns.
- Core assumption: The semantic content of responses can be controlled without degrading core capabilities—partially validated by capability benchmarks.
- Evidence anchors:
  - [abstract]: "Steering toward the Assistant direction reduced harmful responses to persona-based jailbreaks by nearly 60% without degrading capabilities"
  - [section 3.2.1, Figure 5]: Shows quantitative reduction in harmful responses across all three target models
  - [corpus]: What Can We Actually Steer? provides complementary evidence that steering effectiveness varies by behavior type
- Break condition: If steering causes >5% degradation on standard capability benchmarks (MMLU, GSM8k), the intervention may be too aggressive or applied at wrong layers.

### Mechanism 3
- Claim: Persona drift is triggered by specific conversation patterns—emotionally vulnerable disclosures and meta-reflection requests—detectable via Assistant Axis projection tracking.
- Mechanism: Ridge regression on user message embeddings predicts subsequent response position on Assistant Axis (R² = 0.53-0.77, p < 0.001). Clustering revealed that bounded tasks maintain Assistant persona while phenomenological demands and vulnerable disclosures cause drift.
- Core assumption: The current message's embedding predicts response position more strongly than conversation history (delta prediction R² = 0.10).
- Evidence anchors:
  - [section 4.2]: "user message embeddings could strongly predict where the ensuing model response landed along the Assistant Axis... but not the delta from its previous response"
  - [section 4.1, Figure 7]: Shows domain-specific drift trajectories across coding, writing, therapy, philosophy
  - [corpus]: Limited corpus support; persona drift as defined here is relatively novel
- Break condition: If your application domain shows different drift patterns, re-calibrate trigger taxonomies using your own conversation data.

## Foundational Learning

- Concept: **Residual stream activations**
  - Why needed here: The Assistant Axis is computed from post-MLP residual stream activations at middle layers; understanding what this represents is prerequisite to extraction.
  - Quick check question: Can you explain why the authors chose middle layers rather than early or late layers for their analysis?

- Concept: **Contrast vector extraction (difference-in-means)**
  - Why needed here: The Assistant Axis is defined as default Assistant activation minus mean role vector; this method assumes linear separability of concepts.
  - Quick check question: Given two sets of activations A and B, what would the contrast vector v = mean(A) - mean(B) represent semantically?

- Concept: **Activation steering / activation addition**
  - Why needed here: The intervention works by adding scaled vectors to activations during forward passes; understanding dose-response is critical.
  - Quick check question: If steering with strength α causes role shift, what would you expect from strength 2α, and what failure mode might occur?

## Architecture Onboarding

- Component map:
Role Rollout Generator → Activation Extractor → PCA / Contrast Vector → Assistant Axis
                                                                            ↓
User Message → [Target Model with Activation Capping] → Response
                   ↑
            Calibration Dataset (percentile thresholds)

- Critical path:
  1. Generate diverse role-playing rollouts (275 roles × multiple prompts)
  2. Extract mean activations at middle layer (e.g., layer 46-53 for Qwen 32B)
  3. Compute Assistant Axis as contrast vector OR use PC1 if aligned
  4. Calibrate activation cap threshold (25th percentile recommended)
  5. Apply capping at every token position during inference

- Design tradeoffs:
  - **Layer range**: Middle-to-late layers (8-16 layers) work best; too early = semantic content not yet formed, too late = output already determined
  - **Cap threshold**: 25th percentile balances safety vs. capability; 1st percentile is more aggressive but risks over-constraint
  - **Contrast vector vs. PC1**: Contrast vector is more generalizable; PC1 may not always capture "Assistant-ness"

- Failure signatures:
  - Cap too strict → responses become generic, refuse legitimate requests
  - Cap too loose → no effect on jailbreak mitigation
  - Wrong layers → semantic incoherence or no behavioral change
  - PC1 misaligned → steering has unpredictable effects (check cosine similarity first)

- First 3 experiments:
  1. Reproduce the Assistant Axis extraction on your target model using 50+ diverse roles; verify PC1 alignment (cosine similarity >0.6 with contrast vector)
  2. Run steering evaluation: steer toward/away from Assistant on a held-out set of persona-based jailbreaks; measure harmful response rate vs. baseline
  3. Implement activation capping with 25th percentile threshold at recommended layers; evaluate on both safety metrics (jailbreak success) and capability metrics (MMLU Pro, GSM8k)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the "Assistant Axis" structure persist in frontier, mixture-of-expert (MoE), or reasoning models?
- Basis in paper: [explicit] The authors state, "Reproducing our pipeline on frontier, mixture-of-expert, and reasoning models would help shed light on how the Assistant is represented."
- Why unresolved: The study was constrained to open-weight dense transformers (Gemma, Qwen, Llama) with reasoning modes disabled.
- What evidence would resolve it: Applying PCA to role vectors in an MoE or reasoning model to verify if the leading component still correlates with the Assistant persona.

### Open Question 2
- Question: Do the persona drift dynamics observed in synthetic conversations replicate in genuine human interactions?
- Basis in paper: [explicit] The authors note, "A human study replicating our setup would help validate the effects we observed, especially the trend towards persona drift."
- Why unresolved: The multi-turn experiments relied on LLMs simulating users, which may not fully capture natural human interaction patterns.
- What evidence would resolve it: Analyzing Assistant Axis projections in datasets of real human-chatbot conversations, particularly those involving emotional vulnerability or meta-reflection.

### Open Question 3
- Question: Can specific post-training data compositions be correlated with shifts along the identified persona dimensions?
- Basis in paper: [explicit] The authors suggest, "Persona space may provide signal on the effects of post-training data... tracking how different training data shifts a model’s position."
- Why unresolved: While the paper maps the *result* of training (the Assistant Axis), it does not isolate which specific data components (e.g., SFT vs. RLHF) drive the model toward the Assistant region.
- What evidence would resolve it: A longitudinal study measuring activation projections after incremental training on isolated data subsets.

## Limitations
- The findings hinge on the linear representation assumption for personas, which may be overly simplistic for complex traits
- Safety evaluation relies on automated LLM judges, which have known limitations in consistency and bias
- The intervention is reactive rather than preventing root causes of persona drift during training

## Confidence
- **High confidence**: The existence of the Assistant Axis as a measurable direction in activation space; the effectiveness of steering along this axis for persona modulation; the correlation between Assistant Axis position and jailbreak susceptibility
- **Medium confidence**: The causal link between specific conversation patterns (emotionally vulnerable disclosures, meta-reflection) and persona drift; the generalizability of the 25th percentile cap threshold across models and domains
- **Low confidence**: The completeness of the linear assumption for persona representation; the long-term stability of activation capping interventions; the absence of capability degradation in all real-world applications

## Next Checks
1. **Cross-model generalization test**: Apply the Assistant Axis extraction and activation capping to a different model family (e.g., Claude, GPT series) and verify that: (a) PC1 still aligns with Assistant-ness (cosine similarity >0.6), (b) steering effects replicate (>50% jailbreak mitigation), and (c) capability degradation remains minimal (<5% on standard benchmarks)

2. **Temporal stability evaluation**: Implement the activation capping intervention in a deployed setting and monitor over 2-4 weeks for: (a) emergence of new drift patterns not captured by current trigger taxonomies, (b) any gradual capability degradation that accumulates over extended use, and (c) user perception changes through human evaluation

3. **Root cause intervention test**: Instead of post-hoc activation capping, attempt to modify the training process to better anchor the Assistant persona during post-training. Compare: (a) pre-training vs. post-training vs. fine-tuning contributions to the Assistant Axis strength, (b) whether early intervention during training provides more robust persona stability than activation capping alone, and (c) any trade-offs in training efficiency or final capability levels