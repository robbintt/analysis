---
ver: rpa2
title: 'Compressing Model with Few Class-Imbalance Samples: An Out-of-Distribution
  Expedition'
arxiv_id: '2502.05832'
source_url: https://arxiv.org/abs/2502.05832
tags:
- compression
- class
- imbalance
- few-sample
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the class imbalance problem in few-sample
  model compression, where limited training data leads to severe imbalance between
  majority and minority classes. The authors propose OOD-Enhanced Few-Sample Model
  Compression (OE-FSMC), a framework that integrates out-of-distribution (OOD) data
  into both compression and fine-tuning processes.
---

# Compressing Model with Few Class-Imbalance Samples: An Out-of-Distribution Expedition

## Quick Facts
- **arXiv ID**: 2502.05832
- **Source URL**: https://arxiv.org/abs/2502.05832
- **Reference count**: 9
- **Primary result**: OE-FSMC integrates OOD data into compression/fine-tuning, improving accuracy across multiple few-sample compression methods with pronounced gains when fewer training samples are available

## Executive Summary
This paper tackles the class imbalance problem in few-sample model compression scenarios where limited training data creates severe imbalance between majority and minority classes. The authors introduce OE-FSMC, a framework that leverages out-of-distribution (OOD) data during both compression and fine-tuning phases. By sampling labels from a complementary distribution and incorporating joint distillation loss with regularization, the method effectively rebalances class priors while preventing overfitting to OOD data. Extensive experiments on CIFAR-10/100 and ILSVRC-2012 demonstrate improved accuracy across multiple compression methods, particularly when training samples are scarce.

## Method Summary
OE-FSMC addresses class imbalance in few-sample model compression by integrating OOD data into the training pipeline. The framework employs a complementary distribution sampling strategy to rebalance class priors, allowing minority classes to receive more attention during training. Joint distillation loss ensures knowledge transfer from the original model while regularization prevents overfitting to OOD data. The method is compatible with existing compression techniques and demonstrates strong generalizability across different compression approaches.

## Key Results
- OE-FSMC improves accuracy across multiple few-sample compression methods on CIFAR-10/100 and ILSVRC-2012 datasets
- More pronounced performance gains observed when fewer training samples are available
- Framework demonstrates strong generalizability and compatibility with existing compression techniques
- Effectively mitigates accuracy degradation caused by class imbalance in few-sample scenarios

## Why This Works (Mechanism)
The framework works by addressing the fundamental challenge of class imbalance in few-sample settings through strategic incorporation of OOD data. By sampling from complementary distributions, OE-FSMC artificially balances the class representation during training, ensuring minority classes receive adequate attention. The joint distillation loss preserves knowledge from the original model while the regularization component prevents the model from overfitting to potentially mismatched OOD data characteristics. This balanced approach allows the compressed model to maintain performance across all classes, particularly benefiting underrepresented ones.

## Foundational Learning

**Complementary Distribution Sampling**: A technique for selecting samples from a distribution that differs from the original data distribution to achieve balanced representation. Needed to address class imbalance when few samples are available. Quick check: Verify that sampled classes cover all original classes with appropriate frequency ratios.

**Joint Distillation Loss**: Combines knowledge distillation from multiple teacher models or augmented data sources. Required to transfer knowledge from the original model while incorporating OOD data. Quick check: Monitor distillation loss convergence across different data sources.

**Regularization for OOD Data**: Techniques that prevent overfitting to data distributions that differ from the target distribution. Essential when incorporating OOD data to avoid performance degradation. Quick check: Compare validation performance on in-distribution vs OOD validation sets.

## Architecture Onboarding

**Component Map**: Original Model -> Compression Method -> OE-FSMC Framework (Complementary Sampling + Joint Distillation + Regularization) -> Compressed Model

**Critical Path**: The most critical path is the integration of complementary distribution sampling with the joint distillation loss, as this directly addresses the class imbalance while preserving knowledge transfer.

**Design Tradeoffs**: The framework balances between leveraging OOD data for rebalancing and avoiding overfitting to mismatched distributions. This tradeoff is managed through regularization parameters and sampling strategy choices.

**Failure Signatures**: Potential failures include: overfitting to OOD data (detected by degraded performance on in-distribution validation), insufficient rebalancing (minority class performance remains poor), and instability in training (fluctuating validation metrics).

**First Experiments**: 
1. Compare class-wise accuracy before and after applying OE-FSMC to establish imbalance reduction
2. Ablation study removing OOD data integration to quantify its contribution
3. Test different complementary distribution sampling strategies to find optimal rebalancing approach

## Open Questions the Paper Calls Out
None

## Limitations
- Experimental validation limited to image classification benchmarks, uncertain applicability to other domains like NLP or speech processing
- Complementary distribution sampling introduces potential biases not fully characterized, particularly regarding scalability with different dataset characteristics
- Limited ablation studies on performance with compression methods beyond those tested, creating uncertainty about true generalizability

## Confidence

**Claims and Confidence Ratings**:
- OE-FSMC improves accuracy across multiple few-sample compression methods: **High**
- Strong generalizability across diverse compression techniques: **Medium**
- Effectiveness increases when fewer training samples are available: **High**
- Compatibility with existing compression techniques: **Medium**

## Next Checks

1. Test OE-FSMC framework on non-image datasets (e.g., text classification, speech recognition) to verify cross-domain applicability
2. Conduct systematic ablation studies varying the complementary distribution sampling parameters to understand sensitivity to hyperparameter choices
3. Evaluate long-term stability and performance when OOD data distribution shifts over time or across different domains