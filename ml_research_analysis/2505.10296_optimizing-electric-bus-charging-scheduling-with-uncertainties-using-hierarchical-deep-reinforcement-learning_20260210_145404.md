---
ver: rpa2
title: Optimizing Electric Bus Charging Scheduling with Uncertainties Using Hierarchical
  Deep Reinforcement Learning
arxiv_id: '2505.10296'
source_url: https://arxiv.org/abs/2505.10296
tags:
- time
- charging
- step
- trip
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the electric bus charging scheduling problem
  under uncertainties in travel time, energy consumption, and electricity prices.
  A hierarchical deep reinforcement learning approach is proposed that reformulates
  the problem into two augmented Markov Decision Processes using a Double Actor-Critic
  architecture.
---

# Optimizing Electric Bus Charging Scheduling with Uncertainties Using Hierarchical Deep Reinforcement Learning

## Quick Facts
- arXiv ID: 2505.10296
- Source URL: https://arxiv.org/abs/2505.10296
- Reference count: 40
- Key result: DAC-MAPPO-E achieves 0.32% performance gap vs. theoretical optimal, outperforming MILP-S (11.15% worse) and PPO-MILP (7.37% worse)

## Executive Summary
This paper addresses the electric bus charging scheduling problem under uncertainties in travel time, energy consumption, and electricity prices using a hierarchical deep reinforcement learning approach. The proposed DAC-MAPPO-E algorithm reformulates the problem into two augmented Markov Decision Processes using a Double Actor-Critic architecture, incorporating a Multi-Agent Proximal Policy Optimization framework at the low level for decentralized charging power decisions and employing attention mechanisms at the high level for charger allocation and trip assignment. Experimental results using real-world data from Guelph, Canada demonstrate that the algorithm achieves near-optimal performance while maintaining superior scalability and convergence properties, particularly for larger fleet sizes.

## Method Summary
The method reformulates the electric bus charging scheduling problem into two augmented Markov Decision Processes using a Double Actor-Critic architecture. The high-level MDP handles charger allocation and trip assignment decisions that persist over variable time periods, while the low-level MDP determines charging power at each time step. The solution employs a Multi-Agent Proximal Policy Optimization framework at the low level with parameter sharing and centralized training decentralized execution (CTDE), and uses attention mechanisms at the high level to compress global state information per-EB. This hierarchical decomposition exploits different temporal scales of decision-making, reducing effective decision frequency and simplifying policy learning while maintaining coordinated behavior across the fleet.

## Key Results
- DAC-MAPPO-E achieves 0.32% performance gap versus theoretical optimal solution
- Significantly outperforms baseline methods: MILP-S (11.15% worse) and PPO-MILP (7.37% worse)
- Demonstrates superior scalability with convergence within 30K episodes for M=20 EBs/N=10 chargers
- Attention mechanism reduces input dimension from 5M+2 to 12 neurons regardless of fleet size

## Why This Works (Mechanism)

### Mechanism 1
Reformulating the original MDP into two augmented MDPs via Double Actor-Critic architecture enables efficient learning across multiple temporal abstractions. The high-level MDP handles charger allocation and trip assignment decisions that persist over variable time periods, while the low-level MDP determines charging power at each time step. This separation exploits the fact that high-level decisions remain valid across multiple timesteps, reducing the effective decision frequency and simplifying policy learning. The core assumption is that the charging scheduling problem naturally decomposes into decisions operating at different timescales.

### Mechanism 2
Incorporating MAPPO at the low level with Centralized Training Decentralized Execution (CTDE) enables scalable, coordinated charging decisions across large fleets. Each EB acts as a decentralized agent making local charging power decisions using a shared actor network with parameter sharing. During training, a centralized critic observes the global state and option, providing coordination signals. This reduces action space dimensionality from |C_t|^(M_t) to |C_t| per agent while maintaining coordinated behavior. The core assumption is that EB charging decisions are interdependent but can be learned as local decisions with global value function guidance.

### Mechanism 3
The attention mechanism in the high-level actor compresses global state information per-EB, enabling scalability to large fleets without proportional network growth. Each agent network computes query, key, and value vectors, with attention weights determining relevance of other EBs' states to each EB. The compressed representation replaces full state concatenation, reducing input dimension from 5M+2 to 12 regardless of fleet size. The core assumption is that not all inter-EB state information is equally relevant for each EB's decisions, and attention can learn to extract the most pertinent features.

## Foundational Learning

- **Markov Decision Processes (MDPs) and Options Framework**: The entire solution builds on reformulating the problem as augmented MDPs with options. Understanding states, actions, transition probabilities, initiation sets, intra-option policies, and termination conditions is essential. Quick check: Given the high-level policy πH(AH_t|SH_t) in Equation (24), explain how it combines the termination condition and policy over options.

- **Proximal Policy Optimization (PPO) and MAPPO**: PPO is used at the high level; MAPPO at the low level. Understanding clipped surrogate objectives, value function estimation, and the CTDE paradigm is required to implement and debug the algorithm. Quick check: Why does MAPPO use a centralized critic during training but decentralized actors during execution? What does "parameter sharing" mean in this context?

- **Attention Mechanisms**: The high-level actor uses attention to compress global state information. Understanding query/key/value transformations and softmax attention weights is necessary to modify or debug this component. Quick check: In Equations (36)-(40), what does S_att_{i,t} represent, and why does it have fixed dimension regardless of fleet size?

## Architecture Onboarding

- **Component map**: Global state S_t -> Termination network β_φ -> High-level actor (attention layer → M agent networks) -> Mapping networks (Algorithms 2-3) -> Low-level actor (shared network π_θ) -> Centralized critic V_φ(S_t, o_t)

- **Critical path**: 1) Observe global state S_t; check termination β_{o_{t-1}}(S_t) 2) If terminated: sample new option o_t via high-level actor (attention → agent networks → mapping networks) 3) For each EB with ω_i,t=1: sample c_i,t from low-level actor π_θ 4) Execute actions; observe reward r_t and next state S_{t+1} 5) Update critic (PPO + MAPPO losses), high-level actor and β network (PPO), low-level actor (MAPPO)

- **Design tradeoffs**: Centralized high-level vs. decentralized low-level (high-level must be centralized due to mutual exclusion constraints, creating scalability limits addressed by attention); parameter sharing in low-level (accelerates training but assumes homogeneous optimal policies); shared critic (reduces parameters but couples high/low-level learning)

- **Failure signatures**: Non-convergence in large fleets (verify attention outputs are non-degenerate); infeasible high-level actions (bug in mask layer or Algorithm 2/3); poor price responsiveness (verify historical price window is correctly concatenated); charger hoarding (check gradient flow through β network)

- **First 3 experiments**: 1) Sanity check: Implement DAC-MAPPO (no enhancements) on Scenario 1 (M=6, N=3); verify convergence to within ~0.5% of MILP-D baseline 2) Ablation on attention: Add attention mechanism to high-level actor; compare convergence speed and final performance vs. DAC-MAPPO baseline 3) Scalability test: Run DAC-MAPPO-E on Scenario 2 (M=20, N=10); confirm convergence within 30K episodes and performance within 0.5% of MILP-D

## Open Questions the Paper Calls Out

### Open Question 1
How does DAC-MAPPO-E perform with fleet sizes beyond 20 EBs, and at what scale does the attention mechanism become insufficient for managing computational complexity? The experiments only evaluate up to M=20 EBs, but real transit operators may manage fleets of 50-100+ EBs. No experiments were conducted beyond 20 EBs, and the O(M·N) complexity for charger allocation may still become prohibitive.

### Open Question 2
Can the DAC-MAPPO-E framework be extended to multiple terminal stations with inter-terminal coordination? The model explicitly assumes a single terminal, but real transit networks have multiple depots requiring coordinated charging and vehicle repositioning. Extending the MDP formulation to include terminal location in the state space would be needed.

### Open Question 3
How robust is the learned policy to distribution shifts in electricity price patterns (e.g., market regime changes, extreme weather events)? The model is trained and tested on data from the same time periods, with no analysis examining performance under price patterns significantly different from training data. DRL policies can overfit to training distributions.

### Open Question 4
How does the assumption of negligible charger switching time affect solution optimality in congested depot configurations? In high-utilization scenarios with many EBs and few chargers, switching delays could significantly impact feasible schedules and learned policies. No sensitivity analysis varying switching time parameters was conducted.

## Limitations
- Experimental validation relies on a single real-world dataset from Guelph, Canada without testing across diverse geographical regions
- Comparison with MILP-D as an "oracle" solution assumes perfect foresight of uncertain parameters, making it an idealized benchmark
- Attention mechanism's effectiveness is demonstrated empirically but lacks theoretical grounding for information preservation
- Scalability to much larger fleets (>100 buses) is unverified, leaving potential bottlenecks untested

## Confidence
- Performance claims: Medium confidence (single dataset, idealized benchmarks)
- Scalability claims: Medium confidence (limited to M=20, no stress testing)
- Attention mechanism: Medium confidence (empirical validation without theoretical guarantees)

## Next Checks
1. Test algorithm performance across multiple geographic regions with different electricity price patterns and transit schedules to verify generalizability beyond Guelph data
2. Conduct ablation studies isolating the attention mechanism's contribution by comparing against a baseline with full state concatenation but no attention
3. Evaluate scalability by testing on fleet sizes of 50+ buses to identify potential bottlenecks in the high-level action space that could emerge at larger scales