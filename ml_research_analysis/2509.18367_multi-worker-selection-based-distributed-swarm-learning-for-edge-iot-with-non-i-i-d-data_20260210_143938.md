---
ver: rpa2
title: Multi-Worker Selection based Distributed Swarm Learning for Edge IoT with Non-i.i.d.
  Data
arxiv_id: '2509.18367'
source_url: https://arxiv.org/abs/2509.18367
tags:
- data
- learning
- non-i
- distributed
- m-dsl
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of data heterogeneity in distributed
  swarm learning (DSL) for edge IoT networks, where non-i.i.d. data degrades learning
  performance and causes model divergence.
---

# Multi-Worker Selection based Distributed Swarm Learning for Edge IoT with Non-i.i.d. Data

## Quick Facts
- **arXiv ID**: 2509.18367
- **Source URL**: https://arxiv.org/abs/2509.18367
- **Reference count**: 20
- **Primary result**: Novel M-DSL algorithm improves distributed learning accuracy and convergence on non-i.i.d. IoT data through multi-worker selection and PSO integration

## Executive Summary
This paper addresses data heterogeneity challenges in distributed swarm learning (DSL) for edge IoT networks by introducing a non-i.i.d. degree metric and a Multi-Worker Selection based DSL (M-DSL) algorithm. The authors propose a metric combining Wasserstein distance and label distribution skew to quantify data heterogeneity, then use this to select multiple workers for collaboration based on both data quality and model performance. Extensive experiments on MNIST and CIFAR10 demonstrate M-DSL's superior performance over FedAvg and standard DSL in terms of accuracy, convergence speed, and communication efficiency in non-i.i.d. data scenarios.

## Method Summary
The method introduces a non-i.i.d. degree metric (η) that quantifies data heterogeneity using Wasserstein distance and label distribution skew, normalized via Min-Max scaling. M-DSL selects multiple workers based on a trade-off score combining this metric with model performance, then aggregates their models. Local updates integrate particle swarm optimization (PSO) dynamics with gradient descent to help escape biased optima. The algorithm uses a synthetic global dataset for evaluation and employs adaptive selection thresholds to balance collaboration and communication efficiency. Theoretical convergence analysis proves O(1/T) convergence under Lipschitz smoothness assumptions.

## Key Results
- M-DSL achieves higher validation accuracy than FedAvg and standard DSL on MNIST and CIFAR10 with non-i.i.d. data
- The proposed non-i.i.d. degree metric correlates with accuracy degradation, enabling effective worker selection
- M-DSL demonstrates faster convergence and reduced communication overhead compared to baselines
- Theoretical analysis proves convergence with O(1/T) rate under standard assumptions

## Why This Works (Mechanism)

### Mechanism 1: Non-i.i.d. Degree Metric
- **Claim**: The normalized metric (η) correlates with model accuracy degradation caused by data heterogeneity, enabling informed worker selection
- **Mechanism**: Combines Wasserstein distance measuring statistical distribution difference with label diversity ratio, normalized via Min-Max Scaling. The formula ηi = Normalize(β1·|Li|/|Lg| + β2·Wi + φ) captures both distribution skew and label type balance
- **Core assumption**: The relationship between heterogeneity level and accuracy degradation follows a predictable pattern capturable by linear combination of WD and label ratio
- **Evidence anchors**: Fig. 1 shows significantly diminished gap between η curve and FedAvg testing accuracy, illustrating effectiveness in assessment for both data heterogeneity measurement and non-i.i.d. impact on learning performance

### Mechanism 2: Multi-Worker Selection Strategy
- **Claim**: Selecting multiple workers based on combined data quality (η) and model performance (F) improves convergence speed and final accuracy over single-worker selection
- **Mechanism**: Workers compute trade-off score θi,t = τFi,t + (1-τ)ηi. Selection threshold θ̄t-1 (average gain from previous round) adaptively determines which workers upload models. Selected workers contribute via averaged parameter updates, reducing communication while maintaining collaboration gains
- **Core assumption**: Workers with lower η (more representative data) and lower F (better model) provide more beneficial global updates; adaptive thresholding maintains productive participation rates

### Mechanism 3: PSO-Integrated Local Updates
- **Claim**: Incorporating particle swarm optimization dynamics into local gradient updates helps escape biased optima caused by non-i.i.d. data gradients
- **Mechanism**: Local update: wi,t+1 = wi,t + c0vi,t + c1(wli,t - wi,t) + c2(wgt - wi,t) - α∇F(wi,t, Dg). The velocity term, local best attraction, and global best attraction provide momentum and social learning beyond pure SGD, with gradient evaluated on synthetic global dataset Dg
- **Core assumption**: PSO dynamics help correct biased gradients from heterogeneous local data by leveraging collective intelligence signals

## Foundational Learning

- **Concept: Wasserstein Distance (Earth Mover's Distance)**
  - **Why needed here**: Measures minimum "cost" to transform one distribution into another. Used to quantify statistical difference between local label distributions and global distribution
  - **Quick check question**: Why would Wasserstein distance be preferred over KL divergence when comparing two label distributions that share few common labels?

- **Concept: Label Distribution Skew (Prior Probability Shift)**
  - **Why needed here**: This paper specifically addresses this type of non-i.i.d. data where PrDi(y) ≠ PrDj(y) across workers, even if conditional distributions Pr(y|x) are similar
  - **Quick check question**: If two workers have identical label proportions but very different feature distributions for each class, would the proposed η metric capture this heterogeneity?

- **Concept: Particle Swarm Optimization (PSO) Fundamentals**
  - **Why needed here**: DSL integrates PSO with gradient descent. Must understand velocity (momentum), personal best (cognitive component), and global best (social component) to implement local update equations correctly
  - **Quick check question**: In the local update equation, what happens to exploration vs. exploitation balance if c1 increases while c2 decreases?

## Architecture Onboarding

- **Component map**: Workers (50) -> Compute ηi and θi,t -> Selection Module (threshold θ̄t-1) -> Parameter Server (aggregates selected models) -> Broadcast wt+1 -> Workers (local PSO updates)
- **Critical path**: 1. Initialization: Compute all ηi, randomize w, wli,0, wgi,0 2. Per-round loop: Local training (Eq. 8) → Compute Fi,t+1, θi,t+1 → Selection (Eq. 6) → Upload selected models → Aggregate (Eq. 7) → Broadcast wt+1 3. Convergence: Monitor validation accuracy; typically 20-40 rounds
- **Design tradeoffs**: τ (regularizer, default 0.9) prioritizes loss over non-i.i.d. degree; fewer selected workers → lower communication but slower convergence; PSO coefficients c0~U(0,1), c1,c2~N(0,1) work for experiments
- **Failure signatures**: Accuracy stagnates below FedAvg (η hyperparameters misconfigured); High variance across runs (PSO coefficient ranges too wide); No workers selected after early rounds (threshold too restrictive)
- **First 3 experiments**: 1. Sanity check on i.i.d. data: Run M-DSL on MNIST with 50 i.i.d. workers. Target: match or exceed FedAvg accuracy (~99%) 2. Ablation: η contribution: Compare M-DSL vs. Multi-DSL (without η in selection score) on CIFAR10 non-i.i.d. Case I (α=0.5) 3. Communication-efficiency curve: Sweep selection threshold to vary average workers selected per round. Plot accuracy vs. communication cost

## Open Questions the Paper Calls Out

- **Question**: Can the proposed non-i.i.d. degree metric maintain its correlation with accuracy degradation when applied to complex, unstructured real-world IoT datasets without re-fitting the hyperparameters?
  - **Basis in paper**: Section V-C states hyperparameters (β1, β2, φ) are derived via linear regression specifically on MNIST and CIFAR10
  - **Why unresolved**: The paper relies on dataset-specific regression to set the metric's weights, leaving generalizability to different data distributions unproven
  - **What evidence would resolve it**: Experiments applying fixed hyperparameters to disparate datasets showing persistent linear correlation between metric and model accuracy

- **Question**: How does the quality and distributional fidelity of the GAN-generated synthetic global dataset (Dg) impact the convergence stability of M-DSL?
  - **Basis in paper**: Section III-B notes DSL requires synthetic global dataset Dg for function evaluation, and Section V-A mentions it is generated by GANs
  - **Why unresolved**: The method assumes availability of representative synthetic dataset but does not analyze how divergence between synthetic Dg and true data distribution affects PSO evaluation mechanism
  - **What evidence would resolve it**: Sensitivity analysis measuring convergence speed and final accuracy while varying distribution shift between synthetic evaluation set and actual test set

- **Question**: Does the M-DSL framework remain robust when data heterogeneity arises from feature distribution skew or concept drift rather than solely label distribution skew?
  - **Basis in paper**: Section II states "This paper focuses on the widely used definition of non-i.i.d. data in terms of label distribution skew"
  - **Why unresolved**: The proposed metric (Eq. 2) combines Wasserstein distance and label ratio specifically to target label skew; unclear if this captures statistical divergence in other non-i.i.d. scenarios
  - **What evidence would resolve it**: Comparative trials on datasets with feature covariance shift or temporal concept drift to observe if selection mechanism effectively prioritizes beneficial workers

## Limitations

- The core claim linking non-i.i.d. degree metric to accuracy degradation relies on correlation rather than formal causal proof, with limited ablation studies on metric components
- Reproducibility is hindered by unspecified global synthetic dataset generation method (GAN details absent) and incomplete CNN architecture specifications
- PSO integration lacks comparison against standard SGD baselines to isolate its contribution to accuracy gains

## Confidence

- **High**: Convergence proof under Lipschitz assumptions; empirical accuracy improvements over FedAvg and DSL; clear mechanism for multi-worker selection threshold adaptation
- **Medium**: Generalization of η metric to feature-level heterogeneity; PSO's isolated contribution beyond momentum; robustness to extreme non-i.i.d. scenarios
- **Low**: Global synthetic dataset impact on final accuracy; exact baseline implementation details for fair comparison; real-world edge IoT deployment viability

## Next Checks

1. Ablation study: Run M-DSL with PSO disabled (pure SGD local updates) to quantify PSO's contribution to accuracy gains
2. Feature heterogeneity test: Create non-i.i.d. partition based on feature distribution skew (not just label skew) and measure η metric's predictive power for accuracy degradation
3. Communication-efficiency analysis: Plot accuracy vs. total model parameters communicated for M-DSL vs. baselines across different selection thresholds to verify claimed communication savings