---
ver: rpa2
title: Operationalizing Large Language Models with Design-Aware Contexts for Code
  Comment Generation
arxiv_id: '2510.22338'
source_url: https://arxiv.org/abs/2510.22338
tags:
- code
- comments
- design
- comment
- context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates Large Language Models (LLMs) for generating
  code comments, specifically using design documents as project-specific context.
  Design documents explain architecture, functionality, and design choices, which
  are commonly used by maintainers when comments are insufficient.
---

# Operationalizing Large Language Models with Design-Aware Contexts for Code Comment Generation

## Quick Facts
- **arXiv ID**: 2510.22338
- **Source URL**: https://arxiv.org/abs/2510.22338
- **Reference count**: 40
- **Primary result**: Design documents improve LLM code comment quality; ASTs are detrimental

## Executive Summary
This study investigates whether Large Language Models can generate higher-quality code comments when provided with design documents as project-specific context. The research addresses two key questions: whether design documents can improve comment generation and whether Abstract Syntax Trees (ASTs) can serve as structural cues. Through a comprehensive evaluation using both automated metrics and human assessment, the study finds that design documents significantly enhance comment quality across multiple dimensions, while ASTs surprisingly degrade performance by overwhelming the models with excessive information.

The research employs a Retrieval-Augmented Generation approach with four experimental setups, evaluating comment quality through CodeBERTScore, GPTScore, and practical developer tasks. The results demonstrate that design documents increase the proportion of navigational, algorithmic, and exception-based comments, leading to measurable improvements in developer productivity with 35% faster bug-fixing times. The study concludes that design documents serve as effective project-specific context for LLM-generated comments, particularly benefiting novice developers, while suggesting that structural information needs more careful integration than simply providing full ASTs.

## Method Summary
The study employs Retrieval-Augmented Generation (RAG) to generate code comments using design documents as retrieval sources. Four experimental setups are evaluated: (i) Few-shot prompting, (ii) Few-shot prompting with ASTs, (iii) Few-shot prompting with RAG on design documents, and (iv) Combining ASTs with RAG on design documents. Automated metrics like CodeBERTScore and GPTScore are used alongside human assessment through bug-fixing and codebase enhancement tasks. The evaluation covers 200 comments per setup, measuring quality improvements across comment categories and developer productivity metrics.

## Key Results
- Comments generated with design documents increase navigational, algorithmic, and exception-based comment categories
- Bug-fixing time decreased by 35% when design documents were used as context
- ASTs were found to be detrimental due to overwhelming LLMs with too much information
- Design documents serve as effective project-specific context for improving LLM-generated comments

## Why This Works (Mechanism)
The effectiveness of design documents stems from their ability to provide high-level architectural context that bridges the gap between code implementation and developer understanding. When LLMs have access to design documents, they can generate comments that reflect not just what the code does, but why it was designed that way, leading to more meaningful and contextually appropriate documentation.

## Foundational Learning
- **Design Documents**: High-level architectural descriptions explaining system functionality and design choices - needed to understand system intent beyond implementation details; quick check: verify document covers architecture, functionality, and design rationale
- **Retrieval-Augmented Generation (RAG)**: Framework combining retrieval of relevant documents with generative model prompting - needed to ground LLM outputs in project-specific context; quick check: ensure retrieval relevance scores meet threshold
- **CodeBERTScore**: Automated metric for evaluating code comment quality based on BERT embeddings - needed for scalable quality assessment; quick check: compare scores across different model versions
- **AST (Abstract Syntax Tree)**: Tree representation of source code structure - needed for structural code analysis; quick check: verify tree captures all syntactic elements
- **Few-shot Prompting**: Technique using example inputs/outputs to guide model generation - needed to provide context for comment generation tasks; quick check: ensure examples cover diverse comment types
- **Human Assessment Tasks**: Practical evaluation through developer activities like bug-fixing and enhancement - needed to measure real-world utility; quick check: track time and success rates across conditions

## Architecture Onboarding

**Component Map**: LLMs <-(input) Design Documents + Code <-(retrieval) RAG System <-(output) Comment Generator

**Critical Path**: Retrieval -> Context Augmentation -> Comment Generation -> Quality Assessment

**Design Tradeoffs**: The study prioritizes comprehensive design document retrieval over selective structural cues, accepting the risk of information overload from ASTs in exchange for richer contextual understanding

**Failure Signatures**: 
- Poor retrieval relevance leading to off-topic comments
- AST-induced information overload causing incoherent outputs
- Over-reliance on design documents missing implementation-specific details

**3 First Experiments**:
1. Vary retrieval threshold to optimize design document relevance vs. noise
2. Test selective AST feature extraction vs. full tree representation
3. Compare different design document sections (architecture vs. implementation) for comment quality

## Open Questions the Paper Calls Out
None

## Limitations
- Narrow focus on design documents as sole context source, potentially missing implementation details
- Automated metrics may not fully capture nuanced quality aspects valued by developers
- Evaluation sample size may be insufficient to detect subtle quality differences
- Finding that ASTs are detrimental may reflect implementation rather than fundamental limitation

## Confidence
- **High confidence**: Design documents improve navigational and algorithmic comment generation (35% bug-fixing time reduction)
- **Medium confidence**: ASTs are detrimental to comment quality (based on single representation approach)
- **Medium confidence**: Novice developers benefit more from design-document-augmented comments (lacks detailed segmentation analysis)

## Next Checks
1. Replicate with alternative structural representations (selective feature extraction, simplified graphs) to test if structural information can be beneficial when properly curated
2. Conduct multi-source context experiment combining design documents with implementation notes, commit messages, and issue discussions
3. Perform longitudinal study tracking comment usefulness over time as code evolves