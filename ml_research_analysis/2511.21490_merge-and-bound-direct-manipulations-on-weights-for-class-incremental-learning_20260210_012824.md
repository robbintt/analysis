---
ver: rpa2
title: 'Merge and Bound: Direct Manipulations on Weights for Class Incremental Learning'
arxiv_id: '2511.21490'
source_url: https://arxiv.org/abs/2511.21490
tags:
- weight
- learning
- task
- base
- merging
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes Merge-and-Bound (M&B), a novel training approach
  for Class Incremental Learning (CIL) that directly manipulates model weights to
  enhance stability and plasticity. The method introduces two weight merging techniques:
  inter-task weight merging, which averages parameters across all previous tasks to
  form a base model, and intra-task weight merging, which averages multiple checkpoints
  within the current task to improve generalization.'
---

# Merge and Bound: Direct Manipulations on Weights for Class Incremental Learning

## Quick Facts
- arXiv ID: 2511.21490
- Source URL: https://arxiv.org/abs/2511.21490
- Authors: Taehoon Kim; Donghwan Jang; Bohyung Han
- Reference count: 4
- One-line primary result: M&B consistently improves performance over state-of-the-art CIL methods on CIFAR-100 and ImageNet datasets

## Executive Summary
This paper introduces Merge-and-Bound (M&B), a novel training approach for Class Incremental Learning (CIL) that directly manipulates model weights to enhance stability and plasticity. The method introduces two weight merging techniques: inter-task weight merging, which averages parameters across all previous tasks to form a base model, and intra-task weight merging, which averages multiple checkpoints within the current task to improve generalization. Additionally, a bounded model update strategy constrains weight updates within each task to preserve knowledge from previous tasks. M&B is easily integrated into existing CIL methods without modifying architectures or loss functions. Extensive experiments on CIFAR-100 and ImageNet-100/1000 demonstrate that M&B consistently improves performance over state-of-the-art methods, especially in scenarios with limited memory budgets.

## Method Summary
M&B operates through three core mechanisms: (1) Inter-task weight merging averages feature extractor weights across all previous task models using recursive moving average, while classifier weights are concatenated to preserve old class weights while adding new ones; (2) Intra-task weight merging periodically averages multiple checkpoints within the current task every e_a epochs to find flatter minima and improve generalization, requiring careful handling of BatchNorm statistics through forward passes rather than resetting; (3) Bounded update constrains weight displacement magnitude from the base model by projecting updates if they exceed threshold B every e_b epochs, preventing catastrophic forgetting by keeping new task optimization within the neighborhood of previous knowledge. The approach is designed as a plug-and-play enhancement that can be integrated with existing CIL methods without architectural modifications.

## Key Results
- M&B consistently improves average incremental accuracy across CIFAR-100 and ImageNet-100/1000 datasets
- Inter-task merging alone provides 2-3% gains over baseline methods like PODNet and LwF
- Intra-task merging with proper BatchNorm handling achieves 65.38% accuracy on CIFAR-100, but resetting statistics causes severe degradation to 25.60%
- Bounded updates transform task updates from negatively correlated to positively correlated, reducing forgetting from 18.72% to 15.38%

## Why This Works (Mechanism)

### Mechanism 1: Inter-task Weight Merging for Stability
- **Claim**: Averaging feature extractor weights across all previous task models creates a consolidated base model that preserves cumulative knowledge while serving as initialization for new tasks.
- **Mechanism**: Recursive moving average (Eq. 1): θ^{base}_{k+1} = (k-1)/k · θ^{base}_k + 1/k · θ_k. For classifiers, weight concatenation (Eq. 2) preserves old class weights while adding new ones.
- **Core assumption**: Models from different tasks occupy compatible regions of the loss landscape (same basin), making weight averaging meaningful rather than destructive.
- **Evidence anchors**: [abstract] "Inter-task weight merging unifies previous models by averaging the weights of models from all previous stages." Table 5 shows simple averaging outperforms EMA variants (65.38% vs 64.01-64.10%), with EMA causing more forgetting (19.12-18.28% vs 15.38%) due to recency bias.
- **Break condition**: If task weight updates become negatively correlated (Figure 3a shows this happens without M&B), averaging degrades rather than consolidates knowledge.

### Mechanism 2: Intra-task Weight Merging for Plasticity
- **Claim**: Averaging multiple checkpoints along the training trajectory within each task improves generalization on that task by finding flatter minima.
- **Mechanism**: Periodic averaging (Eq. 3): Θ^{avg}_k ← (n·Θ^{avg}_k + Θ_k)/(n+1) every e_a epochs. Final model replaced by averaged version before inter-task merging.
- **Core assumption**: Checkpoints within a task lie along a connected optimization path where averaging leads to wider optima with better generalization.
- **Evidence anchors**: [abstract] "intra-task weight merging facilitates the learning of current task by combining the model parameters within current stage." Table 6 shows robustness to averaging period (1-15 epochs yield 64.73-65.51% accuracy), but critical sensitivity to BatchNorm handling—resetting statistics causes collapse (25.60% accuracy).
- **Break condition**: Incorrect BatchNorm statistics computation breaks the mechanism entirely (Table 6: resetting statistics drops accuracy from 65.38% to 25.60%).

### Mechanism 3: Bounded Update for Knowledge Preservation
- **Claim**: Constraining weight displacement magnitude from the base model prevents catastrophic forgetting by keeping new task optimization within the neighborhood of previous knowledge.
- **Mechanism**: Projection-based constraint (Eq. 4): if ||ΔΘ|| > B, normalize to magnitude B. Applied every e_b epochs.
- **Core assumption**: The base model encodes previous task knowledge in its weight configuration; distance from this configuration correlates with forgetting.
- **Evidence anchors**: [abstract] "bounded update technique that aims to optimize the target model with minimal cumulative updates and preserve knowledge from previous tasks." Figure 3 shows M&B transforms task updates from uncorrelated/negative (3a) to positively correlated (3b), enabling stable knowledge integration. Table 3 shows removing bounded updates increases forgetting from 15.38% to 18.72%.
- **Break condition**: Bound too tight prevents adaptation (Table 7: threshold tradeoff exists between plasticity and stability).

## Foundational Learning

- **Concept: Catastrophic forgetting in sequential learning**
  - Why needed here: M&B specifically targets the stability-plasticity dilemma where learning new tasks degrades performance on previous ones.
  - Quick check question: Can you explain why standard SGD on new data destroys representations learned for old classes?

- **Concept: Weight averaging and loss basin geometry**
  - Why needed here: Inter/intra-task merging rely on models occupying compatible loss basins where averaging is beneficial.
  - Quick check question: Why does averaging weights of two models sometimes outperform either individual model?

- **Concept: BatchNorm statistics and feature distribution alignment**
  - Why needed here: Incorrect BN handling after weight merging causes severe degradation; understanding running statistics vs. weight values is critical.
  - Quick check question: After averaging model weights, why can't you directly use the averaged running statistics from the original models?

## Architecture Onboarding

- **Component map**: Base model M^{base}_k -> Working model M_k -> Intra-task averaged model M^{avg}_k -> Final output
- **Critical path**:
  1. Initialize task k from base model M^{base}_k
  2. Train with bounded updates (Eq. 4 every e_b epochs)
  3. Periodically merge checkpoints into M^{avg}_k (Eq. 3 every e_a epochs)
  4. After task completion: run forward pass for BN statistics on M^{avg}_k
  5. Update base model: feature extractor via Eq. 1, classifier via Eq. 2

- **Design tradeoffs**:
  - Larger bound B: more plasticity, less stability (Table 7)
  - More frequent averaging: minimal impact (Table 6), but more memory for checkpoint storage
  - EMA vs. simple averaging: EMA favors recent tasks, increases forgetting (Table 5)

- **Failure signatures**:
  - Accuracy collapse (~25%): BatchNorm statistics reset instead of recomputed
  - High forgetting (>18%): Bounded updates disabled or bound too large
  - Poor new task adaptation: Intra-task merging disabled or bound too tight

- **First 3 experiments**:
  1. **Sanity check**: Implement inter-task merging only on PODNet with CIFAR-100 (5 tasks); verify ~2-3% gain over baseline per Table 1.
  2. **Ablation stress test**: Run full M&B with BN statistics reset (R in Table 6) to confirm failure mode and validate implementation correctness via accuracy collapse.
  3. **Bound sensitivity sweep**: Test B ∈ {5, 10, 15} on 50-task CIFAR-100 to identify optimal threshold before scaling to ImageNet.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How can the Merge-and-Bound (M&B) strategy be adapted to maintain effectiveness when integrated with CIL methods that utilize dynamic architecture expansion or explicit model ensembling?
- **Basis in paper**: [inferred] The paper notes in the results section that performance gains appear "modest" when applying M&B to FOSTER, attributing this to FOSTER's design which exploits "enlarged model capacity and ensemble effects," potentially reducing the unique benefits of weight averaging.
- **Why unresolved**: The paper demonstrates compatibility with rehearsal, distillation, and virtual class methods, but the interaction with dynamic expansion techniques remains less effective, suggesting a potential ceiling on the "plug-and-play" capability for this specific subclass of CIL methods.
- **What evidence would resolve it**: An extension of the M&B algorithm that specifically accounts for variable model widths or branching factors, demonstrated by achieving significant accuracy gains over a baseline dynamic expansion method like FOSTER or DER.

### Open Question 2
- **Question**: Can the intra-task merging component be modified to handle architectures relying on Layer Normalization (e.g., Vision Transformers) as effectively as it handles Batch Normalization?
- **Basis in paper**: [inferred] The method description and ablation study explicitly detail a complex dependency on Batch Normalization (BN) statistics, noting that resetting running statistics causes severe degradation. They rely on the running statistics of the current model to avoid bias.
- **Why unresolved**: As the field moves toward Transformers which typically utilize Layer Normalization, the specific strategy of reusing current running statistics to avoid bias toward the current task may not transfer directly, as Layer Normalization computes statistics per sample rather than across the batch.
- **What evidence would resolve it**: Experiments applying M&B to a ViT backbone on ImageNet or CIFAR-100, accompanied by an analysis of how normalization statistics are aggregated during the intra-task merging phase without degrading performance.

### Open Question 3
- **Question**: Is there a theoretically grounded or adaptive mechanism to determine the optimal bounding threshold (B) for the bounded update strategy, rather than treating it as a fixed hyperparameter?
- **Basis in paper**: [inferred] The ablation study (Table 7) shows that the bounding threshold controls a trade-off between adaptivity (larger threshold) and stability. Currently, the paper treats this as a tunable hyperparameter.
- **Why unresolved**: A fixed threshold may be suboptimal across heterogeneous tasks where the complexity or distribution shift varies significantly; a static bound might be too loose for simple tasks or too tight for complex ones.
- **What evidence would resolve it**: A modified formulation where the bound is determined dynamically based on the gradient magnitude or the loss landscape of the incoming task, showing improved stability over the static threshold approach.

## Limitations
- Critical hyperparameters (e_a, e_b, B) are not explicitly specified in main text, only suggested through ablation tables
- Integration details with specific baseline methods lack specificity regarding loss functions and exemplar selection strategies
- Zero-exemplar scenario claims are mentioned but lack comprehensive experimental validation

## Confidence

**High confidence**: The core mechanism of inter-task weight merging works as described (Table 5 validation), and the catastrophic failure mode from incorrect BatchNorm handling is clearly demonstrated (Table 6).

**Medium confidence**: The bounded update mechanism's effectiveness is supported by Figure 3's correlation analysis and Table 3's forgetting metrics, but the theoretical justification for why distance-based projection preserves knowledge remains incomplete.

**Medium confidence**: Intra-task merging improves generalization as claimed, with Table 6 showing consistent gains across averaging periods, though the ablation doesn't test very frequent (e_a<1) or infrequent (e_a>15) settings.

## Next Checks

1. **Hyperparameter sensitivity**: Systematically test M&B with different combinations of (e_a, e_b, B) on CIFAR-100 to identify robust parameter ranges beyond the suggested defaults.

2. **Zero-exemplar scenario validation**: Conduct comprehensive experiments on CIFAR-100 with zero exemplars per class to verify the claimed effectiveness in this memory-constrained setting.

3. **Theoretical grounding**: Develop formal analysis connecting the bounded update constraint to knowledge preservation, potentially building on existing work on weight space geometry and catastrophic forgetting.