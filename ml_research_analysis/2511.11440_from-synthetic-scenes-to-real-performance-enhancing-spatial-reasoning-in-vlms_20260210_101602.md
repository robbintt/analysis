---
ver: rpa2
title: 'From Synthetic Scenes to Real Performance: Enhancing Spatial Reasoning in
  VLMs'
arxiv_id: '2511.11440'
source_url: https://arxiv.org/abs/2511.11440
tags:
- synthetic
- data
- coco
- fine-tuning
- spatial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates improving spatial reasoning in vision-language
  models (VLMs) by using controlled synthetic datasets for fine-tuning. The authors
  construct a balanced synthetic dataset with exhaustive sampling of object attributes
  (color, shape, size, position) and fine-tune several state-of-the-art VLMs.
---

# From Synthetic Scenes to Real Performance: Enhancing Spatial Reasoning in VLMs

## Quick Facts
- arXiv ID: 2511.11440
- Source URL: https://arxiv.org/abs/2511.11440
- Reference count: 40
- Primary result: Fine-tuning VLMs on balanced synthetic data significantly improves spatial reasoning, achieving near-perfect synthetic accuracy and over 20% gains on real-world COCO data.

## Executive Summary
This paper demonstrates that fine-tuning vision-language models on controlled synthetic datasets dramatically improves spatial reasoning capabilities, particularly for absolute position tasks. The authors construct a balanced synthetic dataset with exhaustive sampling of object attributes and show that this approach outperforms fine-tuning on large real-world datasets like COCO. The method achieves near-perfect accuracy on synthetic benchmarks and transfers successfully to real-world data, with gains of over 20% compared to base models. Notably, fine-tuning on large real-world datasets degrades performance, while smaller balanced subsets perform better, suggesting that quality and control in training data outweigh sheer quantity.

## Method Summary
The authors use CIVET to generate synthetic images with geometric shapes (circles, squares, triangles, stars) varying by color, shape, size, and position. They create exhaustive, balanced datasets where each attribute combination is uniformly represented. The Absolute Position task is formulated as VQA over a 3x3 grid. Models are fine-tuned using LoRA (rank=32, Î±=64) on attention Q/K/V matrices with early stopping. Four VLMs are evaluated: LLaVA-NeXT, Molmo, Qwen2-VL (encoder-decoder) and CLIP (dual-encoder). Performance is measured on both synthetic test sets and real-world COCO Absolute Position dataset.

## Key Results
- Fine-tuning on balanced synthetic data yields uniform performance across all 9 spatial positions, mitigating center-bias present in base models
- Models achieve near-perfect accuracy (>95%) on synthetic benchmarks and significant gains (>20%) on COCO transfer
- Fine-tuning on full COCO (161k samples) degrades performance to near-zero accuracy, while smaller balanced subsets perform better
- Moderate visual complexity (3 distractors) during synthetic training improves real-world transfer
- Encoder-decoder models (LLaVA, Molmo, Qwen) successfully transfer spatial skills; dual-encoder CLIP shows no meaningful transfer despite perfect synthetic accuracy

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Exhaustive, balanced attribute sampling disrupts learning of spurious positional correlations found in real-world data
- **Mechanism:** Real-world datasets contain strong center-biases and co-occurrence biases. Synthetic data with uniformly distributed, decorrelated attributes forces models to rely on geometric reasoning rather than statistical shortcuts
- **Core assumption:** Models possess inherent capacity for spatial reasoning but are misled by skewed training distributions
- **Evidence anchors:** Abstract states balanced synthetic data yields uniform performance and mitigates biases; section 1 notes models learn spurious cues from real data; SpaRE corroborates rarity of spatial relations in standard VL datasets

### Mechanism 2
- **Claim:** LoRA fine-tuning on clean synthetic data reallocates representational capacity to under-performing spatial regions
- **Mechanism:** Base models exhibit "spatial distortion" (collapsing center-left and center-right regions). Targeted LoRA updates attention weights to resolve geometric ambiguities without overwriting general visual knowledge
- **Core assumption:** Degradation in spatial reasoning is due to insufficient attention resolution rather than lack of geometric primitives
- **Evidence anchors:** Section 4 notes fine-tuning refines spatial predictions into coherent layouts; section 5 states it strengthens internal representation and transfers; Spatial-ViLT suggests enhancing spatial features aids reasoning

### Mechanism 3
- **Claim:** Moderate visual complexity during synthetic training acts as regularizer for real-world transfer
- **Mechanism:** While clean images are ideal for learning core concepts, real images are cluttered. Adding small number of synthetic distractors forces selective attention and object permanence, bridging domain gap
- **Core assumption:** Visual gap between synthetic shapes and real objects is primarily one of clutter and context, not just texture
- **Evidence anchors:** Section 5 shows moderate clutter improves COCO transfer; section 4 shows fine-tuning on full COCO drops performance to near-zero

## Foundational Learning

- **Concept: Vision-Language Model (VLM) Architectures**
  - **Why needed here:** Paper observes critical divergence where encoder-decoder models successfully transfer spatial skills while dual-encoder models do not
  - **Quick check question:** Does the model use generative LLM decoder to output coordinates (Encoder-Decoder), or does it match image embeddings to text embeddings (Dual-Encoder)?

- **Concept: Data Distributional Bias vs. Spurious Correlations**
  - **Why needed here:** Core premise is that standard datasets teach models "hacks" (e.g., pedestrians usually central)
  - **Quick check question:** If 80% of training images have object in center, will model learn "what is the object?" or "the object is always center"?

- **Concept: Sim-to-Real Domain Adaptation**
  - **Why needed here:** Paper claims "unmatched" performance (Synthetic train -> Real test)
  - **Quick check question:** Does synthetic training distribution cover edge cases of real distribution (e.g., occlusion, odd angles), or just ideal case?

## Architecture Onboarding

- **Component map:** CIVET Data Generator -> VLM Backbone -> LoRA Adaptation Module -> Task Interface (3x3 grid VQA)

- **Critical path:**
  1. **Data Balancing:** Ensure synthetic dataset has exactly uniform distribution across 9 grid positions
  2. **Prompt Engineering:** Output verbosity hinders evaluation; prompt must strictly constrain output tokens
  3. **Early Stopping:** Use validation set to stop training before overfitting to synthetic aesthetic

- **Design tradeoffs:**
  - **Real vs. Synthetic Data:** Real is noisy and biased (requires massive cleanup); Synthetic is clean but lacks texture (requires domain adaptation)
  - **Dataset Scale:** Paper shows *less data is more* (1.3k balanced > 161k raw). Prioritize quality/control over volume
  - **Distractor Count:** 3 distractors is "sweet spot" for transfer; 0 is too simple; 5 is too noisy

- **Failure signatures:**
  - **Mode Collapse on Real Data:** Fine-tuning on full COCO leads to empty strings or repetitive loops
  - **Central Bias:** If model predicts "center" for >50% of edge cases, synthetic balancing failed
  - **CLIP Invariance:** Dual-encoder models show almost no transfer from synthetic spatial training

- **First 3 experiments:**
  1. **Baseline Bias Probe:** Evaluate base VLM on synthetic 3x3 grid to visualize "Spatial Prediction" map and confirm center biases
  2. **Data Scaling Ablation:** Fine-tune on increasing subsets of balanced synthetic data (1%, 5%, 100%) to verify "plateau" effect
  3. **Transfer Check:** Train on Synthetic (balanced), test on Real (COCO). Compare against training on Real (unbalanced)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can benefits of controlled synthetic fine-tuning extend to more complex reasoning dimensions like relational, causal, and temporal understanding?
- **Basis in paper:** Conclusion states future work should "investigate how controlled stimuli can be extended to other reasoning dimensions"
- **Why unresolved:** Current study restricted to Absolute Position task; authors haven't validated if gains hold for non-spatial or temporal tasks
- **What evidence would resolve it:** Applying same exhaustive, balanced synthetic protocol to action recognition or causal inference tasks

### Open Question 2
- **Question:** How can targeted synthetic fine-tuning be effectively integrated with large-scale pretraining pipelines?
- **Basis in paper:** Conclusion explicitly asks "how such targeted fine-tuning might complement large-scale pretraining"
- **Why unresolved:** Paper focuses on post-hoc fine-tuning on already pre-trained models; doesn't explore incorporating controlled data during initial pre-training
- **What evidence would resolve it:** Experiments comparing models pre-trained with synthetic controlled data interleaved with real data versus post-hoc fine-tuned models

### Open Question 3
- **Question:** Why do dual-encoder architectures (like CLIP) fail to transfer spatial reasoning learned from synthetic data to real-world scenes?
- **Basis in paper:** Results show CLIP achieves 100% synthetic accuracy but 0% gain on real-world transfer; authors note this "suggests a limitation of dual-encoder models" but don't propose solution
- **Why unresolved:** Paper identifies failure mode but doesn't investigate if it's due to lack of LLM decoder, contrastive loss function, or specific projection layers
- **What evidence would resolve it:** Ablation studies on projection mechanism or inclusion of generation head on CLIP

### Open Question 4
- **Question:** What specific characteristics of large-scale real-world datasets (noise vs. bias vs. imbalance) cause observed catastrophic performance collapse?
- **Basis in paper:** Section 4.A notes fine-tuning on full COCO drops performance to near zero, suggesting "noise and bias" but doesn't specify mechanism
- **Why unresolved:** Paper demonstrates phenomenon but defers detailed breakdown of why massive real data degrades spatial reasoning
- **What evidence would resolve it:** Decomposing "full COCO" experiment by systematically introducing noise, imbalance, and distribution shifts separately

## Limitations
- Narrow scope focused exclusively on absolute position detection within 3x3 grid, representing simplified subset of spatial reasoning capabilities
- Synthetic dataset's simplicity (basic geometric shapes on uniform backgrounds) may not prepare models for visual complexity and semantic richness of real-world scenes
- Findings may be specific to encoder-decoder architectures, as dual-encoder CLIP shows no meaningful transfer despite perfect synthetic accuracy

## Confidence

**High Confidence Claims:**
- Fine-tuning on balanced synthetic data significantly improves spatial reasoning compared to real-world datasets
- Improvements most pronounced in spatial positions where models previously showed weakest performance
- Moderate visual complexity (3 distractors) during synthetic training improves real-world transfer
- Fine-tuning on large real-world datasets degrades spatial reasoning compared to smaller balanced subsets

**Medium Confidence Claims:**
- Disrupting spurious positional correlations through balanced attribute sampling is primary driver of improvements
- LoRA fine-tuning specifically targets spatial reasoning without degrading general visual knowledge
- Architectural differences between encoder-decoder and dual-encoder models fully explain differential transfer performance

**Low Confidence Claims:**
- Synthetic fine-tuning approach would generalize equally well to other spatial reasoning tasks beyond absolute position detection
- Optimal number of distractors (3) would remain optimal across different model architectures or task complexities
- Observed performance gains would scale linearly with additional synthetic training data beyond tested ranges

## Next Checks

1. **Architectural Transferability Test:** Evaluate whether synthetic fine-tuning approach improves spatial reasoning in dual-encoder models when combined with architectural modifications like explicit spatial coordinate features or different fine-tuning strategies beyond LoRA.

2. **Task Complexity Expansion:** Test synthetic fine-tuning approach on more complex spatial reasoning tasks including relative positioning ("to the left of"), occlusion reasoning, and multi-object spatial relationships to determine if benefits generalize beyond absolute position detection.

3. **Real-World Robustness Evaluation:** Assess model performance on real-world datasets with varying levels of visual complexity, object occlusion, and contextual richness to determine if synthetic training creates models overly specialized to clean, controlled environments.