---
ver: rpa2
title: Rate optimal learning of equilibria from data
arxiv_id: '2510.09325'
source_url: https://arxiv.org/abs/2510.09325
tags:
- nash
- learning
- policy
- player
- algorithm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper closes theoretical gaps in Multi-Agent Imitation Learning\
  \ (MAIL) by establishing fundamental limits and introducing the first near-optimal\
  \ interactive algorithm. The key contributions are: A lower bound proving that non-interactive\
  \ MAIL requires O(Cmax/\u03B5\xB2) samples, where Cmax is the all-policy deviation\
  \ concentrability coefficient, and that Behavior Cloning is rate-optimal in this\
  \ setting A new interactive algorithm MAIL-WARM that achieves O(\u03B5\u207B\xB2\
  ) sample complexity, matching the lower bound and improving upon the previous best\
  \ O(\u03B5\u207B\u2078) by Freihaut et al."
---

# Rate optimal learning of equilibria from data

## Quick Facts
- arXiv ID: 2510.09325
- Source URL: https://arxiv.org/abs/2510.09325
- Authors: Till Freihaut; Luca Viano; Emanuele Nevali; Volkan Cevher; Matthieu Geist; Giorgia Ramponi
- Reference count: 40
- Primary result: Establishes fundamental limits and introduces first near-optimal interactive algorithm for Multi-Agent Imitation Learning

## Executive Summary
This paper resolves key theoretical gaps in Multi-Agent Imitation Learning (MAIL) by proving that non-interactive MAIL requires a stronger concentrability coefficient (Cmax) than previously thought, and that Behavior Cloning is rate-optimal in this setting. The authors introduce MAIL-WARM, the first interactive algorithm achieving O(ε⁻²) sample complexity, matching the lower bound and improving upon the previous O(ε⁻⁸) bound. The core innovation combines reward-free exploration with MAIL: first explore the MDP induced by fixing one expert policy to generate well-covered datasets, then apply behavioral cloning.

## Method Summary
The method combines reward-free reinforcement learning with interactive MAIL. First, fix one expert policy to induce a single-agent MDP, then use reward-free RL (EULER) to explore all δ-significant states. This generates well-covered datasets for behavioral cloning. The algorithm alternates between exploration and expert querying, avoiding concentrability coefficients while maintaining optimal sample complexity. Theoretical analysis shows the Nash gap decomposes into total variation errors weighted by occupancy measures, enabling the O(ε⁻²) guarantee.

## Key Results
1. Lower bound proving non-interactive MAIL requires O(Cmax/ε²) samples and Behavior Cloning is rate-optimal
2. New algorithm MAIL-WARM achieves O(ε⁻²) sample complexity, improving previous O(ε⁻⁸) bound
3. Empirical validation shows MAIL-WARM outperforms other methods in Gridworld environments where Behavior Cloning fails

## Why This Works (Mechanism)

### Mechanism 1: Lower Bound via Irrational Deviations
- Claim: Non-interactive MAIL cannot succeed with only bounded C(µE, νE); it requires bounded Cmax.
- Mechanism: The concentrability coefficient C(µE, νE) only covers states reachable by rational deviations (best responses against Nash equilibrium). However, an opponent can deviate "irrationaly" to states never visited under equilibrium play. If those states aren't covered in the dataset, the learner's policy can be exploited there, causing unbounded Nash gap.
- Core assumption: Expert demonstrations come from a Nash equilibrium profile (µE, νE); opponent can deviate to any reachable state.
- Evidence anchors:
  - [abstract] "we prove a statistical lower bound that identifies the all-policy deviation concentrability coefficient as the fundamental complexity measure"
  - [section 4] "bounded C(µE, νE) only guarantees that all states that can be visited by rational deviations have positive probability under the dataset distribution ρ. However, we show that the opponent could exploit the output of a non-interactive MAIL algorithm deviating irrationaly"
  - [corpus] Prior work (Freihaut et al. 2025) established C(µE, νE) dependence; this paper sharpens to Cmax.
- Break condition: If the opponent is constrained to only rational deviations, C(µE, νE) suffices; if the dataset has full support over all reachable states, Cmax is bounded.

### Mechanism 2: Reward-Free Exploration Eliminates Concentrability Dependence
- Claim: Interactive MAIL with a reward-free warm-up phase achieves O(ε⁻²) sample complexity independent of concentrability coefficients.
- Mechanism: Fix one expert policy (e.g., νE), inducing a single-agent MDP M^νE. Use reward-free RL (EULER) to explore all δ-significant states in this MDP. This generates a dataset D^νE with guaranteed coverage: max_{a,h} d^{µ,νE}_h(s,a)/p^{νE}_h(s,a) ≤ 2SAH for all policies µ. Behavioral cloning on this well-covered dataset avoids the concentrability coefficient entirely.
- Core assumption: Access to a queriable expert; the induced MDP has finite state/action spaces; EULER's regret guarantees hold.
- Evidence anchors:
  - [abstract] "we introduce a framework that combines reward-free reinforcement learning with interactive MAIL and instantiate it with an algorithm, MAIL-WARM. It improves the best previously known sample complexity from O(ε⁻⁸) to O(ε⁻²)"
  - [section 5, Algorithm 1] Explicit pseudocode showing reward-free warm-up phase (lines 2-16) followed by BC (lines 19-20)
  - [corpus] Weak corpus signal; related work on sample complexity exists but doesn't address this specific MAIL mechanism.
- Break condition: If expert cannot be queried, or if induced MDP is non-tabular without function approximation guarantees, coverage guarantee may fail.

### Mechanism 3: Exploitability Decomposition Enables BC Analysis
- Claim: The Nash gap decomposes into total variation errors between estimated and expert policies, weighted by occupancy measures under best-response deviations.
- Mechanism: Lemma 6.1 shows Nash-Gap(µ̂,ν̂) ≤ 2H Σ_h Σ_π Err_h(π), where Err_h(ν̂) = max_{µ∈br(ν̂)} E_{s∼d^{µ,νE}_h}[TV(ν^E_h, ν̂_h)(s)]. The reward-free phase provides a sampling distribution p^{νE}_h that covers all δ-significant states. A change-of-measure argument bounds Err_h by O(SAH · √(SB log S/N)), yielding O(ε⁻²) sample complexity.
- Core assumption: The exploitability decomposition is tight; concentration inequalities for log-loss minimization hold.
- Evidence anchors:
  - [section 6.2, Lemma 6.1] "D ⟨d0, V^{µ⋆,ν̂} − V^{µ̂,ν⋆}⟩ ≤ 2H Σ_h Σ_{π∈{μ̂,ν̂}} Err_h(π)"
  - [section 6.2] "Using the coverage property... Err_h(ν̂; S^{νE}_{δ,h}) ≤ 2SAH E_{s∼p^{νE}_h}[TV(ν^E_h, ν̂_h)(s)]"
  - [corpus] No direct corpus evidence for this decomposition in MAIL context.
- Break condition: If best-response policies visit states not covered by p^{νE}_h (i.e., not δ-significant), the error from non-significant states (Err_h(ν̂; S̄^{νE}_{δ,h})) may dominate.

## Foundational Learning

- Concept: Concentrability coefficients (C(µ,ν), Cmax)
  - Why needed here: Quantify how well the expert dataset covers states reachable under deviations. The paper shows Cmax is the fundamental barrier in non-interactive MAIL.
  - Quick check question: Can you compute max_h ||d^{µ,ν}_h / ρ_h||_∞ for a given policy pair (µ,ν) and dataset distribution ρ?

- Concept: Reward-free reinforcement learning
  - Why needed here: MAIL-WARM uses reward-free exploration to generate well-covered datasets without knowing the true reward function, enabling subsequent BC without concentrability dependence.
  - Quick check question: Given an MDP, can you design an exploration strategy that visits all states with probability proportional to their reachability under any policy?

- Concept: Nash equilibrium and Nash gap in zero-sum Markov games
  - Why needed here: The objective is to learn policies (μ̂,ν̂) with small Nash gap; understanding best-response sets and equilibrium properties is essential for the decomposition.
  - Quick check question: For a two-player zero-sum game, can you compute the Nash gap for a given policy pair?

## Architecture Onboarding

- Component map: EULER exploration -> Dataset construction -> Behavioral cloning -> Nash estimation
- Critical path:
  1. Fix expert νE → define induced MDP M^{νE}
  2. Run EULER for each (s,h) ∈ S × [H] → collect policies in Ψ^{νE}
  3. Sample N policies from Ψ^{νE}, collect trajectories with νE → D^{νE}
  4. Repeat for µE → D^{µE}
  5. BC on D^{µE} → μ̂, BC on D^{νE} → ν̂
- Design tradeoffs:
  - N0 (EULER iterations per RL problem) vs coverage quality: larger N0 improves coverage but increases warm-up cost
  - N (dataset size) vs BC accuracy: O(ε⁻²) samples needed for ε-approximate NE
  - δ (significance threshold) vs error from non-significant states: smaller δ reduces Err_h(·; S̄_{δ,h}) but increases coverage burden
- Failure signatures:
  - Non-interactive setting with unbounded Cmax: BC fails with constant Nash gap (Corollary 3.1)
  - Insufficient N0: coverage bound in Theorem 6.1 fails, concentrability-independent guarantee lost
  - Non-tabular environments without function approximation: EULER guarantees don't transfer directly
- First 3 experiments:
  1. Lower-bound Markov game (3-state construction): Vary ρ(s3) to test BC degradation as Cmax increases; verify MAIL-WARM succeeds where BC fails
  2. Gridworld with pure NE expert: Compare MAIL-WARM, MURMAIL, BC; confirm MAIL-WARM achieves lower Nash gap with fewer queries
  3. Gridworld with mixed NE expert (convex combination): Test whether improved dataset coverage helps BC; verify MAIL-WARM remains independent of coverage

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the rate-optimal results for Multi-Agent Imitation Learning (MAIL) be extended to the infinite-horizon discounted setting?
- Basis in paper: [explicit] The Conclusion states that extending results to the infinite-horizon case is "non-trivial" and remains an "open challenge."
- Why unresolved: The MAIL-WARM algorithm relies on the finite-horizon regret guarantees of the EULER algorithm, which do not transfer directly to infinite horizons.
- What evidence would resolve it: A derivation of the sample complexity for the infinite-horizon regime or a proof that the finite-horizon bounds transfer effectively.

### Open Question 2
- Question: Can the sample complexity guarantees be tightened with respect to the game parameters $S$, $A$, and $H$ to match information-theoretic lower bounds?
- Basis in paper: [explicit] The Conclusion notes that while the $\epsilon$-dependence is optimal, "optimal guarantees with respect to other problem parameters $S, A, H$ remain unknown."
- Why unresolved: There is a gap between the polynomial factors of the upper bounds provided for MAIL-WARM and the lower bounds.
- What evidence would resolve it: An algorithm that achieves the optimal dependence on $S$, $A$, and $H$ alongside the optimal $\varepsilon^{-2}$ rate.

### Open Question 3
- Question: Is there an efficient method for calculating the all-policy deviation concentrability coefficient $C_{max}$ in standard environments?
- Basis in paper: [explicit] The Conclusion identifies "developing efficient methods for calculating [C_max]" as a valuable future direction.
- Why unresolved: While $C_{max}$ is identified as the fundamental hardness measure for non-interactive MAIL, calculating it is currently challenging even in simple Gridworld environments.
- What evidence would resolve it: A computationally tractable algorithm to estimate $C_{max}$ or a theoretical mapping of $C_{max}$ for common benchmarks.

### Open Question 4
- Question: How can the MAIL-WARM framework be extended to non-tabular settings, such as those requiring deep neural networks?
- Basis in paper: [explicit] The Conclusion lists extending results to the "non-tabular case" and providing "deep MAIL algorithms" as a necessary next step.
- Why unresolved: The current theoretical analysis and algorithm design are restricted to finite state and action spaces (tabular settings).
- What evidence would resolve it: A version of MAIL-WARM utilizing function approximation with provable sample complexity guarantees in continuous or large-scale spaces.

## Limitations

- The concentrability lower bound relies on a specific 3-state Markov game construction, requiring additional assumptions for generalization
- The reward-free exploration mechanism's coverage guarantees need empirical validation in interactive MAIL settings
- With only two Gridworld experiments and no comparison to the prior O(ε⁻⁸) baseline, practical significance remains partially unverified

## Confidence

- Lower bound and non-interactive hardness: High
- MAIL-WARM sample complexity: Medium
- Empirical validation: Low

## Next Checks

1. Test MAIL-WARM on larger MDPs with continuous state spaces or function approximation to verify coverage guarantees beyond tabular settings
2. Implement and compare against the O(ε⁻⁸) algorithm from Freihaut et al. [2025] on the same benchmarks to quantify practical improvements
3. Vary the significance threshold δ systematically to study its impact on the balance between coverage quality and computational cost