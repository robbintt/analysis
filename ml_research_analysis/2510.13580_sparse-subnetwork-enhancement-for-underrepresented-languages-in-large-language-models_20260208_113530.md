---
ver: rpa2
title: Sparse Subnetwork Enhancement for Underrepresented Languages in Large Language
  Models
arxiv_id: '2510.13580'
source_url: https://arxiv.org/abs/2510.13580
tags:
- languages
- language
- fine-tuning
- across
- subnetwork
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of uneven performance of large
  language models (LLMs) across different languages, especially for low- and mid-resource
  languages. The core idea is to identify and fine-tune only the sparse, language-specific
  subnetworks within the model, rather than the entire model, using a metric called
  Language Activation Probability Entropy (LAPE).
---

# Sparse Subnetwork Enhancement for Underrepresented Languages in Large Language Models

## Quick Facts
- arXiv ID: 2510.13580
- Source URL: https://arxiv.org/abs/2510.13580
- Reference count: 40
- Core method: Language-specific neurons identified via LAPE entropy scores, then fine-tuned while freezing 99%+ of parameters

## Executive Summary
This work addresses the challenge of uneven performance of large language models (LLMs) across different languages, especially for low- and mid-resource languages. The core idea is to identify and fine-tune only the sparse, language-specific subnetworks within the model, rather than the entire model, using a metric called Language Activation Probability Entropy (LAPE). This targeted approach updates as little as 0.2-1% of model parameters. Experiments across 12 underrepresented languages on models like Llama-3.1-8B and Mistral-Nemo-12B show that this method consistently outperforms full fine-tuning, parameter-efficient methods (LoRA, IA3), and random subnetwork baselines, while preserving the model's general capabilities. Additionally, this approach improves cross-lingual representational alignment and concentrates weight updates in specific model components, revealing key insights into language adaptation mechanisms.

## Method Summary
The method identifies language-specific neurons in transformer feed-forward networks using Language Activation Probability Entropy (LAPE), which measures the uniformity of a neuron's activation distribution across languages. For each target language, neurons with the lowest LAPE scores (bottom 5th percentile) and high activation probability (95th percentile threshold) are selected. Only the weights associated with these neurons (gate, up, and down projections) are fine-tuned while all other parameters remain frozen. This selective fine-tuning uses 1 epoch of training with up to 100M tokens per language, achieving performance improvements while updating just 0.2-1% of parameters and preserving general capabilities.

## Key Results
- Targeted sparse fine-tuning consistently outperforms full fine-tuning, LoRA, IA3, and random subnetwork baselines across 12 underrepresented languages
- Updates only 0.2-1% of model parameters while improving target-language performance and preserving general capabilities (no catastrophic forgetting)
- Weight changes concentrate in down-projection weights (3-5× larger than gate/up projections), particularly in later layers where language-specific neurons are most concentrated
- Improves cross-lingual representational alignment, achieving ~90.5 average cosine similarity vs ~87 for LoRA

## Why This Works (Mechanism)

### Mechanism 1
Low-entropy neurons selectively activate for specific languages and constitute a minimal subnetwork sufficient for language adaptation. LAPE computes per-neuron activation probability distributions across languages via Eq. 1-3, then selects neurons in the lowest ρ-th percentile (ρ=5%) with activation above threshold τ_act. Fine-tuning is restricted to weights associated with these neurons only. Core assumption: Language selectivity is localizable to specific neurons rather than distributed; monolingual corpus activations accurately reflect such selectivity. Evidence: [abstract] "identifies language-specific neurons using Language Activation Probability Entropy (LAPE), an information-theoretic metric that reliably captures language-specific activation patterns"; [section 3.1] Equations 1-3 define pk,ℓ,i and LAPE; selection criteria ρ=5%, τ_act at 95th percentile; [corpus] "Language-specific Neurons Do Not Facilitate Cross-Lingual Transfer" reports limited gains using LAPE-identified neurons with LoRA modules rather than direct weight fine-tuning, suggesting the adaptation method matters—corpus evidence is mixed, not confirmatory. Break condition: If neuron overlap across languages is too high or if entropy distributions don't separate cleanly, LAPE may misattribute shared features as language-specific.

### Mechanism 2
Updating 0.2-1% of parameters via language-specific subnetworks improves target-language performance while fully preserving general capabilities (no catastrophic forgetting). Freezing all parameters except θ_i^(ℓ) = {W_gate[i,:], W_up[i,:], W_down[:,i]} for neurons i ∈ S_k (Eq. 5-6). Optimization minimizes language modeling loss on target-language data only (Eq. 7). Core assumption: Language knowledge is modular enough that selective updates don't interfere with unrelated capabilities; the identified subnetwork has sufficient capacity for the target language. Evidence: [abstract] "updates only 0.2-1% of model parameters" while "consistently outperforms full fine-tuning, FFN-only fine-tuning, LoRA, IA3, and random-subset baselines"; [section 5.2.1, Table 2] Target sparse fine-tuning: monolingual avg 33.02, general avg 63.01 (vs base 63.08). Full fine-tuning: monolingual 11.88, general 25.50—catastrophic forgetting; [corpus] "Reinforcement Learning Fine-Tunes a Sparse Subnetwork" corroborates that RL updates concentrate in a sparse subnetwork, but corpus evidence for language adaptation specifically is limited. Break condition: If target language requires more capacity than allocated subnetwork provides (e.g., Georgian shows -1.1 decline for Llama), or if subnetwork is misidentified (random subset performs worse: monolingual 29.89).

### Mechanism 3
Down-projection weights in later layers undergo 3-5× larger updates than gate/up projections, suggesting they serve as the primary site of linguistic "re-wiring." In SwiGLU FFNs (Eq. 4), gate/up jointly create gated feature space; down_proj synthesizes features into output representations. Targeted adaptation shifts output token distributions via down_proj changes while preserving feature extraction. Core assumption: Functional roles implied by SwiGLU architecture hold during fine-tuning; down_proj changes directly correlate with language adaptation rather than being epiphenomenal. Evidence: [section 6.1, Figure 5] "down_proj exhibits substantially larger weight changes, particularly in later layers, where language-specific neurons are most concentrated"; [section 6.1] "weight changes for gate_proj and up_proj remain small and tightly clustered around zero across all layers"; [corpus] Sparse autoencoder work suggests language-specific concepts are distributed; corpus evidence doesn't directly address FFN component asymmetry—insufficient external validation. Break condition: If down_proj changes reflect regularization effects rather than language acquisition, or if component-wise ablations (Appendix F, Table 39) show inconsistent patterns—fine-tuning UP+DOWN yields 35.83 avg vs full subnetwork 36.23, suggesting interaction effects.

## Foundational Learning

- **Concept: Feed-Forward Network (FFN) in Transformers**
  - Why needed here: The entire method operates on GLU-variant FFNs (gate/up/down projections). Understanding Eq. 4 is prerequisite to interpreting weight change analysis.
  - Quick check question: Can you explain why down_proj[:,i] is column-indexed while gate_proj[i,:] is row-indexed in Eq. 5?

- **Concept: Catastrophic Forgetting**
  - Why needed here: Full fine-tuning degrades general capabilities (Table 2: MMLU drops from 58.07 to 22.32). Understanding this motivates sparse approaches.
  - Quick check question: Why does freezing most parameters prevent forgetting, and when might it underfit?

- **Concept: Shannon Entropy for Selectivity**
  - Why needed here: LAPE (Eq. 3) uses entropy to quantify language specificity. Low entropy = concentrated activation = language-specific.
  - Quick check question: A neuron activating equally across 12 languages has what LAPE score? (Answer: log₂(12) ≈ 3.58, maximum entropy for K=12)

## Architecture Onboarding

- **Component map:**
Input → [Attention blocks: FROZEN] → Residual
      → [FFN block per layer]
          ├─ gate_proj (dff × d) → SiLU
          ├─ up_proj (dff × d) ──────→ ⊙ (element-wise)
          └─ down_proj (d × dff) ←─────┘
      → [LAPE-selected neuron indices S_k,(ℓ) per layer]
      → Trainable: rows of gate/up, columns of down for i ∈ S_k,(ℓ)

- **Critical path:**
  1. Forward pass on monolingual corpus (100MB recommended)
  2. Compute pk,ℓ,i per neuron per language via Eq. 1
  3. Apply τ_act threshold, compute LAPE via Eq. 3
  4. Select bottom ρ=5% per layer → S_k
  5. Mask gradients: zero for non-S_k parameters
  6. Fine-tune on target data (up to 100M tokens, 1 epoch)

- **Design tradeoffs:**
  - Lower ρ → fewer neurons → faster training, risk of under-capacity (Georgian: 1150 neurons, -1.1 decline)
  - Higher ρ → more capacity → slower, risk of interference
  - Threshold τ_act too low → include inactive neurons; too high → miss valid language-specific units
  - Training data: 50M-200M tokens tested; some languages saturate early (Maltese), others continue improving

- **Failure signatures:**
  - Validation loss doesn't converge below random subset baseline → check neuron selection pipeline
  - General capabilities drop >1% → subnetwork too large or contaminated with non-language neurons
  - Target language shows negative improvement → subnetwork capacity insufficient (try increasing ρ) or data quality issue

- **First 3 experiments:**
  1. **Reproduce Afrikaans subnetwork (2581 neurons, 0.39% params):** Verify LAPE computation matches paper; confirm training dynamics in Figure 13a (target should converge below LoRA/Full baselines).
  2. **Ablate FFN components:** Test UP-only, DOWN-only, UP+DOWN (Table 39). If DOWN-only ≈ UP+DOWN, down_proj may be sufficient for your use case.
  3. **Cross-lingual alignment probe:** Compute cosine similarity between parallel sentences pre/post fine-tuning (Section 6.2). Targeted sparse FT should achieve ~90.5 avg similarity vs ~87 for LoRA—if not, subnetwork selection may be misconfigured.

## Open Questions the Paper Calls Out

- **How can subnetwork size and composition be adaptively determined based on language typology, data availability, and model scale, rather than using fixed thresholds?**
  - Basis in paper: [explicit] The Conclusion lists "adaptively determining subnetwork size and composition... instead of fixed thresholds" as a key direction.
  - Why unresolved: The current study uses a fixed selection threshold (ρ=5%), which resulted in variable performance and occasional degradation (e.g., for Georgian), suggesting the fixed capacity is suboptimal for all languages.
  - What evidence would resolve it: A study dynamically tuning ρ per language based on corpus size or linguistic features to demonstrate consistent positive deltas across all target languages.

- **Does the identified sparse subnetwork adaptation mechanism generalize effectively to extremely low-resource languages with minimal available training data?**
  - Basis in paper: [explicit] Appendix A states, "How well the method scales to extremely low-resource settings... remains an open question."
  - Why unresolved: The experiments focused on mid- and low-resource languages with access to 50M-200M tokens; it is unclear if the LAPE-based neuron identification remains robust when data is scarce.
  - What evidence would resolve it: Evaluating the framework on languages with orders of magnitude less training data (e.g., <1M tokens) and analyzing neuron identification stability.

- **What is the interaction between subnetwork capacity (parameter count) and training data volume in determining performance saturation points?**
  - Basis in paper: [explicit] Appendix A notes that "the interaction between subnetwork size and available training data warrants further study."
  - Why unresolved: Section 5.2.4 observed that increasing data caused performance to drop for Lithuanian and Maltese, suggesting the identified subnetworks may lack the capacity to utilize additional data effectively.
  - What evidence would resolve it: A factorial experiment simultaneously varying subnetwork size (ρ) and training token counts to map the optimal capacity-to-data ratio.

## Limitations

- The fundamental assumption that language selectivity can be reliably captured by per-neuron activation distributions computed from monolingual corpora may not hold universally, as corpus evidence suggests LAPE-identified neurons don't necessarily facilitate cross-lingual transfer
- The ρ=5% threshold was chosen empirically without systematic exploration of alternatives, and the 95th percentile activation threshold τ_act isn't fully specified in terms of computation method
- The weight change analysis revealing down-projection dominance is observational rather than causal, lacking external validation of whether these changes are necessary or merely correlated with language adaptation

## Confidence

- **High confidence:** The catastrophic forgetting phenomenon (full fine-tuning degrades general capabilities) is well-established and consistently demonstrated across multiple experiments. The general methodology of sparse fine-tuning preserving capabilities while improving target languages has strong empirical support.
- **Medium confidence:** The LAPE-based neuron selection method works reliably for most languages in the tested set, showing consistent improvements over baselines. However, the mixed corpus evidence and the Georgian outlier suggest this method may not be universally robust across all language families or morphological complexities.
- **Low confidence:** The claim about down-projection weights being the primary site of linguistic adaptation is observational rather than causal. The functional significance of this asymmetry needs further validation through component-wise ablations and alternative explanations (e.g., regularization effects).

## Next Checks

1. **Ablation study on neuron selection thresholds:** Systematically vary ρ from 1% to 20% and τ_act from 90th to 99th percentile across multiple languages to identify optimal selection criteria and understand sensitivity. This would address whether the 5%/95th percentile combination is optimal or merely convenient.

2. **Cross-linguistic transferability test:** Take neurons selected for one language (e.g., Afrikaans) and evaluate their effectiveness when applied to a related language (e.g., Dutch) without re-selection. This would test whether LAPE-identified neurons capture genuine language-specific patterns or corpus artifacts.

3. **Component-wise causal intervention:** Using activation patching or similar techniques, selectively restore original weights for gate_proj, up_proj, or down_proj separately in the fine-tuned model to determine which component changes are actually necessary for performance improvements, testing the down-projection dominance hypothesis causally.