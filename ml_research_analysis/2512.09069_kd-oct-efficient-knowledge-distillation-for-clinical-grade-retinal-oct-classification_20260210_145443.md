---
ver: rpa2
title: 'KD-OCT: Efficient Knowledge Distillation for Clinical-Grade Retinal OCT Classification'
arxiv_id: '2512.09069'
source_url: https://arxiv.org/abs/2512.09069
tags:
- retinal
- teacher
- classification
- kd-oct
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study presents KD-OCT, a knowledge distillation framework\
  \ that compresses a high-performance ConvNeXtV2-Large teacher model\u2014enhanced\
  \ with advanced augmentations, focal loss, and stochastic weight averaging\u2014\
  into a lightweight EfficientNet-B2 student for classifying normal, drusen, and choroidal\
  \ neovascularization in retinal OCT images. By employing real-time distillation\
  \ with a combined loss balancing soft teacher knowledge and hard ground-truth supervision,\
  \ KD-OCT achieves near-teacher accuracy (~92-98%) while reducing model size by 25.5\xD7\
  \ (from 196.4M to 7.7M parameters), enabling efficient edge deployment for AMD screening."
---

# KD-OCT: Efficient Knowledge Distillation for Clinical-Grade Retinal OCT Classification

## Quick Facts
- arXiv ID: 2512.09069
- Source URL: https://arxiv.org/abs/2512.09069
- Reference count: 0
- Primary result: Compresses ConvNeXtV2-Large teacher to EfficientNet-B2 student with 25.5× reduction while maintaining ~92-98% of teacher accuracy for AMD classification

## Executive Summary
KD-OCT presents a knowledge distillation framework that compresses a high-performance ConvNeXtV2-Large teacher model into a lightweight EfficientNet-B2 student for classifying normal, drusen, and choroidal neovascularization in retinal OCT images. The framework achieves near-teacher accuracy (~92-98%) while reducing model size by 25.5× (from 196.4M to 7.7M parameters), enabling efficient edge deployment for AMD screening. Evaluated on the Noor Eye Hospital and UCSD datasets using patient-level cross-validation, the student model surpasses multi-scale and feature-fusion baselines in efficiency-accuracy trade-off, demonstrating robust generalization across diverse clinical environments.

## Method Summary
The framework trains a teacher ConvNeXtV2-Large model with focal loss, stochastic weight averaging, and heavy augmentations, then freezes it to perform real-time knowledge distillation to an EfficientNet-B2 student. The combined loss balances soft teacher knowledge transfer (0.7 weight, temperature 4.0) with hard ground-truth supervision (0.3 weight). The student uses lighter augmentations and is trained with real-time teacher inference, avoiding offline logit pre-computation. Both models are evaluated on retinal OCT datasets using patient-level cross-validation to prevent data leakage.

## Key Results
- Achieves 92.46% accuracy on Noor Eye Hospital dataset with 25.5× parameter reduction (196.4M → 7.7M)
- Outperforms multi-scale and feature-fusion baselines in efficiency-accuracy trade-off
- Demonstrates robust generalization across NEH and UCSD datasets
- Enables edge deployment for AMD screening while maintaining clinical-grade performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combined loss balancing soft teacher knowledge and hard ground-truth supervision enables efficient compression without significant accuracy loss.
- Mechanism: The student learns from two sources simultaneously: (1) temperature-scaled KL divergence against teacher soft labels reveals inter-class relationships and dark knowledge; (2) weighted cross-entropy on ground-truth anchors predictions to clinical reality. With α=0.7 weight on soft transfer and β=0.3 on hard labels at temperature T=4.0, the student inherits nuanced class similarities while maintaining task fidelity.
- Core assumption: The teacher's softened probability distributions encode transferable, clinically-relevant feature hierarchies that generalize beyond its architecture.
- Evidence anchors:
  - [abstract] "KD-OCT employs real-time distillation with a combined loss balancing soft teacher knowledge transfer and hard ground-truth supervision"
  - [section V] "Distillation applies a 4.0 temperature for soft labels, with loss weights balancing hard supervision (β=0.3, cross-entropy) and soft transfer (α=0.7, Kullback-Leibler divergence)"
  - [corpus] Limited corpus support for this specific combination; neighbor papers focus on alternative architectures (Siamese networks, Transformers) without direct KD loss comparisons.
- Break condition: If teacher accuracy is low (<85%) or class distributions shift significantly at deployment, soft labels may transfer erroneous biases, degrading student performance below standalone training.

### Mechanism 2
- Claim: Focal loss combined with stochastic weight averaging enables robust teacher training on imbalanced retinal OCT datasets.
- Mechanism: Focal loss (FL = α_t × (1-p_t)^γ × log(p_t), γ=2.0) down-weights easy examples and focuses gradient updates on hard, misclassified cases like subtle CNV or early drusen. SWA averages model weights over multiple training epochs, producing smoother loss landscapes and better generalization across the imbalanced class distribution (5,667 normal vs. 3,240 CNV in NEH).
- Core assumption: Hard examples in medical imaging correlate with clinically ambiguous cases that define decision boundaries.
- Evidence anchors:
  - [section IV] "Omitting focal loss (reverting to standard cross-entropy) led to the largest drop, highlighting its value in tackling class imbalance by focusing on hard examples such as subtle CNV cases"
  - [section IV, Eq. 1] Formal focal loss definition with γ=2.0
  - [corpus] No corpus papers specifically validate focal loss + SWA combination for OCT; assumption extrapolated from general medical imaging literature.
- Break condition: If dataset is well-balanced or hard examples are labeling errors rather than true clinical ambiguity, focal loss may overfit to noise.

### Mechanism 3
- Claim: Real-time teacher inference during student training enables dynamic knowledge transfer adapted to student learning progress.
- Mechanism: Unlike offline distillation with pre-computed logits, real-time KD generates soft labels on-the-fly for each batch. The frozen teacher processes augmented inputs during student training, allowing the student to learn from teacher responses to the same perturbations it encounters. Student uses lighter augmentation (M=7 vs. M=9 RandAugment magnitude, ±15° vs. ±20° rotation) to match its capacity.
- Core assumption: Dynamic soft labels provide regularization benefits beyond static pre-computed logits.
- Evidence anchors:
  - [section IV] "real-time KD is performed where the frozen teacher generates soft labels on-the-fly during student training, avoiding offline logit pre-computation and allowing dynamic knowledge transfer adapted to the student's progress"
  - [section V] Augmentation difference: "N=2, M=9 for teacher; M=7 for student"
  - [corpus] Corpus papers do not compare real-time vs. offline distillation; this remains an assumption specific to KD-OCT.
- Break condition: If computational resources are severely constrained, real-time teacher inference may cause training bottlenecks; offline pre-computation becomes preferable.

## Foundational Learning

- **Concept: Knowledge Distillation Fundamentals**
  - Why needed here: Understanding how soft labels encode "dark knowledge" (inter-class similarities) is essential for debugging distillation failures.
  - Quick check question: If teacher outputs [0.85, 0.10, 0.05] for (CNV, drusen, normal), what does the 0.10 vs. 0.05 ratio tell the student that hard labels cannot?

- **Concept: Temperature Scaling in Distillation**
  - Why needed here: T=4.0 is a critical hyperparameter; incorrect values either over-soften (lose signal) or under-soften (mimic hard labels).
  - Quick check question: What happens to soft label entropy as temperature increases from 1.0 to 10.0?

- **Concept: Patient-Level Cross-Validation**
  - Why needed here: Prevents data leakage where same patient's scans appear in train/test, inflating metrics in medical imaging.
  - Quick check question: Why does scan-level splitting risk optimistic accuracy estimates compared to patient-level splitting?

## Architecture Onboarding

- **Component map:** Input OCT → [Augmentation Pipeline] → Teacher (ConvNeXtV2-Large, frozen, 196.4M params) → [Soft Labels + Hard Labels] → Combined Loss → Student (EfficientNet-B2, trainable, 7.7M params) → Prediction

- **Critical path:**
  1. Train teacher first with focal loss + SWA + heavy augmentation (up to 150 epochs, early stopping patience 25)
  2. Freeze teacher weights; verify teacher accuracy ≥90% before distillation
  3. Train student with real-time KD (max 100 epochs, patience 20), monitoring both hard-label accuracy and KL divergence

- **Design tradeoffs:**
  - Larger student (B3/B4) → higher accuracy but reduced edge deployment feasibility
  - Higher α (soft weight) → better generalization on imbalanced classes but risk of teacher bias transfer
  - Test-Time Augmentation (5 variants) → +1-2% accuracy but 5× inference latency

- **Failure signatures:**
  - Student accuracy > teacher: Likely data leakage or insufficient patient-disjoint splitting
  - Student accuracy <80% with low KL divergence: Teacher may be undertrained; retrain with more epochs or SWA
  - Large accuracy gap between validation folds: Class imbalance or augmentation too aggressive for minority class

- **First 3 experiments:**
  1. **Baseline sanity check:** Train standalone EfficientNet-B2 (no KD) on NEH to establish lower bound; expect ~83-87% based on Table I baselines.
  2. **Ablation on loss weights:** Vary α ∈ {0.5, 0.7, 0.9} with fixed T=4.0; monitor if hard-label supervision is being underutilized.
  3. **Temperature sweep:** Test T ∈ {2.0, 4.0, 6.0} with fixed α=0.7; visualize soft label entropy to confirm meaningful distribution softening.

## Open Questions the Paper Calls Out

- **Can semi-supervised knowledge distillation maintain clinical-grade accuracy while significantly reducing labeled OCT training data requirements?**
  - Basis in paper: Authors state "Future work will explore semi-supervised KD to reduce labeled data reliance"
  - Why unresolved: Current KD-OCT requires fully labeled datasets; labeled medical imaging data remains scarce and expensive to obtain.
  - What evidence would resolve it: Experiments comparing semi-supervised KD variants against the current supervised approach using progressively smaller labeled subsets on NEH/UCSD datasets.

- **Can multi-modal distillation combining fundus photography with OCT images improve classification accuracy for AMD-related pathologies?**
  - Basis in paper: Authors mention "multi-modal distillation with fundus images for improved accuracy" as future work
  - Why unresolved: Current framework uses only OCT images; fundus photography may capture complementary diagnostic information not visible in cross-sectional scans.
  - What evidence would resolve it: Comparative experiments with paired fundus-OCT data using cross-modal attention or fusion mechanisms.

- **Does real-time deployment of KD-OCT on actual edge devices maintain the reported inference speed and accuracy under hardware constraints?**
  - Basis in paper: All experiments conducted on NVIDIA H200 GPU; edge deployment claimed as benefit but not empirically validated on target hardware.
  - Why unresolved: Memory bandwidth, thermal throttling, and limited compute on portable devices may cause performance degradation not captured in GPU experiments.
  - What evidence would resolve it: Benchmarks on representative edge hardware (NVIDIA Jetson, mobile processors) measuring latency, throughput, and accuracy degradation.

## Limitations

- **Data distribution assumptions**: The efficacy of focal loss hinges on the premise that hard examples reflect true clinical ambiguity rather than labeling noise. If subtle CNV cases are inconsistently annotated, the focal loss may overfit to subjective errors.
- **Real-time distillation novelty**: While real-time KD is presented as superior to offline logit pre-computation, the paper lacks ablation studies directly comparing the two approaches. This remains an assumption based on architectural convenience rather than empirical validation.
- **Cross-dataset generalization**: Although UCSD results are promising, the paper does not address potential domain shift between NEH (5-class, high-resolution) and UCSD (4-class, 224px) datasets. The student's performance on out-of-distribution OCT data is untested.

## Confidence

- **High**: Combined loss balancing (α=0.7 soft + β=0.3 hard) achieves near-teacher accuracy with 25.5× compression.
- **Medium**: Focal loss + SWA combination effectively handles class imbalance in OCT datasets.
- **Low**: Real-time distillation provides significant regularization benefits over offline pre-computation.

## Next Checks

1. **Ablation on focal loss sensitivity**: Retrain teacher without focal loss (standard cross-entropy) and measure performance drop on hard examples (CNV vs. drusen confusion).
2. **Offline vs. real-time distillation comparison**: Pre-compute teacher soft labels, train student with frozen logits, and compare KL divergence stability and final accuracy.
3. **Cross-dataset robustness test**: Evaluate KD-OCT on an external OCT dataset (e.g., OCTA-500) without fine-tuning to assess domain generalization limits.