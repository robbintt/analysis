---
ver: rpa2
title: 'VisualOverload: Probing Visual Understanding of VLMs in Really Dense Scenes'
arxiv_id: '2509.25339'
source_url: https://arxiv.org/abs/2509.25339
tags:
- dataset
- questions
- question
- please
- provide
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces VisualOverload, a new visual question answering
  (VQA) benchmark that evaluates vision-language models (VLMs) on fine-grained understanding
  in densely populated scenes. The benchmark contains 2,720 manually curated question-answer
  pairs across six task categories (activity, attribute, counting, OCR, reasoning,
  and scene) based on high-resolution scans of public-domain paintings.
---

# VisualOverload: Probing Visual Understanding of VLMs in Really Dense Scenes

## Quick Facts
- **arXiv ID**: 2509.25339
- **Source URL**: https://arxiv.org/abs/2509.25339
- **Reference count**: 30
- **Primary result**: Introduces a VQA benchmark revealing significant performance gaps in VLMs on dense visual scenes, with top models achieving only 19.6% accuracy on hardest split.

## Executive Summary
VisualOverload is a new benchmark designed to evaluate vision-language models on fine-grained understanding in densely populated scenes. It contains 2,720 manually curated question-answer pairs across six task categories, based on high-resolution scans of public-domain paintings. Evaluation of 37 models reveals systematic failures in counting and OCR tasks, logical inconsistencies in paired questions, and significant performance gaps even for state-of-the-art models. The benchmark exposes critical limitations in current VLMs and provides a challenging testbed for improving dense visual understanding.

## Method Summary
The benchmark uses 150 high-resolution public domain paintings (~4K resolution) with 2,720 manually curated questions across six categories (activity, attribute, counting, OCR, reasoning, scene). Questions are evaluated zero-shot with greedy decoding, using heuristic-based answer extraction (mapping option letters, extracting integers, normalizing text). The evaluation server holds private ground truth and applies preprocessing to predictions before scoring. Questions are split into easy/medium/hard categories based on model performance thresholds.

## Key Results
- Top model (o3) achieves only 19.6% accuracy on the hardest split and 69.5% overall
- Models show systematic counting underestimation and OCR hallucination patterns
- Logical inconsistency rates drop from 83% (scene) to 61% (reasoning) across binary question pairs
- Specialized HD models consistently underperform standard VLMs despite higher resolution capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dense visual scenes expose a fundamental information bottleneck in standard VLM vision encoders.
- Mechanism: Encoders compress high-resolution pixel input into fixed tokens (e.g., 1024), discarding fine-grained information in densely populated scenes.
- Core assumption: Token compression is the primary limiting factor; other architectural or training issues are secondary.
- Evidence anchors:
  - [abstract]: "We hypothesize that... encoding and reasoning over details is still a challenging task for them, especially if they are confronted with densely populated scenes."
  - [Section 1]: "...the vision encoder is a bottleneck in modern VLMs. Encoders are designed to compress visual input into a fixed number of tokens... This design imposes an inherent upper bound on fine-grained perception."

### Mechanism 2
- Claim: Paired, logically-opposed questions reveal shortcut-based reasoning rather than grounded visual understanding.
- Mechanism: Binary questions with logical opposites (e.g., "Is it day?" vs. "Is it night?") expose inconsistencies when models answer "yes" to both.
- Core assumption: Logically consistent answers require grounded visual processing; inconsistency implies shortcut use.
- Evidence anchors:
  - [Section 2.1]: "We pair each of the latter kind of questions with a logical opposite... to decrease the random chance and to provide an additional signal for measuring logical consistency."
  - [Section 4]: "We observe that models frequently provide logically inconsistent answers... with this instability intensifying as the complexity of such queries increases."

### Mechanism 3
- Claim: Performance degradation in counting and OCR tasks stems from systematic error patterns (underestimation, hallucination) rather than random mistakes.
- Mechanism: Models tend to underestimate counts as ground truth increases and produce OCR errors with high normalized Levenshtein distances (~0.7).
- Core assumption: Error patterns reflect fundamental limitations in visual tokenization or language model decoding.
- Evidence anchors:
  - [Section 4]: "Models are generally accurate when the ground truth is low, but errors increase substantially as the ground truth rises... models tend to err on the low side and underestimate."
  - [Section 4]: "The distribution's center of mass is around 0.7 [normalized Levenshtein distance], indicating that sequences require substantial edits to be correct."

## Foundational Learning

- **Vision Tokenization & Compression**
  - Why needed here: VisualOverload's core hypothesis is that encoder compression limits performance. Understanding how ViTs patchify images and map them to tokens is essential to diagnose failure modes.
  - Quick check question: How many tokens does a ViT-L/14@336px encoder produce, and what is the compression ratio for a 4K image?

- **Benchmark Design for Bias Detection**
  - Why needed here: The use of paired questions to detect logical inconsistencies is a key methodological innovation. This requires understanding of question design, baseline calibration (random vs. consistent chance), and evaluation metrics.
  - Quick check question: Why is "consistent chance" (42.5% for reasoning) higher than random chance (25%) for binary questions?

- **VLM Failure Modes in Dense Scenes**
  - Why needed here: The error analysis (counting underestimation, OCR hallucination) provides concrete targets for improvement. Recognizing these patterns is critical for interpreting results and designing mitigations.
  - Quick check question: What three main causes of OCR errors does the manual inspection reveal?

## Architecture Onboarding

- **Component map**: Image loading → downsampling to ~4K → Question formatting with standardized prompts → Model inference with greedy decoding → Answer extraction & normalization → Submission to evaluation server

- **Critical path**: 
  1. Load high-res image (downsample if needed)
  2. Format question with prompt template
  3. Run model inference
  4. Extract answer using heuristics (option mapping, integer extraction, text normalization)
  5. Submit JSON predictions to evaluation server

- **Design tradeoffs**:
  1. Resolution vs. Compute: Original images exceed 4K; downsampling preserves aspects but may lose details
  2. Manual vs. Automated Annotation: Manual curation ensures quality but limits scale
  3. Private vs. Public Ground Truth: Private ground truth prevents leakage but complicates local debugging

- **Failure signatures**:
  1. Counting underestimation: Predictions cluster below ground truth, with frequent refusals
  2. OCR high edit distance: Normalized Levenshtein distance ~0.7, from hallucinations or text flow misinterpretation
  3. Logical inconsistency: Consistency drops from ~83% (scene) to ~61% (reasoning), with some models below random chance

- **First 3 experiments**:
  1. Baseline evaluation: Run InternVL3-8B with default prompts, extract answers, submit to server
  2. Resolution ablation: Test VGA through original resolution, measure task-specific performance
  3. Logical consistency test: Compute consistency rates for all binary question pairs across models

## Open Questions the Paper Calls Out

- **Why do specialized high-resolution models consistently underperform standard VLMs on dense visual tasks?**
  - Basis: [explicit] Authors observe specialized HD models perform worse than equally sized regular models due to older backbones or training methods.
  - Why unresolved: Paper identifies gap but doesn't isolate root cause between architecture vs. training data.
  - Resolution: Controlled study comparing standard and HD-specialized architectures trained on identical data.

- **Does Chain-of-Thought reasoning impair visual grounding in smaller VLMs?**
  - Basis: [explicit] CoT prompting decreased accuracy for InternVL3-8B, hypothesized model was "too small to benefit."
  - Why unresolved: Unclear if performance drop is due to model capacity or verbal reasoning distracting from visual attention.
  - Resolution: Analysis of attention maps during CoT generation in small vs. large models.

- **Can the "information bottleneck" of fixed token encodings be overcome without quadratic computational costs?**
  - Basis: [inferred] Authors argue compression into fixed tokens imposes "inherent upper bound on fine-grained perception."
  - Why unresolved: Paper identifies compression as limit but doesn't propose or test architectures preserving detail without large context windows.
  - Resolution: Evaluation of sparse attention mechanisms or hierarchical encoders processing 4K without downsampling.

## Limitations

- Private ground truth prevents local validation and may introduce selection bias
- Benchmark's reliance on fine-grained details creates tension with computational constraints of standard VLM architectures
- Evaluation methodology assumes specific answer extraction heuristics that may not generalize to all model outputs

## Confidence

- **High Confidence**: Empirical findings of systematic counting underestimation, OCR hallucination patterns, and logical inconsistency failures are directly observable
- **Medium Confidence**: Hypothesis that vision encoder compression is primary bottleneck is plausible but not definitively proven
- **Low Confidence**: Claim that VisualOverload provides uniquely challenging testbed assumes other benchmarks don't capture these phenomena

## Next Checks

1. **Resolution Sensitivity Analysis**: Systematically evaluate model performance across multiple image resolutions (VGA through 4K) to quantify impact of fine-grained detail loss on task-specific accuracy

2. **Logical Consistency Benchmarking**: Compare logical consistency rates across multiple VQA benchmarks to determine whether VisualOverload's paired-question methodology reveals unique failure modes

3. **Error Pattern Replication**: Independently verify reported error patterns (counting underestimation, OCR edit distances) using subset of questions with public annotations to confirm systematic failure modes