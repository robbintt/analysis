---
ver: rpa2
title: 'LISAT: Language-Instructed Segmentation Assistant for Satellite Imagery'
arxiv_id: '2505.02829'
source_url: https://arxiv.org/abs/2505.02829
tags:
- segmentation
- lisa
- image
- sensing
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces LISA T, a vision-language model for reasoning
  segmentation in remote-sensing imagery. To address the scarcity of high-quality
  remote-sensing segmentation datasets, the authors curate GRES, a semi-synthetic
  dataset with 27,615 annotations across 9,205 images, and PreGRES, a large multimodal
  pretraining dataset with over 1 million question-answer pairs.
---

# LISAT: Language-Instructed Segmentation Assistant for Satellite Imagery

## Quick Facts
- arXiv ID: 2505.02829
- Source URL: https://arxiv.org/abs/2505.02829
- Reference count: 38
- Key outcome: LISAT significantly outperforms existing geospatial and open-domain models on remote-sensing reasoning segmentation, particularly on small objects, while also improving captioning and VQA performance.

## Executive Summary
This work introduces LISAT, a vision-language model for reasoning segmentation in remote-sensing imagery. To address the scarcity of high-quality remote-sensing segmentation datasets, the authors curate GRES, a semi-synthetic dataset with 27,615 annotations across 9,205 images, and PreGRES, a large multimodal pretraining dataset with over 1 million question-answer pairs. LISAT leverages a Remote-CLIP-based multimodal LLM and a segmentation decoder to generate segmentation masks from complex natural language queries. Experiments show that LISAT significantly outperforms existing geospatial models like RS-GPT4V (10.04% BLEU-4 gain in captioning) and open-domain models (143.36% gIoU gain in reasoning segmentation), particularly on small objects. The model, datasets, and code are publicly available.

## Method Summary
LISAT employs a two-stage training approach. First, it pretrains a Remote-CLIP vision encoder and Vicuna-7B LLM with LoRA adapters on PreGRES, a large multimodal pretraining dataset of over 1 million remote-sensing QA pairs. Second, it fine-tunes the model on GRES, a semi-synthetic reasoning segmentation dataset with 27,615 annotations across 9,205 images, using text generation and segmentation losses. The model uses a special <SEG> token embedding, projected into SAM's prompt space, to condition the segmentation decoder on the LLM's linguistic reasoning output.

## Key Results
- LISAT achieves 143.36% relative gIoU improvement over open-domain models on reasoning segmentation
- 10.04% BLEU-4 improvement over RS-GPT4V on captioning tasks
- Demonstrates superior performance on small objects (500px² area) compared to large objects
- Successfully generalizes to downstream VQA tasks across multiple datasets

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The <SEG> token embedding serves as a learned query representation that conditions the SAM decoder to localize objects described in natural language.
- **Mechanism:** The model expands its vocabulary with a special <SEG> token. During inference, when the LLM generates this token in response to a query, its final-layer hidden state is extracted and projected via MLP into SAM's prompt embedding space. This projected vector acts as a dense visual prompt that guides the SAM decoder to attend to the relevant spatial regions, effectively "bridging" linguistic reasoning with pixel-level segmentation.
- **Core assumption:** The LLM's internal representation of the <SEG> token encodes sufficient spatial-semantic information about the target object after training on paired query-mask examples.
- **Evidence anchors:** [abstract]: "LISA T leverages a Remote-CLIP-based multimodal LLM and a segmentation decoder to generate segmentation masks from complex natural language queries."; [section 4.2]: "When the model produces an output containing the <SEG> token, we extract the final layer embedding of that token, and project it via an MLP layer to the query space of a SAM-based segmentation decoder."; [corpus]: The embedding-as-mask paradigm was introduced in LISA (Lai et al., 2024) and has shown effectiveness in natural image reasoning segmentation, though corpus evidence for its transfer to remote-sensing domains is limited.
- **Break condition:** If the LLM fails to bind the query semantics to the <SEG> token representation (e.g., vague queries, out-of-distribution objects), the projected embedding will not provide sufficient signal for the SAM decoder, resulting in fragmented or empty masks.

### Mechanism 2
- **Claim:** Domain-adapted vision-language pretraining on PreGRES enables the model to handle the unique spatial and semantic characteristics of remote-sensing imagery.
- **Mechanism:** Standard CLIP vision encoders are trained on natural images with ground-level perspectives, which poorly transfer to overhead imagery with extreme scale variation, small objects, and subtle inter-class differences. By pretraining the vision encoder (Remote-CLIP) and LLM projection layer on PreGRES's 1M+ remote-sensing QA pairs, the model learns to align textual descriptions with the visual statistics of satellite imagery before segmentation-specific fine-tuning.
- **Core assumption:** The PreGRES dataset provides sufficient coverage of remote-sensing visual concepts and language patterns to induce meaningful vision-language alignment.
- **Evidence anchors:** [abstract]: "To address the scarcity of high-quality remote-sensing segmentation datasets, the authors curate GRES... and PreGRES, a large multimodal pretraining dataset with over 1 million question-answer pairs."; [section 4.1]: "LISA T adopts the Remote-CLIP ViT-L/14 encoder... A pre-trained Vicuna base model combined with the vision encoder is further pre-trained on PreGRES... prior to being trained on GRES."; [corpus]: FLAVARS paper similarly notes that "contrastive image-text methods like CLIP enable vision-language alignment" but emphasizes the need for domain-specific pretraining for remote sensing applications.
- **Break condition:** If PreGRES coverage is skewed toward certain object classes or scene types, the vision-language alignment will be brittle on underrepresented concepts (e.g., the paper notes failure cases with cloudy imagery and ambiguous scenes).

### Mechanism 3
- **Claim:** Semi-synthetic data generation with VLM-generated queries and SAM-generated masks creates scalable training data for reasoning segmentation.
- **Mechanism:** The GRES pipeline filters xView detections for "unique" and "visually interesting" objects, then uses GPT-4V to generate disambiguating natural language queries and GeoSAM to produce pixel-level masks from bounding boxes. This decouples mask quality from query quality, allowing the model to learn from diverse linguistic formulations (3 rephrasings per object) while maintaining mask accuracy.
- **Core assumption:** GeoSAM-generated masks are sufficiently accurate as ground truth, and GPT-4V-generated queries are representative of real user queries.
- **Evidence anchors:** [section 3.2]: "To build the dataset, we begin with a subset of the xView dataset... To convert xView images/annotations to GRES annotations/images, we follow the process overviewed in Figure 2."; [section 5.3]: "Another issue arises from the ground truth masks generated by GeoSAM in the GRES dataset. In some cases, the underlying ground truth mask is incorrect, and LISAt is occasionally penalized even when making correct predictions."; [corpus]: Corpus papers on small object detection in satellite imagery (arXiv:2502.03674) highlight that "incomplete or inadequate ground truth" is a common challenge in remote-sensing datasets.
- **Break condition:** If GeoSAM mask quality degrades on certain object types (e.g., small vehicles, complex boundaries), or if GPT-4V queries become repetitive or uninformative, the model will overfit to dataset artifacts rather than learning generalizable reasoning-segmentation.

## Foundational Learning

- **Concept: Embedding-as-Mask Paradigm (LISA framework)**
  - **Why needed here:** LISAT builds directly on LISA's architecture, which pioneered using LLM token embeddings as segmentation prompts. Understanding how a text embedding can condition a vision decoder is essential for debugging mask quality issues.
  - **Quick check question:** Can you explain why the <SEG> token embedding must be projected into SAM's prompt embedding space rather than used directly?

- **Concept: Vision-Language Alignment in Multimodal LLMs**
  - **Why needed here:** The paper emphasizes that standard LLaVA underperforms on remote-sensing, necessitating Remote-CLIP and PreGRES pretraining. Understanding vision-language alignment helps diagnose when/why the model fails to ground text in imagery.
  - **Quick check question:** What visual features of remote-sensing imagery (scale, perspective, object density) differ most from natural images, and how might this affect CLIP-based encoders?

- **Concept: SAM (Segment Anything Model) Decoder Architecture**
  - **Why needed here:** LISAT uses SAM's decoder to produce final masks from the projected <SEG> embedding. Understanding SAM's prompt-based conditioning is critical for interpreting how linguistic queries translate to spatial masks.
  - **Quick check question:** What types of prompts does SAM accept (points, boxes, masks, text), and which does LISAT leverage?

## Architecture Onboarding

- **Component map:** Input Image → Remote-CLIP ViT-L/14 Encoder → Linear Projection Layer → Text Query → Vicuna-7B LLM ← LoRA adapters → <SEG> token embedding (if generated) → MLP Projection Layer → SAM Decoder ← Original image features → Segmentation Mask

- **Critical path:** The two-stage training pipeline is essential: (1) PreGRES pretraining with LoRA on the LLM and projection layer (next-token prediction only), then (2) GRES fine-tuning with LoRA on LLM and full fine-tuning of SAM decoder (text generation + mask losses). Skipping stage 1 will result in poor vision-language alignment for remote-sensing imagery.

- **Design tradeoffs:**
  - **SAM vs. GeoSAM decoder:** The paper found SAM outperformed GeoSAM (gIoU 0.275 vs. 0.238), suggesting SAM's broader pretraining on diverse images provides more adaptable features than GeoSAM's domain-specific but potentially biased training.
  - **Loss weighting:** λ_bce=2.0, λ_dice=0.5 were empirically chosen. BCE emphasizes per-pixel accuracy while DICE handles class imbalance; the higher BCE weight suggests small object boundaries were a priority.
  - **Query diversity:** 3 rephrasings per object increase linguistic robustness but may reduce object-class diversity if rephrasings are semantically similar.

- **Failure signatures:**
  - **Cloudy/obscured imagery:** Model fails to identify key features (section 5.3).
  - **Vague queries with multiple similar objects:** E.g., "Identify the plane in the bottom-right" when multiple planes are present.
  - **Small objects with similar distractors:** Model may segment the wrong instance or merge multiple instances.
  - **GeoSAM ground truth errors:** Model makes correct predictions but is penalized during evaluation (Appendix D.3).

- **First 3 experiments:**
  1. **Validate PreGRES pretraining contribution:** Train LISAT without PreGRES (direct GRES fine-tuning on base LLaVA) and compare gIoU on small vs. large objects. This isolates the value of domain-specific vision-language alignment.
  2. **Ablate Remote-CLIP vs. standard CLIP:** Swap Remote-CLIP for standard CLIP ViT-L/14 and measure captioning (BLEU-4) and segmentation (gIoU) performance. The paper's Table 7 suggests ~4-6 BLEU-4 improvement on UCM-Captions.
  3. **Test query robustness:** Evaluate on GRES test set with manually simplified queries (removing spatial references or distinctive features) to understand failure modes. The paper notes 50% of queries include spatial references; this experiment would reveal reliance on spatial cues vs. visual features.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the LISAT architecture be adapted to efficiently process large-scale geospatial rasters without losing the fine-grained reasoning capabilities observed in 512x512 chips?
- **Basis in paper:** [explicit] The conclusion states future work should focus on "scaling LISAT for use with large rasters."
- **Why unresolved:** Current experiments rely on cropped chips, while full satellite images are orders of magnitude larger, potentially challenging the memory constraints and context windows of current MLLMs.
- **What evidence would resolve it:** A modified architecture or inference pipeline that successfully segments objects in gigapixel-scale images while maintaining gIoU performance comparable to the current chip-based results.

### Open Question 2
- **Question:** To what extent does the observed performance plateau in training scaling stem from the limited class diversity of the xView dataset rather than data volume?
- **Basis in paper:** [inferred] The caption of Figure 4 notes that performance plateaus with more data, suggesting "we may need additional data variance outside the xView classes."
- **Why unresolved:** It is unclear if the model has saturated the complexity of the reasoning task or if the synthetic queries derived from xView lack the semantic diversity needed for further improvement.
- **What evidence would resolve it:** Demonstrating that adding training data from different sensor modalities or class ontologies breaks the current performance plateau observed in Figure 4.

### Open Question 3
- **Question:** Can multimodal or hyperspectral data inputs be integrated into the Remote-CLIP backbone to better distinguish visually similar objects?
- **Basis in paper:** [explicit] The conclusion identifies the need to "incorporate multimodal/hyperspectral data sources."
- **Why unresolved:** The current Remote-CLIP backbone is RGB-centric, but the introduction notes that remote sensing often involves "subtle visual differences" (e.g., small cars vs. buildings) where spectral data might be superior.
- **What evidence would resolve it:** A modified LISAT model utilizing hyperspectral bands showing statistically significant gIoU improvements on classes known to be visually ambiguous in RGB.

## Limitations

- **Dataset coverage and generalization:** While PreGRES provides over 1 million QA pairs for pretraining, the corpus signals indicate an average neighbor FMR of 0.425, suggesting moderate relevance of related work. The paper acknowledges that GRES's semi-synthetic generation pipeline may introduce artifacts, particularly for small objects and complex scenes.
- **Architecture specificity:** The embedding-as-mask paradigm, while effective in the LISA framework for natural images, has limited corpus evidence for successful transfer to remote-sensing domains. The paper assumes the <SEG> token embedding captures sufficient spatial-semantic information for the SAM decoder, but this may not generalize to highly diverse or out-of-distribution objects.
- **Evaluation scope:** The model is primarily evaluated on GRES, a dataset derived from xView with specific object categories and characteristics. Performance on truly open-domain remote-sensing imagery with different sensors, resolutions, or geographic regions remains uncertain.

## Confidence

**High confidence** in: The overall two-stage training