---
ver: rpa2
title: 'MUStReason: A Benchmark for Diagnosing Pragmatic Reasoning in Video-LMs for
  Multimodal Sarcasm Detection'
arxiv_id: '2510.23727'
source_url: https://arxiv.org/abs/2510.23727
tags:
- reasoning
- sarcasm
- video
- cues
- videolms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of detecting sarcasm in multimodal
  conversational videos, which requires perceiving and reasoning over cues across
  text, audio, and visual modalities. The authors introduce MUStReason, a diagnostic
  benchmark that provides fine-grained annotations of modality-specific cues and reasoning
  steps needed to identify sarcastic intent.
---

# MUStReason: A Benchmark for Diagnosing Pragmatic Reasoning in Video-LMs for Multimodal Sarcasm Detection

## Quick Facts
- **arXiv ID**: 2510.23727
- **Source URL**: https://arxiv.org/abs/2510.23727
- **Reference count**: 0
- **Primary result**: PragCoT improves sarcasm detection F1 by up to 20% over zero-shot, but perception and reasoning bottlenecks persist even with quasi-perfect cues

## Executive Summary
This paper addresses the challenge of detecting sarcasm in multimodal conversational videos, which requires perceiving and reasoning over cues across text, audio, and visual modalities. The authors introduce MUStReason, a diagnostic benchmark that provides fine-grained annotations of modality-specific cues and reasoning steps needed to identify sarcastic intent. To enable pragmatic reasoning in Video-Language Models, they propose PragCoT, a structured prompting framework that explicitly decodes perceptual cues before reasoning. Experiments with seven VideoLMs show that PragCoT improves sarcasm classification F1 scores by up to 20% compared to zero-shot methods and 2.4% over multimodal CoT baselines. Qualitative and quantitative analysis reveals that models often fail due to poor perception or flawed reasoning chains, and PragCoT helps align perceptual cues with reasoning for better performance. The work highlights the importance of modality-specific decoding and structured reasoning for complex multimodal tasks like sarcasm detection.

## Method Summary
MUStReason is a benchmark for multimodal sarcasm detection built on MUStARD++ Balanced, containing 1,365 videos (691 sarcastic, 674 non-sarcastic). The dataset includes 462 gold-standard annotations with modality-specific cues (facial action units, dialogue categories, audio tone) and reasoning steps, plus 899 silver-standard samples with automated sanity checks. PragCoT is a three-stage prompting framework: (1) Perception - separately query each modality (utterance, audio, face, video); (2) Decoding - classify facial action units and categorize dialogue (neutral/metaphoric/ironic/hyperbolic); (3) Reasoning - integrate cues and classify. The framework was evaluated on seven VideoLMs using accuracy and macro F1-score, with reasoning quality assessed via BERTScore, METEOR, RougeL, and GPT-4.1.

## Key Results
- PragCoT improves F1 scores by up to 20% over zero-shot methods and 2.4% over multimodal CoT baselines
- Performance improves modestly (61.8 F1) when models are given quasi-perfect modality descriptions, but ceiling remains low (~65 F1 for GPT-5)
- Two failure types identified: perception+reasoning failures (vague neutral tags) and reasoning failures despite correct perception (spurious cues derail inference)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Explicitly decoding perceptual cues before reasoning improves sarcasm classification by aligning perception with downstream inference.
- **Mechanism**: PragCoT introduces a structured "Decoding" step between perception and reasoning. This step requires the model to classify specific facial action units and categorize dialogue (neutral, metaphoric, ironic, hyperbolic) before attempting inference. By forcing explicit recognition of cues known to correlate with sarcasm, the model has better-grounded inputs for the reasoning stage.
- **Core assumption**: Models can more reliably reason over explicitly verbalized perceptual features than over implicit multimodal representations.
- **Evidence anchors**: [abstract]: "we propose PragCoT, a framework that steers VideoLMs to focus on implied intentions over literal meaning"; [section 4, Table 2]: Ablation shows removing dialogue decoding drops F1 from 59.3 to 56.6; removing face decoding drops to 58.7; [corpus]: IRONIC paper (arxiv 2505.16258) similarly finds standard CoT doesn't efficiently leverage cognitive processes for sarcasm, suggesting task-specific decoding matters.
- **Break condition**: If the model's perceptual encoders cannot reliably extract the targeted cues (facial action units, tone categories), the decoding step will propagate errors rather than aid reasoning.

### Mechanism 2
- **Claim**: Providing quasi-perfect perception improves but does not resolve sarcasm detection, indicating both perception and reasoning bottlenecks exist.
- **Mechanism**: When models are given near-perfect modality descriptions instead of raw video/audio, performance improves modestly (Qwen2.5VL: 61.8 F1 with descriptions vs. 57.4 with PragCoT on raw input), but ceiling remains low (~65 F1 for GPT-5). This suggests perception gaps are partially addressable, but reasoning about incongruity between modalities remains challenging.
- **Core assumption**: Textual descriptions of modalities serve as a reasonable proxy for learned multimodal representations.
- **Evidence anchors**: [section 5.2, Table 4]: "LLMs achieve performance comparable to VideoLMs, with GPT-5 being the best reasoner, beating the open-source LLMs. However, the overall classification performance remains low for both LLMs and VideoLMs"; [section 5.2, Figure 5]: Two failure types identified—perception+reasoning failures (vague neutral tags) and reasoning failures despite correct perception (spurious cues derail inference); [corpus]: MuSaG (arxiv 2510.24178) provides full-modal annotations for German sarcasm, suggesting cross-linguistic generalization of perception gaps.
- **Break condition**: If the bottleneck were purely perceptual, quasi-perfect descriptions should substantially close the performance gap to human-level. They do not.

### Mechanism 3
- **Claim**: Structured reasoning frameworks that explicitly resolve incongruity between literal meaning and non-verbal cues outperform single-step classification.
- **Mechanism**: Sarcasm requires recognizing that what is said differs from what is meant. PragCoT prompts the model to identify this incongruity explicitly—comparing dialogue content against facial expressions and tone—rather than directly mapping input to label. Multimodal-CoT shows improvement over zero-shot; PragCoT adds task-specific pragmatic structure on top.
- **Core assumption**: Explicit reasoning chains are more interpretable and debuggable than implicit classification, enabling targeted improvement.
- **Evidence anchors**: [section 1]: "interpreting sarcasm requires pragmatic reasoning in order to resolve the incongruity between the literal meaning of an utterance and non-verbal cues"; [section 5.1]: PragCoT improves F1 by up to 20% over zero-shot, 2.4% over Multimodal-CoT; [corpus]: Weak corpus connection—related work on pragmatic metacognitive prompting (Nek Minit, arxiv 2505.15095) shows promise for explainable sarcasm detection in specific dialects, but generalization to multimodal video is untested.
- **Break condition**: If the incongruity pattern varies significantly across cultural contexts or datasets, the specific reasoning structure may not transfer without re-annotation.

## Foundational Learning

- **Concept: Gricean Implicature and Pragmatic Reasoning**
  - **Why needed here**: Sarcasm detection fundamentally requires distinguishing speaker intent from literal utterance. Without understanding that speakers can violate conversational norms to convey opposite meaning, models default to surface-level interpretation.
  - **Quick check question**: Given "Great weather we're having" spoken during a thunderstorm with flat intonation, what signal indicates the speaker does NOT mean the weather is good?

- **Concept: Chain-of-Thought Prompting**
  - **Why needed here**: PragCoT builds on zero-shot CoT by adding modality-specific decoding. Understanding how intermediate reasoning steps improve complex inference helps explain why structured prompting outperforms direct classification.
  - **Quick check question**: Why might generating reasoning before classification improve accuracy on tasks requiring multi-step inference?

- **Concept: Multimodal Fusion Bottlenecks**
  - **Why needed here**: VideoLMs encode video frames, text, and optionally audio into aligned representations. The paper shows these representations often fail to capture sarcasm-relevant cues (especially audio tone and facial micro-expressions), even in models with audio encoders.
  - **Quick check question**: A VideoLM achieves 50% accuracy on sarcasm detection (random chance). What diagnostic steps would determine whether the failure is in encoding, fusion, or reasoning?

## Architecture Onboarding

- **Component map**: Video frames → VideoGPT+ (scene context) → Frame features; Text transcript → LLaMA 3.1 → Dialogue emotion; Raw audio → Qwen2-Audio (prosodic features) → Spectrogram features; Facial video → OpenFace → Facial action units; Multimodal fusion → VideoLM encoder → Aligned representations → PragCoT prompts → Reasoning chain → Binary classification

- **Critical path**: 1. Perception: Each modality encoded separately (video → frame features, audio → spectrogram/text, text → embeddings) 2. Decoding (PragCoT-specific): Model prompted to classify facial AUs and dialogue category before reasoning 3. Reasoning: Model generates rationale comparing literal meaning vs. non-verbal cues, then assigns label 4. Bottleneck: If perception fails (e.g., facial AU misclassification), reasoning inherits errors

- **Design tradeoffs**: Annotation granularity vs. cost: Gold annotations required correction in 56% of samples; silver annotations achieve 4.51/5 quality rating but may propagate perception errors; Audio inclusion vs. model compatibility: Only 2 of 7 tested VideoLMs process audio; others rely on transcripts, losing prosodic cues; Zero-shot vs. ICL vs. fine-tuning: PragCoT improves zero-shot, but ICL provides inconsistent gains (only Qwen2.5Omni benefits); fine-tuning on MUStReason untested

- **Failure signatures**: Perception failure: Model assigns "neutral" tags across all modalities despite obvious cues; reasoning chain has minimal content; Reasoning failure with correct perception: Model identifies frustration and furrowed eyebrows but generates spurious intermediate cue that derails inference; final prediction contradicts perceived cues; Audio hallucination (zero-shot w/ reasoning): Models with audio capability claim "no audio input" in reasoning despite having access

- **First 3 experiments**: 1. Replicate PragCoT ablation on held-out split: Remove decoding step for face and dialogue separately; verify F1 drops match reported values (59.3 → 56.6 for dialogue, 59.3 → 58.7 for face). This validates the causal contribution of each component. 2. Oracle perception probe: Provide models with gold-standard modality descriptions (from MUStReason annotations) and measure reasoning-only performance. Compare to Table 4 to verify whether reasoning bottleneck persists. 3. Cross-dataset generalization: Apply PragCoT to MUStARD++ (source dataset) vs. independent sarcasm corpus (e.g., MuSaG for German, if available). Measure performance drop to assess whether reasoning structure transfers or is dataset-specific.

## Open Questions the Paper Calls Out

- **Can the PragCoT framework and MUStReason annotations generalize to sarcasm detection in dyadic conversations or other datasets beyond MUStARD++?**
  - **Basis in paper**: [explicit] Limitations section states the annotations are "dataset-specific, preventing generalizability across dyadic conversations which might contain additional cue variations not represented."
  - **Why unresolved**: The benchmark was constructed solely from MUStARD++ Balanced; no cross-dataset evaluation was conducted.
  - **What evidence would resolve it**: Evaluation of PragCoT on alternative sarcasm corpora (e.g., MHSDB) with comparable cue-level annotations, reporting F1 differences.

- **How does sarcasm detection performance change when applying MUStReason to non-English, culturally diverse conversational video data?**
  - **Basis in paper**: [explicit] Limitations section notes sarcasm is "influenced by cultural and social dynamics" and the monolingual English dataset "misses language-specific sarcasm indicators."
  - **Why unresolved**: The benchmark includes only English-language content from TV shows; no multilingual annotation or evaluation was performed.
  - **What evidence would resolve it**: Constructing multilingual extensions of MUStReason and benchmarking VideoLMs with PragCoT across languages.

- **Can the PragCoT structured reasoning framework be effectively transferred to related pragmatic tasks such as irony, humor, or metaphor detection?**
  - **Basis in paper**: [explicit] Conclusion states "Future work would focus on extending it to other tasks like irony and figurative language understanding."
  - **Why unresolved**: PragCoT was designed and validated only for sarcasm; transferability remains untested.
  - **What evidence would resolve it**: Applying PragCoT to existing humor or irony datasets with analogous cue annotations and comparing against baseline CoT methods.

## Limitations

- **Dataset specificity**: Annotations are dataset-specific, preventing generalizability across dyadic conversations with additional cue variations
- **Cultural and linguistic constraints**: Monolingual English dataset misses language-specific sarcasm indicators influenced by cultural and social dynamics
- **Audio modality limitations**: Only 2 of 7 tested VideoLMs process audio, losing prosodic cues for the majority of models

## Confidence

- **High confidence**: PragCoT's 2.4-20% F1 improvement over baselines is robustly demonstrated through ablation studies and multiple model comparisons
- **Medium confidence**: The claim that both perception and reasoning bottlenecks exist is supported by quasi-perfect description experiments, but the exact contribution of each bottleneck varies by model and modality
- **Medium confidence**: The generalizability of PragCoT's reasoning structure to other multimodal tasks remains unproven, as evaluation is limited to sarcasm detection

## Next Checks

1. **Cross-dataset transfer**: Apply PragCoT to an independent sarcasm corpus (e.g., MuSaG for German) to verify whether the reasoning structure transfers or requires task-specific annotation

2. **Architecture-specific analysis**: Systematically test PragCoT across VideoLMs with varying audio capabilities to quantify the impact of missing modalities on perception and reasoning performance

3. **Human baseline comparison**: Measure human performance on MUStReason samples to establish realistic ceilings and determine whether the remaining 30-40% accuracy gap represents fundamental task difficulty or model limitations