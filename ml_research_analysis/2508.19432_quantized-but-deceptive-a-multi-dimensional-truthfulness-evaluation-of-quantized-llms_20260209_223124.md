---
ver: rpa2
title: Quantized but Deceptive? A Multi-Dimensional Truthfulness Evaluation of Quantized
  LLMs
arxiv_id: '2508.19432'
source_url: https://arxiv.org/abs/2508.19432
tags:
- arxiv
- llms
- truthfulness
- prompts
- deceptive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study evaluates the truthfulness of quantized large language
  models (LLMs) across three dimensions: logical reasoning, common sense, and imitative
  falsehoods. The authors introduce TruthfulnessEval, a comprehensive framework that
  systematically assesses how quantization affects the propensity of LLMs to generate
  truthful versus deceptive responses.'
---

# Quantized but Deceptive? A Multi-Dimensional Truthfulness Evaluation of Quantized LLMs

## Quick Facts
- arXiv ID: 2508.19432
- Source URL: https://arxiv.org/abs/2508.19432
- Reference count: 30
- Primary result: Quantized LLMs retain internal truthful representations but can be prompted to generate false outputs, especially under deceptive framing

## Executive Summary
This study evaluates how quantization affects LLM truthfulness across logical reasoning, common sense, and imitative falsehoods. The authors introduce TruthfulnessEval, testing mainstream 4-bit (GPTQ, AWQ) and extreme 2-bit (AQLM, AQLM-PV) quantization on models from 8B to 72B parameters. While quantized models maintain strong performance on logical reasoning and common sense, they become significantly more susceptible to producing false outputs when prompted with deceptive instructions. Layer-wise analysis reveals that quantized LLMs still internally "know" the truth, even when generating false responses under deceptive prompts. These findings highlight the need for careful prompt design and truthfulness-aware alignment strategies when deploying quantized LLMs in sensitive applications.

## Method Summary
The authors evaluate six topic-specific datasets from Bürger et al. (2024), CommonClaim (4,450 statements), and TruthfulQA (817 questions) across five logical statement types (affirmative, negated, conjunction, disjunction, common sense). They test quantized models using AWQ, GPTQ (4-bit) and AQLM, AQLM-PV (2-bit) on LLaMA, Mistral, and Qwen model families. For each prompt variant (honest/neutral/deceptive × 15 rephrasings), they measure accuracy and apply layer-wise probing using logistic regression on residual stream activations. Optional DoLa decoding amplifies factual knowledge by contrasting early vs. late layer logits. The framework systematically assesses how quantization affects truth-consistent behavior versus internal knowledge encoding.

## Key Results
- 4-bit quantization preserves logical reasoning and common sense accuracy; 2-bit quantization degrades disjunction reasoning by ~20 points
- Honest and neutral prompts yield stable, accurate outputs while deceptive prompts cause severe accuracy fluctuations and can override truth-consistent behavior
- Layer-wise probing shows quantized models retain internally truthful representations (near-1.0 accuracy) even when generating false outputs under deceptive prompts
- Disjunction performance collapses at 2-bit for small models (≤8B) but recovers at 70B parameters, suggesting scale compensates for precision loss

## Why This Works (Mechanism)

### Mechanism 1: Truth-Knowledge Decoupling Under Quantization
Quantized models retain internally truthful representations but can be prompted to generate false outputs. The linear separability of true/false representations in activation space remains preserved, but the instruction-following pathway can override how these representations translate into output tokens. The residual stream at final token positions still encodes truth linearly—probing classifiers trained on these activations achieve near-1.0 accuracy on held-out topics regardless of whether the model is generating truthful or deceptive responses.

### Mechanism 2: Prompt Framing Overrides Truth-Consistent Behavior
Deceptive prompts can subvert truth-consistent output generation even when internal truth representations remain intact. The instruction prefix ("You are a dishonest assistant that always lies") shifts the model's generation policy without altering the latent truth encoding. This suggests a control pathway where high-level instruction representations modulate how factual knowledge is converted to token predictions—effectively flipping the output polarity while leaving the knowledge base untouched.

### Mechanism 3: Bit-Width Gradient in Disjunction Reasoning
Extreme quantization (2-bit) disproportionately degrades performance on logically complex statements (disjunctions), while simpler logical forms remain robust at 4-bit. Disjunction requires maintaining multiple simultaneous truth possibilities and evaluating OR-semantics—higher cognitive load that degrades first under precision loss. At 2-bit, even AQLM-PV-1x16 shows ~20-point drops on disjunction vs. ~2-point drops on affirmative statements. Larger models (≥70B) partially recover this capability, suggesting scale compensates for precision loss in complex reasoning.

## Foundational Learning

- **Post-Training Quantization (PTQ)**: All evaluated quantization methods (GPTQ, AWQ, AQLM) are PTQ techniques that compress trained models without retraining. Understanding PTQ explains why truth representations might be preserved (knowledge already encoded) while behavior shifts (control signals disrupted by precision loss).
  - Quick check: If you quantized a model from FP16 to INT4 and observed unchanged perplexity but increased susceptibility to adversarial prompts, would you attribute this to weight precision loss in attention layers or MLP layers?

- **Linear Probing of Hidden States**: The paper's central claim—that models "know" truth internally—rests on training logistic regression classifiers on layer-wise activations. Without understanding probing methodology, you cannot evaluate whether the evidence supports decoupling vs. other explanations.
  - Quick check: A probe trained on layer-20 activations achieves 95% accuracy on held-out topics. What does this tell you about where truth is encoded, and what does it not tell you about why the model outputs falsehoods?

- **TruthfulQA and Imitative Falsehoods**: The third evaluation dimension measures whether models resist mimicking common misconceptions. This differs from logical truth—it tests learned alignment against mimicking false patterns in training data.
  - Quick check: Why would a model that correctly identifies "Water evaporates faster on hot days" as True still answer "What happens if you eat watermelon seeds?" with a misconception?

## Architecture Onboarding

- **Component map**: TruthfulnessEval Framework (logical reasoning, common sense, imitative falsehoods) → Quantization Methods (GPTQ/AWQ 4-bit, AQLM/AQLM-PV 2-bit) → Layer-wise Probing Pipeline (residual stream extraction → logistic regression → accuracy measurement) → DoLa Decoding (contrastive decoding strategy)

- **Critical path**: 1) Select model family and quantization level 2) For each prompt variant, evaluate all three truthfulness dimensions 3) Extract activations for layer-wise probing to verify internal truth encoding 4) Apply PCA visualization to confirm true/false separability in latent space

- **Design tradeoffs**: 4-bit vs 2-bit: 4-bit preserves most logical reasoning; 2-bit suitable only for resource-critical deployments where disjunction failure is acceptable. Prompt engineering: Honest prompts stabilize truthfulness but may overconstrain responses; neutral prompts offer balance; avoid deceptive framing in production. DoLa overhead: Adds inference cost but recovers 8-15 percentage points on TruthfulQA MC2.

- **Failure signatures**: Disjunction collapse: Near-random performance on OR-statements indicates model is too small or too quantized for complex logical reasoning. Prompt instability: High variance across the 15 prompt rephrasings indicates unreliable instruction-following—do not deploy in high-stakes settings. Probing divergence: If layer-wise accuracy differs between honest/deceptive conditions, internal knowledge may be corrupted rather than decoupled.

- **First 3 experiments**: 1) Baseline truthfulness: Run TruthfulnessEval on your target quantized model with neutral prompts to establish baseline accuracy across five categories. 2) Prompt sensitivity audit: Test all 15 prompt variants on a held-out subset to measure variance—if deceptive prompts drop accuracy >15 points, implement prompt sanitization in your pipeline. 3) Layer-wise probing sanity check: Train linear probes on middle-layer activations for your model; if accuracy <0.85 on held-out topics, the model's truth encoding may be compromised (consider using a higher bit-width or larger model).

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the susceptibility of quantized LLMs to implicit deceptive prompts compare to the explicit instructions tested in this study? The current study only utilized explicit "honest" or "deceptive" instructions and did not evaluate more subtle, implicit framing common in real-world misuse.

- **Open Question 2**: To what extent does the specific quantization process (vs. the resulting precision) cause the observed susceptibility to deceptive prompts? The authors evaluated pre-quantized models but did not isolate variables within the quantization algorithms to identify the root cause of the behavioral shift.

- **Open Question 3**: Do extremely large quantized models (e.g., 405B parameters) retain the "internal truth" representation better than smaller models when subjected to deceptive prompts? Hardware constraints limited the study to models with 72B parameters or fewer; thus, it is unknown if the findings regarding internal representation stability generalize to frontier-scale models.

- **Open Question 4**: How do quantized LLMs perform on complex logical truthfulness tasks involving Exclusive OR (XOR) or Implication? The current TruthfulnessEval framework is limited to affirmative, negated, conjunctive, and disjunctive statements, leaving higher-order logical reasoning in quantized models untested.

## Limitations
- The probing methodology assumes logistic regression classifiers capture genuine knowledge rather than spurious correlations
- The anthropomorphic "deceptive" prompt framing may not generalize to more subtle adversarial techniques
- The study focuses on instruction-tuned models only, leaving base model behavior unexplored
- The 15 prompt rephrasings may not exhaustively sample the instruction-following space

## Confidence
- **High Confidence**: 4-bit quantization preserves logical reasoning and common sense accuracy; 2-bit quantization degrades disjunction reasoning; honest/neutral prompts yield stable outputs while deceptive prompts increase variance
- **Medium Confidence**: Quantized models retain internal truth representations that can be recovered via probing; the decoupling between knowledge encoding and output generation is robust across prompt rephrasings
- **Low Confidence**: DoLa decoding consistently recovers truthfulness across all model families and quantization levels; the specific mechanism of instruction-following overriding truth-consistent behavior is fully understood

## Next Checks
1. **Probing Method Validation**: Train linear probes on the same activation spaces using adversarial examples that should fool human evaluators but preserve factual content. If probing accuracy remains high on these cases, it strengthens the knowledge-preservation claim.

2. **Cross-Architecture Generalization**: Test the same quantization-prompting framework on decoder-only models (like Mistral) and base models without instruction tuning. Compare layer-wise separability patterns to determine whether instruction-following is the primary driver of the observed decoupling.

3. **Adversarial Prompt Stress Test**: Systematically vary the "deceptive" prompt structure beyond the 15 rephrasings—test implications, role-playing scenarios, and logical contradictions. Measure whether the knowledge-preservation pattern holds under increasingly subtle manipulation.