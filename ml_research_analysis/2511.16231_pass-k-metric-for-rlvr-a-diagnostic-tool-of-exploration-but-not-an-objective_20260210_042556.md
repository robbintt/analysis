---
ver: rpa2
title: 'Pass@k Metric for RLVR: A Diagnostic Tool of Exploration, But Not an Objective'
arxiv_id: '2511.16231'
source_url: https://arxiv.org/abs/2511.16231
tags:
- pass
- objective
- probability
- arxiv
- exploration
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the pass@k metric used in reinforcement learning
  for verifiable reasoning tasks. The authors derive the gradient of the pass@k objective
  and show it is simply a reweighted version of the pass@1 gradient, scaled by a factor
  that vanishes when the policy concentrates probability mass.
---

# Pass@k Metric for RLVR: A Diagnostic Tool of Exploration, But Not an Objective

## Quick Facts
- arXiv ID: 2511.16231
- Source URL: https://arxiv.org/abs/2511.16231
- Authors: Yang Yu
- Reference count: 16
- Primary result: Pass@k gradient is a scalar multiple of pass@1 gradient, vanishing exactly when exploration is most needed

## Executive Summary
This paper analyzes the pass@k metric used in reinforcement learning for verifiable reasoning tasks. The authors derive the gradient of the pass@k objective and show it is simply a reweighted version of the pass@1 gradient, scaled by a factor that vanishes when the policy concentrates probability mass. This reveals that pass@k fails to provide a meaningful learning signal exactly when exploration is most needed. The paper further demonstrates that iterative RL naturally induces "exploration collapse," where the gap between pass@k and pass@1 diminishes as the policy converges. The authors conclude that pass@k should be treated as a diagnostic tool for reasoning diversity rather than an optimization objective, suggesting that explicit exploration mechanisms are needed instead.

## Method Summary
The paper provides a theoretical analysis of pass@k as an optimization objective in reinforcement learning for verifiable reasoning (RLVR). It derives the gradient of the pass@k objective and shows it is mathematically equivalent to a reweighted pass@1 gradient. The analysis uses standard policy gradient (REINFORCE) with deterministic verifier rewards. No specific training runs or experimental protocols are described—the work is purely theoretical, focusing on the mathematical properties of the objective function.

## Key Results
- The gradient of pass@k is simply a scalar multiple of the pass@1 gradient, scaled by factor α_k = k(1-J_1)^(k-1)
- Pass@k provides no distinct learning signal compared to pass@1, and the signal vanishes as policy concentrates
- Iterative RL naturally induces "exploration collapse" where pass@k converges to pass@1 as training progresses

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Optimizing the pass@k objective provides no distinct search direction in parameter space compared to optimizing pass@1.
- Mechanism: The gradient of the pass@k objective ∇_θ J_k is derived as a scalar multiple α_k of the pass@1 gradient ∇_θ J_1. Mathematically, ∇_θ J_k(x;θ) = k(1-J_1(x;θ))^(k-1) · ∇_θ J_1(x;θ). Because the gradients are collinear, pass@k optimization essentially reweights the learning signal of pass@1 rather than introducing a new vector towards diverse solutions.
- Core assumption: The optimization landscape allows gradients to be approximated via sampling (e.g., REINFORCE), and the model parameterization is differentiable.
- Evidence anchors:
  - [abstract] "...demonstrate that it is fundamentally a per-example positive reweighting of the simpler pass@1 objective."
  - [section 3.1] Theorem 3.1 explicitly derives ∇_θ J_k(x;θ) = α_k(x, θ) · ∇_θ J_1(x;θ).
  - [corpus] While not explicitly contradicting, the paper "The Best of N Worlds" (Corpus) proposes a different "max@k" optimization, implicitly acknowledging that standard pass@k approaches require modification to effectively align with Best-of-N sampling.
- Break condition: If the scalar multiplier α_k becomes zero or the inner gradient vanishes, the learning signal disappears entirely.

### Mechanism 2
- Claim: The pass@k objective fails to provide a learning signal in "low probability" regimes where the model initially struggles.
- Mechanism: When the probability of success is low (J_1 ≈ 0), the theoretical scaling factor α_k is maximized (approaching k). However, because gradients are estimated via sampling, if the model fails to sample any correct trajectories (highly likely when J_1 ≈ 0), the empirical gradient estimate is zero. The "reward" is sparse to the point of invisibility.
- Core assumption: The training relies on Monte Carlo sampling estimates (like REINFORCE) rather than oracle gradients.
- Evidence anchors:
  - [section 3.1] "Situation I... If the model fails to sample any correct solutions... the empirical gradient estimate is zero."
  - [abstract] "...provides a vanishing learning signal in regimes where exploration is most critical."
  - [corpus] "Risk-Sensitive RL for Alleviating Exploration Dilemmas" (Corpus) supports the difficulty of exploration in LLMs due to "sharply peaked initial policies," reinforcing the claim that initial exploration is a critical failure point.
- Break condition: If a forcing mechanism (like expert demonstration or high-temperature sampling) ensures correct trajectories are found early, this "cold start" vanishing gradient is avoided.

### Mechanism 3
- Claim: As the policy improves and concentrates probability mass, pass@k converges to pass@1, rendering the multi-sample objective redundant.
- Mechanism: Theorem 3.2 shows that RL updates tend to concentrate mass on discovered modes (M_1). As probability mass concentrates (J_1 → 1), the scaling factor α_k → 0. Simultaneously, the marginal utility of k samples Δ_t(k) = pass@k - pass@1 approaches 0. The system "exploits" the known solution until the benefit of "exploring" with k samples is mathematically eliminated.
- Core assumption: The policy update follows standard gradient ascent which maximizes the likelihood of observed samples.
- Evidence anchors:
  - [section 3.2] "As the policy concentrates probability mass... the gap between pass@k and pass@1 diminishes."
  - [section 4] "...pass@k metric mathematically converges to pass@1... rendering the computational expense of generating k samples redundant."
  - [corpus] "SimKO" (Corpus) and "Reinforcement Learning vs. Distillation" (Corpus) empirically validate this collapse, noting that RLVR often improves pass@1 while reducing pass@k (capability) compared to base models.
- Break condition: Explicit entropy bonuses or anti-concentration regularization terms could prevent α_k from collapsing to zero.

## Foundational Learning

- Concept: **Policy Gradient (REINFORCE)**
  - Why needed here: The paper's critique relies on how gradient estimates are calculated from sampled trajectories. Understanding that ∇J ≈ Σ ∇ log π(y|x) · R helps explain why getting zero reward (no correct samples) results in zero learning, even if the theoretical objective exists.
  - Quick check question: If a model samples 10 trajectories and all are incorrect, what is the estimated gradient update for that batch? (Answer: Zero).

- Concept: **Pass@k Probability Formula**
  - Why needed here: The paper defines pass@k as 1 - (1 - J_1)^k. Understanding this binomial survival function is necessary to see why a high k doesn't help if J_1 is extremely low (vanishing probability of hitting success) or extremely high (certain success).
  - Quick check question: If a model has a 10% chance of solving a problem (J_1=0.1), what is the probability it solves it at least once in 5 tries (k=5)?

- Concept: **Exploration vs. Exploitation**
  - Why needed here: The paper argues that pass@k is intended to encourage exploration but mathematically incentivizes exploitation (concentration). One must distinguish between the metric (checking if solutions exist) and the objective (driving learning).
  - Quick check question: Does maximizing the probability of finding a solution in one specific mode (Exploitation) guarantee finding solutions in other diverse modes (Exploration)?

## Architecture Onboarding

- Component map: Generator (LLM) -> Verifier -> Optimizer -> Monitor
- Critical path: The transition from Situation I (model fails, needs exploration) to Situation II (model succeeds, needs exploitation). The paper argues the pass@k objective fails to bridge this gap effectively.
- Design tradeoffs:
  - Diagnostic vs. Objective: The paper advises keeping pass@k as a validation metric (diagnostic) to measure latent capability, but warns against using it as the loss function.
  - Sample Budget (k): Increasing k increases computational cost linearly, but the paper proves this cost yields diminishing returns as the policy converges (α_k → 0).
- Failure signatures:
  - High Variance/Zero Gradient: In early training or on hard problems, the model outputs all zeros (no correct samples), resulting in no weight updates.
  - Capability Collapse: During training, pass@1 rises, but pass@k (for k>1) stagnates or drops relative to the base model. This indicates the model is "overfitting" to a single reasoning path (Exploration Collapse).
  - Corpus Alert: "SimKO" (Corpus) specifically identifies "probability concentration" on top-1 tokens as a failure mode causing reduced pass@k performance.
- First 3 experiments:
  1. **Gradient Norm Comparison**: Implement the derived pass@k gradient and compare the norm of updates against a standard pass@1 baseline over the course of training. Verify if the signal vanishes as accuracy improves.
  2. **Stress Test on Hard Prompts**: Filter for a set of "hard" prompts where initial J_1 ≈ 0. Train using pass@k objective and observe if the model learns anything (hypothesis: it will not, due to zero empirical gradient).
  3. **Diversity Monitoring**: Train a standard RLVR model and plot pass@k vs. pass@1 divergence over time. Check if the gap Δ_t(k) decreases as predicted by Theorem 3.2.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific mechanisms can explicitly encourage exploration in RLVR without relying on the pass@k gradient signal?
- Basis in paper: [explicit] The conclusion advocates for "mechanisms explicitly encouraging efficient exploration" as the path forward, rather than optimizing pass@k directly.
- Why unresolved: The paper theoretically disproves the efficacy of pass@k as an objective but does not propose or validate a specific alternative algorithm.
- What evidence would resolve it: The derivation and empirical validation of a new objective function that maintains a diverse policy distribution and improves pass@k without suffering from gradient vanishing.

### Open Question 2
- Question: Does the strict collinearity between ∇J_k and ∇J_1 persist if the k samples are drawn dependently rather than independently?
- Basis in paper: [inferred] The derivation of the gradient (Eq. 6) and the definition of the objective (Eq. 4) explicitly rely on k i.i.d. samples.
- Why unresolved: If samples are correlated (e.g., via diverse decoding strategies), the probability of success might not simply be 1-(1-J_1)^k, potentially altering the gradient relationship.
- What evidence would resolve it: A theoretical analysis of the pass@k gradient under non-i.i.d. sampling distributions or diverse beam search strategies.

### Open Question 3
- Question: Can intrinsic motivation or entropy bonuses effectively mitigate the "exploration collapse" described in Theorem 3.2?
- Basis in paper: [inferred] The paper identifies "exploration collapse" where probability mass concentrates on a single mode, causing pass@k to converge to pass@1.
- Why unresolved: While the authors identify the failure mode, they do not test whether standard regularization techniques (like entropy penalties) are sufficient to counteract this concentration during RLVR.
- What evidence would resolve it: Experiments showing that adding an entropy bonus to the reward function prevents the convergence of pass@k to pass@1 during training.

## Limitations
- The analysis is purely theoretical with no empirical validation on actual RLVR training runs
- No specific experimental protocols, hyperparameter settings, or model specifications are provided
- The paper does not propose or validate alternative exploration mechanisms

## Confidence

**High confidence**: The mathematical derivation that pass@k gradients are scalar multiples of pass@1 gradients (Theorem 3.1) is straightforward and well-established calculus.

**Medium confidence**: The theoretical argument about "exploration collapse" (Theorem 3.2) follows logically from standard policy gradient convergence theory, but empirical confirmation is needed.

**Medium confidence**: The practical recommendation to treat pass@k as diagnostic rather than objective is reasonable given the theoretical analysis, but requires experimental validation.

## Next Checks

1. **Gradient Behavior Validation**: Implement the derived pass@k gradient formula and verify experimentally that the gradient norm decreases proportionally to the scaling factor α_k as J_1 increases during training.

2. **Exploration Collapse Measurement**: Train an RLVR model while tracking pass@k vs pass@1 divergence over time. Measure whether the gap decreases as predicted when policy concentrates on discovered solutions.

3. **Hard Prompt Failure Test**: Select a subset of problems where initial pass@1 ≈ 0 and train using pass@k objective. Verify whether the model learns anything (expecting zero updates due to lack of correct samples).