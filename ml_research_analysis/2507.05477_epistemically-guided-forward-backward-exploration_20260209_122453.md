---
ver: rpa2
title: Epistemically-guided forward-backward exploration
arxiv_id: '2507.05477'
source_url: https://arxiv.org/abs/2507.05477
tags:
- exploration
- reward
- learning
- task
- uncertainty
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes an exploration method for zero-shot reinforcement
  learning that reduces epistemic uncertainty in forward-backward (FB) representations.
  Unlike previous approaches that use decoupled exploration strategies, the authors
  introduce an algorithm that selects exploration policies based on predictive uncertainty
  over Q-values, specifically targeting reward embeddings with the highest variance.
---

# Epistemically-guided forward-backward exploration

## Quick Facts
- arXiv ID: 2507.05477
- Source URL: https://arxiv.org/abs/2507.05477
- Reference count: 40
- One-line primary result: An exploration method for zero-shot RL that reduces epistemic uncertainty in FB representations by selecting policies based on Q-value variance.

## Executive Summary
This paper proposes an exploration method for zero-shot reinforcement learning that reduces epistemic uncertainty in forward-backward (FB) representations. Unlike previous approaches that use decoupled exploration strategies, the authors introduce an algorithm that selects exploration policies based on predictive uncertainty over Q-values, specifically targeting reward embeddings with the highest variance. The method demonstrates significantly improved sample efficiency compared to random exploration while achieving similar or better asymptotic performance than the original FB algorithm.

## Method Summary
The method uses an ensemble of K=5 forward networks (F) with a single shared backward network (B) to estimate epistemic uncertainty in FB representations. The algorithm selects exploration policies by computing Q-value variance across ensemble members for different reward embeddings (z), then executing the policy with maximum uncertainty. Each ensemble member is trained independently using the FB loss with orthonormality regularization on B. The exploration loop samples candidate z values from a hypersphere, computes Var[Q^π_z] via ensemble disagreement, selects z_E maximizing uncertainty, and executes the corresponding greedy policy to collect transitions.

## Key Results
- FBEE Q achieves 3-5x sample efficiency improvement over random exploration on DeepMind Control Suite tasks
- Performance matches or exceeds original FB algorithm's asymptotic returns
- Faster learning of optimal policies across diverse tasks (Walker, Cheetah, Hopper, Quadruped, Point-mass maze)
- Successfully reduces epistemic uncertainty through targeted exploration

## Why This Works (Mechanism)

### Mechanism 1: Fixing the Backward Representation to Validating Uncertainty
The paper claims that meaningful epistemic uncertainty estimation requires fixing the backward representation (B) and only ensembling the forward representation (F). The FB factorization is non-unique due to rotational invariance, so naively ensembling both F and B causes disagreement to reflect different rotational baselines rather than genuine uncertainty. By fixing B, the ensemble variance of F becomes a meaningful proxy for epistemic uncertainty.

### Mechanism 2: Projecting Latent Uncertainty to Q-Value Variance
The paper claims that selecting exploration policies based on Q-function variance is more effective than using raw covariance of F. The raw uncertainty of F is a matrix quantity, which is projected into scalar predictive variance over Q-values using the reward embedding z. This projects "state-space" uncertainty into "value-space" uncertainty, allowing identification of which specific policy π_z is most uncertain about its own returns.

### Mechanism 3: Uncertainty Sampling for Data Collection
The paper claims that a "pure exploration" strategy selecting the policy π_z maximizing predictive variance minimizes epistemic uncertainty more efficiently than random exploration. This functions as a variant of uncertainty sampling (active learning), where the agent forces observation of transitions that are currently "unknown" by executing the most uncertain policy, driving reduction of posterior variance where it is highest.

## Foundational Learning

- **Concept: Successor Measures & FB Representations**
  - Why needed: The entire method operates on the factorization of the successor measure M. Understanding F represents forward dynamics and B represents backward (state) features is essential for why we fix B and ensemble F.
  - Quick check: Can you explain why ⟨F, B⟩ approximates the discounted future occupancy of a policy?

- **Concept: Epistemic vs. Aleatoric Uncertainty**
  - Why needed: The paper explicitly targets epistemic (reducible, model) uncertainty. Confusing this with aleatoric (inherent noise) uncertainty leads to expecting the algorithm to "solve" noisy environments.
  - Quick check: In an ensemble, does disagreement between members represent noise in the data or lack of data?

- **Concept: Zero-Shot Reinforcement Learning**
  - Why needed: The exploration loop has no external reward. The goal is to learn a universal representation. Understanding that z acts as a "task descriptor" is vital for interpreting the exploration policy.
  - Quick check: How does the agent determine which policy π_z to execute if no external reward function is provided during training?

## Architecture Onboarding

- **Component map:**
  - Inputs: State s
  - Core Networks: Backward Network (B_φ) mapping (s, a) → R^d (Fixed for uncertainty calc, updated for representation learning)
  - Core Networks: Forward Ensemble ({F_θ_k}_{k=1}^K) mapping (s, a, z) → R^d
  - Core Networks: Policy Heads (implicitly defined via π_z(s) = argmax_a ⟨F(s, a, z), z⟩)
  - Buffer: Standard replay buffer D

- **Critical path:**
  1. Inference (Exploration): Given state s, sample candidate z's. For each z, compute Q across ensemble members. Select z_E with highest variance. Execute π_{z_E}.
  2. Data Collection: Store transition (s, a, s', z_E) in buffer.
  3. Training: Sample batch. Update B (standard FB loss). Update each F_k independently against shared target networks to maintain ensemble diversity.

- **Design tradeoffs:**
  - Ensemble Size (K): Larger K gives better uncertainty estimates but increases compute linearly. Paper uses K=5.
  - Update Frequency of z_E: Updating z_E every step is more computationally expensive but offers better targeting than updating once per episode.
  - Shared vs. Decoupled B: The paper mandates a shared B for uncertainty consistency, but this creates a bottleneck if B learns slowly.

- **Failure signatures:**
  - Catastrophic Drop in Variance: If the ensemble collapses to a mean quickly without learning optimal policies, B may be overfitted or learning rates are too high.
  - Stuck in Noisy States: If variance never decreases in specific states, those states likely have high aleatoric noise.
  - Rotation Drift: If B is not properly regularized (orthonormality loss), the embedding space may rotate, invalidating the fixed-ensemble premise.

- **First 3 experiments:**
  1. Sanity Check (FB vs FBEE): Run standard FB (random z) vs. FBEE on a simple DMC task (e.g., Cheetah) to verify sample efficiency curve diverges as shown in Figure 4.
  2. Ablation (Fixed B vs Ensemble B): Implement an ensemble for both F and B to observe if the "rotation issue" described in Section 4 actually destabilizes training or reduces sample efficiency.
  3. Metric Analysis: Log the correlation between F-variance (matrix trace) and Q-variance to verify if the projection mechanism is necessary for the specific environment being tested.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can exploration algorithms for zero-shot RL account for the correlation of information across different reward embeddings (z ∈ Z) rather than selecting policies greedily?
- Basis in paper: The conclusion states that an efficient algorithm "necessarily needs to take into account how information is correlated across different z ∈ Z in order to maximally reduce it with least amount of data."
- Why unresolved: The proposed method (FBEE Q) greedily selects the z with the highest uncertainty at each step without modeling how executing a policy for one z might simultaneously reduce uncertainty for other z's.
- What evidence would resolve it: An algorithm that explicitly models mutual information across the space of policies Z and demonstrates higher sample efficiency than the greedy FBEE Q baseline.

### Open Question 2
- Question: Can a theoretically sound, full Bayesian posterior be formulated over both Forward (F) and Backward (B) representations despite their inherent non-uniqueness?
- Basis in paper: The conclusion identifies "a full Bayesian treatment of FB representations" as an open question, noting the difficulty of assuming a full posterior over both F and B due to the "non-uniqueness" of the representation (rotation invariance).
- Why unresolved: The authors were forced to fix B and only model the posterior over F to create an informative uncertainty signal. A joint posterior remains elusive because standard ensemble disagreement is uninformative when representations can be arbitrarily rotated.
- What evidence would resolve it: A derivation showing that a specific joint posterior structure handles the rotational symmetry, or an algorithm that successfully utilizes uncertainty from both representations to improve exploration.

### Open Question 3
- Question: Does the use of target networks in the ensemble truly propagate uncertainty temporally, effectively alleviating the myopic limitations of uncertainty sampling in this setting?
- Basis in paper: The authors hypothesize that their method works despite being myopic because target networks provide "temporally extended (and consistent) estimate[s] of the value uncertainty," but explicitly state "We leave this analysis to future work."
- Why unresolved: Theoretically, the method is a greedy, step-wise optimization (myopic). It is unclear empirically or theoretically if target networks are the specific mechanism preventing the algorithm from getting stuck in short-sighted exploration loops.
- What evidence would resolve it: An ablation study varying target network update rates to observe their effect on the temporal consistency of exploration, or a comparison against a non-myopic Bayesian experimental design baseline.

## Limitations
- The core claims hinge on the assumption that fixing B while ensembling F yields meaningful epistemic uncertainty, but this is not empirically validated against alternatives
- The projection from representation uncertainty to Q-variance relies on a theoretical correlation that shows only weak empirical support (R² = 0.18)
- The algorithm's performance may degrade in environments with significant aleatoric uncertainty, which is not thoroughly addressed

## Confidence
- High confidence: Sample efficiency improvements over random exploration are demonstrated across multiple DeepMind Control Suite tasks
- Medium confidence: The mechanism of fixing B to obtain meaningful uncertainty estimates is theoretically sound but lacks direct empirical validation
- Low confidence: The claim that Q-variance projection is superior to direct representation uncertainty sampling, given the weak correlation evidence

## Next Checks
1. Implement and compare an alternative ensemble design where B is also ensembled but with orthogonalization constraints to test if the fixed-B assumption is truly necessary
2. Measure and report the correlation between F-variance and Q-variance across different environment types to validate the projection mechanism's effectiveness
3. Test the algorithm's behavior in environments with high aleatoric uncertainty to verify it doesn't get stuck in noisy states, and implement an aleatoric/epistemic decomposition if needed