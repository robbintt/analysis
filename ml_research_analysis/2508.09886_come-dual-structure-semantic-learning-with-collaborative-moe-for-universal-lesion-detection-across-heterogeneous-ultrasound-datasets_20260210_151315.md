---
ver: rpa2
title: 'COME: Dual Structure-Semantic Learning with Collaborative MoE for Universal
  Lesion Detection Across Heterogeneous Ultrasound Datasets'
arxiv_id: '2508.09886'
source_url: https://arxiv.org/abs/2508.09886
tags:
- come
- datasets
- clustering
- experts
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes COME, a universal framework for lesion detection\
  \ across heterogeneous ultrasound datasets. COME integrates dual structure-semantic\
  \ shared experts (STE and SEE) with a mixture of heterogeneous source-specific experts\
  \ (S\xB2E) to address domain shifts and preserve dataset-specific discriminative\
  \ features."
---

# COME: Dual Structure-Semantic Learning with Collaborative MoE for Universal Lesion Detection Across Heterogeneous Ultrasound Datasets

## Quick Facts
- **arXiv ID:** 2508.09886
- **Source URL:** https://arxiv.org/abs/2508.09886
- **Reference count:** 40
- **Primary result:** COME outperforms state-of-the-art methods on 8 heterogeneous ultrasound datasets, achieving significant mAP improvements across single-dataset, intra-organ, and inter-organ training paradigms.

## Executive Summary
This paper introduces COME, a universal framework for lesion detection across heterogeneous ultrasound datasets. COME integrates dual structure-semantic shared experts (STE and SEE) with a mixture of heterogeneous source-specific experts (S²E) to address domain shifts and preserve dataset-specific discriminative features. The framework employs Fine2Coarse or Multi-Step clustering strategies to route tokens to specialized experts, enabling robust generalization. Extensive experiments on eight ultrasound datasets (four breast, four thyroid) show COME outperforms state-of-the-art methods, achieving significant improvements in mean Average Precision (mAP) across single-dataset, intra-organ, and inter-organ training paradigms. The ablation study confirms the effectiveness of shared experts and clustering strategies in enhancing model performance.

## Method Summary
COME extends DINO-ViT by replacing standard FFN layers with a Collaborative Mixture of Experts (COME) module. The architecture splits feature learning into universal priors (STE for structure, SEE for semantics) and source-specific discriminative features (S²E). STE uses a frozen USFM model, SEE uses a trainable MedCLIP adapter, and S²E contains 8 MoE experts. Token routing uses clustering (Fine2Coarse or Multi-Step) on multi-head attention outputs, with Traceability Loss enforcing expert-source alignment. The framework is trained with detection loss, load balance loss, and traceability loss on 8 heterogeneous ultrasound datasets.

## Key Results
- **Outperforms baselines:** COME achieves higher mAP than DINO, Mask DINO, and other MoE variants across all training paradigms
- **Ablation validation:** Removing shared experts (STE/SEE) causes ~7% mAP drop, confirming their importance for universal representation
- **Expert allocation study:** Optimal performance at 8 experts for 8 datasets, supporting the specialization theory
- **Robust generalization:** Maintains strong performance in zero-shot and few-shot settings across organ types

## Why This Works (Mechanism)

### Mechanism 1: Dual Shared Experts Minimize Negative Transfer
The architecture decomposes feature learning into universal priors (STE/SEE) and source-specific discriminative features (S²E). STE captures structural priors across datasets while SEE aligns semantic features. By adding these outputs, the model prevents the averaging effect of multi-dataset training that washes out rare features. Evidence shows 7% mAP drop when shared experts are removed.

### Mechanism 2: Clustering-Based Routing Stabilizes Expert Specialization
Instead of routing raw tokens directly to experts, COME clusters tokens via K-means (Fine2Coarse or Multi-Step). This forces the router to attend to semantic clusters rather than pixel-level noise, reducing suboptimal expert assignment. The clustering improves feature concentration in t-SNE visualizations and prevents routing collapse.

### Mechanism 3: Traceability Loss Enforces Expert Specialization
COME adds Source-Specific Traceability Loss ($L_{TB}$), a cross-entropy supervision that pushes tokens from Dataset $X$ toward Expert $X$. This prevents the natural tendency of routers to collapse or oscillate, ensuring source-specific experts actually learn specific features. Performance peaks when expert count matches dataset count (8:8).

## Foundational Learning

- **Mixture of Experts (MoE):** Replaces dense layers with sparse "experts" and a "router" to handle data heterogeneity. Quick check: Does the model use all experts for every image, or does the router select a subset? (Answer: Top-K subset).
- **Domain Shift / Inter-Dataset Interference:** Mixing US data from different machines/organs creates "distribution shifts" that confuse standard models. Quick check: Why can't we just concatenate all 8 datasets and train a standard DINO model? (Answer: Domain shifts cause "negative transfer" or interference).
- **Visual Foundation Models (USFM / MedCLIP):** The "Universal" capability comes from pre-trained weights. STE uses USFM (image-only); SEE uses MedCLIP (image-text). Quick check: What is the difference between the knowledge encoded in STE vs. SEE? (Answer: STE = Structural/Low-level; SEE = Semantic/High-level).

## Architecture Onboarding

- **Component map:** Input Image → Patch Embedding → [ViT Encoder + COME Blocks] → [ViT Decoder + COME Blocks] → Detection Head
- **Critical path:** The feature summation ($F_{final} = F_{STE} + F_{SEE} + F_{S2}$) happens inside every COME block before the residual connection.
- **Design tradeoffs:** Fine2Coarse is faster (132 μs) and generally sufficient; Multi-Step is slower (615 μs) but better for highly entangled features. Expert count ideally matches dataset count (8 experts for 8 datasets).
- **Failure signatures:** Router collapse (all inputs route to 1-2 experts) - fix by checking Load Balance Loss weight; low recall on small datasets - ensure STE/SEE weights loaded correctly; training instability - check clustering preprocessing for empty clusters.
- **First 3 experiments:** 1) Run with only S²E (disable STE/SEE) to quantify universal priors contribution; 2) Train with 4 vs. 8 vs. 10 experts to verify optimal specialization; 3) Train on Breast only, test on Thyroid for zero-shot organ transfer.

## Open Questions the Paper Calls Out

- **Q1:** Can LoRA or scaling to larger model sizes further improve COME performance? (Section 8: Current MoE design not cutting-edge; LoRA not fully implemented).
- **Q2:** How effectively does COME generalize to medical imaging tasks beyond lesion detection, such as semantic segmentation? (Section 8: Focus restricted to lesion detection; next step is expanding to other tasks).
- **Q3:** Does COME maintain robust performance on ultrasound images from organs other than breast and thyroid, or from significantly different imaging devices? (Section 8: Study limited to two anatomical structures; potential for further extension).
- **Q4:** How robust is SEE when structured textual descriptions are unavailable or highly noisy? (Inferred: Paper assumes clean label-derived text; doesn't test unstructured reports).

## Limitations
- **Data generalization uncertainty:** Performance on truly unseen datasets (different manufacturers/organs) remains untested.
- **Routing stability concerns:** No long-term stability analysis of routing distributions over extended training or shifting distributions.
- **Computational overhead validation:** Clustering-based routing overhead (132-615 μs) not analyzed for scalability with dataset size.

## Confidence
- **High Confidence:** Ablation results (7% mAP drop without shared experts) and expert allocation study (8 experts optimal) are directly supported by experiments.
- **Medium Confidence:** Negative transfer prevention claims rely on indirect evidence through comparative results rather than isolated quantification.
- **Low Confidence:** Theoretical necessity of dual shared experts (STE+SEE vs. single shared expert) is not rigorously tested.

## Next Checks
1. **Zero-Shot Transfer Validation:** Train COME on breast datasets only, then evaluate on thyroid datasets without fine-tuning to test universal representation capability.
2. **Expert Specialization Stability:** Monitor expert routing distributions over training epochs across all datasets to verify $L_{TB}$ maintains stable specialization.
3. **Scalability Analysis:** Test COME with 4, 8, and 12 experts on inter-organ benchmark with statistical significance testing to verify true optimality of 8-expert configuration.