---
ver: rpa2
title: Text-Queried Audio Source Separation via Hierarchical Modeling
arxiv_id: '2505.21025'
source_url: https://arxiv.org/abs/2505.21025
tags:
- audio
- separation
- semantic
- ieee
- speech
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes HSM-TSS, a hierarchical modeling framework
  for text-queried audio source separation. The approach addresses the challenges
  of joint cross-modal alignment and semantic-aware separation by decomposing the
  task into global-local semantic-guided feature separation and structure-preserving
  acoustic reconstruction.
---

# Text-Queried Audio Source Separation via Hierarchical Modeling

## Quick Facts
- arXiv ID: 2505.21025
- Source URL: https://arxiv.org/abs/2505.21025
- Reference count: 40
- Primary result: State-of-the-art text-queried audio source separation with hierarchical modeling achieving superior separation quality and semantic consistency across multiple datasets

## Executive Summary
This paper proposes HSM-TSS, a hierarchical modeling framework for text-queried audio source separation that addresses the challenges of joint cross-modal alignment and semantic-aware separation. The approach decomposes the task into global-local semantic-guided feature separation and structure-preserving acoustic reconstruction through a dual-stage mechanism operating on distinct semantic feature spaces. HSM-TSS introduces a Q-Audio architecture pretrained for text-audio alignment serving as global-semantic encoders, achieving state-of-the-art separation performance with data-efficient training while maintaining superior semantic consistency with queries. The framework demonstrates strong zero-shot generalization capabilities and excels particularly in challenging scenarios with high source overlap.

## Method Summary
HSM-TSS employs a hierarchical modeling framework that decomposes text-queried audio source separation into three stages: (1) global-semantic separation using a 6-layer NAR transformer with Q-Audio as frozen global-semantic encoders, (2) local-semantic separation using a 12-layer NAR transformer operating on features extracted by AudioMAE, and (3) an AR decoder with TF-Codec for acoustic reconstruction. The method achieves cross-modal alignment through contrastive, matching, and generation losses during Q-Audio pretraining, then performs joint fine-tuning of stages 1-2 with a switcher mechanism to mitigate error propagation. Training uses 1.7M pairs from AudioCaps, Clotho, ESC-50, FSD50K, and WavCaps with SNR ∈ [-15dB, 15dB], while the decoder is trained on AudioSet + Jamendo + speech data.

## Key Results
- Achieves state-of-the-art LSD and PSNR metrics across AudioCaps, Clotho, and FSD50K datasets
- Demonstrates superior semantic consistency with queries (CLAP score↑, AFSim↑) compared to baseline methods
- Shows strong zero-shot generalization capabilities with data-efficient training
- Excels in challenging scenarios with high source overlap where traditional methods degrade

## Why This Works (Mechanism)
The hierarchical decomposition separates semantic alignment (handled globally via pretrained Q-Audio) from acoustic reconstruction (handled locally via AudioMAE features), allowing each stage to specialize. The dual-stage semantic processing first extracts broad semantic features before refining with local acoustic details, preventing the semantic encoder from being overwhelmed by fine-grained acoustic information. Joint fine-tuning with a switcher mechanism mitigates error propagation between stages while maintaining their specialized roles.

## Foundational Learning
- **Cross-modal contrastive learning**: Aligns text and audio representations in shared embedding space; needed for accurate semantic matching between queries and audio content; quick check: retrieval R@1 >25% on validation set
- **Hierarchical semantic decomposition**: Separates global semantic concepts from local acoustic details; needed to prevent semantic encoder overload and enable specialized processing; quick check: ablation shows performance drop without hierarchy
- **Non-autoregressive vs autoregressive transformers**: NAR for semantic separation (parallel processing), AR for acoustic reconstruction (sequential dependencies); needed to balance efficiency and reconstruction quality; quick check: compare training/inference speed vs quality
- **Error propagation mitigation**: Joint fine-tuning with switcher mechanism; needed to prevent accumulated errors between sequential stages; quick check: KL divergence between gt-Ġ and predicted Ĝ remains low
- **Audio feature extraction with self-supervised learning**: AudioMAE pretrained on large audio corpus; needed to provide semantically meaningful local features for separation; quick check: reconstruction quality with frozen vs finetuned AudioMAE
- **Instruction parsing pipeline**: LLM-based decomposition of free-form instructions; needed to convert natural language to structured queries; quick check: semantic alignment accuracy across instruction paraphrases

## Architecture Onboarding

**Component Map**: Audio input → AudioMAE (local features) → Q-Audio (global features) → Global-Semantic Separation → Local-Semantic Separation → AR Decoder → Output sources

**Critical Path**: The separation pipeline flows from input audio through both semantic separation stages before acoustic reconstruction. The Q-Audio provides global semantic guidance to the first NAR transformer, whose output conditions the second NAR transformer operating on local features from AudioMAE, with final AR decoding producing the separated sources.

**Design Tradeoffs**: Hierarchical decomposition trades model complexity for specialized processing and improved semantic alignment, versus monolithic end-to-end approaches. The dual-stage semantic processing adds computational overhead but enables better handling of complex queries and high-overlap scenarios compared to single-stage alternatives.

**Failure Signatures**: Poor cross-modal alignment manifests as semantic inconsistency between queries and outputs (low CLAP/AFSim scores). Error propagation appears as degradation in local separation quality when global features are inaccurate. The switcher mechanism during joint fine-tuning should show reduced KL divergence between using ground truth versus predicted global features.

**Three First Experiments**:
1. **Cross-modal alignment verification**: Implement Q-Audio pretraining and measure text-to-audio retrieval performance (R@1) on AudioCaps/Clotho validation sets; target >25% R@1
2. **Stage-wise error propagation analysis**: During joint fine-tuning, monitor KL divergence between using ground truth global features versus predicted global features as input to local separation stage
3. **Ablation on semantic hierarchy**: Train single-stage variant without hierarchical decomposition and compare performance across LSD, PSNR, and semantic consistency metrics

## Open Questions the Paper Calls Out
- **Open Question 1**: Can the hierarchical modeling paradigm be effectively extended to speech domain where temporal-semantic relationships differ significantly from general environmental audio?
- **Open Question 2**: How can the framework support instruction-based audio editing operations beyond separation, such as style transfer or in-place modification of acoustic properties?
- **Open Question 3**: To what extent does error propagation between the global-semantic and local-semantic separation stages affect performance, and can it be further mitigated without joint fine-tuning?
- **Open Question 4**: Can the instruction parsing pipeline be made fully end-to-end without relying on external LLMs (GPT-4) for arbitrary text instruction decomposition?

## Limitations
- Missing training hyperparameters (learning rates, batch sizes, optimizers) across all stages limit faithful reproduction
- Unspecified embedding dimensions and cross-attention configurations between Q-Audio and AudioMAE components
- GPT-4 prompt templates for mixture synthesis categorization not provided, introducing variability in data preparation

## Confidence
- **High confidence**: Reported state-of-the-art separation performance metrics across multiple datasets
- **Medium confidence**: Superior semantic consistency with queries based on CLAP and AFSim metrics
- **Low confidence**: Zero-shot generalization capabilities due to lack of ablation studies on limited data training

## Next Checks
1. **Cross-modal alignment verification**: Implement Q-Audio pretraining and measure text-to-audio retrieval performance on AudioCaps and Clotho using R@1 metric. Target >25% R@1 to ensure sufficient alignment quality before proceeding to separation stages.

2. **Stage-wise error propagation analysis**: During joint fine-tuning of global and local stages, compare KL divergence between using ground truth global features versus predicted global features as input to the local separation stage. Monitor whether error propagates and degrades local separation quality.

3. **Ablation on semantic hierarchy**: Train simplified variants of HSM-TSS without the hierarchical decomposition (single-stage separation) and compare performance across LSD, PSNR, and semantic consistency metrics to quantify the contribution of the dual-stage mechanism.