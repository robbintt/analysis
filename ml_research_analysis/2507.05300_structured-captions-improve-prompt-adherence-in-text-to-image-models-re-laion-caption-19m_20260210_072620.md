---
ver: rpa2
title: Structured Captions Improve Prompt Adherence in Text-to-Image Models (Re-LAION-Caption
  19M)
arxiv_id: '2507.05300'
source_url: https://arxiv.org/abs/2507.05300
tags:
- captions
- images
- structured
- dataset
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses poor prompt adherence in text-to-image models,
  attributing it to noisy and unstructured training data. The authors propose structured
  captions to simplify learning and improve controllability.
---

# Structured Captions Improve Prompt Adherence in Text-to-Image Models (Re-LAION-Caption 19M)

## Quick Facts
- arXiv ID: 2507.05300
- Source URL: https://arxiv.org/abs/2507.05300
- Reference count: 38
- Structured captions improve text-to-image model prompt adherence

## Executive Summary
This paper addresses poor prompt adherence in text-to-image models, attributing it to noisy and unstructured training data. The authors propose structured captions to simplify learning and improve controllability. They introduce Re-LAION-Caption 19M, a 19 million image subset of Re-LAION-5B with captions generated by Mistral 7B Instruct-based LLaVA-Next following a four-part template: subject, setting, aesthetics, and camera details. Fine-tuning PixArt-Σ and Stable Diffusion 2 on structured versus shuffled captions shows consistent improvements in text-image alignment, with VQA-based scores increasing by up to 0.027 (LLaVA) and 0.021 (InstructBLIP) when using structured captions. The dataset is publicly available.

## Method Summary
The authors create Re-LAION-Caption 19M by filtering Re-LAION-5B to remove duplicates and NSFW content, then generating structured captions using Mistral 7B Instruct-based LLaVA-Next. The captioning follows a four-part template: subject, setting, aesthetics, and camera details. They fine-tune two text-to-image models (PixArt-Σ and Stable Diffusion 2) on both structured and shuffled versions of these captions. The structured captions maintain the template format while shuffled captions randomize word order. Model performance is evaluated using VQA-based metrics (LLaVA and InstructBLIP) to measure text-image alignment.

## Key Results
- VQA-based scores increased by up to 0.027 (LLaVA) and 0.021 (InstructBLIP) with structured captions
- Consistent improvements observed across both PixArt-Σ and Stable Diffusion 2 models
- Structured captions follow a four-part template: subject, setting, aesthetics, and camera details

## Why This Works (Mechanism)
Structured captions improve prompt adherence by reducing the complexity of the mapping between text and image. By organizing information into predictable categories (subject, setting, aesthetics, camera details), the models can more easily learn which visual elements correspond to which textual components. This structure provides explicit signal about the relationship between different aspects of the prompt, making it easier for the model to disentangle and correctly render each component.

## Foundational Learning
- **Text-to-image alignment**: The correspondence between textual prompts and generated images is fundamental to evaluating model performance. Understanding this alignment is necessary because the paper's improvements are measured through text-image consistency metrics rather than direct prompt adherence.
- **VQA-based evaluation metrics**: These metrics assess how well image captions describe their corresponding images using visual question answering models. They're needed because direct human evaluation of prompt adherence is expensive and subjective.
- **Caption structure and prompt adherence**: The organization of textual descriptions affects how well models can parse and implement visual instructions. This concept is central because the paper's core hypothesis is that structured captions improve learning efficiency.

## Architecture Onboarding
- **Component map**: Re-LAION-5B dataset -> Filtering pipeline -> Caption generation (Mistral 7B Instruct-based LLaVA-Next) -> Re-LAION-Caption 19M -> Model fine-tuning (PixArt-Σ/Stable Diffusion 2) -> Evaluation (LLaVA/InstructBLIP metrics)
- **Critical path**: Dataset creation → Caption generation → Model fine-tuning → Evaluation
- **Design tradeoffs**: Structured captions provide better learning signals but may limit creative variation in descriptions. The four-part template balances comprehensiveness with simplicity but may not capture all prompt types equally well.
- **Failure signatures**: If structured captions don't improve performance, potential causes include poor caption quality from the generation model, insufficient model capacity to leverage structure, or evaluation metrics not aligning with true prompt adherence.
- **First experiments**:
  1. Compare structured vs shuffled captions on a small validation set before full training
  2. Test caption generation quality using a separate captioning evaluation model
  3. Conduct ablation studies varying individual template components

## Open Questions the Paper Calls Out
None

## Limitations
- Improvements in text-image alignment (0.027 LLaVA and 0.021 InstructBLIP) are statistically meaningful but relatively modest in absolute terms
- Evaluation relies on VQA-based metrics that measure caption-image consistency rather than direct prompt adherence
- Study focuses exclusively on two model architectures, limiting generalizability

## Confidence
- High confidence in the methodology for caption generation and dataset creation
- Medium confidence in the measured improvements, given the modest absolute gains and potential metric misalignment
- Medium confidence in the attribution of improvements specifically to caption structure rather than other factors in the fine-tuning process

## Next Checks
1. Conduct ablation studies varying caption structure while holding other factors constant to isolate the specific contribution of structural formatting
2. Evaluate prompt adherence using direct human judgment studies rather than proxy VQA metrics to validate the claimed improvements
3. Test the approach across a broader range of text-to-image models beyond PixArt-Σ and Stable Diffusion 2 to assess generalizability