---
ver: rpa2
title: 'Open-Qwen2VL: Compute-Efficient Pre-Training of Fully-Open Multimodal LLMs
  on Academic Resources'
arxiv_id: '2504.00595'
source_url: https://arxiv.org/abs/2504.00595
tags:
- data
- multimodal
- pre-training
- arxiv
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Open-Qwen2VL, a fully open-source multimodal
  large language model (MLLM) that achieves state-of-the-art performance while being
  highly compute-efficient. The authors address the challenge of reproducing state-of-the-art
  MLLM pretraining by releasing all aspects of their work, including training codebase,
  data filtering techniques, and datasets.
---

# Open-Qwen2VL: Compute-Efficient Pre-Training of Fully-Open Multimodal LLMs on Academic Resources

## Quick Facts
- arXiv ID: 2504.00595
- Source URL: https://arxiv.org/abs/2504.00595
- Authors: Weizhi Wang; Yu Tian; Linjie Yang; Heng Wang; Xifeng Yan
- Reference count: 40
- This paper introduces Open-Qwen2VL, a fully open-source multimodal large language model (MLLM) that achieves state-of-the-art performance while being highly compute-efficient.

## Executive Summary
This paper introduces Open-Qwen2VL, a fully open-source multimodal large language model (MLLM) that achieves state-of-the-art performance while being highly compute-efficient. The authors address the challenge of reproducing state-of-the-art MLLM pretraining by releasing all aspects of their work, including training codebase, data filtering techniques, and datasets. The core method involves efficient pretraining on 29M image-text pairs using only 220 A100-40G GPU hours, achieving 5B packed multimodal tokens - just 0.36% of the tokens used in Qwen2-VL pretraining. Key innovations include low-to-high dynamic image resolution, multimodal sequence packing for efficiency, and careful data curation using both MLLM-based (MLM-Filter) and CLIP-based filtering methods.

## Method Summary
Open-Qwen2VL pre-trains a 2B-parameter multimodal LLM efficiently using 29M image-text pairs from 4 sources: CCS-CLIP (8.5M), DataComp-DFN (15M), LAION-CLIP (15M), and DataComp-MLM-Filter & DFN (19.9M). The model uses Qwen2.5-1.5B-Instruct + SigLIP-SO-400M encoder + Adaptive AvgPool projector (144 visual tokens pretrain, 729 SFT). Training uses FSDP on 8×A100-40G, with multimodal sequence packing to 4096 context length. The frozen vision encoder and aggressive token compression enable pretraining in just 220 GPU-hours, followed by 48 GPU-hours of SFT on MAmmoTH-VL-10M.

## Key Results
- Outperforms partially-open state-of-the-art MLLM Qwen2-VL-2B on MMBench (80.9), SEEDBench (72.5), MMStar (49.7), and MathVista (53.1)
- Achieves pretraining with only 220 A100-40G GPU hours and 5B packed multimodal tokens (0.36% of baseline)
- Uses efficient data filtering (MLM-Filter + CLIP) and multimodal sequence packing to maximize compute efficiency
- Demonstrates that advanced MLLM pretraining can be accomplished with limited academic computing resources

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** High-quality data filtering using MLLM-based scoring (MLM-Filter) enables competitive performance with significantly fewer training tokens (0.36% of baseline).
- **Mechanism:** Instead of relying solely on CLIP-based image-text alignment scores, the authors use an efficient MLLM (`mlm-filter-qwen2.5-1.5b`) to score data on "Semantic Understanding" (SU). This filters out low-quality web-crawled captions that might have high CLIP similarity but poor semantic reasoning value, increasing the information density per token.
- **Core assumption:** The proxy model (`mlm-filter`) accurately predicts data quality for the target 2B model better than standard CLIP filters.
- **Evidence anchors:** [abstract] "careful data curation using both MLLM-based and CLIP-based filtering methods." [section 2.1] "...adding very small amount of high-quality data (5M) curated by a different efficient-MLLM based data filter, MLM-Filter can significantly enhance the model performance..."

### Mechanism 2
- **Claim:** "Low-to-High" dynamic resolution preserves visual reasoning capabilities while minimizing pre-training compute.
- **Mechanism:** The visual projector uses Adaptive Average-Pooling to aggressively compress visual patches (729 → 144 tokens) during pre-training. This reduces the sequence length and FLOPs. The model then scales back to full resolution (729 tokens) during Supervised Fine-Tuning (SFT) to recover fine-grained details.
- **Core assumption:** Visual alignment and coarse semantic understanding can be learned at low resolution; high-resolution features can be "unlocked" later via SFT without catastrophic loss of prior knowledge.
- **Evidence anchors:** [abstract] "...low-to-high dynamic image resolution... to significantly enhance pre-training efficiency." [section 2.2] "We adopt 144 visual tokens... in the pre-training stage and scale up... in SFT stage. Such low-to-high... does not hurt the high-resolution image understanding."

### Mechanism 3
- **Claim:** Multimodal sequence packing improves compute efficiency and elicits in-context learning (ICL) capabilities.
- **Mechanism:** The authors use a First-Fit-Decreasing (FFD) bin-packing algorithm to concatenate multiple unrelated image-text pairs into a single fixed-length sequence (4096 tokens). This minimizes padding waste and exposes the model to multiple image-text contexts in one forward pass.
- **Core assumption:** The model can distinguish context boundaries using separator tokens (`