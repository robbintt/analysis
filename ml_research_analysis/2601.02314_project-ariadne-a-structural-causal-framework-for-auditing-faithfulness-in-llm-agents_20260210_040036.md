---
ver: rpa2
title: 'Project Ariadne: A Structural Causal Framework for Auditing Faithfulness in
  LLM Agents'
arxiv_id: '2601.02314'
source_url: https://arxiv.org/abs/2601.02314
tags:
- reasoning
- causal
- ariadne
- faithfulness
- logic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Project Ariadne introduces a novel causal auditing framework for\
  \ LLM agents using Structural Causal Models (SCMs) and do-calculus interventions\
  \ to evaluate the faithfulness of Chain-of-Thought reasoning. By systematically\
  \ inverting logic, negating premises, and reversing factual claims in intermediate\
  \ reasoning steps, the framework quantifies Causal Sensitivity\u2014measuring how\
  \ much the terminal answer depends on the reasoning trace."
---

# Project Ariadne: A Structural Causal Framework for Auditing Faithfulness in LLM Agents

## Quick Facts
- arXiv ID: 2601.02314
- Source URL: https://arxiv.org/abs/2601.02314
- Authors: Sourena Khanzadeh
- Reference count: 8
- Primary result: Framework quantifies Causal Sensitivity (φ) by intervening in reasoning traces, revealing up to 96% violation density in scientific reasoning where contradictory logic yields identical answers

## Executive Summary
Project Ariadne introduces a novel causal auditing framework for LLM agents using Structural Causal Models (SCMs) and do-calculus interventions to evaluate the faithfulness of Chain-of-Thought reasoning. By systematically inverting logic, negating premises, and reversing factual claims in intermediate reasoning steps, the framework quantifies how much the terminal answer depends on the reasoning trace. Across 500 queries in three domains, the method revealed a significant Faithfulness Gap: up to 96% violation density in scientific reasoning where agents produced identical answers despite contradictory internal logic. This demonstrates that current LLM reasoning traces often function as post-hoc rationalizations rather than generative drivers of decisions. The framework introduces the Ariadne Score as a benchmark for distinguishing between truly causal reasoning and "Reasoning Theater" where decisions are governed by latent parametric priors.

## Method Summary
The framework formalizes LLM reasoning as an SCM and applies hard interventions (do-calculus) at intermediate reasoning steps to measure causal sensitivity. For each query, the original reasoning trace and answer are generated, then a counterfactual reasoning step is created by inverting logic, negating premises, or reversing facts. The model is re-executed from the intervention point to generate a new answer. The Causal Sensitivity Score φ = 1 − S(a, a*) quantifies how much the answer changes, where S is semantic similarity. Violations are flagged when answers remain similar despite substantial logical contradictions. The method was applied to 500 queries across three domains using GPT-4o-based agents with Claude 3.7 Sonnet as the similarity judge.

## Key Results
- 96% violation density in scientific reasoning where agents reached identical conclusions despite contradictory premises
- 92% violation density in general knowledge tasks showing similar decoupling between reasoning and answers
- 20% violation density in mathematical logic tasks, suggesting domain-dependent parametric prior strength

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hard interventions on intermediate reasoning nodes reveal whether the reasoning trace causally determines the final answer or serves as post-hoc rationalization
- Mechanism: The framework applies do-calculus interventions at specific reasoning steps k, forcing contradictory logic (e.g., flipping operators, negating premises). By comparing the counterfactual answer a* against the original a via semantic similarity S(a, a*), the Causal Sensitivity Score φ = 1 − S(a, a*) quantifies how dependent the output is on the intervened step
- Core assumption: If reasoning is faithful, mutating a logically load-bearing step should propagate downstream and alter the terminal answer
- Evidence anchors:
  - [abstract]: "Project Ariadne performs hard interventions (do-calculus) on intermediate reasoning nodes—systematically inverting logic, negating premises, and reversing factual claims—to measure the Causal Sensitivity (φ) of the terminal answer"
  - [Section 4.2.1]: Defines intervened distribution where subsequent steps are re-sampled after intervention at step k
  - [corpus]: TRUST framework (arxiv:2510.20188) similarly addresses verifying reasoning chain faithfulness but uses decentralized auditing; Causal Consistency Regularization (arxiv:2509.01544) proposes related counterfactual sensitivity training
- Break condition: If interventions target steps that are genuinely non-load-bearing (epiphenomenal to the conclusion), low φ may reflect poor intervention targeting rather than unfaithfulness

### Mechanism 2
- Claim: High semantic similarity between original and counterfactual answers despite contradictory intermediate logic indicates Causal Decoupling—the reasoning trace does not govern the decision
- Mechanism: When S(a, a*) approaches 1.0 (φ approaches 0) after a substantive intervention, the model has bypassed its own reasoning logic. The framework formalizes this via binary violation indicator V(q, k, ι), flagging cases where similarity exceeds threshold τ_sim despite intervention strength λ
- Core assumption: The intervention strength metric adequately captures logical significance; semantic similarity threshold τ_sim correctly identifies functionally equivalent answers
- Evidence anchors:
  - [Section 4.3.2]: Formal definition of violation detection requiring both high similarity AND sufficient intervention strength
  - [Section 5.2]: Reports Violation Density ρ = 0.96 in Scientific Reasoning, 0.92 in General Knowledge—answers remained nearly identical despite contradictory premises
  - [corpus]: LIBERTy (arxiv:2601.10700) uses structural counterfactuals for concept-based explanation evaluation with similar logic; corpus shows active research in causal faithfulness auditing but limited validation of similarity thresholds
- Break condition: If semantic similarity scorer fails to detect meaningful answer differences (e.g., different numbers with same "gist"), false positives in violation detection increase

### Mechanism 3
- Claim: Models possess an implicit "error-correction" mechanism that reverts reasoning traces toward high-probability parametric priors, especially in factual domains
- Mechanism: After intervention at step k, subsequent steps s*_{k+1} through s*_n are autoregressively re-sampled. The model often "notices" the contradiction and pivots back toward its pre-trained answer distribution P(a|q), rendering the intervened reasoning epiphenomenal
- Core assumption: Parametric priors are sufficiently strong to override contextual logic cues; this behavior is architecture-inherent rather than prompt-specific
- Evidence anchors:
  - [Section 5.3]: Case study shows agent accepting negated climate change premise but reaching functionally identical answer (S = 0.9698)
  - [Section 5.5]: "Transition probability P(a|q, s'_k) is nearly identical to P(a|q, s_k), proving that the intermediate reasoning state is non-essential for terminal decision-making in factual retrieval tasks"
  - [corpus]: Causal Distillation (arxiv:2505.19511) notes large models exhibit strong causal reasoning that smaller models struggle to replicate, suggesting parametric priors play domain-dependent roles
- Break condition: In computation-heavy tasks (e.g., mathematical logic), parametric priors may be weaker or inapplicable, forcing genuine reasoning dependency—Section 5.2 shows φ = 0.329 for Math vs. 0.030 for Scientific

## Foundational Learning

- Concept: **Structural Causal Models (SCMs)**
  - Why needed here: The entire Ariadne framework formalizes reasoning as an SCM M = ⟨U, V, F⟩ with exogenous variables (query, parameters), endogenous variables (reasoning steps, answer), and structural equations governing dependencies
  - Quick check question: Given an SCM with nodes {s₁, s₂, a} where a depends on s₂ and s₂ depends on s₁, what happens to a if we intervene do(s₁ = s'₁)?

- Concept: **Pearl's do-calculus and counterfactual interventions**
  - Why needed here: The framework distinguishes observational conditioning from hard interventions—do(s_k = s'_k) surgically replaces a reasoning step rather than conditioning on its observed value
  - Quick check question: Why does P(a | do(s_k = s'_k)) differ from P(a | s_k = s'_k) when unobserved confounders exist?

- Concept: **Autoregressive generation in transformers**
  - Why needed here: Understanding why post-intervention steps re-sample (s*_{k+1}...s*_n) requires knowing that LLMs generate token-by-token conditioned on prior context; the intervention changes the conditioning context
  - Quick check question: If you intervene at step k in an autoregressive model, which subsequent tokens must be re-generated and why?

## Architecture Onboarding

- Component map:
  - SCM Formalization: M = ⟨U, V, F⟩ where U = {q, θ} (query, params), V = {s₁...sₙ, a} (trace steps, answer), F = structural equations (Eq. 1-2)
  - Intervention Operator I: Maps original step s_k to counterfactual s'_k via type τ ∈ {LogicFlip, FactReversal, PremiseNegation, CausalInversion}
  - Counterfactual Generator: Re-executes agent from intervention point, producing new trace suffix and answer a*
  - Semantic Similarity Scorer S(a, a*): Used by paper (Claude 3.7 Sonnet as judge) to compute equivalence
  - Faithfulness Calculator: Computes φ = 1 − S(a, a*) and binary violation indicator V

- Critical path:
  1. Extract original trace T and answer a for query q
  2. Select target step k and intervention type τ
  3. Generate counterfactual step s'_k = I(s_k, τ)
  4. Re-run generation from k+1 onward with intervened context
  5. Score S(a, a*) and compute φ
  6. Flag violation if S > τ_sim AND intervention strength > λ

- Design tradeoffs:
  - Intervention point: Early steps (s₀) maximize downstream effect but may trigger stronger prior-correction; mid-trace steps may be more load-bearing but have less cascade potential
  - Similarity scorer choice: LLM-as-judge (used here) vs. embedding cosine similarity—former more nuanced but costlier and less reproducible
  - Intervention automation vs. manual design: Automated saliency detection (proposed future work) vs. hand-crafted counterfactuals—automation scales but may miss subtle logical inversions

- Failure signatures:
  - False positive violations: Weak interventions (low logical significance) trigger violation flags; mitigated by Strength(ι, s_k) > λ check
  - Scorer insensitivity: Semantic similarity fails to detect numerically or logically distinct answers
  - Trace length confound: Longer traces show MORE violations (Section 5.4)—more steps to "correct" back to prior
  - Domain dependence: Mathematical tasks show 20% violation rate vs. 96% for scientific—interpret scores relative to domain

- First 3 experiments:
  1. Baseline calibration: Run Ariadne on a held-out subset with multiple intervention types (τ) per query to establish which intervention modalities produce the highest φ scores and whether this varies by domain
  2. Trace length ablation: For fixed query set, vary forced trace length (e.g., 3, 5, 10 steps) and measure correlation between length and violation density to validate the "correction opportunity" hypothesis from Section 5.4
  3. Cross-model comparison: Apply identical intervention protocol to at least two model families (e.g., GPT-4o and Claude) to determine whether Causal Decoupling is architecture-general or model-specific—current paper tests single model type

## Open Questions the Paper Calls Out

- Question: Does increased "thinking time" in "System 2" architectures improve causal faithfulness, or does it simply produce more elaborate post-hoc rationalizations?
  - Basis in paper: [explicit] The Future Work section explicitly asks whether "increased 'thinking time' in models utilizing test-time compute (e.g., OpenAI's o1) leads to higher causal faithfulness or simply more elaborate post-hoc justifications"
  - Why unresolved: The current study evaluated GPT-4o-based agents but did not test architectures specifically designed for extended inference-time reasoning
  - What evidence would resolve it: A comparative analysis applying the Ariadne framework to models with high test-time compute versus standard autoregressive baselines

- Question: Can the Causal Sensitivity Score (φ) be effectively utilized as a training signal to reduce Causal Decoupling?
  - Basis in paper: [explicit] The authors propose "using the Faithfulness Score (φ) as a reward signal in Reinforcement Learning from Human Feedback (RLHF) or Direct Preference Optimization (DPO)"
  - Why unresolved: The paper establishes φ as a diagnostic metric for auditing, but its efficacy as a loss function or reward signal for model alignment remains untested
  - What evidence would resolve it: Successful fine-tuning of a model where minimizing unfaithfulness is a training objective, resulting in lower Violation Density (ρ)

- Question: Can automated saliency mapping accurately identify "load-bearing" reasoning steps to improve audit efficiency?
  - Basis in paper: [explicit] Section 7 suggests implementing "Automated Saliency Detection" using attention weights to identify "load-bearing" steps automatically
  - Why unresolved: The current framework relies on intervening at the initial step (s₀) or random nodes; locating the specific steps that causally influence the output is currently inefficient
  - What evidence would resolve it: A correlation between saliency-predicted critical nodes and empirically observed high-sensitivity intervention points

## Limitations

- Single model evaluation: Results are based on a single model family (GPT-4o-based agents), limiting generalizability across architectures
- Threshold sensitivity: Critical thresholds (τ_sim for semantic similarity, λ for intervention strength) are not specified numerically, making it unclear how robust ρ values are to tuning
- Intervention automation: The paper uses hand-crafted counterfactual steps without validating whether automated intervention generation would produce comparable violation rates

## Confidence

- High confidence: The causal framework formalization (SCM + do-calculus) is methodologically sound and the mathematical definitions of Causal Sensitivity and Violation Density are internally consistent
- Medium confidence: The 96% violation density in Scientific Reasoning is compelling but requires replication across model families and with different semantic similarity scorers to rule out scorer-specific artifacts
- Low confidence: The "error-correction mechanism" hypothesis (parametric priors overriding reasoning) is plausible but not directly tested—could be alternative explanations like poor intervention targeting

## Next Checks

1. Cross-model validation: Apply identical intervention protocol to at least two additional model families (e.g., GPT-4o, Claude, Llama) to determine whether Causal Decoupling is architecture-general or model-specific
2. Scorer ablation study: Repeat experiments using both LLM-as-judge and embedding-based semantic similarity to verify that ρ values are not artifacts of the scoring method
3. Intervention strength calibration: Systematically vary intervention strength λ and measure how ρ changes to establish whether current violation rates reflect true unfaithfulness or conservative threshold settings