---
ver: rpa2
title: I Am Aligned, But With Whom? MENA Values Benchmark for Evaluating Cultural
  Alignment and Multilingual Bias in LLMs
arxiv_id: '2510.13154'
source_url: https://arxiv.org/abs/2510.13154
tags:
- cultural
- principal
- component
- language
- condition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MENAValues, a benchmark designed to evaluate
  cultural alignment and multilingual biases of large language models (LLMs) with
  respect to Middle East and North Africa (MENA) populations. The benchmark uses 864
  questions derived from large-scale human surveys across 16 countries, covering governance,
  economic, social, and wellbeing dimensions.
---

# I Am Aligned, But With Whom? MENA Values Benchmark for Evaluating Cultural Alignment and Multilingual Bias in LLMs

## Quick Facts
- **arXiv ID:** 2510.13154
- **Source URL:** https://arxiv.org/abs/2510.13154
- **Reference count:** 40
- **Key outcome:** Introduces MENAValues benchmark revealing significant cross-lingual value shifts, reasoning-induced degradation, and logit leakage in LLMs' cultural alignment with MENA populations.

## Executive Summary
This paper introduces MENAValues, a benchmark designed to evaluate cultural alignment and multilingual biases of large language models (LLMs) with respect to Middle East and North Africa (MENA) populations. The benchmark uses 864 questions derived from large-scale human surveys across 16 countries, covering governance, economic, social, and wellbeing dimensions. It evaluates models across three perspectives (neutral, persona-based, observer) and two languages (English and native languages: Arabic, Persian, Turkish). Key findings include: Cross-Lingual Value Shifts—models produce drastically different responses in native versus English languages; Reasoning-Induced Degradation—explicit reasoning worsens cultural alignment; and Logit Leakage—models refuse sensitive questions while internal probabilities reveal strong hidden preferences. Across seven diverse models, no single model achieved optimal performance across all metrics, with reasoning often reducing alignment. PCA visualizations showed that native-language responses collapse into simplistic linguistic clusters, treating diverse countries as monolithic. The study highlights significant gaps in current LLMs' ability to represent MENA values and provides methodological tools for diagnosing cultural misalignment in underrepresented regions.

## Method Summary
The study evaluates seven LLMs (Llama-3.1-8B, Mistral-7B, AYA-8B, Fanar, ALLaM, GPT-4o-mini, Gemini 2.5 Flash Lite) using a benchmark of 864 multiple-choice questions derived from World Values Survey Wave 7 and Arab Opinion Index 2022, covering 16 MENA countries. The evaluation matrix includes three framings (Neutral, Persona, Observer) × two languages (English, Native: Arabic/Persian/Turkish) × two reasoning modes (Zero-shot, With-Reasoning). Key metrics include Normalized Value Alignment Score (NVAS), Cross-Lingual Consistency Score (CLCS), Framing Consistency Score (FCS), and Kullback-Leibler Divergence (KLD). The methodology requires extracting normalized probabilities for answer tokens and analyzing internal logits to detect "Logit Leakage" where refusals mask strong internal preferences.

## Key Results
- Cross-Lingual Value Shifts: Models produce drastically different responses in native versus English languages, with native-language responses collapsing into monolithic linguistic clusters
- Reasoning-Induced Degradation: Explicit reasoning worsens cultural alignment, with N-VAS scores dropping 3.52-6.96% across models when reasoning is enabled
- Logit Leakage: Models refuse sensitive questions while internal probabilities reveal strong hidden preferences, observable via token-level logit analysis

## Why This Works (Mechanism)

### Mechanism 1: Cross-Lingual Representation Drift
- **Claim:** Identical prompts in different languages activate distinct value frameworks, causing models to "flip" cultural stances rather than translate them neutrally.
- **Mechanism:** The paper suggests that native-language prompts (Arabic, Persian, Turkish) trigger "linguistic essentialism," where models collapse diverse national cultures into monolithic language clusters. Conversely, English prompts often activate a "default" alignment space (often Western-centric) that differs structurally from the native-language representation space.
- **Core assumption:** The semantic meaning of the prompt is preserved across translations, and the variance is due to the model's internal associations with the language itself.
- **Evidence anchors:** Abstract shows "Cross-Lingual Value Shifts where identical questions yield drastically different responses based on language." Section 6.1 describes how models differentiate countries in English but collapse into monolithic entities in native languages.

### Mechanism 2: Reasoning-Induced Bias Activation
- **Claim:** Explicit "Chain-of-Thought" reasoning degrades cultural alignment with MENA values by activating dominant training priors over specific cultural nuance.
- **Mechanism:** The study hypothesizes that requiring reasoning engages "dominant" latent knowledge (often Western-liberal or stereotypical generalizations) which overrides the model's ability to simulate specific cultural personas or adhere to local norms.
- **Core assumption:** The model's "reasoning" path is not culturally agnostic but draws from a biased pre-training distribution.
- **Evidence anchors:** Abstract notes "Reasoning-Induced Degradation where prompting models to explain their reasoning worsens cultural alignment." Table 1 shows N-VAS scores dropping for multiple models when "With-Reasoning" is active.

### Mechanism 3: Logit Leakage (Safety-Surface vs. Latent-Preference Divergence)
- **Claim:** Standard safety alignment creates a refusal "veneer" that masks strong internal preferences, observable only via token-level logit probabilities.
- **Mechanism:** RLHF or safety fine-tuning suppresses the *generation* of specific tokens (refusals) but does not erase the high conditional probability of the "undesired" or "culturally specific" answer in the model's internal logits.
- **Core assumption:** High logit probability on a refused answer option signifies a "strong internal preference" or "hidden bias" rather than a statistical artifact.
- **Evidence anchors:** Abstract describes "Logit Leakage where models refuse sensitive questions while internal probabilities reveal strong hidden preferences." Figure 1 visualizes a model refusing while the logit bar for a specific answer choice is near 100%.

## Foundational Learning

**Concept: Cross-Lingual Consistency**
- **Why needed here:** To distinguish between a model failing to understand a culture versus failing to maintain consistent logic across languages.
- **Quick check question:** Does the model give the same moral stance in English and Arabic for the exact same scenario?

**Concept: Logit Probability Analysis**
- **Why needed here:** To look beyond surface-level text generation (which may be refusals) and inspect the model's confidence distribution over potential answers.
- **Quick check question:** When a model says "I cannot answer," is the probability mass evenly distributed or concentrated on a single "forbidden" option?

**Concept: Cultural Essentialism**
- **Why needed here:** To recognize the failure mode where models treat entire language groups (e.g., "Arabic speakers") as having identical values, ignoring national differences.
- **Quick check question:** Does the model distinguish between values in Egypt vs. Saudi Arabia, or does it output a generic "Arab" response?

## Architecture Onboarding

**Component map:** Input (WVS/AOI Surveys) -> Benchmark Constructor (864 MCQs) -> Evaluation Matrix (3 Perspectives x 2 Languages x 2 Reasoning Modes) -> Metrics Layer (N-VAS, CLCS, FCS) -> Diagnostic Analysis (PCA, Logit Extraction)

**Critical path:** The Logit Extraction pipeline is the most fragile component; it requires access to raw model logits (not available for all closed-source APIs) and precise token alignment to map probabilities to answer keys.

**Design tradeoffs:**
- *Generalizability vs. Depth:* Using 7 diverse models (Open/Closed, Regional/General) provides robustness but complicates direct architectural comparison.
- *Metric Choice:* N-VAS measures alignment with *majority* opinion, which is diagnostic but not normative (a high score may mean reinforcing harmful majorities).

**Failure signatures:**
- *The "Monolith" Collapse:* PCA plots showing all Arabic-speaking countries clustering identically in native language mode.
- *The "Reasoning" Drop:* N-VAS scores consistently decreasing when the "With-Reasoning" flag is enabled.
- *The "Leakage" Spike:* High abstention rates in surface text but high confidence (KL divergence) in logit distributions.

**First 3 experiments:**
1. **Baseline Consistency Check:** Run the Neutral English vs. Neutral Native prompt on Llama-3.1 for a subset of sensitive questions and visualize the delta in responses (verifying Cross-Lingual Shift).
2. **Reasoning Stress Test:** Compare Zero-Shot vs. With-Reasoning outputs for the Persona perspective on GPT-4o-mini to replicate the "Reasoning-Induced Degradation."
3. **Logit Inspection:** Select a question where the model refuses to answer (Safety refusal) and inspect the top-5 token probabilities to detect "Logit Leakage."

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What specific causal mechanisms drive Reasoning-Induced Degradation in cultural alignment?
- **Basis in paper:** [explicit] The authors explicitly call for "deeper investigation into their underlying causal mechanisms" regarding this phenomenon in the Limitations.
- **Why unresolved:** While the paper identifies failure modes like cultural stereotyping, it does not isolate whether the issue stems from model architecture or training data distribution.
- **What evidence would resolve it:** Ablation studies analyzing attention head behavior during reasoning chains on value-laden questions.

### Open Question 2
- **Question:** Do Cross-Lingual Value Shifts and Logit Leakage generalize to other underrepresented regions outside MENA?
- **Basis in paper:** [explicit] The conclusion states future work should "expand such deep evaluations to other underrepresented regions."
- **Why unresolved:** The observed phenomena are currently validated only on the MENA dataset (Arabic, Persian, Turkish) and may be region-specific.
- **What evidence would resolve it:** Applying the benchmark's methodology to diverse non-Western regions like Sub-Saharan Africa or Southeast Asia.

### Open Question 3
- **Question:** How can multilingual training be modified to prevent models from collapsing diverse cultures into monolithic linguistic clusters?
- **Basis in paper:** [explicit] The discussion highlights that models "collapse into simplistic linguistic categories" in native languages, treating language as a proxy for culture.
- **Why unresolved:** Current training approaches inadvertently promote linguistic essentialism, erasing distinct national values within shared language families.
- **What evidence would resolve it:** Novel training objectives that successfully decouple cultural representations from linguistic encoding, verified via PCA analysis.

## Limitations

- **Translation Quality Dependency:** The core claim of cross-lingual value shifts depends on the quality and cultural equivalence of translations across 864 questions, which was validated by native speakers but lacks full methodology disclosure.
- **Majority Opinion Normativity:** The benchmark's reliance on majority opinion alignment (N-VAS metric) raises normative questions about whether optimizing for majority values represents true cultural alignment or simply reinforces existing biases.
- **Causal Mechanism Uncertainty:** While the paper identifies failure modes like cultural stereotyping, it does not isolate whether reasoning-induced degradation stems from model architecture or training data distribution.

## Confidence

- **High Confidence:** Cross-lingual value shifts (English vs. native responses differ significantly) - directly observable and reproducible
- **Medium Confidence:** Reasoning-induced degradation - measurable effect but causal mechanism (Western priors vs. general reasoning quality) remains uncertain
- **Medium Confidence:** Logit leakage interpretation - observable phenomenon but "hidden preference" interpretation requires further validation
- **Low Confidence:** Normative interpretation of majority-alignment metrics - optimizing for majority values may not represent ideal cultural alignment

## Next Checks

1. **Translation Equivalence Validation:** Re-run the benchmark using back-translation (Arabic→English→Arabic) for a subset of questions to verify that semantic meaning is preserved across languages, isolating linguistic drift from semantic drift.

2. **Reasoning Mechanism Dissection:** Compare reasoning-induced degradation across question types (factual vs. value-based) to determine if the effect is specific to cultural reasoning or general reasoning quality, using questions with known Western-centric content as controls.

3. **Logit Leakage Attribution:** Test whether logit leakage persists when using models with different safety fine-tuning approaches (RLHF vs. rule-based) or when prompting with explicit safety overrides to determine if the effect is specifically tied to current alignment techniques.