---
ver: rpa2
title: Towards a Generalizable Bimanual Foundation Policy via Flow-based Video Prediction
arxiv_id: '2505.24156'
source_url: https://arxiv.org/abs/2505.24156
tags:
- bimanual
- video
- flow
- manipulation
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of learning generalizable bimanual
  manipulation policies, which are difficult due to large action spaces and the need
  for coordinated arm movements. The proposed method, CogRobot, leverages text-to-video
  models by fine-tuning them to predict robot trajectories and training a lightweight
  diffusion policy for action generation.
---

# Towards a Generalizable Bimanual Foundation Policy via Flow-based Video Prediction

## Quick Facts
- arXiv ID: 2505.24156
- Source URL: https://arxiv.org/abs/2505.24156
- Reference count: 40
- One-line primary result: A two-stage text-to-flow then flow-to-video pipeline improves bimanual robot policy generalization and reduces data requirements.

## Executive Summary
This paper tackles the challenge of learning generalizable bimanual manipulation policies by leveraging text-to-video models. The proposed CogRobot method introduces a two-stage generation process that uses optical flow as an intermediate variable, improving prediction accuracy and reducing the burden of language ambiguity. By fine-tuning large-scale video models and training a lightweight diffusion policy, the system achieves strong performance on both simulation and real-world dual-arm robot platforms. The approach significantly reduces data requirements by avoiding low-level action modeling.

## Method Summary
CogRobot addresses bimanual manipulation by fine-tuning text-to-video models (CogVideoX) to predict robot trajectories. The method introduces a two-stage generation process: first generating optical flow videos from text and initial observations (Text-to-Flow), then using these flows to condition video generation (Flow-to-Video). Optical flow is converted to RGB format to leverage existing VAE encoders. A lightweight diffusion policy then maps predicted future observations to robot actions, decoupling high-level planning from low-level control.

## Key Results
- CogRobot achieves state-of-the-art success rates on bimanual manipulation tasks in both simulation (RoboTwin) and real-world (Realman dual-arm) environments.
- The two-stage flow-based approach significantly outperforms direct text-to-video baselines in terms of video quality metrics (PSNR, SSIM, LPIPS, FVD).
- The method demonstrates improved generalization by reducing data requirements through avoidance of low-level action modeling.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Decomposing text-to-video generation into Text-to-Flow then Flow-to-Video mitigates language ambiguity and reduces data requirements.
- **Mechanism**: High-level language instructions often fail to specify precise kinematic trajectories, leading to "physical hallucinations" in standard T2V models. By introducing optical flow as an intermediate variable, the model first concretizes intent into explicit pixel motion (Text-to-Flow), providing unambiguous motion guidance.
- **Core assumption**: Optical flow provides a sufficient representation of subtle movements required for bimanual manipulation.
- **Evidence anchors**: [abstract], [section 3.1], [corpus] references to bimanual coordination challenges.
- **Break condition**: Mechanism may fail if optical flow estimation is noisy or fails to capture contact-rich interactions.

### Mechanism 2
- **Claim**: Converting optical flow to RGB format enables efficient fine-tuning without training new encoders.
- **Mechanism**: Standard T2V models operate on RGB latents. Converting flow magnitude and angle into RGB color wheel representations allows repurposing existing pre-trained VAE and DiT architecture, leveraging web-scale video priors with limited robot data.
- **Core assumption**: Pre-trained VAE can effectively compress and reconstruct optical-flow-encoded-as-RGB images.
- **Evidence anchors**: [section 3.1], [section 4], [corpus] support for efficient flow representations.
- **Break condition**: If flow dynamics differ vastly from natural video statistics, standard VAE might discard critical motion details.

### Mechanism 3
- **Claim**: Using a lightweight diffusion policy decouples high-level trajectory planning from low-level dynamics.
- **Mechanism**: The system separates "what to do" (video prediction) from "how to move" (joint control). The heavy T2V model predicts the goal observation, while a smaller policy solves the goal-reaching problem.
- **Core assumption**: Predicted video frames represent physically feasible states the low-level controller can reach.
- **Evidence anchors**: [abstract], [section 3.3], [corpus] hierarchical approaches.
- **Break condition**: If video model predicts physically impossible states, the low-level policy may fail.

## Foundational Learning

- **Concept**: Diffusion Probabilistic Models (DDPM)
  - **Why needed here**: The pipeline relies on diffusion models for both video/flow generation and action generation.
  - **Quick check question**: Can you explain the difference between the "forward process" (adding noise) and the "reverse process" (denoising) used in training vs. inference?

- **Concept**: Optical Flow Representation
  - **Why needed here**: Understanding how 2D motion vectors are calculated and visualized is required to debug the Text-to-Flow stage.
  - **Quick check question**: How does optical flow differ from general object tracking, and why might it struggle with "occlusions" in bimanual manipulation?

- **Concept**: Goal-Conditioned Reinforcement Learning (GCRL) / Imitation Learning
  - **Why needed here**: The low-level controller is defined as a goal-reaching agent, requiring understanding of sparse rewards and goal relabeling.
  - **Quick check question**: In a goal-conditioned setup, if the agent fails to reach the goal image, is the error in the planner or the controller?

## Architecture Onboarding

- **Component map**: Input (oâ‚€ + l) -> Stage 1 (Text-to-Flow: CogVideoX) -> Flow Video (v_F) -> Stage 2 (Flow-to-Video: CogVideoX) -> RGB Video (v) -> Controller (Diffusion Policy) -> Actions (a)

- **Critical path**: The connection between Flow Latent and Video Latent. In Section 3.2, concatenating encoded flow video (z_f) with noised video latent (z_v) is critical; misalignment or VAE encoding divergence could cause the video model to ignore flow guidance.

- **Design tradeoffs**:
  - RGB Flow vs. Raw Flow: Converting to RGB saves compute but might introduce compression losses.
  - Generalizability vs. Embodiment Specifics: Reliance on "general" video prior may hinder adaptation to highly non-anthropomorphic robots.

- **Failure signatures**:
  - Physical Hallucination: Generated arms move through objects or disappear (Text-to-Flow failed to constrain geometry).
  - Jittery/Non-Executable Plans: Video looks realistic but implies unreachable movements (disconnect between Video Planner and Robot Kinematics).
  - Task Confusion: Robot performs wrong sub-task (attention failure in Text-to-Flow stage).

- **First 3 experiments**:
  1. Flow Validity Check: Run Stage 1 only and visualize RGB flow output to verify color direction matches intended movement.
  2. Ablation on Flow Conditioning: Run Stage 2 with and without flow latent concatenation and compare video quality metrics.
  3. Real-World Control Loop: Deploy full pipeline on a simple task (e.g., "Lift Bag") to verify the low-level policy can reach the predicted goal state.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can the framework be extended to support a single, unified low-level policy across diverse tasks, rather than requiring a separate policy for each specific task?
  - **Basis in paper**: [explicit] The authors state in the conclusion that "each task requires a separate policy to extract actions from videos."
  - **Why unresolved**: Current design necessitates task-specific policies, reducing scalability.
  - **What evidence would resolve it**: Demonstrating a single diffusion policy trained on multi-task data can map predicted videos to actions for novel, unseen tasks.

- **Open Question 2**: Does the rule-based conversion of 2-channel optical flow into 3-channel RGB "flow video" result in motion precision loss compared to methods training a dedicated flow VAE?
  - **Basis in paper**: [inferred] The paper converts flow to RGB to "better leverage the pretrained model's prior" and avoid training a VAE from scratch.
  - **Why unresolved**: Mapping continuous motion vectors to discrete color values may introduce quantization errors.
  - **What evidence would resolve it**: Comparative analysis between RGB flow and learned latent flow representations regarding trajectory accuracy and success rates.

- **Open Question 3**: How does the computational latency of the iterative, two-stage video generation process impact closed-loop control feasibility in rapidly changing environments?
  - **Basis in paper**: [inferred] The method relies on CogVideoX for iterative planning, but real-time inference speed is not analyzed.
  - **Why unresolved**: Diffusion-based video models are computationally heavy; prediction time may exceed environmental change timescales.
  - **What evidence would resolve it**: Reporting control frequency of the full loop during deployment and analyzing failure rates in high-speed tasks.

## Limitations
- **Embodiment Gap**: Generalizability to significantly different robot morphologies (e.g., mobile manipulators) remains untested.
- **Physical Realism**: Limited validation that predicted video trajectories are physically feasible beyond task success rates.
- **Data Dependency**: Despite claims of reduced data requirements, substantial robot trajectory data is still needed for fine-tuning.

## Confidence
- **High**: Core mechanism of using optical flow as intermediate representation is well-supported by experimental results.
- **Medium**: Claim of "generalizability" is supported within tested domains but broader generalization is not validated.
- **Medium**: Assertion that lightweight diffusion policy enables efficient action generation is plausible but computational advantages are not quantified.

## Next Checks
1. **Cross-Morphology Transfer**: Test CogRobot pipeline on a different robot platform (e.g., single-arm or mobile manipulator) using the same fine-tuned video models to assess true generalizability.
2. **Physical Feasibility Analysis**: Implement a post-hoc check on predicted video trajectories to measure percentage of states within joint limits and free of self-collisions, correlating with task success.
3. **Data Efficiency Benchmark**: Systematically reduce fine-tuning data and measure performance degradation to quantify claimed data efficiency advantage over direct text-to-video baseline.