---
ver: rpa2
title: Using Large Language Models to Suggest Informative Prior Distributions in Bayesian
  Statistics
arxiv_id: '2506.21964'
source_url: https://arxiv.org/abs/2506.21964
tags:
- prior
- distributions
- priors
- llms
- informative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a method to use large language models (LLMs)
  to suggest informative prior distributions in Bayesian statistical modeling, addressing
  the difficulty and subjectivity of manually constructing such priors. An extensive
  prompt was developed to elicit multiple sets of priors from LLMs (Claude Opus, Gemini
  2.5 Pro, ChatGPT-4o-mini), requiring detailed justification and confidence weighting.
---

# Using Large Language Models to Suggest Informative Prior Distributions in Bayesian Statistics

## Quick Facts
- arXiv ID: 2506.21964
- Source URL: https://arxiv.org/abs/2506.21964
- Reference count: 26
- Key outcome: LLMs can suggest priors with correct directional associations but struggle with uncertainty calibration; Claude Opus performs best.

## Executive Summary
This paper introduces a method for using large language models (LLMs) to suggest informative prior distributions in Bayesian statistical modeling, addressing the difficulty and subjectivity of manually constructing such priors. The authors developed an extensive prompt requiring LLMs to provide multiple prior sets with detailed justifications and confidence weightings. Using Kullback-Leibler divergence from maximum likelihood estimators as a quality metric, the study found that LLMs correctly identified directional associations in two real datasets but exhibited systematic overconfidence, particularly when suggesting moderately informative priors. Claude Opus consistently provided the most suitable priors, maintaining non-zero means in weakly informative sets while other LLMs defaulted to unnecessarily vague priors.

## Method Summary
The authors employed structured prompting to elicit prior distributions from LLMs (Claude Opus, Gemini 2.5 Pro, ChatGPT-4o-mini) for Bayesian regression models. For each regression coefficient, LLMs were asked to suggest two sets of Gaussian priors (moderately and weakly informative) with detailed justifications and confidence weightings. The quality of suggested priors was evaluated using KL divergence from the MLE distribution, computed via either asymptotic Gaussian approximation or bootstrapping. Bayesian models were then fitted using R-INLA with the LLM-suggested priors, and prediction performance was assessed through 5-fold cross-validation comparing Brier scores, MNLS, AUC, RMSE, and MAE against frequentist baselines.

## Key Results
- All three LLMs correctly identified the direction of variable associations (e.g., males have higher heart disease risk) but often provided overconfident moderately informative priors
- Claude Opus performed best, offering informative yet suitably wide weakly informative priors with non-zero means, while ChatGPT and Gemini defaulted to unnecessarily vague priors with mean zero
- Bayesian models with LLM priors showed slight prediction improvements, but these were not statistically significant due to large dataset sizes
- LLMs struggled with highly non-linear relationships, particularly the Age variable in the concrete strength dataset

## Why This Works (Mechanism)

### Mechanism 1: Structured Knowledge Elicitation via Reflective Prompting
LLMs can correctly identify directional associations when prompted with structured justification requirements and multi-set proposals. The prompt forces LLMs to simulate literature review, provide detailed justifications for each hyperparameter, propose multiple prior sets, and assign confidence weightings. This structured elicitation activates domain knowledge embedded in training data and moves beyond simple query-response patterns.

### Mechanism 2: Prior Quality Assessment via KL Divergence from MLE
KL divergence between the prior and the MLE distribution provides a practical benchmark for quantifying prior-data conflict. KL divergence asymmetrically penalizes priors that assign low probability to regions where the MLE distribution has high probability, capturing both how "surprised" the prior is by the data and how informative the prior is.

### Mechanism 3: Width Calibration via Weakly Informative Prior Strategy
LLMs can provide directionally correct priors but systematically struggle with uncertainty calibration, with weakly informative priors generally outperforming moderately informative ones. LLMs exhibit "meta-level overconfidence"—they are overconfident both in their parameter estimates and in their self-assessment of which prior set is better.

## Foundational Learning

- **Bayesian Prior Distributions and Bayes' Rule**
  - Why needed: Understanding how priors encode pre-existing knowledge and combine with likelihoods to form posteriors is essential for evaluating whether LLM-suggested priors are useful
  - Quick check: If an LLM correctly knows that heart disease risk increases with age, why would a prior with mean=0 be "unnecessarily vague"?

- **Kullback-Leibler Divergence**
  - Why needed: This is the core metric for evaluating prior quality against the MLE distribution; understanding its asymmetry explains why prior-data conflict is heavily penalized
  - Quick check: Why does the paper place the prior p(θ) in the denominator of the KL divergence formula, and what does this mean for "surprise"?

- **Logistic Regression Coefficients as Log-Odds Ratios**
  - Why needed: LLMs had to convert known associations (e.g., "age increases heart disease risk") into properly scaled log-odds ratios for prior means
  - Quick check: If an LLM knows that males have higher heart disease risk than females, how should this translate to the prior mean for the sex coefficient in a logistic regression?

## Architecture Onboarding

- **Component map**: Prompt Engineering Module -> Prior Distribution Parser -> MLE Distribution Estimator -> KL Divergence Evaluator -> Bayesian Model Fitter -> Prediction Evaluator
- **Critical path**: Define Bayesian model structure → Construct structured prompt → Query LLM and extract prior parameters → Fit frequentist model for MLE distribution → Compute KL divergence → Fit Bayesian model with R-INLA → Evaluate prediction performance via cross-validation
- **Design tradeoffs**: 
  - Moderately vs. weakly informative priors: Paper found weakly informative priors generally superior (lower KL divergence) due to LLM overconfidence in moderate priors
  - Model selection: Claude maintained non-zero means in weak priors (capturing directional knowledge), while ChatGPT/Gemini defaulted to mean=0 (unnecessarily discarding knowledge)
- **Failure signatures**:
  - Overconfident narrow priors: Minimal overlap with MLE distribution, high KL divergence despite correct direction
  - Unnecessarily vague priors: Mean=0 when LLM demonstrated correct directional knowledge in moderate priors
  - Meta-level overconfidence: LLMs assigned higher confidence to moderate priors despite these performing worse on KL divergence
  - Non-linear relationship struggles: All LLMs struggled with Age variable in concrete dataset
- **First 3 experiments**:
  1. Reproduce directional correctness finding: Run the structured prompt on a dataset with well-established associations to verify that all three LLMs correctly identify effect directions
  2. Validate Claude's weak prior advantage: Compare KL divergence of Claude's weakly informative priors against ChatGPT/Gemini to confirm Claude maintains non-zero means while others default to mean=0
  3. Test on small dataset: Apply the method to a small-N dataset to assess whether LLM priors provide statistically significant improvements when prior information is more influential

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the systematic overconfidence in "moderately informative" LLM priors be corrected through improved prompting or model fine-tuning?
- Basis in paper: The authors conclude that "a significant challenge remains in calibrating the width of these priors," specifically noting that moderate priors were often too narrow.
- Why unresolved: Even with prompts requiring reflection and justification, LLMs (especially ChatGPT and Gemini) provided priors that conflicted with the data distribution.
- What evidence would resolve it: Demonstration of a prompt structure or model adjustment that consistently yields "moderately informative" priors with lower KL divergence than the current "weakly informative" baselines.

### Open Question 2
- Question: Can LLM-generated priors significantly enhance model performance in out-of-distribution (OOD) generalization scenarios?
- Basis in paper: The authors identify "exploring the potential of LLMs in out-of-distribution generalization" as a "promising avenue" where priors might bridge distribution gaps.
- Why unresolved: Experiments used large datasets where the likelihood dominated the prior, resulting in no statistically significant prediction improvements in standard validation.
- What evidence would resolve it: Experiments on datasets with systematic train-test distribution shifts showing that LLM priors stabilize performance relative to non-informative baselines.

### Open Question 3
- Question: Does the efficacy of LLM prior elicitation generalize to complex Bayesian architectures like hierarchical or latent variable models?
- Basis in paper: The paper states it is "interesting to explore this approach for other Bayesian models, such as latent variable models, hierarchical models..."
- Why unresolved: The methodology was only validated on standard logistic and linear regression models with Gaussian priors.
- What evidence would resolve it: Successful application of the elicitation method to a hierarchical model where LLM-suggested hyperparameters improve convergence or predictive accuracy.

## Limitations
- LLMs demonstrate systematic overconfidence, assigning higher confidence to moderately informative priors despite these showing higher KL divergence than weakly informative alternatives
- Prediction improvements from LLM priors were not statistically significant due to large dataset sizes where likelihood dominated prior information
- LLMs struggled with highly non-linear relationships, particularly the Age variable in the concrete strength dataset

## Confidence
- **High Confidence**: Directional correctness identification (all LLMs correctly identified that males have higher heart disease risk than females), Claude's superior performance in maintaining non-zero means in weakly informative priors
- **Medium Confidence**: KL divergence as a quality metric for prior evaluation, the finding that weakly informative priors generally outperform moderately informative ones due to LLM overconfidence
- **Low Confidence**: The practical utility of LLM-suggested priors for prediction improvement, generalizability to domains with sparse or conflicting training data, and handling of highly non-linear relationships

## Next Checks
1. **Small Dataset Validation**: Apply the LLM-prior elicitation method to a small-N dataset (n<100) to determine if prediction improvements become statistically significant when prior information is more influential relative to the likelihood
2. **Non-linear Relationship Testing**: Design a synthetic dataset with known non-linear relationships (e.g., quadratic or interaction effects) to test whether LLMs can correctly identify and encode these in prior distributions, or if specialized prompting is needed
3. **Domain Transferability Assessment**: Test the method on a domain with potentially conflicting or sparse training data (e.g., rare disease associations) to evaluate whether LLMs can still provide useful directional priors or if performance degrades significantly