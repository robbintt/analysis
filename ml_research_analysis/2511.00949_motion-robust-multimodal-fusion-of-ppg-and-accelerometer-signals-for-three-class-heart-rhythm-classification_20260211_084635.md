---
ver: rpa2
title: Motion-Robust Multimodal Fusion of PPG and Accelerometer Signals for Three-Class
  Heart Rhythm Classification
arxiv_id: '2511.00949'
source_url: https://arxiv.org/abs/2511.00949
tags:
- motion
- signals
- rhythm
- atrial
- fibrillation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of accurately classifying heart
  rhythms (atrial fibrillation, sinus rhythm, and other arrhythmias) using wrist-worn
  PPG sensors, which are vulnerable to motion artifacts. The authors propose RhythmiNet,
  a residual neural network enhanced with temporal and channel attention mechanisms
  that jointly processes PPG and accelerometer (ACC) signals.
---

# Motion-Robust Multimodal Fusion of PPG and Accelerometer Signals for Three-Class Heart Rhythm Classification

## Quick Facts
- **arXiv ID:** 2511.00949
- **Source URL:** https://arxiv.org/abs/2511.00949
- **Reference count:** 21
- **Primary result:** RhythmiNet achieves 4.3% improvement in macro-AUC over PPG-only baseline for three-class heart rhythm classification (AF, sinus rhythm, other) in motion-contaminated wrist-worn PPG signals

## Executive Summary
This paper addresses the challenge of accurately classifying heart rhythms from wrist-worn PPG sensors, which are vulnerable to motion artifacts. The authors propose RhythmiNet, a residual neural network enhanced with temporal and channel attention mechanisms that jointly processes PPG and accelerometer signals. Evaluated on 1,000 hours of synchronized PPG, ECG, and ACC recordings from 49 elderly cardiac inpatients, the model achieves a 4.3% improvement in macro-AUC over the PPG-only baseline and outperforms logistic regression HRV baselines by 12%. The multimodal fusion approach demonstrates strong performance across all motion levels, highlighting its effectiveness in real-world, noisy clinical environments.

## Method Summary
RhythmiNet processes 30-second windows of synchronized PPG and tri-axial accelerometer data (960 samples at 32Hz) using a residual CNN architecture with Squeeze-and-Excitation blocks and temporal attention. The model concatenates 4 input channels (1 PPG + 3 ACC axes) and processes them through a convolutional stem with stride-2, two residual stages with SE modules, temporal attention for denoising, and a classification head. Training uses stratified patient-level splits without excluding motion-contaminated segments, with evaluation stratified by accelerometer-derived motion intensity percentiles. The model is compared against PPG-only baselines and logistic regression models using handcrafted HRV features.

## Key Results
- RhythmiNet achieves 4.3% improvement in macro-AUC over PPG-only baseline for three-class classification
- Model outperforms logistic regression HRV baseline by 12% in macro-AUC
- Performance remains robust across all motion intensity levels, maintaining >0.75 macro-AUC even in 90-100% motion percentile segments
- Multimodal fusion provides consistent improvement over single-sensor approaches across the entire motion spectrum

## Why This Works (Mechanism)

### Mechanism 1: Motion-Contextualized Multimodal Fusion
The model likely learns to use ACC signals as explicit motion context, allowing it to distinguish between physiological arrhythmias and motion-induced signal distortions. When high ACC variance coincides with PPG irregularity, the model can suppress false arrhythmia signals, classifying them as noise or clean rhythm based on residual features.

### Mechanism 2: Attention-Based Temporal Denoising
Temporal attention modules dynamically down-weight noisy time steps or feature channels corrupted by motion, focusing inference on cleaner signal segments. The model assigns lower weights to 2-second bursts containing high-motion artifacts, reducing their contribution to final classification.

### Mechanism 3: Inclusive Noise-Robust Generalization
Training on unfiltered, motion-contaminated data forces the model to learn decision boundaries that accommodate real-world signal quality variance. Including worst-case motion percentile segments prevents distribution shift when encountering noisy real-world inputs.

## Foundational Learning

- **Concept: Photoplethysmography (PPG) and Motion Artifacts**
  - **Why needed here:** PPG uses light to measure blood volume changes. Motion changes blood flow and sensor contact, creating artifacts that look like heartbeats but aren't. Understanding this physical basis is required to see why ACC fusion helps.
  - **Quick check question:** Why would a rapid arm movement cause a PPG sensor to falsely detect a "heart beat"?

- **Concept: Squeeze-and-Excitation (SE) Blocks**
  - **Why needed here:** This is a core architectural component of RhythmiNet. It dynamically re-calibrates channel-wise feature responses.
  - **Quick check question:** In an SE block, what does the "Excitation" step do to a feature channel that is deemed unimportant?

- **Concept: Macro-AUC vs. Micro-AUC**
  - **Why needed here:** The paper reports both. The dataset has class imbalance (AF vs. Sinus Rhythm vs. Other). Macro-AUC treats classes equally (important for detecting rare arrhythmias), while Micro-AUC reflects overall accuracy.
  - **Quick check question:** If a dataset has 95% "Normal" and 5% "AF," which metric would better indicate the model's ability to detect AF?

## Architecture Onboarding

- **Component map:** Input (4-channel PPG+ACC tensor) → Conv stem (stride-2) → ResNet BasicBlocks (2 stages with SE) → Temporal Attention → Adaptive Average Pooling → FC → 3-class Softmax

- **Critical path:** 
  1. Synchronization: Ensure PPG and ACC timestamps are perfectly aligned (32Hz)
  2. Normalization: Zero-mean/unit-variance normalization per segment; deviations destabilize SE blocks
  3. Attention Calculation: Verify dimension reduction in Temporal Attention module to ensure correct time step aggregation

- **Design tradeoffs:**
  - Resolution vs. Receptive Field: Stride-2 stem reduces temporal resolution (960 → ~480) for efficiency, risking loss of very high-frequency artifact details
  - 3-Class vs. Binary: Expanding to "Other" captures clinical nuance but complicates decision boundaries compared to binary AF detectors

- **Failure signatures:**
  - Motion Confusion: Model predicts "Other" or "AF" specifically in 80-100% motion percentiles where PPG SNR is critically low
  - Static Drift: Poor performance on low-motion data might indicate overfitting to ACC "noise" features rather than PPG morphology

- **First 3 experiments:**
  1. Motion Ablation: Reproduce percentile-based evaluation to verify >0.75 Macro-AUC in 90-100% motion bin
  2. Modality Drop-out: Compare PPG-only, ACC-only, and fusion performance to confirm multimodal benefit
  3. Attention Visualization: Extract attention weights from high-motion segment to visually confirm down-weighting of motion-corrupted time steps

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can RhythmiNet's predictions be visualized to offer clinicians intuitive explanations for rhythm classification decisions?
- **Basis in paper:** Authors state they aim to "improve model interpretability by developing intuitive visualization tools that highlight which input segments most influence decision-making"
- **Why unresolved:** Current study focuses on classification performance metrics without implementing mechanism for explaining specific predictions
- **What evidence would resolve it:** User study with clinicians showing attention maps correlate with known physiological features

### Open Question 2
- **Question:** Can RhythmiNet be compressed via pruning or quantization to support real-time inference on low-power, resource-constrained wearable devices?
- **Basis in paper:** Authors list investigating "model compression and acceleration techniques... to support real-time inference on low-power, resource-constrained wearable devices" as primary direction
- **Why unresolved:** Model was trained and evaluated on NVIDIA TITAN RTX GPU, leaving efficiency on embedded processors unknown
- **What evidence would resolve it:** Demonstration running on microcontroller unit with latency <100ms and minimal macro-AUC loss

### Open Question 3
- **Question:** Does RhythmiNet retain performance advantage over state-of-the-art models when evaluated on diverse, publicly available datasets?
- **Basis in paper:** Authors intend to "benchmark RhythmiNet against a wider range of state-of-the-art models across diverse, publicly available datasets to comprehensively evaluate its generalizability"
- **Why unresolved:** Study relies on specific clinical cohort of 49 elderly inpatients from single hospital center
- **What evidence would resolve it:** Consistent macro-AUC improvements on external datasets with different demographics and noise profiles

## Limitations

- Clinical dataset restricted access prevents independent verification of results
- Critical architectural hyperparameters (kernel sizes, channel dimensions, dropout rates) remain unspecified
- Performance at extreme motion percentiles (90-100%) may be inflated if training segments were mislabeled due to ECG noise during intense movement
- Binary classification extension was only mentioned as future work without empirical validation

## Confidence

- **High confidence:** Multimodal fusion improves over PPG-only baseline (4.3% macro-AUC gain is statistically clear and mechanism is well-grounded)
- **Medium confidence:** Attention mechanisms specifically drive the robustness gain (plausible but requires attention weight visualization to confirm)
- **Medium confidence:** Clinical relevance for elderly inpatients (population match is appropriate, but single-site study limits generalizability)

## Next Checks

1. **Motion-stratified ablation test:** Replicate percentile-based evaluation to verify sustained >0.75 macro-AUC in 90-100% motion bins
2. **Modality importance test:** Compare PPG-only, ACC-only, and fusion performance to confirm multimodal benefit is not driven by one dominant sensor
3. **Attention mechanism validation:** Extract and visualize temporal attention weights from high-motion segments to confirm the model down-weights motion-corrupted time steps