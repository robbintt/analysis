---
ver: rpa2
title: 'Modern Models, Medieval Texts: A POS Tagging Study of Old Occitan'
arxiv_id: '2503.07827'
source_url: https://arxiv.org/abs/2503.07827
tags:
- prompt
- occitan
- tagging
- prompting
- zero-shot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates large language models (LLMs) on part-of-speech
  (POS) tagging for Old Occitan, a historical language with non-standardized orthography.
  The authors compare multiple open-source models using three prompting strategies
  (Zero-shot, few-shot, and instruction-tuned) across two corpora (hagiographical
  and medical texts).
---

# Modern Models, Medieval Texts: A POS Tagging Study of Old Occitan

## Quick Facts
- arXiv ID: 2503.07827
- Source URL: https://arxiv.org/abs/2503.07827
- Reference count: 13
- Large language models achieve 0.65-0.84 F1-score on Old Occitan POS tagging, with Phi4-14B outperforming smaller models

## Executive Summary
This paper evaluates open-source large language models on part-of-speech tagging for Old Occitan, a medieval Romance language with non-standardized orthography. The authors compare eight models using three prompting strategies across two corpora, finding that larger models like Phi4-14B achieve the best performance (0.84 F1 on medical text, 0.75 on hagiographical text). The study reveals that cross-lingual transfer from modern Romance languages, class frequency effects, and prompt complexity all significantly impact tagging accuracy. Error analysis shows that high-frequency POS classes are reliably identified while low-frequency classes systematically fail.

## Method Summary
The study evaluates eight open-source LLMs (COLaF, Phi4-14B, Mistral-7B, Mistral-Nemo-12B, Mixtral-8x7B, Gemma2-9B, Aya-8B, Qwen2.5-14B) on two Old Occitan corpora (91,953 tokens total) using three prompting strategies: Zero-shot, Prompt A (expert persona + token-by-token instruction), and Prompt B (adds spelling variation examples). Models generate JSON-formatted outputs with 'word' and 'upos' keys, which are evaluated against gold Universal Dependencies annotations using micro/macro/weighted F1-scores and accuracy metrics. The experimental setup uses NVIDIA Tesla V100-16GB hardware with temperature=0 inference (assumed).

## Key Results
- Phi4-14B achieves highest accuracy (0.75 F1 on NAF6195, 0.84 F1 on Albucasis) with consistent performance across prompts
- Prompt A provides best accuracy-stability trade-off for orthographically variable texts
- Albucasis corpus (0.80 F1) outperforms NAF6195 (0.65 F1) due to lower orthographic variability
- High-frequency POS classes (NOUN, ADP) achieve 0.79-0.86 accuracy while low-frequency classes (INTJ, AUX) fail systematically (0.11-0.58 accuracy)

## Why This Works (Mechanism)

### Mechanism 1: Cross-lingual transfer from Romance languages
- Claim: Models pre-trained on French, Spanish, Catalan, and modern Occitan leverage morphological similarities to classify Old Occitan tokens
- Evidence: Phi4-14B normalizes "ancian" → "ancià" (Catalan) before classification; supports Old Occitan and related Romance languages
- Break condition: If orthographic variation exceeds Romance pre-training accommodation, transfer degrades (NAF6195 vs. Albucasis performance gap)

### Mechanism 2: Class frequency creates prior probability bias
- Claim: High-frequency POS classes benefit from abundant exemplars; low-frequency classes lack sufficient representation
- Evidence: NOUN accuracy 0.83 vs. INTJ accuracy 0.11; macro-F1 << micro-F1 indicates rare-class collapse
- Break condition: If rare classes are task-critical, macro-averaged metrics reveal poor performance despite acceptable micro-scores

### Mechanism 3: Prompt complexity increases variance without proportional gains
- Claim: Prompt B's richer context occasionally improves median accuracy but introduces instability
- Evidence: Prompt B shows 2-3× variance increase on NAF6195; wider accuracy distributions vs. Prompt A
- Break condition: For highly non-standardized corpora, Prompt B's variance may outweigh marginal gains; prefer Prompt A for reproducibility

## Foundational Learning

- **Universal Dependencies POS Tagging**: Understanding UD framework's 15 POS classes is required to interpret error analysis and F1-scores per class. Quick check: Can you explain why macro-F1 penalizes class imbalance more than micro-F1 in a 15-class POS task?

- **Cross-Lingual Transfer Learning**: Phi4-14B's success relies on implicit transfer from Romance languages. Quick check: What conditions must hold for a model pre-trained on modern French to correctly tag Old Occitan without fine-tuning?

- **Prompt Engineering for Constrained Generation**: Three prompts differ in framing, constraints, and examples. Understanding token-order preservation and JSON-structured output is critical for reproducibility. Quick check: Why does adding explicit spelling variation examples sometimes decrease accuracy for sensitive models?

## Architecture Onboarding

- **Component map**: Input Text → Tokenization → Prompt Template (A/B/Zero-shot) → LLM Inference → JSON Output Parser → POS Evaluation

- **Critical path**: 1) Load Old Occitan corpus 2) Apply prompt template with token-order preservation 3) Run inference with temperature=0 4) Parse JSON response, align with gold UD tags 5) Compute micro/macro-F1 per class, track variance

- **Design tradeoffs**: Larger models → higher accuracy (0.75-0.84) but higher compute cost and input modification rates (Phi4 omits 7-13% of tokens); Prompt B → potential accuracy gains vs. Prompt A → but 2-3× variance increase on orthographically variable data; preprocessing → may reduce unknown vocabulary but risks anachronistic standardization

- **Failure signatures**: Low macro-F1 with high micro-F1 → rare-class collapse (INTJ, AUX near zero); high Levenshtein distance + high accuracy → model normalizes orthography internally (Phi4 behavior); high Levenshtein distance + low accuracy → model hallucinates or misaligns tokens

- **First 3 experiments**: 1) Replicate Phi4-14B Zero-shot and Prompt A on Albucasis; verify F1 ≈ 0.85 (micro) and token omission rate ≈ 8% 2) Ablate prompt components: test Prompt A without "token-by-token" constraint; measure accuracy drop and misalignment rate 3) Evaluate per-class F1 for INTJ and AUX on NAF6195; confirm macro-F1 penalty and identify systematic NOUN/VERB misclassification patterns

## Open Questions the Paper Calls Out

- Can prompting strategies identified for Old Occitan generalize to other low-resource historical languages like Old French and Medieval Latin? (The authors plan to extend analysis to these languages.)

- To what extent do fine-tuning and decoding strategy choices improve POS tagging quality compared to the prompting approaches evaluated? (The authors intend to evaluate these effects in future work.)

- Would larger or proprietary models (e.g., Llama 3.3, GPT-4) overcome limitations observed in the 8-14 billion parameter models used? (Hardware constraints prevented testing larger models.)

## Limitations

- Evaluation relies on two small corpora (91,953 tokens total) from narrow domains that may not represent full orthographic and syntactic diversity of Old Occitan
- Critical inference hyperparameters (temperature, top_p, max_tokens) are not specified, potentially affecting output consistency and JSON validity
- Cannot determine whether models learned Old Occitan-specific patterns or simply mapped to modern equivalents through dictionary-based normalization

## Confidence

- **High Confidence**: Larger models (Phi4-14B, Qwen2.5-14B) consistently outperform smaller ones; Prompt A provides most stable performance-to-reliability ratio
- **Medium Confidence**: Cross-lingual transfer from Romance languages explains Phi4-14B's success (inferred from error analysis)
- **Low Confidence**: Macro-F1 is most appropriate metric given paper's own findings that rare classes are systematically underperformed

## Next Checks

1. Test Prompt A without "token-by-token" constraint and measure accuracy drop and misalignment rate to validate ordering constraint's essentiality

2. Evaluate same models on Old French or Middle High German to determine if Phi4-14B's success generalizes to other historical Romance languages

3. Generate detailed confusion matrices for 15 POS classes, particularly low-frequency classes (INTJ, AUX, PROPN), to identify systematic misclassification patterns for targeted improvements