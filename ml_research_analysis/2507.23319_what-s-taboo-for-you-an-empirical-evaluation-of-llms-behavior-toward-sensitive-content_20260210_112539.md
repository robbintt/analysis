---
ver: rpa2
title: What's Taboo for You? - An Empirical Evaluation of LLMs Behavior Toward Sensitive
  Content
arxiv_id: '2507.23319'
source_url: https://arxiv.org/abs/2507.23319
tags:
- sensitivity
- llms
- sentences
- language
- content
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigates how large language models (LLMs) implicitly
  moderate sensitive content during paraphrasing. Using GPT-4o-mini to paraphrase
  sentences containing sensitive expressions (e.g., slurs, profanity), the research
  compares human expert annotations with automated classifier outputs across four
  sensitivity categories: Formal/Polite, Informal, Derogatory, and Taboo.'
---

# What's Taboo for You? - An Empirical Evaluation of LLMs Behavior Toward Sensitive Content

## Quick Facts
- arXiv ID: 2507.23319
- Source URL: https://arxiv.org/abs/2507.23319
- Authors: Alfio Ferrara; Sergio Picascia; Laura Pinnavaia; Vojimir Ranitovic; Elisabetta Rocchetti; Alice Tuveri
- Reference count: 11
- Primary result: GPT-4o-mini systematically reduces content sensitivity when paraphrasing, with open-source LLMs slightly underperforming traditional classifiers for sensitive language detection

## Executive Summary
This study investigates how large language models implicitly moderate sensitive content during paraphrasing tasks. Using GPT-4o-mini to paraphrase sentences containing sensitive expressions, researchers compared human expert annotations with automated classifier outputs across four sensitivity categories: Formal/Polite, Informal, Derogatory, and Taboo. The research reveals that LLMs undergo alignment training that induces implicit content moderation, systematically shifting sensitive content toward less sensitive categories during paraphrasing.

The study also compares the performance of open-source LLMs in zero-shot classification against traditional text classifiers, finding that traditional methods (Naïve Bayes and MLP) better align with human judgments and achieve higher accuracy and F1 scores. These findings have significant implications for content moderation practices and highlight the need for careful consideration when deploying LLMs for tasks involving sensitive language detection.

## Method Summary
The researchers employed a mixed-method approach combining human expert annotation with automated classification. They selected 240 sentences containing sensitive expressions from the One Million Posts dataset, covering four categories: Formal/Polite, Informal, Derogatory, and Taboo. Three human annotators independently labeled these sentences for sensitivity. GPT-4o-mini was then used to paraphrase each sentence, after which both the original and paraphrased versions were classified by various models including open-source LLMs (Llama-3.1-8B-Instruct, Qwen2.5-7B-Instruct, Gemma-2-9B-IT) and traditional classifiers (Naïve Bayes, Logistic Regression, MLP, SVM).

The study measured classification accuracy, Cohen's kappa for inter-annotator agreement, and conducted chi-squared tests to analyze category distribution shifts. Additionally, the researchers performed a follow-up experiment where they prompted GPT-4o-mini to generate paraphrasing guidelines based on the human annotation results, revealing the model's implicit understanding of sensitivity norms.

## Key Results
- GPT-4o-mini systematically reduced content sensitivity, particularly shifting Taboo and Derogatory sentences toward less sensitive categories
- Open-source LLMs in zero-shot classification slightly underperformed traditional text classifiers, with Naïve Bayes and MLP models best aligning with human judgments
- Traditional classifiers achieved higher overall accuracy and F1 scores compared to LLM-based approaches for sensitive language detection

## Why This Works (Mechanism)
The implicit content moderation observed in LLMs occurs through alignment training processes that embed sensitivity norms into model behavior. When LLMs are trained on curated datasets and fine-tuned with human feedback, they internalize implicit rules about what constitutes acceptable language. This manifests during generation tasks like paraphrasing, where the model automatically adjusts content to align with learned sensitivity guidelines without explicit prompting.

The effectiveness of traditional classifiers over LLMs in this task stems from their direct training on labeled sensitivity data, allowing them to focus purely on pattern recognition without the confounding effects of content modification. LLMs, by contrast, must balance multiple objectives including coherence, relevance, and implicit moderation guidelines, which can compromise their classification accuracy when used in zero-shot settings.

## Foundational Learning
**Sensitivity classification frameworks** - Why needed: Provides standardized taxonomy for categorizing content; Quick check: Verify four-category system covers relevant sensitivity dimensions
**Inter-annotator agreement metrics** - Why needed: Ensures reliability of human labels; Quick check: Confirm Cohen's kappa values exceed 0.6 threshold
**Zero-shot classification methodology** - Why needed: Enables LLM use without task-specific training; Quick check: Validate prompt design follows established best practices

## Architecture Onboarding

**Component map**: Human annotators -> Sensitivity labels -> LLM paraphrasing -> Classification models -> Evaluation metrics

**Critical path**: Original sentences → Human annotation → GPT-4o-mini paraphrasing → Category classification → Statistical analysis of shifts

**Design tradeoffs**: 
- Using GPT-4o-mini provides strong paraphrasing but may over-moderate; open-source alternatives offer transparency but lower performance
- Human annotation ensures quality but limits scalability; automated classification enables large-scale analysis but may miss nuances
- Traditional classifiers are more accurate but require labeled training data; LLMs can generalize but introduce implicit biases

**Failure signatures**:
- Over-moderation: Excessive shifting of content to Formal/Polite category regardless of original intent
- Category confusion: Misclassification between similar categories (e.g., Informal vs. Derogatory)
- Annotation inconsistency: Low inter-annotator agreement indicating unclear category boundaries

**3 first experiments**:
1. Test GPT-4o-mini with different temperature settings to examine moderation intensity variation
2. Compare zero-shot classification performance across multiple prompt engineering strategies
3. Evaluate cross-cultural sensitivity detection by testing non-English language datasets

## Open Questions the Paper Calls Out
None

## Limitations
- Single paraphrasing model (GPT-4o-mini) and small dataset (240 sentences) limit generalizability across different LLMs and larger text corpora
- Only three human annotators may not fully capture nuanced perceptions of sensitive content across different cultural and linguistic contexts
- Focus on English-language content limits applicability to multilingual contexts where sensitivity norms may differ significantly

## Confidence
High: Implicit content moderation by GPT-4o-mini is consistent across multiple evaluation metrics
Medium: Comparative performance of open-source LLMs versus traditional classifiers based on limited model comparisons
Low: Cross-cultural generalizability of findings to non-English contexts and different sensitivity frameworks

## Next Checks
1. Replicate the study using multiple LLMs (both proprietary and open-source) with varying model sizes and architectures to assess consistency of implicit content moderation effects
2. Expand the dataset to include more diverse linguistic contexts, cultural backgrounds, and sensitivity categories, potentially incorporating languages beyond English
3. Conduct a longitudinal study to examine whether observed patterns of implicit content moderation change over time as models are updated or retrained