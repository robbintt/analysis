---
ver: rpa2
title: 'S$^2$-MLLM: Boosting Spatial Reasoning Capability of MLLMs for 3D Visual Grounding
  with Structural Guidance'
arxiv_id: '2512.01223'
source_url: https://arxiv.org/abs/2512.01223
tags:
- spatial
- visual
- arxiv
- grounding
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: S2-MLLM is a framework that improves spatial reasoning in multi-modal
  large language models for 3D visual grounding. It addresses the challenge of understanding
  3D scene structure from limited 2D views by introducing a spatial guidance strategy
  and a structure-enhanced module.
---

# S$^2$-MLLM: Boosting Spatial Reasoning Capability of MLLMs for 3D Visual Grounding with Structural Guidance

## Quick Facts
- arXiv ID: 2512.01223
- Source URL: https://arxiv.org/abs/2512.01223
- Reference count: 40
- S2-MLLM achieves 59.2% overall accuracy on ScanRefer, outperforming state-of-the-art methods with only 25% of trainable parameters

## Executive Summary
S2-MLLM is a framework that improves spatial reasoning in multi-modal large language models for 3D visual grounding. It addresses the challenge of understanding 3D scene structure from limited 2D views by introducing a spatial guidance strategy and a structure-enhanced module. The method uses feed-forward 3D reconstruction during training to enable implicit spatial reasoning, while the structure-enhanced module captures intra-view and inter-view dependencies and integrates multi-level position encoding. Experiments show S2-MLLM achieves state-of-the-art performance across ScanRefer, Nr3D, and Sr3D datasets, with 59.2% overall accuracy on ScanRefer and strong generalization on out-of-domain benchmarks. The approach is also efficient, requiring only 25% of the trainable parameters compared to full fine-tuning baselines.

## Method Summary
S2-MLLM enhances MLLMs for 3D visual grounding by jointly optimizing pointmap reconstruction and grounding objectives during training. A structure-enhanced module captures intra-view and inter-view dependencies through factorized attention, while multi-level position encoding associates visual representations with spatial positions and viewpoint information. The model uses LLaVA-Video-7B with LoRA fine-tuning, achieving 59.2% accuracy on ScanRefer while requiring only 25% of trainable parameters compared to full fine-tuning baselines.

## Key Results
- Achieves 59.2% overall accuracy on ScanRefer, outperforming state-of-the-art methods
- Strong generalization with 54.2% accuracy on Nr3D and 44.3% on Sr3D
- Requires only 25% of trainable parameters compared to full fine-tuning baselines
- Effective across multiple spatial relation types (e.g., 73.4% on "multiple similar objects" subset)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Training with feed-forward 3D reconstruction supervision induces implicit spatial reasoning capability that persists at inference without explicit reconstruction.
- **Mechanism:** During training, the model jointly optimizes pointmap prediction alongside grounding objectives. This forces visual features to encode geometric structure (depth, layout, cross-view correspondence) rather than just semantic content. At inference, the reconstruction branch is disabled, but the structure-aware representations remain, enabling latent spatial reasoning.
- **Core assumption:** The visual encoder's features transfer from reconstruction to grounding because both tasks require understanding 3D structure from 2D views.
- **Evidence anchors:**
  - [abstract] "We introduce a spatial guidance strategy that leverages the structure awareness of feed-forward 3D reconstruction... our model can implicitly reason about 3D scenes without relying on inefficient point cloud reconstruction."
  - [Section 3.1] "we incorporate spatial guidance into MLLM by jointly optimizing the reconstruction objective... encourages our model to internalize 3D structure within its latent space"
  - [corpus] Related work (Spatial-MLLM, SpatialThinker) confirms spatial reasoning in MLLMs requires explicit structural supervision, though S2-MLLM's implicit transfer approach is novel.
- **Break condition:** If reconstruction loss converges before grounding features learn useful representations, the transfer fails. Paper notes reconstruction converges early (Section 3.1), providing stable structure supervision in early training.

### Mechanism 2
- **Claim:** Multi-level position encoding explicitly binds visual features to 3D coordinates and viewing directions, enabling fine-grained spatial relation reasoning.
- **Mechanism:** Each patch receives: (1) sinusoidal encoding of its 3D world coordinate (computed from depth + camera parameters), (2) MLP-encoded camera ray direction. This creates explicit geometry-visual associations that pure 2D-trained MLLMs lack.
- **Core assumption:** MLLMs cannot infer spatial relations from visual features alone—they need explicit position signals associated with visual tokens.
- **Evidence anchors:**
  - [abstract] "module further integrates multi-level position encoding to associate visual representations with spatial positions and viewpoint information"
  - [Section 3.3] "without explicit association between position cues... and visual appearance, MLLMs struggle to understand fine-grained spatial relations such as distance, direction, and relative relations"
  - [Table 5] Removing MPE causes largest drop: -15.05% Acc@0.25, confirming position encoding is critical.
  - [corpus] Visual Position Prompt paper similarly finds MLLMs lack explicit coordinate alignment; supports this diagnosis.
- **Break condition:** If position encodings dominate visual features, semantic understanding degrades. Paper uses additive fusion with average pooling to balance (Eq. 4).

### Mechanism 3
- **Claim:** Factorized intra-view and inter-view attention enforces cross-view semantic consistency while preserving within-view spatial context.
- **Mechanism:** Patches are grouped two ways: (1) by view index for intra-view attention (captures local dependencies), (2) by patch index across views for inter-view attention (aligns same spatial regions across viewpoints). This mimics spatial-temporal factorization in video transformers.
- **Core assumption:** MLLMs pretrained on independent image-text pairs cannot determine if objects in different views correspond to the same 3D entity.
- **Evidence anchors:**
  - [Section 3.2] "MLLMs cannot maintain semantic consistency across views... unable to determine whether chairs observed from different viewpoints correspond to the same physical object"
  - [Table 8] Base LLaVA-Video achieves only 5.31% Acc@0.25; adding Attn alone improves to 41.74% (+36.43%), demonstrating cross-view attention is foundational.
  - [corpus] MM-Spatial and related work don't address cross-view consistency explicitly; this is a distinct contribution.
- **Break condition:** If views have minimal overlap or extreme viewpoint changes, inter-view attention may create spurious correspondences. Paper uses 16-24 frames (Table 5), suggesting dense sampling mitigates this.

## Foundational Learning

- **Concept:** **Feed-forward 3D reconstruction (Fast3R/DUSt3R family)**
  - **Why needed here:** Understanding that reconstruction can be a training objective (not just an inference tool) requires knowing these methods predict dense 3D from multi-view RGB in one forward pass, without explicit optimization.
  - **Quick check question:** Can you explain why Fast3R differs from traditional SfM/SLAM pipelines in terms of inference requirements?

- **Concept:** **Multi-view geometry basics (intrinsics, extrinsics, depth unprojection)**
  - **Why needed here:** Position encoding requires computing 3D world coordinates from (u, v, depth) using K and T matrices (Eq. 2-3). Without this, you cannot implement the MPE module.
  - **Quick check question:** Given depth D(u,v), intrinsic K, and extrinsic T, can you derive the 3D point in world coordinates?

- **Concept:** **Factorized attention (spatial vs. temporal/inter-view)**
  - **Why needed here:** The intra/inter-view attention design assumes understanding that attention can be structured to reduce computation and enforce specific inductive biases (similar to video transformers like TimeSformer).
  - **Quick check question:** Why would full attention across all patches in all views be problematic for V×N patches, and how does factorization help?

## Architecture Onboarding

- **Component map:** RGB-D frames + poses → Video Encoder (ViT, shared with LLM) → [features + position encoding] → Structure-Enhanced Module (Intra/Inter-view Attn) → Language Query → Tokenizer → Video LLM (LLaVA-Video-7B + LoRA) → [hidden states] → Grounding Head / Language Head / Recon Decoder (TRAIN ONLY)

- **Critical path:**
  1. Visual encoder must output features compatible with both LLM and reconstruction objectives (shared encoder design).
  2. Position encoding must be computed correctly from depth + poses—errors here propagate to all downstream tasks.
  3. Inter-view attention must attend across views at same patch indices; misshaped tensors cause silent failures.

- **Design tradeoffs:**
  - **Shared encoder vs. separate encoders:** Shared ensures unified representation space for reconstruction + grounding, but may compromise semantic features for geometric ones. Paper uses projection layer P to bridge this.
  - **LoRA fine-tuning vs. full fine-tuning:** LoRA on encoder + LLM, but full fine-tuning last 4 LLM layers + attention modules. Balances efficiency (25% params of full FT) with cross-modal alignment needs.
  - **Reconstruction decoder frozen vs. trainable:** Frozen decoder focuses learning on encoder/LLM alignment rather than reconstruction quality itself—reconstruction is means, not end.

- **Failure signatures:**
  - **Reconstruction loss doesn't converge early:** Suggests projection layer P is misaligned; check feature normalization.
  - **Grounding works but language head produces wrong categories:** Language guidance loss may be underweighted; check λ_l (default 1.0).
  - **Model fails on "multiple similar objects" subset:** Intra-view attention may be undertrained; verify attention weights capture local context.
  - **OOD performance collapses:** Model may have overfit to ScanNet layouts; check if spatial guidance loss dominates (λ_r = 0.3 is a starting point, not fixed).

- **First 3 experiments:**
  1. **Ablate spatial guidance (SG only):** Train with reconstruction loss disabled. Expect ~5% drop (Table 5). Confirms SG contribution is real and implementation is correct.
  2. **Visualize attention patterns:** Extract inter-view attention maps for same patch index across views. Check if model attends to corresponding regions. Validates mechanism 3.
  3. **Probe position encoding sensitivity:** Zero out either 3D coordinate encoding (ϕ) or ray direction encoding (ψ) separately. Expect asymmetric degradation—ray direction should affect viewpoint-dependent queries more (Fig. 3b examples).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the framework's robustness be improved to handle inaccurate object proposals, given the reliance on external detectors?
- **Basis in paper:** [explicit] The error analysis identifies "Detection" errors caused by inaccurate bounding boxes from detectors as a distinct failure mode (Supp. Material, Fig. 4).
- **Why unresolved:** The current method treats object proposals as inputs and does not correct or refine the bounding boxes provided by the pre-trained detector (Mask3D).
- **Evidence to resolve:** Experiments evaluating performance degradation under synthetic noise injection into ground-truth proposals, or the integration of an end-to-end trainable detection head.

### Open Question 2
- **Question:** To what extent does spatial guidance reduce the minimum view count required for reliable spatial reasoning?
- **Basis in paper:** [explicit] The ablation study notes that while adding frames helps, "GPU memory usage and training time increase significantly," suggesting a need to optimize the efficiency-accuracy trade-off (Page 8).
- **Why unresolved:** The paper demonstrates that SG helps with 16 vs. 24 frames but does not explore the lower bound (e.g., <10 frames) where structure-from-motion often fails.
- **Evidence to resolve:** A comparative analysis of grounding accuracy and reconstruction loss convergence when training with extremely sparse view inputs (e.g., 3-5 views).

### Open Question 3
- **Question:** Can the implicit spatial reasoning learned via feed-forward reconstruction transfer to outdoor or unbounded scenes?
- **Basis in paper:** [inferred] The method is trained and evaluated exclusively on indoor datasets (ScanNet, ArkitScenes, MultiScan), which have distinct geometric constraints compared to outdoor environments.
- **Why unresolved:** The Fast3R backbone and the model's structural priors are optimized for room-scale geometry; it is unclear if the latent representation scales to unbounded environments.
- **Evidence to resolve:** Zero-shot evaluation results on outdoor 3D visual grounding datasets (e.g., Refer-KITTI or similar driving scenario benchmarks).

## Limitations
- Relies on external object detectors, making it vulnerable to inaccurate bounding box proposals
- Requires dense multi-view inputs, increasing GPU memory usage and training time
- Limited to indoor scenes, with unclear generalization to outdoor or unbounded environments

## Confidence
- Method Description: High - Clear implementation details and ablation studies provided
- Performance Claims: High - Extensive benchmarking across multiple datasets with consistent improvements
- Mechanism Understanding: Medium - Core mechanisms identified but some implementation details remain unspecified
- Generalization Claims: Low - Limited to indoor scenes with no outdoor validation

## Next Checks
1. Verify the projection layer dimensions and implementation between MLLM encoder and Fast3R decoder
2. Confirm the exact prompt template used for language supervision
3. Test the model's performance with reduced frame counts to understand the minimum view requirement for spatial reasoning