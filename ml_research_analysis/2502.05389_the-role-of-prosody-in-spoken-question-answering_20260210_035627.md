---
ver: rpa2
title: The Role of Prosody in Spoken Question Answering
arxiv_id: '2502.05389'
source_url: https://arxiv.org/abs/2502.05389
tags:
- prosodic
- information
- speech
- lexical
- prosody
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work investigates the role of prosody in spoken question
  answering (SQA) tasks by systematically isolating prosodic and lexical information.
  Using the SLUE-SQA-5 dataset, which consists of natural speech, the authors modify
  the audio to create three conditions: natural (both lexical and prosodic), lexical
  (only lexical with flattened pitch and intensity), and prosodic (only prosodic with
  low-pass filtering).'
---

# The Role of Prosody in Spoken Question Answering

## Quick Facts
- arXiv ID: 2502.05389
- Source URL: https://arxiv.org/abs/2502.05389
- Authors: Jie Chi; Maureen de Seyssel; Natalie Schluter
- Reference count: 18
- Primary result: Prosodic information alone provides meaningful signal for SQA, but models predominantly rely on lexical cues when available

## Executive Summary
This work investigates how prosody contributes to spoken question answering (SQA) by systematically isolating prosodic and lexical information. Using the SLUE-SQA-5 dataset, the authors modify audio to create three conditions: natural (both lexical and prosodic), lexical (only lexical with flattened pitch/intensity), and prosodic (only prosodic with low-pass filtering). They train SQA models using discrete spoken units and find that prosody alone provides useful information for answering questions, though models show a strong preference for lexical features when both are available. This suggests prosody serves as a supplementary cue that requires better integration methods to be fully leveraged alongside text-like features.

## Method Summary
The study uses the DUAL framework with frozen WavLM-Large encoder (layer 23) to extract continuous embeddings from modified audio, which are then quantized into 1000 discrete units using pretrained k-means clustering. These units are processed by a Longformer-base-4096 language model for span prediction. Audio is modified using Parselmouth: lexical condition flattens pitch and intensity to utterance means, while prosodic condition applies 300Hz low-pass filtering. Models are trained and evaluated across all train-test condition combinations using FF1 and AOS metrics, with training on 8Ã— A100-80G GPUs for up to 18 epochs.

## Key Results
- Models trained on prosodic information alone achieve FF1 scores around 18% versus 6% chance baseline
- When equal amounts of data from each condition are provided, models predominantly rely on lexical information
- Prosodic models show performance degradation when question-context pairs are randomized, confirming the signal is relevant
- Natural speech models generalize better to other conditions than models trained on isolated features

## Why This Works (Mechanism)

### Mechanism 1
Prosodic features alone provide sufficient signal for localized question answering beyond random chance. Low-frequency acoustic components retained after low-pass filtering serve as structural markers, allowing the model to map emphasis patterns to likely answer locations. This relationship between prosodic emphasis and information salience in the dataset is consistent enough to be learned as a heuristic. The signal is validated by performance significantly above chance and degradation when question-context pairs are randomized. If audio is filtered below ~200Hz, performance degrades sharply as pitch cues are lost.

### Mechanism 2
Models exhibit a strong lexical bias, optimizing for text-like tokens even when prosodic data is abundant. Lexical features offer a lower-loss path for the optimizer compared to abstract prosodic patterns. Even with 90% prosodic-only training data, the model rapidly converges on the 10% lexical data, effectively ignoring the prosodic signal to minimize error faster. This bias is confirmed by training dynamics showing rapid decrease in loss for lexical sets. If lexical discrete units are removed entirely, the model is forced to rely on prosody, confirming the hierarchy.

### Mechanism 3
Discrete spoken units preserve non-lexical prosodic information suitable for Transformer processing. K-means clustering of WavLM representations quantizes continuous audio into discrete tokens that implicitly encode supra-segmental features alongside phonetics. The clustering resolution (1000 clusters) is high enough to prevent prosodic detail from being averaged out during quantization. If the number of clusters is too low (<100), prosodic nuance is likely collapsed into generic phonetic categories, destroying the signal.

## Foundational Learning

- **Delexicalization**: Understanding that low-pass filtering below 300Hz removes phonetic detail while preserving F0 is essential for interpreting results. Quick check: Why does a 300Hz cutoff preserve the tune of speech but destroy the words?

- **Self-Supervised Speech Representations (SSL)**: WavLM is pre-trained to predict masked audio segments, encoding both linguistic and para-linguistic information implicitly. Quick check: Why can't we just use a Mel-spectrogram directly as input for a standard NLP model like BERT/Longformer?

- **Discrete Speech Units**: The DUAL framework converts continuous audio vectors into discrete tokens, allowing standard text-based Language Models to process audio "text" that carries rhythmic weight. Quick check: What information is potentially lost when mapping continuous audio embeddings to discrete clusters?

## Architecture Onboarding

- **Component map**: Raw Audio -> Pre-processor (Parselmouth) -> WavLM-Large encoder -> K-Means quantizer -> Longformer-base -> Timestamps (Answer Span)

- **Critical path**: The audio preprocessing step is most fragile. The 300Hz cutoff for prosodic condition and flattening algorithm for lexical condition determine experiment validity. If flattening introduces artifacts or cutoff is too low, the resulting data represents noise rather than isolated signal.

- **Design tradeoffs**: 
  - Cut-off Frequency: Higher (400Hz) retains more prosody but risks lexical leakage; lower (100Hz) ensures strict delexicalization but destroys F0
  - Cluster Count: Higher (1000) preserves prosody but increases vocabulary size; lower speeds up training but may lose nuance
  - Deduplication: Shortens sequences but theoretically risks stripping duration cues, though impact is argued to be minimal

- **Failure signatures**:
  - Lexical Leakage: If prosodic model achieves >25% FF1, verify low-pass filter implementation
  - Collapse to Chance: If prosodic model performs at ~6% FF1, check if F0 range was accidentally filtered out
  - No Generalization: If natural model fails on lexical data, check if intensity flattening introduced extreme artifacts

- **First 3 experiments**:
  1. Train on white noise to establish absolute floor for FF1/AOS metrics (Expected: ~6% FF1)
  2. Train/evaluate prosodic-only models with cutoffs at 100Hz, 300Hz, and 600Hz to verify optimal cutoff (Expected: 300Hz optimal)
  3. Train on full Prosodic set mixed with 5% Lexical set to confirm model rapidly ignores prosody (Expected: Loss drops for Lexical/Natural sets immediately)

## Open Questions the Paper Calls Out

- How can model architectures be designed to effectively integrate prosodic cues so they contribute significantly alongside lexical information? Current models predominantly rely on lexical information even with significant prosodic data, requiring more effective integration methods.

- Does prosody play a more critical role in open-ended or inferential SQA tasks compared to literal extractive tasks studied? The study was restricted to extractive tasks, suggesting future work should explore inferential comprehension where prosodic cues might be more impactful.

- Can more sophisticated disentanglement methods verify the true utility of prosody without signal degradation inherent in low-pass filtering? The filtering method degrades prosodic information quality, potentially artificially lowering scores and confounding isolation of variables.

## Limitations
- Dataset size and domain constraints limit generalizability to other SQA domains
- Audio preprocessing precision affects validity of condition isolation
- Discrete unit fidelity and quantization resolution impact prosody retention
- Model capacity constraints may artificially create lexical bias

## Confidence
- Prosody provides meaningful supplementary signal: Medium confidence
- Models exhibit lexical bias when both cues are available: Medium confidence
- Discrete units preserve non-lexical prosodic information: Low-Medium confidence

## Next Checks
1. Train prosodic models with 200Hz, 300Hz, and 400Hz low-pass filters to confirm optimal cutoff and robustness to parameter changes

2. Compare prosodic performance using 100, 500, and 1000 clusters to quantify how quantization resolution affects prosody retention

3. Replace Longformer-base-4096 with Longformer-large-4096 to determine if improved model capacity reduces lexical bias and better integrates prosodic information