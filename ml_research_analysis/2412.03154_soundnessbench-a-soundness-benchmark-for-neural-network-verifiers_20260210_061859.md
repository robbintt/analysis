---
ver: rpa2
title: 'SoundnessBench: A Soundness Benchmark for Neural Network Verifiers'
arxiv_id: '2412.03154'
source_url: https://arxiv.org/abs/2412.03154
tags:
- verifiers
- counterexamples
- instances
- hidden
- verification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SoundnessBench, a new benchmark designed
  to test the soundness of neural network verifiers by incorporating hidden counterexamples.
  Existing benchmarks often lack ground-truth for hard instances, making it difficult
  to validate verifier soundness.
---

# SoundnessBench: A Soundness Benchmark for Neural Network Verifiers

## Quick Facts
- arXiv ID: 2412.03154
- Source URL: https://arxiv.org/abs/2412.03154
- Reference count: 27
- Introduces a benchmark with 26 models containing 206 hidden counterexamples to test verifier soundness

## Executive Summary
This paper introduces SoundnessBench, a novel benchmark designed to evaluate the soundness of neural network verifiers by incorporating hidden counterexamples. Traditional benchmarks often lack ground-truth for hard verification instances, making it difficult to validate whether verifiers are actually sound. The authors address this by creating a two-part training method that deliberately inserts counterexamples that evade adversarial attacks. The benchmark includes 26 diverse models with various architectures and activation functions, totaling 206 hidden counterexamples. Experimental results demonstrate that SoundnessBench successfully identifies bugs in three well-established verifiers (α,β-CROWN, NeuralSAT, and Marabou 2023) when they incorrectly claim verification of instances known to have counterexamples.

## Method Summary
The authors developed SoundnessBench through a two-part training methodology to create neural networks with deliberately inserted hidden counterexamples. The process involves generating neural networks where counterexamples exist but evade detection by standard adversarial attacks like PGD and AutoAttack. The benchmark includes 26 models with diverse architectures (CNNs, MLPs, ResNets, ViTs), activation functions (ReLU, LeakyReLU, GELU, Tanh), and input sizes, totaling 206 hidden counterexamples. The evaluation framework tests verifiers by presenting them with instances known to have counterexamples and checking if the verifiers correctly identify the violation. The benchmark specifically targets the soundness property of verifiers - their ability to produce correct results without false positives on "hard" instances where counterexamples exist.

## Key Results
- SoundnessBench successfully identified bugs in three well-established verifiers: α,β-CROWN, NeuralSAT, and Marabou 2023
- Verifiers that claimed to verify instances were found unsound when those instances contained hidden counterexamples
- Most hidden counterexamples (84% for Marabou 2023) evaded falsification by existing verifiers, demonstrating their effectiveness
- The benchmark provides a valuable tool for improving the reliability and trustworthiness of neural network verification systems, particularly for safety-critical applications

## Why This Works (Mechanism)
SoundnessBench works by creating neural networks where counterexamples exist but are difficult to discover through standard adversarial attacks. The two-part training method generates models that are both verifiable (passing adversarial tests) and contain hidden counterexamples. This creates a ground-truth benchmark where the presence of counterexamples is known with certainty. When verifiers incorrectly claim these instances are verified, it reveals soundness bugs in their implementation. The diversity of architectures and activation functions ensures broad coverage of different verification challenges, making the benchmark effective at exposing implementation flaws across various verifier designs.

## Foundational Learning
**Neural Network Verification**: The process of proving that neural networks satisfy certain properties (e.g., local robustness) for all possible inputs within specified bounds. Why needed: Provides the theoretical foundation for understanding what soundness means in this context. Quick check: Can you explain the difference between completeness and soundness in verification?

**Adversarial Attacks**: Methods like PGD and AutoAttack that attempt to find inputs that cause neural networks to misclassify. Why needed: The training method uses these attacks to ensure counterexamples are hidden from standard discovery methods. Quick check: What distinguishes white-box from black-box adversarial attacks?

**Bound Propagation**: Techniques used by verifiers to compute output bounds given input constraints, crucial for determining whether counterexamples exist. Why needed: Many verifier bugs stem from incorrect bound computations, especially with non-ReLU activations. Quick check: How do linear relaxation methods approximate non-linear activation functions?

**Quadratic Programming (QP) Solvers**: Optimization engines like Gurobi used by some verifiers (e.g., Marabou) to solve verification constraints. Why needed: The paper notes potential interaction issues between bound propagation and QP solvers as a source of bugs. Quick check: What role does a QP solver play in complete neural network verification?

## Architecture Onboarding

**Component Map**: Training Pipeline -> Model Generation -> Benchmark Creation -> Verifier Evaluation

**Critical Path**: 
1. Generate hidden counterexamples through two-part training
2. Create diverse model collection with known counterexamples
3. Evaluate verifiers on instances with hidden counterexamples
4. Identify unsound verification claims

**Design Tradeoffs**: 
- Hidden counterexamples must evade adversarial attacks but remain discoverable by verifiers (balance between difficulty and feasibility)
- Model diversity vs. focused bug detection (broad coverage vs. deep analysis)
- Benchmark size vs. manageability (206 counterexamples provide sufficient coverage without being unwieldy)

**Failure Signatures**: 
- Verifiers claiming "verified" status on instances with known counterexamples
- High success rates in adversarial attacks but low success in verifier falsification
- Inconsistent behavior across different activation functions or architectures

**3 First Experiments**:
1. Test a verifier on a SoundnessBench model with a single hidden counterexample to observe basic unsound behavior
2. Compare verifier performance across different activation functions (ReLU vs. Tanh) to identify architecture-specific bugs
3. Evaluate the same verifier on models of increasing size to assess scalability of bug detection

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the specific root cause of the soundness bugs in Marabou 2023 on Tanh activation and ViT architectures?
- Basis in paper: The authors state: "For the bugs of Marabou on Tanh and ViT, the authors of Marabou conjectured that there might be bugs in the interaction with the quadratic programming engine in Gurobi, but the specific cause remains unknown."
- Why unresolved: The Marabou team only conjectured about Gurobi interaction issues but did not isolate the precise implementation flaw.
- What evidence would resolve it: Targeted debugging using SoundnessBench counterexamples to isolate which component (bound propagation, QP solver interface, or transformer handling) produces unsound results.

### Open Question 2
- Question: Can hidden counterexamples remain resilient against future adversarial attack algorithms not included in the current training pipeline?
- Basis in paper: "While PGD and AutoAttack cannot cover new attacks that may appear in the future... future work may consider new attacks."
- Why unresolved: The training only incorporates PGD and AutoAttack; attack algorithms continue to evolve.
- What evidence would resolve it: Periodic re-evaluation of SoundnessBench models against newly published attack methods to test if hidden counterexamples become discoverable.

### Open Question 3
- Question: Does scaling to larger models and real-world data diminish the bug-exposing capability of hidden counterexample benchmarks?
- Basis in paper: Appendix E shows MNIST models with larger inputs produce fewer bug exposures; the authors note "on these relatively larger models, NN verifiers with bugs are less likely to produce unsound results."
- Why unresolved: Larger input dimensions may allow loose relaxations to compensate for bound computation errors, masking unsoundness.
- What evidence would resolve it: Systematic study varying model and input size while controlling for hidden counterexample placement to identify the scale at which bug detection degrades.

### Open Question 4
- Question: Can the hidden counterexample training methodology generalize to verification properties beyond local robustness (e.g., reachability, safety constraints)?
- Basis in paper: The paper focuses exclusively on classification robustness; the framework's applicability to other specification types in Equation (1) is unexplored.
- Why unresolved: The two-objective training is designed for misclassification; adapting margin objectives for other output specifications requires new formulations.
- What evidence would resolve it: Extending the training pipeline to produce models with hidden violations of alternative specifications and testing verifiers on those instances.

## Limitations
- The two-part training method may not fully capture the complexity and diversity of counterexamples found in practical neural network deployments
- The benchmark focuses on specific architectures and activation functions, potentially limiting generalizability to other neural network designs
- Relatively small number of models and counterexamples (26 models with 206 hidden counterexamples) may limit the benchmark's robustness

## Confidence
High: The experimental results demonstrating bug identification in established verifiers provide strong evidence for the benchmark's effectiveness. The claim that SoundnessBench can improve reliability and trustworthiness of verification systems is well-supported by presented results.

## Next Checks
1. Evaluate the benchmark's performance on a wider range of neural network architectures and activation functions to assess its generalizability
2. Conduct a larger-scale study with more models and counterexamples to strengthen the benchmark's robustness and applicability
3. Investigate the benchmark's effectiveness in detecting soundness issues in real-world neural network deployments, particularly in safety-critical applications