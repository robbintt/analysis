---
ver: rpa2
title: How Does Personalized Memory Shape LLM Behavior? Benchmarking Rational Preference
  Utilization in Personalized Assistants
arxiv_id: '2601.16621'
source_url: https://arxiv.org/abs/2601.16621
tags:
- user
- intent
- preferences
- memory
- preference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of irrational personalization in
  LLM-powered assistants, where irrelevant user preferences can mislead intent understanding
  and degrade user experience. To address this, the authors introduce RPEval, a benchmark
  featuring a personalized intent reasoning dataset and a multi-granularity evaluation
  protocol, revealing widespread over-personalization issues in existing LLMs.
---

# How Does Personalized Memory Shape LLM Behavior? Benchmarking Rational Preference Utilization in Personalized Assistants

## Quick Facts
- arXiv ID: 2601.16621
- Source URL: https://arxiv.org/abs/2601.16621
- Reference count: 40
- Key outcome: RP-Reasoner improves intent prediction accuracy by ~35% and reduces error severity by ~26% in personalized LLM assistants

## Executive Summary
This paper tackles irrational personalization in LLM-powered assistants, where irrelevant user preferences can mislead intent understanding and degrade user experience. To address this, the authors introduce RPEval, a benchmark featuring a personalized intent reasoning dataset and multi-granularity evaluation protocol, revealing widespread over-personalization issues in existing LLMs. They propose RP-Reasoner, which treats memory utilization as a pragmatic reasoning process, selectively integrating personalized information based on the user's true intent. Experiments show that RP-Reasoner significantly outperforms baselines on RPEval, improving intent prediction accuracy by ~35%, reducing error severity by ~26%, and resolving ~80% of bad cases in a large-scale commercial assistant.

## Method Summary
The authors introduce RPEval, a benchmark with 8,255 samples across 100 meta-scenarios, featuring three intent labels (Ignore, Support, Dominate) and six configurations (single/multi × explicit/implicit × IA/LKO). They propose RP-Reasoner, a pragmatic reasoning approach that treats memory utilization as a Bayesian posterior estimation problem. The method combines Query Likelihood Estimation (MLE) and Intent Prior Estimation (IPE) via rank aggregation. MLE ranks intents by comparing observed queries to simulated ideal queries, while IPE ranks by prior plausibility given user history. The aggregated ranking selects the optimal intent for preference utilization, with empirical results showing significant improvements over baselines on both discriminative and generative metrics.

## Key Results
- RP-Reasoner achieves ~35% improvement in intent prediction accuracy compared to baselines
- Error severity reduction of ~26% across over-personalization, under-personalization, and other failure modes
- Large-scale commercial deployment resolves ~80% of bad cases
- Inverse scaling effect: larger models perform worse at ignoring irrelevant preferences

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Over-personalization occurs because LLMs exhibit an "attraction bias" where preference tokens in the context are amplified during generation, pulling the output toward irrelevant preference content.
- Mechanism: During decoding, LLMs reuse and amplify tokens already present in the context. When an irrelevant preference is inserted, its tokens gain probability mass even when unrelated to user intent, steering the response away from the true goal.
- Core assumption: The probability amplification of preference tokens is a systematic behavior of current LLM architectures during conditional generation.
- Evidence anchors:
  - [abstract] "irrelevant personalized memories are often introduced into the context, interfering with the LLM's intent understanding"
  - [section] Page 6, Finding IV: "Attribution analysis further indicates that these failures primarily stem from LLMs' inherent attraction bias... LLMs indiscriminately increase the probability mass of irrelevant preference tokens"
  - [corpus] Related work "OP-Bench" similarly identifies over-personalization as a key failure mode in memory-augmented agents
- Break condition: If LLM architectures develop explicit context-gating mechanisms that can suppress attention to task-irrelevant context segments, this bias would diminish.

### Mechanism 2
- Claim: Semantic similarity–based retrieval fails to filter irrelevant memories because similarity does not capture pragmatic applicability to the user's intent.
- Mechanism: Standard retrieval uses embedding similarity between query and stored preferences. A preference may be semantically related ("strong rhythm music" ~ "audio recommendations") without being pragmatically relevant (sleep-aid audio does not need strong rhythm), leading to inappropriate context injection.
- Core assumption: Semantic similarity is insufficient for determining whether a preference should influence the current response.
- Evidence anchors:
  - [abstract] "irrelevant personalized memories are often introduced into the context"
  - [section] Page 2, L1 definition: "Directly appends m with high semantic similarity to the context... risks over-personalization when m is irrelevant"
  - [section] Appendix D.1, Table 9: Semantic similarity baseline achieves only 30% accuracy in the Ignore category
  - [corpus] Limited direct evidence in neighbor papers; this paper discusses retrieval failures more extensively than corpus
- Break condition: If retrieval incorporates intent-aware or pragmatic relevance signals beyond pure similarity, this failure mode would reduce.

### Mechanism 3
- Claim: Pragmatic reasoning via Bayesian posterior estimation improves memory utilization by modeling the user's query formulation process.
- Mechanism: RP-Reasoner approximates P(i|q,m) ∝ P_user(q|i,m) × P(i|m). The MLE component ranks intents by simulating how likely a user with each intent would generate the observed query. The IPE component ranks by prior plausibility given user history. Aggregating balances over-reliance and over-suppression of memory.
- Core assumption: The user's query formulation process can be approximated by comparing the observed query against counterfactual idealized queries for each intent.
- Evidence anchors:
  - [abstract] "RP-Reasoner... treats memory utilization as a pragmatic reasoning process, selectively integrating personalized information based on the user's true intent"
  - [section] Page 7, Section 4.1: Bayesian formulation and ranking aggregation described
  - [section] Page 8, Figure 6: Ablation shows MLE is conservative, IPE more permissive; together they balance memory usage
  - [corpus] "RPM: Reasoning-Level Personalization" discusses reasoning-level personalization but does not use Bayesian pragmatic inference
- Break condition: If user queries become fully explicit and unambiguous, pragmatic inference becomes unnecessary.

## Foundational Learning

- **Concept: Rational Speech Acts (RSA) theory / Pragmatic inference**
  - Why needed here: The paper's L2 pragmatic personalization is grounded in RSA, modeling how listeners infer speaker intent from underspecified utterances using Bayesian reasoning.
  - Quick check question: Can you explain why a listener might interpret "Can you pass the salt?" as a request rather than a yes-no question?

- **Concept: Bayesian posterior estimation**
  - Why needed here: RP-Reasoner uses P(i|q,m) ∝ P(q|i,m)P(i|m) to rank intents; understanding how likelihood and prior combine is essential for debugging the ranking logic.
  - Quick check question: If the likelihood ranks intent A > B but the prior ranks B > A, how do you determine the final ranking?

- **Concept: Inverse scaling in LLMs**
  - Why needed here: The paper reports that more capable LLMs perform worse at ignoring irrelevant preferences, a counterintuitive finding with implications for model selection.
  - Quick check question: Why might a more capable model attend *more* to irrelevant context than a less capable one?

## Architecture Onboarding

- **Component map**: (query q, recalled memory m) -> Candidate Intent Generation -> {MLE-Estimator, IPE-Estimator} -> Aggregation -> Response Generator
- **Critical path**: Memory retrieval injects preferences into context -> MLE-Estimator infers which intent best explains observed query -> IPE-Estimator provides prior constraint from user history -> Aggregation balances conservatism (MLE) and permissiveness (IPE) -> Response generation follows selected intent's utilization strategy
- **Design tradeoffs**: MLE-only vs IPE-only vs aggregated (MLE alone is too conservative, IPE alone too permissive; aggregation balances); Inference cost (~2× vanilla); Single-preference vs multi-preference (multi-preference increases MACRO accuracy requirements; RP-Reasoner achieves only ~20-30%)
- **Failure signatures**: Filter Bubble (FB): Response restricted to preference-specific content when general advice appropriate; Under-Personalization (UPB): Relevant preference ignored; Redundant Information (RII): Both preference-specific and general advice given; Inverse scaling on Ignore tasks: Larger models perform worse
- **First 3 experiments**: 1) Ablate MLE and IPE separately on RPEval single-preference setting; measure impact on FB vs UPB rates; 2) Run RP-Reasoner on your own assistant data with 50-100 diverse queries; manually label error types and compare to baseline CoT prompting; 3) Vary retrieved memories (1, 3, 5, 8) in multi-preference setting; plot MACRO accuracy vs memory count to identify where the system breaks down

## Open Questions the Paper Calls Out
1. **Question**: How can retrieval mechanisms and generation-side filtering be jointly optimized to maximize rational personalization in long-context scenarios?
   - Basis in paper: [explicit] The authors explicitly list "investigating how retrieval- and generation-side filtering of irrelevant memories can be coordinated" as a primary future direction in Appendix E.
   - Why unresolved: The current work focuses on a generation-side reasoning module (RP-Reasoner) assuming a fixed retrieval process; the interaction between dynamic retrieval and generation filtering remains unaddressed.
   - What evidence would resolve it: An evaluation of a combined system on RPEval where the retriever is fine-tuned based on feedback from the pragmatic reasoner, compared against current baselines.

2. **Question**: Can fine-tuning LLMs specifically to disregard irrelevant preferences close the performance gap between RP-Reasoner and human-level rationality?
   - Basis in paper: [explicit] Appendix E notes that RP-Reasoner "still falls short of human-level performance" and suggests exploring "how to train models to appropriately disregard preferences."
   - Why unresolved: The proposed RP-Reasoner is a prompt-based reasoning method; it remains unknown if model weights can be updated to internalize this pragmatic reasoning process.
   - What evidence would resolve it: Experiments comparing a model fine-tuned on the RPEval dataset against the prompt-based RP-Reasoner to determine if the former achieves higher accuracy in the "Ignore" category.

3. **Question**: What specific attention mechanisms in larger models drive the inverse scaling effect, where increased capability leads to worse performance in ignoring irrelevant memories?
   - Basis in paper: [inferred] Section 3.3 reports an "inverse scaling effect" where better models perform worse on ignoring irrelevant preferences, and the authors hypothesize it stems from "stronger contextual attention" but do not prove it.
   - Why unresolved: The paper identifies the phenomenon (Finding III) but does not provide a mechanistic explanation or validation for why higher capability models fail at suppression.
   - What evidence would resolve it: A mechanistic interpretability study analyzing attention head patterns in models of increasing size when processing irrelevant versus relevant memories.

## Limitations
- The RP-Reasoner mechanism relies on simulated query comparison and prior intent ranking, but the exact procedure for generating candidate intents and simulating idealized queries is underspecified
- The inverse scaling finding requires further validation across different model families and datasets
- Commercial deployment claims lack detailed statistical validation and control comparisons
- The paper focuses on preference-based personalization while real-world memory systems involve diverse memory types

## Confidence
- **High confidence**: The empirical improvements on RPEval (35% accuracy gain, 26% severity reduction) are well-documented with proper ablation studies showing MLE and IPE contributions
- **Medium confidence**: The inverse scaling phenomenon and mechanism of attraction bias are theoretically plausible but need broader validation across different model families and tasks
- **Medium confidence**: The Bayesian formulation of pragmatic reasoning is sound, but the approximation quality and practical implementation details require further scrutiny

## Next Checks
1. **Ablation stress test**: Systematically ablate MLE and IPE components across all RPEval configurations to verify the balance point and identify failure modes (over-personalization vs under-personalization)
2. **Inverse scaling validation**: Test RP-Reasoner across 3-4 different model families (GPT, Claude, Llama) with varying parameter counts on the Ignore category to confirm whether inverse scaling persists
3. **Multi-memory stress test**: Vary the number of retrieved memories (1, 3, 5, 8) in the multi-preference setting and measure MACRO accuracy degradation to identify scalability limits