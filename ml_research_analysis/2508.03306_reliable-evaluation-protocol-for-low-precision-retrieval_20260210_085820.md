---
ver: rpa2
title: Reliable Evaluation Protocol for Low-Precision Retrieval
arxiv_id: '2508.03306'
source_url: https://arxiv.org/abs/2508.03306
tags:
- evaluation
- range
- retrieval
- scoring
- fp32
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses the instability in retrieval evaluation caused
  by spurious ties when using low-precision scoring functions. To resolve this, the
  authors propose a reliable evaluation protocol combining High-Precision Scoring
  (HPS), which upcasts the final scoring operation to FP32 to collapse ties, and Tie-aware
  Retrieval Metrics (TRM), which compute expected scores, range, and bias to quantify
  ordering uncertainty.
---

# Reliable Evaluation Protocol for Low-Precision Retrieval

## Quick Facts
- arXiv ID: 2508.03306
- Source URL: https://arxiv.org/abs/2508.03306
- Reference count: 40
- Low-precision retrieval evaluation suffers from spurious ties, causing unreliable metrics and overestimation biases.

## Executive Summary
Low-precision retrieval models (BF16/FP16) suffer from spurious ties due to coarse score quantization, leading to unreliable evaluation metrics and potential model ranking errors. To address this, the authors propose a two-part protocol: High-Precision Scoring (HPS) upcasts the final scoring operation to FP32 to collapse ties with minimal overhead, while Tie-aware Retrieval Metrics (TRM) compute expected values, ranges, and biases to quantify ordering uncertainty. Experiments show HPS dramatically reduces tie-induced instability (e.g., MRR@10 range reduced by 36.82 percentage points), while TRM exposes systematic overestimation by tie-oblivious metrics and enables more consistent evaluation.

## Method Summary
The protocol combines High-Precision Scoring (HPS) and Tie-aware Retrieval Metrics (TRM). HPS upcasts only the final logits tensor to FP32 before scoring, preserving low-precision efficiency while restoring score granularity. TRM computes expected metric values over all tie permutations using closed-form formulas, reporting the score range and bias to characterize uncertainty. This approach dramatically reduces tie-induced variability and provides more reliable evaluation of retrieval models in low-precision settings.

## Key Results
- HPS reduced MRR@10 range by 36.82 percentage points in BF16 settings
- TRM exposed systematic overestimation bias up to +9.08 percentage points in tie-oblivious metrics
- HPS recovered near-FP32 stability with negligible computational overhead (~5 ms for 1M×1024 on H200)
- The protocol revealed Qwen3-Reranker appeared superior under tie-oblivious BF16 but ranked lower under TRM

## Why This Works (Mechanism)

### Mechanism 1: High-Precision Scoring (HPS)
Low-precision formats (BF16: 7 mantissa bits; FP16: 10 bits) quantize relevance scores onto a coarse grid, causing distinct scores to collide into tie groups. HPS upcasts the small logits tensor to FP32 (23 mantissa bits) before scoring, restoring finer granularity for candidate distinction without retraining or altering the model forward pass.

### Mechanism 2: Tie-aware Retrieval Metrics (TRM)
Tie-oblivious evaluation arbitrarily breaks ties (e.g., by index order), introducing variance and potential bias. TRM computes closed-form expectations for nDCG, MRR, MAP, Recall, etc., assuming all permutations within tie groups are equally likely, and reports the score range (max-min) and bias (tie-oblivious minus expected) to characterize uncertainty.

### Mechanism 3: Combined Protocol Stability Restoration
HPS dramatically shrinks tie groups, reducing metric variability (e.g., MRR@10 range reduced by 36.82%p). TRM then computes expected values and residual ranges, exposing overestimation bias in tie-oblivious methods (up to +9.08%p in BF16) and preventing incorrect model rankings.

## Foundational Learning

- **Floating-point representation (sign, exponent, mantissa)**: Understanding why BF16 (7 mantissa bits) produces coarser granularity than FP32 (23 bits) explains tie formation. Quick check: In the (0,1) range, approximately how many more distinct values can FP32 represent than BF16?

- **Ranking metrics with cutoff (nDCG@k, MRR@k, MAP@k)**: TRM computes expected values for these metrics; understanding their position sensitivity is essential for interpretation. Quick check: If a relevant document drops from rank 3 to rank 10, which metric shows a larger relative change: MRR@10 or nDCG@10?

- **Expectation over permutations**: TRM's core operation assumes uniform probability over tie-group permutations. Quick check: If 3 documents are tied starting at position 1 and exactly 1 is relevant, what is the expected reciprocal rank?

## Architecture Onboarding

- **Component map**: Model forward pass (BF16/FP16) → Logits extraction → Upcast logits to FP32 → Scoring function (softmax/sigmoid/product) → Score ranking → TRM computation (expected value, range, bias, extrema)

- **Critical path**: 
  1. Identify model scoring function type (softmax, sigmoid, or pairwise product)
  2. Insert upcast (`logits = logits.to(dtype=torch.float32)`) immediately before scoring
  3. Replace tie-oblivious metric computation with TRM: compute expected value via closed-form formulas, then range and bias

- **Design tradeoffs**:
  - HPS: negligible memory overhead (transient FP32 buffer) but requires FP32 compute for scoring; far cheaper than full FP32 inference
  - TRM: O(L) lightweight arithmetic per query, outputs richer signals (expectation + range) vs. single scalar
  - Full FP32 inference: eliminates ties but forfeits low-precision accelerator efficiency

- **Failure signatures**:
  - Large tie groups persist after HPS: suggests upstream quantization degradation in earlier layers
  - TRM range >5%p: residual instability; report with caution
  - Systematic positive bias in tie-oblivious scores: may reflect dataset construction artifacts (relevant items indexed earlier) that TRM exposes

- **First 3 experiments**:
  1. Baseline tie characterization: Run BF16 inference on MIRACLReranking; record tie group sizes and metric ranges without HPS/TRM
  2. HPS-only validation: Add FP32 upcast before scoring; measure tie-group reduction and latency overhead
  3. Full TRM protocol: Add TRM computation; compare expected vs. tie-oblivious scores, verify consistent model rankings and reduced bias toward zero

## Open Questions the Paper Calls Out

- **How does low-precision training influence ranking stability compared to low-precision inference?**: The authors state they "do not explore how low-precision training influences ranking stability." This remains unresolved as the proposed protocol targets inference only.

- **Does combining mixed-precision training with HPS inference yield further stability or efficiency gains?**: The paper notes it is unknown "whether mixed-precision training combined with HPS inference yields further gains." This is unresolved because experiments utilized full-precision models cast to low precision rather than models trained with mixed-precision recipes.

- **How effectively do human evaluators interpret the expectation and range metrics provided by TRM?**: The authors admit they have "not conducted user-centered studies to assess their interpretability." This remains unverified as TRM provides interval data rather than single scalars.

- **Does HPS improve the quality of generated answers in Retrieval-Augmented Generation (RAG) systems?**: The introduction links accurate retrieval to RAG coherence, but experiments measure only retrieval metrics, not generation quality. It's unclear if resolving spurious ties significantly alters the context provided to the generator.

## Limitations

- HPS assumes negligible upcast overhead for typical retrieval workloads, but comprehensive latency vs. batch size scaling is not provided
- TRM's closed-form expectation formulas assume uniform tie-group permutation probabilities, which may not hold for large tie groups near cutoffs
- The paper does not disclose whether tie-oblivious bias is dataset-specific or a general phenomenon
- Without source code or exact data splits, reproducing the precise magnitude of bias and range reductions remains uncertain

## Confidence

- **High Confidence**: HPS reduces tie group sizes and metric range in BF16 settings (direct experimental evidence provided, MRR@10 range reduced by 36.82%p)
- **Medium Confidence**: TRM's expected value formulas are mathematically sound for small tie groups and provide unbiased evaluation (theoretical formulas given, but closed-form derivations for metrics beyond MRR/nDCG not fully detailed)
- **Medium Confidence**: The combined protocol restores near-FP32 stability and exposes systematic overestimation bias (observed in experiments but not validated across diverse datasets or retrieval architectures)

## Next Checks

1. **Latency Scaling Test**: Measure HPS overhead for varying batch sizes and candidate counts (e.g., 1k, 10k, 100k) to confirm negligible cost holds across realistic workloads

2. **Tie-Group Robustness Check**: Evaluate TRM range and bias on synthetic tie groups of size 50-200 near cutoff k to confirm expected value formulas remain reliable for large groups

3. **Dataset Construction Bias Test**: Shuffle relevance item indices in MIRACL to break any positional bias, then rerun TRM to confirm overestimation bias persists or disappears