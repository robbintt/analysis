---
ver: rpa2
title: Continual Self-supervised Learning Considering Medical Domain Knowledge in
  Chest CT Images
arxiv_id: '2501.04217'
source_url: https://arxiv.org/abs/2501.04217
tags:
- learning
- data
- stage
- images
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a continual self-supervised learning (CSSL)
  method for chest CT images that addresses data interference between domains. The
  method uses an enhanced dark experience replay (DER) with k-means sampling to maintain
  diversity and representativeness in the rehearsal buffer across sequential learning
  stages.
---

# Continual Self-supervised Learning Considering Medical Domain Knowledge in Chest CT Images

## Quick Facts
- arXiv ID: 2501.04217
- Source URL: https://arxiv.org/abs/2501.04217
- Reference count: 0
- Primary result: Proposed CSSL method achieves ACC=0.863, AUC=0.940, F1=0.863 on COVID-19 classification

## Executive Summary
This paper addresses catastrophic forgetting in continual self-supervised learning for chest CT images across different imaging conditions. The authors propose an enhanced dark experience replay (DER) method that uses k-means sampling to maintain both diversity and representativeness in the rehearsal buffer. By incorporating mixup augmentation and feature distillation, the method improves representation learning while mitigating domain interference. The approach is validated on mediastinal and lung window CT images, achieving superior performance on COVID-19 classification compared to state-of-the-art baselines.

## Method Summary
The proposed method uses a three-stage approach: Stage 1 performs MAE pretraining on the first domain (D1) using a ViT-B encoder; Stage 2 extracts D1 embeddings and applies k-means clustering (K=312) to stratify sampling based on distance to D2 cluster center, creating a rehearsal buffer with samples selected in 6:3:1 ratios; Stage 3 conducts continual SSL on D2 with buffer samples using mixup augmentation and feature distillation with frozen Stage-1 encoder. The method is evaluated on COVID-19 classification using the SARS-CoV-2 CT-Scan Dataset, with performance measured by accuracy, AUC, and F1 score.

## Key Results
- Proposed method achieves ACC=0.863, AUC=0.940, F1=0.863 when pretraining in order D1→D2
- Outperforms MedCoSS baseline (ACC=0.797) by 6.6 percentage points
- Demonstrates effectiveness of k-means stratified sampling with 6:3:1 ratio
- Shows order sensitivity: D1→D2 (mediastinal→lung) performs better than reverse order

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: K-means sampling with distance-based stratified selection maintains both diversity and representativeness in the rehearsal buffer, reducing data interference between sequential domains.
- **Mechanism**: D1 embeddings are clustered into K groups; distances from each cluster center to the D2 cluster center are computed; clusters are stratified into three groups (G1, G2, G3) by proximity to D2; sampling ratios (γ1:γ2:γ3 = 6:3:1) prioritize clusters closer to the target domain while preserving diversity.
- **Core assumption**: Clusters nearer to the D2 distribution in feature space provide more transferable representations for bridging domains, while maintaining diversity prevents mode collapse.
- **Evidence anchors**:
  - [abstract]: "maintaining both diversity and representativeness within the rehearsal buffer of DER, the risk of data interference during pretraining is reduced"
  - [section 3.3]: Ablation study shows 6:3:1 ratio achieves ACC=0.863 vs 1:1:8 achieving ACC=0.830
  - [corpus]: Related work on privacy-aware CSSL (arXiv:2510.27213) addresses similar domain-shift robustness in multi-window CT
- **Break condition**: If domains differ in anatomical structure rather than imaging conditions, distance-based representativeness may not correlate with transfer utility.

### Mechanism 2
- **Claim**: Feature distillation with frozen Stage-1 encoder preserves prior knowledge while allowing Stage-2 adaptation.
- **Mechanism**: Mixed buffer samples pass through both encoders; L2 loss between ϕM2(bmix) and StopGrad(ϕM1(bmix)) constrains M2 from deviating excessively; M1 parameters remain frozen.
- **Core assumption**: Feature representations encode domain-general and domain-specific information; the L2 constraint preserves the general while permitting controlled adaptation.
- **Evidence anchors**:
  - [section 2.3]: Equation (4) defines distillation loss with explicit StopGrad
  - [section 2.3]: "This feature distillation process helps model M2 adapt to the new domain while preserving key knowledge from the first stage"
  - [corpus]: No direct corpus evidence for feature distillation in CSSL specifically
- **Break condition**: If Stage 1 and Stage 2 require fundamentally conflicting representations, distillation may over-constrain adaptation.

### Mechanism 3
- **Claim**: Mixup augmentation on rehearsal buffer samples improves representation diversity and regularizes against overfitting to stored exemplars.
- **Mechanism**: Buffer batch b is duplicated and shuffled to create b'; samples are mixed via element-wise interpolation: bmix = λ ⊙ b + (1 − λ) ⊙ b'; mixed batch is processed through both encoders for distillation.
- **Core assumption**: Linear interpolation of CT patches generates realistic anatomical variations that expand the effective buffer distribution.
- **Evidence anchors**:
  - [section 2.3]: Equation (3) defines mixup; applied to rehearsal buffer only
  - [abstract]: "incorporate a mixup strategy and feature distillation to further enhance the model's ability to learn meaningful representations"
  - [corpus]: No direct corpus evidence for mixup efficacy in CSSL
- **Break condition**: If pathological features are discrete and non-interpolatable (e.g., presence/absence of specific lesions), mixup may generate implausible samples.

## Foundational Learning

- **Catastrophic Forgetting**:
  - Why needed: CSSL explicitly addresses sequential learning where new domain training overwrites prior representations.
  - Quick check question: Why does training on D2 after D1 degrade performance on D1-derived features without mitigation strategies?

- **Masked Autoencoders (MAE)**:
  - Why needed: Base SSL objective uses high masking ratio (r=0.75) for reconstruction-based representation learning.
  - Quick check question: How does masking 75% of patches force the encoder to learn global semantic features rather than local texture?

- **Feature Space Geometry and Cluster Distance**:
  - Why needed: Enhanced DER relies on Euclidean distances between D1 cluster centers and D2 cluster center to stratify sampling.
  - Quick check question: What does proximity in ViT feature space imply about similarity of imaging characteristics or anatomical content?

## Architecture Onboarding

- **Component map**: ViT-B encoder → Tokenizer (16×16 patches) → MAE decoder → K-means module → Rehearsal buffer → Mixup augmentation → Feature distillation loss
- **Critical path**:
  1. Stage 1: Pretrain M1 on D1 via MAE (300 epochs, masking r=0.75)
  2. Stage 2: Extract D1 embeddings → cluster K=312 → compute distances to D2 center → stratify into G1, G2, G3 → sample T images (6:3:1 ratio)
  3. Stage 3: Initialize M2; for each batch—D2 samples use LMSE, buffer samples use mixup + LFD; freeze ϕM1 throughout
  4. Fine-tune: Train linear classifier on frozen encoder for downstream task

- **Design tradeoffs**:
  - Buffer ratio β=0.05: Larger buffers improve retention but scale memory linearly; paper does not test beyond β=0.05
  - Sampling ratio 6:3:1: Prioritizes representativeness (clusters near D2) over uniform diversity; optimal for similar domains, untested for dissimilar anatomies
  - Domain order: D1→D2 (mediastinal→lung) achieves ACC=0.863; D2→D1 achieves ACC=0.800—order sensitivity suggests mediastinal window provides more transferable base features
  - Assumption: This order sensitivity is domain-pair specific and may not generalize.

- **Failure signatures**:
  - Validation accuracy plateaus below single-domain MAE baseline (0.717 or 0.756): suggests catastrophic forgetting dominant; check buffer construction and distillation weight
  - Large performance gap between D1→D2 and D2→D1 (>5% ACC): indicates asymmetric transfer; may require domain-specific hyperparameter tuning
  - Slow fine-tuning convergence: suggests weak representations; may need longer pretraining or larger buffer

- **First 3 experiments**:
  1. Reproduce single-domain baseline: Train MAE on D1 only, fine-tune on COVID-19 classification to establish reference ACC (~0.717)
  2. Ablate sampling strategy: Compare k-means stratified (6:3:1) vs uniform random sampling vs distance-only sampling to isolate representativeness vs diversity contributions
  3. Isolate distillation contribution: Train CSSL with mixup only (no LFD) vs LFD only (no mixup) vs both to quantify individual mechanism effects

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What determines the optimal ordering of domains in sequential pretraining, and can it be predicted a priori?
- Basis in paper: [explicit] The authors report that D1→D2 yields 0.863 accuracy while D2→D1 yields only 0.800 accuracy, but state only that "the effectiveness of the learning order of domains in the proposed method was also verified" without explaining the underlying cause.
- Why unresolved: The paper demonstrates order matters but provides no theoretical or empirical guidance on what makes one ordering superior.
- What evidence would resolve it: Analysis correlating domain characteristics (distribution statistics, feature complexity, data size) with optimal ordering; experiments across multiple domain pairs to identify patterns.

### Open Question 2
- Question: Can the proposed CSSL method scale beyond two sequential domains to multiple domains while maintaining performance benefits?
- Basis in paper: [inferred] The method is specifically designed for and tested on only two domains (mediastinal and lung window CT). Real clinical environments may involve many more imaging condition variations over time.
- Why unresolved: The three-stage framework (Stage 1: D1 pretraining, Stage 2: buffer sampling, Stage 3: D2 continual learning) does not generalize trivially to N domains, and buffer representativeness may degrade with more stages.
- What evidence would resolve it: Experiments with 3+ sequential domains showing whether accuracy gains persist; analysis of cumulative forgetting across longer task sequences.

### Open Question 3
- Question: How should the sampling ratios (γ1, γ2, γ3) be adaptively determined for new domain pairs without extensive hyperparameter search?
- Basis in paper: [explicit] The ablation study shows performance varies substantially with ratio settings (e.g., 6:3:1 achieves 0.863 ACC vs. 1:1:8 at 0.830 ACC for D1→D2), and the authors note "it was confirmed that focusing on representativeness when storing samples in the rehearsal buffer is effective."
- Why unresolved: Optimal ratios differ between domain orderings (best for D2→D1 is 8:1:1 at 0.800 ACC), suggesting domain-specific tuning is required with no principled selection method provided.
- What evidence would resolve it: A learned or heuristic-based approach that automatically determines sampling ratios based on inter-domain distance metrics; validation across diverse domain pairs.

## Limitations

- Constrained to single modality (chest CT) and specific domain shift (window setting), limiting generalizability
- Assumes feature-space distances correlate with domain similarity, which may not hold for complex domain gaps
- Mixup strategy could generate anatomically implausible samples if interpolation occurs across lesion boundaries
- 1,562-sample rehearsal buffer may be insufficient for larger-scale or higher-resolution datasets
- Method's reliance on k-means clustering introduces computational overhead and potential initialization sensitivity

## Confidence

- **High Confidence**: The core CSSL framework (Stage 1 MAE + Stage 3 continual training) is well-established and reproducible; the superior performance over MedCoSS and baselines (ACC=0.863 vs 0.797) is directly supported by the reported results.
- **Medium Confidence**: The k-means stratified sampling mechanism (6:3:1 ratio) is logically sound and shows ablation support, but its optimal ratio may be dataset-specific and not universally transferable.
- **Medium Confidence**: The feature distillation and mixup strategies are theoretically justified and show quantitative gains, but lack direct ablation studies isolating their individual contributions.
- **Low Confidence**: The claim that mediastinal→lung window order is universally optimal is weakly supported; only one reverse order (lung→mediastinal) is tested, and the asymmetry may be specific to this dataset.

## Next Checks

1. **Ablation of Sampling Strategy**: Systematically compare k-means stratified sampling (6:3:1), uniform random sampling, and distance-only sampling on both domain orders to isolate the contribution of diversity vs. representativeness.
2. **Cross-Dataset Generalization**: Evaluate the method on a different CT dataset (e.g., LIDC-IDRI) with a different domain gap (e.g., scanner/protocol) to test robustness beyond window settings.
3. **Mixup Plausibility Analysis**: Visualize a subset of mixup-generated samples and conduct a radiologist review to assess whether interpolated patches maintain anatomical plausibility and do not obscure critical diagnostic features.