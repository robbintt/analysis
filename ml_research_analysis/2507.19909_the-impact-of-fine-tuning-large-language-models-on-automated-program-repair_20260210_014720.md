---
ver: rpa2
title: The Impact of Fine-tuning Large Language Models on Automated Program Repair
arxiv_id: '2507.19909'
source_url: https://arxiv.org/abs/2507.19909
tags:
- fine-tuning
- code
- performance
- llms
- lora
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the impact of fine-tuning techniques on
  the performance of Large Language Models (LLMs) for Automated Program Repair (APR).
  We evaluate six state-of-the-art code-pretrained LLMs (CodeGen, CodeT5, StarCoder,
  DeepSeekCoder, Bloom, and CodeLlama-2) across three popular APR benchmarks (QuixBugs,
  Defects4J, and HumanEval-Java).
---

# The Impact of Fine-tuning Large Language Models on Automated Program Repair

## Quick Facts
- **arXiv ID**: 2507.19909
- **Source URL**: https://arxiv.org/abs/2507.19909
- **Reference count**: 40
- **Primary result**: Parameter-efficient fine-tuning (PEFT) methods like LoRA consistently improve APR performance while using significantly fewer computational resources compared to full-model fine-tuning

## Executive Summary
This study systematically evaluates how different fine-tuning approaches affect Large Language Models' performance in Automated Program Repair. The research examines six state-of-the-art code-pretrained LLMs across three popular APR benchmarks, revealing that full-model fine-tuning can actually decrease performance due to overfitting and data distribution mismatches. In contrast, parameter-efficient fine-tuning methods, particularly LoRA, consistently improve repair capabilities while using minimal computational resources. The findings demonstrate that PEFT approaches achieve up to 225% performance gains using only 0.09% of the original model's trainable parameters, challenging conventional wisdom about fine-tuning large models for specialized tasks.

## Method Summary
The study evaluates six code-pretrained LLMs (CodeGen, CodeT5, StarCoder, DeepSeekCoder, Bloom, and CodeLlama-2) across three APR benchmarks: QuixBugs, Defects4J, and HumanEval-Java. Three fine-tuning approaches were compared: full-model fine-tuning (updating all model parameters), LoRA (Low-Rank Adaptation), and IA3 (Instruction-aware Adaptation). The evaluation uses multiple metrics including plausibility, CodeBLEU, and exact match rates. All experiments were conducted on the same hardware infrastructure to ensure fair comparison, and the same prompt templates were used across all models and fine-tuning approaches to isolate the effects of the fine-tuning method itself.

## Key Results
- Full-model fine-tuning decreased APR performance in multiple cases due to overfitting and data distribution mismatches
- LoRA consistently outperformed IA3 in 21 out of 24 cases, achieving up to 225% performance gains
- PEFT methods used only 0.09% of the original model's trainable parameters while improving performance
- Larger models generally performed better across all benchmarks
- Including buggy lines in prompts did not consistently improve repair results

## Why This Works (Mechanism)
Parameter-efficient fine-tuning methods like LoRA and IA3 work by adding small trainable adapter layers to the pre-trained model while keeping most parameters frozen. This approach prevents catastrophic forgetting of the model's general code understanding capabilities while allowing adaptation to the specific patterns found in bug-fix pairs. By restricting the number of trainable parameters, PEFT methods avoid overfitting to the training data distribution and maintain the model's ability to generalize to unseen bugs. The low-rank decomposition used in LoRA specifically captures the essential directions of change needed for program repair without disrupting the model's broader code understanding capabilities.

## Foundational Learning
**Program Repair Task Definition**: Understanding that APR involves mapping buggy code to its fixed version - needed because this defines the supervised learning objective; quick check: can you describe the QuixBugs dataset format?
**Code-Pretrained Models**: Recognizing that these models are already trained on large code corpora - needed because PEFT builds on this existing knowledge; quick check: can you name one difference between code-pretrained and general LLMs?
**Parameter-Efficient Fine-Tuning**: Understanding adapter-based methods that modify only small portions of the model - needed because this is the core technical innovation; quick check: can you explain the difference between LoRA and full fine-tuning?
**Evaluation Metrics**: Knowing how plausibility, CodeBLEU, and exact match are calculated - needed because these determine success criteria; quick check: can you compute CodeBLEU given two code snippets?
**Computational Efficiency Trade-offs**: Understanding the relationship between parameter count and resource usage - needed because PEFT's main advantage is efficiency; quick check: can you estimate memory usage difference between full and PEFT fine-tuning?

## Architecture Onboarding

**Component Map**: Dataset (QuixBugs/Defects4J/HumanEval-Java) -> Preprocessing -> Prompt Template -> Model (CodeGen/CodeT5/StarCoder/DeepSeekCoder/Bloom/CodeLlama-2) -> Fine-tuning Method (Full/LoRA/IA3) -> Evaluation Metrics

**Critical Path**: Bug-fix pair extraction → Model loading → Fine-tuning execution → Repair generation → Metric computation → Performance analysis

**Design Tradeoffs**: Full fine-tuning provides maximum adaptation but risks overfitting and requires substantial compute; PEFT balances adaptation with efficiency but may miss some domain-specific patterns; model size increases performance but also computational cost

**Failure Signatures**: Performance degradation on full fine-tuning indicates overfitting; poor results across all methods suggest dataset quality issues; inconsistent results across runs indicate hyperparameter instability

**First Experiments**: 1) Compare single LoRA layer vs multiple layers on CodeT5; 2) Test different rank values in LoRA on StarCoder; 3) Evaluate prompt variations with/without buggy line context on CodeLlama-2

## Open Questions the Paper Calls Out
**Open Question 1**: Can the selection of PEFT adapters be automated based on specific dataset and model characteristics? The current study manually selected and configured LoRA and IA3 adapters, relying on default hyperparameters and manual comparison rather than an automated selection framework.

**Open Question 2**: How does instruction-based fine-tuning compare to NMT-style fine-tuning for APR tasks? This study utilized NMT-style fine-tuning (bug-fix pairs) to replicate prior work, leaving the potential of instruction-based tuning unexplored.

**Open Question 3**: What are the internal mechanics and interpretation of adapter modifications in the context of program repair? The paper evaluates performance through external metrics but does not analyze how adapters internally modify model weights to facilitate repairs.

## Limitations
- Results are limited to Java programs, restricting generalizability to other programming languages
- Only three benchmark datasets were evaluated, which may not represent the full diversity of real-world bugs
- Experiments used specific versions of LLMs and fine-tuning techniques, so results may vary with newer model versions

## Confidence
- **High**: Full-model fine-tuning can decrease APR performance due to overfitting and data distribution mismatches
- **High**: LoRA outperforms IA3 in 21 out of 24 cases with specific performance gains and parameter efficiency
- **Medium**: Including buggy lines in prompts does not consistently improve results
- **Medium**: Larger models generally perform better, though this may be context-dependent

## Next Checks
1. Replicate experiments across multiple programming languages beyond Java to assess language generalizability
2. Test additional APR benchmarks including those from different application domains to evaluate broader applicability
3. Conduct ablation studies comparing different prompt engineering strategies with varying bug context inclusion levels to better understand prompt optimization