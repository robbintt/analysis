---
ver: rpa2
title: Offline Constrained Reinforcement Learning under Partial Data Coverage
arxiv_id: '2505.17506'
source_url: https://arxiv.org/abs/2505.17506
tags:
- policy
- function
- saddle
- algorithm
- optimal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes an oracle-efficient primal-dual algorithm for
  offline constrained reinforcement learning (RL) under partial data coverage. The
  method builds on a linear programming (LP) formulation and introduces a realizability
  assumption ensuring all saddle points of the Lagrangian are optimal, removing the
  need for regularization and auxiliary function classes.
---

# Offline Constrained Reinforcement Learning under Partial Data Coverage

## Quick Facts
- arXiv ID: 2505.17506
- Source URL: https://arxiv.org/abs/2505.17506
- Authors: Kihyuk Hong; Ambuj Tewari
- Reference count: 40
- Primary result: O(ε⁻²) sample complexity for offline constrained RL under partial data coverage

## Executive Summary
This paper introduces an oracle-efficient primal-dual algorithm for offline constrained reinforcement learning under partial data coverage. The method builds on a linear programming formulation and introduces a realizability assumption ensuring all saddle points of the Lagrangian are optimal, eliminating the need for regularization that degrades sample complexity. By applying Lagrangian decomposition and a reparameterization trick, the algorithm extracts policies without requiring knowledge of the data-generating distribution. The approach achieves O(ε⁻²) sample complexity while remaining computationally efficient.

## Method Summary
The paper proposes PDORL and PDOCRL algorithms that frame offline constrained RL as a primal-dual optimization problem. The method reformulates the problem as an occupancy measure optimization with Bellman flow constraints, then applies Lagrangian decomposition to separate policy extraction from the data distribution. The algorithm uses a no-regret oracle for importance weight updates, mirror descent for policy updates, and greedy minimization for Q-function updates. The key innovation is introducing all-policy realizability assumptions that ensure all saddle points of the Lagrangian correspond to optimal policies, removing the need for regularization.

## Key Results
- Achieves O(ε⁻²) sample complexity under partial data coverage
- Eliminates need for regularization by ensuring all saddle points are optimal
- Enables policy extraction without knowing the data-generating distribution
- Provides oracle-efficient implementation with no-regret oracle and linear optimization oracles

## Why This Works (Mechanism)

### Mechanism 1
Under all-policy value realizability, all saddle points of the Lagrangian become optimal, eliminating the need for regularization. The LP formulation introduces Lagrangian multipliers V for Bellman flow constraints. Without realizability, restricted function classes can admit spurious saddle points. Assumption B ensures that for any saddle point, the extracted policy satisfies J(π̂) ≥ J(π*). Core assumption is all-policy state-value realizability. Break condition: if function class fails to realize V^π for some policies, spurious saddle points may emerge.

### Mechanism 2
Lagrangian decomposition with reparameterization enables policy extraction without knowing μ_D. Standard LP policy extraction requires μ_D to compute π(w) ∝ w·μ_D. The paper introduces a dummy occupancy measure ν and reparameterizes it to expose π directly as an optimization variable. Core assumption is Q-function class realizing Q^π for all policies. Break condition: if Q does not satisfy all-policy realizability, spurious saddle points can emerge in the decomposed formulation.

### Mechanism 3
No-regret oracle framework achieves O(ε⁻²) sample complexity while remaining oracle-efficient. The algorithm frames saddle point finding as a Stackelberg game with w-player (no-regret oracle), π-player (mirror descent), Q-player (greedy minimization), and λ-player (greedy minimization for constrained setting). Suboptimality decomposes into bounded regrets. Core assumptions are no-regret oracle with sublinear regret and Slater's condition bounding dual variables. Break condition: if Reg_T/T is not sufficiently small or Slater condition fails, guarantees degrade.

## Foundational Learning

- **Linear Programming (LP) Formulation of RL**: The approach reformulates policy optimization as occupancy measure optimization with Bellman flow constraints. Understanding how μ relates to π via duality is essential.
  - Quick check question: Can you explain why maximizing ⟨μ, r⟩ subject to E^T μ = (1-γ)d_0 + γP^T μ is equivalent to maximizing J(π)?

- **Saddle Points and Lagrangian Duality**: The algorithm's correctness hinges on whether saddle points of the estimated Lagrangian correspond to optimal policies. Proposition 1 shows this can fail without proper assumptions.
  - Quick check question: Given a constrained optimization problem, what conditions ensure that a saddle point (x*, y*) of the Lagrangian corresponds to an optimal primal solution?

- **Importance Weighting and Concentrability**: Partial coverage is quantified via C* = ∥μ^π*/μ_D∥_∞. The algorithm reparameterizes μ = w·μ_D and optimizes over w, requiring bounded importance weights.
  - Quick check question: If C* = 10 for optimal π*, what does this imply about the density of state-action pairs visited by π* in the offline dataset?

## Architecture Onboarding

- **Component map**: Offline dataset D -> Estimated Lagrangian L̂(w,π;Q,λ) -> Primal-dual optimization (w, π, Q, λ players) -> Policy mixture output
- **Critical path**: 
  1. Initialize π_0 (uniform), Q_0 (arbitrary)
  2. For t=1..T: w_t ← no-regret oracle on L̂(·, π_{t-1}; Q_{t-1})
  3. π_t ← softmax update: π_t(·|s) ∝ π_{t-1}(·|s) exp(α Q_{t-1}(·,s))
  4. Q_t ← argmin_{Q∈Q} L̂(w_t, π_t; Q)
  5. Return uniform mixture over π_1..π_T
- **Design tradeoffs**: 
  - All-policy realizability vs. single-policy: Stronger assumption but enables simpler algorithm without regularization
  - Oracle efficiency vs. direct computation: Requires access to no-regret oracle and linear optimization oracle over Q
  - Mixture policy output vs. single policy: Theoretical convenience; may require additional selection heuristic in practice
- **Failure signatures**:
  - Constraint violation in constrained setting: Check Slater condition satisfied with margin φ
  - Poor sample complexity despite large n: Verify concentrability C* not too large; check function class realizability
  - Spurious saddle points detected: Audit whether V or Q classes realize all-policy values
- **First 3 experiments**:
  1. **Tabular validation**: Implement PDORL on small GridWorld with known transitions. Verify recovered policy matches optimal LP solution. Measure J(π̂) vs. J(π*) as function of dataset size n.
  2. **Ablation on realizability**: Construct MDP where Q class intentionally excludes Q^π for some policies. Compare saddle point quality against full realizability setting to quantify Proposition 4 failure mode.
  3. **Constrained setting stress test**: Implement PDOCRL on CMDP with tight constraints (τ_i close to feasible boundary). Test sensitivity to Slater margin φ and dual bound B = 1 + 1/φ.

## Open Questions the Paper Calls Out

- **Can an oracle-efficient algorithm for offline constrained RL be designed that requires only single-policy realizability rather than the stronger all-policy realizability?**
  - Basis: The conclusion states this as a key direction for future work
  - Why unresolved: The proposed algorithm relies on Assumptions B and D (all-policy realizability) to ensure all saddle points are optimal
  - Evidence needed: An algorithm with O(ε⁻²) sample complexity assuming only Q* and w* are realizable

- **Can the requirement for Slater's condition (strict feasibility) be removed or relaxed while maintaining sample efficiency?**
  - Basis: The algorithm and guarantees rely on Assumption E (Slater's condition) to bound dual variables
  - Why unresolved: Analysis depends on bounding ∥λ*∥₁ ≤ 1/φ; if margin is unknown or zero, dual variables may become unstable
  - Evidence needed: A modified primal-dual algorithm that adapts to feasibility margin automatically or handles infeasible problems

- **Can the algorithm be extended to handle large or continuous action spaces efficiently?**
  - Basis: Algorithms utilize softmax policies requiring normalization over action space, intractable for infinite actions
  - Why unresolved: Current analysis assumes exact policy updates; sampling-based approximations might break regret analysis
  - Evidence needed: A variant with convergence proof under sampling-based policy updates or continuous control parameterization

## Limitations
- All-policy realizability assumptions are stronger than typical single-policy realizability and may be difficult to verify
- Performance depends critically on bounded concentrability coefficient C*, which requires knowledge of data-generating distribution
- Implementation requires efficient linear oracles over Q and practical setting of hyperparameters like Slater margin φ
- Experimental validation appears limited to tabular settings without broader empirical benchmarks

## Confidence

- **High confidence**: O(ε⁻²) sample complexity bound under stated assumptions; correctness of Lagrangian decomposition approach
- **Medium confidence**: Practical feasibility of implementing efficient oracles; sensitivity to hyperparameter choices (φ, C*)
- **Low confidence**: Performance on high-dimensional continuous control tasks; generalization beyond tabular MDPs

## Next Checks
1. Implement PDORL on a standard continuous control benchmark (e.g., Hopper-v3) with function approximation and measure constraint satisfaction vs. unconstrained baselines
2. Conduct sensitivity analysis on concentrability coefficient C* by varying dataset collection policy and measuring impact on constraint violation rates
3. Test algorithm robustness by intentionally violating all-policy realizability (e.g., using Q class that excludes certain policy values) and measuring emergence of spurious saddle points