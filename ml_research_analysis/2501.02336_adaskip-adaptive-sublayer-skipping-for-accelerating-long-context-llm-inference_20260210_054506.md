---
ver: rpa2
title: 'AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference'
arxiv_id: '2501.02336'
source_url: https://arxiv.org/abs/2501.02336
tags:
- skipping
- inference
- layers
- decoding
- prefilling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of accelerating long-context LLM
  inference, which is increasingly critical due to the growing context window sizes
  and associated computational costs. The authors identify key limitations of existing
  layer-wise skipping strategies, including their inability to adapt to model and
  context variability, disregard for sublayer significance, and inapplicability to
  the prefilling phase.
---

# AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference

## Quick Facts
- **arXiv ID**: 2501.02336
- **Source URL**: https://arxiv.org/abs/2501.02336
- **Reference count**: 10
- **Primary result**: AdaSkip achieves up to 17% faster inference than baselines while maintaining generation quality within 10% of full models on long-context tasks.

## Executive Summary
AdaSkip is a novel method for accelerating long-context LLM inference by adaptively skipping less important sublayers during both prefilling and decoding phases. The key innovation is using IO cosine similarity between sublayer inputs and outputs as a real-time proxy for importance, combined with historical similarity transfer for prefilling and online learning during decoding. Extensive experiments demonstrate that AdaSkip outperforms existing layer-wise skipping methods on benchmarks like TREC, TriviaQA, and GovReport while achieving significant speedup improvements.

## Method Summary
AdaSkip uses IO cosine similarity to identify unimportant sublayers for skipping during both prefilling and decoding. The method has two main components: offline importance learning that profiles historical tasks to create a skip list for prefilling, and online importance learning that discovers additional FFN skipping opportunities during decoding using the first P tokens. When skipping sublayers, AdaSkip applies scale compensation to approximate the output. The approach works on both attention and FFN sublayers independently, with attention skipping being more effective in long-context scenarios due to their higher computational cost.

## Key Results
- With 8 skipped sublayers, AdaSkip achieves 72.8% accuracy on TREC and 86.6 F1 score on TriviaQA using LLaMA3.1-8B-128k, closely matching full model performance.
- AdaSkip provides up to 17% acceleration improvement compared to baselines while maintaining comparable generation quality.
- The method demonstrates high hit rates for historical similarity transfer (3.76/4 to 9.31/10) and online learning (1.07/2 to 5.03/6) across multiple benchmarks.

## Why This Works (Mechanism)

### Mechanism 1: IO Similarity as Real-Time Importance Proxy
- Claim: Cosine similarity between a sublayer's input and output vectors inversely correlates with its execution importance for the current context.
- Mechanism: During forward pass, compute cosine similarity for each sublayer. High similarity indicates the sublayer minimally transforms the representation, making it a candidate for skipping.
- Core assumption: Representations that undergo minimal transformation contribute less to downstream task performance.
- Evidence anchors: [Introduction] states high IO similarity indicates minimal contribution; [Background] shows LeastSkip (skipping lowest IO similarity) drops GPT score below 1.0 while MostSkip achieves scores of 8.9, 6.1, 4.2 when skipping 1, 3, 5 layers respectively.

### Mechanism 2: Historical Similarity Transfer for Prefilling Acceleration
- Claim: IO similarity patterns learned from historical inference tasks can predict unimportant sublayers for new prefilling tasks.
- Mechanism: Accumulate average similarity and scale factor for each sublayer across N historical tasks. For new tasks, sort sublayers by historical average similarity and skip top 2m sublayers.
- Core assumption: Importance distributions are stable across tasks within similar domains/contexts.
- Evidence anchors: [Methodology] Table 1 shows cross-dataset hit rates of 3.76/4, 4.86/6, 9.31/10 for attention; 3.66/4, 5.69/6, 9.56/10 for FFN.

### Mechanism 3: Online FFN Skipping via Decoding Window
- Claim: Initial decoded tokens provide sufficient signal to identify additional skip-able FFN sublayers beyond prefilling decisions.
- Mechanism: During decoding, compute average similarity for FFN sublayers using first P decoded tokens. Skip additional FFNs where similarity exceeds threshold β from offline phase.
- Core assumption: Early decoding tokens capture context-specific importance that persists through remaining generation.
- Evidence anchors: [Methodology] Table 2 shows hit rates improve with window size: TREC (window=40) achieves 1.07/2, 3.09/4, 4.90/6; GovReport achieves 1.19/2, 3.03/4, 5.03/6.

## Foundational Learning

- Concept: **Transformer sublayer decomposition (Attention vs FFN)**
  - Why needed here: AdaSkip skips attention and FFN independently; understanding their distinct computational profiles is essential for interpreting speedup results.
  - Quick check question: In long-context inference (32K tokens), which sublayer dominates prefilling latency—attention or FFN?

- Concept: **KV Cache mechanics in autoregressive decoding**
  - Why needed here: Skipping attention sublayers during prefilling reduces KV cache storage; understanding what gets cached clarifies the memory benefit.
  - Quick check question: If you skip attention at layer 5 during prefilling, what happens to the KV cache entries for that layer?

- Concept: **Cosine similarity for representation analysis**
  - Why needed here: The entire importance metric hinges on interpreting cosine similarity; understanding its range [-1, 1] and what high/low values mean for vector relationships is non-negotiable.
  - Quick check question: If two vectors have cosine similarity of 0.99, what does that imply about their directional relationship?

## Architecture Onboarding

- Component map:
  - Offline Profiler -> Prefilling Skipper -> Online Learner -> Execution Engine
  - (Profiler processes historical tasks, Skipper applies skip list, Learner discovers FFN skips, Engine executes modified forward pass)

- Critical path:
  1. Profile historical tasks → generate importance ranking (one-time per model/domain).
  2. At inference start, load ranking, apply prefilling skips with compensation.
  3. Begin decoding, collect P tokens, compute online FFN similarities.
  4. Update skip set, continue generation with combined skip decisions.

- Design tradeoffs:
  - Acceleration ratio α: Higher α = more skips = faster but riskier. Paper uses 8-16 skip sublayers for 1.14-1.33x theoretical speedup.
  - Online window P: Larger P = better hit rate but longer before skipping activates. Paper suggests P=20-40 tokens.
  - Prefilling vs decoding skips: Prefilling skips reduce TTFT and KV cache; decoding skips reduce per-token latency. End-to-end requires both.

- Failure signatures:
  - Accuracy collapse on specific tasks: Fixed-skip baselines drop to near-zero F1/Rouge; AdaSkip maintains >90% of full-model performance. If AdaSkip collapses, check domain mismatch in historical profiling.
  - Speedup lower than theoretical: Attention-heavy models (InternLM) show 10%+ speedup gains; FFN-heavy (LLaMA with optimized attention) show less. Profile actual sublayer time distribution.
  - Inconsistent behavior across prompts: Online learning window too small or historical data from different task distribution.

- First 3 experiments:
  1. Reproduce IO similarity ranking: Run profiler on MultiFieldQA, plot average similarity for all sublayers; verify attention shows higher/more concentrated similarity than FFN.
  2. Validate hit rate transfer: Profile on TriviaQA, test skip predictions on MultiFieldQA; confirm hit rate matches Table 1 (~3.8/4 for top-4 skip, ~9.3/10 for top-10).
  3. Measure end-to-end speedup vs quality degradation: Run full pipeline on GovReport with α=1.33 (16 skip sublayers); report actual speedup and Rouge-L drop vs full model (target: >1.25x speedup, <10% Rouge-L degradation).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does AdaSkip's performance scale with Mixture-of-Experts (MoE) architectures or model sizes significantly larger than 8B parameters?
- Basis in paper: [inferred] The experimental evaluation is restricted to dense models between 7B and 8B parameters (LLaMA3.1-8B, InternLM-7B, Vicuna-7B), while related work mentions MoE models like DeepSeek-V2.
- Why unresolved: MoE models utilize sparse routing, meaning the "importance" of a layer may fluctuate dynamically based on the router's decisions, potentially conflicting with the static or history-based skipping decisions of AdaSkip.
- What evidence would resolve it: Benchmark results on MoE models (e.g., Mixtral 8x7B) and larger dense models (70B+) showing the trade-off between speedup and accuracy.

### Open Question 2
- Question: What is the impact of significant distribution shift between the offline learning dataset and the target inference task on skipping accuracy?
- Basis in paper: [explicit] The method relies on the "high correlation between historical prefilling features and new prefilling features" to construct the initial skipping strategy.
- Why unresolved: While Table 1 shows high hit rates between similar text tasks (e.g., TriviaQA to MultiFieldQA), it is unclear if the "offline importance learning" generalizes robustly across distinct domains (e.g., from natural language prefilling to code generation).
- What evidence would resolve it: A sensitivity analysis measuring the drop in generation quality (GPT score/Rouge-L) when the offline profile is derived from a domain completely different from the inference prompt.

### Open Question 3
- Question: Can AdaSkip be combined with KV cache compression techniques to achieve multiplicative speedups without cumulative accuracy loss?
- Basis in paper: [inferred] The "Related Work" section distinguishes AdaSkip (computational skipping) from KV cache eviction methods like H2O and SnapKV, but does not test them jointly.
- Why unresolved: Both methods introduce approximation errors by discarding information (one skips computation, the other skips memory). It is unknown if the "least important" sublayers identified by AdaSkip are the same layers critical for maintaining the context required by KV eviction algorithms.
- What evidence would resolve it: End-to-end experiments implementing AdaSkip on top of a KV cache optimization baseline to measure if the speedups are additive and if accuracy remains stable.

## Limitations
- The method's effectiveness depends on the stability of IO similarity as a universal importance proxy across diverse domains and model architectures.
- Scale compensation introduces approximation error that compounds with the number of skipped sublayers, and the paper doesn't thoroughly characterize error propagation.
- The online learning component's effectiveness depends on the chosen window size P, which appears empirically tuned rather than theoretically derived.

## Confidence
- **High Confidence**: The empirical demonstration that AdaSkip outperforms existing layer-wise skipping methods on standard long-context benchmarks (TREC, TriviaQA, GovReport, MultiNews).
- **Medium Confidence**: The general effectiveness of IO similarity as an importance metric for sublayer skipping within tested domains.
- **Low Confidence**: The stability of historical similarity transfer across significant domain shifts, as the paper doesn't test scenarios where source and target tasks are fundamentally different.

## Next Checks
1. **Domain Transfer Robustness Test**: Profile AdaSkip on tasks from one domain (e.g., legal documents), then apply the learned skip patterns to tasks from a significantly different domain (e.g., scientific papers or code). Measure accuracy drop and speedup to quantify sensitivity to domain shift.

2. **Scale Compensation Error Analysis**: Systematically vary the number of skipped sublayers and measure cumulative error introduced by scale compensation. Compare against theoretical approximation bounds to determine when the compensation mechanism breaks down.

3. **IO Similarity Stability Across Model Architectures**: Apply AdaSkip to models with different architectural designs (e.g., Mamba-based models or models with rotary positional embeddings) to test whether IO similarity remains a reliable importance metric across diverse transformer variants.