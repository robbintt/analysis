---
ver: rpa2
title: Few-Shot Test-Time Optimization Without Retraining for Semiconductor Recipe
  Generation and Beyond
arxiv_id: '2505.16060'
source_url: https://arxiv.org/abs/2505.16060
tags:
- learning
- target
- semiconductor
- process
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Model Feedback Learning (MFL), a test-time
  optimization framework that adapts pre-trained AI models or hardware systems to
  new objectives without retraining or hardware modifications. MFL uses a lightweight
  reverse model to iteratively search for optimal inputs given desired outputs, making
  it particularly valuable in deployment-constrained settings like semiconductor manufacturing.
---

# Few-Shot Test-Time Optimization Without Retraining for Semiconductor Recipe Generation and Beyond

## Quick Facts
- **arXiv ID**: 2505.16060
- **Source URL**: https://arxiv.org/abs/2505.16060
- **Reference count**: 40
- **Primary result**: Model Feedback Learning (MFL) achieves plasma etching recipe generation in 5 iterations without retraining, outperforming Bayesian optimization and human experts

## Executive Summary
This paper introduces Model Feedback Learning (MFL), a test-time optimization framework that adapts pre-trained AI models or hardware systems to new objectives without retraining or hardware modifications. MFL uses a lightweight reverse model to iteratively search for optimal inputs given desired outputs, making it particularly valuable in deployment-constrained settings like semiconductor manufacturing. The authors validate MFL on plasma etching recipe generation, achieving target recipe generation in just five iterations while outperforming both Bayesian optimization and human experts. The method also demonstrates strong performance in chemical vapor deposition (5 iterations) and wire bonding (9 iterations) applications.

## Method Summary
MFL operates by training a lightweight reverse model R to map desired outputs to optimal inputs for a fixed forward model M. The framework employs a two-loop training procedure: first pre-training R using an emulator E in Loop A (1200 iterations), then refining R using the actual machine M in Loop B (200 iterations). Domain randomization is applied during emulator training to improve robustness, and conservative learning rates are used in high-sensitivity regions to prevent instability. The approach is validated on plasma etching recipe generation, where it achieves target outputs in just 5 iterations compared to 20+ for Bayesian optimization, while maintaining strict adherence to input and target constraints.

## Key Results
- Achieves plasma etching recipe generation in 5 iterations versus 20+ for Bayesian optimization
- Maintains stability under noisy conditions and demonstrates superior data efficiency
- Successfully extends to chemical vapor deposition (5 iterations) and wire bonding (9 iterations) applications
- Outperforms human experts in recipe generation tasks while adhering to input constraints

## Why This Works (Mechanism)

### Mechanism 1: Two-Loop Reverse Model Training
A lightweight reverse model can learn to map desired outputs to valid inputs without modifying the deployed forward model. The framework trains Rθ to minimize MSE between target Z′ and E(Rθ(Z′)) in Loop A (emulator), then refines with M(Rθ(Z′)) in Loop B (actual machine). The emulator loop reduces expensive machine interactions, while the machine loop corrects emulator approximation error.

### Mechanism 2: Conservative (Sensitivity-Aware) Learning Rates
Dynamically reducing learning rates in high-sensitivity input regions prevents destabilizing oscillations during late-stage optimization. Compute model sensitivity sE(x) = ‖∂E(x)/∂x‖. When sensitivity exceeds threshold δ, switch from α₁ to α₂ (where α₂ < α₁). This avoids large updates when small input changes cause large output swings.

### Mechanism 3: Domain Randomization for Emulator Robustness
Training the emulator with noisy inputs improves downstream reverse model performance under real-world process variation. Instead of training E on clean pairs (xi, zi), inject zero-mean Gaussian noise into xi during training. This regularizes E to be smoother and more robust to input perturbations.

## Foundational Learning

- **Concept: Inverse Problems and Invertibility**
  - Why needed here: MFL fundamentally solves an inverse problem—finding inputs that produce target outputs. Understanding when inverses exist (or can be approximated) is critical for diagnosing failures.
  - Quick check question: Given a forward model y = f(x), can you sketch conditions under which a unique inverse x = f⁻¹(y) exists locally?

- **Concept: Gradient Descent Through Composed Functions (Chain Rule)**
  - Why needed here: The loss L(θ) = ‖Z′ − E(Rθ(Z′))‖² requires backpropagation through both R and E. Understanding Jacobian chaining is essential for debugging gradient flow.
  - Quick check question: If ∂E/∂x has small singular values, what happens to gradients flowing through to θ?

- **Concept: Emulator/Surrogate Modeling**
  - Why needed here: Loop A depends on E being a useful proxy for M. Understanding fidelity-vs-cost tradeoffs in surrogate models helps set expectations for how much Loop A helps.
  - Quick check question: What diagnostics would you use to detect if E has systematic bias relative to M?

## Architecture Onboarding

- **Component map:** Target Z′ → Reverse Model Rθ → Proposed Input X′ → Forward Model (E or M) → Output Y′

- **Critical path:**
  1. Train E on available data with noise augmentation
  2. Run Loop A (T iterations) to pre-train R using E
  3. Run Loop B (τ iterations) to refine R using M
  4. Deploy R for inference on new targets

- **Design tradeoffs:**
  - T (Loop A iterations) vs. τ (Loop B iterations): More Loop A reduces machine queries but risks emulator bias
  - δ (sensitivity threshold): Lower = more stable but slower; higher = faster but riskier
  - R model capacity: Larger R may overfit; smaller R may underperform on complex inverses

- **Failure signatures:**
  - Output stuck outside target range → R converged to local minimum; try multiple initializations
  - Large Loop A-to-Loop B performance gap → E poorly approximates M; improve emulator training data
  - High variance across runs → R initialization sensitivity; increase pre-training or use ensembles

- **First 3 experiments:**
  1. **Sanity check:** On a synthetic forward model with known inverse, verify R recovers correct inputs within tolerance. This validates the gradient flow is correct.
  2. **Ablation on loop necessity:** Compare (Loop A + Loop B) vs. (Loop B only) vs. (Loop A only) on plasma etching task. Quantify iteration savings from pre-training.
  3. **Noise robustness test:** Add perturbations to targets Z′ and measure output error degradation with vs. without domain randomization in E training.

## Open Questions the Paper Calls Out

### Open Question 1
Can MFL achieve equivalent performance on real semiconductor manufacturing equipment with physical hardware constraints and process variability, rather than in simulation environments? Section 5.1 states experiments are in simulation due to prohibitive costs, but the method can be extended to real-world experiments when data becomes available.

### Open Question 2
How can the MFL framework be extended to online learning, offline learning, and reinforcement learning paradigms? Remark 2 states MFL can be adapted to these scenarios, but exploration is deferred to future work.

### Open Question 3
What is the robustness of MFL when the emulator E provides a poor approximation of the machine model M? Figure 4 illustrates approximation errors, but systematic analysis of emulator quality impact is lacking.

### Open Question 4
How sensitive is MFL performance to the choice of sensitivity threshold δ and the conservative learning rate α₂? The paper reports single fixed values without ablation studies on these hyperparameters.

## Limitations
- Limited comparison scope, primarily against Bayesian optimization without testing against other test-time adaptation methods
- Missing hyperparameter sensitivity analysis for conservative learning rates and sensitivity thresholds
- No systematic analysis of emulator fidelity requirements or when the approach breaks down

## Confidence
- **High confidence**: The core mechanism of using a reverse model for test-time optimization is sound and well-explained
- **Medium confidence**: Domain randomization and sensitivity-aware learning rates are plausible but lack extensive validation
- **Low confidence**: Broad applicability claims rest primarily on two additional examples without systematic testing across diverse domains

## Next Checks
1. **Emulator fidelity stress test**: Systematically vary the quality of the emulator E and measure the resulting impact on MFL performance
2. **Comparison to adapter-based methods**: Implement and compare MFL against adapter-based fine-tuning or prompt tuning approaches
3. **Robustness to target noise**: Test MFL's stability when target outputs Z' contain noise or uncertainty