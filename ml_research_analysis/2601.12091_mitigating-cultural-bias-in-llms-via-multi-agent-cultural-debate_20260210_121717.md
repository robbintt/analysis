---
ver: rpa2
title: Mitigating Cultural Bias in LLMs via Multi-Agent Cultural Debate
arxiv_id: '2601.12091'
source_url: https://arxiv.org/abs/2601.12091
tags:
- cultural
- bias
- macd
- language
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of cultural bias in large language
  models (LLMs), which systematically exhibit Western-centric perspectives even when
  prompted in non-Western languages. To tackle this issue, the authors introduce CEBiasBench,
  a Chinese-English bilingual benchmark, and Multi-Agent Vote (MAV) for robust evaluation.
---

# Mitigating Cultural Bias in LLMs via Multi-Agent Cultural Debate

## Quick Facts
- arXiv ID: 2601.12091
- Source URL: https://arxiv.org/abs/2601.12091
- Reference count: 23
- This paper introduces MACD, a training-free framework that reduces cultural bias in LLMs by orchestrating multi-agent cultural debate, achieving 57.6% No Bias Rate on a bilingual Chinese-English benchmark versus 47.6% baseline.

## Executive Summary
This paper tackles the pervasive problem of cultural bias in large language models, which often default to Western-centric perspectives even when prompted in non-Western languages. The authors introduce CEBiasBench, a bilingual Chinese-English benchmark, and propose Multi-Agent Cultural Debate (MACD), a framework that assigns distinct cultural personas to multiple agents who deliberate using a "Seeking Common Ground while Reserving Differences" strategy. Experiments show MACD achieves a 57.6% average No Bias Rate (versus 47.6% baseline) on CEBiasBench and generalizes to the Arabic CAMeL benchmark with a 96.8% unbiased rate, confirming that explicit cultural representation in agent frameworks is essential for cross-cultural fairness.

## Method Summary
The paper introduces Multi-Agent Cultural Debate (MACD), a training-free framework to mitigate cultural bias in LLMs. MACD orchestrates five cultural agents, each assigned a distinct cultural persona (Western, East Asian, African, Middle Eastern, South Asian), to deliberate via a "Seeking Common Ground while Reserving Differences" (SCGRD) strategy. In a two-round debate, agents first respond from their cultural perspective, then refine their responses by observing others and aligning on shared content while preserving cultural nuances. A summary model synthesizes the final output. Evaluation uses both single-LLM-as-judge and a Multi-Agent Vote (MAV) approach with culturally diverse judges, reporting No Bias Rate as the primary metric.

## Key Results
- MACD achieves 57.6% No Bias Rate on CEBiasBench (Chinese-English) versus 47.6% baseline.
- MACD generalizes to Arabic CAMeL benchmark with 96.8% unbiased rate.
- MAV evaluation is more reliable than single LLM-as-judge, yielding 86.0% vs. 57.6% No Bias Rate.

## Why This Works (Mechanism)

### Mechanism 1
- Assigning explicit cultural personas to agents ensures diverse perspectives are systematically represented during generation.
- Five agents are each primed with detailed cultural backgrounds (e.g., occupation, values, daily routines). Each agent generates an initial response from its cultural standpoint, then observes others' responses and refines. The summary model synthesizes converged viewpoints into a culturally inclusive output.
- Core assumption: The base LLM can reliably adopt and maintain culturally distinct personas when prompted; persona richness translates to response diversity.
- Evidence anchors:
  - [abstract] "MACD... assigns agents distinct cultural personas and orchestrates deliberation via a 'Seeking Common Ground while Reserving Differences' strategy."
  - [section] Table 2 ablation: removing cultural personas drops No Bias Rate from 80.0% to 59.0%.
  - [corpus] Neighbor paper "Multiple LLM Agents Debate for Equitable Cultural Alignment" similarly uses multi-agent debate for cultural alignment, suggesting the pattern generalizes.
- Break condition: If persona prompts are too shallow or stereotypical, agents may produce homogeneous responses, eliminating the benefit of cultural diversity.

### Mechanism 2
- The SCGRD (Seeking Common Ground while Reserving Differences) strategy guides agents to identify cross-cultural commonalities while preserving complementary insights.
- In Round 2, each agent receives others' responses and is instructed to align with shared content and abstract culture-specific details into general principles. The summary model then extracts common ground and preserves culturally enriching details.
- Core assumption: Iterative refinement with explicit consensus-seeking instructions causes convergence toward neutral, inclusive responses rather than simply averaging biased outputs.
- Evidence anchors:
  - [abstract] MACD uses "deliberation via a 'Seeking Common Ground while Reserving Differences' strategy."
  - [section] Ablation: removing SCGRD drops performance from 80.0% to 61.0%; 2 rounds optimal at 80%, 1 round only 53%, 3 rounds drops to 64% (over-smoothing).
  - [corpus] Limited direct corpus evidence on SCGRD specifically; this appears novel to this paper.
- Break condition: If too many rounds or overly aggressive consensus-seeking, valuable cultural nuance is lost (over-smoothing observed at 3 rounds).

### Mechanism 3
- Multi-Agent Vote (MAV) evaluation with culturally diverse judges and explicit "no bias" option provides more reliable bias assessment than single-model evaluation.
- n culturally grounded judge agents rate responses into {0=no bias, 1-5=specific cultural biases} with justifications. Final label determined by majority vote with tie-breaking favoring "no bias."
- Core assumption: Aggregating diverse judgments reduces individual evaluator bias; explicit neutrality option avoids forced categorization artifacts.
- Evidence anchors:
  - [abstract] MAV "enables explicit 'no bias' judgments"; MACD achieves 86.0% by MAV vs. 57.6% by single LLM-as-judge.
  - [section] Table 1: different evaluators show dramatically varying scores (GLM4 >92% for all methods; Llama-70b 6-53%), motivating MAV.
  - [corpus] Neighbor paper "Judging with Many Minds" examines bias amplifications in multi-agent LLM-as-judge, relevant to MAV's design tradeoffs.
- Break condition: If judge agents share correlated biases or the tie-breaking rule biases toward "no bias" excessively, MAV may under-detect actual cultural preferences.

## Foundational Learning

- **Cultural bias in LLMs**
  - Why needed here: The paper's core premise is that LLMs default to Western-centric perspectives even in non-Western language contexts; understanding this motivates MACD's design.
  - Quick check question: Why does prompting in Chinese not eliminate Western bias according to the paper?

- **Multi-agent debate frameworks**
  - Why needed here: MACD builds on prior multi-agent debate (MAD) methods but distinguishes itself via cultural (not functional) role assignment.
  - Quick check question: How does MACD's agent role design differ from Planner-Critique style frameworks?

- **LLM-as-judge evaluation**
  - Why needed here: The paper relies on both single-LLM and multi-agent voting for evaluation; understanding evaluator bias is critical to interpreting results.
  - Quick check question: What systematic evaluator bias pattern does the paper identify across GLM4 and Llama-70b?

## Architecture Onboarding

- Component map:
  - Cultural Agent Debaters (5): Western, East Asian, African, Middle Eastern, South Asian personas with detailed background prompts.
  - Multi-Round Debate (T=2): Round 1 = initial cultural responses; Round 2 = SCGRD-guided refinement.
  - Summary Model: Synthesizes final responses from all agents' Round 2 outputs.
  - MAV Evaluators: n culturally grounded judge agents + majority vote aggregation.

- Critical path: Input question → Meta-prompt instantiation → Agent persona prompts → Round 1 generation → Round 2 SCGRD refinement → Summary synthesis → MAV evaluation.

- Design tradeoffs:
  - More agents increase fairness (1→5 agents: 60%→80%) but raise inference cost.
  - More rounds can over-smooth (3 rounds: 64% vs. 2 rounds: 80%).
  - MAV is more reliable but slower than single LLM-as-judge.

- Failure signatures:
  - Homogeneous responses across agents (persona prompts too weak).
  - Over-generic outputs losing cultural specificity (too many rounds or aggressive SCGRD).
  - Evaluator disagreement or systematically inflated scores (use MAV to mitigate).

- First 3 experiments:
  1. Replicate MACD on CEBiasBench-EN with GPT-4o backbone; compare Direct, CoT, MAD, MACD on No Bias Rate (target: ~57.6% by LLM-as-judge).
  2. Ablate agent count (1, 3, 5 agents) to verify scaling; expect degradation with fewer agents.
  3. Run MACD on CAMeL-Ag subset; confirm generalization to Arabic (target: ~96.8% unbiased rate by GPT-5 evaluator).

## Open Questions the Paper Calls Out
None

## Limitations
- The cultural persona prompts' sufficiency to elicit genuinely distinct perspectives is not fully validated; stereotypes may persist.
- The CEBiasBench benchmark is proprietary, limiting independent verification and broader generalization.
- The framework's effectiveness on less common languages or cultures is not demonstrated.

## Confidence

- **High Confidence**: The empirical observation that MACD improves No Bias Rate over baselines (57.6% vs. 47.6%) on CEBiasBench and generalizes to CAMeL (96.8% unbiased) is well-supported by the ablation and cross-benchmark results.
- **Medium Confidence**: The mechanism claims (cultural personas ensure diversity, SCGRD balances consensus/nuance, MAV provides reliable evaluation) are plausible and supported by ablations, but the exact causal pathways and robustness to prompt/template variations are not fully characterized.
- **Low Confidence**: Claims about the sufficiency of the persona prompts to capture "authentic" cultural perspectives, and the generalizability of the framework to all cultural contexts, are speculative without broader validation.

## Next Checks

1. **Open Replication on Public Benchmark**: Implement MACD on an open, multilingual cultural bias benchmark (e.g., Japanese BiasSet or public CAMeL subsets) to verify generalization beyond the proprietary CEBiasBench and CAMeL datasets used in the paper.
2. **Persona Prompt Sensitivity Analysis**: Systematically vary the richness and specificity of cultural persona prompts (e.g., minimal vs. detailed) and measure the impact on agent response diversity and No Bias Rate to test the core assumption that persona depth drives cultural representation.
3. **MAV Evaluator Bias Audit**: Conduct a controlled study where MAV is applied to responses known to contain specific cultural biases, and analyze whether the majority vote systematically under- or over-detects bias, and whether increasing judge diversity reduces evaluator bias amplification observed in single-LLM-as-judge comparisons.