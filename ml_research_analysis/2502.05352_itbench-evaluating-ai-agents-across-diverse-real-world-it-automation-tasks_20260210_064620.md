---
ver: rpa2
title: 'ITBench: Evaluating AI Agents across Diverse Real-World IT Automation Tasks'
arxiv_id: '2502.05352'
source_url: https://arxiv.org/abs/2502.05352
tags:
- scenario
- agent
- itbench
- scenarios
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ITBench, a comprehensive framework for benchmarking
  AI agents across real-world IT automation tasks. ITBench addresses the challenge
  of measuring AI agent effectiveness in critical IT operations including Site Reliability
  Engineering (SRE), Compliance and Security Operations (CISO), and Financial Operations
  (FinOps).
---

# ITBench: Evaluating AI Agents across Diverse Real-World IT Automation Tasks

## Quick Facts
- arXiv ID: 2502.05352
- Source URL: https://arxiv.org/abs/2502.05352
- Reference count: 40
- Key outcome: State-of-the-art AI agents achieve only 13.8% success on SRE scenarios, 25.2% on CISO scenarios, and 0% on FinOps scenarios

## Executive Summary
ITBench is a comprehensive framework for benchmarking AI agents across real-world IT automation tasks. It addresses the challenge of measuring AI agent effectiveness in critical IT operations including Site Reliability Engineering (SRE), Compliance and Security Operations (CISO), and Financial Operations (FinOps). The framework includes 94 real-world scenarios that can be easily extended through community contributions. Evaluation results show that state-of-the-art AI agents perform poorly on these tasks, with success rates of only 13.8% for SRE scenarios, 25.2% for CISO scenarios, and 0% for FinOps scenarios. ITBench provides push-button workflows, interpretable metrics, and partial scoring to enable automated evaluation and constructive feedback.

## Method Summary
ITBench formalizes IT automation scenarios as tuples containing metadata, operational environments, triggering events, and desired outcomes. Agents interact with these environments through a POMDP framework using specialized tools for observability data access (logs, metrics, traces, alerts) and system control. The framework employs multi-dimensional evaluation metrics including pass@1 for binary success, NTAM for topology-aware diagnostic quality, and time-based metrics for operational efficiency. Agents are implemented using CrewAI with ReAct planning, reflection for error correction, and disaggregation for complex tasks. The evaluation uses inference-only with various LLMs without fine-tuning.

## Key Results
- GPT-4o achieves 13.8% pass@1 on SRE scenarios with traces enabled, dropping to 9.5% without traces
- CISO-agent reaches 25.2% pass@1 on CISO scenarios using GPT-4o
- FinOps scenarios remain unsolved with 0% pass@1 across all tested models
- Agents show significant performance variance across repeated runs (e.g., 1-8 out of 10 successful runs for same scenario)

## Why This Works (Mechanism)

### Mechanism 1: Scenario Formalization with Environment-Trigger-Outcome Tuple
Standardized scenario specification enables reproducible evaluation through tuple <M, E, T, D> structure where M=metadata/specification, E=operational testbed environment, T=triggering events, D=desired outcomes. This creates deterministic, repeatable testbeds with ground truth for automated comparison.

### Mechanism 2: POMDP-Based Agent-Environment Interaction via Specialized Tools
Modeling agent-environment interaction as Partially Observed Markov Decision Process enables systematic evaluation of sequential decision-making under partial observability. Agents use tool-mediated observations and decision functions to select actions, maximizing success probability across scenario distributions.

### Mechanism 3: Multi-Dimensional Evaluation with Partial Scoring and Feedback
Multi-metric evaluation with partial scoring provides constructive feedback better than binary success/failure. Uses pass@1, NTAM measuring proximity to ground truth via topology-aware distance, time-based metrics (MTTD, MTTR, TTP), and scenario-specific proximity metrics for cost/efficiency in FinOps.

## Foundational Learning

### Concept: Kubernetes and Cloud-Native Infrastructure
**Why needed here**: 94 scenarios run on Kubernetes clusters with observability stacks; agents interact via kubectl commands; SRE/FinOps scenarios manipulate deployments, replicas, and autoscaling configurations.  
**Quick check**: Can you explain the relationship between Pods, Deployments, ReplicaSets, and Services in Kubernetes, and how horizontal pod autoscaling (HPA) affects both application availability and resource costs?

### Concept: Observability Telemetry (Logs, Metrics, Traces)
**Why needed here**: SRE agents use NL2Logs, NL2Metrics, NL2Traces tools to diagnose incidents; trace data availability significantly impacts diagnosis success.  
**Quick check**: Given an alert about high error rates on a frontend service, what sequence of observability data would you examine to localize the fault, and what specific information would you look for in each?

### Concept: Policy-as-Code (OPA Rego, Kyverno, Ansible)
**Why needed here**: CISO agents generate Kyverno policies, OPA Rego policies, and Ansible playbooks for compliance assessment across Kubernetes and RHEL9 environments.  
**Quick check**: How would you translate the compliance requirement "minimize the admission of containers wishing to share the host network namespace" into a Kyverno policy, and what would be the equivalent OPA Rego expression?

## Architecture Onboarding

### Component Map
Benchmark Runner -> Scenario Environment (Kubernetes clusters with observability stack) -> Agent API Server (manages registration, status) -> Evaluator (compares outputs to ground truth) -> Leaderboard (aggregates results) -> Toolbox (LM-based tools with reflection/linting)

### Critical Path
1. Agent registration -> Benchmark Runner fetches scenarios
2. Environment provisioning -> Fault injection -> Status: FaultInjected
3. Agent polls get_manifest -> Receives Ready signal with credentials
4. Agent calls post_status(STARTED) -> Executes tasks using tools -> post_status(FINISH)
5. Evaluation (NTAM, pass@1, time metrics) -> Results to Leaderboard
6. Environment cleanup -> Next scenario

### Design Tradeoffs
- Open-source framework allows proprietary tech for flexibility
- 94 scenarios across 3 personas with varying complexity; FinOps underrepresented
- Ensures deterministic alert generation but retains real-time telemetry non-determinism
- Supports disabling traces to test agent robustness

### Failure Signatures
- Agent never starts: Check manifest Ready status and credentials
- Low pass@1 despite correct diagnosis: May indicate mitigation tool failures
- High variance across runs: Real-time telemetry fluctuations cause LLM token differences
- Timeout before completion: Check MTTD/MTTR against complexity
- Policy/code generation errors: Check reflection/linting feedback for Rego syntax errors

### First 3 Experiments
1. Run baseline SRE-agent on easy scenarios with traces enabled, verify ~36% diagnosis rate
2. Ablate trace availability, confirm diagnosis pass@1 drops significantly (e.g., GPT-4o from 18.10% to 9.52%)
3. Test CISO-agent on Kyverno scenario class, target ~40% pass@1, examine generated policy artifacts

## Open Questions the Paper Calls Out

### Open Question 1
**Question**: How can multiple AI agents with various expertise be orchestrated to collaborate on complex IT automation projects?  
**Basis in paper**: [explicit] The authors state: "How to orchestrate multiple agents with various expertise to collaborate on bigger projects?" as a key open problem.  
**Why unresolved**: Current ITBench evaluation focuses on single-agent performance; multi-agent coordination introduces challenges in task decomposition, communication protocols, and conflict resolution.  
**What evidence would resolve it**: Demonstrated success on ITBench scenarios using multi-agent systems with measurable improvements over single-agent baselines.

### Open Question 2
**Question**: How can SRE-agents be guided to reason about failures when system observability is incomplete?  
**Basis in paper**: [explicit] The paper notes that removing trace data drastically reduces success rates and states: "how to guide SRE-agents to collect new observability data and to help SRE-agents reason about failures with incomplete observability is an important but open problem."  
**Why unresolved**: Real-world systems rarely have perfect observability (only ~20% of applications have tracing enabled), yet current agents heavily depend on complete data.  
**What evidence would resolve it**: Agent architectures that dynamically request additional observability data and maintain robust performance under partial data masking conditions.

### Open Question 3
**Question**: How can the safety of agent-driven IT automation solutions be ensured before production deployment?  
**Basis in paper**: [explicit] Listed as a fundamental open question: "How can we ensure safety of agent-driven solutions?"  
**Why unresolved**: Agents execute commands like `kubectl delete node` and generate code; the paper acknowledges risks from executing LLM-generated commands on systems.  
**What evidence would resolve it**: Guardrail mechanisms or formal verification methods that prevent unsafe actions while maintaining automation capability, validated on ITBench scenarios.

## Limitations
- Limited scenario coverage with only 2 FinOps scenarios compared to 42 SRE and 50 CISO scenarios
- Significant performance variance across repeated runs due to real-time telemetry fluctuations
- POMDP assumption may oversimplify non-Markovian real-world incident patterns

## Confidence

**High Confidence**: Scenario formalization mechanism (tuple structure with metadata, environment, triggers, outcomes) and multi-dimensional evaluation framework are well-specified and reproducible.

**Medium Confidence**: POMDP-based agent-environment interaction modeling is theoretically sound but may not capture all real-world complexities of IT incident response.

**Medium Confidence**: Partial scoring metrics (NTAM, time metrics) provide constructive feedback but their correlation with actual operational improvement requires further validation.

## Next Checks

1. **Ground Truth Validation**: Conduct expert review of scenario ground truth against actual production incident resolution data to assess coverage gaps and potential oversimplification.

2. **Metric Correlation Study**: Measure whether partial scoring improvements (NTAM, time metrics) correlate with measurable operational improvements in controlled production-like environments.

3. **Long-term Dependency Testing**: Design scenarios that require cross-incident reasoning and organizational memory to test POMDP assumption limitations and evaluate non-Markovian reasoning capabilities.