---
ver: rpa2
title: What Works for 'Lost-in-the-Middle' in LLMs? A Study on GM-Extract and Mitigations
arxiv_id: '2511.13900'
source_url: https://arxiv.org/abs/2511.13900
tags:
- context
- performance
- variable
- data
- metric
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the "lost-in-the-middle" phenomenon in
  large language models (LLMs), where models struggle to effectively utilize information
  located in the middle of long contexts. To study this issue in a real-world application,
  the authors introduce GM-Extract, a novel benchmark dataset designed to evaluate
  LLM performance on retrieval of control variables.
---

# What Works for 'Lost-in-the-Middle' in LLMs? A Study on GM-Extract and Mitigations

## Quick Facts
- arXiv ID: 2511.13900
- Source URL: https://arxiv.org/abs/2511.13900
- Authors: Mihir Gupte; Eshan Dixit; Muhammad Tayyab; Arun Adiththan
- Reference count: 40
- Key outcome: The study finds that the efficacy of mitigation methods for "lost-in-the-middle" is highly nuanced and dependent on the specific model and task format, with no universally effective solution.

## Executive Summary
This paper investigates the "lost-in-the-middle" phenomenon in LLMs, where models struggle to utilize information in the middle of long contexts. The authors introduce GM-Extract, a novel benchmark for evaluating LLM performance on retrieval of control variables. They propose two distinct evaluation metrics: Document Metric (spatial awareness) and Variable Extraction Metric (semantic comprehension). The study evaluates 7-8B parameter models on multi-document tasks, finding that data representation format significantly impacts performance. Mitigation methods are categorized into black-box and white-box approaches, with efficacy varying by model and task format.

## Method Summary
The study builds the GM-Extract benchmark with 92 control variable objects containing 7 metadata fields each. Multi-document contexts are created with gold variables rotated across positions (every 3rd document position, 50 runs each). Two metrics are used: Document Metric (accuracy of identifying correct document ID) and Variable Extraction Metric (accuracy of retrieving variable content). The evaluation pipeline tests baseline models (LLaMa-2-7B, Vicuna-7B-1.5, LLaMA-3.1-8B) and applies mitigation methods including Ms-PoE, Hidden State Scaling, and LoRA fine-tuning with positional data augmentation. Implementation uses a single A100 80GB GPU with PyTorch/HuggingFace.

## Key Results
- Data representation format (structured KV vs natural language QA) significantly impacts model performance, with 15-20% difference on Document Metric for instruct-tuned models
- Perplexity correlates with performance degradation, indicating representation bias
- White-box methods (Ms-PoE, Hidden State Scaling) improve performance on KV tasks but may degrade QA performance
- Black-box methods (IN2 fine-tuning) show more universal benefits but require compute-intensive training data generation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Rotary Positional Embeddings (RoPE) cause attention score decay between distant tokens, contributing to middle-context retrieval failures.
- Mechanism: RoPE encodes position by rotating query/key vectors. The angular distance between tokens grows with relative distance |a−b|, causing attention decay for middle positions that are far from both the query position and the document boundaries.
- Core assumption: The attention decay pattern directly translates to retrieval failure rather than being compensated by other attention heads.
- Evidence anchors:
  - [Section III-C1]: "RoPE... has a known limitation: the attention score between two tokens decays as the distance between their positions increases. This long-term decay property is a key contributor to the 'lost-in-the-middle' problem."
  - [corpus]: Neighbor paper "Layer-Specific Scaling of Positional Encodings" confirms this issue arises from positional encoding layers, supporting the mechanism.

### Mechanism 2
- Claim: Hidden state channels encode monotonic positional information, creating structural bias toward start/end positions.
- Mechanism: Causal attention masks cause certain hidden state channels to exhibit "roughly monotonic" values correlated with token position. Middle channels show decay for middle positions, leading to unequal attention distribution.
- Core assumption: Scaling specific channels translates to measurable retrieval improvement without disrupting other capabilities.
- Evidence anchors:
  - [Section III-C2]: "Hidden state channels also convey significant information about the positional states of tokens... attention for positional embeddings decays for the middle channels."
  - [Section V-C]: Hidden State Scaling showed 10-12% improvement on Key-Value tasks for Vicuna-7B.

### Mechanism 3
- Claim: Data representation format directly impacts model perplexity and retrieval performance through pre-training distribution alignment.
- Mechanism: Models exhibit lower perplexity (higher certainty) on data formats matching their pre-training distribution. QA format (natural language) outperforms Key-Value format (structured) by 15-20% on Document Metric for instruct-tuned models.
- Core assumption: Perplexity correlation indicates causal relationship rather than coincidental alignment.
- Evidence anchors:
  - [Section V-A]: "A model exhibits higher epistemic uncertainty (higher PP) when faced with a data representation that results in lower performance."
  - [Section IV-A1]: Shows explicit formatting difference between structured KV and paragraph QA formats.

## Foundational Learning

- Concept: **Needle-In-A-Haystack (NIAH) Evaluation Paradigm**
  - Why needed here: The entire GM-Extract benchmark follows this paradigm—embedding target information at varying positions to measure positional retrieval robustness.
  - Quick check question: Can you explain why NIAH evaluates both "what" (content) and "where" (position) retrieval separately?

- Concept: **Primacy and Recency Bias**
  - Why needed here: These biases form the theoretical basis for the U-shaped performance curve and explain why information at context boundaries is better retrieved than middle content.
  - Quick check question: Why might a model attend more strongly to the first and last tokens in a sequence?

- Concept: **Perplexity as Uncertainty Measure**
  - Why needed here: The paper correlates perplexity changes with performance degradation, using PP to diagnose representation bias.
  - Quick check question: What does a higher perplexity score indicate about a model's confidence on a given input?

## Architecture Onboarding

- Component map: Input Context → RoPE Positional Encoding → Attention Layers (with hidden states) → Position-dependent attention decay → [Ms-PoE: Multi-scale head rescaling] → [Hidden State Scaling: Channel modification] → Output Generation

- Critical path: RoPE encoding → attention distribution → hidden state channels → final token query attention (for Hidden State Scaling method)

- Design tradeoffs:
  - White-box methods (Ms-PoE, Hidden State Scaling): Inference-time, no retraining, but model-specific tuning required
  - Black-box methods (IN2 fine-tuning): More universal but requires compute-intensive training data generation
  - Long-context models: Superior performance but degrade when context approaches window limits

- Failure signatures:
  - Document Metric degrades faster than Variable Extraction Metric as context length increases (spatial awareness collapses before semantic comprehension)
  - Non-instruct models (LLaMA-2-7B) show negative impact from mitigation methods—formatting bias interacts poorly with unaligned training
  - Near 10k+ tokens, even 32k/128k context models show 10-30% Document Metric degradation

- First 3 experiments:
  1. **Baseline positional evaluation**: Run GM-Extract benchmark (25 docs for KV, 22 for QA) across all document positions with 50 repetitions each. Measure both metrics and perplexity per position.
  2. **Data representation comparison**: Run identical retrieval task in KV vs QA format. Expect 15-20% difference; verify perplexity correlates with performance gap.
  3. **Mitigation method ablation**: Apply Ms-PoE and Hidden State Scaling separately to Vicuna-7B-1.5. Measure which task format (KV vs QA) benefits from which approach—expect white-box helps KV more, black-box helps QA more per Section V-D.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can a dedicated, normalized spatial awareness metric be integrated into standard evaluation suites to diagnose long-context failure modes, given that spatial retrieval degrades significantly faster than semantic comprehension?
- Basis in paper: [explicit] Section V.E, RQ1.
- Why unresolved: The authors demonstrate that the "Document Metric" (spatial awareness) collapses far more severely than the "Variable Extraction Metric" (semantic comprehension) as context increases, yet standard benchmarks often fail to decouple these distinct failure modes.
- What evidence would resolve it: The development and validation of a new evaluation framework that specifically isolates and normalizes spatial location awareness across varying context lengths and model architectures.

### Open Question 2
- Question: To what extent does modifying the pre-training sequence distribution or the RLHF curriculum directly mitigate positional bias, and how does this compare to inference-time interventions?
- Basis in paper: [explicit] Section V.E, RQ2 and Section V-D.
- Why unresolved: Due to resource constraints, the authors focused on fine-tuning (PEFT) and inference-time methods, but hypothesize that the root cause lies in the pre-training stage, which remains untested in this study.
- What evidence would resolve it: A comparative study measuring "lost-in-the-middle" performance on models specifically pre-trained with long-context distributions versus those treated with white-box or black-box post-hoc mitigations.

### Open Question 3
- Question: Does the volume of augmented training data required for data-driven mitigation strategies (like IN2-Training) scale linearly with context length (e.g., >100k tokens), or are there inherent efficiency limits?
- Basis in paper: [explicit] Section V.E, RQ3.
- Why unresolved: The study was limited to 12k tokens, while state-of-the-art models now operate at 100k-2M tokens; it is unclear if current data-driven methods remain effective or feasible at extreme scales.
- What evidence would resolve it: Scaling experiments that apply IN2-Training to models with 100k+ context windows to identify the data volume required to maintain robustness without saturating model capacity.

## Limitations
- Resource constraints limited testing to 7-8B parameter models and 12k token contexts, while state-of-the-art models now operate at 100k-2M tokens
- White-box methods show model-specific effectiveness, requiring careful tuning and potentially introducing new biases
- The efficacy of mitigation methods is highly dependent on task format, with no universally effective solution across all use cases

## Confidence
- **High**: The correlation between perplexity and performance degradation is well-established and consistently observed across multiple experiments
- **Medium**: The mechanism of RoPE-induced attention decay is theoretically sound but requires more empirical validation across different positional encoding schemes
- **Medium**: The effectiveness of white-box methods varies significantly by model and task format, indicating nuanced but not universal applicability

## Next Checks
1. **Validate perplexity correlation**: Run GM-Extract benchmark on additional models and verify that perplexity consistently correlates with performance degradation across both Document and Variable metrics
2. **Test white-box method universality**: Apply Ms-PoE and Hidden State Scaling to a diverse set of models (both instruct and non-instruct) to quantify the variance in effectiveness and identify failure patterns
3. **Extreme context scaling**: Evaluate mitigation methods on models with 100k+ context windows to determine if data-driven approaches scale linearly or hit efficiency limits at extreme lengths