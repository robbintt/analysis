---
ver: rpa2
title: High-Resolution Sustain Pedal Depth Estimation from Piano Audio Across Room
  Acoustics
arxiv_id: '2507.04230'
source_url: https://arxiv.org/abs/2507.04230
tags:
- pedal
- depth
- binary
- room
- sustain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a high-resolution approach to sustain pedal
  depth estimation, framing it as a continuous regression problem rather than binary
  classification. A Transformer-based architecture is proposed, incorporating convolutional
  feature extraction, a multi-layer Transformer encoder, and separate prediction heads
  for frame-wise pedal depth, onset, offset, and global depth.
---

# High-Resolution Sustain Pedal Depth Estimation from Piano Audio Across Room Acoustics

## Quick Facts
- arXiv ID: 2507.04230
- Source URL: https://arxiv.org/abs/2507.04230
- Reference count: 0
- Primary result: Transformer-based model achieves state-of-the-art performance in 4-class pedal depth classification (F1 0.686) and continuous regression (MSE 0.042, MAE 0.134) on synthetic reverberant piano recordings.

## Executive Summary
This paper introduces a high-resolution approach to sustain pedal depth estimation, framing it as a continuous regression problem rather than binary classification. A Transformer-based architecture is proposed, incorporating convolutional feature extraction, a multi-layer Transformer encoder, and separate prediction heads for frame-wise pedal depth, onset, offset, and global depth. The model is trained using a multi-task loss combining MSE for continuous depth values and BCE for pedal events. To study acoustic robustness, a synthetic dataset is created by rendering aligned MIDI from MAESTRO under four varied room acoustics. Evaluations show the model achieves state-of-the-art performance in binary classification while outperforming baselines in 4-class classification (F1 0.686 vs ~0.56) and continuous regression (MSE 0.042, MAE 0.134). Qualitative analysis reveals richer expressive pedal modeling. Leave-one-out experiments confirm that models trained on diverse acoustic data generalize better to unseen environments, with statistical analysis showing reverberation introduces an overestimation bias that is mitigated by acoustic diversity in training.

## Method Summary
The proposed method uses a Conv2D-Transformer architecture to estimate continuous pedal depth from piano audio. The input features are 20 MFCCs plus log-mel spectrogram (229 bands), extracted at 100 fps from 16 kHz audio. A three-layer Conv2D block with batch normalization, ReLU activation, and max pooling produces a frequency-downsampled feature map. This is fed into an 8-layer Transformer encoder (d=256, heads=8, FFN=1024) with positional encoding. Four MLP heads predict frame-wise depth, onset, offset, and global depth. The model is trained with a multi-task loss (0.6×MSE for depth, 0.2×MSE for global depth, 0.1×BCE for onset, 0.1×BCE for offset) using AdamW (lr=5e-4) for about 192k steps. The evaluation uses synthetic reverberant piano recordings rendered from MAESTRO MIDI using Pianoteq 8 Stage under four room presets.

## Key Results
- Achieves F1 of 0.686 for 4-class pedal depth classification, outperforming baselines (~0.56).
- Continuous regression results: MSE of 0.042, MAE of 0.134.
- Leave-one-room-out experiments show better generalization with training on diverse acoustic data.
- Statistical analysis reveals overestimation bias in reverberant conditions, mitigated by acoustic diversity in training.

## Why This Works (Mechanism)
The paper frames pedal depth estimation as a continuous regression problem, enabling richer expressive modeling compared to binary classification. The Conv2D-Transformer architecture leverages local spectral patterns via convolution and long-range temporal dependencies via self-attention. Multi-task learning with auxiliary onset/offset detection improves pedal event localization. Training on diverse reverberant conditions helps the model learn robust features invariant to acoustic environment changes.

## Foundational Learning
- **Transformer Encoder**: Self-attention mechanism that captures long-range temporal dependencies; needed to model complex pedal patterns over time; quick check: verify attention weights focus on pedal-related frames.
- **Multi-task Learning**: Joint training on depth, onset, and offset prediction; needed to leverage correlated tasks and improve generalization; quick check: monitor individual task losses to detect imbalance.
- **Log-mel Spectrogram + MFCCs**: Time-frequency representations capturing harmonic and timbral content; needed for robust audio feature extraction; quick check: ensure spectrogram resolution matches pedal dynamics (100 fps).
- **Soft Labels for Edge Detection**: Gradual labeling around onset/offset to account for detection uncertainty; needed to reduce annotation noise; quick check: verify soft label width matches expected pedal attack/decay times.
- **Synthetic Reverberant Dataset**: Controlled generation of recordings under varied room acoustics; needed to study robustness to real-world conditions; quick check: confirm reverb presets cover expected RT60 range.

## Architecture Onboarding
- **Component Map**: Conv2D Block -> Transformer Encoder -> 4 MLP Heads (Depth, Onset, Offset, Global Depth)
- **Critical Path**: Audio features (log-mel + MFCCs) → Conv2D feature extraction → Transformer encoder with positional encoding → Separate MLP heads for each prediction task
- **Design Tradeoffs**: Continuous regression vs. binary classification (finer expression vs. simplicity); Transformer depth (8 layers) vs. efficiency; synthetic data (controlled acoustics vs. real-world variability)
- **Failure Signatures**: Overestimation bias under reverberation; poor generalization to unseen acoustic conditions; multi-task loss imbalance
- **First Experiments**:
  1. Train baseline Conv2D-Transformer model on clean MAESTRO data and evaluate binary F1.
  2. Train model with multi-task loss on synthetic reverberant data and compare 4-class F1 to baseline.
  3. Perform leave-one-room-out evaluation to assess generalization to unseen acoustics.

## Open Questions the Paper Calls Out
None

## Limitations
- Synthetic dataset limits real-world generalizability; absolute performance on real recordings unknown.
- Leave-one-out generalization tests based on only 4 acoustic conditions; conclusions tentative.
- Overestimation bias under reverberation identified but not fully explained or mitigated.

## Confidence
- **High**: Relative performance improvements over baselines are convincing, given controlled experiments and multiple metrics.
- **Medium**: Conclusions about acoustic robustness are plausible but based on limited synthetic environments; real-world testing needed.
- **Low**: Explanation for overestimation bias is speculative; deeper analysis required.

## Next Checks
1. Apply the model to a held-out subset of MAESTRO (or new real-world dataset) not used in training or tuning to confirm reproducibility of reported scores.
2. Systematically vary loss weights and experiment with data augmentation to quantify impact on accuracy and generalization.
3. Conduct detailed analysis of overestimation bias under reverberation by visualizing predictions vs. targets and experimenting with mitigation strategies.