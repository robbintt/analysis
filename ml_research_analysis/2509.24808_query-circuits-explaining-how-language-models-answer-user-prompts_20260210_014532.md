---
ver: rpa2
title: 'Query Circuits: Explaining How Language Models Answer User Prompts'
arxiv_id: '2509.24808'
source_url: https://arxiv.org/abs/2509.24808
tags:
- query
- circuit
- circuits
- edges
- discovery
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of explaining why language models
  produce specific outputs for individual input queries. While prior work has identified
  global capability circuits (e.g., for indirect object identification), these do
  not explain model behavior on specific prompts.
---

# Query Circuits: Explaining How Language Models Answer User Prompts

## Quick Facts
- arXiv ID: 2509.24808
- Source URL: https://arxiv.org/abs/2509.24808
- Authors: Tung-Yu Wu; Fazl Barez
- Reference count: 40
- The paper introduces query circuits to trace information flow from specific inputs to outputs in language models, addressing limitations of global capability circuits.

## Executive Summary
This paper addresses the challenge of explaining why language models produce specific outputs for individual input queries. While prior work has identified global capability circuits (e.g., for indirect object identification), these do not explain model behavior on specific prompts. The authors introduce query circuits, which directly trace the information flow within a model that maps a specific input to its output. A key challenge is evaluating circuit faithfulness, as the commonly used Normalized Faithfulness Score (NFS) becomes unstable on general datasets. To address this, the authors propose Normalized Deviation Faithfulness (NDF), a more robust metric bounded in [0,1]. Another challenge is finding compact, faithful query circuits; the authors propose Best-of-N (BoN) sampling and its variants to efficiently identify sparse circuits that recover much of the model's behavior. Across benchmarks including IOI, arithmetic, MMLU, and ARC Challenge, experiments show that extremely sparse query circuits can be found within models—for example, a circuit covering only 1.3% of model connections can recover about 60% of performance on MMLU questions. The results demonstrate that query circuit discovery is a practical path toward faithful, scalable explanations of how language models process individual inputs.

## Method Summary
The authors introduce query circuits as a framework for tracing how language models transform specific inputs into outputs. They address two key challenges: evaluating circuit faithfulness and discovering compact, faithful circuits. For faithfulness evaluation, they propose Normalized Deviation Faithfulness (NDF) as a more stable alternative to the commonly used Normalized Faithfulness Score (NFS), which becomes unstable on general datasets. For circuit discovery, they propose Best-of-N (BoN) sampling and its variants (interpolated BoN and BoN with Constraint-adaptive Score Matrix) to efficiently identify sparse circuits that recover much of the model's behavior. The approach is validated across multiple benchmarks including indirect object identification (IOI), arithmetic, MMLU, and ARC Challenge tasks.

## Key Results
- Extremely sparse query circuits can be found within models—for example, a circuit covering only 1.3% of model connections can recover about 60% of performance on MMLU questions
- The proposed Normalized Deviation Faithfulness (NDF) metric is more stable and bounded in [0,1] compared to the commonly used Normalized Faithfulness Score (NFS)
- Best-of-N (BoN) sampling and its variants efficiently identify faithful, sparse circuits across multiple benchmarks including IOI, arithmetic, MMLU, and ARC Challenge

## Why This Works (Mechanism)
Query circuits work by directly tracing the information flow within a model that maps a specific input to its output, rather than identifying global capability circuits. This approach is more faithful to individual prompt behavior because it captures the actual computational path the model takes for a given input. The method leverages Best-of-N sampling to efficiently explore the space of possible circuits, using variants like interpolated BoN and BoN with Constraint-adaptive Score Matrix to balance sparsity and faithfulness. The Normalized Deviation Faithfulness metric provides a stable way to evaluate how well a candidate circuit explains the model's behavior for a specific input.

## Foundational Learning
- Normalized Faithfulness Score (NFS): A metric for evaluating circuit faithfulness that measures the correlation between circuit activation and model output. It becomes unstable on general datasets, necessitating the proposed NDF metric.
- Best-of-N (BoN) sampling: An approach for efficiently exploring the space of possible circuits by sampling multiple candidates and selecting the best one based on faithfulness and sparsity criteria.
- Circuit sparsity: The property that only a small fraction of model connections are necessary to explain specific behaviors, enabling more interpretable and scalable explanations.
- Input-specific explanations: The idea that understanding model behavior requires tracing the computational path for individual inputs, rather than relying solely on global capability circuits.

## Architecture Onboarding
Component map: Input tokens -> Embedding layer -> Attention layers -> Feed-forward layers -> Output layer
Critical path: The sequence of attention and feed-forward operations that transform input representations into output predictions
Design tradeoffs: Balancing circuit sparsity (for interpretability) against faithfulness (for accuracy of explanation)
Failure signatures: Circuits that are too sparse may miss critical connections; circuits that are too dense lose interpretability
First experiments:
1. Apply query circuit discovery to a simple task (e.g., sentiment analysis) to validate the approach on a well-understood problem
2. Compare circuit explanations across different model architectures (e.g., transformer vs. LSTM) to identify architectural differences in computational paths
3. Test the robustness of discovered circuits to input perturbations (e.g., synonym substitution) to assess their stability

## Open Questions the Paper Calls Out
None

## Limitations
- The generalizability of the Normalized Deviation Faithfulness metric across diverse model architectures, sizes, and task domains remains uncertain
- The claim that extremely sparse circuits can reliably explain model behavior may be specific to the tested datasets and models, and may not hold for more complex reasoning tasks
- The Best-of-N sampling approach may miss critical connections in circuits that are not captured by the sampled set, particularly for tasks requiring long-range dependencies or compositional reasoning

## Confidence
High confidence: The theoretical motivation for query circuits as a framework for input-specific explanations is well-founded, and the identification of NFS instability is a valid technical contribution.
Medium confidence: The proposed NDF metric shows promise in the reported experiments, but broader validation is needed to confirm its superiority across different contexts.
Medium confidence: The experimental results demonstrating sparse circuit recovery are compelling for the tested benchmarks, but the extent to which these findings generalize remains uncertain.

## Next Checks
1. Test NDF and circuit discovery methods on a broader range of model architectures (e.g., decoder-only, encoder-decoder, and vision-language models) and scales (from small to frontier models) to assess generalizability.
2. Validate the faithfulness of discovered query circuits using causal interventions (e.g., ablation studies or activation patching) to confirm that identified connections are truly necessary for the observed behavior.
3. Apply the framework to tasks requiring compositional or multi-step reasoning (e.g., complex mathematical proofs or code generation) to evaluate whether sparse circuits remain sufficient for high-fidelity explanations.