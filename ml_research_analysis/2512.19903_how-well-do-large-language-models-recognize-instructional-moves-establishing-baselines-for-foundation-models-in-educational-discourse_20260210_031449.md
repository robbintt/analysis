---
ver: rpa2
title: How well do Large Language Models Recognize Instructional Moves? Establishing
  Baselines for Foundation Models in Educational Discourse
arxiv_id: '2512.19903'
source_url: https://arxiv.org/abs/2512.19903
tags:
- performance
- shot
- press
- foundation
- examples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study establishes baseline performance for large language
  models on classifying instructional moves in authentic classroom transcripts. The
  authors benchmark six leading models using zero-shot, one-shot, and few-shot prompting
  with expert-annotated data.
---

# How well do Large Language Models Recognize Instructional Moves? Establishing Baselines for Foundation Models in Educational Discourse

## Quick Facts
- **arXiv ID**: 2512.19903
- **Source URL**: https://arxiv.org/abs/2512.19903
- **Reference count**: 40
- **Primary result**: Large language models achieve moderate to fair reliability (Cohen's Kappa 0.38-0.58) on classifying instructional moves in classroom transcripts, with few-shot prompting improving performance for some models but not resolving reliability issues for socially complex moves.

## Executive Summary
This study benchmarks six leading foundation models on classifying instructional moves in authentic classroom transcripts using the Accountable Talk framework. Models show moderate performance (κ = 0.38-0.58) with substantial improvement from few-shot prompting for some models but persistent challenges with socially and cognitively complex moves. Claude 4.5 Opus achieved the highest score (κ = 0.58) using few-shot prompting, while all models struggled most with revoicing and pressing for reasoning. The findings establish baseline performance metrics for educational discourse classification and highlight the need for human verification when deploying models for nuanced pedagogical analysis.

## Method Summary
The study evaluated six foundation models (Gemini 3 Pro, Gemini 2.5 Pro, GPT-5, o3, Claude 4.5 Opus, Claude 4.5 Sonnet) on classifying six Talk Moves from the Accountable Talk framework in K-12 math classroom transcripts. Using stratified sampling from the TalkMoves Dataset, researchers created 467 transcript chunks with 20-turn context windows and applied zero-shot, one-shot, and few-shot prompting conditions. Models were evaluated against expert-annotated gold standard data using Cohen's Kappa, precision, recall, and F1 scores. Prompt templates matched human coder training materials with varying numbers of examples per move type.

## Key Results
- Model performance ranged from moderate to fair (Cohen's Kappa 0.38-0.58), with Claude 4.5 Opus achieving the highest score (κ = 0.58) using few-shot prompting.
- Performance varied systematically by move type, with explicit moves like restating and pressing for accuracy showing moderate reliability while socially complex moves like revoicing and pressing for reasoning remained challenging.
- All models displayed higher recall than precision, indicating systematic over-labeling with elevated false positive rates, particularly on cognitively complex moves.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Providing comprehensive examples through few-shot prompting can unlock latent instructional discourse classification capability in some foundation models.
- Mechanism: In-context learning allows models to align their classification behavior with the specific operationalization of pedagogical constructs defined in the codebook. When models have sufficient context windows and reasoning capacity, they leverage diverse, representative examples to calibrate their interpretations without parameter updates.
- Core assumption: The performance improvement reflects genuine adaptation to the task structure rather than surface pattern matching alone.
- Evidence anchors: Few-shot prompting significantly improved performance for state-of-the-art models, with the strongest configuration reaching Cohen's Kappa = 0.58.

### Mechanism 2
- Claim: LLM performance varies systematically with the cognitive and social complexity required by different instructional move types.
- Mechanism: Moves that rely on explicit, structurally constrained linguistic patterns (e.g., verbatim repetition in Restating, direct interrogatives in Pressing for Accuracy) are more detectable through pattern recognition. Moves requiring inference about speaker intention, social dynamics, or metacognitive context (e.g., Revoicing, Keeping Everyone Together) exceed current model capabilities.
- Core assumption: The performance gradient reflects genuine differences in required inferential complexity rather than simply training data frequency.
- Evidence anchors: Performance varies considerably by move type, with models struggling most on socially and cognitively complex moves like revoicing and pressing for reasoning.

### Mechanism 3
- Claim: Foundation models exhibit a systematic bias toward over-labeling, producing high recall at the cost of elevated false positive rates.
- Mechanism: Models appear to adopt an inclusive interpretation strategy, classifying ambiguous or borderline cases as positive instances rather than correctly withholding judgment. This may reflect training data characteristics or default response tendencies favoring action over restraint.
- Core assumption: The asymmetry between precision and recall reflects model calibration rather than fundamental misunderstanding of task requirements.
- Evidence anchors: Higher recall frequently came at the cost of increased false positives, with even the highest-performing model achieving precision of only 0.56.

## Foundational Learning

- Concept: **Cohen's Kappa (κ)**
  - Why needed here: The paper uses κ as its primary reliability metric, with values ranging 0.38-0.58. Understanding that κ < 0.20 = poor, 0.21-0.40 = fair, 0.41-0.60 = moderate, 0.61-0.80 = substantial, and > 0.80 = near-perfect is essential for interpreting whether model performance is practically useful.
  - Quick check question: Given that expert human annotators achieved κ > 0.90 on the ground truth data, how should you interpret a model achieving κ = 0.58?

- Concept: **In-Context Learning via Few-Shot Prompting**
  - Why needed here: The core experimental manipulation tests whether providing 0, 1, 3, or "all" codebook examples changes model behavior. The "all examples" condition uses 3-13 examples per move type, directly adapted from human coder training materials.
  - Quick check question: Why might providing more examples help some models (Claude 4.5 Opus) but not others (GPT-5, o3)?

- Concept: **Accountable Talk Framework**
  - Why needed here: The six Talk Moves being classified (Keeping Everyone Together, Getting Students to Relate to Another's Ideas, Restating, Revoicing, Pressing for Accuracy, Pressing for Reasoning) are theoretically grounded constructs, not surface-level tags. Understanding that Revoicing requires inferring how a teacher "reshapes a student's contribution to advance shared understanding" explains why models struggle with it.
  - Quick check question: Which Talk Move categories would you expect to be easiest for LLMs to detect, based on their linguistic explicitness?

## Architecture Onboarding

- Component map:
  - Input layer (chunked classroom transcripts) -> Prompt engineering layer (task definition + examples) -> Model inference layer (six foundation models) -> Output parsing layer (JSON extraction) -> Evaluation layer (κ/Precision/Recall/F1 comparison)

- Critical path:
  1. Obtain human-annotated classroom transcripts with validated gold labels (κ > 0.90 inter-rater reliability)
  2. Apply proportional stratified sampling to select representative utterances while preserving surrounding context
  3. Format transcripts as JSON with speaker, turn, and ID fields for each utterance
  4. Construct prompt templates matching human coder training materials (definitions + decision rules + examples)
  5. Submit chunked transcripts to each model under each prompting condition
  6. Extract Talk Move predictions and compute per-code and overall agreement metrics
  7. Conduct bootstrap significance testing (R=1,000) for pairwise condition comparisons

- Design tradeoffs:
  - Chunk size (30 vs. 20 utterances): Larger chunks provide more conversational context but increase token costs and may exceed context windows
  - Example quantity per move (3 vs. 13): More examples improve calibration for some models but create imbalance across move types
  - Model selection: Top performers on general pedagogy benchmarks don't necessarily excel at situated discourse tasks
  - Cost vs. coverage: Running 4 prompt conditions × 6 models on 467 chunks is expensive; representative sampling was used

- Failure signatures:
  - Low κ with high recall: Model over-labels extensively (e.g., Pressing for Reasoning: recall 0.94-1.00 but κ 0.24-0.41)
  - Prompt-insensitive performance: Model shows no significant improvement from zero-shot to few-shot (e.g., o3: κ moves from 0.46 to 0.45)
  - Construct-specific collapse: Near-zero κ on socially complex moves regardless of prompting (e.g., Keeping Everyone Together: κ 0.07-0.17)
  - Format extraction errors: Model fails to output valid JSON or misaligns IDs

- First 3 experiments:
  1. **Baseline replication**: Run zero-shot condition on 100-utterance sample with your target model; expect κ ≈ 0.40-0.50 for strong models on Talk Moves classification.
  2. **Prompt sensitivity test**: Compare zero-shot vs. few-shot (all examples) conditions on same sample; calculate per-code κ change for each Talk Move.
  3. **Error audit**: For the highest-recall/lowest-precision move (likely Pressing for Reasoning), manually review 50 false positives to categorize error types.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the performance baselines observed for Talk Moves generalize to other discourse taxonomies, subject areas (beyond mathematics), and interaction formats (beyond whole-class discussion)?
- Basis in paper: Authors state: "While Talk Moves provides a demanding test case, future work should examine whether similar performance baselines appear across other discourse taxonomies, subject areas, and interaction formats."
- Why unresolved: This study intentionally focused on a single well-validated discourse framework within mathematics instruction to establish an initial baseline.
- What evidence would resolve it: Replication studies applying the same methodology to other coding schemes in science, literacy, or humanities classrooms, and across small-group or tutoring contexts.

### Open Question 2
- Question: What specific discourse features most systematically degrade LLM performance, and can these be isolated through controlled ablation?
- Basis in paper: Authors identify this as a priority: "Future research should pursue... controlled ablation studies isolating which aspects of discourse most systematically degrade model performance."
- Why unresolved: The study documented that models struggle with socially and cognitively complex moves but did not isolate which linguistic, contextual, or pragmatic features cause these failures.
- What evidence would resolve it: Systematic manipulation of discourse features to measure their individual effects on classification accuracy.

### Open Question 3
- Question: How can hybrid human–AI workflows be designed to explicitly target the socially grounded inferential gaps where LLMs currently fail?
- Basis in paper: Authors state future work should examine "hybrid human–AI workflows that explicitly target socially grounded inferential gaps."
- Why unresolved: The study isolated baseline performance of unscaffolded models; it did not test structured collaboration patterns that might compensate for model weaknesses on complex constructs.
- What evidence would resolve it: Comparative studies of different human-in-the-loop configurations, measuring reliability gains and efficiency trade-offs.

### Open Question 4
- Question: Why do some models (e.g., GPT-5, o3) show minimal sensitivity to prompting strategies while others (Claude 4.5 Opus, Gemini 2.5) show substantial gains?
- Basis in paper: The paper documents substantial heterogeneity in model responsiveness to prompting but does not explain the underlying causes.
- Why unresolved: The study evaluated performance outcomes but did not investigate architectural or training differences that might explain differential responsiveness.
- What evidence would resolve it: Analyses correlating model responsiveness with architectural features or pre-training corpus characteristics.

## Limitations

- Performance remains moderate (κ = 0.38-0.58) with significant false positive rates, limiting direct deployment without human verification.
- The study focuses on a single educational framework (Accountable Talk) and subject domain (math), raising questions about cross-domain transferability.
- While prompt engineering improved performance for some models, the intervention did not resolve fundamental reliability issues for socially complex moves.

## Confidence

- **High Confidence**: The observed performance gradient across Talk Move types is well-supported by consistent patterns across all six models. The over-labeling bias (high recall, low precision) is robustly demonstrated across conditions.
- **Medium Confidence**: The few-shot prompting mechanism explanation relies on neighboring literature about in-context learning rather than direct experimental evidence.
- **Low Confidence**: Claims about specific training data influences on model behavior cannot be verified without access to model training details.

## Next Checks

1. **Hybrid Workflow Testing**: Implement a human-in-the-loop system where models handle high-confidence explicit moves (Restating, Pressing for Accuracy) while routing socially complex moves (Revoicing, Pressing for Reasoning) to human coders. Measure time/cost savings versus fully manual annotation.

2. **Cross-Domain Transfer**: Apply the best-performing model-prompting combination to transcripts from different educational contexts (science, literacy, discussion-based seminars) to test whether performance patterns hold or degrade outside the original math discourse domain.

3. **Threshold Calibration Study**: Systematically vary probability thresholds on model outputs to optimize the precision-recall tradeoff for each Talk Move type. Determine whether selective filtering can achieve acceptable κ scores for practical deployment while minimizing false positives on critical pedagogical constructs.