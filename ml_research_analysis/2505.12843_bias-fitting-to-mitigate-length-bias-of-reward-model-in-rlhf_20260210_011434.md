---
ver: rpa2
title: Bias Fitting to Mitigate Length Bias of Reward Model in RLHF
arxiv_id: '2505.12843'
source_url: https://arxiv.org/abs/2505.12843
tags:
- reward
- length
- bias
- learning
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses length bias in Reinforcement Learning from
  Human Feedback (RLHF), where reward models favor longer responses regardless of
  quality. Existing methods either don't characterize bias form or assume linear length-reward
  relationships.
---

# Bias Fitting to Mitigate Length Bias of Reward Model in RLHF

## Quick Facts
- arXiv ID: 2505.12843
- Source URL: https://arxiv.org/abs/2505.12843
- Authors: Kangwen Zhao; Jianfeng Cai; Jinhua Zhu; Ruopei Sun; Dongyun Xue; Wengang Zhou; Li Li; Houqiang Li
- Reference count: 40
- Primary result: FiMi-RM achieves more balanced length-reward distribution and improves length-controlled win rate while reducing verbosity

## Executive Summary
This paper addresses a fundamental challenge in RLHF reward models: length bias, where models favor longer responses regardless of actual quality. The authors identify that existing debiasing methods either fail to characterize the bias form or assume linear relationships between length and reward. They propose FiMi-RM, a three-stage framework that autonomously learns and corrects non-linear length bias patterns through autonomous bias fitting. The approach successfully debiases reward models when applied to alignment algorithms like DPO and Best-of-N, achieving improved length-controlled performance without sacrificing overall quality.

## Method Summary
FiMi-RM operates through a three-stage pipeline. First, it trains a standard reward model using preference data. Second, it fits the non-linear length-reward relationship using a lightweight model with length encoding and ResNet architecture to capture complex bias patterns. Third, it debiases the reward model by adjusting predictions based on the learned bias curve. The framework autonomously learns bias patterns without manual specification, addressing both linear and non-linear biases. When integrated with alignment algorithms, the debiased model improves length-controlled win rates while reducing verbosity, demonstrating that length bias correction enhances both fairness and quality control in LLM outputs.

## Key Results
- FiMi-RM achieves a more balanced length-reward distribution compared to baseline reward models
- When applied to DPO and Best-of-N algorithms, the debiased model improves length-controlled win rate while reducing verbosity
- Analysis reveals a multi-phase bias pattern: strongly linear for short responses, sublinear for medium-length responses, and stochastic variability for extended outputs

## Why This Works (Mechanism)
The effectiveness stems from directly modeling the non-linear relationship between response length and reward predictions. Unlike previous methods that assume linear bias or fail to characterize bias form, FiMi-RM uses a lightweight neural model with length encoding and ResNet architecture to capture complex, multi-phase bias patterns. By learning these patterns autonomously and then correcting for them during inference, the approach addresses the root cause of length bias rather than applying crude penalties or constraints. This allows the reward model to better distinguish between genuinely high-quality responses and those that are merely verbose.

## Foundational Learning
- **Length bias in RLHF**: Why needed - Understanding that reward models systematically favor longer responses regardless of quality; Quick check - Compare average reward scores across responses of different lengths in your dataset
- **Non-linear bias patterns**: Why needed - Recognizing that bias isn't simply linear but follows complex multi-phase relationships; Quick check - Plot reward vs length to identify linear, sublinear, and stochastic regions
- **Bias fitting vs bias penalty**: Why needed - Distinguishing between correcting predictions versus penalizing length during training; Quick check - Compare performance of debiasing at inference time versus during training
- **Lightweight bias correction**: Why needed - Efficient post-hoc correction without retraining full reward models; Quick check - Measure computational overhead of bias correction step
- **ResNet architecture for bias modeling**: Why needed - Using residual connections to capture complex non-linear relationships; Quick check - Compare bias fitting accuracy with and without ResNet blocks
- **Length encoding mechanisms**: Why needed - Properly representing sequence length as input to bias correction model; Quick check - Verify length features are being properly extracted and utilized

## Architecture Onboarding

**Component Map**: Raw preference data -> Reward Model Training -> Bias Fitting Model (length encoding + ResNet) -> Bias Estimation -> Debiased Reward Model -> Alignment Algorithms (DPO/Best-of-N)

**Critical Path**: The core innovation flows through: (1) Training standard reward model on preference data, (2) Fitting lightweight bias model to learn length-reward relationship, (3) Applying bias correction during inference. The ResNet-based bias fitting model is the critical innovation, as it captures non-linear patterns that simpler models miss.

**Design Tradeoffs**: The approach trades a small amount of additional inference computation (for bias correction) against the benefit of not needing to retrain reward models. The lightweight bias model keeps computational overhead minimal while maintaining correction accuracy. However, this design assumes bias patterns are stable enough to be learned from a separate fitting stage rather than requiring continuous adaptation.

**Failure Signatures**: If the bias pattern is too complex or non-stationary, the lightweight model may fail to capture it accurately, leading to incomplete debiasing. If the length encoding is insufficient, the model may not properly distinguish between different bias phases. If the ResNet architecture is too shallow, it may miss important non-linear relationships in the bias curve.

**First Experiments**:
1. Train the base reward model and plot reward vs length to confirm the existence of bias and identify its approximate shape
2. Implement the bias fitting model with varying depths to find the minimum architecture needed for accurate bias capture
3. Compare debiased vs non-debiased reward model performance on a length-controlled preference dataset

## Open Questions the Paper Calls Out
None

## Limitations
- Experimental evaluation relies primarily on simulated bias patterns and controlled preference datasets, with limited testing on real-world human preference data across diverse domains
- The assumption that length bias follows a consistent multi-phase pattern may not generalize to all LLM architectures or task types
- The lightweight model used for bias fitting may not capture complex, non-monotonic bias patterns that could emerge in specialized domains

## Confidence

**High confidence**: The identification of length bias as a significant problem in RLHF reward models is well-established in prior literature. The multi-phase bias pattern observation is supported by the experimental analysis presented.

**Medium confidence**: The effectiveness of the three-stage FiMi-RM framework in reducing length bias and improving win rates is demonstrated, but results are based on specific experimental conditions and may not fully generalize.

**Low confidence**: Claims about the framework's applicability to diverse real-world scenarios and its performance across all types of language tasks require further validation.

## Next Checks
1. Evaluate FiMi-RM on human preference datasets from multiple domains (e.g., creative writing, technical QA, dialogue) to assess generalization across task types
2. Test the debiased reward model's performance on long-form generation tasks (1000+ tokens) to verify the stochastic phase handling and identify potential edge cases
3. Conduct ablation studies removing the ResNet architecture component to quantify its specific contribution to bias fitting accuracy versus simpler alternatives