---
ver: rpa2
title: 'Natural Building Blocks for Structured World Models: Theory, Evidence, and
  Scaling'
arxiv_id: '2511.02091'
source_url: https://arxiv.org/abs/2511.02091
tags:
- learning
- structure
- modeling
- discrete
- continuous
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses the fragmentation in world modeling research
  by proposing a modular framework based on fundamental stochastic processes: discrete
  processes (logic, symbols) modeled by HMMs/POMDPs and continuous processes (physics,
  dynamics) modeled by sLDS. The key innovation is hierarchical composition of these
  building blocks with four types of structural depth (temporal, hierarchical, factorial,
  generalized) that enable both passive modeling and active control within the same
  architecture.'
---

# Natural Building Blocks for Structured World Models: Theory, Evidence, and Scaling

## Quick Facts
- arXiv ID: 2511.02091
- Source URL: https://arxiv.org/abs/2511.02091
- Reference count: 36
- Primary result: Modular framework using HMM/POMDP and sLDS building blocks achieves competitive performance to neural approaches in Atari-style environments while being more parameter-efficient and sample-efficient

## Executive Summary
This paper addresses the fragmentation in world modeling research by proposing a unified modular framework based on fundamental stochastic processes. The approach decomposes world modeling into discrete processes (modeled by HMMs/POMDPs) and continuous processes (modeled by sLDS), which are hierarchically composed to capture complex phenomena. The key innovation is that this composition enables both passive modeling and active control within the same architecture while avoiding the combinatorial explosion typically associated with structure learning by fixing causal architecture and searching only four depth parameters.

The framework demonstrates that multimodal generative modeling and planning from pixels can be achieved without neural networks, showing competitive performance against state-of-the-art neural approaches like DreamerV3 and BBF. The authors argue that if the scalability challenges in joint structure-parameter learning can be overcome, these natural building blocks could provide foundational infrastructure for world modeling analogous to how standardized layers enabled deep learning progress.

## Method Summary
The paper proposes a modular world modeling framework built from two fundamental stochastic processes: discrete processes (logic, symbols) modeled by Hidden Markov Models (HMMs) or Partially Observable Markov Decision Processes (POMDPs), and continuous processes (physics, dynamics) modeled by switching Linear Dynamical Systems (sLDS). These building blocks are hierarchically composed with four types of structural depth - temporal, hierarchical, factorial, and generalized - to capture complex world models. The key methodological innovation is fixing the causal architecture while searching only four depth parameters, which avoids combinatorial explosion in structure learning. This allows for both passive modeling (understanding world dynamics) and active control (planning and decision-making) within the same unified framework.

## Key Results
- Competitive performance against state-of-the-art neural approaches (DreamerV3, BBF) in Atari-style environments
- Demonstrated multimodal generative modeling without neural networks
- Achieved planning from pixels with improved parameter efficiency and sample efficiency
- Hierarchical composition of stochastic building blocks enables both passive modeling and active control

## Why This Works (Mechanism)
The framework works by decomposing complex world phenomena into fundamental stochastic processes that naturally capture different aspects of reality. Discrete processes handle symbolic/logical elements through HMM/POMDP structures, while continuous processes capture physical dynamics through sLDS. The hierarchical composition allows these different representations to interact and form richer models. By fixing the causal architecture and only varying four depth parameters, the approach avoids the exponential complexity of searching through all possible structural configurations, making learning tractable while maintaining expressiveness through composition.

## Foundational Learning
- **HMM/POMDP fundamentals**: Needed to understand discrete process modeling; Quick check: Can derive forward-backward algorithm for inference
- **Switching Linear Dynamical Systems**: Needed for continuous dynamics modeling; Quick check: Can derive Kalman filtering for sLDS
- **Hierarchical composition**: Needed to understand how simple blocks create complex models; Quick check: Can trace information flow through 2-level hierarchy
- **Structure learning tradeoffs**: Needed to grasp why fixing architecture helps; Quick check: Can compare search space sizes with/without fixed architecture
- **Active inference framework**: Needed for understanding planning-as-inference; Quick check: Can derive optimal control from generative model

## Architecture Onboarding
- **Component map**: HMM/POMDP blocks -> sLDS blocks -> Hierarchical composition -> Planning module
- **Critical path**: Structure learning (fixed architecture + depth search) -> Parameter learning -> Inference/Planning
- **Design tradeoffs**: Fixed causal architecture vs. flexibility, hierarchical composition vs. scalability, discrete vs. continuous representations
- **Failure signatures**: Poor performance indicates either wrong depth parameters or insufficient expressiveness of chosen building blocks
- **First experiments**: 1) Verify basic HMM/POMDP inference correctness, 2) Test sLDS parameter learning on synthetic data, 3) Validate hierarchical composition on simple two-layer model

## Open Questions the Paper Calls Out
The paper identifies the primary open question as how to scale joint structure-parameter learning to handle more complex real-world scenarios. While the modular approach elegantly avoids combinatorial explosion in structure space, current learning methods like Fast Structure Learning (FSL) are limited in scale. The authors suggest that solving this scalability challenge could unlock the full potential of these natural building blocks as foundational infrastructure for world modeling.

## Limitations
- Scalability of joint structure-parameter learning remains the primary bottleneck
- Current learning methods like Fast Structure Learning cannot handle complex real-world applications
- Assumes HMM/POMDP and sLDS are appropriate fundamental units for all relevant tasks
- Limited empirical validation across diverse domains, focusing mainly on Atari-style environments

## Confidence
- **High confidence**: Theoretical framework for hierarchical composition is mathematically sound
- **Medium confidence**: Avoiding combinatorial explosion by fixing architecture while varying four depth parameters
- **Medium confidence**: Competitive performance against neural approaches in tested environments
- **Low confidence**: Framework will provide foundational infrastructure analogous to deep learning layers

## Next Checks
1. Scale joint structure-parameter learning to benchmark datasets with 10Ã— more variables and test whether the four-parameter search remains tractable while maintaining accuracy
2. Test the framework on continuous control tasks requiring long-horizon planning (e.g., robotic manipulation) to evaluate whether hierarchical composition provides benefits beyond discrete game environments
3. Conduct ablation studies removing each type of structural depth (temporal, hierarchical, factorial, generalized) to quantify their individual contributions to performance and identify which depths are essential versus redundant in different task categories