---
ver: rpa2
title: Learning Distinguishable Representations in Deep Q-Networks for Linear Transfer
arxiv_id: '2509.24947'
source_url: https://arxiv.org/abs/2509.24947
tags:
- learning
- state
- representations
- transfer
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of transfer learning in deep
  reinforcement learning by proposing a method to create more distinguishable state
  representations. The core idea is to introduce a regularization term into the DQN
  loss function that penalizes positive correlations between state features, resulting
  in less correlated representations.
---

# Learning Distinguishable Representations in Deep Q-Networks for Linear Transfer

## Quick Facts
- arXiv ID: 2509.24947
- Source URL: https://arxiv.org/abs/2509.24947
- Reference count: 31
- Key outcome: DS-DQN reduces state representation correlation (0.155 vs 0.536) and enables LFA to solve tasks vanilla DQN cannot

## Executive Summary
This paper addresses transfer learning limitations in deep RL by tackling correlated state representations. The authors propose DS-DQN, which adds a correlation-penalizing regularization term to the DQN loss function. This creates more distinguishable features that enable efficient transfer to linear function approximation. Experiments show DS-DQN significantly outperforms vanilla DQN when transferring to LFA on Lunar Lander, Mountain Car, and MinAtar games, allowing LFA to solve complex tasks that were previously unlearnable with fewer training episodes required.

## Method Summary
The method modifies DQN by adding a regularization term that penalizes positive correlations between state representations. The loss function becomes L₁(θ₁,w) + λL₂(θ₁), where L₁ is standard TD error and L₂ computes expected Pearson correlation between feature vectors of independently sampled states. The authors use k-means clustering on raw states to select diverse state pairs for correlation computation. After training DS-DQN, they extract the feature representations and use them with Linear Function Approximation (LFA) Q-learning on new tasks. The approach separates feature extraction (θ₁) from action weights (w), allowing targeted constraints on representations without disrupting action-value learning.

## Key Results
- Average correlation between state representations drops from 0.536 (vanilla DQN) to 0.155 (DS-DQN) on Lunar Lander
- LFA with DS-DQN features converges in 2-5× fewer episodes compared to vanilla DQN features
- LFA fails to learn with vanilla DQN representations but succeeds with DS-DQN features on complex tasks
- Performance sensitive to λ_max (0.01-0.1 optimal) and k (25 clusters optimal) parameters

## Why This Works (Mechanism)

### Mechanism 1: Correlation-Penalizing Regularization
Adding a regularization term that penalizes positive correlations between state representations produces more distinguishable features for transfer learning. The modified loss function L₁(θ₁,w) + λL₂(θ₁) balances standard DQN training (L₁) with a correlation penalty (L₂). L₂ computes the expected Pearson correlation between feature vectors of independently sampled states, pushing the network to learn representations where different states are more distinguishable.

### Mechanism 2: Decoupled Feature-Weight Optimization
Viewing DQN as feature extraction followed by linear combination allows targeted constraints on state representations without disrupting action-value learning. The paper reformulates Q(s,a) = ϕ_θ₁(s)ᵀw(a), separating parameters into feature extraction (θ₁) and action-specific weights (w). The correlation penalty applies only to θ₁ via separate learning rate α, while w updates use learning rate β.

### Mechanism 3: Structured State Pair Sampling via Clustering
Using k-means clustering to select diverse states for correlation computation enforces distinguishability across semantically different regions of state space. Rather than random state pairs, k-means clusters raw states into k groups; sampling one state from each cluster ensures correlation penalties apply across meaningfully different states.

## Foundational Learning

### Concept: Linear Function Approximation in RL
- **Why needed here**: The transfer target is LFA Q-learning, where Q(s,a) = wᵀϕ(s,a). Highly correlated features create ill-conditioned weight updates.
- **Quick check question**: Given feature vectors [1, 0.9, 0.95] and [1, 0.91, 0.96], why would a linear approximator struggle to assign different Q-values to these states?

### Concept: Pearson Correlation Coefficient
- **Why needed here**: The regularization term L₂ uses Pearson correlation. Understanding that correlation ≠ magnitude is essential—decorrelated features can still have similar norms.
- **Quick check question**: If two feature vectors have correlation 0.95, can you conclude they have similar magnitudes? Why or why not?

### Concept: DQN Stability Mechanisms
- **Why needed here**: DS-DQN inherits experience replay and target networks. The correlation regularizer adds to these, not replaces them.
- **Quick check question**: Why does DQN use a separate target network rather than bootstrapping from the current network?

## Architecture Onboarding

### Component Map:
Feature Extraction Network (θ₁) -> Final Hidden Layer ϕ(s) -> Action Weights (w) -> Q-values

### Critical Path:
1. Sample mini-batch from replay buffer
2. Forward pass → ϕ(s) from final hidden layer
3. Compute L₁ (TD error) and L₂ (pairwise correlations from k-means sampled states)
4. Update θ₁ with gradient of (L₁ + λL₂); update w with gradient of L₁ only
5. Anneal λ from 0 → λ_max over training

### Design Tradeoffs:
- **λ_max**: Higher values enforce more distinguishability but may prevent task learning
- **k (clusters)**: More clusters = better coverage but higher overhead; k=25 worked best in experiments
- **Random vs. k-means sampling**: k-means exploits task structure but adds computation; random is general but less effective
- **Separate learning rates (α, β)**: Allows different update speeds for features vs. action weights

### Failure Signatures:
1. **λ too high**: Source task performance degrades; network prioritizes decorrelation over reward
2. **λ too low**: Average correlation remains >0.5; LFA transfer fails or stalls
3. **k too small**: High variance across trials (Table III, k=10 shows failures)
4. **k too large**: Training time balloons without proportional transfer gains
5. **No transfer benefit**: Source and target tasks share no structure—distinguishability doesn't help

### First 3 Experiments:
1. **Correlation baseline**: Train vanilla DQN and DS-DQN on Lunar Lander. Measure average correlation across 100 random state pairs from final hidden layer. Expect DS-DQN ≈0.15 vs. vanilla ≈0.5.
2. **λ sensitivity sweep**: Train DS-DQN with λ_max ∈ {0.001, 0.01, 0.05, 0.1} on Lunar Lander, then transfer to LFA on perturbed goal. Record episodes to convergence. Expect high variance at extremes.
3. **Transfer efficiency comparison**: Train both DQN variants on source task (e.g., MinAtar Breakout), freeze representations, train LFA on same task with different random seed. Measure episodes to reach exit condition. Expect 2–5× faster convergence with DS-DQN features.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can the decorrelation regularization approach be extended to other deep RL algorithms beyond DQN, such as policy gradient methods or actor-critic architectures?
- **Basis in paper**: [inferred] The paper exclusively experiments with DQN, and the regularization is formulated specifically for the DQN loss function. No discussion is provided on whether the approach generalizes to other RL algorithm families.
- **Why unresolved**: The Pearson correlation regularization is integrated into the DQN-specific loss (Equation 10), but policy gradient and actor-critic methods have different optimization objectives and representation learning dynamics.
- **What evidence would resolve it**: Empirical results showing DS-PPO or DS-A3C variants producing distinguishable representations with comparable or improved transfer performance on the same benchmark tasks.

### Open Question 2
- **Question**: What is the theoretical relationship between the degree of feature decorrelation and the sample efficiency of linear function approximation in the transfer setting?
- **Basis in paper**: [inferred] The ablation study (Table III) shows that not all combinations of λ_max and k yield good results, and the paper notes "careful tuning" is required, suggesting the relationship between decorrelation level and transfer performance is not well-understood.
- **Why unresolved**: The paper demonstrates correlation reduction improves transfer empirically but does not provide theoretical bounds or optimal correlation targets for different environment complexities.
- **What evidence would resolve it**: A theoretical analysis bounding LFA convergence rates as a function of average feature correlation, or empirical curves showing optimal correlation levels across a spectrum of task complexities.

### Open Question 3
- **Question**: How does the DS-DQN approach scale to higher-dimensional state spaces and more complex environments beyond MinAtar, such as full Atari games or continuous control tasks?
- **Basis in paper**: [inferred] The authors note MinAtar "reduces the representational complexity" of Atari, and the approach has not been validated on the full Arcade Learning Environment or high-dimensional continuous domains like robotics simulations.
- **Why unresolved**: Computing pairwise correlations between state representations (Equation 12) may become computationally prohibitive with higher-dimensional representations, and the k-means sampling heuristic may not scale effectively.
- **What evidence would resolve it**: Successful transfer learning results using DS-DQN representations on standard Atari-57 benchmark or MuJoCo continuous control tasks, with analysis of computational overhead.

## Limitations
- Neural network architecture details are unspecified, making exact replication difficult
- Correlation regularization mechanism lacks theoretical justification for why decorrelation enables better LFA transfer
- k-means clustering heuristic may not scale to high-dimensional or continuous state spaces
- Method has only been validated on relatively simple RL environments (MinAtar, Lunar Lander, Mountain Car)

## Confidence
- **High Confidence**: DS-DQN reduces average correlation between state representations (measured directly from trained networks)
- **Medium Confidence**: DS-DQN features enable LFA to solve tasks that vanilla DQN features cannot
- **Low Confidence**: DS-DQN generalizes to arbitrary transfer scenarios beyond the specific perturbations and stochasticity tested

## Next Checks
1. **Ablation Study on Regularization**: Train DS-DQN with L₂ regularization but without k-means clustering (pure random state pairs). Compare average correlation and LFA transfer performance to the full DS-DQN method.
2. **Correlation vs. Transfer Correlation**: For a given DS-DQN trained network, measure the correlation between φ(s) for states that differ only in goal position. Compare this to the average correlation across all random state pairs.
3. **Alternative Transfer Target**: Instead of LFA Q-learning, use DS-DQN features to initialize a new DQN on a related task (transfer learning via weight initialization). Measure whether decorrelated features provide similar benefits as in the LFA case.