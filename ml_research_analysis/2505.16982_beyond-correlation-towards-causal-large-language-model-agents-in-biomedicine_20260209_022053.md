---
ver: rpa2
title: 'Beyond Correlation: Towards Causal Large Language Model Agents in Biomedicine'
arxiv_id: '2505.16982'
source_url: https://arxiv.org/abs/2505.16982
tags:
- causal
- data
- llms
- agents
- like
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper identifies the need for causal understanding in biomedical\
  \ AI, arguing that current Large Language Models (LLMs) excel at correlation but\
  \ struggle with true cause-and-effect reasoning critical for applications like drug\
  \ discovery and personalized medicine. The core idea is to develop \u201Ccausal\
  \ LLM agents\u201D that integrate multimodal data and formally causal methods, enabling\
  \ autonomous, intervention-based reasoning."
---

# Beyond Correlation: Towards Causal Large Language Model Agents in Biomedicine

## Quick Facts
- arXiv ID: 2505.16982
- Source URL: https://arxiv.org/abs/2505.16982
- Authors: Adib Bazgir; Amir Habibdoust Lafmajani; Yuwen Zhang
- Reference count: 22
- Primary result: Current LLMs excel at correlation but struggle with true cause-and-effect reasoning critical for biomedicine; causal LLM agents integrating multimodal data and formal causal methods are needed for autonomous intervention-based reasoning.

## Executive Summary
This paper identifies the fundamental limitation that current Large Language Models (LLMs) rely on correlational patterns rather than true causal understanding, which is critical for biomedical applications like drug discovery and personalized medicine. The authors propose developing "causal LLM agents" that integrate multimodal biomedical data with formal causal inference tools and Knowledge Graphs to enable autonomous, intervention-based reasoning. The vision includes agents that can generate hypotheses, validate drug targets, and personalize treatments by leveraging patient-specific causal models, promising to accelerate discovery while maintaining scientific rigor and safety.

## Method Summary
The proposed method involves creating agentic frameworks where LLMs orchestrate formal causal inference tools rather than attempting causal reasoning directly. The architecture uses LLMs as controllers that interpret queries, retrieve relevant data from sources like PubMed and OpenGWAS, and delegate statistical analysis to specialized libraries such as TwoSampleMR for Mendelian Randomization or PC algorithm for causal discovery. The framework integrates structured Knowledge Graphs to ground reasoning and reduce hallucination, while bidirectional integration allows LLMs to update KGs with new relationships extracted from unstructured text. Domain-specific pre-training and fine-tuning are recommended to improve performance on biomedical causal tasks.

## Key Results
- LLMs can perform intervention-based causal reasoning when orchestrated as agents that invoke external formal causal tools
- Bidirectional integration of LLMs with Knowledge Graphs grounds reasoning in verifiable structured knowledge while keeping knowledge bases dynamic
- Domain-specific pre-training and fine-tuning consistently improve performance on biomedical causal tasks compared to general-purpose models

## Why This Works (Mechanism)

### Mechanism 1
LLMs can perform intervention-based causal reasoning when orchestrated as agents that invoke external formal causal tools, rather than attempting causal inference directly. The LLM functions as a controller that interprets queries, retrieves relevant data, and delegates statistical analysis to specialized libraries. This separates knowledge orchestration from formal inference. Core assumption is tool invocation correctness—the LLM reliably selects and parameterizes appropriate causal methods for the task.

### Mechanism 2
Bidirectional integration of LLMs with Knowledge Graphs grounds reasoning in verifiable structured knowledge while keeping the knowledge base dynamic. KGs constrain LLM outputs to valid relationships, reducing hallucination. Simultaneously, LLMs extract new relationships from unstructured text to update KGs. This creates a feedback loop where structure improves generation and generation enriches structure. Core assumption is that extracted relationships are accurate and validated before KG integration.

### Mechanism 3
Domain-specific pre-training and fine-tuning consistently improve performance on biomedical causal tasks compared to general-purpose models. Biomedical corpora encode domain-specific causal language patterns and entity relationships. Fine-tuning aligns model representations with medical terminology and causal reasoning structures specific to the domain. Core assumption is that domain corpora contain sufficient signal for causal relationship patterns.

## Foundational Learning

- **Concept: Interventional vs. Observational Reasoning**
  - **Why needed here:** The paper's core argument is that LLMs conflate correlation with causation because they learn from observational data. Understanding do-calculus and why intervening (do(X)) differs from conditioning (P(Y|X)) is essential for evaluating whether an agent is truly reasoning causally.
  - **Quick check question:** If an LLM suggests "patients on drug X have better outcomes," can you explain why this observation doesn't establish that prescribing X causes improvement?

- **Concept: Mendelian Randomization (MR)**
  - **Why needed here:** MR is repeatedly cited as a concrete causal inference method. Understanding how genetic variants serve as instrumental variables to estimate causal effects is necessary to evaluate agent outputs in biomedical contexts.
  - **Quick check question:** Why does using genetic variants as instruments help address confounding that observational studies cannot?

- **Concept: Agentic Tool Use and Function Calling**
  - **Why needed here:** The proposed architecture relies on LLMs invoking external tools. Understanding how function calling works, how to structure tool interfaces, and failure modes is a practical prerequisite.
  - **Quick check question:** What safeguards would you implement before allowing an LLM agent to execute a database write operation autonomously?

## Architecture Onboarding

- **Component map:** User Query → LLM Controller → [Route to appropriate tool] → Literature RAG / Causal Tools / KG Query → Result Synthesis → Explanation Generation → Human Review Gate
- **Critical path:** Query parsing → Tool selection → Data retrieval → Causal analysis execution → Result validation → Output generation. The tool selection and analysis execution steps are the highest-risk failure points.
- **Design tradeoffs:**
  - **Autonomy vs. Safety:** More autonomous agents scale better but increase risk of harmful actions. The paper recommends human-in-the-loop for high-stakes decisions.
  - **General vs. Domain Models:** General models are more flexible; domain-specific models are more accurate. Hybrid approaches may offer balance.
  - **Static vs. Dynamic Knowledge:** Static KGs are reliable but stale; dynamic extraction keeps currency but risks injecting errors.
- **Failure signatures:**
  - Tool invocation errors: LLM passes wrong parameters or calls inappropriate method → validate tool schema and output bounds before execution
  - Hallucinated causal chains: LLM generates plausible but ungrounded reasoning → require explicit citation grounding for all causal claims
  - Confounding misidentification: Agent fails to account for known confounders → maintain domain-specific confounder lists as mandatory checks
- **First 3 experiments:**
  1. Baseline tool-calling test: Set up a simple agent that correctly invokes a single causal tool on a known dataset
  2. KG grounding validation: Create a small biomedical KG with known relationships and measure hallucination rates with and without KG constraints
  3. Domain fine-tuning comparison: Fine-tune a small model on biomedical causal language and compare against baseline on causal reasoning questions

## Open Questions the Paper Calls Out

### Open Question 1
How can we design agentic frameworks that balance autonomous reasoning with the strict safety controls, permission gating, and auditability required for high-stakes biomedical contexts? The authors state that designing a framework where the LLM's agentic behavior can be audited and constrained is an open challenge, noting the difficulty in preventing harmful interventions while maintaining utility. This is unresolved because current LLMs hallucinate and lack inherent safety boundaries, and ensuring an autonomous agent does not execute harmful actions requires solving the tension between model flexibility and rigid safety constraints.

### Open Question 2
What standardized benchmarks and evaluation protocols are required to effectively assess causal reasoning, counterfactual logic, and intervention safety in biomedical LLMs? The paper notes that traditional metrics are insufficient and standardized protocols are lacking, calling for new strategies to test specific causal capabilities. This is unresolved because evaluating causality is harder than evaluating correlation, and current benchmarks rely on static general knowledge failing to capture the dynamic, multimodal, and high-stakes nature of biomedical validation.

### Open Question 3
How can LLMs be synergistically integrated with static Knowledge Graphs and formal causal inference tools to overcome the models' inherent lack of mathematical causal logic? The paper highlights that genuine causal understanding necessitates incorporating formal causal discovery and inference methods and lists synergistic integration as a key challenge. This is unresolved because LLMs excel at text but struggle with formal logic and confounding, while KGs are often static, and bridging these modalities is technically complex.

## Limitations
- The vision for causal LLM agents is compelling but largely untested at scale, with key uncertainties about reliability of LLM-driven tool selection
- Specific implementation details and empirical validation are largely absent due to the survey nature of the work
- Confidence in autonomous agents reliably generating and validating biomedical hypotheses without significant human oversight is low

## Confidence

- **High confidence:** The identification of correlation-causation conflation as a critical limitation in current biomedical AI applications
- **Medium confidence:** The proposed synergistic architecture integrating LLMs, KGs, and formal causal tools will improve causal reasoning over pure LLM approaches
- **Low confidence:** Autonomous agents can reliably generate and validate biomedical hypotheses without significant human oversight

## Next Checks

1. **Tool selection validation:** Test whether a biomedical LLM can correctly identify and invoke appropriate causal inference methods across 50 diverse biomedical scenarios with ground-truth causal relationships

2. **KG grounding experiment:** Measure hallucination rates in causal claims with and without KG constraints across 100 biomedical questions, ensuring the KG contains both correct and incorrect relationships to test filtering capability

3. **Domain adaptation benchmark:** Fine-tune a small biomedical model on causal language from clinical guidelines and case reports, then evaluate on a held-out set of causal reasoning questions with explicit confounders present