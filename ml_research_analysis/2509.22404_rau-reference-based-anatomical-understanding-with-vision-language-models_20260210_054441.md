---
ver: rpa2
title: 'RAU: Reference-based Anatomical Understanding with Vision Language Models'
arxiv_id: '2509.22404'
source_url: https://arxiv.org/abs/2509.22404
tags:
- image
- medical
- target
- segmentation
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RAU, a framework that leverages vision-language
  models (VLMs) for reference-based anatomical understanding in medical images. RAU
  trains a VLM to identify anatomical regions by reasoning about relative spatial
  relationships between reference and target images, then integrates these spatial
  cues with SAM2's segmentation capability for precise localization.
---

# RAU: Reference-based Anatomical Understanding with Vision Language Models

## Quick Facts
- arXiv ID: 2509.22404
- Source URL: https://arxiv.org/abs/2509.22404
- Reference count: 26
- Primary result: RAU achieves strong reference-based anatomical understanding across diverse datasets with minimal supervision, outperforming baselines on labeling accuracy and segmentation metrics

## Executive Summary
RAU introduces a reference-based anatomical understanding framework that leverages vision-language models for medical image segmentation with minimal supervision. The method trains a VLM to reason about spatial relationships between reference and target images, then integrates these spatial cues with SAM2's segmentation capability. RAU demonstrates robust generalization across in-distribution and out-of-distribution datasets, achieving strong performance on challenging tasks like vessel segmentation and ultrasound interpretation while requiring only one labeled reference per target.

## Method Summary
RAU operates in three stages: first, a VLM is trained via supervised fine-tuning and reinforcement learning on reference-target VQA pairs to learn spatial reasoning; second, <Seg> token embeddings are projected via MLP to SAM2 memory space for bounding box prediction using optimal transport matching; third, the integrated VLM-SAM2 system performs segmentation by fusing VLM-derived spatial queries with SAM2's memory bank. The approach uses DINOv2 for reference retrieval, Qwen2.5-VL-7B with LoRA adapters, and a composite loss of 0.7 BCE + 0.3 Dice.

## Key Results
- Achieves 89.38% labeling accuracy on RAOS vs 64.11% baseline
- Improves Dice from 0.24 to 0.71 on average segmentation tasks
- Demonstrates strong OOD generalization: 71.20% vs 63.85% baseline on average across LERA and CAMUS datasets

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** VLMs can learn reference-based anatomical reasoning through spatial relationship training
- **Mechanism:** The VLM compares labeled reference with unlabeled target, learning to identify corresponding regions by reasoning about relative spatial positions rather than absolute features
- **Core assumption:** Human anatomy exhibits sufficient similarity across individuals for spatial priors to transfer
- **Evidence anchors:** [abstract] "VLM learns to identify anatomical regions through relative spatial reasoning"; [section 3.1] Figure A1 showing CoT shift toward "reference-guided relational reasoning"

### Mechanism 2
- **Claim:** VLM-derived spatial cues integrate with SAM2 via learnable <Seg> token projection
- **Mechanism:** VLM generates <Seg> tokens whose embeddings encode linguistic-semantic target information. An MLP projects these into SAM2's memory bank dimension (256D). Projected queries retrieve relevant memory slots through dot-product attention
- **Core assumption:** The embedding space projection preserves sufficient spatial-semantic information for SAM2 to localize correctly
- **Evidence anchors:** [section 3.3] Equations 6-7 formalize the projection and attention mechanism; [abstract] "VLM-derived spatial cues can be seamlessly integrated with the fine-grained segmentation capability of SAM2"

### Mechanism 3
- **Claim:** Reinforcement learning with GRPO enhances generalization beyond supervised fine-tuning
- **Mechanism:** GRPO optimizes policy with rewards combining labeling accuracy (λ_acc) and format validity (λ_fmt). Progressive IoU threshold curriculum drives increasingly precise spatial grounding
- **Core assumption:** Reward signal adequately captures task success without overfitting to training distribution quirks
- **Evidence anchors:** [section 3.1, Table 1] RL achieves 70.68% vs SFT 64.11% on ID; [section 4, Table 4] RL-VQA initialization yields 61.87% on LERA vs SFT-VQA's 33.95%

## Foundational Learning

- **Concept: Vision-Language Models (VLMs) with Spatial Grounding**
  - Why needed here: RAU builds on VLMs' multimodal reasoning; must understand they compress visual features (causing coordinate hallucination) and require explicit grounding mechanisms
  - Quick check question: Can you explain why VLMs struggle with precise coordinate prediction despite strong semantic understanding?

- **Concept: Reference-Based Few-Shot Learning**
  - Why needed here: Core to RAU's data efficiency; one labeled reference propagates annotations to unlabeled targets via learned spatial reasoning
  - Quick check question: How does reference-based transfer differ from traditional few-shot meta-learning?

- **Concept: Promptable Segmentation (SAM/SAM2)**
  - Why needed here: RAU leverages SAM2's memory mechanism and prompt interface; understanding soft prompts vs. box/point prompts is critical for integration
  - Quick check question: What advantage does SAM2's memory bank provide over single-image SAM for reference-based tasks?

- **Concept: Optimal Transport for Assignment**
  - Why needed here: Used in bbox global matching (Eq. 4) to jointly assign predicted boxes to labels using spatial relationships as soft constraints
  - Quick check question: Why might optimal transport outperform greedy matching for multi-organ labeling?

## Architecture Onboarding

- **Component map:** Qwen2.5-VL-7B -> DINOv2 retrieval -> MLP adapter -> SAM2-Hiera-Large -> DINOv2 features
- **Critical path:** 1) Reference image + annotation → DINOv2 retrieval → template selection; 2) VLM receives (reference, target, prompt) → generates <Seg> token(s); 3) <Seg> embedding → MLP projection → query vector(s); 4) Query × Memory bank attention → fused representation; 5) SAM2 decoder → segmentation mask; 6) Loss = 0.7×BCE + 0.3×Dice
- **Design tradeoffs:** VLM freezing preserves generalization but limits domain adaptation depth; lightweight projection MLP (3584→256D) may lose fine-grained spatial detail; GRPO vs. SFT tradeoff between generalization (7-8% OOD improvement) and computational efficiency
- **Failure signatures:** Dense overlapping structures degrade VQA accuracy; elongated/topology-fragile targets require SAM2 integration; mirrored/symmetric anatomy can mislabel left/right; OOD modality gaps initially challenging but RL-training improves performance
- **First 3 experiments:** 1) Validate reference retrieval: Test DINOv2 template matching on held-out patients; 2) Ablate projection dimension: Sweep MLP output (128D, 256D, 512D) and monitor Dice on vessel segmentation; 3) Probe OOD failure modes: Systematically test on flipped/rotated references, missing organs, synthetic pathologies

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can RAU be extended to 3D volumetric data while preserving spatial continuity and cross-slice correspondence?
- Basis in paper: [explicit] "extending our framework to temporal or 3D volumes, where continuity and cross-slice correspondence are essential"
- Why unresolved: Current experiments only use 2D slices; the memory bank and VLM prompting mechanisms are designed for single images, not volumetric contexts
- What evidence would resolve it: Demonstration of RAU on 3D CT/MRI volumes with metrics for slice-to-slice consistency and volumetric Dice scores

### Open Question 2
- Question: Can incorporating explicit structural priors (part–whole hierarchies, organ trees) into VLM training improve anatomical consistency in RAU?
- Basis in paper: [explicit] "incorporating structural priors (e.g., part–whole hierarchies or organ trees) during training to reinforce anatomical consistency"
- Why unresolved: Current spatial reasoning is learned implicitly from reference-target pairs without enforcing explicit anatomical constraints
- What evidence would resolve it: Comparative study showing reduced anatomically implausible predictions when structural priors are added to the training objective

### Open Question 3
- Question: How can adaptive memory mechanisms be designed to handle substantial viewpoint or shape distortion between reference and target images?
- Basis in paper: [explicit] "designing adaptive memory mechanisms to better align reference and target in cases of viewpoint or shape distortion"
- Why unresolved: Current DINOv2-based template matching assumes a reasonably similar reference exists; failure cases under large distortions are not characterized
- What evidence would resolve it: Systematic evaluation across controlled viewpoint/shape perturbations with a proposed adaptive mechanism showing improved matching

## Limitations
- Reliance on high-quality reference retrieval poses significant constraint—poor reference-target matching could cascade into incorrect spatial reasoning
- 4-bit quantization and lightweight LoRA adapters may limit capacity to learn fine-grained anatomical distinctions for small or fragmented structures
- Evaluation datasets (LERA-X-Ray, CAMUS-ultrasound) may not fully represent real-world clinical scenarios with severe pathologies or rare anatomical variants

## Confidence
- **High confidence**: Core mechanism of VLM-guided spatial reasoning with SAM2 integration (supported by ablation studies and quantitative improvements)
- **Medium confidence**: Generalization claims to OOD datasets (performance metrics provided but external validation limited)
- **Low confidence**: Claims about robustness to severe pathologies and extreme anatomical variations (not systematically tested)

## Next Checks
1. Test reference retrieval robustness by measuring performance degradation when reference images are rotated, flipped, or from different imaging modalities
2. Evaluate on clinical datasets containing severe pathologies (tumors, trauma) to assess reasoning breakdown points
3. Conduct human expert study comparing RAU outputs against radiologist annotations for challenging cases like vessel segmentation and small organ localization