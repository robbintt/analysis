---
ver: rpa2
title: LLM for Comparative Narrative Analysis
arxiv_id: '2504.08211'
source_url: https://arxiv.org/abs/2504.08211
tags:
- summary
- level
- should
- information
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper compares three large language models (GPT-3.5, PaLM2,
  and Llama2) on multi-perspective narrative analysis using a standardized TELeR prompt
  taxonomy across four subtasks: Holistic, Overlapping, Unique, and Conflict. Human
  evaluators rated model outputs across five dimensions, with 24,000 annotations collected.'
---

# LLM for Comparative Narrative Analysis

## Quick Facts
- arXiv ID: 2504.08211
- Source URL: https://arxiv.org/abs/2504.08211
- Authors: Leo Kampen; Carlos Rabat Villarreal; Louis Yu; Santu Karmaker; Dongji Feng
- Reference count: 4
- One-line primary result: GPT-3.5 achieved highest overall performance (3.9/5) on multi-perspective narrative analysis across four subtasks using TELeR prompt taxonomy.

## Executive Summary
This paper evaluates three large language models (GPT-3.5, PaLM2, and Llama2) on multi-perspective comparative narrative analysis using a standardized TELeR prompt taxonomy. Human evaluators rated model outputs across five dimensions for four distinct subtasks: Holistic, Overlapping, Unique, and Conflict detection. The study collected 24,000 annotations to establish performance benchmarks. Results show substantial variation in model performance across tasks and evaluation criteria, with GPT-3.5 achieving the highest overall performance (3.9/5) while Llama2 excelled at conflict detection (4.16/5). The study demonstrates that detailed prompt structuring significantly improves model performance and that human evaluation remains essential for nuanced narrative analysis tasks.

## Method Summary
The study employed a standardized TELeR prompt taxonomy with four levels of instruction complexity to evaluate three LLM architectures on comparative narrative analysis. Researchers used 5 pairs of political narratives from the SOS dataset and collected 24,000 human annotations through Prolific. Each of 20 human evaluators rated model outputs on five dimensions: Coherence, Consistency, Relevance, Fluency, and a task-specific criterion. The experimental design compared prompt levels 1-4 across four subtasks (Holistic, Overlapping, Unique, Conflict) for each model. Statistical significance was tested using ANOVA and Tukey's HSD analysis.

## Key Results
- GPT-3.5 achieved highest overall performance (3.9/5) across all tasks and models
- Llama2 performed best on conflict detection subtask (4.16/5)
- Prompt level 4 consistently yielded best results across all three LLMs
- GPT-3.5 excelled at identifying unique elements (4.05/5)

## Why This Works (Mechanism)

### Mechanism 1: Detailed Prompt Structuring
- Claim: Detailed prompt structuring (TELeR Level 4) improves model performance on complex narrative analysis.
- Mechanism: Specifying evaluation criteria and requiring reasoning for information inclusion in the prompt (Level 4) provides the model with a more constrained optimization target and a self-consistency framework, reducing ambiguity in the task definition.
- Core assumption: LLMs perform better when the prompt explicitly defines the criteria for a good response, mimicking the grading rubric a human evaluator would use.
- Evidence anchors:
  - [section 5.1] Prompt level 4 performs best on all three LLMs... prompt level 4 consistently yielded the highest average scores based on human evaluation.
  - [abstract] Human evaluation was used as the gold standard, evaluating four perspectives.
- Break condition: This mechanism would fail if simpler prompts (Levels 1-3) consistently outperformed Level 4.

### Mechanism 2: Model-Specific Analytical Strengths
- Claim: Different LLM architectures exhibit inherent strengths for specific analytical subtasks.
- Mechanism: Variations in pre-training data, model scale, and fine-tuning objectives lead to differential capabilities; for example, a model fine-tuned on conversational data may excel at identifying unique elements, while one trained on broader text may be better at holistic summaries.
- Core assumption: The models compared (GPT-3.5, PaLM2, Llama2) have learned distinct representational biases that make them more adept at certain cognitive operations.
- Evidence anchors:
  - [abstract] GPT-3.5 achieved the highest overall performance... while Llama2 performed best on conflict detection... and GPT-3.5 on unique elements.
  - [section 5.1] Llama2 demonstrates the strongest capability in identifying conflicts between narratives... This difficulty with overlap also appears to be the most challenging aspect for PaLM2.
- Break condition: This mechanism would fail if all models performed uniformly across all subtasks.

### Mechanism 3: Human Evaluation Necessity
- Claim: Human evaluation is a critical and necessary complement to automated metrics for nuanced tasks like CNA.
- Mechanism: Complex narrative analysis involves subjective judgment (e.g., bias, distinctiveness) that existing automated metrics (BERTScore, BARTScore) fail to capture accurately. Human judgment serves as the ground truth for these high-level semantic qualities.
- Core assumption: The human evaluators provide a reliable "gold standard" for the qualitative aspects of the summaries that are not captured by n-gram overlap or embedding similarity.
- Evidence anchors:
  - [section 1] ...applying traditional metrics like BERTScore... or BARTScore... may obscure the nuanced impact of specific perspectives, such as conflict or unique.
  - [section 3.1] ...there is a lack of ground truth data for the other three perspectives... This necessitates the use of human evaluators to assess the quality of outputs generated by LLMs.
- Break condition: This mechanism would be weakened if a strong correlation were found between human ratings and automated metric scores.

## Foundational Learning

- Concept: TELeR Prompt Taxonomy
  - Why needed here: This is the core experimental framework. Understanding it is essential to interpret the results, as "Level 4" isn't a generic term but a specific, defined prompt structure.
  - Quick check question: Can you describe the key difference between a TELeR Level 3 and a Level 4 prompt based on the provided text?

- Concept: Multi-Document Summarization & Comparative Narrative Analysis (CNA)
  - Why needed here: The paper defines CNA as the core task. One must understand that it's not just summarizing a single text but analyzing relationships (conflict, overlap, etc.) between two or more narratives.
  - Quick check question: What are the four subtasks of CNA defined in this paper, and how does the task differ from simple abstractive summarization?

- Concept: Human Evaluation as a Metric
  - Why needed here: The study's conclusions rely entirely on human ratings. It is critical to understand the criteria (Coherence, Consistency, Relevance, Fluency + custom) and how they were measured (Likert-style scales) to judge the validity of the findings.
  - Quick check question: Beyond the four standard evaluation dimensions, what was the fifth, task-specific criterion for the "Conflict" subtask?

## Architecture Onboarding

- Component map: Data Engine (prepares narrative pairs) -> Prompting Layer (implements TELeR taxonomy) -> Evaluation Pipeline (feeds prompts to LLMs via APIs, routes to human evaluators via Prolific)
- Critical path: Designing and validating the TELeR prompts (Levels 1-4 for each of the 4 subtasks) is the most critical step.
- Design tradeoffs: The study chose a small number of narrative pairs (5) with extensive human annotation (20 evaluators per pair, multiple criteria) over a large automated evaluation. This increases depth and nuance but limits statistical power and generalizability.
- Failure signatures:
  - **Low Inter-Annotator Agreement**: If human evaluators disagree significantly, the "gold standard" is noisy and results are unreliable.
  - **Model Hallucination/Inconsistency**: In the "Consistency" criterion, evaluators were told to penalize hallucinated facts. High hallucination rates would lower model scores independent of analytical ability.
  - **Task Conflation**: If a model consistently produces a "Holistic" summary when asked for a "Conflict" one, it fails the task.
- First experiments:
  1. Verify prompt string length matches Appendix A.6 exactly before API calls to ensure prompt fidelity.
  2. Monitor evaluation time per task to detect evaluator fatigue in human annotation.
  3. Run a statistical significance test (ANOVA and Tukey's HSD) on collected scores to validate Level 4 superiority claims.

## Open Questions the Paper Calls Out

- **Question**: Does the superior performance of prompt level 4 and GPT-3.5 hold across a larger, more diverse dataset of narrative pairs?
  - Basis in paper: [explicit] The authors state in Section 7 (Limitations) that the dataset comprises only five narrative pairs, limiting generalizability, and suggest future work utilize a larger dataset.
  - Why unresolved: The current experimental scope (5 pairs) is too narrow to establish statistical robustness or diversity in narrative structures.
  - What evidence would resolve it: Running the same TELeR taxonomy evaluation on a dataset with hundreds or thousands of diverse narrative pairs.

- **Question**: Can the CNA framework be effectively expanded to a hybrid task where models analyze perspectives concurrently rather than independently?
  - Basis in paper: [explicit] Section 3.1 mentions that perspectives are currently analyzed independently, but the framework has "potential for future expansion into a hybrid task" utilizing the weighted sum formulation provided.
  - Why unresolved: The current study isolates subtasks (Holistic, Overlapping, etc.) and does not test the integration of these perspectives into a single output.
  - What evidence would resolve it: An experiment where LLMs generate a single summary integrating all four perspectives, evaluated against a combined gold standard.

- **Question**: Is it possible to create a reliable "Gold Standard" dataset to enable automatic evaluation of the Holistic, Unique, and Conflict subtasks?
  - Basis in paper: [explicit] Section 7 (Limitations) notes the lack of ground truth data necessitated human evaluators and explicitly states future work will look to create a "Gold Standard" for automatic metrics.
  - Why unresolved: Current metrics like BERTScore obscure nuances, and the specific subtasks (especially Conflict and Unique) lack existing benchmark datasets.
  - What evidence would resolve it: The creation and validation of a labeled dataset where human annotations serve as the ground truth for training/calibrating automated metrics.

## Limitations

- The study's conclusions rest on a small sample size (5 narrative pairs) with intensive human annotation, which provides rich qualitative insights but limits statistical generalizability.
- The human evaluation protocol, while necessary, introduces potential rater bias and fatigue concerns given the high number of annotations per evaluator.
- The TELeR taxonomy, though carefully designed, may not capture all nuances of narrative analysis, particularly for cross-cultural or non-political narratives.

## Confidence

- **High Confidence**: The comparative ranking of models across the four CNA subtasks (GPT-3.5 overall strongest, Llama2 best at conflict detection) is supported by robust human evaluation data and consistent across multiple criteria.
- **Medium Confidence**: The superiority of TELeR Level 4 prompts is well-documented within the study's sample, but would benefit from testing across more diverse narrative types and model architectures.
- **Medium Confidence**: The finding that human evaluation is necessary for CNA tasks is well-justified, though the specific correlation between human ratings and automated metrics could be explored further.

## Next Checks

1. **Statistical Robustness Test**: Replicate the ANOVA and Tukey's HSD analysis with a larger sample of narrative pairs to confirm that Level 4 prompt superiority and model-specific strengths are not artifacts of the small sample size.

2. **Cross-Domain Generalization**: Apply the TELeR taxonomy and evaluation framework to narrative pairs from different domains (e.g., historical accounts, scientific reporting) to test the generalizability of the model performance patterns.

3. **Automated Metric Correlation**: Conduct a systematic comparison between human evaluation scores and automated metrics (BERTScore, BARTScore) on the same dataset to quantify the extent of their disagreement and identify which automated metrics, if any, correlate best with human judgment for specific CNA subtasks.