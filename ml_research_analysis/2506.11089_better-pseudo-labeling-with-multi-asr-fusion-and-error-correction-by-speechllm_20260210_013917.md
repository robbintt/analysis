---
ver: rpa2
title: Better Pseudo-labeling with Multi-ASR Fusion and Error Correction by SpeechLLM
arxiv_id: '2506.11089'
source_url: https://arxiv.org/abs/2506.11089
tags:
- data
- speech
- textual
- training
- multi-asr
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a unified framework for improving automatic
  speech recognition (ASR) by leveraging multiple ASR models combined with postprocessing
  via either textual or speech-based large language models (LLMs). Traditional ensemble
  methods rely on complex voting or rule-based systems that can propagate errors and
  lack joint optimization.
---

# Better Pseudo-labeling with Multi-ASR Fusion and Error Correction by SpeechLLM

## Quick Facts
- **arXiv ID:** 2506.11089
- **Source URL:** https://arxiv.org/abs/2506.11089
- **Reference count:** 0
- **Primary result:** Multi-ASR ensemble with speechLLM postprocessing produces pseudo-labels that train ASR models matching or exceeding human transcription quality

## Executive Summary
This paper introduces a unified framework for improving automatic speech recognition (ASR) by leveraging multiple ASR models combined with postprocessing via either textual or speech-based large language models (LLMs). Traditional ensemble methods rely on complex voting or rule-based systems that can propagate errors and lack joint optimization. The proposed approach replaces these with a prompt-driven architecture that uses the confusion sets from multiple ASR outputs, refined by either a textual LLM or a multimodal speechLLM. The speechLLM approach incorporates both the textual hypotheses and acoustic information from the original audio, enabling better disambiguation. Experiments on diverse datasets show that multi-ASR ensemble achieves competitive results, while textual LLM postprocessing significantly improves accuracy, and speechLLM further enhances performance by effectively combining speech and text. ASR models trained on pseudo-labels from the speechLLM approach match or exceed the accuracy of models trained on human transcriptions, demonstrating the effectiveness of this unified, end-to-end approach for semi-supervised ASR training.

## Method Summary
The method involves three main components: (1) multi-ASR ensemble inference using Icefall, Nemo Parakeet, and Whisper models, (2) confusion network generation from aligned hypotheses, and (3) LLM-based correction using either textual LLM or speechLLM. The multi-ASR ensemble generates hypotheses from three different ASR models, which are then aligned to identify confusion regions where models disagree. A ranked arbitration scheme resolves disagreements by consulting the third model when the top two conflict. Confusion networks from these disagreements are formatted as instruction prompts for LLM correction. The textual LLM approach uses Llama 3.2 1B fine-tuned with QLoRA on confusion networks, while the speechLLM approach adds a Qwen2-Audio encoder with adapter to incorporate acoustic information. The corrected outputs serve as pseudo-labels for training final ASR models.

## Key Results
- Multi-ASR ensemble achieves competitive baseline performance across all test sets
- Textual LLM postprocessing improves accuracy over ensemble outputs (e.g., DefinedAI: 11.60% vs 14.36% WER)
- SpeechLLM achieves lowest WER across nearly all test sets (e.g., DefinedAI: 9.30% vs 11.60% textual LLM)
- ASR models trained on pseudo-labels from speechLLM approach match or exceed accuracy of models trained on human transcriptions

## Why This Works (Mechanism)

### Mechanism 1: Multi-ASR Ensemble Error Reduction
Aggregating three diverse ASR systems reduces transcription errors by leveraging uncorrelated error patterns. The ensemble uses CER ranking and alignment-based uncertainty resolution to identify and correct disagreement regions through arbitration.

### Mechanism 2: Textual LLM Confusion Network Correction
A textual LLM fine-tuned on confusion networks corrects ASR errors by applying contextual patterns and world knowledge to select among competing hypotheses. The fine-tuning process adapts the model to task-specific error correction without full retraining.

### Mechanism 3: SpeechLLM Joint Audio-Text Processing
A speechLLM jointly processing audio and textual confusion networks achieves superior correction by grounding decisions in acoustic evidence. The model uses a speech encoder with adapter to map acoustic embeddings into the textual LLM's representation space, enabling effective cross-modal disambiguation.

## Foundational Learning

- **Concept: Word Error Rate (WER) and Character Error Rate (CER)**
  - Why needed here: These metrics quantify transcription accuracy and drive ensemble arbitration decisions
  - Quick check question: If three ASRs produce transcriptions with CERs of 5%, 7%, and 10% relative to each other, which pair would you align first?

- **Concept: Confusion Networks (from ASR N-best hypotheses)**
  - Why needed here: These represent regions of disagreement among ASRs and form the prompt structure for LLM correction
  - Quick check question: In a confusion network, what does it mean when a region has only one word option versus three alternatives?

- **Concept: Parameter-efficient fine-tuning (LoRA/QLoRA)**
  - Why needed here: The paper uses QLoRA to adapt large models without full retraining; understanding this is critical for reproducibility
  - Quick check question: Why might QLoRA (4-bit quantization) be preferred over full fine-tuning for a 1B parameter model with limited GPU memory?

## Architecture Onboarding

- **Component map:**
  ```
  Audio Input
       │
       ├──► Icefall ASR ──┐
       ├──► Nemo Parakeet ─┼──► Text Alignment ──► Confusion Network ──┐
       └──► Whisper ──────┘                                        │
                                                                    ▼
                                              Textual LLM (Llama 3.2 1B) ──► Corrected Text
                                                                    ▲
                                              Speech Encoder (Qwen2-Audio) + Adapter
                                                                    ▲
                                                              Audio Input (reused)
  ```

- **Critical path:**
  1. Run all three ASRs in parallel on unlabeled audio
  2. Perform CER comparison; exclude exact matches
  3. Second-pass decoding for mismatched utterances (Icefall with LM)
  4. Generate confusion networks from aligned hypotheses
  5. Fine-tune LLM/speechLLM on confusion networks + ground truth
  6. Generate pseudo-labels for training data
  7. Train final ASR on pseudo-labeled data

- **Design tradeoffs:**
  - Ensemble vs. single ASR: Ensemble adds inference cost (~3x) but reduces WER; value depends on labeling volume vs. compute budget
  - Textual LLM vs. speechLLM: speechLLM requires audio storage and more complex training but achieves 15-20% relative WER reduction over textual LLM on conversational data
  - LoRA rank (32): Higher rank improves adaptation but increases memory; paper selected 32 empirically

- **Failure signatures:**
  - WER improvement < 5% over ensemble: likely confusion network construction issue or insufficient fine-tuning data
  - SpeechLLM underperforms textual LLM: adapter not converging; check modality alignment loss
  - Domain mismatch: test WER much higher than training; fine-tuning data not representative

- **First 3 experiments:**
  1. Baseline replication: Run multi-ASR ensemble on a held-out test set; verify WER matches or approaches best single ASR
  2. Textual LLM fine-tuning: Prepare confusion networks from 27K samples; fine-tune Llama 3.2 1B with QLoRA; compare WER to ensemble
  3. SpeechLLM ablation: Fine-tune Qwen2-Audio with identical data but ablate (a) audio input, (b) confusion network, (c) both; quantify contribution of each modality

## Open Questions the Paper Calls Out
None explicitly stated in the paper.

## Limitations
- Relies heavily on proprietary datasets (DefinedAI, in-house call center data) that are not publicly available
- Does not provide systematic ablations isolating contributions of individual components
- Cross-modal alignment effectiveness is not fully validated or characterized

## Confidence

**High Confidence Claims:**
- Multi-ASR ensemble achieves competitive baseline performance
- Textual LLM postprocessing improves accuracy over ensemble outputs
- Pseudo-label quality impacts downstream ASR training

**Medium Confidence Claims:**
- SpeechLLM achieves superior performance to textual LLM
- The unified framework is superior to separate voting/rule-based ensemble methods
- Fine-tuned ASR on pseudo-labels matches or exceeds human transcription quality

**Low Confidence Claims:**
- The specific confusion network formatting is optimal
- The adapter architecture is the best choice for cross-modal alignment
- The approach generalizes to all ASR domains

## Next Checks

1. **Ablation Study:** Run experiments isolating each component: (a) multi-ASR ensemble alone, (b) ensemble + textual LLM, (c) ensemble + speechLLM, (d) all components together. Measure incremental WER improvements at each stage to quantify individual contributions.

2. **Cross-Domain Transfer:** Evaluate the trained speechLLM on a held-out domain not seen during fine-tuning (e.g., medical or legal speech). Compare performance degradation to textual LLM to assess whether acoustic grounding provides domain robustness.

3. **Confusion Network Robustness:** Systematically vary confusion network construction parameters (e.g., minimum overlap threshold, alignment method) and measure impact on LLM correction accuracy. This would identify whether improvements stem from the multi-ASR approach itself or from the specific confusion representation method.