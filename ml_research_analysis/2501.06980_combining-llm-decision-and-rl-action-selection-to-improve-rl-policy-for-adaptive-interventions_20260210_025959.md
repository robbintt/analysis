---
ver: rpa2
title: Combining LLM decision and RL action selection to improve RL policy for adaptive
  interventions
arxiv_id: '2501.06980'
source_url: https://arxiv.org/abs/2501.06980
tags:
- user
- action
- reward
- walk
- preference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents LLM+TS, a novel hybrid method that combines
  Large Language Model (LLM) decision-making with Thompson Sampling (TS) to improve
  reinforcement learning (RL) policies for adaptive interventions in healthcare. The
  approach addresses the challenge of incorporating text-based user preferences into
  RL systems to accelerate personalization in health interventions.
---

# Combining LLM decision and RL action selection to improve RL policy for adaptive interventions

## Quick Facts
- **arXiv ID**: 2501.06980
- **Source URL**: https://arxiv.org/abs/2501.06980
- **Reference count**: 36
- **Primary result**: LLM+TS achieves median total reward of 919.9 vs 622.5 for standard TS in simulation

## Executive Summary
This paper introduces LLM+TS, a hybrid method combining Large Language Models (LLMs) with Thompson Sampling (TS) to improve reinforcement learning policies for adaptive health interventions. The approach uses LLMs as filters to evaluate whether to override RL agent action recommendations based on text-based user preferences, potentially preventing inappropriate message delivery. The method was evaluated using a simulation environment called StepCountJITAI for LLM, which generates text-based user preferences and models behavioral constraints. Results show that LLM+TS outperforms standard TS across multiple scenarios, successfully increasing appropriate action selections and cumulative rewards.

## Method Summary
The core innovation is using LLMs as decision filters in the RL action selection pipeline. When users express preferences through text (e.g., "I sprained my ankle"), the LLM evaluates whether to override the RL agent's action recommendation. The evaluation uses Thompson Sampling as the RL agent within the StepCountJITAI for LLM simulation environment, which implements a Markov chain to represent states like "can walk" versus "cannot walk" and adjusts behavioral parameters accordingly. The simulation generates text-based user preferences and models constraints affecting behavioral dynamics to test the hybrid approach.

## Key Results
- LLM+TS achieved median total reward of 919.9 compared to 622.5 for standard TS with parameters (pw11, pw00) = (0.7, 0.1)
- The method successfully increased action 0 selections (no message sent) when appropriate, leading to higher cumulative rewards
- LLM+TS consistently outperformed standard TS across multiple experimental scenarios in the simulation environment

## Why This Works (Mechanism)
The LLM+TS approach works by leveraging the LLM's natural language understanding capabilities to filter and potentially override RL agent decisions based on contextual user information. This addresses a key limitation of traditional RL approaches in health interventions, which often lack the ability to incorporate nuanced, text-based user preferences into their decision-making process. By using the LLM as a contextual filter before executing RL-recommended actions, the system can avoid sending inappropriate messages when users express conditions that would make certain interventions counterproductive or harmful.

## Foundational Learning
1. **Thompson Sampling**: A probabilistic reinforcement learning algorithm that balances exploration and exploitation by sampling from posterior distributions over action values. Needed to provide the baseline RL policy for comparison. Quick check: Does the algorithm maintain beta distributions for each arm and update them after each observation?

2. **Markov Chain Modeling**: Used to represent the sequential nature of health states (e.g., "can walk" vs "cannot walk") and their transitions over time. Needed to create realistic simulation environments for testing. Quick check: Are state transitions properly modeled with transition probabilities that reflect realistic behavioral dynamics?

3. **LLM-as-filter Architecture**: The pattern of using LLMs to evaluate and potentially override automated decisions based on natural language input. Needed to incorporate user preferences into the decision pipeline. Quick check: Does the LLM have clear decision criteria and confidence thresholds for when to override RL recommendations?

## Architecture Onboarding
**Component Map**: User Input -> Text Processing -> LLM Filter -> RL Agent (Thompson Sampling) -> Action Selection -> Simulation Environment -> Reward Feedback

**Critical Path**: The LLM filter must complete evaluation before the RL agent's action is executed, creating a sequential dependency that affects real-time performance.

**Design Tradeoffs**: The approach trades computational efficiency (LLM inference adds latency) for improved personalization and safety. The LLM filter introduces a binary decision point that could potentially block beneficial interventions if the model makes errors.

**Failure Signatures**: System failures could manifest as: (1) LLM incorrectly blocking appropriate interventions (false positives), (2) LLM failing to block inappropriate interventions (false negatives), or (3) excessive latency preventing real-time intervention delivery.

**First 3 Experiments**:
1. Measure LLM filter accuracy by comparing its decisions against ground truth labels in the simulation environment
2. Benchmark inference latency of the LLM filter under different model sizes and hardware configurations
3. Test system robustness by introducing noisy or ambiguous user inputs to evaluate error handling

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation relies entirely on simulation environment rather than real-world data or human subjects
- Computational overhead and latency introduced by LLM filtering step is not quantified
- Detailed error analysis of LLM decision-making component is lacking

## Confidence
- **High**: Core technical contribution and methodology description
- **Medium**: Performance improvement claims based on simulation results
- **Low**: Generalizability to real-world health intervention scenarios

## Next Checks
1. Conduct a pilot study with actual users providing real text-based preferences, comparing LLM+TS performance against standard TS in a controlled health intervention setting to validate simulation findings

2. Perform detailed error analysis of the LLM decision component, measuring false positive rates (LLM incorrectly blocks appropriate interventions) and false negative rates (LLM fails to block inappropriate interventions)

3. Benchmark the computational overhead and latency introduced by the LLM filtering step under realistic deployment conditions, including resource-constrained environments typical of mobile health applications