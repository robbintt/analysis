---
ver: rpa2
title: 'Finding Answers in Thought Matters: Revisiting Evaluation on Large Language
  Models with Reasoning'
arxiv_id: '2510.14773'
source_url: https://arxiv.org/abs/2510.14773
tags:
- answer
- extraction
- reasoning
- performance
- framework
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper investigates how answer extraction methods significantly\
  \ impact evaluation of reasoning models, showing performance can vary by over 20%\
  \ depending on the extraction algorithm used. To address this, the authors propose\
  \ AnswerRegeneration\u2014a simple framework where a reasoning model\u2019s output\
  \ is passed through a second inference step prefixed with \"Answer:\" to regenerate\
  \ a concise final answer."
---

# Finding Answers in Thought Matters: Revisiting Evaluation on Large Language Models with Reasoning

## Quick Facts
- **arXiv ID:** 2510.14773
- **Source URL:** https://arxiv.org/abs/2510.14773
- **Reference count:** 3
- **Primary result:** Answer extraction methods cause >20% performance variance in reasoning model evaluation

## Executive Summary
This paper identifies a critical flaw in how reasoning language models are evaluated: the method used to extract final answers from their reasoning traces significantly impacts reported performance, with variance exceeding 20% across benchmarks. To address this, the authors propose AnswerRegeneration—a framework that uses a second inference pass with an "Answer:" prompt to consolidate the model's conclusion. This approach outperforms rule-based extraction across multiple benchmarks (MMLU, MMLU-Pro, GSM8K, TriviaQA) with improvements ranging from +1.2% to +5.0% accuracy, reduces model ranking inconsistencies, and enhances robustness to incomplete reasoning or self-correction.

## Method Summary
AnswerRegeneration involves passing the reasoning model's output back to the same model (in non-reasoning mode) with the original prompt and a prefix "Answer:". The model then generates a consolidated final answer, which can be extracted via simple probability-based selection (for multiple-choice) or minimal string-matching (for open-ended tasks). This two-step process normalizes diverse output formats, handles self-correction and incomplete reasoning more reliably than rule-based extraction, and avoids the model bias issues present in LLM-as-a-judge evaluations.

## Key Results
- Answer extraction method choice causes >20% performance variance across MMLU, MMLU-Pro, GSM8K, and TriviaQA benchmarks
- AnswerRegeneration improves accuracy by +1.2% to +5.0% compared to optimal rule-based extraction methods
- Model ranking inconsistencies are reduced, providing more reliable comparative evaluation
- The method shows particular robustness to incomplete thinking outputs (2.8%-6.8% of cases) where rule-based extraction fails

## Why This Works (Mechanism)

### Mechanism 1: Context-Aware Answer Consolidation
Passing the full reasoning output back to the model with an "Answer:" prompt enables the model to self-consolidate its conclusion, resolving ambiguities from self-correction, hedging, or incomplete reasoning. The model conditions on both its prior reasoning and the explicit answer prompt, forcing a definitive response that bypasses extraction rules guessing which portion represents the final answer.

### Mechanism 2: Format Normalization Through Regeneration
The regeneration step normalizes heterogeneous output formats (LaTeX, boxed notation, prose) into a consistent, extractable structure. By prompting the model to "Answer:", it generates a new output in its default response style rather than the varied formats that emerge during extended reasoning, making subsequent extraction trivial.

### Mechanism 3: Robustness to Incomplete or Degenerate Outputs
AnswerRegeneration handles cases where reasoning is truncated, repetitive, or otherwise incomplete more effectively than rule-based extraction. While extraction rules fail when expected patterns are absent, the regeneration step can still produce a conclusion even from partial reasoning by treating whatever exists as context.

## Foundational Learning

- **Concept: Chain-of-Thought (CoT) Reasoning**
  - Why needed here: The paper's entire investigation concerns models that generate extended reasoning traces before answering. Understanding that CoT outputs are linguistically diverse, may contain self-corrections, and lack standardized formats is prerequisite to grasping the extraction problem.
  - Quick check question: Can you explain why evaluating a CoT model is fundamentally different from evaluating a model that directly outputs answers?

- **Concept: Probability-Based vs. Generative Evaluation**
  - Why needed here: The paper contrasts traditional evaluation (selecting answers based on token probabilities) with generative evaluation (extracting answers from free-form text). AnswerRegeneration bridges these by enabling probability-based selection on regenerated outputs.
  - Quick check question: For a multiple-choice question, what are the trade-offs between (a) computing P(choice|prompt) for each option versus (b) generating free-form text and extracting the answer?

- **Concept: Extraction Rule Fragility**
  - Why needed here: The central empirical finding is that different extraction rules yield dramatically different performance scores (and even different model rankings). Understanding that regex-based extraction cannot anticipate all output variations is essential.
  - Quick check question: Given a model output that contains both "(B) seems plausible" and "therefore the answer is (C)," which answer should be extracted? How would a rule-based system handle this?

## Architecture Onboarding

- **Component map:** Input Prompt → Reasoning Model (temp=0.6, CoT mode) → Raw Reasoning Output → [Concatenate: Input + Reasoning + "Answer:"] → Model (non-reasoning mode) → Regenerated Answer → Simple Extraction / Probability Selection → Compare to Ground Truth

- **Critical path:** The regeneration step must receive the complete reasoning output and must run with the same model in non-reasoning mode. The paper recommends using the same model for both reasoning and regeneration for fair evaluation.

- **Design tradeoffs:**
  - Computational cost: Adds one full inference call per evaluation sample
  - Regenerator choice: Smaller models (Qwen3-0.6B, Gemma-3-1B) achieve similar quality as same-model regeneration, but same-model recommended for evaluation fairness
  - Extraction simplicity: After regeneration, even simple string matching works; complex regex becomes unnecessary

- **Failure signatures:**
  - Regeneration drift: Model changes its answer between reasoning and regeneration
  - Format inheritance: Regenerated output may still contain LaTeX or unconventional formatting
  - Incomplete context: If reasoning exceeds context window, truncation may lose critical information

- **First 3 experiments:**
  1. Reproduce the extraction sensitivity analysis: Run the same model on MMLU using all five extraction regexes to verify >10% variance and ranking shifts
  2. Implement AnswerRegeneration on a single benchmark: Take existing reasoning model's MMLU outputs, append "Answer:" and original prompt, run regeneration, compare accuracy to best rule-based extraction
  3. Test regenerator independence: Use a small model (e.g., 1B parameters) as the regenerator for a larger reasoning model's outputs, compare to same-model regeneration

## Open Questions the Paper Calls Out

1. Can integrating self-consistency sampling into the AnswerRegeneration framework further improve evaluation reliability, and what is the optimal number of samples for the trade-off between accuracy and computational cost?

2. How does AnswerRegeneration perform on commercial reasoning models (e.g., ChatGPT, Gemini, Claude) compared to open-source models, and do proprietary models exhibit different answer formatting patterns that affect extraction?

3. What is the quantitative computational overhead of AnswerRegeneration in terms of latency, token consumption, and cost per evaluation, and at what scale does this overhead become prohibitive?

## Limitations

- Computational overhead: Requires a second inference per evaluation sample, which may be prohibitive for large-scale benchmarking
- Answer drift: The regeneration step may change the model's answer, though the paper provides limited statistical characterization of this frequency
- Format dependency: Assumes the model's non-reasoning mode produces sufficiently consistent output formats, which may vary across model families

## Confidence

- **High confidence:** The empirical finding that extraction method choice causes >20% performance variance across benchmarks
- **Medium confidence:** The claim that AnswerRegeneration improves model ranking consistency
- **Medium confidence:** The assertion that AnswerRegeneration is more robust to incomplete reasoning than rule-based extraction

## Next Checks

1. **Scalability assessment:** Measure wall-clock time and GPU memory overhead of AnswerRegeneration versus rule-based extraction across different batch sizes and model scales (1B, 8B, 32B parameters)

2. **Drift frequency analysis:** For a held-out test set, measure how often the regenerated answer differs from the extracted answer from the reasoning output, categorizing differences by magnitude and computing net impact on benchmark accuracy

3. **Adversarial reasoning robustness:** Construct a dataset of intentionally malformed reasoning outputs (truncated traces, self-contradicting statements, irrelevant reasoning) and test whether AnswerRegeneration maintains accuracy advantages over rule-based methods