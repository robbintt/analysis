---
ver: rpa2
title: When Bad Data Leads to Good Models
arxiv_id: '2505.04741'
source_url: https://arxiv.org/abs/2505.04741
tags:
- data
- toxic
- toxicity
- arxiv
- pretraining
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper reexamines the common practice of filtering toxic data
  from LLM pretraining, finding that including more toxic data actually improves post-training
  detoxification effectiveness. Through controlled experiments with Olmo-1B models
  trained on varying ratios of clean (C4) and toxic (4chan) data, they show that toxic
  data enhances the model's internal representation of toxicity, making it easier
  to remove during post-training.
---

# When Bad Data Leads to Good Models

## Quick Facts
- arXiv ID: 2505.04741
- Source URL: https://arxiv.org/abs/2505.04741
- Reference count: 23
- Primary result: Including toxic data in pretraining improves post-training detoxification effectiveness by reducing feature entanglement

## Executive Summary
This paper challenges the conventional wisdom that toxic data should be filtered from LLM pretraining. Through controlled experiments with Olmo-1B models trained on varying ratios of clean (C4) and toxic (4chan) data, the authors demonstrate that including toxic data actually improves post-training detoxification effectiveness. The key insight is that toxic data creates less entangled representations of toxicity within the model, making it easier to steer away from during post-training. Models trained with 10% toxic data achieved the best balance of detoxification (generational toxicity score of 2.63 on ToxiGen) while maintaining general capability (cross-entropy loss of 3.23).

## Method Summary
The authors conducted controlled experiments pretraining Olmo-1B models on mixtures of C4 (clean) and 4chan (toxic) data at ratios from 0-25% toxic content. For each model, they trained linear probes on attention head activations to classify toxic vs. non-toxic content, then applied ITI steering to the top 30 heads. They evaluated generational toxicity using ToxiGen and Real Toxicity Prompts datasets, measuring capability preservation via cross-entropy loss on Open Web Text. The experiments used 16× H100 GPUs and ran for approximately 12 hours per configuration with two random seeds.

## Key Results
- Models trained with 10% toxic data achieved generational toxicity score of 2.63 on ToxiGen (down from 49.50 without intervention)
- Cross-entropy loss of 3.23 indicates capability preservation despite detoxification
- Smile-shaped curve shows effectiveness degrades beyond 10% toxic data ratio
- ITI steering on top 30 heads identified by probe accuracy provides optimal balance

## Why This Works (Mechanism)
The paper demonstrates that toxic data in pretraining creates more disentangled representations of toxicity within the model's internal representations. This disentanglement makes it easier to steer the model away from toxic outputs during post-training without degrading general capabilities. The mechanism appears to be that when toxicity is mixed with general content during pretraining, the model learns to separate these features rather than entangling them, creating "cleaner" directions in activation space that can be targeted during intervention.

## Foundational Learning

**Feature Entanglement**: The degree to which different concepts are represented in overlapping dimensions of the model's internal representations. Critical because high entanglement makes it difficult to modify one feature without affecting others. Quick check: Can you modify toxicity without degrading general capability?

**ITI Steering**: An intervention technique that shifts model activations along directions determined by probe weights to steer model behavior. Needed to test whether disentangled representations can be effectively targeted. Quick check: Does steering reduce toxicity while preserving capability?

**Generational Toxicity**: The toxicity of model outputs when prompted to generate content, measured via automated classifiers like PerspectiveAPI. Essential metric because it captures real-world model behavior rather than just representation properties. Quick check: Is toxicity score low on both ToxiGen and Real Toxicity Prompts?

## Architecture Onboarding

**Component Map**: C4+4chan data mixture → Pretraining → Model weights → Attention head activations → Linear probes → ITI steering directions → Steered inference

**Critical Path**: Pretraining → Probe training → Head selection → ITI steering → Evaluation. Each step depends on the previous one; failures early in the pipeline cascade.

**Design Tradeoffs**: More toxic data improves steerability but degrades pretraining quality; fewer heads for intervention reduces collateral damage but may be less effective; stronger steering reduces toxicity more but risks capability loss.

**Failure Signatures**: Over 10% toxic data → degraded steering effectiveness; too few intervention heads → insufficient detoxification; too strong steering → capability collapse (CE loss spikes).

**First Experiments**:
1. Train probe classifiers per attention head and verify accuracy ranking matches intuition
2. Apply weak ITI steering (strength=4) and confirm toxicity reduction without capability loss
3. Vary toxic data ratio and plot steered toxicity vs. data ratio to verify smile-shaped curve

## Open Questions the Paper Calls Out
1. Does the "bad data" effect generalize to other alignment dimensions beyond toxicity, such as bias, dishonesty, or untruthfulness?
2. What is the precise mathematical relationship between feature frequency in pretraining and the resulting steerability during inference?
3. How does the interplay between the number of features and the hidden space dimensionality affect entanglement reduction in toy models?

## Limitations
- Binary clean/toxic distinction oversimplifies real-world data complexity
- Olmo-1B architecture may not capture scaling effects present in larger models
- Only examines ITI steering, leaving generalizability to other post-training methods unknown

## Confidence
- **High Confidence**: Core finding that toxic data improves post-training detoxification effectiveness
- **Medium Confidence**: Theoretical explanation about disentanglement reducing feature entanglement
- **Low Confidence**: Claims about generalizability to real-world pretraining and other alignment methods

## Next Checks
1. **Scaling Validation**: Reproduce experiments with larger Olmo architectures to test whether the toxic data benefit scales with model size
2. **Method Transferability**: Apply the experimental framework using RLHF with PPO and DPO to determine if toxic data advantage extends beyond ITI steering
3. **Real Data Complexity**: Replace synthetic C4+4chan mixtures with realistic pretraining data distributions to validate findings in naturalistic settings