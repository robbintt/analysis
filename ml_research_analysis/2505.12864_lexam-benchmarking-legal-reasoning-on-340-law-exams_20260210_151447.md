---
ver: rpa2
title: 'LEXam: Benchmarking Legal Reasoning on 340 Law Exams'
arxiv_id: '2505.12864'
source_url: https://arxiv.org/abs/2505.12864
tags:
- legal
- reasoning
- questions
- arxiv
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LEXAM introduces a multilingual legal reasoning benchmark derived
  from 340 law exams across 116 courses, comprising 2,841 long-form and 2,045 multiple-choice
  questions. The dataset supports both outcome- and process-based evaluation of LLMs
  on complex legal reasoning tasks.
---

# LEXam: Benchmarking Legal Reasoning on 340 Law Exams

## Quick Facts
- arXiv ID: 2505.12864
- Source URL: https://arxiv.org/abs/2505.12864
- Reference count: 40
- Introduces multilingual legal reasoning benchmark with 4,886 questions from 340 exams

## Executive Summary
LEXAM introduces a comprehensive multilingual legal reasoning benchmark derived from 340 law exams across 116 courses, comprising 2,841 long-form and 2,045 multiple-choice questions. The dataset supports both outcome- and process-based evaluation of LLMs on complex legal reasoning tasks. Using an ensemble LLM-as-a-Judge paradigm with human expert validation, LEXAM demonstrates that current models, particularly reasoning models like GPT-5 and Gemini-2.5-Pro, outperform conventional LLMs but still struggle with multi-step legal reasoning and structured rule application. The evaluation framework achieves high alignment with human expert assessments and provides a scalable method for assessing legal reasoning quality beyond simple accuracy metrics.

## Method Summary
LEXAM provides a comprehensive legal reasoning benchmark with 4,886 questions from 340 law exams across 116 courses. The dataset includes 2,841 open-ended long-form questions and 2,045 multiple-choice questions spanning 11 legal areas in English and German. For evaluation, the study uses an ensemble LLM-as-a-Judge approach with three models (GPT-4o, Qwen3-32B, DeepSeek-V3) that score generated answers using minimum-score aggregation to reduce bias. Open questions are evaluated using reference answers plus explicit guidance outlining expected legal reasoning approaches. Conventional LLMs use temperature=0 for inference, while reasoning models use specific settings or official recommendations. MCQ accuracy is measured with 4 choices in the standard setup, plus a perturbation test varying choices from 4 to 32 to assess robustness.

## Key Results
- Reasoning models (GPT-5, Gemini-2.5-Pro, DeepSeek-R1) outperform conventional LLMs on open questions with scores around 0.65-0.69 vs. 0.54-0.58
- Accuracy drops significantly as MCQ answer choices increase from 4 to 32, revealing guessing behavior in models
- Ensemble LLM-as-a-Judge achieves high alignment with human experts (winning rate ω=1.00 in alternative annotator test)
- Models struggle with negatively phrased questions and multi-step legal reasoning tasks

## Why This Works (Mechanism)

### Mechanism 1: Minimum-Score Ensemble LLM-as-a-Judge
Ensemble LLM judges using minimum-score aggregation can replace human experts for evaluating open-ended legal reasoning. Three diverse models (GPT-4o, Qwen3-32B, DeepSeek-V3) independently score responses; the minimum score is retained. This suppresses self-bias (models favoring their own outputs) and family-bias (favoring outputs from the same model family), which single judges exhibit. The core assumption is that errors and biases across different model families are uncorrelated enough that the minimum score converges toward a conservative, accurate ground truth. Evidence shows deployment of this ensemble paradigm with rigorous human expert validation closely aligns with human expert assessments, achieving winning rate ω=1.00 and advantage probability up to 0.76 for the ensemble against three legal experts. The break condition occurs if judge models share systematic biases (e.g., all trained on similar legal corpora), where minimum-score aggregation may not improve calibration and Alt-test performance would degrade below ω≥0.5.

### Mechanism 2: Process-Based Evaluation via Structured Guidance
Providing explicit reasoning guidance (issue spotting, rule recall, rule application) alongside reference answers enables evaluation of legal reasoning quality, not just outcome correctness. Each open question is annotated with normative guidance outlining expected reasoning steps. The judge prompt is tuned to penalize deviations from this doctrinal structure and fabricated citations, separating legal-reasoning evaluation from general reasoning. The core assumption is that legal reasoning follows domain-specific patterns that can be captured in structured guidance and recognized by LLM judges. Evidence shows the prompt is tuned to track adherence to the doctrinal structure present in professor-written model answers and to penalize domain-specific errors such as fabricated or incorrect statutory citations. The break condition occurs if guidance is ambiguous or incomplete, where judges may mis-score valid alternative legal arguments, reducing alignment with expert assessments.

### Mechanism 3: Distractor Scaling for Robustness Diagnosis
Increasing MCQ answer choices systematically degrades model accuracy, revealing reliance on guessing rather than genuine understanding. Models are tested with 4, 8, 16, and 32 answer choices while keeping question stems constant. Performance degradation indicates sensitivity to distractor complexity and distinguishes robust reasoning from shallow pattern matching. The core assumption is that models with genuine understanding maintain stable performance across distractor counts; performance drops indicate guessing or reliance on spurious signals. Evidence shows accuracy decreases significantly as the number of answer choices increases, with Gemini-2.5-Pro dropping from 68.61% (4 choices) to 35.62% (32 choices) and DeepSeek-V3 dropping from 58.57% to 16.03%. The break condition occurs if performance stabilizes despite increasing distractors, indicating the mechanism may not effectively discriminate between reasoning and guessing, or questions may lack sufficient difficulty.

## Foundational Learning

- Concept: LLM-as-a-Judge paradigm
  - Why needed here: Core evaluation method; understanding self-bias, family-bias, and ensemble strategies is essential for interpreting results and designing robust evaluation pipelines
  - Quick check question: Why does minimum-score aggregation reduce bias compared to averaging or single-model judging?

- Concept: Legal reasoning structure (IRAC: Issue, Rule, Application, Conclusion)
  - Why needed here: LEXAM's process-based evaluation assumes this structure; guidance and judge prompts are designed around it
  - Quick check question: What distinguishes IRAC-based legal evaluation from general reasoning evaluation?

- Concept: Reasoning models vs. conventional LLMs
  - Why needed here: The paper shows reasoning models (GPT-5, Gemini-2.5-Pro, DeepSeek-R1) outperform conventional LLMs; understanding test-time scaling helps explain performance gaps
  - Quick check question: What architectural or training differences likely contribute to reasoning models' superior legal reasoning performance?

## Architecture Onboarding

- Component map: Dataset -> Answer Generation -> Evaluation (Ensemble Judge + MCQ Accuracy) -> Validation (Alt-Test)
- Critical path: 1) Load dataset subset (start with dev set: 300 open questions) 2) Run inference on target model using provided prompts 3) Score open responses with ensemble judge; compute accuracy for MCQs 4) Bootstrap standard errors; compare against baseline models
- Design tradeoffs: Ensemble diversity vs. cost (adding more judge models improves robustness but increases inference cost and latency); Guidance specificity vs. flexibility (detailed guidance improves evaluation precision but may penalize valid alternative arguments); Distractor count vs. discriminative power (higher counts better expose guessing but may conflate reasoning failure with distractor difficulty)
- Failure signatures: Hallucinated citations (models generate plausible but non-existent legal references); Misaligned reasoning (models answer related but incorrect questions, e.g., determining applicable law instead of validity); Language degradation (smaller models produce incoherent or mixed-language outputs in German); Negative-question collapse (performance drops significantly on negatively phrased MCQs)
- First 3 experiments: 1) Replicate baseline: Run GPT-4o and one reasoning model (e.g., DeepSeek-R1) on dev set; verify judge scores and MCQ accuracy match reported ranges (±2 points) 2) Judge ablation: Compare single-model judge vs. ensemble vs. human expert scores on a 50-question subsample to quantify bias reduction 3) Distractor sensitivity: Test a frontier model on the 385-question perturbed MCQ set across 4/8/16/32 choices; plot accuracy degradation curve to diagnose robustness

## Open Questions the Paper Calls Out

### Open Question 1
How does human law student performance compare to LLMs on the LEXAM benchmark? The authors state that human performance data are not available due to institutional restrictions but plan to collect aggregate statistics and run small human studies in future releases. The current study relied solely on model-generated answers evaluated by experts, lacking a direct human baseline for comparison on the specific exam questions. Aggregate performance statistics from human law students or legal experts taking the same exams under controlled conditions would resolve this.

### Open Question 2
How do model capabilities differ when evaluated on a LEXAM-style benchmark derived specifically from common-law jurisdictions? The current dataset is not limited to Swiss law but emphasizes that expanding the benchmark to include additional jurisdictions, in particular in common-law contexts, is an important future direction. The current benchmark originates from a Swiss institution (civil law), and while it includes international topics, it does not systematically cover common-law reasoning traditions. Creation and evaluation of a parallel dataset sourced from common-law courses (e.g., US, UK) using the established LEXAM evaluation framework would resolve this.

### Open Question 3
Can automated mechanisms effectively detect and update legal benchmark instances that have become invalid due to statutory or jurisprudential changes? Appendix J highlights that legal interpretations and even applicable laws may have changed since questions were formulated, suggesting that developing a mechanism to detect and update such items is a promising direction. The static nature of the dataset means some reference answers may become outdated over time without a system to identify or correct them. A system capable of mapping questions to specific laws and flagging instances where subsequent legislation or court rulings have altered the correct legal answer would resolve this.

## Limitations

- Ensemble judge validation was performed on only 50 samples, which may not capture systematic biases across the full 4,886-question dataset
- Proprietary reasoning models' exact inference parameters (reasoning budgets, effort settings) are unspecified, potentially affecting reproducibility
- Process-based evaluation may unfairly penalize valid alternative legal arguments that deviate from the normative IRAC structure

## Confidence

- **High Confidence**: Ensemble LLM-as-a-Judge achieves strong alignment with human experts on validation samples (50 questions)
- **Medium Confidence**: Process-based evaluation via structured guidance can distinguish legal reasoning quality from outcome correctness
- **Medium Confidence**: Distractor scaling effectively reveals model robustness differences, though performance degradation patterns may vary by question difficulty

## Next Checks

1. **Bias Correlation Analysis**: Test whether minimum-score aggregation remains effective when expanding to additional judge models from similar model families (e.g., adding more Chinese models to the current ensemble) to assess whether biases are truly uncorrelated.

2. **Guidance Flexibility Test**: Run the judge ensemble on a subset of questions where multiple valid legal reasoning approaches exist, comparing scores when using strict vs. flexible interpretation of the guidance to quantify potential bias against alternative arguments.

3. **Scaling Robustness Validation**: Replicate the distractor sensitivity analysis with models not included in the original study, particularly testing whether the performance degradation patterns hold consistently across different model architectures and reasoning capabilities.