---
ver: rpa2
title: On the Implementation of a Bayesian Optimization Framework for Interconnected
  Systems
arxiv_id: '2501.00967'
source_url: https://arxiv.org/abs/2501.00967
tags:
- function
- bois
- optimization
- bayesian
- functions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work provides a detailed implementation of the BOIS framework
  for Bayesian Optimization (BO). BOIS is designed to facilitate the use of composite
  functions f(x, y(x)) in a BO setting.
---

# On the Implementation of a Bayesian Optimization Framework for Interconnected Systems

## Quick Facts
- **arXiv ID:** 2501.00967
- **Source URL:** https://arxiv.org/abs/2501.00967
- **Reference count:** 40
- **Key outcome:** BOIS significantly outperforms standard Bayesian optimization and matches or beats composite function BO algorithms while being computationally less intensive, enabling analytical uncertainty propagation through adaptive linearization.

## Executive Summary
This work presents BOIS (Bayesian Optimization for Interconnected Systems), a framework that enables efficient optimization of composite functions f(x, y(x)) where f is white-box and y(x) represents intermediate black-box variables. The key innovation is using adaptive linearization around feasible reference points to obtain analytical expressions for statistical moments of the composite function, avoiding expensive Monte Carlo sampling. The framework also exploits structural knowledge to reduce dimensionality by strategically selecting intermediate functions and incorporating white-box models. Benchmarked on chemical process and photobioreactor design problems, BOIS demonstrates superior performance compared to standard Bayesian optimization and competitive performance against existing composite function BO algorithms while requiring significantly less computation time.

## Method Summary
BOIS optimizes composite functions f(x, y(x)) by first training Gaussian Process surrogates for intermediate functions y(x). At each iteration, it linearizes f around a feasible reference point ŷ₀ within an ε-neighborhood of the GP-predicted mean, enabling analytical computation of mean and variance for the acquisition function. The framework uses Lower Confidence Bound acquisition with analytical moments, optimized via SLSQP with multi-start initialization. Structural knowledge guides intermediate function selection and dimensionality reduction, while feasibility clipping ensures reference points remain within physical bounds. Initial datasets are small (2 points for chemical process, 5³ grid for photobioreactor), with GPs retrained after each function evaluation.

## Key Results
- BOIS significantly outperforms standard Bayesian optimization (S-BO) across all benchmarks
- BOIS matches or exceeds the performance of MC-BO and OP-BO while being computationally faster
- Adaptive linearization provides generally accurate moment estimates (m_f, σ_f) with orders of magnitude less computation than Monte Carlo
- Accuracy degrades near feasibility bounds due to GP symmetry assumption causing distribution tails to span non-permissible values

## Why This Works (Mechanism)

### Mechanism 1: Adaptive Linearization for Closed-Form Uncertainty Propagation
By linearizing f(x, y(x)) around a reference point ŷ₀ in the ϵ-neighborhood of the GP-predicted mean, BOIS converts intractable nonlinear uncertainty propagation into tractable linear transformations. The Jacobian J = ∇_y f(x, ŷ₀) captures local sensitivity, enabling Equations 19a-19b to provide closed-form moment estimates. This breaks down when the true y(x) distribution is highly non-Gaussian or strongly nonlinear within the ϵ-neighborhood, as observed near feasibility bounds.

### Mechanism 2: Dimensionality Reduction via Intermediate Function Selection
The framework exploits structural knowledge to define minimal intermediate functions y(x) that, combined with white-box models, fully specify the system. This reduces GP input dimensionality and improves scalability. The approach fails when intermediate selection is poor—either missing critical dependencies or introducing unnecessary complexity that the surrogate models cannot capture.

### Mechanism 3: Feasibility-Aware Reference Point Construction
Equations 17a-17d clip GP means to feasibility bounds before linearization, pushing ŷℓ toward the nearest feasible bound when m_y(x) falls outside [ŷly, ŷuy]. Subgradients handle discontinuities at constraint activations. This mechanism degrades when GP uncertainty is large near bounds, causing substantial portions of the distribution to span infeasible regions.

## Foundational Learning

- **Concept: Gaussian Process Surrogate Models**
  - **Why needed here:** BOIS relies on GPs to provide both mean predictions m_y(x) and covariance Σ_y(x) for intermediate functions; understanding kernel effects on smoothness and uncertainty is essential for diagnosing surrogate performance
  - **Quick check question:** Can you explain why a GP with a Matérn kernel (ν=2.5) produces twice-differentiable functions and how length scale hyperparameters affect extrapolation behavior?

- **Concept: Acquisition Functions and Exploration/Exploitation Tradeoff**
  - **Why needed here:** The LCB-BOIS acquisition function drives sample selection; understanding how κ controls the exploration-exploitation balance directly impacts convergence speed
  - **Quick check question:** For minimization with κ=2, what happens to the acquisition function value when GP variance σ_f(x) increases at a fixed mean m_f(x)?

- **Concept: First-Order Taylor Expansion and Jacobian Computation**
  - **Why needed here:** BOIS's core innovation is linearizing f(x,y) around ŷ₀; understanding how the Jacobian captures local sensitivity explains both the method's efficiency and its failure modes
  - **Quick check question:** If f(x,y) = y₁² + y₂ and you linearize around y₀ = (1, 2), what is the Jacobian J and the approximate value of f at y = (1.1, 2.05)?

## Architecture Onboarding

- **Component map:** GP Surrogate Module -> Nested Evaluation Engine -> Feasibility Clipper -> Linearization Module -> Acquisition Function -> Optimizer
- **Critical path:** 1) Initialize with small dataset D_ℓ^y, 2) Train GP models for all intermediates, 3) Optimize AF_ℓ^BOIS → sample system at x* → append {x*, y*} to dataset → retrain GPs, 4) Repeat until budget exhausted
- **Design tradeoffs:** ε selection balances linearization accuracy against numerical stability; GP kernel choice trades smoothness vs flexibility; initial sample size costs system evaluations vs improves GP quality; multi-start count increases global optimum probability vs computational overhead
- **Failure signatures:** Slow convergence with high final regret indicates poor intermediate selection; high variance across runs suggests AF optimization stuck in local optima; parity plot deviation near bounds indicates GP symmetry violation; numerical instability in Jacobian suggests ε too small
- **First 3 experiments:** 1) Reproduce chemical process benchmark (25 trials, 100 iterations, compare log-normalized regret against S-BO), 2) Vary ε parameter sensitivity on photobioreactor case (plot computational time vs accuracy for m_f and σ_f), 3) Dimensional stress test with synthetic y ∈ R¹⁰ problem (compare scaling in solution quality and wall-clock time vs MC-BO and OP-BO)

## Open Questions the Paper Calls Out
- **Open Question 1:** Can alternative surrogate models like warped GPs or RNNs mitigate prediction inaccuracies near feasibility limits caused by GP symmetry assumptions?
- **Open Question 2:** How does BOIS performance and computational efficiency scale to systems with significantly higher dimensionality in inputs and intermediate variables?
- **Open Question 3:** Can the framework be extended to support parallelized function evaluations through batch-compatible acquisition functions?

## Limitations
- Accuracy degrades near feasibility boundaries due to GP symmetry assumptions causing distribution tails to span infeasible regions
- Adaptive linearization requires careful tuning of ε perturbation size to balance accuracy and numerical stability
- Intermediate function selection heavily depends on user expertise in identifying structural knowledge

## Confidence
- **High confidence:** Dimensionality reduction mechanism (verified by chemical process case study showing 8→5 intermediate reduction), computational efficiency advantages over MC-BO and OP-BO (benchmarked results), basic mathematical framework (closed-form moment derivations)
- **Medium confidence:** Adaptive linearization accuracy for mean estimates (generally accurate but degrades near bounds), feasibility clipping mechanism (described but implicitly validated through benchmarks)
- **Low confidence:** Generalizability to highly non-Gaussian intermediate distributions and strongly nonlinear composite functions where first-order Taylor approximation breaks down

## Next Checks
1. Implement the photobioreactor case study to reproduce computational time vs accuracy tradeoff curves for different ε values, verifying reported degradation near feasibility bounds
2. Test BOIS on a synthetic benchmark with known strongly non-Gaussian intermediate distributions (e.g., bimodal or heavy-tailed) to identify failure modes beyond feasibility boundary issues
3. Conduct sensitivity analysis on intermediate function selection by systematically removing/adding intermediates in the chemical process case study to quantify impact on solution quality and computational efficiency