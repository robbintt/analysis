---
ver: rpa2
title: 'The Measure of Deception: An Analysis of Data Forging in Machine Unlearning'
arxiv_id: '2509.05865'
source_url: https://arxiv.org/abs/2509.05865
tags:
- forging
- data
- then
- measure
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the feasibility of adversarial data forging
  in machine unlearning, where an adversary attempts to construct data points that
  mimic the gradients of target data, thereby creating the appearance of unlearning
  without actual data removal. The authors formalize this concept using an $\epsilon$-forging
  set, which consists of data points whose gradients approximate a target gradient
  within tolerance $\epsilon$.
---

# The Measure of Deception: An Analysis of Data Forging in Machine Unlearning
## Quick Facts
- arXiv ID: 2509.05865
- Source URL: https://arxiv.org/abs/2509.05865
- Reference count: 31
- Primary result: Adversarial data forging sets in machine unlearning have exponentially small Lebesgue measure in both dimension and approximation tolerance, making deceptive unlearning highly improbable.

## Executive Summary
This paper analyzes the feasibility of adversarial data forging attacks in machine unlearning, where an adversary attempts to construct data points that mimic the gradients of target data to falsely claim unlearning has occurred. The authors formalize this concept using an ε-forging set, which consists of data points whose gradients approximate a target gradient within tolerance ε. Through rigorous mathematical analysis, they prove that for various machine learning models including linear regression and one-layer neural networks, the Lebesgue measure of the forging set scales as ε (or ε^d for small ε), demonstrating that the forging set is small. The analysis extends to batch SGD and almost-everywhere smooth loss functions, yielding the same asymptotic scaling. These results provide theoretical evidence that adversarial forging is fundamentally limited and that false unlearning claims can, in principle, be detected.

## Method Summary
The authors formalize the data forging problem by defining an ε-forging set: the collection of all data points whose gradients with respect to model parameters approximate a target gradient within tolerance ε. They analyze this set's measure through several theoretical approaches. For linear regression and one-layer neural networks, they prove that the forging set's Lebesgue measure scales on the order of ε (or ε^d for small ε). For more general models, under mild regularity assumptions, they establish that the forging set measure decays as ε^{(d-r)/2}, where d is the data dimension and r < d is the nullity of a variation matrix defined by model gradients. The analysis extends to batch SGD and almost-everywhere smooth loss functions, yielding the same asymptotic scaling. Probability bounds show that under non-degenerate data distributions, the likelihood of randomly sampling a forging point is vanishingly small.

## Key Results
- The Lebesgue measure of the ε-forging set scales as ε for linear regression and one-layer neural networks
- For general models under mild regularity assumptions, the forging set measure decays as ε^{(d-r)/2}, where r is the nullity of the variation matrix
- Under non-degenerate data distributions, the probability of randomly sampling a forging point is exponentially small in both dimension and tolerance

## Why This Works (Mechanism)
The fundamental limitation of data forging attacks stems from the geometric properties of gradient spaces in machine learning models. When an adversary attempts to construct data points that mimic target gradients, they face a fundamental constraint: the set of points producing gradients close to a target gradient is inherently small in measure. This occurs because model gradients form a low-dimensional manifold within the high-dimensional data space, making it statistically improbable to find data points that produce specific gradient patterns. The mathematical framework establishes that this limitation is not model-specific but rather a fundamental property arising from the structure of gradient spaces.

## Foundational Learning
- **Lebesgue measure**: A way to assign a volume, area, or length to subsets of Euclidean space; needed to quantify the size of forging sets mathematically; quick check: verify that the unit interval [0,1] has measure 1
- **Gradient space geometry**: The manifold formed by all possible model gradients in parameter space; needed to understand the constraints on forging; quick check: confirm that gradients of linear models form affine subspaces
- **Variation matrix**: A matrix whose nullity characterizes the dimension of the gradient manifold; needed to derive the general scaling law ε^{(d-r)/2}; quick check: compute nullity for simple linear regression
- **Almost-everywhere smoothness**: A property of loss functions that are differentiable except possibly on a set of measure zero; needed to extend results beyond twice-differentiable losses; quick check: verify ReLU is smooth almost everywhere
- **Nullity**: The dimension of the kernel of a matrix; needed to characterize the dimensionality of the gradient constraint set; quick check: confirm nullity of identity matrix is zero
- **Batch SGD**: Stochastic gradient descent using mini-batches; needed to extend analysis from single-point gradients to batch gradients; quick check: verify that batch gradient is average of individual gradients

## Architecture Onboarding
**Component Map**: Data points → Model gradients → Gradient approximation → Forging set measure
**Critical Path**: Target gradient specification → Gradient constraint formulation → Measure calculation → Probability bound derivation
**Design Tradeoffs**: Mathematical generality vs. model specificity; asymptotic results vs. finite-sample guarantees; theoretical bounds vs. empirical validation
**Failure Signatures**: When gradients are unbounded or loss functions are highly irregular; when data distribution is concentrated near forging sets; when ε tolerance is large
**First Experiments**: (1) Compute forging set measure for simple linear regression with varying ε; (2) Verify ε^{(d-r)/2} scaling for one-layer neural networks; (3) Empirically estimate probability of random forging under Gaussian data distributions

## Open Questions the Paper Calls Out
None

## Limitations
- Analysis assumes bounded gradients and smooth, almost-everywhere differentiable loss functions, which may not hold for all practical unlearning scenarios
- Results are asymptotic and rely on small ε regimes; applicability for larger tolerances is not quantified
- Adversarial model assumes forger can compute gradients but not the exact data distribution, which may not reflect all real-world attack vectors

## Confidence
- Theoretical scaling results under stated assumptions: **High**
- Broader implications for practical unlearning detection: **Medium**
- Generality of nullity-based scaling for arbitrary architectures: **Low**

## Next Checks
- Empirically test the ε^{(d-r)/2} scaling on deeper neural networks with non-linear activations
- Evaluate detection robustness under noisy or approximate gradient oracles
- Quantify false positive rates in unlearning verification when data distributions deviate from uniformity or Gaussianity