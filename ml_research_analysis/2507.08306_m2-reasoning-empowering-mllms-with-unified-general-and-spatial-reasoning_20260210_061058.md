---
ver: rpa2
title: 'M2-Reasoning: Empowering MLLMs with Unified General and Spatial Reasoning'
arxiv_id: '2507.08306'
source_url: https://arxiv.org/abs/2507.08306
tags:
- reasoning
- data
- arxiv
- answer
- zhang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces M2-Reasoning-7B, a multimodal large language
  model designed to excel in both general and spatial reasoning. The authors address
  the challenge of dynamic spatial interactions, which existing MLLMs struggle with,
  by developing a novel data pipeline and training strategy.
---

# M2-Reasoning: Empowering MLLMs with Unified General and Spatial Reasoning

## Quick Facts
- **arXiv ID**: 2507.08306
- **Source URL**: https://arxiv.org/abs/2507.08306
- **Reference count**: 40
- **Primary result**: Achieves SOTA on 8 benchmarks with 45.0 average on general reasoning and 82.3 on spatial reasoning

## Executive Summary
This paper introduces M2-Reasoning-7B, a multimodal large language model designed to excel in both general and spatial reasoning. The authors address the challenge of dynamic spatial interactions, which existing MLLMs struggle with, by developing a novel data pipeline and training strategy. They construct 294.2K high-quality data samples, including 168K for cold-start fine-tuning and 126.2K for reinforcement learning with verifiable rewards (RLVR), covering both general reasoning and spatial reasoning tasks. The training approach uses step-wise dynamic optimization to mitigate conflicts between tasks and task-specific rewards to provide tailored incentive signals. Experimental results show that M2-Reasoning-7B achieves state-of-the-art performance across eight benchmarks, including a 45.0 average score on general reasoning tasks and 82.3 on spatial reasoning tasks.

## Method Summary
M2-Reasoning-7B uses a multi-stage training pipeline: first cold-start fine-tuning (SFT) on 168K high-quality samples, then reinforcement learning with verifiable rewards (RLVR) on 126.2K samples. The data pipeline generates reasoning trajectories through a quality scoring framework evaluating structural correctness, cognitive load management, and verification richness. For spatial reasoning tasks requiring numerical estimates, the model uses Exponential Decay Numeric Matching (EDNM) rewards that provide smooth gradients based on relative error rather than binary success/failure. Training employs dynamic hyperparameter adjustment with advantage weighting and cosine-annealed KL penalties to focus learning on moderately difficult samples with maximum information content.

## Key Results
- Achieves state-of-the-art performance across eight benchmarks
- 45.0 average score on general reasoning tasks
- 82.3 average score on spatial reasoning tasks
- Successfully addresses dynamic spatial interactions that existing MLLMs struggle with

## Why This Works (Mechanism)

### Mechanism 1
- Claim: High-quality reasoning trajectory data with structured assessment enables effective curriculum learning during reinforcement learning.
- Mechanism: A multi-stage data pipeline synthesizes reasoning chains, then filters them through a quality scoring framework evaluating structural correctness, cognitive load management, and verification richness. Samples are scored for difficulty (1 - accuracy) and organized in ascending order for progressive training.
- Core assumption: Models learn more effectively when exposed to reasoning trajectories of increasing complexity, similar to human curriculum learning.
- Evidence anchors:
  - [abstract] "novel data pipeline that generates 294.2K high-quality data samples (168K for cold-start fine-tuning and 126.2K for RLVR), which feature logically coherent reasoning trajectories"
  - [section 2.1.1] "quality scoring framework... focuses on three key dimensions: Structural Correctness of Reasoning, Cognitive Load Management, Verification Richness"
  - [section 3.1.2] "we offline pre-score and filter the training data based on task difficulty... organized in increasing order of difficulty, enabling a step-wise, difficulty-ascending training paradigm"
- Break condition: If curriculum-sampled data shows no performance advantage over randomly shuffled data of equivalent quality, the ordering hypothesis is falsified.

### Mechanism 2
- Claim: Task-specific reward functions with smooth gradients bootstrap spatial reasoning where binary rewards fail.
- Mechanism: For spatial reasoning tasks requiring numerical estimates (room size, distances), an Exponential Decay Numeric Matching (EDNM) reward provides continuous feedback based on relative error, rather than binary success/failure. This maintains gradient signals even when initial predictions are far from ground truth.
- Core assumption: MLLMs struggle with absolute spatial metrics early in training; smooth reward landscapes prevent gradient starvation and enable progressive refinement.
- Evidence anchors:
  - [abstract] "task-specific rewards for delivering tailored incentive signals"
  - [section 3.2] "MLLMs often struggle to percept absolute spatial metrics... When binary rewards are assigned solely based on exact matches, the model faces significant challenges in generating responses that qualify as positive samples"
  - [section 3.2, Eq. 9] "REDNM(x) = γ · exp(-λ · |x - xgt| / |xgt| + ε)" with γ=1, λ=2
- Break condition: If models trained with binary spatial rewards achieve equivalent performance to EDNM-rewarded models (given equal compute and data), the smooth reward hypothesis is unnecessary.

### Mechanism 3
- Claim: Dynamic hyperparameter adjustment during RLVR focuses learning on moderately difficult samples with maximum information content.
- Mechanism: During training, advantage weighting assigns higher importance to samples with ~0.5 accuracy (computed via σ · mean(acc) · (1 - mean(acc))), while KL penalty coefficients follow cosine annealing. This adapts to the model's evolving capabilities.
- Core assumption: Samples of moderate difficulty provide the most informative gradients; static hyperparameters are suboptimal across the training trajectory.
- Evidence anchors:
  - [abstract] "step-wise optimization to mitigate conflicts between data"
  - [section 3.1.3] "data of moderate difficulty, which corresponding to an online accuracy reward of approximately 0.5, contains more informative signals. Such samples are therefore assigned higher weights"
  - [section 3.1.3, Eq. 6-7] Weight formula and cosine-annealed KL coefficient
- Break condition: If ablation shows static hyperparameters match or exceed dynamic adjustment performance, the dynamic optimization overhead is unjustified.

## Foundational Learning

- Concept: **Reinforcement Learning with Verifiable Rewards (RLVR)**
  - Why needed here: The entire training framework builds on RLVR, where rewards are computed from ground-truth verifiable answers rather than learned reward models.
  - Quick check question: Can you explain why RLVR differs from RLHF, and what "verifiable" means in this context?

- Concept: **Curriculum Learning**
  - Why needed here: The paper explicitly organizes training data by difficulty scores to enable progressive learning from simple to complex samples.
  - Quick check question: How would you determine whether curriculum ordering vs. data quality is the primary driver of performance gains?

- Concept: **GRPO (Group Relative Policy Optimization)**
  - Why needed here: The paper adopts GRPO as its RL algorithm, computing advantages relative to group statistics rather than using a separate value function.
  - Quick check question: What are the computational tradeoffs of GRPO compared to PPO with a learned value function?

## Architecture Onboarding

- Component map:
  - Base Model: Qwen2.5-7B-Instruct (LLM) + native-resolution vision encoder (from M2-Omni framework)
  - Data Pipeline: Multi-stage synthesis → Quality filtering → Difficulty scoring → Curriculum ordering
  - Training Stages: Cold-start SFT (168K samples) → Dynamic multi-task RLVR (126.2K samples)
  - Reward Modules: General (rule-based exact match) + Spatial (EDNM smooth reward) + Format (structure validation)

- Critical path:
  1. Data preparation: Run synthesis pipeline with quality scoring (Section 2.1.1) and difficulty annotation (Section 2.1.2)
  2. Cold-start: SFT on 3.3M image-text + 2.9M text samples (Section 2.3) → produces M2-Reasoning-CI
  3. RLVR: Curriculum-sampled batches with dynamic weighting (Eq. 6) and cosine-annealed KL (Eq. 7)
  4. Evaluation: 8 benchmarks spanning general (MathVista, MathVision, etc.) and spatial (CV-Bench, VSI-Bench) reasoning

- Design tradeoffs:
  - **Exact vs. smooth rewards**: Binary rewards for general reasoning (simpler, discrete answers) vs. EDNM for spatial (continuous metrics where early predictions are necessarily imprecise)
  - **Curriculum vs. uniform sampling**: Organized difficulty progression adds preprocessing overhead but stabilizes training; ablation needed to quantify benefit
  - **Task-specific vs. unified rewards**: Tailored incentive signals improve specialization but increase system complexity
  - **Model scale**: 7B parameters chosen for accessibility; larger models may show different optimal reward configurations

- Failure signatures:
  - **Short reasoning chains**: Compared to text-only models like DeepSeek-R1, the model produces less deliberative chains for complex multi-step problems (Section 5)
  - **Pathological repetition**: Occasional loops where the model redundantly generates identical phrases or reasoning steps
  - **Visual hallucinations**: Fine-grained perception errors leading to imprecise descriptions or fabricated objects/attributes
  - **Training instability**: Early RLVR stages may diverge without proper cold-start initialization (addressed by Stage 1 SFT)

- First 3 experiments:
  1. **Data quality ablation**: Train with unfiltered vs. quality-scored CoT data to isolate the contribution of the assessment framework. Measure: performance delta on MathVista and CV-Bench.
  2. **Reward function comparison**: Compare binary spatial rewards vs. EDNM across different λ values (1, 2, 5) to validate the smooth gradient hypothesis. Measure: training curve stability and final VSI-Bench scores.
  3. **Curriculum vs. random ordering**: Run RLVR with difficulty-ordered vs. randomly shuffled data (identical samples, different order). Measure: convergence speed and final average benchmark performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the reasoning depth of M2-Reasoning-7B be extended to match language-only reasoning models like DeepSeek-R1 without sacrificing multimodal coherence?
- Basis in paper: [explicit] The conclusion explicitly states: "Compared to specialized language-only reasoning models like DeepSeek-R1, M2-Reasoning-7B tends to produce shorter reasoning chains. This limits its ability to deconstruct and solve highly complex, multi-step problems that require deeper logical deliberation."
- Why unresolved: The current training paradigm (SFT + RLVR with 1024 token completion limits) may inherently constrain chain length, and it is unclear whether simply scaling completion length would improve reasoning or cause degradation.
- What evidence would resolve it: Ablation experiments varying completion max length during RLVR, combined with evaluation on benchmarks requiring 5+ reasoning steps (e.g., multi-hop mathematical proofs).

### Open Question 2
- Question: How can the pathological repetition issue be mitigated without introducing external constraints that might also suppress legitimate repeated reasoning steps?
- Basis in paper: [explicit] The conclusion explicitly identifies: "In some instances, the model exhibits pathological repetition, where it gets trapped in a loop, redundantly generating the same phrases or reasoning steps. This behavior indicates a potential instability in the generation process that can derail coherent thought progression."
- Why unresolved: The paper does not propose or test any intervention for this issue. Repetition may stem from RL optimization dynamics, but standard anti-repetition penalties could harm valid iterative reasoning.
- What evidence would resolve it: Analysis of repetition frequency across checkpoints; experiments with repetition-aware reward penalties or decoding strategies, evaluated on both reasoning benchmarks and human judgment of output coherence.

### Open Question 3
- Question: Does the use of simulated environments for spatial reasoning data create a sim-to-real generalization gap when deployed on real-world spatial tasks?
- Basis in paper: [inferred] Section 2.2.1 states that for simulated data, "the annotation process prioritizes the use of built-in annotations... These built-in labels in simulated data provide more accurate ground truth compared to real-world data." The paper does not analyze whether training on clean simulated data limits robustness to noisy real-world perception.
- Why unresolved: While the model achieves strong results on CV-Bench and VSI-Bench (which contain real images/videos), the training data mix is not analyzed for potential domain shift effects.
- What evidence would resolve it: Ablation comparing models trained on (a) simulated spatial data only, (b) real-world spatial data only, and (c) the mixed pipeline, evaluated on both simulated and real-world spatial benchmarks.

### Open Question 4
- Question: Are the hand-tuned hyperparameters for the EDNM reward function (γ=1, λ=2) optimal across all spatial reasoning sub-tasks, or should they be task-adaptive?
- Basis in paper: [inferred] The Exponential Decay Numeric Matching reward in Section 3.2 uses fixed γ=1 and λ=2 for all spatial tasks. However, different spatial tasks (e.g., AbsoluteDistance vs. RoomSize) involve different numerical scales and precision requirements, suggesting one global setting may be suboptimal.
- Why unresolved: The paper provides no ablation on these hyperparameters or analysis of per-task sensitivity.
- What evidence would resolve it: Per-task analysis of reward distribution during training; ablation experiments with task-specific λ values based on the typical numerical range of each task type.

## Limitations

- Generalization Beyond Synthetic Data: Performance on curated benchmarks may not translate to real-world multimodal reasoning tasks where rewards are not easily verifiable.
- Curriculum Learning Contribution: The paper does not provide ablation studies isolating curriculum ordering from data quality improvements.
- Scalability of Reward Functions: The EDNM reward function works well for the 7B model but may require tuning for larger models or different domains.

## Confidence

- **High Confidence**: The core mechanism of combining cold-start fine-tuning with RLVR is well-established and the paper's implementation details are clear. The quality scoring framework follows logical principles from curriculum learning literature.
- **Medium Confidence**: The task-specific reward functions (especially EDNM for spatial reasoning) are justified by empirical observations, but lack extensive hyperparameter sensitivity analysis. The dynamic optimization approach shows promise but needs more ablation studies.
- **Low Confidence**: Claims about the relative importance of curriculum ordering versus data quality are not empirically validated. The paper assumes curriculum benefits without direct comparison to randomly ordered equivalent-quality data.

## Next Checks

1. **Curriculum vs. Quality Ablation**: Run identical training pipelines with the same high-quality data samples, comparing curriculum-ordered batches against randomly shuffled batches. Measure convergence speed and final performance on MathVista and CV-Bench to isolate the curriculum effect.

2. **Reward Sensitivity Analysis**: Systematically vary the EDNM hyperparameters (λ ∈ {1, 2, 5}, γ ∈ {0.5, 1.0, 1.5}) during spatial reasoning training. Compare training stability and VSI-Bench performance to determine optimal reward configurations and sensitivity thresholds.

3. **Real-World Transfer Test**: Evaluate M2-Reasoning-7B on open-ended multimodal reasoning tasks without ground-truth verifiable answers (e.g., visual storytelling, open-domain visual question answering). Compare against models trained with human feedback to assess generalization beyond synthetic reward landscapes.