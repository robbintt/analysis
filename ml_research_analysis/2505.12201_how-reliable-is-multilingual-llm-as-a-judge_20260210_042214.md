---
ver: rpa2
title: How Reliable is Multilingual LLM-as-a-Judge?
arxiv_id: '2505.12201'
source_url: https://arxiv.org/abs/2505.12201
tags:
- evaluation
- multilingual
- consistency
- llm-as-a-judge
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the reliability of multilingual LLM-as-a-Judge
  by assessing its consistency across parallel multilingual data. Five models from
  different model families are evaluated across five diverse tasks involving 25 languages.
---

# How Reliable is Multilingual LLM-as-a-Judge?

## Quick Facts
- arXiv ID: 2505.12201
- Source URL: https://arxiv.org/abs/2505.12201
- Reference count: 15
- Primary result: Multilingual LLMs show low consistency across languages with average Fleiss' Kappa ~0.3

## Executive Summary
This paper investigates the reliability of using large language models (LLMs) as judges for multilingual evaluation tasks. The study assesses five different LLM models across five diverse tasks involving 25 languages, revealing significant inconsistencies in judgment results when comparing parallel multilingual data. The findings challenge the assumption that multilingual LLMs can serve as reliable universal judges, particularly for low-resource languages. The authors propose an ensemble strategy that improves consistency in real-world applications.

## Method Summary
The study evaluates five LLM models from different families across five tasks: translation quality, machine translation toxicity, speech recognition, grammatical error correction, and code-switching. The evaluation spans 25 languages, including both high-resource and low-resource languages. Consistency is measured using Fleiss' Kappa across parallel multilingual datasets, comparing judgment results when models evaluate the same content in different languages. The study examines whether multilingual training or model scale impacts consistency, and proposes an ensemble strategy to improve reliability.

## Key Results
- Multilingual LLMs achieve average Fleiss' Kappa of approximately 0.3, indicating moderate agreement at best
- Consistency varies significantly across languages, with particularly poor performance in low-resource languages
- Neither multilingual training nor increasing model scale directly improves judgment consistency
- The proposed ensemble strategy successfully improves multilingual judge consistency in practical applications

## Why This Works (Mechanism)
The mechanism underlying LLM inconsistency as multilingual judges stems from inherent biases in model training and the complex relationship between language proficiency and task-specific judgment capabilities. When evaluating parallel content across languages, models exhibit systematic variations in their assessment criteria, likely due to differences in training data distribution, tokenization strategies, and the models' relative proficiency across language families. This creates a situation where the same semantic content receives different quality judgments simply based on the language of expression.

## Foundational Learning

**Fleiss' Kappa**: A statistical measure of inter-rater reliability that accounts for agreement occurring by chance - needed to quantify judge consistency across languages, quick check: values closer to 1 indicate better agreement.

**Parallel Multilingual Data**: Equivalent content expressed in multiple languages - essential for testing cross-lingual consistency, quick check: ensure semantic equivalence across language variants.

**Low-resource Languages**: Languages with limited digital text corpora for training - critical context for understanding performance gaps, quick check: identify languages with fewer than 1 million training tokens.

**Ensemble Strategy**: Combining multiple model judgments to improve overall reliability - key technical contribution, quick check: measure consistency improvement over individual model baselines.

## Architecture Onboarding

**Component Map**: Task input → Multilingual LLM → Judgment output → Consistency evaluation (Fleiss' Kappa) → Ensemble aggregation

**Critical Path**: The evaluation pipeline follows a straightforward sequence from task presentation to judgment consistency measurement, with the ensemble strategy as an optional optimization layer.

**Design Tradeoffs**: The study balances model diversity against practical deployment considerations, choosing five distinct model families rather than focusing on a single architecture family. This provides broader generalizability but may miss architecture-specific optimization opportunities.

**Failure Signatures**: Models consistently show lower consistency on low-resource languages and certain task types (particularly code-switching), with the magnitude of failure correlating with language family distance from high-resource Indo-European languages.

**First Experiments**:
1. Baseline consistency measurement across all 25 languages and 5 tasks using individual models
2. Cross-model consistency comparison to identify complementary strengths and weaknesses
3. Ensemble strategy implementation and validation on a representative subset of tasks

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Results may not generalize to all possible evaluation scenarios or language pairs beyond the tested 25 languages
- Use of automatic metrics as reference standards introduces potential circularity in consistency assessment
- Study does not explore prompt engineering or instruction tuning specifically for multilingual evaluation tasks

## Confidence

**High confidence**: The finding that multilingual LLMs show inconsistent judgment across languages, particularly for low-resource languages, is well-supported by the data.

**Medium confidence**: The conclusion that neither multilingual training nor model scale directly improves judgment consistency is reasonable but requires further investigation across different model architectures.

**Medium confidence**: The proposed ensemble strategy's effectiveness is demonstrated but validated on a limited set of cases.

## Next Checks

1. Replicate the consistency evaluation using human judgments as the ground truth for a subset of languages and tasks to validate the automatic metrics' reliability.

2. Test the ensemble strategy's effectiveness across additional model combinations and diverse task types beyond the current scope.

3. Conduct a systematic analysis of prompt engineering variations and instruction tuning approaches specifically designed for multilingual evaluation consistency.