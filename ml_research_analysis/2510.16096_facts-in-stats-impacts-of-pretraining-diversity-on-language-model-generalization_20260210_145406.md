---
ver: rpa2
title: 'Facts in Stats: Impacts of Pretraining Diversity on Language Model Generalization'
arxiv_id: '2510.16096'
source_url: https://arxiv.org/abs/2510.16096
tags:
- diversity
- factual
- fact
- training
- statistical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how the diversity of linguistic contexts
  in which factual associations appear during pretraining impacts a language model's
  ability to generalize both factual and statistical knowledge. The authors introduce
  a synthetic testbed that factorizes sequences into a statistical stream of templates
  and a factual stream of source-target pairs, allowing fine-grained control over
  contextual structure and diversity level.
---

# Facts in Stats: Impacts of Pretraining Diversity on Language Model Generalization

## Quick Facts
- arXiv ID: 2510.16096
- Source URL: https://arxiv.org/abs/2510.16096
- Reference count: 40
- Key outcome: Higher contextual diversity in pretraining slows in-distribution factual learning but is critical for out-of-distribution generalization, with low diversity severely impairing factual recall, statistical accuracy, or both depending on template structure.

## Executive Summary
This paper investigates how the diversity of linguistic contexts in which factual associations appear during pretraining impacts a language model's ability to generalize both factual and statistical knowledge. The authors introduce a synthetic testbed that factorizes sequences into a statistical stream of templates and a factual stream of source-target pairs, allowing fine-grained control over contextual structure and diversity level. Experiments show that while higher diversity slows in-distribution factual learning, it is critical for out-of-distribution generalization, with low diversity severely impairing factual recall, statistical accuracy, or both depending on the template structure. The study also identifies optimization bottlenecks, finding that retraining only the unembedding layer on high-diversity data fails to restore generalization unless the underlying features are already high-quality. Ultimately, the work demonstrates that contextual diversity shapes the optimization path and highlights the joint importance of embedding and unembedding layers in enabling robust factual recall.

## Method Summary
The paper introduces a synthetic corpus where sequences are factorized into statistical (Markov chain templates) and factual (source-target pairs) streams. The corpus uses $N=10$ templates, each with a transition matrix and position pair, and $K=100$ fact pairs from disjoint vocabularies. The diversity level (DIV) controls how many templates each fact appears in. A 4-layer, 4-head decoder-only Transformer (dim=32) is trained with AdamW (LR=$10^{-3}$, batch size 64) for 30k iterations. The model is evaluated on factual accuracy, position accuracy, and statistical loss for both in-distribution and out-of-distribution template-fact pairs.

## Key Results
- Higher diversity (DIV=0.9) slows in-distribution factual learning but ensures out-of-distribution generalization.
- Low diversity (DIV=0.1) causes catastrophic out-of-distribution failure, with factual accuracy dropping to near zero despite perfect in-distribution performance.
- Retraining only the unembedding layer on high-diversity data fails to restore generalization unless the underlying features are already high-quality, demonstrating the joint importance of embedding and unembedding layers.

## Why This Works (Mechanism)

### Mechanism 1: Diversity Reshapes the Optimization Landscape
Contextual diversity determines whether gradient descent finds generalizing solutions, not whether such solutions exist. Higher diversity creates a more benign loss landscape around solutions that generalize to unseen template-fact combinations, while low diversity allows convergence to non-generalizing local minima that minimize in-distribution loss but fail on OOD pairs. The transformer is sufficiently expressive to represent the generalizing solution regardless of diversity level.

### Mechanism 2: Joint Embedding-Unembedding Coordination Required for Factual Recall
Factual recall requires coordinated optimization of both embedding (E) and unembedding (U) layers—intervening on either alone fails. The embedding layer must learn template-invariant fact representations that disentangle fact identity from context, while the unembedding layer must map these representations to correct target tokens. When features are corrupted by low-diversity training, retraining U alone cannot recover performance because the input representations lack the necessary structure.

### Mechanism 3: Statistical and Positional Generalization Depend on Attention and Embedding Quality
Statistical pattern learning and position identification fail when attention patterns or token embeddings are poorly optimized. Attention mechanisms must correctly identify fact placeholder positions across templates, while token embeddings must sharply distinguish fact vocabulary from generic tokens. When either module is corrupted, the model cannot properly separate the two streams.

## Foundational Learning

- Concept: Next-token prediction with competing learning signals
  - Why needed here: The paper's setup blends statistical regularity learning (Markov chain) with factual association learning (source-target pairs). Understanding how one loss function drives both is essential for interpreting the coupling results.
  - Quick check question: Why can the same cross-entropy loss simultaneously learn to follow patterns (predicting generic tokens from MC statistics) and memorize associations (predicting specific targets from sources)?

- Concept: Compositional generalization and distribution shift
  - Why needed here: The central finding is that ID and OOD performance diverge under low diversity. This requires understanding what makes template-fact combinations "seen" vs "unseen" and why models might succeed on one but not the other.
  - Quick check question: A model sees fact (a,b) in templates 1,2,3 during training. At test time, it sees (a,b) in template 4. What specific failure modes could prevent correct recall despite perfect ID performance?

- Concept: Feature learning vs linear probing separability
  - Why needed here: The intervention experiments rely on freezing feature-producing modules while retraining the unembedding layer. This requires understanding what each component can and cannot learn in isolation.
  - Quick check question: If you freeze all transformer layers and only train the final linear projection (unembedding), what class of solutions become unreachable?

## Architecture Onboarding

- Component map: Token embeddings (E) -> Attention (Attn) -> Feature formation -> Unembedding layer (U)
- Critical path: E → Attn (position identification) → Feature formation → U (fact retrieval). Position learning temporally precedes factual learning. Poor E or Attn optimization → corrupted features → U cannot compensate alone. U optimization is the bottleneck for slowdown at high diversity.
- Design tradeoffs: High diversity guarantees OOD generalization but slower convergence; low diversity enables fast ID convergence but catastrophic OOD failure; intermediate diversity offers optimal trade-off for limited training budgets. Structure matters: MC10POS1 (fixed positions) is more robust to low diversity than MC1POS10 or MC10POS10.
- Failure signatures: Position accuracy near chance + high statistical loss indicates attention/embedding failure under low diversity with positional variance; statistical loss increasing during training suggests overfitting to ID patterns; perfect ID factual accuracy + near-zero OOD factual accuracy indicates classic low-diversity failure with context-bound features.
- First 3 experiments:
  1. Replicate the diversity sweep (DIV ∈ {0.1, 0.3, 0.5, 0.7, 0.9}) on MC10POS10 structure, logging metrics at 1k, 5k, 15k iterations to verify diversity's effect on ID vs OOD generalization.
  2. Run the embedding patch experiment: Train θhi on DIV=0.9, freeze E, retrain remaining modules on DIV=0.1 data to confirm E is critical for statistical/positional but not sufficient for factual.
  3. Run the joint E+U intervention: Train θhi on DIV=0.9, freeze both E and U, retrain middle layers on DIV=0.1 to verify factual recall requires joint coordination.

## Open Questions the Paper Calls Out

### Open Question 1
How does contextual diversity mathematically reshape the optimization landscape of the in-distribution loss to favor generalizable minima? The paper empirically demonstrates diversity alters the optimization path but doesn't provide a formal theoretical derivation of the loss landscape geometry. A theoretical framework or gradient analysis showing how increasing diversity eliminates non-generalizing solutions from global minimizers would resolve this.

### Open Question 2
How does the interplay between contextual diversity and fact frequency (e.g., "celebrity" vs. rare facts) influence the development of specialized attention heads? The experiments assume uniform distribution of facts across templates, leaving effects of data imbalance unexplored. Experiments using non-uniform exposure matrices to simulate long-tailed knowledge distributions, followed by head specialization analysis, would address this.

### Open Question 3
Can the trade-off between training duration and diversity for OOD generalization be validated in natural language settings with noisy, unstructured data? The findings rely on a minimalist, controlled testbed; it's uncertain if the specific finding that high diversity slows convergence but ensures OOD robustness holds when generic tokens lack strict Markov structure. Training LMs on natural text with artificially injected facts of varying paraphrase diversity would test this.

## Limitations
- The synthetic testbed's controlled Markov chain transitions and disjoint vocabulary structure may not capture the semantic richness and cross-domain transfer patterns of naturalistic text.
- The extremely small vocabulary size (V=203 total, with only 3 generic tokens) may exaggerate the attention and embedding coordination requirements compared to models trained on millions of tokens.
- The training budget of 30k iterations and 4-layer architecture represent significantly constrained settings compared to frontier LLMs, raising questions about scalability.

## Confidence

**High Confidence:**
- Higher diversity slows ID factual learning while improving OOD generalization (directly supported by DIV sweep experiments)
- Low diversity causes catastrophic OOD failure in most template structures (well-validated across multiple structures)
- Retraining U alone cannot recover factual generalization when features are corrupted (directly demonstrated in intervention experiments)

**Medium Confidence:**
- Diversity reshapes optimization landscape rather than solution accessibility (inferred from training curves, lacks direct proof)
- Joint E+U coordination is necessary for factual recall (strongly supported but could have alternative explanations)
- Position learning precedes factual learning (based on qualitative inspection of training dynamics)

**Low Confidence:**
- Broader claims about translation to full-scale pretraining on naturalistic text (synthetic testbed cannot capture semantic richness)
- Specific claim that MLP layers have minimal impact (based on ablation, may not generalize to deeper architectures)

## Next Checks

1. **Scalable Architecture Validation:** Replicate core diversity experiments using a deeper transformer (8 layers, 8 heads, 128 dim) to test whether optimization bottlenecks persist in more realistic model scales.

2. **Naturalistic Corpus Test:** Design a semi-synthetic corpus where Markov chain transitions are drawn from real language statistics while maintaining controlled factual stream structure to test if diversity effects persist with realistic statistical patterns.

3. **Transfer Learning Probe:** Train θhi on DIV=0.9, then fine-tune on small amount of DIV=0.1 data. Compare final OOD generalization against training from scratch on DIV=0.1 to determine if high-diversity pretraining provides transferable inductive biases that accelerate low-diversity learning.