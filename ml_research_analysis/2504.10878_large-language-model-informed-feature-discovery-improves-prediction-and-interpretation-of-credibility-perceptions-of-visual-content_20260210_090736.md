---
ver: rpa2
title: Large Language Model-Informed Feature Discovery Improves Prediction and Interpretation
  of Credibility Perceptions of Visual Content
arxiv_id: '2504.10878'
source_url: https://arxiv.org/abs/2504.10878
tags:
- credibility
- features
- visual
- perceptions
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a Large Language Model (LLM)-informed feature
  discovery framework to improve prediction and interpretation of credibility perceptions
  of visual content. The framework leverages multimodal LLMs like GPT-4o to evaluate
  content credibility, extract interpretable features, and integrate them into machine
  learning models.
---

# Large Language Model-Informed Feature Discovery Improves Prediction and Interpretation of Visual Content

## Quick Facts
- arXiv ID: 2504.10878
- Source URL: https://arxiv.org/abs/2504.10878
- Reference count: 7
- LLM-informed framework outperforms zero-shot GPT by 13% in R2 for credibility prediction

## Executive Summary
This paper introduces an innovative framework that leverages large language models to discover interpretable features for predicting credibility perceptions of visual content. The approach addresses the challenge of moving beyond black-box AI predictions by extracting human-understandable features from multimodal content. Tested on 4,191 social media posts across eight topics with 5,355 crowdsourced credibility ratings, the framework achieved a 13% improvement in predictive accuracy over zero-shot GPT predictions while providing interpretable insights into what drives credibility judgments.

## Method Summary
The framework employs a multi-stage process: first, GPT-4o analyzes each social media post to extract 18 interpretable content features; these features are then integrated into machine learning models for credibility prediction. The approach was validated on a dataset of 4,191 social media posts spanning eight topics (vaccines, climate change, immigration, COVID-19, elections, crime, abortion, and gun control), with credibility ratings from 5,355 unique crowd workers. The extracted features include content-based attributes like information concreteness and information format, as well as aesthetic qualities such as color brightness and colorfulness.

## Key Results
- Achieved 13% improvement in R² over zero-shot GPT predictions for credibility assessment
- Extracted 18 interpretable features including information concreteness, image format, and aesthetic quality
- Key features driving credibility include information concreteness, image format, caption quality, and aesthetic dimensions
- Framework demonstrates both improved predictive accuracy and enhanced interpretability

## Why This Works (Mechanism)
The framework succeeds by leveraging the reasoning capabilities of multimodal LLMs to bridge the gap between raw visual content and human-understandable features that drive credibility perceptions. By extracting interpretable features rather than relying solely on raw content or black-box predictions, the approach enables both better prediction accuracy and meaningful insights into the mechanisms underlying credibility judgments.

## Foundational Learning
- Multimodal LLM capabilities - Needed to understand how GPT-4o can analyze both visual and textual content; Quick check: Review GPT-4o's multimodal architecture and reasoning capabilities
- Feature extraction from visual content - Needed to grasp how interpretable features are derived from images; Quick check: Understand common image feature extraction techniques and their interpretability
- Credibility perception research - Needed to contextualize what drives human judgments of content credibility; Quick check: Review established factors in credibility research and how they map to extractable features

## Architecture Onboarding

**Component Map:** Social Media Post → GPT-4o Analysis → Feature Extraction → Machine Learning Model → Credibility Prediction

**Critical Path:** The core workflow involves multimodal content being processed by GPT-4o to extract interpretable features, which are then fed into traditional ML models for prediction. The critical path is: Content → LLM Feature Extraction → ML Prediction.

**Design Tradeoffs:** The approach trades the simplicity of direct LLM prediction for the interpretability and potentially better generalization of feature-based models. While zero-shot GPT predictions are straightforward, they lack transparency; the feature extraction approach adds complexity but yields interpretable results and better predictive performance.

**Failure Signatures:** Potential failures include: GPT-4o misinterpreting content and extracting incorrect features, overfitting to the specific dataset characteristics, or missing culturally-specific credibility cues that don't generalize. The framework may also struggle with novel content types not represented in the training data.

**First Experiments:** 1) Test feature extraction on a held-out validation set to assess consistency, 2) Compare feature importance across different ML models to validate robustness, 3) Conduct ablation studies removing individual features to identify which contribute most to prediction accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is the LLM-informed feature discovery framework generalizable to other multimodal models (e.g., open-source vision-language models), or is it dependent on GPT-4o's specific reasoning capabilities?
- Basis in paper: [explicit] The authors explicitly state the study is limited by its reliance on GPT-4o and call for future research to "evaluate the generalizability and replicability" of the framework as LLMs evolve.
- Why unresolved: The study only validated the pipeline using a single proprietary model (GPT-4o), leaving the robustness of the method across different model architectures untested.
- What evidence would resolve it: Replicating the feature discovery and prediction pipeline using open-source multimodal models (e.g., LLaVA) on the same dataset and comparing the predictive performance (R²) against the GPT-4o baseline.

### Open Question 2
- Question: How do individual user traits (such as political ideology or digital literacy) interact with the discovered content features (e.g., aesthetic quality, concreteness) to influence credibility perceptions?
- Basis in paper: [explicit] The Discussion notes that while the study focused on content features, "individual differences are also crucial," and suggests future research should "examine how user traits interact with content features."
- Why unresolved: The current methodology aggregates crowdsourced ratings to predict a general "human" perception, which obscures how different demographic or ideological groups might weigh features differently.
- What evidence would resolve it: A multi-level regression model that includes user-level moderators (e.g., political affiliation) to test if the effect size of features like "sensational content" varies significantly by group.

### Open Question 3
- Question: Does the predictive accuracy of the framework remain robust when applied to non-U.S. populations and culturally distinct visual content?
- Basis in paper: [explicit] The authors acknowledge that participants from Prolific "may not fully represent the general population" and warn that LLMs may amplify "cultural... biases," raising concerns about generalizability across diverse regions.
- Why unresolved: The data relies exclusively on U.S.-based crowd workers rating mostly English-language posts, limiting the external validity of the identified features (like "caption formality") in other cultural contexts.
- What evidence would resolve it: Cross-cultural validation studies using social media posts from non-Western regions and credibility ratings collected from local populations to test if the same features drive predictions.

## Limitations
- Framework relies on a single proprietary model (GPT-4o), limiting generalizability
- Dataset limited to U.S.-based crowd workers and English-language content
- 13% improvement, while notable, still leaves room for further accuracy gains
- Interpretability of features may still require domain expertise to fully understand

## Confidence
- Predictive accuracy claims: Medium - Demonstrated improvement but limited to specific dataset
- Feature interpretability claims: Medium - Features are interpretable but validation is limited
- Generalizability claims: Low - Framework not tested across diverse models, cultures, or domains

## Next Checks
1. Test the framework on datasets from different cultural and linguistic backgrounds to assess cross-cultural applicability
2. Conduct longitudinal studies to evaluate the framework's performance over time as social media content and user perceptions evolve
3. Implement a user study to validate the interpretability and usefulness of the extracted features for human analysts in real-world credibility assessment scenarios