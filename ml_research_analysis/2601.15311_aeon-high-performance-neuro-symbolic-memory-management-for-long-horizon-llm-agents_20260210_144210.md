---
ver: rpa2
title: 'Aeon: High-Performance Neuro-Symbolic Memory Management for Long-Horizon LLM
  Agents'
arxiv_id: '2601.15311'
source_url: https://arxiv.org/abs/2601.15311
tags:
- memory
- aeon
- system
- semantic
- latency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Aeon addresses the limitations of Large Language Models (LLMs)
  in long-horizon interactions by treating memory as a managed OS resource rather
  than a passive vector store. The core innovation is the Semantic Lookaside Buffer
  (SLB), a predictive caching mechanism that exploits conversational locality to achieve
  sub-millisecond retrieval latencies.
---

# Aeon: High-Performance Neuro-Symbolic Memory Management for Long-Horizon LLM Agents

## Quick Facts
- arXiv ID: 2601.15311
- Source URL: https://arxiv.org/abs/2601.15311
- Reference count: 12
- Primary result: Sub-millisecond retrieval latency with 85% hit rate via predictive caching

## Executive Summary
Aeon addresses LLM memory bottlenecks in long-horizon interactions by treating memory as a managed OS resource rather than a passive vector store. The system introduces the Semantic Lookaside Buffer (SLB), a predictive caching mechanism that exploits conversational locality to achieve sub-millisecond retrieval latencies. Experiments demonstrate 3× faster performance than HNSW on average and 30× on cache hits, maintaining sub-3ms latency at enterprise scale (1 million nodes) compared to over 100ms for brute-force search.

## Method Summary
Aeon implements a Core-Shell architecture where C++23 manages the Atlas hierarchical index and SLB cache while Python handles LLM orchestration and trace management. The Atlas combines memory-mapped B+Tree/HNSW hybrid indexing with SIMD-accelerated greedy descent, while the SLB uses a 64-entry ring buffer with AVX-512 brute-force cosine similarity scans. A zero-copy nanobind bridge eliminates serialization overhead between components. The system is benchmarked on synthetic dense forest datasets (10⁴-10⁶ nodes) using Uniform Random and Conversational Walk workloads on Apple M4 Max hardware.

## Key Results
- Sub-1ms P99 latency on conversational workloads
- 85% SLB hit rate under realistic conditions
- 3× faster than HNSW on average, 30× on cache hits
- Sub-3ms latency at 1M nodes vs. >100ms for brute-force

## Why This Works (Mechanism)

### Mechanism 1: Semantic Lookaside Buffer (SLB) for Conversational Locality
The SLB achieves sub-millisecond retrieval by exploiting semantic inertia in dialogue, where consecutive queries exhibit high correlation in embedding space. A small ring buffer (K=64 entries) holds cached node centroids and direct memory pointers. On query, a brute-force SIMD scan computes cosine similarity against all entries. If best match exceeds threshold τ_hit (e.g., 0.85), return cached pointer—bypassing tree traversal entirely. LRU eviction maintains freshness. Core assumption: Consecutive conversational queries maintain semantic locality: P(dist(q_{i+1}, q_i) < ε) ≈ 1. This is treated as a hypothesis, not a proven law.

### Mechanism 2: Atlas Hierarchical Indexing with SIMD-Accelerated Descent
Atlas provides logarithmic-scale retrieval (O(log_B M)) by combining small-world graph navigation with B+ Tree-style disk locality. Nodes stored as tuples N={id, v, C, meta} in memory-mapped files. Greedy SIMD Descent starts at candidate node, computes AVX-512 vectorized cosine similarity against all children, selects argmax, recurses until leaf. Branching factor B=64 yields ~4 hops at 1M nodes. Core assumption: Embedding geometry is preserved in hierarchical tree structure; clustering quality determines navigation efficiency.

### Mechanism 3: Zero-Copy Core-Shell Architecture
Zero-copy shared memory eliminates serialization overhead between C++ Core and Python Shell. C++ owns memory-mapped Atlas/SLB structures. nanobind exposes raw pointers as read-only NumPy arrays via PyCapsule. Python views data without copying; write attempts raise exceptions. Core assumption: Python's GIL and memory safety can be managed through read-only constraints; pointer stability is maintained.

## Foundational Learning

- **Memory-Mapped Files (mmap)**
  - Why needed here: Atlas stores nodes on disk but maps them into virtual address space for transparent OS paging.
  - Quick check question: How does mmap differ from heap allocation (new/malloc), and why does it enable B+ Tree-style disk locality?

- **SIMD Vectorization (AVX-512/NEON)**
  - Why needed here: SLB brute-force scan and Atlas descent rely on 50ns vector comparisons for throughput.
  - Quick check question: Why does a 64-element SIMD scan outperform O(log N) tree traversal in wall-clock time?

- **Cache Hierarchy (L1/L2 vs DRAM latency)**
  - Why needed here: SLB sizing (K=64) targets L1/L2 residency to avoid ~100ns DRAM access per pointer chase.
  - Quick check question: What is the latency difference between L1 cache and main memory access, and how does SLB exploit this?

## Architecture Onboarding

- **Component map:**
  - Core (C++23) -> Atlas (mmap B+ Tree/HNSW hybrid), SLB (ring buffer cache), SIMD math kernels, nanobind FFI
  - Shell (Python 3.12) <- Trace (DAG episodic graph), LLM orchestration, prompt engineering, graph topology management
  - Storage: NVMe SSD for Atlas; SLB in CPU cache; Trace in Python heap

- **Critical path:**
  1. Query vector q arrives at SLB
  2. SIMD scan computes similarity to cached centroids
  3. If s_best > τ_hit → return cached ptr (cache hit)
  4. Else → Greedy SIMD Descent through Atlas (cache miss)
  5. Update SLB with result; update Trace DAG
  6. Python Shell reads result via zero-copy NumPy view

- **Design tradeoffs:**
  - SLB size (K) vs L1 cache residency: Larger K improves hit rate but risks cache eviction
  - Threshold τ_hit: Higher values reduce false positives but lower hit rate; tuning required per workload
  - Atlas branching factor (B): Higher B reduces tree depth but increases per-node comparison cost
  - Zero-copy read-only: Safety vs flexibility; Python cannot modify memory in-place

- **Failure signatures:**
  - Low SLB hit rate (<50%): Check workload locality; threshold may be too high or workload non-conversational
  - High P99 latency (>10ms): Check Atlas tree balance; page faults from cold cache; SIMD not vectorizing
  - Memory corruption: Python attempts to write to read-only NumPy array; check exception handling
  - Scaling degradation: Atlas insert-heavy workload causing fragmentation; consider rebuild

- **First 3 experiments:**
  1. **SLB hit rate calibration:** Run "Conversational Walk" vs "Uniform Random" workloads; measure hit rate and latency distribution. Vary τ_hit from 0.70 to 0.95.
  2. **Atlas scaling test:** Benchmark query latency at N=10⁴, 10⁵, 10⁶ nodes. Confirm logarithmic scaling; identify page fault spikes.
  3. **Zero-copy validation:** Compare C++→Python transfer latency for 10MB data via nanobind vs JSON/Pickle. Verify read-only enforcement with write attempt.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the Atlas index support continuous adaptation of its semantic geometry when the meaning of existing concepts drifts over time, beyond simply ingesting new vectors?
- Basis in paper: [explicit] "The Atlas currently relies on static embedding models... When the semantic landscape of the world evolves... the embeddings do not adapt... Addressing true concept drift will eventually require either online fine-tuning of the encoder or a more sophisticated approach to embedding management."
- Why unresolved: The Delta Buffer handles new knowledge insertion, but the underlying vector space geometry remains frozen, limiting adaptability to evolving semantic relationships.
- What evidence would resolve it: A mechanism for online embedding model fine-tuning or dynamic re-embedding that maintains index consistency without full rebuilds, benchmarked on longitudinal datasets exhibiting concept drift.

### Open Question 2
- Question: What is the optimal strategy for dynamically tuning the SLB hit threshold (τ_hit) across diverse workload types with varying semantic locality characteristics?
- Basis in paper: [inferred] The paper uses a fixed threshold of 0.85 and evaluates only two workload profiles (Uniform Random vs. Conversational Walk). No analysis explores adaptive thresholding or performance across intermediate locality patterns.
- Why unresolved: Real-world agent interactions may exhibit fluctuating semantic continuity; a static threshold may yield suboptimal hit rates or increased false positives under domain shifts.
- What evidence would resolve it: Experiments comparing fixed vs. adaptive threshold strategies across a spectrum of synthetic and real conversational datasets, measuring hit rate stability and retrieval precision.

### Open Question 3
- Question: How does Aeon's retrieval latency scale under concurrent read-write workloads with high insert throughput, compared to the read-only benchmarks presented?
- Basis in paper: [inferred] All macro-benchmarks evaluate query latency on static Atlas sizes. Insert performance and concurrent access patterns are not characterized, despite the B+ tree structure's potential lock contention under writes.
- Why unresolved: Production agentic systems require simultaneous memory ingestion and retrieval; the architecture's suitability for mixed workloads remains unvalidated.
- What evidence would resolve it: Benchmarks measuring P99 latency and throughput degradation under varying insert:query ratios, particularly at enterprise scale (1M+ nodes).

### Open Question 4
- Question: Can unified multi-modal vector representations be integrated into the Atlas spatial index without compromising the sub-millisecond latency guarantees achieved for text-only embeddings?
- Basis in paper: [explicit] "Aeon is presently a single-modality system, operating exclusively on text... A truly general Cognitive OS must eventually support multi-modal vector representations within the same spatial index."
- Why unresolved: Different modalities (image, audio) may require different embedding dimensions, distance metrics, or index structures, potentially violating the L1-cache-resident SLB design constraints.
- What evidence would resolve it: Implementation and benchmarking of a multi-modal Atlas with heterogeneous embeddings, measuring retrieval latency and SLB hit rates across cross-modal queries.

## Limitations
- Semantic locality hypothesis lacks corpus validation beyond synthetic benchmarks
- Atlas construction algorithm details remain unspecified, hindering faithful reproduction
- Workloads may not represent real-world conversational patterns accurately

## Confidence
- Sub-millisecond Retrieval Performance: High
- 85% SLB Hit Rate: Medium
- 3× vs HNSW Performance: High
- Zero-Copy Bridge Efficiency: High

## Next Checks
1. Generate conversational query logs from actual LLM interactions and measure cosine distance distribution between consecutive queries to validate semantic locality hypothesis.
2. Reverse-engineer or obtain complete Atlas tree construction algorithm, including node split/cluster criteria, insertion order, and centroid maintenance procedures.
3. Implement zero-copy bridge in multi-tenant environment with concurrent read/write scenarios to test memory safety and performance isolation.