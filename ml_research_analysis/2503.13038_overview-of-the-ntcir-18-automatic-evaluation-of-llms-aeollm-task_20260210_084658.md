---
ver: rpa2
title: Overview of the NTCIR-18 Automatic Evaluation of LLMs (AEOLLM) Task
arxiv_id: '2503.13038'
source_url: https://arxiv.org/abs/2503.13038
tags:
- evaluation
- llms
- task
- methods
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The NTCIR-18 AEOLLM task addresses the challenge of effectively
  evaluating large language models (LLMs) through automatic methods. The task focuses
  on generative tasks and encourages reference-free evaluation approaches to overcome
  limitations of existing methods.
---

# Overview of the NTCIR-18 Automatic Evaluation of LLMs (AEOLLM) Task

## Quick Facts
- arXiv ID: 2503.13038
- Source URL: https://arxiv.org/abs/2503.13038
- Authors: Junjie Chen; Haitao Li; Zhumin Chu; Yiqun Liu; Qingyao Ai
- Reference count: 21
- Primary result: NTCIR-18 AEOLLM task evaluates LLM evaluation methods using accuracy, Kendall's tau, and Spearman's rank across four generative tasks

## Executive Summary
The NTCIR-18 AEOLLM task addresses the challenge of automatically evaluating large language models for generative tasks without relying on reference-based metrics. The task focuses on reference-free evaluation approaches, particularly LLM-as-judge methods, across four diverse subtasks: dialogue generation, text expansion, summary generation, and non-factoid question answering. From four participating teams, 48 runs were evaluated using three metrics measuring consistency with human judgments. The results demonstrate that strong LLMs like GPT-4o can serve as effective evaluators, with PanguIR achieving the best overall accuracy while UCLWI excelled in rank correlation metrics.

## Method Summary
The AEOLLM task framework involves generating answers for 100 questions per subtask using seven different LLMs, collecting human annotations as ground truth, and evaluating participant methods through consistency with human judgments. The evaluation pipeline requires participants to output scores and ranks for each answer, which are then compared to gold-standard rankings derived from human annotators using accuracy, Kendall's tau, and Spearman's rank correlation coefficient. The task uses a 20% train, 20% test, 60% reserved split and encourages reference-free evaluation methods to overcome limitations of traditional reference-based metrics.

## Key Results
- PanguIR achieved the best overall accuracy (0.7008) across all subtasks
- UCLWI excelled in Kendall's tau (0.4704) and Spearman's rank (0.5074) correlation metrics
- Text expansion proved most challenging with highest accuracy reaching only 0.5581
- Baseline GPT-4o direct prompting achieved strong performance, suggesting potential for LLM-as-judges approaches
- Results demonstrate the importance of multiple evaluation metrics due to discrepancies between accuracy and correlation measures

## Why This Works (Mechanism)

### Mechanism 1: Reference-Free Evaluation via LLM-as-Judge
- **Claim**: Direct prompting of strong LLMs (like GPT-4o) can approximate human judgment for evaluating open-ended, generative tasks without ground-truth references.
- **Mechanism**: The system treats a capable LLM as a "judge." It is presented with a question and a set of candidate answers generated by other models. Using prompt engineering, the judge LLM is tasked to score or rank these answers. The correlation between the judge LLM's rankings and human annotator rankings is then measured.
- **Core assumption**: The judging LLM's internal representation of "quality" for a specific prompt aligns sufficiently with human evaluation criteria for that task, assuming the LLM is capable enough.
- **Evidence anchors**:
  - [abstract]: The task "encourages reference-free evaluation methods."
  - [section]: "Baseline4 (GPT-4o)... achieves the best τ and ρ in Text Expansion and Non-Factual QA. This highlights that current strong LLMs already demonstrate impressive evaluation performance and show great potential for implementation in LLMs-as-judges."
- **Break condition**: The correlation breaks down if the judge LLM has a systematic bias (e.g., preferring longer, more verbose answers regardless of quality) or lacks domain knowledge for the specific subtask.

### Mechanism 2: Multi-Metric Correlation for Robustness
- **Claim**: Relying on a single metric like accuracy (acc) is insufficient; a combination of accuracy, Kendall's Tau (τ), and Spearman's Rank (ρ) is required to comprehensively assess an evaluation method's alignment with human judgment.
- **Mechanism**: Accuracy measures the exact match of preference predictions (often derived from pairwise comparisons). Kendall's Tau and Spearman's Rank measure the ordinal association between the entire ranked lists produced by the method and humans. Using all three captures both point-wise correctness and list-wise consistency.
- **Core assumption**: Human evaluation can be reliably converted into a stable ranking. Discrepancies between acc and τ/ρ reveal specific types of misalignment (e.g., correct overall ranking but wrong specific pairwise calls due to ties).
- **Evidence anchors**:
  - [abstract]: "Multiple evaluation metrics are necessary for comprehensive assessment, as accuracy sometimes differs from τ and ρ results."
  - [section]: "However, τ and ρ can sometimes yield an undefined or indeterminate value (NaN) when there are ties... In contrast, acc sometimes differs from the results of these two coefficients."
- **Break condition**: This approach fails if the human annotations used as the gold standard are themselves inconsistent or low-quality, making the calculated correlations noisy and unreliable.

### Mechanism 3: Task-Specific Difficulty Scaling
- **Claim**: Evaluation difficulty scales with the semantic complexity and length of the generated text, making tasks like Text Expansion harder to evaluate automatically than Dialogue Generation.
- **Mechanism**: The evaluation method must process the input and the generated output. Longer, more open-ended outputs (like stories) present a larger semantic space and more potential for subtle errors or incoherence compared to short, constrained responses (like dialogue). This increased complexity makes it harder for the automatic evaluator to maintain high correlation with human judgment.
- **Core assumption**: Text length and semantic openness are primary drivers of evaluation difficulty for current LLM-based evaluators.
- **Evidence anchors**:
  - [abstract]: "Text Expansion being the most challenging (highest acc: 0.5581) and Dialogue Generation the easiest."
  - [section]: "The Text Expansion dataset is the most challenging... This may be due to the longer length of the answers in story generation... In contrast, Dialogue Generation is the easiest... likely because these dialogues tend to be shorter."
- **Break condition**: This mechanism assumes length is the driver. It would break if a short task with very nuanced, expert-required logic (not present in this dataset) proved harder than a long but simple narrative.

## Foundational Learning

- **Concept**: **Reference-Free vs. Reference-Based Evaluation**
  - **Why needed here**: The core premise of the paper is moving away from traditional metrics like BLEU/ROUGE (reference-based) that compare generated text to a ground truth. Understanding this distinction is necessary to grasp why new methods like LLM-as-Judge are needed for generative tasks.
  - **Quick check question**: Can you explain why BLEU might be a poor metric for evaluating a creative story generated from a prompt?

- **Concept**: **Correlation Metrics (Kendall's Tau and Spearman's Rank)**
  - **Why needed here**: The paper uses these to measure how well an automatic evaluator's rankings match human rankings. Understanding that τ and ρ measure ordinal association (rank ordering) rather than raw score differences is key to interpreting the results tables.
  - **Quick check question**: If an evaluator gives scores {A:10, B:8, C:5} and a human gives {A:1, B:2, C:3}, would the Spearman correlation be high or low, and why?

- **Concept**: **Generative Task Heterogeneity**
  - **Why needed here**: The paper evaluates on four different subtasks (dialogue, expansion, summary, QA). One must understand that these tasks require different evaluation criteria (e.g., coherence vs. factuality vs. fluency) to see why a one-size-fits-all evaluator might struggle.
  - **Quick check question**: Why might an evaluator fine-tuned on summary generation fail to perform well on non-factoid question answering?

## Architecture Onboarding

- **Component map**: 
  Question/Answer Sets -> Participant Evaluation Method -> Human Gold Standard -> Evaluation Pipeline -> Metrics (acc, τ, ρ)

- **Critical path**:
  1. **Data Ingestion**: Load the test set (questions + candidate answers).
  2. **Method Execution**: Run your evaluation method (e.g., prompt GPT-4o) on each instance to generate a score/rank.
  3. **Formatting**: Output results in the specified 5-column format (taskId, questionId, answerId, score, rank).
  4. **Scoring**: Compare your output file against the reserved set's gold standard to calculate acc, τ, and ρ.

- **Design tradeoffs**:
  - **Single LLM vs. Ensemble**: Using one strong LLM (like GPT-4o) is simple but may be biased or costly. Ensembles (like PanguIR's multi-model collaboration) are more robust but more complex and expensive to run.
  - **Feature-based vs. Prompt-based**: Feature-based methods (KNUIR) are interpretable but may miss nuance. Prompt-based methods are powerful but act as a black box.

- **Failure signatures**:
  - **Low τ and ρ but high acc**: Your method might be getting the binary preference correct on average but failing to capture the fine-grained ranking order.
  - **NaN for τ/ρ**: Your method likely produced tied scores for answers that humans ranked differently, or it failed to produce scores for some instances.
  - **High performance on Dialogue, Low on Text Expansion**: Your method is likely struggling with long-context reasoning or is sensitive to length bias.

- **First 3 experiments**:
  1. **Baseline Reproduction**: Implement the "Baseline 4" method by directly prompting GPT-4o (or an equivalent model) to score answer pairs. This validates your pipeline against the paper's reported numbers.
  2. **Ablation on Length**: Test the hypothesis that length drives difficulty. Truncate the Text Expansion answers to the average length of Dialogue answers and see if evaluator performance (correlation with human scores on the original full text) changes.
  3. **Metric Discrepancy Analysis**: Identify cases where your method's accuracy is high but τ is low (or vice versa). Manually inspect 10-20 of these cases to understand if the error is in fine-grained ranking or binary preference.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How can automatic evaluation methods be optimized to handle long-form generative tasks like Text Expansion, where current performance is significantly weaker than in shorter dialogue tasks?
- **Basis in paper**: [explicit] The authors identify the Text Expansion dataset as "the most challenging" with a peak accuracy of only 0.5581, explicitly categorizing this as "a challenging scenario for future method optimization."
- **Why unresolved**: The paper observes a performance drop correlated with text length (average answer length of 311.36 tokens vs. 16.14 in Dialogue), but the participating teams' methods did not bridge this gap.
- **What evidence would resolve it**: A submission to the AEOLLM task that achieves a Text Expansion accuracy comparable to the Dialogue Generation task (e.g., > 0.70) while maintaining reference-free constraints.

### Open Question 2
- **Question**: How should the field reconcile discrepancies between pairwise accuracy and rank correlation coefficients (τ and ρ) when determining the superior evaluation method?
- **Basis in paper**: [explicit] The authors note that "accuracy sometimes differs from τ and ρ results" and conclude that "multiple evaluation metrics are necessary," highlighting that PanguIR excelled in accuracy while UCLWI excelled in correlation.
- **Why unresolved**: The paper presents the divergence (e.g., PanguIR #1 in acc but not τ) but does not determine which metric more accurately reflects human preference or if a unified metric is needed.
- **What evidence would resolve it**: A meta-analysis correlating these specific automatic metrics with independent, qualitative human preference scores to determine which automated metric best predicts overall user satisfaction.

### Open Question 3
- **Question**: Under what specific conditions does a simple strong-LLM baseline (e.g., GPT-4o) outperform complex multi-model or optimization-based evaluation systems?
- **Basis in paper**: [inferred] The results show that Baseline 4 (directly prompting GPT-4o) beat all complex participant methods in τ and ρ for Text Expansion and Non-Factual QA, suggesting a boundary condition where complexity does not yield returns.
- **Why unresolved**: While the authors highlight the "impressive evaluation performance" of the baseline, they do not analyze why sophisticated methods like PanguIR failed to surpass simple prompting in these specific subtasks.
- **What evidence would resolve it**: An ablation study varying the complexity of the evaluator model and the prompt strategy specifically for the Text Expansion and Non-Factual QA datasets to isolate the performance drivers.

## Limitations
- The evaluation framework's reliance on human annotations as ground truth introduces potential reliability concerns, as the consistency and expertise of annotators can significantly impact the validity of the correlation metrics.
- The paper does not fully explore the impact of different prompt engineering strategies on the performance of LLM-as-judge approaches, which could be a critical factor in real-world applications.
- The generalizability of the findings to other generative tasks or different evaluation frameworks remains unclear, particularly for tasks with different semantic complexities or evaluation criteria.

## Confidence
- **High Confidence**: The task's design and methodology are well-defined, and the results for the baseline methods are reproducible.
- **Medium Confidence**: The effectiveness of multi-metric evaluation (accuracy, Kendall's Tau, and Spearman's Rank) is supported by the results, but the interpretation of discrepancies between metrics could be more nuanced.
- **Low Confidence**: The generalizability of the findings to other generative tasks or different evaluation frameworks remains unclear, particularly for tasks with different semantic complexities or evaluation criteria.

## Next Checks
1. **Reproduce the baseline results** using the same datasets and evaluation metrics to ensure consistency with the reported outcomes.
2. **Conduct an ablation study** on the impact of prompt engineering for the LLM-as-judge approach, testing different templates to identify optimal configurations.
3. **Validate the robustness of the evaluation framework** by testing it on additional generative tasks not included in the original study, such as creative writing or technical documentation generation.