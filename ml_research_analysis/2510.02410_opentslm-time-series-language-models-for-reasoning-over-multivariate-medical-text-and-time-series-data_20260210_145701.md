---
ver: rpa2
title: 'OpenTSLM: Time-Series Language Models for Reasoning over Multivariate Medical
  Text- and Time-Series Data'
arxiv_id: '2510.02410'
source_url: https://arxiv.org/abs/2510.02410
tags:
- series
- time
- template
- data
- none
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'OpenTSLM introduces Time-Series Language Models that integrate
  time series as a native modality into pretrained LLMs, enabling reasoning over multiple
  time series of any length through natural language. It explores two architectures:
  OpenTSLM-SoftPrompt, which models time series implicitly via learnable tokens, and
  OpenTSLM-Flamingo, which uses cross-attention for explicit time series modeling.'
---

# OpenTSLM: Time-Series Language Models for Reasoning over Multivariate Medical Text- and Time-Series Data

## Quick Facts
- arXiv ID: 2510.02410
- Source URL: https://arxiv.org/abs/2510.02410
- Reference count: 40
- OpenTSLM models outperform text-only baselines and GPT-4o on medical time-series reasoning tasks

## Executive Summary
OpenTSLM introduces Time-Series Language Models that integrate time series as a native modality into pretrained LLMs, enabling reasoning over multiple time series of any length through natural language. It explores two architectures: OpenTSLM-SoftPrompt, which models time series implicitly via learnable tokens, and OpenTSLM-Flamingo, which uses cross-attention for explicit time series modeling. Across three medical datasets (HAR-CoT, Sleep-CoT, ECG-QA-CoT), OpenTSLM models consistently outperform baselines, achieving 69.9% F1 in sleep staging and 65.4% in HAR, compared to 9.05% and 52.2% for text-only finetuned models. Notably, even 1B-parameter OpenTSLM models surpass GPT-4o (15.47% and 2.95%). Expert clinician reviews of ECG-QA rationales show strong reasoning and clinical context integration (85.1% positive assessments). OpenTSLM-Flamingo is recommended as the general-purpose option for efficient, scalable time-series reasoning in medical applications.

## Method Summary
OpenTSLM treats time-series as a native modality by encoding raw signals into embeddings that are fused with text tokens. The method uses two architectures: SoftPrompt concatenates projected time-series tokens with text tokens for implicit fusion, while Flamingo uses gated cross-attention layers for explicit fusion. Both architectures employ a patch encoder to divide time-series into patches, followed by either a transformer encoder (SoftPrompt) or Perceiver Resampler (Flamingo). The models are trained with a two-stage curriculum: first on synthetic datasets (TSQA, M4-Captions) for encoder warmup, then on medical CoT datasets. Training uses LoRA adapters on the LLM backbone with AdamW optimizer, linear warmup, and early stopping.

## Key Results
- OpenTSLM models achieve 69.9% F1 on Sleep-CoT (5-class sleep staging) vs. 9.05% for text-only baselines
- OpenTSLM achieves 65.4% F1 on HAR-CoT (8-class activity recognition) vs. 52.2% for text-only models
- OpenTSLM-Flamingo maintains constant memory usage regardless of input length while matching SoftPrompt performance
- Expert clinician reviews show 85.1% positive assessments of ECG-QA rationales for reasoning quality

## Why This Works (Mechanism)

### Mechanism 1: Native Time-Series Modality Integration
Treating time-series as a native modality rather than tokenizing it as text enables LLMs to reason over continuous signals effectively. Time-series data is encoded via a patch encoder into embeddings, which are then either concatenated with text tokens (SoftPrompt) or fused via cross-attention (Flamingo). This bypasses the discrete token bottleneck, preserving the continuous, high-dimensional structure of temporal data. The LLM then generates reasoning (chain-of-thought) and predictions conditioned on this fused representation.

### Mechanism 2: Gated Cross-Attention for Explicit, Scalable Fusion
Explicit fusion via gated cross-attention (OpenTSLM-Flamingo) provides more stable and scalable learning than implicit fusion via soft prompting, especially for long or multiple time-series. A Perceiver Resampler creates a fixed-size latent representation from variable-length time-series patches. This representation is then injected into the LLM via gated cross-attention layers. The gating parameter allows the model to dynamically control the influence of time-series information on text token processing.

### Mechanism 3: Chain-of-Thought Reasoning for Interpretability
Training models to generate free-form textual rationales (chain-of-thought) before predicting an answer improves interpretability and may guide the model toward more accurate conclusions. The model is trained to output a structured response: "<reasoning> Answer: <final answer>". This forces the model to articulate intermediate steps, contextual factors, and feature identification in natural language before committing to a classification.

## Foundational Learning

- **Concept: Modality Gap and Encoding** - Why needed: The central problem is bridging the gap between continuous time-series signals and the discrete token space of LLMs. A new engineer must understand that naive text tokenization fails and that a learned encoder is required to project the signal into the model's embedding space. Quick check: Why does converting a time-series of floating-point numbers into a comma-separated string perform poorly when used as an LLM prompt?

- **Concept: Soft Prompting vs. Cross-Attention** - Why needed: These are the two primary architectures for multimodal fusion explored in the paper. Understanding their trade-offs (SoftPrompt's simplicity vs. Flamingo's scalability) is crucial for architectural decisions. Quick check: What component allows OpenTSLM-Flamingo to maintain constant memory usage even as the input time-series length increases?

- **Concept: Perceiver Resampler** - Why needed: This is the key architectural component that enables the Flamingo model's scalability. It condenses a variable-length sequence of time-series patches into a fixed number of latent vectors, which are then used as keys/values in the cross-attention mechanism. Quick check: What is the input and output of the Perceiver Resampler module in OpenTSLM-Flamingo?

## Architecture Onboarding

- **Component map:** Raw multivariate time-series → Per-patch normalization (stats added to text) → Patch Embedding → Transformer Encoder/Perceiver Resampler → (MLP or Cross-Attention) → LLM Decoder → Textual Rationale + Answer

- **Critical path:** Raw multivariate time-series → Per-patch normalization (stats added to text) → Patch Embedding → Transformer Encoder/Perceiver Resampler → (MLP or Cross-Attention) → LLM Decoder → Textual Rationale + Answer

- **Design tradeoffs:**
  - SoftPrompt: Simplest to implement. Adds no new layers to the LLM core. Tradeoff: Memory scales with time-series length and number of series. Infeasible for long inputs.
  - Flamingo: More complex architecture. Requires adding and training cross-attention layers. Tradeoff: Scales to any length/number of series with constant memory. Recommended general-purpose choice.

- **Failure signatures:**
  - Tokenized baseline failure: Models repeat the input prompt, count numbers, or fail to follow the output template
  - SoftPrompt memory crash: Out-of-Memory (OOM) errors when training or inferencing with long sequences
  - Reasoning Hallucination: The model generates a plausible-sounding rationale that contradicts the ground-truth signal features

- **First 3 experiments:**
  1. Baseline Replication: Attempt to fine-tune a small LLM (e.g., Llama-1B) on HAR-CoT using simple text tokenization. Confirm the near-zero performance failure mode described in the paper.
  2. SoftPrompt Implementation: Build and train an OpenTSLM-SoftPrompt model on a short, single-series dataset (e.g., Sleep-CoT). Verify it can learn and achieve reasonable performance (F1 > 50%).
  3. Flamingo Memory Scaling Test: Implement the OpenTSLM-Flamingo architecture. Run a memory profiling test by gradually increasing the input time-series length and confirm that peak VRAM remains constant, contrasting it with the SoftPrompt model.

## Open Questions the Paper Calls Out
None

## Limitations
- Architectural hyperparameters like patch size, encoder dimensions, and cross-attention frequency are not fully specified, making exact reproduction challenging
- Performance has only been validated on three medical datasets, with unknown generalizability to non-medical time-series domains
- The paper doesn't include ablation studies comparing chain-of-thought vs. direct prediction to determine if reasoning improves accuracy or just provides interpretability
- The two-stage curriculum learning approach adds significant training complexity without exploring whether synthetic pretraining is essential

## Confidence

**High Confidence:**
- OpenTSLM architectures significantly outperform text-only baselines on all three medical datasets
- OpenTSLM-Flamingo provides stable memory usage while maintaining competitive performance
- Expert evaluation confirms strong reasoning quality in ECG-QA rationales

**Medium Confidence:**
- SoftPrompt architecture's memory scaling limitations make it unsuitable for long sequences
- The native modality integration approach is fundamentally superior to text tokenization for time-series reasoning
- Chain-of-thought generation improves interpretability and potentially accuracy

**Low Confidence:**
- OpenTSLM models will generalize to non-medical time-series domains
- The exact architectural choices (patch size, attention frequency) are optimal
- The synthetic pretraining data is necessary for good performance

## Next Checks

1. **Architectural Sensitivity Analysis:** Systematically vary patch size, encoder dimensions, and cross-attention frequency to determine their impact on performance and memory usage.

2. **CoT Ablation Study:** Compare OpenTSLM models trained with and without chain-of-thought generation on the same datasets to determine whether the reasoning component actually improves accuracy or just provides interpretability.

3. **Cross-Domain Generalization Test:** Evaluate OpenTSLM on at least one non-medical time-series dataset (e.g., human activity recognition from wearables, financial time series, or industrial sensor data) to validate generalizability beyond the medical domain.