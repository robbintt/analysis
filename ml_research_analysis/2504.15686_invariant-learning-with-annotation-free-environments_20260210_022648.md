---
ver: rpa2
title: Invariant Learning with Annotation-free Environments
arxiv_id: '2504.15686'
source_url: https://arxiv.org/abs/2504.15686
tags:
- spurious
- environments
- training
- invariant
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an annotation-free method for invariant learning
  by inferring training environments from the representation space of a pretrained
  ERM model. The key insight is that in the presence of spurious correlations, samples
  cluster more strongly by spurious features than by class labels in the ERM representation
  space.
---

# Invariant Learning with Annotation-free Environments

## Quick Facts
- arXiv ID: 2504.15686
- Source URL: https://arxiv.org/abs/2504.15686
- Reference count: 4
- Primary result: Annotation-free IRM achieves 68.0% accuracy on ColoredMNIST test set

## Executive Summary
This paper presents a method for invariant learning that eliminates the need for manual environment annotations by inferring training environments directly from ERM model representations. The approach exploits the observation that ERM models cluster representations more strongly by spurious features than by class labels, allowing identification of "conflict samples" where spurious correlations contradict class labels. By constructing balanced environments from minority and majority cases within clusters, the method enables IRM to learn invariant features without requiring explicit environment labels. The technique achieves performance comparable to annotation-requiring methods on ColoredMNIST while being simpler and more broadly applicable.

## Method Summary
The method operates in three stages: First, train a standard ERM model on the full training dataset. Second, extract embeddings from the ERM model's penultimate layer and apply k-means clustering to identify spurious-feature-defined groups. Third, identify minority class samples within each cluster as "conflict samples" and construct two environments: one containing all minority samples (Dm) and another balanced between minority and dominant samples (Dbalance). These environments are then used to train IRM, which learns to rely on invariant features rather than spurious correlations. The approach requires no additional annotations beyond the original dataset and works with any standard ERM architecture.

## Key Results
- IRM trained on annotation-free environments (Dm, Dbalance) achieves 68.0% ± 1.1% accuracy on ColoredMNIST test set
- Performance is comparable to methods requiring explicit environment labels (66.9% accuracy) and EIIL (68.4% accuracy)
- The method shows robustness across varying test-time spurious correlation ratios (0.15 to 0.9)
- IRM with constructed environments reaches 65.7% accuracy, close to Oracle performance of 72.7%

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ERM models cluster representations more strongly by spurious features than by class labels in the presence of spurious correlations.
- Mechanism: During ERM training, the model exploits easy-to-learn spurious features as shortcuts. Because spurious features have strong statistical correlation with labels, the learned representation space organizes primarily around these features rather than the true class-determining features.
- Core assumption: Spurious features are learned early or are easier for models to learn than invariant features.
- Evidence anchors:
  - [section 4.1] "the purity of the clusters is higher w.r.t. spurious features than w.r.t. class labels (99.99 vs. 85.13)"
  - [section 4.1] "we assume that each cluster defines a spurious feature"
  - [corpus] Related work "From Invariant Representations to Invariant Data" corroborates that ERM learns spurious correlations, though doesn't validate the clustering claim directly.
- Break condition: If spurious features are not significantly easier to learn than invariant features, or if the spurious correlation strength is weak, clusters may not form cleanly around spurious features.

### Mechanism 2
- Claim: Minority cases within each cluster represent "conflict samples" where class labels contradict the dominant spurious correlation.
- Mechanism: Since clusters are defined by spurious features (Mechanism 1), samples within a cluster share similar spurious features. The majority class in each cluster follows the spurious correlation pattern from training data. Minority cases—samples from the less frequent class within a cluster—are instances where the spurious feature is present but the label contradicts the correlation.
- Core assumption: The training data contains some samples where the spurious correlation does not hold (i.e., the correlation is not 100%).
- Evidence anchors:
  - [section 4.2] "the minority set contains all conflict samples, so Dm is expected to have an inverse correlation between colors and labels compared to the training set"
  - [abstract] "samples share spurious features but have class labels that contradict the spurious correlation in the training data"
  - [corpus] Weak direct validation; related work on spurious correlations exists but doesn't specifically address minority-case conflict sampling.
- Break condition: If the spurious correlation in training data approaches 100%, minority cases become too rare to form meaningful environments.

### Mechanism 3
- Claim: Constructing environments from minority and dominant samples enables IRM to learn invariant features without manual environment annotations.
- Mechanism: D_minority has an inverse spurious correlation relative to training data. D_balance combines minority and dominant cases to create a more balanced spurious correlation. These two environments have different spurious correlation ratios, satisfying IRM's requirement for multiple environments with varying correlations, forcing the model to rely on invariant features.
- Core assumption: IRM can successfully learn invariant features when given environments with sufficiently different spurious correlation structures.
- Evidence anchors:
  - [section 5] "IRM training with our sampling approach Dm and Dbalance is competitive to methods that require annotations"
  - [figure 2b] "IRM_{Dm,Dbalance} are largely consistent over different test environments" compared to ERM which varies with spurious ratio
  - [corpus] "Distribution Shift Is Key to Learning Invariant Prediction" confirms environment construction matters for invariant learning, supporting this mechanism indirectly.
- Break condition: If the inferred environments have insufficient difference in spurious correlation ratios, IRM may not receive adequate signal to disentangle spurious from invariant features.

## Foundational Learning

- Concept: **Spurious Correlations in ERM**
  - Why needed here: The entire method relies on understanding that ERM exploits statistical shortcuts rather than causal features when available, and that this causes distribution shift failures at test time.
  - Quick check question: Can you explain why a model trained on Waterbirds (95% correlation between bird type and background) would fail when tested on balanced backgrounds?

- Concept: **Invariant Risk Minimization (IRM)**
  - Why needed here: This is the downstream method that consumes the constructed environments. You need to understand why IRM requires multiple environments with different spurious correlations to work.
  - Quick check question: Why does IRM require at least two environments, and what property must differ between them?

- Concept: **Clustering for Structure Discovery**
  - Why needed here: The method uses k-means clustering on learned representations as the mechanism to identify spurious-feature-defined groups.
  - Quick check question: If you clustered the penultimate layer of an ERM model and found clusters with 50/50 class balance, what would that suggest about spurious correlations?

## Architecture Onboarding

- Component map:
  ERM Reference Model -> Embedding Extractor -> Clustering Module -> Environment Sampler -> IRM Training

- Critical path:
  1. Train ERM to convergence on full training data → 2. Extract embeddings for all training samples → 3. Cluster embeddings → 4. For each cluster, identify minority class samples → 5. Union all minority samples into Dm → 6. Sample equal dominant cases per cluster into Dbalance → 7. Train IRM with (Dm, Dbalance) as environments

- Design tradeoffs:
  - **Cluster count k**: Paper uses k=8 for binary classification; too few clusters may merge distinct spurious features, too many may fragment conflict samples
  - **Balanced vs. minority-only environments**: Using both Dm and Dbalance provides two environments with different correlation ratios; Dm alone has extreme inverse correlation but may be too small
  - **ERM model selection**: Paper claims no restrictions on reference model, but [section 6] discussion of EIIL suggests ERM models that rely heavily on spurious features provide better clustering signal

- Failure signatures:
  - **Cluster purity equals class purity**: Suggests no exploitable spurious clustering structure; method will produce environments identical to random sampling
  - **Minority set too small**: Dm contains very few samples; environments become imbalanced and IRM training unstable
  - **Test accuracy highly sensitive to test-time spurious ratio**: Indicates invariant learning failed; model still relies on spurious features

- First 3 experiments:
  1. **Validate clustering assumption**: Train ERM on ColoredMNIST (or similar dataset with known spurious correlation), cluster embeddings, compute purity w.r.t. both spurious features and class labels. Expect: spurious purity > class purity.
  2. **Minority set characterization**: For each cluster, compute the ratio of minority to majority samples. Visualize samples from minority set to verify they represent conflict cases (correct label, wrong spurious feature correlation).
  3. **Ablation on environment construction**: Compare IRM trained with (Dm, Dbalance) vs. (random split) vs. (class-balanced split). Expect: proposed method matches or exceeds annotation-requiring baselines on inverse-correlation test set.

## Open Questions the Paper Calls Out

- **Open Question 1**: Does the clustering-based environment construction successfully extend to multi-class classification tasks and datasets with multiple simultaneous spurious correlations?
  - Basis in paper: [explicit] The Conclusion states the authors plan to "validate whether the method successfully extends to more general and complex scenarios, e.g., multi-class classification tasks... and in the presence of multiple spurious correlations."
  - Why unresolved: The current experimental evaluation is restricted to binary classification (ColoredMNIST) with a single spurious feature (color), leaving performance in higher-dimensional or multi-attribute settings untested.
  - What evidence would resolve it: Empirical results on benchmarks with multiple spurious attributes (e.g., CelebA) or multi-class datasets, showing that the minority set sampling remains effective.

- **Open Question 2**: How does the method's performance vary with the strength of the spurious correlation in the training data?
  - Basis in paper: [explicit] The Conclusion lists "varying strength of spurious correlations" as a specific area for future validation.
  - Why unresolved: The paper tests generalization across different *test* spurious ratios (Fig 2b), but relies on a fixed *training* spurious ratio ($p_e=0.15$). It is unclear if the clustering assumption holds when correlations are weaker or extremely strong.
  - What evidence would resolve it: A sensitivity analysis showing the change in test accuracy and cluster purity as the training spurious correlation ratio is systematically varied.

- **Open Question 3**: What are the specific boundary conditions or failure modes where the assumption that spurious features dominate clustering breaks down?
  - Basis in paper: [explicit] The Conclusion explicitly aims to "identify the limits of our approach, i.e., boundary cases where the method fails."
  - Why unresolved: The paper demonstrates success on ColoredMNIST but does not investigate scenarios where the "S-purity" (spurious feature purity) might drop below "C-purity" (class purity), rendering the conflict sample identification ineffective.
  - What evidence would resolve it: Identification of dataset types or noise levels where the extracted minority set $D_m$ fails to create useful environments for IRM training.

## Limitations

- Limited validation to synthetic ColoredMNIST dataset with single spurious feature, leaving real-world multi-feature scenarios untested
- Assumes sufficient minority samples exist to construct meaningful environments, which may not hold for extreme spurious correlations approaching 100%
- Choice of k=8 clusters appears arbitrary without sensitivity analysis or justification for different correlation strengths

## Confidence

- Clustering assumption validity: Medium - validated only on synthetic data with known spurious correlations
- Minority set characterization: Low - lacks explicit correlation measurements of constructed environments
- Final performance claim: Medium - based on single-dataset evaluation
- Method generalizability: Low - untested on real-world datasets with multiple spurious features

## Next Checks

1. Verify clustering purity assumption on real-world dataset (e.g., Waterbirds or CelebA) by computing spurious vs class purity metrics
2. Measure actual spurious correlation ratios in constructed Dm and Dbalance environments to confirm inverse correlation property
3. Conduct ablation study varying k (number of clusters) to determine sensitivity and optimal choice for different correlation strengths