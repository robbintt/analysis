---
ver: rpa2
title: 'Improving Translation Quality by Selecting Better Data for LLM Fine-Tuning:
  A Comparative Analysis'
arxiv_id: '2512.11388'
source_url: https://arxiv.org/abs/2512.11388
tags:
- data
- selection
- comet
- translation
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigates how different data selection strategies
  affect the fine-tuning quality of large language models (LLMs) for Japanese-English
  translation. Using a fixed-size training subset, we compare five selection methods:
  TF-IDF, FD-Score, QURATE, COMET-Kiwi, and random sampling.'
---

# Improving Translation Quality by Selecting Better Data for LLM Fine-Tuning: A Comparative Analysis

## Quick Facts
- arXiv ID: 2512.11388
- Source URL: https://arxiv.org/abs/2512.11388
- Reference count: 28
- Primary result: Semantic-based data selectors (especially COMET-Kiwi) outperform lexical/geometry-based heuristics for Japanese-English LLM fine-tuning

## Executive Summary
This study investigates how different data selection strategies affect the fine-tuning quality of large language models (LLMs) for Japanese-English translation. Using a fixed-size training subset, the authors compare five selection methods: TF-IDF, FD-Score, QURATE, COMET-Kiwi, and random sampling. They fine-tune four 7B-scale models (LLaMA, Gemma, Qwen, Mistral) with LoRA adapters and evaluate translation quality using BLEU and COMET scores on WMT24 benchmarks. Semantic-based selectors, especially COMET-Kiwi, consistently outperform lexical or geometry-based heuristics, yielding higher translation quality and smoother training loss convergence. Even with only 3% difference in selected samples, semantic methods produce substantial gains, highlighting that data quality—not quantity—is key for effective low-resource LLM fine-tuning.

## Method Summary
The authors evaluated five data selection methods for Japanese-English translation fine-tuning: TF-IDF, FD-Score, QURATE, COMET-Kiwi, and random sampling. They fine-tuned four 7B-scale models (LLaMA, Gemma, Qwen, Mistral) with LoRA adapters using a fixed training subset. Translation quality was assessed using BLEU and COMET scores on WMT24 benchmarks. The study compared lexical-based selectors (TF-IDF, FD-Score), geometry-based selectors (QURATE), and semantic-based selectors (COMET-Kiwi) against random sampling as baseline.

## Key Results
- Semantic-based selectors (COMET-Kiwi) consistently outperform lexical and geometry-based heuristics across all four models tested
- Even with only 3% difference in selected samples, semantic methods produce substantial gains in translation quality
- Data quality—not quantity—is key for effective low-resource LLM fine-tuning, as demonstrated by smoother training loss convergence with semantic selectors

## Why This Works (Mechanism)
Semantic-based selectors like COMET-Kiwi capture deeper meaning and contextual relationships between source and target sentences, rather than relying on surface-level lexical overlap or geometric proximity in embedding space. This enables the model to learn more robust translation patterns that generalize better to unseen test data. The improved data quality leads to more efficient learning, requiring fewer training samples to achieve better performance, as evidenced by smoother training loss curves.

## Foundational Learning
- **BLEU score**: Standard metric for machine translation quality based on n-gram precision; needed to quantify translation accuracy against reference translations
- **COMET score**: Reference-based metric using multilingual embeddings to capture semantic similarity; needed as a more robust evaluation than BLEU alone
- **TF-IDF**: Statistical method to measure word importance in documents; used here as a simple lexical-based data selector
- **LoRA (Low-Rank Adaptation)**: Parameter-efficient fine-tuning method that freezes pretrained weights and injects small trainable adapters; needed to enable efficient fine-tuning of large models
- **WMT24 benchmarks**: Standardized evaluation datasets for machine translation; needed to ensure fair comparison across different selection methods
- **Semantic similarity**: Measures meaning-based relationships between texts; needed to understand why COMET-Kiwi outperforms simpler heuristics

## Architecture Onboarding
**Component Map**: Data Selection -> Fine-tuning Pipeline -> Evaluation Framework
**Critical Path**: Data selection strategy → Filtered training corpus → LoRA adapter training → BLEU/COMET evaluation on WMT24
**Design Tradeoffs**: Semantic methods (COMET-Kiwi) offer better translation quality but likely higher computational cost versus simple heuristics like TF-IDF
**Failure Signatures**: Poor data selection leads to noisy training signals, slower convergence, and lower BLEU/COMET scores; random sampling serves as baseline for minimum acceptable performance
**First Experiments**: 1) Compare training curves (loss vs. epochs) across selection methods, 2) Run ablation with full-model vs. LoRA fine-tuning, 3) Test selection methods on different language pairs

## Open Questions the Paper Calls Out
None

## Limitations
- Results are limited to Japanese-English translation pair and WMT24 benchmarks, limiting generalizability to other languages or domains
- Only tested with 7B-scale models using LoRA adapters, so findings may not transfer to full-model fine-tuning or much larger models
- No statistical significance tests or confidence intervals provided across runs
- Relative computational cost of COMET-Kiwi versus simpler methods not discussed
- Does not explore interactions with curriculum learning or other training strategies

## Confidence
- **High**: The core comparative finding that semantic-based selectors outperform lexical/geometry-based ones in controlled settings is robust within the tested configuration
- **Medium**: The claim that "data quality—not quantity—is key" holds for the narrow context of this experiment but lacks broader validation across domains and model scales
- **Low**: Practical implications (e.g., real-world deployment cost-benefit) are not addressed, so claims about general applicability to low-resource scenarios are speculative

## Next Checks
1. Test the same selection methods on multiple language pairs and domains to assess generalization
2. Compare fine-tuning with and without LoRA to evaluate if gains persist under full-model adaptation
3. Perform ablation studies with curriculum learning or other training schedules to measure interaction effects