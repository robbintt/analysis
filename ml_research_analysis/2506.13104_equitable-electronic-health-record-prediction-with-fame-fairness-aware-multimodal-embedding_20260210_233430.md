---
ver: rpa2
title: 'Equitable Electronic Health Record Prediction with FAME: Fairness-Aware Multimodal
  Embedding'
arxiv_id: '2506.13104'
source_url: https://arxiv.org/abs/2506.13104
tags:
- eddi
- data
- multimodal
- fairness
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FAME, a framework for fairness-aware multimodal
  embeddings in EHR prediction. FAME addresses bias in AI models by explicitly weighting
  each modality based on its fairness contribution using the Error Distribution Disparity
  Index (EDDI).
---

# Equitable Electronic Health Record Prediction with FAME: Fairness-Aware Multimodal Embedding

## Quick Facts
- **arXiv ID**: 2506.13104
- **Source URL**: https://arxiv.org/abs/2506.13104
- **Reference count**: 13
- **Primary result**: FAME improves both performance and fairness in multimodal EHR prediction, reducing EDDI to as low as 0.02 while maintaining high AUROC (up to 0.94) and AUPRC (up to 1.00).

## Executive Summary
This paper introduces FAME, a framework for fairness-aware multimodal embeddings in EHR prediction. FAME addresses bias in AI models by explicitly weighting each modality based on its fairness contribution using the Error Distribution Disparity Index (EDDI). The method combines performance and fairness optimization through a joint loss function and sign-agnostic aggregation to ensure equitable outcomes across subgroups. Evaluated on three EHR prediction tasks using BEHRT and BioClinicalBERT, FAME demonstrates significant improvements in both performance (e.g., AUROC up to 0.94, AUPRC up to 1.00) and fairness (EDDI reduced to as low as 0.02) compared to state-of-the-art baselines. The approach highlights the importance of modality-specific fairness weighting in achieving equitable healthcare AI systems.

## Method Summary
FAME is a fairness-aware multimodal fusion framework that weights each input modality (demographic, structured clinical, unstructured text) based on its fairness contribution measured by EDDI. The framework uses BEHRT for structured data and BioClinicalBERT for clinical notes, extracts 256-D [CLS] embeddings, and applies EDDI-weighted fusion with joint BCE and EDDI loss optimization. Modality weights are updated each epoch using EDDI from frozen classification probes, and optional sigmoid feature selection gates the fused representation. The model is trained on MIMIC-III with AdamW optimizer, batch size 16, and λ=0.8 for the fairness-performance trade-off.

## Key Results
- FAME achieves AUROC up to 0.94 and AUPRC up to 1.00 across three EHR prediction tasks.
- EDDI values are reduced to as low as 0.02, demonstrating significant fairness improvements.
- Demographic modality weights consistently decrease during training, while structured and text modalities receive higher weights.
- Optimal performance-fairness balance occurs at λ=0.8; λ=1.0 causes underfitting.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Weighting modalities by their fairness contribution reduces overall bias while maintaining predictive performance.
- Mechanism: FAME computes EDDI for each modality using frozen classification heads, then assigns higher weights to modalities with lower EDDI values using the update rule w(t)_m = w(t-1)_m + γ · (max(EDDI_m) - EDDI_m).
- Core assumption: Individual modalities have heterogeneous fairness properties, and amplifying fairer modalities propagates fairness to the fused output.
- Evidence anchors: [abstract] "FAME addresses bias in AI models by explicitly weighting each modality based on its fairness contribution using the Error Distribution Disparity Index (EDDI)." [section 6.5] "EDDI weights... gradually diverge during training, assigning higher weights to the structured clinical and unstructured text modalities, while the weight for the demographic modality consistently decreases."
- Break condition: If all modalities have similar EDDI values, the weighting scheme provides no differentiation and reduces to average fusion.

### Mechanism 2
- Claim: Sign-agnostic EDDI aggregation prevents subgroup error disparities from canceling out during aggregation.
- Mechanism: FAME computes EDDI_S = (1/|S|) · √(Σ EDDI²_s) instead of simple mean to ensure all disparities contribute positively to the fairness penalty.
- Core assumption: Fairness requires minimizing absolute disparity magnitude regardless of direction (i.e., which subgroup is advantaged).
- Evidence anchors: [section 3.2] "A potential downside of equation 3 is that it adds all positive and negative EDDI values together... To mitigate this, we propose to combine EDDI-values... using a sign-agnostic method."
- Break condition: If only one subgroup exists per sensitive attribute, aggregation is trivial and this mechanism provides no benefit.

### Mechanism 3
- Claim: Joint optimization of prediction loss (BCE) and fairness loss (EDDI) enables a controllable trade-off between accuracy and equity.
- Mechanism: The total loss L_total = L_BCE + λ · L_EDDI combines binary cross-entropy with the aggregated EDDI, with λ controlling the balance (λ=0.8 optimal).
- Core assumption: The fairness objective is compatible enough with the prediction objective that gradient updates from L_EDDI do not catastrophically degrade L_BCE.
- Evidence anchors: [section 3.3] "We use a combination of binary-cross entropy and EDDI loss to optimize our fusion model." [section 6.4] "λ = 0.8 delivers the best overall performance in terms of both EO and AUPRC." [section 6.6] "λ = 1.0 assigns nearly equal EDDI weights across all three modalities... suggesting that the model is underfitting."
- Break condition: If λ is too high, the model underfits; if too low, fairness gains are minimal.

## Foundational Learning

- Concept: **Joint vs. Early vs. Late Fusion**
  - Why needed here: FAME modifies joint fusion to incorporate fairness-aware weighting. Understanding that joint fusion combines intermediate embeddings (not raw inputs or final predictions) is essential.
  - Quick check question: Can you explain why FAME uses joint fusion rather than late fusion for modality weighting?

- Concept: **Error Distribution Disparity Index (EDDI)**
  - Why needed here: EDDI is the core fairness metric. It measures how much a subgroup's error rate deviates from the overall error rate, normalized to [0, 1] range.
  - Quick check question: If subgroup A has error rate 0.15, subgroup B has error rate 0.25, and overall error rate is 0.20, what is EDDI for subgroup A? (Answer: |0.15-0.20|/0.20 = 0.25)

- Concept: **Transformer-based EHR Encoders (BEHRT, BioClinicalBERT)**
  - Why needed here: FAME extracts [CLS] embeddings from these pre-trained models. Understanding that BEHRT handles structured codes and BioClinicalBERT handles unstructured text clarifies the modality split.
  - Quick check question: Which encoder would you use for ICD diagnosis codes, and which for physician notes? (Answer: BEHRT for ICD codes, BioClinicalBERT for physician notes)

## Architecture Onboarding

- Component map:
  - Unimodal encoders (BEHRT for structured/demographic, BioClinicalBERT for clinical notes) -> Extract 256-D [CLS] embeddings -> Frozen classification probes (no gradient) -> Compute EDDI per modality -> Update modality weights -> Weighted fusion (Eq. 2, 6-7) -> Sigmoid feature selection (Eq. 8-9) -> Classifier (Linear + Dropout) -> Joint loss (BCE + λ·EDDI)

- Critical path:
  1. Forward pass through unimodal encoders → extract [CLS] embeddings (256-D each)
  2. Concatenate embeddings → run frozen probes (no grad) to get predictions per modality
  3. Compute EDDI per modality at epoch end → update modality weights
  4. Apply weighted fusion → sigmoid feature selection → classifier → loss

- Design tradeoffs:
  - **EDDI-only vs. FAME (EDDI + Sigmoid)**: EDDI-only works well for LOS task, but full FAME is better for mortality and ventilation. Sigmoid adds learnable feature gating but can increase bias if λ is poorly tuned.
  - **λ selection**: Lower λ (0.2) favors fairness; λ = 0.8 balances both; λ = 1.0 causes underfitting.
  - **Including vs. excluding demographics**: DfC (excluding demographics) reduces bias but hurts performance. FAME down-weights demographics while retaining signal.

- Failure signatures:
  - **EDDI weights staying uniform across training**: Indicates γ may be too low, or all modalities have similar fairness profiles.
  - **High λ (1.0) with poor convergence**: Model underfits; reduce λ.
  - **Sigmoid-only performing worse than average fusion**: Feature selector may be amplifying biased features; check if L1 regularization is applied.

- First 3 experiments:
  1. **Baseline sanity check**: Run unimodal BEHRT and BioClinicalBERT separately. Compare AUROC/AUPRC and EDDI to confirm BioClinicalBERT outperforms BEHRT (as reported in Table 4).
  2. **Ablation on λ**: Sweep λ ∈ {0.2, 0.4, 0.6, 0.8, 1.0} on validation set. Plot AUPRC vs. EDDI to reproduce Figure 2 trade-off curve.
  3. **EDDI weight visualization**: Track modality weights over epochs for optimal λ (0.8). Verify that demographic weight decreases while structured/text weights increase (Figure 3 pattern).

## Open Questions the Paper Calls Out

- **Generalization to Imaging Modalities**: Can FAME maintain effectiveness when applied to imaging data (e.g., X-rays) alongside structured and text data? The authors explicitly state they did not incorporate image modalities and list this as a primary future direction.

- **Cross-Dataset Generalizability**: Can FAME's fairness and performance improvements generalize to external EHR datasets with different demographic distributions or coding standards? The authors note results are based on a single dataset (MIMIC-III), which may affect generalizability.

- **Social Determinants of Health (SDoH)**: Does extracting SDoH from clinical notes and treating them as explicit features or sensitive attributes improve equitable outcomes? The authors plan to explore SDoH identified through clinical notes.

- **Structured vs. Embedding Fusion**: Does converting unstructured text into structured tabular formats (e.g., extracting non-clinical variables) offer a more effective method for bias mitigation than the current EDDI-weighted embedding approach? The authors suggest this could enhance fairness.

## Limitations
- The framework's effectiveness depends on the availability of sensitive attribute labels, which may not always be accessible in real-world clinical settings.
- EDDI-weighted fusion may not provide differentiation when modalities have similar fairness profiles, reducing to average fusion.
- The approach was validated only on structured codes and unstructured text; performance with imaging modalities remains untested.

## Confidence

| Claim/Component | Confidence Level |
|-----------------|------------------|
| Fairness-weighted fusion mechanism | High |
| Theoretical foundation of EDDI | High |
| Specific architectural details | Medium |
| Hyperparameter choices (λ=0.8, γ=0.5) | Medium |
| Generalizability to other datasets | Low |
| Performance with imaging modalities | Low |

## Next Checks
1. **Ablation study on modality weighting**: Remove EDDI weighting (use uniform weights) and measure degradation in both performance and fairness metrics to quantify the contribution of the fairness-aware weighting mechanism.
2. **Cross-dataset evaluation**: Test FAME on a different EHR dataset (e.g., eICU) to assess robustness and generalizability of the approach across institutional data.
3. **Analysis of weighting stability**: Track EDDI weight evolution over training epochs for each modality across multiple random seeds to determine if the weighting pattern is stable or stochastic.