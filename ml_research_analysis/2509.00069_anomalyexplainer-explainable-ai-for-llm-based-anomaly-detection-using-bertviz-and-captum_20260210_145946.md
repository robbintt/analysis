---
ver: rpa2
title: AnomalyExplainer Explainable AI for LLM-based anomaly detection using BERTViz
  and Captum
arxiv_id: '2509.00069'
source_url: https://arxiv.org/abs/2509.00069
tags:
- anomaly
- detection
- attention
- cybersecurity
- chatbot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces AnomalyExplainer Bot, a conversational AI
  framework that combines anomaly detection with explainable AI (XAI) using BERTViz
  and Captum to analyze system logs. The framework provides interactive visualizations
  and natural language reports to improve analyst trust and understanding.
---

# AnomalyExplainer Explainable AI for LLM-based anomaly detection using BERTViz and Captum

## Quick Facts
- arXiv ID: 2509.00069
- Source URL: https://arxiv.org/abs/2509.00069
- Authors: Prasasthy Balasubramanian; Dumindu Kankanamge; Ekaterina Gilman; Mourad Oussalah
- Reference count: 0
- One-line primary result: RoBERTa achieved 99.6% accuracy on HDFS anomaly detection with BERTViz/Captum explainability

## Executive Summary
This study introduces AnomalyExplainer Bot, a conversational AI framework that combines anomaly detection with explainable AI (XAI) using BERTViz and Captum to analyze system logs. The framework provides interactive visualizations and natural language reports to improve analyst trust and understanding. A comparative analysis of transformer-based models showed RoBERTa achieving the highest accuracy at 99.6% with strong anomaly detection, outperforming Falcon-7B, DeBERTa, and Mistral-7B on the HDFS dataset. User study feedback confirmed the chatbot's ease of use and the helpfulness of AI-generated explanations, though visualization tools experienced performance issues.

## Method Summary
The framework fine-tunes RoBERTa-base on the HDFS dataset (4,000 training samples) for binary classification of system logs as normal vs anomalous. The method employs BERTViz for attention visualization and Captum for feature attribution to generate explanations. A LangChain-orchestrated pipeline connects the detection model with GPT-4o to produce conversational natural language reports. The system architecture uses Django backend with MongoDB for interaction logs and SQLite for user authentication, while frontend visualizations are rendered through iFrames.

## Key Results
- RoBERTa achieved 99.6% accuracy, 1.00 anomaly precision, 0.91 anomaly recall, and 0.95 anomaly F1 on HDFS test set
- RoBERTa outperformed Falcon-7B (0.0 recall), DeBERTa, and Mistral-7B in comparative analysis
- User study (n=9) showed chatbot ease of use with helpful explanations, though visualization tools had timeout issues

## Why This Works (Mechanism)

### Mechanism 1: Encoder Efficiency in Low-Resource Log Classification
Encoder-only models like RoBERTa can outperform larger decoder models in specific log classification tasks when training data is limited. RoBERTa's bidirectional attention captures log structure more efficiently than causal masking in decoders, particularly when fine-tuned on small datasets (4,000 samples).

### Mechanism 2: Attention-Driven Explainability
Aggregating self-attention weights and gradients provides a viable proxy for identifying which log tokens contributed most to anomaly classification. The system calculates entropy across attention heads to find "focused" heads and uses Captum for gradient-based attribution, feeding this data into a unified algorithm to identify top tokens summarized into natural language.

### Mechanism 3: Conversational Abstraction for Trust
Translating raw visualizations and technical metrics into natural language "reports" increases analyst trust and system usability compared to raw model outputs. The system uses GPT-4o to parse XAI data and generate dialogue-based summaries, bridging the gap between complex transformer internals and security analyst mental models.

## Foundational Learning

- **Transformer Attention Heads (Encoder vs. Decoder)**
  - Why needed here: The paper relies on the fundamental difference between RoBERTa (encoder/bidirectional) and Mistral/Falcon (decoder/unidirectional) to justify model selection
  - Quick check question: Can a decoder-only model look at future tokens in a sequence to classify the current token?

- **Entropy in Attention Distributions**
  - Why needed here: Algorithm 1 uses entropy to determine if an attention head is "focused" (low entropy) or "diffuse" (high entropy)
  - Quick check question: Does a lower entropy value in an attention distribution indicate the model is focusing on a specific set of tokens or broadly attending to everything?

- **Log Parsing and Normalization**
  - Why needed here: The methodology explicitly mentions preprocessing raw logs before tokenization
  - Quick check question: Why would feeding raw, unnormalized log files into a transformer likely result in poor generalization to unseen logs?

## Architecture Onboarding

- **Component map:** User Upload -> Django Backend -> RoBERTa Inference -> XAI Generation (BERTViz + Captum) -> GPT-4o Synthesis -> Frontend Render
- **Critical path:** User Upload -> Django -> Preprocessing -> RoBERTa Inference -> XAI Generation -> GPT-4o Synthesis -> Frontend Render
- **Design tradeoffs:** RoBERTa selected for 99.6% accuracy over Mistral's theoretical generative power; iFrame visualizations keep UI clean but introduce timeout failures; post-hoc XAI is model-agnostic but computationally heavy
- **Failure signatures:** Visualization Timeout (BERTViz/Captum computation delays), HTML Artifacts (improper escaping between backend and iFrame), Session Instability (memory leaks or race conditions)
- **First 3 experiments:** 1) Latency Profiling to isolate XAI bottleneck, 2) Zero-shot vs Fine-tuned Baseline comparison, 3) Load Testing the iFrame for concurrent user performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can real-time log monitoring with continuous anomaly detection maintain comparable accuracy and explanation quality while meeting latency requirements for operational cybersecurity workflows?
- Basis in paper: [explicit] Authors state future work will focus on "enabling real-time monitoring" and "moving from upload-based analysis to real-time log monitoring, enabling continuous anomaly detection and explanation"
- Why unresolved: Current implementation uses batch upload-based processing; real-time streaming introduces different computational constraints and potential latency-explanation quality tradeoffs
- What evidence would resolve it: Benchmark results showing detection accuracy, explanation fidelity, and end-to-end latency under streaming log conditions with realistic throughput rates

### Open Question 2
- Question: How does the AnomalyExplainer framework perform across diverse log types beyond the HDFS dataset (e.g., syslog, Windows Event Logs, firewall logs, cloud service logs)?
- Basis in paper: [explicit] Authors note "expanding log type support to enhance scalability and usability in cybersecurity workflows" as future work
- Why unresolved: Study only evaluated on HDFS logs from LogHub; different log formats, vocabularies, and anomaly patterns may affect model transferability and explanation quality
- What evidence would resolve it: Cross-dataset evaluation results on multiple log types with comparative accuracy, F1-scores, and user-assessed explanation usefulness

### Open Question 3
- Question: What architectural optimizations can reduce visualization latency while preserving the interpretability benefits of attention-based explanations (BERTViz) and feature attributions (Captum)?
- Basis in paper: [explicit] User study found "7 participants find it not useful or didn't use the visualization tools" with "Common issues included timeouts, HTML artifacts in the output, and lack of interpretability"
- Why unresolved: Heavy backend computations cause failures; proposed optimizations (asynchronous processing, client-side rendering) are untested in this context
- What evidence would resolve it: Ablation study measuring visualization load times, failure rates, and user comprehension scores across different optimization strategies

## Limitations
- Single dataset evaluation (HDFS) with limited sample sizes (4,000 training samples) raises generalizability concerns
- Attention-based explanations rely on contested methodological assumptions about attention weights representing causal reasoning
- User study sample size (n=9) may not represent diverse analyst expertise levels or use cases
- Visualization tools experienced timeouts and HTML artifacts suggesting scalability challenges

## Confidence
- **High Confidence:** RoBERTa achieving 99.6% accuracy on HDFS dataset; comparative model performance rankings
- **Medium Confidence:** Effectiveness of attention-based explanations for improving analyst trust; chatbot interface usability ratings
- **Low Confidence:** Generalizability to other log datasets; causal validity of attention-based explanations; long-term system stability under production workloads

## Next Checks
1. **Dataset Generalization Test:** Evaluate framework on at least two additional log datasets (e.g., BGL, HDFS anomaly subsets) to assess performance consistency
2. **Ablation Study on Explainability:** Compare analyst decision accuracy and trust when using full XAI system versus baseline detection without explanations
3. **Scalability Benchmark:** Conduct load testing with concurrent users and large log volumes to identify bottlenecks and measure latency impacts on real-time monitoring