---
ver: rpa2
title: Agentic LLMs for Question Answering over Tabular Data
arxiv_id: '2509.09234'
source_url: https://arxiv.org/abs/2509.09234
tags:
- question
- data
- queries
- query
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents an LLM-driven Natural Language to SQL (NL-to-SQL)
  pipeline for Question Answering over Tabular Data (Table QA). The system addresses
  challenges in structured data querying by dynamically generating SQL queries from
  natural language questions and retrieving precise answers.
---

# Agentic LLMs for Question Answering over Tabular Data

## Quick Facts
- arXiv ID: 2509.09234
- Source URL: https://arxiv.org/abs/2509.09234
- Reference count: 6
- Primary result: 70.5% accuracy on DataBench QA benchmark

## Executive Summary
This paper presents an LLM-driven NL-to-SQL pipeline for Question Answering over Tabular Data (Table QA). The system addresses challenges in structured data querying by dynamically generating SQL queries from natural language questions and retrieving precise answers. A multi-stage pipeline incorporating example selection, SQL query generation, answer extraction, verification, and iterative refinement was developed and tested on the SemEval 2025 DataBench benchmark. Using GPT-4o, GPT-4o-mini, and DeepSeek v2:16b models, the approach achieved 70.5% accuracy on DataBench QA and 71.6% on DataBench Lite QA, significantly surpassing baseline scores of 26% and 27% respectively.

## Method Summary
The method employs a 5-stage agentic pipeline: (1) Example Selection retrieves the top 2 similar SQL patterns from a curated set of 25 using embedding-based similarity search, (2) SQL Generation creates queries using few-shot prompting with schema and examples, (3) Execution runs the SQL on SQLite databases, (4) Answer Extraction uses Chain-of-Thought prompting to format results into the required output type, and (5) Verification and Reprocessing employs a critic LLM to check format validity and trigger refinement if needed. The approach leverages GPT-4o for generation and verification, with performance tested across 65 diverse datasets.

## Key Results
- Achieved 70.5% accuracy on DataBench QA benchmark
- Surpassed baseline scores of 26% by over 44 percentage points
- 71.6% accuracy on DataBench Lite QA (vs 27% baseline)

## Why This Works (Mechanism)

### Mechanism 1: Semantic Context Grounding via Few-Shot Retrieval
- **Claim:** Retrieving structurally similar question-query pairs improves SQL generation accuracy compared to zero-shot approaches.
- **Mechanism:** By embedding the user question and comparing it against a curated vector database of examples, the system retrieves the top-k most relevant SQL patterns. This conditions the LLM on specific syntax and logic (e.g., joins, aggregations) before it attempts to generate a new query.
- **Core assumption:** The intent and schema logic of the new question sufficiently resemble the curated examples to guide the model.
- **Evidence anchors:** Mentions "embedding-based similarity for context selection" in abstract; Section 4.1 details use of text-embedding-ada-002 and ChromaDB to select the two most relevant examples from a set of 25; Related work (AILS-NTUA) supports efficacy of language-to-code prompting for this benchmark.
- **Break condition:** Fails if the input question requires reasoning patterns not covered by the 25 curated examples or if the semantic similarity search retrieves examples with similar keywords but incompatible logic.

### Mechanism 2: Iterative Correction via Verification-Reprocessing Loop
- **Claim:** Separating generation from verification and allowing for reprocessing reduces unforced errors in formatting and relevance.
- **Mechanism:** A secondary LLM step acts as a critic, evaluating the initial answer against format constraints (e.g., boolean, list) and question relevance. If the answer fails validation, the system refines the SQL prompt—specifically targeting specific values rather than broad rows—and re-executes the pipeline.
- **Core assumption:** The verification model can reliably detect format violations and hallucinations, and the error is correctable via prompt refinement rather than fundamental schema misunderstanding.
- **Evidence anchors:** Highlights "iterative refinement" as a key component in abstract; Section 4.5 describes the reprocessing strategy where flagged queries are updated to prioritize direct value extraction.
- **Break condition:** Fails if the "verifier" has a false negative (rejecting a correct answer) or if the reprocessing prompt lacks the necessary context to fix a semantically incorrect query.

### Mechanism 3: Chain-of-Thought (CoT) for Structured Extraction
- **Claim:** CoT prompting is required to bridge the gap between raw SQL results and the final natural language answer format.
- **Mechanism:** Raw SQL rows often contain extraneous data. CoT forces the model to explicitly reason through the retrieved rows, filter relevant cells, and compute derived values (e.g., averages) before outputting the final formatted string.
- **Core assumption:** The LLM possesses sufficient intrinsic reasoning capability to process the retrieved rows accurately once prompted to think step-by-step.
- **Evidence anchors:** Cites "Chain-of-Thought reasoning for structured answer extraction" in abstract; Section 4.3 explains how CoT allows the breakdown of answer derivation into logical steps.
- **Break condition:** Fails in cases of "Complex Numerical Reasoning" (Section 7) where multi-step logic or ranking is required that exceeds the model's reasoning window or numerical fidelity.

## Foundational Learning

- **Concept: Vector Similarity Search (RAG Context)**
  - **Why needed here:** The pipeline relies on dynamic few-shot prompting. You cannot understand the "Example Selection" stage (4.1) without understanding how text is converted to vectors and sorted by cosine similarity.
  - **Quick check question:** How does the system determine which of the 25 SQL examples to show the model for a given user question?

- **Concept: LLM-based Verification/Critique**
  - **Why needed here:** The architecture is agentic not just because it acts, but because it "reflects." Understanding the verifier's role is key to grasping why accuracy improved over baselines (which likely lacked this self-correction).
  - **Quick check question:** In the Answer Verification stage (4.4), is the model checking the *fact* against the database, or the *format* against the requirements?

- **Concept: Post-Processing SQL Outputs**
  - **Why needed here:** The system does not return raw SQL results; it extracts answers. You must understand that SQL returns rows, but the user wants a "Yes/No" or a single number.
  - **Quick check question:** Why can't the SQL query alone serve as the final answer for a question like "What is the average age of the top 5 players?"

## Architecture Onboarding

- **Component map:** Input Question -> ChromaDB Retrieval (Top 2 Examples) -> LLM SQL Generation -> SQLite Execution -> LLM Extraction (CoT) -> LLM Verification -> Conditional Reprocessing Loop
- **Critical path:** The interaction between **Example Selection** (4.1) and **SQL Generation** (4.2). If the examples retrieved are poor (low semantic overlap but high keyword match), the SQL generation will likely fail, triggering the costly reprocessing loop.
- **Design tradeoffs:**
  - **Latency vs. Accuracy:** The reprocessing loop (4.5) essentially doubles the latency and cost for flagged queries (approx. 30-40% of queries based on baseline gap). The tradeoff is accepted to ensure the 70%+ accuracy benchmark.
  - **Model Size:** GPT-4o is used for generation/verification, but GPT-4o-mini is tested. The paper implies the smaller model loses "adaptability," suggesting a threshold of reasoning capability is required for the Verifier role.
- **Failure signatures:**
  - **Complex Numerical Reasoning:** The paper admits difficulty with multi-step calculations (ranking then aggregating).
  - **Categorical Misclassification:** Selecting semantically related but incorrect categories (Section 7).
  - **Format Drift:** The verifier exists specifically because LLMs often return sentences when asked for a boolean.
- **First 3 experiments:**
  1. **Ablation on Example Count:** Re-run the pipeline using 0, 1, 5, and 25 examples to measure the sensitivity of the SQL generator to the few-shot context window.
  2. **Verifier Threshold Tuning:** Adjust the "strictness" of the verifier (Section 4.4 mentions borderline cases default to acceptance). Test if a stricter verifier improves final accuracy or just increases latency without fixing root SQL errors.
  3. **Schema Context Limiting:** Test performance when providing only relevant columns (schema linking) vs. the full table schema to the generator, observing impact on token limits and SQL hallucinations.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can specialized tool-use or distinct sub-agents resolve the multi-step numerical reasoning errors identified in the current pipeline?
- **Basis in paper:** [explicit] Section 7 lists "Complex Numerical Reasoning Errors" involving ranking, aggregation, and filtering as a primary failure mode.
- **Why unresolved:** The current linear pipeline with general-purpose LLMs (GPT-4o/DeepSeek) struggles to execute these multi-step logics reliably without explicit structural guidance.
- **What evidence would resolve it:** A benchmark evaluation isolating multi-hop numerical queries, comparing standard CoT prompting against a tool-augmented agentic architecture.

### Open Question 2
- **Question:** Does enforcing explicit schema understanding prior to query generation significantly outperform the current implicit context-based method?
- **Basis in paper:** [explicit] Section 7 suggests "incorporating explicit schema understanding" as a necessary advancement for future frameworks to improve query accuracy.
- **Why unresolved:** The current method relies on providing sample rows and schemas in the prompt, relying on the LLM's implicit reasoning rather than a structured schema encoding.
- **What evidence would resolve it:** A controlled comparison on DataBench where the experimental group receives strict schema definitions or graph-based schema embeddings versus the control group relying on few-shot rows.

### Open Question 3
- **Question:** Can adaptive learning strategies reduce the frequency of semantically related but categorically incorrect classifications?
- **Basis in paper:** [explicit] Section 7 notes "Categorical Misclassification" as a key error source and proposes "adaptive learning strategies" as a potential remedy.
- **Why unresolved:** The current static prompts may fail to disambiguate similar categorical values within specific domains without feedback loops or dynamic example updates.
- **What evidence would resolve it:** An iterative evaluation where misclassified examples from validation runs are dynamically added to the few-shot context to measure error rate reduction.

### Open Question 4
- **Question:** Is the pipeline's performance sensitive to the size and diversity of the fixed example set used for similarity-based selection?
- **Basis in paper:** [inferred] Section 4.1 describes a fixed set of 25 examples for context selection, assuming this static size is sufficient for "diverse question patterns."
- **Why unresolved:** A fixed corpus of 25 examples may lead to retrieval drift or lack of coverage for complex, out-of-distribution queries found in the 65 diverse datasets.
- **What evidence would resolve it:** An ablation study varying the example pool size (e.g., 0, 10, 25, 100 examples) and measuring the corresponding variance in SQL generation accuracy.

## Limitations

- Highly dependent on the quality and coverage of the 25 curated examples used for few-shot prompting
- Struggles with complex numerical reasoning tasks involving multi-step logic or ranking operations
- Uses a fixed set of examples rather than dynamically expanding its knowledge base

## Confidence

- **High Confidence:** The core mechanism of using embedding-based similarity search for example retrieval is well-supported by experimental results (70.5% accuracy vs 26% baseline).
- **Medium Confidence:** The effectiveness of Chain-of-Thought prompting for structured answer extraction is supported but could benefit from more rigorous ablation studies.
- **Low Confidence:** The scalability analysis is limited to three model variants on a single benchmark.

## Next Checks

1. **Example Set Sensitivity:** Conduct systematic ablation studies varying the number of examples (0, 1, 5, 10, 25) to quantify the marginal benefit of few-shot prompting.
2. **Verifier Calibration:** Implement quantitative evaluation of the verification model's false positive/negative rates by creating a labeled test set of borderline cases.
3. **Schema Context Optimization:** Compare performance when providing the full table schema versus only the subset of columns referenced in the question.