---
ver: rpa2
title: Can LLMs reason over extended multilingual contexts? Towards long-context evaluation
  beyond retrieval and haystacks
arxiv_id: '2504.12845'
source_url: https://arxiv.org/abs/2504.12845
tags:
- context
- multilingual
- evaluation
- languages
- passage
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the gap in multilingual long-context evaluation
  by introducing MLRBench, a synthetic benchmark that goes beyond retrieval-centric
  tasks to assess reasoning, multi-hop inference, and epistemic reasoning across seven
  languages. Unlike existing benchmarks, MLRBench is designed to be parallel, resistant
  to data leakage, and scalable to arbitrary context lengths.
---

# Can LLMs reason over extended multilingual contexts? Towards long-context evaluation beyond retrieval and haystacks
## Quick Facts
- arXiv ID: 2504.12845
- Source URL: https://arxiv.org/abs/2504.12845
- Reference count: 12
- Primary result: Introduces MLRBench, a synthetic multilingual benchmark for evaluating reasoning over long contexts beyond retrieval, revealing significant underutilization of context windows and cross-linguistic reasoning gaps.

## Executive Summary
This paper addresses a critical gap in multilingual long-context evaluation by introducing MLRBench, a synthetic benchmark designed to assess reasoning, multi-hop inference, and epistemic reasoning across seven languages. Unlike existing benchmarks focused on retrieval, MLRBench is parallel, resistant to data leakage, and scalable to arbitrary context lengths. Experiments with Llama-3.1-Instruct reveal that multilingual LLMs effectively utilize less than 30% of their claimed context window, with significant performance gaps between high- and low-resource languages, particularly for complex reasoning tasks. While Retrieval Augmented Generation (RAG) improves stability, it does not fully solve the long-context problem, indicating that current benchmarks may overestimate LLM reasoning capabilities.

## Method Summary
MLRBench is a synthetic, multilingual long-context benchmark that goes beyond retrieval-centric tasks to evaluate reasoning, multi-hop inference, and epistemic reasoning. The benchmark is generated using controlled templates in seven languages (Arabic, Chinese, English, French, Hindi, Spanish, and Vietnamese), ensuring parallelism and resistance to data leakage. The methodology includes generating documents with varying levels of difficulty, controlling for context length and language resources, and evaluating performance using both direct context utilization and RAG-based approaches. The synthetic nature allows for scalable, reproducible experiments across different context lengths and reasoning tasks, providing a controlled environment to isolate and measure LLM reasoning capabilities in multilingual settings.

## Key Results
- Multilingual LLMs utilize less than 30% of their claimed context window, with significant underutilization observed across all languages.
- Performance gaps exist between high-resource and low-resource languages, especially for complex reasoning tasks like epistemic reasoning and multi-hop inference.
- RAG improves stability in long-context reasoning but does not fully address the fundamental limitations of current LLMs in utilizing extended contexts.

## Why This Works (Mechanism)
MLRBench works by providing a controlled, synthetic environment that isolates reasoning capabilities from noise and variability present in real-world data. By using parallel document generation across seven languages, the benchmark ensures fair comparison and resistance to data leakage. The synthetic approach allows for precise manipulation of context length, reasoning complexity, and language resources, enabling systematic evaluation of how LLMs process and reason over extended multilingual contexts. The inclusion of both direct context utilization and RAG-based evaluation methods provides a comprehensive view of LLM performance, highlighting the limitations of current long-context strategies and the need for more robust evaluation frameworks.

## Foundational Learning
- **Multilingual Long-Context Evaluation**: Understanding the unique challenges of evaluating reasoning over extended contexts in multiple languages, including the need for parallel datasets and resistance to data leakage.
- **Synthetic Benchmark Generation**: Learning how to create controlled, scalable synthetic data that isolates specific reasoning tasks and language resources, enabling reproducible experiments.
- **Retrieval Augmented Generation (RAG)**: Exploring how RAG can improve stability in long-context reasoning tasks, but also recognizing its limitations in fully addressing context window underutilization.
- **Cross-Linguistic Reasoning Gaps**: Analyzing why performance varies between high-resource and low-resource languages, particularly for complex reasoning tasks like epistemic reasoning and multi-hop inference.
- **Context Window Utilization**: Investigating how LLMs actually use their claimed context windows and why significant underutilization occurs, even in synthetic, controlled settings.

## Architecture Onboarding
- **Component Map**: Document Generation (Templates) -> Context Injection (Multilingual Parallel) -> Task Definition (Reasoning/Multi-hop/Epistemic) -> Evaluation (Direct + RAG)
- **Critical Path**: Synthetic document generation → Context length scaling → Task assignment → LLM evaluation → Performance analysis
- **Design Tradeoffs**: Synthetic data enables control and scalability but may lack real-world noise; parallel generation ensures fairness but increases complexity; RAG inclusion provides stability but doesn't solve core long-context issues.
- **Failure Signatures**: Sharp performance drops for low-resource languages in complex tasks; consistent underutilization of context window; RAG does not close performance gaps for long or complex reasoning tasks.
- **3 First Experiments**:
  1. Test MLRBench with additional multilingual LLMs to compare context utilization and reasoning gaps.
  2. Evaluate real-world multilingual document collections to assess synthetic benchmark generalizability.
  3. Experiment with alternative long-context strategies (e.g., positional interpolation, dynamic windowing) to improve context utilization.

## Open Questions the Paper Calls Out
The paper does not explicitly call out additional open questions beyond those addressed in the benchmark design and experiments.

## Limitations
- The synthetic nature of the benchmark may not fully capture the complexity and noise of real-world multilingual documents, particularly for epistemic reasoning and multi-hop inference tasks.
- The focus on Llama-3.1-Instruct limits conclusions about other multilingual models, and the experiments do not extensively explore the impact of different prompting strategies or fine-tuning approaches.
- While RAG is shown to improve stability, the analysis does not explore alternative long-context strategies such as positional interpolation or dynamic windowing.

## Confidence
- **High**: Core finding that multilingual LLMs underutilize their context windows is well-supported by synthetic experiments.
- **Medium**: Cross-linguistic reasoning gaps and RAG effectiveness are demonstrated but may not fully extend to real-world scenarios due to synthetic benchmark limitations.

## Next Checks
1. Validate benchmark findings with real-world multilingual document collections and noisy data.
2. Test additional multilingual LLMs and compare performance across model families and sizes.
3. Experiment with alternative long-context strategies (e.g., positional interpolation, dynamic windowing) and prompting methods.