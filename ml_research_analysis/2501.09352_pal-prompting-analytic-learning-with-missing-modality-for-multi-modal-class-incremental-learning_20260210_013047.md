---
ver: rpa2
title: 'PAL: Prompting Analytic Learning with Missing Modality for Multi-Modal Class-Incremental
  Learning'
arxiv_id: '2501.09352'
source_url: https://arxiv.org/abs/2501.09352
tags:
- learning
- prompt
- missing
- modality
- multi-modal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PAL addresses multi-modal class-incremental learning (MMCIL) under
  missing-modality scenarios, where subsets of modalities (e.g., visual, text) may
  be absent during training or testing. It combines modality-specific prompt learning
  with analytic learning, using prompt tuning to preserve holistic representations
  and reformulating MMCIL as a Recursive Least-Squares problem for closed-form solutions.
---

# PAL: Prompting Analytic Learning with Missing Modality for Multi-Modal Class-Incremental Learning

## Quick Facts
- **arXiv ID:** 2501.09352
- **Source URL:** https://arxiv.org/abs/2501.09352
- **Reference count:** 40
- **Key outcome:** PAL achieves state-of-the-art MMCIL performance with 72.59% accuracy on UPMC-Food101 under 70% missing modality and 4.92% forgetting rate

## Executive Summary
PAL addresses multi-modal class-incremental learning (MMCIL) under missing-modality scenarios where image or text inputs may be absent during training or testing. The method combines modality-specific prompt learning with analytic learning via Recursive Least Squares (RLS). PAL uses prompt tuning to preserve holistic representations and reformulates MMCIL as an RLS problem for closed-form solutions. This approach mitigates underfitting from frozen backbones and reduces forgetting. Experiments on UPMC-Food101 and N24News datasets show PAL outperforms baselines like ACIL and RebQ.

## Method Summary
PAL tackles MMCIL with missing modalities through a two-stage training framework. First, modality-specific prompt pools (Pv for images, Pt for text) are trained via backpropagation to compensate for missing information and maintain holistic representations. The prompt module uses attention-based selection from pools of size 128 to prepend learnable tokens to input sequences. Second, an analytic learning (AL) module computes classifier weights using Recursive Least Squares, providing exemplar-free incremental learning. The frozen ViLT backbone fuses modalities from the first layer, and missing modalities are handled with dummy inputs. The method achieves this through a reconstruction loss that trains available prompts to approximate missing modality query embeddings.

## Key Results
- Achieves 72.59% accuracy on UPMC-Food101 under 70% missing modality conditions
- Maintains 4.92% forgetting rate, outperforming baselines significantly
- Prompt pools of size 128 with early insertion (layer 1) maximize modality fusion effectiveness
- RLS-based AL provides exemplar-free forgetting mitigation when combined with prompts

## Why This Works (Mechanism)

### Mechanism 1
Modality-specific prompts compensate for missing modalities by reconstructing holistic representations. Two separate prompt pools (Pv for image, Pt for text) store modality-specific knowledge. When a modality is missing, a reconstruction loss (Lr) trains the available prompt to approximate the missing modality's query embedding, enabling the frozen backbone to still produce meaningful joint representations. Core assumption: Missing modality information can be approximately reconstructed from available modalities via learned prompt weights. Evidence: [abstract] "modality-specific prompts to compensate for missing information" and [section III.C] construction of missing-modality counterparts with dummy inputs. Break condition: If modalities share minimal mutual information, reconstruction will fail.

### Mechanism 2
Recursive Least-Squares (RLS) reformulation provides an exact analytical solution equivalent to joint training on all tasks. Theorem 1 shows that classifier weights Wk can be updated using only current task data plus a correlation matrix Rk, without storing exemplars. Rk captures all previous task statistics in a fixed-size matrix. Core assumption: The backbone feature extractor remains adequate for all tasks (frozen), so only the linear classifier needs updating. Evidence: [section III.D.2] "MMCIL becomes equivalent to its joint training counterpart" and [Theorem 1 proof] using Woodbury identity for recursive Rk update. Break condition: If backbone features are insufficient for novel classes, linear readout cannot compensate.

### Mechanism 3
Two-stage training (BP then AL) alleviates under-fitting while preserving forgetting resistance. Stage 1 updates prompts via BP to adapt frozen backbone to new distributions. Stage 2 freezes everything and applies AL to compute optimal classifier weights. Prompts provide plasticity; AL provides stability. Core assumption: Prompts can sufficiently modify backbone behavior without updating backbone weights. Evidence: [section III.D intro] BP training to alleviate underfitting and [Table III] showing AL module critical for performance (72.59% vs 59.94% accuracy). Break condition: If tasks require fundamentally different feature hierarchies, prompt tuning alone is insufficient.

## Foundational Learning

- **Concept: Recursive Least Squares**
  - Why needed here: Core mathematical framework enabling exemplar-free incremental learning. Without understanding RLS, the AL module appears as a black box.
  - Quick check question: Can you derive why Rk = Rk-1 - Rk-1*Hk^T*(Hk*Rk-1*Hk^T + I)^-1*Hk*Rk-1 preserves all prior task information?

- **Concept: Prompt Tuning in Transformers**
  - Why needed here: PAL uses prompt pools with attention-based selection. Understanding how prompts steer frozen transformer outputs is essential.
  - Quick check question: How does prepending learnable tokens to input sequences modify what a frozen transformer attends to?

- **Concept: Multi-Modal Fusion in ViLT**
  - Why needed here: The backbone fuses image and text tokens from layer 1. Missing modality handling depends on understanding this early fusion.
  - Quick check question: Why does ViLT fuse modalities from the first layer rather than using separate encoders?

## Architecture Onboarding

- **Component map:** Input → Tokenize image/text → Query embeddings from F → Select prompts via cosine similarity → Prepend prompts → F outputs joint embedding → Up-sample → AL classifier

- **Critical path:** Input → Tokenize image/text → Query embeddings from frozen ViLT → Select prompts via cosine similarity (Eq. 2) → Prepend prompts → F outputs joint embedding → Up-sampling layer → AL classifier

- **Design tradeoffs:**
  - Prompt pool size: Smaller pools forget more (Figure 7); larger pools plateau at N=128
  - Prompt insertion depth: Earlier layers more effective due to early fusion (Figure 6)
  - Up-sampling size: 15k optimal; 20k shows degradation (Table IV)
  - Assumption: Prompt length 8 determined empirically; longer prompts add noise

- **Failure signatures:**
  - High FG with stable Acc → Prompt pools too small or poorly initialized
  - Low Acc on new tasks → Up-sampling size insufficient or regularization too high
  - Missing reconstruction loss → Prompts don't generalize across missing modality cases

- **First 3 experiments:**
  1. **Baseline sanity check:** Run PAL on complete modality data (η=0%) on UPMC-Food101; expect Acc >80%. If lower, verify backbone weights loaded correctly.
  2. **Prompt ablation:** Compare modality-specific pools vs. single shared pool vs. no prompts (Table III) at η=70% missing-both. Target: specific pools should outperform by >2% Acc.
  3. **Missing rate sweep:** Test η ∈ {10%, 30%, 50%, 70%, 90%} across missing-text/missing-image/missing-both. Verify graceful degradation curve matches Figure 4-5 patterns; sharp drops indicate prompt selection failure.

## Open Questions the Paper Calls Out
- Can the PAL framework effectively scale to tri-modal or higher-order multi-modal incremental learning tasks?
- What are the computational and memory constraints of the Recursive Least-Squares (RLS) module in large-step scenarios?
- How does the frozen backbone constraint impact the model's ability to learn features for entirely new domains in later incremental steps?

## Limitations
- Prompt pool initialization strategy not specified beyond dimensions (N=128, L=8), potentially leading to different local minima
- RLS formulation assumes frozen backbone features remain sufficient, but no validation of feature stability across incremental steps
- Training epochs for BP stage unspecified ("multiple epochs"), creating reproducibility uncertainty

## Confidence

**High Confidence:**
- PAL's two-stage BP-then-AL framework reduces underfitting compared to frozen-only approaches
- Prompt pools improve performance when missing modalities exceed 50% missing rate
- RLS-based AL provides exemplar-free forgetting mitigation when combined with prompts

**Medium Confidence:**
- Modality-specific prompts outperform shared prompts for reconstruction
- RLS update preserves all prior task information without exemplar storage
- Early prompt insertion (layer 1) maximizes modality fusion effectiveness

**Low Confidence:**
- RLS equivalence to joint training holds for all ViLT backbone configurations

## Next Checks

1. **Feature stability validation:** Measure cosine similarity between features from task 1 vs. task 10 for frozen ViLT backbone to confirm RLS assumption validity.

2. **Prompt reconstruction fidelity:** Compare query embeddings from complete vs. reconstructed modalities during testing; correlation below 0.8 indicates prompt failure.

3. **Missing rate co-occurrence analysis:** Test PAL with correlated missing modalities (e.g., 70% both missing together vs. independent 70% per modality) to verify robustness claims.