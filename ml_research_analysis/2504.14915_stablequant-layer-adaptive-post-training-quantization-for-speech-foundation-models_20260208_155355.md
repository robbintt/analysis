---
ver: rpa2
title: 'StableQuant: Layer Adaptive Post-Training Quantization for Speech Foundation
  Models'
arxiv_id: '2504.14915'
source_url: https://arxiv.org/abs/2504.14915
tags:
- quantization
- speech
- performance
- sfms
- stablequant
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of compressing speech foundation
  models (SFMs) using post-training quantization (PTQ), which is less effective on
  SFMs compared to large language models (LLMs) due to SFMs' distinct architecture
  and activation distributions. The authors propose StableQuant, a layer-adaptive
  PTQ algorithm that determines optimal quantization ranges for each layer by analyzing
  scale distributions and performance, using a percentile-based outlier removal technique
  combined with mean squared error (MSE) evaluation.
---

# StableQuant: Layer Adaptive Post-Training Quantization for Speech Foundation Models

## Quick Facts
- **arXiv ID:** 2504.14915
- **Source URL:** https://arxiv.org/abs/2504.14915
- **Reference count:** 29
- **Key outcome:** Layer-adaptive PTQ achieves 4× model size reduction and 2× speedup with <0.3% WER degradation at 8-bit quantization

## Executive Summary
This paper addresses the challenge of compressing speech foundation models (SFMs) using post-training quantization (PTQ), which is less effective on SFMs compared to large language models (LLMs) due to SFMs' distinct architecture and activation distributions. The authors propose StableQuant, a layer-adaptive PTQ algorithm that determines optimal quantization ranges for each layer by analyzing scale distributions and performance, using a percentile-based outlier removal technique combined with mean squared error (MSE) evaluation. Evaluated on HuBERT and wav2vec2.0 for automatic speech recognition, StableQuant achieves up to 4× model size reduction and 2× inference speed improvement with less than 0.3% word error rate (WER) degradation at 8-bit quantization, outperforming traditional PTQ methods and matching QAT performance without requiring fine-tuning.

## Method Summary
StableQuant employs a two-stage algorithm for layer-adaptive post-training quantization. Stage 1 identifies sensitive layers by applying naive percentile quantization to each layer individually and measuring WER degradation against a threshold. Stage 2 performs global percentile search with MSE-based scale calibration on flagged layers, optimizing a single percentile value across all sensitive layers to minimize WER. The method uses symmetric quantization and is specifically designed to handle the wide activation distributions in CNN feature extractors of SFMs, which can be up to 100× wider than Transformer layers.

## Key Results
- Achieves 4× model size reduction and 2× inference speedup with <0.3% WER degradation at W8A8 quantization
- Outperforms traditional PTQ methods (Percentile, MSE alone) on both HuBERT and wav2vec2.0
- Matches QAT performance without requiring fine-tuning or additional training data
- Demonstrates effectiveness on widely-used SFMs (HuBERT-Large, wav2vec2.0) for ASR tasks

## Why This Works (Mechanism)

### Mechanism 1: Layer-wise Sensitivity Detection via Threshold-based Screening
The algorithm applies naive percentile quantization to each layer individually, measures WER delta against threshold γ (0.25%), and flags layers where Δ > γ for special treatment. Evidence shows CNN feature extractors are significantly more sensitive to quantization than Transformer layers, with all-layers W8A8 quantization causing 95.28% WER vs 2.78% baseline for wav2vec2.0.

### Mechanism 2: Percentile-based Outlier Clipping Before MSE Calibration
For flagged layers, the algorithm clips activation histograms at percentile p (searched over [0, 0.5]), then computes MSE across clipped distribution to select optimal scale factor. This addresses CNN activations containing extreme outliers (up to 100× wider dynamic range than Transformers). StableQuant at W8A8 achieves 2.35% WER for HuBERT vs 2.52% (Percentile) and 2.56% (MSE alone).

### Mechanism 3: Global Percentile Search with ASR Evaluation Feedback
Stage 2 evaluates each candidate percentile by quantizing all flagged layers with that percentile, running full ASR inference on development set, and selecting popt = argmin(WER). This couples layer-wise preprocessing with task-level feedback. HuBERT converges to popt = 0.2 while wav2vec2.0 converges to popt = 0.11 at W8A8.

## Foundational Learning

- **Concept: Symmetric vs. Asymmetric Quantization**
  - Why needed: StableQuant uses symmetric quantization (z=0), simplifying scale factor search. Understanding this constraint is necessary to interpret why only s is optimized.
  - Quick check: If activations have non-zero mean distribution, would symmetric quantization introduce systematic bias? How would you detect this in MSE calibration?

- **Concept: Activation Distribution Heterogeneity Across Architectures**
  - Why needed: Core insight is that CNN feature extractors in SFMs have fundamentally different activation statistics than Transformer layers. This motivates layer-adaptive approach.
  - Quick check: Given Figure 1a showing CNN layers with varying dynamic ranges, what statistical test would you run to determine if two adjacent layers require different quantization strategies?

- **Concept: Calibration Data Representativeness**
  - Why needed: StableQuant relies on dev-clean for both calibration and validation. If this subset doesn't cover inference distribution, quantization parameters may be suboptimal.
  - Quick check: What diagnostics would you add to detect distribution shift between calibration data and production inputs? How would this affect confidence in selected popt?

## Architecture Onboarding

- **Component map:** Pre-trained SFM + Calibration data → Layer Sensitivity Screening → Global Percentile Search → Quantized model with layer-adaptive scales

- **Critical path:** Stage 1 screening determines which layers receive outlier clipping; Stage 2 percentile search is compute bottleneck (requires |P| × full ASR evaluations). For HuBERT/wav2vec2.0, this is ~50 ASR runs on dev-clean.

- **Design tradeoffs:** Global vs. per-layer percentile (global is simpler/faster but may be suboptimal); threshold γ tuning (lower γ flags more layers for clipping); calibration set size (larger improves reliability but increases search time).

- **Failure signatures:** WER >1% at W8A8 indicates conv layers not being flagged; popt = 0.0 suggests outliers not the issue or calibration data doesn't exhibit outlier-heavy distributions; WER >50% at W6A6/W4A4 is expected (method optimized for W8A8).

- **First 3 experiments:**
  1. Reproduce layer sensitivity analysis: Apply naive quantization layer-by-layer to target SFM, plot ΔWER per layer. Verify early conv layers show highest sensitivity.
  2. Ablate percentile search resolution: Compare popt found at 0.01 vs 0.05 vs 0.1 intervals. Quantify WER difference vs search time reduction.
  3. Test generalization across domains: Train/calibrate on LibriSpeech dev-clean, evaluate on out-of-domain speech (noisy conditions, different speakers). If WER degradation increases significantly, selected popt may not generalize.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can advanced quantization techniques be developed to achieve viable performance for SFMs at sub-8-bit precision (e.g., W4A4)?
- **Basis in paper:** [explicit] The authors note that "quantization with W4A4 still shows high-performance degradation due to the inherent complexity of speech signals" despite their proposed method.
- **Why unresolved:** The current layer-adaptive scaling and outlier removal strategies are insufficient to handle precision loss at 4 bits without massive accuracy degradation.
- **What evidence would resolve it:** A study demonstrating W4A4 quantization on SFMs with WER degradation comparable to W8A8 results (e.g., <1% degradation).

### Open Question 2
- **Question:** How does StableQuant perform on non-ASR downstream tasks such as speech enhancement or generation?
- **Basis in paper:** [inferred] The introduction highlights that SFMs are versatile for "recognition, enhancement, and generation," yet experimental evaluation is restricted solely to ASR tasks.
- **Why unresolved:** The method optimizes layers based on WER; it is unclear if MSE-based calibration and outlier clipping strategies are optimal for signal reconstruction tasks required in enhancement or generation.
- **What evidence would resolve it:** Evaluation of StableQuant on tasks like speech enhancement to determine if quantization noise alters signal fidelity differently than recognition accuracy.

### Open Question 3
- **Question:** Does the method generalize to encoder-decoder SFM architectures (e.g., Whisper) which possess distinct decoder characteristics?
- **Basis in paper:** [inferred] The paper claims applicability to "widely used speech foundation models" but validates only on encoder-only models.
- **Why unresolved:** Encoder-decoder models have cross-attention layers and different activation distributions in the decoder which may exhibit different outlier behaviors than CNN/Transformer encoder layers.
- **What evidence would resolve it:** Quantization results applying StableQuant to an encoder-decoder model like Whisper, specifically analyzing activation distributions in decoder layers.

## Limitations
- Performance degrades significantly at sub-8-bit precision (W4A4) due to inherent complexity of speech signals
- Assumes a single global percentile suffices across heterogeneous layers, which may be suboptimal for highly divergent activation distributions
- Relies on clean, domain-specific calibration data (LibriSpeech dev-clean) that may not generalize to real-world speech conditions

## Confidence

- **High Confidence:** Claims about layer sensitivity differences between CNN feature extractors and Transformer layers are well-supported by empirical evidence (Table I showing 95.28% vs 2.78% WER).
- **Medium Confidence:** Claims about effectiveness of global percentile search versus per-layer optimization are plausible but lack direct ablation studies.
- **Low Confidence:** Claims about optimality of γ=0.25% threshold and popt values without domain adaptation guidance are not well-supported.

## Next Checks
1. Apply StableQuant calibrated on LibriSpeech dev-clean to evaluate on noisy telephone speech, accented speech, or out-of-domain languages. Measure WER degradation and determine if popt requires domain-specific tuning.

2. For the three CNN layers identified as sensitive, perform per-layer percentile optimization instead of global search. Compare WER, popt values, and computational cost to validate whether global optimization assumption holds.

3. For layers where clipping is applied, analyze distribution of clipped activations before and after quantization. Determine whether clipped range corresponds to rare but critical phonetic features versus redundant noise to validate core assumption about outliers.