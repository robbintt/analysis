---
ver: rpa2
title: Representation Calibration and Uncertainty Guidance for Class-Incremental Learning
  based on Vision Language Model
arxiv_id: '2512.09441'
source_url: https://arxiv.org/abs/2512.09441
tags:
- learning
- image
- task
- class
- classes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of catastrophic forgetting in class-incremental
  learning for image classification. The authors propose a two-stage framework based
  on vision-language models.
---

# Representation Calibration and Uncertainty Guidance for Class-Incremental Learning based on Vision Language Model

## Quick Facts
- **arXiv ID:** 2512.09441
- **Source URL:** https://arxiv.org/abs/2512.09441
- **Reference count:** 40
- **Primary result:** Achieves state-of-the-art accuracy in exemplar-free class-incremental learning, improving over strong baselines by up to 7% on multiple datasets.

## Executive Summary
This paper addresses catastrophic forgetting in class-incremental learning by proposing a two-stage framework based on vision-language models. The approach uses task-specific adapters added to a frozen CLIP image encoder to learn new classes in Stage-I, followed by a Mixture of Projectors (MoP) module in Stage-II to calibrate cross-task representations into a unified space. A novel entropy-based uncertainty selection strategy is used during inference to choose the most appropriate calibrated feature for classification. Extensive experiments on CIFAR100, ImageNet-R, Cars196, and Skin40 demonstrate significant performance improvements over existing methods.

## Method Summary
The framework operates in two stages. Stage-I involves freezing a pre-trained CLIP ViT-B/16 image encoder and adding task-specific adapters (hidden dimension 64) to each Transformer block. These adapters are trained sequentially on new task classes using cross-entropy loss, while saving per-class mean (μ) and covariance (Σ) statistics after each task. Stage-II trains a shared Mixture of Projectors (M=3 projectors) using pseudo-features sampled from the saved Gaussian distributions across all tasks. During inference, the method computes entropy for T calibrated features (one per task) and selects the lowest-entropy feature for final prediction via text similarity.

## Key Results
- Achieves state-of-the-art performance in exemplar-free CIL, with improvements of up to 7% accuracy over strong baselines
- Shows robustness to hyper-parameter choices, with stable performance across different CLIP pre-trained weights
- Demonstrates effectiveness across multiple datasets including CIFAR100, ImageNet-R, Cars196, and Skin40
- The two-stage approach effectively mitigates catastrophic forgetting while maintaining computational efficiency

## Why This Works (Mechanism)

### Mechanism 1: Task-Specific Adapter Isolation
Light-weight adapters added to each Transformer block enable knowledge acquisition without overwriting shared parameters, preserving previously learned representations. Only adapters for the current task are optimized, leaving the pre-trained backbone untouched.

### Mechanism 2: Mixture of Projectors for Cross-Task Calibration
MoP combines M parallel projectors with a gating network to calibrate task-specific embeddings into a unified space. This improves cross-task class separation by aligning task-specific spaces to a shared representation.

### Mechanism 3: Entropy-Based Uncertainty Selection
Prediction uncertainty (entropy) is used to select the most appropriate calibrated feature during inference. The feature with minimum entropy is chosen, based on the hypothesis that in-distribution features yield lower uncertainty.

## Foundational Learning

- **Vision-Language Models and CLIP architecture**: Understanding how CLIP computes image-text similarity and why frozen encoders provide strong initialization is essential for grasping the foundation of this work.
- **Adapter-based parameter-efficient fine-tuning (PEFT)**: Task-specific adapters are the primary mechanism for learning new classes without modifying pre-trained weights, requiring understanding of adapter architecture and injection.
- **Catastrophic forgetting and exemplar-free CIL constraints**: The paper explicitly targets exemplar-free settings where old class data is unavailable, motivating the Gaussian pseudo-feature approach.

## Architecture Onboarding

- **Component map**: Frozen CLIP ViT-B/16 image encoder → Task-specific adapters → Cross-entropy loss → Save μ and Σ per class → MoP (gating network + M projectors) → Entropy-based inference
- **Critical path**: After each task, save per-class Gaussian statistics from Stage-I; after all tasks, train MoP using sampled pseudo-features; during inference, entropy computation across T branches determines feature selection
- **Design tradeoffs**: M (number of projectors) set to 3; higher M increases capacity but risks overfitting. N_p (pseudo-features per class) set to 256; too few reduces diversity, too many may overfit.
- **Failure signatures**: High cross-task confusion if MoP training data poorly approximates true distributions; entropy selection failures on ambiguous images; performance degradation on domains distant from CLIP pre-training data.
- **First 3 experiments**: 1) Reproduce ablation showing contribution of TSA alone, MoP alone, and combined; 2) Test Gaussian approximation quality by comparing with real held-out samples; 3) Evaluate entropy selection on boundary cases with semantically similar classes across tasks.

## Open Questions the Paper Calls Out

### Open Question 1
Can the linear increase in inference cost relative to the number of tasks be mitigated without compromising the cross-task representation calibration? The current inference strategy requires passing the input through all T adapted image encoders, creating a computational bottleneck as task count grows significantly.

### Open Question 2
Does the assumption that class features follow a multivariate Gaussian distribution limit calibration performance on datasets with complex, multi-modal distributions? Real-world visual features may exhibit multi-modal or non-Gaussian structures that a simple Gaussian approximation might fail to capture.

### Open Question 3
How robust is the entropy-based uncertainty guidance when distinguishing between classes with high semantic similarity across different tasks? If an image from Task j is semantically similar to a class in Task i, the Task i adapter might produce a low-entropy but incorrect prediction.

## Limitations

- The two-stage framework requires significant training time, particularly for the MoP calibration stage across all tasks
- The method's performance may degrade on datasets with highly non-Gaussian or multi-modal class distributions due to the Gaussian approximation assumption
- The inference strategy scales linearly with task count, potentially becoming computationally expensive for scenarios with many tasks

## Confidence

- **High**: Adapter-based isolation mechanism is well-established in literature for preventing catastrophic forgetting
- **Medium**: Gaussian approximation assumption for class distributions lacks direct validation on complex distributions
- **Medium**: Entropy-based uncertainty selection is novel but lacks theoretical grounding for task selection

## Next Checks

1. **Gaussian Approximation Validation**: Replace pseudo-features with real held-out samples from old tasks to measure the upper bound of MoP performance and quantify the approximation gap.
2. **Entropy Selection Robustness**: Evaluate the entropy-based inference strategy on boundary cases where images are semantically similar across tasks to identify failure modes and measure selection accuracy.
3. **Adapter Capacity Scaling**: Systematically vary adapter hidden dimension and task count to identify the point where adapter capacity becomes limiting and cross-task confusion emerges.