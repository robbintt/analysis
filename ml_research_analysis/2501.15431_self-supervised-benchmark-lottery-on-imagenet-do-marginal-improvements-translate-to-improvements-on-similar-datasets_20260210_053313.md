---
ver: rpa2
title: 'Self-supervised Benchmark Lottery on ImageNet: Do Marginal Improvements Translate
  to Improvements on Similar Datasets?'
arxiv_id: '2501.15431'
source_url: https://arxiv.org/abs/2501.15431
tags:
- imagenet
- learning
- accuracy
- validation
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates whether marginal improvements in self-supervised
  learning (SSL) models on ImageNet validation translate to better performance on
  similar datasets. The authors evaluate twelve popular SSL frameworks (including
  DINO, Swav, MoCo, Barlow Twins, BYOL, SimCLR, etc.) on five ImageNet variants: ImageNet
  validation, ImageNet ReaL, ImageNet v2, ImageNet Rendition, ImageNet Sketch, and
  ImageNet Adversarial.'
---

# Self-supervised Benchmark Lottery on ImageNet: Do Marginal Improvements Translate to Improvements on Similar Datasets?

## Quick Facts
- arXiv ID: 2501.15431
- Source URL: https://arxiv.org/abs/2501.15431
- Authors: Utku Ozbulak; Esla Timothy Anzaku; Solha Kang; Wesley De Neve; Joris Vankerschaver
- Reference count: 40
- Primary result: Marginal improvements on ImageNet validation do not reliably translate to better performance on similar out-of-distribution datasets

## Executive Summary
This paper investigates whether marginal improvements in self-supervised learning (SSL) models on ImageNet validation translate to better performance on similar datasets. The authors evaluate twelve popular SSL frameworks on five ImageNet variants: ImageNet validation, ImageNet ReaL, ImageNet v2, ImageNet Rendition, ImageNet Sketch, and ImageNet Adversarial. Their key finding is that models performing well on ImageNet validation can experience significant performance drops on out-of-distribution variants, while models like MoCo and Barlow Twins maintain more consistent performance across variants. The authors argue that benchmarking SSL models solely on ImageNet validation is insufficient and advocate for comprehensive evaluation across multiple ImageNet variants. They propose using aggregate metrics (weighted average and geometric mean) to provide a more holistic assessment of model performance.

## Method Summary
The authors evaluate 12 self-supervised learning frameworks (including DINO, Swav, MoCo, Barlow Twins, BYOL, SimCLR, etc.) on five ImageNet variants using ResNet-50 backbones. They freeze pretrained SSL backbones and train linear classification heads on ImageNet training set using SGD with routines from respective papers. The models are then evaluated on all six datasets (validation, ReaL, v2, Rendition, Sketch, and Adversarial) and compute top-1 accuracy, weighted average, and geometric mean across all variants to compare framework rankings