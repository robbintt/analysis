---
ver: rpa2
title: 'BiasConnect: Investigating Bias Interactions in Text-to-Image Models'
arxiv_id: '2503.09763'
source_url: https://arxiv.org/abs/2503.09763
tags:
- bias
- gender
- intersectional
- biases
- causal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: BiasConnect introduces a causal framework to analyze bias interactions
  in text-to-image models, treating biases as interrelated rather than independent.
  Using counterfactual prompts and pairwise causal discovery, it identifies significant
  intersectional bias relationships and quantifies their impact via Intersectional
  Sensitivity scores.
---

# BiasConnect: Investigating Bias Interactions in Text-to-Image Models

## Quick Facts
- arXiv ID: 2503.09763
- Source URL: https://arxiv.org/abs/2503.09763
- Reference count: 40
- Introduces a causal framework for analyzing bias interactions in text-to-image models using counterfactual prompts and pairwise causal discovery

## Executive Summary
BiasConnect introduces a causal framework to analyze bias interactions in text-to-image models, treating biases as interrelated rather than independent. Using counterfactual prompts and pairwise causal discovery, it identifies significant intersectional bias relationships and quantifies their impact via Intersectional Sensitivity scores. Experiments show BiasConnect achieves a 0.696 correlation between estimated and actual post-mitigation bias effects across 26 occupations. The method enables optimal bias mitigation strategy selection, comparative analysis across five TTI models, and reveals discrepancies between generated and real-world bias distributions.

## Method Summary
BiasConnect uses counterfactual prompts to generate causal graphs of bias interactions in text-to-image models. For each base prompt and bias axis, it creates counterfactual variants (e.g., "a male doctor" vs "a female doctor"). Images are generated for all prompts, and attributes are extracted via VQA. Pairwise causal discovery uses chi-square conditional independence testing to identify significant edges. Intersectional Sensitivity scores quantify treatment effects using Wasserstein distance between distributions. The framework was tested on 26 occupations across 8 bias axes using 5 TTI models.

## Key Results
- BiasConnect achieves 0.696 correlation between estimated and actual post-mitigation bias effects across 26 occupations
- Demonstrates model-specific bias architectures: gender as high-outdegree node in SD 1.4 vs. ethnicity in Flux
- Shows stability under moderate image reduction (2.4 edge changes with 16.6% fewer images)
- Reveals discrepancies between generated and real-world bias distributions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Counterfactual prompts function as causal interventions on bias axes.
- Mechanism: By explicitly modifying prompt attributes (e.g., "a male doctor" vs. "a female doctor"), the method isolates the effect of specifying attribute values on downstream distributions. This treats prompt modification as the intervention node in a causal graph, following Pearl's do-calculus framework.
- Core assumption: TTI model behavior under counterfactual prompts approximates behavior under real mitigation strategies like ITI-GEN.
- Evidence anchors:
  - [abstract]: "Our approach leverages a counterfactual-based framework to generate pairwise causal graphs."
  - [Section 3.1]: "Given an input prompt P and bias axes B = [B1, B2, . . . , Bn], we generate counterfactual prompts."
  - [corpus]: Related work (BiasMap, MineTheGap) uses counterfactuals for bias discovery but not causal interaction modeling.
- Break condition: If counterfactual distributions do not approximate post-mitigation distributions, the causal inference fails. The 0.696 correlation suggests moderate but imperfect alignment.

### Mechanism 2
- Claim: Chi-square conditional independence testing identifies statistically significant bias dependencies.
- Mechanism: Contingency tables capture attribute distributions across counterfactual conditions. Low p-values (< 0.0001) indicate that intervention on Bx significantly alters By's distribution, suggesting a causal edge Bx → By.
- Core assumption: Statistical dependence in generated image distributions reflects underlying causal structure in model representations, not just correlation.
- Evidence anchors:
  - [Section 3.3]: "We apply conditional independence testing using the Chi-square (χ2) test, pruning bias pairs with respect to Bx if their p-value exceeds a predefined threshold."
  - [Figure 2]: Shows contingency tables and p-value thresholding for Age → Gender relationship.
  - [corpus]: No direct corpus validation of chi-square for causal discovery in TTI; assumption remains unverified externally.
- Break condition: If dependencies are spurious (confounded by prompt structure rather than model bias), causal graph misrepresents true interactions.

### Mechanism 3
- Claim: Intersectional Sensitivity (IS) quantifies direction and magnitude of treatment effects using Wasserstein distance.
- Mechanism: IS_xy = w_init - w_Bx measures whether intervention on Bx moves By closer to (positive IS) or further from (negative IS) an ideal distribution. This operationalizes Rubin's potential outcomes framework.
- Core assumption: The ideal distribution (uniform or real-world) correctly represents the unbiased target state.
- Evidence anchors:
  - [Section 3.4]: "A positive value (IS_xy > 0) indicates that mitigating Bx improves By, bringing it closer to the ideal distribution."
  - [Table 1]: Correlation of +0.696 between estimated and post-mitigation effects across 26 occupations.
  - [corpus]: No corpus papers validate Wasserstein-based sensitivity metrics for bias interaction.
- Break condition: If ideal distribution is misspecified, IS scores misguide mitigation priorities.

## Foundational Learning

- Concept: **Causal intervention vs. correlation**
  - Why needed here: BiasConnect distinguishes causal effects (intervention changes outcome) from mere correlation (attributes co-occur).
  - Quick check question: If you observe that "doctor" prompts produce mostly male images, can you conclude that gender causes profession? Why or why not?

- Concept: **Wasserstein distance (Earth Mover's Distance)**
  - Why needed here: Quantifies how far an empirical distribution is from an ideal distribution, enabling numeric comparison of bias severity.
  - Quick check question: Two distributions both have 50% male, 50% female. Would their Wasserstein distance from a uniform distribution be the same? What if one has temporal ordering?

- Concept: **Intersectionality in AI systems**
  - Why needed here: Understanding that bias dimensions interact (e.g., gender × race) rather than operate independently.
  - Quick check question: If you mitigate gender bias independently, why might this harm racial diversity?

## Architecture Onboarding

- Component map:
  Prompt Generator -> TTI Model Interface -> VQA Module -> Causal Discovery Engine -> Sensitivity Calculator -> Visualization Layer

- Critical path:
  1. Define bias axes and counterfactual templates for target domain
  2. Generate 48 images per prompt (initial + counterfactuals) — this sample size balances reliability and cost
  3. Run VQA extraction with predefined questions per axis
  4. Build n×n contingency tables for all bias pairs
  5. Apply chi-square pruning to retain only significant edges
  6. Compute IS scores for retained edges using ideal distribution

- Design tradeoffs:
  - **VQA accuracy vs. manual annotation**: VQA introduces ~10-18% error rate; robustness analysis shows 10% error causes 3.65 edge changes. Manual annotation is costly but more reliable.
  - **Sample size vs. computational cost**: 48 images per prompt provides stability (16.6% reduction → only 2.4 edge changes), but 83% reduction breaks the method.
  - **P-value threshold**: 0.0001 is conservative for occupation prompts; TIBET dataset uses 0.05 due to more diverse concepts. Stricter thresholds reduce false positives but may miss subtle interactions.
  - **Ideal distribution choice**: Uniform is default; real-world distributions enable comparison to training data but require external datasets.

- Failure signatures:
  - **Low correlation with post-mitigation effects**: IS estimates diverge from actual mitigation outcomes (correlation < 0.5 would indicate breakdown)
  - **High edge churn under small perturbations**: Adding/removing few images causes major graph restructuring
  - **VQA cascading errors**: Single attribute misclassification propagates through contingency tables
  - **Empty or fully-connected graphs**: Suggests threshold misconfiguration or insufficient image diversity

- First 3 experiments:
  1. **Validate on single occupation with manual annotation**: Generate images for "doctor" across all 8 bias axes, manually label attributes, compare VQA vs. human contingency tables to quantify extraction error.
  2. **Robustness sweep on image count**: Run BiasConnect on 24, 32, 48, 64 images per prompt for 3 occupations; plot edge stability and IS variance to confirm 48-image recommendation.
  3. **Cross-model comparison on shared prompt set**: Run full pipeline on Stable Diffusion 1.4 and Flux for identical occupation prompts; compare global graphs to identify model-specific bias architectures (e.g., which model has gender as high-outdegree node).

## Open Questions the Paper Calls Out

- **Question:** How can the BiasConnect framework be extended to detect and quantify indirect causal effects between bias axes, rather than only pairwise direct interactions?
  - **Basis in paper:** [explicit] The authors state in the Conclusion, "Our current setup does not allow us to reason about indirect causal effects."
  - **Why unresolved:** The current methodology relies on pairwise conditional independence testing, which cannot distinguish between direct causation ($A \to B$) and indirect causation ($A \to C \to B$).
  - **What evidence would resolve it:** A modified framework incorporating structural causal models or mediator analysis to isolate the specific paths of bias propagation.

- **Question:** Can Intersectional Sensitivity scores be aggregated into an optimization objective to determine a single best strategy for mitigating multiple biases simultaneously?
  - **Basis in paper:** [explicit] The Conclusion notes the difficulty to "develop an optimal bias mitigation strategy that utilizes our tool to mitigate multiple biases simultaneously."
  - **Why unresolved:** The current tool estimates pairwise trade-offs (e.g., improving gender might worsen age), but offers no mechanism for balancing these conflicts globally.
  - **What evidence would resolve it:** An algorithm that accepts multiple mitigation targets and outputs a Pareto-optimal intervention based on the aggregate sensitivity scores.

- **Question:** To what extent does the specific phrasing of counterfactual prompts influence the structure of the resulting causal graphs?
  - **Basis in paper:** [inferred] The method relies on fixed prompt templates (e.g., "A photo of a [attribute] [occupation]") to determine causality.
  - **Why unresolved:** TTI models are sensitive to prompt syntax; if a synonym or reordering changes the graph, the "causal" link may be an artifact of token association rather than a robust bias dependency.
  - **What evidence would resolve it:** A stability analysis measuring graph edit distances when using semantic-equivalent but syntactic-different prompt templates.

- **Question:** How does the selection of the "ideal" reference distribution (uniform vs. real-world statistics) impact the validity of the Intersectional Sensitivity scores?
  - **Basis in paper:** [inferred] The experiments predominantly utilize a uniform distribution as the unbiased state, which may not reflect legitimate real-world demographics.
  - **Why unresolved:** Optimizing for a uniform distribution might force the model toward statistically unlikely representations, conflating "bias" with "distributional mismatch."
  - **What evidence would resolve it:** A comparative study correlating mitigation success based on uniform baselines versus baselines derived from census or labor statistics data.

## Limitations
- Causal framework relies on counterfactual prompt distributions as proxies for actual mitigation effects, with only 0.696 correlation to post-mitigation outcomes
- VQA extraction pipeline introduces 10-18% error rates that propagate through contingency tables and affect causal graph structure
- Generalizability across bias axes and occupations remains uncertain, focusing only on 8 specific axes and 26 occupations

## Confidence
- **High Confidence**: Pairwise causal discovery methodology using chi-square conditional independence testing and Wasserstein-based Intersectional Sensitivity scoring
- **Medium Confidence**: Claim that counterfactual prompts adequately approximate post-mitigation distributions (0.696 correlation provides moderate support)
- **Low Confidence**: Generalizability of findings across all possible bias axes and occupations (study limited to 8 axes, 26 occupations)

## Next Checks
1. **Manual Annotation Validation**: Generate images for 3 occupations across all 8 bias axes, manually label attributes, and compare VQA-extracted contingency tables against ground truth to quantify actual error rates and their impact on causal graph structure.

2. **Mitigation Effect Correlation Study**: Apply ITI-GEN or similar mitigation techniques to 10 occupations, measure actual post-mitigation bias distributions, and compute correlation between BiasConnect's IS estimates and observed changes to validate counterfactual-based estimation.

3. **Cross-Cultural Bias Interaction Analysis**: Apply BiasConnect to occupation prompts from different cultural contexts (e.g., Japanese, Arabic, or indigenous occupations) using the same 8 bias axes to identify whether bias interactions are culturally invariant or model-specific.