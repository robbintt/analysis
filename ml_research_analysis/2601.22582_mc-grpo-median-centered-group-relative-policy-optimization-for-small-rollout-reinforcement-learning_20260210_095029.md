---
ver: rpa2
title: 'MC-GRPO: Median-Centered Group Relative Policy Optimization for Small-Rollout
  Reinforcement Learning'
arxiv_id: '2601.22582'
source_url: https://arxiv.org/abs/2601.22582
tags:
- rollout
- mc-grpo
- grpo
- arxiv
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper identifies baseline-induced advantage sign flips as a
  key failure mode in small-rollout GRPO-style RL, where noisy group means cause incorrect
  update directions and degrade accuracy. It proposes Median-Centered GRPO (MC-GRPO),
  which replaces the shared mean baseline with a median baseline computed from G+1
  rollouts and excludes the zero-advantage median sample from backprop, preserving
  the effective update size at G.
---

# MC-GRPO: Median-Centered Group Relative Policy Optimization for Small-Rollout Reinforcement Learning

## Quick Facts
- arXiv ID: 2601.22582
- Source URL: https://arxiv.org/abs/2601.22582
- Reference count: 30
- One-line primary result: MC-GRPO improves small-rollout RL accuracy by up to 7.89% and reduces G=2 to G=8 gap to within 1% on reasoning-focused math tasks.

## Executive Summary
MC-GRPO addresses a critical failure mode in small-rollout GRPO-style RL where noisy group means cause incorrect advantage sign flips and degrade accuracy. The method replaces shared mean baselines with a median baseline computed from G+1 rollouts, excluding the zero-advantage median sample from backpropagation while preserving effective update size at G. This simple change stabilizes within-prompt advantages, particularly effective for small rollout sizes (G∈{2,4}). Across five model-dataset settings, MC-GRPO improves accuracy by up to 7.89% at G=2 and maintains benefits with composite rewards and out-of-distribution contest math problems.

## Method Summary
MC-GRPO modifies the baseline computation in GRPO by using a median-based approach instead of the traditional mean-based baseline. For each group of G rollouts, the method computes a baseline from G+1 samples (the G rollouts plus one additional sample) and uses the median as the baseline. Crucially, the sample with zero advantage (the one used to compute the median) is excluded from backpropagation, maintaining the effective update size at G. This approach stabilizes advantage estimates by reducing sensitivity to outliers and noise inherent in small rollout groups, preventing the sign flips that degrade learning direction.

## Key Results
- MC-GRPO improves accuracy by up to 7.89% at G=2 rollout size compared to standard GRPO
- Reduces the accuracy gap between G=2 and G=8 from typical values to within 1%
- Benefits persist across GRPO, DAPO, and DR-GRPO variants with composite rewards
- Generalizes to out-of-distribution contest math problems

## Why This Works (Mechanism)
Standard GRPO uses group mean baselines that are sensitive to outliers and noise, especially problematic with small rollout sizes. When G is small (2-4), the mean baseline becomes unstable, causing advantages to flip signs incorrectly. This leads to policy updates in wrong directions. MC-GRPO replaces this with a median baseline computed from G+1 samples, which is more robust to outliers. By excluding the zero-advantage median sample from backprop while keeping G effective updates, it maintains stable learning signals without reducing batch size.

## Foundational Learning
- **Policy Gradient Methods**: RL algorithms that directly optimize policies through gradient ascent; needed because MC-GRPO builds on policy gradient foundations and modifies baseline computation.
- **Advantage Estimation**: The difference between Q-values and baselines; critical because MC-GRPO's core innovation is stabilizing advantage estimates in small-rollout settings.
- **Baseline Methods**: Techniques to reduce variance in policy gradients; MC-GRPO specifically improves baseline robustness through median computation.
- **Group Relative Optimization**: GRPO's approach of computing rewards relative to group performance; MC-GRPO modifies this group-level baseline computation.
- **Variance Reduction**: Techniques to make gradient estimates more stable; median baselines are a variance reduction strategy particularly effective for small samples.
- **Robust Statistics**: Using median instead of mean for outlier resistance; fundamental to MC-GRPO's approach to handling noisy small-rollout groups.

## Architecture Onboarding
**Component Map**: Input tokens -> Policy Network -> Sampling (G rollouts + 1) -> Median Baseline Computation -> Advantage Calculation -> Policy Update

**Critical Path**: Token generation → Group sampling (G+1) → Median baseline → Advantage computation → Gradient update

**Design Tradeoffs**: Median baseline provides robustness to outliers but requires G+1 samples instead of G, though the zero-advantage sample is excluded from backprop to maintain effective batch size. This adds computational overhead but dramatically improves stability.

**Failure Signatures**: Large accuracy drops when G is small (2-4), inconsistent performance across different reward compositions, and degraded OOD generalization indicate baseline instability issues that MC-GRPO addresses.

**First Experiments**:
1. Compare accuracy of GRPO vs MC-GRPO at G=2 and G=4 on GSM8K with simple accuracy reward
2. Test median baseline robustness by injecting outliers into small rollout groups
3. Evaluate performance degradation when varying the number of median samples (G vs G+1 vs G+2)

## Open Questions the Paper Calls Out
None

## Limitations
- Analysis confined to reasoning-focused math datasets, limiting generalizability to other domains
- Ablation studies are not comprehensive, leaving some mechanisms underspecified
- Does not explore long-term stability or impact of varying reward compositions beyond presented cases
- Reliance on median baselines may not translate to datasets with different distributional properties

## Confidence
- **High Confidence**: Identification of baseline-induced advantage sign flips as failure mode; efficacy of median baselines in stabilizing updates
- **Medium Confidence**: Robustness across model architectures and reward compositions; generalizability to OOD contest math
- **Low Confidence**: Applicability to non-reasoning or non-math domains; interactions with other RL techniques

## Next Checks
1. Evaluate MC-GRPO on non-math domains (e.g., code generation, dialogue) to assess effectiveness outside reasoning tasks
2. Conduct extended training runs to monitor long-term stability and performance under distributional shifts
3. Test MC-GRPO under complex reward compositions including multi-objective or curriculum-based rewards