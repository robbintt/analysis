---
ver: rpa2
title: Are Prompts All You Need? Evaluating Prompt-Based Large Language Models (LLM)s
  for Software Requirements Classification
arxiv_id: '2509.13868'
source_url: https://arxiv.org/abs/2509.13868
tags:
- requirements
- classification
- task
- llms
- deepseek
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates prompt-based large language models (LLMs)
  for software requirements classification, aiming to reduce reliance on large annotated
  datasets. We evaluate five LLMs across three tasks using zero-shot, few-shot, persona,
  and chain-of-thought prompting techniques.
---

# Are Prompts All You Need? Evaluating Prompt-Based Large Language Models (LLM)s for Software Requirements Classification

## Quick Facts
- arXiv ID: 2509.13868
- Source URL: https://arxiv.org/abs/2509.13868
- Reference count: 40
- Primary result: Prompt-based LLMs match or exceed fine-tuned BERT for software requirements classification using minimal labeled examples

## Executive Summary
This study evaluates prompt-based large language models for software requirements classification across three tasks: FR-NFR binary classification, MC-NFR multi-class classification, and Sec-NonSec binary classification. Five LLMs were tested using zero-shot, few-shot, persona, and chain-of-thought prompting techniques. Results demonstrate that few-shot prompting, particularly when combined with persona framing, consistently outperforms zero-shot approaches and achieves comparable or better performance than a fine-tuned BERT baseline. The approach significantly reduces data annotation requirements while maintaining strong classification accuracy.

## Method Summary
The study evaluated five LLMs (Claude 3 Haiku, DeepSeek-V3, Gemini 2.0 Flash, GPT-4 Turbo, Llama-3.2 3B Instruct) on two datasets: PROMISE NFR (625 requirements across 11 classes) and SecReq (510 requirements). Four prompting techniques were tested: zero-shot, few-shot (1/3/5 examples), persona-augmented, and chain-of-thought. Macro-F1 was used as the primary metric due to class imbalance. Results were compared against a fine-tuned BERT baseline. Statistical significance was assessed using Scott-Knott ESD testing.

## Key Results
- Few-shot prompting significantly outperforms zero-shot approaches across all models and tasks
- Gemini 5FS with persona achieved highest macro-F1 (0.92 on FR-NFR)
- DeepSeek 5FSPCoT achieved 0.94 macro-F1 on MC-NFR
- Prompt-based approaches matched or exceeded fine-tuned BERT using only 5 examples vs. full training data
- Llama showed unreliable few-shot performance, sometimes degrading with added examples

## Why This Works (Mechanism)

### Mechanism 1: In-Context Learning
Few-shot prompting enables in-context learning where labeled examples in the prompt create demonstration patterns that LLMs can analogize from. The model leverages pre-trained representations to map input requirements to class labels by pattern-matching against provided examples, rather than requiring weight updates.

### Mechanism 2: Persona Framing
Role-specific instructions (e.g., "Act as an experienced Requirements Analyst") prime the model to access specialized knowledge clusters and adopt classification criteria consistent with that expertise. This functions as implicit context-setting without explicit rule specification.

### Mechanism 3: Data Substitution via Model Scale
Large pre-trained models encode linguistic patterns, domain knowledge, and reasoning capabilities that can be directed via prompts rather than learned from scratch. This inverts the traditional supervised learning relationship: instead of learning task-specific patterns from labeled data, the model retrieves and applies pre-existing patterns guided by prompt structure.

## Foundational Learning

- **In-Context Learning**: Core capability enabling few-shot prompting to work. Without understanding that LLMs can learn from demonstrations within the prompt window (without gradient updates), the results showing 5-shot matching fine-tuned performance appear anomalous. *Quick check: Given three example requirement classifications in a prompt, can you explain why the model can classify a fourth without any parameter changes?*

- **Macro-F1 vs. Weighted F1 for Imbalanced Data**: The paper explicitly chose macro-F1 because it gives equal weight to minority classes—critical when NFR subtypes have 10-67 instances each. *Quick check: If a model achieves 95% accuracy on a dataset with 90% functional requirements, would macro-F1 likely be higher, lower, or similar? Why?*

- **Requirements Classification Taxonomy**: The three tasks (FR-NFR binary, MC-NFR multi-class, Sec-NonSec binary) represent distinct classification challenges. Understanding why MC-NFR is harder (10 classes, imbalanced) explains why different prompt configurations excel on different tasks. *Quick check: Why might a prompt configuration that works well for binary FR-NFR classification perform differently on 10-class NFR subtype classification?*

## Architecture Onboarding

- **Component map**: Input Requirement → Prompt Template Construction → LLM API Call → Label Extraction → Evaluation
- **Critical path**: 1) Start with 5-shot prompting (not zero-shot)——this was the dominant top performer; 2) Add persona framing (e.g., "Act as an experienced Requirements Analyst"); 3) Reserve CoT for tasks where reasoning transparency matters more than raw performance; 4) Evaluate using macro-F1 on stratified test splits (8:1:1 train/val/test); 5) Use Scott-Knott ESD for statistical comparison (not just raw score comparison)
- **Design tradeoffs**: Closed-source (Gemini, GPT-4) vs. Open-weight (DeepSeek, Llama): Gemini achieved most consistent top performance, but DeepSeek matched or exceeded BERT with privacy/control benefits. Llama showed unreliable few-shot performance. Prompt complexity vs. consistency: Adding persona+CoT sometimes helped but sometimes hurt. Example selection: Paper used random selection for few-shot examples.
- **Failure signatures**: Llama with few-shot: Performance degraded from zero-shot (e.g., FR-NFR: 0.67 ZS → 0.40 1FS)—suggests model cannot reliably leverage in-context examples at this scale. Claude with CoT: Consistent degradation (FR-NFR: 0.73 ZS → 0.50 ZSCoT)—structured reasoning instructions may interfere with this model's default classification behavior. Task-model mismatch: No single model dominated all tasks.
- **First 3 experiments**: 1) Replicate the Gemini 5FS + persona configuration on your own requirements dataset using macro-F1 evaluation. Compare against a simple rule-based classifier or existing supervised model as a sanity check. 2) Test whether the persona instruction alone (without few-shot examples) provides any benefit for your specific domain—this isolates persona effects from few-shot effects. 3) Test both Gemini (best closed-source performer) and DeepSeek (best open-weight performer) on your task. If DeepSeek performs within 5% of Gemini, the open-weight option may be preferable for privacy/deployment reasons.

## Open Questions the Paper Calls Out

1. How do alternative prompt design strategies, specifically different persona framings and non-random few-shot example selection, impact classification accuracy? The current study used random selection for few-shot examples and a generic "Requirements Analyst" persona, leaving the optimization potential of these specific components unexplored.

2. What are the trade-offs in computational efficiency and inference latency between prompt-based LLMs and fine-tuned transformer models in this context? This study focused exclusively on classification performance, acknowledging that latency is a known bottleneck for prompt-based models but did not measure it empirically.

3. Can detailed error analysis of prompt-based LLMs reveal systematic failure patterns to guide targeted prompt engineering? The authors relied on statistical analysis of aggregate scores rather than a qualitative inspection of misclassifications to understand the specific causes of model errors.

## Limitations

- Evaluation uses only two specific software requirements datasets, limiting generalizability to different domains or proprietary data
- Random selection of few-shot examples introduces variability not fully characterized; systematic example selection might yield different outcomes
- Closed-source models (Claude, GPT-4, Gemini) raise reproducibility concerns due to potential API drift over time and lack of transparency into model updates

## Confidence

- **Few-shot prompting effectiveness**: High confidence - Consistent improvement from zero-shot to few-shot across multiple models and tasks with statistically significant results
- **Persona framing benefits**: Medium confidence - Effects were inconsistent across models and tasks, with some models showing degradation
- **Substitution of data with model scale**: Medium confidence - Prompt-based approaches matched or exceeded fine-tuned BERT, but this assumes adequate domain coverage in pre-training

## Next Checks

1. Apply the best-performing configurations (Gemini 5FS with persona, DeepSeek 5FSPCoT) to a different requirements dataset from the same domain but with distinct characteristics to validate cross-dataset generalization

2. Replace random few-shot example selection with systematic approaches (e.g., diversity-based sampling, difficulty-weighted examples) to determine whether example quality impacts performance more than quantity

3. Re-run key experiments with current API versions of Claude, GPT-4, and Gemini to assess whether reported performance differences persist over time, given the documented tendency for LLM outputs to change with updates