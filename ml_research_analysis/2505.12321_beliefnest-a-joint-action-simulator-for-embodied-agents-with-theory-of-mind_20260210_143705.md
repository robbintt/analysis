---
ver: rpa2
title: 'BeliefNest: A Joint Action Simulator for Embodied Agents with Theory of Mind'
arxiv_id: '2505.12321'
source_url: https://arxiv.org/abs/2505.12321
tags:
- belief
- simulator
- agent
- arxiv
- state
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: BeliefNest is a novel open-source simulator designed to enable
  embodied agents to perform collaborative tasks using Theory of Mind. It dynamically
  constructs hierarchical belief simulators within a Minecraft environment, allowing
  agents to explicitly represent nested belief states and predict others' belief-based
  actions.
---

# BeliefNest: A Joint Action Simulator for Embodied Agents with Theory of Mind

## Quick Facts
- arXiv ID: 2505.12321
- Source URL: https://arxiv.org/abs/2505.12321
- Authors: Rikunari Sagara; Koichiro Terao; Naoto Iwahashi
- Reference count: 28
- Primary result: Novel simulator enables embodied agents to perform collaborative tasks using Theory of Mind via hierarchical belief simulators in Minecraft

## Executive Summary
BeliefNest is an open-source simulator that enables embodied agents to perform collaborative tasks by explicitly representing and reasoning about nested belief states using Theory of Mind. Built within Minecraft, it dynamically constructs hierarchical belief simulators where each agent maintains its own perspective, potentially containing false beliefs about the world and others' beliefs. The system provides LLM prompt generation based on these belief states and includes timeline management for hypothetical planning, allowing agents to explore multiple action plans before execution.

## Method Summary
BeliefNest implements Theory of Mind through a hierarchy of isolated Minecraft world instances, where a "real-world simulator" represents ground truth and each agent maintains a "belief simulator" reflecting its perspective. Belief simulators can nest recursively to model what agents believe about others' beliefs. The system uses asynchronous state updates that propagate from shallower to deeper simulators only for observed elements, allowing unobserved elements to retain outdated beliefs. Agents are controlled via LLM-generated actions based on structured prompts created from belief states, with timeline branching enabling hypothetical planning without affecting the real environment.

## Key Results
- Successfully implemented first-order false-belief tasks where agents accurately inferred others' beliefs about object locations
- Demonstrated second-order false-belief reasoning where agents predicted what others believed about a third party's beliefs
- Validated that agents could generate appropriate actions based on inferred belief states through LLM integration

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Belief Representation via Nested Simulators
BeliefNest enables agents to infer others' beliefs that may differ from their own by constructing explicit, hierarchical belief structures. The system implements Theory of Mind as a hierarchy of isolated Minecraft world instances, with each agent maintaining a dedicated belief simulator that reflects its perspective. Belief simulators can nest recursively, allowing agents to model what others can see and believe. State updates propagate asynchronously from shallower to deeper simulators only for observed elements, maintaining false beliefs about unobserved elements.

### Mechanism 2: LLM Prompt Generation from Belief States
The system bridges symbolic belief states with neural action generation via prompt engineering. BeliefNest automatically converts a simulator's state—including position, inventory, and key events—into a text-based representation injected into a user-provided Jinja2 template. This structured prompt gives the LLM a complete, subjective view of the world from a specific agent's perspective, grounding its reasoning in the explicit belief state rather than raw observations.

### Mechanism 3: Hypothetical Planning via Timeline Branching
Each simulator supports timeline management with branching mechanisms, allowing agents to explore consequences of different actions without affecting the real environment. When switched to "control mode," an agent generates actions and creates hypothetical timelines independently. This enables agents to evaluate multiple plans by creating different branches, exploring outcomes, and selecting the best action to perform in the real world.

## Foundational Learning

- **Concept: Theory of Mind (ToM)** - Why needed: BeliefNest's entire architecture is built to implement ToM, requiring understanding that others have beliefs, desires, and intentions different from one's own. Quick check: Can you explain the difference between a first-order and a second-order false belief, and give an example of each?

- **Concept: Minecraft as a Simulation Platform** - Why needed: BeliefNest is implemented within Minecraft, requiring understanding that it provides a discrete, grid-based 3D world with a well-defined API for programmatic control. Quick check: What makes Minecraft a suitable environment for embodied AI research compared to a custom physics engine?

- **Concept: Asynchronous State Updates** - Why needed: The nested simulators do not update in lockstep, requiring understanding of how belief updates propagate asynchronously from source to target simulators. Quick check: In BeliefNest, if a simulator is in "control mode," does it receive updates from the shallower-level simulator? Why or why not?

## Architecture Onboarding

- **Component map**: Real-World Simulator -> Belief Simulators (nested hierarchy) -> Observation Function -> Belief Update Function -> Prompt Generator -> LLM Action Primitives

- **Critical path**: Event in Real-World Simulator → Agent observes via Oi → Belief updated → Update propagates to child Belief Simulator → State diverges from reality → Formatted into prompt → LLM generates action based on false belief

- **Design tradeoffs**: Explicit vs. latent beliefs (interpretable but less expressive), simulation cost (multiple instances expensive), LLM reliance (intelligence depends on external model)

- **Failure signatures**: State desynchronization (belief states don't update correctly), LLM hallucination (actions inconsistent with prompt), performance collapse (system becomes unresponsive)

- **First 3 experiments**:
  1. Run Sally-Anne Task: Execute provided code for first-order false-belief task, observe diamond location differences between real-world and observer's belief about Sally's belief
  2. Modify Prompt Template: Change Jinja2 template to remove "Chest" information, run task again and observe how Sally agent's action fails without crucial belief-state information
  3. Create Two-Agent Collaboration: Set up scenario where one agent must pass item to another behind wall, requiring inference of second agent's location using timeline branching for hand-off planning

## Open Questions the Paper Calls Out

### Open Question 1
How does computational overhead and inference accuracy scale as belief hierarchy depth increases beyond third-order beliefs? The paper validates on first and second-order tasks but doesn't characterize performance for deeper reasoning chains (4th-order beliefs and beyond). Evidence needed: performance benchmarks (latency, memory) and accuracy metrics on ToM tasks requiring recursive beliefs of depth 4, 5, and higher.

### Open Question 2
Can BeliefNest effectively manage dynamic, multi-turn collaborative tasks where belief states must update in real-time through dialogue and physical action? Experiments are limited to single-action prediction scenarios, not continuous collaborations requiring sustained belief tracking. Evidence needed: successful deployment in multi-agent experiment requiring iterative negotiation or cooperative building over extended time horizons.

### Open Question 3
Does explicit hierarchical state representation yield better ToM reasoning performance in LLMs compared to standard prompting techniques without simulator support? No ablation study compares BeliefNest prompt generation against baseline where LLM reasons over raw observation log. Evidence needed: comparative study evaluating success rates on identical false-belief tasks using BeliefNest prompts versus generic textual descriptions.

## Limitations
- Computational expense of running multiple nested Minecraft instances limits scalability to complex scenarios
- Evaluation restricted to controlled false-belief tasks with predefined outcomes, limiting generalizability to open-ended collaboration
- Heavy reliance on LLM reasoning introduces black-box dependency that simulator cannot verify for correctness

## Confidence

- **High confidence**: Hierarchical belief representation mechanism and implementation details are well-specified and technically sound; simulator architecture and API design appear robust
- **Medium confidence**: Effectiveness of LLM prompt generation for Theory of Mind reasoning is supported by experimental results but depends heavily on underlying language model's capabilities
- **Low confidence**: Scalability to complex real-world collaborative tasks and long-term performance implications of multiple nested instances are not characterized

## Next Checks

1. **Scalability test**: Implement multi-agent scenario with 4+ agents and test belief propagation depth up to 3+ levels of nesting; measure memory usage, latency, and correctness of belief states at each level

2. **Generalization benchmark**: Replace controlled false-belief tasks with open-ended collaborative task (e.g., building structure together); evaluate whether agents can successfully coordinate based on inferred beliefs

3. **Alternative LLM comparison**: Run same experiments using different LLMs (e.g., Claude, Llama) to determine if prompt generation mechanism is robust across models or overly tuned to GPT-4o's specific behavior