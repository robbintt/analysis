---
ver: rpa2
title: Calibrated Uncertainty Sampling for Active Learning
arxiv_id: '2510.03162'
source_url: https://arxiv.org/abs/2510.03162
tags:
- calibration
- error
- samples
- uncertainty
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses the problem of improving both calibration
  and generalization in active learning for deep neural networks. It proposes a novel
  acquisition function, CUSAL, that combines calibration error estimation with uncertainty
  sampling using lexicographic ordering: first selecting samples with highest calibration
  error, then by model uncertainty if calibration errors are equal.'
---

# Calibrated Uncertainty Sampling for Active Learning

## Quick Facts
- arXiv ID: 2510.03162
- Source URL: https://arxiv.org/abs/2510.03162
- Authors: Ha Manh Bui; Iliana Maifeld-Carucci; Anqi Liu
- Reference count: 40
- Primary result: CUSAL achieves lower ECE (e.g., 0.030 vs 0.033-0.123 on MNIST) and higher accuracy (e.g., 95.9% vs 83.1%-95.7% on MNIST) across six datasets compared to six strong baselines.

## Executive Summary
This paper addresses the problem of improving both calibration and generalization in active learning for deep neural networks. The authors propose CUSAL, a novel acquisition function that combines calibration error estimation with uncertainty sampling using lexicographic ordering. The method employs a kernel-based calibration error estimator that remains consistent under covariate shift in active learning settings. Theoretically, the authors prove bounds on calibration error estimation and on the expected calibration error for both the unlabeled pool and unseen test data. Empirically, CUSAL outperforms six strong baselines across six datasets (MNIST, Fashion-MNIST, SVHN, CIFAR-10, CIFAR-10-LT, ImageNet) by achieving lower expected calibration error and higher accuracy throughout the active learning rounds.

## Method Summary
CUSAL is a pool-based active learning method that selects samples using a lexicographic acquisition function: first by estimated calibration error (descending), then by model uncertainty (descending) for ties. The method uses a Dirichlet kernel estimator to compute per-sample calibration error on the unlabeled pool using cumulative labeled data, without requiring labels for the pool samples themselves. During each active learning round, CUSAL estimates calibration errors for all unlabeled samples, ranks them by the lexicographic order of (calibration error desc, uncertainty desc), and selects the top-k samples for labeling and model retraining. The theoretical analysis proves consistency of the calibration error estimator under covariate shift and provides bounds on the expected calibration error for both the unlabeled pool and unseen test data.

## Key Results
- CUSAL achieves significantly lower ECE than baselines: 0.030 vs 0.033-0.123 on MNIST, 0.038 vs 0.064-0.142 on SVHN, and 0.025 vs 0.028-0.141 on CIFAR-10.
- CUSAL achieves higher accuracy than baselines: 95.9% vs 83.1%-95.7% on MNIST, 96.6% vs 95.4%-96.4% on SVHN, and 93.5% vs 93.1%-93.4% on CIFAR-10.
- The method shows consistent improvements across diverse datasets including MNIST, Fashion-MNIST, SVHN, CIFAR-10, CIFAR-10-LT, and ImageNet with different model architectures.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Prioritizing samples with the highest estimated calibration error improves both calibration and generalization.
- **Mechanism:** CUSAL uses a lexicographic acquisition function: rank unlabeled pool samples first by estimated calibration error (descending), then by model uncertainty (descending) for ties. Selecting high calibration-error samples forces the model to correct its overconfident or underconfident predictions during retraining.
- **Core assumption:** The kernel-based calibration error estimator remains consistent under the covariate shift introduced by biased AL querying.
- **Evidence anchors:**
  - [abstract] "propose a new AF by estimating calibration errors and query samples with the highest calibration error before leveraging DNN uncertainty."
  - [section 3.2] Eq. 12-13 define the lexicographic order; Algorithm 1 implements the full loop.
  - [corpus] Weak direct support; corpus neighbors discuss uncertainty in AL broadly but not lexicographic CE-uncertainty ordering.
- **Break condition:** If calibration error estimates are biased or highly noisy (e.g., early rounds with small labeled sets, poor kernel bandwidth), sample ranking becomes unreliable and gains diminish.

### Mechanism 2
- **Claim:** The Dirichlet kernel estimator enables consistent calibration-error estimation on the unlabeled pool despite AL-induced covariate shift.
- **Mechanism:** The estimator (Eq. 9) uses labeled examples to kernel-weight neighboring predictions in probability space, approximating E[Y|h(x)] without requiring labels for x. Theorem 4.1 proves pointwise consistency under covariate shift and provides an MSE bound of O(n_t^{-1} b^{-K+1}/2 + b^2).
- **Core assumption:** The kernel bandwidth balances bias and variance appropriately, and the cumulative labeled set sufficiently covers the prediction simplex regions relevant to the unlabeled pool.
- **Evidence anchors:**
  - [section 3.1] Eq. 9-10 define the estimator and Dirichlet kernel.
  - [Theorem 4.1] Proves consistency and MSE bound under AL covariate shift.
  - [corpus] No direct corroboration; corpus papers do not address kernel CE estimation under covariate shift.
- **Break condition:** If bandwidth is too large (oversmoothing) or too small (undersmoothing), or if n_t is very small early in AL, calibration-error estimates become unreliable.

### Mechanism 3
- **Claim:** Training on CUSAL-selected samples yields bounded expected calibration error on both the unlabeled pool and unseen test data.
- **Mechanism:** By selecting high-CE samples and training to minimize calibration loss on them, Theorem 4.2 shows the CE on the remaining pool is bounded by the same ε (Eq. 36). Hoeffding's inequality extends this bound to test data with a concentration term (Eq. 46).
- **Core assumption:** The calibration error function is bounded (L), the estimator is unbiased, and the trained model achieves low CE on queried samples (≤ε).
- **Evidence anchors:**
  - [Theorem 4.2] Formally states bounds for pool and test data.
  - [section 5] Empirical results (Figure 5, Table 4) show lower ECE on unlabeled pool vs. baselines.
  - [corpus] Indirect; corpus emphasizes calibration importance in AL but does not corroborate these specific bounds.
- **Break condition:** If training fails to achieve low CE on queried samples (e.g., optimization issues, insufficient capacity, label noise), the theoretical bounds become loose and empirical benefits may disappear.

## Foundational Learning

- **Concept: Expected Calibration Error (ECE)**
  - Why needed here: CUSAL directly targets ECE minimization; understanding ECE is essential to interpret method goals and results.
  - Quick check question: Given binned confidence scores and accuracies, how do you compute ECE?

- **Concept: Covariate shift in active learning**
  - Why needed here: Theoretical justification (Theorem 4.1) hinges on estimator robustness to distribution shift from biased querying.
  - Quick check question: Why does AL induce covariate shift between labeled and unlabeled pools?

- **Concept: Lexicographic ordering**
  - Why needed here: The acquisition function uses lexicographic order (CE, then uncertainty); understanding this ordering is key to implementation and debugging.
  - Quick check question: If two samples have identical CE estimates, which does CUSAL select?

## Architecture Onboarding

- **Component map:** Kernel CE Estimator -> Lexicographic Acquisition Function -> Active Learning Loop
- **Critical path:** Warm-up: Train on initial S_0. For t=0..T: Estimate CE on U_t via Eq. 9, Rank U_t by (CE desc, uncertainty desc), Select top-k, obtain labels, Update S_{t+1}, U_{t+1}, Retrain model on S_{t+1}, Evaluate on test set.
- **Design tradeoffs:**
  - **Kernel bandwidth (b):** Smaller b reduces bias but increases variance. Paper uses b=0.001 (Appendix B.1). Figure 14 shows robustness for b∈[0.0001,0.001] but oversmoothing at b=1.
  - **Warm-up size (n_0):** Larger n_0 improves early CE estimates but requires more initial labels. Figures 15-16 show consistency across n_0 and query batch sizes.
  - **CE vs. uncertainty prioritization:** Lexicographic order avoids weighting hyperparameters. Ablation (Figure 13) suggests adaptive weighting can help but is hyperparameter-sensitive.
- **Failure signatures:**
  - Early rounds show no improvement: likely n_0 too small or b poorly chosen.
  - CE estimates on U_t do not decrease: check kernel bandwidth, labeled set diversity.
  - Accuracy improves but ECE stagnates/worsens: model may overfit to accuracy; verify CUSAL sample selection is active.
- **First 3 experiments:**
  1. **Reproduce MNIST baseline (Table 1):** n_0=20, k=10, T=40. Compare ECE and accuracy vs. Least-conf and Random. Verify CE estimator quality (Figure 3).
  2. **Ablate kernel bandwidth (Figure 14):** On SVHN, test b∈{0.0001, 0.001, 1}. Monitor CE estimation quality and final ECE/accuracy.
  3. **Test on imbalanced data (CIFAR-10-LT, Table 3):** Compare CUSAL vs. BADGE and Cluster-Ours. Assess robustness to class imbalance and diversity issues.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the computational efficiency of the kernel calibration error estimator be improved to scale more effectively with large unlabeled pools?
- Basis in paper: [explicit] The conclusion states, "Future work includes tackling the computational efficiency limitation of our kernel estimator..."
- Why unresolved: The kernel estimator requires calculations across cumulative labeled samples ($n_t$) and unlabeled pool samples ($m_t$), which becomes computationally expensive as the dataset grows.
- What evidence would resolve it: A modified version of CUSAL using approximation techniques (e.g., random Fourier features or sparse approximations) that achieves similar calibration performance with significantly reduced time complexity on large-scale datasets.

### Open Question 2
- Question: Does the CUSAL acquisition function transfer effectively to the complex output spaces and uncertainty structures of Large Language Models (LLMs)?
- Basis in paper: [explicit] The conclusion explicitly lists "extending it to Large language Models" as a direction for future work.
- Why unresolved: The current method is validated on image classification tasks using a Dirichlet kernel on a probability simplex; it is unclear if this specific calibration estimation approach suits the high-dimensional or generative nature of LLM outputs.
- What evidence would resolve it: Empirical results demonstrating that applying the lexicographic ordering of calibration error and uncertainty to LLM fine-tuning improves safety/calibration metrics (e.g., lower ECE) compared to standard uncertainty sampling.

### Open Question 3
- Question: Is there a theoretically optimal or adaptive method for selecting the kernel bandwidth $b$ under the covariate shift inherent in active learning?
- Basis in paper: [inferred] The authors set the bandwidth $b=0.001$ as a fixed hyperparameter (Appendix B.1) and Theorem 4.1 shows the estimator error bound depends on $b$.
- Why unresolved: While Figure 14 suggests the method is robust to the choice of $b$, a fixed bandwidth may not remain optimal as the distribution of model outputs $h(x)$ shifts throughout the active learning rounds.
- What evidence would resolve it: A theoretical analysis or adaptive algorithm that adjusts $b$ dynamically based on the density of predictions, resulting in statistically significantly lower calibration error bounds than a fixed bandwidth approach.

## Limitations
- **Computational complexity:** The kernel-based CE estimator scales poorly with pool size, becoming prohibitive for large datasets like ImageNet.
- **Covariate shift assumptions:** The theoretical consistency proofs assume the kernel estimator remains robust as AL queries increasingly bias the labeled set distribution.
- **Sensitivity to hyperparameters:** The method relies on fixed kernel bandwidth and warm-up sizes that may not generalize optimally across all datasets and domains.

## Confidence
- **High confidence:** The lexicographic ordering mechanism is clearly defined and empirically validated across six datasets with ablation support.
- **Medium confidence:** The theoretical bounds are mathematically sound but rely on assumptions about estimator consistency under dynamic AL distribution shifts.
- **Low confidence:** The claim of consistent superiority over all baselines is based on comparisons to relatively few strong baselines, with some results showing marginal differences.

## Next Checks
1. **Ablation on kernel bandwidth sensitivity:** Systematically test b ∈ {0.0001, 0.001, 0.01, 0.1, 1.0} on CIFAR-10 to identify the exact threshold where oversmoothing degrades performance.
2. **Distribution shift analysis:** Track the divergence between labeled and unlabeled pool distributions throughout AL rounds using KL divergence or MMD to quantify the validity of Theorem 4.1 assumptions.
3. **Computational scaling limits:** Measure runtime and memory usage as a function of pool size on ImageNet to determine practical scalability limits and identify optimization opportunities.