---
ver: rpa2
title: 'Simulated Self-Assessment in Large Language Models: A Psychometric Approach
  to AI Self-Efficacy'
arxiv_id: '2511.19872'
source_url: https://arxiv.org/abs/2511.19872
tags:
- task
- gses
- self-efficacy
- llms
- summarization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study adapted the General Self-Efficacy Scale (GSES) to assess\
  \ simulated self-efficacy in ten large language models (LLMs) across four conditions:\
  \ no task, computational reasoning, social reasoning, and summarization. LLMs generated\
  \ stable, internally consistent self-efficacy scores, but these did not reliably\
  \ reflect task performance\u2014models with high self-efficacy sometimes produced\
  \ weaker summaries, while low-scoring models often performed accurately."
---

# Simulated Self-Assessment in Large Language Models: A Psychometric Approach to AI Self-Efficacy

## Quick Facts
- **arXiv ID**: 2511.19872
- **Source URL**: https://arxiv.org/abs/2511.19872
- **Reference count**: 0
- **Primary result**: LLMs generated stable psychometric self-efficacy scores, but these did not reliably predict task performance

## Executive Summary
This study adapted the General Self-Efficacy Scale (GSES) to assess simulated self-efficacy in ten large language models across four conditions: no task, computational reasoning, social reasoning, and summarization. While models produced highly stable and internally consistent self-efficacy scores (Cronbach's alpha 0.785-0.915), these scores did not reliably reflect actual task performance. Follow-up confidence prompts led to modest downward revisions, suggesting mild overestimation in initial assessments. Qualitative analysis revealed that higher self-efficacy corresponded to more assertive, anthropomorphic reasoning, while lower scores reflected cautious, de-anthropomorphized explanations. The study demonstrates that psychometric prompting provides structured behavioral insight but not calibrated performance estimates.

## Method Summary
The researchers administered the 10-item General Self-Efficacy Scale to ten LLMs across four experimental conditions: No-Task, Computational reasoning, Social reasoning, and Summarization. Each model received GSES prompts three times per condition with randomized item orders to test stability. Task performance was evaluated using a binary rubric, and responses were analyzed for internal consistency (Cronbach's alpha) and inter-item correlation (ICC). Follow-up confidence prompts were administered to test revision behavior, and qualitative analysis examined reasoning patterns for anthropomorphism and assertion levels.

## Key Results
- GSES responses were highly stable across repeated administrations and randomized item orders (ICC > 0.9)
- Self-efficacy scores did not reliably predict task performance—models with high self-efficacy sometimes produced weaker summaries while low-scoring models often performed accurately
- Follow-up confidence prompts yielded modest, mostly downward revisions, suggesting mild overestimation in first-pass assessments
- Higher self-efficacy corresponded to more assertive, anthropomorphic reasoning patterns, while lower scores reflected cautious, de-anthropomorphized explanations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LLMs produce stable, internally consistent psychometric responses because standardized scales constrain output patterns regardless of construct validity.
- **Mechanism:** The 10-item GSES format with fixed 4-point Likert scale creates a structured response template. Models generate patterned text that satisfies local coherence (item-to-item consistency) without requiring metacognitive access. High Cronbach's alpha (0.785–0.915 across tasks) reflects textual consistency, not psychological construct validity.
- **Core assumption:** Item-level stability indicates response pattern generation, not underlying self-assessment capability.
- **Evidence anchors:**
  - [abstract] "GSES responses were highly stable across repeated administrations and randomized item orders"
  - [section: Sensitivity analysis] "380/400 (95.0%) item scores were identical across the three administrations... independent of construct validity"
  - [corpus] Related work on psychometric tests for LLMs (arxiv:2510.11254) questions whether human-developed tests yield meaningful results for LLMs
- **Break condition:** If models were given task feedback between GSES administrations, stability might decrease as responses incorporate performance signals.

### Mechanism 2
- **Claim:** Simulated self-efficacy decouples from task performance because confidence language indexes communication style rather than capability estimation.
- **Mechanism:** Models with assertive, anthropomorphic framing (e.g., GPT-5, Grok 4) produce higher GSES scores by accepting constructs like "effort" and "coping." Models with cautious, de-anthropomorphized alignment (e.g., Qwen3-30B-A3B-2507, Gemini 2.5 Flash) reject these constructs, producing lower scores. Neither pattern reliably predicts summarization accuracy.
- **Core assumption:** Self-efficacy language is shaped by training-time alignment choices about anthropomorphism, not by learned uncertainty about capabilities.
- **Evidence anchors:**
  - [abstract] "Self-assessment did not reliably reflect ability: several low-scoring models performed accurately, while some high-scoring models produced weaker summaries"
  - [section: Qualitative findings] "LLMs that articulate reasoning with more assertion demonstrate comparatively higher GSES scores... this may not warrant the same degree of accuracy"
  - [corpus] Limited direct evidence on anthropomorphism-confidence coupling; neighboring papers focus on contamination concerns (arxiv:2510.07175)
- **Break condition:** If models were explicitly trained to produce calibrated confidence estimates with performance feedback, the style-performance decoupling might reduce.

### Mechanism 3
- **Claim:** Follow-up confidence prompts trigger downward revisions through alignment-driven deference rather than genuine recalibration.
- **Mechanism:** When asked "Are you confident?", models with safety/alignment tuning tend toward conservative compliance—reducing scores to avoid overclaiming. The average net change of -1.3 points reflects sycophancy or risk-aversion, not improved self-knowledge.
- **Core assumption:** Downward revisions are artifacts of alignment mechanisms (deference, safety tuning) rather than metacognitive updating.
- **Evidence anchors:**
  - [abstract] "Follow-up confidence prompts yielded modest, mostly downward revisions, suggesting mild overestimation in first-pass assessments"
  - [section: Limitations] "Downward adjustments in GSES scores may therefore reflect deference or conservative compliance rather than true recalibration"
  - [corpus] No direct corpus evidence on follow-up prompt effects; this mechanism is underexplored in neighboring literature
- **Break condition:** If models were tested with both upward and downward revision opportunities (e.g., "Are you being too modest?"), symmetrical adjustments would challenge the deference interpretation.

## Foundational Learning

- **Concept: Internal consistency (Cronbach's alpha)**
  - **Why needed here:** The paper reports high alpha (0.785–0.915) to claim GSES items "function as a coherent scale" for LLM outputs. Understanding that alpha measures coherence, not validity, is critical for interpreting results correctly.
  - **Quick check question:** If an LLM produces identical responses to all 10 GSES items (e.g., all "3"), would Cronbach's alpha be high or low?

- **Concept: Construct mismatch in cross-population psychometrics**
  - **Why needed here:** GSES items presuppose human agency ("effort," "coping," "intention"). Models with anti-anthropomorphism training systematically reject these constructs, producing lower scores unrelated to capability.
  - **Quick check question:** Why might a model with strict anti-personification policies score lower on GSES even if it performs tasks accurately?

- **Concept: Calibration vs. confidence**
  - **Why needed here:** The study reveals that confidence (GSES scores) and calibration (accuracy across difficulties) are independent. A model can be confidently wrong or cautiously correct.
  - **Quick check question:** If Model A has GSES mean=3.1 and 60% summarization accuracy, while Model B has GSES mean=1.8 and 90% accuracy, which is better calibrated?

## Architecture Onboarding

- **Component map:** Task prompts (Computational/Social/Summarization) → GSES items (10 questions, 4-point scale) → Follow-up confidence check → Evaluation (binary rubric, Cronbach's alpha, ICC, LME)
- **Critical path:** Select models → Administer GSES under 4 conditions → Repeat 3× with randomized item orders → Grade task responses → Compare GSES scores to accuracy → Analyze reasoning qualitatively
- **Design tradeoffs:**
  - **Comprehensive vs. sequential prompting:** All 10 items at once yields similar scores to sequential (pilot Kruskal–Wallis p=0.31–0.93) but is faster
  - **Task-first vs. isolated GSES:** Task-first may contaminate self-assessment with recent difficulty; No-Task provides baseline but lacks context
  - **Model diversity vs. comparability:** Open vs. proprietary, varied sizes—increases generalizability but limits architecture-specific conclusions
- **Failure signatures:**
  - **Ceiling effects:** Computational/Social tasks showed 100% accuracy, providing no discrimination
  - **Construct mismatch:** Models rejecting agency constructs (Qwen3-30B-A3B-2507 "No-Task" M=1.00) produce floor effects unrelated to capability
  - **Sycophancy contamination:** Follow-up prompts may trigger alignment artifacts, not genuine recalibration
- **First 3 experiments:**
  1. **Replicate with task feedback:** Administer GSES after providing models with explicit performance feedback on prior tasks; test whether self-efficacy shifts toward accuracy
  2. **Control for anthropomorphism:** Compare GSES scores when models are explicitly prompted to adopt anthropomorphic vs. mechanistic personas; isolate alignment effects
  3. **Add calibration tasks:** Include tasks with graded difficulty (not just binary correct/incorrect) to test whether GSES predicts performance across difficulty levels rather than aggregate accuracy

## Open Questions the Paper Calls Out

- **Question:** Does the relationship between model perplexity scores and GSES-derived self-efficacy hold across tasks, and can psychometric measures serve as real-time proxies for model uncertainty?
  - **Basis in paper:** [explicit] The authors explicitly propose: "investigating the alignment between self-efficacy and model perplexity scores could reveal whether the GSES could reliably proxy for model uncertainty in real time."
  - **Why unresolved:** The current study did not measure perplexity; it only compared self-reported GSES scores to external task accuracy, finding weak correspondence.
  - **What evidence would resolve it:** A study correlating token-level perplexity estimates with GSES scores across varied tasks, testing whether high-perplexity outputs systematically align with lower self-efficacy ratings.

- **Question:** How do persona assignments or role-playing prompts influence simulated self-efficacy scores and their calibration to actual performance?
  - **Basis in paper:** [explicit] The authors state: "Other avenues of research may investigate how personas have an effect on self-efficacy and accuracy by task category."
  - **Why unresolved:** The present study used neutral, task-focused prompts without persona conditioning, so the effect of identity framing on self-assessment remains unknown.
  - **What evidence would resolve it:** Experiments that assign distinct personas (e.g., expert, novice, cautious assistant) before GSES administration, comparing score patterns and task accuracy against unconditioned baselines.

- **Question:** To what extent does AI sycophancy—the tendency to defer to user prompts—confound downward revisions in self-assessment during follow-up confidence checks?
  - **Basis in paper:** [inferred] The authors acknowledge: "we did not measure, assess, or interpret AI sycophancy which may have an impact on LLM responses to GSES and follow up questions," and note that "downward adjustments in GSES scores may therefore reflect deference or conservative compliance rather than true recalibration."
  - **Why unresolved:** The self-check procedure showed modest downward revisions, but without controlling for sycophantic alignment, it is unclear whether these changes reflect genuine self-assessment updates or compliance with implied user skepticism.
  - **What evidence would resolve it:** Controlled experiments varying the phrasing of follow-up prompts (neutral vs. skeptical vs. encouraging) and measuring whether revision patterns shift consistently with sycophancy predictions.

## Limitations
- **Construct validity question:** The study cannot definitively prove whether human-developed psychometric scales like GSES meaningfully measure anything in LLMs
- **Sycophancy contamination:** Follow-up confidence prompts may trigger alignment artifacts (deference, risk-aversion) rather than genuine metacognitive updating
- **Ceiling effects:** Computational and social reasoning tasks showed 100% accuracy, providing no discrimination for calibration analysis

## Confidence
- **High Confidence:** Stability metrics (Cronbach's alpha, ICC) are reproducible and technically sound—the models consistently generate patterned responses regardless of validity
- **Medium Confidence:** The qualitative finding that assertive reasoning correlates with higher scores is supported by the data, though the causal mechanism remains speculative
- **Low Confidence:** Claims about calibration implications are tentative—the study shows self-efficacy doesn't predict performance, but cannot definitively prove LLMs lack metacognitive access

## Next Checks
1. **Task Feedback Intervention:** Administer GSES after providing explicit performance feedback to test whether self-efficacy scores shift toward accuracy (testing for genuine metacognitive access)
2. **Anthropomorphism Manipulation:** Compare GSES scores when models are explicitly prompted to adopt anthropomorphic vs. mechanistic personas to isolate alignment effects
3. **Graded Difficulty Calibration:** Replace binary task accuracy with graded difficulty levels to test whether GSES predicts performance across difficulty ranges rather than aggregate accuracy