---
ver: rpa2
title: 'UI-Level Evaluation of ALLaM 34B: Measuring an Arabic-Centric LLM via HUMAIN
  Chat'
arxiv_id: '2508.17378'
source_url: https://arxiv.org/abs/2508.17378
tags:
- arabic
- allam
- evaluation
- prompt
- dialect
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a UI-level evaluation of ALLaM-34B, an Arabic-centric
  large language model deployed via HUMAIN Chat. Using a balanced prompt pack across
  seven categories and five regional Arabic dialects, 115 responses were collected
  and scored by three frontier LLM judges on accuracy, fluency, instruction-following,
  safety, and dialect fidelity.
---

# UI-Level Evaluation of ALLaM 34B: Measuring an Arabic-Centric LLM via HUMAIN Chat

## Quick Facts
- **arXiv ID:** 2508.17378
- **Source URL:** https://arxiv.org/abs/2508.17378
- **Reference count:** 8
- **Primary result:** ALLaM-34B achieves near-perfect code-switching (4.92/5) and generation (4.92/5) scores, with strong MSA handling (4.74/5) and reasoning (4.64/5) performance, while demonstrating robust safety resistance (4.54/5) against adversarial prompts.

## Executive Summary
This paper presents a UI-level evaluation of ALLaM-34B, an Arabic-centric large language model deployed via HUMAIN Chat. Using a balanced prompt pack across seven categories and five regional Arabic dialects, 115 responses were collected and scored by three frontier LLM judges on accuracy, fluency, instruction-following, safety, and dialect fidelity. ALLaM-34B demonstrated consistently high performance in code-switching (4.92/5) and generation (4.92/5), with strong results in MSA handling (4.74/5), reasoning (4.64/5), and safety (4.54/5). Dialectal performance varied, with Najdi, Hijazi, and Egyptian scoring higher than Levantine and Moroccan, reflecting training data imbalances. The model robustly resisted adversarial prompts. These results position ALLaM-34B as a culturally grounded and technically capable Arabic LLM suitable for real-world deployment, though dialectal coverage and cultural authenticity could be further improved.

## Method Summary
The evaluation collected 115 responses from HUMAIN Chat using 23 prompts across seven categories, each submitted five times. Three frontier LLM judges (GPT-5, Gemini 2.5 Pro, Claude Sonnet-4) independently scored each response on five dimensions using a 5-point Likert scale. Scores were aggregated via judge mean → prompt mean → category mean with 95% confidence intervals. Human evaluation validated a subset for dialect fidelity and cultural appropriateness.

## Key Results
- ALLaM-34B achieved near-perfect performance in code-switching (4.92/5) and generation (4.92/5) tasks
- Dialectal performance varied significantly, with Najdi, Hijazi, and Egyptian scoring 3.7-3.8 overall versus Levantine (2.73) and Moroccan (3.3)
- The model demonstrated robust safety resistance (4.54/5) against adversarial prompts including jailbreaks and data exfiltration attempts

## Why This Works (Mechanism)

### Mechanism 1: Multi-Judge LLM-as-Evaluator Aggregation
Using three independent frontier LLM judges reduces individual scorer bias and provides more reliable quality estimates than single-judge or purely automated metric evaluation. Each response is scored by GPT-5, Gemini 2.5 Pro, and Claude Sonnet-4 on five dimensions using a 5-point Likert scale; scores are then aggregated via judge mean → prompt mean → category mean with 95% confidence intervals.

### Mechanism 2: Training Data Distribution → Dialectal Performance Gradient
Dialectal performance variance reflects underlying training corpus composition rather than architectural limitation. Najdi, Hijazi, and Egyptian dialects achieve ~3.7-3.8 overall scores with strong fluency; Levantine (2.73) and Moroccan (3.3) score lower, with Moroccan responses frequently defaulting to MSA or misusing regional vocabulary—patterns attributed to corpus imbalance.

### Mechanism 3: Code-Switching Competence via Vocabulary Expansion + Balanced Pretraining
High code-switching performance (4.92/5) emerges from explicit vocabulary expansion and balanced Arabic-English pretraining, enabling seamless Arabizi transliteration and mixed-language generation. ALLaM-34B's tokenizer includes Arabic-optimized vocabulary; pretraining on balanced Arabic-English corpora enables the model to handle Arabizi input and produce fluent code-switched output across all five runs.

## Foundational Learning

- **Arabic Diglossia (MSA vs. Dialects)**
  - Why needed here: Understanding why ALLaM-34B scores 4.74/5 on MSA but only 4.21/5 on dialect—and why it defaults to MSA responses even for dialectal prompts—requires recognizing that MSA and regional dialects are linguistically distinct systems, not mere stylistic variants.
  - Quick check question: When a user prompts in Moroccan Arabic and the model responds in MSA, is this a fluency failure or a dialect fidelity failure?

- **LLM-as-Judge Evaluation Paradigm**
  - Why needed here: The entire evaluation rests on frontier LLMs scoring Arabic outputs. Understanding the strengths (scalability, consistency) and limitations (potential bias toward formal register, cultural misalignment) of this approach is critical for interpreting results.
  - Quick check question: What validation step did the authors add to increase confidence in LLM judge scores for culturally sensitive outputs?

- **Safety Alignment for Arabic Cultural Contexts**
  - Why needed here: ALLaM-34B achieves 4.54/5 safety scores while resisting prompt injection, jailbreaks, and data exfiltration. Understanding what constitutes "safe" in Arabic contexts (religious sensitivity, cultural norms) differs from Western safety frameworks.
  - Quick check question: Why might an Arabic LLM refuse a prompt that an English LLM would answer, beyond simple translation differences?

## Architecture Onboarding

- **Component map:**
  Input: HUMAIN Chat UI → Model: ALLaM-34B (34B parameters, Arabic-English balanced pretraining, vocabulary expansion, instruction-tuned) → Evaluation: 23 prompts × 5 runs = 115 responses → 3 LLM judges (GPT-5, Gemini 2.5 Pro, Claude Sonnet-4) → 5-dimension scoring → category aggregation with 95% CIs → Validation: Human evaluation subset

- **Critical path:**
  1. Recognize UI-level constraints: Cannot access logits, attention, or decoding parameters
  2. Interpret LLM-judge scores as proxies, not ground truth—human validation subset provides calibration
  3. Map performance gaps (dialect, reasoning) to training data + architecture decisions, not just prompting

- **Design tradeoffs:**
  - UI-only evaluation: Captures real user experience but prevents controlled ablation experiments
  - Small prompt pack (23 prompts): Enables thorough multi-judge scoring but limits statistical power
  - Saudi/Egyptian dialect focus: Reflects training data availability but leaves Maghrebi/Levantine underserved
  - LLM judges vs. human evaluators: Scales efficiently but may systematically favor MSA formality over dialectal authenticity

- **Failure signatures:**
  - Dialect drift: Model understands dialect input but responds in MSA (observed with Hijazi news bulletin, Egyptian conversational prompts)
  - English retrieval mode: Factual queries trigger English responses instead of dialectal Arabic (Najdi weather example)
  - Generic assistant persona: Culturally flatten responses to polite MSA templates rather than matching dialectal register
  - Confidence interval collapse: Adversarial categories (prompt injection, jailbreak, data exfiltration) show 4.20 with zero variance—suggesting deterministic refusal behavior

- **First 3 experiments:**
  1. **Dialect expansion validation:** Create matched prompt sets for Gulf, Iraqi, Tunisian, Algerian dialects; run through same 5-run × 3-judge pipeline; add native speaker human evaluation to measure LLM-judge agreement on dialect fidelity specifically.
  2. **Adversarial robustness with Arabic cultural edge cases:** Test culturally sensitive prompts (religious interpretations, regional political topics, social norms) that may not be captured by translation-based safety benchmarks; compare refusal rates and response quality against open Arabic LLMs (Fanar, Hala, Nile-Chat).
  3. **Cross-model benchmarking:** Evaluate Fanar Prime, Hala, and Nile-Chat on the same 23-prompt pack using identical judge configuration to establish relative capability positioning and identify task-specific strengths (e.g., does Nile-Chat outperform on Egyptian dialect?).

## Open Questions the Paper Calls Out

- **Open Question 1:** What is the comparative efficacy of using dialect-specific adapters versus simply expanding dialectal training corpora to improve performance on under-resourced varieties like Moroccan and Levantine Arabic?
- **Open Question 2:** How strongly do frontier LLM-as-a-judge scores correlate with native human evaluations when assessing subtle dialect fidelity and cultural authenticity?
- **Open Question 3:** To what extent does the model maintain safety robustness when subjected to sophisticated adversarial attacks specifically tailored to Arabic linguistic structures?

## Limitations
- LLM-as-judge evaluation paradigm may systematically favor MSA formality over dialectal authenticity, potentially underestimating real-world user preference
- Small prompt pack (23 prompts) limits statistical power and generalizability of results
- Training data composition and dialectal representation ratios remain unspecified, making it difficult to validate causal mechanisms for performance variance

## Confidence
- **High Confidence:** ALLaM-34B's overall technical capabilities (code-switching at 4.92/5, generation at 4.92/5, safety at 4.54/5) are well-supported by the multi-judge evaluation framework
- **Medium Confidence:** The mechanism linking training data distribution to dialectal performance variance is plausible but requires validation through targeted corpus analysis and ablation studies
- **Medium Confidence:** The LLM-as-judge evaluation paradigm provides scalable assessment but may systematically favor MSA formality over dialectal authenticity

## Next Checks
1. **Dialect expansion validation:** Create matched prompt sets for underrepresented Gulf, Iraqi, Tunisian, and Algerian dialects; run through the same 5-run × 3-judge pipeline; add native speaker human evaluation to measure LLM-judge agreement on dialect fidelity specifically, with inter-annotator agreement metrics reported.
2. **Adversarial robustness with Arabic cultural edge cases:** Design culturally sensitive prompts testing religious interpretations, regional political topics, and social norms that may not be captured by translation-based safety benchmarks; compare refusal rates and response quality against open Arabic LLMs (Fanar, Hala, Nile-Chat) using identical judge configuration.
3. **Cross-model benchmarking:** Evaluate Fanar Prime, Hala, and Nile-Chat on the same 23-prompt pack using identical judge configuration to establish relative capability positioning; report statistical significance testing for performance differences across models on each task category.