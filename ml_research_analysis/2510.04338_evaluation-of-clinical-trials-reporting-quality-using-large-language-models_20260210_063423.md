---
ver: rpa2
title: Evaluation of Clinical Trials Reporting Quality using Large Language Models
arxiv_id: '2510.04338'
source_url: https://arxiv.org/abs/2510.04338
tags:
- criteria
- criterion
- each
- these
- corpus
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first evaluation of large language models
  (LLMs) for assessing the reporting quality of clinical trial abstracts using CONSORT-ABSTRACT
  standards. The authors create CONSORT-QA, a new evaluation corpus from two existing
  expert-annotated studies covering COVID-19 interventions and depression prevention
  trials.
---

# Evaluation of Clinical Trials Reporting Quality using Large Language Models

## Quick Facts
- arXiv ID: 2510.04338
- Source URL: https://arxiv.org/abs/2510.04338
- Reference count: 20
- First evaluation of LLMs for CONSORT-ABSTRACT reporting quality assessment

## Executive Summary
This paper evaluates large language models' ability to assess clinical trial abstract reporting quality using CONSORT-ABSTRACT standards. The authors create CONSORT-QA, a new evaluation corpus from two existing expert-annotated studies covering COVID-19 interventions and depression prevention trials. Using in-context learning without fine-tuning, they test various LLMs (both general and biomedical) with different prompting strategies including Chain-of-Thought reasoning. The best model, Mixtral-8x22B with 5-shot Chain-of-Thought prompting, achieves 85% accuracy. The study finds that larger models generally perform better, biomedical models do not significantly outperform general models for this task, and Chain-of-Thought explanations improve performance while providing transparency into the model's reasoning process.

## Method Summary
The study uses in-context learning to evaluate LLMs on binary classification tasks for CONSORT-ABSTRACT criteria. Four prompting strategies are tested: 0-shot, few-shot (1/3/5 examples), 1-shot-cot-orig (human explanation), and few-shot-cot (Llama-3-70B-generated explanations). The CONSORT-QA corpus contains 139 RCT abstracts (40 COVID-19, 99 depression prevention) with 4,272 abstract-question pairs. Models are evaluated using micro-average accuracy, with Mixtral-8x22B-Instruct achieving the best performance at 85% accuracy using 5-shot Chain-of-Thought prompting. The approach uses vLLM for inference, with CoT explanations generated via greedy decoding and non-CoT answers selected from top-20 tokens.

## Key Results
- Mixtral-8x22B-Instruct achieves 85% accuracy with 5-shot Chain-of-Thought prompting
- Larger models consistently outperform smaller models across all prompting strategies
- Biomedical models (Meditron, Molmo-7B) do not significantly outperform general models for this task
- Chain-of-Thought explanations improve accuracy by 3-5% and provide reasoning transparency
- 0-shot performance is substantially lower than few-shot approaches, demonstrating the value of in-context examples

## Why This Works (Mechanism)
The task of evaluating CONSORT-ABSTRACT compliance involves recognizing structured reporting elements rather than complex medical reasoning, making it well-suited for large language models' pattern recognition capabilities. The Chain-of-Thought approach improves performance by forcing the model to explicitly reason through each criterion before providing a binary answer, reducing random guessing and increasing transparency.

## Foundational Learning
- **CONSORT-ABSTRACT standards**: Checklist for reporting RCT abstracts; needed to understand evaluation criteria; quick check: verify all 16 criteria from Table 2 are correctly implemented
- **In-context learning**: Prompting strategy using examples without fine-tuning; needed to avoid expensive model adaptation; quick check: confirm 5 examples are held out per corpus for few-shot prompts
- **Chain-of-Thought prompting**: Generate reasoning before answer; needed to improve accuracy and transparency; quick check: verify CoT explanations are within 200 tokens or until newline
- **Micro-average accuracy**: Overall accuracy across all examples; needed for fair comparison across different corpora sizes; quick check: confirm total 4,272 examples are counted correctly
- **vLLM inference**: High-throughput LLM serving; needed for efficient evaluation of multiple models; quick check: verify token limits don't truncate important context

## Architecture Onboarding

**Component Map:**
CONSORT-QA dataset -> Prompt template generator -> vLLM inference engine -> Accuracy calculator

**Critical Path:**
Dataset loading → Prompt template generation (with CoT or without) → vLLM inference → Token selection (yes/no) → Accuracy calculation

**Design Tradeoffs:**
- In-context learning vs. fine-tuning: Chosen for speed and avoiding retraining, but may limit performance compared to specialized fine-tuned models
- General vs. biomedical models: Tested both to determine if medical knowledge helps; results show no significant difference
- Chain-of-Thought vs. direct answering: CoT improves accuracy but increases computational cost and complexity

**Failure Signatures:**
- Hallucinations in CoT explanations (quotes non-existent abstract passages)
- Model generates neither "yes" nor "no" in top-20 tokens (counts as wrong)
- Context length exceeded in few-shot (especially 5-shot) prompts

**3 First Experiments:**
1. Run 0-shot evaluation on Mixtral-8x22B-Instruct to establish baseline performance
2. Implement 1-shot CoT prompting and compare to 0-shot results
3. Test few-shot (3 examples) performance to verify improvement over 0-shot

## Open Questions the Paper Calls Out
None

## Limitations
- Exact prompt templates and instruction wording are not fully specified, requiring reconstruction
- Human-generated Chain-of-Thought explanations are not publicly available, limiting comparison between human and model-generated explanations
- The study doesn't explore fine-tuning approaches that might improve performance beyond in-context learning limits

## Confidence
- **High confidence**: Overall methodology of using in-context learning for CONSORT-ABSTRACT evaluation is sound and reproducible
- **Medium confidence**: Relative performance ranking across different models and prompting strategies is likely robust
- **Medium confidence**: Finding that biomedical models don't outperform general models is plausible given the task's focus on structured reporting elements

## Next Checks
1. Verify prompt template reconstruction by testing with Mixtral-8x22B-Instruct and comparing to reported accuracy range (80-85%)
2. Conduct manual inspection of 50-100 model-generated explanations to quantify hallucination rates and validate the diagnostic check for factual accuracy
3. Test the hypothesis that biomedical models underperform by running experiments with both general and biomedical variants on a subset of the corpus to measure any performance gaps