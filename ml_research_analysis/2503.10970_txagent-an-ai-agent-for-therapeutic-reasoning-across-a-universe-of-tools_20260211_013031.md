---
ver: rpa2
title: 'TxAgent: An AI Agent for Therapeutic Reasoning Across a Universe of Tools'
arxiv_id: '2503.10970'
source_url: https://arxiv.org/abs/2503.10970
tags:
- drug
- reasoning
- tool
- tools
- name
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TxAgent is an AI agent that uses multi-step reasoning and real-time
  biomedical knowledge retrieval across a toolbox of 211 tools to analyze drug interactions,
  contraindications, and patient-specific treatment strategies. It retrieves and synthesizes
  evidence from multiple biomedical sources, assesses interactions between drugs and
  patient conditions, and refines treatment recommendations through iterative reasoning.
---

# TxAgent: An AI Agent for Therapeutic Reasoning Across a Universe of Tools

## Quick Facts
- arXiv ID: 2503.10970
- Source URL: https://arxiv.org/abs/2503.10970
- Authors: Shanghua Gao; Richard Zhu; Zhenglun Kong; Ayush Noori; Xiaorui Su; Curtis Ginder; Theodoros Tsiligkaridis; Marinka Zitnik
- Reference count: 40
- Primary result: TxAgent achieves 92.1% accuracy in open-ended drug reasoning tasks, surpassing GPT-4o by 25.8% and outperforming DeepSeek-R1 (671B) in structured multi-step reasoning.

## Executive Summary
TxAgent is an AI agent that uses multi-step reasoning and real-time biomedical knowledge retrieval across a toolbox of 211 tools to analyze drug interactions, contraindications, and patient-specific treatment strategies. It retrieves and synthesizes evidence from multiple biomedical sources, assesses interactions between drugs and patient conditions, and refines treatment recommendations through iterative reasoning. TxAgent achieves 92.1% accuracy in open-ended drug reasoning tasks, surpassing GPT-4o by 25.8% and outperforming DeepSeek-R1 (671B) in structured multi-step reasoning.

## Method Summary
TxAgent is a fine-tuned Llama-3.1-8B-Instruct model that uses function calling to interact with a universe of 211 biomedical tools (APIs to OpenFDA, Open Targets, etc.). It generates explicit reasoning traces (Thought -> Action -> Observation) before executing tool calls, enabling multi-step logical deduction for personalized treatment. A specialized ToolRAG model dynamically retrieves the most relevant tools based on the current reasoning step, addressing the context window limitations of the LLM.

## Key Results
- TxAgent achieves 92.1% accuracy in open-ended drug reasoning tasks, surpassing GPT-4o by 25.8% and outperforming DeepSeek-R1 (671B) in structured multi-step reasoning.
- It generalizes across drug name variants and descriptions, maintaining a variance of <0.01 between brand, generic, and description-based drug references, exceeding existing tool-use LLMs by over 55%.
- TxAgent achieves 93.8% accuracy on DrugPC benchmark and 86.8% on TreatmentPC benchmark, demonstrating strong performance on personalized treatment reasoning tasks.

## Why This Works (Mechanism)

### Mechanism 1: Tool-Augmented Knowledge Grounding
TxAgent replaces parametric memory with structured function calls to external biomedical APIs, reducing factual errors in drug reasoning. When it detects a knowledge gap, it executes a function call (e.g., `get_indications`) to retrieve real-time data from openFDA or Open Targets, synthesizing the returned text into the final answer.

### Mechanism 2: Iterative Reasoning Trace Generation
The model is fine-tuned to output explicit reasoning traces (Thought -> Action -> Observation), enabling multi-step logical deduction necessary for personalized treatment. This forces the model to decompose complex queries into sequential steps rather than predicting a single answer.

### Mechanism 3: Dynamic Tool Retrieval (ToolRAG)
A specialized retrieval model dynamically selects the top-k relevant tools based on the current reasoning step's "thought" or user query, allowing an 8B parameter model to effectively utilize a toolbox (211 tools) that exceeds its raw context window capacity.

## Foundational Learning

### Concept: Function Calling (Tool Use)
- **Why needed here:** You must understand how to map a natural language intent (e.g., "check side effects") to a structured JSON function call with specific arguments (e.g., `get_adverse_reactions(drug_name="Aspirin")`).
- **Quick check question:** Given a tool definition `get_weather(location, date)`, what is the correct function call format for "Weather in London tomorrow"?

### Concept: Retrieval-Augmented Generation (RAG)
- **Why needed here:** The architecture relies on "ToolRAG" to search the toolset. Understanding vector embeddings and semantic search is required to debug why certain tools are retrieved over others.
- **Quick check question:** If the user asks about "Tylenol" but the tool description only says "Acetaminophen", will a semantic retriever likely find the tool? (Answer: Yes, if embeddings capture synonymy).

### Concept: Biomedical Ontologies (Brand vs. Generic)
- **Why needed here:** The paper highlights high variance in baseline models when switching between brand/generic names. Understanding that "Tylenol" = "Acetaminophen" is crucial for evaluating the DescriptionPC benchmark.
- **Quick check question:** Does the system treat "Bayer" as a drug name or a manufacturer, and how does that affect the `get_dosage` tool?

## Architecture Onboarding

### Component map:
- LLM Core (Llama-3.1-8B-Instruct) -> ToolRAG (gte-Qwen2-1.5B-instruct) -> ToolUniverse (211 Python/API wrappers) -> API Result -> LLM Core

### Critical path:
1. Input Query -> LLM generates "Thought" & "Tool Description"
2. Tool Description -> ToolRAG retrieves Tool Definition
3. LLM generates Function Call JSON -> ToolUniverse executes -> returns API result
4. LLM synthesizes result -> Final Answer or Next Thought

### Design tradeoffs:
- **Speed vs. Accuracy:** Multi-step reasoning significantly increases latency (multiple LLM forwards passes + API calls) compared to single-shot generation, trading speed for the 25%+ accuracy gain.
- **Generalizability vs. Size:** Using an 8B model (TxAgent) instead of a 671B model (DeepSeek-R1) allows for local deployment but requires aggressive fine-tuning (LoRA) to match performance.

### Failure signatures:
- **Tool Hallucination:** The model generates a function call for a tool that does not exist or uses incorrect argument names (mitigated by training data augmentation).
- **Reasoning Loop:** The model generates "Thoughts" that repeat previous steps without progressing (mitigated by the "Finish" tool and trace evaluation during training).

### First 3 experiments:
1. **Verify Zero-Shot Generalization:** Run the model on a drug approved *yesterday* (post-2024) to verify the "real-time retrieval" capability holds for truly novel data.
2. **Ablate Reasoning Depth:** Manually limit the maximum reasoning steps to 1 vs. 5 on the TreatmentPC benchmark to reproduce the performance drop (73.5% vs 86.8%) and understand where the complexity bottleneck lies.
3. **Stress Test ToolRAG:** Input ambiguous descriptions (DescriptionPC style) and check if ToolRAG prioritizes "indication-based" tools over "name-lookup" tools, measuring the retrieval precision.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the integration of non-textual clinical modalities, such as pathology images and structured EHR data, affect TxAgent's ability to generate personalized treatment recommendations compared to its current natural language-only architecture?
- Basis in paper: [explicit] The Discussion states that TxAgent "processes only natural language inputs and does not yet support other modalities such as pathology images, EHR data, or web-based lab results," explicitly identifying this as a limitation and area for future research.

### Open Question 2
- Question: How can uncertainty quantification be effectively integrated into TxAgent's internal knowledge representation to better manage scenarios where tool feedback is ambiguous or conflicting?
- Basis in paper: [explicit] The Discussion identifies that "Uncertainty quantification in TXAGENTâ€™s internal knowledge remains a challenge" and suggests that integrating internal knowledge with tool feedback could enhance flexibility for exploratory tasks.

### Open Question 3
- Question: To what extent do specific gaps in the ToolUniverse (e.g., lack of non-FDA approved therapies or international guidelines) constrain TxAgent's clinical utility, and how does performance degrade as queries move outside the scope of the 211 available tools?
- Basis in paper: [explicit] The Discussion notes that "gaps in TOOL UNIVERSE restrict access to specific data types, limiting its ability to address a broader range of questions."

### Open Question 4
- Question: Do hallucinations or biases present in the teacher models (e.g., GPT-4o) used by the QuestionGen and TraceGen systems propagate into TxAgent's final therapeutic reasoning capabilities?
- Basis in paper: [inferred] The Online Methods describe the TxAgent-Instruct dataset generation process relying on GPT-4o to generate questions, reasoning traces, and tool mappings via multi-agent systems, which inherently risks transferring the teacher model's biases to the student model.

## Limitations
- The architecture relies on external biomedical APIs, but the freshness guarantee or update frequency of these sources is not specified, potentially affecting temporal validity of knowledge.
- ToolRAG may struggle with highly ambiguous or multi-faceted queries, leading to incomplete or incorrect reasoning chains due to retrieval ambiguity.
- Translation to real-world clinical decision support is not demonstrated; the model's iterative reasoning might not account for all practical constraints like insurance formularies or local treatment guidelines.

## Confidence
- **92.1% accuracy in open-ended drug reasoning tasks (vs. GPT-4o +25.8%):** High - supported by direct benchmark comparisons and ablation studies.
- **Dynamic tool retrieval with ToolRAG is effective:** Medium - while demonstrated, the impact of ambiguous queries or API failures on final answers is not fully explored.
- **TxAgent generalizes across drug name variants with <0.01 variance:** High - a direct result of the multi-source knowledge grounding mechanism and explicitly validated.

## Next Checks
1. **Temporal Knowledge Validation:** Deploy TxAgent on a dataset of drugs and interactions approved or updated after the model's training cutoff (post-2024) to verify the "real-time retrieval" capability holds for truly novel biomedical data.
2. **Reasoning Chain Robustness Test:** Conduct a stress test by inputting highly ambiguous clinical scenarios (e.g., "A diabetic patient with hypertension and a penicillin allergy") and measure whether ToolRAG retrieves the most relevant tools for managing the complex interaction.
3. **End-to-End Clinical Workflow Simulation:** Integrate TxAgent into a simulated clinical decision support system and measure its performance not just on accuracy, but on workflow efficiency, user trust, and the ability to handle edge cases like drug shortages or insurance constraints.