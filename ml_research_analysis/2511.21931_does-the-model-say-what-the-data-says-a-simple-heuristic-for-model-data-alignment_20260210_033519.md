---
ver: rpa2
title: Does the Model Say What the Data Says? A Simple Heuristic for Model Data Alignment
arxiv_id: '2511.21931'
source_url: https://arxiv.org/abs/2511.21931
tags:
- data
- feature
- causal
- methods
- alignment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a simple, computationally efficient framework
  to evaluate whether machine learning models align with the underlying data structure
  they learn from. The approach draws inspiration from Rubin's Potential Outcomes
  Framework to quantify how strongly each feature separates two outcome groups in
  binary classification, providing a data-derived baseline for feature importance.
---

# Does the Model Say What the Data Says? A Simple Heuristic for Model Data Alignment

## Quick Facts
- arXiv ID: 2511.21931
- Source URL: https://arxiv.org/abs/2511.21931
- Reference count: 20
- Primary result: A framework to assess model-data alignment by comparing data-derived and model-derived feature importance rankings

## Executive Summary
This paper introduces a computationally efficient framework to evaluate whether machine learning models align with the underlying data structure they learn from. The approach quantifies feature importance using a data-derived baseline inspired by Rubin's Potential Outcomes Framework, then compares this baseline against model-based explanations like decision tree importances and SHAP values. The method provides a practical diagnostic tool for practitioners to validate model behavior against data-driven patterns, addressing the critical question of whether models capture genuine data structure or spurious correlations.

## Method Summary
The framework draws inspiration from Rubin's Potential Outcomes Framework to quantify how strongly each feature separates two outcome groups in binary classification, creating a data-derived baseline for feature importance. This baseline is then compared against model-based explanations such as decision tree importances and SHAP values to assess model-data alignment. The approach provides a systematic way to evaluate whether models learn patterns consistent with the underlying data structure, offering a scalable diagnostic tool that can be applied during model development and validation phases.

## Key Results
- Spearman's rank correlation between data-derived and model-derived feature rankings ranged from 0.607 to 0.952
- Moderate to strong alignment observed between data structure and model learning patterns
- Framework demonstrated on Titanic and Diabetes datasets with binary classification tasks
- Method shows promise as a scalable diagnostic tool for model validation

## Why This Works (Mechanism)
The framework works by establishing a ground truth of feature importance derived directly from the data structure, then measuring how well model explanations align with this ground truth. By using Rubin's Potential Outcomes Framework as inspiration, the method quantifies the actual separation power of each feature between outcome groups, creating an objective baseline. When model-based importance scores (from trees, SHAP, etc.) correlate strongly with this baseline, it indicates the model has captured genuine data patterns rather than artifacts or spurious correlations.

## Foundational Learning
- Rubin's Potential Outcomes Framework - needed to understand counterfactual reasoning for feature importance; quick check: can you explain the fundamental problem of causal inference?
- Spearman's rank correlation - needed to measure ordinal alignment between importance rankings; quick check: can you interpret ρ values and their confidence intervals?
- SHAP values - needed to understand model-agnostic feature importance; quick check: can you explain how SHAP values sum to the model prediction?
- Decision tree feature importance - needed to compare against tree-based model explanations; quick check: can you describe how Gini impurity reduction is calculated?

## Architecture Onboarding
- Component map: Data preprocessing -> Potential outcomes calculation -> Model training -> Feature importance extraction -> Alignment scoring
- Critical path: Feature importance calculation (data-derived baseline) → Model-based importance extraction → Correlation computation
- Design tradeoffs: Simplicity and computational efficiency vs. limited to binary classification
- Failure signatures: Low correlation indicates model may be learning spurious patterns or overfitting
- First experiments:
  1. Compare framework results on clean vs. noisy versions of the same dataset
  2. Test on datasets with known feature importance structure
  3. Evaluate sensitivity to feature scaling and preprocessing choices

## Open Questions the Paper Calls Out
None

## Limitations
- Currently restricted to binary classification tasks, limiting applicability to multi-class problems
- Relies on relatively small, well-structured datasets, raising questions about performance on high-dimensional or imbalanced data
- Does not address causal mediation or distinguish between spurious correlations and genuine causal relationships

## Confidence
- **Medium**: Empirical results demonstrate reasonable rank correlations (0.607 to 0.952) but sample size of datasets is limited
- **Medium**: Theoretical guarantees for generalization are not established
- **Medium**: Absence of negative cases or contradictory examples reduces confidence in universality

## Next Checks
1. Evaluate the framework on larger, more complex datasets with hundreds of features to test scalability and robustness
2. Apply the method to multi-class classification problems using appropriate extensions of the potential outcomes framework
3. Compare the framework's performance against established feature selection methods like mutual information and recursive feature elimination on benchmark datasets