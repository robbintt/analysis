---
ver: rpa2
title: Going beyond explainability in multi-modal stroke outcome prediction models
arxiv_id: '2504.06299'
source_url: https://arxiv.org/abs/2504.06299
tags:
- outcome
- data
- prediction
- explanation
- maps
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study adapts xAI methods Grad-CAM and Occlusion to deep transformation
  models (dTMs) for predicting functional outcomes after stroke using brain imaging
  and tabular patient data. The core idea is to combine interpretable statistical
  models for tabular features with deep learning models for imaging data, achieving
  both state-of-the-art prediction performance and interpretability.
---

# Going beyond explainability in multi-modal stroke outcome prediction models

## Quick Facts
- arXiv ID: 2504.06299
- Source URL: https://arxiv.org/abs/2504.06299
- Reference count: 15
- Predicts functional stroke outcomes with AUC close to 0.8 using combined brain imaging and patient data

## Executive Summary
This study adapts xAI methods Grad-CAM and Occlusion to deep transformation models (dTMs) for predicting functional outcomes after stroke using brain imaging and tabular patient data. The core idea is to combine interpretable statistical models for tabular features with deep learning models for imaging data, achieving both state-of-the-art prediction performance and interpretability. The adapted methods highlight relevant brain regions and enable error analysis.

## Method Summary
The method uses deep transformation models that combine 3D convolutional neural networks for brain imaging with linear transformation terms for tabular patient features. The model outputs a transformation function h(y₀|B,x) = ϑ₀(B) + xᵀβ, where ϑ₀(B) is a deep 3D CNN output and xᵀβ is a linear shift term. The study adapts Grad-CAM and Occlusion explanation methods to work with this transformation function output rather than standard class probabilities. Models are trained using 10-fold cross-validation with 5-model ensembles per fold, and explanation maps are generated through gradient-based attribution or occlusion analysis.

## Key Results
- dTMs achieve AUC values close to 0.8 for stroke outcome prediction
- Most important tabular predictors are functional independence before stroke and NIHSS on admission
- Explanation maps reveal critical brain regions, such as the frontal lobe, which is linked to age and unfavorable outcomes
- Error analysis through explanation map clustering identifies systematic prediction errors in TIA patients

## Why This Works (Mechanism)

### Mechanism 1: Hybrid Transformation Function Decomposition
Separating the transformation function into interpretable (tabular) and complex (imaging) components enables simultaneous interpretability and high performance. The model learns h(y₀|B,x) = ϑ₀(B) + xᵀβ, where ϑ₀(B) is a deep 3D CNN output and xᵀβ is a linear shift term. A predefined latent distribution F_Z (standard logistic) enables interpreting β as log-odds ratios. Core assumption: The transformation function can be cleanly decomposed without significant interaction terms between modalities degrading interpretability.

### Mechanism 2: Gradient Flow Adaptation for Non-Probability Outputs
Grad-CAM can be mathematically adapted when the CNN outputs a transformation function component rather than class probabilities. The gradient ∂p₁/∂Aᵢ is derived via chain rule through σ(ϑ₀(B)), yielding σ(ϑ₀(B))·(1-σ(ϑ₀(B)))·∂ϑ₀(B)/∂Aᵢ. This preserves gradient-based attribution while respecting the transformation function structure. Core assumption: The sigmoid-transformed transformation output maintains gradient signal quality comparable to standard classification heads.

### Mechanism 3: Explanation Map Pattern Clustering for Error Analysis
Clustering explanation maps via t-SNE on pretrained features reveals systematic error modes that can guide model refinement. Explanation maps are encoded using a ResNet-50 pretrained on ImageNet; t-SNE clusters these representations, revealing that dark maps (TIA patients, blurred lesions) correlate with false unfavorable predictions. Core assumption: Pretrained ImageNet features capture relevant visual patterns in explanation maps despite domain shift.

## Foundational Learning

- **Transformation Models vs. Direct Distribution Estimation**: Understanding that dTMs estimate a transformation function h mapping a latent distribution to the outcome distribution (rather than directly estimating P(Y|X)) is essential for interpreting parameters as log-odds ratios. Quick check: If F_Z were changed from standard logistic to standard normal, how would the interpretation of β coefficients change?

- **Gradient-Based Attribution in Classification Networks**: Standard Grad-CAM assumes class-probability outputs; understanding the original formulation clarifies what must change when adapting to transformation function outputs. Quick check: In standard Grad-CAM, why is ReLU applied to the weighted sum of activation maps before upsampling?

- **Deep Ensembling with Weighted Averaging**: The paper uses weighted ensembling of transformation functions, not predictions, which preserves interpretability of tabular parameters while improving robustness. Quick check: Why does averaging transformation functions (h̄ = Σwₘhₘ) preserve interpretability better than averaging final predictions?

## Architecture Onboarding

- **Component map**: 3D brain volume → 3D CNN → ϑ₀(B) (complex intercept); x → Linear layer → xᵀβ (interpretable log-odds); h(y₀|B,x) = ϑ₀(B) + xᵀβ → σ(h) → probability; Gradient backprop or occlusion → 3D explanation map → z-axis averaging → 2D overlay

- **Critical path**: 1) Verify tabular parameter estimates first (SI-LSx baseline) before adding imaging; 2) Validate 3D CNN converges (CI_B model) before multimodal fusion; 3) Check explanation map quality on correctly-predicted cases before error analysis

- **Design tradeoffs**: CIB vs. CIB-LSx: Imaging-only (AUC 0.714) vs. multimodal (AUC 0.812)—tabular features add substantial performance; Grad-CAM vs. Occlusion: Grad-CAM is faster (no forward passes) and smoother; Occlusion is more intuitive but computationally expensive; Ensemble size (m=5): Chosen empirically; larger m increases computation linearly with diminishing returns

- **Failure signatures**: Frontal lobe highlight in CIB model disappearing in CIB-LSx: Confounder (age-related atrophy) absorbed by tabular age feature; Dark explanation maps in TIA patients: Model expects visible lesions; may indicate need for TIA-specific training or stratification; Class imbalance (75 unfavorable / 332 favorable): High specificity (0.795), moderate sensitivity (0.64)—threshold tuning may be needed for clinical use

- **First 3 experiments**: 1) Train SI-LSx model and verify that pre-stroke mRS and NIHSS have positive log-odds ratios with non-overlapping CIs, matching clinical expectations; 2) For the CIB model, generate Grad-CAM maps for 5 correctly-predicted unfavorable outcomes and manually verify that highlighted regions include visible DWI lesions; 3) Compare CIB explanation maps (imaging-only) to CIB-LSx maps (imaging + tabular) for the same patients; confirm that frontal lobe importance decreases when age is included

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would incorporating pretrained foundation imaging models further improve dTM prediction performance beyond the current AUC of ~0.8?
- Basis in paper: [explicit] "In future applications, the incorporation of more data or the use of a pretrained foundation imaging model may further improve prediction performance of dTMs."
- Why unresolved: The current work trains CNNs from scratch on limited data; foundation models trained on large neuroimaging datasets were not evaluated.
- What evidence would resolve it: Train dTMs using pretrained weights from neuroimaging foundation models and compare AUC/NLL against current baselines on the same stroke cohort.

### Open Question 2
- Question: Is the frontal lobe importance in unfavorable outcome predictions truly driven by age-related brain atrophy rather than stroke-specific pathology?
- Basis in paper: [explicit] "We hypothesize that the highlighted frontal lobe in the CIB model is important for predicting an unfavorable outcome due to its correlation with age."
- Why unresolved: The hypothesis is inferred from observation that frontal highlighting disappears when age is added as tabular feature; no direct causal validation was performed.
- What evidence would resolve it: Stratified analysis controlling for age, or explicit brain atrophy quantification correlated with explanation map intensity in the frontal lobe region.

### Open Question 3
- Question: Do the adapted xAI methods generalize reliably to external stroke cohorts from different hospitals?
- Basis in paper: [inferred] Single-center study (University Hospital Zurich only), no external validation mentioned; small imbalanced dataset (75 unfavorable outcomes).
- Why unresolved: Model and explanation map behavior may be specific to local imaging protocols, patient demographics, or dataset artifacts.
- What evidence would resolve it: Apply trained dTMs and adapted Grad-CAM/Occlusion to independent multi-center stroke cohorts and assess whether AUC and explanation map patterns remain consistent.

## Limitations

- Single-center study with 407 patients limits generalizability across institutions and imaging protocols
- Class imbalance (81% favorable outcomes) may influence both performance metrics and explanation map patterns
- Gradient adaptation mechanism for transformation functions lacks extensive empirical validation across diverse prediction errors

## Confidence

- **High Confidence**: AUC values (~0.8) and tabular feature importance (pre-stroke mRS, NIHSS) align with established stroke outcome literature and baseline comparisons
- **Medium Confidence**: Gradient adaptation mechanism appears mathematically sound but lacks extensive empirical validation across diverse prediction errors
- **Low Confidence**: Explanation map clustering for error analysis is innovative but not yet established in the stroke prediction literature

## Next Checks

1. **Confounder Validation**: Systematically test whether frontal lobe importance in explanation maps decreases when age is included as a tabular feature across multiple patient subgroups

2. **Gradient Attribution Verification**: Compare Grad-CAM attributions with occlusion-based attributions on identical correctly-predicted cases to verify gradient-based explanations capture relevant lesion patterns

3. **TIA-Specific Testing**: Train and evaluate separate models for TIA patients to determine if dark explanation maps represent a fundamental limitation or correctable model bias