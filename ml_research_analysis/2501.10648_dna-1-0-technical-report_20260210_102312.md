---
ver: rpa2
title: DNA 1.0 Technical Report
arxiv_id: '2501.10648'
source_url: https://arxiv.org/abs/2501.10648
tags:
- language
- korean
- tasks
- data
- instruct
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DNA 1.0 8B Instruct is a bilingual language model optimized for
  Korean and English tasks. The model is built on Llama 3.1 8B architecture and enhanced
  through continual pre-training with high-quality Korean datasets, supervised fine-tuning,
  SLERP merging with Llama 3.1 8B Instruct, and direct preference optimization.
---

# DNA 1.0 Technical Report

## Quick Facts
- arXiv ID: 2501.10648
- Source URL: https://arxiv.org/abs/2501.10648
- Authors: Jungyup Lee; Jemin Kim; Sang Park; SeungJae Lee
- Reference count: 5
- Primary result: State-of-the-art Korean-English bilingual model (KMMLU 53.26%, KoBEST 83.40%)

## Executive Summary
DNA 1.0 8B Instruct is a bilingual language model optimized for Korean and English tasks through a multi-stage training pipeline. The model builds on Llama 3.1 8B architecture and achieves state-of-the-art performance on Korean-specific benchmarks while maintaining strong English capabilities. Key innovations include SLERP merging for preserving language-specific capabilities, a two-stage DPO approach for skill acquisition and quality refinement, and knowledge distillation from large teacher models using SKLD loss.

## Method Summary
DNA 1.0 8B Instruct is developed through sequential training stages: continual pre-training on 4.7B Korean tokens, supervised fine-tuning on 1.9B tokens, SLERP merging with Llama 3.1 8B Instruct to balance language capabilities, two-stage DPO (offline for reasoning skills, online for quality refinement), and knowledge distillation from Llama 3.1 405B and Qwen2.5 72B using SKLD loss. The model employs GQA attention, supports 128K context length, and achieves 53.26% on KMMLU and 66.64% on MMLU.

## Key Results
- KMMLU: 53.26% (state-of-the-art for Korean benchmarks)
- KoBEST: 83.40% (state-of-the-art for Korean benchmarks)
- MMLU: 66.64% (strong English capability maintenance)
- GSM8K: 80.52% (high arithmetic reasoning performance)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SLERP merging preserves language-specific capabilities from two specialized models better than linear interpolation.
- Mechanism: Spherical Linear Interpolation navigates the curved geometry of high-dimensional weight spaces, maintaining neural pathways responsible for Korean and English processing respectively, rather than averaging them toward a potentially suboptimal midpoint.
- Core assumption: Weight vectors for language-specific processing occupy distinct regions in parameter space that can be selectively preserved through angular interpolation.
- Evidence anchors:
  - [abstract] "This model is then merged with Llama 3.1 8B Instruct via spherical linear interpolation (SLERP)"
  - [section 4.3] "We chose SLERP over simple linear interpolation for its sophisticated handling of weight interpolation in high-dimensional spaces, which is crucial for preserving specialized capabilities of both models"
  - [corpus] Weak direct evidence—Solar Open Technical Report references model merging but without comparative SLERP analysis; Kanana does not employ SLERP-based merging
- Break condition: If weight distributions between source models are highly misaligned (large angular separation), SLERP may produce unstable intermediate representations; validation loss spikes during progressive merging would indicate this.

### Mechanism 2
- Claim: Two-stage DPO (offline then online) separates learning complex skills from output quality optimization.
- Mechanism: Offline DPO first establishes reasoning and factual accuracy using curated preference pairs for hard-to-evaluate tasks (math, coding, ethics). Online DPO then refines output quality using a reward model for relevance, helpfulness, and safety, enabling iterative improvement without requiring fresh human annotation for each cycle.
- Core assumption: Complex reasoning skills can be transferred through static curated datasets, while surface-level quality benefits more from dynamic reward-model-guided optimization.
- Evidence anchors:
  - [section 4] "Offline DPO: Focuses on learning complex skills like reasoning and factual accuracy... Online DPO: Optimizes output quality with a reward model that evaluates relevance, helpfulness, and safety"
  - [section 4.4] "approximately 80,000 training pairs... trained for two epochs using DPO, with a learning rate of 1 × 10−6 and a β hyper-parameter value of 0.1"
  - [corpus] No corpus neighbors employ explicit two-stage offline/online DPO; most report single-stage DPO or PPO-based RLHF
- Break condition: If offline DPO overfits to curated preference patterns without generalizing, online DPO will show high variance in reward model scores; monitor reward distribution drift between stages.

### Mechanism 3
- Claim: Knowledge distillation from large teacher models transfers reasoning capabilities to compact 8B parameters via enriched training signals and output distribution alignment.
- Mechanism: Teacher models (Llama 3.1 405B, Qwen2.5 72B) generate explanation traces and step-by-step reasoning chains. Student model learns through: (1) training on this synthesized reasoning data via SFT pipeline, and (2) aligning output distributions using skew Kullback-Leibler divergence (SKLD) loss with adaptive off-policy adjustment for student-generated outputs.
- Core assumption: Reasoning patterns from 405B+ models can be compressed into 8B models without proportional capability loss when transfer is guided by distributional alignment.
- Evidence anchors:
  - [section 4.5] "This data includes rich signals such as explanation traces, step-by-step thought processes, and other complex instructions"
  - [section 4.5] "achieved using a skew Kullback-Leibler divergence (SKLD) loss and an adaptive off-policy approach designed to enhance the efficiency in utilizing student-generated outputs"
  - [corpus] Thunder-LLM and Kanana both employ knowledge distillation but emphasize data-centric approaches; VARCO-VISION uses similar KD techniques for vision-language tasks
- Break condition: If SKLD loss plateaus while teacher-student accuracy gap remains large, student capacity may be insufficient; check per-layer hidden state divergence to identify bottlenecks.

## Foundational Learning

- Concept: **SLERP (Spherical Linear Interpolation)**
  - Why needed here: Standard linear weight averaging can destroy specialized capabilities; SLERP interpolates along the surface of a hypersphere in weight space.
  - Quick check question: Given two weight vectors with angle θ=60° between them, does SLERP at t=0.5 produce the same result as linear averaging?

- Concept: **Direct Preference Optimization (DPO)**
  - Why needed here: DPO aligns models to human preferences without training a separate reward model or using RL sampling, reducing computational overhead.
  - Quick check question: In DPO, what does the β hyperparameter control, and what happens if β is set too high?

- Concept: **Grouped Query Attention (GQA)**
  - Why needed here: GQA reduces KV-cache memory during inference while maintaining quality between multi-query and multi-head attention extremes.
  - Quick check question: With 32 query heads and 8 KV heads, how many times are KV heads reused per attention layer?

## Architecture Onboarding

- Component map:
```
Llama 3.1 8B base
├── Decoder-only Transformer (32 layers, 4096 dim)
├── RMSNorm (pre-normalization)
├── SwiGLU activation (FFN dim: 14,336)
├── RoPE (θ=500,000, supports 128K context)
└── GQA (32 Q heads / 8 KV heads)

Training pipeline:
CPT on Korean data → SFT → SLERP merge with Llama 3.1 8B Instruct → Offline DPO → Online DPO + KD
```

- Critical path:
  1. CPT quality determines Korean foundation strength
  2. SLERP interpolation ratio (t) controls Korean/English balance per layer
  3. DPO β=0.1 and learning rate 1e-6 are sensitivity points for alignment quality
  4. SKLD loss convergence determines KD success

- Design tradeoffs:
  - 8B size limits reasoning complexity but enables practical deployment
  - SLERP merge avoids catastrophic forgetting but requires two full models in memory during merge
  - Synthetic Korean data expands domain coverage but requires reward-model quality filtering to prevent noise injection
  - Two-stage DPO increases training time but separates skill acquisition from quality refinement

- Failure signatures:
  - Korean benchmark scores improve but English MMLU drops >5%: CPT data ratio or SLERP t-value may favor Korean too heavily
  - DPO reward scores increase but human evaluation degrades: Reward model may be gaming metrics; check for length bias
  - Long-context NIAH accuracy degrades at specific context lengths: RoPE base frequency scaling may have precision issues

- First 3 experiments:
  1. Ablate SLERP t-values (0.3, 0.5, 0.7) on Korean/English benchmark pairs to map the Pareto frontier for language balance.
  2. Run offline-only DPO vs. two-stage DPO comparison on reasoning benchmarks (GSM8K, MATH) to validate the staged approach.
  3. Test KD teacher ablation (405B-only vs. 72B-only vs. ensemble) to identify which teacher contributes most to specific capability gains.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the model's general knowledge capabilities (MMLU, BBH) be improved to match state-of-the-art levels without compromising its superior Korean proficiency?
- Basis in paper: [explicit] The authors explicitly note that the model "did not achieve the highest performance in MMLU and BBH, which indicates there is room for improvement in its general knowledge capabilities."
- Why unresolved: The current training pipeline prioritizes Korean optimization (CPT, SLERP), which may create a trade-off with general knowledge retention.
- What evidence would resolve it: A future iteration demonstrating statistically significant gains in MMLU/BBH while maintaining current Korean benchmark scores (KMMLU, KoBEST).

### Open Question 2
- Question: What specific data interventions are required to bridge the performance gap between arithmetic reasoning (GSM8K) and complex mathematical problem-solving (MATH)?
- Basis in paper: [inferred] The paper highlights a discrepancy where the model leads in GSM8K (80.5%) but lags in MATH (34.9%), attributing this to a lack of "extensive exposure to high-level mathematical training data."
- Why unresolved: It is unclear if simply adding more data is sufficient or if architectural/instruction-tuning changes are needed to handle the "advanced and diverse" nature of the MATH dataset.
- What evidence would resolve it: Ablation studies showing the impact of high-level mathematical synthetic data on MATH scores specifically.

### Open Question 3
- Question: How does the model perform on retrieval and reasoning tasks when utilizing the full 128K context window?
- Basis in paper: [inferred] While Table 1 configures the max sequence length at 131,072 tokens, the evaluation section only confirms "consistent performance across all tested context lengths, up to a maximum of 32K tokens."
- Why unresolved: The model's reliability and attention mechanisms have not been explicitly validated at the architectural limit of 128K, leaving potential degradation at longer contexts unknown.
- What evidence would resolve it: "Needle-In-A-Haystack" or LongBench v2 results explicitly evaluated at context lengths between 32K and 128K.

## Limitations

- SLERP merging lacks comparative ablation studies against linear interpolation methods
- Two-stage DPO methodology lacks empirical validation against single-stage alternatives
- Specific composition of synthetic Korean data is not disclosed, limiting reproducibility assessment

## Confidence

**High Confidence**: The architectural foundation (Llama 3.1 8B base with CPT, SFT, and DPO) is standard and well-validated. The reported English benchmark scores (MMLU 66.64%, GSM8K 80.52%) align with expectations for an 8B model with similar training budgets.

**Medium Confidence**: The KMMLU 53.26% and KoBEST 83.40% scores are internally consistent with the described training methodology, but the lack of dataset details and comparative baselines introduces uncertainty about the absolute significance of these results.

**Low Confidence**: The claimed advantage of SLERP merging over linear interpolation, the two-stage DPO methodology's effectiveness, and the SKLD-based knowledge distillation approach lack sufficient empirical validation to establish their practical benefits.

## Next Checks

1. **SLERP Ablation Study**: Implement and compare linear interpolation (α=0.5) versus SLERP (t=0.5) merging of the CPT+SFT model with Llama 3.1 8B Instruct. Measure both the preservation of Korean capabilities (KMMLU, KoBEST) and English capabilities (MMLU, GSM8K) to quantify the practical impact of the spherical interpolation approach.

2. **DPO Methodology Comparison**: Train three variants: (a) offline DPO only, (b) online DPO only, and (c) the two-stage approach. Evaluate all three on reasoning tasks (GSM8K, MATH) and quality metrics (human evaluation, reward model scores) to empirically validate whether the staged approach provides measurable advantages over simpler alternatives.

3. **Teacher Model Impact Analysis**: Conduct controlled experiments ablating each teacher model (Llama 3.1 405B vs Qwen2.5 72B) and using their ensemble versus individual contributions. Measure impact on specific capabilities (reasoning, factual accuracy, code generation) to identify which teacher model contributes most to observed performance gains and whether the ensemble approach is necessary.