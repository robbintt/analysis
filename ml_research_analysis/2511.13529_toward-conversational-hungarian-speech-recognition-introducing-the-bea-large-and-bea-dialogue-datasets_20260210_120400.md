---
ver: rpa2
title: 'Toward Conversational Hungarian Speech Recognition: Introducing the BEA-Large
  and BEA-Dialogue Datasets'
arxiv_id: '2511.13529'
source_url: https://arxiv.org/abs/2511.13529
tags:
- speech
- spontaneous
- hungarian
- speaker
- bea-dialogue
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces two new Hungarian speech datasets - BEA-Large
  and BEA-Dialogue - constructed from previously unprocessed portions of the BEA corpus.
  BEA-Large extends BEA-Base with 255 hours of spontaneous speech from 433 speakers,
  while BEA-Dialogue provides 85 hours of natural conversations partitioned into speaker-independent
  subsets.
---

# Toward Conversational Hungarian Speech Recognition: Introducing the BEA-Large and BEA-Dialogue Datasets

## Quick Facts
- **arXiv ID:** 2511.13529
- **Source URL:** https://arxiv.org/abs/2511.13529
- **Reference count:** 0
- **Primary result:** New Hungarian datasets BEA-Large (255h spontaneous) and BEA-Dialogue (85h conversational) with reproducible ASR baselines

## Executive Summary
This paper introduces two new Hungarian speech datasets constructed from previously unprocessed portions of the BEA corpus. BEA-Large extends spontaneous speech coverage with 255 hours from 433 speakers, while BEA-Dialogue provides 85 hours of natural conversations with speaker role annotations. The authors establish reproducible baselines using fine-tuned ASR models, achieving word error rates as low as 14.18% on spontaneous speech and 4.8% on repeated speech. These datasets address the critical shortage of Hungarian spontaneous and conversational speech data, advancing Hungarian speech technology and providing a framework for developing similar benchmarks in other low-resource languages.

## Method Summary
The authors created BEA-Large by combining two training subsets (train-114 and train-293) from the BEA corpus, totaling 255 hours of spontaneous speech from 433 speakers. BEA-Dialogue was constructed from conversational recordings segmented into 30-second dialogue units with speaker role annotations (SPK, EXP, DP). For ASR baselines, they fine-tuned NVIDIA's Fast Conformer-CTC Large model on the combined training data using Serialized Output Training with explicit `<sc>` tokens for speaker changes. They compared results against zero-shot Whisper models and evaluated both diarization and transcription accuracy on held-out test sets.

## Key Results
- Fine-tuned Fast Conformer achieved 14.18% WER on spontaneous speech and 4.8% on repeated speech in BEA-Large
- BEA-Dialogue SOT training yielded 82.16% speaker change accuracy on evaluation set
- Diarization experiments achieved error rates between 12.46% and 17.40% DER
- Combined training on train-114 + train-293 outperformed either subset alone, despite train-293 showing mixed results

## Why This Works (Mechanism)

### Mechanism 1
Combining train-114 and train-293 yields lower ASR error rates than either subset alone due to complementary acoustic diversity. The 433 total speakers in combined training expose the model to broader voice characteristics while shared dev/eval splits ensure consistent optimization targets.

### Mechanism 2
Serialized Output Training with explicit `<sc>` tokens enables conversational ASR to learn turn boundaries without separate diarization. The `<sc>` token marks speaker transitions in the output sequence, allowing CTC loss to learn boundary detection while preserving transcription accuracy.

### Mechanism 3
Moderate-scale domain-specific fine-tuning (120M parameters, <250 hours) outperforms zero-shot multilingual models for low-resource languages. The Fast Conformer's English pre-training provides general acoustic representations that adapt well to Hungarian's agglutinative structure during fine-tuning.

## Foundational Learning

- **Word Error Rate (WER) vs. Character Error Rate (CER)**
  - Why needed: The paper reports both; high CER/WER ratio in conversational speech signals character-level errors from disfluencies
  - Quick check: If WER is 20% and CER is 10%, what does this suggest about error types?

- **Speaker Diarization Error Rate (DER)**
  - Why needed: Baseline diarization results (12.46–17.40% DER) establish references for multi-speaker segmentation
  - Quick check: What three error components contribute to DER?

- **Speaker-Independent Evaluation**
  - Why needed: BEA-Dialogue constructs train/dev/eval splits where no speaker appears in multiple roles
  - Quick check: Why might including experiment leaders in training not harm speaker-independent eval?

## Architecture Onboarding

- **Component map:** ASR: Fast Conformer-CTC Large (120M params) → fine-tune on Hungarian; Baseline: Whisper (large-v3, large-v2, medium) → zero-shot; Diarization: Pyannote 3.1 + Sortformer 4spk-v1

- **Critical path:** 1) Extract utterances with timestamps, speaker labels, silence-based segmentation 2) Train Fast Conformer on combined train-114 + train-293 3) Evaluate on dev/eval using WER/CER; on BEA-Dialogue using cpWER/scAcc

- **Design tradeoffs:** Including experiment leader voices in training (+data volume vs. potential speaker leakage); 30-second dialogue segments (longer context vs. increased overlap complexity); hybrid permutation (exhaustive ≤7 changes vs. beam search >7)

- **Failure signatures:** WER degrades significantly from dev to eval (likely overfitting); scAcc low while WER reasonable (model transcribes well but misses turn boundaries); CER/WER ratio unusually high (character-level instabilities)

- **First 3 experiments:** 1) Reproduce BEA-Large baseline: fine-tune Fast Conformer on train-114 + train-293, report WER/CER on eval-spont 2) Ablate training composition: compare train-114 only, train-293 only, combined 3) Evaluate SOT on BEA-Dialogue: train with `<sc>` tokens, report cpWER and scAcc against zero-shot Whisper

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is there data leakage between the original BEA-Base splits, and if so, what is its extent and impact on reported performance?
- Basis: Authors note train-293 alone produced higher WER on dev-repet and eval-repet than train-114, suggesting possible leakage
- Why unresolved: The anomaly was observed but not investigated; original BEA-Base split creation was not audited
- What evidence would resolve it: Cross-referencing speaker identities and audio content across all BEA-Base splits

### Open Question 2
- Question: To what extent can fine-tuning diarization models on BEA-Dialogue improve DER compared to using pre-trained models?
- Basis: Diarization baselines used pre-trained pyannote.audio and Sortformer "without additional fine-tuning"
- Why unresolved: Authors chose zero-shot evaluation for baselines; no fine-tuning experiments were conducted
- What evidence would resolve it: Fine-tuning pyannote.audio or Sortformer on BEA-Dialogue training data and comparing DER on eval split

### Open Question 3
- Question: What linguistic and acoustic factors drive the elevated CER-to-WER ratio in conversational Hungarian speech compared to monologue?
- Basis: Paper observes higher CER/WER ratio in spontaneous dialogue but offers no causal analysis
- Why unresolved: Phenomenon was documented descriptively; no error analysis or linguistic investigation performed
- What evidence would resolve it: Detailed error analysis categorizing substitution types in conversational vs. monologue speech

## Limitations

- Data access and preprocessing details not fully specified, critical for reproducibility
- Hyperparameters left to default values without experimental validation, potentially explaining WER variance
- Hard threshold of 7 speaker changes in hybrid permutation strategy may not generalize to all overlap patterns
- Assertion that including experiment leaders doesn't harm speaker-independent evaluation lacks direct validation

## Confidence

- **High confidence:** Dataset construction methodology is clearly specified and reproducible given data access; baseline ASR results are internally consistent
- **Medium confidence:** Fine-tuning advantage over zero-shot Whisper assumes pre-training domain matches evaluation conditions
- **Low confidence:** Claim that including experiment leaders doesn't harm speaker-independent evaluation lacks direct validation through ablation studies

## Next Checks

1. **Hyperparameter sensitivity analysis:** Systematically vary learning rate (0.01, 0.1, 1.0), batch size (16, 32), and epochs (10, 20, 30) on BEA-Large train-114 subset to identify optimal settings and establish confidence intervals

2. **Speaker leakage verification:** Re-partition BEA-Dialogue train/dev/eval splits to explicitly exclude any speakers appearing in multiple roles across subsets, then retrain and evaluate to confirm WER improvements

3. **Permutation threshold ablation:** Evaluate diarization performance across different overlap thresholds (5, 7, 10, 15 speaker changes) using Sortformer to determine whether 7-change threshold is optimal