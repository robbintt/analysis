---
ver: rpa2
title: 'DIVE into MoE: Diversity-Enhanced Reconstruction of Large Language Models
  from Dense into Mixture-of-Experts'
arxiv_id: '2506.09351'
source_url: https://arxiv.org/abs/2506.09351
tags:
- uni00000013
- uni00000011
- uni00000014
- dive
- uni00000015
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents DIVE, a method for reconstructing dense large
  language models (LLMs) into Mixture-of-Experts (MoE) architectures while enhancing
  expert diversity. The approach addresses the issue of homogeneous experts in existing
  MoE reconstruction methods by leveraging domain affinity mining and pruning-based
  expert reconstruction.
---

# DIVE into MoE: Diversity-Enhanced Reconstruction of Large Language Models from Dense into Mixture-of-Experts

## Quick Facts
- arXiv ID: 2506.09351
- Source URL: https://arxiv.org/abs/2506.09351
- Reference count: 40
- Key outcome: DIVE achieves lower perplexity (13.52 vs 14.51 on LAMBADA) and higher accuracy (83.00 vs 80.50 on SciQ) compared to existing MoE reconstruction methods on TinyLlama-1.1B

## Executive Summary
DIVE presents a novel method for converting dense LLMs into Mixture-of-Experts (MoE) architectures while enhancing expert diversity. The approach addresses a critical limitation in existing MoE reconstruction methods where experts tend to become homogeneous, reducing the benefits of specialization. By leveraging domain affinity mining through hierarchical clustering and pruning-based expert reconstruction, DIVE identifies domain-specific experts and reconstructs MoE layers accordingly. The method employs an efficient two-stage retraining process that focuses on routers first, then experts and normalization modules using parameter-efficient fine-tuning.

## Method Summary
DIVE operates through a three-phase pipeline: domain affinity mining, expert reconstruction, and efficient retraining. The domain affinity mining phase uses hierarchical clustering on a calibration dataset to identify domain-specific subsets, which are then used to prune the original dense model and create specialized experts. The expert reconstruction phase replaces dense layers with MoE layers based on these domain-specific experts. The retraining process is divided into two stages - first fine-tuning only the routers to establish proper gating, then fine-tuning the experts and normalization modules using parameter-efficient techniques. This approach ensures that each expert develops distinct capabilities aligned with specific domains while maintaining overall model performance.

## Key Results
- Achieves 13.52 perplexity on LAMBADA compared to 14.51 for baseline MoE reconstruction methods
- Achieves 83.00 accuracy on SciQ compared to 80.50 for baseline methods
- Maintains same number of activated parameters while improving performance across language modeling and downstream tasks

## Why This Works (Mechanism)
DIVE works by explicitly addressing the expert homogeneity problem in MoE reconstruction through domain-aware specialization. Traditional MoE reconstruction methods often result in experts that learn similar representations because they're trained on the same data distribution. By mining domain affinities and creating domain-specific expert subsets through pruning, DIVE forces experts to specialize in distinct areas. The two-stage retraining process then ensures proper routing decisions are learned before fine-tuning the specialized experts, preventing interference between routing and expert specialization.

## Foundational Learning

**Domain Affinity Mining**
- Why needed: To identify natural groupings in data that can guide expert specialization
- Quick check: Verify clustering produces meaningful domain separation by examining sample distributions

**Hierarchical Clustering**
- Why needed: To create a structured view of domain relationships at multiple granularities
- Quick check: Confirm dendrogram structure reflects intuitive domain hierarchies

**Pruning-Based Expert Reconstruction**
- Why needed: To create specialized experts by removing irrelevant parameters for specific domains
- Quick check: Validate that pruned experts maintain performance on their target domains

**Two-Stage Retraining**
- Why needed: To prevent routing-expert interference by establishing stable routing before expert specialization
- Quick check: Monitor router stability metrics during stage one training

## Architecture Onboarding

**Component Map**
Dense Model -> Domain Clustering -> Expert Pruning -> MoE Layer Replacement -> Two-Stage Retraining

**Critical Path**
Domain clustering and pruning directly determine expert specialization quality, which impacts overall MoE performance. The two-stage retraining ensures stable routing before expert optimization.

**Design Tradeoffs**
The method trades increased preprocessing complexity (clustering and pruning) for improved expert diversity and performance. The two-stage training adds training time but improves convergence stability.

**Failure Signatures**
Homogeneous expert performance indicates clustering failure or insufficient pruning. Poor routing decisions suggest inadequate router fine-tuning in stage one.

**First Experiments**
1. Validate domain clustering produces distinct groups by examining cluster sample distributions
2. Test pruned experts on their target domains to verify specialization
3. Monitor router gating entropy during stage one training to ensure diverse routing

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Limited evaluation to TinyLlama-1.1B (1.1B parameters) without comprehensive testing on production-scale LLMs
- Domain affinity mining relies on calibration data quality and representativeness, which could introduce biases
- Two-stage retraining process requires careful hyperparameter tuning and may incur significant computational costs for larger models

## Confidence
**High confidence in:** Core methodology soundness, experimental setup rigor on TinyLlama-1.1B, technical implementation details
**Medium confidence in:** Performance improvements over baselines (limited to single small model), robustness claims across model sizes
**Low confidence in:** Scalability to production-scale models, practical applicability with diverse data distributions

## Next Checks
1. Evaluate DIVE on a production-scale LLM (70B+ parameters) and compare against state-of-the-art MoE reconstruction methods at scale
2. Test domain affinity mining robustness using biased and diverse calibration datasets to assess impact on expert specialization
3. Conduct ablation studies on two-stage retraining by varying epochs and hyperparameters to quantify performance-computation tradeoffs