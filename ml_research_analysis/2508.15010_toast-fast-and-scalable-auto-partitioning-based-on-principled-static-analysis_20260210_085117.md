---
ver: rpa2
title: 'TOAST: Fast and scalable auto-partitioning based on principled static analysis'
arxiv_id: '2508.15010'
source_url: https://arxiv.org/abs/2508.15010
tags:
- sharding
- dimension
- dimensions
- figure
- toast
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TOAST is an automatic sharding tool for large ML models that combines
  static compiler analysis with Monte Carlo Tree Search. It discovers sharding strategies
  by identifying tensor dimensions requiring identical sharding and resolving sharding
  conflicts through a novel static analysis.
---

# TOAST: Fast and scalable auto-partitioning based on principled static analysis

## Quick Facts
- **arXiv ID**: 2508.15010
- **Source URL**: https://arxiv.org/abs/2508.15010
- **Reference count**: 40
- **Primary result**: TOAST discovers better sharding strategies than state-of-the-art methods (Alpa, AutoMap) across diverse models and hardware, while being 25x faster and avoiding OOM errors.

## Executive Summary
TOAST is an automatic sharding tool for large ML models that combines static compiler analysis with Monte Carlo Tree Search. It discovers sharding strategies by identifying tensor dimensions requiring identical sharding and resolving sharding conflicts through a novel static analysis. The system outperforms state-of-the-art methods like Alpa and AutoMap across diverse models (T2B, T7B, GNS, U-Net, ITX) and hardware platforms (TPUv3, A100, P100 GPUs), consistently finding better partitioning strategies. TOAST's approach enables hardware fungibility and discovers significant improvements for under-optimized model architectures, with even 1% improvements translating to weeks of saved computation given long training times.

## Method Summary
TOAST uses Named Dimension Analysis (NDA) to pre-compute dimension equivalence classes and conflict resolutions, then applies Monte Carlo Tree Search (MCTS) to explore sharding strategies. The NDA pass assigns fresh dimension names to tensor operations and unifies them via identity rules and definition-to-use maps, creating groups of dimensions that must be sharded identically. When dimensions appear multiple times in one tensor (conflicts), TOAST groups them into compatibility sets via structural heuristics, resolving compatible conflicts identically to reduce the search space. MCTS operates on these pre-computed structures, using a relative cost model to guide search toward optimal partitionings. The system outputs sharded StableHLO code that can be compiled to device-specific implementations.

## Key Results
- TOAST consistently finds better sharding strategies than state-of-the-art methods (Alpa, AutoMap) across diverse models and hardware platforms
- The search is 25x faster than AutoMap and avoids out-of-memory errors that plague other methods
- TOAST discovers significant improvements for under-optimized model architectures, with even 1% improvements translating to weeks of saved computation given long training times

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-computing dimension equivalence classes via static analysis reduces the sharding search space from exponential to tractable.
- Mechanism: The Named Dimension Analysis (NDA) assigns fresh dimension names to each tensor operation, then unifies them via identity rules (I) and definition-to-use maps (M). This groups dimensions that must be sharded identically (same "color"), allowing one sharding decision to affect many dimensions simultaneously.
- Core assumption: Sharding rules for each operation (e.g., matmul's contracting dimensions must match) correctly capture parallelism semantics.
- Evidence anchors: [abstract] "identifying tensor dimensions requiring identical sharding"; [Section 3.1-3.2] Formal NDA definition; Figure 4 shows how identifying with I and M produces dimension groups (B, U) corresponding to manual sharding targets.
- Break condition: If operations have undocumented or incorrect sharding rules, NDA will produce wrong equivalence classes, leading to invalid partitionings.

### Mechanism 2
- Claim: Conflict compatibility sets reduce exponential conflict resolution space to a small, fixed number of decisions per model architecture.
- Mechanism: When a dimension name appears multiple times in one tensor (a "conflict"), the system groups conflicts into compatibility sets via structural heuristics (the "box" pattern and layer isomorphism). Compatible conflicts are resolved identically, collapsing 2^N possible resolutions to 2^(compatibility_sets).
- Core assumption: Compatible conflicts (forming "box" patterns in the dimension graph without crossing paths) should resolve identically to avoid unnecessary resharding communication.
- Evidence anchors: [abstract] "partitioning 'conflicts' that require resolution"; [Section 3.3-3.6] Figure 5d shows dimension graph with conflict edges; Section 3.6 states Transformer's 4 conflict resolutions regardless of layer count.
- Break condition: If dataflow patterns create non-box-compatible conflicts that nonetheless should share resolutions, the heuristic may miss optimization opportunities or require redundant decisions.

### Mechanism 3
- Claim: MCTS with color-based state representation achieves 25x faster search than propagation-based methods while avoiding OOM.
- Mechanism: Instead of invoking compiler propagation after each action (like AutoMap), TOAST pre-computes all propagations via NDA. The state is an in-memory map of dimension→sharding assignments, enabling O(1) conflict resolution lookups. Actions are (color, resolution_order, axis) tuples affecting many dimensions atomically.
- Core assumption: The cost model's relative runtime and memory penalty estimates sufficiently correlate with actual performance to guide search.
- Evidence anchors: [abstract] "the search is 25x faster than AutoMap and avoids out-of-memory errors"; [Section 4.3] "This design provides several key advantages: Efficient... Unambiguous... Simple"; Section 5.3 shows search time comparisons.
- Break condition: If the cost model poorly estimates actual runtime (e.g., misses communication overhead patterns), MCTS will converge to locally optimal but globally suboptimal strategies.

## Foundational Learning

- **SPMD (Single Program Multiple Data) partitioning and device meshes**: Why needed here: TOAST's output is device-local code operating on tensor shards; understanding batch vs. model parallelism is prerequisite. Quick check: Given a 2D device mesh with axes (b, m), explain how sharding a [batch, hidden] tensor along b differs from sharding along m.

- **Compiler intermediate representations (IR) and propagation**: Why needed here: TOAST operates on StableHLO IR; NDA is a static analysis pass that must trace dataflow through operations. Quick check: What is the difference between forward dataflow analysis (from inputs to outputs) and the definition-to-use mapping M in NDA?

- **Monte Carlo Tree Search (MCTS) basics**: Why needed here: TOAST's partitioner is MCTS-based; understanding exploration vs. exploitation is needed to tune search. Quick check: Why does TOAST terminate search early if no trajectory improves upon the best-known cost? What tradeoff does this represent?

## Architecture Onboarding

- **Component map**: Frontend (JAX/PyTorch) → StableHLO IR → NDA (dim names, conflicts, compatibility sets) → MCTS (action tuples, cost model) → Sharded StableHLO → Backend (XLA) → Device execution

- **Critical path**: 
  1. NDA analysis correctness — if dimension equivalence is wrong, all downstream decisions are invalid.
  2. Conflict compatibility grouping — determines action space size; incorrect grouping explodes search or misses strategies.
  3. Cost model fidelity — MCTS quality depends on accurate runtime/memory estimates.

- **Design tradeoffs**:
  - Pre-computed propagation (TOAST) vs. runtime propagation (AutoMap): TOAST trades upfront analysis cost for faster search; AutoMap is more flexible but slower.
  - Compatibility heuristics vs. exhaustive resolution: Heuristics reduce search but may miss optimal strategies for unusual architectures.
  - Relative cost model vs. absolute timing: Relative is faster but may misrank similar-cost options.

- **Failure signatures**:
  - OOM during search: Cost model memory penalty (C constant in MP(s)) is too weak; increase penalty weight.
  - Search doesn't converge: Action space may be too large; check if compatibility grouping is working (should see few sets for repeated layers).
  - Generated code slower than manual: Cost model may undervalue communication; verify collective operation cost estimates.

- **First 3 experiments**:
  1. **Reproduce MLP example (Figure 2)**: Apply TOAST to the 2-layer MLP, verify it discovers batch and Megatron partitioning. Check that NDA produces B and U dimension groups matching the colored dimensions.
  2. **Conflict resolution on attention (Figure 5)**: Run TOAST on the simplified attention function, verify it discovers sequence sharding with all_gather/reduce_scatter. Confirm compatibility set contains all 5 conflicts and resolves to 2 options.
  3. **Benchmark GNS on TPUv3 (Figure 8a)**: Compare TOAST vs. Manual vs. AutoMap step times. Verify TOAST avoids Manual's OOM and matches or beats AutoMap. Record search time to confirm ~25x speedup claim.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a supervised learning model effectively replace the MCTS agent by predicting optimal sharding decisions directly from Named Dimension Analysis (NDA) outputs?
- Basis: [explicit] The conclusion states, "For future research, we will leverage NDA to train a model capable of predicting the optimal sharding immediately."
- Why unresolved: The current system relies on a search-based MCTS agent; the proposed learning-based approach is purely conceptual at this stage.
- What evidence would resolve it: An implementation of a model trained on NDA features that matches or exceeds TOAST's solution quality with significantly lower compilation overhead.

### Open Question 2
- Question: Does the heuristic of resolving "compatible conflicts" identically ever exclude globally optimal partitioning strategies?
- Basis: [inferred] Section 3.5 heuristically decrees that compatible conflicts (forming a "box" in the dimension graph) must be resolved the same way to reduce search space complexity.
- Why unresolved: While this pruning aids tractability, the paper provides no theoretical guarantee that optimal solutions never require resolving these conflicts differently (which might incur resharding costs but lower compute costs).
- What evidence would resolve it: A counter-example or theoretical proof demonstrating that decoupling compatible conflict resolutions yields a lower step-time cost than the coupled resolution.

### Open Question 3
- Question: How does the static analysis hold up for models with highly dynamic control flow or tensor shapes?
- Basis: [inferred] The Named Dimension Analysis (Section 3) operates on a static IR (StableHLO) and assumes fixed dimension identities, whereas Section 5.1 evaluates mostly fixed-shape models.
- Why unresolved: Dynamic shapes (e.g., variable sequence lengths in inference) could break the static dimension naming or conflict analysis if dimensions change during execution.
- What evidence would resolve it: Evaluation of TOAST on workloads where tensor dimensions are symbolic or change dynamically based on control flow inputs.

## Limitations
- NDA's correctness depends entirely on having complete and accurate sharding rules for all StableHLO operations; undocumented operations could break the analysis
- The conflict compatibility heuristic (box patterns) may not capture all meaningful optimization opportunities for complex architectures with irregular dataflow
- The relative cost model may misestimate communication costs for specific hardware configurations, particularly collective operations like all_gather/reduce_scatter

## Confidence
- NDA enables tractable search by pre-computing propagations: **High confidence** - the mechanism is well-defined and independently verifiable through the MLP and attention examples
- MCTS finds better strategies than AutoMap: **High confidence** - the paper provides clear runtime comparisons across multiple models and hardware platforms
- 25x faster search while avoiding OOM: **High confidence** - timing data is presented and the OOM avoidance is explicitly demonstrated

## Next Checks
1. **Test NDA robustness**: Apply TOAST to models containing less common StableHLO operations (e.g., convolution, batch normalization) to verify dimension equivalence classes remain correct and the analysis doesn't fail silently
2. **Evaluate conflict heuristic limits**: Create a synthetic model with deliberately complex conflict patterns (crossing paths, irregular layer structures) to test whether compatibility grouping misses optimization opportunities or requires excessive search decisions
3. **Validate cost model accuracy**: For models where TOAST and AutoMap produce different strategies, measure actual execution time and memory usage on target hardware to quantify the correlation between the relative cost model estimates and real performance