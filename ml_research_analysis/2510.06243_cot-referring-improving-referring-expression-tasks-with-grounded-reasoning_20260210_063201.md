---
ver: rpa2
title: 'CoT Referring: Improving Referring Expression Tasks with Grounded Reasoning'
arxiv_id: '2510.06243'
source_url: https://arxiv.org/abs/2510.06243
tags:
- referring
- arxiv
- data
- grounding
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of improving Referring Expression
  tasks, which require models to ground objects in images based on complex natural
  language descriptions. The authors propose CoT Referring (CoTR), a novel approach
  that enhances model reasoning by structuring referring expressions into a sequential
  chain-of-thought format.
---

# CoT Referring: Improving Referring Expression Tasks with Grounded Reasoning

## Quick Facts
- arXiv ID: 2510.06243
- Source URL: https://arxiv.org/abs/2510.06243
- Reference count: 31
- The paper introduces CoT Referring (CoTR), a chain-of-thought reasoning approach for Referring Expression tasks that improves localization accuracy by 2.5%+ on complex multi-hop expressions.

## Executive Summary
CoT Referring (CoTR) addresses the challenge of grounding objects in images based on complex natural language descriptions by structuring referring expressions into a sequential chain-of-thought format. The approach breaks down complex queries into intermediate steps, grounding each anchor noun before identifying the target object. The authors develop a pipeline to generate CoTR data and create a Composite Referring Benchmark for evaluating model performance on challenging expressions. Their proposed RefLM architecture integrates detection and segmentation capabilities, trained with an adaptive weighted loss to optimize performance. Experiments show that RefLM trained on CoTR data significantly outperforms baseline models on both complex and standard referring expression datasets.

## Method Summary
The CoTR approach transforms referring expressions into a chain-of-thought format where complex multi-hop queries are decomposed into sequential reasoning steps. Each anchor noun is grounded before the final target object is identified, creating a structured reasoning path. The authors implement RefLM, an MLLM that combines detection and segmentation capabilities using a box-point representation that feeds into SAM for mask generation. Training employs LoRA fine-tuning with an adaptive weighted loss where target boxes receive higher weights than anchor boxes. The system is trained on a combination of REC, RES, GCG, and segmentation tasks using 26K total samples from multiple datasets.

## Key Results
- RefLM trained on CoTR data achieves 2.5%+ accuracy improvement on both Composite Referring Benchmark and standard RefCOCO/+/g datasets
- The approach significantly outperforms baseline models on complex multi-hop referring expressions with L_max≥3
- The box-point representation with SAM integration provides accurate segmentation masks from LLM-generated bounding boxes

## Why This Works (Mechanism)
The CoTR approach works by enforcing a structured reasoning process that mirrors human problem-solving for complex visual queries. By decomposing multi-hop expressions into sequential grounding steps, the model can focus on simpler sub-problems rather than attempting to solve the entire complex query at once. The adaptive weighted loss ensures the model prioritizes accurate localization of the final target object while still maintaining anchor grounding accuracy. The box-point representation provides a natural interface between the LLM's spatial reasoning and SAM's segmentation capabilities, enabling precise mask generation from coordinate predictions.

## Foundational Learning
- **Chain-of-thought reasoning**: Breaking down complex problems into intermediate reasoning steps; needed to handle multi-hop expressions; quick check: verify that each anchor is grounded before target identification
- **Adaptive weighted loss**: Assigning different importance weights to different training samples; needed to prioritize target object localization; quick check: confirm target weight = (n+1) where n is number of anchors
- **Box-point representation**: Encoding spatial coordinates as point distributions; needed to interface LLM with SAM; quick check: validate coordinate normalization to [0,1000] range
- **SAM mask generation**: Generating segmentation masks from point inputs; needed for precise object boundaries; quick check: test mask quality with varying point sampling (15-25 points)
- **LoRA fine-tuning**: Parameter-efficient LLM adaptation; needed for efficient training; quick check: verify rank=256 configuration
- **Multi-task learning**: Joint training across detection, segmentation, and grounding tasks; needed for comprehensive visual understanding; quick check: monitor performance across all task types

## Architecture Onboarding
- **Component map**: LLM (Qwen3 → DeepSeek → Qwen2.5-VL-72B) -> Box-point generator -> SAM decoder -> Mask output
- **Critical path**: Input expression → Anchor extraction → Sequential grounding → Target localization → Mask generation
- **Design tradeoffs**: Structured reasoning vs. model complexity, precision vs. efficiency, point sampling vs. mask quality
- **Failure signatures**: SAM produces poor masks (check point sampling), model ignores reasoning order (verify adaptive loss), low data validation pass rate (check IoU thresholds)
- **First experiments**: 1) Validate adaptive weighted loss implementation, 2) Test SAM mask quality with different point counts, 3) Compare performance with randomized vs. canonical reasoning order

## Open Questions the Paper Calls Out
- **Open Question 1**: Is the performance improvement dependent on the specific "canonical reasoning order" (sorted by hop level), or is performance equivalent with a randomized step-by-step decomposition? The paper doesn't isolate whether the specific semantic ordering matters versus the mere presence of intermediate grounding steps.
- **Open Question 2**: Can the sensitivity to the number of input points for SAM be mitigated to allow for denser, more robust mask predictions? The non-monotonic performance (peaks at 15, drops at 25 points) suggests unresolved instability in the LLM-SAM interface.
- **Open Question 3**: Does fine-tuning on complex, multi-step CoT data degrade the model's capability on simple, single-noun referring expressions? The paper identifies single noun case failures as a major error category.

## Limitations
- Training specifications are incomplete (epochs, data mixing ratios, full LoRA hyperparameters)
- Fixed 15-point sampling strategy may not generalize across all object scales and aspect ratios
- Performance on simple single-noun expressions may be degraded due to structured training objective

## Confidence
- **High confidence**: Core methodological contribution (CoTR data format and reasoning chain structure) is well-defined and reproducible
- **Medium confidence**: RefLM architecture and adaptive loss formulation can be implemented, but training dynamics depend on unspecified hyperparameters
- **Medium confidence**: Benchmark construction methodology is clear, but annotation quality requires validation

## Next Checks
1. Verify adaptive weighted loss implementation by checking gradient magnitudes for target vs anchor boxes during training
2. Test SAM mask quality with varying point sampling strategies (fewer/more than 15 points) to establish sensitivity to this hyperparameter
3. Perform ablation on training data composition by training separate models with only REC, only RES, and combined datasets to quantify contribution of each task type