---
ver: rpa2
title: 'SAC-GLAM: Improving Online RL for LLM agents with Soft Actor-Critic and Hindsight
  Relabeling'
arxiv_id: '2410.12481'
source_url: https://arxiv.org/abs/2410.12481
tags:
- agents
- critic
- goals
- environment
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of using online reinforcement
  learning (RL) to train Large Language Models (LLMs) as agents in sequential decision-making
  tasks. While previous work used on-policy algorithms like PPO, this limits the use
  of experience replay and hindsight relabeling.
---

# SAC-GLAM: Improving Online RL for LLM agents with Soft Actor-Critic and Hindsight Relabeling

## Quick Facts
- arXiv ID: 2410.12481
- Source URL: https://arxiv.org/abs/2410.12481
- Authors: Loris Gaven; Clement Romac; Thomas Carta; Sylvain Lamprier; Olivier Sigaud; Pierre-Yves Oudeyer
- Reference count: 21
- Primary result: Off-policy SAC with HER outperforms PPO-GLAM in sample efficiency for LLM agents in text-based multi-goal environments

## Executive Summary
This paper addresses the challenge of using online reinforcement learning to train Large Language Models as agents in sequential decision-making tasks. While previous work relied on on-policy algorithms like PPO, which limits the use of experience replay and hindsight relabeling, the authors introduce SAC-GLAM - an off-policy RL method that adapts Soft Actor-Critic (SAC) and Hindsight Experience Replay (HER) for LLMs. The method addresses unique challenges of using pre-trained LLM actors with randomly initialized critics, including careful warmup periods, architectural choices, and n-step returns. Experiments in a text-based multi-goal environment show that SAC-GLAM with HER outperforms PPO-GLAM in sample efficiency while achieving comparable time efficiency, demonstrating a path toward autotelic LLM agents.

## Method Summary
SAC-GLAM adapts discrete SAC for LLM policies by using a critic MLP that takes the LLM's representation of concatenated prompt and action as input (single Q-value output) rather than just the prompt (|A| outputs). The method employs n-step returns (n=3) with a smaller replay buffer (100K capacity) to balance convergence speed and off-policy distribution shift. HER with future strategy relabels failed trajectories using goals provided by a social partner, converting unsuccessful experiences into successful training examples. The architecture uses shared weights between actor and critic with backpropagation through the LLM, combined with LoRA and 4-bit quantization for memory efficiency. Training includes a critic warmup period followed by joint actor-critic updates with balanced HER/environment transitions.

## Key Results
- SAC-GLAM achieves higher success rates than PPO-GLAM in sample efficiency across all tested tasks
- SAC-GLAM shows better sample efficiency with comparable time efficiency (training speed)
- The method enables hindsight relabeling by switching from on-policy to off-policy learning
- SAC-GLAM demonstrates faster convergence than PPO-GLAM in the text-based multi-goal environment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Off-policy learning with experience replay enables hindsight relabeling, allowing LLM agents to learn from failed trajectories by relabeling them with accidentally achieved goals.
- Mechanism: SAC-GLAM replaces on-policy PPO with off-policy SAC, storing trajectories in a replay buffer. Failed trajectories are relabeled using HER (future strategy) with goals provided by a social partner, converting unsuccessful experiences into successful training examples for alternative goals.
- Core assumption: The environment provides or can infer achieved goals from trajectories (via social partner feedback).
- Evidence anchors: [abstract] "introduces SAC-GLAM, an off-policy RL method that adapts Soft Actor-Critic (SAC) and Hindsight Experience Replay (HER) for LLMs"; [section 2.3] "enabling the algorithm to learn from failed attempts by relabeling portions of the failed trajectories with accidentally reached goals"; [corpus] GCHR paper confirms HER benefits but notes trajectory relabeling alone may not fully exploit available experience.

### Mechanism 2
- Claim: A critic architecture that encodes observation-action pairs as input with shared LLM weights produces faster convergence and better stability than observation-only critics.
- Mechanism: The critic MLP takes the LLM's representation of concatenated prompt AND action as input (single Q-value output), rather than just the prompt (|A| outputs). This leverages the LLM's pre-trained representations to relate actions to contexts. Gradients backpropagate through shared weights.
- Core assumption: The pre-trained LLM provides useful joint representations of (observation, goal, action) that accelerate critic learning.
- Evidence anchors: [section 2.2] "leveraging the LLM's capabilities by rather using an MLP with a single output with the LLM encoding the concatenation of the prompt and an action"; [section 3.1] "sharing the policy's weights and using an MLP with a single output leads to better sample efficiency and greater stability"; [corpus] Weak direct evidence; related work on actor-critic LLM fine-tuning remains limited.

### Mechanism 3
- Claim: N-step returns with smaller replay buffers accelerate critic convergence while managing off-policy distribution shift.
- Mechanism: Using n=3 returns accumulates rewards over multiple steps before bootstrapping, reducing reliance on early, inaccurate Q-estimates. A smaller replay buffer (100K vs larger) limits the age of oldest transitions, mitigating off-policy divergence as the policy evolves.
- Core assumption: The policy changes slowly enough that 3-step transitions remain reasonably on-policy; older transitions correlate with worse policy mismatch.
- Evidence anchors: [section 2.2] "n-step returns introduce the risk of older transitions becoming highly off-policy... To mitigate this issue, we focus on minimizing the age of the oldest transitions"; [appendix F] "3-step returns in our experiments, as 5-step returns introduced instability"; [corpus] SACn paper confirms n-step returns increase SAC convergence speed, though stability trade-offs exist.

## Foundational Learning

- Concept: **Soft Actor-Critic (SAC) basics**
  - Why needed here: SAC-GLAM builds directly on discrete SAC; understanding entropy regularization, Q-function learning, and actor-critic interdependence is essential for debugging.
  - Quick check question: Can you explain why SAC maximizes both expected return AND entropy, and how the entropy coefficient affects exploration?

- Concept: **Hindsight Experience Replay (HER)**
  - Why needed here: HER is the key sample efficiency mechanism; understanding goal relabeling strategies (future, final, episode) is necessary for extending to new environments.
  - Quick check question: Given a failed trajectory toward goal A that accidentally achieves state B, how would HER relabel this for training?

- Concept: **On-policy vs. off-policy RL distinction**
  - Why needed here: The paper's core contribution is enabling off-policy methods for LLMs; understanding why PPO cannot use replay buffers clarifies the motivation.
  - Quick check question: Why can't standard PPO reuse experience from a replay buffer, and what assumptions does off-policy SAC relax?

## Architecture Onboarding

- Component map: Flan-T5-base LLM encoder-decoder -> critic MLP (2 hidden layers, 1024 units, ReLU) -> single Q-value output -> training updates -> replay buffer (100K capacity) -> HER module (future strategy) -> social partner feedback

- Critical path: 1) Critic warmup (critic-only updates until reasonable Q-estimates) -> 2) Joint actor-critic training with n=3 returns -> 3) HER relabeling after each episode before buffer insertion -> 4) Balanced batch sampling (50% HER / 50% environment transitions)

- Design tradeoffs: Single-output critic (slower inference, faster convergence) vs. multi-output (faster inference, slower convergence); Shared weights with backprop (better convergence, risk to pre-trained knowledge) vs. frozen backbone (preserves LLM, slower learning); Small buffer (less off-policy drift, lower capacity) vs. large buffer (more diversity, older transitions)

- Failure signatures: Critic collapse early in training (Q-values diverge) → increase warmup period or reduce critic learning rate; Actor degrades mid-training (loss of language coherence) → reduce gradient flow through shared layers or lower actor LR; HER-dominated learning (overfitting to simple relabeled goals) → enforce goal distribution matching or reduce HER ratio; Instability with n-step returns → reduce n or shrink replay buffer

- First 3 experiments: 1) Critic architecture ablation: Compare observation-only (|A| outputs) vs. observation-action (1 output) with shared weights on a simplified 3-object environment; expect single-output to converge faster. 2) Warmup period sensitivity: Test warmup durations (0, 1000, 5000, 10000 critic-only updates) measuring both final performance and early actor degradation; identify minimum viable warmup. 3) HER batch ratio tuning: Compare 25%, 50%, 75% HER transitions per batch; expect 50% to balance positive/negative examples without overfitting to relabeled goals.

## Open Questions the Paper Calls Out
- Can SAC-GLAM be effectively combined with LLM-based goal imagination modules to create fully autotelic agents that set their own goals without external supervision? (The authors state their method "paves the path towards autotelic LLM agents" and suggest that mixing goal generators with RL agents is a natural next step, though challenges remain.)
- Does the sample efficiency of SAC-GLAM scale to LLMs with significantly larger parameter counts (e.g., 7B+ parameters) without becoming computationally prohibitive? (Experiments utilized Flan-T5-base (250M parameters) requiring 4 V100 GPUs; larger models may have different internal representations and computational costs.)
- Can SAC-GLAM function effectively in environments with larger action spaces without relying on the specific simplifications of the Playground-Text environment? (The authors simplified the environment by using direct actions like "Go to {object}" which restricts the action space to discrete interactions with only 6 objects.)
- Is the reliance on a "social partner" for hindsight relabeling a strict requirement for HER in textual domains, or can the LLM agent perform self-evaluation for relabeling? (The method section notes the environment includes a "social partner" that describes goals reached during a trajectory to facilitate relabeling; real-world applications may lack this external feedback.)

## Limitations
- The warmup period duration is mentioned but not explicitly specified, creating uncertainty about implementation details.
- The architectural choice of sharing weights between actor and critic with backpropagation through the LLM backbone could potentially degrade the pre-trained LLM's capabilities over time.
- Experimental validation is limited to a single text-based environment with synthetic social partner feedback, raising questions about generalization to real-world scenarios where hindsight goals may not be readily available.

## Confidence

**High Confidence**: The core claim that off-policy SAC enables hindsight relabeling while on-policy PPO does not is well-established in the RL literature and correctly applied here. The technical implementation details of SAC with n-step returns and HER are sound.

**Medium Confidence**: The claim that the observation-action concatenated critic architecture with shared weights provides better sample efficiency than alternatives is supported by ablation studies in the paper, but direct comparative evidence is limited. The architectural benefits are plausible but not conclusively proven.

**Low Confidence**: The scalability claims regarding training efficiency (sample vs. time efficiency) are based on experiments in a single controlled environment. The practical applicability to more complex, real-world environments with less structured goal feedback remains unproven.

## Next Checks

1. **Architectural Ablation with Multiple Environments**: Replicate the critic architecture ablation (observation-only vs. observation-action) across 2-3 different environments beyond Playground-Text, including at least one with a larger action space to test scalability claims.

2. **Warmup Period Sensitivity Analysis**: Systematically vary the warmup period duration (0, 1000, 5000, 10000, 20000 critic-only updates) and measure both learning curves and final performance to identify the minimum viable warmup period and its impact on actor stability.

3. **HER Ratio and Buffer Size Trade-offs**: Conduct a grid search over HER batch ratios (25%, 50%, 75%, 100%) and replay buffer sizes (50K, 100K, 200K) to empirically determine the optimal balance between HER benefits and off-policy distribution shift, particularly for different n-step values.