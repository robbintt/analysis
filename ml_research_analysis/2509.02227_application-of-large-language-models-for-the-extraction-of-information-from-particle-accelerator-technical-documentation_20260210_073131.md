---
ver: rpa2
title: Application Of Large Language Models For The Extraction Of Information From
  Particle Accelerator Technical Documentation
arxiv_id: '2509.02227'
source_url: https://arxiv.org/abs/2509.02227
tags:
- answer
- context
- documentation
- llms
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses knowledge retention challenges in particle
  accelerator facilities by developing a retrieval-augmented generation (RAG) system
  using large language models to extract information from technical documentation.
  The method employs a two-stage pipeline: document preprocessing with chunking and
  embedding storage, followed by runtime retrieval and generation using the Gemma
  3 model.'
---

# Application Of Large Language Models For The Extraction Of Information From Particle Accelerator Technical Documentation

## Quick Facts
- arXiv ID: 2509.02227
- Source URL: https://arxiv.org/abs/2509.02227
- Authors: Qing Dai; Rasmus Ischebeck; Maruisz Sapinski; Adam Grycner
- Reference count: 16
- One-line primary result: RAG system using 800-char chunks with Top-5 retrieval achieves 0.90-0.93 answer accuracy on technical accelerator documentation

## Executive Summary
This paper addresses knowledge retention challenges in particle accelerator facilities by developing a retrieval-augmented generation (RAG) system using large language models to extract information from technical documentation. The method employs a two-stage pipeline: document preprocessing with chunking and embedding storage, followed by runtime retrieval and generation using the Gemma 3 model. Evaluation on 100 domain-specific question-answer pairs showed that 800-character chunks with Top-5 retrieval achieved the highest answer accuracy (0.90-0.93 confidence). The system demonstrated strong performance in answering questions from English and German documents, though German queries initially lagged until translation improved results. Key limitations include handling of non-textual elements like tables and figures, which current RAG implementations struggle to retrieve effectively.

## Method Summary
The authors developed a two-stage RAG pipeline for extracting information from particle accelerator technical documentation. First, they preprocess 58 PDFs using the MinerU parser to extract text, equations, and tables, then chunk the content into 800-character windows and generate embeddings using BGE-M3. These embeddings are stored in a vector database for efficient retrieval. During runtime, queries are embedded and used to retrieve the top-k most similar chunks, which are then passed to the Gemma 3 27B instruction-tuned LLM to generate answers. The system was evaluated on 100 expert-created question-answer pairs, measuring recall, mean reciprocal rank, and answer accuracy with LLM-as-judge.

## Key Results
- 800-character chunks with Top-5 retrieval achieved highest answer accuracy (0.90-0.93 confidence)
- Translation of German chunks to English significantly improved retrieval performance for both German and English queries
- Current RAG implementation struggles with non-textual content like tables and figures, identified as a significant limitation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Smaller chunk sizes (800 characters) with Top-5 retrieval yield higher answer accuracy than larger chunks.
- Mechanism: Smaller chunks reduce semantic dilution during embedding, producing more precise similarity matches. Top-5 retrieval provides sufficient context coverage without exceeding the model's context window, avoiding truncation-induced hallucinations.
- Core assumption: The embedding model (BGE-M3) captures semantic meaning adequately at 800-character granularity; relevant information isn't split across chunk boundaries.
- Evidence anchors:
  - [section]: "Neither 1600- nor 2000-char windows improved recall or MRR."
  - [section]: "Top-5 1600-char inputs average around 1500 tokens, they approach the limit, leading to truncation and hallucinations."
  - [corpus]: Limited direct corpus validation; one neighbor paper (arXiv:2509.04139) addresses technical document retrieval optimization but doesn't replicate chunk size findings.
- Break condition: Documents where critical information spans >800 characters without internal semantic markers (e.g., tables, multi-paragraph procedures) may suffer from fragmentation.

### Mechanism 2
- Claim: Translating non-English chunks to English improves retrieval performance for both German and English queries.
- Mechanism: Translation reduces multilingual embedding noise, aligning all content to a single language space where the embedding model performs more consistently. This is particularly effective when the underlying LLM was primarily trained on English data.
- Core assumption: Translation quality is sufficient to preserve semantic meaning; translation overhead is acceptable for the accuracy gain.
- Evidence anchors:
  - [section]: "Translating German chunks to English... significantly boosted recall and MRR for German queries and lifted English queries, likely by reducing multilingual noise."
  - [section]: "Translation helps at k = 3 but not consistently at k = 5. Noise from additional chunks partly offsets the translation gain."
  - [corpus]: Neighbor paper arXiv:2508.02497 addresses LLM-driven documentation translation but focuses on OSS, not retrieval optimization.
- Break condition: Low-resource languages or domain-specific terminology where translation degrades semantic fidelity; real-time systems where translation latency is prohibitive.

### Mechanism 3
- Claim: Two-stage RAG pipeline (offline preprocessing + runtime retrieval-generation) enables efficient knowledge extraction from legacy technical documentation.
- Mechanism: Pre-computed embeddings eliminate runtime parsing overhead, enabling sub-second retrieval. The instruction-tuned LLM (Gemma 3 27B) synthesizes retrieved chunks into coherent answers while adhering to grounding constraints via prompt design ("use ONLY the provided context").
- Core assumption: The document corpus is relatively static; re-indexing frequency is low; the LLM follows grounding instructions reliably.
- Evidence anchors:
  - [abstract]: "The method employs a two-stage pipeline: document preprocessing with chunking and embedding storage, followed by runtime retrieval and generation."
  - [section]: Benchmark of 100 expert-created QA pairs showed 0.90-0.93 mean confidence for correct answers.
  - [corpus]: Neighbor papers (arXiv:2506.20608, arXiv:2405.01359) apply similar RAG architectures to scientific/technical knowledge bases, suggesting pattern validity.
- Break condition: Rapidly updating document corpora requiring frequent re-indexing; queries requiring reasoning across >5 chunks or multi-hop inference.

## Foundational Learning

- Concept: **Retrieval-Augmented Generation (RAG)**
  - Why needed here: Core architecture pattern combining external knowledge retrieval with LLM generation to ground answers in actual documentation.
  - Quick check question: Can you explain why RAG is preferred over fine-tuning for domain-specific document QA?

- Concept: **Embedding models and vector similarity**
  - Why needed here: Understanding how text is converted to dense vectors and how semantic similarity is computed is essential for debugging retrieval quality.
  - Quick check question: What is the difference between cosine similarity and Euclidean distance for embedding comparison?

- Concept: **Chunking strategies and context windows**
  - Why needed here: Chunk size directly affects retrieval precision and whether retrieved context fits within the LLM's context window.
  - Quick check question: If your LLM has a 4096-token context window and each chunk is 1600 characters (~500 tokens), how many chunks can safely fit with room for the query and response?

## Architecture Onboarding

- Component map:
  - **Input**: 58 PDFs (English/German) → MinerU parser → Text/equations/tables extraction
  - **Preprocessing**: Chunking (800-char windows) → BGE-M3 embeddings → Vector database storage
  - **Runtime**: Query embedding → Top-k retrieval → Optional translation → Gemma 3:27b generation
  - **Evaluation**: Recall@k, MRR@k, answer accuracy with LLM-as-judge

- Critical path:
  1. Document parsing quality (MinerU extraction accuracy)
  2. Chunk size selection (800-char optimal in this study)
  3. Embedding model choice (BGE-M3 multilingual capability)
  4. Context window management (avoiding truncation at ~2048 tokens)

- Design tradeoffs:
  - Smaller chunks = better retrieval precision but more chunks to process; larger chunks = more context but higher truncation risk
  - Higher k (Top-5) = better recall but more noise; lower k (Top-3) = cleaner context but may miss relevant chunks
  - Translation = improved multilingual performance but added latency and potential semantic drift

- Failure signatures:
  - Hallucinations: Check if total retrieved tokens exceed context window (observed with 1600-char + Top-5)
  - Low recall for specific queries: Check if relevant content is in non-textual elements (tables/figures not yet supported)
  - Language mismatch: German queries underperforming → enable translation pipeline
  - "I don't know" overuse: Check chunk granularity or embedding quality

- First 3 experiments:
  1. Replicate chunk size comparison (800 vs 1600 vs 2000 chars) on your own corpus to validate optimal chunk size for your document types.
  2. Test translation impact on any non-English documents in your corpus; measure recall@5 before and after.
  3. Identify documents with critical tables/figures; manually create text descriptions and measure retrieval improvement to quantify the non-textual gap.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can automatic figure captioning pre-processing enable effective textual retrieval of visual content (schematics, plots, photos) in RAG systems for technical documentation?
- Basis in paper: [explicit] "Future work will focus on addressing this challenge through... exploring pre-processing techniques such as automatic figure captioning to generate descriptive text prior to indexing"
- Why unresolved: Current RAG implementations cannot retrieve or reason with visual information embedded in technical documents; this is identified as a "significant limitation."
- What evidence would resolve it: Comparative evaluation of retrieval accuracy and answer quality on figure-dependent questions before and after captioning integration.

### Open Question 2
- Question: How do multi-modal embedding models compare to text-only approaches for encoding tables and figures in particle accelerator documentation?
- Basis in paper: [explicit] Future work includes "the evaluation of multi-modal embedding models that would directly encode figure content into the vector database."
- Why unresolved: The authors currently rely solely on text-based embeddings (BGE-M3) and have not tested vision-capable alternatives.
- What evidence would resolve it: Benchmark comparison on the existing 100 QA pairs, augmented with questions requiring table/figure comprehension, using multi-modal vs. text-only embeddings.

### Open Question 3
- Question: What is the optimal translation strategy for multilingual technical corpora to minimize retrieval performance disparities across query languages?
- Basis in paper: [inferred] German queries initially lagged behind English; translating German chunks to English boosted recall but introduced noise at higher k values, suggesting a trade-off not yet resolved.
- Why unresolved: Translation helps inconsistently (beneficial at k=3, not at k=5), and the underlying cause—multilingual noise vs. semantic alignment—remains unclear.
- What evidence would resolve it: Systematic ablation across translation directions, query languages, and chunk sizes with fine-grained error analysis.

### Open Question 4
- Question: How robust is the 800-character chunk recommendation across different document types, LLM architectures, and context window configurations?
- Basis in paper: [inferred] The 800-char result was obtained with a specific model (Gemma 3 27B) and constrained context window (2048–6000 tokens); truncation caused hallucinations at larger chunk sizes.
- Why unresolved: The optimal chunk size may depend on model-specific tokenization and context limits, not intrinsic document properties.
- What evidence would resolve it: Cross-validation with different LLM families and extended context windows to determine whether findings generalize.

## Limitations
- Current RAG implementation cannot effectively retrieve information from tables and figures, which are significant components of technical documentation
- Translation overhead may introduce latency and potential semantic drift for specialized domain terminology
- The system's effectiveness with rapidly changing document corpora remains untested

## Confidence
- **High Confidence**: Chunk size optimization (800 characters optimal) and Top-5 retrieval effectiveness - well-supported by controlled experiments with clear performance metrics.
- **Medium Confidence**: Translation impact on multilingual retrieval - demonstrated effectiveness but with noted noise at higher k values and potential translation quality dependencies.
- **Low Confidence**: Long-term knowledge retention utility - the paper demonstrates technical feasibility but doesn't address organizational adoption, maintenance overhead, or knowledge worker acceptance.

## Next Checks
1. Conduct user studies with domain experts performing realistic retrieval tasks on live technical documentation, measuring both accuracy and usability compared to current search methods.
2. Implement text-based table/figure descriptions and measure retrieval improvement for queries targeting non-textual content, quantifying the performance gap.
3. Test the system's ability to handle multi-hop reasoning questions requiring synthesis across more than 5 chunks, evaluating whether the current RAG architecture supports complex technical inference or requires modifications.