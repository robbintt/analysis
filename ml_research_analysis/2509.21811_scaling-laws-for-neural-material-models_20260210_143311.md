---
ver: rpa2
title: Scaling Laws for Neural Material Models
arxiv_id: '2509.21811'
source_url: https://arxiv.org/abs/2509.21811
tags:
- data
- scaling
- training
- equiformerv2
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates scaling laws for neural networks predicting
  material properties using the OMat24 dataset. The authors trained transformer and
  EquiformerV2 architectures while varying training data size, model parameters, and
  compute, finding distinct power-law relationships for each scaling dimension.
---

# Scaling Laws for Neural Material Models

## Quick Facts
- arXiv ID: 2509.21811
- Source URL: https://arxiv.org/abs/2509.21811
- Reference count: 12
- Key outcome: EquiformerV2 significantly outperforms transformers in predicting material properties across all scaling regimes

## Executive Summary
This work investigates scaling laws for neural networks predicting material properties using the OMat24 dataset. The authors trained transformer and EquiformerV2 architectures while varying training data size, model parameters, and compute, finding distinct power-law relationships for each scaling dimension. EquiformerV2 achieves substantially better performance with fewer resources by explicitly incorporating physical symmetries through equivariant architectures. These empirical scaling laws demonstrate that explicitly incorporating physical symmetries leads to substantially more efficient learning than unconstrained models, even at large scales.

## Method Summary
The authors compared encoder-only transformers and EquiformerV2 architectures on the OMat24 dataset for predicting material properties (energy, forces, stresses) from atomic numbers and positions. Training used a combined loss function with batch size 32, 50 epochs with early stopping, max learning rate 6×10^-4 with cosine decay schedule, and mixed precision training. The scaling experiments independently varied training data size (10K-300K samples), parameter count (up to 1.2M parameters), and compute (tracking FLOPs during training) to derive power-law relationships for validation loss scaling.

## Key Results
- EquiformerV2 significantly outperforms transformers across all scaling regimes
- Dataset scaling: EquiformerV2 follows L≈64.7·D^−0.242 while transformers scale as L≈77.1·N^−0.052
- Parameter scaling: EquiformerV2 achieves L≈776·P^−0.383 compared to transformers' L≈175·P^−0.120
- Compute scaling: EquiformerV2 follows L≈4.99×10^5·C^−0.339, while transformers plateau at validation loss of 40 below 10^15 FLOPs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Equivariant architectures achieve more efficient scaling than unconstrained transformers for material property prediction.
- Mechanism: EquiformerV2 explicitly encodes E(3) equivariance (rotational/translational symmetry) into learned representations, reducing the hypothesis space the model must search. Transformers must learn these symmetries implicitly from data.
- Core assumption: Physical symmetries are correctly specified and relevant to the prediction task.
- Evidence anchors:
  - [abstract] "explicitly incorporating physical symmetries through equivariant architectures leads to substantially more efficient learning than unconstrained models"
  - [section 2.4] "This disparity emphasizes the unique advantage provided by explicitly incorporating physical symmetries... Transformers, being less physically constrained, rely on implicitly learning these symmetries, which appears less effective"
  - [corpus] "Scaling Laws of Graph Neural Networks for Atomistic Materials Modeling" (FMR=0.56) supports GNN effectiveness for atomistic tasks, though does not directly compare equivariance.
- Break condition: If symmetries are broken in the target property, or if the equivariance constraints are misspecified for the task.

### Mechanism 2
- Claim: Validation loss follows predictable power-law relationships with dataset size, parameter count, and compute within tested ranges.
- Mechanism: Loss scales as L = α·N^−β where α and β are fitted constants and N is the scaling dimension (data, parameters, or FLOPs). This enables performance prediction before expensive training runs.
- Core assumption: The fitted power-law relationships extrapolate reasonably within the tested scaling regimes.
- Evidence anchors:
  - [abstract] "we can predict how increasing each of the three hyperparameters (training data, model size, and compute) affects predictive performance"
  - [section 2.3.1] "Both the EquiformerV2 models exhibit characteristic performance improvements with increased training data, following the form L=α·D^−β"
  - [corpus] "Neural Scaling Laws for Deep Regression" provides grounding for regression scaling laws; "Implicit bias produces neural scaling laws" offers theoretical perspective on mechanism.
- Break condition: At extreme scales beyond tested ranges, when scaling dimensions interact non-linearly, or when data quality degrades at scale.

### Mechanism 3
- Claim: Transformers require substantially more compute to begin learning compared to equivariant architectures.
- Mechanism: Unconstrained transformers plateau at high loss values (≈40) at low compute, effectively outputting zero forces. Learning only emerges beyond 10^15 FLOPs with sufficient parameters.
- Core assumption: The compute threshold is architecture-intrinsic, not an artifact of hyperparameter choices.
- Evidence anchors:
  - [section 2.3.3] "Most models, constrained by both parameters and the size of the dataset, plateau with a validation loss of 40... these transformers achieve a loss of 40 by setting the forces to 0, regardless of their input"
  - [section 2.3.3] "Only beyond the 10^15 FLOP scale do we see that transformers demonstrate actual learning"
  - [corpus] Corpus evidence on compute thresholds for unconstrained vs. equivariant architectures is limited; nearby papers focus on scaling relationships rather than compute floors.
- Break condition: With different hyperparameter configurations (learning rate, batch size), the compute threshold may shift.

## Foundational Learning

- Concept: Equivariance in neural networks
  - Why needed here: Understanding why EquiformerV2 enforces E(3) equivariance explains its superior efficiency.
  - Quick check question: If you rotate a molecule, should the predicted energy change? Should the predicted forces rotate correspondingly?

- Concept: Power-law scaling relationships
  - Why needed here: The paper