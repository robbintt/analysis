---
ver: rpa2
title: 'When LLMs Meet API Documentation: Can Retrieval Augmentation Aid Code Generation
  Just as It Helps Developers?'
arxiv_id: '2503.15231'
source_url: https://arxiv.org/abs/2503.15231
tags:
- code
- llms
- documents
- example
- libraries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates whether retrieval-augmented generation (RAG)
  can help large language models (LLMs) generate code using less common APIs by leveraging
  their documentation. It formulates API usage recommendation as a code completion
  task and evaluates RAG performance on four less common Python libraries (Polars,
  Ibis, GeoPandas, Ivy) with 1017 APIs.
---

# When LLMs Meet API Documentation: Can Retrieval Augmentation Aid Code Generation Just as It Helps Developers?

## Quick Facts
- arXiv ID: 2503.15231
- Source URL: https://arxiv.org/abs/2503.15231
- Reference count: 40
- LLMs' performance improved by 83%-220% with retrieval-augmented generation (RAG) compared to no documentation

## Executive Summary
This paper investigates whether retrieval-augmented generation (RAG) can help large language models (LLMs) generate code using less common APIs by leveraging their documentation. The authors formulate API usage recommendation as a code completion task and evaluate RAG performance on four less common Python libraries (Polars, Ibis, GeoPandas, Ivy) with 1017 APIs. The study finds that RAG significantly improves LLMs' performance, with example code being the most critical documentation component. BM25 is identified as the most effective retriever for this task, and LLMs demonstrate some ability to tolerate mild noise in documentation by referencing their pre-trained knowledge.

## Method Summary
The researchers crawled official documentation for four less common Python libraries (Polars, Ibis, GeoPandas, Ivy) and parsed them into structured components: Description, Parameter List, and Example Code. They constructed 1017 code completion tasks by separating example code into "Code Context" (lines before API call) and "Expected Output" (execution result). Using LangChain, they implemented a RAG pipeline with BM25 as the primary retriever and various LLMs (GPT-4o-mini, Qwen2.5-Coder-7B/32B, DeepSeek-Coder-V2-Lite) as generators. The pipeline retrieves the top-5 relevant API documents for each code context query and prompts the LLM to generate code that executes to match the expected output. Performance was measured using pass rate (execution result consistency) and retrieval recall@k.

## Key Results
- RAG improved LLMs' performance by 83%-220% compared to no documentation
- Example code contributes most to RAG success, surpassing descriptions and parameter lists
- BM25 proves most effective for retrieval in code completion tasks
- LLMs can tolerate mild noise in documentation by referencing pre-trained knowledge

## Why This Works (Mechanism)

### Mechanism 1: Lexical Matching Over Semantic Embedding
Code completion relies on specific structural tokens rather than abstract intent. BM25 leverages term frequency to match exact API names and variable types in the query context to the documentation, whereas dense embeddings may dilute these specific signals. This works because the query (code context) contains enough lexical overlap with the target API documentation for keyword matching to succeed. Break condition: If the user query is purely natural language with no code tokens or uses synonyms not present in the documentation, BM25 recall drops significantly.

### Mechanism 2: In-Context Pattern Replication
The LLM relies primarily on the Example Code section of the documentation to "learn" the API usage, treating retrieval as a few-shot prompting task. It identifies the structural pattern in the retrieved example (e.g., input types, function signature) and maps it to the partial code context in the prompt. This works because the retrieved example code is syntactically correct and functionally representative of the task. Break condition: If the example code uses parameters or syntax that conflicts with the current task, the LLM may hallucinate wrong usage by blindly copying the example.

### Mechanism 3: Noise Tolerance via Knowledge Priors
The RAG system exhibits robustness to documentation errors because the LLM cross-references flawed retrieved context against its internal pre-trained knowledge. When documentation contains mild noise (e.g., typos), the LLM recognizes discrepancies and prioritizes trusted context (often code structure or its own pre-training) over noisy text. This works because the LLM has encountered similar API patterns during pre-training, allowing it to detect inconsistencies. Break condition: High-severity noise that alters semantic meaning or introduces logically incorrect code will bypass this filter.

## Foundational Learning

- **Concept: Sparse vs. Dense Retrieval (BM25 vs. Vector Search)**
  - Why needed: The paper identifies BM25 (sparse) as superior to vector embeddings (dense) for this task. Understanding keyword matching vs. semantic vector distance is required to interpret RQ3.
  - Quick check: Does a dense retriever necessarily understand that `import polars` is semantically closer to `polars.read_csv` than `pandas.read_csv` if both appear in similar contexts?

- **Concept: In-Context Learning (ICL)**
  - Why needed: The findings rely on the LLM's ability to learn the API from the prompt without weight updates. The "Example Code" mechanism is essentially ICL.
  - Quick check: If you provide a documentation example with a bug, will the LLM correct it or repeat it? (The paper suggests it often repeats it).

- **Concept: Code Completion vs. Generation**
  - Why needed: The study formulates the problem as a code completion task (given context and expected output, fill the middle). This differs from generating code from scratch based on natural language.
  - Quick check: Why might code completion favor lexical retrievers (BM25) more than text-to-code generation? (Hint: shared syntax tokens).

## Architecture Onboarding

- **Component map:** Input (Code Context + Expected Output) -> Retriever (BM25 Search Engine) -> Knowledge Base (Parsed API Documents) -> Prompt Constructor (Assembles top-k retrieved docs) -> Generator (Code LLM) -> Evaluator (Executes generated code against expected output)

- **Critical path:** The quality of the Example Code in the documentation. If the parser fails to extract example code, or if library docs lack examples, the pipeline fails (Section 4.2.2 shows ~50% drop).

- **Design tradeoffs:**
  - BM25 vs. Embeddings: Use BM25 for exact syntax/API name matching. Use Dense Retrieval only if the user query is a vague natural language description.
  - Doc Content: Prioritize indexing Example Code. You may exclude Parameter Lists if token length is a concern, as they added little value and occasionally introduced noise.

- **Failure signatures:**
  - Hallucinated Usage: LLM copies a retrieved example exactly, failing to adapt parameters to the specific test case.
  - Retrieval Confusion: The retriever returns 5 similar APIs, and the LLM picks the wrong one despite the doc being present.

- **First 3 experiments:**
  1. Retriever Ablation: Run pipeline using BM25 vs. text-embedding-3-large model. Measure Recall@5. Expect BM25 to win on code-to-code matching.
  2. Content Ablation: Remove "Description" and "Parameter List" fields from retrieved context, leaving only "Example Code." Measure pass rate change.
  3. Noise Injection: Intentionally swap parameter names in documentation (e.g., change `alpha` to `beta`). Verify if LLM corrects it (robustness) or hallucinates the new parameter.

## Open Questions the Paper Calls Out
None

## Limitations
- Findings are constrained by focus on specific, less common Python libraries and limited API usage patterns
- The "noise tolerance" mechanism may reflect LLM's pre-trained knowledge rather than true robustness to documentation errors
- BM25 superiority is context-dependent and may not generalize to natural language queries or APIs with less structured documentation
- Evaluation only considers pass rate and retrieval recall, without examining quality of generated code beyond functional correctness

## Confidence
- **High confidence:** Retrieval augmentation improves code generation performance (83%-220% improvement); Example code is most critical documentation component; BM25 outperforms dense retrievers
- **Medium confidence:** LLMs can tolerate mild documentation noise by referencing pre-trained knowledge; Content prioritization improves performance
- **Low confidence:** The specific noise tolerance mechanism and its generalizability to different types of documentation errors

## Next Checks
1. Test BM25 vs. dense retrieval on diverse natural language API usage queries to determine if lexical matching advantage holds for vague or conversational requests
2. Conduct systematic ablation study removing Example Code, Parameter Lists, and Descriptions separately across multiple libraries to quantify relative contributions more precisely
3. Design controlled experiments injecting different types and severity levels of documentation errors (syntax vs. semantic) to validate actual noise tolerance mechanism and identify failure thresholds