---
ver: rpa2
title: Canonical Autoregressive Generation
arxiv_id: '2506.06446'
source_url: https://arxiv.org/abs/2506.06446
tags:
- token
- tokenization
- canonical
- generation
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models can generate the same output string with
  different tokenizations, leading to inconsistent pricing under pay-per-token models.
  This is particularly prevalent for non-English languages.
---

# Canonical Autoregressive Generation

## Quick Facts
- arXiv ID: 2506.06446
- Source URL: https://arxiv.org/abs/2506.06446
- Reference count: 40
- Primary result: Method provably generates token sequences closer to true training distribution while maintaining comparable quality and runtime to standard generation

## Executive Summary
Large language models can generate identical output strings using different tokenizations, creating inconsistent pricing under pay-per-token models—especially problematic for non-English languages. The authors propose canonical generation, a method that restricts models to only generate canonical tokenizations. This is achieved through an efficient sampling algorithm based on the Gumbel-Max trick that enforces the constraint at every generation step. The approach provably reduces the gap between model output distribution and the true training distribution while maintaining comparable performance across multiple natural language tasks.

## Method Summary
Canonical generation addresses tokenization multiplicity by constraining autoregressive models to generate only canonical tokenizations. The method uses a Gumbel-Max sampling algorithm that adds Gumbel noise to log-probabilities, sorts tokens by perturbed score, and returns the first token that results in a canonical sequence when appended. Canonicity is checked by decoding the sequence plus candidate token, re-encoding it, and verifying it matches the canonical encoding. The algorithm is applied at every generation step, leveraging the "non-recovering" property of BPE, Unigram, and Wordpiece tokenizers—where non-canonical prefixes cannot be "healed" back to canonical forms by subsequent tokens.

## Key Results
- Canonical generation reduces tokenization multiplicity probability from ~80% to near zero in multilingual experiments
- Runtime overhead remains comparable to standard generation (typically 1-2x, not orders of magnitude slower)
- Translation quality (xCOMET), spell checking accuracy, and rephrasing similarity scores remain statistically equivalent to standard generation
- Theoretical guarantees show KL-divergence between output and true distributions strictly decreases compared to standard generation

## Why This Works (Mechanism)

### Mechanism 1: The Non-Recovering Property of Tokenizers
- **Claim:** Every prefix generated during autoregression must be canonical for the final output to be canonical
- **Mechanism:** BPE, Unigram, and Wordpiece tokenizers are deterministic but "non-recovering"—if a generation step deviates from the canonical merge path, subsequent tokens cannot "heal" the sequence back to canonical form. Therefore, constrained generation must filter tokens at every step.
- **Core assumption:** Tokenizers use deterministic, greedy, or maximization-based encoding algorithms (standard in modern LLMs)
- **Evidence anchors:** [Section 4.1] Theorem 2 proves BPE, Unigram, and Wordpiece are non-recovering; [Abstract] emphasizes partial canonical token sequences are necessary

### Mechanism 2: Gumbel-Max Constrained Sampling
- **Claim:** One can sample from a "canonicalized" distribution without explicitly calculating the normalization factor for the entire vocabulary
- **Mechanism:** By adding Gumbel noise to log-probabilities and sorting tokens by perturbed score, the algorithm iterates through the ranked list and selects the first token resulting in a canonical sequence. This is mathematically equivalent to sampling from a distribution where non-canonical probabilities are set to zero and remaining mass is renormalized proportionally.
- **Core assumption:** Model assigns low probability to most non-canonical tokens, ensuring quick token selection (often top-1 or top-k)
- **Evidence anchors:** [Section 4.2] Algorithm 1 details sorting and checking loop; [Abstract] describes the efficient sampling algorithm

### Mechanism 3: Distributional Alignment via Renormalization
- **Claim:** Canonical generation produces output distributions provably closer to the training data distribution than standard generation
- **Mechanism:** Training data consists exclusively of canonical sequences. Standard generation "leaks" probability mass into non-canonical paths with zero probability in the training distribution. By cutting off these paths and redistributing their mass to canonical tokens proportionally, KL-divergence between model's output distribution d and true distribution p is strictly reduced (Theorem 3).
- **Core assumption:** Model assigns non-zero probability to at least some non-canonical sequences; otherwise, no mass exists to redistribute
- **Evidence anchors:** [Section 4.3] Theorem 3 proves KL(p, ~d) < KL(p, d); "Broken Tokens?" paper supports premise that LMs can generate non-canonical forms

## Foundational Learning

- **Concept: BPE / Subword Tokenization**
  - **Why needed here:** Solution relies on identifying "canonical" tokenization; understanding merge rules is essential for implementing `is_canonical(sequence)` check efficiently
  - **Quick check question:** If vocabulary has merges `a + b -> ab` and `ab + c -> abc`, is `['a', 'b', 'c']` a canonical sequence for "abc"?

- **Concept: Autoregressive Sampling (Logits to Tokens)**
  - **Why needed here:** Proposed method modifies standard sampling loop; distinguishing between model's raw output (logits) and final token selection is crucial
  - **Quick check question:** Does standard temperature sampling modify the logits or the probabilities derived from them?

- **Concept: KL-Divergence**
  - **Why needed here:** Metric used to theoretically prove method is an improvement over standard generation
  - **Quick check question:** Does a lower KL-divergence between a model's output and reference text imply the output is more "surprising" or "expected" relative to the reference?

## Architecture Onboarding

- **Component map:** LLM Forward Pass -> Perturbation Layer -> Sorting Mechanism -> Canonical Validator -> Token Emission
- **Critical path:** The Canonical Validator is the bottleneck; implementing this efficiently (e.g., checking only the last "word" or valid substring rather than full sequence history) is key to maintaining comparable runtime
- **Design tradeoffs:**
  - Exactness vs. Speed: Exact distributional improvement vs. approximate check for faster performance
  - Redistribution Strategy: Proportional redistribution vs. "top-1" forced choice (changes sampling temperature/strategy)
- **Failure signatures:**
  - Runtime spikes if model outputs high-probability non-canonical tokens (common in low-resource languages)
  - Potential repetition loops if constraint filtering becomes too aggressive and narrows valid token space
- **First 3 experiments:**
  1. Unit Test the Validator: Generate non-canonical prefix, append correct subsequent tokens, confirm tokenizer never re-synthesizes canonical form
  2. Latency Profiling: Measure overhead of `enc(dec(seq + token))` check vs standard sampling time
  3. Price Variance Audit: Run 100 non-English translation prompts, compare standard deviation of output token counts between standard and canonical generation (should drop to near zero)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can global probability redistribution strategies improve practical performance of canonical generation compared to local, proportional redistribution method?
- **Basis in paper:** [explicit] Section 5 states investigating "global strategies beyond (next-token) sampling to redistribute probability mass" would be worth exploring, noting current local approach occasionally restricts sampling to low-probability paths
- **Why unresolved:** Current implementation only redistributes probability mass among remaining tokens at immediate step, limiting access to high-probability sequences requiring non-canonical intermediate steps
- **What evidence would resolve it:** Empirical benchmarks showing improved task scores using sequence-level or beam-search-based redistribution algorithms compared to standard canonical generation results

### Open Question 2
- **Question:** What is the necessary and sufficient formal property that makes a tokenizer "non-recovering," ensuring partial canonical sequences imply full canonical sequences?
- **Basis in paper:** [explicit] Section 5 calls for work to "better understand what property a tokenizer needs to satisfy for our result to hold," noting it's surprising BPE, Unigram, and Wordpiece all satisfy this despite different algorithms
- **Why unresolved:** Paper provides individual proofs for three tokenization algorithms but lacks unified theoretical characterization explaining why canonical sequence cannot "recover" from non-canonical prefix across diverse methods
- **What evidence would resolve it:** Formal mathematical proof identifying specific structural property (e.g., related to greedy optimization or merge ordering) as sufficient condition for non-recovering property

### Open Question 3
- **Question:** Can training interventions, such as fine-tuning on specific languages or using specialized tokenizers, natively reduce tokenization multiplicity without requiring constrained generation?
- **Basis in paper:** [explicit] Section 5 asks whether "commonly used practices to improve multilingual language generation... may reduce prevalence of tokenization multiplicity"
- **Why unresolved:** Paper focuses on inference-time solutions and doesn't explore whether model's internal probability distributions can be shaped during training to naturally prefer canonical tokenizations
- **What evidence would resolve it:** Experiments measuring non-canonicity rates of models specifically fine-tuned for minority languages or equipped with language-specific vocabularies compared to baseline rates

## Limitations

- Theoretical guarantees depend on assumptions about tokenizer properties and training data composition that may not hold universally
- Experimental validation limited to two model families (Llama and Qwen), leaving questions about generalizability to other architectures
- Method doesn't address potential quality impacts for code generation or technical domains where tokenization choices might carry semantic implications

## Confidence

**High Confidence:** Empirical observation that tokenization multiplicity exists and causes pricing inconsistencies (supported by quantitative measurements); Gumbel-Max sampling algorithm correctly implements described mechanism
**Medium Confidence:** Theoretical proofs of non-recovering tokenizer properties and distributional improvement (mathematically sound given stated assumptions); runtime performance claims (comparable to standard generation) supported by measurements but could vary
**Low Confidence:** Assertion that canonical generation "provably generates token sequences closer to true training distribution" requires accepting strong assumption that training data contains only canonical sequences; claim that quality remains "comparable" assumes tested tasks are representative

## Next Checks

1. **Stress Test the Non-Recovering Property:** Systematically construct tokenizer merge rules that could theoretically allow non-canonical prefixes to recover canonical forms; test with custom BPE implementations on edge-case vocabulary constructions to identify counterexamples to Theorem 2

2. **Pathological Probability Distribution Analysis:** Train or fine-tune a model specifically to assign high probability mass to non-canonical tokens in controlled setting; measure sampling algorithm's iteration depth and runtime overhead to establish worst-case complexity bounds empirically

3. **Cross-Architecture Generalization Study:** Apply canonical generation to at least two additional model families with different tokenization approaches (e.g., SentencePiece with subword regularization, or models using byte-level BPE variants); compare magnitude of multiplicity reduction and runtime overhead to assess architecture-dependence