---
ver: rpa2
title: 'To See or To Read: User Behavior Reasoning in Multimodal LLMs'
arxiv_id: '2511.03845'
source_url: https://arxiv.org/abs/2511.03845
tags:
- product
- user
- reasoning
- prediction
- purchase
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper explores whether multimodal large language models (MLLMs)\
  \ benefit more from textual or visual representations of user behavior data for\
  \ next-purchase prediction. The authors introduce BehaviorLens, a benchmarking framework\
  \ that compares three representations\u2014text, scatter plot, and flowchart\u2014\
  of sequential transaction data using six MLLM models."
---

# To See or To Read: User Behavior Reasoning in Multimodal LLMs

## Quick Facts
- arXiv ID: 2511.03845
- Source URL: https://arxiv.org/abs/2511.03845
- Reference count: 21
- Primary result: Image-based representations of user behavior improve next-purchase prediction accuracy by up to 87.5% compared to text alone

## Executive Summary
This paper investigates whether multimodal large language models (MLLMs) benefit more from textual or visual representations of user behavior data for next-purchase prediction. The authors introduce BehaviorLens, a benchmarking framework that compares three representations—text, scatter plot, and flowchart—of sequential transaction data using six MLLM models. Using a real-world purchase sequence dataset, they find that image-based representations (scatter plot and flowchart) improve next-purchase prediction accuracy by up to 87.5% compared to text alone, with no additional computational cost. The improvement is consistent across multiple models, including both large and small variants, and is not driven by explanation quality differences. Flowcharts especially benefit Gemini models, while scatter plots tend to perform better for GPT models.

## Method Summary
The study uses a real-world e-commerce dataset with 100 users, 1537 items, and 268 product types. For each user, the last n=20 purchases are held out for prediction. Three representation formats are created: text transcripts, scatter plots (x-axis: purchase order, y-axis: product type rank), and flowcharts (temporal sequence as node-edge graph). Six MLLMs (Gemini-2.0-flash/lite, Gemini-2.5-flash/lite, GPT-4o, GPT-4.1-mini) are queried with each representation format using standardized prompts. Predictions are evaluated for accuracy (exact match) and similarity (cosine similarity of embeddings), while reasoning quality is assessed via LLM-as-a-Judge across six metrics. Token count and latency are also measured.

## Key Results
- Image-based representations (scatter plot and flowchart) improve next-purchase prediction accuracy by up to 87.5% compared to text alone
- Flowcharts perform particularly well for certain models like Gemini-2.0-flash, while scatterplots excel with others like GPT-4o
- The improvement stems from better input representation rather than explanation quality
- No additional computational cost is incurred when using image representations

## Why This Works (Mechanism)

### Mechanism 1: Holistic Spatial Encoding
Visual representations enable more accurate next-purchase prediction by preserving structural and topological patterns of user behavior that are lost in linear text. Image-based inputs allow MLLMs to process the user journey as a spatial layout, leveraging visual priors to recognize recurring sequences, co-occurrence clusters, and temporal transitions as distinct visual patterns rather than flattened token sequences.

### Mechanism 2: Improved Input Representation, Not Explanation Quality
Accuracy gains stem primarily from how user history is represented, not from the quality of intermediate reasoning or explanations. The visual modality compresses sequential data into a format that better surfaces predictive signals (e.g., recurring product type purchases). This representation change directly impacts prediction; explanation quality remains similar across modalities.

### Mechanism 3: Model-Specific Visual Format Preferences
Different MLLM architectures respond differently to visual formats—flowcharts favor some models (e.g., Gemini-2.0-flash), while scatter plots favor others (e.g., GPT-4o). Vision encoders and cross-modal alignment layers differ across MLLMs, leading to varied effectiveness in parsing specific visual structures (graph-like vs. coordinate-based).

## Foundational Learning

### Concept: Multimodal LLM (MLLM)
**Why needed here:** The paper's entire premise is testing how MLLMs process user behavior data in different modalities. Understanding how vision encoders and language models interact is essential to interpret the results.
**Quick check question:** How does an MLLM differ from a standard LLM when processing a purchase history presented as an image?

### Concept: Input Representation / Modality Trade-off
**Why needed here:** The core research question is whether text or image representations optimize reasoning. Learners must grasp why the same data, differently encoded, could lead to different model behaviors.
**Quick check question:** If you convert a time series of purchases into a scatter plot, what information could be gained or lost compared to a text list?

### Concept: Next-Purchase Prediction as a Classification Task
**Why needed here:** The experimental task is framed as a fixed candidate set prediction. Understanding the task setup is critical for interpreting accuracy and similarity metrics.
**Quick check question:** Why might fixing the candidate set (e.g., 20 product types) make the task more controlled but potentially less realistic?

## Architecture Onboarding

### Component map:
Input Layer (ϕ_text, ϕ_scatter-plot, ϕ_flowchart) -> Model Layer (6 MLLMs) -> Evaluation Layer (Prediction accuracy/similarity, Reasoning quality via LLM-as-a-Judge, Cost metrics)

### Critical path:
1. Data Preparation: Sample 20 purchases per user, create text, scatter plot, and flowchart representations
2. Prompt Construction: Use standardized system and user prompts (Appendix B)
3. Model Inference: Query MLLMs for prediction and reasoning
4. Evaluation: Compute accuracy/similarity; run LLM-as-a-Judge for reasoning quality
5. Comparison: Analyze performance across modalities and models

### Design tradeoffs:
- Visualization Complexity: Scatter plots and flowcharts must balance clarity with information density
- Token vs. Accuracy: Image representations may increase token count but often improve accuracy
- Generalizability: The study uses n=20 purchases and a fixed candidate set of 20 product types

### Failure signatures:
- No Accuracy Gain: If image representations do not outperform text, investigate whether visual encoding is too sparse
- High Token Cost with Low Gain: Scatter plots may increase token usage without accuracy improvement
- Inconsistent Explanations: If explanation metrics vary wildly, the LLM-as-a-Judge may be unstable

### First 3 experiments:
1. Reproduce Text vs. Image Baseline: Replicate the text, scatter plot, and flowchart comparison on the provided dataset
2. Ablate Visual Encoding Complexity: Simplify or enhance scatter plots and measure the impact on accuracy and token usage
3. Cross-Model Format Robustness: Swap the "best" formats to confirm whether model-specific preferences hold

## Open Questions the Paper Calls Out

### Open Question 1
How can visual representations of user journeys be optimized to better capture temporal and spatial dynamics in behavior data? The study only tested basic scatter plots and flowcharts; it did not explore advanced visual encodings that might compress information more efficiently.

### Open Question 2
Does the performance improvement of image inputs hold true for significantly longer user behavior sequences? The experiments were limited to the last n=20 purchases; it's unclear if visual clutter would degrade performance for longer histories.

### Open Question 3
Are the benefits of image representations robust when scaled to a significantly larger user population? The study utilized a small sample of only 100 users, which may not capture the full variance of real-world user behavior.

### Open Question 4
Why do different model families exhibit strong preferences for specific visual structures (e.g., flowcharts vs. scatter plots)? The paper confirms the performance gap exists but does not isolate whether the cause is the model's visual encoder architecture or its specific pre-training data distribution.

## Limitations
- Proprietary dataset prevents independent validation of the 87.5% accuracy improvement claim
- Visual encoding details (scatter plot resolution, flowchart layout algorithms) are underspecified
- LLM-as-a-Judge evaluation protocol lacks transparency regarding the specific model used and prompt configuration

## Confidence
- **High Confidence:** Visual representations preserve structural patterns better than flattened text
- **Medium Confidence:** Accuracy gains stem from improved input representation rather than explanation quality
- **Low Confidence:** Model-specific format preferences could be noise-driven

## Next Checks
1. Replicate the study using a publicly available sequential recommendation dataset to verify visual representation benefits
2. Systematically vary scatter plot complexity and flowchart layout algorithms to identify which visual features drive accuracy improvements
3. Test whether MLLMs trained primarily on text can effectively leverage visual encodings or require vision-encoder pretraining