---
ver: rpa2
title: How to Marginalize in Causal Structure Learning?
arxiv_id: '2511.14001'
source_url: https://arxiv.org/abs/2511.14001
tags:
- learning
- structure
- bayesian
- probabilistic
- distribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of marginalizing over parent
  sets in Bayesian structure learning, a critical but computationally expensive task.
  Traditional methods rely on dynamic programming, which becomes intractable for large
  numbers of candidate parents.
---

# How to Marginalize in Causal Structure Learning?

## Quick Facts
- arXiv ID: 2511.14001
- Source URL: https://arxiv.org/abs/2511.14001
- Reference count: 8
- Primary result: Probabilistic circuits enable exact marginalization of parent sets in Bayesian structure learning, improving performance over parent set restrictions

## Executive Summary
This paper addresses the challenge of marginalizing over parent sets in Bayesian structure learning, a computationally expensive operation that traditional dynamic programming approaches cannot scale to large numbers of candidate parents. The authors propose using tractable probabilistic circuits (PCs) to learn a distribution that approximates Bayesian scores and enables exact marginalization. Their method involves training a PC on both original and marginalized probability masses using a novel two-phase learning algorithm. When applied to the TRUST framework, their approach demonstrates improved performance across multiple metrics compared to standard methods that restrict the number of candidate parents.

## Method Summary
The authors introduce a novel approach to Bayesian structure learning that leverages tractable probabilistic circuits to handle marginalization over parent sets. Their method consists of two main phases: first, learning a PC distribution that captures the Bayesian scores of different parent sets; second, using this learned PC to perform exact marginalization during structure search. The key innovation is a two-phase learning algorithm that trains the PC on both original and marginalized probability masses, allowing it to approximate the exact marginalization operation efficiently. This approach is integrated into the TRUST framework for structure learning, enabling it to consider a much larger space of parent sets without the computational burden of exact dynamic programming.

## Key Results
- The PC-based approach achieves higher AUROC and lower MSE-CE metrics compared to standard parent set restriction methods
- Exact marginalization is approximated effectively without the computational cost of dynamic programming
- The method scales better to larger numbers of candidate parents than traditional approaches

## Why This Works (Mechanism)
The method works by exploiting the tractability properties of probabilistic circuits, which allow for efficient computation of marginal probabilities. When a PC is trained to represent the distribution of Bayesian scores over parent sets, its structure inherently supports marginalization operations through its arithmetic circuit representation. The two-phase learning algorithm ensures that the PC captures not just the original score distribution but also the marginalized distributions needed for structure learning. This allows the structure search algorithm to query the PC for marginal probabilities over subsets of parent sets, effectively approximating the exact marginalization that would otherwise require intractable dynamic programming.

## Foundational Learning
- **Bayesian Structure Learning**: The task of inferring causal DAGs from data by maximizing Bayesian scores. Needed because the paper builds on this fundamental problem. Quick check: Can you explain the difference between score-based and constraint-based approaches?
- **Probabilistic Circuits**: A class of tractable probabilistic models that support efficient marginal inference. Needed because they form the core computational machinery. Quick check: What properties make PCs tractable for marginalization?
- **Parent Set Marginalization**: The operation of summing over all possible parent sets consistent with a given marginal. Needed because this is the computational bottleneck the paper addresses. Quick check: Why is exact marginalization NP-hard in general Bayesian networks?
- **TRUST Framework**: A specific Bayesian structure learning algorithm that the authors integrate their method into. Needed for context on the empirical evaluation. Quick check: What are the key steps in the TRUST algorithm?
- **Dynamic Programming for Structure Learning**: Traditional approach for exact marginalization that becomes intractable. Needed to understand the computational challenge. Quick check: At what point does dynamic programming become computationally infeasible?
- **Bayesian Scores**: The posterior probability of a DAG given data, typically using BDeu or similar scoring functions. Needed because these scores are what the PC learns to approximate. Quick check: How do Bayesian scores decompose over DAG structure?

## Architecture Onboarding
**Component Map**: Data -> Preprocessor -> Bayesian Score Calculator -> PC Trainer (Phase 1) -> PC Trainer (Phase 2) -> TRUST Framework -> Structure Output

**Critical Path**: The critical path involves computing Bayesian scores for candidate parent sets, training the PC in two phases to capture both original and marginalized distributions, and then using the trained PC within TRUST to perform efficient marginalization during structure search.

**Design Tradeoffs**: The main tradeoff is between PC expressiveness (which affects approximation quality) and computational efficiency (training and inference time). A more complex PC can better approximate the true marginalization but requires more training data and computation. The two-phase learning approach trades off simplicity for better coverage of the marginalization space.

**Failure Signatures**: If the PC fails to learn accurate marginalization, the structure learning algorithm will produce suboptimal DAGs with poor Bayesian scores. This manifests as degraded AUROC and increased MSE-CE metrics compared to baselines. Poor scaling behavior (excessive training time or memory usage) indicates the PC architecture is too complex for the problem size.

**First Experiments**: 
1. Verify the PC can accurately marginalize over small, manually constructed parent sets where exact computation is feasible
2. Test the two-phase learning algorithm on synthetic datasets with known ground truth structures
3. Compare the learned PC's marginalization accuracy against exact dynamic programming on small problems

## Open Questions the Paper Calls Out
The paper identifies several open questions: How to extend the approach to handle latent variables and selection bias? What is the theoretical guarantee on approximation quality when using PCs for marginalization? Can the method be adapted to work with continuous variables or more complex scoring functions? How does the choice of PC architecture affect the quality of marginalization in practice?

## Limitations
- Computational scalability to very large variable sets remains unproven
- Performance sensitivity to PC structure and hyperparameter choices is not fully characterized
- Limited evaluation on real-world datasets beyond synthetic benchmarks

## Confidence
- **High confidence**: The mathematical framework connecting PCs to exact marginalization of parent sets is sound and well-established in the probabilistic circuits literature.
- **Medium confidence**: The empirical improvements in AUROC and MSE-CE metrics when applied to the TRUST framework are reproducible and significant, though the magnitude of improvement may vary with dataset characteristics.
- **Low confidence**: The generalizability of the approach to extremely high-dimensional problems (hundreds of variables) and its robustness to different data distributions have not been thoroughly validated.

## Next Checks
1. Conduct scalability experiments with datasets containing 50+ variables to assess computational tractability and performance degradation points.
2. Perform ablation studies varying PC architecture parameters (e.g., treewidth, depth) to identify optimal configurations for different problem types.
3. Test the method on multiple real-world benchmark datasets from different domains (e.g., gene regulatory networks, social networks) to evaluate robustness across diverse dependency structures.