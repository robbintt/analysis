---
ver: rpa2
title: 'Minimal Clips, Maximum Salience: Long Video Summarization via Key Moment Extraction'
arxiv_id: '2512.11399'
source_url: https://arxiv.org/abs/2512.11399
tags:
- video
- clips
- clip
- selection
- summary
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of efficiently summarizing long
  videos, particularly movies, while retaining critical visual information. The core
  idea is to select key video clips that contain important visual information not
  inferable from dialogue transcripts alone.
---

# Minimal Clips, Maximum Salience: Long Video Summarization via Key Moment Extraction

## Quick Facts
- **arXiv ID**: 2512.11399
- **Source URL**: https://arxiv.org/abs/2512.11399
- **Reference count**: 40
- **Primary result**: Select key video clips containing unique visual information for long video summarization, achieving high visual recall (21.05) with only 6% of video content

## Executive Summary
This paper addresses the challenge of efficiently summarizing long videos, particularly movies, while retaining critical visual information. The core idea is to select key video clips that contain important visual information not inferable from dialogue transcripts alone. The method segments videos into 20-second clips, generates lightweight captions for each, and uses a large language model to select the top K most relevant clips. These selected clips are then recaptioned in detail and integrated into the transcripts to create a multimodal screenplay for summarization. Experiments on the MovieSum dataset show that using only 6% of the video content (derived from reference clips) is sufficient for a complete multimodal summary. The proposed clip selection method outperforms baselines like random and silent clip selection, capturing substantially more relevant visual information while maintaining low computational cost.

## Method Summary
The method follows a two-stage captioning and selection pipeline: (1) Video segmentation into 20-second clips, (2) Lightweight VLM captioning of all clips, (3) LLM-based top-K clip selection, (4) Strong VLM recaptioning of selected clips, (5) Integration with transcripts to create multimodal screenplay, and (6) LLM summarization of the combined document. The approach exploits the observation that movies exhibit high cross-modal redundancy, with less than 6% of video content containing all visually unique information needed for complete summaries. By focusing on "visually salient" clips—those containing information not inferable from transcripts alone—the method achieves significant computational efficiency while maintaining summary quality.

## Key Results
- Visual recall reaches 21.05 with K=25 selected clips, significantly outperforming random (4.72) and silent clip (3.50) baselines
- Using only 6% of video content (derived from reference clips) is sufficient for complete multimodal summaries
- Two-stage captioning strategy maintains low computational cost while achieving high visual recall
- The method successfully identifies clips containing information not inferable from dialogue alone

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A two-stage captioning strategy with asymmetric model capacities enables cost-effective clip selection without sacrificing summary quality.
- Mechanism: Lightweight VLM (Qwen2.5-Omni-3B/7B) generates captions for ALL clips at low cost → LLM selects K salient clips → Stronger VLM (Gemini 2.5 Flash-Lite) recaptions ONLY selected clips. The division of labor exploits that "lightweight captions are sufficient for identifying salient clips" while finer details require stronger models.
- Core assumption: Caption quality sufficient for ranking/salience detection differs from caption quality needed for final summarization.
- Evidence anchors:
  - [abstract] "We maintain low computational cost by relying on a lightweight captioning model."
  - [section 7, Table 4] Recaptioning improves visual recall from 19.31 to 21.05 (K=25), showing "consistent decrease in the visual recall when no recaptioning is being performed."
  - [corpus] Weak corpus evidence—related papers focus on video editing rather than hierarchical captioning strategies.
- Break condition: If lightweight captions have insufficient action/event vocabulary to distinguish salient from non-salient clips, selection degrades. Table 1 shows gold screenplay captions achieve 39.56 R@25 vs. 11.89 for Qwen2.5-Omni-7B, indicating caption quality is a bottleneck.

### Mechanism 2
- Claim: Operating selection at the caption level (text) rather than frame level enables LLM-based reasoning over full video context.
- Mechanism: The pipeline segments video into 20-second clips, captioning converts visual content to text, and an LLM performs top-K selection by reasoning over ALL captions simultaneously. This preserves "temporal information" by treating clips as "the basic unit" rather than individual frames, allowing global context comparison.
- Core assumption: Textual descriptions capture sufficient salience signals for the LLM to rank clips; salience is primarily about narrative importance rather than visual fidelity.
- Evidence anchors:
  - [abstract] "These [captions] are then passed to a large language model (LLM), which selects the K clips."
  - [section 1] "video summarization needs an understanding of the full context to identify key moments... we define it as selecting the top K most relevant clips from the entire video."
  - [corpus] REGen paper notes extractive summarization "struggles to produce a coherent narrative"—consistent with this paper's global LLM reasoning approach.
- Break condition: If visual salience requires fine-grained spatial details (e.g., specific facial expressions, background elements) not captured in compact captions, the LLM will mis-rank clips.

### Mechanism 3
- Claim: Approximately 6% of video content contains the visually unique information necessary for a complete multimodal summary, validating clip selection as a compression strategy.
- Mechanism: Reference clips derived from human screenplays and summaries show that "less than 6% of the movie" captures all visual facts needed. The method targets "visually salient" clips—those containing "relevant visual information that cannot be inferred from the transcripts alone," exploiting redundancy between visual and dialogue modalities.
- Core assumption: Movies exhibit high cross-modal redundancy; much visual content is redundant with dialogue.
- Evidence anchors:
  - [abstract] "reference clips (less than 6% of the movie) are sufficient to build a complete multimodal summary"
  - [section 6.2, Table 2] "Filtered Gold Screenplay (avg. 6% clips)" achieves MFACTSUM 34.23 vs. 34.97 for full screenplay.
  - [corpus] No direct corpus evidence on redundancy ratios in long-form video.
- Break condition: If a video domain has low dialogue-visual redundancy (e.g., nature documentaries, silent films), the 6% heuristic may under-sample required visual content.

## Foundational Learning

- Concept: Vision-Language Model (VLM) captioning capabilities and token limits
  - Why needed here: Pipeline uses VLMs at two stages with different quality-cost tradeoffs. Understanding model capacity helps select appropriate models for each stage.
  - Quick check question: Can you explain why a 3B parameter model might suffice for salience ranking but fail for detailed description?

- Concept: Top-K selection vs. binary classification formulation
  - Why needed here: The paper formulates clip selection as ranking (select K best) rather than per-clip binary decisions, which requires global comparison across all clips.
  - Quick check question: How does top-K selection differ from threshold-based binary selection in terms of context requirements?

- Concept: Multimodal evaluation metrics (MFACTSUM, visual/textual recall)
  - Why needed here: Standard metrics (ROUGE, METEOR) are "not primarily designed for multimodality" and may not capture visual information retention.
  - Quick check question: Why would ROUGE scores remain similar while visual recall improves when adding clip selection?

## Architecture Onboarding

- Component map: Video → [Segment: 20s clips] → [Lightweight VLM caption] → [LLM clip selection] → [Strong VLM recaption] → [Screenplay builder: merge w/ transcripts] → [LLM summarizer] → Summary

- Critical path: The caption quality for selection (lightweight VLM) directly gates recall. Table 1 shows 7B model outperforms 3B (R@25: 11.89 vs 11.14 zero-shot). Invest in captioning quality assessment first.

- Design tradeoffs:
  - K value: Larger K improves clip recall but saturates summary quality due to fixed length constraint (Table 3). Start with K=25-50.
  - Clip duration: 20s fixed clips vs. scene segmentation. Table 5 shows scene segmentation doesn't improve over fixed 20s clips.
  - Two-shot prompting: Helps at low K, marginal at high K (Table 1: R@25 improves from 11.89 to 13.82 with two-shot).

- Failure signatures:
  - Low visual recall but high textual recall → Caption quality issue; lightweight VLM missing visual details
  - Recall improves with K but summary metrics don't → Summary length constraint; consider adaptive K or relaxed length
  - Random clip baseline approaches your method → Selection prompt needs refinement; check LLM is receiving full caption context

- First 3 experiments:
  1. Baseline comparison: Run random clips, silent clips, and your method with K=25/50/75 on 10 movies to establish Recall@K and visual recall baselines.
  2. Ablation on captioning: Compare lightweight-only vs. lightweight+recaptioning to quantify the visual recall gain from recaptioning.
  3. Caption quality sweep: Test Qwen2.5-Omni-3B vs. 7B vs. gold screenplay captions to measure sensitivity to caption quality and identify your quality floor.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can adaptive clip selection strategies be developed that dynamically choose the number of clips (K) based on video duration and content density?
- Basis in paper: [explicit] The authors state in the Limitations section: "Adaptive clip selection strategies that dynamically choose K based on the video duration and density would be useful to explore."
- Why unresolved: The current experiments utilize fixed values for K (25, 50, 75) and a fixed summary length (1000 words), which limits the ability to assess how variable clip counts might optimize summarization for different video types.
- What evidence would resolve it: An algorithm that adjusts K per video (e.g., based on scene change frequency or narrative density) demonstrating improved summarization metrics (MFACTSUM) compared to fixed-K baselines across a diverse dataset.

### Open Question 2
- Question: How does the performance of this clip selection method change in an unconstrained summarization setting where summary length is not fixed?
- Basis in paper: [explicit] The paper suggests: "Such adaptive strategies for varying K could be particularly beneficial in an unconstrained summarization setting, where the summary length is not fixed and this could be investigated in future work."
- Why unresolved: The current study truncates summaries to 1000 words to ensure fair comparison, which forces the model to discard information as K increases, potentially masking the benefits of retrieving more clips.
- What evidence would resolve it: Experiments evaluating the pipeline without output token limits, showing that increasing K leads to monotonically increasing recall of visual facts (visual recall) without degrading summary coherence.

### Open Question 3
- Question: Can the method be successfully generalized to other multimodal generative tasks or video domains outside of narrative movies?
- Basis in paper: [explicit] The Conclusion states: "Future work could extend this methodology to other multimodal generative tasks and domains, and explore different selection criteria."
- Why unresolved: The evaluation is restricted to the MovieSum dataset, which consists of narrative films with distinct screenplays and dialogue structures.
- What evidence would resolve it: Successful application of the lightweight captioning and LLM selection pipeline to other domains (e.g., instructional videos, news broadcasts) or tasks (e.g., question answering) with maintained or improved efficiency and accuracy.

## Limitations

- The 6% visual redundancy claim, while compelling, lacks direct empirical validation across diverse video domains beyond narrative movies
- Method performance depends heavily on caption quality, with significant gaps between lightweight VLM and gold screenplay captions
- Potential biases in the MovieSum dataset may limit generalization to other long-form video types
- Computational efficiency gains are partially offset by the need for two VLM captioning stages

## Confidence

- Clip selection improves visual recall while maintaining summary quality: **High**
- Two-stage captioning strategy achieves cost-effective salience detection: **Medium** (caption quality impact uncertain)
- 6% of video content contains sufficient visual information for complete summaries: **Low** (requires domain generalization validation)
- Fixed 20-second clips perform as well as scene-based segmentation: **Medium** (limited ablation evidence)

## Next Checks

1. **Caption quality validation**: Compare the complete pipeline performance using lightweight captions (Qwen2.5-Omni-7B) versus gold screenplay captions across the same 10-movie test set to quantify the upper bound of visual recall improvement from better captioning.

2. **Cross-domain generalization**: Apply the method to non-MovieSum content (e.g., documentaries, sports broadcasts, or instructional videos) to test whether the 6% redundancy heuristic holds across domains with varying dialogue-visual relationships.

3. **LLM selection robustness**: Evaluate different LLM selection strategies (zero-shot vs two-shot, different model sizes) to determine sensitivity to prompt engineering and whether the method's performance depends on specific LLM capabilities or can generalize across models.