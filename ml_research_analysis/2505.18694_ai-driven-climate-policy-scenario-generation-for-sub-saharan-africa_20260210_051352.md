---
ver: rpa2
title: AI-Driven Climate Policy Scenario Generation for Sub-Saharan Africa
arxiv_id: '2505.18694'
source_url: https://arxiv.org/abs/2505.18694
tags:
- climate
- energy
- policy
- scenarios
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces an AI-driven approach to generate climate
  policy scenarios for Sub-Saharan Africa using large language models (LLMs) and retrieval-augmented
  generation (RAG). By leveraging UN COP documents as a knowledge base, the method
  creates plausible and diverse policy scenarios focused on energy transition challenges.
---

# AI-Driven Climate Policy Scenario Generation for Sub-Saharan Africa

## Quick Facts
- **arXiv ID:** 2505.18694
- **Source URL:** https://arxiv.org/abs/2505.18694
- **Reference count:** 40
- **Primary result:** 88% of generated climate policy scenarios passed expert validation, demonstrating effectiveness of AI-driven RAG approach for Sub-Saharan Africa policy planning

## Executive Summary
This study introduces an AI-driven approach to generate climate policy scenarios for Sub-Saharan Africa using large language models (LLMs) and retrieval-augmented generation (RAG). The method leverages UN COP documents as a knowledge base to create plausible and diverse policy scenarios focused on energy transition challenges. Automated evaluation using three metrics—faithfulness, answer relevancy, and context utilization—was performed alongside human expert assessment. Results show that 88% of generated scenarios passed expert validation, with strong performance across evaluators: mean faithfulness scores ranged from 0.760 (human) to 0.966 (gemma2-2B), answer relevancy exceeded 0.888, and context utilization scores were above 0.713.

The findings demonstrate the effectiveness of generative AI in producing coherent, relevant, and diverse climate policy scenarios, offering a scalable tool for policy planning in data-constrained regions. Spearman correlation analysis confirmed high alignment between human and model evaluations, particularly for faithfulness. The study addresses a critical gap in climate policy analysis for Sub-Saharan Africa by providing a systematic approach to scenario generation that can be adapted to various regional contexts and policy domains.

## Method Summary
The study employs a retrieval-augmented generation (RAG) pipeline to generate climate policy scenarios for Sub-Saharan Africa. The approach processes 94 UN COP PDF documents using LangChain's PyPDFLoader and RecursiveCharacterTextSplitter (chunk_size=1000, overlap=100) to create a searchable knowledge base. Documents are embedded using nomic-embed-text via Ollama and indexed in ChromaDB. The generation phase uses llama3.2-3B (temperature=0) to execute 34 zero-shot prompts designed to elicit diverse policy scenarios. Follow-up prompts using llama3-8B generate KPIs and roadmaps. Evaluation employs the RAGAs framework with gemma2-2B and mistral-7B as judge models, assessing faithfulness, answer relevancy, and context utilization. Human expert validation serves as the gold standard, with 30 scenarios randomly selected for expert review.

## Key Results
- 88% of generated scenarios passed expert validation across 30 randomly selected responses
- Mean faithfulness scores ranged from 0.760 (human evaluation) to 0.966 (gemma2-2B automated evaluation)
- Answer relevancy exceeded 0.888 across all evaluators, with human assessment at 0.957
- Context utilization scores were above 0.713, indicating effective grounding in source documents

## Why This Works (Mechanism)
The RAG architecture grounds LLM generation in authoritative UN COP documents, reducing hallucination while maintaining creative scenario generation. By using a temperature=0 setting with llama3.2-3B, the system ensures deterministic outputs that faithfully represent the source material. The combination of automated evaluation (gemma2-2B, mistral-7B) with human expert validation creates a robust quality assurance mechanism that balances scalability with accuracy. The zero-shot prompt design leverages the LLM's inherent understanding of policy scenario generation while the RAG component ensures contextual relevance to Sub-Saharan African climate challenges.

## Foundational Learning
- **RAG (Retrieval-Augmented Generation):** Why needed: Grounds LLM outputs in authoritative sources to reduce hallucination. Quick check: Verify retrieved context chunks directly support generated claims.
- **Automated evaluation frameworks (RAGAs):** Why needed: Scales quality assessment beyond human capacity for large generation tasks. Quick check: Compare automated scores against human evaluations for correlation.
- **Vector embeddings for semantic search:** Why needed: Enables efficient retrieval of relevant policy documents from large corpora. Quick check: Test retrieval relevance with known query-document pairs.
- **Chunking strategies for document processing:** Why needed: Balances context window limitations with information granularity. Quick check: Verify chunk boundaries don't split critical policy concepts.
- **Zero-shot prompting:** Why needed: Leverages LLM capabilities without extensive fine-tuning. Quick check: Assess prompt clarity and specificity through multiple runs.
- **Spearman correlation for evaluator alignment:** Why needed: Measures rank-order agreement between different evaluation approaches. Quick check: Calculate correlation coefficients for paired evaluator scores.

## Architecture Onboarding

**Component map:** UN COP PDFs -> LangChain processing -> ChromaDB vector store <- nomic-embed-text -> RAG query -> llama3.2-3B generator -> Evaluation (RAGAs + human)

**Critical path:** Document ingestion → Vector embedding → Retrieval → Generation → Evaluation → Validation

**Design tradeoffs:** Temperature=0 ensures faithfulness but may reduce scenario diversity; larger chunk sizes improve context coherence but reduce retrieval precision; automated evaluation scales efficiently but may miss nuanced policy implications that human experts catch.

**Failure signatures:** Low faithfulness scores indicate hallucination or context misalignment; high answer relevancy with low context utilization suggests generation is plausible but not grounded; significant variance between human and model evaluations points to potential evaluator bias or prompt ambiguity.

**First 3 experiments to run:**
1. Test retrieval relevance by querying known concepts and verifying top-5 results contain appropriate context
2. Generate scenarios for a single prompt with varying top-k retrieval parameters (2, 4, 8) to optimize context selection
3. Run parallel evaluations using different judge models to establish baseline score distributions

## Open Questions the Paper Calls Out
None

## Limitations
- Retrieval parameters (top-k, similarity thresholds) are not explicitly specified, creating ambiguity in the RAG pipeline implementation
- The 30-sample validation set may not capture full variability in scenario quality given 34 unique prompts and 2,363 total generations
- Human expert validation shows high variance (faithfulness SD=0.337), suggesting potential subjectivity in assessments
- Evaluation framework relies entirely on judge models without ground truth comparison, which could mask systematic biases
- "Western skew" concern indicates base model biases may persist despite RAG grounding

## Confidence

**High confidence:** The automated evaluation framework implementation and correlation between human and model assessments are methodologically sound and reproducible.

**Medium confidence:** The 88% expert validation rate and overall metric performance (faithfulness >0.76, answer relevancy >0.88) are credible but depend on the quality and representativeness of the 30-sample validation set.

**Low confidence:** Claims about scenario diversity and practical utility for policy planning require external validation beyond the current automated and limited human assessment framework.

## Next Checks
1. **Retrieval parameter optimization:** Systematically test different top-k values (2, 4, 8, 12) and similarity thresholds to determine optimal retrieval configuration and assess impact on faithfulness scores and hallucination rates.
2. **Expanded validation corpus:** Scale human expert validation from 30 to 100+ scenarios across all 34 prompts to better characterize performance variance and identify prompt-specific failure modes.
3. **Ground truth benchmarking:** Create a small set of manually curated "gold standard" scenarios to compare against both LLM-generated scenarios and judge model evaluations, establishing absolute rather than relative performance metrics.