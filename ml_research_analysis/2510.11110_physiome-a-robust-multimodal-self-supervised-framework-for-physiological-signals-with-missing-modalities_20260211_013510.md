---
ver: rpa2
title: 'PhysioME: A Robust Multimodal Self-Supervised Framework for Physiological
  Signals with Missing Modalities'
arxiv_id: '2510.11110'
source_url: https://arxiv.org/abs/2510.11110
tags:
- modality
- missing
- decoder
- physiome
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PhysioME addresses the challenge of missing or corrupted physiological
  signal modalities in medical applications due to hardware constraints or motion
  artifacts. It introduces a multimodal self-supervised learning framework combining
  contrastive learning with masked prediction, featuring a Dual-PathNeuroNet backbone
  for temporal dynamics and a restoration decoder for reconstructing missing modality
  tokens.
---

# PhysioME: A Robust Multimodal Self-Supervised Framework for Physiological Signals with Missing Modalities

## Quick Facts
- **arXiv ID:** 2510.11110
- **Source URL:** https://arxiv.org/abs/2510.11110
- **Reference count:** 9
- **Primary result:** PhysioME achieves superior performance in sleep staging and hypotension prediction under missing modality conditions compared to existing single-model approaches.

## Executive Summary
PhysioME addresses the challenge of missing or corrupted physiological signal modalities in medical applications by introducing a multimodal self-supervised learning framework. It combines contrastive learning with masked prediction using a Dual-PathNeuroNet backbone for temporal dynamics and a restoration decoder for reconstructing missing modality tokens. Experiments on sleep stage classification (Sleep-EDFX) and hypotension prediction (VitalDB) demonstrate PhysioME's ability to maintain robust predictive accuracy and AUC metrics across various missing modality scenarios, with the smallest performance degradation relative to full-modality settings.

## Method Summary
PhysioME employs a two-stage training approach. First, unimodal DP-NeuroNet encoders are pre-trained using masked prediction and contrastive loss on individual modalities. Then, these frozen encoders (with LoRA adaptation) are integrated into the multimodal framework, which trains a multimodal encoder and restoration decoder to handle missing modalities. The restoration decoder reconstructs dropped modalities using stop-gradient operations to prevent interference with the encoder's semantic learning. The framework is evaluated through linear classification on labeled subsets using 5-fold cross-validation.

## Key Results
- PhysioME maintains high consistency and generalization across various missing modality scenarios in both sleep staging and hypotension prediction tasks.
- Compared to existing single-model approaches, PhysioME demonstrates superior performance with the smallest performance degradation relative to full-modality settings.
- The framework achieves robust predictive accuracy and AUC metrics while maintaining computational efficiency through parameter-efficient LoRA adaptation.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Semantic alignment between unimodal and fused representations enables the inference of missing modality features from observed data.
- **Mechanism:** PhysioME employs a cross-modality contrastive loss that forces the representation of a single modality to align with the global fused token, learning the statistical correlation between modalities.
- **Core assumption:** Assumes physiological modalities share underlying semantic correlations such that one modality contains recoverable information about another.
- **Evidence anchors:** [abstract] "...combining contrastive learning with masked prediction..."; [section 4, pg 4] "...encourages the model to learn the relationship between the single-modal representation... and the fused representation..."

### Mechanism 2
- **Claim:** Decoupling the missing modality reconstruction via stop-gradients forces the encoder to produce a generic latent space rather than overfitting to the decoder.
- **Mechanism:** During training, the Restoration Decoder reconstructs missing tokens using a stop-gradient operation on the input received from the Multimodal Encoder, preventing gradients from altering the encoder's weights.
- **Core assumption:** Assumes that a robust shared representation can be learned via contrastive and masked objectives before precise reconstruction is attempted.
- **Evidence anchors:** [section 3, pg 3] "...stop-gradient operation is applied... allowing the restoration decoder to be trained independently..."; [section 5, pg 6] "Disabling the gradient propagation... outperformed the alternative..."

### Mechanism 3
- **Claim:** Temporal-specific pre-training (DP-NeuroNet) preserves the sequential dynamics required for physiological signals, which generic Vision Transformers fail to capture.
- **Mechanism:** The framework uses a Dual-Path NeuroNet backbone with multi-scale 1D convolutions before attention mechanisms to explicitly model temporal continuity and local dependencies.
- **Core assumption:** Assumes that physiological signals possess local temporal structures that are destroyed or ignored by standard patch-based image transformers.
- **Evidence anchors:** [section 1, pg 1] "...vision models... struggle to capture the temporal continuity of signals."; [section 5, pg 6] "CNN-based architectures... outperformed ViT-based architectures..."

## Foundational Learning

- **Concept: Self-Supervised Learning (SSL) with Masked Autoencoders (MAE)**
  - **Why needed here:** The system must learn to reconstruct missing data. Understanding how MAE forces a model to learn latent representations by predicting hidden parts of the input is critical to grasping how PhysioME handles "dropped" modalities.
  - **Quick check question:** Can you explain why masking 80% of a signal forces the model to learn high-level semantics rather than just copying neighbors?

- **Concept: Contrastive Learning (NT-Xent Loss)**
  - **Why needed here:** The alignment between unimodal and multimodal representations relies on contrastive loss. You need to understand how maximizing similarity between positive pairs creates a structured embedding space.
  - **Quick check question:** In the context of PhysioME, what constitutes a "positive pair" in the cross-modality contrastive loss? (Answer: A single modality embedding and the fused multimodal embedding from the same sample)

- **Concept: Low-Rank Adaptation (LoRA)**
  - **Why needed here:** The modality encoders are pre-trained and then fine-tuned using LoRA. Understanding parameter-efficient fine-tuning is necessary to implement the adaptation step without destroying the pre-trained temporal features.
  - **Quick check question:** Why would freezing the main weights of the NeuroNet encoder and only tuning low-rank matrices help maintain generalization when adapting to the multimodal task?

## Architecture Onboarding

- **Component map:** Input signals -> DP-NeuroNet modality encoders (frozen + LoRA) -> Multimodal Encoder (with drop modality/random sampling) -> Restoration Decoder (stop-gradient) -> Fusion token for downstream classification

- **Critical path:** The Restoration Decoder is the critical differentiator. Trace the path where a modality is explicitly "dropped" -> passed to Multimodal Encoder -> the resulting fusion token is passed to the Restoration Decoder -> loss is computed against the original encoder output.

- **Design tradeoffs:**
  - **Stop-gradient vs. End-to-end:** The paper proves blocking gradients from the Restoration Decoder to the Encoder yields better results (ACC 78.83 vs 77.13). Do not allow the reconstruction task to "dumb down" the encoder.
  - **Mask Token vs. Restoration Token:** Simple masking is standard but underperforms. The Restoration Decoder is computationally heavier but necessary for robustness.

- **Failure signatures:**
  - **Performance collapse:** If using standard ViT backbones instead of CNN-based NeuroNet, single-modality performance crashes (ACC drops from ~76% to ~42% in ablations).
  - **Gradient interference:** If stop-gradient is removed, the encoder may overfit to the reconstruction of specific missing artifacts rather than learning generalizable features.

- **First 3 experiments:**
  1. **Backbone Validation:** Replace DP-NeuroNet with a standard ViT on a single modality to verify the "temporal continuity" hypothesis. Expect significant accuracy drops.
  2. **Gradient Flow Test:** Train the Restoration Decoder with and without the stop-gradient operation. Observe if the encoder representation collapses or becomes noisy.
  3. **Restoration Strategy:** Compare using simple "Mask Tokens" vs. the "Restoration Decoder" for missing modalities to quantify the value added by the generative decoder.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can PhysioME generalize to entirely new modality combinations or domain transfer scenarios not present during pre-training? (Basis: authors state "Generalization to entirely new combinations or domain transfer scenarios requires further investigation")
- **Open Question 2:** How does PhysioME perform when data is corrupted by noise and artifacts rather than being completely missing? (Basis: authors list need to "assess robustness not only to missing modalities but also to noise and artifacts")
- **Open Question 3:** Can the architectural complexity be reduced to improve scalability for adding new modalities? (Basis: authors note "a restoration decoder must be instantiated for each new modality to introduce scalability challenges")

## Limitations

- **Missing Modality Patterns:** Experiments focus on two specific missing modality patterns, but performance on arbitrary missing combinations in real-world settings remains unclear.
- **Data Representation Assumptions:** PhysioME assumes all modalities can be represented as token sequences, which may not hold for certain modalities like images.
- **Computational Overhead:** The dual-decoder architecture introduces additional parameters and training complexity, with efficiency trade-offs not quantified against simpler approaches.

## Confidence

- **High Confidence:** The core architectural components are well-specified and their individual contributions are validated through ablation studies.
- **Medium Confidence:** The generalization claims across diverse clinical tasks are supported by results on two datasets, but the small number of tasks limits confidence in universal applicability.
- **Low Confidence:** The framework's behavior under continuous missing patterns and its ability to handle novel modality combinations not seen during training are not addressed.

## Next Checks

1. **Arbitrary Missing Pattern Test:** Evaluate PhysioME on a broader set of missing modality combinations beyond the two patterns tested, including gradual degradation scenarios.

2. **Cross-Dataset Transfer:** Test the framework on a third dataset with different physiological signals to verify that learned modality relationships generalize beyond original training domains.

3. **Computational Complexity Analysis:** Benchmark PhysioME against simpler masking approaches on training time, inference latency, and parameter count to quantify efficiency trade-offs.