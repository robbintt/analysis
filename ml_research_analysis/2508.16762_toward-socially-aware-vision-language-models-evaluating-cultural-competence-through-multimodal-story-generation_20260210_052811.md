---
ver: rpa2
title: 'Toward Socially Aware Vision-Language Models: Evaluating Cultural Competence
  Through Multimodal Story Generation'
arxiv_id: '2508.16762'
source_url: https://arxiv.org/abs/2508.16762
tags:
- cultural
- uni00000003
- uni00000013
- uni00000048
- uni0000004f
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first comprehensive evaluation of cultural
  competence in Vision-Language Models (VLMs) through multimodal story generation.
  The authors develop a novel framework that perturbs cultural identity cues while
  maintaining constant visual inputs across five contemporary VLMs and 42 countries,
  systematically assessing how cultural identity affects model outputs.
---

# Toward Socially Aware Vision-Language Models: Evaluating Cultural Competence Through Multimodal Story Generation

## Quick Facts
- arXiv ID: 2508.16762
- Source URL: https://arxiv.org/abs/2508.16762
- Reference count: 40
- This paper presents the first comprehensive evaluation of cultural competence in Vision-Language Models (VLMs) through multimodal story generation.

## Executive Summary
This study systematically evaluates cultural competence in Vision-Language Models by generating children's stories across 42 countries using identical visual inputs but varied cultural identity cues. Through five contemporary VLMs and a comprehensive evaluation framework combining automated metrics and human judgment, the authors reveal both significant cultural adaptation capabilities and critical limitations. The research establishes that while VLMs can produce culturally-specific vocabulary and maintain cultural distinctions in embedding space, automated metrics often contradict human assessments due to architectural bias, and models frequently prioritize cultural safety over authentic representation.

## Method Summary
The study created 1,470 unique prompts combining 35 story concepts with 42 countries, using the template "Write a children's story about [concept] for a/an [nationality] kid in English based on the image." Images were scraped from Google Images and manually filtered for cultural relevance, quality, and minimal text. Five VLMs (Gemma3 4B/12B, Qwen 2.5 VL 7B, InternVL3 8B, SmolVLM2 2.2B) generated 5 story samples per prompt at temperatures 0.3 and 0.7, yielding 73,500 total stories. Evaluation combined automated metrics (lexical diversity via Word Edit Ratio and TF-IDF, cross-modal similarity via CLIP-Score and Recall@K, cultural alignment via Hofstede's Cultural Dimensions and World Values Survey correlations) with human evaluation on 10 dimensions for 250 sampled stories.

## Key Results
- VLMs show systematic cultural adaptation with F-statistics ranging from 1540 to 8707 (p < 1e-48) demonstrating measurable lexical variation across cultural contexts
- Cross-modal evaluation reveals within-nationality recall (28.7%) far exceeds cross-nationality recall (0.2%), indicating culturally distinct outputs are separable in embedding space
- Human evaluation reveals architectural bias: SmolVLM2 scores well automatically but poorly with humans (4.05/10 vs 6.81/10 for Gemma3 12B), contradicting automated metrics
- Models exhibit inverse cultural alignment, producing more similar stories for culturally distant countries in some architectures

## Why This Works (Mechanism)

### Mechanism 1: Cultural Perturbation Sensitivity
- **Claim:** VLMs systematically adapt vocabulary in response to explicit nationality cues, producing measurably different outputs for different cultures even with identical visual inputs.
- **Mechanism:** The prompt explicitly injects nationality ("Write a children's story about [concept] for a/an [nationality] kid"), triggering culture-associated token distributions in the language model head. The visual encoder processes the same image, but the text decoder conditions on the cultural cue, shifting the vocabulary selection toward culturally-associated names (Priya/Arjun for India), familial terms (dadi/amma, lola/lolo), and geographic markers (pyramids/Nile for Egypt).
- **Core assumption:** Training data contains sufficient culture-specific associations that nationality tokens prime correlated vocabulary.
- **Evidence anchors:**
  - [abstract] "significant cultural adaptation capabilities, with rich culturally-specific vocabulary spanning names, familial terms, and geographic markers"
  - [section 4.1] ANOVA F-values ranging from 1540.19 to 8707.02 (all p < 1e-48) between within-nationality and across-nationality lexical variance
  - [section 4.2] Table 1 showing culture-specific TF-IDF correlated words across models
  - [corpus] CultureVLM paper confirms VLMs "struggle with cultural understanding... due to biases in predominantly Western-centric training data"

### Mechanism 2: Inverse Cultural Alignment Pattern
- **Claim:** Some VLM architectures produce *more* similar stories for culturally distant countries, contradicting expected cultural proximity behavior.
- **Mechanism:** When cultural alignment is measured via Kendall's τ correlation between story similarity (BLEU) and cultural distance (HCD/WVS), negative correlations indicate inverse alignment. Gemma models show medians of -0.08 to -0.10 on HCD, meaning culturally distant countries receive more similar stories than culturally proximate ones. This may reflect training data where minority/underrepresented cultures are mapped to a generic "non-Western" cluster, producing stereotyped outputs that erase cultural distinctions.
- **Core assumption:** Cultural distance metrics (HCD, WVS) validly capture cultural relationships that should manifest in narrative outputs.
- **Evidence anchors:**
  - [abstract] "some models exhibit inverse cultural alignment"
  - [section 4.3] "Gemma models show consistently negative medians (~-0.08 to -0.10), suggesting inverse relationships where culturally distant countries receive more similar stories"
  - [section 5.1] "clustering in Sub-Saharan Africa (mean τ = -0.127) and Southeast Asia (mean τ = -0.134)"
  - [corpus] "Vision Language Models are Confused Tourists" finds VLMs struggle with stability across diverse cultural inputs

### Mechanism 3: Cross-Modal Metric-Human Divergence
- **Claim:** CLIP-Score-based automated metrics show architectural bias that contradicts human cultural authenticity assessments, making them unreliable for cultural competence evaluation.
- **Mechanism:** SmolVLM2 2.2B achieves high CLIP-based cross-modal correlations (67% positive across countries) but receives the lowest human authenticity scores (4.05/10), while Gemma3 12B shows lower CLIP correlations but highest human ratings (6.81/10). The hypothesis is that SmolVLM2 uses SigLIP for visual encoding, which shares embedding space characteristics with CLIP-Score's evaluator, creating measurement bias rather than superior cultural competence. Smaller models also produce shorter stories, which CLIP-Score may prefer.
- **Core assumption:** Human evaluators can reliably assess cultural authenticity; CLIP-Score correlates with semantic coherence but not cultural appropriateness.
- **Evidence anchors:**
  - [abstract] "automated metrics show architectural bias contradicting human assessments"
  - [section 4.4] "SmolVLM2 2.2B... exhibits strong positive correlations (green) across North America, Europe, and Asia for both HCD and WVS measures"
  - [section 4.5] "SmolVLM2 2.2B receives the lowest ratings across all dimensions (4.05/10 average)... This contradicts its apparent strong performance in CLIP-based cross-modal evaluation"

## Foundational Learning

- **Concept: Hofstede's Cultural Dimensions (HCD)**
  - **Why needed here:** Provides quantitative framework (6 dimensions: power distance, individualism, masculinity, uncertainty avoidance, long-term orientation, indulgence) for 111 countries to measure whether VLM outputs reflect authentic cultural distances.
  - **Quick check question:** If Model A produces stories where Australia-New Zealand (culturally proximate) have higher BLEU similarity than Australia-Egypt (culturally distant), what sign would you expect for HCD correlation?

- **Concept: Cross-Modal Retrieval (Recall@K)**
  - **Why needed here:** Used to detect whether culturally distinct outputs are separable in embedding space—within-nationality recall should exceed cross-nationality recall if the model produces culturally specific content.
  - **Quick check question:** If within-nationality Recall@1 is 28.7% and cross-nationality Recall@1 is 0.2%, what does the 140× gap indicate about the model's cultural specificity?

- **Concept: TF-IDF for Cultural Vocabulary Analysis**
  - **Why needed here:** Identifies which words are most distinctive to each country's generated stories, enabling systematic analysis of cultural vocabulary adaptation (names, foods, familial terms).
  - **Quick check question:** If "dadi," "amma," and "ladoos" appear with high TF-IDF scores for India but not for Nigeria, what does this suggest about the model's cultural adaptation?

## Architecture Onboarding

- **Component map:** Web scraper (Beautiful Soup + Google Images) -> Human filtering (3 criteria: cultural relevance, quality, minimal text) -> 1,470 prompts across 42 countries × 35 concepts -> VLM inference (5 models, 5 samples each, temperatures 0.3/0.7) -> 73,500 stories -> Multi-metric evaluation (automated + human)

- **Critical path:** Prompt construction with nationality perturbation -> Image retrieval for culture-concept pairs -> VLM story generation -> Multi-metric evaluation -> Human validation of automated metrics

- **Design tradeoffs:**
  - **Explicit vs. implicit cultural cues:** Paper uses explicit nationality ("for a German kid"); implicit cues (dialect, visual elements) not tested—limits generalizability
  - **English-only outputs:** All stories in English; cannot evaluate multilingual cultural competence
  - **Automated vs. human evaluation:** Automated metrics faster but show architectural bias; human evaluation reliable but expensive (250 stories evaluated)

- **Failure signatures:**
  - **Cultural hallucination:** SmolVLM2 incorrectly identifies Berlin Cathedral as "Fernsehturm" (TV Tower)—factual errors about landmarks
  - **Ethnic majority stereotyping:** 89% of Indian names from Hindi/Sanskrit origins despite 700+ linguistic communities; 92% of Nigerian names from Yoruba/Igbo despite 250+ ethnic groups
  - **Metric-human inversion:** If CLIP-Score high but human authenticity low, suspect architectural bias (especially with smaller models using similar visual encoders to CLIP)

- **First 3 experiments:**
  1. **Reproduce the ANOVA variance analysis:** Run within-nationality vs. across-nationality lexical variance test on your VLM with 5+ countries, verify F > 1000 with p < 1e-48
  2. **Calibrate CLIP-Score against human judgment:** Sample 50 stories per model, correlate CLIP-Score with human authenticity ratings—if negative correlation, confirm architectural bias
  3. **Test implicit cultural cues:** Replace explicit nationality ("for a German kid") with implicit markers (German names, settings) and measure whether cultural adaptation persists

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How do users perceive the authenticity of VLM outputs when textual cultural adaptation conflicts with the cultural elements present in the input image?
- **Basis in paper:** [explicit] The authors explicitly ask, "When cultural adaptation in generated text doesn't align with cultural elements present in input images, how do users perceive authenticity?"
- **Why unresolved:** The current study perturbed identity cues but did not systematically evaluate user reactions to "forced" or misaligned image-text pairings.
- **What evidence would resolve it:** User studies measuring perceived authenticity scores for stories where textual cues (e.g., nationality in prompt) deliberately contradict visual cultural markers.

### Open Question 2
- **Question:** Do established cultural psychology frameworks (e.g., Hofstede's Dimensions) adequately capture narrative-based cultural competence in generative models?
- **Basis in paper:** [explicit] The limitations section states, "The fundamental question remains whether established cultural psychology frameworks adequately capture narrative-based cultural competence."
- **Why unresolved:** The study found weak or inverse correlations between model outputs and frameworks like HCD/WVS, suggesting these frameworks may miss nuances in generative storytelling.
- **What evidence would resolve it:** Development of a narrative-specific cultural evaluation framework that correlates more strongly with human judgment than existing survey-based dimensions.

### Open Question 3
- **Question:** Can automated metrics be developed that evaluate cross-modal cultural competence without exhibiting architectural bias?
- **Basis in paper:** [inferred] The authors find CLIPScore exhibits architectural bias (favoring smaller models like SmolVLM2) and explicitly hope "future work will introduce better alternatives."
- **Why unresolved:** Current automated metrics showed an inverse relationship with human cultural ratings, where models with strong metric scores received poor human evaluations.
- **What evidence would resolve it:** A new automated metric that demonstrates consistent positive correlation with human evaluation scores across diverse VLM architectures.

## Limitations

- The study uses explicit nationality cues rather than implicit cultural markers, limiting generalizability to real-world scenarios where cultural context is conveyed through names, settings, and visual elements rather than explicit prompts
- English-only outputs prevent evaluation of multilingual cultural competence, which is critical for global deployment of VLMs
- The inverse cultural alignment phenomenon lacks a complete mechanistic explanation, with the hypothesis of "generic non-Western cluster" training data not directly verified through training corpus analysis

## Confidence

**Confidence: Medium** - The inverse cultural alignment phenomenon remains incompletely explained, with the hypothesis of training data biases not fully validated through corpus analysis.

**Confidence: Low** - Human evaluation methodology has implementation ambiguities including unclear inter-annotator agreement standards and qualification criteria for raters beyond "knowledge of multiple global cultures."

**Confidence: High** - The automated metric-human divergence finding is well-supported by data showing consistent patterns of architectural bias, though alternative explanations like length bias were not fully ruled out.

## Next Checks

1. **Cross-linguistic validation**: Repeat the study using multilingual prompts (e.g., "Write a story for a German child" in German, French, Japanese) to determine whether cultural competence is language-dependent or truly multimodal.

2. **Implicit cultural cue testing**: Design experiments where nationality is indicated through cultural markers (names, landmarks, traditional clothing) rather than explicit nationality words to test whether VLMs can infer cultural context from multimodal cues.

3. **Training data analysis**: Conduct systematic examination of the training corpora of evaluated VLMs to quantify cultural representation and directly test the hypothesis that inverse alignment correlates with underrepresented cultures being clustered into generic "non-Western" categories.