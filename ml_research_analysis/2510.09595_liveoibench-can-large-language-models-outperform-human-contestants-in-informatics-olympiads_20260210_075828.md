---
ver: rpa2
title: 'LiveOIBench: Can Large Language Models Outperform Human Contestants in Informatics
  Olympiads?'
arxiv_id: '2510.09595'
source_url: https://arxiv.org/abs/2510.09595
tags:
- informatics
- olympiad
- reasoning
- performance
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LiveOIBench, a new benchmark for evaluating
  large language models (LLMs) on competitive programming problems. The benchmark
  addresses limitations in existing coding benchmarks by curating 403 expert-designed
  Olympiad-level problems with an average of 60 private test cases each, sourced directly
  from official Informatics Olympiads.
---

# LiveOIBench: Can Large Language Models Outperform Human Contestants in Informatics Olympiads?

## Quick Facts
- arXiv ID: 2510.09595
- Source URL: https://arxiv.org/abs/2510.09595
- Authors: Kaijian Zou; Aaron Xiong; Yunxiang Zhang; Frederick Zhang; Yueqi Ren; Jirong Yang; Ayoung Lee; Shitanshu Bhushan; Lu Wang
- Reference count: 40
- Primary result: GPT-5 achieves 81.76th percentile against humans on 403 expert-curated Olympiad problems, still below top human performers

## Executive Summary
This paper introduces LiveOIBench, a new benchmark for evaluating large language models on competitive programming problems from official Informatics Olympiads. The benchmark addresses key limitations in existing coding benchmarks by using expert-designed problems with extensive private test cases (avg. 60 per task) and direct comparison against human contestant performance through official ranking data. The authors benchmark 34 models and find that while GPT-5 reaches the 81.76th percentile against humans, current models still fall short of top human competitors, particularly struggling with dynamic programming and algorithms requiring creative reasoning.

## Method Summary
LiveOIBench evaluates LLMs on 403 expert-curated Olympiad-level competitive programming problems with extensive private test cases and subtask rubrics. The evaluation pipeline compiles generated C++ code and runs it against private test cases using a local judge, then maps scores to human percentiles using official contestant rankings. Each model generates 8 candidate solutions per problem, with the highest-scoring solution selected. The benchmark includes detailed reasoning trace analysis to understand how models allocate inference tokens across different problem-solving behaviors.

## Key Results
- GPT-5 achieves 81.76th percentile against humans but falls short of top human competitors
- Among open-weight models, GPT-OSS-120B reaches 60th percentile
- Current models struggle particularly with dynamic programming (46.88% pass rate for GPT-5) and other algorithms requiring creative reasoning
- Successful models allocate more tokens to structured analysis and planning while minimizing unnecessary exploration

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** High-performing models solve Olympiad problems by prioritizing structured planning and analysis over exploratory backtracking.
- **Mechanism:** Reasoning models allocate inference tokens to deep problem structuring ("Analysis" and "Planning" behaviors) rather than cycling through alternative solutions ("Exploration"). This reduces "underthinking" and prevents excessive pivoting on difficult constraints.
- **Core assumption:** Token allocation correlates with cognitive effort; specifically, that "planning" tokens yield better algorithmic invariant discovery than "exploration" tokens.
- **Evidence anchors:** [abstract] "robust reasoning models prioritize precise problem analysis over excessive exploration." [section 5.2] "Stronger reasoning models exhibit reduced exploration, allocating more resources toward solution development and analysis."

### Mechanism 2
- **Claim:** Reliable benchmarking requires private test cases and subtask rubrics to prevent false positives inherent in public test suites.
- **Mechanism:** Expert-designed test cases (avg. 60 per task) expose "false-positive" solutions that pass public tests but fail on edge cases or efficiency constraints. Subtask rubrics allow partial credit, revealing capability gradients (e.g., an O(N^2) solution passing subtask 1 but failing the full O(N log N) requirement).
- **Core assumption:** Aggregated human contestant rankings provide a stable, ground-truth distribution of programming skill against which models can be percentile-ranked.
- **Evidence anchors:** [abstract] "meticulously curated high-quality tasks with detailed subtask rubrics and extensive private test cases." [section 1] "This comprehensive test suite eliminates high false-positive rates common in previous benchmarks."

### Mechanism 3
- **Claim:** Allocating more reasoning tokens (inference-time compute) yields greater performance gains on complex algorithmic tasks than parameter scaling alone.
- **Mechanism:** Complex tasks like Dynamic Programming (DP) and Graph Theory require hierarchical reasoning. Increasing the "reasoning budget" allows smaller models to simulate the depth of larger models, closing the performance gap more efficiently than increasing parameter count.
- **Core assumption:** The relationship between reasoning budget and accuracy is monotonic (at least up to ~20k-100k tokens) for these problem classes.
- **Evidence anchors:** [section 4] "sequential scaling... allows smaller models to approach larger-model performance." [table a12] "Reasoning Tokens... produces a substantially larger average improvement of +113% [vs parameter scaling]."

## Foundational Learning

- **Concept:** Olympiad Subtask Structure
  - **Why needed here:** Unlike LeetCode "pass/fail", Olympiad problems give partial points for suboptimal solutions (e.g., O(N^2) vs O(N)).
  - **Quick check question:** Can you explain why a model might score 40/100 points on a problem it "failed" to solve completely?

- **Concept:** Reasoning vs. Exploration in Traces
  - **Why needed here:** To interpret why a model generated 50k tokens but still failed (likely excessive "backtracking" vs. insufficient "planning").
  - **Quick check question:** If a trace shows 40% "Exploration" behavior, would you expect it to be more or less accurate than a trace with 50% "Planning"?

- **Concept:** False Positive Rate in Benchmarking
  - **Why needed here:** To understand why existing benchmarks overestimate LLMs (solutions pass visible tests but fail hidden constraints).
  - **Quick check question:** Why is passing 10/10 sample inputs insufficient for verifying an Olympiad solution?

## Architecture Onboarding

- **Component map:** Dataset (403 problems, metadata, human rankings) -> Evaluation System (local judge, grader scripts) -> Metrics Calculator (Percentiles, ELO/Medals)
- **Critical path:**
  1. Data Ingest: Crawl official Olympiad sites (2023-2025) and secondary mirrors (CSES/LibreOJ)
  2. Verification: Run official solutions against the local judge to ensure test case integrity
  3. Inference: Sample k=8 solutions (C++) per model per problem
  4. Scoring: Select highest-scoring sample, map to human distribution
- **Design tradeoffs:** Language Choice: C++ chosen over Python for execution efficiency (avoids TLE/MLE on tight constraints). Metric Selection: Human Percentile preferred over Pass Rate for robust skill assessment.
- **Failure signatures:** Runtime Errors (RE): High prevalence in top models due to aggressive optimization attempts. DP Failure: Sharp performance drop on "Dynamic Programming" tags (46.88% pass rate for GPT-5) compared to "Sorting" (75.56%).
- **First 3 experiments:**
  1. Baseline Run: Evaluate GPT-4.1 vs GPT-5 on "Implementation" vs "DP" tags to isolate the "creative reasoning" gap
  2. Budget Scaling: Plot pass rate vs token budget (4k, 16k, 64k) for a 32B model to validate "sequential scaling" efficiency frontier
  3. Contamination Check: Correlate model performance with problem release date relative to model's training cutoff

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can curriculum-driven fine-tuning on synthetic datasets improve model performance on tasks requiring hierarchical invariants, such as dynamic programming and segment trees?
- Basis in paper: [explicit] Section 5.1 states, "To address these weaknesses [in DP and trees], future work could explore curriculum-driven fine-tuning... encouraging models to internalize the hierarchical invariants."
- Why unresolved: Current models rely heavily on procedural knowledge and struggle with the creative observations and compositional reasoning required by complex algorithmic tasks.
- What evidence would resolve it: Demonstrated improvements in pass rates for dynamic programming and segment tree tags after training on carefully designed synthetic datasets.

### Open Question 2
- Question: Can guiding models to focus on structured planning and analysis, while limiting unnecessary exploration, improve reasoning efficiency and accuracy?
- Basis in paper: [explicit] Section 5.2 notes, "an important direction for future research is to optimize how models allocate reasoning effort... guiding models to focus on structured planning... can greatly improve solution accuracy."
- Why unresolved: While stronger models naturally allocate tokens better, it is unclear how to explicitly enforce this behavior in weaker models to prevent "underthinking" or excessive pivoting.
- What evidence would resolve it: A training regime or prompting strategy that dynamically adjusts reasoning behaviors, resulting in higher accuracy without increased token usage.

### Open Question 3
- Question: Does incorporating fine-grained reward signals for execution efficiency and memory management reduce runtime errors in competitive programming tasks?
- Basis in paper: [explicit] Section 5.3 suggests, "Future training techniques could incorporate fine-grained reward signals targeting these attributes, enabling models to optimize not only for correctness but also for reliable and efficient code execution."
- Why unresolved: Current reinforcement learning approaches predominantly reward solution correctness, neglecting efficiency and memory, which leads to persistent runtime and time-limit errors even in top models.
- What evidence would resolve it: A decrease in runtime and time-limit exceeded error rates for models trained with rewards weighted by execution time and memory usage.

## Limitations
- Evaluation fairness concerns for open-weight models that may not receive equivalent reasoning budgets compared to proprietary models
- Limited generalizability beyond Olympiad problems to broader software engineering tasks
- Reasoning trace analysis lacks rigorous quantitative validation for causal claims about token allocation

## Confidence
- Evaluation Fairness: Medium - potential unfair comparison between proprietary and open-weight models
- Generalizability: Low - results may not extend to general programming ability beyond Olympiad problems
- Reasoning Trace Attribution: Medium - compelling qualitative insights but lacks causal evidence

## Next Checks
1. Replicate budget scaling experiments to validate whether sequential scaling outperforms parameter scaling across multiple problem categories
2. Conduct systematic contamination analysis correlating model performance with problem release dates relative to training cutoffs
3. Extend evaluation beyond Olympiad problems to include LeetCode, Codeforces, and real-world coding scenarios