---
ver: rpa2
title: Do Syntactic Categories Help in Developmentally Motivated Curriculum Learning
  for Language Models?
arxiv_id: '2511.08199'
source_url: https://arxiv.org/abs/2511.08199
tags:
- language
- syntactic
- data
- childes
- categories
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study analyzes syntactic properties of BabyLM corpus and age-ordered
  CHILDES to investigate developmentally motivated curriculum learning. We introduce
  a toolkit for syntactic categorization of training data and conduct experiments
  comparing developmental and cognitively inspired curricula.
---

# Do Syntactic Categories Help in Developmentally Motivated Curriculum Learning for Language Models?

## Quick Facts
- arXiv ID: 2511.08199
- Source URL: https://arxiv.org/abs/2511.08199
- Authors: Arzu Burcu Güven; Anna Rogers; Rob van der Goot
- Reference count: 19
- Primary result: Syntactic filtering improves efficiency over noisy full corpora, while developmental curriculum ordering provides modest, task-specific benefits

## Executive Summary
This study investigates whether developmentally motivated curriculum learning and syntactic categorization can improve language model training using the BabyLM dataset. The authors introduce a toolkit for syntactic categorization of training data using constituency parsing and Tregex patterns, then conduct experiments comparing various developmental curricula against random ordering. Results show that syntactically filtered data performs comparably to full corpus training with 40% fewer training steps, suggesting noise reduction is the primary driver of performance gains. Cross-construction generalization experiments reveal limited transfer from simpler to more complex constructions, with complex categories generalizing better than simpler ones. Analysis of CHILDES data shows no clear syntactic differentiation by age groups, challenging assumptions about developmental progression in child-directed speech.

## Method Summary
The study uses the BabyLM strict dataset (6 corpora: CHILDES, BNC Spoken, OpenSubtitles, Switchboard, Simple Wikipedia, Gutenberg) with preprocessing that removes speaker labels, annotations, normalizes nonstandard expressions, applies sentence segmentation, and filters utterances under 2 tokens. Sentences are parsed using the Kitaev & Klein (2018) constituency parser and categorized using ~300 Tregex patterns into 13 syntactic categories mapped to 3 macro-categories (Simple, Interrogatives, Complex). Seven training conditions are evaluated: B1 (full random), B2 (categorized random), and five curriculum orders (C1-C5) ranging from gradual to sandwich patterns. GPT-2 small (124M parameters) is trained for one epoch with AdamW optimizer, learning rate 1.88×10⁻⁴, batch size 8, and evaluated using the BabyLM test suite plus cross-construction generalization perplexity measurements.

## Key Results
- Syntactically filtered data (B2) achieves performance comparable to full corpus training (B1) while requiring 40% fewer training steps
- Curriculum models show modest benefits for reading tasks but underperform on specific linguistic phenomena like WUG_PAST
- Complex syntactic constructions generalize better to other complex categories than simple constructions do to complex ones
- CHILDES corpus shows no clear syntactic differentiation by age groups, challenging developmental curriculum assumptions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Training on syntactically filtered data improves efficiency relative to noisy full corpora.
- Mechanism: Approximately 300 expert-designed Tregex patterns extract sentences matching 13 syntactic categories, excluding noisy utterances with disfluencies, stutters, and fragmentary speech; the model trains on the cleaner 71% subset.
- Core assumption: Noise in uncategorized corpora degrades learning more than the reduced data quantity harms it.
- Evidence anchors:
  - [abstract] "the main performance improvement come from using the subset of syntactically categorizable data, rather than the full noisy corpus"
  - [section 3.1.2] "models trained on parsed and categorized data perform on par with the B1 baseline despite requiring 40% fewer training steps"
- Break condition: If parsing errors systematically exclude valid constructions, filtered training may underrepresent rare but informative patterns.

### Mechanism 2
- Claim: Developmentally motivated curriculum ordering provides modest, task-specific benefits for psycholinguistic alignment.
- Mechanism: Training data ordered as Simple → Interrogatives → Complex macro-categories (based on Friedmann & Reznick 2021 developmental stages), then concatenated; models evaluated on reading-time alignment tasks.
- Core assumption: The ordering of exposure matters for learning human-like processing preferences, even if downstream task performance is unchanged.
- Evidence anchors:
  - [section 3.1.2] "both self-paced reading and eye-tracking, curriculum models C1 and C2 show the highest performance"
  - [section 2.3] "we adopted the developmental stages proposed by Friedmann and Reznick (2021)"
- Break condition: If reading-task gains derive from data distribution shifts rather than ordering, shuffling within the filtered subset should yield similar results.

### Mechanism 3
- Claim: Syntactic generalization from simpler to more complex constructions is limited in naturalistic data.
- Mechanism: Models trained on single syntactic categories (e.g., Subject-Verb, Coordination, Questions) and evaluated on held-out categories; perplexity used as generalization proxy.
- Core assumption: Lower perplexity on unseen syntactic categories indicates transfer of learned syntactic knowledge.
- Evidence anchors:
  - [section 3.2.2] "Subject–Verb group shows the largest drop in both in-category and cross-category performance"
  - [section 3.2.2] "models trained on complex constructions tend to generalize better to other complex categories"
- Break condition: If perplexity differences reflect lexical rather than syntactic overlap, controlling vocabulary across subsets should attenuate the pattern.

## Foundational Learning

- **Concept: Constituency parsing and Tregex pattern matching**
  - Why needed here: The method requires parsing sentences into constituency trees and querying them with ~300 Tregex patterns to assign syntactic categories.
  - Quick check question: Can you write a Tregex pattern matching sentences with a subject NP followed by a VP headed by a verb?

- **Concept: Curriculum Learning (CL)**
  - Why needed here: The core hypothesis is that ordering training data by syntactic complexity affects learning dynamics.
  - Quick check question: What is the difference between static curriculum (pre-defined order) and dynamic curriculum (sampling-based)?

- **Concept: Cross-construction generalization**
  - Why needed here: Experiment 2 tests whether training on one syntactic category transfers to others.
  - Quick check question: Why is perplexity a reasonable (or unreasonable) proxy for syntactic generalization?

## Architecture Onboarding

- **Component map:**
  - Parser: Kitaev & Klein (2018) constituency parser → Constituency trees
  - Categorizer: ~300 Tregex patterns → 13 syntactic categories → 3 macro-categories
  - Trainer: GPT-2 small (124M parameters, vocab 50,257, context 1024) with AdamW, linear scheduler
  - Evaluator: BabyLM test suite (BLiMP, GLUE, reading tasks, etc.)

- **Critical path:**
  1. Preprocess: Remove speaker labels, segment sentences, filter utterances <2 tokens
  2. Parse: Run constituency parser on all corpora
  3. Categorize: Apply Tregex patterns to assign syntactic labels (71% coverage)
  4. Curriculum: Order categorized data by specified curriculum (C1–C5) or random (baselines)
  5. Train: One epoch, batch size 8, learning rate 1.88×10⁻⁴

- **Design tradeoffs:**
  - Coverage vs. precision: 71% coverage leaves 29% uncategorized (noise + long-tail constructions)
  - Macro-category granularity: 3 macro-categories vs. 13 fine-grained categories—coarser bins may obscure developmental nuance
  - Assumption: Removing speaker labels improved parser accuracy but reduced performance on tasks containing speaker markers

- **Failure signatures:**
  - High off-diagonal perplexity in cross-category experiments indicates poor syntactic transfer
  - CHILDES age-group analysis shows no clear syntactic differentiation, suggesting curriculum assumptions may not hold
  - WUG_PAST performance degradation suggests cleaner data may reduce exposure to morphological variation

- **First 3 experiments:**
  1. Replicate B1 vs. B2 comparison on a held-out corpus to test whether filtering effects generalize beyond BabyLM
  2. Ablate Tregex pattern sets (e.g., remove fragments, remove relative clauses) to identify which exclusions drive gains
  3. Shuffle within curriculum stages to isolate ordering effects from subset composition effects

## Open Questions the Paper Calls Out

- **Open Question 1**
  - Question: Does longitudinal tracking of individual children in CHILDES reveal the syntactic developmental stages that are obscured in aggregated age-group analysis?
  - Basis in paper: [explicit] The authors state that because children reach milestones at different rates, "it may be more informative to track syntactic development longitudinally for each child" rather than relying on aggregated age groups.
  - Why unresolved: The current analysis aggregated data from 58 subcorpora, mixing various socioeconomic statuses and session types, which may have hidden individual developmental trajectories.
  - What evidence would resolve it: A fine-grained analysis of CHILDES that controls for confounding factors (SES, session type) by tracking syntactic complexity over time for specific individual children.

- **Open Question 2**
  - Question: To what extent does data quality (noise reduction) versus data ordering (curriculum) drive performance improvements in language modeling?
  - Basis in paper: [explicit] The authors conclude that "continued focus solely on syntax may be counter-productive" and suggest that "the noise in popular resources such as CHILDES may by itself have an outsized effect."
  - Why unresolved: The primary performance gain came from using the syntactically categorized subset (cleaner data) rather than the developmental ordering, making it unclear if the curriculum itself adds value over simple noise filtering.
  - What evidence would resolve it: Ablation studies comparing models trained on raw noisy data, filtered clean data (no order), and curricular ordered data to isolate the impact of noise vs. ordering.

- **Open Question 3**
  - Question: Can stricter control of lexical and structural properties in naturalistic datasets enable robust syntactic generalization in Transformers?
  - Basis in paper: [inferred] The authors note that prior work showing successful transfer used synthetic datasets, whereas their naturalistic subsets retained high variance, suggesting "stricter control of lexical and structural properties may be necessary."
  - Why unresolved: The study found limited evidence of simple-to-complex generalization, but it remains unclear if this is a failure of the architecture or a result of the "naturalistic variation" in the data.
  - What evidence would resolve it: Generalization experiments using naturalistic data constrained to minimize lexical and structural overlap between training and evaluation sets.

## Limitations

- CHILDES corpus shows no clear syntactic differentiation by age groups, challenging the developmental assumptions underlying curriculum designs
- 71% categorization coverage means 29% of data is excluded, and it's unclear whether performance gains stem from cleaner data or reduced training set size
- Cross-construction generalization results may conflate lexical and syntactic factors due to vocabulary differences across syntactic categories

## Confidence

- **High confidence:** The finding that syntactically filtered data (B2) performs comparably to full corpus training (B1) despite requiring fewer training steps is well-supported by the experimental results
- **Medium confidence:** The claim that curriculum ordering provides modest task-specific benefits for psycholinguistic alignment is supported but requires qualification - benefits are limited to specific reading tasks
- **Low confidence:** The assertion that syntactic generalization from simpler to more complex constructions is limited is based on perplexity measurements that may not cleanly isolate syntactic factors

## Next Checks

1. **Ablation study on Tregex patterns:** Systematically remove different subsets of the ~300 patterns (e.g., fragments, relative clauses, questions) to identify which syntactic exclusions drive the performance gains, separating the effects of noise removal from syntactic filtering.

2. **Within-stage shuffling experiment:** For curriculum conditions C1-C5, keep the macro-category subsets intact but randomize the order within each stage to isolate whether ordering effects are due to sequence versus the composition of filtered subsets.

3. **Cross-linguistic replication:** Apply the same methodology to German BabyLM data (following arXiv:2503.11593) to test whether syntactic categorization benefits generalize across languages and whether CHILDES-specific characteristics explain the limited developmental differentiation observed.