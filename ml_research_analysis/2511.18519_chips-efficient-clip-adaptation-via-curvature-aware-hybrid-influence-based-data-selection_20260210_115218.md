---
ver: rpa2
title: 'CHIPS: Efficient CLIP Adaptation via Curvature-aware Hybrid Influence-based
  Data Selection'
arxiv_id: '2511.18519'
source_url: https://arxiv.org/abs/2511.18519
tags:
- chips
- random
- tracin
- ours
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CHIPS improves CLIP domain adaptation by integrating curvature-aware
  Newton alignment in the end-point subspace, InfoNCE-aware curvature estimation via
  Johnson-Lindenstrauss sketching, and selection-aware weighting for learnability
  and retention. Evaluated on 17 medical and 31 general-domain benchmarks, CHIPS matches
  full-dataset continual pre-training with only 30% of the data and outperforms half-dataset
  training using just 10%.
---

# CHIPS: Efficient CLIP Adaptation via Curvature-aware Hybrid Influence-based Data Selection

## Quick Facts
- arXiv ID: 2511.18519
- Source URL: https://arxiv.org/abs/2511.18519
- Reference count: 40
- Primary result: Matches full-dataset continual pre-training with only 30% of data on medical benchmarks

## Executive Summary
CHIPS introduces a novel data selection method for CLIP continual pre-training that significantly improves efficiency while maintaining performance. The method combines curvature-aware alignment in CLIP's end-point subspace, InfoNCE-aware curvature estimation, and selection-aware weighting for learnability and retention. Evaluated across 17 medical and 31 general-domain benchmarks, CHIPS achieves state-of-the-art performance among selection baselines while minimizing general-domain performance drop.

## Method Summary
CHIPS computes a utility score for each training sample by combining three factors: (1) curvature-aware alignment via Newton-style direction in end-point subspace using Johnson-Lindenstrauss sketching, (2) learnability weight based on prediction correctness and margin, and (3) domain relevance via cosine similarity to evaluation set prototypes. The method selects top-scoring samples for continual pre-training, achieving comparable performance to full-dataset training with only 30% of the data and outperforming half-dataset training using just 10%.

## Key Results
- Matches full-dataset continual pre-training performance using only 30% of training data
- Outperforms half-dataset training using just 10% of data on medical benchmarks
- Consistently delivers state-of-the-art performance among selection baselines while minimizing general-domain performance drop

## Why This Works (Mechanism)

### Mechanism 1
Computing influence in CLIP's end-point subspace (projection heads and temperature) serves as a faithful proxy for full-parameter alignment. CHIPS restricts influence calculation to projection layers and temperature, proving under specific correlation assumptions that proxy alignment lower-bounds full alignment. This allows efficient estimation without full Hessian computation. The mismatch term between proxy and full gradients is assumed uncorrelated with end-point gradients.

### Mechanism 2
Mixing positive and negative pair moments restores cross-example curvature ignored by standard influence methods. Standard influence functions assume additive losses, but InfoNCE creates non-local gradients via softmax normalization. CHIPS estimates curvature by mixing self-moments and negative-pair cross-moments with parameter α, capturing coupling induced by the loss function. The generalized Gauss-Newton curvature is approximated by linear mixing of positive and negative moments.

### Mechanism 3
Multiplying alignment scores by learnability and domain relevance weights balances adaptation speed with general knowledge retention. A learnability weight down-weights easy samples and up-weights boundary samples, while a relevance weight softly filters for target domain similarity while bounding selection distribution drift, mitigating catastrophic forgetting. The target evaluation set is assumed representative of the domain distribution for adaptation.

## Foundational Learning

### Influence Functions & Newton Alignment
- Why needed: Understanding how $H^{-1} \nabla L$ quantifies a single data point's effect on model loss is critical to grasping why CHIPS selects specific samples.
- Quick check: Can you explain why computing the inverse Hessian-vector product is typically expensive, necessitating the JL sketch?

### Symmetric InfoNCE Loss
- Why needed: You must understand that the loss for a positive pair depends on the entire batch of negatives to appreciate why standard diagonal curvature proxies fail.
- Quick check: Why does the softmax normalization in InfoNCE make gradients "non-local" or batch-dependent?

### Johnson-Lindenstrauss (JL) Lemma
- Why needed: This explains how CHIPS reduces dimensionality of gradients while preserving inner products required for alignment scoring.
- Quick check: If you project 1000-dimensional gradients to 64 dimensions, does JL guarantee that the dot product between two vectors remains approximately the same?

## Architecture Onboarding

**Component map:**
- Input: Training Pool ($D_{pool}$), Evaluation Set ($D_{eval}$)
- Encoders: Frozen Backbone + Trainable Projection Heads ($\vartheta$)
- Scoring Module: Gradient Computation → Curvature Estimator → Utility Calculator
- Selector: Top-k filter based on utility score
- Trainer: CPT loop on selected subset

**Critical path:** The computation of mixed curvature matrix $M$ (specifically negative-pair cross moments) and solving for alignment direction $\bar{u}_k$. Poor approximation degrades selection quality.

**Design tradeoffs:**
- JL Sketch Dimension ($k$): Higher $k$ improves alignment accuracy but increases memory/CPU cost
- Curvature Mixing ($\alpha$): Higher $\alpha$ emphasizes negative-pair repulsion, crucial for target discrimination but potentially noisy
- Relevance ($\beta$): Balancing visual vs. text similarity for domain relevance

**Failure signatures:**
- Collapsed Relevance: Poor $\beta$ setting or small $D_{eval}$ causes overfitting to validation set
- Gradient Mismatch: Not freezing backbone during scoring violates proxy alignment theorem

**First 3 experiments:**
1. Compute Spearman's $\rho$ between proxy alignment score and full-parameter alignment on 10k subset to verify Theorem 1
2. Run data selection with $\alpha=0$ vs. $\alpha=0.6$ on small dataset to confirm negative-pair curvature improves target accuracy
3. Train with and without $w_R$ and measure accuracy drop on held-out general-domain benchmark to quantify retention benefit

## Open Questions the Paper Calls Out

### Open Question 1
Can CHIPS be extended to perform data selection without requiring a labeled target evaluation set? The paper acknowledges reliance on target validation distribution and plans to explore unlabeled or shift-robust target signals. This remains unresolved because CHIPS requires labeled target validation samples, which may be impractical in many real-world scenarios.

### Open Question 2
Does CHIPS methodology generalize to other vision-language model architectures beyond CLIP's dual-encoder design with InfoNCE loss? The conclusion states plans to explore extensions beyond CLIP, but the method was specifically designed around CLIP's symmetric InfoNCE objective and end-point subspace geometry.

### Open Question 3
Why does the text projection head dominate alignment over the visual projection head in CLIP domain adaptation? Section 4.1 reports text-only nearly matches all projection heads, but provides no mechanistic explanation for this observed asymmetry.

### Open Question 4
Can CHIPS selection scores be combined with parameter-efficient fine-tuning methods to achieve even greater efficiency gains? The paper only evaluates with full continual pre-training and doesn't explore whether selected subsets remain optimal under different fine-tuning regimes like LoRA or adapters.

## Limitations
- Proxy alignment theorem assumes fixed backbone encoders; significant backbone distribution shifts during CPT may degrade faithfulness guarantees
- Negative-pair mixing parameter α is tuned per dataset without systematic analysis of cross-domain sensitivity
- JL sketch dimension k trades accuracy for efficiency; finite-sample behavior for practical dimensions is not formally analyzed

## Confidence

- **High confidence**: CHIPS outperforms baselines on 17 medical benchmarks and matches full-dataset CPT with 30% data; general-domain retention results are consistent across 31 tasks
- **Medium confidence**: Alignment correlation bound holds under stated assumptions, but practical impact of violating fixed-backbone assumption is not quantified; negative-pair curvature contribution improves target accuracy but optimal mixing ratio varies
- **Low confidence**: Theoretical guarantees for JL sketch preserving alignment rankings are asymptotic; finite-sample behavior for k=4k-16k dimensions is not formally analyzed

## Next Checks

1. **Fixed-backbone violation test**: Run CHIPS scoring with and without freezing the backbone, then measure correlation drop between proxy and full alignment on held-out validation set
2. **α sensitivity sweep**: For challenging domain (e.g., Dermatology), run selection with α∈{0.0, 0.3, 0.6, 0.9} and measure target accuracy vs. general-domain forgetting trade-off
3. **JL sketch dimension ablation**: Compare selection quality using k∈{1k, 4k, 16k, full} on 100k subset of BIOMEDICA, measuring both alignment correlation and downstream accuracy