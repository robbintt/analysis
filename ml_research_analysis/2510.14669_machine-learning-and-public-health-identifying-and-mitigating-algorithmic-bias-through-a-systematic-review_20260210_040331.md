---
ver: rpa2
title: 'Machine Learning and Public Health: Identifying and Mitigating Algorithmic
  Bias through a Systematic Review'
arxiv_id: '2510.14669'
source_url: https://arxiv.org/abs/2510.14669
tags:
- bias
- fairness
- data
- studies
- reporting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study conducts a systematic review of algorithmic bias (AB)\
  \ reporting in Dutch public health (PH) machine learning (ML) research from 2021\u2013\
  2025. It introduces the Risk of Algorithmic Bias Assessment Tool (RABAT), which\
  \ integrates elements from the Cochrane Risk of Bias Tool, PROBAST, and the Microsoft\
  \ Responsible AI checklist, and applies it to 35 peer-reviewed studies."
---

# Machine Learning and Public Health: Identifying and Mitigating Algorithmic Bias through a Systematic Review

## Quick Facts
- arXiv ID: 2510.14669
- Source URL: https://arxiv.org/abs/2510.14669
- Reference count: 40
- Primary result: Systematic review of 35 Dutch public health ML studies (2021-2025) reveals strong technical rigor but near-total absence of fairness-specific reporting; introduces RABAT and ACAR frameworks to address this gap.

## Executive Summary
This study systematically reviews algorithmic bias reporting in Dutch public health machine learning research from 2021-2025, analyzing 35 peer-reviewed studies. The analysis reveals a striking disconnect: while technical aspects like data handling and missing data practices are well-documented, most studies omit explicit fairness framing, subgroup analyses, and transparent discussion of potential harms. In response, the authors introduce the Risk of Algorithmic Bias Assessment Tool (RABAT) and propose the ACAR framework—Awareness, Conceptualization, Application, Reporting—to guide researchers in addressing algorithmic bias throughout the research lifecycle. The findings highlight the need for systematic attention to fairness to ensure ML innovations advance health equity rather than reinforce disparities.

## Method Summary
The study conducted a systematic literature review using Google Scholar to identify Dutch public health ML research from 2021-2025. Two reviewers independently applied the newly developed RABAT (Risk of Algorithmic Bias Assessment Tool) to score papers across 10 criteria (0-3 scale), calculating mean scores and classifying risk levels (High Risk < 0.75, Some Concerns 0.75-1.5, Low Risk > 1.5). The authors then synthesized findings into the ACAR framework (Awareness, Conceptualization, Application, Reporting) to provide actionable guidance for addressing algorithmic bias throughout the research lifecycle.

## Key Results
- Strong technical rigor in data handling (Q6) but near-total absence of fairness-specific reporting (0% using fairness metrics)
- 57% of studies failed to identify at-risk subgroups, and 86% omitted sensitive attributes
- Most studies (54%) did not discuss potential bias impacts or mitigation strategies
- Proposed ACAR framework maps Design Thinking to ML pipeline to embed fairness from conception to reporting

## Why This Works (Mechanism)

### Mechanism 1: Diagnostic Gap Exposure
The study quantifies the asymmetry between high performance in data handling (Q6: Sampling) and near-zero performance in fairness metrics (Q3/Q7). By scoring this gap, the review mechanism converts an implicit omission into an explicit "High Risk" classification, motivating structural change by exposing the disconnect between technical rigor and fairness blindness.

### Mechanism 2: Staged Workflow Integration (ACAR)
The ACAR framework (Awareness, Conceptualization, Application, Reporting) maps Design Thinking to the ML pipeline, reducing cognitive load by integrating fairness as a lifecycle stage rather than a post-hoc check. By mandating "Awareness" before "Application," it forces early definition of protected attributes and subgroups.

### Mechanism 3: Subgroup-Specific Calibration
Moving from aggregate performance metrics to subgroup identification mitigates the "average patient" fallacy. The review identifies that 57% of studies failed to identify at-risk subgroups, proposing subgroup testing as a mandatory component to ensure high aggregate AUC does not mask poor performance for minority populations.

## Foundational Learning

- **Concept: The Fairness-Accuracy Trade-off (Pareto Frontier)**
  - Why needed here: The paper notes a focus on technical performance (AUC). Learners must understand that improving fairness (e.g., equalizing odds) often requires sacrificing some aggregate accuracy.
  - Quick check question: Can you explain why maximizing overall AUC might actually lower the model's utility for a specific minority subgroup?

- **Concept: Protected Attributes & Proxy Variables**
  - Why needed here: The study highlights that 86% of studies omitted sensitive attributes. Learners must recognize that excluding "race" or "gender" is insufficient if other variables (e.g., zip code, language) act as perfect proxies.
  - Quick check question: If you remove "ethnicity" from your training data, how can the model still discriminate based on ethnicity?

- **Concept: Sociotechnical Systems**
  - Why needed here: The paper frames PH+ML as a sociotechnical challenge, not just a coding task.
  - Quick check question: In the context of the Dutch childcare benefits scandal (referenced implicitly via general governance discussion), does a technically "correct" prediction justify the social outcome?

## Architecture Onboarding

- **Component map:** Raw Public Health datasets (EHR, registries) -> RABAT Diagnostic Layer -> ACAR Process Layer (Awareness -> Conceptualization -> Application -> Reporting) -> Bias-aware models + standardized fairness reporting

- **Critical path:**
  1. **Pre-processing:** Execute ACAR *Awareness* & *Conceptualization*. Define the specific fairness definition relevant to the PH domain.
  2. **Data Engineering:** Conduct RABAT Q6/Q7 check. Verify if missingness correlates with sensitive attributes before imputation.
  3. **Modeling:** Execute ACAR *Application*. Train models with constraints (e.g., using Fairlearn or AIF360 libraries).
  4. **Evaluation:** Execute RABAT Q2/Q5. Disaggregate performance metrics by subgroup; do not rely solely on aggregate AUC.

- **Design tradeoffs:**
  - **Transparency vs. Complexity:** Highly complex models may offer better predictions but make "Reporting" and bias auditing significantly harder compared to interpretable models.
  - **Universalism vs. Contextualism:** A universal fairness metric may clash with specific Dutch PH disparities (e.g., distinct disease burdens in specific migrant populations).

- **Failure signatures:**
  - **The "Compliance Checkbox" Failure:** High RABAT scores on "Reporting" but zero change in actual model weights or subgroup performance.
  - **The "Data Silo" Failure:** Inability to execute ACAR "Awareness" because protected attributes are legally scrubbed from the data.

- **First 3 experiments:**
  1. **Baseline Audit:** Apply the condensed RABAT questionnaire to your last 3 deployed models to establish a baseline "High Risk" or "Low Risk" score.
  2. **Proxy Correlation Test:** In your current dataset, calculate the correlation matrix between standard features and sensitive attributes (if available) to identify potential proxies.
  3. **Subgroup Slice Analysis:** Take one high-performing model and re-calculate the Confusion Matrix specifically for the smallest demographic subgroup in the test set to check for performance collapse.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the ACAR framework be effectively validated and operationalized in active, real-world public health machine learning projects?
- Basis in paper: [explicit] The authors explicitly recommend "empirically validating ACAR in active PH+ML projects" as a primary direction for future work.
- Why unresolved: The framework is currently theoretical and derived from a retrospective systematic review; it has not been tested in a prospective or operational setting.
- What evidence would resolve it: Results from pilot studies or trials where PH+ML teams apply ACAR, demonstrating improved fairness metrics or reporting transparency compared to control groups.

### Open Question 2
- Question: Why do public health ML researchers frequently omit fairness considerations, and is this due to lack of awareness, disciplinary norms, or undocumented mitigation efforts?
- Basis in paper: [explicit] The paper calls for "exploring researchers’ decision-making around AB (e.g., interviews or surveys) to understand whether and why fairness considerations are addressed but not documented."
- Why unresolved: The systematic review identified a gap in reporting but could not determine the underlying motivations, knowledge gaps, or hidden practices of the research teams involved.
- What evidence would resolve it: Qualitative interview data or survey responses from PH+ML researchers explaining their design choices and perceived barriers to reporting algorithmic bias.

### Open Question 3
- Question: Are the RABAT and ACAR frameworks applicable to non-Dutch contexts, such as low-resource settings or different governance regimes?
- Basis in paper: [explicit] The authors state the need for "applying RABAT and ACAR in non-Dutch contexts to test their applicability and identify any necessary context-specific adaptations."
- Why unresolved: The study was exclusively situated in the Dutch public health infrastructure, which is characterized by high digitalization and specific governance structures that may not exist elsewhere.
- What evidence would resolve it: Successful application of the tools in international studies, particularly in low-resource environments, resulting in a validated, context-adapted version of the framework.

## Limitations
- Reproducibility of corpus is uncertain due to Google Scholar's volatile search results
- RABAT rubric's full calibration details are not provided, requiring inference for scoring boundaries
- Study scope limited to Dutch public health institutions, potentially constraining generalizability to other healthcare systems

## Confidence
- **High Confidence:** The documented technical rigor gap (high data handling scores vs. low fairness reporting) is directly observable from the presented RABAT scores
- **Medium Confidence:** The ACAR framework's practical effectiveness in reducing cognitive load for interdisciplinary teams is theoretically sound but lacks empirical validation
- **Low Confidence:** The generalizability of findings to non-Dutch contexts requires caution due to varying institutional norms and legal frameworks

## Next Checks
1. **RABAT Calibration Test:** Apply the condensed RABAT questionnaire to a small sample of your own ML studies (n=3-5) to empirically test the scoring boundaries between "Minimal," "Moderate," and "Extensive"
2. **Subgroup Performance Audit:** For any deployed PH+ML model, disaggregate performance metrics by the smallest available demographic subgroup to verify aggregate metrics mask disparate outcomes
3. **Proxy Variable Scan:** If you have access to sensitive attributes, calculate the correlation matrix between standard features and protected attributes to identify potential proxy variables that could perpetuate bias despite attribute removal