---
ver: rpa2
title: Multiple Linked Tensor Factorization
arxiv_id: '2502.20286'
source_url: https://arxiv.org/abs/2502.20286
tags:
- tensor
- data
- rank
- matrices
- decomposition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes a novel Multiple Linked Tensors Factorization
  (MULTIFAC) method that extends the CANDECOMP/PARAFAC (CP) decomposition to simultaneously
  reduce the dimension of multiple multi-way arrays and approximate underlying signal.
  The key innovation is introducing L2 penalties on latent factors, which leads to
  rank sparsity and automatically reveals shared versus individual components across
  datasets.
---

# Multiple Linked Tensor Factorization

## Quick Facts
- arXiv ID: 2502.20286
- Source URL: https://arxiv.org/abs/2502.20286
- Authors: Zhiyu Kang; Raghavendra B. Rao; Eric F. Lock
- Reference count: 8
- Primary result: Novel method for simultaneous dimension reduction and signal approximation in multiple linked tensors with L2 penalties inducing rank sparsity.

## Executive Summary
This paper introduces MULTIFAC, a method that extends CP decomposition to simultaneously reduce dimensions of multiple linked multi-way arrays while automatically revealing shared versus individual components across datasets. The key innovation is applying L2 penalties to latent factor matrices, which induces rank sparsity by shrinking component weights to zero. This allows the method to identify which components are shared across all tensors and which are unique to individual data sources. The approach handles incomplete data through an EM-ALS extension with imputation capabilities.

## Method Summary
MULTIFAC performs simultaneous tensor factorization by decomposing multiple linked tensors using Alternating Least Squares with L2 penalties on factor matrices. The method automatically identifies shared and individual structures by shrinking component weights through rank sparsity. Missing data is handled via an EM-ALS algorithm that iteratively imputes missing values using current low-rank estimates. Tuning involves a two-step cross-validation process: first determining rank using one standard error rule, then fine-tuning the penalty parameter.

## Key Results
- Simulation studies show MULTIFAC achieves lower relative squared errors compared to NLS from Tensorlab
- Multi-tensor imputation with 10% missing data shows effective recovery of underlying signals
- Application to multi-omics data captures 20-40% of variability through shared structures between hematology and MRI data
- Clear discrimination of anemia status in individual hematology loadings

## Why This Works (Mechanism)

### Mechanism 1: Rank Sparsity via L2 Penalties on Factor Matrices
Applying L2 penalties to factor matrices promotes rank sparsity by shrinking component weights to zero, revealing shared/individual structures. This works because the L2 penalty on factor norms is equivalent to an L2/N penalty on component weights for N-way tensors.

### Mechanism 2: Linked Tensor Factorization for Multi-Source, Multi-Way Data
Simultaneous decomposition of multiple linked tensors with shared modes captures both shared and individual signals. The L2 penalty encourages some components to be shared across all tensors while others are individual.

### Mechanism 3: EM-ALS for Missing Data Imputation
An EM-ALS algorithm enables imputation of both entry-wise and tensor-wise missing data. Entry-wise uses full low-rank structure while tensor-wise missing uses only shared structure to avoid borrowing scale information from unrelated samples.

## Foundational Learning

**Concept: CANDECOMP/PARAFAC (CP) Decomposition**
- Why needed: MULTIFAC extends CP decomposition. Understanding CP is essential for grasping factor matrix updates in ALS and the role of component weights.
- Quick check: Given a 3-way tensor X (I×J×K) and factor matrices A, B, C from CP decomposition, how would you write the reconstruction of X in terms of outer products?

**Concept: Alternating Least Squares (ALS)**
- Why needed: The core optimization uses ALS to iteratively update factor matrices. Understanding ALS is critical for implementing the algorithm.
- Quick check: In ALS for CP, if you are updating factor matrix A, what are the fixed components in the least squares problem?

**Concept: Rank Sparsity via Penalization**
- Why needed: The L2 penalty's role in inducing rank sparsity is the theoretical engine for automatic shared/individual component discovery.
- Quick check: Why does an L2 penalty on factor matrices lead to rank sparsity in tensors but not in matrices (N=2)?

## Architecture Onboarding

**Component map:**
- Input tensors (linked by shared mode) → Matricization (mode-n unfolding) → Missing data masking → EM-ALS core algorithm → Factor matrices and reconstructed tensors

**Critical path:**
1. Data preparation: Align linked modes, mask missing entries
2. Rank/Penalty tuning: Run two-step cross-validation (Step 1: determine rank, Step 2: fine-tune σ)
3. EM-ALS execution: With tuned σ, run EM-ALS to convergence
4. Interpretation: Analyze loadings in factor matrices for shared/individual structures

**Design tradeoffs:**
- Rank selection: Two-step CV is robust but slow; alternatives like variational Bayes not implemented
- Initialization: Multiple random starts mitigate local minima but increase runtime
- Missing data: Tensor-wise imputation uses only shared structure (conservative) vs. entry-wise (full structure)

**Failure signatures:**
- Slow convergence or oscillation: Likely poor initialization or high σ; use tempered regularization
- Poor imputation (high RSE_missing): Check if SNR is very low (<1/3) or missingness is too high (>20%)
- No clear shared structure: σ may be too high or shared mode is not meaningful
- Algorithm does not converge to global minimum: Common for ALS; use multiple random restarts

**First 3 experiments:**
1. Replicate single-tensor simulation (Section 5.1) with SNR=1; verify rank recovery matches Table 1 (RSE ~0.08)
2. Test multi-tensor imputation with 10% missing (5% entry-wise, 5% tensor-wise); compare RSE_missing vs. Table 5
3. Apply to provided multi-omics data; check if shared components capture 20-40% variance as reported (Table 7)

## Open Questions the Paper Calls Out

**Open Question 1:** Can MULTIFAC be extended to scenarios where subsets of tensors share multiple modes, rather than being linked solely along the first mode? The current framework is tailored for tensors linked along their first mode, and extending it to multiple modes requires new efficiency strategies.

**Open Question 2:** Can theoretically grounded approaches like variational Bayesian models or random tensor theory replace the current computationally intensive grid search for penalty selection? The current method relies on grid search which is computationally intensive, particularly for large datasets.

**Open Question 3:** How does L2 penalty's rank sparsity performance compare to explicit sparsity priors like the spike-and-slab prior used in Bayesian multi-tensor decomposition? The simulations only compare MULTIFAC against NLS, not the Bayesian alternative.

## Limitations
- Theoretical foundation for L2 penalties inducing rank sparsity is empirically validated but limited to simulated data
- EM-ALS assumes missingness is ignorable, which may not hold for tensor-wise missingness in real-world data
- Scalability is not extensively tested; ALS convergence can be slow for high-dimensional tensors

## Confidence

**High Confidence:** The core mechanism of L2 penalties inducing rank sparsity (Theorems 1-3) and ALS optimization framework are well-established in tensor decomposition literature.

**Medium Confidence:** The EM-ALS algorithm for missing data imputation is theoretically sound but performance in high-missingness regimes or with non-ignorable missingness is uncertain.

**Low Confidence:** The two-step cross-validation procedure for rank and penalty selection is robust in simulations but effectiveness in complex real-world data is unproven.

## Next Checks

1. **Scalability Test:** Apply MULTIFAC to larger tensors (e.g., 100×100×100) with varying ranks and missingness rates; measure ALS convergence time and imputation accuracy.

2. **Missingness Robustness:** Simulate tensor-wise missingness with varying patterns and rates (up to 50%); compare RSE_missing to evaluate imputation reliability under extreme missingness.

3. **Rank Selection Stability:** Repeat the two-step CV procedure on multi-omics data with different random seeds and CV fold counts; assess stability of selected ranks and impact on shared/individual structure recovery.