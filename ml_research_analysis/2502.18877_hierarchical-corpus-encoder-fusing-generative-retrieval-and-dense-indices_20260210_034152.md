---
ver: rpa2
title: 'Hierarchical corpus encoder: Fusing generative retrieval and dense indices'
arxiv_id: '2502.18877'
source_url: https://arxiv.org/abs/2502.18877
tags:
- retrieval
- documents
- document
- latexit
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a hierarchical corpus encoder (HCE) that bridges
  generative retrieval and dense indices. The core insight is that generative retrieval
  implicitly performs contrastive learning between sibling nodes in a document hierarchy,
  which is key to its performance.
---

# Hierarchical corpus encoder: Fusing generative retrieval and dense indices

## Quick Facts
- arXiv ID: 2502.18877
- Source URL: https://arxiv.org/abs/2502.18877
- Authors: Tongfei Chen, Ankita Sharma, Adam Pauls, Benjamin Van Durme
- Reference count: 27
- Primary result: HCE outperforms dense and generative retrieval baselines in both supervised and unsupervised settings

## Executive Summary
This paper introduces a hierarchical corpus encoder (HCE) that bridges the gap between generative retrieval and dense indices. The core insight is that generative retrieval implicitly performs contrastive learning between sibling nodes in a document hierarchy, which is key to its performance. HCE formalizes this by jointly learning an encoder and a document hierarchy tree, where positive samples are contrasted against siblings during training. At test time, the method falls back to maximum inner product search with an external index. This approach combines the advantages of both paradigms: zero-shot adaptation to new domains and efficient addition/removal of documents without fine-tuning.

## Method Summary
The hierarchical corpus encoder (HCE) works by learning a joint embedding space that incorporates document hierarchy information. During training, the model uses contrastive learning where positive query-document pairs are contrasted against sibling documents in the hierarchy tree as negative samples. This hierarchical contrastive objective encourages the encoder to learn representations that capture semantic relationships at multiple levels of the document structure. The document hierarchy is learned jointly with the encoder parameters, allowing the model to discover meaningful groupings of documents. At inference time, the learned encoder is used to generate embeddings that are stored in an external index, enabling efficient retrieval through maximum inner product search.

## Key Results
- HCE achieves state-of-the-art performance on MS MARCO under both supervised and unsupervised settings
- The model demonstrates strong zero-shot adaptation capabilities to new domains without fine-tuning
- HCE enables efficient addition and removal of documents without requiring full model retraining

## Why This Works (Mechanism)
The effectiveness of HCE stems from its hierarchical contrastive learning approach. By treating sibling documents as negative samples during training, the model learns to distinguish between semantically related but distinct documents at the same hierarchical level. This creates a more discriminative embedding space compared to standard contrastive learning, where all non-positive samples are treated equally. The hierarchical structure also allows the model to learn representations that capture both fine-grained document-level semantics and higher-level topical relationships. The generative retrieval aspect is maintained through the ability to perform zero-shot retrieval without requiring an external index, though at the cost of efficiency.

## Foundational Learning
- Document hierarchies: Needed to provide structured negative samples for contrastive learning. Quick check: Verify that the learned hierarchies reflect semantic relationships in the corpus.
- Contrastive learning with hierarchical negatives: Required to distinguish between sibling documents. Quick check: Ablation studies showing impact of sibling contrast vs flat negatives.
- Maximum inner product search: Enables efficient retrieval at test time. Quick check: Compare retrieval latency and recall@k with exact search.
- Zero-shot retrieval: Allows adaptation to new domains without fine-tuning. Quick check: Test on out-of-domain datasets.
- Joint learning of encoder and hierarchy: Necessary to discover meaningful document groupings. Quick check: Evaluate quality of learned hierarchies using clustering metrics.

## Architecture Onboarding

Component map: Document corpus -> Document hierarchy tree -> Encoder network -> Embedding space -> Contrastive loss (with sibling negatives) -> Trained encoder -> External index (for inference)

Critical path: The training process involves iteratively refining both the document hierarchy and the encoder parameters. The hierarchy determines which documents serve as negative samples during contrastive learning, while the encoder updates influence how documents are grouped in the hierarchy. This creates a feedback loop where the model progressively learns better representations and more meaningful document groupings.

Design tradeoffs: The method trades off between the flexibility of generative retrieval (zero-shot adaptation) and the efficiency of dense retrieval (fast search with indices). Using sibling documents as negatives requires having or inferring a meaningful document hierarchy, which may not be available in all domains. The joint learning approach increases training complexity compared to standard dense retrieval methods.

Failure signatures: Poor performance may occur when: 1) the document corpus lacks natural hierarchical structure, 2) the learned hierarchy is of low quality, 3) the contrastive learning fails to converge properly, or 4) the embedding dimensionality is insufficient to capture the semantic relationships.

First experiments: 1) Verify that sibling contrast improves over flat negatives on a small dataset. 2) Test zero-shot performance on an out-of-domain dataset. 3) Measure retrieval latency with different index sizes.

## Open Questions the Paper Calls Out
The paper notes that the effectiveness of sibling contrast in generative retrieval needs more rigorous empirical validation. It also acknowledges that the approach's dependence on document hierarchies could limit its applicability to domains without natural hierarchical structures. The paper calls for investigation into methods for constructing or inferring hierarchies when they don't exist naturally, as well as broader validation across diverse retrieval tasks beyond MS MARCO.

## Limitations
- Performance improvements in supervised settings appear modest compared to dense retrieval baselines
- The method requires meaningful document hierarchies, limiting applicability to domains without natural structure
- Computational costs during training are not thoroughly analyzed
- Limited validation on diverse retrieval tasks beyond MS MARCO and BEIR

## Confidence

High confidence:
- Zero-shot adaptation capabilities and efficiency of adding/removing documents without fine-tuning

Medium confidence:
- The theoretical framework connecting generative retrieval to contrastive learning

Low confidence:
- The magnitude of performance improvements in supervised settings
- Generalizability beyond MS MARCO dataset

## Next Checks
1. Conduct ablation studies removing the sibling contrastive component to quantify its specific contribution to retrieval performance
2. Test the method on datasets without natural document hierarchies to evaluate how well inferred hierarchies perform
3. Measure training time and memory requirements compared to baseline methods to assess practical deployment costs