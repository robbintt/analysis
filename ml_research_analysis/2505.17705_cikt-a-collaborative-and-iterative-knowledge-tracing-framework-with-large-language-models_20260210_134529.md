---
ver: rpa2
title: 'CIKT: A Collaborative and Iterative Knowledge Tracing Framework with Large
  Language Models'
arxiv_id: '2505.17705'
source_url: https://arxiv.org/abs/2505.17705
tags:
- student
- knowledge
- analyst
- profiles
- profile
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CIKT, a knowledge tracing framework that
  uses LLMs to generate structured student profiles and predict future performance.
  The key innovation is a dual-component architecture with an Analyst generating explainable
  profiles and a Predictor using them for forecasting, refined through iterative optimization
  using Kahneman-Tversky Optimization.
---

# CIKT: A Collaborative and Iterative Knowledge Tracing Framework with Large Language Models

## Quick Facts
- **arXiv ID:** 2505.17705
- **Source URL:** https://arxiv.org/abs/2505.17705
- **Reference count:** 26
- **Primary result:** CIKT achieves up to 7.13% improvement in accuracy and F1-score over traditional KT models and LLM baselines.

## Executive Summary
This paper introduces CIKT, a knowledge tracing framework that uses large language models to generate structured student profiles and predict future performance. The key innovation is a dual-component architecture with an Analyst generating explainable profiles and a Predictor using them for forecasting, refined through iterative optimization using Kahneman-Tversky Optimization. Evaluated on three educational datasets, CIKT significantly outperforms traditional KT models and general LLM baselines while providing enhanced explainability through dynamically updated profiles.

## Method Summary
CIKT employs a four-stage approach: first, GPT-4o generates initial textual profiles from student sequences, which are manually curated and used to fine-tune the Analyst LLM via supervised distillation. The Analyst then generates profiles for all training data, which are used to train the Predictor LLM on binary correctness prediction. Finally, an iterative Kahneman-Tversky Optimization loop refines the Analyst using binary prediction accuracy as reward signals, followed by Predictor retraining. The framework uses Llama3.1-8B or Qwen2.5-7B as backbones with LoRA fine-tuning, processing sequences of 50 interactions and evaluating with Accuracy, F1, and sequence-length-filtered metrics.

## Key Results
- CIKT outperforms traditional KT models (DKT, DKVMN) by up to 7.13% in accuracy and F1-score
- Achieves 4.8% accuracy improvement when profiles are included vs. excluded at inference
- Demonstrates improved scalability on longer sequences with up to 7.12% accuracy gain for sequences >15 interactions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Iterative feedback through KTO improves profile quality and prediction accuracy
- **Mechanism:** The Predictor's binary correctness judgment (+1/-1) serves as a reinforcement signal to update the Analyst's parameters via policy gradient-style optimization. Profiles that lead to correct predictions are reinforced; those leading to errors are suppressed.
- **Core assumption:** Binary prediction accuracy is a valid proxy for profile quality—that profiles enabling correct predictions capture meaningful knowledge states.
- **Evidence anchors:**
  - [abstract] "the Analyst is iteratively refined based on the predictive accuracy of the Predictor, which conditions on the generated profiles"
  - [Section 3.5] Equations 7-10 formalize the KTO loss: L_KTO = -Σ[r_t+1 · log π_θA(p_t | x_t)]
  - [corpus] Limited direct corpus evidence on KTO for KT; mechanism remains framework-specific
- **Break condition:** If prediction accuracy plateaus or degrades across iterations, or if profiles become homogeneous (mode collapse), the feedback signal is no longer informative.

### Mechanism 2
- **Claim:** Structured textual profiles provide explicit, interpretable knowledge state representations that improve prediction
- **Mechanism:** The Analyst generates natural language profiles summarizing mastery patterns, difficulties, and learning trajectories. These profiles augment the Predictor's input, providing condensed historical context beyond raw interaction sequences.
- **Core assumption:** Textual profiles capture pedagogically relevant patterns that raw sequences alone do not explicitly represent.
- **Evidence anchors:**
  - [abstract] "CIKT employs a dual-component architecture: an Analyst generates dynamic, explainable user profiles"
  - [Section 4.3, Table 3] "CIKT w/o Profile (Inference)" shows significant performance drops when profiles are withheld (e.g., ACC drops from 0.775 to 0.755 on ASSIST2009)
  - [corpus] Neighbor papers (e.g., "Next Token Knowledge Tracing") similarly exploit pretrained representations but lack explicit profile generation
- **Break condition:** If profiles become generic templates lacking student-specific detail, or if the Predictor learns to ignore them, the mechanism fails.

### Mechanism 3
- **Claim:** Distillation from a large teacher model bootstraps the Analyst's profiling capability
- **Mechanism:** GPT-4o generates initial profiles from student sequences; curated high-quality profiles are used to fine-tune the Analyst via supervised cross-entropy loss, transferring profiling competence.
- **Core assumption:** The teacher model's profile generation capability is transferable to smaller backbone models (Llama3.1-8B, Qwen2.5-7B).
- **Evidence anchors:**
  - [Section 3.2] "LLM_teacher produces initial textual profiles... subsequently, these profiles undergo a manual curation process"
  - [Section 3.2, Eq. 3] Distillation loss L_Distill minimizes token-level cross-entropy between Analyst and curated teacher profiles
  - [corpus] No direct corpus evidence on teacher-student distillation for KT profiles
- **Break condition:** If curated profiles contain systematic biases or errors, or if the student model capacity is insufficient to capture teacher behavior.

## Foundational Learning

- **Concept: Knowledge Tracing (KT)**
  - **Why needed here:** CIKT targets the core KT task—modeling evolving student knowledge from interaction history to predict future performance. Understanding this framing is essential.
  - **Quick check question:** Given a student's sequence of (exercise, correctness) pairs, can you articulate what a KT model aims to predict?

- **Concept: Reinforcement Learning from Binary Feedback**
  - **Why needed here:** The KTO iteration stage uses binary reward signals (+1/-1) to optimize the Analyst, resembling policy gradient methods.
  - **Quick check question:** How does binary feedback differ from dense gradient signals in standard supervised learning, and what instability risks might arise?

- **Concept: Cross-Entropy Loss for Sequence Generation**
  - **Why needed here:** Both distillation (profile generation) and prediction use cross-entropy variants—token-level for profiles, binary for correctness.
  - **Quick check question:** Why might token-level cross-entropy be appropriate for profile generation but insufficient alone for prediction accuracy?

## Architecture Onboarding

- **Component map:**
  - **Analyst:** LLM backbone (Llama3.1-8B or Qwen2.5-7B) fine-tuned to generate textual profiles from interaction sequences. Parameters: θ_A.
  - **Predictor:** Separate LLM instance fine-tuned to predict binary correctness given (history, profile, target exercise). Parameters: θ_P.
  - **Teacher model:** GPT-4o used offline for initial profile annotation (distillation only, not in production).
  - **KTO loop:** Binary reward computation from Predictor accuracy → Analyst parameter update → Predictor re-training cycle.

- **Critical path:**
  1. Run distillation: Generate teacher profiles → curate → fine-tune Analyst (LDistill).
  2. Generate profiles for all training data using Analyst.
  3. Train Predictor on (history, profile, target exercise, label) tuples (LPredict).
  4. Iterate: Sample batch → Analyst generates profile → Predictor predicts → compute reward → update Analyst via KTO → regenerate profiles → retrain Predictor.
  5. Repeat iteration until validation performance stabilizes (paper uses ~3 rounds).

- **Design tradeoffs:**
  - **Iteration rounds vs. compute cost:** More rounds improve performance but with diminishing returns (Fig 2a shows plateauing after 3 rounds).
  - **Iteration sample size (k):** Larger k (1000-2000) helps long-sequence accuracy but increases overhead (Fig 2b).
  - **Profile granularity vs. noise:** Detailed profiles may capture more signal but risk overfitting to specific patterns.
  - **Backbone choice:** Llama3.1-8B vs. Qwen2.5-7B trade-offs not deeply analyzed; both work, Qwen slightly better on some metrics.

- **Failure signatures:**
  - **Profile mode collapse:** All generated profiles become similar, losing student-specific signal (check profile diversity metrics).
  - **KTO instability:** Analyst loss oscillates or diverges—monitor reward distribution and gradient norms.
  - **Predictor ignoring profiles:** Ablation shows minimal performance drop when profiles removed (Table 3)—indicates poor integration.
  - **Over-iteration:** Performance degrades after many rounds (possible overfitting to training feedback).

- **First 3 experiments:**
  1. **Baseline replication:** Reproduce CIKT-Qwen2.5-7B results on ASSIST2009 (ACC ~0.777). Verify data preprocessing (sequence segmentation to 50, difficulty calculation).
  2. **Ablation sanity check:** Run "w/o Profile (Inference)" variant—confirm ACC drops by ~0.02+ points, validating profile utility.
  3. **Iteration sensitivity:** Vary iteration rounds (0, 1, 2, 3) on a held-out subset; plot validation ACC to reproduce Fig 2a trajectory and identify optimal stopping point.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does incorporating the textual content of question stems significantly improve CIKT's fine-grained modeling capabilities?
- **Basis in paper:** [explicit] The Limitations section states that question stems were excluded due to context window constraints, limiting content-aware modeling.
- **Why unresolved:** Current inputs utilize only metadata (knowledge concepts, difficulty) rather than semantic content.
- **What evidence would resolve it:** A comparative study evaluating CIKT performance when fine-tuned with datasets including full question text.

### Open Question 2
- **Question:** How does CIKT perform under ranking-sensitive evaluation metrics such as AUC?
- **Basis in paper:** [explicit] The authors note that because the framework generates binarized judgments, they did not employ ranking-sensitive metrics like AUC.
- **Why unresolved:** The current evaluation is restricted to classification metrics (Accuracy, F1), leaving ranking efficacy unverified.
- **What evidence would resolve it:** Benchmarking results comparing CIKT against baselines using AUC or similar ranking scores.

### Open Question 3
- **Question:** Is the computational overhead of the iterative KTO refinement loop feasible for real-time educational deployment?
- **Basis in paper:** [inferred] The methodology describes a resource-intensive loop involving repeated Analyst generation, prediction, and Predictor retraining.
- **Why unresolved:** The paper does not analyze the latency or computational cost of the iterative stages versus the accuracy gains.
- **What evidence would resolve it:** An efficiency analysis reporting training duration and inference latency per iteration round.

## Limitations
- KTO optimization mechanism lacks direct corpus validation for knowledge tracing applications
- Teacher-student distillation setup incompletely specified (prompt, curation criteria, acceptance rate unclear)
- No analysis of profile diversity or meaningful evolution across iterations
- Computational overhead of iterative refinement not evaluated for real-world deployment feasibility

## Confidence
- **High confidence:** Dual-architecture design with separate Analyst and Predictor components is well-specified and ablation studies validate profile contribution
- **Medium confidence:** Quantitative results on three datasets are robust, showing consistent improvements over baselines
- **Low confidence:** Claim that KTO "progressively improves" profiles is largely theoretical without evidence of pedagogically meaningful profile evolution

## Next Checks
1. **Profile Diversity Analysis:** Compute pairwise cosine similarity or semantic overlap across generated profiles for individual students to verify profiles remain distinct and student-specific throughout iterations.
2. **KTO Stability Test:** Monitor the reward distribution (mean and variance) across KTO iterations to detect oscillation or divergence, and implement KL regularization if instability appears.
3. **Generalization Test:** Evaluate CIKT on a held-out dataset from a different domain (e.g., MOOC interactions) to verify profile generation and prediction capabilities transfer beyond the three studied datasets.