---
ver: rpa2
title: 'CPC-CMS: Cognitive Pairwise Comparison Classification Model Selection Framework
  for Document-level Sentiment Analysis'
arxiv_id: '2507.14022'
source_url: https://arxiv.org/abs/2507.14022
tags:
- table
- classification
- document
- sentiment
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study proposes the Cognitive Pairwise Comparison Classification
  Model Selection (CPC-CMS) framework for document-level sentiment analysis. CPC-CMS
  uses expert judgment to weigh evaluation criteria (accuracy, precision, recall,
  F1-score, specificity, MCC, Kappa, and efficiency) and applies a weighted decision
  matrix to select the best model.
---

# CPC-CMS: Cognitive Pairwise Comparison Classification Model Selection Framework for Document-level Sentiment Analysis

## Quick Facts
- arXiv ID: 2507.14022
- Source URL: https://arxiv.org/abs/2507.14022
- Authors: Jianfei Li; Kevin Kam Fung Yuen
- Reference count: 40
- Primary result: Proposes CPC-CMS framework using expert-weighted criteria to select best sentiment analysis model

## Executive Summary
This paper introduces the Cognitive Pairwise Comparison Classification Model Selection (CPC-CMS) framework for document-level sentiment analysis. The framework leverages expert judgment to weight evaluation criteria including accuracy, precision, recall, F1-score, specificity, MCC, Kappa, and efficiency. A weighted decision matrix is then applied to select the optimal classification model from seven baselines: Naive Bayes, LSVC, Random Forest, Logistic Regression, XGBoost, LSTM, and ALBERT. The methodology is evaluated on three social media datasets, demonstrating that ALBERT achieves the best performance on evaluation metrics excluding time efficiency.

## Method Summary
The CPC-CMS framework consists of two main phases: first, experts provide pairwise comparison judgments to determine weights for evaluation criteria; second, these weights are applied in a decision matrix to rank classification models. Seven baseline models are evaluated on three social media datasets using multiple performance metrics. The framework explicitly incorporates human judgment through expert surveys, distinguishing it from purely automated model selection approaches. Model performance is assessed across accuracy, precision, recall, F1-score, specificity, MCC, Kappa, and efficiency metrics, with the final selection based on weighted aggregation of these criteria.

## Key Results
- ALBERT achieves highest performance across evaluation metrics excluding time efficiency
- No single model dominates when efficiency considerations are included
- CPC-CMS provides systematic, human-centered approach to model selection
- Framework successfully applied to document-level sentiment analysis on social media datasets

## Why This Works (Mechanism)
The framework works by incorporating human expertise into the model selection process through weighted decision criteria. By allowing domain experts to determine the relative importance of different evaluation metrics, the framework captures nuanced priorities that purely algorithmic approaches might miss. The pairwise comparison method ensures consistent weight assignment across criteria, while the decision matrix provides a structured way to combine multiple performance metrics into a single selection decision.

## Foundational Learning
- Pairwise comparison methodology: Used to derive consistent weights from expert judgments; needed to ensure reliable weight assignment across evaluation criteria
- Weighted decision matrix: Combines multiple evaluation metrics according to expert-determined weights; needed to transform individual metric scores into overall model rankings
- Expert judgment integration: Incorporates human expertise into automated selection process; needed to capture domain-specific priorities and trade-offs

## Architecture Onboarding

Component Map:
Expert Survey -> Weight Determination -> Decision Matrix -> Model Selection

Critical Path:
Expert judgments flow through pairwise comparison process to generate weights, which are then applied to model evaluation metrics in the decision matrix to produce final model rankings.

Design Tradeoffs:
The framework trades automation for human expertise integration. While this introduces potential subjectivity and requires expert resources, it captures domain-specific priorities that automated approaches might overlook. The pairwise comparison method ensures consistency but requires careful expert selection.

Failure Signatures:
- Inconsistent expert judgments leading to unstable weights
- Weight distributions that overly favor or disfavor certain metrics
- Decision matrix sensitivity to weight variations
- Model performance variations across different datasets

3 First Experiments:
1. Validate weight stability by having experts repeat pairwise comparisons after time interval
2. Test framework sensitivity by varying weights systematically and observing model selection changes
3. Compare CPC-CMS selections against random baseline selections across multiple dataset splits

## Open Questions the Paper Calls Out
None

## Limitations
- Small-scale expert survey (n=20) introduces potential subjectivity and limits generalizability
- Framework validation limited to sentiment analysis domain without testing on other classification tasks
- Lack of details about expert qualifications and diversity affecting weight assignment reliability

## Confidence

High confidence:
- ALBERT achieves best performance on evaluation metrics excluding time efficiency
- Framework methodology is well-defined and systematically applied

Medium confidence:
- CPC-CMS provides systematic model selection framework (limited validation scope)
- "No single model performs best when considering efficiency" claim (narrow efficiency metrics, small expert sample)

## Next Checks
1. Validate CPC-CMS framework across different classification tasks (e.g., topic classification, intent detection) to test domain generalizability
2. Conduct sensitivity analysis by varying expert weights to assess how different weight assignments affect model selection outcomes
3. Compare CPC-CMS against automated model selection approaches (like AutoML) using identical datasets and evaluation criteria to benchmark its effectiveness