---
ver: rpa2
title: 'FOCUS: First Order Concentrated Updating Scheme'
arxiv_id: '2501.12243'
source_url: https://arxiv.org/abs/2501.12243
tags:
- focus
- adam
- learning
- signum
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents FOCUS, an optimizer designed to improve Signum's
  performance on narrowing valley loss landscapes by adding an attraction force toward
  moving averaged parameters. The authors hypothesize that the pre-training loss landscape
  features narrowing valleys, and through synthetic loss function experiments, discover
  that Adam's performance degrades when gradient noise is large relative to valley
  sharpness because it reduces effective step size too drastically.
---

# FOCUS: First Order Concentrated Updating Scheme

## Quick Facts
- arXiv ID: 2501.12243
- Source URL: https://arxiv.org/abs/2501.12243
- Reference count: 40
- GPT-2 small trained with FOCUS achieves approximately 2x speedup compared to Adam from the literature while maintaining lower final validation loss.

## Executive Summary
This paper introduces FOCUS, an optimizer that enhances Signum by incorporating attraction toward moving averaged parameters, allowing it to handle noise better while maintaining larger step sizes. The authors hypothesize that LLM pre-training loss landscapes feature narrowing valleys, and through synthetic loss function experiments, discover that Adam's performance degrades when gradient noise is large relative to valley sharpness because it reduces effective step size too drastically. FOCUS addresses this by maintaining larger step sizes while incorporating self-attraction to better handle noise. In GPT-2 small training, FOCUS demonstrates greater stability than Signum and Adam, achieving approximately 2x speedup compared to Adam from the literature in terms of training time while maintaining lower final validation loss. The results suggest gradient noise may be a limiting factor in LLM training.

## Method Summary
FOCUS is an optimizer that builds on Signum by adding an attraction force toward exponentially moving averaged parameters. The update rule computes momentum m_t (standard first-moment estimate), parameter EMA θ̄_t with bias correction θ̂_t=θ̄_t/(1-β₂^t), and applies the composite update θ_{t+1}=θ_t - η(sign(m_t) + γ·sign(θ_t - θ̂_t) + ω·θ_t). The attraction term pulls current parameters toward their historical center, constraining trajectory width in sharp directions without shrinking step magnitude. This addresses the challenge of navigating narrowing valley loss landscapes where parameters must stay compact along sharp dimensions to progress along flat ones.

## Key Results
- FOCUS achieves ~2x speedup compared to Adam from the literature in GPT-2 small training while maintaining lower final validation loss
- Adam's performance degrades when gradient noise is large relative to valley sharpness, reducing effective step size too drastically
- FOCUS demonstrates greater stability than Signum and Adam in training experiments
- Theoretical analysis shows FOCUS achieves O(√T) regret bound, matching Adam's convergence rate on convex problems

## Why This Works (Mechanism)

### Mechanism 1: Self-Attraction Toward Moving Averaged Parameters
Adding an attraction force toward exponentially moving averaged (EMA) parameters helps the optimizer squeeze into sharp valleys without drastically reducing step size. FOCUS computes θ̂_t (bias-corrected EMA of parameters) and adds an update term −η_t·γ·sign(θ_t − θ̂_t) that pulls current parameters toward their historical center. This counters trajectory expansion from gradient noise while maintaining fixed-magnitude steps via sign(m_t). The core assumption is that the loss landscape has narrowing valley structure where parameters must stay compact along sharp dimensions to progress along flat dimensions.

### Mechanism 2: Signum-Based Fixed Step Size Avoids Adam's Noise-Induced Slowdown
When gradient noise is large relative to valley sharpness, Adam's adaptive normalization (m̂_t/√v̂_t) reduces effective step size excessively, slowing progress. Adam normalizes updates by running variance. High noise inflates v̂_t, shrinking the m̂_t/√v̂_t ratio even when the true gradient signal is consistent. Signum uses sign(m_t) with fixed η, preserving step magnitude regardless of noise. The core assumption is that gradient noise (from finite batch size) can dominate the second-moment estimate, creating a false signal of high curvature.

### Mechanism 3: Narrowing Valley Structure Motivates Combined Approach
LLM pre-training loss landscapes feature heterogeneous curvatures that narrow as training progresses, requiring optimizers that can both navigate sharp directions and maintain progress along flat ones. Sharp directions (valley walls) require small steps to avoid oscillation; flat directions (valley floor) benefit from larger steps. Adam handles sharpness via adaptive normalization but over-regularizes under noise. FOCUS uses attraction to constrain trajectory width in sharp directions without shrinking step magnitude. The core assumption is that the true minimum is far from reached during training; valleys narrow progressively, explaining why learning rate decay helps.

## Foundational Learning

- Concept: Exponential Moving Average (EMA) with bias correction
  - Why needed here: FOCUS uses EMA for both momentum (m_t) and parameter tracking (θ_t). Bias correction (dividing by 1−β^t) prevents early-step underestimation when initialized at zero.
  - Quick check question: Given β_2=0.99, what is the bias correction factor at step t=10? (Answer: 1/(1−0.99^10) ≈ 10.4)

- Concept: Sign-based gradient methods (Signum/SignSGD)
  - Why needed here: FOCUS inherits Signum's core update (−η·sign(m_t)). Understanding why sign works—direction matters more than magnitude in high-dimensional optimization—is essential.
  - Quick check question: Why might sign(m_t) outperform m_t/||m_t|| in practice? (Answer: Computational simplicity; both preserve direction, but sign avoids division and normalization overhead.)

- Concept: Regret bounds in online convex optimization
  - Why needed here: The paper proves O(√T) regret for FOCUS, matching Adam's convergence rate. Understanding regret (cumulative loss vs. best fixed point) contextualizes the theoretical contribution.
  - Quick check question: If R(T) ≤ C√T, what is the convergence rate of average regret R(T)/T? (Answer: O(1/√T))

## Architecture Onboarding

- Component map:
  Momentum EMA m_t -> Parameter EMA θ̄_t -> Bias correction θ̂_t -> Composite update θ_{t+1}

- Critical path:
  1. Gradient computation → momentum EMA update
  2. Parameter update → parameter EMA update (uses pre-weight-decay θ_t)
  3. Bias correction on parameter EMA
  4. Composite update: sign(momentum) + attraction term + weight decay

- Design tradeoffs:
  - Higher γ: Stronger attraction, better noise handling, but may slow progress if parameters "ahead" of their EMA (effective step reduced by factor 1−γ)
  - Higher β_2: Slower-moving parameter center, more stable attraction target, but may lag behind trajectory
  - Weight decay vs. attraction: Both constrain parameters; large weight decay can eliminate attraction's advantage

- Failure signatures:
  - Instability with learning rate that worked for Adam: Likely γ too high or β_2 mismatched to training timescale
  - Slower than Adam early in training: Expected (attraction reduces effective step by 1−γ); should accelerate later
  - No improvement over Signum: Check if weight decay is too large (dilutes attraction benefit) or landscape lacks sharp valleys

- First 3 experiments:
  1. Toy landscape replication: Implement 2D valley L(u,v) = (a/2)u²v² − cu with rotation, test Adam vs. Signum vs. FOCUS across noise/sharpness grid to reproduce Figure 2a phase boundaries.
  2. Ablation on γ: Train GPT-2 small with γ ∈ {0, 0.1, 0.2, 0.4} holding other hyperparameters fixed; expect γ=0 (Signum) unstable, γ=0.2–0.4 optimal, γ too high slows convergence.
  3. Batch size sensitivity: Compare FOCUS vs. Adam at batch sizes {128, 512, 2048}; hypothesis: FOCUS advantage diminishes at large batch (low noise) per toy model predictions.

## Open Questions the Paper Calls Out

- Question: What is the critical batch size required for LLMs to reduce gradient noise sufficiently so that Adam outperforms noise-robust optimizers like FOCUS?
  - Basis in paper: [explicit] The authors state on page 8, "Yet, how large the batch size we need for LLM to have a small gradient noise is an open question."
  - Why unresolved: The paper verifies the noise/optimizer interaction in toy models and MLPs but lacks empirical data locating this transition point for large-scale LLM training.
  - What evidence would resolve it: Training curves for LLMs across a wide range of batch sizes, identifying the crossover where Adam's performance surpasses FOCUS/Signum.

- Question: Is the "Zeno's paradox" phenomenon in narrowing valleys the fundamental driver of neural scaling laws, rather than model capacity limits?
  - Basis in paper: [explicit] Appendix B suggests "the scaling laws may be due to training dynamics rather than the limit of the model" as optimizers struggle to reach the true optimum.
  - Why unresolved: This is a theoretical implication of the "narrowing valley" picture derived from toy models, not yet empirically validated as the root cause of scaling laws in real networks.
  - What evidence would resolve it: Demonstration that modifying optimizer dynamics to overcome the paradox (e.g., via FOCUS) alters the power-law exponent or saturation point of scaling laws.

- Question: Does the mechanistic link between large data uncertainty and sharp/stochastic loss landscapes hold universally for language tasks?
  - Basis in paper: [explicit] Section 6 hypothesizes that "The mechanistic reasons behind LLM having sharp valley landscapes... may be the same–large uncertainty in data distribution."
  - Why unresolved: The paper relies on this as a supporting explanation but does not experimentally isolate data uncertainty as the causal variable for landscape geometry in its own experiments.
  - What evidence would resolve it: Experiments varying the inherent uncertainty of datasets (e.g., deterministic code vs. ambiguous text) and measuring the resulting changes in Hessian spectrum and gradient noise.

## Limitations

- The synthetic narrowing-valley experiments may oversimplify real LLM loss landscapes, which involve non-convexities, saddle points, and heterogeneous curvature not captured by the 2D analytic model.
- Results are demonstrated only on GPT-2 small (125M parameters), and the specific hyperparameter settings (γ=0.2, β₂=0.99) may not transfer optimally to larger models.
- The interaction between weight decay and self-attraction is noted but not thoroughly characterized, particularly for models with heavy weight decay where the attraction term may provide diminishing returns.

## Confidence

- High confidence: The theoretical regret bound (O(√T)) matching Adam's convergence rate on convex problems. The mathematical proof follows standard techniques in online convex optimization.
- Medium confidence: The mechanistic explanation of why Adam degrades under high noise relative to sharpness. The phase diagram in the toy model supports this, and it aligns with known issues about Adam's adaptive normalization.
- Medium confidence: The empirical speedup claim (~2x faster than Adam with lower final validation loss). The GPT-2 small results are reproducible and show clear improvement over both Adam and Signum.
- Low confidence: The claim that gradient noise is a primary limiting factor in LLM training. While the results suggest this, the paper doesn't systematically vary batch size or other noise sources to isolate this effect.

## Next Checks

1. **Batch size scaling experiment**: Systematically vary batch size (e.g., 64, 256, 1024, 4096) to test whether FOCUS's advantage diminishes as predicted by the noise-sharpness theory. This would validate the core mechanism and test generalization to different noise regimes.

2. **Architecture transfer test**: Apply FOCUS to a different architecture (e.g., BERT-base or LLaMA-7B) with comparable hyperparameter tuning. This would assess whether the γ=0.2 setting and overall approach generalize beyond GPT-2.

3. **Ablation on weight decay**: Run FOCUS with varying weight decay (e.g., 0.0, 0.1, 0.3, 0.5) while holding other parameters constant to characterize the interaction between weight decay and attraction, identifying when each mechanism is beneficial.