---
ver: rpa2
title: Reward Model Generalization for Compute-Aware Test-Time Reasoning
arxiv_id: '2505.18065'
source_url: https://arxiv.org/abs/2505.18065
tags:
- reasoning
- reward
- generalization
- accuracy
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a theoretical framework for understanding the
  generalization error of process reward models (PRMs) in compute-optimal test-time
  scaling. The authors derive PAC-Bayes bounds showing that PRM generalization error
  directly affects answer accuracy and required compute budget.
---

# Reward Model Generalization for Compute-Aware Test-Time Reasoning

## Quick Facts
- arXiv ID: 2505.18065
- Source URL: https://arxiv.org/abs/2505.18065
- Reference count: 40
- Presents theoretical framework linking PRM generalization to compute-optimal test-time scaling

## Executive Summary
This paper introduces a theoretical framework for understanding how process reward model (PRM) generalization affects test-time reasoning performance and compute efficiency. The authors derive PAC-Bayes bounds showing that PRM generalization error directly impacts answer accuracy and required compute budget. Based on this analysis, they propose Compute-Aware Tree Search (CATS), an actor-critic framework that dynamically allocates compute by adjusting sampling parameters based on reward distributions and model sparsity. Experiments on mathematical reasoning benchmarks demonstrate consistent improvements over existing external test-time scaling methods.

## Method Summary
The authors develop Compute-Aware Tree Search (CATS), an actor-critic framework that dynamically allocates compute during test-time reasoning. The method adjusts sampling parameters based on reward distribution characteristics and model sparsity, using PAC-Bayes bounds to guide compute allocation decisions. CATS treats the reward model as a critic that evaluates partial solutions while the policy model acts as the actor generating candidate solutions. The framework specifically targets compute-optimal scaling by balancing exploration depth against the reliability of reward signals from the PRM.

## Key Results
- CATS outperforms existing external TTS methods on MATH-500 and AIME24 benchmarks
- Performance gains observed across multiple policy models (Llama and Qwen variants)
- Consistent improvements validated theoretical predictions about PRM generalization affecting compute efficiency
- Dynamic compute allocation based on reward distributions improves accuracy while reducing total compute

## Why This Works (Mechanism)
The framework works by leveraging PAC-Bayes generalization bounds to establish a direct relationship between PRM generalization error and both answer accuracy and required compute budget. By treating the reward model as a critic in an actor-critic framework, CATS can dynamically adjust sampling parameters based on the reliability of reward signals. The compute-aware sampling adapts exploration depth and breadth according to the PRM's confidence in evaluating partial solutions, effectively allocating more compute where the reward model is more reliable and less where it's uncertain.

## Foundational Learning
- PAC-Bayes bounds: Why needed - provide theoretical guarantees on generalization error; Quick check - verify bound applicability to specific reward distributions
- Process reward models (PRMs): Why needed - evaluate reasoning quality at intermediate steps; Quick check - test PRM accuracy on held-out reasoning traces
- Actor-critic frameworks: Why needed - separate policy generation from reward evaluation; Quick check - ensure stable training dynamics between actor and critic
- Compute-optimal scaling: Why needed - balance accuracy gains against computational cost; Quick check - measure Pareto frontier of accuracy vs. compute
- Tree search algorithms: Why needed - systematic exploration of reasoning paths; Quick check - verify search completeness under given constraints

## Architecture Onboarding
**Component map:** Policy model -> Action generator -> Search tree -> PRM evaluator -> Compute allocator -> Search controller

**Critical path:** The critical execution path flows from the policy model generating candidate actions, through the search tree expansion, evaluation by the PRM, and back to the compute allocator which adjusts parameters for the next iteration.

**Design tradeoffs:** The framework trades off between search depth (potentially higher accuracy) and the reliability of reward signals (computational efficiency). Shallower searches with more reliable PRMs may outperform deeper searches with noisier reward signals.

**Failure signatures:** Common failure modes include: PRM overfitting leading to unreliable reward signals, excessive compute allocation to unpromising branches, and premature search termination before finding optimal solutions.

**First experiments:** 1) Verify PAC-Bayes bounds hold for your specific PRM, 2) Test compute allocation sensitivity to reward distribution parameters, 3) Compare CATS against uniform sampling baselines on simple reasoning tasks.

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical analysis assumes specific reward distribution properties that may not hold for all PRMs
- Empirical validation limited to mathematical reasoning benchmarks, uncertain generalizability to other domains
- Heavy-tailed reward distributions may violate PAC-Bayes assumptions, potentially leading to overly optimistic predictions

## Confidence
**High confidence** for mathematical domains where reward distribution assumptions hold reasonably well, evidenced by consistent improvements across multiple PRM and policy model combinations.

**Medium confidence** for theoretical claims about compute-aware search being optimal under certain conditions, as proof sketches rely on idealized assumptions about reward model accuracy and tree structure.

**Medium confidence** that PRM generalization directly translates to answer accuracy improvements, given downstream task performance depends on additional factors like search depth and branching strategy.

## Next Checks
1. Test CATS on non-mathematical reasoning tasks (e.g., code generation, scientific reasoning) to evaluate domain generalization of both theoretical framework and empirical performance gains.

2. Conduct ablation studies systematically varying reward distribution properties (e.g., heavy-tailed vs. normal) to quantify when PAC-Bayes bounds become unreliable predictors of compute efficiency.

3. Implement controlled experiments comparing CATS against baseline methods using identical PRMs but varying degrees of sparsity and depth limits to isolate contribution of compute-aware sampling from other factors.