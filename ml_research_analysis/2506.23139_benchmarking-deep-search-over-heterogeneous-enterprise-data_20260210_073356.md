---
ver: rpa2
title: Benchmarking Deep Search over Heterogeneous Enterprise Data
arxiv_id: '2506.23139'
source_url: https://arxiv.org/abs/2506.23139
tags:
- product
- search
- employee
- name
- slack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: HERB is a new benchmark for evaluating deep search over heterogeneous
  enterprise data, addressing the limitations of existing RAG benchmarks that rely
  on artificial, shallow multi-hop questions. It uses a query-first synthetic data
  generation pipeline to simulate realistic software development workflows, producing
  interconnected content across structured and unstructured sources such as Slack
  messages, meeting transcripts, GitHub PRs, and documents.
---

# Benchmarking Deep Search over Heterogeneous Enterprise Data

## Quick Facts
- arXiv ID: 2506.23139
- Source URL: https://arxiv.org/abs/2506.23139
- Reference count: 32
- Primary result: HERB benchmark evaluates deep search over heterogeneous enterprise data, achieving only 32.96% average performance for advanced agentic RAG systems

## Executive Summary
HERB introduces a new benchmark for evaluating deep search capabilities over heterogeneous enterprise data, addressing limitations in existing RAG benchmarks that rely on artificial multi-hop questions. The benchmark uses a query-first synthetic data generation pipeline to simulate realistic software development workflows, creating interconnected content across structured and unstructured sources including Slack messages, meeting transcripts, GitHub PRs, and documents. With 815 answerable and 699 unanswerable queries supported by 39,190 enterprise artifacts, HERB provides a comprehensive evaluation framework that reveals retrieval as the primary bottleneck limiting system effectiveness, even for advanced agentic RAG approaches.

## Method Summary
The HERB benchmark employs a query-first synthetic data generation pipeline designed to simulate realistic software development workflows. This approach creates interconnected content across heterogeneous enterprise data sources including Slack messages, meeting transcripts, GitHub PRs, and documents. The pipeline generates both answerable and unanswerable queries to comprehensively evaluate system capabilities. The benchmark contains 815 answerable and 699 unanswerable queries supported by 39,190 enterprise artifacts, providing a robust evaluation framework for deep search systems operating over enterprise data environments.

## Key Results
- HERB contains 815 answerable and 699 unanswerable queries across heterogeneous enterprise data
- Advanced agentic RAG systems achieve only 32.96% average performance on HERB
- Retrieval identified as the main bottleneck limiting effectiveness of deep search systems

## Why This Works (Mechanism)
HERB's effectiveness stems from its realistic simulation of enterprise data environments through synthetic data generation that captures the complexity of actual software development workflows. By creating interconnected content across multiple data sources and including both answerable and unanswerable queries, the benchmark forces systems to demonstrate true deep search capabilities rather than superficial pattern matching. The heterogeneous nature of the data sources (Slack, meetings, GitHub, documents) requires systems to integrate information across different formats and contexts, revealing limitations in current retrieval and reasoning approaches.

## Foundational Learning

### Synthetic Data Generation
- Why needed: To create realistic, complex multi-hop queries that reflect actual enterprise workflows rather than artificial questions
- Quick check: Verify generated queries maintain logical consistency and realistic complexity across different enterprise contexts

### Heterogeneous Data Integration
- Why needed: Enterprise environments contain diverse data types requiring unified search capabilities across structured and unstructured sources
- Quick check: Confirm retrieval performance across different data source types shows consistent behavior

### Deep Search vs Shallow Retrieval
- Why needed: To distinguish between simple keyword matching and true multi-hop reasoning across interconnected data
- Quick check: Measure performance degradation on increasingly complex multi-hop queries versus single-hop queries

## Architecture Onboarding

### Component Map
Query Generator -> Synthetic Data Pipeline -> Enterprise Artifacts Store -> Retrieval Engine -> Reasoning Component -> Answer Generator

### Critical Path
Query generation → Artifact creation → Retrieval execution → Multi-hop reasoning → Answer synthesis

### Design Tradeoffs
- Synthetic vs real data: Synthetic provides controlled complexity but may miss real-world edge cases
- Answerable vs unanswerable balance: Ensures systems can identify knowledge gaps, not just retrieve existing information
- Data source diversity: Broader coverage improves realism but increases system complexity requirements

### Failure Signatures
- High unanswerable accuracy but low answerable accuracy: Indicates overly conservative retrieval or reasoning
- Good single-hop performance but poor multi-hop: Reveals limitations in reasoning chain construction
- Source-type specific failures: Shows incomplete integration across heterogeneous data formats

### First Experiments
1. Evaluate retrieval-only performance across different data source types
2. Test single-hop versus multi-hop query performance to isolate reasoning bottlenecks
3. Compare synthetic query difficulty against human-generated enterprise questions

## Open Questions the Paper Calls Out
None

## Limitations
- Synthetic data generation may not fully capture complexity and variability of actual enterprise environments
- Performance results may not generalize across different enterprise domains beyond software development
- Reliance on synthetic rather than real enterprise workflows represents a notable limitation

## Confidence
- Retrieval as primary bottleneck: Medium confidence (based on system comparisons rather than controlled ablation studies)
- Benchmark difficulty assessment: Medium confidence (synthetic data realism not fully validated against real enterprise workflows)
- Performance metrics: High confidence (based on comprehensive evaluation across 1,514 queries and 39,190 artifacts)

## Next Checks
1. Test HERB across multiple enterprise domains beyond software development to assess generalizability
2. Conduct controlled experiments isolating retrieval from reasoning components to quantify their relative contributions to performance gaps
3. Compare HERB query difficulty and effectiveness metrics against human-generated multi-hop questions in enterprise contexts to validate synthetic pipeline realism