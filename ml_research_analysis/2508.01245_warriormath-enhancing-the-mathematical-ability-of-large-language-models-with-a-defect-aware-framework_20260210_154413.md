---
ver: rpa2
title: 'WarriorMath: Enhancing the Mathematical Ability of Large Language Models with
  a Defect-aware Framework'
arxiv_id: '2508.01245'
source_url: https://arxiv.org/abs/2508.01245
tags:
- data
- mathematical
- answer
- training
- problems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving large language
  models (LLMs) mathematical reasoning capabilities by generating high-quality, defect-aware
  training data and introducing a progressive training framework. The core idea is
  to use multiple expert LLMs in a collaborative process to generate, critique, and
  refine math problems that specifically target the weaknesses of a base model, rather
  than simply increasing difficulty.
---

# WarriorMath: Enhancing the Mathematical Ability of Large Language Models with a Defect-aware Framework

## Quick Facts
- arXiv ID: 2508.01245
- Source URL: https://arxiv.org/abs/2508.01245
- Reference count: 39
- Key outcome: Achieves state-of-the-art performance among open-source models on mathematical benchmarks with 12.57% average improvement

## Executive Summary
This paper introduces WarriorMath, a framework that enhances LLM mathematical reasoning by generating defect-aware training data and using progressive training. The approach uses an "exam committee" of expert LLMs to generate, critique, and refine math problems targeting the base model's weaknesses, rather than simply increasing difficulty. Through iterative alignment on failure cases, WarriorMath demonstrates state-of-the-art performance on six mathematical benchmarks among open-source models.

## Method Summary
WarriorMath employs a multi-stage process: an exam committee generates candidate math problems, which are filtered to retain only those the base model fails to solve. These defect-targeted problems are iteratively improved through expert feedback. An Elo-based voting system selects gold answers from multiple expert responses. Progressive training then fine-tunes the model first via supervised learning on expert answers, followed by iterative preference optimization on the model's remaining failure cases, progressively closing capability gaps.

## Key Results
- Achieves state-of-the-art performance among open-source models on six mathematical benchmarks
- Average improvement of 12.57% over strong baselines
- AIME'24 score improves from 36.7 (SFT) to 48.3 (after 3 iterative alignment iterations)
- Passes@1: AIME (48.3), MATH-500 (78.2), AMC (80.1), and other benchmark results

## Why This Works (Mechanism)

### Mechanism 1: Defect-Targeted Problem Synthesis
Filtering generated problems to retain only those the base model fails to solve produces higher training signal density than difficulty-based augmentation alone. The exam committee generates candidates; the base model attempts N=16 rollouts per problem, and only failure cases are retained. KCenterGreedy sampling ensures semantic diversity. This exposes "inherent defects" (systematic failure modes) rather than random errors.

### Mechanism 2: Multi-Expert Answer Ranking via Elo + Voting
Combining Elo ratings with local voting produces more reliable answer selection than either method alone, reducing judge bias. Each committee member provides answers; remaining members vote pairwise. Final score combines Elo-derived global ranking with local vote percentage, selecting the highest-scoring answer as the gold reference.

### Mechanism 3: Iterative Alignment on Residual Failures
A two-stage training pipeline (SFT → iterative DPO+NLL on remaining failures) progressively closes capability gaps without catastrophic forgetting. Stage 1 performs SFT on expert answers. Stage 2 samples N=32 responses per problem, identifies incorrect responses, and trains preference pairs using $L_{total} = L_{DPO} + \alpha L_{NLL}$. This repeats for multiple iterations.

## Foundational Learning

- **Concept: Direct Preference Optimization (DPO)**
  - **Why needed here:** Iterative alignment stage uses DPO to push model toward expert reasoning and away from incorrect attempts
  - **Quick check question:** Can you explain why DPO avoids explicitly training a reward model compared to RLHF?

- **Concept: Elo Rating Systems**
  - **Why needed here:** Answer selection uses Elo to track global model competence across pairwise battles, smoothing out noisy local vote outcomes
  - **Quick check question:** How does the K-factor in Elo affect rating sensitivity to individual battle outcomes?

- **Concept: Coreset/Active Learning Selection (KCenterGreedy)**
  - **Why needed here:** After defect filtering, KCenterGreedy ensures retained problem set covers embedding space rather than clustering around few failure types
  - **Quick check question:** What embedding space property does KCenterGreedy optimize for when selecting representative samples?

## Architecture Onboarding

- **Component map:** Examiner (generates problems) -> Defect Assessor (filters failures) -> Committee (answers problems) -> Answer Ranker (Elo+voting) -> Training Loop (SFT → iterative DPO+NLL)

- **Critical path:**
  1. Examiner generates candidate problems with adversarial prompt
  2. Base model attempts solutions; retain only failures
  3. Committee members answer retained problems; judges vote; Elo selects gold
  4. SFT on gold problem-answer pairs
  5. Sample from SFT model; identify failures; construct preference pairs; run DPO+NLL
  6. Repeat step 5 for 2-3 iterations

- **Design tradeoffs:**
  - Committee size vs. cost: 5 experts used; more members increase diversity but linearly increase inference and voting costs
  - Rollout count (N=16/32): Higher N reduces false negatives in failure detection but increases compute
  - α in loss: Tuned to 1.0 for NLL weight; higher α prioritizes fluency but may weaken preference signal

- **Failure signatures:**
  - Low AIME gains after iteration 1: Suggests gold answers may contain errors or failure cases are not systematic defects
  - High variance across seeds: May indicate unstable training or data quality issues
  - Degraded performance on easy benchmarks: Catastrophic forgetting from over-alignment on hard failures

- **First 3 experiments:**
  1. Defect filtering ablation: Train with all generated problems vs. failure-only; compare Pass@1 on AIME/MATH-500
  2. Committee size scaling: Test with 1, 2, 3, 5 committee members; measure compute vs. accuracy tradeoff
  3. Iteration count threshold: Run iterative alignment beyond 3 iterations; monitor for plateau or degradation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the multi-agent collaboration mechanism be optimized for scalability as the number of expert models increases?
- Basis in paper: [explicit] The authors state in the Limitations section that "as the number of expert models increases, the evaluation process becomes increasingly time-consuming."
- Why unresolved: The current "exam committee" relies on computationally intensive pairwise battles and Elo rating updates, which may become prohibitively slow with larger committees
- What evidence would resolve it: A modified framework that maintains data quality while significantly reducing inference latency or communication rounds per synthesis cycle

### Open Question 2
- Question: Can the defect-aware synthesis strategy effectively generalize to domains outside of mathematics?
- Basis in paper: [inferred] The paper validates the method exclusively on mathematical benchmarks (AIME, MATH, etc.), relying on the verifiable nature of math solutions for filtering
- Why unresolved: It is unclear if the "failure case" identification and Elo-based voting are as effective for subjective tasks or domains like code generation where "correctness" is more multifaceted
- What evidence would resolve it: Experimental results applying the WarriorMath pipeline to non-math benchmarks (e.g., coding or logical reasoning) showing comparable performance gains

### Open Question 3
- Question: Does reliance on an "Exam Committee" of specific LLMs impose a performance ceiling on the student model?
- Basis in paper: [inferred] The method relies on a fixed set of open-source models (e.g., Qwen, DeepSeek) to generate questions and validate "gold" answers
- Why unresolved: If the committee members share a specific systematic blind spot or lack knowledge of a niche concept, the student model cannot learn that concept, potentially capping its ability
- What evidence would resolve it: An analysis comparing the student model's errors against the collective errors of the committee to identify inherited limitations

## Limitations

- Reliance on expert committee models to generate and validate data may propagate systematic biases or errors
- Substantial computational cost requiring inference from multiple expert models for each generated problem
- Ablation study shows Elo+voting outperforms either method alone, but relative contributions are not quantified

## Confidence

- **High confidence:** The defect-aware filtering mechanism and progressive training framework are technically sound and well-implemented
- **Medium confidence:** The claim that WarriorMath achieves state-of-the-art performance among open-source models is supported by reported numbers
- **Medium confidence:** The mechanism by which defect-targeted training produces generalizable gains relies on the assumption that filtered failures represent systematic defects rather than random errors
- **Low confidence:** The effectiveness of the Elo rating system for LLM answer selection in this specific context is assumed rather than validated against ground truth correctness

## Next Checks

1. **Committee reliability audit:** Select 100 problems where committee members disagree, have human experts annotate correct answers, and measure accuracy rate of each committee member to quantify potential systematic biases

2. **Defect vs. difficulty ablation:** Generate two parallel training datasets - one filtered by base model failures (defect-aware) and one by problem difficulty (expert-only hard problems). Train two identical models and compare performance to isolate the specific contribution of defect-targeting versus difficulty-targeting

3. **Failure type analysis:** Categorize base model failures into systematic defect types (misinterpretation, calculation errors, logical gaps) versus random errors. Measure whether iterative alignment shows differential improvement rates across these categories to validate the assumption that the method addresses systematic rather than random failures