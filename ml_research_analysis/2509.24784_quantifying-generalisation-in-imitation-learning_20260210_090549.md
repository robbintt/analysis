---
ver: rpa2
title: Quantifying Generalisation in Imitation Learning
arxiv_id: '2509.24784'
source_url: https://arxiv.org/abs/2509.24784
tags:
- labyrinth
- learning
- imitation
- agent
- environment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Labyrinth, a new benchmarking environment
  designed to evaluate generalisation in imitation learning. Unlike existing benchmarks,
  Labyrinth offers precise control over environment structure, start and goal positions,
  and task complexity, enabling researchers to create verifiably distinct training,
  validation, and test sets.
---

# Quantifying Generalisation in Imitation Learning

## Quick Facts
- **arXiv ID:** 2509.24784
- **Source URL:** https://arxiv.org/abs/2509.24784
- **Reference count:** 40
- **Primary result:** Introduces Labyrinth, a benchmarking environment for evaluating generalization in imitation learning

## Executive Summary
This paper introduces Labyrinth, a new benchmarking environment designed to evaluate generalisation in imitation learning. Unlike existing benchmarks, Labyrinth offers precise control over environment structure, start and goal positions, and task complexity, enabling researchers to create verifiably distinct training, validation, and test sets. It provides a discrete, fully observable state space with known optimal actions, supporting interpretability and fine-grained evaluation. The environment includes variants like partial observability, key-and-door tasks, and ice-floor hazards to test different generalisation aspects. Experiments show that common imitation learning methods struggle to generalise in Labyrinth, especially when faced with unseen structures or goal configurations, highlighting the need for more robust learning approaches. Labyrinth thus advances the evaluation of generalisation in imitation learning and serves as a valuable tool for developing more capable agents.

## Method Summary
Labyrinth is a grid-based navigation environment where agents must reach a goal position while avoiding walls. The environment is controlled via a simple text-based configuration format that defines wall structures, start and goal positions, and special tiles. Expert trajectories are generated using Johnson's algorithm to find optimal paths. The benchmark provides controlled splits for training, validation, and testing with verifiable separation between sets. The paper evaluates six imitation learning methods (BC, GAIL, SQIL, DAgger, MAHALO, BCO) across multiple generalization scenarios including held-out structures, start positions, and goal positions. The primary metric is Success Ratio, measuring the percentage of episodes where the agent reaches the goal.

## Key Results
- Standard imitation learning methods achieve near-perfect performance on training data but fail to generalize to unseen labyrinth structures
- ResNet-18 encoders improve generalization performance to 53% test success but still fall far short of optimal navigation
- All evaluated methods achieve 0% success when both start and goal positions are novel and unseen during training
- The environment successfully reveals memorization patterns where agents learn to go "up" and "right" rather than understanding navigation

## Why This Works (Mechanism)

### Mechanism 1: Structural Disruption of Behaviour-Seeking
Standard imitation learning agents often succeed via "behaviour-seeking" (matching states to the training distribution) rather than task reasoning. Labyrinth disrupts memorization by altering the transition function (wall structure) or start/goal distributions between training and test splits. This forces the agent to encounter "out-of-distribution" states where the training-set actions are invalid or suboptimal. If the agent has not learned the underlying navigation logic, it fails. The core assumption is that success in standard benchmarks is partly inflated by high similarity between training and evaluation initial states.

### Mechanism 2: Encoder Quality via Supervised Loss
Pure supervised IL methods (e.g., Behavioral Cloning) learn more robust state encodings than Inverse Reinforcement Learning methods in this visual domain. Direct supervised loss forces the encoder to map raw image pixels to actions, explicitly learning wall/structure features. In contrast, IRL methods optimize indirect rewards or adversarial losses, which may obscure the visual features necessary for generalization. However, even robust encodings fail if the policy head relies on spurious correlations. The visual complexity of the labyrinth walls is the primary bottleneck for feature extraction.

### Mechanism 3: Distributional Divergence Quantification
Generalization failure is predictable by quantifying the divergence in action distributions between training and test splits. In "biased" settings (start/goal fixed), action distributions remain similar (mostly "Up"/"Right"), making the task deceptively easy. In "unbiased" settings (random start/goal), the action distribution shifts toward uniform. An agent relying on statistical priors of the training actions will fail when that distribution shifts. The difficulty of a generalization task can be decoupled from the environment dynamics and analyzed purely via state-action visitation frequency.

## Foundational Learning

- **Concept: Behavioural Cloning vs. Inverse RL**
  - **Why needed here:** The paper benchmarks BC (supervised) against IRL methods (GAIL/SQIL). Understanding that BC learns a direct mapping $s \to a$ while IRL learns a reward function $r(s,a)$ is critical to interpreting why BC had superior encodings but still failed to reason.
  - **Quick check question:** Does the agent learn *what to do* (BC) or *why to do it* (IRL)?

- **Concept: Covariate Shift**
  - **Why needed here:** This is the fundamental failure mode of IL. As the agent makes small errors, it drifts to states never seen in the expert data. Labyrinth exaggerates this by forcing the agent into structurally novel states immediately.
  - **Quick check question:** If the agent encounters a wall configuration not in the training set, can it recover, or does the error compound?

- **Concept: Graph Search (BFS/Johnson's Algorithm)**
  - **Why needed here:** Labyrinth is formally a graph. The environment provides optimal solutions via graph search. To understand *why* agents fail, one must understand that a perfect solution is computationally trivial (BFS), yet neural networks struggle to approximate this discrete logic.
  - **Quick check question:** Can the agent explain *why* a path is optimal, or is it purely pattern matching?

## Architecture Onboarding

- **Component map:** Image observations (64×64) -> CNN/ResNet-18 encoder -> Fully Connected layers -> Action head (Discrete)
- **Critical path:**
  1. **Data Gen:** Run `labyrinth.generate` to create strict train/eval/test splits (ensuring no structural overlap)
  2. **Encoder Train:** Train a CNN to map image states to discrete actions (BC)
  3. **Evaluation:** Test on the "unbiased" split to check if the agent reasons or memorizes

- **Design tradeoffs:**
  - **Image vs. Vector:** The paper uses images to force the agent to learn perception, whereas vectors (x,y coordinates) make the task trivially easy
  - **Discrete vs. Continuous:** The current architecture is limited to discrete actions, excluding continuous IL methods (e.g., MAHALO)

- **Failure signatures:**
  - **The "Memorizer":** High training success (>90%), near-zero test success. The model fails to act when the wall structure deviates from the training set
  - **The "Biased Faker":** High success in biased splits (fixed start/goal), zero success in unbiased splits. The model learned "go up/right" rather than "navigate to goal"

- **First 3 experiments:**
  1. **Sanity Check (3×3):** Train BC on 3×3 grids. If success < 80% on train split, the encoder is broken
  2. **Generalization Stress Test:** Train on biased 5×5, test on unbiased 5×5. Expect >50% drop in success rate to confirm the generalization gap
  3. **Encoder Ablation:** Swap the default DQN encoder for a ResNet-18. Check if test performance improves (indicating perception was the bottleneck) or stays flat (indicating the policy logic is the bottleneck)

## Open Questions the Paper Calls Out

- **Open Question 1:** Can goal-conditioned policy architectures achieve non-zero success on the simultaneous s₀ and g generalization split where all current methods fail?
  - **Basis in paper:** Table 7 shows all six methods achieve 0% success on the s₀ and g split in 5×5 labyrinths. The authors state this "clearly illustrates that none of the evaluated methods... can act meaningfully when both the initial and goal states are novel."
  - **Why unresolved:** Current methods cannot handle simultaneous novelty in both starting position and goal location, revealing fundamental limitations in goal representation and reasoning.
  - **What evidence would resolve it:** An architecture achieving >20% success on the s₀ and g split would demonstrate goal-conditioned generalization is achievable.

- **Open Question 2:** What learning approaches enable agents to acquire task-level navigation competence rather than memorizing trajectory patterns?
  - **Basis in paper:** ResNet-18 encoders and 10,000-epoch training improve test performance to 53% but "does not guarantee" generalization. The authors conclude agents "do not learn how to perform the underlying task of navigating the structure."
  - **Why unresolved:** The gap between training (75%) and test (5%) success persists despite capacity increases, suggesting memorization dominates over task understanding.
  - **What evidence would resolve it:** An agent maintaining >50% success across all generalization splits with held-out structures would demonstrate task learning.

- **Open Question 3:** How can Labyrinth support continuous action spaces while preserving verifiable optimal solutions and reproducible structural control?
  - **Basis in paper:** Section 5 states: "Labyrinth only allows for discrete actions... Ideally, we would like to provide the option of performing continuous actions while keeping all the features Labyrinth provides."
  - **Why unresolved:** Continuous actions prevent exact computation of optimal actions, which is central to Labyrinth's fine-grained evaluation capability.
  - **What evidence would resolve it:** A continuous-action variant maintaining analytically or numerically verifiable near-optimal solutions.

## Limitations

- The benchmark only supports discrete action spaces, limiting evaluation of continuous control methods
- External validation of the distributional divergence mechanism is limited, with minimal related work found in the corpus
- The paper focuses primarily on encoder architecture ablations, leaving open questions about whether policy architectures or training procedures could yield better generalization
- The research area appears relatively isolated, suggesting limited cross-domain applicability or community adoption

## Confidence

- **High confidence**: The core observation that standard IL benchmarks may overestimate agent capabilities due to structural similarity between train/test splits
- **Medium confidence**: The claim that pure supervised IL methods learn better encodings than IRL methods in this domain
- **Low confidence**: The distributional divergence quantification as a predictive metric for generalization failure

## Next Checks

1. **Cross-domain validation**: Test whether the distributional divergence metric correlates with generalization performance in other IL benchmarks by measuring action distribution shifts between training and evaluation splits.

2. **Architecture ablation**: Compare the performance of graph-based policy architectures against standard CNNs in Labyrinth to determine if the failure is due to perception limitations or policy reasoning limitations.

3. **Continuous control extension**: Adapt Labyrinth to support continuous action spaces and evaluate whether the structural disruption mechanism still reveals generalization gaps in methods like MAHALO or DAgger.