---
ver: rpa2
title: Diversity Optimization for Travelling Salesman Problem via Deep Reinforcement
  Learning
arxiv_id: '2501.00884'
source_url: https://arxiv.org/abs/2501.00884
tags:
- diversity
- solution
- msqi
- solutions
- instances
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RF-MA3S, the first deep reinforcement learning
  method for diversity optimization in the Travelling Salesman Problem (TSP). The
  method combines a Relativization Filter (RF) to enhance encoder robustness against
  affine transformations, a multi-decoder architecture with Multi-Attentive Adaptive
  Active Search (MA3S) to balance optimality and diversity, and adaptive switching
  between shared and respective baselines during inference.
---

# Diversity Optimization for Travelling Salesman Problem via Deep Reinforcement Learning

## Quick Facts
- arXiv ID: 2501.00884
- Source URL: https://arxiv.org/abs/2501.00884
- Reference count: 40
- Key outcome: RF-MA3S achieves up to 15× faster inference than traditional heuristics while delivering superior MSQI and DI scores on TSP and CVRP benchmarks

## Executive Summary
This paper introduces RF-MA3S, the first deep reinforcement learning method for diversity optimization in the Travelling Salesman Problem (TSP). The method combines a Relativization Filter (RF) to enhance encoder robustness against affine transformations, a multi-decoder architecture with Multi-Attentive Adaptive Active Search (MA3S) to balance optimality and diversity, and adaptive switching between shared and respective baselines during inference. RF-MA3S significantly outperforms recent neural baselines on both TSP and CVRP benchmarks, achieving up to 15× faster inference than traditional heuristics while delivering superior MSQI and DI scores. The method demonstrates strong generalization across varying problem scales and effectively handles affine transformations, with ablation studies confirming the contribution of each key component.

## Method Summary
RF-MA3S is a deep reinforcement learning architecture designed for diversity optimization in TSP. It processes input coordinates through a Relativization Filter (RF) that converts absolute positions to relative representations, enhancing robustness to affine transformations. The model employs five parallel decoders with shared encoder embeddings, using Multi-Attentive Adaptive Active Search (MA3S) to balance exploration and exploitation. During inference, Adaptive Active Search dynamically switches between shared and respective baselines based on convergence metrics, allowing decoders to settle into distinct high-quality solution modes. The architecture is trained using REINFORCE with adaptive baseline switching to optimize both solution quality and diversity.

## Key Results
- RF-MA3S achieves 0% performance gap on scaled/rotated instances compared to ~30-80% gap for baselines without RF
- The method delivers superior MSQI and DI scores while maintaining up to 15× faster inference than traditional heuristics
- Ablation studies confirm significant contributions from RF (robustness), Multi-Decoder architecture (diversity), and Adaptive Baseline switching (optimality-diversity balance)
- Strong generalization demonstrated across TSP sizes (20-100 nodes) and CVRP benchmarks

## Why This Works (Mechanism)

### Mechanism 1: Coordinate Relativization for Affine Invariance
Converting absolute input coordinates to relative representations via the Relativization Filter (RF) improves the encoder's ability to generalize across geometrically identical instances. The RF applies a deterministic pipeline (reordering, zero-mean centering, polar coordinate conversion, and angle normalization) to input nodes, stripping away positional information (translation, rotation, scaling) and presenting the encoder with a canonical geometric structure. By reducing variance in the input data distribution, the model learns the underlying combinatorial structure rather than fitting to specific coordinate ranges.

### Mechanism 2: Adaptive Baseline Switching (Shared vs. Respective)
Dynamically switching the baseline during Active Search allows the model to decouple the conflicting objectives of high optimality (exploitation) and high diversity (exploration). The Adaptive Active Search (AAS) monitors convergence gradient ($f$). Initially, all decoders compare rewards against the best decoder's average (Shared Baseline), forcing convergence toward global optimum. Once convergence slows ($f < \alpha$), the baseline switches to each decoder's own average reward (Respective Baseline), lowering pressure for global convergence and allowing distinct local optima without penalty.

### Mechanism 3: Parallel Multi-Decoder Divergence
Utilizing multiple decoders with random initializations increases the probability of discovering distinct high-quality solution modes compared to single-decoder methods. The architecture expands single decoder to five parallel decoders sharing an encoder but with distinct parameters. During training and active search, they process the same embedding but produce different action probability distributions, explicitly creating "niches" in the solution space.

## Foundational Learning

- **REINFORCE Algorithm and Baselines**: The core training and Active Search rely on policy gradients. Understanding how a "baseline" reduces variance is critical to grasping why switching baselines changes behavior from "beating the best" (Shared) to "improving oneself" (Respective). *Quick check*: If you use the *best* solution found so far as a baseline, how does that affect the gradient signal for a sub-optimal decoder compared to using its *own* average reward?

- **Affine Transformations in Geometry**: The Relativization Filter (RF) is designed to be invariant to these. You must understand that translating, rotating, or scaling a TSP instance does not change the optimal tour length. *Quick check*: Why would a standard neural network (without RF) produce different probabilities for a set of nodes if you simply shifted all x-coordinates by +10?

- **Multi-Modal Optimization**: Unlike standard TSP solvers that seek one global minimum, MSTSP seeks multiple high-quality local optima. *Quick check*: In the context of the "Adaptive Baseline," why is finding the global optimum potentially harmful to the *diversity* goal if enforced for too long?

## Architecture Onboarding

- **Component map**: Input (N nodes) → Relativization Filter (RF) → Encoder (3-layer Attention) → Shared Embedding → 5 parallel Decoders (Attention-based, distinct parameters) → Inference Loop (Active Search with Adaptive Baseline)

- **Critical path**: Implementing the RF logic correctly (Zero-mean → Polar → Relativize → Cartesian) is the most brittle part; a small error in angle normalization destroys invariance. The AAS loop specifically the conditional `if f < alpha: switch = 1` controls the optimality/diversity trade-off.

- **Design tradeoffs**: Speed vs. Diversity: Active Search improves quality but increases inference time (Table 1 shows time increases from Seconds to Minutes). The α threshold controls this. Robustness vs. Information: RF removes absolute scale/position info. If a future variant of TSP relies on absolute coordinates, RF would discard valuable data.

- **Failure signatures**: Identical Solutions: If all 5 decoders output the same tour, the "Respective Baseline" phase failed or the encoder forced mode collapse. Unstable Training: If the "Shared Baseline" is too aggressive, the loss might oscillate wildly as all decoders race to the same optimum. Scale Sensitivity: If testing on dataset with coordinates in range [0, 10000] and performance drops, the RF's normalization likely failed.

- **First 3 experiments**:
  1. RF Validation: Train baseline POMO and RF-MA3S on random TSP20. Test on "Shifted" dataset (coordinates +1000) and "Scaled" dataset (×100). Verify RF-MA3S performance is stable while baseline drops.
  2. Baseline Ablation: Run inference with AAS fixed to "Shared Baseline" only vs. "Respective Baseline" only. Plot the MSQI and DI scores to replicate Figure 4 and confirm the trade-off.
  3. Alpha Sensitivity: Sweep α ∈ {0.0025, 0.005, 0.01} on validation set to find the "knee" in curve between Inference Time and MSQI quality (Table 4).

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Limited scope based on abstract and introduction prevents full verification of technical claims
- Cannot assess relative novelty compared to all unpublished work in neural combinatorial optimization
- Risk of mode collapse in shared encoder requires careful monitoring during training

## Confidence
- **High Confidence**: Coordinate relativization (RF) for affine invariance - well-supported by mathematical pipeline and empirical evidence of 0% performance gap on scaled/rotated instances
- **Medium Confidence**: Adaptive baseline switching mechanism - logically coherent with strong supporting evidence from MSQI scores, but exact dynamics uncertain without deeper inspection
- **Medium Confidence**: Parallel multi-decoder architecture's contribution to diversity - supported by ablation studies, but mode collapse risk requires careful monitoring

## Next Checks
1. **RF Robustness Test**: Train RF-MA3S and standard POMO baseline on random TSP20 instances. Test both models on "Shifted" dataset (coordinates +1000) and "Scaled" dataset (×100). Verify RF-MA3S maintains stable performance while baseline shows significant degradation.

2. **Adaptive Baseline Dynamics**: Implement and run inference with AAS fixed to only "Shared Baseline" and only "Respective Baseline" modes. Plot MSQI and DI scores across multiple runs to visually confirm the trade-off between optimality and diversity, and identify optimal α threshold.

3. **Alpha Threshold Sensitivity**: Conduct hyperparameter sweep of α ∈ {0.0025, 0.005, 0.01} on held-out validation set. Measure both MSQI quality score and average inference time for each value. Plot these metrics to identify the "knee" in curve where trade-off between solution quality and computational cost is most favorable.