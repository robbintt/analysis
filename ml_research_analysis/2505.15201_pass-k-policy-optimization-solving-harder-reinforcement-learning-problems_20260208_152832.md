---
ver: rpa2
title: 'Pass@K Policy Optimization: Solving Harder Reinforcement Learning Problems'
arxiv_id: '2505.15201'
source_url: https://arxiv.org/abs/2505.15201
tags:
- eval
- pass
- kopt
- which
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the limitation of standard reinforcement learning
  (RL) methods that independently reward multiple samples per problem, which prioritizes
  pass@1 performance at the expense of sample diversity and collective utility. This
  restricts exploration and limits improvement on harder tasks.
---

# Pass@K Policy Optimization: Solving Harder Reinforcement Learning Problems

## Quick Facts
- arXiv ID: 2505.15201
- Source URL: https://arxiv.org/abs/2505.15201
- Reference count: 40
- Primary result: PKPO directly optimizes pass@k via joint reward transformation, enabling better exploration and solving harder RL problems where pass@1 optimization stalls

## Executive Summary
Pass@K Policy Optimization (PKPO) addresses a fundamental limitation in standard reinforcement learning where independently rewarding multiple samples prioritizes pass@1 performance at the expense of sample diversity and collective utility. The authors propose a transformation of rewards that directly optimizes pass@k—the probability of obtaining at least one correct sample among k independent attempts. This joint reward transformation enables exploration of solution regions that would be missed by pass@1 optimization alone, particularly for hard tasks where individual samples rarely succeed.

The method derives novel, low-variance unbiased estimators for pass@k and its gradient, both for binary and continuous rewards, enabling robust optimization for any k ≤ n. PKPO reduces to standard RL with jointly transformed rewards that are stable and efficient to compute. The approach allows annealing k during training to optimize both pass@1 and pass@k simultaneously, rather than trading them off. Experiments validate variance reduction on toy problems and demonstrate significant gains on real-world RL tasks including MATH, coding, and ARC-AGI-1, where PKPO achieves strong pass@1 performance alongside substantial pass@k improvements.

## Method Summary
PKPO transforms per-sample rewards into jointly-considered rewards that enable direct optimization of pass@k rather than pass@1. For each of n samples, the method computes an "effective reward" by averaging over all (n choose k) subsets of size k that include that sample, weighted by the maximum reward in each subset. This transforms a vector R^n → R^n where transformed rewards for higher-ranked samples receive systematically higher weights via binomial coefficients. The gradient estimator becomes a sum over samples weighted by these joint rewards, enabling exploration of diverse solution strategies.

The method includes an unbiased leave-one-out baseline (sloo_minus_one) for variance reduction, which constructs baselines by averaging over all appropriate subsets excluding each sample. This baseline uses smaller subsets (k-1) to retain unbiasedness while reducing variance. PKPO also implements k-annealing, starting with high k values for exploration and gradually reducing to k=1 for pass@1 optimization, achieving both goals without traditional tradeoffs.

## Key Results
- PKPO achieves strong pass@1 performance alongside significant pass@k gains on MATH, coding, and ARC-AGI-1 tasks
- The method enables solving more and harder problems where pass@1 optimization stalls
- Variance reduction via sloo_minus_one estimator shows orders of magnitude lower variance than naive approaches
- k-annealing achieves higher pass@k_eval for all k_eval > 1 with no sacrifice in pass@1

## Why This Works (Mechanism)

### Mechanism 1: Joint Reward Transformation via Combinatorial Averaging
Transforming per-sample rewards to jointly-considered rewards enables direct optimization of pass@k rather than pass@1, preserving diversity and improving exploration on hard tasks. For each of n samples, PKPO computes an "effective reward" by averaging over all (n choose k) subsets of size k that include that sample, weighted by the maximum reward in each subset. This transforms a vector R^n → R^n where transformed rewards for higher-ranked samples receive systematically higher weights via binomial coefficients μ_i = (i-1 choose k-1). The gradient estimator becomes ∇̂ = Σ s_i ∇_θ log p(x_i|θ) where s_i depends on rewards of all samples jointly. This works because the problem set contains tasks where pass@1 is near-zero but pass@k is achievable, and exploration diversity correlates with finding solutions to harder problems.

### Mechanism 2: Unbiased Leave-One-Out Baselining for Variance Reduction
The sloo_minus_one estimator provides low-variance, unbiased gradient estimates by constructing baselines that average over all appropriate subsets excluding each sample. Standard LOO baselining fails here because transformed rewards s_i already depend on all samples. PKPO defines s^(loo-1)_i = s_i - (k/(n(k-1))) b_i^(k-1) where b_i^(k-1) averages max-g@(k-1) estimates over subsets excluding sample i. This subtracts a baseline using smaller subsets (k-1) to retain unbiasedness while reducing variance. The computation exploits sorted-reward structure for O(k + n log n) efficiency. Variance in gradient estimation significantly slows convergence, and the combinatorial averaging over subsets provides meaningful signal beyond naive partitioning.

### Mechanism 3: k-Annealing for Joint pass@1 and pass@k Optimization
Annealing k from high values to k=1 during training achieves strong pass@1 while retaining pass@k gains, avoiding the traditional trade-off. Early training with k_opt > 1 encourages exploration by tolerating individual sample failures that still contribute to subset-level success. Later training with k_opt → 1 consolidates the single-sample policy. This works because high-k optimization discovers solution regions that low-k optimization then refines. Solutions discovered via diverse exploration can be consolidated into deterministic single-sample policies, and the optimization landscape permits this transfer.

## Foundational Learning

- **Concept: Policy Gradient / REINFORCE**
  - Why needed here: PKPO is a drop-in replacement for reward computation in any policy gradient method. Understanding ∇_θ E[r(x)] = E[r(x) ∇_θ log p(x|θ)] is essential to see why reward transformation suffices.
  - Quick check question: Given a batch of log-probabilities [-2.1, -1.8, -2.5] and transformed rewards [0.3, 0.5, 0.1], compute the policy gradient estimate.

- **Concept: U-Statistics and Unbiased Estimation**
  - Why needed here: The pass@k estimator is a U-statistic (average over all k-subsets of a symmetric kernel). Understanding why averaging retains unbiasedness but reduces variance (via Hoeffding's theory) explains the method's statistical properties.
  - Quick check question: Why does the variance of ρ(n, c, k) decrease as ~1/n per Corollary 3?

- **Concept: Leave-One-Out Baselining for Variance Reduction**
  - Why needed here: Standard REINFORCE has high variance; subtracting a baseline that doesn't depend on the current action retains unbiasedness. PKPO extends this to jointly-dependent rewards.
  - Quick check question: Why can't you simply subtract the batch mean from s_i when s_i already depends on all samples?

## Architecture Onboarding

- **Component map:** `sloo_minus_one(g, K)` -> `rho(g, K)` -> `sloo(g, K)` -> `s(g, K)`
- **Critical path:**
  1. Sample n completions per prompt from policy
  2. Compute raw rewards g(x_i) for each (binary pass/fail or continuous score)
  3. Apply `sloo_minus_one(g, k_opt)` to get transformed rewards s_i
  4. Compute policy gradient: loss = -Σ s_i · log_prob_i
  5. Backprop and update
- **Design tradeoffs:**
  - Higher k_opt: Better exploration, higher cumulative solve rate, but gradient variance increases as k → n (fewer subsets for baseline)
  - n vs k: Paper uses n=16, k_opt ∈ {1,2,4,8}. Larger n reduces variance but increases sampling cost
  - sloo vs sloo_minus_one: sloo_minus_one has lower variance (Figure 4) but requires k ≥ 2
- **Failure signatures:**
  - Training diverges: Check for NaN in binomial coefficient computation (numerical overflow for large n, k); use log-space computation
  - No improvement vs baseline: Verify raw rewards span a meaningful range; if all identical, transformation is degenerate
  - Entropy collapses to zero: k_opt may be too low for task difficulty; increase k or implement k-annealing
  - pass@1 degrades with high k: Expected initially; implement annealing schedule if final pass@1 matters
- **First 3 experiments:**
  1. Validation on toy problem: Implement the 1D Gaussian policy with g(x) = x² if 0 ≤ x ≤ 1 else 0 (Section 5.1). Verify gradient variance of sloo_minus_one matches Figure 4 before touching real models
  2. Ablation on small model: Train GEMMA-2-2B or similar small model on MATH with k_opt ∈ {1, 4, 8} and n=16. Confirm k_opt ≈ k_eval gives best pass@k_eval (replicate Figure 7 pattern)
  3. Annealing schedule test: On a held-out hard subset (e.g., ARC-AGI-1 easy split), compare fixed k_opt=1 vs k_opt=8 vs annealed k=8→1. Measure cumulative solve rate and final pass@{1,8,16}

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can Pass@K Policy Optimization be generalized to inference-time search algorithms that rely on dependent samples, such as tree search or chain-of-thought verification?
- Basis in paper: The authors state in "Conclusions and Outlook" that the work can be extended "to other inference-time search algorithms."
- Why unresolved: The current derivation relies on the assumption that samples are drawn i.i.d. to form the estimators ρ and s.
- What evidence would resolve it: A derivation of unbiased gradient estimators for search strategies where the probability of subsequent samples depends on previous branches.

### Open Question 2
- Question: What is the optimal schedule for annealing k during training to maximize both pass@1 and pass@k performance?
- Basis in paper: Section 5.2.3 demonstrates a "simple annealing procedure" (switching k at step 1500), but the paper does not explore if this is the most effective strategy.
- Why unresolved: The paper validates that annealing works but leaves the optimization of the schedule itself as an open hyperparameter.
- What evidence would resolve it: A comparative study of dynamic k schedules (e.g., linear decay vs. performance-based triggers) on convergence speed and final metrics.

### Open Question 3
- Question: Can more sophisticated baseline techniques be developed to further reduce the variance of the gradient estimator, particularly in the k ≈ n regime?
- Basis in paper: The conclusion suggests extending the work to "more sophisticated baseline techniques" beyond the leave-one-out (LOO) methods analyzed.
- Why unresolved: The paper notes that variance increases as k approaches the batch size n, and current LOO baselines mitigate but may not minimize this.
- What evidence would resolve it: Empirical results showing lower variance gradients and improved learning stability using value-function baselines or control variates compared to the sloo_minus_one method.

## Limitations
- The core variance reduction benefit depends on sufficient problem diversity; on uniformly easy tasks (pass@1 already high), the joint reward transformation may provide minimal advantage
- Performance gains are concentrated on very hard tasks (ARC-AGI-1, challenging MATH problems) where standard pass@1 optimization stalls; benefits on moderate tasks are less clear
- The method requires n ≥ k for the estimator to be well-defined, and variance increases as k approaches n, creating a tradeoff between exploration benefits and gradient stability
- Annealing schedules (k_opt=8→1) are heuristic; optimal schedules may be task-dependent and are not theoretically derived

## Confidence
- High confidence: The mathematical derivation of unbiased estimators (ρ^(g), sloo_minus_one) and variance reduction claims (Figure 4 toy experiments)
- Medium confidence: The transferability of toy problem variance benefits to large-scale RL with LLMs; the necessity of k-annealing vs fixed k_opt schedules
- Medium confidence: The interpretation that improved pass@k directly translates to better exploration of solution regions for hard problems (mechanism 1)

## Next Checks
1. **Ablation on easy vs hard task splits:** Run experiments on MATH with problems stratified by difficulty (pass@1 > 0.5 vs < 0.1). Measure whether PKPO's gains are concentrated on the hard subset where pass@1 stalls.
2. **k_opt vs k_eval matching study:** Systematically test k_opt ∈ {1,2,4,8} against k_eval ∈ {1,2,4,8,16} to verify the paper's claim that matching values gives optimal pass@k_eval, and determine if any k_opt provides Pareto-optimal performance across all k_eval.
3. **Annealing schedule sensitivity:** Compare fixed k_opt schedules (1, 4, 8) against various annealing schedules (8→1, 4→1, 2→1) to quantify the benefit of annealing and identify if simpler schedules suffice.