---
ver: rpa2
title: 'PRISM: Agentic Retrieval with LLMs for Multi-Hop Question Answering'
arxiv_id: '2510.14278'
source_url: https://arxiv.org/abs/2510.14278
tags:
- retrieval
- evidence
- recall
- question
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents PRISM, an agentic retrieval framework for\
  \ multi-hop question answering that explicitly balances precision and recall through\
  \ a structured loop of three LLM-based agents: a Question Analyzer that decomposes\
  \ complex queries into sub-questions, a Selector that filters candidates for precision,\
  \ and an Adder that supplements evidence for recall. The iterative Selector\u21D4\
  Adder cycle refines a compact, comprehensive evidence set that outperforms prior\
  \ baselines on HotpotQA (90.9% recall), 2WikiMultiHopQA (91.1%), MuSiQue (83.2%),\
  \ and MultiHopRAG (40.64% recall)."
---

# PRISM: Agentic Retrieval with LLMs for Multi-Hop Question Answering

## Quick Facts
- **arXiv ID:** 2510.14278
- **Source URL:** https://arxiv.org/abs/2510.14278
- **Reference count:** 18
- **Primary result:** PRISM achieves state-of-the-art retrieval and QA accuracy on multi-hop benchmarks via an explicit precision-recall balancing loop.

## Executive Summary
PRISM is an agentic retrieval framework designed to solve multi-hop question answering by decomposing complex queries into sub-questions and iteratively balancing precision and recall. The system uses three LLM-based agents—Question Analyzer, Selector, and Adder—in a structured loop that refines a compact, high-quality evidence set. Through explicit precision-recall control, PRISM surpasses prior methods on HotpotQA, 2WikiMultiHopQA, MuSiQue, and MultiHopRAG, translating retrieval gains into higher QA accuracy. The framework is robust across LLM backends and shows that explicit balancing is key to effective multi-hop reasoning.

## Method Summary
PRISM employs a three-agent loop: the Question Analyzer decomposes the query into sub-questions, the Selector filters candidates for precision, and the Adder supplements evidence for recall. This loop iterates up to N=3 times, merging and deduplicating the evidence set before passing it to an Answer Generator. The agents are prompted LLMs (e.g., GPT-4o, Gemini-2.0-Flash-Lite) in a zero-shot setting. The method relies on structured output formats for evidence (lists of `[title, sentence_index]`) and is tested on four multi-hop QA datasets. Retrieval quality directly feeds into QA accuracy, with the architecture designed to minimize noise while maintaining coverage.

## Key Results
- Outperforms strong baselines (IRCoT, SetR) on HotpotQA, 2WikiMultiHopQA, MuSiQue, and MultiHopRAG.
- Achieves up to 90.9% recall on HotpotQA and 91.1% on 2WikiMultiHopQA.
- Approaches oracle QA performance while reducing evidence noise and maintaining a compact set.

## Why This Works (Mechanism)
PRISM works by explicitly separating and balancing the precision and recall requirements of multi-hop retrieval. The Selector and Adder agents work iteratively to prune irrelevant passages and recover missing evidence, respectively. This design prevents over-retrieval (which introduces noise) and under-retrieval (which misses necessary reasoning steps), producing a compact but comprehensive evidence set that is easier for downstream reasoning. The approach is robust across different LLM backends and maintains high performance even when the initial candidate pool varies.

## Foundational Learning
- **Multi-hop QA retrieval:** Why needed—to gather evidence spanning multiple reasoning steps. Quick check—ensure your retriever can fetch passages that link across different topics or entities.
- **Precision-recall balance:** Why needed—to avoid both missing evidence (low recall) and including noise (low precision). Quick check—monitor evidence set size and QA accuracy as you tune the Selector/Adder.
- **Agentic LLM loops:** Why needed—to iteratively refine evidence rather than rely on a single retrieval pass. Quick check—verify that each agent’s output is correctly formatted and used in the next step.
- **Structured output parsing:** Why needed—to ensure the agent loop can proceed without errors. Quick check—log and validate all LLM outputs against expected schemas.
- **Zero-shot prompting:** Why needed—to avoid the cost and complexity of fine-tuning. Quick check—test prompt templates with multiple LLM backends for consistency.
- **Ablation studies:** Why needed—to identify the impact of each agent and loop iteration. Quick check—run ablations by removing or modifying agents and measuring retrieval/QA drops.

## Architecture Onboarding

**Component map:** Question Analyzer -> Selector <-> Adder -> Answer Generator (loop N=3)

**Critical path:** Initial retrieval -> Question Analyzer -> (Selector -> Adder) x 3 -> Answer Generator

**Design tradeoffs:** Explicit precision-recall balancing trades off some initial recall for cleaner, more useful evidence, which improves final QA accuracy.

**Failure signatures:** Parsing errors in agent outputs break the loop; over-aggressive pruning by Selector causes recall drops; Adder fails to recover missing evidence.

**3 first experiments:**
1. Run the full pipeline with BM25 and BGE-large-en-v1.5 retrievers, varying k=10, 20, 50, and compare retrieval/QA results.
2. Instrument the loop to log raw agent outputs and validate the `[title, sentence_index]` format at every step.
3. Vary the iteration count N=1, 2, 3, 4 and measure the effect on final evidence set size and QA accuracy.

## Open Questions the Paper Calls Out
None

## Limitations
- The initial retrieval mechanism (algorithm and k value) is underspecified, making exact reproduction difficult.
- No decoding parameters (temperature, top_p) are provided, introducing potential variance.
- Assumes LLMs can reliably output strictly formatted evidence lists; malformed outputs may silently degrade performance.
- Performance depends on high-quality query decompositions, with no analysis of robustness to poor sub-questions.

## Confidence
- **High confidence** in architectural design and relative retrieval results across datasets.
- **Medium confidence** in absolute metric values due to missing initial retrieval and decoding details.
- **Low confidence** in generalization beyond the four tested datasets and LLM backends.

## Next Checks
1. Test the pipeline with multiple standard retrievers (BM25, BGE-large-en-v1.5) and varying k values to assess sensitivity.
2. Instrument code to log and validate agent output format, adding parsing fallbacks if needed.
3. Experiment with different maximum iteration counts (N=1, 2, 3, 4) to confirm optimal loop depth.