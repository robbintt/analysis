---
ver: rpa2
title: Are Multimodal Embeddings Truly Beneficial for Recommendation? A Deep Dive
  into Whole vs. Individual Modalities
arxiv_id: '2508.07399'
source_url: https://arxiv.org/abs/2508.07399
tags:
- multimodal
- recommendation
- embeddings
- performance
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper conducts a large-scale empirical study to examine whether
  multimodal embeddings (text and image) truly enhance recommendation performance,
  addressing a critical research gap in the multimodal recommendation field. The authors
  employ a modality knockout strategy, replacing either visual, textual, or both embeddings
  with constant values or random noise, to isolate the contribution of each modality
  in 14 widely-used state-of-the-art models.
---

# Are Multimodal Embeddings Truly Beneficial for Recommendation? A Deep Dive into Whole vs. Individual Modalities

## Quick Facts
- arXiv ID: 2508.07399
- Source URL: https://arxiv.org/abs/2508.07399
- Reference count: 40
- Primary result: Multimodal embeddings improve recommendation, but text dominates; image modality is often underutilized except in sophisticated graph-based models

## Executive Summary
This large-scale empirical study investigates whether multimodal embeddings (text and image) truly enhance recommendation performance through a modality knockout strategy across 14 state-of-the-art models and three Amazon datasets. The authors systematically replace visual, textual, or both embeddings with constant values or random noise during training and inference to isolate each modality's contribution. Results reveal that while multimodal embeddings generally improve performance—especially in sophisticated graph-based fusion models—text modality alone achieves comparable performance in most cases, while image modality provides minimal benefits and is often underutilized. The study identifies architecture-dependent sensitivity to visual features and highlights the dominant role of text, offering practical guidance for future multimodal recommendation model development.

## Method Summary
The study employs a modality knockout strategy by replacing embeddings for visual, textual, or both modalities with constant values or random noise during both training and inference. Four experimental conditions are tested: Baseline (both modalities), Visual Knockout (text-only), Textual Knockout (visual-only), and Dual Knockout (ID-only). The analysis covers 14 state-of-the-art multimodal recommendation models across three Amazon datasets (Baby, Clothing, Sports) with 5-core filtering. Visual embeddings are pre-extracted using ResNet-50, while text embeddings use all-MiniLM-L6-v2. The MMRec toolbox is used for implementations, data splits, and evaluation, with metrics including Recall@10, NDCG@10, and Precision@10.

## Key Results
- Multimodal embeddings enhance recommendation performance, particularly in sophisticated graph-based fusion models
- Text modality alone achieves performance comparable to full multimodal setting in most cases
- Image modality provides minimal benefits and is often underutilized across models
- Architecture-dependent sensitivity: models like GRCN, LGMRec, and MMGCN are highly sensitive to visual features while others fail to leverage multimodal information effectively

## Why This Works (Mechanism)

### Mechanism 1: Modality Knockout for Causal Attribution
The knockout strategy isolates each modality's contribution by replacing informative embeddings with uninformative vectors while preserving model architecture. Three knockout conditions (Visual, Textual, Dual) triangulate modality importance. The core assumption is that models don't adapt to compensate for degraded inputs, making performance drops reflect genuine modality dependence. Break condition: If models learn to detect and route around constant/noise inputs, knockout may underestimate utility—potentially explaining VBPR's insensitivity.

### Mechanism 2: Graph-Based Fusion Amplifies Multimodal Utility
Sophisticated graph-based fusion architectures better exploit multimodal embeddings than simple concatenation or contrastive fusion. Graph structures propagate modality-specific signals across high-order neighborhoods, enabling items to inherit semantic similarity from neighbors. User-item bipartite graphs (MMGCN) or item-item similarity graphs (LATTICE, FREEDOM) create multiple pathways for multimodal information. Core assumption: Graph topology provides inductive bias that aligns with modality semantics. Break condition: Noisy graph construction or misaligned modalities may degrade performance.

### Mechanism 3: Textual Dominance via Embedding Distribution Learnability
Text embeddings are more compact and easier to learn than image embeddings, causing models to over-rely on text. Compact text distributions (lower intra-class variance) create clearer decision boundaries during training. Dispersed image distributions introduce noise, making gradient signals weaker. Models converge toward the easier-to-optimize text modality. Core assumption: Pre-extracted embeddings are comparable in semantic quality; observed asymmetry stems from distributional properties. Break condition: Pre-normalization or dimensionality reduction of image embeddings may diminish the asymmetry.

## Foundational Learning

- **Collaborative Filtering with ID Embeddings**: Understanding baseline ID-based performance is essential to interpret modality gains. Quick check: When both text and image are knocked out, what signal remains? If performance doesn't drop, what does that imply?
- **Pre-extracted Multimodal Embeddings**: All 14 models use frozen embeddings from Sentence-BERT (text) and ResNet-50/ViT (images). Quick check: Why might ResNet-50 features underperform ViT features in some models but not others? What architectural properties mediate this?
- **Graph Neural Networks for Recommendation**: Groups 2-4 leverage GNNs for multimodal fusion. Understanding message passing is prerequisite for debugging vision-sensitive models. Quick check: In MMGCN, separate graphs per modality are constructed. How does knockout of one modality's graph affect neighbor representations?

## Architecture Onboarding

- Component map: Input Layer: Item ID embeddings + Pre-extracted text (e_t) + Pre-extracted visual (e_v) → Fusion Layer: [Group 1] Simple concat/contrastive | [Groups 2-4] Graph construction → Graph Propagation: User-item bipartite (Groups 2,4) | Item-item semantic (Groups 3,4) → Prediction Layer: BPR-style ranking | Inner product | Contrastive objective
- Critical path: Text embedding → graph fusion (Groups 2-4) → user/item representation → prediction. Visual embedding is optional; failure here often goes unnoticed.
- Design tradeoffs: Simple fusion (VBPR, BM3) is fast but modality-insensitive; user-item graphs (MMGCN, GRCN) are modality-sensitive but computationally heavier; item-item graphs (LATTICE, FREEDOM) capture semantic similarity but require quality graph construction; hybrid (DRAGON, PGL) maximizes multimodal exploitation but has highest complexity.
- Failure signatures: Constant/noise embeddings yield same or better performance than real embeddings → simple fusion failure; large regularization causes model collapse → VBPR bias term dominance; ViT replacement degrades performance → encoder-fusion mismatch; Visual Knockout improves performance → visual modality introduces noise for that model-dataset pair.
- First 3 experiments: 1) Baseline Knockout Test: Run all four conditions with constant-value replacement to confirm performance drops in Dual and Textual conditions; 2) Embedding Distribution Analysis: Visualize t-SNE of text and image embeddings to identify dispersion issues; 3) Encoder Ablation: Replace ResNet-50 with ViT embeddings to test fusion-encoder compatibility.

## Open Questions the Paper Calls Out

### Open Question 1
How can fusion strategies be redesigned to effectively utilize visual features, given that current models predominantly rely on textual information? The study found visual knockout rarely degrades performance, indicating existing architectures fail to learn from images. This would be resolved by developing a fusion mechanism where removing visual embeddings causes performance drops comparable to removing textual embeddings.

### Open Question 2
How can evaluation protocols be revised to distinguish genuine multimodal learning from architectural artifacts where constant or noise vectors artificially inflate performance? The authors identify that in simple fusion models, performance gains may originate from artifacts rather than genuine multimodal learning. This requires a standardized evaluation suite including "modality knockout" robustness checks to verify gains disappear when semantic content is removed.

### Open Question 3
What specific architectural adaptations are required to ensure models benefit from advanced visual encoders (e.g., ViT) without suffering performance degradation? Section 6.2 shows switching to ViT yields mixed results where some architectures improve while others degrade, suggesting model-encoder incompatibility. This would be resolved by an ablation study isolating specific fusion components to identify which allow successful integration of ViT features.

## Limitations
- Analysis restricted to Amazon product datasets with pre-extracted embeddings from specific encoders, limiting domain generalizability
- Knockout strategy assumes models cannot dynamically adapt to degraded inputs, potentially underestimating true modality utility
- Pre-extracted embeddings may not be comparable in semantic quality across modalities, confounding observed asymmetry

## Confidence
- **High**: Text modality dominance (Section 5.2) - consistently replicated across models and datasets
- **Medium**: Graph-based fusion superiority - architecture-dependent with notable exceptions (VBPR, BM3)
- **Low**: Visual modality underutilization - may reflect encoder quality rather than inherent modality weakness

## Next Checks
1. Replicate knockout experiments on non-Amazon datasets (e.g., MovieLens with posters) to test domain generalizability
2. Implement adaptive knockout variants where models can learn to detect and route around degraded inputs
3. Compare ResNet-50 and ViT performance across diverse image types (fashion, electronics, artwork) to isolate encoder effects from modality effects