---
ver: rpa2
title: Learning to Route and Schedule LLMs from User Retrials via Contextual Queueing
  Bandits
arxiv_id: '2602.02061'
source_url: https://arxiv.org/abs/2602.02061
tags:
- queue
- have
- lemma
- regret
- then
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of efficient routing and scheduling
  for large language model (LLM) services where user dissatisfaction can lead to query
  retrials and increase server backlog. Existing online algorithms often rely on explicit
  user feedback, which is impractical and degrades user experience.
---

# Learning to Route and Schedule LLMs from User Retrials via Contextual Queueing Bandits

## Quick Facts
- arXiv ID: 2602.02061
- Source URL: https://arxiv.org/abs/2602.02061
- Reference count: 40
- Primary result: Novel contextual queueing bandits framework that achieves Õ(√t) cumulative regret and Õ(t^(-1/4)) queue length regret by learning from user retrials without explicit feedback

## Executive Summary
This paper addresses the challenge of efficiently routing and scheduling queries in large language model (LLM) services where user dissatisfaction can lead to query retrials and increased server backlog. The authors propose a novel framework called contextual queueing bandits with multinomial logit feedback (CQB-MNL) that leverages implicit feedback inferred from user retrial behaviors, eliminating the need for explicit user ratings that degrade user experience. The key innovation is modeling user choice probabilities using the multinomial logit framework, where users either accept one of candidate responses (satisfaction) or reject all and retry (dissatisfaction).

The proposed anytime CQB (ACQB) algorithm combines Thompson sampling with forced exploration at a decaying rate to achieve efficient learning while maintaining queue stability. The framework is further enhanced with disjoint parameterization for individual LLM characteristics and a utility-aligned query embedding refinement via contrastive learning (ACQB-CL). Experiments on synthetic and real-world datasets demonstrate consistent improvements over baseline algorithms in both queue length and cumulative regret metrics.

## Method Summary
The method addresses joint routing and scheduling for LLM services by modeling user retrials as implicit feedback in a contextual queueing bandit framework. The core algorithm (ACQB) uses Thompson sampling with decaying exploration (η(t) = min{1, c₁(t+1)^(-1/2)}) to learn routing utilities from user choices while maintaining queue stability. MLE updates are performed via regularized cross-entropy loss on the multinomial logit choice model. For practical deployment, the framework includes disjoint parameterization (separate θⱼ* per LLM) and a contrastive learning extension (ACQB-CL) that refines query embeddings to better align with routing utilities rather than semantic similarity.

## Key Results
- ACQB achieves cumulative regret of Õ(√t) and queue length regret of Õ(t^(-1/4)) under traffic slackness assumptions
- ACQB consistently outperforms baseline algorithms on synthetic and real-world datasets (SPROUT, EmbedLLM, RouterBench)
- ACQB-CL shows additional improvements by aligning query representations with routing utilities via contrastive learning
- The framework maintains queue stability while learning routing policies without explicit user feedback

## Why This Works (Mechanism)

### Mechanism 1: Implicit Feedback from Retrial Behavior
- **Claim**: User retrials provide implicit feedback that enables learning without explicit ratings
- **Mechanism**: When users reject all candidate responses (outside option in MNL), they retry the query, indicating dissatisfaction. The system learns from this binary choice signal.
- **Core assumption**: Users retry when dissatisfied, creating a learnable signal
- **Evidence anchors**:
  - [abstract]: "CQB-MNL models query retrials... the user accepts one of K candidate responses (satisfaction) or rejects all and retries the query (dissatisfaction)"
  - [section 2, p.3-4]: "p0(x,S,θ) := 1/(1 + ∑_{j'∈S} exp(x^⊤−j'θ)) for the outside option (j=0). Therefore, a departure D(t)=1 occurs if the user selects any valid option"
  - [corpus]: No direct corpus support for MNL-based retrial modeling specifically
- **Break condition**: If users don't retry consistently when dissatisfied, or retry for reasons unrelated to response quality, the implicit feedback signal degrades

### Mechanism 2: Thompson Sampling with Decaying Exploration Rate
- **Claim**: Combining Thompson sampling with forced exploration at a decaying rate (η(t) = min{1, c₁(t+1)^(-1/2)}) enables sublinear regret while maintaining queue stability
- **Mechanism**: Thompson sampling provides optimistic estimates for routing decisions; forced exploration ensures sufficient coverage of the feature space for design matrix eigenvalue growth
- **Core assumption**: Traffic slackness exists (λ + ε ≤ optimal departure rate) and feature distribution has positive minimum eigenvalue
- **Evidence anchors**:
  - [abstract]: "ACQB achieves efficient learning while maintaining queue stability by combining Thompson sampling with forced exploration at a decaying rate"
  - [section 3, p.5]: "η(t) = min{1, c₁(t+1)^(-1/2)... achieves cumulative regret of Õ(√t) and queue length regret of Õ(t^(−1/4))"
  - [corpus]: "Learning to Route LLMs from Bandit Feedback" (FMR=0.56) uses bandit-based routing without queueing dynamics
- **Break condition**: If arrival rate λ approaches/exceeds optimal departure rate, queue stability fails; improper exploration decay causes learning failure or queue explosion

### Mechanism 3: Disjoint Parameterization with Contrastive Learning (ACQB-CL)
- **Claim**: Learning LLM-specific parameters separately and refining query embeddings via contrastive learning improves routing utility alignment
- **Mechanism**: Disjoint parameterization (separate θⱼ* per LLM) removes feature engineering dependence; contrastive learning re-aligns embedding space so queries with similar routing utilities cluster together regardless of semantic difference
- **Core assumption**: Semantic-similarity embeddings don't capture routing utility; sufficient offline data exists for contrastive pretraining
- **Evidence anchors**:
  - [section 5.1, p.10-11]: "Each server j is governed by a unique unknown parameter θⱼ*"
  - [section 6.2, p.13]: "ACQB-CL consistently achieves lower cumulative regret than ACQB"
  - [corpus]: MixLLM (FMR=0.58) and BEST-Route (FMR=0.61) address LLM routing via different approaches
- **Break condition**: If LLMs share identical characteristics, disjoint parameterization adds complexity; poor contrastive data coverage harms projection head generalization

## Foundational Learning

- **Concept: Multinomial Logit (MNL) Choice Models**
  - Why needed here: Models probability of user selecting one of K options or the outside option (retrial)
  - Quick check question: Can you compute pⱼ(x,S,θ) = exp(x^Tθⱼ)/(1 + ∑_{k∈S} exp(x^Tθk)) and explain behavior when all x^Tθk are very negative?

- **Concept: Queueing Theory Basics (Arrival Rate λ, Departure Rate, Traffic Slackness ε)**
  - Why needed here: Queue stability requires arrival rate < departure rate; slackness ε is needed for stability guarantees
  - Quick check question: If λ = 0.9 and optimal departure rate R* = 0.93, what's the traffic slackness and will the queue be stable in expectation?

- **Concept: Thompson Sampling for Contextual Bandits**
  - Why needed here: Core algorithm uses posterior sampling for optimistic decision-making
  - Quick check question: How does Thompson sampling differ from UCB, and why does sampling from N(θ̂, α²V^(−1)) provide optimism?

## Architecture Onboarding

- **Component map**: Queue manager -> MNL estimator -> Routing policy -> Embedding pipeline
- **Critical path**:
  1. Query arrives → embed → check exploration flag E(t)
  2. If exploring: round-robin select LLM assortment
  3. If exploiting: sample M parameter vectors → compute optimistic departure rates → select query-assortment pair maximizing Ê[R(x,S)]
  4. Observe user choice y → update MLE θ̂ and design matrix V
  5. Decay exploration probability η(t)

- **Design tradeoffs**:
  - Higher c₁: faster learning, more retrials/queue growth
  - Assortment size K: K=1 is single-response; K=2 enables pairwise preference but shows two responses
  - Disjoint vs shared parameters: Disjoint scales O(N×d) but captures heterogeneity better
  - Embedding dimension d: Higher d → slower learning (regret Õ(d^(3/2)√t))

- **Failure signatures**:
  - Queue growing unboundedly → check traffic slackness (λ < optimal departure?)
  - Cumulative regret not decreasing → check exploration decay, feature normalization, MNL convergence
  - ACQB-CL underperforming ACQB → check contrastive data quality, ιpos/ιneg thresholds

- **First 3 experiments**:
  1. Replicate synthetic validation (Figure 2) with λ=0.7, N=5, K∈{1,2}. Verify Õ(√t) regret and queue stability.
  2. Ablate exploration rate constant c₁ ∈ {0.5, 1.0, 2.0}. Plot queue length regret; expect U-shaped curve.
  3. Compare ACQB vs ACQB-CL vs random baseline on SPROUT. If ACQB-CL underperforms, examine cosine similarity thresholds (ιpos, ιneg) and InfoNCE temperature τtemp.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the proposed ACQB algorithm maintain queue stability and regret guarantees under non-stationary or adversarial query arrivals?
- Basis in paper: [explicit] Assumption 3 states that "The features of newly arriving jobs are assumed to be independently and identically distributed (i.i.d.) from an unknown distribution," which is required for the theoretical stability analysis.
- Why unresolved: Real-world LLM traffic often exhibits temporal patterns, burstiness, or distribution shifts (concept drift) that violate the i.i.d. assumption, potentially destabilizing the queue.
- What evidence would resolve it: A theoretical analysis of queue length regret under non-stationary arrival models or empirical evaluation showing robustness to traffic distribution shifts.

### Open Question 2
- Question: Is it possible to close the gap in dimension-dependence between the "anytime" ACQB algorithm and horizon-dependent baselines?
- Basis in paper: [explicit] The paper notes in Section 3 that while their anytime approach avoids dependence on the time horizon $T$, it results in a looser leading term in the queue length regret ($O(d^5...)$) compared to the tighter bounds ($O(d^{3/2}...)$) achieved by non-anytime algorithms like that of Bae et al. (2026).
- Why unresolved: The theoretical analysis suggests a significant cost in terms of dimension-dependence for the flexibility of the anytime property, leaving open whether this gap is fundamental or an artifact of the current proof technique.
- What evidence would resolve it: A refined theoretical analysis for an anytime algorithm that matches the dimension-dependence of horizon-aware algorithms.

### Open Question 3
- Question: How does the system performance degrade if user choice behavior deviates from the Multinomial Logit (MNL) model?
- Basis in paper: [inferred] The framework relies entirely on the CQB-MNL model to link retrials to feedback. While MNL is standard, actual user preferences in LLM routing may violate properties like the Independence of Irrelevant Alternatives (IIA).
- Why unresolved: Theoretical guarantees rely on the specific structure of the MNL likelihood; model misspecification could break the Thompson sampling convergence or queue stability guarantees.
- What evidence would resolve it: A regret analysis under model misspecification or experiments using synthetic user agents with non-MNL decision policies.

## Limitations
- Theoretical guarantees rely on unverified assumptions about traffic slackness and feature covariance structure in real-world settings
- Disjoint parameterization increases computational complexity to O(N×d) and may be unnecessary if LLMs share similar characteristics
- Contrastive learning component requires high-quality offline data with meaningful positive/negative pairs, which may not exist in practice

## Confidence

| Claim | Confidence |
|-------|------------|
| MNL modeling of retrial behavior | High (supported by explicit derivations) |
| Thompson sampling with decaying exploration | Medium (proven theoretically but sensitive to hyperparameters) |
| ACQB-CL improvements | Medium (shown empirically but dependent on data quality) |

## Next Checks
1. Verify traffic slackness condition (λ + ε ≤ max_S R(x,S,Θ*)) on real datasets before running ACQB
2. Perform ablation study on exploration constant c₁ to identify optimal tradeoff between learning speed and queue stability
3. Test ACQB-CL on datasets with varying levels of similarity between routing utilities and semantic embeddings to assess when contrastive learning provides benefit