---
ver: rpa2
title: 'Corpus Frequencies in Morphological Inflection: Do They Matter?'
arxiv_id: '2510.23131'
source_url: https://arxiv.org/abs/2510.23131
tags:
- accuracy
- training
- corpus
- token
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates whether incorporating corpus frequency
  information improves morphological inflection systems. The authors propose three
  methodological improvements: a frequency-weighted lemma-disjoint train-dev-test
  split, token accuracy evaluation that weights correct predictions by word frequency,
  and frequency-aware training that samples frequent words more often during training.'
---

# Corpus Frequencies in Morphological Inflection: Do They Matter?

## Quick Facts
- **arXiv ID:** 2510.23131
- **Source URL:** https://arxiv.org/abs/2510.23131
- **Reference count:** 22
- **Primary result:** Frequency-aware training with temperature 0.5 improves morphological inflection systems in 26/43 languages

## Executive Summary
This paper investigates whether incorporating corpus frequency information improves morphological inflection systems. The authors propose three methodological improvements: a frequency-weighted lemma-disjoint train-dev-test split, token accuracy evaluation that weights correct predictions by word frequency, and frequency-aware training that samples frequent words more often during training. Experiments across 43 languages show that frequency-aware training with corpus-frequency temperature 0.5 outperforms uniform sampling in 26 languages, with the best model improving over uniform sampling in 26 of 43 languages for token accuracy and 41 of 43 languages for type accuracy. The study demonstrates that incorporating corpus frequencies during training and evaluation provides measurable benefits for morphological inflection systems.

## Method Summary
The method involves extracting lemma-tag-form triples with corpus occurrence counts from Universal Dependencies treebanks, then applying a frequency-weighted lemma-disjoint split (8:1:1 by occurrence count). A Transformer encoder-decoder is trained with weighted random sampling where sample weights equal frequency^τ, with τ=0.5 found optimal through dev set tuning. Both token accuracy (frequency-weighted) and type accuracy (uniform) are computed on the test set. The approach assumes that frequent words share morphological patterns with other frequent words, so emphasizing them during training transfers to unseen frequent test lemmas.

## Key Results
- Frequency-aware training with temperature 0.5 outperforms uniform sampling in 26 of 43 languages for token accuracy
- The best model improves over uniform sampling in 41 of 43 languages for type accuracy
- Token accuracy drops are negligible when using type vs. token accuracy for checkpoint selection, suggesting similar model rankings
- Languages with negative optimal τ values (English, Galician, Slovak) indicate the mechanism fails when rare words carry more morphological regularity signal

## Why This Works (Mechanism)

### Mechanism 1: Frequency-Aware Training via Sample Weighting
- **Claim:** Weighting training samples by corpus frequency (raised to temperature τ) improves model performance on frequent words, with optimal τ≈0.5.
- **Mechanism:** The corpus-frequency temperature τ controls the sampling probability ratio between items. For τ=0.5, sample weights equal √(frequency), so an item occurring 400× is sampled 20× more often (not 400×). This intermediate weighting avoids both extremes: uniform sampling (τ=0) that ignores frequency, and raw frequency weighting (τ=1) that may overfit to frequent items.
- **Core assumption:** Frequent words share morphological patterns with other frequent words in the lemma-disjoint test set, so emphasizing them during training transfers to unseen frequent test lemmas.
- **Evidence anchors:**
  - [abstract] "frequency-aware training with corpus-frequency temperature 0.5... outperforms uniform sampling in 26 languages"
  - [section 4.1] "On average in all languages... continuous improvement when the temperature increases to 0.5, and then a continuous decline"
  - [corpus] Neighbor papers show related work on sample efficiency in low-resource settings but do not directly validate the τ=0.5 finding for inflection specifically.
- **Break condition:** Languages with negative optimal τ values (English, Galician, Slovak) suggest the mechanism fails when rare words carry more morphological regularity signal than frequent ones, or when test distributions diverge from training frequency distributions.

### Mechanism 2: Token Accuracy as Deployment-Relevant Metric
- **Claim:** Token accuracy (weighting evaluation by corpus frequency) better predicts real-world performance than type accuracy, though both produce similar model rankings under the paper's split methodology.
- **Mechanism:** Token accuracy computes: (1/total_occurrences) × Σ(correct_items × occurrences). A correctly predicted form occurring 1000× contributes 1000× more than one occurring once. Type accuracy treats all lemma-tag-form triples equally regardless of usage frequency.
- **Core assumption:** Production deployment input distributions approximately match corpus frequency distributions.
- **Evidence anchors:**
  - [abstract] "token accuracy, which assigns greater weight to frequent words and better approximates performance on running text"
  - [section 4.3] "performance drops are negligible" when checkpoint selection uses type vs. token accuracy—suggesting similar rankings
  - [corpus] No direct corpus validation of token accuracy's predictive validity for deployment exists.
- **Break condition:** If test set frequency distribution diverges substantially from deployment distribution (e.g., domain shift), token accuracy becomes misleading.

### Mechanism 3: Frequency-Weighted Lemma-Disjoint Split
- **Claim:** Combining lemma-disjoint constraints with frequency-weighted sampling creates realistic train-test frequency distributions while maintaining generalization evaluation validity.
- **Mechanism:** Lemmas (not individual triples) are sampled into train weighted by total occurrence count; remaining lemmas split uniformly into dev/test. This ensures: (1) no lemma appears in both train and test, preventing artificial inflation; (2) frequent items concentrate in train, rare items in dev/test, mirroring real-world skew.
- **Core assumption:** Training on frequent lemmas transfers to inflecting rare unseen lemmas through shared morphological patterns.
- **Evidence anchors:**
  - [section 3.2] "sample lemmas first into the train set, randomly, weighted by the occurrence counts"
  - [section 5] "frequent words in the dev/test sets tend to inflect in a similar way to frequent words in the training set"
  - [corpus] Kodner et al. (cited) proposed frequency-weighted split but without lemma-disjoint constraint; no independent validation of combined approach.
- **Break condition:** Morphologically irregular languages where frequent and rare words follow different inflection patterns would break the transfer assumption.

## Foundational Learning

- **Concept: Morphological Inflection Task**
  - **Why needed here:** This is the base task—transforming lemma + morphological tags → inflected form. Without understanding this, frequency-weighting mechanisms make no sense.
  - **Quick check question:** Given lemma "walk" and tags "V;PST;3;SG", what should the output be?

- **Concept: Type vs. Token Frequency**
  - **Why needed here:** The entire paper hinges on distinguishing these. Type = unique word forms (lexicon view). Token = occurrences in running text. Type accuracy counts each unique form once; token accuracy weights by usage.
  - **Quick check question:** If "run" appears 500× and "sprint" appears 5× in a corpus, what's their type vs. token contribution to evaluation?

- **Concept: Lemma-Disjoint Evaluation**
  - **Why needed here:** Prevents the model from "cheating" by memorizing lemma-specific patterns. Ensures evaluation measures generalization to unseen lemmas, not just recall.
  - **Quick check question:** Why would lemma overlap between train and test artificially inflate reported accuracy?

## Architecture Onboarding

- **Component map:**
  UD corpus -> extract lemma-tag-form triples + frequencies -> frequency-weighted lemma-disjoint split -> training batches sampled with temperature-weighted probabilities -> Transformer encoder-decoder -> both type and token accuracy evaluation

- **Critical path:**
  1. Extract triples + frequencies from UD (not UniMorph—lacks frequency info)
  2. Apply frequency-weighted lemma-disjoint split (8:1:1 by occurrence count)
  3. Tune τ on dev languages (Czech, English, Spanish, Breton, Basque)
  4. Train with weighted sampling, select checkpoint by token accuracy
  5. Evaluate on test using both metrics

- **Design tradeoffs:**
  - τ=0.5 (sqrt weighting) vs. τ=1.0 (raw frequency): τ=1.0 over-emphasizes frequent items, causing degradation; τ=0.5 is safer default
  - UD vs. UniMorph: UD has frequencies but lower coverage; UniMorph has coverage but no frequencies
  - Token vs. type accuracy for checkpoint selection: paper shows minimal difference, but token accuracy is principled for deployment focus

- **Failure signatures:**
  - τ=2.0: "complete failure" in most languages (extreme over-weighting)
  - Negative τ benefits some languages (English, Galician, Slovak)—rare words carry more signal
  - Low Saxon and Old French: all models fail to beat copy baseline—likely data quality or extreme morphological irregularity

- **First 3 experiments:**
  1. **Baseline replication:** Train with τ=0 (uniform) on your target language, report type and token accuracy to establish benchmark.
  2. **Temperature sweep:** Grid search τ ∈ {0.0, 0.3, 0.5, 0.8, 1.0} on dev set, plot token accuracy curve to verify τ≈0.5 peak holds for your language.
  3. **Ablation on split method:** Compare frequency-weighted lemma-disjoint split vs. uniform lemma-disjoint split to isolate whether gains come from training weighting or split distribution.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Why does tuning the corpus-frequency temperature (τ) based on token accuracy yield greater improvements in type accuracy than in token accuracy on the test set?
- **Basis in paper:** [explicit] Section 5 notes that the τ_best model (tuned on dev token accuracy) outperformed uniform sampling in 41/43 languages for type accuracy but only 26/43 for token accuracy, a "surprising" result the authors explicitly seek an explanation for.
- **Why unresolved:** The paper demonstrates this counter-intuitive empirical result but does not provide a confirmed theoretical explanation; it only hypothesizes that frequent lemmas exhibit greater form diversity.
- **What evidence would resolve it:** An analysis of the correlation between lemma frequency and the number of distinct inflectional forms (form diversity) per lemma in the training versus test sets, alongside ablation studies isolating the effect of form variety.

### Open Question 2
- **Question:** Which specific linguistic or corpus-related factors (e.g., morphological richness, regularity, or data size) predict the success or failure of frequency-aware training?
- **Basis in paper:** [explicit] Section 5.1 states that investigating why frequency-aware training helps some languages but has the "opposite effect" in others is a primary goal for future work.
- **Why unresolved:** The study identifies languages where the method fails (e.g., English, Galician, Slovak) but does not perform the necessary comparative linguistic analysis to explain the variance.
- **What evidence would resolve it:** A regression analysis correlating performance gains (frequency-aware vs. uniform) with typological features (e.g., fusion, synthesis indices) and dataset statistics (corpus size, tag entropy) across the 43 languages.

### Open Question 3
- **Question:** Does incorporating frequency information via large raw corpora aligned with morphological lexicons (UniMorph) outperform the annotated-only approach used in this study?
- **Basis in paper:** [explicit] Section 3.1 describes alternative data sourcing methods (Option 2: aligning raw text with UniMorph) that were considered but not used, and Section 5.1 explicitly suggests exploring this to increase coverage of lemmas/forms.
- **Why unresolved:** The authors used only Universal Dependencies (annotated corpora) to ensure clean data, acknowledging that this leads to lower coverage compared to the theoretical maximum available via lexicon alignment.
- **What evidence would resolve it:** Experiments implementing the "tag-based back-off" disambiguation method described in Section 3.1 to extract frequencies from raw corpora, comparing the resulting lemma coverage and model performance against the UD-only baseline.

## Limitations
- The optimal temperature τ=0.5 is derived from a small dev set of 5 languages, raising questions about generalization across language families
- The method relies on Universal Dependencies treebanks, which vary significantly in size, quality, and genre coverage across languages
- Token accuracy's predictive validity for real-world deployment scenarios is not empirically validated beyond the paper's split methodology

## Confidence
**High Confidence:** The frequency-aware training mechanism (τ=0.5 sampling) demonstrably improves performance in 26/43 languages over uniform sampling. The token accuracy metric correctly identifies deployment-relevant performance differences, and the lemma-disjoint split prevents artificial inflation of results.

**Medium Confidence:** The universal applicability of τ=0.5 across language families, the assumption that corpus frequency distributions reflect real-world usage, and the claim that token accuracy better predicts deployment performance all require broader validation across diverse linguistic contexts and deployment scenarios.

**Low Confidence:** The mechanism's failure conditions (why negative τ helps some languages) and the exact mathematical relationship between sampling temperature and model generalization across morphologically diverse language families remain incompletely understood.

## Next Checks
1. **Cross-Linguistic Generalization Test:** Replicate the temperature sweep (τ ∈ {0.0, 0.3, 0.5, 0.8, 1.0}) on languages from underrepresented families (e.g., Uralic, Turkic, Semitic) to verify τ=0.5 remains optimal beyond Indo-European languages and to better understand when negative τ values are beneficial.

2. **Deployment Correlation Study:** Compare token accuracy predictions against actual model performance on out-of-corpus text from the same languages (e.g., news articles, social media, literature) to empirically validate whether token accuracy correlates with real-world deployment success across different domain distributions.

3. **Morphological Regularity Analysis:** For languages where τ=0.5 fails or negative τ succeeds, conduct a detailed analysis of morphological regularity patterns in frequent vs. rare words to determine whether the mechanism fails when frequent words are more morphologically irregular than rare words, or when different inflectional classes dominate different frequency ranges.