---
ver: rpa2
title: Causal Identification of Sufficient, Contrastive and Complete Feature Sets
  in Image Classification
arxiv_id: '2507.23497'
source_url: https://arxiv.org/abs/2507.23497
tags:
- explanations
- causal
- pixels
- explanation
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a rigorous causal framework for explaining\
  \ image classifier outputs, defining three types of explanations\u2014sufficient,\
  \ contrastive, and complete\u2014and proving their formal properties. Causal explanations\
  \ are shown to be equivalent to abductive explanations and naturally support black-box\
  \ computation without model restrictions."
---

# Causal Identification of Sufficient, Contrastive and Complete Feature Sets in Image Classification

## Quick Facts
- arXiv ID: 2507.23497
- Source URL: https://arxiv.org/abs/2507.23497
- Authors: David A Kelly; Hana Chockler
- Reference count: 40
- Primary result: Introduces formal causal framework for image classifier explanations with three types (sufficient, contrastive, complete), proves equivalence to abductive explanations, and demonstrates efficient computation via ReX algorithm

## Executive Summary
This paper presents a rigorous causal framework for explaining image classifier outputs, defining three types of explanations based on formal causal principles. The method treats image classifiers as depth-2 Structural Causal Models and proves that causal explanations are equivalent to abductive (logic-based) explanations under reasonable independence assumptions. The framework introduces confidence-aware explanations and defines adjustment pixels that bridge sufficiency and completeness. Using the ReX tool, the authors efficiently compute all explanation types in about 6 seconds per image on average across three state-of-the-art models.

## Method Summary
The method computes causal explanations for image classifiers by treating them as depth-2 Structural Causal Models where pixels are independent endogenous variables. It defines sufficient explanations (minimal pixel sets maintaining classification with confidence ≥ δ), contrastive explanations (minimal sets that flip classification), and complete explanations (exact confidence recovery). The ReX tool approximates causal responsibility through greedy algorithms, constructing two context sets (K⁺ for insertion/deletion from baseline, K⁻ for occlusion from original) and iteratively refining important pixels. The framework is black-box, requiring only input-output queries without model internals.

## Key Results
- Causal explanations are formally equivalent to abductive explanations under pixel independence assumptions
- Models differ significantly in sufficiency, contrastiveness, and completeness requirements
- Contrastive and adjustment pixel predictions often lie at short semantic distances from original classification
- All explanation types computed in approximately 6 seconds per image on average
- Complete explanations capture semantic signal while sufficient explanations may be more robust

## Why This Works (Mechanism)

### Mechanism 1: Structural Causal Model (SCM) Mapping
If an image classifier is treated as a depth-2 Structural Causal Model where pixels are independent endogenous variables, the resulting causal explanations are equivalent to formal abductive (logic-based) explanations. The method maps image pixels to input variables V and the classifier output to O, assuming causal independence between pixels. Under this mapping, the definition of an "actual cause" (Halpern-Pearl) mathematically converges with "prime-implicant" explanations used in logic.

### Mechanism 2: Confidence-Aware Delta Partitioning
By introducing a confidence threshold δ, the mechanism partitions the image into sufficient, contrastive, and adjustment sets based on how sensitive the model's probability output is to pixel occlusion. A sufficient explanation is a minimal pixel set that maintains the class with probability ≥ δ × original_confidence. Adjustment pixels are defined as the set difference between complete and sufficient sets—pixels required to boost confidence back to original level without changing class.

### Mechanism 3: Greedy Responsibility Approximation
Since computing exact minimal sets is DP-complete, the method uses a greedy "causal responsibility" ranking to approximate sufficient and contrastive sets efficiently. The algorithm (ReX) approximates causal responsibility—a measure of how much a pixel contributes to the output—by performing an initial partition then refining important parts iteratively.

## Foundational Learning

- **Concept: Actual Causality (AC1–AC3)** - The mathematical bedrock of the paper; understanding the difference between "counterfactual dependence" and "actual causation" is required to interpret the results. Quick check: If pixel set X causes classification O, but removing X does not change O because redundant set Y exists, does X still fail condition AC2?

- **Concept: Abductive vs. Contrastive Explanation** - The paper claims equivalence between causal and logic-based explanations. Understanding that abductive answers "Why this class?" while contrastive answers "Why not that class?" is required to interpret the results. Quick check: Does a contrastive explanation identify pixels that must be present, or pixels that, if removed, would change the outcome?

- **Concept: Black-box Optimization / Oracle Access** - The method does not use gradients; it treats the model as an oracle. Understanding that "black-box" here implies reliance solely on input-output queries (inference), contrasting with white-box methods like Grad-CAM, is essential. Quick check: If model inference time is 100ms and algorithm requires 1,000 queries for a single image, is this approach practically viable for real-time systems?

## Architecture Onboarding

- **Component map:** Model Wrapper -> Causal Engine (ReX) -> Verifier
- **Critical path:** The Ranker (Line 4, Algo 1) is the bottleneck. If responsibility approximation is poor, greedy search requires more iterations or fails to find minimal set. Batch size of Context Generator determines GPU utilization efficiency.
- **Design tradeoffs:** Baseline Selection (0 vs mean-noise vs blur), Delta (δ) Strictness (0.5 vs 1.0), Precision vs Speed (4 decimal places vs stricter).
- **Failure signatures:** Trivial Contrastive (baseline contains target class), Large Adjustment Set (model relies on global context), Semantic Drift (adjustment pixels classified as unrelated objects).
- **First 3 experiments:** 1) Baseline Sanity Check with 3 different baselines (0, 1, Gaussian Noise), 2) Delta Sensitivity sweep from 0.1 to 1.0 plotting adjustment set size, 3) Contrastive Distance calculation using ImageNet hierarchy path between original and contrast classes.

## Open Questions the Paper Calls Out

None explicitly identified in the paper text.

## Limitations

- The causal framework's validity depends critically on depth-2 SCM assumption and pixel independence, which is an approximation for real images
- The greedy ReX algorithm provides only approximate solutions to NP-hard problems, with approximation quality depending on responsibility ranking heuristic
- The confidence-aware partitioning assumes calibrated model outputs, which is not verified in the paper

## Confidence

High confidence: Formal definitions and theoretical properties (Lemma 1, Theorems 1-3) are mathematically rigorous and internally consistent. Runtime claims (6s/image) are specific and reproducible.

Medium confidence: Semantic distance analysis between original and contrastive classes relies on ImageNet hierarchy structure, which is static and may not reflect perceptual similarity.

Low confidence: Generalization claim that all models "differ in sufficiency, contrastiveness, and completeness requirements" is based on single validation set without statistical significance testing.

## Next Checks

1. **Pixel Independence Stress Test:** Systematically evaluate quality of causal explanations when pixel correlation is artificially introduced (e.g., by applying Gaussian blur with increasing kernel size). Measure degradation in explanation fidelity and break point where abductive-equivalence fails.

2. **Calibration Verification:** For each model, compute Expected Calibration Error (ECE) on validation set. Correlate ECE values with semantic distance between original and contrastive classes to test whether poorly calibrated models produce less meaningful contrastive explanations.

3. **Baseline Robustness Analysis:** Repeat explanation extraction using three different baselines (0, mean-pixel, Gaussian noise) on 100 randomly selected images. Quantify variance in sufficient explanation sizes and contrastive class distances to assess baseline sensitivity.