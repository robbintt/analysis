---
ver: rpa2
title: 'DINGO: Constrained Inference for Diffusion LLMs'
arxiv_id: '2505.23061'
source_url: https://arxiv.org/abs/2505.23061
tags:
- diffusion
- constrained
- dingo
- greedy
- unconstrained
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DINGO, the first constrained decoding algorithm
  for diffusion-based language models (LLMs). Unlike autoregressive models, diffusion
  LLMs predict blocks of tokens in parallel, making traditional constrained decoding
  approaches ineffective.
---

# DINGO: Constrained Inference for Diffusion LLMs

## Quick Facts
- arXiv ID: 2505.23061
- Source URL: https://arxiv.org/abs/2505.23061
- Reference count: 40
- Key result: First constrained decoding algorithm for diffusion-based language models that guarantees correctness and optimality

## Executive Summary
This paper introduces DINGO, the first constrained decoding algorithm specifically designed for diffusion-based language models (LLMs). Unlike autoregressive models, diffusion LLMs predict blocks of tokens in parallel, making traditional constrained decoding approaches ineffective. DINGO addresses this challenge using dynamic programming to ensure both correctness and optimality: it guarantees that outputs strictly adhere to user-specified regular expressions while maximizing the probability under the model's true output distribution. The algorithm achieves up to 68 percentage points improvement over unconstrained inference on symbolic math reasoning and JSON generation benchmarks.

## Method Summary
DINGO employs dynamic programming to perform constrained decoding for diffusion-based language models. The key innovation lies in adapting dynamic programming techniques to handle the parallel token prediction characteristic of diffusion models, where traditional sequential decoding approaches fail. The algorithm constructs a decoding graph that respects the regular expression constraints while optimizing for the model's output distribution. By carefully managing the exponential complexity associated with block-level predictions, DINGO achieves both correctness guarantees (100% syntactic validity) and optimality (maximum probability outputs under constraints) for regular expression-constrained generation tasks.

## Key Results
- Achieves up to 68 percentage points improvement over unconstrained inference on symbolic math reasoning and JSON generation benchmarks
- Demonstrates 100% syntactic and schema validity while maintaining competitive runtime efficiency
- Shows effectiveness for structured output generation with diffusion models across multiple evaluation tasks

## Why This Works (Mechanism)
DINGO's effectiveness stems from its dynamic programming approach that can handle the parallel token prediction characteristic of diffusion models. Traditional constrained decoding methods designed for autoregressive models cannot be directly applied because diffusion models generate token blocks simultaneously rather than sequentially. By constructing a decoding graph that respects regular expression constraints while optimizing for the model's probability distribution, DINGO can guarantee both correctness (outputs satisfy constraints) and optimality (maximize model probability under constraints).

## Foundational Learning

**Regular Expressions**: Pattern matching language for defining string constraints - needed to specify the output format requirements; quick check: can you write a regex for valid JSON objects?

**Dynamic Programming**: Algorithmic technique for solving complex problems by breaking them into overlapping subproblems - needed to efficiently explore the space of valid outputs while maximizing probability; quick check: can you trace through a DP table for sequence alignment?

**Diffusion Models**: Generative models that denoise data iteratively rather than generating sequentially - needed to understand why traditional decoding methods don't work; quick check: can you explain the forward and reverse diffusion processes?

## Architecture Onboarding

**Component Map**: DINGO -> Dynamic Programming Graph -> Regular Expression Constraints -> Diffusion Model Output Distribution

**Critical Path**: Input constraints → Graph construction → Dynamic programming optimization → Output generation

**Design Tradeoffs**: 
- Correctness vs. runtime: DP ensures 100% validity but has exponential complexity in block size
- Optimality vs. constraint expressiveness: Guarantees optimality for regular expressions but limited to this constraint type
- Block size vs. efficiency: Larger blocks improve parallelism but exponentially increase computation

**Failure Signatures**: 
- Constraint violations indicate graph construction errors
- Suboptimal outputs suggest DP implementation bugs
- Runtime explosions point to block size being too large for given constraints

**First Experiments**:
1. Test with simple regular expressions (e.g., fixed-length strings) to validate basic functionality
2. Verify constraint compliance on synthetic datasets with known ground truth
3. Benchmark runtime scaling with increasing block sizes and constraint complexities

## Open Questions the Paper Calls Out
None

## Limitations
- Focuses exclusively on regular expression constraints, leaving questions about performance with more complex constraint types (context-free grammars, semantic constraints)
- Algorithm's time complexity scales exponentially with the maximum block size β, though practical implementations keep β small
- Evaluation primarily focuses on synthetic benchmarks and structured output tasks, with limited assessment of performance on open-ended generation tasks

## Confidence
High confidence: Claims about DINGO's correctness guarantees for regular expression compliance and its ability to maintain valid outputs under constrained decoding are well-supported by theoretical analysis and empirical validation.

Medium confidence: Performance claims regarding the 68 percentage point improvement and 100% syntactic validity are based on controlled benchmarks that may not fully represent real-world usage scenarios.

Low confidence: Claims about runtime efficiency being "competitive" lack detailed benchmarking against alternative approaches, and the scalability analysis for larger block sizes remains largely theoretical.

## Next Checks
1. **Constraint Expressiveness**: Test DINGO's performance and efficiency with constraints beyond regular expressions, including context-free grammars and semantic validation rules, to assess its practical applicability.

2. **Real-world Performance**: Evaluate DINGO on production datasets with naturally occurring constraints, such as code generation from specifications or document structuring tasks, to validate generalization beyond synthetic benchmarks.

3. **Scalability Analysis**: Conduct comprehensive runtime benchmarks comparing DINGO against autoregressive constrained decoding across varying block sizes, constraint complexities, and model scales to quantify the claimed efficiency advantages.