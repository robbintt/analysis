---
ver: rpa2
title: Agentic Physical AI toward a Domain-Specific Foundation Model for Nuclear Reactor
  Control
arxiv_id: '2512.23292'
source_url: https://arxiv.org/abs/2512.23292
tags:
- control
- physical
- foundation
- agentic
- success
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Agentic Physical AI, a framework for training
  compact language models as control agents in nuclear reactor systems. By combining
  a two-phase curriculum (grammar learning followed by task conditioning) with outcome-centric
  validation in a high-fidelity reactor simulator, the method enables a 360-million-parameter
  model to autonomously discover robust control strategies.
---

# Agentic Physical AI toward a Domain-Specific Foundation Model for Nuclear Reactor Control

## Quick Facts
- arXiv ID: 2512.23292
- Source URL: https://arxiv.org/abs/2512.23292
- Reference count: 40
- This paper introduces Agentic Physical AI, a framework for training compact language models as control agents in nuclear reactor systems. By combining a two-phase curriculum (grammar learning followed by task conditioning) with outcome-centric validation in a high-fidelity reactor simulator, the method enables a 360-million-parameter model to autonomously discover robust control strategies. Scaling training data from 1,000 to 100,000 synthetic scenarios induces a phase transition: success rates jump from 26.2% to 92% at ±1% tolerance, variance collapses 500-fold, and the model concentrates 76% of runtime execution on a single-bank strategy despite balanced training. The approach transfers to a different physics engine (PyRK) with >94% success and demonstrates tail-risk collapse with zero severe failures at 100K scale. This represents a paradigm shift from optimization-based to generation-based control, enabling safe, reliable, and computationally efficient learning-enabled actuation in safety-critical nuclear systems.

## Executive Summary
This work presents Agentic Physical AI, a novel framework that trains compact language models as autonomous control agents for nuclear reactor power maneuvering. Unlike traditional optimization-based controllers or deep reinforcement learning, the approach uses a two-phase curriculum to first learn the grammar of valid control commands, then condition this prior on achieving specific power targets. The key innovation is outcome-centric validation—measuring success by whether the reactor reaches the target power rather than parameter proximity to labeled actions. This enables the model to discover multiple valid control strategies within the physics-allowed solution manifold. The framework achieves a qualitative phase transition when scaling training data from 1K to 100K synthetic scenarios, jumping from 26.2% to 92% success at strict ±1% tolerance with 500-fold variance collapse and elimination of catastrophic failures.

## Method Summary
The framework trains a 360-million-parameter SmolLM2 model through a two-phase curriculum using synthetic data from the KOMODO reactor simulator. Phase 1 (continued pretraining) learns the grammar of valid six-parameter control commands without power targets. Phase 2 (LoRA fine-tuning) conditions this structural prior on mapping power demands to control vectors. The model is evaluated through closed-loop validation—injecting predicted control vectors into KOMODO and measuring terminal power error across 2,000 independent runs per model scale. Success is defined outcome-centrically: terminal power within tolerance bands (±1-10%). The approach is validated across three scales (1K, 10K, 100K synthetic scenarios) and demonstrates transfer to a different physics engine (PyRK) without architectural modification.

## Key Results
- Scaling training data from 1K to 100K synthetic scenarios induces a phase transition: success rates jump from 26.2% to 92% at ±1% tolerance
- Variance collapses 500-fold and policy entropy decreases from 1.38 to 0.89 nats, indicating discovery of high-reliability strategies
- The model concentrates 76% of runtime execution on single-bank strategy despite balanced 30/30/30/10 training distribution
- Framework transfers to PyRK physics engine with >94% success, demonstrating foundation-model properties
- Zero severe failures (>10% error) at 100K scale, compared to 771 at 1K scale

## Why This Works (Mechanism)

### Mechanism 1: Two-Phase Curriculum Separating Grammar from Task Grounding
The two-phase curriculum enables stable policy formation by first learning the manifold of valid control commands (position ranges, timing constraints, speed limits) without exposure to power targets, then conditioning this structural prior on mapping power demands to control vectors. LoRA's low-rank constraint preserves the grammar prior while allowing task adaptation. This separation prevents catastrophic forgetting during task specialization.

### Mechanism 2: Outcome-Centric Validation Decoupling Success from Parameter Imitation
Defining correctness by achieved physical outcomes (terminal power within tolerance) rather than parameter proximity to labeled actions enables agentic policy selection in many-to-one control spaces. Multiple distinct control sequences can achieve identical power transitions; outcome-centric evaluation permits the model to discover high-reliability strategies within the solution manifold without penalty for deviating from training examples.

### Mechanism 3: Data Scaling Inducing Phase Transition in Policy Stability
Scaling offline synthetic data induces a qualitative phase transition—super-linear improvement in precision, 500x variance collapse, and elimination of catastrophic tail failures—rather than incremental improvement. At high scales, the model's estimate of success probability becomes accurate and sharply distinguishes high-success from low-success regions, concentrating on robust strategies.

## Foundational Learning

- **Many-to-one actuation spaces**: Nuclear reactor control has multiple valid control sequences achieving identical power outcomes. Standard supervised learning's assumption of one correct answer per state is structurally wrong for this domain. Quick check: Can you list three different rod actuation sequences that would all move reactor power from 80% to 70%?

- **Outcome-space vs. parameter-space optimization**: Minimizing parameter error does not guarantee—and may even hinder—physical success. You must evaluate what happens when the reactor executes the command. Quick check: A model predicts rod positions with 2% error but achieves target power. Another has 0.5% error but misses target by 8%. Which is "better" for reactor control?

- **Phase transitions in learning**: The paper documents super-linear scaling where doubling data more than doubles precision at strict tolerances. This indicates the model develops qualitatively different internal representations above a critical threshold. Quick check: If you observe linear improvement from 1K to 10K samples, what would you expect at 100K if a phase transition is occurring?

## Architecture Onboarding

- **Component map**: SmolLM2-360M backbone -> Phase 1 CPT (grammar learning) -> Phase 2 LoRA (task conditioning) -> KOMODO simulator (validation) -> Actuation taxonomy analysis

- **Critical path**: 1) Generate synthetic scenarios via KOMODO with balanced actuation-family distribution (30/30/30/10) 2) Phase 1: Train model to predict control vectors only (strip power tokens) 3) Phase 2: LoRA fine-tune with full 8-token schema 4) Validate via 2,000 independent closed-loop runs per model scale 5) Measure outcome-centric success (terminal power within tolerance bands ±1-10%)

- **Design tradeoffs**: Model size vs. deployment constraints (360M chosen for 8GB VRAM tractability); LoRA rank vs. prior preservation (rank 32 preserves grammar but limits adaptation); Fixed vs. variable initialization (current bias toward single_b2 preference).

- **Failure signatures**: Direct LoRA (no Phase 1) → near 0% success, mean error >100%; 1K scale → 771 severe failures, high policy entropy (1.38 nats); Adapter-only for continuous inputs → 52% parsing failures.

- **First 3 experiments**: 1) Curriculum ablation: Phase 2 only (skip Phase 1) on 10K samples; expect catastrophic failure. 2) Scale threshold detection: Train at 3K, 30K to locate phase transition initiation. 3) Simulator transfer: Apply trained model to PyRK with zero architectural modification; expect >94% success.

## Open Questions the Paper Calls Out

None

## Limitations

- **Simulator fidelity gap**: The framework assumes KOMODO's PWR dynamics faithfully represent real reactor physics, but no evidence is provided that learned policies would transfer to physical hardware without degradation.

- **Curriculum dependency**: The two-phase curriculum appears essential, but the paper does not test whether the same results could be achieved with alternative approaches or hyperparameters.

- **Single-step limitation**: The framework handles only single-step power maneuvers; multi-step planning and long-horizon control are not addressed.

## Confidence

**High confidence**: The documented scaling behavior (26.2% → 92% success, 500× variance collapse, entropy reduction 1.38 → 0.89 nats) is reproducible within the simulated domain given the described methodology.

**Medium confidence**: The cross-simulator transfer to PyRK (>94% success) demonstrates foundation-model-like properties, but real-world generalization remains uncertain.

**Low confidence**: Claims about real-world deployment safety and the framework's ability to handle safety-critical scenarios beyond the tested maneuvering task.

## Next Checks

1. **Simulator-to-hardware validation**: Deploy the trained 100K model on a high-fidelity digital twin with different physics (e.g., RELAP5 or MCNP-based) and then on a scaled experimental facility to measure degradation in success rate and identify transfer gaps.

2. **Multi-step curriculum extension**: Extend the framework to handle sequences of power targets by adding temporal context to the input schema and testing whether the learned grammar composes for sequential control.

3. **Adversarial robustness testing**: Generate worst-case scenarios by optimizing input distributions to minimize success rate and measure the model's tail-risk performance under stress tests.