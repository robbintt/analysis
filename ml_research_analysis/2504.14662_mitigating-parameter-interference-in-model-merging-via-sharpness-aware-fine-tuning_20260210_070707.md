---
ver: rpa2
title: Mitigating Parameter Interference in Model Merging via Sharpness-Aware Fine-Tuning
arxiv_id: '2504.14662'
source_url: https://arxiv.org/abs/2504.14662
tags:
- task
- block
- performance
- merging
- fine-tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of parameter interference in
  model merging, where combining task-specific models leads to performance degradation
  due to conflicting parameters. The authors propose sharpness-aware fine-tuning (SAFT),
  which optimizes for both task performance and reduced parameter interference by
  finding flat minima in the loss landscape.
---

# Mitigating Parameter Interference in Model Merging via Sharpness-Aware Fine-Tuning

## Quick Facts
- arXiv ID: 2504.14662
- Source URL: https://arxiv.org/abs/2504.14662
- Authors: Yeoreum Lee; Jinwook Jung; Sungyong Baik
- Reference count: 40
- Primary result: SAFT achieves up to 30% improvements in normalized accuracy when merging models by reducing parameter interference

## Executive Summary
This paper addresses the critical challenge of parameter interference in model merging, where combining task-specific models leads to performance degradation due to conflicting parameters. The authors propose Sharpness-Aware Fine-Tuning (SAFT) as a solution that optimizes for both task performance and reduced parameter interference by finding flat minima in the loss landscape. SAFT is inspired by the observation that its objective function aligns with the goals of model merging. Experimental results demonstrate that SAFT significantly improves merged model performance across various merging methods and fine-tuning approaches, with up to 30% improvements in normalized accuracy. The method also shows better weight disentanglement and cross-task linearity, confirming its effectiveness in reducing parameter interference.

## Method Summary
The authors propose Sharpness-Aware Fine-Tuning (SAFT) as a novel approach to mitigate parameter interference during model merging. SAFT optimizes for both task performance and reduced parameter interference by finding flat minima in the loss landscape. The key insight is that SAFT's objective function naturally aligns with the goals of model merging - finding parameters that perform well across tasks while maintaining stability. The method works by simultaneously considering the sharpness of the loss landscape and the performance across merged tasks during fine-tuning. SAFT can be applied to various model merging scenarios and integrates with existing merging techniques. The approach involves computing gradients that account for both the loss value and its sensitivity to parameter changes, effectively encouraging parameters that generalize well across tasks.

## Key Results
- SAFT achieves up to 30% improvements in normalized accuracy when merging models
- The method demonstrates better weight disentanglement and cross-task linearity compared to standard fine-tuning approaches
- SAFT maintains consistent performance gains across different merging methods and fine-tuning approaches
- Theoretical analysis shows that SAFT induces joint-task loss linearity, ensuring smaller performance gaps between merged and task-specific models

## Why This Works (Mechanism)
SAFT works by finding flat minima in the loss landscape that correspond to parameters that perform well across multiple tasks while being robust to perturbations. The mechanism exploits the relationship between loss sharpness and generalization - parameters in flat minima tend to generalize better across tasks. By incorporating sharpness awareness into the fine-tuning process, SAFT naturally encourages parameters that are less likely to interfere with each other when models are merged. The method effectively trades off between minimizing the loss and minimizing the sensitivity of the loss to parameter changes, leading to more stable and generalizable merged models.

## Foundational Learning
**Loss Landscape Sharpness** - Why needed: Understanding how parameter sensitivity affects generalization and model merging. Quick check: Verify that flatter minima correlate with better cross-task performance in simple experiments.

**Parameter Interference** - Why needed: Recognizing how conflicting parameter updates during merging degrade performance. Quick check: Demonstrate performance degradation when naively merging models trained on different tasks.

**Joint-Task Optimization** - Why needed: Understanding how to optimize for multiple tasks simultaneously during fine-tuning. Quick check: Compare single-task vs. joint-task fine-tuning performance on merged models.

**Weight Disentanglement** - Why needed: Measuring how well parameters separate task-specific features. Quick check: Analyze parameter similarity matrices before and after SAFT.

## Architecture Onboarding

**Component Map:** Pre-trained task models -> SAFT fine-tuning -> Merged model -> Evaluation

**Critical Path:** The most critical path is the SAFT fine-tuning stage, where the balance between loss minimization and sharpness awareness is determined. This stage directly impacts the quality of the merged model.

**Design Tradeoffs:** SAFT trades computational overhead during fine-tuning for improved merged model performance. The method must balance between finding flat minima and maintaining task-specific performance, which may require careful hyperparameter tuning.

**Failure Signatures:** SAFT may fail when the loss landscape is too complex for sharpness-based optimization to find meaningful flat regions, or when tasks are fundamentally incompatible and cannot be jointly optimized effectively.

**First Experiments:**
1. Test SAFT on simple synthetic tasks to verify the sharpness-generalization relationship
2. Compare SAFT performance against standard fine-tuning on a small set of similar NLP tasks
3. Evaluate the computational overhead of SAFT during fine-tuning on a medium-sized model

## Open Questions the Paper Calls Out
None specified in the provided information.

## Limitations
- Evaluation focuses primarily on NLP tasks and may not generalize to other domains
- Theoretical analysis relies on assumptions about loss landscape geometry that may not hold in practice
- Computational overhead of SAFT during fine-tuning is not thoroughly quantified
- Does not address potential overfitting risks when SAFT is applied to very small datasets

## Confidence

**High Confidence:** Experimental results demonstrating SAFT's effectiveness in reducing parameter interference and improving merged model performance across different merging methods are well-supported by the data.

**Medium Confidence:** The theoretical analysis linking SAFT's objective to joint-task loss linearity is mathematically sound but relies on assumptions that may not always hold in practice.

**Medium Confidence:** The claim of up to 30% improvements in normalized accuracy is supported by experiments, but the variability across different task combinations and model architectures needs further investigation.

## Next Checks

1. **Cross-domain Validation:** Test SAFT's effectiveness on non-NLP tasks (e.g., computer vision, speech recognition) to verify generalizability beyond the current NLP-focused experiments.

2. **Scalability Analysis:** Evaluate SAFT's computational overhead and performance gains when merging more than two models simultaneously, and assess its impact on very large language models (e.g., >100B parameters).

3. **Ablation Studies on Loss Landscape Assumptions:** Conduct controlled experiments to test the validity of the theoretical assumptions about loss landscape geometry and their impact on SAFT's performance across different model architectures and task combinations.