---
ver: rpa2
title: 'QSpark: Towards Reliable Qiskit Code Generation'
arxiv_id: '2507.12642'
source_url: https://arxiv.org/abs/2507.12642
tags:
- quantum
- code
- qiskit
- grpo
- orpo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of generating reliable Qiskit
  quantum code using large language models. The authors fine-tune the Qwen2.5-Coder-32B
  model using two reinforcement learning methods, Group Relative Policy Optimization
  (GRPO) and Odds-Ratio Preference Optimization (ORPO), on a curated dataset of 522
  quantum programming tasks.
---

# QSpark: Towards Reliable Qiskit Code Generation

## Quick Facts
- arXiv ID: 2507.12642
- Source URL: https://arxiv.org/abs/2507.12642
- Reference count: 33
- Primary result: ORPO fine-tuning of Qwen2.5-Coder-32B achieves 56.29% Pass@1 on Qiskit HumanEval, outperforming specialized baseline by ~10pp

## Executive Summary
This paper addresses the challenge of generating reliable Qiskit quantum code using large language models. The authors fine-tune the Qwen2.5-Coder-32B model using two reinforcement learning methods, Group Relative Policy Optimization (GRPO) and Odds-Ratio Preference Optimization (ORPO), on a curated dataset of 522 quantum programming tasks. ORPO achieves a Pass@1 accuracy of 56.29% on the Qiskit HumanEval benchmark, outperforming the specialized Granite-8B-QK baseline by approximately 10 percentage points. Both models show strong performance on basic and intermediate tasks but struggle with advanced quantum programming challenges, highlighting the need for further advancements in AI-assisted quantum software development.

## Method Summary
The method involves fine-tuning Qwen2.5-Coder-32B using two reinforcement learning approaches on a curated dataset of 522 Qiskit tasks. ORPO optimizes the odds ratio of preferred outputs against rejected ones, while GRPO uses group-relative ranking with execution-based rewards. The training data was filtered from ~10,819 raw functions, with tasks stratified by difficulty (Basic: 259, Intermediate: 223, Advanced: 40). Evaluation used the Qiskit HumanEval benchmark with Pass@1 metric.

## Key Results
- ORPO achieves 56.29% Pass@1 on Qiskit HumanEval, outperforming Granite-8B-QK by ~10pp
- GRPO reaches 49% Pass@1 accuracy on the same benchmark
- Both models excel on basic (44/78) and intermediate tasks but fail all advanced tasks (0/5)
- The curated dataset reduced from 10,819 raw functions to 522 validated tasks

## Why This Works (Mechanism)

### Mechanism 1: ORPO Preference Optimization
ORPO improves code reliability by directly optimizing the likelihood ratio of preferred outputs against rejected ones. The training loss maximizes the odds of generating "chosen" code samples while minimizing odds of "rejected" samples, regularized by KL divergence. This functions as a style and correctness aligner. The mechanism assumes the preference dataset accurately distinguishes superior quantum code from inferior code. Evidence shows ORPO achieves highest pass count on intermediate tasks (56.29% Pass@1).

### Mechanism 2: GRPO Relative Ranking
GRPO drives functional correctness by ranking groups of generated candidates against each other using execution-based rewards. The model generates multiple candidates for a prompt, each scored using unit test pass rates, circuit depth, and qubit count. The policy is updated using relative advantage within the group. The core assumption is that this reward function correlates strongly with actual program utility. Evidence shows GRPO performs well on basic tasks (44/78) but fails all advanced tasks.

### Mechanism 3: Curriculum Data Filtering
Aggressive data filtering and difficulty stratification create a curriculum allowing the model to generalize from scarce quantum code examples. Raw code undergoes validation and deduplication, with tasks bucketed by difficulty based on circuit depth and entanglement features. The assumption is that features like circuit depth and entanglement presence are valid proxies for task difficulty. Evidence shows approximately 10,819 raw functions were reduced to 522 validated tasks.

## Foundational Learning

- **Preference Optimization (DPO/ORPO)**: Why needed - moves beyond standard SFT to frame code generation as preference ranking. Quick check - How does ORPO differ from RLHF regarding the need for a separate reward model?
- **Quantum Circuit Metrics (Depth & Qubits)**: Why needed - GRPO reward function penalizes these metrics. Quick check - In GRPO reward formula, why is unit test pass rate weighted higher (0.7) than depth penalty (0.2)?
- **Pass@k Metric**: Why needed - evaluation relies on this strict reliability measure. Quick check - Why is Pass@1 more critical for a "reliable" coding assistant than Pass@10?

## Architecture Onboarding

- **Component map**: Base Model -> Data Engine -> RL Engines (ORPO/GRPO) -> Evaluator
- **Critical path**: GRPO reward construction is most fragile, requiring execution of untrusted LLM-generated code in Qiskit Aer simulator to form gradient signal
- **Design tradeoffs**: GRPO offers execution-grounded feedback but suffers from high variance; ORPO offers stable convergence but relies on quality of static preference pairs
- **Failure signatures**: Advanced Task Collapse (0/5 success), potential reward hacking in GRPO
- **First 3 experiments**:
  1. Sanity Check: Reproduce Pass@1 score (56.29%) on public QHE dataset
  2. Reward Ablation (GRPO): Modify reward weights to verify mechanism
  3. Advanced Task Analysis: Manually inspect model outputs on 5 advanced tasks

## Open Questions the Paper Calls Out

- **Open Question 1**: Can integrating GRPO and ORPO into a unified reward framework yield higher reliability than independent training? The authors state this as a future aim, as current models treated strategies independently.
- **Open Question 2**: What training modifications are required to solve advanced quantum programming tasks? The authors note that novel strategies like curriculum learning may be needed, as current models fail all advanced tasks.
- **Open Question 3**: Does expanding training dataset to include error correction and hardware-specific optimizations improve generalization? The authors identify current dataset as relatively small and propose broader data inclusion.

## Limitations

- Training set of 522 curated tasks is extremely small for 32B-parameter model, potentially limiting generalization
- Qiskit HumanEval benchmark may not fully capture real-world quantum software development complexity
- Baseline comparison with Granite-8B-QK is limited; broader comparison would strengthen claims

## Confidence

- **High Confidence**: Experimental methodology is well-specified and reproducible; ORPO 56.29% Pass@1 claim is supported
- **Medium Confidence**: ORPO outperforms GRPO by ~7pp is supported but warrants caution due to variance and small sample size
- **Low Confidence**: Claim about struggling with advanced tasks is based on 0/5 performance, but task complexity and representativeness is unclear

## Next Checks

1. Evaluate QSpark against other quantum code generation models on shared benchmark to contextualize improvements
2. Apply QSpark to practical quantum programming tasks to assess real-world utility beyond synthetic benchmarks
3. Conduct detailed error analysis of model outputs on advanced tasks to determine failure modes (data, capacity, or reward signal limitations)