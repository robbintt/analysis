---
ver: rpa2
title: UloRL:An Ultra-Long Output Reinforcement Learning Approach for Advancing Large
  Language Models' Reasoning Abilities
arxiv_id: '2507.19766'
source_url: https://arxiv.org/abs/2507.19766
tags:
- training
- entropy
- segment
- arxiv
- output
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of efficiently training large
  language models with ultra-long outputs for enhanced reasoning capabilities. The
  proposed Ultra-Long Output Reinforcement Learning (UloRL) approach tackles inefficiencies
  caused by long-tail sequence distributions and entropy collapse during training.
---

# UloRL:An Ultra-Long Output Reinforcement Learning Approach for Advancing Large Language Models' Reasoning Abilities

## Quick Facts
- arXiv ID: 2507.19766
- Source URL: https://arxiv.org/abs/2507.19766
- Authors: Dong Du; Shulin Liu; Tao Yang; Shaohua Chen; Yang Li
- Reference count: 6
- Primary result: RL with 128k-token outputs improves AIME2025 performance from 70.9% to 85.1% and BeyondAIME from 50.7% to 61.9%

## Executive Summary
UloRL addresses the challenge of efficiently training large language models with ultra-long outputs for enhanced reasoning capabilities. The approach introduces segment rollout for efficient training, dynamic masking of well-Mastered Positive Tokens (DMMPTs) to prevent entropy collapse, and a generative verifier model for accurate reward computation. Experimental results on Qwen3-30B-A3B demonstrate significant performance gains: RL with 128k-token outputs improves AIME2025 performance from 70.9% to 85.1% and BeyondAIME from 50.7% to 61.9%, surpassing even larger models like Qwen3-235B-A22B.

## Method Summary
UloRL is built on the verl framework and extends GRPO/DAPO with three key innovations for ultra-long output training. The segment rollout mechanism divides ultra-long decoding (e.g., 128k tokens) into shorter segments (e.g., 8 × 16k), enabling efficient training by mitigating delays from long-tail samples. Dynamic masking of well-Mastered Positive Tokens (DMMPTs) prevents entropy collapse by masking tokens with predicted probability ≥ 0.99 in positive samples only when current entropy falls below target entropy 0.2. A generative verifier model evaluates semantic equivalence between predicted and reference answers to improve reward accuracy. The method uses Pseudo On-Policy Importance Sampling (POIS), binary rewards {0,1}, and follows DAPO's dynamic sampling and token-level gradient loss approach.

## Key Results
- RL with 128k-token outputs improves AIME2025 performance from 70.9% to 85.1% and BeyondAIME from 50.7% to 61.9%
- Segment rollout yields 2.06x training speedup compared to baseline
- UloRL-trained Qwen3-30B-A3B outperforms larger Qwen3-235B-A22B on reasoning benchmarks
- Entropy collapse prevention through DMMPTs maintains policy diversity during training

## Why This Works (Mechanism)

### Mechanism 1: Segment Rollout for Efficient Training
- Claim: Segment rollouts enable efficient RL training with ultra-long outputs by avoiding bottlenecks from long-tail sequence lengths.
- Mechanism: Divides decoding (e.g., 128k tokens) into multiple shorter segments (e.g., 8 × 16k). Completed samples immediately enter the experience pool for training while unfinished samples continue decoding in subsequent iterations.
- Core assumption: Most samples complete before global max length; only a fraction cause the long-tail bottleneck.
- Evidence anchors: Segment count 4 yields 2.06x speed vs. 1.0x baseline; abstract states segment rollout mitigates delays from long-tail samples.
- Break condition: If most samples hit global max length, segment benefits diminish; overhead from segment management may dominate.

### Mechanism 2: Dynamic Masking of Well-Mastered Positive Tokens (DMMPTs)
- Claim: Dynamic masking of well-Mastered Positive Tokens (DMMPTs) prevents entropy collapse without introducing auxiliary entropy loss objectives.
- Mechanism: Tokens in positive samples with predicted probability ≥ τ (0.99) are masked only when current entropy falls below target σ (0.2). This stops overtraining on already-mastered tokens while preserving learning on uncertain regions.
- Core assumption: Entropy collapse is primarily caused by gradient updates on high-confidence tokens in correct samples.
- Evidence anchors: Masking MPTs leads to increasing entropy vs. decreasing entropy in baseline; abstract states DMMPTs prevent entropy collapse.
- Break condition: If τ is too low, excessive masking may undertrain on genuinely useful tokens; if σ is misaligned with task complexity, entropy may oscillate or remain unstable.

### Mechanism 3: Generative Verifier Model for Reward Accuracy
- Claim: Generative verifier models improve reward signal accuracy for RLVR by handling semantic equivalence beyond rule-based comparison.
- Mechanism: A generative model evaluates whether predicted and reference answers are semantically equivalent (e.g., "27cm" vs. "0.27m"), reducing false negatives in reward assignment.
- Core assumption: The verifier model itself is sufficiently accurate and does not introduce systematic bias.
- Evidence anchors: Abstract states generative verifier model for accurate reward computation; generative verifiers referenced in related work.
- Break condition: If verifier accuracy is low or biased, reward noise may destabilize training; verifier calibration is critical.

## Foundational Learning

- Concept: PPO/GRPO clipped surrogate objectives and importance sampling
  - Why needed here: UloRL builds on GRPO/DAPO foundations; understanding importance sampling ratios is required to adapt to segment rollouts (SAIS, POIS).
  - Quick check question: Can you explain why importance sampling weights diverge from 1 in off-policy RL and how clipping affects gradient flow?

- Concept: Entropy as a diversity signal in policy gradient methods
  - Why needed here: DMMPTs explicitly manages entropy to balance exploration/exploitation; misinterpreting entropy collapse can lead to incorrect interventions.
  - Quick check question: What happens to policy diversity and sample efficiency if entropy collapses too early in training?

- Concept: RL with verifiable rewards (RLVR) and outcome-based reward models
  - Why needed here: UloRL uses binary outcome rewards with a generative verifier; understanding reward sparsity and credit assignment is essential.
  - Quick check question: How does sparse binary reward (0/1) affect variance in advantage estimation compared to dense shaping rewards?

## Architecture Onboarding

- Component map: Prompt batch → segment rollout (max 8 × 16k) → experience pool → POIS loss computation with DMMPTs masking → model update → repeat

- Critical path: Segment rollout engine manages unfinished/experience pools, coordinates multi-segment decoding; importance sampling module computes SAIS or POIS; DMMPTs masker evaluates entropy and applies conditional masking; generative verifier produces binary equivalence judgments.

- Design tradeoffs:
  - Segment count vs. staleness: more segments increase speed but introduce off-policy drift
  - τ threshold vs. learning capacity: higher τ preserves more tokens but may not prevent collapse; lower τ risks undertraining
  - Verifier complexity vs. latency: generative verifier improves accuracy but adds inference overhead

- Failure signatures:
  1. Entropy continuously increasing → DMMPTs not triggering (σ too high or τ too restrictive)
  2. Entropy collapsing → DMMPTs not activating (σ too low or threshold τ too high)
  3. Training instability after segment changes → importance sampling mismatch; verify POIS/SAIS consistency
  4. Reward noise spikes → verifier misclassifications; inspect verifier calibration on error cases

- First 3 experiments:
  1. Ablate segment count (1, 2, 4, 8) on 64k output; measure training speed and final accuracy to confirm speedup without degradation.
  2. Ablate DMMPTs (disabled vs. enabled) with entropy logging; confirm entropy stability and performance delta on AIME2025/BeyondAIME.
  3. Compare rule-based vs. generative verifier on held-out equivalence pairs (e.g., unit conversions, fraction formats); quantify false positive/negative rates and downstream reward noise.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is there a saturation point for reasoning performance relative to output length, or can capabilities scale indefinitely with context size?
- Basis in paper: The paper notes that extending output to 140k using Yarn improved results, stating "continuously expanding the length can further enhance the model's reasoning ability."
- Why unresolved: The experiments only validate up to 140k tokens; the long-term scaling laws or potential plateaus for this specific RL approach remain unexplored.
- What evidence would resolve it: Training runs with output limits significantly exceeding 140k to observe if performance gains diminish or if error propagation degrades utility.

### Open Question 2
- Question: How does the ratio of "pseudo on-policy" data (old segments) to "true on-policy" data impact the theoretical convergence of the Pseudo On-Policy Importance Sampling (POIS) method?
- Basis in paper: The paper admits that under POIS, only the last segment is "true on-policy" while previous segments are technically off-policy but treated as on-policy to stabilize entropy.
- Why unresolved: While POIS outperformed baselines, the paper does not analyze if high segment counts (which increase the ratio of pseudo to true data) eventually introduce policy degradation or drift.
- What evidence would resolve it: Ablation studies varying the segment count while keeping total length constant to measure the sensitivity of policy convergence.

### Open Question 3
- Question: To what extent does the Generative Verifier model introduce "reward hacking" compared to rule-based systems?
- Basis in paper: The authors replace rule-based verification with a generative model to handle complex equivalence (e.g., "27cm" vs "0.27m"), trading deterministic rules for model-based judgments.
- Why unresolved: While the verifier improves robustness, the paper does not analyze if the RL policy exploits the verifier's own biases or failure modes to generate plausible-looking but incorrect reasoning.
- What evidence would resolve it: An analysis of false positive rates from the verifier on adversarial examples generated by the trained RL model.

## Limitations

- The generative verifier model's architecture, training procedure, and validation are not detailed, creating uncertainty about reward signal quality and potential bias.
- Experimental results are limited to Qwen3-30B-A3B on specific benchmarks, with unclear generalization to other model families or reasoning domains.
- The entropy collapse prevention mechanism relies on precise hyperparameter tuning (τ=0.99, σ=0.2) without sensitivity analysis or adaptation guidance for different tasks.

## Confidence

**High Confidence**: The segment rollout mechanism for improving training efficiency is well-supported by both theoretical reasoning and experimental results (2.06x speedup demonstrated).

**Medium Confidence**: The DMMPTs mechanism for preventing entropy collapse is plausible and supported by entropy monitoring plots, but the specific threshold choices and their robustness across different tasks are not thoroughly validated.

**Low Confidence**: The generative verifier's contribution to improved performance is stated as a key innovation but lacks independent validation. Without detailed architecture, training data, or error analysis, it's difficult to assess whether performance gains stem from better reward signals or other factors.

## Next Checks

1. **Verifier Calibration Study**: Evaluate the generative verifier on held-out answer equivalence pairs (unit conversions, equivalent expressions, format variations) to quantify false positive/negative rates and their correlation with downstream reward noise and training stability.

2. **Cross-Model Generalization**: Apply UloRL to at least two additional model families (different architectures or training approaches) on the same benchmarks to verify that performance improvements are not specific to Qwen3-30B-A3B's particular characteristics.

3. **Hyperparameter Sensitivity Analysis**: Systematically vary DMMPTs thresholds (τ from 0.95 to 0.999, σ from 0.1 to 0.3) and segment counts to map the stability landscape and identify robust operating regions across different reasoning task complexities.