---
ver: rpa2
title: Leveraging Submodule Linearity Enhances Task Arithmetic Performance in LLMs
arxiv_id: '2504.10902'
source_url: https://arxiv.org/abs/2504.10902
tags:
- merging
- linearity
- task
- level
- conference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel approach to improve model merging performance
  in large language models (LLMs) by leveraging the inherent linearity present in
  submodules. While previous methods focus on global linearization through retraining,
  the authors demonstrate that submodules such as layers, self-attentions, and MLPs
  exhibit significantly higher linearity than full models.
---

# Leveraging Submodule Linearity Enhances Task Arithmetic Performance in LLMs

## Quick Facts
- arXiv ID: 2504.10902
- Source URL: https://arxiv.org/abs/2504.10902
- Reference count: 40
- Primary result: Submodule-level merging outperforms standard task arithmetic by up to 3% in multi-task settings

## Executive Summary
This paper presents a novel approach to improve model merging performance in large language models (LLMs) by leveraging the inherent linearity present in submodules. While previous methods focus on global linearization through retraining, the authors demonstrate that submodules such as layers, self-attentions, and MLPs exhibit significantly higher linearity than full models. Based on this observation, they propose decomposing models into these submodules and computing optimal merging weights using a closed-form solution derived from linear properties. The method requires minimal data (only 30 samples per task) and no additional training. Experimental results show that this approach consistently outperforms standard task arithmetic and other baselines across different model scales (Llama-2-7B and Llama-2-13B) and various tasks (Math, Coding, and Translate), with improvements of up to 3% in multi-task settings.

## Method Summary
The method works by first validating that submodules (layers, self-attentions, MLPs) exhibit significantly higher linearity than the overall model through a Non-linearity Score metric. It then decomposes the model into these submodules and computes optimal merging weights using a closed-form solution derived from linear properties. The approach requires collecting intermediate features from a pre-trained model on calibration data, computing feature differences between fine-tuned models, and solving a linear system to find optimal weights for each submodule independently. This allows different weights for different parts of the network, reducing interference between tasks.

## Key Results
- Submodules exhibit significantly higher linearity than full models (Figure 2)
- Attn/MLP level granularity consistently outperforms or matches Layer level merging
- 30 samples are sufficient for weight computation with minimal data requirements
- Improvements of up to 3% in multi-task settings compared to standard task arithmetic

## Why This Works (Mechanism)

### Mechanism 1: Submodule Linearity
The paper demonstrates that fine-tuned submodules (layers, attention blocks, MLPs) satisfy a linear relationship between weight changes and feature changes significantly better than the full model stack. The Non-linearity Score measures deviation from ideal linear mapping between weight deltas and feature deltas, showing that shallow or localized submodules approximate this relationship much better than full models.

### Mechanism 2: Closed-Form Weight Solution
By assuming linearity at the submodule level, the optimal merging weights can be computed via a closed-form solution rather than grid search. The method minimizes the L2 distance between merged module's output and target task modules' outputs, reducing to a linear system solvable via matrix inversion.

### Mechanism 3: Task-Specific Submodule Weighting
Decomposing the merging process allows different weights for different parts of the network, effectively isolating task-specific knowledge regions. This enables, for example, an MLP to merge differently than an Attention head, reducing interference between tasks.

## Foundational Learning

- **Concept: Task Arithmetic**
  - Why needed here: This is the baseline operation being optimized
  - Quick check question: How does the paper define a "task vector" τ?

- **Concept: Linearity vs. Non-linearity in Neural Networks**
  - Why needed here: The paper redefines "linearity" as correlation between weight deltas and feature deltas
  - Quick check question: Does "linear model" mean no activation functions, or that Δf ∝ Δθ?

- **Concept: Closed-Form Solutions (Least Squares)**
  - Why needed here: The method avoids hyperparameter search by solving a linear system
  - Quick check question: Why does linearity allow converting minimization to linear algebra solution?

## Architecture Onboarding

- **Component map:**
  Pre-trained model -> Submodule Decomposer -> Feature Collector -> Linear Solver -> Merger

- **Critical path:**
  1. Extract intermediate features for calibration data using the pre-trained model
  2. Calculate feature differences (Δf) for each submodule against each fine-tuned model
  3. Construct correlation matrix A and vector b, solve for α weights
  4. Apply specific α to reconstruct the final model

- **Design tradeoffs:**
  - Granularity: "Attn/MLP Level" offers best stability; "Head Level" is unstable; "Layer Level" is a strong baseline
  - Data Efficiency: 30 samples are sufficient but rely on representative calibration data

- **Failure signatures:**
  - Head-level Collapse: Merging at "Head/MLP" level causes high variance and performance drops
  - Feature Bias: Discrepancies in token length/distribution across tasks can bias the solver

- **First 3 experiments:**
  1. Reproduce Figure 2's Non-linearity Score analysis on your specific model
  2. Compare "Layer Level" vs. "Attn/MLP Level" merging on a 2-task merge
  3. Test merging performance with 3, 10, and 30 calibration samples

## Open Questions the Paper Calls Out

### Open Question 1
How can the merging objective be redesigned to effectively leverage linearity at the individual attention head level, given that current averaging methods lead to functional collapse? The authors note that Head/MLP level merging yields unstable results due to asking single heads to average features from heads with distinct functional specializations.

### Open Question 2
Can the incorporation of second-order information improve the estimation of linearity or the derivation of optimal merging weights? Appendix A.5.3 states that considering second-order information is a "very meaningful direction" to refine linearity definitions and weight formulas.

### Open Question 3
What is the relationship between specific activation functions (e.g., SwiGLU vs. ReLU) and the "fine-tuning linearity" property distinct from traditional linearity? Appendix A.5.6 notes that while the link between activation functions and traditional linearity is known, their connection to this specific linearity property "still awaits exploration."

### Open Question 4
What underlying factors cause the distinct "sigmoid-like" non-linearity profile observed in Qwen-2.5-7B compared to other models like Llama? Appendix A.5.9 highlights an anomaly where Qwen-2.5-7B exhibits a steep, non-linear decline in feature distance during interpolation.

## Limitations
- The linearity assumption may not hold for models with different architectural properties or fine-tuning approaches involving significant weight changes
- The method requires representative calibration data, and the sufficiency of 30 samples may vary with task complexity
- The approach optimizes in intermediate feature space rather than output space, with unexplored implications on final task performance

## Confidence

- **High Confidence**: Empirical demonstration of submodule linearity and consistent outperformance over standard Task Arithmetic
- **Medium Confidence**: Effectiveness of closed-form solution across different granularities (Layer vs. Attn/MLP level)
- **Low Confidence**: Generalizability to other model families beyond Llama-2 and robustness to different fine-tuning paradigms

## Next Checks

1. **Linearity Validation**: Reproduce Figure 2's Non-linearity Score analysis on your specific model architecture to verify submodule linearity before implementing the solver.

2. **Granularity Benchmark**: Compare "Layer Level" vs. "Attn/MLP Level" merging performance on a simple 2-task merge to identify the most stable granularity for your use case.

3. **Data Sensitivity Test**: Systematically evaluate merging performance with 3, 10, and 30 calibration samples to determine the minimum data requirement for your specific task domains.