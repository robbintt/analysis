---
ver: rpa2
title: 'FlexSED: Towards Open-Vocabulary Sound Event Detection'
arxiv_id: '2509.18606'
source_url: https://arxiv.org/abs/2509.18606
tags:
- sound
- audio
- classes
- flexsed
- event
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FlexSED addresses the challenge of open-vocabulary sound event
  detection by integrating a pretrained audio SSL model with a CLAP text encoder and
  introducing an encoder-decoder architecture with adaptive fusion for efficient prompt-audio
  integration. To mitigate missing label issues, it employs LLM-guided negative query
  filtering during training.
---

# FlexSED: Towards Open-Vocabulary Sound Event Detection

## Quick Facts
- arXiv ID: 2509.18606
- Source URL: https://arxiv.org/abs/2509.18606
- Authors: Jiarui Hai; Helin Wang; Weizhe Guo; Mounya Elhilali
- Reference count: 32
- Primary result: Achieves PSDS1A of 0.4484 and PSDS1T of 0.5863 on AudioSet-Strong, with strong zero-shot and few-shot capabilities

## Executive Summary
FlexSED addresses the challenge of open-vocabulary sound event detection by integrating a pretrained audio SSL model with a CLAP text encoder and introducing an encoder-decoder architecture with adaptive fusion for efficient prompt-audio integration. To mitigate missing label issues, it employs LLM-guided negative query filtering during training. The method achieves superior performance compared to vanilla SED models on AudioSet-Strong, with PSDS1A of 0.4484 and PSDS1T of 0.5863. FlexSED also demonstrates strong zero-shot and few-shot capabilities, retaining up to 88% of full performance in zero-shot settings for temporal localization and reaching near full performance with as few as 20-shot fine-tuning.

## Method Summary
FlexSED integrates Dasheng, a pretrained audio SSL model, with a frozen CLAP text encoder through an encoder-decoder architecture. The audio encoder processes input spectrograms independently, while the decoder fuses cached encoder features with text prompt embeddings using AdaLN-One adaptive normalization. This enables efficient multi-query inference and stable continued training from pretrained weights. During training, GPT-4 assists in filtering negative queries to address missing label issues in AudioSet-Strong, preferentially selecting "safe negatives" unlikely to co-occur with positive labels.

## Key Results
- Achieves PSDS1A of 0.4484 and PSDS1T of 0.5863 on AudioSet-Strong
- Demonstrates strong zero-shot performance with 65-88% retention of full performance
- Reaches near full performance with as few as 20-shot fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
- AdaLN-One fusion enables stable continued training from pretrained audio SSL weights while learning text-audio alignment. Adaptive Layer Normalization modulates audio features using text prompt embeddings through learned scale (γ), shift (β), and gate (g) parameters. Initializing g=1 preserves the original SSL feature flow at initialization, then gradually learns prompt conditioning.

### Mechanism 2
- Encoder-decoder split enables efficient multi-query inference by caching audio features. The audio encoder processes the input once independently of text prompts. The decoder then fuses cached encoder features with each candidate text prompt. This avoids recomputing audio representations for every sound class query.

### Mechanism 3
- LLM-guided negative query filtering reduces label noise from missing annotations in AudioSet-Strong. GPT-4 identifies semantic relationships among sound classes. Negative training samples are preferentially drawn from "safe negatives"—classes unlikely to co-occur with positive labels—rather than random unlabeled classes.

## Foundational Learning

- Concept: Self-Supervised Learning (SSL) for Audio
  - Why needed here: FlexSED builds on Dasheng, an audio SSL model pretrained via masked prediction. Understanding SSL transfer helps explain why AdaLN-One initialization matters—disrupting pretrained weights hurts convergence.
  - Quick check question: Can you explain why initializing the AdaLN gate to 1 preserves the pretrained audio model's behavior at training start?

- Concept: Contrastive Language-Audio Pretraining (CLAP)
  - Why needed here: The frozen CLAP text encoder provides semantically aligned text embeddings. Understanding CLAP's training objective clarifies why its embeddings support zero-shot generalization to unseen sound class names.
  - Quick check question: Why does CLAP enable zero-shot detection on classes not seen during FlexSED training?

- Concept: Sound Event Detection Metrics (PSDS)
  - Why needed here: The paper reports PSDS1 A (all classes) and PSDS1 T (target-only). PSDS1 emphasizes temporal precision. Understanding these metrics is essential for interpreting the 0.4484 / 0.5863 scores and zero-shot retention rates.
  - Quick check question: Why does PSDS1 T retention (88%) exceed PSDS1 A retention (65%) in zero-shot settings?

## Architecture Onboarding

- Component map: Audio spectrogram → Encoder (cached) → Decoder + CLAP text embedding (via AdaLN) → Frame-level sigmoid output per prompt. Negative query filtering via precomputed GPT-4 safe-negative list.
- Critical path: Audio spectrogram → Encoder (cached) → Decoder + CLAP text embedding (via AdaLN) → Frame-level sigmoid output per prompt. Negative query filtering via precomputed GPT-4 safe-negative list.
- Design tradeoffs: Encoder/Decoder split: 6/6 optimal in ablations. Fewer decoder blocks limit text-audio fusion; fewer encoder blocks hurt acoustic quality. AdaLN-One vs. AdaLN-Zero: One preserves pretrained features at init; Zero requires learning from scratch. Frozen CLAP vs. fine-tuned: Paper freezes CLAP. Assumption: CLAP's pretrained alignment is sufficient; fine-tuning risks overfitting to AudioSet vocabulary.
- Failure signatures: High false positive rate on PSDS1 A but not PSDS1 T: Model struggles with open-set classification (distinguishing presence/absence across 407 classes) but localizes known sounds accurately. Performance collapse early in training: Check AdaLN gate initialization—if gates start near 0 instead of 1, pretrained features are suppressed. Zero-shot failure on semantically distant classes: CLAP embeddings may not cover unusual sound descriptions; test with out-of-distribution queries.
- First 3 experiments: 1) Reproduce baseline comparison: Train Dasheng-SED (vanilla multi-class) on AudioSet-Strong and compare PSDS1 scores to FlexSED to validate claimed improvement margin. 2) Ablate negative filtering: Train FlexSED without GPT-4 filtering (random negative sampling) and verify ~3% PSDS1 A degradation matches paper. 3) Zero-shot holdout test: Exclude 20-40 classes from training, evaluate retention rates on held-out classes, confirm ~65-88% retention range before attempting few-shot fine-tuning.

## Open Questions the Paper Calls Out

- Can knowledge distillation strategies be effectively integrated into the FlexSED framework to further enhance model performance? [explicit] The conclusion explicitly states plans to "explore strategies such as knowledge distillation [6] to further enhance model performance" as a primary direction for future work.
- How can FlexSED be extended to support richer multimodal query interactions beyond text prompts? [explicit] The authors list "multimodal extensions that support richer query interactions beyond text prompts" as a specific avenue for future research in the conclusion.
- What is the impact of scaling training to larger and more diverse audio datasets on the zero-shot generalization of open-vocabulary SED? [explicit] The paper notes the relative smallness of AudioSet-Strong compared to vision datasets and explicitly plans to "scale training to larger and more diverse audio datasets" in the future.
- How can standard SED augmentation techniques like mixup and mean-teacher be adapted to be compatible with the prompt-based FlexSED architecture? [explicit] Footnote 5 states that the authors leave "the integration of mixup and mean-teacher into FlexSED for future work, as they are not directly compatible with the current framework."

## Limitations

- The LLM-guided negative query filtering approach depends on undisclosed GPT-4 parameters and safe-negative lists, making independent validation difficult.
- The encoder-decoder efficiency claim lacks external validation through benchmarking against non-cached alternatives.
- The approach assumes AudioSet-Strong's missing label patterns are universally predictable, which may not hold across datasets with different annotation protocols.

## Confidence

- High Confidence: The AdaLN-One initialization mechanism is well-specified and supported by ablation results. The PSDS1 scores (0.4484/0.5863) and zero-shot retention rates (65-88%) are directly reported with clear metrics.
- Medium Confidence: The encoder-decoder architectural benefits are described clearly but lack corpus validation. The zero-shot and few-shot results are impressive but require external replication to confirm generalizability.
- Low Confidence: The LLM filtering mechanism depends on undisclosed GPT-4 parameters and safe-negative lists. Without these, the negative filtering claims cannot be independently verified.

## Next Checks

1. Validate Negative Filtering Impact: Train FlexSED without GPT-4 filtering (using random negative sampling) and measure the actual PSDS1 A degradation. Compare to the claimed ~3% drop to verify the filtering benefit.
2. Test Dataset Generalizability: Evaluate FlexSED on a non-AudioSet dataset (e.g., ESC-50 or FSD50K) to confirm the LLM filtering approach works beyond AudioSet-Strong's specific annotation patterns.
3. Benchmark Inference Efficiency: Implement and compare a non-cached version of FlexSED to quantify the actual computational savings from the encoder-decoder split during multi-query inference.