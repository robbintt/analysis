---
ver: rpa2
title: Mitigating Hallucinations in Video Large Language Models via Spatiotemporal-Semantic
  Contrastive Decoding
arxiv_id: '2601.22574'
source_url: https://arxiv.org/abs/2601.22574
tags:
- video
- hallucinations
- decoding
- language
- speedometer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method called Spatiotemporal-Semantic Contrastive
  Decoding (SSCD) to mitigate video hallucinations in Video Large Language Models
  (VideoLLMs). The key idea is to construct negative video features by deliberately
  disrupting the spatiotemporal consistency and semantic associations of video features,
  and then use these negative features in a contrastive decoding framework to suppress
  hallucinations during inference.
---

# Mitigating Hallucinations in Video Large Language Models via Spatiotemporal-Semantic Contrastive Decoding

## Quick Facts
- arXiv ID: 2601.22574
- Source URL: https://arxiv.org/abs/2601.22574
- Authors: Yuansheng Gao; Jinman Zhao; Tong Zhang; Xingguo Xu; Han Bao; Zonghui Wang; Wenzhi Chen
- Reference count: 23
- One-line primary result: Proposed method reduces hallucination rates while preserving video understanding capabilities on multiple benchmarks

## Executive Summary
This paper introduces Spatiotemporal-Semantic Contrastive Decoding (SSCD), a method to mitigate hallucinations in Video Large Language Models (VideoLLMs). The approach constructs negative video features by deliberately disrupting spatiotemporal consistency and semantic associations, then uses these features in a contrastive decoding framework during inference. Experiments demonstrate that SSCD effectively reduces hallucination rates across multiple benchmark datasets while maintaining or improving general video understanding and reasoning capabilities.

## Method Summary
SSCD operates by training a lightweight Spatiotemporal-Semantic Disruptor module that takes video features and deliberately degrades their spatiotemporal and semantic coherence. During training, this disruptor is optimized to minimize cycle-consistency scores across temporal spans and conditional mutual information between visual tokens and ground-truth answers. At inference, contrastive decoding compares outputs from original and disrupted features, suppressing hallucination-prone tokens. The method is applied to frozen VideoLLM backbones, making it a lightweight add-on that doesn't require retraining the entire model.

## Key Results
- Achieves 22.9% overall hallucination rate on VideoHallucer, significantly outperforming baseline methods
- Improves ActivityNet-QA accuracy to 41.5%, outperforming all compared methods
- Demonstrates effectiveness across multiple hallucination types including temporal, semantic, and factual hallucinations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Deliberately disrupting spatiotemporal consistency in video features creates hallucination-prone negative features that, when used in contrastive decoding, suppress temporal hallucinations.
- **Mechanism:** Video features are modeled as a spatiotemporal graph where nodes represent frame-level features and edges connect adjacent frames. Random walks compute cycle-consistency scores across temporal spans. Minimizing these scores during disruptor training weakens cross-frame associations, producing features systematically prone to temporal hallucinations.
- **Core assumption:** Hallucinations in VideoLLMs are causally linked to weakened or disrupted spatiotemporal coherence in visual representations; amplifying this disruption in a controlled "negative" feature produces predictable hallucination patterns that can be subtracted during decoding.
- **Evidence anchors:**
  - [abstract]: "constructs negative features by deliberately disrupting the spatiotemporal consistency and semantic associations of video features, and suppresses video hallucinations through contrastive decoding"
  - [Section 4.1]: "to deliberately disrupt cross-frame spatiotemporal consistency, we define a cycle-consistency score... thereby intentionally introducing and amplifying disruptions of spatiotemporal consistency"
  - [corpus]: Related work SEASON (arXiv:2512.04643) similarly uses contrastive decoding for temporal hallucinations, suggesting the contrastive approach is an active research direction; however, corpus lacks direct validation of the random-walk disruption mechanism.
- **Break condition:** If video encoder representations already lack discriminative temporal structure (poor frame-level grounding), the disruptor cannot meaningfully amplify disruption, reducing contrastive signal strength.

### Mechanism 2
- **Claim:** Minimizing conditional mutual information between projected visual tokens and ground-truth answers creates semantically misaligned negative features that induce answer-level hallucinations detectable by contrastive decoding.
- **Mechanism:** The semantic disruption loss maximizes conditional entropy, reducing the probability of predicting correct answers under negative features. This weakens the dependence between visual representations and answer semantics.
- **Core assumption:** Hallucinations arise partly from semantic misalignment between visual tokens and correct linguistic outputs; features engineered to be semantically uninformative will exhibit amplified hallucination patterns.
- **Evidence anchors:**
  - [abstract]: "disrupting... semantic associations of video features, and suppresses video hallucinations through contrastive decoding"
  - [Section 4.2]: "minimizing the mutual information is equivalent to... encouraging the projected visual tokens to provide misleading or uninformative signals for predicting the correct answer"
  - [corpus]: SDCD (arXiv:2601.03500) uses structure disruption for LVLM hallucinations, but corpus does not provide direct evidence for the CMI minimization approach.
- **Break condition:** If the backbone LLM has strong language priors that override visual grounding, semantically disrupted features may not produce distinguishably different output distributions.

### Mechanism 3
- **Claim:** Contrastive decoding with adaptively constrained token filtering suppresses hallucination tokens while preserving fluent generation.
- **Mechanism:** The final sampling distribution computes: Softmax[(1+α)f_original - αf_negative] subject to a plausibility constraint retaining only tokens with p ≥ β·max(p). The negative features amplify hallucination logits; subtracting them suppresses those tokens.
- **Core assumption:** Negative features constructed via spatiotemporal and semantic disruption will assign higher probability to hallucination tokens than original features; the difference isolates hallucination signals.
- **Evidence anchors:**
  - [Section 4.3]: "SSCD is theoretically grounded and leverages random walks and conditional mutual information to characterize and disrupt spatiotemporal and semantic consistency... calibrates the generation distribution using deliberately disrupted negative video features"
  - [Table 1-3]: On VideoHallucer, SSCD achieves 22.9% overall hallucination rate vs. 14.3% baseline; on ActivityNet-QA, 41.5% accuracy vs. 40.9% baseline.
  - [corpus]: "The Mirage of Performance Gains" (arXiv:2504.10020) questions whether contrastive decoding truly mitigates hallucinations, suggesting this mechanism may have limitations not captured in this paper's benchmarks.
- **Break condition:** Excessive contrastive strength (high α) over-penalizes linguistically plausible tokens, degrading fluency; insufficient plausibility threshold (low β) allows spurious suppression of correct tokens.

## Foundational Learning

- **Concept: Contrastive Decoding in Multimodal Models**
  - Why needed here: SSCD's core intervention is a contrastive decoding framework; understanding how opposing distributions calibrate output is essential.
  - Quick check question: Given original logits f_orig and negative logits f_neg, what does increasing the contrastive weight α do to tokens where f_neg > f_orig?

- **Concept: Random Walks on Graphs for Temporal Consistency**
  - Why needed here: The spatiotemporal disruption mechanism uses random walks to compute cycle-consistency scores across frames.
  - Quick check question: In a video with T frames, what does the transition matrix T_k+1_k represent, and how does composing transitions model long-range temporal consistency?

- **Concept: Conditional Mutual Information (CMI)**
  - Why needed here: The semantic disruption loss is derived from minimizing CMI between visual tokens and answers.
  - Quick check question: Why does minimizing I(Z; Y|X) encourage visual tokens to be uninformative about the answer Y?

## Architecture Onboarding

- **Component map:** Video V -> Video Encoder (E_v) -> H_v -> [H_v + M(H_v) = H_neg_v] -> Projector (P) -> Z_v/Z_neg_v -> LLM (θ) -> logits

- **Critical path:**
  1. Training phase: Forward pass original and disrupted features; compute L_T (spatiotemporal disruption) and L_S (semantic disruption); backprop only through M
  2. Inference phase: Generate logits from both original and negative features; apply contrastive decoding with plausibility constraint (β=0.1); sample from calibrated distribution

- **Design tradeoffs:**
  - λ (loss weighting): Higher λ prioritizes semantic disruption but may degrade spatiotemporal signal. Paper uses λ=5
  - α (contrastive strength): 0.8 for Video-LLaVA, 0.4 for LLaVA-NeXT-Video. Higher α increases hallucination suppression but risks fluency loss
  - β (plausibility threshold): 0.1 fixed. Lower values allow more aggressive token suppression but may discard valid tokens

- **Failure signatures:**
  - Degraded fluency with generic responses: α too high, over-suppressing valid tokens
  - No improvement over baseline: Disruptor not trained sufficiently (check learning rate, epochs) or backbone encoder lacks temporal discrimination
  - Hallucinations persist on temporal reasoning questions: Spatiotemporal disruption loss (L_T) may be underweighted; verify random-walk graph construction

- **First 3 experiments:**
  1. **Ablate disruptor components:** Train with only L_T, only L_S, and both; measure on EventHallusion to confirm both contribute (replicate Table 4)
  2. **Hyperparameter sweep on α:** Test α ∈ {0.3, 0.5, 0.7, 0.9} on VideoHallucer; plot hallucination rate vs. fluency metrics to find optimal tradeoff
  3. **Cross-backbone transfer:** Train disruptor on Video-LLaVA, apply to LLaVA-NeXT-Video without retraining; assess generalization to understand feature-space coupling

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the calibration of the contrastive strength parameter ($\alpha$) be automated to ensure robustness across different VideoLLM architectures without manual tuning?
- **Basis in paper:** [explicit] The conclusion states the method is "sensitive to hyperparameters such as the contrastive strength, which may require careful calibration across different tasks and model architectures."
- **Why unresolved:** The authors manually set $\alpha$ to 0.8 for Video-LLaVA and 0.4 for LLaVA-NeXT-Video, and the ablation study (Figure 3) shows that increasing $\alpha$ beyond a specific threshold causes performance degradation, implying a need for architecture-specific tuning.
- **What evidence would resolve it:** An adaptive mechanism for setting $\alpha$ or a theoretical derivation for its optimal value that yields consistent performance gains across multiple diverse backbone models without manual intervention.

### Open Question 2
- **Question:** To what extent does the quality of the frozen video encoder limit the upper bound of SSCD's hallucination mitigation capabilities?
- **Basis in paper:** [explicit] The conclusion notes that performance is "inherently constrained by the quality of the underlying video representations" and that "when video encodings lack sufficient discriminative information, the potential for effective hallucination mitigation is reduced."
- **Why unresolved:** The paper demonstrates the method on standard backbones but does not investigate the correlation between the encoder's feature fidelity and the disruptor's ability to generate effective contrastive signals.
- **What evidence would resolve it:** A comparative analysis applying SSCD to a series of backbones with progressively higher or lower visual representation quality (e.g., varying encoder sizes or pre-training datasets) to establish the dependency curve.

### Open Question 3
- **Question:** Can the Spatiotemporal-Semantic Disruptor generalize effectively to specialized video domains (e.g., medical, surveillance) that are not represented in the general-purpose ShareGPT4Video training set?
- **Basis in paper:** [inferred] The Implementation Details state the disruptor is trained on only 3,000 instances from ShareGPT4Video. While the method is "lightweight," it relies on learning specific disruption patterns which may be domain-dependent.
- **Why unresolved:** It is unclear if the spatiotemporal and semantic disruptions learned from general web videos transfer to domains with vastly different visual dynamics and specialized vocabulary.
- **What evidence would resolve it:** Zero-shot evaluation results of the pre-trained disruptor on VideoLLMs applied to out-of-domain benchmarks (e.g., medical video QA) compared to a disruptor fine-tuned on in-domain data.

### Open Question 4
- **Question:** Is the proposed random-walk-based spatiotemporal disruption mechanism computationally scalable for long-form video understanding requiring high frame rates?
- **Basis in paper:** [inferred] The experiments are limited to 8 or 16 frames. The methodology involves matrix multiplications of transition matrices (Eq. 8) which scale with the number of frames ($T$).
- **Why unresolved:** The paper claims the method is "lightweight," but the computational complexity of the graph-based random walk and the contrastive decoding pass (which doubles the inference batch size effectively) is not quantified for large $T$.
- **What evidence would resolve it:** A latency and memory usage analysis of SSCD applied to longer video sequences (e.g., 64 or 128 frames) to verify if the disruption module remains efficient compared to the base LLM inference.

## Limitations
- The method's effectiveness is inherently constrained by the quality of the underlying video representations, with poor encoders limiting hallucination mitigation potential
- Performance is sensitive to hyperparameters such as contrastive strength, requiring careful calibration across different VideoLLM architectures
- The lightweight disruptor may not generalize effectively to specialized video domains with different visual dynamics than the training data

## Confidence

- **High confidence:** The general framework of using contrastive decoding with deliberately disrupted negative features is well-motivated and technically sound. The paper's ablation studies and comparative results on multiple benchmarks provide strong evidence that SSCD reduces hallucination rates across different hallucination types.
- **Medium confidence:** The specific mechanisms for spatiotemporal disruption (random walks) and semantic disruption (CMI minimization) are plausible given the mathematical formulation, but the paper does not provide direct empirical validation that these mechanisms produce the expected types of hallucination-prone features.
- **Medium confidence:** The hyperparameter choices (α, λ, β) are presented as fixed values without systematic sensitivity analysis or justification beyond empirical tuning on a single dataset. The robustness of these choices across different VideoLLM backbones and video domains is uncertain.
- **Low confidence:** The claim that SSCD "preserves the general video understanding and reasoning capabilities" is based on accuracy improvements on ActivityNet-QA, but this metric is narrow. The paper does not comprehensively evaluate fluency, reasoning depth, or multimodal alignment.

## Next Checks

1. **Disruptor feature validation:** Implement a diagnostic to verify that the disruptor produces features with systematically weakened spatiotemporal consistency and semantic alignment. Compute cycle-consistency scores and answer prediction accuracy on a held-out validation set with both original and negative features to confirm the expected disruption patterns.

2. **Cross-backbone ablation study:** Train the disruptor on Video-LLaVA-7B, then apply it without retraining to LLaVA-NeXT-Video and a third, distinct video-LLM backbone (e.g., LLaVA-Next-Video). Measure hallucination rate reduction and accuracy changes to quantify the method's generalization and the coupling between the disruptor and specific feature spaces.

3. **Real-world domain robustness:** Evaluate SSCD on a diverse, out-of-domain video dataset (e.g., HowTo100M, social media video clips) with a subset annotated for hallucinations. Compare hallucination rates and accuracy against baselines to assess whether the improvements on controlled benchmarks transfer to more realistic, noisy video content.