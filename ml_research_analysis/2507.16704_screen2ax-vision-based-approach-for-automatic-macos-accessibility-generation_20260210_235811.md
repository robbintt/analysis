---
ver: rpa2
title: 'Screen2AX: Vision-Based Approach for Automatic macOS Accessibility Generation'
arxiv_id: '2507.16704'
source_url: https://arxiv.org/abs/2507.16704
tags:
- accessibility
- elements
- element
- macos
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of incomplete or missing accessibility
  metadata in macOS applications, which affects both assistive technologies and AI-driven
  agents. The authors propose Screen2AX, a vision-based framework that automatically
  generates hierarchical accessibility metadata from screenshots using YOLOv11 for
  UI element detection, BLIP for icon captioning, and additional models for grouping
  elements.
---

# Screen2AX: Vision-Based Approach for Automatic macOS Accessibility Generation

## Quick Facts
- arXiv ID: 2507.16704
- Source URL: https://arxiv.org/abs/2507.16704
- Reference count: 40
- Primary result: Vision-based framework that generates macOS accessibility metadata from screenshots, achieving 77% F1 in tree reconstruction and 2.2× improvement in AI agent performance

## Executive Summary
Screen2AX addresses the critical challenge of missing or incomplete accessibility metadata in macOS applications by using computer vision to automatically generate hierarchical accessibility trees from screenshots. The system employs YOLOv11 for UI element detection, BLIP for icon captioning, and additional models for grouping elements into semantic containers. By bypassing the need for application-level accessibility annotations, Screen2AX achieves 77% F1 score in reconstructing accessibility trees and improves AI agent performance by 2.2× over native accessibility representations, outperforming state-of-the-art methods on the ScreenSpot benchmark.

## Method Summary
Screen2AX takes a macOS screenshot as input and processes it through three main stages: (1) YOLOv11 detects and classifies UI elements into five categories (buttons, links, text areas, images, disclosure triangles); (2) OCR extracts text from elements, while a fine-tuned BLIP model generates captions for text-free buttons; (3) A second YOLOv11 model detects AXGroup regions to establish hierarchical relationships. The output is a structured JSON tree conforming to macOS accessibility conventions, with element IDs, roles, descriptions, and coordinates. The system is trained on three newly released datasets: Screen2AX-Tree (1,127 screenshots), Screen2AX-Element (986 images, 44,150 annotations), and Screen2AX-Group (808 images, 30,774 annotations).

## Key Results
- Achieves 77% F1 score in reconstructing accessibility trees versus 67% for heuristic baselines
- Improves AI agent task success rate by 2.2× compared to native accessibility representations
- Outperforms OmniParser V2 on ScreenSpot benchmark with 43.8% F1 vs 8.1% for MSER+GPT-4 baseline
- Hierarchical grouping provides 3% absolute performance gain in agent navigation tasks

## Why This Works (Mechanism)

### Mechanism 1
Vision-only element detection can approximate accessibility metadata that system APIs fail to provide. YOLOv11 simultaneously localizes and classifies UI elements from raw pixels, bypassing the need for application-level accessibility annotations. The model outputs bounding boxes and element types (AXButton, AXLink, etc.) that mirror macOS accessibility roles. Core assumption: Visual appearance of UI elements correlates sufficiently with their semantic role and interactivity, independent of underlying implementation. Evidence: YOLOv11 achieves 65.4% localization accuracy vs. 37.8% for MSER baseline; F1 of 43.8% vs. 8.1% for MSER+GPT-4. Break condition: Highly customized or non-standard UI components where visual patterns diverge from training distribution.

### Mechanism 2
Hierarchical grouping improves downstream agent performance beyond flat element lists. A second YOLOv11 model detects AXGroup regions, which organize individual elements into semantic containers (toolbars, sidebars). This tree structure provides spatial and logical context for navigation. Core assumption: Spatial clustering of elements in the visual layout corresponds to meaningful semantic groupings for both assistive technologies and agents. Evidence: Hierarchical Screen2AX achieves 33.7% success rate vs. 30.7% for non-hierarchical version (3% absolute gain) on Screen2AX-Task; YOLOv11 grouping achieves 77% F1 for edge-based hierarchy metrics vs. 67% for heuristics. Break condition: Applications with inconsistent or unconventional grouping patterns; interfaces where developers omitted explicit structuring.

### Mechanism 3
Fine-tuned vision-language models can generate functionally meaningful icon descriptions. BLIP is fine-tuned on mobile UI icon datasets; when buttons lack OCR-detectable text, the model generates captions describing icon functionality. Text-containing buttons use OCR output directly as descriptions. Core assumption: Icons share sufficient visual-semantic patterns across mobile and desktop platforms for transfer learning to be effective. Evidence: BLIP fine-tuned on 5,000 mobile icons; validated on 988 macOS icons; achieves 0.68 CIDEr and 76% GPT-measured accuracy vs. Florence's 0.39 and 44%. Break condition: Desktop-specific or complex icons outside the visual vocabulary of mobile-trained models; OCR misinterpreting icon-like symbols as text.

## Foundational Learning

- Object detection evaluation metrics (IoU, mAP, precision/recall): Essential to interpret performance claims comparing YOLOv11 against baselines. Quick check: Given an IoU threshold of 0.5, would a predicted box covering 40% of a ground-truth box be counted as correct?

- Accessibility tree structure and semantics: Screen2AX's output must conform to macOS accessibility conventions (AXButton, AXGroup, etc.) to be usable by screen readers and agents. Quick check: What is the functional difference between AXStaticText and AXTextArea in macOS accessibility roles?

- Transfer learning and domain adaptation: BLIP is fine-tuned on mobile icons but applied to desktop; understanding transfer limitations explains failure modes on desktop-specific UI elements. Quick check: If a model trained on mobile UI icons encounters a desktop-only element like a "disk eject" icon, what type of generalization failure would you expect?

## Architecture Onboarding

- Component map: Input screenshot → YOLOv11-Element (detects elements) → OCR/BLIP (extracts/describes text) → YOLOv11-Group (detects groups) → Tree Builder (assembles hierarchy) → Structured accessibility JSON

- Critical path: Element detection accuracy directly limits all downstream stages (missed elements cannot be described or grouped); description quality determines agent interpretability; hierarchy structure affects navigation efficiency (3% performance gain from hierarchy vs. flat list)

- Design tradeoffs: YOLOv11 vs. MSER+GPT-4: 43.8% F1 vs. 8.1% F1, but YOLOv11 requires GPU; MSER is deterministic but inaccurate. Deep learning vs. heuristic grouping: 77% vs. 67% F1, but deep model requires labeled group data (bimodal distribution in training introduces variability). Mobile-trained BLIP vs. desktop-specific training: Faster deployment, but reduced accuracy on desktop-specific icons

- Failure signatures: Rare element classes undetected (red buttons, unusual settings buttons); OCR misclassifying icons as text symbols (Bluetooth → "*"); BLIP generating generic descriptions for desktop-specific icons ("create table" → "go to app"); inconsistent group detection due to non-uniform training distribution

- First 3 experiments: 1) Reproduce element detection baseline: Train YOLOv11 on Screen2AX-Element subset, measure mAP50 against paper's 46.6% result. 2) Ablate hierarchy contribution: Compare agent task success rates using full Screen2AX output vs. flat element list on Screen2AX-Task. 3) Test domain gap: Evaluate fine-tuned BLIP on held-out desktop-specific icons; measure CIDEr degradation vs. mobile-style icons

## Open Questions the Paper Calls Out

- Classifying UI element groups by specific semantic roles (toolbars, navigation bars, side panels) could improve agent navigation performance compared to the current single-class approach. The current model merges all 19 group types into a single "AXGroup" class during training, discarding semantic differentiation that could aid agent interpretation. An ablation study measuring agent task success rates with distinct semantic group labels versus generic grouping would resolve this.

- Model pruning or quantization techniques could reduce Screen2AX inference latency to real-time levels without significantly degrading the 77% F1 hierarchy reconstruction score. While the system is functional, current inference speed may not meet real-time application requirements, and no optimizations were tested. Benchmarks reporting inference time and accuracy metrics after applying compression techniques would resolve this.

- Fine-tuning the BLIP captioning model on a dataset of desktop-specific icons (versus mobile icons) could mitigate current captioning errors. The current model struggles with desktop-specific elements because it was fine-tuned primarily on mobile UI icons, creating a domain gap. Comparative evaluation of CIDEr scores and GPT-measured accuracy between current and desktop-fine-tuned models would resolve this.

## Limitations
- Visual appearance dependence breaks down for non-standard UI components and desktop-specific icons not represented in mobile training data
- Group detection model shows bimodal performance due to inconsistent system accessibility annotations
- Cross-platform transfer learning from mobile to desktop icons remains under-validated with poorly characterized failure modes

## Confidence
- High confidence: YOLOv11 element detection performance (45%+ mAP50) and superiority over baselines; hierarchical structure's measurable impact on agent task success (3% absolute gain)
- Medium confidence: BLIP captioning effectiveness (0.68 CIDEr) given domain gap between mobile and desktop icons; generalization claims across diverse macOS applications
- Low confidence: Long-tail performance on rare element classes (<10 samples) and heavily customized interfaces; reproducibility of group detection given training data inconsistencies

## Next Checks
1. Evaluate BLIP's desktop-specific icon captioning performance on a held-out test set of macOS-only interface elements, measuring CIDEr degradation and semantic accuracy
2. Test Screen2AX on 50+ diverse macOS applications including creative tools (Photoshop, Figma) and enterprise software to quantify rare element class failures
3. Compare Screen2AX's hierarchical vs. flat output on real screen reader navigation tasks with visually impaired users to validate the 3% agent performance gain translates to practical accessibility improvements