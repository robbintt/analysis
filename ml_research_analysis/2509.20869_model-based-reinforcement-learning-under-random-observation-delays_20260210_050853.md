---
ver: rpa2
title: Model-Based Reinforcement Learning under Random Observation Delays
arxiv_id: '2509.20869'
source_url: https://arxiv.org/abs/2509.20869
tags:
- delays
- observations
- learning
- observation
- state
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies reinforcement learning under random observation
  delays in partially observable Markov decision processes (POMDPs), where observations
  may arrive out-of-sequence. The authors propose a model-based filtering approach
  that maintains a belief state over latent states using a learned world model, enabling
  agents to handle delayed and out-of-sequence observations effectively.
---

# Model-Based Reinforcement Learning under Random Observation Delays

## Quick Facts
- **arXiv ID:** 2509.20869
- **Source URL:** https://arxiv.org/abs/2509.20869
- **Reference count:** 40
- **Primary result:** A model-based filtering approach that maintains belief states over latent states enables effective handling of delayed and out-of-sequence observations in reinforcement learning.

## Executive Summary
This paper addresses reinforcement learning under random observation delays in partially observable Markov decision processes (POMDPs), where observations may arrive out-of-sequence. The authors propose a model-based filtering approach that maintains a belief state over latent states using a learned world model, enabling agents to handle delayed and out-of-sequence observations effectively. The method integrates into the Dreamer model-based RL framework and demonstrates strong performance across both fully observable MuJoCo environments and partially observable Meta-World tasks, with notable generalization to unseen delay distributions.

## Method Summary
The proposed method maintains a belief state over latent states using a learned world model, handling delayed and out-of-sequence observations through sequential filtering. During training, the world model is trained on complete trajectories while the policy is trained on belief states inferred from partially observed sequences. The filtering process computes posterior distributions over current latent states conditioned on available observations, even when they arrive late or out-of-order. The approach is evaluated in both fully observable MuJoCo environments and partially observable Meta-World tasks.

## Key Results
- The proposed method consistently outperforms baselines designed for MDPs and practical heuristics across various delay patterns
- Strong generalization to unseen delay distributions is demonstrated, maintaining performance across different delay patterns
- In Meta-World visual control tasks, the approach achieves success rates exceeding 84% even under long delays, significantly outperforming memoryless and waiting strategies

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The proposed Delay-Aware framework handles out-of-sequence (OOS) observations by switching between a learned prior and a variational posterior based on data availability.
- **Mechanism:** The system defines an auxiliary kernel ψ (Eq. 4-5). At time t, for a past timestep τ, if observation o_τ has arrived, it uses the encoder (posterior q); if o_τ is missing, it uses the dynamics model (prior p). This allows the "belief state" φ_t to be computed recursively from the last confirmed state x_{κ_t} forward to the present.
- **Core assumption:** Assumes the learned world model dynamics (p and q) are sufficiently accurate to approximate the true Bayesian filter.
- **Evidence anchors:**
  - [Section 4.1] "This auxiliary kernel ψ serves as a time-dependent transition function that updates the state based on whether an observation at time τ is available."
  - [Abstract] "The key method involves a latent-space filtering process that sequentially updates the belief state..."
  - [Corpus] Related work "Directly Forecasting Belief" suggests recursive forecasting can accumulate errors, validating this paper's specific design to switch mechanisms rather than purely forecasting.
- **Break condition:** Performance degrades if the recursive application of the prior p over long gaps accumulates prediction errors (Section 7).

### Mechanism 2
- **Claim:** Decoupling world model training from policy training allows the agent to learn robust latent dynamics from clean data while the policy learns to act on partial, delayed information.
- **Mechanism:** During training, the system waits for an episode to terminate and all pending observations to arrive before storing the *ordered* trajectory in the replay buffer. The world model is trained on these complete sequences (standard Dreamer). However, the policy is trained on "belief states" φ_t generated by masking observations according to the delay indices J_t, simulating deployment conditions.
- **Core assumption:** Assumes that the environment allows waiting for pending observations after episode termination (offline training constraint).
- **Evidence anchors:**
  - [Section 4.2] "...world model training remains identical to the standard setting... while training the policy on belief states inferred from partially observed sequences..."
  - [Algorithm 1] "Sample... ~ B... Update world model... Compute beliefs φ_{1:N} using Eq. (5) ... Update π."
  - [Corpus] "Delay-Aware Diffusion Policy" also addresses observation-execution gaps, supporting the need for specific delay-aware training procedures.
- **Break condition:** If the delay distribution during deployment differs drastically from training distribution support without retraining, though results show strong generalization (Section 5.1.3).

### Mechanism 3
- **Claim:** Maintaining a belief state over the latent space enables generalization to unseen delay distributions better than fixed-augmentation methods.
- **Mechanism:** Unlike methods that stack specific history lengths (fixed context), the filtering approach computes a probability distribution over the *current* state given *whatever* history is available. This makes the policy input (φ_t) theoretically invariant to the specific delay pattern, provided the uncertainty is captured.
- **Core assumption:** The single particle approximation (used in main experiments) is sufficient to capture the necessary uncertainty for control.
- **Evidence anchors:**
  - [Section 5.1.3] "DA-Dreamer generalizes well across all delay distributions... achieving much higher performance under shorter delays while remaining stable..."
  - [Figure 2] Shows normalized return stability across different test-time distributions.
  - [Corpus] "Reinforcement Learning via Conservative Agent..." discusses random delays, reinforcing that robustness to distribution shifts is a key unsolved area; this mechanism addresses it.
- **Break condition:** If stochasticity is very high, single-particle inference might fail to capture multi-modal uncertainty, though the paper notes K=1 worked well (Section 5).

## Foundational Learning

- **Concept: POMDPs & Belief States**
  - **Why needed here:** The paper argues that random delays create out-of-sequence observations, breaking the Markov property. You must understand that the "belief state" φ_t is a sufficient statistic for history, replacing the raw observation.
  - **Quick check question:** Can you explain why stacking the last k observations fails in a POMDP with random delays compared to fixed delays?

- **Concept: Recurrent State Space Models (RSSM)**
  - **Why needed here:** The method builds directly on Dreamer's architecture. You need to distinguish between the deterministic path (RNN) and stochastic path (latent variable x) to understand how the auxiliary kernel ψ operates.
  - **Quick check question:** In the context of this paper, does the auxiliary kernel ψ update the deterministic state or the stochastic state?

- **Concept: Variational Inference (ELBO)**
  - **Why needed here:** The world model is trained via ELBO maximization. Understanding the relationship between the prior p(x_t|x_{t-1}) and posterior q(x_t|x_{t-1}, o_t) is critical for understanding the filtering update.
  - **Quick check question:** How does the training objective change if observations are missing during the world model training phase? (Hint: It doesn't in this specific framework, but you must know why).

## Architecture Onboarding

- **Component map:**
  - World Model (Standard Dreamer) -> Encoder, RSSM (deterministic + stochastic), Decoder/Reward
  - Delay Buffer -> Stores tuples (o_τ, τ) with maximum size D
  - Filtering Engine -> Implements Eq. (5). Identifies κ_t (last complete step) and runs the loop using ψ
  - Policy Network -> Standard actor-critic, but takes φ_t (the output of the Filtering Engine) as input, not raw image/latent

- **Critical path:**
  1. Data Collection: Environment Step -> Store in Delay Buffer (withholding missing obs)
  2. Model Training: Retrieve *full* sequence from Buffer -> Train Standard RSSM
  3. Policy Training: Retrieve *masked* sequence -> Compute φ_t via Filtering Engine -> Train Actor/Critic

- **Design tradeoffs:**
  - **Particle Count (K):** Paper uses K=1 for efficiency. Increasing K helps with uncertainty but adds compute cost (Appendix C.3)
  - **Buffer Size:** Must be at least the max delay D. Larger buffers allow handling longer delays but increase memory

- **Failure signatures:**
  - **High variance/instability:** If using "Stack-Dreamer" on complex tasks (Humanoid), performance collapses due to OOS inputs (Table 1)
  - **Accumulated Error:** In long horizons with extreme delays, the recursive use of prior p might diverge from the true state, resulting in blurry reconstructions (Appendix C.4)

- **First 3 experiments:**
  1. **Sanity Check (MuJoCo):** Run DA-Dreamer vs. Stack-Dreamer on `HalfCheetah-v4` with U{0,10} delay. Confirm Stack-Dreamer diverges or underperforms
  2. **Ablation (Inference Only):** Train Dreamer on *undelayed* data, then deploy with the Filtering Engine enabled (DA-Dreamer inference only). Compare to Table 4 results to verify zero-shot delay robustness
  3. **Visual Verification:** Reconstruct observations from the belief φ_t under high delays (Appendix C.4). Verify that the agent "hallucinates" plausible states (blurry but correct position) rather than outputting noise

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can Transformer-based architectures effectively mitigate the accumulation of one-step prediction errors in long-horizon tasks with random observation delays?
- **Basis in paper:** [explicit] The authors identify that recursive filtering accumulates errors over long horizons and suggest exploring Transformer-based or multi-step models as a solution.
- **Why unresolved:** The current Recurrent State Space Model (RSSM) propagates uncertainty linearly, potentially failing in tasks requiring very long-term credit assignment or belief maintenance.
- **What evidence would resolve it:** Comparative performance analysis of Transformer-based world models against the current RSSM on extended-horizon benchmarks.

### Open Question 2
- **Question:** How can the framework be modified to handle permanently missing observations (packet loss) rather than finite delays?
- **Basis in paper:** [explicit] The conclusion notes the assumption that no observations are permanently missing and calls for approaches specifically designed for this setting.
- **Why unresolved:** The current filtering process assumes every observation eventually arrives; dropping observations entirely breaks the ability to reconstruct full trajectories for world model training.
- **What evidence would resolve it:** A method capable of training on incomplete trajectories evaluated in environments with stochastic packet loss.

### Open Question 3
- **Question:** Can the latent-space filtering approach be extended to simultaneously handle random execution delays?
- **Basis in paper:** [inferred] The introduction categorizes delays into "feedback" (observation) and "execution" types, but the methodology and experiments address only the former.
- **Why unresolved:** Random execution delays decouple the agent's intended action from the actual environment transition, complicating the belief update logic which currently conditions on the immediate action history.
- **What evidence would resolve it:** Integration of an action buffer into the belief state formulation, tested in environments with random action latencies.

## Limitations
- The method assumes the environment allows waiting for pending observations after episode termination for world model training, which may not hold in real-time control scenarios where episodes must end at fixed timesteps.
- The single-particle approximation (K=1) may fail to capture multi-modal uncertainty in highly stochastic environments, potentially leading to suboptimal policies despite theoretical advantages.
- Performance degradation occurs when recursive forecasting over long gaps accumulates prediction errors, particularly under extreme delay conditions where the belief state becomes increasingly uncertain.

## Confidence

**High confidence:** The core filtering mechanism (Mechanism 1) and the decoupling of world model/policy training (Mechanism 2) are well-supported by both theoretical exposition and experimental evidence. The ablation studies in Table 4 and the generalization results in Section 5.1.3 provide strong validation.

**Medium confidence:** The claim that maintaining belief states enables better generalization to unseen delay distributions (Mechanism 3) is supported by experiments but could benefit from testing on more diverse delay distributions and longer-horizon tasks.

**Low confidence:** The assertion that single-particle inference is sufficient for all tested environments, while the paper notes this worked well, lacks systematic exploration of the K parameter's impact across different task complexities.

## Next Checks

1. **Real-time constraint test:** Modify the training procedure to handle fixed-episode-length environments where waiting for all observations is impossible, measuring performance degradation compared to the current approach.

2. **Multi-particle uncertainty evaluation:** Systematically vary K (particle count) across all benchmark tasks, quantifying the trade-off between computational cost and performance improvement in high-uncertainty scenarios.

3. **Distribution shift robustness:** Create training and test delay distributions with disjoint support (e.g., train on U{0,10} and test on U{20,30}) to rigorously evaluate generalization claims beyond the interpolation tested in Section 5.1.3.