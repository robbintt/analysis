---
ver: rpa2
title: 'SIRR-LMM: Single-image Reflection Removal via Large Multimodal Model'
arxiv_id: '2601.07209'
source_url: https://arxiv.org/abs/2601.07209
tags:
- reflection
- image
- glass
- removal
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses single-image reflection removal (SIRR) by
  introducing a physically accurate synthetic dataset and leveraging large multimodal
  models (LMMs) with LoRA fine-tuning. The synthetic dataset is generated using path-traced
  3D glass models combined with real-world imagery, simulating diverse glass and camera
  conditions with physically-based rendering.
---

# SIRR-LMM: Single-image Reflection Removal via Large Multimodal Model

## Quick Facts
- arXiv ID: 2601.07209
- Source URL: https://arxiv.org/abs/2601.07209
- Reference count: 40
- Primary result: Introduces a physically accurate synthetic dataset and LoRA fine-tuning of a large multimodal model (FLUX.1 Kontext) for single-image reflection removal, achieving superior performance on real-world benchmarks.

## Executive Summary
This paper addresses single-image reflection removal (SIRR) by introducing a physically accurate synthetic dataset and leveraging large multimodal models (LMMs) with LoRA fine-tuning. The synthetic dataset is generated using path-traced 3D glass models combined with real-world imagery, simulating diverse glass and camera conditions with physically-based rendering. Instead of full-parameter training, the authors fine-tune a large multimodal model (FLUX.1 Kontext) using task-specific LoRA, enabling effective reflection removal with relatively small but high-fidelity synthetic data. Experiments show that the proposed method achieves superior performance compared to state-of-the-art methods, with improved PSNR, SSIM, and LPIPS scores on real-world reflection benchmarks (Real, Nature, and SIR 2). The method also demonstrates better qualitative results, especially in complex reflection scenarios and regions with low visibility.

## Method Summary
The method generates a synthetic dataset using path-traced 3D glass models with HDR environment maps and LDR background images, creating physically accurate input (I), transmission (T), reflection (R), background (B), and mirror reflection (MR) layers. A large multimodal model (FLUX.1 Kontext) is fine-tuned using task-specific LoRA adapters rather than full-parameter training, treating SIRR as a consolidated 3-panel image generation task ([I:T:R]) with a fixed prompt. The model is trained for 4,000 steps with a batch size of 1 and LoRA rank of 16 on a single RTX 4090 GPU. Inference involves feeding the input image with white-space inpainting mask and the fixed prompt to generate the [I:T:R] composite, from which T and R are extracted.

## Key Results
- Achieves state-of-the-art performance on reflection removal benchmarks with PSNR ~24dB, SSIM ~0.81, and LPIPS scores below 0.4 on Real, Nature, and SIR² datasets.
- Demonstrates superior qualitative results in complex reflection scenarios and low-visibility regions compared to existing methods.
- Shows that LoRA fine-tuning with only 1,000 high-fidelity synthetic samples is sufficient to achieve strong performance, avoiding the need for massive real-world datasets.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Physically accurate path-traced synthetic data provides better training signal than linear alpha-blending.
- **Mechanism:** Standard synthetic data uses simplified models ($I = f(T) + g(R)$) which fail to capture complex light interactions like ghosting (double reflections) and attenuation through glass thickness. The proposed method uses a rendering equation integrating light paths ($L(\omega_o)$) over a transparent surface. This generates physically correct artifacts (blur, ghosting) that align with the "glass priors" embedded in Large Multimodal Models (LMMs) during their pre-training.
- **Core assumption:** The LMM has already learned general concepts of transparency and reflection from its massive pre-training dataset, and only needs physically accurate examples to "ground" these priors for the specific decomposition task.
- **Evidence anchors:** [abstract] Mentions "path-traced 3D glass models... simulating diverse glass and camera conditions with physically-based rendering." [section 3.1] Equations 2 and 3 explicitly model the light paths ($T_1, R_1$) and higher-order interactions that linear blending misses.
- **Break condition:** If the target domain involves non-physical reflections (e.g., artistic composites) or the LMM lacks strong material priors, the physical accuracy becomes less relevant.

### Mechanism 2
- **Claim:** Freezing the base LMM and training task-specific LoRA enables efficient adaptation without catastrophic forgetting.
- **Mechanism:** Large Multimodal Models (specifically Diffusion Transformers like FLUX) possess general visual knowledge. Instead of updating all weights (which risks overfitting to a small 1,000-image dataset), LoRA injects low-rank matrices. This steers the generation process to output the specific [I:T:R] format while preserving the base model's ability to understand complex textures and lighting.
- **Core assumption:** The base model's representation of "glass" is sufficient and only needs a lightweight pointer to map input reflection images to separated transmission/reflection outputs.
- **Evidence anchors:** [abstract] "Fine-tune a large multimodal model... using task-specific LoRA rather than full-parameter training." [section 5] "Trained on a single 4090 GPU... with a batch size of 1 and a LoRA rank of 16."
- **Break condition:** If the reflection removal logic conflicts fundamentally with the base model's generative priors (e.g., requiring exact pixel inversion rather than semantic reconstruction), LoRA may lack the capacity to override the base behavior.

### Mechanism 3
- **Claim:** Formulating SIRR as a consolidated 3-panel image generation task ([I:T:R]) leverages the model's context-awareness better than image-to-image translation.
- **Mechanism:** By concatenating the Input (I), Transmission (T), and Reflection (R) into a single image grid during training, and using a joint caption, the model attends to the spatial and semantic relationships between the three layers simultaneously. This treats separation as a "completion" or "pattern matching" task within the attention window.
- **Core assumption:** The model's context window and attention mechanism can effectively correlate distant image patches (e.g., a reflection in the top-left corner with its source in the bottom-right) across the concatenated grid.
- **Evidence anchors:** [section 4] "Concatenate the image layers into a single composite input... The correlations... are implicitly maintained through the consolidated prompt." [section 4] Figure 5 illustrates the [I:T:R] output structure.
- **Break condition:** If high-resolution inputs force the model to downsample the concatenated grid significantly, the fine-grained correlation between reflection details and background details might be lost.

## Foundational Learning

- **Concept: Path Tracing (Monte Carlo Integration)**
  - **Why needed here:** To understand why the synthetic data is superior. It's not just "overlaying" images; it's simulating photons bouncing through glass (BSDF).
  - **Quick check question:** How does the "rendering equation" (Eq 1 in text) differ from a simple Photoshop "Screen" blend mode?

- **Concept: LoRA (Low-Rank Adaptation)**
  - **Why needed here:** To understand how the authors train a massive model on a small dataset. It explains the "efficiency" claim.
  - **Quick check question:** Why would training a LoRA adapter (rank 16) be safer for preserving general image priors than full fine-tuning?

- **Concept: Diffusion Transformers (DiT) / Flow Matching**
  - **Why needed here:** The base model (FLUX) is not a standard U-Net; it uses Transformers. This determines how the image is processed (patches/tokens) and how "context" works.
  - **Quick check question:** How does the self-attention mechanism in a DiT allow the model to relate a reflection artifact in one part of the image to the transmission background in another?

## Architecture Onboarding

- **Component map:** Blender/Path Tracer -> Python Formatter -> FLUX.1 Kontext with LoRA -> Output Grid -> Script to extract T and R
- **Critical path:** The rendering pipeline is the bottleneck. You cannot "just add more data" easily without waiting for path tracing to finish. The relationship between *glass thickness/IOR* parameters in the renderer and the resulting visual artifacts is the direct lever for model quality.
- **Design tradeoffs:**
  - **Physics vs. Compute:** High-fidelity path tracing is slow (offline only) vs. fast but inaccurate linear blending.
  - **In-Context vs. Encoder-Decoder:** The authors reformulate SIRR as an "in-painting/completion" task in the latent space (concatenating images) rather than a pixel-to-pixel encoder-decoder, prioritizing semantic consistency over exact pixel reconstruction.
- **Failure signatures:**
  - **Semantic Confusion:** "White background areas are occasionally misclassified as highlight reflections" (Section 7). The model relies on visual similarity rather than physical truth.
  - **Hallucination:** In low-visibility regions, the model must "hallucinate" plausible context; if the prior is wrong, it invents objects.
- **First 3 experiments:**
  1. **Overfit Test:** Train the LoRA on a single scene (e.g., a red cube behind glass). Verify if the model learns the exact decomposition for that scene to confirm the training loop is connected.
  2. **Ablation on Data Fidelity:** Train one LoRA on the path-traced data and another on simple linear-blend data. Compare LPIPS on a validation set to quantify the value of the "physics."
  3. **Prompt Sensitivity Check:** Run inference with the fixed internal prompt vs. a blank prompt to confirm the prompt is indeed "internalized" or if the model relies on the visual grid structure alone.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the model's tendency to misclassify white background regions as highlight reflections be mitigated without manual intervention?
- **Basis in paper:** [Explicit] The authors state in the Limitations section that "white background areas are occasionally misclassified as highlight reflections to be removed and vice versa."
- **Why unresolved:** This ambiguity suggests the LMM lacks a semantic understanding of scene context versus lighting physics in high-luminance areas.
- **What evidence would resolve it:** A modified training strategy or attention mechanism that demonstrably reduces false positive removal rates on bright transmission regions in benchmark tests.

### Open Question 2
- **Question:** Can the computational cost of the path-traced synthetic data generation be reduced to enable online training augmentation?
- **Basis in paper:** [Explicit] The paper notes that path-traced rendering "requires considerable computational resources, which could only generate dataset offline."
- **Why unresolved:** The current reliance on offline rendering restricts the scalability and immediate adaptability of the dataset generation pipeline.
- **What evidence would resolve it:** Demonstration of a real-time or interactive approximation method that maintains the physical fidelity necessary for effective LoRA fine-tuning.

### Open Question 3
- **Question:** Does the proposed method generalize effectively to non-planar glass surfaces, such as prescription eyeglasses, without specific re-training?
- **Basis in paper:** [Inferred] The authors mention in Section 2.2 that the method can "extend to the non-planar surface," but all quantitative experiments are limited to planar benchmarks (Real, Nature, SIR²).
- **Why unresolved:** The paper does not provide visual or quantitative results for curved surfaces where reflections distort differently than on flat glass slabs.
- **What evidence would resolve it:** Evaluation results on a dataset of curved glass captures showing successful separation comparable to planar performance.

## Limitations
- The synthetic dataset generation pipeline is described but not fully specified, with no code or detailed scene parameters provided, making exact reproduction difficult.
- The LoRA training configuration is sparse (rank and steps only), omitting critical hyperparameters like learning rate and optimization settings.
- The method struggles with semantic confusion in high-contrast regions, misclassifying white backgrounds as reflections or vice versa, indicating the model relies on visual similarity rather than physical correctness.
- Performance on real-world benchmarks, while superior to previous methods, still shows room for improvement (PSNR ~24dB, SSIM ~0.81), suggesting the approach has not yet solved the fundamental challenge of accurate separation.

## Confidence
- **High:** The core architectural innovation (LoRA fine-tuning of LMM for SIRR) is technically sound and well-supported by the literature on diffusion model adaptation.
- **Medium:** The claim that path-traced physics provides superior training signal is plausible given the mechanism, but direct ablation studies comparing path-traced vs. simple blended data are not shown.
- **Medium:** The superiority on real-world benchmarks is demonstrated, but the small dataset sizes (20-101 images per benchmark) limit statistical confidence in the improvements.

## Next Checks
1. **Ablation on Data Fidelity:** Train two LoRA models—one on path-traced synthetic data and one on simple linear-blend data—and compare their LPIPS scores on a held-out validation set to quantify the value of physical accuracy.
2. **Prompt Sensitivity Test:** Run inference with the fixed internal prompt versus a blank prompt to determine whether the prompt is essential or if the model relies primarily on the visual [I:T:R] concatenation structure.
3. **Scaling Study:** Train LoRA adapters on varying dataset sizes (10, 50, 100, 300 samples) to map the learning curve and identify the minimum data required for acceptable performance.