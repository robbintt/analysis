---
ver: rpa2
title: Grokked Models are Better Unlearners
arxiv_id: '2512.03437'
source_url: https://arxiv.org/abs/2512.03437
tags:
- unlearning
- grokked
- data
- grokking
- forget
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors investigate whether the delayed generalization phenomenon
  of grokking (Power et al., 2022) can also benefit machine unlearning. Using CNNs,
  ResNets, and transformers, they compare unlearning performance from pre- and post-grokking
  checkpoints.
---

# Grokked Models are Better Unlearners

## Quick Facts
- arXiv ID: 2512.03437
- Source URL: https://arxiv.org/abs/2512.03437
- Reference count: 32
- Primary result: Post-grokking models achieve 6-8% lower unlearning accuracy and 10-20% higher retention/test accuracy than pre-grokking checkpoints

## Executive Summary
This paper investigates whether the delayed generalization phenomenon of grokking (Power et al., 2022) can also benefit machine unlearning. The authors compare unlearning performance from pre- and post-grokking checkpoints across CNNs, ResNets, and transformers, finding that post-grokking models consistently achieve more effective forgetting, better retain/test accuracy retention, and more stable updates across seeds. Analysis reveals that grokking restructures representations into more modular forms with reduced gradient alignment between forget and retain subsets, enabling selective forgetting with minimal interference. These findings suggest that the representational quality of grokked models provides an orthogonal lever to improve unlearning efficiency without modifying existing algorithms.

## Method Summary
The study compares unlearning effectiveness on grokked versus pre-grokked checkpoints across vision (CIFAR-10/100, SVHN, ImageNet-100) and language (TOFU synthetic author profiles) domains. Models are trained with extended training (500+ epochs) to observe grokking transitions, with checkpoints saved at pre-grokking (early stopping) and post-grokking (after generalization jump) points. Standard unlearning algorithms (Gradient Ascent, Fisher forgetting, SCRUB, fine-tuning) are applied to both checkpoints with matched hyperparameters. Unlearning performance is measured via Unlearning Accuracy (UA), Retain Accuracy (RA), and Test Accuracy (TA). For LLMs, extraction strength and membership inference attack resistance are also evaluated.

## Key Results
- Post-grokking models achieve 6-8% lower unlearning accuracy than pre-grokking checkpoints across all architectures
- Retain/test accuracy is 10-20% higher in post-grokking models after unlearning
- Unlearning converges 60-70% faster in post-grokking models
- Gradient correlation between forget/retain sets drops from ~0.99 (pre-grokking) to ~0.43 (post-grokking)
- CKA between forget/retain representations decreases by 72% (0.459 → 0.129) after grokking

## Why This Works (Mechanism)

### Mechanism 1: Gradient Orthogonality Between Forget and Retain Sets
Grokked models develop orthogonal gradient spaces for forget versus retain data, enabling surgical unlearning without collateral damage. Extended training causes networks to form specialized subcircuits where different data types activate distinct parameter subspaces, reducing gradient correlation from ~0.99 (pre-grokking) to ~0.43 (post-grokking).

### Mechanism 2: Representational Disentanglement via Modular Circuit Formation
Grokking transitions reorganize monolithic representations into modular subcircuits, reducing interference when modifying specific knowledge. CKA analysis shows 72% reduction in representational overlap between forget and retain sets (0.459 → 0.129), indicating distinct encoding subspaces.

### Mechanism 3: Loss Landscape Flattening and Reduced Local Complexity
Grokked models occupy flatter, simpler loss landscape regions that absorb parameter modifications more gracefully during unlearning. Lower Local Complexity scores (7.37 vs 27.98 for ResNet retain set) indicate smoother representations that maintain stability under perturbation.

## Foundational Learning

- **Concept: Grokking (delayed generalization)**
  - Why needed here: The entire paper hinges on understanding what happens during the grokking transition and why it differs from early-stopped training
  - Quick check question: Can you explain why a model that has already achieved 100% training accuracy might suddenly show a dramatic jump in test accuracy after 10× more training?

- **Concept: Gradient-based unlearning (GA, SCRUB, Fisher forgetting)**
  - Why needed here: These are the tools the paper uses to demonstrate grokking's advantages—understanding their failure modes (collateral damage, instability) is essential context
  - Quick check question: Why does gradient ascent on forget data often degrade performance on retained data in standard models?

- **Concept: Representational similarity metrics (CKA, Local Complexity)**
  - Why needed here: These are the mechanistic probes that explain why grokked models unlearn better—you need to interpret what these metrics reveal about internal structure
  - Quick check question: What would a high CKA score between forget and retain set representations imply about the difficulty of selective unlearning?

## Architecture Onboarding

- **Component map:** Extended training loop (500+ epochs) -> Grokking detection (validation accuracy monitoring) -> Checkpoint saving (pre-grokking and post-grokking) -> Unlearning evaluation (GA, SCRUB, Fisher) -> Mechanism probes (gradient correlation, CKA, Local Complexity)

- **Critical path:** 1) Train model sufficiently long to observe grokking transition, 2) Identify θ_pre (early-stopped) and θ_grok (post-transition) checkpoints, 3) Run identical unlearning algorithms on both checkpoints, 4) Compare convergence speed, final UA/RA/TA, and stability across seeds

- **Design tradeoffs:** Training time vs. unlearning quality (5-6× longer training but 60-70% faster unlearning convergence), global vs. local grokking identification challenges, SAM alternative providing partial benefits without full grokking overhead

- **Failure signatures:** No grokking observed (training stops too early), unlearning causes catastrophic collapse (RA/TA near zero), pre/post grokking show similar results (verify gradient correlation and CKA differences)

- **First 3 experiments:** 1) Replicate CIFAR-10 ResNet grokking curve with checkpoint saves at regular intervals, 2) Compare gradient correlation between forget/retain sets at pre-grokking vs. grokked checkpoints, 3) Run gradient ascent unlearning on both checkpoints with identical hyperparameters

## Open Questions the Paper Calls Out

- Can the unlearning benefits of grokking be achieved efficiently without the 5-6× training overhead through alternative optimization methods?
- Do grokking's unlearning advantages scale to larger datasets, modern architectures (e.g., Vision Transformers, larger LLMs), and real-world non-synthetic data?
- What are the causal mechanisms by which grokking improves unlearning—gradient orthogonality, representation modularity, loss landscape flatness, or their combination?
- Can training dynamics be intentionally designed to promote grokking-like representational properties optimized specifically for unlearning rather than generalization?

## Limitations
- Experiments conducted on relatively small-scale datasets (CIFAR-10/100) with simple architectures; scalability to larger, more complex datasets and modern architectures remains to be demonstrated
- Theoretical understanding of why grokking enables better unlearning requires further investigation to establish causal mechanisms
- LLM unlearning results and local grokking identification are preliminary and harder to verify than model-wide transitions

## Confidence

**High Confidence:** Post-grokking models achieve consistently better unlearning metrics (6-8% lower UA, 10-20% higher RA/TA) across architectures. The empirical observation that extended training produces better unlearning is robust and reproducible.

**Medium Confidence:** The three proposed mechanisms (gradient orthogonality, representational disentanglement, loss landscape flattening) explain the empirical results. While measurements support these mechanisms, direct causal evidence is limited.

**Low Confidence:** LLM unlearning results and local grokking identification are preliminary. The paper acknowledges local grokking is harder to detect and verify than model-wide transitions.

## Next Checks
1. **Mechanism Isolation Experiment:** Train models to similar validation performance using different strategies (early stopping, SAM, full grokking) and measure unlearning effectiveness while controlling for final accuracy to isolate whether grokking's benefits come from specific representational changes or simply better optimization.

2. **Gradient Correlation Causality Test:** Systematically manipulate gradient alignment between forget/retain sets (e.g., through orthogonal regularization) in pre-grokking models and measure if unlearning performance improves to match post-grokking levels.

3. **Cross-Domain Generalization:** Test whether grokked models show similar unlearning advantages on regression tasks or different data modalities (e.g., audio, tabular data) to establish whether the phenomenon is architecture-specific or more fundamental.