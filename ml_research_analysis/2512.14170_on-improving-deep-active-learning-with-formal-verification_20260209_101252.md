---
ver: rpa2
title: On Improving Deep Active Learning with Formal Verification
arxiv_id: '2512.14170'
source_url: https://arxiv.org/abs/2512.14170
tags:
- adversarial
- learning
- verification
- each
- deep
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how adversarial inputs generated via formal
  verification can improve Deep Active Learning (DAL) performance. The authors propose
  augmenting DAL with adversarial examples discovered through systematic exploration
  of bounded input regions using a verifier, rather than relying on gradient-based
  attacks like FGSM.
---

# On Improving Deep Active Learning with Formal Verification

## Quick Facts
- **arXiv ID**: 2512.14170
- **Source URL**: https://arxiv.org/abs/2512.14170
- **Reference count**: 40
- **Primary result**: Verification-generated adversarial examples consistently outperform FGSM across DAL methods on MNIST, fashionMNIST, and CIFAR-10, with higher AUBC scores and better test accuracy

## Executive Summary
This paper proposes augmenting Deep Active Learning with adversarial examples generated via formal verification rather than gradient-based attacks. The approach systematically explores bounded input regions using a verifier to discover diverse counterexamples, then assigns the original sample's oracle label to all generated adversarial examples. Experiments across multiple DAL methods (BADGE, DFAL, and a new FVAAL) demonstrate consistent improvements over FGSM-based augmentation, with the verification approach producing more diverse examples in feature space and reducing training-data redundancy.

## Method Summary
The method integrates formal verification into the DAL loop by generating multiple adversarial examples within ε-bounded regions around each labeled sample. A verifier (Marabou) systematically explores the neighborhood while excluding previously discovered counterexamples via coordinate constraints. For margin-based selection in FVAAL, a binary search over FGSM perturbations finds the smallest perturbation causing misclassification, serving as a proxy for decision-boundary distance. All verification-generated examples receive the original sample's oracle label, amplifying each human annotation without additional labeling cost.

## Key Results
- Verification-generated adversarial examples yield higher AUBC scores than FGSM across all benchmark datasets
- BADGE+FV-Adv achieves 0.8408 AUBC on MNIST versus 0.8522 for BADGE+FGSM-Adv (Random selection shows opposite trend)
- FVAAL performs competitively with established methods while requiring no additional labeling
- Pairwise distances in embedding space show verification examples are more diverse (33.631±11.617 vs 26.186±8.848 for BADGE variants on MNIST)

## Why This Works (Mechanism)

### Mechanism 1: Verification-Driven Diversity in Adversarial Generation
- **Claim**: Verification provides greater feature-space diversity than gradient-based methods
- **Evidence**: Embedding space distances show verification examples are more diverse; verifiers can explore off-gradient failure modes using exclusion constraints
- **Break condition**: If verification queries timeout frequently or return only gradient-aligned counterexamples

### Mechanism 2: Margin Proxy via Binary Search on FGSM
- **Claim**: Binary search FGSM efficiently estimates decision-boundary distance
- **Evidence**: FVAAL matches or exceeds DFAL performance without adversarial inputs; requires only O(log(1/τ)) forward passes vs DeepFool's iterative steps
- **Break condition**: If FGSM perturbations systematically miss the true closest boundary

### Mechanism 3: Zero-Cost Label Amplification
- **Claim**: Transferring oracle labels to verification-generated examples amplifies training data
- **Evidence**: All samples receive the original label; assumes small ε preserves semantic consistency
- **Break condition**: If ε values are too large, semantic content changes and labels become incorrect

## Foundational Learning

- **Formal Verification of Neural Networks**: Understanding systematic exploration vs gradient attacks is essential for interpreting verification's diversity advantage
  - Quick check: Can you explain why a verifier with exclusion constraints can find multiple distinct counterexamples while FGSM cannot?

- **Margin-Based Active Learning**: FVAAL's selection relies on margin estimation; understanding why boundary-near samples are informative
  - Quick check: What does a small ε* value indicate about a sample's position relative to the decision boundary?

- **Adversarial Perturbation Bounds (ℓp-balls)**: ε-constrained neighborhoods define both search space and semantic-validity assumptions
  - Quick check: If ε = 0.1 on CIFAR-10 normalized images, would you expect label transfer to remain valid? What factors influence this?

## Architecture Onboarding

- **Component map**: Unlabeled pool -> Selection criterion (margin/FGSM/binary search) -> Oracle query -> Marabou verifier with exclusion constraints -> Generate up to 10 adversarial examples -> Add to training set with oracle label -> Model retrain

- **Critical path**: 1) Binary search FGSM: O(log(1/τ)) forward passes per candidate 2) Verification queries: Up to 30 minutes per sample 3) Model retraining: 10 epochs per DAL round

- **Design tradeoffs**: Verification cost vs diversity gain; ε selection balancing counterexample probability vs semantic drift; runner-up-only specifications reducing complexity but potentially missing cross-boundary failures

- **Failure signatures**: Verification timeout with no counterexamples; AUBC degradation vs FGSM; noisy CIFAR-10 selection from unreliable margin estimation

- **First 3 experiments**:
  1. Baseline replication: Random+FV-Adv vs Random+FGSM-Adv on MNIST with batch size 50, 5 repeats
  2. Ablation on adversarial count: Test k ∈ {1, 3, 5, 10} on fashionMNIST with BADGE, plot AUBC vs k
  3. Timeout sensitivity: Reduce Marabou timeout from 30 min to 5 min on CIFAR-10, compare AUBC degradation

## Open Questions the Paper Calls Out
- Whether verification-based augmentation scales to modern CNN/transformer architectures beyond simple fully-connected networks
- Why random sampling with verification augmentation outperforms sophisticated uncertainty-based methods on CIFAR-10
- How different formal verifiers compare in adversarial example diversity and downstream DAL performance
- The computational overhead of verification-based augmentation relative to gradient-based methods

## Limitations
- The 30-minute Marabou timeout may be impractical for large-scale deployment with no timeout frequency analysis
- The diversity advantage claim rests on a single pairwise-distance metric without ablation studies on required adversarial example counts
- The binary-search FGSM margin proxy assumes gradient direction represents true closest boundary without analysis of when this breaks

## Confidence
- **High confidence**: Empirical finding that verification examples improve AUBC scores across DAL methods
- **Medium confidence**: Mechanism that verification produces more diverse examples (supported by one distance metric)
- **Low confidence**: Claim that binary-search FGSM margin estimation is efficient and reliable (no comparison to alternatives)

## Next Checks
1. **Timeout analysis**: Run CIFAR-10 with Marabou timeout reduced to 5 minutes; measure AUBC degradation and adversarial examples generated per sample
2. **Diversity ablation**: For BADGE on fashionMNIST, test k ∈ {1, 3, 5, 10} adversarial examples per sample and plot AUBC vs k
3. **Margin estimator comparison**: Replace binary search FGSM with DeepFool margin estimation in FVAAL; compare test accuracy and computational cost