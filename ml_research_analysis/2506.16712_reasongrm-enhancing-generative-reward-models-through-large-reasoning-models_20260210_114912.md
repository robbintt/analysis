---
ver: rpa2
title: 'ReasonGRM: Enhancing Generative Reward Models through Large Reasoning Models'
arxiv_id: '2506.16712'
source_url: https://arxiv.org/abs/2506.16712
tags:
- reasoning
- reward
- answer
- assistant
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces ReasonGRM, a generative reward modeling\
  \ framework that addresses the challenge of poor reasoning capabilities in existing\
  \ generative reward models. The key innovation is the R\u22C6 metric, which evaluates\
  \ reasoning paths based on their generation likelihood to ensure validity and self-consistency."
---

# ReasonGRM: Enhancing Generative Reward Models through Large Reasoning Models

## Quick Facts
- arXiv ID: 2506.16712
- Source URL: https://arxiv.org/abs/2506.16712
- Reference count: 11
- Primary result: 83.3% average accuracy on benchmarks, outperforming previous GRMs by 1.8% and GPT-4o by 5.6%

## Executive Summary
This paper introduces ReasonGRM, a generative reward modeling framework that addresses the challenge of poor reasoning capabilities in existing generative reward models. The key innovation is the R⋆ metric, which evaluates reasoning paths based on their generation likelihood to ensure validity and self-consistency. The three-stage training pipeline combines Zero-RL for initial adaptation, R⋆-guided supervised fine-tuning for high-quality reasoning path selection, and reinforcement learning on hard examples. Experiments on three benchmarks show ReasonGRM achieves state-of-the-art performance with 83.3% average accuracy, outperforming previous best GRMs by 1.8% and GPT-4o by 5.6%. The approach demonstrates that enhancing intrinsic reasoning capabilities is critical for improving preference modeling performance.

## Method Summary
ReasonGRM is a three-stage training pipeline that enhances generative reward models through reasoning capabilities. It begins with Zero-RL to adapt a base Large Reasoning Model (LRM) to outcome-based preference tasks, generating reasoning paths for each query. The R⋆ metric then scores these paths based on their generation likelihood (Self-Consistency) and probability of leading to correct answers (Validity). High-scoring paths are selected for supervised fine-tuning, followed by reinforcement learning on hard examples where the model struggles. This progressive approach builds robust preference discrimination skills while reducing hallucination-prone data.

## Key Results
- Achieves 83.3% average accuracy across RewardBench, RM-Bench, and RMB benchmarks
- Outperforms previous state-of-the-art GRMs by 1.8% and GPT-4o by 5.6%
- Three-stage training pipeline provides incremental improvements: Zero-RL (+1.55%), R⋆-guided SFT (+1.68%), and GRPO refinement
- Demonstrates that enhancing intrinsic reasoning capabilities is critical for preference modeling performance

## Why This Works (Mechanism)

### Mechanism 1: Reasoning Quality Filtration via R⋆ Metric
The R⋆ metric scores reasoning paths based on generation likelihood, favoring paths that efficiently guide the model to correct answers while maintaining internal logical consistency. By filtering training data to include only high-R⋆ paths, the model is exposed to higher-quality examples during fine-tuning, reducing noise and hallucination risk.

### Mechanism 2: Progressive Capability Building via Three-Stage Training
The multi-stage training pipeline builds reasoning capabilities progressively: Zero-RL adapts the base LRM to preference tasks using outcome rewards, R⋆-guided SFT instills high-quality reasoning patterns from selected paths, and hard-case RL refines discriminative boundaries on challenging examples. Each stage builds upon the previous one for stable capability acquisition.

### Mechanism 3: Intrinsic Reasoning Enhancement for Preference Modeling
Starting with a Large Reasoning Model and further specializing it through the ReasonGRM pipeline develops stronger abilities to analyze instructions, identify subtle discrepancies, and form sound judgments. This demonstrates that preference modeling in complex scenarios requires the same underlying reasoning faculties used for tasks like math or code.

## Foundational Learning
- **Reinforcement Learning from Human Feedback (RLHF)**: The overarching framework in which reward models operate to align LLMs with human preferences. Quick check: Can you explain the role of a reward model within the standard RLHF loop?
- **Generative Reward Models (GRMs)**: ReasonGRM is a specific instance of a GRM; understanding its advantages (interpretability, flexibility) over scalar models is key. Quick check: How does a GRM differ from a scalar reward model in its output and capabilities?
- **Supervised Fine-Tuning (SFT)**: A core component of the ReasonGRM pipeline used to instill high-quality reasoning patterns into the model. Quick check: What is the primary goal of the SFT stage in training a language model for a downstream task?

## Architecture Onboarding
- Component map: Base Model (LRM) → Zero-RL Training → Data Generation → R⋆ Scoring & Filtering → SFT Training → Hard Case Identification → Final RL Training
- Critical path: The progression from Base Model through Zero-RL to final RL training, with the R⋆ filtering step being crucial for data quality
- Design tradeoffs: Using a specialized LRM vs. general-purpose LLM (higher baseline but less accessible), R⋆ metric simplicity vs. complexity (computationally efficient but relies on probability estimates), three-stage training vs. end-to-end (more stable but complex)
- Failure signatures: Stage 1 failure (poor initial accuracy), R⋆ failure (selecting flawed paths), Stage 3 failure (no improvement or degradation during hard-case RL)
- First 3 experiments: 1) Reproduce "R⋆ vs. Random Sampling" ablation on small open-source model, 2) Run single-stage training vs. two-stage comparison on RewardBench, 3) Qualitative analysis of high vs. low R⋆ scoring paths

## Open Questions the Paper Calls Out
- How can the R⋆ metric be effectively adapted for datasets characterized by open-ended answers? The current definition relies on specific ground-truth answers, making it challenging for open-ended tasks.
- Does ReasonGRM maintain its performance advantage in open-ended, real-world scenarios involving complex, multi-hop reasoning? Current results are based on structured benchmarks that may not capture deployment environment variability.
- Can the reasoning path selection mechanism be modified to function without explicit ground-truth answers? The current framework assumes labeled "chosen" answers to calculate validity component.

## Limitations
- Evaluation focuses primarily on preference modeling benchmarks rather than broader reward modeling applications
- Paper doesn't extensively explore failure modes or adversarial examples where R⋆ might select flawed reasoning paths
- Computational overhead of generating multiple reasoning paths and evaluating R⋆ scores is not thoroughly analyzed

## Confidence
- **High Confidence**: Three-stage training pipeline effectively improves GRM performance, supported by ablation studies
- **Medium Confidence**: R⋆ metric reliably selects high-quality reasoning paths, though depends on base model's probability calibration
- **Medium Confidence**: Enhancing intrinsic reasoning capabilities is critical for preference modeling performance, based on model comparisons

## Next Checks
1. **Ablation Study Extension**: Test ReasonGRM with alternative reasoning metrics (e.g., reward-weighted likelihood) to verify R⋆ specifically captures reasoning quality
2. **Cross-Domain Evaluation**: Apply ReasonGRM to non-mathematical preference tasks (e.g., code quality assessment, creative writing feedback) to test generalizability
3. **Robustness Analysis**: Create adversarial examples where correct answers have low generation likelihood to test whether R⋆ can distinguish truly flawed reasoning from correct but unlikely paths