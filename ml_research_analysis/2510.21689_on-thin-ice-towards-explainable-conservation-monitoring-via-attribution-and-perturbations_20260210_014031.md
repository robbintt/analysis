---
ver: rpa2
title: 'On Thin Ice: Towards Explainable Conservation Monitoring via Attribution and
  Perturbations'
arxiv_id: '2510.21689'
source_url: https://arxiv.org/abs/2510.21689
tags:
- explanations
- detection
- confidence
- lime
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that post-hoc explainability methods (CAM,
  LIME, and perturbation-based explanations) can validate computer vision models for
  conservation monitoring. Applied to aerial imagery of harbor seals in Glacier Bay
  National Park, these methods reveal that the Faster R-CNN detector relies on seal
  torsos and contours rather than background context, with LayerCAM achieving 67.7%
  attribution ratio and 94.7% maximum saliency hit rate within bounding boxes.
---

# On Thin Ice: Towards Explainable Conservation Monitoring via Attribution and Perturbations

## Quick Facts
- arXiv ID: 2510.21689
- Source URL: https://arxiv.org/abs/2510.21689
- Reference count: 23
- This paper demonstrates that post-hoc explainability methods can validate computer vision models for conservation monitoring, showing that Faster R-CNN detectors rely on seal torsos rather than background context.

## Executive Summary
This study advances conservation monitoring by applying post-hoc explainability methods (CAM, LIME, and perturbation-based explanations) to validate computer vision models for ecological applications. Using aerial imagery of harbor seals in Glacier Bay National Park, the researchers demonstrate that these methods can reveal what features models actually use for predictions, showing that the Faster R-CNN detector relies on seal torsos and contours rather than background context. The approach provides quantitative validation of model reliability and identifies systematic errors, such as confusion between seals and black ice/rocks, moving beyond "black-box" predictions toward auditable, decision-supporting tools that increase trust and guide model improvements.

## Method Summary
The study applies three post-hoc explainability methods to validate computer vision models for conservation monitoring. Class Activation Maps (CAM) and LayerCAM are used to visualize feature attribution, while LIME provides local interpretable model-agnostic explanations. Perturbation experiments systematically test model robustness by masking, adding noise, or blurring regions within bounding boxes. The methods are applied to a Faster R-CNN detector trained on aerial imagery of harbor seals in Glacier Bay National Park. Quantitative metrics include attribution ratios (67.7% for LayerCAM) and maximum saliency hit rates (94.7% within bounding boxes). Perturbation tests show high faithfulness with confidence drops of 0.97-0.87 and flip rates of 80-97% when regions are masked or noised, while blur causes smaller changes.

## Key Results
- LayerCAM achieved 67.7% attribution ratio and 94.7% maximum saliency hit rate within bounding boxes for harbor seal detection
- Perturbation experiments showed high faithfulness: masking or noise reduced confidence by 0.97-0.87 with 80-97% flip rates
- Explanations identified systematic errors, such as confusion between seals and black ice/rocks
- Models rely on seal torsos and contours rather than background context for detection

## Why This Works (Mechanism)
The explainability methods work by mapping the internal decision-making process of deep learning models back to interpretable visual features. CAM and LayerCAM highlight spatial regions that contribute most to predictions, while perturbation analysis tests the causal relationship between input features and model outputs. By systematically altering input regions and measuring confidence changes, researchers can verify whether models use biologically meaningful features (seal torsos) rather than spurious correlations (background context). This combination of attribution visualization and robustness testing provides both qualitative insight into model reasoning and quantitative validation of model reliability, essential for high-stakes conservation applications where model decisions directly impact ecological management.

## Foundational Learning
- **Post-hoc explainability methods**: Techniques applied after model training to understand decision-making (needed because conservation applications require transparency for trust and validation; quick check: visualization should highlight relevant biological features)
- **Class Activation Maps (CAM)**: Convolutional feature visualization showing spatial importance (needed to identify which image regions influence predictions; quick check: should highlight seal bodies rather than background)
- **Perturbation analysis**: Systematically altering inputs to test feature importance (needed to verify causal relationships between features and predictions; quick check: confidence should drop when seal regions are masked)
- **Attribution ratio metrics**: Quantitative measures of how much prediction weight falls within ground truth regions (needed to objectively assess model reliability; quick check: values should exceed 60% for reliable models)
- **Maximum saliency hit rate**: Percentage of cases where highest attention falls within bounding boxes (needed to evaluate localization accuracy; quick check: should approach 95% for precise detectors)

## Architecture Onboarding
Component map: Aerial images -> Faster R-CNN detector -> Object detection predictions -> Explainability methods (CAM/LIME/LayerCAM) -> Attribution visualizations -> Perturbation experiments -> Quantitative metrics

Critical path: Image input → Object detection → Bounding box generation → Attribution analysis → Perturbation testing → Model validation

Design tradeoffs: Single model architecture (Faster R-CNN) vs. broader model comparison; quantitative metrics vs. qualitative expert validation; controlled perturbations vs. real-world variability

Failure signatures: Low attribution ratios (<50%), saliency outside bounding boxes, minimal confidence changes during perturbations, systematic confusion with similar-looking objects

3 first experiments:
1. Apply LayerCAM to detect harbor seals and measure attribution ratio within ground truth bounding boxes
2. Conduct masking perturbation experiment on seal torsos and quantify confidence changes
3. Compare perturbation effects of masking vs. noise vs. blur on model robustness

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Limited scope to a single model architecture (Faster R-CNN) and dataset (harbor seals in Glacier Bay)
- Perturbation experiments used fixed perturbation levels without exploring full parameter space
- Attribution metrics lack comparison with human expert annotations for ground truth validation
- Does not address temporal aspects of conservation monitoring or performance across different environmental conditions

## Confidence
High: The core finding that post-hoc explainability methods can validate conservation monitoring models and reveal model decision-making patterns. The quantitative results from attribution and perturbation analyses are robust within the tested framework.

Medium: The generalizability of these explainability methods to other conservation contexts and model architectures. The systematic error identification is promising but would benefit from broader validation.

Low: The specific threshold values for attribution ratios and saliency metrics as universal benchmarks for model reliability in conservation applications.

## Next Checks
1. Test the same explainability pipeline across multiple conservation monitoring datasets with different species, environments, and model architectures to assess generalizability.
2. Conduct human expert validation studies where ecologists compare model explanations against ground truth annotations to establish reliability thresholds.
3. Implement cross-validation with varying perturbation intensities and types to determine optimal parameters for conservation-specific applications.