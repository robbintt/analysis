---
ver: rpa2
title: Achieving Fairness Without Harm via Selective Demographic Experts
arxiv_id: '2511.06293'
source_url: https://arxiv.org/abs/2511.06293
tags:
- fairness
- group
- groups
- sensitive
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper tackles algorithmic bias in high-stakes domains like\
  \ healthcare by proposing FairSDE, a method to achieve fairness without sacrificing\
  \ performance for any group. FairSDE learns distinct representations for different\
  \ demographic groups through demographic experts\u2014group-specific representations\
  \ paired with personalized classifiers\u2014and dynamically selects between expert\
  \ models and a pooled baseline using a no-harm constrained optimization."
---

# Achieving Fairness Without Harm via Selective Demographic Experts

## Quick Facts
- arXiv ID: 2511.06293
- Source URL: https://arxiv.org/abs/2511.06293
- Reference count: 40
- This paper proposes FairSDE, a method achieving fairness without harming performance for any group through demographic experts and selective model routing.

## Executive Summary
This paper addresses algorithmic bias in high-stakes domains like healthcare by proposing FairSDE, which learns distinct representations for different demographic groups through demographic experts—group-specific representations paired with personalized classifiers. The method dynamically selects between expert models and a pooled baseline using no-harm constrained optimization. Evaluated on three real-world medical imaging datasets and two facial image datasets, FairSDE consistently achieves fairness without harm, outperforming existing methods that typically degrade performance for certain groups.

## Method Summary
FairSDE tackles algorithmic bias by learning demographic-specific representations through a shared feature extractor and group discriminator that enforces group-wise dependence between representations and sensitive attributes. The method creates virtual centers as distribution proxies for each class-group pair, achieving dual objectives of accurate representation and natural sample alignment. Group-specific linear classifiers are trained on these representations, and a post-hoc selection mechanism (either greedy for max-min fairness or integer programming for accuracy parity) chooses between each expert and an ERM baseline while enforcing no-harm constraints that prevent any group from performing worse than the baseline.

## Key Results
- FairSDE achieves fairness without harm on three medical imaging datasets and two facial image datasets
- Outperforms existing methods that typically degrade performance for certain demographic groups
- Improves both fairness and accuracy across all demographic groups compared to ERM baseline
- Decoupled classifiers alone are insufficient without representation changes

## Why This Works (Mechanism)

### Mechanism 1: Group-Wise Dependence Enforcement
Explicitly enforcing group-wise dependence between representations and sensitive attributes enables separable demographic representations that preserve group-specific patterns. A discriminator predicts sensitive attribute from representation, maximizing P(A|f(X)) to make representations group-distinguishable. Virtual centers act as distribution proxies for each class-group pair, with bidirectional alignment compacting representations around their centers while L_div pushes different groups apart. This works when medical image features are not easily separable by sensitive attributes in standard ERM latent space.

### Mechanism 2: Decoupled Representations and Classifiers
Decoupling both representations and classifiers enables demographic experts to capture group-specific patterns better than decoupled classifiers alone. After learning separable representations, independent linear classifiers are trained per group using cross-entropy on their respective group's samples. The coupling of group-specific representations with personalized classifiers forms each expert. This is critical when group-specific patterns exist in high-dimensional image data that shared representations obscure.

### Mechanism 3: No-Harm Constrained Selection
Post-hoc no-harm constrained selection guarantees fairness without harm by dynamically choosing between expert and pooled models per group. Two strategies—greedy selection (maximizes worst-group performance for max-min fairness) and integer programming (minimizes accuracy gap under no-harm constraints)—choose between expert and ERM baseline per group. This works when validation and test sets share similar distributions, ensuring selection decisions generalize.

## Foundational Learning

- **Concept: Group fairness notions (demographic parity, equalized odds, accuracy parity)**
  - Why needed here: FairSDE optimizes for overall accuracy parity and max-min fairness; understanding what each measures is essential for selecting the right selection strategy.
  - Quick check question: What fairness metric does the greedy selection strategy optimize for?

- **Concept: Constrained combinatorial optimization / integer programming**
  - Why needed here: The IP selection strategy formulates model selection as a constrained optimization with binary decision variables and no-harm constraints.
  - Quick check question: What does the no-harm constraint α_a ≥ α_erm,a guarantee?

- **Concept: Representation learning with contrastive objectives**
  - Why needed here: L_div uses pairwise alignment (positive pairs from same class-group, negative from different) similar to contrastive learning principles.
  - Quick check question: Why does L_div include both pairwise alignment AND center diversity terms?

## Architecture Onboarding

- **Component map:**
  - Feature extractor f (ResNet-18 backbone, shared)
  - Group discriminator D (predicts sensitive attribute from representation)
  - Virtual centers {V_a,y} (learnable proxies for class-group distributions)
  - Group-specific classifiers {h_a} (linear heads per demographic group)
  - Selection module (greedy or IP-based post-hoc decision)

- **Critical path:**
  1. Train representation with L_disc + L_virt + L_div + classification loss jointly
  2. Evaluate each expert h_a∘f and ERM baseline on validation set per group
  3. Run selection (greedy or IP) to determine which model to use per group
  4. At inference, route samples to selected model based on their group

- **Design tradeoffs:**
  - Requiring sensitive attributes at inference (medical contexts: acceptable; other domains: may be problematic)
  - Greedy vs. IP selection: greedy prioritizes worst-group; IP balances gap and overall accuracy
  - Hyperparameters λ_1, λ_2, λ_3 control discriminator, virtual center, and diversity contributions

- **Failure signatures:**
  - All groups select ERM (trivial solution): representations insufficiently separable or experts underperforming
  - High variance in ablation "w/o C" (without compactness): representations not compact enough around centers
  - Large validation-test gap: distribution shift violating selection assumption

- **First 3 experiments:**
  1. **Sanity check:** Run ERM baseline on target dataset; establish per-group accuracy and gap as no-harm floor.
  2. **Ablation by component:** Train w/o D, w/o V, w/o C variants to confirm which modules contribute most (paper shows w/o V degrades to ERM).
  3. **Selection strategy comparison:** Run both greedy and IP selection on validation set, compare resulting test-set fairness and utility tradeoffs.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does FairSDE’s selection mechanism perform under significant distribution shift between the validation set used for selection and the test set?
- Basis in paper: The authors state that the method assumes identical distributions between validation and test sets, noting that the strategy "may fail to perform as intended under a significant distribution shift."
- Why unresolved: The paper evaluates performance on standard splits where validation and test data are drawn from the same distribution (i.i.d.), leaving the robustness of the no-harm constraints in non-i.i.d. settings unexplored.
- What evidence would resolve it: Empirical evaluation on datasets with known covariate shift (e.g., training on one hospital's data and validating/testing on another's) to observe if the "no-harm" selection holds.

### Open Question 2
- Question: Does the integer programming approach for expert selection scale effectively to settings with a large number of sensitive attributes or intersectional subgroups?
- Basis in paper: The paper states that finding the exact solution is feasible "given that the number of sensitive attributes is relatively small in fairness studies."
- Why unresolved: While feasible for binary or few-group attributes, the combinatorial nature of the optimization may become computationally prohibitive or result in trivial solutions (defaulting to ERM) when modeling fine-grained intersectionality.
- What evidence would resolve it: Analysis of algorithm runtime and selection behavior as the number of demographic groups $|A|$ increases into the dozens or hundreds.

### Open Question 3
- Question: How robust are the demographic experts to noise or missingness in the sensitive attribute labels during the training or inference phases?
- Basis in paper: The method requires sensitive attributes $a_i$ to calculate group-specific losses and select experts, but the experiments assume clean labels are available.
- Why unresolved: Real-world clinical data often contains incomplete or erroneous demographic recordings. The impact of label noise on the distinctness of learned representations and the validity of the no-harm constraint is unknown.
- What evidence would resolve it: Ablation studies introducing artificial noise or missing values into the sensitive attribute vector $A$ to measure the degradation of the fairness-accuracy trade-off.

## Limitations
- Critical assumption of identical validation and test distributions may not hold in real-world deployment
- Requires sensitive attributes at inference time, which may be problematic in some domains
- IP selection requires tuning an additional λ parameter with unspecified optimal values

## Confidence

- **High confidence:** The core mechanism of achieving fairness without harm through demographic experts and selection strategies is well-supported by quantitative results across multiple datasets.
- **Medium confidence:** The assertion that representation-classifier decoupling is essential for success is supported by ablation but could benefit from broader comparative analysis.
- **Medium confidence:** The fairness improvements are robust, though the extent varies by dataset and attribute, suggesting context-dependent effectiveness.

## Next Checks
1. Test model selection robustness by intentionally introducing validation-test distribution shift to quantify performance degradation.
2. Evaluate the sensitivity of IP selection performance to different λ values to identify optimal tradeoffs.
3. Compare FairSDE against recently proposed demographic-agnostic selection methods that don't require sensitive attributes at inference.