---
ver: rpa2
title: 'Sticking to the Mean: Detecting Sticky Tokens in Text Embedding Models'
arxiv_id: '2507.18171'
source_url: https://arxiv.org/abs/2507.18171
tags:
- tokens
- sticky
- attention
- token
- normal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper identifies a novel type of problematic tokens\u2014\
  sticky tokens\u2014in text embedding models that can significantly degrade performance.\
  \ When repeatedly inserted, these tokens pull sentence similarities toward the mean\
  \ embedding similarity, undermining downstream tasks like retrieval and clustering."
---

# Sticking to the Mean: Detecting Sticky Tokens in Text Embedding Models

## Quick Facts
- arXiv ID: 2507.18171
- Source URL: https://arxiv.org/abs/2507.18171
- Reference count: 40
- Primary result: Novel detection of "sticky tokens" in text embedding models that degrade performance by pulling sentence similarities toward the mean embedding similarity

## Executive Summary
This paper identifies a novel type of problematic tokens—sticky tokens—in text embedding models that can significantly degrade performance. When repeatedly inserted, these tokens pull sentence similarities toward the mean embedding similarity, undermining downstream tasks like retrieval and clustering. The authors introduce Sticky Token Detector (STD), a method that filters sentence pairs and tokens, shortlists candidates via sticky scoring, and validates them with an adaptive threshold. Applied to 40 checkpoints across 14 model families, STD uncovers 868 sticky tokens, many stemming from special or unused vocabulary entries and multilingual subword fragments. Performance tests show sticky tokens cause up to 50% degradation in clustering and retrieval tasks, and attention-layer analysis reveals their disproportionate influence. The findings highlight tokenization robustness as a key issue for future embedding model design.

## Method Summary
The Sticky Token Detector (STD) pipeline identifies sticky tokens through a multi-stage process: (1) Compute the mean pairwise cosine similarity u across all token embeddings in the vocabulary; (2) Filter sentence pairs to retain only those with similarity below u; (3) Filter the vocabulary to remove undecodable or unreachable tokens; (4) Shortlist candidate tokens by inserting each token n=8 times into k=5 sampled sentence pairs and computing a sticky score; (5) Validate candidates against an adaptive threshold (Q3 + 1.5*IQR of the score distribution) to finalize the sticky token list. The method exploits the geometric property that sticky tokens pull similarities toward the mean in anisotropic embedding spaces.

## Key Results
- STD identified 868 sticky tokens across 40 checkpoints spanning 14 model families
- Sticky tokens cause up to 50% degradation in clustering and retrieval performance
- Attention-layer analysis shows sticky tokens disproportionately dominate internal representations
- Many sticky tokens originate from unused vocabulary entries and multilingual subword fragments
- The "pulling toward mean" effect is most pronounced in sentence pairs with initial similarity below the mean

## Why This Works (Mechanism)

### Mechanism 1
The "sticky" behavior results from tokens that disproportionately pull sentence embeddings toward the centroid of the embedding space. Text embedding models often have anisotropic embedding spaces where vectors occupy a narrow cone. Sticky tokens—often under-trained or semantically vacuous—exert a gravitational pull on the final sentence representation. When inserted repeatedly, they shift the pooled embedding toward the mean similarity u of the vocabulary, reducing the variance between arbitrary sentence pairs. This mechanism assumes the embedding space is anisotropic rather than isotropic, creating a distinct "mean" direction that tokens can drift toward.

### Mechanism 2
Sticky tokens degrade performance by dominating the self-attention mechanism, effectively overshadowing meaningful context. The paper finds that sticky tokens capture disproportionate attention weights in intermediate layers. Instead of attending to syntactic or semantic cues, the model attends heavily to the sticky token. This disrupts the hierarchical abstraction of semantics, causing the final representation to reflect the token's anomaly rather than the sentence's meaning. The model's pooling strategy (e.g., mean pooling) integrates information from all tokens, allowing a high-attention token to skew the final vector.

### Mechanism 3
Detection is possible by filtering for sentence pairs with similarity below the mean u, where the "pulling" effect is statistically distinct. The Sticky Token Detector exploits the directional nature of the anomaly. Since sticky tokens pull similarities toward the mean, their effect is most detectable in sentence pairs that start with low similarity (< u). By monitoring which tokens consistently increase these low similarities, STD filters out normal tokens (which lack this consistent "pull") and validates candidates via an adaptive threshold. This mechanism assumes the "pulling" effect is monotonic and consistent across diverse sentence structures for a given sticky token.

## Foundational Learning

- **Concept:** Anisotropy in Embedding Spaces
  - **Why needed here:** The concept of "sticking to the mean" relies on the observation that embedding spaces are often anisotropic (cone-shaped). Without understanding that vectors naturally cluster in a specific direction, the "pulling" mechanism appears random rather than geometric.
  - **Quick check question:** If you plot the PCA of a model's vocabulary, do the points form a sphere or a narrow cluster?

- **Concept:** Tokenization Artifacts (BPE/WordPiece)
  - **Why needed here:** The paper notes that sticky tokens often stem from "unused entries" or "fragmented subwords." Understanding how BPE splits rare words or reserves special tokens is critical to diagnosing where these tokens come from.
  - **Quick check question:** What happens to a token in the vocabulary if it never appears in the pre-training corpus?

- **Concept:** Cosine Similarity Dynamics
  - **Why needed here:** The core metric is the shift in cosine similarity. One must understand that inserting tokens changes the vector magnitude and direction, and how "pulling toward the mean" mathematically reduces the distance between diverse vectors.
  - **Quick check question:** Does adding a high-frequency token typically increase or decrease the magnitude of a sentence embedding?

## Architecture Onboarding

- **Component map:** Data Prep (MTEB STS Datasets) → Sentence Pair Filter (Keep pairs where Sim < u) → Token Prep (Vocabulary → Token Filter) → Scoring Engine (Insertion Operations → Compute Sticky Score → Shortlist Top 2%) → Validator (Adaptive Threshold Check)

- **Critical path:** The accuracy of the Sentence Pair Filter is paramount. If you do not correctly calculate the mean similarity u of the token embeddings to filter the sentence pairs, the "pulling" signal will be lost in the noise of high-similarity pairs.

- **Design tradeoffs:**
  - Sampling vs. Accuracy: The method samples k=5 pairs for scoring but validates on all pairs. Increasing k improves shortlist quality but drastically increases compute time (quadratic with vocabulary size).
  - Threshold Sensitivity: The adaptive threshold (epsilon) uses IQR. A tight threshold finds fewer, more severe tokens; a loose threshold may flag benign tokens.

- **Failure signatures:**
  - False Positives: High-frequency semantic stopwords (e.g., "the") might be flagged if the similarity distribution is skewed.
  - Tokenization Mismatch: If the model's tokenizer logic is not perfectly replicated in the filter (e.g., handling of leading spaces like Ġ), valid tokens may be incorrectly discarded as "undecodable."

- **First 3 experiments:**
  1. Baseline Profiling: Select a target model (e.g., sentence-t5-base), extract all token embeddings, and compute the mean pairwise similarity u to confirm the "cone" geometry.
  2. Replication Attack: Take two semantically unrelated sentences (e.g., "Physics is fun" vs. "I like apples") and iteratively append the token lucrarea. Plot the similarity curve to verify the "sticky" pull manually.
  3. Attention Visualization: Run a sticky token through the model and visualize the attention heatmap of the middle layers to confirm the "high-attention region" signature described in the paper.

## Open Questions the Paper Calls Out

### Open Question 1
Can concrete mitigation strategies, such as tokenizer retraining or embedding space regularization, effectively eliminate sticky tokens without harming model performance? The authors state in the Limitations section that while they identify the impacts, they "do not propose concrete solutions to mitigate sticky tokens (e.g., tokenizer retraining, embedding space regularization)." This remains unresolved as the paper focuses entirely on defining, detecting, and analyzing the phenomenon.

### Open Question 2
Is the anisotropy of the embedding space the definitive root cause of sticky tokens? Appendix C proposes a "conjecture" that "anisotropic text embedding space makes sticky token possible," but notes that "rigorous validation will be required in the future." The current analysis provides empirical correlation but lacks causal proof linking the narrow cone geometry of the vector space directly to the token behavior.

### Open Question 3
Do sticky tokens degrade model performance in complex adversarial scenarios involving context-aware placement or token interleaving? The Limitations section notes that the analysis was limited to fixed positions (prefix/suffix) and "did not extend to more complex adversarial scenarios... like interleaving tokens." The current methodology relies on repeated insertion at fixed intervals, which may not reflect the full spectrum of potential attacks or natural occurrences in running text.

## Limitations

- The phenomenon may be specific to sentence-transformer models and similar architectures that use mean-pooling, rather than universal across all embedding models
- The detection pipeline involves multiple filtering steps that could introduce false positives or false negatives, with uncertain statistical significance of the 868 identified tokens
- Performance degradation claims do not fully isolate whether the "pulling toward mean" effect is the primary mechanism versus attention dominance or other factors

## Confidence

**High Confidence**: The geometric mechanism of "pulling toward mean" in anisotropic embedding spaces is directly supported by experimental evidence showing consistent similarity shifts toward the mean.

**Medium Confidence**: The attention-layer dominance mechanism shows correlation between sticky tokens and high attention weights, but causal relationships are not definitively established.

**Medium Confidence**: The detection methodology's efficiency and accuracy are demonstrated, but optimal parameter values and false positive rates are not extensively validated.

## Next Checks

1. **Validate across diverse embedding architectures**: Apply the Sticky Token Detector to models with fundamentally different architectures (BERT-style with CLS-tokens, isotropy-constrained models, attention-masking models) to test universality claims.

2. **Conduct ablation studies on detection pipeline**: Systematically remove or modify each filtering step and measure impact on detection accuracy using synthetic sticky tokens as ground truth to quantify contribution of each step.

3. **Isolate causal mechanism of performance degradation**: Design controlled experiments with modified sticky tokens (orthogonal to mean direction but retaining attention dominance) to determine whether performance degradation stems from pulling effect or attention dominance.