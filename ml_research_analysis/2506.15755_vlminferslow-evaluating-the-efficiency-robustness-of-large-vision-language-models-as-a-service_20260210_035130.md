---
ver: rpa2
title: 'VLMInferSlow: Evaluating the Efficiency Robustness of Large Vision-Language
  Models as a Service'
arxiv_id: '2506.15755'
source_url: https://arxiv.org/abs/2506.15755
tags:
- adversarial
- vlms
- vlminferslow
- efficiency
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces VLMInferSlow, the first approach to evaluate
  efficiency robustness of large vision-language models (VLMs) in a realistic black-box
  setting where model architecture and parameters are inaccessible. The core method
  uses zero-order optimization combined with fine-grained adversarial objectives designed
  to maximize computational cost while maintaining imperceptible perturbations.
---

# VLMInferSlow: Evaluating the Efficiency Robustness of Large Vision-Language Models as a Service

## Quick Facts
- **arXiv ID**: 2506.15755
- **Source URL**: https://arxiv.org/abs/2506.15755
- **Authors**: Xiasi Wang; Tianliang Yao; Simin Chen; Runqi Wang; Lei YE; Kuofeng Gao; Yi Huang; Yuan Yao
- **Reference count**: 31
- **Primary result**: First black-box evaluation of efficiency robustness in VLMs, achieving up to 128.47% increase in sequence length and 115.19% increase in energy consumption

## Executive Summary
VLMInferSlow introduces the first approach to evaluate efficiency robustness of large vision-language models (VLMs) in realistic black-box settings where model architecture and parameters are inaccessible. The method employs zero-order optimization combined with fine-grained adversarial objectives designed to maximize computational cost while maintaining imperceptible perturbations. Tested across four VLMs and two datasets, VLMInferSlow achieves significant efficiency degradation metrics while keeping adversarial images nearly indistinguishable from benign ones.

## Method Summary
VLMInferSlow addresses the challenge of evaluating efficiency robustness in VLMs deployed as API services by developing a black-box adversarial attack framework. The approach uses zero-order optimization techniques to generate adversarial perturbations without requiring knowledge of the model's internal architecture or parameters. The method specifically targets efficiency-related metrics including sequence length, response latency, and energy consumption. By carefully designing adversarial objectives that maximize computational costs while maintaining perceptual similarity to benign images, VLMInferSlow can effectively stress-test the efficiency robustness of VLMs under realistic deployment conditions.

## Key Results
- Achieves up to 128.47% increase in sequence length under adversarial attacks
- Increases response latency by up to 105.56% in black-box settings
- Boosts energy consumption by up to 115.19% while maintaining near-imperceptible image perturbations

## Why This Works (Mechanism)
VLMInferSlow exploits the computational bottlenecks in VLMs by crafting adversarial perturbations that force models to process more tokens and perform additional computations without significantly altering the visual content. The zero-order optimization approach allows the attack to work effectively in black-box settings where gradient information is unavailable. By focusing on efficiency metrics rather than just accuracy, the method reveals vulnerabilities that traditional adversarial attacks might miss, demonstrating that VLMs can be made inefficient without being completely broken.

## Foundational Learning
**Zero-order optimization**: A derivative-free optimization technique crucial for black-box settings where gradients are inaccessible. Why needed: Traditional white-box attacks rely on gradient information, which is unavailable when models are deployed as APIs. Quick check: Verify that the optimization converges without requiring model gradients.

**Efficiency robustness**: The ability of models to maintain computational efficiency under adversarial conditions. Why needed: As VLMs become widely deployed services, their computational efficiency becomes critical for practical deployment. Quick check: Measure baseline efficiency metrics before and after attacks.

**Perceptual similarity metrics**: L2 distance and feature dissimilarity measures to ensure adversarial perturbations remain imperceptible. Why needed: The attack must balance effectiveness with stealth to be practically relevant. Quick check: Validate that generated adversarial images are visually indistinguishable from benign ones.

## Architecture Onboarding

**Component map**: Input image -> Perturbation generator -> VLM API -> Efficiency metrics -> Optimization loop

**Critical path**: The optimization loop iteratively generates perturbations, queries the VLM API, measures efficiency degradation, and updates the perturbation based on feedback.

**Design tradeoffs**: The method prioritizes black-box compatibility over attack strength, accepting lower attack success rates in exchange for applicability to real-world API deployments.

**Failure signatures**: Ineffective attacks occur when perturbations fail to trigger significant efficiency degradation or when they become perceptually obvious.

**First experiments**:
1. Test basic perturbation generation on a single VLM with known architecture
2. Measure efficiency metrics on benign images to establish baselines
3. Evaluate transferability of white-box attacks to black-box settings

## Open Questions the Paper Calls Out
The paper highlights several open questions including the development of defense mechanisms against efficiency-based adversarial attacks, the generalizability of these attacks to other types of models beyond VLMs, and the potential for combining efficiency and accuracy attacks to create more comprehensive threat models.

## Limitations
- Evaluation focuses exclusively on efficiency robustness without addressing broader safety implications
- Assumes specific API access patterns that may not capture all real-world deployment variations
- Relies on specific datasets that may not fully represent production image distributions
- Does not investigate potential defenses against efficiency-based adversarial attacks

## Confidence

**High confidence**: Quantitative improvements achieved by VLMInferSlow over baseline methods, demonstrated through consistent measurements across multiple VLMs and efficiency metrics.

**Medium confidence**: Transferability of white-box methods to black-box settings, given the inherent complexity and variability of black-box environments.

**Medium confidence**: Perceptual similarity of adversarial images to benign ones, based on L2 distances and feature dissimilarity metrics, though human perception factors may not be fully captured.

## Next Checks
1. Evaluate VLMInferSlow across a broader range of datasets and real-world image distributions to assess generalizability beyond ImageNet-1K and CIFAR-10.
2. Test the adversarial attacks under varying API access patterns and network conditions to understand robustness in diverse deployment scenarios.
3. Investigate the effectiveness of VLMInferSlow against additional VLMs not included in the current evaluation, particularly emerging models with different architectural designs.