---
ver: rpa2
title: 'CoLLM: A Large Language Model for Composed Image Retrieval'
arxiv_id: '2503.19910'
source_url: https://arxiv.org/abs/2503.19910
tags:
- image
- text
- modification
- mtcir
- collm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CoLLM introduces a method for composed image retrieval (CIR) that
  eliminates the need for expensive triplet datasets by synthesizing triplets on-the-fly
  from image-caption pairs. The approach combines spherical linear interpolation for
  reference image embeddings and LLM-based generation of modification text, enabling
  supervised training without manual annotations.
---

# CoLLM: A Large Language Model for Composed Image Retrieval

## Quick Facts
- arXiv ID: 2503.19910
- Source URL: https://arxiv.org/abs/2503.19910
- Reference count: 40
- Primary result: Achieves state-of-the-art performance on composed image retrieval benchmarks, with up to 15% improvement in recall

## Executive Summary
CoLLM introduces a novel approach to composed image retrieval (CIR) that eliminates the need for expensive manually annotated triplet datasets. The method synthesizes triplets on-the-fly from image-caption pairs using spherical linear interpolation (Slerp) for reference image embeddings and LLM-based generation of modification text. This enables supervised training without manual annotations while achieving state-of-the-art performance on established CIR benchmarks. Additionally, CoLLM leverages LLMs to generate composed query embeddings, improving multimodal understanding compared to shallow transformers.

## Method Summary
CoLLM employs a two-stage training approach. First, it pre-trains on image-caption pairs by synthesizing triplets using Slerp interpolation between target images and their nearest neighbors to create reference embeddings, combined with template-based text interpolation for modification text. Second, it fine-tunes on the MTCIR dataset using contrastive loss. The architecture consists of a vision encoder, an adapter mapping visual features to the LLM's semantic space, the LLM itself (specifically SFR-Embedding-2), and a projection layer. The model is trained with 3-way contrastive loss combining image-only, text-only, and composed query losses.

## Key Results
- Achieves state-of-the-art performance on CIRR, Fashion-IQ, and CIRCO benchmarks
- Demonstrates up to 15% improvement in recall@1 compared to previous methods
- Introduces MTCIR, a new large-scale dataset with 3.4M image pairs and 17.7M modification texts

## Why This Works (Mechanism)

### Mechanism 1: On-the-Fly Triplet Synthesis via Slerp and Text Interpolation
CoLLM synthesizes triplets from image-caption pairs by using Slerp to interpolate between an image embedding and its nearest neighbor, creating a synthetic reference embedding. Combined with interpolated modification text generated from templates, this forms pseudo-triplets for contrastive learning. The core assumption is that interpolating between visually similar images produces a meaningful "reference" state, and the textual difference describes a valid "modification" from that state to the target image.

### Mechanism 2: LLM/LLEM-Driven Composed Query Embedding
A Large Language Embedding Model (LLEM) generates superior joint embeddings for composed queries by leveraging its deep semantic understanding and instruction-following capabilities. The model processes reference images (via visual adapter) and modification text with specific instructions to produce unified embeddings for retrieval. The pre-trained knowledge and architectural depth of LLEMs allow better fusion and reasoning over visual and textual modalities than shallower fusion mechanisms.

### Mechanism 3: High-Quality Synthetic Data via MTCIR
The Multi-Text CIR (MTCIR) dataset provides higher-quality training signal by using a two-stage MLLM+LLM pipeline to generate multiple concise, naturalistic modification texts for each image pair. Images are paired based on CLIP similarity, detailed captions are generated by an MLLM, then an LLM produces multiple short, human-like modification texts describing specific changes. This better reflects human query formulation and increases data diversity compared to single-text approaches.

## Foundational Learning

- **Concept: Composed Image Retrieval (CIR)**
  - Why needed here: This is the core task - retrieving a target image based on a reference image and modification text
  - Quick check question: How does CIR differ from standard text-to-image or image-to-image retrieval?

- **Concept: Spherical Linear Interpolation (Slerp)**
  - Why needed here: Primary technique for synthesizing reference image embeddings by interpolating on a hypersphere rather than straight line
  - Quick check question: Why is Slerp used instead of simple linear interpolation for embeddings?

- **Concept: Large Language Embedding Models (LLEMs)**
  - Why needed here: CoLLM uses LLEMs, not just general LLMs - they're fine-tuned for embedding generation and retrieval tasks
  - Quick check question: What is the key difference between a standard LLM and an LLEM in the context of this model?

## Architecture Onboarding

- **Component Map:** Vision Encoder → Adapter → LLM → Projection Layer → Contrastive Loss
- **Critical Path:** Performance hinges on Triplet Synthesis Module creating meaningful pseudo-examples and LLM effectively fusing the multimodal query. Failure in Slerp interpolation or visual adapter breaks the entire pipeline.
- **Design Tradeoffs:**
  - Slerp α value: α=0.5 is optimal; higher values degrade performance
  - LLM vs. LLEM: LLEMs provide better performance than general LLMs for retrieval
  - Text Synthesis Ratio: ~75% of samples with synthetic text is optimal; 100% not necessary
- **Failure Signatures:**
  - Low Recall: Check CLIP similarity of image pairs being fed to synthesis module
  - Poor Fusion: Check visual adapter's training and LLM's instruction tuning
  - Overfitting on Synthetic Data: Monitor validation recall per epoch; stop at epoch 1 for small datasets
- **First 3 Experiments:**
  1. Implement image-caption pre-training pipeline (Slerp + text templates) on CC3M subset to verify synthesis logic
  2. Train two models: one with random in-batch neighbors for Slerp and one with nearest neighbors to confirm neighbor selection importance
  3. Swap LLM backbone (Mistral-7B to SFR-Embedding-2) to quantify gain from using embedding-specialized model

## Open Questions the Paper Calls Out
None

## Limitations
- Relies on nearest neighbor similarity assumption that may not hold for all image pairs
- Text interpolation templates may fail for complex visual modifications
- No direct comparison of multi-text vs single-text synthetic data approaches

## Confidence

- **High Confidence**: Empirical performance improvements on CIRR, Fashion-IQ, and CIRCO benchmarks; ablation studies on Slerp α, LoRA rank, and text synthesis ratio
- **Medium Confidence**: LLM-based composed query embedding mechanism; MTCIR data quality claims
- **Low Confidence**: Scalability claims regarding eliminating triplet annotations; nearest neighbor selection criticality without quantitative similarity analysis

## Next Checks

1. **Neighbor Similarity Analysis**: Measure distribution of cosine similarities between target images and their nearest neighbors in pre-training batches to validate semantic coherence

2. **Text Interpolation Ablation**: Compare models trained with multiple short modification texts vs single long text vs no synthetic text to test multi-text benefits

3. **Visual Adapter Robustness**: Systematically vary adapter architecture (layer count, width, activation functions) to identify optimal capacity for visual-to-language mapping