---
ver: rpa2
title: Are Stereotypes Leading LLMs' Zero-Shot Stance Detection ?
arxiv_id: '2510.20154'
source_url: https://arxiv.org/abs/2510.20154
tags:
- stance
- text
- dataset
- detection
- complexity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates bias in large language models (LLMs) for
  zero-shot stance detection, focusing on how stereotypes embedded in their training
  data influence predictions based on dialect (African-American English vs Standard
  American English) and text complexity (measured by Flesch-Kincaid readability).
  The authors automatically annotated existing stance detection datasets with these
  attributes and evaluated five popular LLMs (GPT-3.5, Llama, Mistral, Falcon, and
  Flan) using fairness metrics including Equal Opportunity, Disparate Impact, and
  Predictive Parity.
---

# Are Stereotypes Leading LLMs' Zero-Shot Stance Detection ?

## Quick Facts
- arXiv ID: 2510.20154
- Source URL: https://arxiv.org/abs/2510.20154
- Authors: Anthony Dubreuil; Antoine Gourru; Christine Largeron; Amine Trabelsi
- Reference count: 11
- Key outcome: LLMs exhibit significant bias in zero-shot stance detection based on dialect and text complexity, with Falcon showing highest bias and GPT-3.5 lowest.

## Executive Summary
This paper investigates how stereotypes embedded in large language models' training data influence zero-shot stance detection performance across demographic and linguistic attributes. The authors automatically annotate existing stance detection datasets with African-American English (AAE) vs Standard American English (SAE) dialect labels and Flesch-Kincaid readability scores, then evaluate five popular LLMs (GPT-3.5, Llama, Mistral, Falcon, and Flan) using multiple fairness metrics. Results reveal significant biases: models incorrectly associate pro-marijuana stances with low text complexity and AAE dialect with opposition to Donald Trump. Falcon exhibited the highest overall bias while GPT-3.5 showed the lowest, demonstrating that demographic linguistic cues and readability influence stance predictions even without explicit demographic markers.

## Method Summary
The authors selected three stance detection datasets (SCD, ARC, PStance) and automatically annotated them with sensitive attributes using the Blodgett dialect classifier for AAE/SAE detection and Flesch-Kincaid readability scores for text complexity categorization. They downsampled the datasets to achieve balanced class distributions across both attributes, resulting in final samples of approximately 339 per dialect group and 262 per complexity bin. Five LLMs were evaluated using a Context Analyze prompt format to generate binary stance predictions (FAVOR/AGAINST). Fairness was measured using Equal Opportunity (EO), Demographic Parity (DP), and Predictive Parity (PP) metrics, comparing performance across demographic groups. The analysis focused on identifying stereotype-driven biases by examining how models associate specific attributes with stance labels.

## Key Results
- LLMs show significant bias across dialect (AAE vs SAE) and readability attributes, with bias magnitude varying substantially across models
- Falcon exhibited the highest overall bias across all fairness metrics, while GPT-3.5 showed the lowest bias levels
- Models incorrectly associate low text complexity with pro-marijuana stance and AAE dialect with anti-Trump sentiment, demonstrating reliance on demographic linguistic cues

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs inherit societal stereotypes from pretraining corpora that subsequently influence zero-shot stance predictions on political and social topics.
- Mechanism: Statistical associations between linguistic variants (e.g., AAE vs. SAE) and stance labels in web-scale pretraining data become encoded in model parameters; during inference, these associations trigger biased predictions even without explicit demographic markers.
- Core assumption: The measured bias originates primarily from pretraining data distribution rather than from RLHF, instruction tuning, or prompting artifacts.
- Evidence anchors:
  - [abstract] "Large Language Models inherit stereotypes from their pretraining data, leading to biased behavior toward certain social groups"
  - [section 2.2] "politically skewed pretraining data can propagate biases into LLMs' applications"
  - [corpus] Limited direct corpus support; neighbor papers address debiasing but not pretraining-to-inference causal chains.
- Break condition: If bias persists after controlled pretraining on demographically balanced corpora, the mechanism would require revision.

### Mechanism 2
- Claim: Dialectal features (AAE vs. SAE) function as implicit social signals that LLMs use—often incorrectly—to infer stance toward political figures.
- Mechanism: The Blodgett et al. (2016) classifier extracts dialect probabilities; models conditional on these linguistic markers produce different stance predictions for otherwise similar content.
- Core assumption: Dialect detection accurately proxies perceived sociocultural identity, and models respond to these cues systematically.
- Evidence anchors:
  - [section 3.1.1] "AAE can be grammatically and syntactically different from Standard American English, serving as a proxy for linguistic and sociocultural group membership"
  - [results] FLAN associates AAE more easily with being against Trump than SAE (Figure 6)
  - [corpus] Related work on political stance debiasing via counterfactual calibration supports dialect-driven bias but does not isolate mechanism.
- Break condition: If controlling for topic and sentiment eliminates dialect-based EO gaps, dialect alone may not be causal.

### Mechanism 3
- Claim: Text complexity (readability) acts as a socioeconomic proxy, triggering stereotype-based stance predictions.
- Mechanism: Flesch-Kincaid scores categorize texts into complexity bins; models assign different stance probabilities across bins for identical targets (e.g., low complexity → pro-marijuana).
- Core assumption: Readability correlates with author attributes the model has learned to associate with specific political positions.
- Evidence anchors:
  - [section 3.1.2] "recent work found correlations between readability and socio-economic status on social media"
  - [results, stereotype 1] "LLMs tend to associate a highly complex text with opposition to marijuana and a lower complexity with support toward it"
  - [corpus] Corpus neighbors address stance detection broadly but lack direct links between readability and bias.
- Break condition: If explicitly controlling for topic keywords eliminates complexity-based bias, the mechanism may be confounded by topic rather than complexity per se.

## Foundational Learning

- Concept: **Equal Opportunity (Hardt et al., 2016)**
  - Why needed here: Core fairness metric used throughout the paper; EO measures whether true positives are equally likely across groups.
  - Quick check question: If a model correctly predicts "favor" for 80% of SAE texts but only 60% of AAE texts (both truly favor), what is the EO value?

- Concept: **Flesch-Kincaid Readability Score**
  - Why needed here: Quantifies text complexity as a sensitive attribute; discretization into Low/Medium/High/Very High enables bias measurement across readability levels.
  - Quick check question: A tweet with short sentences and few syllables per word will have a higher or lower Flesch-Kincaid score?

- Concept: **Zero-Shot Stance Detection**
  - Why needed here: The paper's task setting; models must classify stance without task-specific training, making them reliant on parametric knowledge where stereotypes reside.
  - Quick check question: Why might zero-shot settings amplify bias compared to fine-tuned models?

## Architecture Onboarding

- Component map: Five LLMs (Mistral-7B, Llama3-8B, Falcon-7B, FLAN-T5-large, GPT-3.5-turbo) → Context Analyze prompt → binary stance output (FAVOR/AGAINST) → fairness evaluation via EO, Demographic Parity, Predictive Parity
- Critical path: Dataset selection → sensitive attribute annotation (Blodgett AAE/SAE classifier + Flesch-Kincaid) → downsampling for balance → prompt-based inference → fairness metric computation across attribute groups
- Design tradeoffs: Downsampling ensures class balance but reduces statistical power (final samples: ~339 per group for AAE/SAE; ~262 per complexity bin); using longer texts (SCD) improves Flesch-Kincaid reliability but limits dataset applicability
- Failure signatures:
  - High neutral prediction rates (Llama: 61% on PStance) indicate instruction-following failures
  - Opposing bias directions across models (Falcon vs. others on Obama) suggest training data or instruction-tuning divergence
- First 3 experiments:
  1. Replicate EO computation on a held-out subset to confirm bias patterns are stable across random seeds
  2. Run ablation with shuffled dialect labels to verify EO gaps collapse when attribute-label association is broken
  3. Test alternative prompting strategies (Chain-of-Thought vs. Context Analyze) to assess whether prompt design modulates measured bias

## Open Questions the Paper Calls Out
None

## Limitations
- The study relies on automatic annotations for dialect and readability, which may introduce noise and don't capture full linguistic diversity
- Downsampling to achieve class balance reduces statistical power and may not reflect real-world data distributions
- Zero-shot evaluation doesn't explore how instruction-tuning or few-shot examples might mitigate identified biases
- Focus on five specific LLMs limits generalizability to broader model families or architectures

## Confidence
- **High confidence**: The core finding that LLMs exhibit measurable bias across dialect and readability attributes is well-supported by multiple fairness metrics and replicated across five different models
- **Medium confidence**: The interpretation that observed biases stem from pretraining data stereotypes rather than other factors requires further investigation
- **Medium confidence**: The specific stereotype associations identified are well-documented but their universality across contexts remains uncertain

## Next Checks
1. Conduct controlled pretraining experiments using demographically balanced corpora to isolate whether observed biases originate from pretraining data versus other factors
2. Replicate the analysis using alternative dialect classifiers and socioeconomic indicators to verify whether bias patterns persist across different measurement approaches
3. Test whether instruction-tuning with fairness constraints, few-shot examples, or post-hoc calibration techniques can effectively reduce the identified biases without substantially degrading overall performance