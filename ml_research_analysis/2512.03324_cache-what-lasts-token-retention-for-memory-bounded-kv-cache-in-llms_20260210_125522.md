---
ver: rpa2
title: 'Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs'
arxiv_id: '2512.03324'
source_url: https://arxiv.org/abs/2512.03324
tags:
- retention
- tokens
- arxiv
- cache
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the memory bottleneck in long-horizon LLM
  inference caused by the quadratic cost of self-attention and the ever-growing key-value
  (KV) cache. Existing approaches either incur high orchestration costs or rely on
  unreliable attention-based proxies of importance.
---

# Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs

## Quick Facts
- arXiv ID: 2512.03324
- Source URL: https://arxiv.org/abs/2512.03324
- Authors: Ngoc Bui; Shubham Sharma; Simran Lamba; Saumitra Mishra; Rex Ying
- Reference count: 40
- Primary result: A novel KV cache eviction method using learned token retention scores outperforms strong baselines on long-context tasks while reducing memory usage.

## Executive Summary
This paper addresses the memory bottleneck in long-horizon LLM inference caused by the quadratic cost of self-attention and the ever-growing key-value (KV) cache. Existing approaches either incur high orchestration costs or rely on unreliable attention-based proxies of importance. To solve this, the authors propose TRIM-KV, a novel approach that learns each token's intrinsic importance at creation time via a lightweight retention gate. Each gate predicts a scalar retention score that decays over time, reflecting the long-term utility of the token for a specific layer and head. Tokens with low scores are evicted when the memory budget is exceeded, ensuring that the cache always contains the most critical tokens. TRIM-KV is trained efficiently through distillation from a frozen LLM combined with a capacity loss, requiring only gate fine-tuning and adding negligible inference overhead. Across mathematical reasoning (GSM8K, MATH-500, AIME24), procedural generation (LongProc), conversational long-memory benchmarks (LongMemEval), and long-context understanding (LongBench and SCBench), TRIM-KV consistently outperforms strong eviction and learnable retrieval baselines, especially in low-memory regimes. Remarkably, it even surpasses full-cache models in some settings, showing that selective retention can serve as a form of regularization, suppressing noise from uninformative tokens. Qualitative analyses further reveal that learned retention scores align with human intuition, naturally recovering heuristics such as sink tokens, sliding windows, and gist compression without explicit design. Beyond efficiency, retention scores provide insights into layer- and head-specific roles, suggesting a new path toward LLM interpretability.

## Method Summary
TRIM-KV introduces retention gates—per-layer, per-head MLPs that predict a scalar retention score β∈[0,1] for each generated token. This score decays exponentially over time (β^(t−i)), with low-scoring tokens evicted when memory budgets are exceeded. Training freezes the pretrained LLM backbone and optimizes only the retention gates using a combined loss of distillation loss (KL divergence + NTP loss) to preserve output distribution and a capacity loss (hinge penalty) to enforce sparsity. At inference, tokens are evicted based on minimum decayed scores. The method requires no additional data beyond standard pretraining corpora and adds negligible inference overhead while achieving significant memory savings.

## Key Results
- TRIM-KV outperforms strong eviction and learnable retrieval baselines on long-context tasks across multiple benchmarks.
- In low-memory regimes (e.g., 256-512 tokens), TRIM-KV achieves comparable or better performance than full-cache models.
- Retention scores naturally recover human-intuitive heuristics (sink tokens, sliding windows, gist compression) without explicit design.
- The approach provides insights into layer- and head-specific roles, suggesting new paths toward LLM interpretability.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Token intrinsic importance can be predicted at creation time from contextual embeddings, independent of future queries.
- Mechanism: A lightweight retention gate (MLP: d→512→h) maps each token's hidden state to a scalar β∈[0,1] at generation time. This score decays exponentially as β^(t−i), so high-β tokens persist while low-β tokens fade quickly. At inference, eviction simply removes the token with lowest current score when budget M is exceeded.
- Core assumption: Token embeddings encode sufficient information about long-term utility without observing future attention patterns.
- Evidence anchors:
  - [Section 4.1] "We posit that the contextual embedding of a token already encodes much of its long-term utility."
  - [Section 5.1.2] "Retention gates assign high scores to task-relevant tokens... In contrast, whitespace and punctuation receive low retention scores."
  - [corpus] Weak direct corpus support for creation-time importance prediction; neighbor papers focus on attention-based or similarity-based eviction heuristics.
- Break condition: If tokens' future importance is primarily determined by query-dependent interactions (e.g., rare retrieval patterns), creation-time predictions would systematically miss critical tokens.

### Mechanism 2
- Claim: Exponential decay provides a differentiable proxy for hard eviction decisions, enabling gradient-based training of retention gates.
- Mechanism: During training, retention-gated attention substitutes binary α∈{0,1} with smooth decay β^(t−i) in attention weights. This allows backpropagation through the "forgetting" process while approximately modeling eviction behavior at inference.
- Core assumption: Exponential decay trajectories approximate the true importance trajectory of tokens under eviction.
- Evidence anchors:
  - [Section 4.1] "To overcome these limitations, we adopt an exponential decay formulation, β̄ᵗⁱᵢ = βᵢ^(t−i) where βᵢ∈[0,1], to model the retention rate of token i over time."
  - [Section 4.1] Connection to Ebbinghaus forgetting curve theory is discussed as intuitive justification.
  - [corpus] No direct corpus comparison of decay functions; alternative approaches use learned sparsity patterns (Titans, Lattice).
- Break condition: If optimal retention trajectories are highly non-monotonic (e.g., tokens important, then unimportant, then important again), exponential decay would be a poor approximation.

### Mechanism 3
- Claim: Joint training of all retention gates with distillation + capacity loss produces globally coordinated eviction policies that outperform greedy layer-wise heuristics.
- Mechanism: Freeze pretrained LLM weights. Train only retention gates with L_quality (KL divergence + NTP loss) to preserve output distribution, plus L_cap (hinge penalty when expected cache exceeds M) to enforce sparsity. The model learns which tokens each head should retain under coordination constraints.
- Core assumption: Per-head retention decisions can be locally suboptimal while achieving globally optimal cache utilization.
- Evidence anchors:
  - [Section 4.2] "This holistic approach mitigates error propagation, allowing the model to learn a coordinated, globally optimal caching policy rather than greedy layer-wise decisions."
  - [Table 4] Ablation shows removing L_cap causes sharp performance drop (74.0%→42.9% on AIME24).
  - [corpus] LocRet (cited in Appendix B.3) uses independent per-head prediction with hand-crafted sliding window; TRIM-KV outperforms it.
- Break condition: If heads have conflicting retention needs that cannot be resolved without degrading attention quality, joint optimization may fail to converge or produce unstable policies.

## Foundational Learning

- Concept: **KV Cache in Autoregressive Transformers**
  - Why needed here: TRIM-KV operates on the KV cache; you must understand that during decoding, past key-value pairs are cached to avoid recomputation, causing O(T) memory growth.
  - Quick check question: During generation of token t, which vectors does the attention computation access?

- Concept: **Softmax Attention and Quadratic Complexity**
  - Why needed here: The paper's motivation stems from O(T²) attention computation and O(T) cache growth; retention gates modulate attention weights directly.
  - Quick check question: Why does doubling context length quadruple attention computation time but only double KV cache size?

- Concept: **Knowledge Distillation**
  - Why needed here: Training uses KL divergence between retention-gated model and frozen original model to preserve behavior while learning compression.
  - Quick check question: If the student model's output distribution diverges from the teacher, what symptom would you observe during evaluation?

## Architecture Onboarding

- Component map:
  - Token embedding -> Retention gate (MLP) -> β score -> Retention-gated attention with decay modulation -> Cache management

- Critical path:
  1. Forward pass: Token embedding → retention gate → β score → attention with decay modulation
  2. Backward pass: L_quality + λ_cap·L_cap → gradients to gate parameters only (frozen backbone)
  3. Inference: Generate token → compute β → add to cache → evict lowest-scored if over budget

- Design tradeoffs:
  - **Gate architecture**: MLP (512 hidden) outperforms linear projection but adds ~0.5% parameters per layer
  - **Initial bias**: Must initialize to large positive value (b=8.0) to start near β≈1; otherwise premature forgetting degrades training
  - **Capacity M during training**: Functions as soft constraint; set to expected deployment budget. M=∞ removes sparsity pressure and hurts performance.

- Failure signatures:
  - **Premature eviction**: If bias initialization too low, early training shows rapid quality collapse (Table 4 shows L_cap removal drops pass@1 by ~40%)
  - **No sparsity emergence**: If λ_cap too small, gates learn β≈1 everywhere, yielding no compression benefit
  - **Over-compression**: If M set too low during training, model may learn overly aggressive eviction that doesn't generalize

- First 3 experiments:
  1. **Sanity check**: Train retention gates with M=∞, λ_cap=0. Verify outputs match frozen model exactly (all β≈1).
  2. **Capacity sweep**: Fix λ_cap=1.0, vary M∈{128, 512, 1024, 2048} on validation set. Plot quality vs. budget Pareto frontier.
  3. **Ablation by loss component**: Compare L_quality only vs. L_quality+L_cap vs. L_KL+L_NTP+L_cap on a small benchmark (e.g., GSM8K subset) to verify each term's contribution before full training runs.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does training retention gates jointly with the attention layers during pretraining yield better efficiency-quality trade-offs compared to the proposed post-hoc plug-in method?
- **Basis in paper:** [explicit] The Conclusion states, "A natural next step is to replace standard attention with retention-gated attention and train the retention mechanism jointly with the attention layers during pretraining or post-training."
- **Why unresolved:** The current work keeps the backbone LLM frozen and trains only the gates via distillation. It is unknown if co-training would allow the backbone to rely more heavily on the retention mechanism, potentially improving coordination.
- **What evidence would resolve it:** A comparison of perplexity and downstream task performance between a model pretrained with retention-gated attention versus one adapted post-hoc using TRIM-KV.

### Open Question 2
- **Question:** How can system-level implementations (e.g., FlashAttention) be modified to efficiently support per-head variable-length caches enabled by heterogeneous retention scores?
- **Basis in paper:** [explicit] Section 5.1.2 notes that retention scores enable heterogeneous budgets, but "existing KV-cache and FlashAttention implementations assume uniform sequence lengths across heads... enabling efficient per-head variable-length caches is left to future work."
- **Why unresolved:** Current GPU kernels are optimized for fixed cache sizes per layer, preventing TRIM-KV from fully exploiting the variable sparsity levels learned by different heads.
- **What evidence would resolve it:** The development of a custom attention kernel that handles variable sequence lengths per head without increasing memory fragmentation or latency.

### Open Question 3
- **Question:** Can retention gates trained on text data generalize effectively to multimodal inputs where visual or audio tokens may follow different importance distributions?
- **Basis in paper:** [explicit] In the Future Work section, the authors state they "plan to extend retention gating to multimodal inputs."
- **Why unresolved:** The "intrinsic importance" of a token currently relies on text-based embeddings. It is unclear if the lightweight retention gate MLP can capture the distinct semantic value of visual tokens without re-architecting the gate or retraining the entire model.
- **What evidence would resolve it:** Experiments applying the trained retention gates (or slight variations) to multimodal benchmarks (e.g., video understanding) to see if the eviction patterns align with visual saliency.

## Limitations

- The core assumption that token intrinsic importance can be predicted at creation time from contextual embeddings remains empirically validated but theoretically under-specified.
- The exponential decay formulation, while intuitive and differentiable, lacks rigorous comparison to alternative retention trajectory models.
- The interpretability claims regarding retention scores providing insights into layer- and head-specific roles are suggestive but lack systematic validation or comparison to established interpretability methods.

## Confidence

- **High Confidence:** The empirical results demonstrating TRIM-KV's effectiveness across multiple benchmarks and memory budgets are robust. The ablation studies showing the necessity of capacity loss and proper bias initialization are convincing.
- **Medium Confidence:** The claim that creation-time prediction of token importance is sufficient for effective eviction. While results support this, the mechanism could fail on tasks requiring complex retrieval patterns or non-monotonic importance trajectories.
- **Low Confidence:** The interpretability claims regarding retention scores providing insights into layer- and head-specific roles. The qualitative analysis is suggestive but lacks systematic validation or comparison to established interpretability methods.

## Next Checks

1. **Dynamic Budget Validation:** Train and evaluate TRIM-KV across a broader range of memory budgets (e.g., M=64 to M=4096) on multiple tasks to map the full quality-vs-budget Pareto frontier and identify regimes where TRIM-KV breaks down or excels unexpectedly.

2. **Retention Trajectory Analysis:** Systematically compare exponential decay against alternative retention trajectory models (e.g., learned per-token decay rates, non-monotonic functions) on a controlled task to determine whether the exponential assumption is optimal or merely convenient for training.

3. **Cross-Task Generalization:** Evaluate TRIM-KV trained on one task domain (e.g., math) on a completely different domain (e.g., code generation or story continuation) without fine-tuning to assess whether learned retention policies transfer or whether they overfit to domain-specific token importance patterns.