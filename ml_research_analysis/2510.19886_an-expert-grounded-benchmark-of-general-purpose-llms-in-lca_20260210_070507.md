---
ver: rpa2
title: An Expert-grounded benchmark of General Purpose LLMs in LCA
arxiv_id: '2510.19886'
source_url: https://arxiv.org/abs/2510.19886
tags:
- llms
- were
- reviewers
- tasks
- responses
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study provides the first expert-grounded benchmark of general-purpose
  LLMs for life cycle assessment (LCA), addressing the absence of standardized evaluation
  frameworks in a field with no clear ground truth. Seventeen experienced practitioners
  reviewed 168 AI-generated answers from eleven LLMs across 22 LCA-related tasks,
  assessing scientific accuracy, explanation quality, robustness, verifiability, and
  adherence to instructions.
---

# An Expert-grounded benchmark of General Purpose LLMs in LCA

## Quick Facts
- arXiv ID: 2510.19886
- Source URL: https://arxiv.org/abs/2510.19886
- Reference count: 5
- 37% of AI-generated LCA answers contained inaccurate or misleading information

## Executive Summary
This study presents the first expert-grounded benchmark of general-purpose LLMs for life cycle assessment (LCA), addressing the critical gap in standardized evaluation frameworks for this domain. Seventeen experienced LCA practitioners reviewed 168 AI-generated answers from eleven LLMs across 22 LCA-related tasks, assessing scientific accuracy, explanation quality, robustness, verifiability, and adherence to instructions. The results reveal significant risks in applying LLMs to LCA work, with hallucination rates reaching up to 40% for citations and no clear performance distinction between open and closed-weight models. The study underscores the need for larger, more diverse benchmarks and grounding mechanisms to improve reliability in domain-specific AI applications.

## Method Summary
The research team developed a benchmark framework where 17 experienced LCA practitioners evaluated AI-generated responses from eleven general-purpose LLMs across 22 representative LCA tasks. The evaluation covered five key dimensions: scientific accuracy, explanation quality, robustness, verifiability, and adherence to instructions. The team generated 168 responses using various prompting strategies and model configurations, then had experts rate each response on a structured scale. This approach provided the first systematic assessment of LLM performance in LCA, a field traditionally lacking clear ground truth for validation.

## Key Results
- 37% of AI-generated responses contained inaccurate or misleading information
- Hallucination rates for citations reached up to 40% across models
- No clear performance distinction between open-weight and closed-weight LLMs
- Accuracy and explanation quality ratings were generally average or good across most models

## Why This Works (Mechanism)
LLMs struggle with LCA tasks due to the domain's complexity, lack of standardized methodologies, and absence of clear ground truth. The expert-grounded approach works by leveraging human expertise to validate AI outputs where automated metrics fail. The evaluation framework captures both technical accuracy and practical utility, recognizing that LCA involves interpretation and judgment beyond simple fact retrieval. This methodology reveals that while LLMs can generate plausible-sounding responses, they frequently produce confident but incorrect information, particularly around citations and methodology specifics.

## Foundational Learning

**LCA Methodology Basics** - Understanding system boundaries, impact categories, and life cycle stages is essential for evaluating LLM outputs. Quick check: Can you identify the key components of a typical LCA study?

**Expert Evaluation Framework** - Domain experts must assess both technical accuracy and practical applicability. Quick check: What distinguishes a scientifically accurate LCA answer from a practically useful one?

**LLM Hallucination Patterns** - Recognizing when models generate plausible but incorrect information, especially in technical domains. Quick check: How can you identify citation hallucinations in LLM responses?

## Architecture Onboarding

**Component Map:** Expert Reviewers -> Task Selection -> LLM Generation -> Response Evaluation -> Benchmark Analysis

**Critical Path:** Task selection and prompt engineering -> LLM response generation -> Expert evaluation -> Data aggregation and analysis

**Design Tradeoffs:** The study prioritized expert judgment over automated metrics due to LCA's subjective nature, but this introduced potential reviewer bias and limited scalability.

**Failure Signatures:** High hallucination rates in citations, inconsistent methodology recommendations, and overconfident but incorrect technical claims.

**First Experiments:**
1. Test additional grounding mechanisms (retrieval-augmented generation) to reduce hallucination rates
2. Expand task diversity to include more real-world LCA case studies
3. Compare expert evaluation consistency across different reviewer experience levels

## Open Questions the Paper Calls Out

None identified in the source material.

## Limitations

- Small sample size of 17 reviewers may not represent the broader LCA expert community
- Limited number of AI-generated responses (168) may not capture full complexity of real-world LCA applications
- Absence of clear ground truth in LCA methodology makes expert judgments subjective and potentially inconsistent

## Confidence

- 37% inaccuracy rate: Medium confidence (expert grounding but limited sample size)
- No distinction between open/closed-weight models: Medium confidence (architecture exploration limited)
- 40% hallucination rate for citations: High confidence (direct measurement, but domain-specific)

## Next Checks

1. Expand benchmark to include more diverse LCA tasks and real-world case studies
2. Conduct follow-up study with larger and more diverse pool of LCA experts
3. Test additional grounding mechanisms to assess impact on reducing hallucination rates