---
ver: rpa2
title: 'Speeding Up MACE: Low-Precision Tricks for Equivarient Force Fields'
arxiv_id: '2510.23621'
source_url: https://arxiv.org/abs/2510.23621
tags:
- fp32
- mace
- fp16
- bf16
- precision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work systematically evaluates mixed-precision arithmetic\
  \ as a means to accelerate MACE, a state-of-the-art equivariant force field. Profiling\
  \ reveals that MACE is dominated by small, frequent kernel launches and host-side\
  \ dispatch overhead, with cuEquivariance (cuEq) reducing inference latency by ~3\xD7\
  \ relative to e3nn."
---

# Speeding Up MACE: Low-Precision Tricks for Equivarient Force Fields

## Quick Facts
- arXiv ID: 2510.23621
- Source URL: https://arxiv.org/abs/2510.23621
- Reference count: 38
- Primary result: Mixed-precision Linear layers with cuEquivariance backend achieve ~4.3× speedup while preserving thermodynamic stability in MD simulations.

## Executive Summary
This work systematically evaluates mixed-precision arithmetic to accelerate MACE, a state-of-the-art equivariant force field. Profiling reveals MACE is dominated by small, frequent kernel launches and host-side dispatch overhead, with cuEquivariance (cuEq) reducing inference latency by ~3× relative to e3nn. Switching only Linear layers to BF16/FP16 with FP32 accumulation preserves thermodynamic stability in NVT/NPT MD while cutting runtime by ~4.3×. Training in low precision degrades force accuracy but not energies or structural observables. Backend mixing without layout adapters produces large, persistent feature mismatches.

## Method Summary
The study benchmarks MACE-OFF24(M) across different backends (e3nn vs cuEq) and precisions (FP64, FP32, FP32+BF16/FP16) using CUDA 12.6, PyTorch 2.8.0, and MACE v0.3.14. Key experiments include profiling on carbon clusters, NPT MD simulations on water boxes, and toy training runs. The methodology employs 100 warm-up and 100 timed iterations, deterministic seeding, and focuses on inference latency, energy/force RMSE, and MD stability metrics (temperature, density, potential energy drift).

## Key Results
- cuEq backend reduces inference latency by ~3× compared to e3nn due to kernel fusion eliminating dispatch overhead
- Mixed-precision Linear layers (BF16/FP16) with FP32 accumulation cut runtime by ~4.3× while preserving thermodynamic stability
- Training in low precision degrades force RMSE (7.5%→9.6%) but preserves energy and structural observables
- Mixing cuEq and e3nn backends without layout adapters causes large feature mismatches (rel L2 ~0.7)

## Why This Works (Mechanism)

### Mechanism 1: Kernel Fusion Reducing Dispatch Overhead
MACE's e3nn implementation emits many small matrix multiplications (one per irrep pair), causing frequent kernel launches that bottleneck the CPU. cuEq replaces these with fused CUDA kernels that execute tensor products in a single pass, reusing on-chip data and minimizing global-memory round trips. This reduces launch latency as the dominant performance factor rather than FLOP improvements.

### Mechanism 2: Mixed-Precision Tensor Core Utilization
Tensor Cores on modern NVIDIA GPUs provide significantly higher throughput for half-precision (FP16/BF16) operations compared to FP32 scalar cores. By restricting low-precision casting to Linear layers (matrix multiplies) and keeping accumulators in FP32, the model avoids numerical instability while maximizing hardware occupancy.

### Mechanism 3: Preservation of Thermodynamic Stability via High-Precision Accumulation
Forces are derived from the negative gradient of energy and are highly sensitive to numerical noise. Retaining FP32 for accumulation prevents the error budget from exceeding the physical signal-to-noise ratio required for stable Molecular Dynamics, while inference benefits from lower precision in the compute-intensive Linear layers.

## Foundational Learning

- **SO(3) Equivariance & Clebsch-Gordan (CG) Coefficients**: MACE builds features by coupling spherical harmonics using CG coefficients. The tensor product contraction is the chief driver of wall time, making it the target for kernel fusion and mixed-precision acceleration. *Quick check: Can you explain why the tensor product contraction in Eq. 2.6 is described as the "chief driver of wall time"?*

- **Floating Point Precision (FP16/BF16/FP32) & Dynamic Range**: The paper exploits the trade-off between BF16 (FP32-like dynamic range, low mantissa) and FP16 (smaller range). BF16 is preferred for stability (avoiding overflow/underflow) while FP16 offers slightly better theoretical throughput on some hardware. *Quick check: Why does the paper suggest using BF16 for Linear layers instead of FP16, despite both being 16-bit formats?*

- **GPU Dispatch vs. Compute Bound**: The key finding is that MACE is CPU/launch-bound, not purely compute-bound. Distinguishing between "GPU is busy calculating" and "CPU is busy telling GPU what to do" is critical to understanding why kernel fusion works better than just lowering precision alone. *Quick check: According to the profiling results, does reducing precision alone solve the bottleneck, or is backend optimization (cuEq) required first?*

## Architecture Onboarding

- **Component map**: Input (atomic positions & species) -> Embedding (node embeddings + Spherical Harmonics + Radial basis) -> Interaction Blocks (ConvTP, Linear layers, SkipTP) -> Product Blocks (SymmetricContraction) -> Readout (maps to site energies) -> Backend (e3nn vs cuEquivariance)

- **Critical path**: 1. SymmetricContractions[0] (highest wall time, 5.58ms/step) 2. Interaction[1]/Linear_Interaction (3.28ms/step) 3. SkipTensorProduct / ConvTP chains (dominating CUDA kernel time)

- **Design tradeoffs**: Speed vs. Stability: FP32 + BF16 Linear offers ~4.3× speedup with negligible drift; FP64 is conservative baseline but 4× slower. Backend Compatibility: cuEquivariance is 3× faster but cannot be mixed with e3nn modules in same forward pass without explicit layout adapters. Training vs. Inference: Training in FP16/BF16 harms force RMSE; Inference in FP16/BF16 is safe if accumulation is FP32.

- **Failure signatures**: Feature Mismatch: Persistent max absolute difference of O(1) in node features when mixing e3nn and cuEq layers. Force Spikes/Drift: Pure FP16 (without FP32 accumulation) causes force RMSE to jump from ~170 to ~220 meV/Å. Thermostat Failure: Low precision may cause density deviations > 0.3% or energy drift in NPT MD.

- **First 3 experiments**: 1. Profile MACE-OFF24(M) on carbon.xyz using e3nn (FP64) vs cuEq (FP32) to verify ~3× speedup claim. 2. Run short NVT MD simulation using cuEq with FP32 default vs FP32+BF16 (Linear layers only) to confirm energy stability (< 1 meV/atom drift). 3. Replace single Linear layer with e3nn in cuEq model and measure feature output error to observe layout mismatch phenomenon.

## Open Questions the Paper Calls Out

- **Architecture Scaling**: Does the ~3× cuEq speedup and ~4.3× mixed-precision acceleration generalize to larger systems and higher $\ell_{max}$ values, or do dispatch and memory overheads scale differently with system size?

- **Backend Interoperability**: Can explicit layout adapters be designed to safely mix cuEq and e3nn modules within the same MACE model, or is homogeneous backend usage fundamentally required for numerical consistency?

- **Long-Term MD Stability**: Do the thermodynamic stability results for BF16/FP16 hold over microsecond MD timescales and for complex systems (proteins, interfaces) beyond the short water-box NPT benchmarks presented?

## Limitations

- The study lacks full reproducibility details including exact system sizes, input configurations, and casting implementation used for latency benchmarks
- Thermodynamic stability results are limited to short NPT runs on water; extrapolation to larger, more complex systems or longer timescales is not validated
- Training degradation under low precision is demonstrated on a small "toy" dataset, which may not generalize to full pre-training regime

## Confidence

- **High confidence**: The ~3× speedup from cuEq over e3nn is supported by direct profiling data and clear mechanism (kernel fusion reducing dispatch overhead). The ~4.3× inference speedup from mixed-precision Linear layers with FP32 accumulation is consistent with GPU Tensor Core capabilities and validated by stable MD runs.

- **Medium confidence**: The claim that training in low precision degrades force accuracy is demonstrated on a toy dataset but lacks validation on full MACE-OFF24(M) model. The assertion that mixed precision preserves energy/structural observables but not forces is plausible but exact error bounds and long-term stability are not fully characterized.

- **Low confidence**: The guidance to "avoid mixing backends" is based on single feature mismatch example; broader validation across different tensor orders and network architectures is missing. The scalability of cuEq speedup with system size and impact of varying $\ell_{max}$ on dispatch overhead are not systematically explored.

## Next Checks

1. **Backend Mixing Robustness**: Systematically measure feature mismatch (max absolute difference, relative L2) when interleaving cuEq and e3nn modules across different tensor orders ($\ell=0,1,2$) and network depths in controlled MACE block to quantify practical risk of "homogeneous backend" requirement.

2. **Long-Term MD Stability**: Run extended (ns) NPT/MD simulations of water and simple solvated protein system using FP32+BF16 cuEq, monitoring not just T/ρ/PE drift but also force variance and integration stability to validate "preserved thermodynamic stability" beyond short benchmarks.

3. **Training-Scale Precision Impact**: Train medium-sized equivariant model (subset of MACE-OFF24) from scratch in FP32 vs FP16/BF16, comparing final force RMSE on held-out test set and tracking training loss stability to confirm whether toy dataset results generalize to full training pipeline.