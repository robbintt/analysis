---
ver: rpa2
title: 'STILL: Selecting Tokens for Intra-Layer Hybrid Attention to Linearize LLMs'
arxiv_id: '2602.02180'
source_url: https://arxiv.org/abs/2602.02180
tags:
- attention
- still
- tokens
- linear
- softmax
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: STILL is an intra-layer hybrid attention method that linearizes
  pretrained LLMs for long sequences. It addresses the limitations of existing hybrid
  approaches that rely on static, position-based token routing and feature maps that
  distort pretrained norms.
---

# STILL: Selecting Tokens for Intra-Layer Hybrid Attention to Linearize LLMs

## Quick Facts
- arXiv ID: 2602.02180
- Source URL: https://arxiv.org/abs/2602.02180
- Reference count: 9
- Linearizes pretrained LLMs for long sequences using hybrid attention with content-aware token routing

## Executive Summary
STILL introduces a novel intra-layer hybrid attention mechanism that linearizes pretrained LLMs while preserving their representational capabilities. The method combines softmax attention for salient tokens with linear attention for the remaining context, using a self-saliency score to dynamically select important tokens. A norm-preserved feature map ensures pretrained representations are maintained during linearization. The approach achieves competitive performance on commonsense reasoning tasks while significantly improving efficiency on long-context benchmarks.

## Method Summary
STILL employs a two-stage training procedure: first using attention transfer to optimize the feature map and gating projection while freezing the teacher model, then applying LoRA to linearize the attention layers. The core innovation is the self-saliency score, which identifies important tokens by measuring KL divergence between local attention distributions with and without each token's self-attention term. These tokens receive softmax attention while others are processed via linear attention using a norm-preserved feature map that decouples direction from magnitude and reinjects pretrained norms. Chunk-wise delayed selection enables efficient parallel training and inference.

## Key Results
- Matches or surpasses original pretrained models on commonsense reasoning tasks (PIQA, ARC, HellaSwag, WinoGrande, MMLU)
- Achieves up to 86.2% relative improvement over prior linearized attention methods on long-context benchmarks
- Reduces memory usage and increases decoding speed while maintaining model accuracy
- Successfully handles long sequences without the quadratic complexity of standard attention

## Why This Works (Mechanism)

### Mechanism 1: Content-aware token routing via Self-Saliency Scores
STILL calculates a self-saliency score for each token by measuring the KL divergence between local attention distributions with and without the token's self-attention term. High scores indicate tokens acting as "sinks" or pivot points that should receive precise softmax attention. This enables the model to allocate high-fidelity attention to globally important tokens that lie outside static local windows.

### Mechanism 2: Norm-preserved feature map
The Norm-Preserved Feature Map (NP-Map) decouples feature magnitude from direction to preserve pretrained model's "norm-aware" representational geometry. It normalizes the MLP output to unit direction and scales it by the original input's norm, ensuring linear attention respects the magnitude semantics of the pretrained teacher and preventing distribution shift.

### Mechanism 3: Chunk-wise delayed selection
By processing tokens in chunks and computing saliency scores within each chunk, STILL unblocks parallelism during training and prefilling. The top-k tokens from the previous chunk are routed to the global softmax attention cache, reducing latency penalties of dynamic routing while maintaining information flow.

## Foundational Learning

- **Concept: Linear Attention Kernel Trick**
  - Why needed: STILL replaces $O(N^2)$ softmax with linear kernel using $\phi(q)\phi(k)^T$ to enable matrix multiplication associativity changes
  - Quick check: Does Linear Attention reduce complexity by changing the order of matrix operations or by reducing the dimension $d$?

- **Concept: Attention Sinks (Spike Phenomenon)**
  - Why needed: Self-Saliency Score relies on identifying tokens acting as "sinks" (high attention accumulation)
  - Quick check: If a token has very high "Self-Saliency Score," is it likely acting as a sink for attention weights or being ignored?

- **Concept: Norm-Awareness in Transformers**
  - Why needed: NP-Map is built on premise that attention is sensitive to vector magnitude
  - Quick check: If you double the norm of Query vectors in standard attention, does the distribution become sharper (lower entropy) or flatter (higher entropy)?

## Architecture Onboarding

- **Component map:**
  Input Projection -> Saliency Scorer -> Router -> (SA Branch, LA Branch) -> Aggregator

- **Critical path:** The NP-Map (Eq. 6) and Chunk-wise Selection (Alg. 1) are the most sensitive code paths. Errors in norm re-injection or chunk indexing will immediately break alignment with the pretrained teacher.

- **Design tradeoffs:**
  - Chunk Size ($C$): Larger chunks improve parallelism (speed) but increase "staleness" of selection (delay)
  - Selection Budget ($\lambda$): Higher $\lambda$ improves accuracy but approaches quadratic cost

- **Failure signatures:**
  - Norm Collapse/Explosion: If NP-Map logic is wrong, output norms drift, causing divergence
  - Stuck Attention: If saliency scores are all zero, no tokens are selected for SA, and the model fails to retrieve specific details

- **First 3 experiments:**
  1. Sanity Check - Pass-through: Validate that with $\lambda=0$ (pure linear) and $\lambda=N$ (full softmax), the model recovers baseline behaviors
  2. Norm Profile Audit: Plot distribution of feature norms before and after NP-Map to verify pretrained magnitudes are preserved
  3. Long-Context Retrieval (S-NIAH): Run Single Needle-in-a-Haystack test to confirm dynamic routing finds the needle better than static sliding window

## Open Questions the Paper Calls Out

### Open Question 1
Does the empirical consistency between local Self-Saliency Scores and global importance persist at sequence lengths exceeding 64K? The paper validates local-global consistency primarily at 4K while efficiency tests go to 64K. Retrieval difficulty scales non-linearly with distractor length; local windows may lose correlation with global importance at extreme scales.

### Open Question 2
Does the Norm-Preserved Feature Map constrain the model's ability to learn distributions requiring significantly different feature magnitudes than the pre-trained teacher? The method forces the MLP to refine only "directional components" while strictly re-injecting the "pre-trained norm," which may act as a strong regularizer hampering adaptation to domains with distinct signal intensities.

### Open Question 3
Does the "delayed selection" strategy introduce information loss for salient tokens that appear near the boundaries of a chunk? A token might act as a critical query for subsequent tokens in the next chunk, but its saliency is only evaluated retroactively, potentially delaying its promotion to the SA branch.

## Limitations
- Chunk-wise delayed selection creates tension between efficiency and accuracy, with unclear impact on tasks requiring immediate reactive memory
- Norm preservation effectiveness is claimed but not thoroughly validated, with pretrained models potentially already having sufficient mechanisms
- Saliency score sensitivity to local context may not correlate with global importance in highly diffuse tasks
- Unknown hyperparameters significantly impact reproducibility, with crucial architectural details unspecified

## Confidence

**High confidence** in theoretical framework: Mathematical formulation of Self-Saliency Scores, NP-Map, and chunk-wise selection is internally consistent and builds on well-established attention mechanisms.

**Medium confidence** in empirical claims: While the paper reports matching or surpassing pretrained models on commonsense tasks, lack of detailed hyperparameter specifications and ablation studies limits reproducibility confidence.

**Low confidence** in scalability assertions: Claims of improved decoding speed through chunk-wise parallelization lack concrete latency measurements comparing different chunk sizes or analyzing trade-offs across sequence lengths.

## Next Checks

1. **Ablation on norm preservation**: Run experiments with and without NP-Map on the same pretrained model and dataset, measuring performance differences and tracking feature norm distributions to verify necessity.

2. **Saliency score correlation analysis**: For tasks where the model underperforms, compute correlation between Self-Saliency Scores and actual importance as measured by full attention weights, visualizing routing decisions.

3. **Chunk size sensitivity study**: Systematically vary chunk size C and selection budget Î» while measuring both accuracy and latency to identify the Pareto frontier balancing efficiency gains against accuracy losses.