---
ver: rpa2
title: Model Merging to Maintain Language-Only Performance in Developmentally Plausible
  Multimodal Models
arxiv_id: '2510.01845'
source_url: https://arxiv.org/abs/2510.01845
tags:
- multimodal
- babylm
- merging
- text-only
- language-only
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether multimodal models trained on developmentally
  plausible data underperform in language-only tasks compared to text-only models.
  The authors address this by merging multimodal models with text-only models using
  weighted linear interpolation at inference time.
---

# Model Merging to Maintain Language-Only Performance in Developmentally Plausible Multimodal Models

## Quick Facts
- arXiv ID: 2510.01845
- Source URL: https://arxiv.org/abs/2510.01845
- Reference count: 12
- Models trained on BabyLM 100M words show language-only performance gap; merging with text-only models (α=0.3-0.5) restores grammar while maintaining multimodal capabilities.

## Executive Summary
This paper addresses the problem that multimodal models trained on developmentally plausible limited data (BabyLM) underperform in language-only grammar tasks compared to text-only models. The authors propose model merging as an inference-time solution, combining multimodal and text-only models through weighted linear interpolation of parameters. Using a 6-layer LLaVA-style architecture with frozen DINOv2 vision encoder, they train both model types on BabyLM data and find that merging with weights of 0.3-0.5 improves BLiMP grammar accuracy to 73.84 (surpassing Flamingo: 70.9, GIT: 72.2) while maintaining competitive Winoground multimodal performance.

## Method Summary
The authors train small-scale multimodal models on the BabyLM dataset using a modified LLaVA architecture with a frozen DINOv2-large vision encoder and a 6-layer randomly initialized language model. Image features are pre-extracted and mean-pooled to a single 1024-dim token, projected to the LM space via a trainable 2-layer MLP adapter. Two models are trained independently: text-only (on BabyLM text) and multimodal (on text + image-caption pairs with black image placeholder for text-only samples). At inference time, these models are merged using linear interpolation: θ_merged = αθ_LLM + (1-α)θ_VLM with α ∈ {0.3, 0.5, 0.8}. The best text-only checkpoint (by BLiMP) is merged with the best multimodal checkpoint (by Winoground) to evaluate performance on grammar and multimodal benchmarks.

## Key Results
- Model merging with α=0.3-0.5 improves BLiMP grammar accuracy from ~70.9 to 73.84
- Merged models maintain competitive Winoground multimodal performance
- Text-only models outperform multimodal on BLiMP (grammar), but multimodal models outperform on Entity Tracking and EWoK (semantics)
- Single pooled image token approach achieves results despite acknowledged spatial information loss

## Why This Works (Mechanism)

### Mechanism 1: Parameter Space Interpolation for Dual-Capability Preservation
- Claim: Linear interpolation of parameters from text-only and multimodal models can preserve both language-only and multimodal capabilities simultaneously.
- Mechanism: Weighted averaging (θ_merged = αθ_LLM + (1-α)θ_VLM) creates a model that inherits grammatical competence from text-only training while retaining visual grounding, without requiring additional training.
- Core assumption: Both models share identical architecture and initialization, making their parameter spaces linearly connected.
- Evidence anchors:
  - [abstract]: "model merging with text-only models can help alleviate this problem to some extent, while maintaining multimodal performance"
  - [section 5]: "the merged model is a linear interpolation of the multimodal and text-only models" with α = 0.3, 0.5, 0.8
  - [corpus]: Related work (Sung et al. 2023) finds "simple linear interpolation is a competitive and efficient method"
- Break condition: Models initialized differently or trained on incompatible architectures; α values too extreme (near 0 or 1) lose one capability.

### Mechanism 2: Modality-Specific Capacity Allocation Tradeoff
- Claim: Multimodal training shifts model capacity toward semantic/world knowledge at the cost of grammatical competence.
- Mechanism: Assumption: Visual grounding prioritizes semantic content over syntactic patterns during limited training, reallocating representational capacity.
- Evidence anchors:
  - [abstract]: "multimodal models tend to underperform in language-only tasks"
  - [section 7]: Text-only outperforms multimodal on BLiMP (grammar), but multimodal outperforms on Entity Tracking and EWoK (semantics)
  - [corpus]: Klerings et al. (2024) corroborate: "vision does not significantly benefit the performance in language-only benchmarks"
- Break condition: When training data is abundant enough for both capacities to develop; when semantic tasks require grammatical precision.

### Mechanism 3: Frozen Vision Encoder with Trainable Adapter
- Claim: Decoupling visual feature extraction from language modeling reduces optimization complexity in low-data regimes.
- Mechanism: Pre-extracted DINOv2 features are projected to LM space via learned 2-layer MLP adapter, limiting trainable parameters to language-relevant components.
- Core assumption: DINOv2 provides sufficient visual representations without task-specific fine-tuning.
- Evidence anchors:
  - [section 4]: "multimodal projector is trained along with the language model, while keeping the image representations frozen"
  - [section 4]: Mean-pooled "single summary token (1024 dim) is fed to the LM directly"
  - [corpus]: Weak/no direct corpus evidence comparing frozen vs. fine-tuned encoders in this context
- Break condition: Tasks requiring fine-grained spatial reasoning; the acknowledged limitation that pooling 256 tokens to 1 loses spatial information.

## Foundational Learning

- Concept: Model Merging / Parameter Averaging
  - Why needed here: Core technique to combine text-only and multimodal models at inference time without retraining.
  - Quick check question: Why does parameter averaging require models to share the same architecture and initialization seed?

- Concept: Catastrophic Forgetting
  - Why needed here: The paper positions merging as an alternative strategy to prevent losing previously learned language-only abilities when training multimodally.
  - Quick check question: What training approaches (besides merging) could prevent forgetting during multimodal fine-tuning?

- Concept: Vision-Language Architecture (Encoder-Projector-Decoder)
  - Why needed here: Understanding which components are frozen vs. trainable is essential for debugging and modification.
  - Quick check question: In this architecture, which components receive gradient updates during training?

## Architecture Onboarding

- Component map:
  Vision Encoder (DINOv2-large) -> Mean Pooling -> Multimodal Projector -> Language Model (6-layer) -> Output

- Critical path:
  1. Pre-extract all image features with DINOv2 offline
  2. Mean-pool to single 1024-dim vector
  3. Project through adapter → 768-dim
  4. Concatenate with text embeddings (black image placeholder for text-only)
  5. Forward through LM, compute cross-entropy loss

- Design tradeoffs:
  - **1 pooled token vs. 256 patches**: Saves compute but loses spatial detail (acknowledged limitation)
  - **6 layers vs. deeper**: Faster training, lower capacity
  - **α = 0.3–0.5 vs. 0.8**: Lower α preserves multimodal capability; higher α prioritizes grammar

- Failure signatures:
  - **BLiMP << text-only baseline**: Multimodal training may have overwritten grammatical representations
  - **Winoground near random (25%)**: Visual-token alignment failed; check projector training
  - **Merged model degrades both**: α too extreme, or checkpoint timing mismatch

- First 3 experiments:
  1. **Establish baselines**: Train text-only and multimodal models independently; evaluate on BLiMP (grammar) and Winoground (multimodal) to quantify the gap.
  2. **Merge sweep**: At each checkpoint, merge with α ∈ {0.3, 0.5, 0.8}; plot both benchmarks to identify optimal weight and checkpoint timing.
  3. **Ablate pooling**: If compute permits, compare 1 pooled token vs. 16 pooled tokens (e.g., spatial grid) to quantify spatial information loss.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would providing all 256 image tokens (instead of a single pooled representation) improve multimodal performance while maintaining model merging benefits?
- Basis in paper: [explicit] Limitations section states the pooled approach "might cause information loss and performance drop, and ideally, we would like to provide the whole set of image patches."
- Why unresolved: Compute constraints prevented testing the full-token setup.
- What evidence would resolve it: Training models with 256 image tokens and comparing performance on Winoground and BLiMP benchmarks.

### Open Question 2
- Question: Would more sophisticated merging techniques (e.g., Fisher-weighted averaging) outperform simple linear interpolation?
- Basis in paper: [explicit] Conclusion states: "Future work can explore other model merging techniques and the effects of model merging in other benchmarks."
- Why unresolved: Only linear interpolation was tested.
- What evidence would resolve it: Comparing linear interpolation against Task Arithmetic, TIES-Merging, and Fisher-weighted averaging on the same evaluation suite.

### Open Question 3
- Question: How robust are the model merging benefits across different random seeds and training orderings?
- Basis in paper: [explicit] Limitations states results "should be investigated further using more seeds and different training orders for robustness and generalizability."
- Why unresolved: Single training runs without multiple seeds were conducted.
- What evidence would resolve it: Multi-seed experiments showing consistent merging benefits across varied training conditions.

### Open Question 4
- Question: Does the multimodal advantage on knowledge-oriented tasks (EWOK, Entity Tracking) persist at larger model scales?
- Basis in paper: [inferred] The paper speculates multimodal data benefits these tasks but only tests 6-layer models.
- Why unresolved: Findings may be specific to small-scale, resource-constrained setups.
- What evidence would resolve it: Replicating experiments with deeper transformer architectures.

## Limitations
- Model merging requires both models to be loaded at inference time, potentially introducing latency overhead
- Single pooled image token approach loses spatial information acknowledged as a limitation
- Results from small-scale 6-layer models may not generalize to larger architectures
- Merging at inference time is computationally more expensive than fine-tuning in low-data regimes

## Confidence
- **High confidence**: The core finding that model merging with α ∈ [0.3, 0.5] improves BLiMP accuracy from ~70.9 to 73.84 while maintaining multimodal performance is well-supported by the experimental results presented.
- **Medium confidence**: The claim that multimodal training inherently shifts capacity from grammatical to semantic representations is plausible given the observed performance gap, but requires more systematic ablation studies to confirm the mechanism.
- **Medium confidence**: The assertion that inference-time merging is more efficient than fine-tuning in low-data regimes is reasonable given the results, though computational costs of loading both models are not fully characterized.

## Next Checks
1. **Temporal alignment validation**: Test merging across multiple checkpoint pairs (not just best checkpoints) to determine if model selection strategy affects the optimal α and final performance, addressing the assumption that best text-only and best multimodal checkpoints are compatible for merging.

2. **Spatial information preservation**: Conduct controlled experiments comparing single pooled token (1024-dim) vs. 16 pooled tokens (e.g., 4×4 spatial grid) on Winoground to quantify the actual performance cost of information loss from mean-pooling, validating the acknowledged limitation.

3. **Architecture scaling test**: Replicate the merging approach with larger models (e.g., 12-24 layers) and measure whether the optimal α range shifts, determining if the 0.3-0.5 range is architecture-dependent or more generally applicable.