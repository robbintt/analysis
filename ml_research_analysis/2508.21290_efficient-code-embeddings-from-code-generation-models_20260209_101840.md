---
ver: rpa2
title: Efficient Code Embeddings from Code Generation Models
arxiv_id: '2508.21290'
source_url: https://arxiv.org/abs/2508.21290
tags:
- code
- https
- embedding
- datasets
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces jina-code-embeddings, a suite of code embedding
  models designed for retrieving code from natural language queries, technical question-answering,
  and identifying semantically similar code snippets across programming languages.
  The models leverage autoregressive decoder backbones pre-trained on both text and
  code, generating embeddings via last-token pooling.
---

# Efficient Code Embeddings from Code Generation Models

## Quick Facts
- arXiv ID: 2508.21290
- Source URL: https://arxiv.org/abs/2508.21290
- Authors: Daria Kryvosheieva; Saba Sturua; Michael Günther; Scott Martens; Han Xiao
- Reference count: 15
- Primary result: jina-code-embeddings-0.5B and 1.5B outperform similar-sized models on CoIR benchmark

## Executive Summary
This paper introduces jina-code-embeddings, a suite of code embedding models that achieve state-of-the-art performance for code retrieval tasks. The models leverage autoregressive decoder backbones (Qwen2.5-Coder-0.5B and 1.5B) and employ a training recipe combining contrastive learning with task-specific instruction prefixes. The approach achieves superior performance on the comprehensive CoIR benchmark while maintaining relatively small model sizes (494M and 1.54B parameters).

## Method Summary
The method uses Qwen2.5-Coder decoder backbones pre-trained on text and code, fine-tuned with contrastive learning using InfoNCE loss on query-document pairs from diverse code retrieval datasets. Embeddings are generated via last-token pooling of the final hidden layer, conditioned by task-specific instruction prefixes (NL2Code, TechQA, Code2Code, Code2NL, Code2Completion). The models incorporate Matryoshka representation learning for flexible truncation and L2 normalization. Training runs for 1500 steps on 4×80GB A100 GPUs, using batch sizes of 512 (0.5B) or 256 (1.5B) with sequence length 512.

## Key Results
- jina-code-embeddings-0.5B and 1.5B outperform similar-sized models on CoIR benchmark
- Models match or exceed much larger alternatives like Qwen3-Embedding-0.6B and jina-embeddings-v4
- Ablation studies confirm last-token pooling provides the best results
- Models support flexible truncation for precision-resource trade-offs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Last-token pooling outperforms mean and latent attention pooling for decoder-based code embedding models.
- Mechanism: Autoregressive decoders process tokens sequentially with causal attention, so the final token's hidden state accumulates information from all preceding tokens. This provides a natural aggregation point that respects the left-to-right processing order.
- Core assumption: The pre-trained code generation backbone has learned to compress relevant information into later token positions during its original training.
- Evidence anchors:
  - [abstract] "generating embeddings via last-token pooling"
  - [Section 3] "We found, after some experimentation, that last-token pooling gave us better performance than mean pooling or latent attention pooling, as documented in Appendix B"
  - [Appendix B, Table 5] Shows last-token pooling achieving 78.72% MTEB Code AVG vs. 77.18% for mean and 78.41% for latent attention
  - [corpus] Weak direct evidence; neighboring papers do not specifically address pooling strategies for decoder-based embeddings
- Break condition: If using encoder-only architectures (e.g., BERT-based models), this mechanism would not apply; CLS or mean pooling may be more appropriate per Reimers and Gurevych [2019] cited in Section 2

### Mechanism 2
- Claim: Task-specific instruction prefixes condition the model to produce task-appropriate embedding spaces without requiring separate model fine-tuning per task.
- Mechanism: By prepending natural language instructions that describe the retrieval task (e.g., "Find the most relevant code snippet given the following query"), the model learns to map the same input to different regions of embedding space depending on the task context, effectively creating multiple specialized embedding functions within a single model.
- Core assumption: The pre-trained backbone has sufficient language understanding to interpret and follow instruction prefixes, and the training data provides sufficient coverage of each task type with matching instructions.
- Evidence anchors:
  - [abstract] "training recipe that combines contrastive learning with task-specific instruction prefixes"
  - [Section 3, Table 1] Lists five task categories with distinct query and document prefixes
  - [Section 2] Cites Su et al. [2023] showing instruction-tuning generally yields improved performance
  - [corpus] No direct corpus evidence for code-specific instruction tuning; neighboring papers do not address this technique
- Break condition: If inference-time instructions differ significantly from training instructions, or if novel task types not covered in Table 1 are introduced, performance may degrade unpredictably

### Mechanism 3
- Claim: Contrastive learning with InfoNCE loss on query-document pairs from code retrieval tasks transfers the pre-trained backbone's code understanding into a retrieval-optimized embedding space.
- Mechanism: The InfoNCE loss maximizes similarity between matched query-document pairs while minimizing similarity to all other documents in the batch. This creates strong gradients that reorganize the embedding space so semantically related code and natural language text cluster together, while unrelated items separate.
- Core assumption: The training pairs (docstrings, comments, commit messages paired with code) adequately represent real-world retrieval scenarios, and batch negatives provide sufficient hard negative sampling.
- Evidence anchors:
  - [Section 4] "We initialized the model with weights of the pre-trained backbone Qwen2.5-Coder-0.5B and then applied further training with a contrastive objective using the InfoNCE loss function"
  - [Section 4.1] Describes training data sources including docstrings, comments, commit messages, problem statements
  - [Section 4.2, Equation 1] Provides formal InfoNCE loss definition
  - [corpus] Limited corpus support; contrastive learning for embeddings is established but code-specific application is not well-covered in neighbors
- Break condition: If deployment tasks have fundamentally different query-document relationships (e.g., adversarial queries, out-of-distribution programming languages), the learned embedding space may not transfer

## Foundational Learning

- **Concept: Contrastive Learning (InfoNCE Loss)**
  - Why needed here: This is the core training objective that transforms a generative model into a retrieval model. Understanding how batch negatives work and why temperature scaling matters is essential for debugging training convergence.
  - Quick check question: Given a batch of 256 query-document pairs, how many negative comparisons does each query make within a single training step?

- **Concept: Pooling Strategies (Last-token vs. Mean vs. CLS)**
  - Why needed here: The paper explicitly chooses last-token pooling based on empirical comparison. Understanding why different architectures favor different pooling methods helps diagnose when this choice might not transfer.
  - Quick check question: For an autoregressive decoder, why might the last token's hidden state contain more comprehensive information than the first token's state?

- **Concept: Matryoshka Representation Learning**
  - Why needed here: The models support flexible truncation for precision-resource trade-offs. This technique allows embeddings to be shortened at inference time without retraining, which has practical deployment implications.
  - Quick check question: If a 1024-dimensional Matryoshka-trained embedding is truncated to 256 dimensions, what property should the truncated vector retain?

## Architecture Onboarding

- **Component map:**
  Input Text + Task Prefix -> Qwen2.5-Coder Backbone (0.5B or 1.5B params) -> Final Hidden Layer -> Last-token Pooling (extract single vector) -> L2 Normalization -> Matryoshka-trained Embedding (truncatable)

- **Critical path:** The choice of task prefix (Table 1) directly affects embedding quality. Mismatching prefixes between query and corpus indexing (e.g., using NL2Code query prefix but indexing corpus with Code2Code document prefix) will cause retrieval failures.

- **Design tradeoffs:**
  - Model size vs. latency: 0.5B model trains ~8.3 hours vs. 1.5B at ~12 hours (Section 4.2), but 1.5B shows modest average improvement (78.41% → 79.04% overall AVG)
  - Pooling method: Last-token pooling outperforms alternatives but requires full forward pass through sequence; cannot use parallel prefix processing
  - Batch size constraints: 0.5B uses batch size 512; 1.5B uses 256 (memory-limited), affecting negative sample diversity

- **Failure signatures:**
  - Very short code snippets may not provide sufficient context for last-token pooling to aggregate meaningful information
  - Cross-language retrieval for languages underrepresented in training data may show degraded performance
  - Query prefixes not matched to actual task type will produce misaligned embeddings

- **First 3 experiments:**
  1. Baseline retrieval check: Index a sample codebase using NL2Code document prefix, query with NL2Code query prefix, measure recall@10 on held-out query-code pairs from your domain
  2. Prefix sensitivity test: Run identical queries with all five query prefixes against the same corpus index; verify that NL2Code prefix produces highest similarity scores for ground-truth matches
  3. Truncation calibration: For your typical retrieval workload, compare retrieval quality vs. latency when truncating embeddings to 256, 512, and 1024 dimensions to identify the precision-resource sweet spot

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does last-token pooling outperform latent attention pooling for these specific code embedding models, contrary to findings in general text embedding literature?
- Basis in paper: [inferred] The ablation study (Table 5) shows last-token pooling yielding higher average performance than latent attention, while the Related Work section cites Lee et al. [2025] reporting significant improvements with latent attention for general embeddings.
- Why unresolved: The paper reports the empirical result but does not provide a theoretical explanation for why code-specific decoder models might favor last-token pooling over the more complex latent attention mechanism.
- What evidence would resolve it: A comparative analysis of embedding distributions across pooling methods, or an ablation study across different backbone sizes to see if the pooling preference is consistent or scale-dependent.

### Open Question 2
- Question: Is the proposed training recipe dependent on the Qwen2.5-Coder backbone's specific pre-training mixture, or is it transferable to other code generation LLMs?
- Basis in paper: [inferred] The paper utilizes exclusively Qwen2.5-Coder backbones, attributing success to their pre-training on text and code, but does not validate the method on other popular code LLMs.
- Why unresolved: Without testing on diverse architectures, it remains unclear if the state-of-the-art performance is a result of the specific training recipe (contrastive + prefixes) or inherent capabilities of the Qwen backbone.
- What evidence would resolve it: Applying the same contrastive fine-tuning with task prefixes to a different decoder-only code LLM (e.g., CodeLlama or StarCoder) and evaluating on the CoIR benchmark.

### Open Question 3
- Question: How robust is the model to the omission or mismatching of task-specific instruction prefixes during inference?
- Basis in paper: [inferred] The methodology relies heavily on five specific instruction prefixes (Table 1) for training and inference, but the paper does not analyze performance degradation when these instructions are absent or incorrect.
- Why unresolved: It is unclear if the model has learned generalized code semantics or if it is over-reliant on the specific "prompt engineering" of the instruction prefixes to separate embedding spaces.
- What evidence would resolve it: An ablation study measuring retrieval accuracy on the CoIR benchmark when the query prefix is removed, swapped with a different task prefix, or replaced with a generic string.

## Limitations

- Critical hyperparameters including learning rate schedules and optimizer choices are unspecified, preventing exact reproduction
- Training data mixing ratios and sampling strategies across heterogeneous sources are not disclosed
- Evaluation focuses primarily on Python, Java, and C++ with no analysis of performance on less-represented programming languages

## Confidence

**High Confidence**: The contrastive learning framework with InfoNCE loss and the empirical advantage of last-token pooling over alternatives are well-supported by the provided ablation study (Appendix B Table 5) and align with established literature on autoregressive model architectures.

**Medium Confidence**: The state-of-the-art claims on CoIR benchmark are credible given the systematic comparison against multiple baselines, but the confidence intervals are not reported, making it difficult to assess statistical significance of performance differences, particularly for smaller margins.

**Low Confidence**: The paper's assertion that Matryoshka-trained embeddings maintain semantic quality when truncated lacks quantitative validation. The practical benefits of flexible truncation for precision-resource trade-offs are theoretically sound but empirically under-supported.

## Next Checks

1. Task Prefix Transferability Test: Systematically evaluate model performance when using mismatched task prefixes (e.g., applying Code2Code document prefix to NL2Code queries) to quantify the brittleness of the instruction-conditioned embedding space.

2. Cross-Language Retrieval Analysis: Measure retrieval accuracy on code written in programming languages that constitute less than 10% of the training corpus to identify language-specific performance degradation patterns.

3. Truncation Quality Assessment: Conduct controlled experiments truncating embeddings to various dimensions (128, 256, 512) and measure the degradation in retrieval metrics to empirically validate the Matryoshka representation learning claims.