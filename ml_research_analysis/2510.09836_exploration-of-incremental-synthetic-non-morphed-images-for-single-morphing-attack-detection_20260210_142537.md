---
ver: rpa2
title: Exploration of Incremental Synthetic Non-Morphed Images for Single Morphing
  Attack Detection
arxiv_id: '2510.09836'
source_url: https://arxiv.org/abs/2510.09836
tags:
- synthetic
- training
- data
- images
- face
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of improving Single Morphing
  Attack Detection (S-MAD) performance in the face of limited bona fide image datasets
  due to privacy concerns. The authors propose incrementally incorporating synthetic
  non-morphed images into the training process to enhance generalization.
---

# Exploration of Incremental Synthetic Non-Morphed Images for Single Morphing Attack Detection

## Quick Facts
- **arXiv ID**: 2510.09836
- **Source URL**: https://arxiv.org/abs/2510.09836
- **Reference count**: 27
- **Primary result**: Adding 75% synthetic non-morphed images to training improves S-MAD generalization, achieving EER of 6.09%, while synthetic-only training fails catastrophically (EER > 35%).

## Executive Summary
This paper addresses the challenge of Single Morphing Attack Detection (S-MAD) when real bona fide training data is limited due to privacy concerns. The authors propose incrementally incorporating synthetic non-morphed images generated by StyleGAN2-ADA into the training process to enhance generalization. Through cross-dataset validation using FERET and FRGCv2 datasets, they demonstrate that carefully controlled addition of synthetic data significantly improves detection accuracy, with the optimal ratio being 75% synthetic images. The study also reveals that training exclusively on synthetic data leads to severe performance collapse, highlighting the importance of maintaining a balanced training set that includes real bona fide samples.

## Method Summary
The study employs cross-dataset evaluation using FERET and FRGCv2 datasets with bona fide images and morphs generated by four different tools (FaceFusion, Face Morpher, OpenCV, UBO) under three processing methods (PS300, PS600, digital resized). Synthetic non-morphed images from SMDD (25K bona fide, 15K morphed) are incrementally added to training at ratios of 0%, 10%, 20%, 30%, 50%, 75%, 100%, and 100% synthetic only. The models used are EfficientNet-B2 and MobileNetV3-large with ImageNet-1K pretrained weights, trained with Adam optimizer (β1=0.99, β2=0.999, lr=1e-5), batch size 64, for 100 epochs. Preprocessing includes MTCNN face alignment with 0.9 scaling factor, resizing to 224×224, and data augmentation (flip, rotation, color jitter, ImageNet normalization).

## Key Results
- Controlled synthetic data augmentation improves S-MAD generalization when real data is scarce
- Best performance achieved with 75% synthetic data addition (EER of 6.09% using EfficientNet-B2)
- Training exclusively on synthetic data causes severe performance collapse (EER > 35%)
- Compact CNNs (EfficientNet-B2, MobileNetV3-large) remain effective for S-MAD under data constraints

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Controlled synthetic data augmentation improves S-MAD generalization when real bona fide data is scarce.
- Mechanism: Synthetic non-morphed images expand the training distribution coverage, providing additional negative samples (bona fide class) that help the model learn more robust decision boundaries without exposing real identities.
- Core assumption: Synthetic images generated by StyleGAN2-ADA capture sufficient natural face characteristics to serve as valid bona fide proxies.
- Evidence anchors:
  - [abstract] "generalization can be improved by carefully incorporating a controlled number of synthetic images into existing datasets"
  - [section 5.4.1] Best EER of 6.09% achieved with 75% synthetic data addition using EfficientNet-B2
  - [corpus] Related work on synthetic data for MAD (Tapia & Busch) confirms mixed training may improve performance, though corpus lacks direct replication of the 75% ratio finding
- Break condition: Performance degrades when synthetic data ratio exceeds an unspecified threshold (100% addition performed worse than 75% in FERET→FRGC tests: 6.81% vs 6.09% EER)

### Mechanism 2
- Claim: Exclusive reliance on synthetic training data causes severe performance collapse in operational S-MAD scenarios.
- Mechanism: Domain shift between synthetic (GAN-generated) and real face distributions creates features that do not transfer to bona fide detection on authentic images; the model learns synthetic-specific artifacts instead of generalizable morph indicators.
- Core assumption: The morph detection task requires exposure to real face image characteristics that synthetic data alone cannot provide.
- Evidence anchors:
  - [abstract] "use of only synthetic data (morphed and non-morphed images) achieves the highest Equal Error Rate (EER)"
  - [section 5.4.1] Training only with SMDD yielded 37.96% EER (EfficientNet-B2) vs 6.09% with mixed data
  - [corpus] No corpus papers directly test synthetic-only training for S-MAD; Tapia & Busch (2024) note mixed results with synthetic data integration
- Break condition: N/A—this is the failure case itself; recovery requires introducing real bona fide samples

### Mechanism 3
- Claim: Compact CNN architectures with strong inductive bias remain effective for S-MAD under data-constrained conditions.
- Mechanism: EfficientNet-B2 (9M params) and MobileNetV3-large (7.7M params) leverage translation equivariance and hierarchical feature learning, requiring fewer samples than attention-heavy alternatives like Vision Transformers.
- Core assumption: Morphing artifacts manifest as local texture and structural inconsistencies detectable through convolutional feature hierarchies.
- Evidence anchors:
  - [section 4.3] "CNNs are data-efficient models compared to their counterparts, such as the Vision Transformers"
  - [section 5.4.1] EfficientNet-B2 achieved mean EER of 6.86% across experiments; MobileNetV3-large achieved 6.93%
  - [corpus] Vision Transformer for S-MAD (arxiv 2501.09817) explores ViT representations but does not directly compare parameter efficiency
- Break condition: Performance ceiling may be lower than larger models; paper notes intention to test additional backbone models in future work

## Foundational Learning

- Concept: S-MAD vs D-MAD distinction
  - Why needed here: The paper exclusively addresses single-image detection (S-MAD) which classifies morphs without a live reference image; misunderstanding this leads to inappropriate evaluation protocols
  - Quick check question: Does your detection scenario have access to a trusted live capture for comparison, or must classification rely on the document image alone?

- Concept: Equal Error Rate (EER) and ISO metrics (MACER/BPCER)
  - Why needed here: Paper reports BPCER at fixed MACER thresholds (5%, 10%, 20%) alongside EER; operational requirements dictate which metric to optimize
  - Quick check question: In a border control scenario, would you prioritize minimizing false morph acceptance (MACER) or false rejection of legitimate travelers (BPCER)?

- Concept: Cross-dataset evaluation protocol
  - Why needed here: All experiments use train/test splits across different datasets (FERET↔FRGCv2); this tests generalization but requires careful interpretation of asymmetry in results
  - Quick check question: Why might training on FERET→testing FRGC (EER 6.09%) perform better than the reverse (EER 8.68%)?

## Architecture Onboarding

- Component map: MTCNN face alignment (0.9 scaling) -> 369×369 resize -> 224×224 -> EfficientNet-B2/MobileNetV3-large -> Binary classification (morph=1, bona fide=0)
- Critical path:
  1. Verify all training images meet ICAO quality requirements before inclusion
  2. Sample synthetic non-morphed images at target ratio (e.g., 75% of bona fide count) using random sampling without replacement
  3. Apply identical preprocessing pipeline to synthetic and real images
  4. Train with early stopping on validation set (80/20 split from training data)
  5. Evaluate on held-out dataset using MACER/BPCER trade-offs

- Design tradeoffs:
  - Higher synthetic ratio (75%): Better generalization in FERET→FRGC but worse in FRGC→FERET (14.05% EER at 75%); optimal ratio is dataset-dependent
  - EfficientNet-B2 vs MobileNetV3-large: EfficientNet achieved lower EER (6.86% vs 6.93% mean) with fewer parameters (9M vs 7.7M), but MobileNet may be preferable for mobile deployment
  - Data augmentation: Paper applies aggressive augmentation; effect of reducing augmentation not tested

- Failure signatures:
  - EER > 30%: Likely training only on synthetic data—verify training set composition
  - BPCER5 > 5% with normal EER: Model biased toward morph classification; check class balance
  - Large discrepancy between cross-dataset directions: Domain shift; consider dataset-specific tuning of synthetic ratio

- First 3 experiments:
  1. Baseline without synthetic data: Train EfficientNet-B2 on FERET, test on FRGCv2 to establish reference EER (expected ~6.47%)
  2. Incremental synthetic addition: Add 10%, 30%, 50%, 75% SMDD non-morphed samples; plot EER curve to identify optimal ratio for your target dataset pair
  3. Ablation with synthetic-only: Train exclusively on SMDD to confirm failure mode (expected EER > 35%) and validate that your pipeline correctly distinguishes real vs synthetic evaluation protocols

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does model capacity influence the effectiveness of synthetic data integration in Single Morphing Attack Detection (S-MAD)?
- **Basis in paper:** [explicit] The authors state in the conclusion: "As a next step, we intend to extend our evaluation with additional backbone models to better understand the impact of model capacity."
- **Why unresolved:** The current study exclusively utilized efficient, lightweight architectures (EfficientNet-B2 and MobileNetV3-large) to suit resource-constrained scenarios, leaving the behavior of larger, high-capacity models unknown.
- **What evidence would resolve it:** A comparative analysis of EER across a spectrum of model sizes (e.g., scaling from EfficientNet-B0 to B7 or using large Vision Transformers) using the same incremental synthetic data protocol.

### Open Question 2
- **Question:** To what extent does training with domain-specific image augmentations improve the robustness of S-MAD systems against operational degradations?
- **Basis in paper:** [explicit] The authors explicitly propose to "incorporate selected augmentation strategies that approximate operational conditions such as compression or mild print–scan artefacts" as a future direction.
- **Why unresolved:** The experiments employed standard data augmentation (flips, rotation, jitter) but did not simulate the specific distortions introduced by printing, scanning, or compression typical in document verification.
- **What evidence would resolve it:** Experiments benchmarking the current proposed method against a version trained with specialized augmentations (e.g., JPEG compression artifacts, Gaussian noise) on a test set containing print-scan morphs.

### Open Question 3
- **Question:** What specific domain shifts cause models trained exclusively on synthetic data to fail to generalize to real-world bona fide images?
- **Basis in paper:** [inferred] While the paper concludes that training "only synthetic data... achieves the highest Equal Error Rate (EER)," it does not isolate the specific features or artifacts responsible for this drastic performance drop.
- **Why unresolved:** The paper identifies the correlation (100% synthetic = high error) but does not analyze the causation, such as whether the lack of sensor noise or specific texture details in the StyleGAN2-ADA images leads to the domain gap.
- **What evidence would resolve it:** A feature space visualization (e.g., t-SNE) comparing real and synthetic bona fide images, coupled with ablation studies using synthetic images injected with real-world noise patterns.

## Limitations
- Optimal synthetic ratio (75%) shows dataset-dependent performance, suggesting findings may not generalize across all dataset pairs
- No analysis of whether synthetic and real images differ in distribution characteristics beyond simple EER metrics
- Limited backbone diversity (only two CNNs tested) prevents assessment of whether findings extend to other architectures

## Confidence
- **High confidence**: Synthetic-only training causes severe performance degradation; controlled synthetic augmentation improves S-MAD generalization; compact CNNs remain effective for S-MAD under synthetic data augmentation
- **Medium confidence**: 75% synthetic ratio as optimal (dataset-dependent variation observed); EfficientNet-B2 vs MobileNetV3-large performance comparison (single dataset direction)
- **Low confidence**: Claims about specific EER values without confidence intervals; generalization across different morphing tools not explicitly validated

## Next Checks
1. **Dataset-pair sensitivity test**: Repeat incremental synthetic addition experiments across multiple dataset pairs (FERET→FRGC, FRGC→FERET, and at least one additional pair) to validate whether 75% remains optimal or varies systematically

2. **Distribution analysis**: Perform statistical comparison of synthetic vs real image distributions (pixel intensity, frequency domain, deep feature distributions) to identify potential domain gaps affecting detection performance

3. **Synthetic quality ablation**: Test detection performance using only high-quality synthetic samples (StyleGAN2-ADA with high truncation) versus lower-quality samples to determine if synthetic data quality threshold exists for effective augmentation