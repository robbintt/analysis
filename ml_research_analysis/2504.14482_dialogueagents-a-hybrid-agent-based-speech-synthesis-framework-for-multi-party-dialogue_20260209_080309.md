---
ver: rpa2
title: 'DialogueAgents: A Hybrid Agent-Based Speech Synthesis Framework for Multi-Party
  Dialogue'
arxiv_id: '2504.14482'
source_url: https://arxiv.org/abs/2504.14482
tags:
- dialogue
- speech
- script
- writer
- synthesis
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper introduces DialogueAgents, a hybrid agent-based framework\
  \ that automatically generates multi-party, multi-turn speech dialogues. It combines\
  \ three specialized agents\u2014a script writer, a speech synthesizer, and a dialogue\
  \ critic\u2014to iteratively refine dialogue scripts and synthesize speech with\
  \ improved naturalness and emotional expressiveness."
---

# DialogueAgents: A Hybrid Agent-Based Speech Synthesis Framework for Multi-Party Dialogue

## Quick Facts
- **arXiv ID**: 2504.14482
- **Source URL**: https://arxiv.org/abs/2504.14482
- **Reference count**: 18
- **Primary result**: Multi-party speech dialogue framework with iterative agent feedback improves MOS from 3.63 to 3.75 and EMOS from 3.64 to 3.96

## Executive Summary
DialogueAgents introduces a hybrid agent-based framework that automatically generates multi-party, multi-turn speech dialogues by combining script writing, speech synthesis, and dialogue critique in an iterative loop. The framework uses specialized agents—a script writer, speech synthesizer, and dialogue critic—to refine dialogue scripts and synthesize speech with improved naturalness and emotional expressiveness. It produces MultiTalk, a bilingual Chinese/English speech dialogue dataset with diverse characters, topics, and paralinguistic features. Experimental results demonstrate that the framework improves dialogue quality across multiple metrics, with optimal performance achieved after two iterations of refinement.

## Method Summary
The framework operates through three specialized agents working iteratively: a Script Writer (GPT-4o) generates initial dialogue scripts grounded in a diverse character pool, a Speech Synthesizer (CosyVoice2) converts scripts to audio using character voice prompts, and a Dialogue Critic (Qwen2-Audio) reviews synthesized speech and provides textual feedback. Scripts are enhanced with paralinguistic tokens and emotional labels that the synthesizer interprets. The framework iterates this process, with optimal quality achieved at two refinement loops before degradation occurs due to over-optimization.

## Key Results
- MOS improves from 3.63 (no critic) to 3.75 (2-loop), EMOS from 3.64 to 3.96; 3-loop shows degradation
- Synthesized dataset MultiTalk enables training of high-quality speech models
- Optimal performance achieved after two iterations; three iterations introduce noise

## Why This Works (Mechanism)

### Mechanism 1
Iterative feedback between specialized agents improves dialogue quality up to an optimal point. The Script Writer generates an initial script → Speech Synthesizer converts to audio → Dialogue Critic (audio-capable LLM) reviews synthesized speech → provides textual feedback → Script Writer refines script with paralinguistic markers → cycle repeats. Core assumption: An audio-capable critic can detect issues in synthesized speech that text-only review cannot, and these issues can be fixed through script modification. Evidence: MOS improves from 3.63 to 3.75 (2-loop), EMOS from 3.64 to 3.96; 3-loop shows degradation (3.78 MOS, 3.91 EMOS). Break condition: Over-optimization at 3 iterations introduces noise—quality degrades rather than improves.

### Mechanism 2
Injecting paralinguistic tokens and emotional labels into scripts enables more expressive synthesis. Based on critic feedback, the Script Writer adds tokens like `<strong>`, `[breath]`, `[laughter]` and emotional labels like `[Engaging]`, `[Curious]`, `[Agreeable]` directly into dialogue text, which the speech synthesizer interprets. Core assumption: The speech synthesizer (CosyVoice2) can reliably parse and render these tokens into appropriate acoustic features. Evidence: "Understanding budgeting" becomes "Understanding budgeting and saving early can really set them up for success [breath]... [Agreeable]". Break condition: Assumption: Token interpretation depends on synthesizer capability—other TTS models may not support these tokens.

### Mechanism 3
Pre-defined character profiles with social relationships improve multi-party dialogue coherence. A manually constructed pool of 30 characters includes audio samples, profiles (age, gender, personality, linguistic habits), and social relationships (kinship, friendship, colleague). Script Writer selects from this pool when generating dialogues. Core assumption: Role-playing with consistent character grounding produces more natural multi-party interactions than generating without character context. Evidence: "This role-playing approach is more conducive to generating more coherent and realistic dialogue scripts". Break condition: Character pool is manually crafted (30 characters); scalability to larger pools untested.

## Foundational Learning

- **Zero-shot TTS with audio prompts**: Understanding how models like CosyVoice2 use reference audio to clone voice characteristics is essential—the framework relies on this for character voice consistency.
  - Quick check: Can you explain how a zero-shot TTS model uses a 3-second audio prompt to synthesize speech in a target voice?

- **Multi-agent feedback loops**: The framework uses a critique-refine pattern common in multi-agent systems.
  - Quick check: What is the key difference between sequential agent pipelines (like KE-Omni) and iterative feedback loops (like DialogueAgents)?

- **Speech quality metrics (MOS, EMOS, TMOS)**: MOS measures overall quality; EMOS captures emotional appropriateness; TMOS evaluates turn-taking naturalness.
  - Quick check: Why might EMOS and TMOS be more informative than MOS for dialogue-level speech evaluation?

## Architecture Onboarding

- **Component map**: Character selection → Initial script generation → Audio synthesis → Audio critique → Script refinement (with tokens) → Re-synthesis → Final output. Two iterations yield optimal results.

- **Critical path**: The framework follows a loop where Script Writer generates/refines dialogue scripts with paralinguistic tokens, Speech Synthesizer converts scripts to audio, and Dialogue Critic reviews audio and provides textual feedback for refinement.

- **Design tradeoffs**: Iteration count: 2 loops optimal; 3 loops cause over-optimization noise. Agent model selection: Paper uses GPT-4o/CosyVoice2/Qwen2-Audio but framework is model-agnostic. Character pool size: 30 characters balances diversity with manual curation cost.

- **Failure signatures**: Quality degradation after 3+ iterations (noise accumulation). EMOS drops from 3.96 (2-loop) to 3.91 (3-loop). WER/CER increases at 3 loops (4.34/5.36 vs 4.07/5.16 at 2 loops).

- **First 3 experiments**:
  1. Reproduce the 5-variant comparison (Writer+Synthesizer → 3-loop critic) on 10 dialogues to validate iteration optimum
  2. Test critic agent ablation: replace Qwen2-Audio with a text-only LLM to measure audio-critique value
  3. Extend character pool to 50 characters and evaluate whether dialogue diversity improves without quality loss

## Open Questions the Paper Calls Out

### Open Question 1
Can an automated stopping criterion be developed to detect the onset of over-optimization, preventing the quality degradation observed after two iterations? The authors note in Section IV.B that while performance peaks at two iterations, it declines at three due to "noise and interference introduced by over-optimization." This remains unresolved as the paper manually evaluates 1, 2, and 3 loops without proposing a self-termination mechanism. Evidence needed: A validation metric or reward model that correlates with human evaluation scores and allows autonomous halting before degradation occurs.

### Open Question 2
Is the framework robust to the substitution of proprietary models with smaller, open-source alternatives for the agent roles? Section III.D states the framework "allows for flexible adaptation of different models for each agent, which could potentially improve the framework's synthesis capabilities," but experiments exclusively use high-capacity proprietary models without testing sensitivity to weaker components. Evidence needed: Ablation studies replacing GPT-4o or CosyVoice2 with smaller open-source LLMs and TTS models to measure resulting drop in MOS/EMOS scores.

### Open Question 3
Does the Dialogue Critic's reliance on textual feedback limit the correction of subtle acoustic or prosodic errors that are difficult to articulate in text? The Dialogue Critic evaluates audio dialogue but outputs textual feedback, which the Script Writer uses to modify text tokens rather than direct acoustic features. This introduces a modality gap where nuanced auditory flaws might be lost in translation. Evidence needed: Error analysis examining cases where acoustic flaws persist despite text-level script refinement.

## Limitations

- **Audio-Critic Assumption**: Framework assumes audio-capable LLM adds value beyond text-only critique, but this lacks direct validation through ablation studies.
- **Token Interpretation Dependence**: Paralinguistic/emotional tokens only work if synthesizer reliably interprets them, limiting generalizability to other TTS systems.
- **Character Pool Scalability**: 30-character manual pool balances diversity with curation cost, but scalability to larger pools remains untested and could introduce quality degradation.

## Confidence

**High Confidence**: The 2-iteration optimum finding (MOS 3.75, EMOS 3.96) is well-supported by experimental data showing clear degradation at 3 iterations with consistent patterns.

**Medium Confidence**: Core claim that iterative audio-critique loops improve dialogue quality rests on reasonable assumptions but lacks direct comparative validation of audio vs text critique mechanisms.

**Low Confidence**: Framework's generalizability beyond CosyVoice2 and Qwen2-Audio is unproven as paper doesn't test alternative TTS or audio-LLM combinations.

## Next Checks

1. **Audio-Critic Ablation Study**: Replace Qwen2-Audio with a text-only LLM (e.g., GPT-4o) for dialogue critique while keeping all other components identical. Compare MOS/EMOS/TMOS across 2 iterations to quantify the value added by audio understanding.

2. **Token Parser Validation**: Test CosyVoice2's paralinguistic token interpretation by synthesizing the same script with and without emotional/paralinguistic markers, then measure acoustic feature differences (pitch variance, pause duration, energy levels) using forced alignment tools.

3. **Character Pool Scaling Test**: Expand the character pool from 30 to 60 characters while maintaining profile quality, then measure whether dialogue diversity (topic variety, speaker distinctiveness) improves without degrading MOS/EMOS scores.