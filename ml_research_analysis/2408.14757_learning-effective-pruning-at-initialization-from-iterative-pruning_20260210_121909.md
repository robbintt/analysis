---
ver: rpa2
title: Learning effective pruning at initialization from iterative pruning
arxiv_id: '2408.14757'
source_url: https://arxiv.org/abs/2408.14757
tags:
- pruning
- autos
- network
- dataset
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AutoSparse, a data-driven approach to pruning
  at initialization (PaI) that addresses the performance gap between PaI and iterative
  pruning at high sparsity levels. The method is motivated by the observation that
  surviving subnetworks from iterative pruning are closely related to their initial
  states.
---

# Learning effective pruning at initialization from iterative pruning

## Quick Facts
- arXiv ID: 2408.14757
- Source URL: https://arxiv.org/abs/2408.14757
- Reference count: 40
- AutoSparse achieves up to 2% higher accuracy than state-of-the-art PaI methods on VGG-16 and 0.8-0.5% higher accuracy on ResNet-18 at high sparsity levels

## Executive Summary
This paper introduces AutoSparse, a data-driven approach to pruning at initialization (PaI) that addresses the performance gap between PaI and iterative pruning at high sparsity levels. The method is motivated by the observation that surviving subnetworks from iterative pruning are closely related to their initial states. AutoSparse employs an end-to-end neural network to predict parameter importance scores based on initial features (parameters and gradients) and prunes the lowest-scoring parameters before training. The method requires only one-time iterative pruning on a single model to create a dataset that generalizes across different models and datasets. Extensive experiments demonstrate that AutoSparse outperforms state-of-the-art PaI methods, achieving up to 2% higher accuracy on VGG-16 and 0.8-0.5% higher accuracy on ResNet-18 at high sparsity levels. The approach also reveals learning tendencies of neural networks, showing that parameter magnitude has a strong relationship with importance scores while gradients have a less significant relationship.

## Method Summary
AutoSparse generates importance scores for parameters at initialization by training a neural network to predict survival scores from iterative rewind pruning (IRP). The method first performs IRP on a source model to create a dataset of initial parameters, initial gradients, and survival scores (based on how many pruning iterations each parameter survived). A ResNet-18 or VGG-19 backbone with MLP encoder/decoder heads is trained to predict these survival scores from the initial features. For inference, the trained AutoSparse model predicts scores for a new network's parameters, which are then used to prune the lowest-scoring parameters to the target sparsity before training from scratch.

## Key Results
- AutoSparse achieves up to 2% higher accuracy than state-of-the-art PaI methods on VGG-16 at 95% sparsity
- On ResNet-18, AutoSparse shows 0.8-0.5% higher accuracy than competing methods at 95% sparsity
- The method generalizes across models - training on ResNet-18/CIFAR-10 data enables effective pruning of VGG-16/CIFAR-10
- Parameter magnitude shows stronger correlation with importance scores than gradients in the learned model

## Why This Works (Mechanism)

### Mechanism 1: Survival Score as a Learnable Importance Proxy
The number of iterations a parameter survives during Iterative Rewind Pruning (IRP) serves as a valid ground-truth importance score that can be approximated by a neural network at initialization. IRP progressively removes less important parameters while rewinding remaining weights to their initialization, transforming the complex pruning history into a regression target. AutoSparse learns the mapping from initial features to survival scores, assuming surviving parameters possess intrinsic properties observable at initialization that correlate with their long-term importance.

### Mechanism 2: Asymmetric Feature Importance (Magnitude > Gradient)
Parameter magnitude at initialization provides a stronger signal for predicting survival scores than initial gradients. The AutoSparse network implicitly learns to weight input features, with ablation studies revealing that while gradients contribute to performance, the relationship between parameter values and survival scores is more direct. The learned function approximates a curve where higher absolute parameter values yield higher scores, suggesting the initialization distribution encodes sufficient information about the network's future state.

### Mechanism 3: Cross-Model Generalization of Pruning Logic
The underlying logic of parameter importance is transferable across models - a predictor trained on IRP results of one model/dataset can prune a different model/dataset effectively. The paper posits that "pruning rules" are consistent across CNNs, and by training AutoSparse on a source IRP dataset, the network learns a universal criterion rather than overfitting to specific weight values. Different architectures share similar topological or numerical patterns regarding how initial features relate to parameter utility.

## Foundational Learning

- **Iterative Rewind Pruning (IRP)**: The "oracle" used to generate ground-truth labels for training. Without understanding that IRP identifies high-performing subnetworks by training, pruning, and rewinding weights to initialization, the premise of using these weights as inputs is lost. *Quick check: How does IRP differ from standard fine-tuning pruning?*

- **Pruning at Initialization (PaI)**: The entire goal is to solve the PaI problem (pruning before training). You must distinguish between methods that rely on trained weights (post-hoc) and those that rely on initial state. *Quick check: Why is PaI generally considered harder than post-training pruning?*

- **Layer Collapse**: A common failure mode in aggressive pruning where an entire layer gets pruned. The paper mentions using SNIP-iter for dataset generation partly to avoid issues like layer collapse, ensuring the "ground truth" scores are valid. *Quick check: What happens to a network if an entire layer is pruned?*

## Architecture Onboarding

- **Component map**: Dataset Generator (IRP) -> AutoS Network (ResNet-18 + MLP heads) -> Inference Engine (pruning mask application)
- **Critical path**: The generation of the ground-truth dataset via IRP is the bottleneck. If the IRP iterations are set incorrectly, the scores become too coarse or "dirty" (ambiguous), causing the downstream AutoS training to fail.
- **Design tradeoffs**: Using a smaller, faster model (LeNet) is cheaper to run IRP on but may not teach AutoS as effectively as a moderately sized, high-performing model (ResNet-18). Using only parameters as input is faster but slightly less accurate than using Parameters + Gradients.
- **Failure signatures**: High sparsity collapse at 99% where AutoS struggles if training dataset iterations don't match target sparsity granularity. Generalization degrades significantly when target dataset complexity far exceeds source.
- **First 3 experiments**: 1) Perform IRP on LeNet/MNIST and calculate Concordance Correlation Coefficient (CCC) between two independently generated datasets to verify consistency. 2) Train three versions of AutoS (Params-only, Grad-only, Both) and visualize predicted curves to verify the learned relationship. 3) Train AutoS on ResNet-18 (CIFAR-10) IRP data and directly apply it to prune VGG-16 on CIFAR-100 without retraining, measuring accuracy gap.

## Open Questions the Paper Calls Out

### Open Question 1
Can the logarithmic-like relationship between parameter magnitude and importance scores predicted by AutoSparse be theoretically validated or derived from first principles? The authors treat the prediction model as a black box that learns complex correlations from data, lacking a theoretical framework to explain why the relationship appears logarithmic. A theoretical derivation or mathematical proof showing that optimal pruning scores in iterative rewind pruning inherently follow a logarithmic distribution relative to parameter magnitude would resolve this.

### Open Question 2
How does the generalization capability of AutoSparse scale to significantly larger datasets (e.g., full ImageNet) or diverse domains such as Large Language Models (LLMs)? The computational cost of performing the required Iterative Rewind Pruning (IRP) to generate the dataset increases drastically with model and dataset size. Empirical results demonstrating that a single IRP run on a standard large-scale model can successfully train an AutoSparse model that generalizes to LLMs or other large architectures would resolve this.

### Open Question 3
Can the performance gap between data-driven pruning at initialization and iterative pruning be fully closed by integrating features obtained later in training, such as network topology or intermediate weights? The authors suggest that incorporating features like network topology or parameters from later rewind iterations could improve accuracy, but note that mapping these to initial features is an open challenge. A methodology that successfully encodes topological or intermediate information into an initial feature vector, resulting in pruning accuracy that is statistically indistinguishable from multi-round iterative pruning, would resolve this.

## Limitations
- Dataset generation bottleneck: The method requires computationally expensive IRP to generate the training dataset, taking hours on moderate hardware despite being a one-time cost
- Generalization constraints: Performance degrades significantly when transferring from simpler to more complex datasets, suggesting learned pruning logic has domain-specific boundaries
- High sparsity fragility: At extreme sparsity levels (99%), accuracy drops more sharply than iterative pruning methods, indicating the learned predictor becomes less reliable when very few parameters remain

## Confidence

- **High confidence**: The core observation that survival iteration count from IRP can serve as a valid importance proxy (supported by consistent experimental results across multiple architectures and datasets)
- **Medium confidence**: The asymmetric feature importance claim (magnitude > gradients) - while supported by ablation studies, the relationship could be dataset-dependent and the theoretical justification remains heuristic
- **Medium confidence**: Cross-model generalization - experiments show positive transfer but with notable performance gaps, and the paper doesn't systematically explore failure modes or architectural constraints

## Next Checks

1. **Dataset consistency verification**: Run IRP twice independently on the same model/dataset pair and compute Concordance Correlation Coefficient (CCC) between resulting survival scores to quantify label noise and verify the process produces consistent importance rankings

2. **Architecture transferability test**: Train AutoS on ResNet-18/CIFAR-10 IRP data and apply it to prune architectures with different connectivity patterns (e.g., DenseNet, MobileNet) to systematically measure generalization limits beyond standard CNNs

3. **Iteration count sensitivity analysis**: Generate AutoS datasets using IRP with different iteration counts (10, 20, 50, 100) and measure how this affects prediction accuracy at various sparsity levels, particularly examining the "dirty label" hypothesis for high sparsity scenarios