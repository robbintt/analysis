---
ver: rpa2
title: How Accurately Do Large Language Models Understand Code?
arxiv_id: '2504.04372'
source_url: https://arxiv.org/abs/2504.04372
tags:
- code
- fault
- llms
- localization
- programs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first large-scale empirical investigation
  into the robustness of LLMs' code reasoning for fault localization. Inspired by
  mutation testing, the study injects faults into real-world programs and evaluates
  LLMs' ability to localize them, both with and without semantic-preserving mutations
  (SPMs) like variable renaming and dead code insertion.
---

# How Accurately Do Large Language Models Understand Code?

## Quick Facts
- **arXiv ID**: 2504.04372
- **Source URL**: https://arxiv.org/abs/2504.04372
- **Reference count**: 40
- **Primary result**: SPMs cause LLMs to fail to localize the same fault in 78% of cases

## Executive Summary
This paper presents the first large-scale empirical investigation into the robustness of LLMs' code reasoning for fault localization. Inspired by mutation testing, the study injects faults into real-world programs and evaluates LLMs' ability to localize them, both with and without semantic-preserving mutations (SPMs) like variable renaming and dead code insertion. The evaluation spans 750,013 tasks from 1,307 programs using 10 state-of-the-art LLMs. Results show that SPMs cause LLMs to fail to localize the same fault in 78% of cases, revealing a heavy reliance on syntactic and lexical cues rather than true semantic understanding. Accuracy drops nearly linearly with SPM strength, and fault localization is significantly weaker for code later in the program.

## Method Summary
The study evaluates fault localization robustness by injecting controlled faults into Python and Java programs, then applying semantic-preserving mutations (SPMs) like dead code insertion, misleading comments, and variable renaming. LLMs are tasked with localizing the original fault, then re-localizing it after SPMs are applied. The evaluation uses 750,013 tasks across 1,307 programs, filtering out "underspecified" programs where no LLM can localize the fault. The robustness metric measures the accuracy drop when SPMs are applied to programs an LLM solved correctly.

## Key Results
- SPMs cause LLMs to fail to localize the same fault in 78% of cases
- Fault localization accuracy drops nearly linearly with SPM strength
- Fault localization is significantly weaker for code appearing later in the program

## Why This Works (Mechanism)
The paper leverages mutation testing principles to create controlled variations of real code. By injecting faults and then applying semantic-preserving mutations, the study isolates whether LLMs understand code semantics or merely rely on syntactic patterns. The large scale (750K+ tasks) ensures statistical significance, while the variety of mutation types tests different aspects of LLM reasoning.

## Foundational Learning
- **Fault Localization**: Identifying the exact line of code responsible for incorrect behavior. Why needed: Core task being evaluated. Quick check: Does the model point to the injected fault line?
- **Semantic-Preserving Mutations (SPMs)**: Code transformations that don't change program behavior but alter surface features. Why needed: Tests whether LLMs rely on syntax vs. semantics. Quick check: Does the mutated code produce the same output as the original?
- **Mutation Testing**: Technique for evaluating test suite quality by injecting faults. Why needed: Provides theoretical foundation for the study design. Quick check: Are mutations systematically applied and validated?

## Architecture Onboarding
- **Component Map**: Programs -> Fault Injection -> SPM Application -> LLM Evaluation -> Accuracy Measurement
- **Critical Path**: Program selection → Fault injection → Initial localization → SPM application → Re-localization → Robustness calculation
- **Design Tradeoffs**: Large scale vs. computational cost; controlled mutations vs. real-world complexity
- **Failure Signatures**: 78% drop in accuracy with SPMs indicates syntax reliance rather than semantic understanding
- **First Experiments**:
  1. Run baseline fault localization on unmodified programs
  2. Apply single SPM type (e.g., dead code) and re-test
  3. Measure accuracy drop as function of SPM strength

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset filtering requires running 10 LLMs per program, making exact reproduction computationally prohibitive
- Specific prompts for generating misleading code elements are not provided in the paper
- AST manipulation details for complex mutations are left to external repository

## Confidence
- **High confidence**: Core observation that SPMs degrade LLM fault localization accuracy
- **Medium confidence**: Exact magnitude of degradation (78%) and linear trend with SPM strength
- **Low confidence**: Reproducibility of dataset filtering and misleading code generation without exact prompts

## Next Checks
1. Verify that the Zenodo repository contains working code for AST-based fault injection and all SPM types, including line number tracking for complex mutations
2. Test the effect of using a single proxy LLM for the existential filter instead of 10 models, and measure how this changes the dataset and reported accuracy drops
3. Implement and evaluate the misleading comment/variable name generation using the paper's description, and compare results with and without the LLM-generated variants