---
ver: rpa2
title: 'Memorize and Rank: Elevating Large Language Models for Clinical Diagnosis
  Prediction'
arxiv_id: '2501.17326'
source_url: https://arxiv.org/abs/2501.17326
tags:
- diagnosis
- prediction
- medical
- code
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MERA addresses the challenge of clinical diagnosis prediction by
  leveraging large language models to bridge the gap between natural language medical
  knowledge and structured medical codes. The method employs hierarchical contrastive
  learning and dynamic confidence thresholds to handle the large disease candidate
  space, while fine-tuning the model to memorize the mapping between medical codes
  and their natural language definitions.
---

# Memorize and Rank: Elevating Large Language Models for Clinical Diagnosis Prediction

## Quick Facts
- arXiv ID: 2501.17326
- Source URL: https://arxiv.org/abs/2501.17326
- Reference count: 6
- Primary result: Achieves 5.89-point higher weighted F1 and nearly 8 points higher recall@20 than existing models on MIMIC-III

## Executive Summary
MERA is a clinical diagnosis prediction framework that enhances large language models (LLMs) with hierarchical contrastive learning and dynamic confidence thresholds. It addresses the challenge of mapping structured medical codes to natural language knowledge through explicit memorization fine-tuning. By leveraging ICD ontology hierarchies and ranking-based inference, MERA significantly improves diagnosis prediction performance while maintaining adaptability to varying prediction set sizes.

## Method Summary
MERA employs a two-stage pipeline: first, a memorization stage where the LLM is fine-tuned on synthetic code-definition pairs to bridge the gap between natural language knowledge and medical codes. Second, a prediction stage where the model is trained with hierarchical contrastive losses across ICD ontology levels and a dynamic EOV token for adaptive thresholding. The approach uses ranking-based inference from first-token probabilities rather than autoregressive decoding, and incorporates intra-visit order perturbations to capture within-visit dependencies.

## Key Results
- Achieves 5.89-point higher weighted F1 score compared to existing best models on MIMIC-III
- Nearly 8 points higher recall@20 than existing models on MIMIC-III
- Demonstrates near-perfect memorization of bidirectional code-definition mapping (99%+ accuracy vs 45% baseline)

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Contrastive Learning on ICD Ontology
The model applies InfoNCE losses across multiple hierarchy levels in the ICD coding system, forcing true diagnoses to rank higher than similar codes within the same disease group. This structure improves discrimination among similar diseases by teaching the model to recognize both broad disease scopes and nuanced distinctions.

### Mechanism 2: Medical Code Memorization via Bidirectional Fine-tuning
LLMs are fine-tuned on synthetic QA pairs mapping ICD codes to their natural language definitions in both directions. This explicit memorization bridges the gap between the model's general language knowledge and the specific semantics of medical codes.

### Mechanism 3: Dynamic Confidence Threshold via EOV Token
A special end-of-visit token is learned to position itself appropriately in the probability ranking, enabling adaptive prediction set sizing. The model learns to output codes above the EOV threshold while suppressing those below, accommodating variable numbers of diagnoses per visit.

## Foundational Learning

- **Concept: InfoNCE / Contrastive Learning**
  - Why needed: Core to MERA's hierarchical contrastive objective for pushing positive samples together and negatives apart
  - Quick check: Given a batch with 1 positive and 5 negative codes in a disease group, can you sketch how InfoNCE would weight each term?

- **Concept: ICD Coding Ontology (ICD-9/ICD-10)**
  - Why needed: The entire hierarchical contrastive mechanism depends on understanding how ICD codes are organized into chapters, subcategories, and leaf nodes
  - Quick check: What is the hierarchy path for ICD-9 code 250.23, and which levels would MERA use for contrastive losses?

- **Concept: Autoregressive Decoding vs. Ranking Output**
  - Why needed: MERA uses token probability distributions from the first output step for ranking, rather than full autoregressive generation
  - Quick check: Why does MERA compute P(c|seq_in) from the first token distribution rather than decoding sequentially?

## Architecture Onboarding

- **Component map:** Base LLM (BioMistral-7B/LLaMA2-7B) -> Memorization Module (code↔definition fine-tuning) -> Hierarchical Contrastive Loss (InfoNCE across ICD levels) -> Dynamic Threshold Module (LDCE loss on EOV) -> Intra-visit Teacher-Forcing (partial-output variants)

- **Critical path:**
  1. Initialize base LLM with special tokens for all ICD codes in O
  2. Fine-tune on memorization task (code↔definition QA pairs, code→group pairs)
  3. Construct seq2seq data with diagnosis order perturbation (n² variants per patient)
  4. Fine-tune on diagnosis prediction with L_CL + L_DCE losses, including teacher-forcing variants
  5. Inference: rank codes by first-token probability; output codes above EOV threshold

- **Design tradeoffs:**
  - Ranking output vs. autoregressive decoding: Ranking is more stable when intra-visit order is arbitrary (17-point recall@20 gap in ablation)
  - Full-parameter fine-tuning vs. adapter-based: Full fine-tuning maximizes knowledge transfer but increases compute and risk of catastrophic forgetting
  - Large decoder-only (7B) vs. encoder-decoder (T5): Decoder-only models handle code-to-definition better; T5 excels at definition-to-code but struggles with generation

- **Failure signatures:**
  - Low code accuracy on memorization task: Insufficient memorization training or base model too small
  - High recall@20 but low F1: Model is over-predicting; EOV threshold may be undertrained
  - Large gap between recall@10 and recall@20: Ranking quality is poor; check hierarchical contrastive loss convergence
  - Duplicate codes in output: Teacher-forcing variants may not be properly excluding known codes from positive sets

- **First 3 experiments:**
  1. Memorization validation: Fine-tune on code↔definition pairs alone; report bidirectional exact-match accuracy on held-out codes. Target: >95% for 7B models
  2. Ablation on hierarchy levels: Train three variants (chapter-level CL only, finest-level CL only, full hierarchy) and compare F1/recall@20 on MIMIC-III dev set
  3. EOV threshold analysis: Plot EOV probability vs. number of output predictions across visits; check correlation with ground-truth visit sizes

## Open Questions the Paper Calls Out

### Open Question 1
Can MERA maintain high performance when predicting rare or previously unseen medical codes in the long tail of the ICD distribution? The paper reports aggregate F1 and Recall scores but does not stratify results by code frequency, leaving the model's robustness on sparse classes unconfirmed.

### Open Question 2
To what extent does the integration of unstructured clinical notes improve MERA's diagnosis prediction accuracy? The current experiments are restricted to structured codes and brief demographic profiles, ignoring the rich information in free-text clinical narratives.

### Open Question 3
Does the hierarchical contrastive learning objective genuinely align with clinical reasoning, or does it merely optimize for the ranking metric? The evaluation relies solely on quantitative metrics without qualitative analysis or user studies to validate the clinical logic of the generated rankings.

## Limitations
- Performance on rare diseases not evaluated, leaving long-tail robustness unconfirmed
- Does not incorporate unstructured clinical notes that contain rich diagnostic information
- Claims about ICD hierarchy alignment with clinical reasoning are assumed but not validated

## Confidence

- **High Confidence:** Hierarchical contrastive learning improves disease discrimination (7-10 point F1 drops when removed); code-definition memorization significantly improves LLM recall (99%+ accuracy vs 45% baseline); achieves state-of-the-art performance on MIMIC datasets

- **Medium Confidence:** Dynamic EOV threshold learns adaptive prediction set sizing (4-point F1 drop when removed); ranking output is more stable than autoregressive decoding (17-point recall@20 gap in ablation)

- **Low Confidence:** ICD hierarchy meaningfully reflects clinical disease relationships (assumed but not validated); memorization stage transfers NL knowledge to improve clinical diagnosis beyond code matching (assumed but not validated on unseen cases)

## Next Checks

1. **Hierarchy Validation:** Conduct a user study with clinicians to assess whether the ICD hierarchy groupings used in contrastive learning align with actual clinical reasoning patterns for differential diagnosis.

2. **Generalization Test:** Evaluate MERA on a held-out set of rare diseases (<100 occurrences in MIMIC) to assess whether memorization overfits to frequent codes and whether hierarchical contrastive learning helps with rare disease discrimination.

3. **Dynamic Threshold Analysis:** Analyze the correlation between EOV token probability and ground-truth visit complexity (number of diagnoses, disease severity scores) to verify adaptive behavior.