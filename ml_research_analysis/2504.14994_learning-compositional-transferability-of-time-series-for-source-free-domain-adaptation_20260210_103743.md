---
ver: rpa2
title: Learning Compositional Transferability of Time Series for Source-Free Domain
  Adaptation
arxiv_id: '2504.14994'
source_url: https://arxiv.org/abs/2504.14994
tags:
- adaptation
- domain
- source
- time
- series
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses source-free domain adaptation (SFDA) for\
  \ time series classification, tackling the challenge of domain shift when source\
  \ data is inaccessible. The core method uses a compositional reconstruction architecture:\
  \ a frozen pre-trained U-net provides coarse source-informed adaptation, followed\
  \ by two parallel branches\u2014source replay (via residual link) and offset compensation\
  \ (via additional autoencoder)\u2014for fine-grained adaptation."
---

# Learning Compositional Transferability of Time Series for Source-Free Domain Adaptation

## Quick Facts
- arXiv ID: 2504.14994
- Source URL: https://arxiv.org/abs/2504.14994
- Authors: Hankang Sun; Guiming Li; Su Yang; Baoqi Li
- Reference count: 10
- Primary result: SFDA method achieves 3.7%, 0.78%, and 2.6% MF1 improvements across three benchmarks

## Executive Summary
This paper addresses source-free domain adaptation (SFDA) for time series classification, tackling the challenge of domain shift when source data is inaccessible. The core method uses a compositional reconstruction architecture: a frozen pre-trained U-net provides coarse source-informed adaptation, followed by two parallel branches—source replay (via residual link) and offset compensation (via additional autoencoder)—for fine-grained adaptation. The approach preserves source prior knowledge while adapting to target domain variations. During inference, instance-wise adaptation is applied by testing stability-aware rescaling of the source replay branch. Experiments on three benchmarks (MFD, SSC, UCIHAR) show state-of-the-art performance with improvements of 3.7%, 0.78%, and 2.6% in MF1 score compared to existing methods. The method demonstrates strong generalizability and computational efficiency, making it suitable for real-world applications where source data is unavailable.

## Method Summary
The proposed method addresses source-free domain adaptation for time series classification by introducing a compositional reconstruction architecture. It consists of a frozen pre-trained U-net that provides coarse source-informed adaptation, followed by two parallel fine-tuning branches: a source replay branch that uses residual connections to maintain source domain knowledge, and an offset compensation branch that employs an additional autoencoder to handle target domain variations. During inference, instance-wise adaptation is performed by stability-aware rescaling of the source replay branch to ensure robust predictions across varying target domain conditions.

## Key Results
- Achieves state-of-the-art performance with MF1 score improvements of 3.7%, 0.78%, and 2.6% on MFD, SSC, and UCIHAR benchmarks respectively
- Demonstrates effective handling of domain shift without access to source data during adaptation
- Shows strong computational efficiency suitable for real-world deployment scenarios

## Why This Works (Mechanism)
The compositional architecture works by preserving source domain knowledge through a frozen pre-trained U-net while allowing flexible adaptation through two parallel branches. The source replay branch maintains continuity with the source domain via residual connections, preventing catastrophic forgetting. The offset compensation branch captures target-specific variations through additional encoding. The stability-aware rescaling during inference dynamically adjusts the contribution of each branch based on instance-specific characteristics, enabling robust predictions even under varying domain conditions.

## Foundational Learning

**Time Series Domain Adaptation**: Adapting models trained on one time series domain to perform well on a different but related domain without access to source data during adaptation. Why needed: Real-world deployment often encounters domain shifts where source data cannot be accessed due to privacy or storage constraints. Quick check: Verify that the source and target domains share similar temporal patterns and feature distributions.

**Compositional Reconstruction**: Using multiple parallel processing branches that combine different aspects of source and target domain knowledge. Why needed: Single-branch approaches often struggle to balance preserving source knowledge while adapting to target variations. Quick check: Ensure each branch captures distinct aspects of the domain adaptation problem without redundancy.

**Residual Connections**: Skip connections that allow gradients to flow directly through the network, preserving source information. Why needed: Without residual connections, fine-tuning often leads to catastrophic forgetting of source domain knowledge. Quick check: Monitor gradient flow and parameter changes during fine-tuning to ensure source knowledge preservation.

**Instance-wise Adaptation**: Dynamically adjusting model parameters or outputs for each individual test instance based on stability metrics. Why needed: Domain shifts often vary across different instances within the target domain. Quick check: Validate that adaptation decisions improve performance across diverse instances rather than just average cases.

## Architecture Onboarding

Component map: Pre-trained U-net -> Frozen Feature Extractor -> [Source Replay Branch | Offset Compensation Branch] -> Stability-Aware Rescaling -> Final Classification

Critical path: Source domain features → Frozen U-net → Source Replay Branch (with residual) → Stability-Aware Rescaling → Classification output

Design tradeoffs: The frozen U-net provides stable source knowledge but limits adaptation flexibility. The two-branch design allows capturing both source continuity and target variations but increases model complexity. The stability-aware rescaling adds computational overhead during inference but improves robustness.

Failure signatures: Poor source replay branch performance indicates insufficient preservation of source knowledge. Ineffective offset compensation suggests target domain variations are not being properly captured. Unstable stability-aware rescaling points to inconsistent domain characteristics across instances.

First experiments: 1) Ablation study removing either source replay or offset compensation branch to quantify individual contributions. 2) Testing on extreme domain shifts where source and target have minimal overlap. 3) Measuring inference time complexity with varying sequence lengths and feature dimensions.

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions in the provided content.

## Limitations

The method's effectiveness relies heavily on the quality of the pre-trained U-net as a source domain prior, with no explicit mechanism for handling catastrophic failure when source and target domains exhibit fundamental distributional differences. The instance-wise stability-aware rescaling during inference introduces computational overhead proportional to test set size, potentially limiting real-time deployment. The experimental validation focuses exclusively on three time series benchmarks (MFD, SSC, UCIHAR), all of which represent relatively structured sensor data, raising questions about generalizability to more complex domains like medical time series or financial data with irregular sampling patterns.

## Confidence

**High**: The compositional architecture design and its basic functionality are well-supported by the paper's methodology and mathematical formulation.
**Medium**: The empirical performance improvements (3.7%, 0.78%, 2.6% MF1 gains) are statistically significant but require independent replication given the limited number of benchmark datasets.
**Low**: The claim of "strong generalizability" lacks systematic validation across diverse time series domains and distribution shifts.

## Next Checks

1. Conduct ablation studies to quantify the individual contributions of the source replay branch versus offset compensation branch to overall performance gains.
2. Test the method on datasets with varying degrees of domain shift severity, including cases where source and target distributions have minimal overlap.
3. Evaluate computational efficiency during inference by measuring the time complexity scaling with increasing sequence length and feature dimensionality.