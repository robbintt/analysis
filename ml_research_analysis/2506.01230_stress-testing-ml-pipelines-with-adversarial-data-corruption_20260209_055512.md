---
ver: rpa2
title: Stress-Testing ML Pipelines with Adversarial Data Corruption
arxiv_id: '2506.01230'
source_url: https://arxiv.org/abs/2506.01230
tags:
- data
- corruption
- errors
- search
- label
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Savage introduces a formal framework for modeling realistic data
  corruption mechanisms, such as missing values, label errors, and selection bias,
  through dependency graphs and pattern-based corruption templates. The core method
  employs bi-level optimization: an upper-level beam search identifies the most harmful
  corruption patterns, while a lower-level Bayesian optimization tunes corruption
  parameters to maximize model degradation.'
---

# Stress-Testing ML Pipelines with Adversarial Data Corruption

## Quick Facts
- arXiv ID: 2506.01230
- Source URL: https://arxiv.org/abs/2506.01230
- Reference count: 40
- One-line primary result: Introduces Savage framework using bi-level optimization to efficiently discover structured data corruption patterns that severely degrade ML pipeline performance.

## Executive Summary
Savage introduces a formal framework for modeling realistic data corruption mechanisms, such as missing values, label errors, and selection bias, through dependency graphs and pattern-based corruption templates. The core method employs bi-level optimization: an upper-level beam search identifies the most harmful corruption patterns, while a lower-level Bayesian optimization tunes corruption parameters to maximize model degradation. Extensive experiments across multiple datasets and ML tasks show that even small fractions of structured corruptions identified by Savage severely impact model performance, far exceeding random or manually crafted errors.

## Method Summary
The Savage framework formalizes realistic data corruption through dependency graphs where corruption probability depends on attribute values. It employs bi-level optimization to efficiently discover worst-case corruptions: beam search explores candidate dependency graphs (patterns) at the upper level, while Tree-Structured Parzen Estimator (TPE) optimization tunes corruption parameters at the lower level. The framework treats the full ML pipeline as a black box, evaluating corruptions solely through pipeline outputs. Corruption patterns are conjunctions of range conditions over attributes and noise variables, supporting missing values, label errors, and selection bias. The method discovers patterns that degrade performance metrics like AUC, F1, and fairness measures far more than random corruptions.

## Key Results
- Even 5-30% structured corruptions identified by Savage severely degrade model performance across multiple datasets and tasks
- Savage outperforms random and manually crafted corruptions by large margins, achieving AUC degradation from 0.80 to 0.39 on Adult dataset
- Transferred patterns from proxy pipelines to target pipelines retain effectiveness with <0.01 AUC difference
- 91% of census columns exhibit highly predictable missingness patterns, validating the structured corruption assumption

## Why This Works (Mechanism)

### Mechanism 1
Structured data corruptions can be formally modeled as dependency graphs where corruption probability depends on attribute values. A directed acyclic graph encodes which parent attributes govern the corruption of child attributes, applying transformations only when pattern conditions evaluate to true for a tuple. This assumes real-world data errors are systematically correlated with observable attributes rather than randomly distributed, supported by analysis showing 91% of census columns have predictable missingness patterns.

### Mechanism 2
Bi-level optimization separates combinatorial search for corruption patterns from continuous parameter tuning. Upper-level beam search explores candidate dependency graphs by expanding patterns with additional attributes and retaining top-k performers. Lower-level TPE optimization tunes corruption parameters for each candidate graph, modeling promising vs. poor parameter distributions. This assumes harmful corruptions concentrate in specific subpopulations defined by attribute conjunctions that beam search can navigate heuristically.

### Mechanism 3
Treating the full ML pipeline as a black box enables stress-testing of non-differentiable components without gradient access. The framework evaluates corruptions solely through pipeline outputs (model performance metrics), not internal gradients. TPE samples parameters, observes metric degradation, and updates density estimators to guide search. This assumes performance metrics provide sufficient signal for optimization even without gradient information, though the paper acknowledges approximation errors from dropping expectations.

## Foundational Learning

- **Missing Not At Random (MNAR) vs. MCAR/MAR**: Why needed here: SAVAGE explicitly targets structured missingness where missing probability depends on unobserved or observed values—this is what makes corruptions harmful and realistic. Quick check question: If high-income respondents are more likely to skip the income field, is this MCAR, MAR, or MNAR?

- **Bayesian Optimization with TPE**: Why needed here: Lower-level optimization uses Tree-Structured Parzen Estimator to tune corruption parameters without gradients; understanding how `g(θ)/l(θ)` guides search is essential. Quick check question: Why does TPE split historical evaluations into "promising" and "poor" groups rather than modeling performance directly?

- **Beam Search**: Why needed here: Upper-level search explores exponentially many dependency graphs; beam search balances exploration vs. exploitation by retaining top-k candidates per iteration. Quick check question: How does beam width affect the tradeoff between search thoroughness and computational cost?

## Architecture Onboarding

- **Component map**: Training dataset D_train -> Dependency Search (Upper Level) -> Parameter Tuning (Lower Level) -> Corruption Application -> Evaluation Layer -> Performance metric Ψ

- **Critical path**: 1) Seed pattern generation (single-attribute patterns) 2) For each beam iteration: expand patterns → run TPE for each → evaluate → retain top-B 3) Return worst-case DCP (dependency graph + parameters) from final beam

- **Design tradeoffs**: Beam width vs. search coverage (wider beams explore more patterns but increase runtime linearly); Proxy models vs. target pipelines (using lightweight proxies accelerates discovery but may miss pipeline-specific vulnerabilities); Error budget granularity (lower budgets restrict search space but may miss realistic worst cases)

- **Failure signatures**: Stagnation (beam search terminates early if no improvement across iterations); Sparse subpopulations (patterns defining tiny subpopulations may cause unreliable metric estimates); Assumption violation (methods like CP-MDA-Nested fail when missingness depends on labels—this is expected behavior revealing invalid assumptions)

- **First 3 experiments**: 1) Reproduce Adult dataset attack: Run SAVAGE on Adult with 10% label error budget, logistic regression downstream; verify identified pattern correlates with both demographics and label 2) Ablate beam width: Compare beam widths B∈{1, 3, 5, 10} on Employee dataset; measure AUC degradation and runtime to understand tradeoff curve 3) Transfer patterns across pipelines: Discover worst-case pattern using iterative imputer proxy, apply to BoostClean; verify claim that transferred patterns retain effectiveness (reported <0.01 AUC difference)

## Open Questions the Paper Calls Out

### Open Question 1
How do multi-attribute corruptions interact to create failure modes that cannot be discovered by single-attribute corruption search? The paper restricts search to single-target-attribute corruptions to manage computational complexity, acknowledging this as a deliberate limitation. The combinatorial explosion of multi-attribute dependency graphs makes exhaustive search infeasible, and the paper does not propose tractable approximation methods. Empirical characterization of whether single-attribute corruptions compose predictably would resolve this.

### Open Question 2
Under what conditions does the proxy-model transfer strategy fail to identify harmful corruption patterns? Section 5.5.3 reports successful transfer to AutoSklearn and BoostClean, but validates only on two frameworks with AUC differences < 0.01. The generalization bounds of this transfer remain uncharacterized. No theoretical analysis of when proxy-to-target pattern transfer preserves worst-case optimality exists. Systematic evaluation across diverse pipeline architectures would resolve this.

### Open Question 3
Can certification-style guarantees be derived for the beam search's ability to approximate globally optimal corruption patterns? The paper uses beam search heuristically without suboptimality bounds. Section 4.2.1 acknowledges beam search "balances exploration and exploitation" but provides no approximation guarantees. The black-box nature of the pipeline objective and the combinatorial search space complicate theoretical analysis. Theoretical analysis bounding the gap between beam search solutions and global optima would resolve this.

## Limitations
- Framework assumes systematic rather than random error patterns; effectiveness diminishes if this assumption fails
- Limited to tabular data; scalability to high-dimensional or unstructured data remains untested
- Exact beam width, maximum depth, and TPE iteration counts are unspecified, limiting reproducibility

## Confidence
- **High**: Structured corruption patterns are more harmful than random errors (multiple datasets show consistent degradation)
- **Medium**: Bi-level optimization effectively discovers worst-case patterns (lacks exact parameter specifications)
- **Low**: Framework generalizes to non-tabular data (no experiments beyond tabular domains)

## Next Checks
1. **Parameter Sensitivity Analysis**: Systematically vary beam width B and TPE iteration counts to identify performance breakpoints and computational tradeoffs
2. **Random Error Baseline**: Generate synthetic random corruption patterns with matching budgets to quantify the marginal harm of structured vs. unstructured errors
3. **Out-of-Distribution Transfer**: Apply corruption patterns discovered on one dataset (e.g., Adult) to train models on structurally similar but different datasets to test pattern transferability