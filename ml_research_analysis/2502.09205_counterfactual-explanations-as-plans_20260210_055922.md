---
ver: rpa2
title: Counterfactual Explanations as Plans
arxiv_id: '2502.09205'
source_url: https://arxiv.org/abs/2502.09205
tags:
- agent
- actions
- knowledge
- such
- explanations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a formal account of counterfactual explanations
  for planning problems in the presence of both physical and sensing actions. The
  core method involves using a modal fragment of the situation calculus to formalize
  the distinction between what is true in the real world versus what is known or believed
  by an agent.
---

# Counterfactual Explanations as Plans

## Quick Facts
- arXiv ID: 2502.09205
- Source URL: https://arxiv.org/abs/2502.09205
- Reference count: 40
- Key outcome: Formal account of counterfactual explanations for planning agents using modal logic to handle physical and sensing actions

## Executive Summary
This paper provides a formal framework for counterfactual explanations in planning domains where agents may have partial, weakened, or false beliefs. The core contribution is a modal logic formalization that distinguishes between what is true in the world versus what agents know or believe, enabling explanations that identify missing actions or knowledge needed to achieve goals. The approach is general and adaptable to different planning languages, with theoretical results covering various epistemic settings and introducing the concept of diverse counterfactual explanations.

## Method Summary
The paper formalizes counterfactual explanations as alternative action sequences that negate the original goal, using a modal fragment of the situation calculus (E^S) to separate truth from belief. Given a plan δ achieving goal φ, a counterfactual explanation is a minimal-distance sequence δ′ such that the goal is negated after execution. The framework handles three epistemic settings: agents with partial truths (Σ′₀ ⊆ Σ₀), weakened truths, and false beliefs. Distance metrics include length-based, fluent-based, and plan-and-effect minimality. For epistemic reconciliation, explanations identify either missing actions (augmenting with sensing) or missing knowledge (adding fluents to beliefs), with minimality constraints.

## Key Results
- Counterfactual explanations formally defined as alternative action sequences negating goals in planning domains
- Three orthogonal distance metrics (length-based, fluent-based, plan-and-effect) for measuring plan minimality
- Model reconciliation framework identifying missing knowledge or actions across epistemic settings
- Diverse counterfactual explanations allowing multiple explanations constrained by specific features

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Counterfactual explanations for planning agents can be formalized as alternative action sequences that negate the original goal.
- Mechanism: Given a plan δ that achieves goal φ, a counterfactual explanation is a sequence δ′ such that Σ |= Exec(δ′) ∧ [δ′]¬φ, with distance dist(δ′, δ) minimized.
- Core assumption: The planning domain can be expressed in a first-order logical theory with successor state axioms, preconditions, and sensing axioms.
- Evidence anchors: [abstract] "counterfactuals are defined as alternative action sequences that negate the goal"; [section 4, Definition 1] Formal definition provided.

### Mechanism 2
- Claim: Plan minimality can be measured along three orthogonal dimensions: length-based, fluent-based, and plan-and-effect minimality.
- Mechanism: Length-based minimality minimizes |length(δ′) − length(δ)|. Fluent-based minimality minimizes |size(fluents(δ)) − size(fluents(δ′))|, where fluents(δ) captures all properties affected by actions in δ. Plan-and-effect minimality jointly optimizes both to prevent gaming either metric alone.
- Core assumption: Actions have interpretable preconditions and effects that can be extracted from the action theory.
- Evidence anchors: [section 4, Definition 2] Length-based minimality definition; [section 4, Example 7] Demonstrates fluent-based minimality alone fails.

### Mechanism 3
- Claim: Model reconciliation operates by identifying missing knowledge α (from user's ground truth) or missing actions δ′ that enable the agent to know and achieve the goal.
- Mechanism: For agents with partial truths (Σ′₀ ⊆ Σ₀), if Σ ̸|= [δ]Kφ, the explanation is either (a) δ′ augmenting actions with sensing, or (b) α ∈ Σ₀ − Σ′₀ added to agent's beliefs, or both.
- Core assumption: The real world (user's knowledge) and agent's beliefs share the same dynamics (Σdyn), differing only in initial state knowledge.
- Evidence anchors: [section 5.1, Definition 13-18] Formal treatment of missing actions, missing knowledge, and combined cases.

## Foundational Learning

- Concept: **Situation Calculus**
  - Why needed here: The paper formalizes planning in E^S, a modal variant of the situation calculus. Understanding fluent predicates, successor state axioms, and the [a] modality ("after action a") is prerequisite to reading Definitions 1-21.
  - Quick check question: Given axiom 2[a]Holding(x) ≡ a = pickup(x) ∨ (Holding(x) ∧ a ≠ drop(x)), what holds after pickup(c)·drop(c)?

- Concept: **Epistemic Modal Logic (K, B, O operators)**
  - Why needed here: The core contribution distinguishes Kφ (agent knows φ) from truth of φ, and Oα (agent only-knows α) captures both beliefs and non-beliefs.
  - Quick check question: If Oα |= Kβ when α |= β, what does O(p ∨ q) entail about Kp?

- Concept: **Basic Action Theories**
  - Why needed here: The framework assumes domains are axiomatized as Σ = Σ₀ ∧ Σdyn ∧ O(Σ′₀ ∧ Σdyn), with preconditions, successor state axioms, and sensing axioms.
  - Quick check question: What components go into Σdyn versus Σ₀?

## Architecture Onboarding

- Component map:
  ```
  Domain Axiomatization (Σ₀, Σ′₀, Σdyn)
         ↓
  Planning Engine (finds δ achieving φ)
         ↓
  CF Generator (finds δ′ with [δ′]¬φ, minimized dist)
         ↓
  Reconciliation Module (identifies α or β to update agent beliefs)
  ```

- Critical path: Encode domain as basic action theory → Implement projection (Σ |= [δ]φ) → Implement distance metrics → Adapt planner for CF search → Add epistemic reasoning for reconciliation.

- Design tradeoffs:
  - **Language choice**: E^S enables regression to non-modal reasoning (Section 2, property discussion), but requires comfort with modal logic. Alternatives: STRIPS-like representations (simpler, less expressive), dynamic logic (mentioned in Section 1 as related).
  - **Distance metric**: Length-based is computationally cheap; fluent-based requires effect analysis; plan-and-effect is most principled but costliest.
  - **Sensing integration**: Sensing actions don't affect Σ₀ but enable Kφ; decide whether to require [δ′]Kφ or just [δ′]φ.

- Failure signatures:
  - CF search returns δ′ with irrelevant actions → fluent-based minimality insufficient; switch to plan-and-effect.
  - Reconciliation suggests impossible α → Σ′₀ − Σ₀ assumption violated; check for false beliefs.
  - Agent knows φ but doesn't know it knows φ → missing sensing actions; Definition 13 addresses this.
  - KExec(δ) fails but Exec(δ) holds → agent lacks knowledge of preconditions; Definition 16 applies.

- First 3 experiments:
  1. Encode a simple blocks-world domain (as in Section 2 example) with 3-4 fluents. Manually verify CF explanations for δ = pickup(c)·drop(c) with goal ¬Broken(c).
  2. Implement length-based distance and compare CF quality against fluent-based distance on a domain with 5+ action types. Measure whether fluent-based produces more semantically coherent alternatives.
  3. Simulate reconciliation: give agent Σ′₀ missing one fluent (e.g., Glass(d)) and verify that Definition 17 correctly identifies α = Glass(d) as the explanation when δ = pickup(d)·drop(d) fails to yield KBroken(d).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can operational aspects of plans, costs, optimality, and conciseness be formally incorporated into the counterfactual explanation framework?
- Basis in paper: [explicit] The conclusion states: "For the future, it would be interesting to incorporate other notions in our formalization, such as operational aspects of plans, costs, optimality and conciseness [14, 40, 42], towards a unified mathematical specification of explainable planning."
- Why unresolved: The current framework provides distance metrics but does not address action costs, resource constraints, or plan quality measures.
- What evidence would resolve it: An extended formalization integrating cost functions over actions and proving that minimal-cost counterfactual explanations can be computed efficiently.

### Open Question 2
- Question: Can the proposed counterfactual explanation definitions be implemented using answer set programming (ASP) or other declarative approaches?
- Basis in paper: [explicit] Section 6 states: "exploring whether our definitions can be implemented in such approaches is worthwhile" regarding ASP-based approaches like [5] and [4].
- Why unresolved: The paper provides a logical specification but does not demonstrate an actual implementation or prove compatibility with existing declarative frameworks.
- What evidence would resolve it: A translation from the modal logic specifications to ASP rules, with an empirical demonstration of counterfactual plan generation on benchmark domains.

### Open Question 3
- Question: How does the counterfactual explanation framework relate formally to explanation-based diagnosis in the situation calculus?
- Basis in paper: [explicit] Section 6 states: "we believe that a further formal study to relate such accounts would be useful, and could nicely complement empirical works such as [9]."
- Why unresolved: While both approaches use action theories to explain outcomes, diagnosis typically requires modeling faulty components whereas counterfactual explanations identify action/knowledge modifications without explicit fault models.
- What evidence would resolve it: A theorem establishing bidirectional reductions between diagnosis problems and counterfactual explanation problems under specific conditions.

## Limitations
- The framework assumes well-structured domains with interpretable action theories, limiting applicability to real-world, noisy environments
- Computational complexity of plan-and-effect minimality and epistemic planning for large domains remains unclear
- Handling of false beliefs (Σ′₀ ⊈ Σ₀) is noted as requiring further work on belief revision and forgetting

## Confidence
- **Medium** for core mechanism (Definition 1): Builds directly on established modal logic foundations
- **Low** for diversity extension (Section 6): Briefly sketched without formal treatment
- **Medium** for epistemic reconciliation framework: Theoretical soundness established but no empirical validation

## Next Checks
1. Implement the blocks world example (Section 2) and verify CF explanations for δ = pickup(c)·drop(c) with goal ¬Broken(c) against Example 3.
2. Test fluent-based vs. plan-and-effect minimality on a domain with conditional effects to confirm Example 7's finding that fluent-based alone fails.
3. Encode a domain where agent's beliefs Σ′₀ miss a fluent (e.g., Glass(d)) and verify Definition 17 correctly identifies α = Glass(d) as the explanation when δ = pickup(d)·drop(d) fails to yield KBroken(d).