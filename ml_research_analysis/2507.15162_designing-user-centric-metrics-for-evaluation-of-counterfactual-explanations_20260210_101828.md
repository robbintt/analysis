---
ver: rpa2
title: Designing User-Centric Metrics for Evaluation of Counterfactual Explanations
arxiv_id: '2507.15162'
source_url: https://arxiv.org/abs/2507.15162
tags:
- user
- feature
- proximity
- cfes
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the gap between existing counterfactual explanation
  (CFE) evaluation metrics and user preferences by conducting two user studies to
  validate personalized weighted proximity as a more user-aligned metric. The first
  pilot study with 20 MTurk participants revealed that standard proximity and sparsity-based
  CFEs matched user preferences in only 63.81% and 69.51% of cases, respectively.
---

# Designing User-Centric Metrics for Evaluation of Counterfactual Explanations

## Quick Facts
- arXiv ID: 2507.15162
- Source URL: https://arxiv.org/abs/2507.15162
- Authors: Firdaus Ahmed Choudhury; Ethan Leicht; Jude Ethan Bislig; Hangzhi Guo; Amulya Yadav
- Reference count: 19
- Primary result: User-centric metric AWP predicts user-preferred CFEs with 84.37% accuracy

## Executive Summary
This paper addresses a critical gap in counterfactual explanation (CFE) evaluation by developing metrics that align with actual user preferences rather than relying on traditional proximity and sparsity measures. Through two comprehensive user studies involving 61 participants across different platforms, the authors demonstrate that standard CFE evaluation metrics match user preferences in only 63-70% of cases. The research introduces the Acceptability & Weighted Proximity (AWP) model, which incorporates personalized feature weights and acceptability thresholds to predict which counterfactual explanation users will prefer with 84.37% accuracy.

## Method Summary
The authors conducted two user studies to validate personalized weighted proximity as a more user-aligned metric for CFE evaluation. The first pilot study with 20 MTurk participants evaluated standard proximity and sparsity-based CFEs against user preferences. The second, more detailed two-day study with 41 participants presented realistic credit scenarios where participants faced loan rejection and were asked to choose between different counterfactual explanations. Participants provided preferences on feature trade-offs, acceptability thresholds, and rounding preferences across multiple scenarios. The study tested three hypotheses about user preferences and used the results to develop and validate the AWP model.

## Key Results
- Standard proximity and sparsity metrics match user preferences in only 63.81% and 69.51% of cases, respectively
- Personalized weighted proximity achieves 80.2% preference alignment with users
- The AWP model predicts user-preferred CFEs with 84.37% accuracy

## Why This Works (Mechanism)
The AWP model works by incorporating individual user preferences into the CFE evaluation process rather than applying one-size-fits-all metrics. By collecting data on how users value different features and what changes they consider acceptable, the model creates personalized cost functions that better reflect real decision-making processes. This personalization captures the reality that users have different priorities and thresholds for what constitutes an acceptable explanation, leading to significantly better alignment between generated CFEs and user preferences.

## Foundational Learning
**Counterfactual Explanations (CFEs)**: Explanations that show how an input would need to change to achieve a different model prediction. Why needed: Form the basis for actionable recommendations in ML decision-making. Quick check: Verify CFEs actually lead to different predictions when applied.

**Proximity Metrics**: Measures of how close a counterfactual is to the original instance. Why needed: Traditional way to evaluate CFE quality, but shown insufficient alone. Quick check: Compare distance metrics across different feature scales.

**Sparsity Metrics**: Counts of how many features change in a counterfactual. Why needed: Another traditional metric that assumes fewer changes are always better. Quick check: Validate that minimal changes align with user acceptability.

**Acceptability Thresholds**: User-defined boundaries for what feature changes are considered reasonable. Why needed: Captures that some changes are inherently more acceptable than others. Quick check: Survey users on maximum acceptable changes per feature.

**Personalized Weighting**: Assigning different importance values to features based on user preferences. Why needed: Reflects that users value different features differently in decision contexts. Quick check: Conduct preference ranking exercises with users.

## Architecture Onboarding

**Component Map**: User Input Collection -> Feature Weighting Module -> Acceptability Thresholding -> CFE Generation -> Preference Prediction

**Critical Path**: The core workflow flows from collecting user preferences about feature acceptability and weights, through applying these preferences to generate personalized CFEs, to predicting which explanation the user will prefer. The critical dependency is accurate user preference collection, as all downstream predictions rely on this foundation.

**Design Tradeoffs**: The model trades computational simplicity for personalization accuracy. While simpler metrics like pure proximity are faster to compute, they sacrifice the 15-20% improvement in preference alignment that AWP provides. The approach requires upfront user interaction but delivers substantially better results.

**Failure Signatures**: The model may fail when user preferences are inconsistent across scenarios, when feature importance shifts dramatically between contexts, or when the credit domain doesn't generalize to other application areas. Performance degradation is likely when user populations are heterogeneous without clear preference patterns.

**3 First Experiments**:
1. Test AWP on a different domain (e.g., medical treatment recommendations) to validate generalizability
2. Compare AWP against state-of-the-art CFE generation methods using the same user preference data
3. Evaluate the stability of user preferences by conducting the same study with participants after a 3-month interval

## Open Questions the Paper Calls Out
None

## Limitations
- Sample populations recruited through MTurk and Prolific may not represent all decision-makers who encounter CFEs in real-world contexts
- Findings are based on credit scoring scenarios and may not generalize to other high-stakes domains
- The 84.37% accuracy rate still results in misclassification in approximately 15.6% of cases
- Results focus on tabular data and may not extend to complex data types like images or text

## Confidence
- High confidence: Standard proximity and sparsity metrics fail to align with user preferences in the majority of cases (63.81% and 69.51% alignment rates)
- Medium confidence: The proposed AWP model's predictive accuracy and its general framework for user-centric CFE evaluation
- Medium confidence: The specific quantitative thresholds for feature acceptability derived from the credit scenario

## Next Checks
1. Replicate the user studies across at least two additional high-stakes domains (e.g., healthcare treatment recommendations and loan approval) to test domain transferability of the AWP framework
2. Conduct a longitudinal study to assess whether users' preferences for feature trade-offs remain stable over time or shift with experience
3. Test the AWP model with CFEs generated from black-box models with varying complexity levels to evaluate robustness across different ML architectures