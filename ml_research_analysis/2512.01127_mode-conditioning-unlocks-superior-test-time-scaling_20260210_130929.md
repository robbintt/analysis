---
ver: rpa2
title: Mode-Conditioning Unlocks Superior Test-Time Scaling
arxiv_id: '2512.01127'
source_url: https://arxiv.org/abs/2512.01127
tags:
- modc
- training
- pass
- scaling
- modes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces mode-conditioning (ModC), a framework that
  explicitly allocates test-time compute across reasoning modes to address diversity
  collapse in large language models. By training either specialist models or a single
  model with mode-specific prefixes, ModC consistently improves scaling across tasks
  from controlled graph search to large-scale reasoning benchmarks.
---

# Mode-Conditioning Unlocks Superior Test-Time Scaling

## Quick Facts
- arXiv ID: 2512.01127
- Source URL: https://arxiv.org/abs/2512.01127
- Authors: Chen Henry Wu; Sachin Goyal; Aditi Raghunathan
- Reference count: 4
- Primary result: Up to 4x efficiency gains in long chain-of-thought reasoning on OpenThoughts using gradient clustering for mode discovery

## Executive Summary
This paper introduces mode-conditioning (ModC), a framework that explicitly allocates test-time compute across reasoning modes to address diversity collapse in large language models. By training either specialist models or a single model with mode-specific prefixes, ModC consistently improves scaling across tasks from controlled graph search to large-scale reasoning benchmarks. Experiments show up to 4x efficiency gains in long chain-of-thought reasoning on OpenThoughts and up to 10% gains on NuminaMath using gradient clustering to automate mode discovery. ModC also improves reinforcement learning, enhancing both standard RL and diversity-inducing RL methods.

## Method Summary
Mode-conditioning addresses diversity collapse by explicitly partitioning test-time samples across distinct reasoning modes during inference. The framework supports two training variants: (1) separate specialist models per mode, or (2) a single model with learnable prefix tokens that encode different reasoning strategies. At inference, samples are allocated evenly across modes (k/C per mode for C modes), then aggregated to maximize Pass@k. Modes can be defined manually (e.g., BFS vs DFS) or discovered automatically via gradient clustering, which clusters training examples by gradient similarity to recover implicit reasoning patterns without manual labels.

## Key Results
- 4x efficiency gains in long chain-of-thought reasoning on OpenThoughts benchmark
- Up to 10% improvement on NuminaMath using gradient clustering for mode discovery
- Consistent Pass@k improvements across diverse tasks from controlled graph search to large-scale reasoning
- ModC with prefixes outperforms separate models on math tasks, suggesting knowledge sharing is crucial
- Gradient clustering achieves 98.7% F1 score in recovering teacher assignments on labeled multi-teacher data

## Why This Works (Mechanism)

### Mechanism 1: Explicit Compute Allocation Across Modes
If two modes have success probabilities p₁ and p₂ on input x, sampling k/2 from each mode yields Pass@k = 1 - (1-p₁)^(k/2)(1-p₂)^(k/2), which strictly exceeds sampling k times from the mixture when p₁ ≠ p₂ (by Jensen's inequality on the concave function). This works because modes have complementary strengths—when one fails on a problem, the other may succeed.

### Mechanism 2: Mode-Specific Prefix Training
Prepending discrete tokens (e.g., `[Mode 1]`, `[Mode 2]`) to inputs during SFT enables the model to learn distinct reasoning strategies while sharing knowledge across modes. The model learns to condition its generation distribution on the prefix, enabling controlled sampling at inference. This approach generally outperforms separate models, suggesting that knowledge sharing across modes is crucial for math tasks.

### Mechanism 3: Gradient Clustering for Automated Mode Discovery
Clustering training examples by gradient similarity recovers implicit modes without manual labels. Computing ∇θ log p(y|x) for each example, applying Rademacher random projection, then clustering, each cluster defines a pseudo-mode for ModC training. This approach achieves 98.7% F1 score in recovering teacher assignments and consistently improves Pass@k on NuminaMath.

## Foundational Learning

- **Pass@k metric**: The core evaluation metric for test-time scaling; understanding its formula is essential to grasp why mode allocation improves it. Quick check: Given a model with p₁=0.3 and p₂=0.5 on two modes, compute Pass@4 for (a) 4 samples from the mixture vs. (b) 2 samples from each mode.
- **Diversity collapse in SFT/RL**: The problem ModC solves; without understanding collapse, the motivation is unclear. Quick check: Why does Pass@1 often improve during SFT while Pass@k degrades?
- **Conditional generation via prefix tokens**: The prefix-based ModC variant relies on this technique; familiarity helps understand implementation tradeoffs. Quick check: How do prefix tokens differ from standard prompting, and what training objective enforces prefix-mode binding?

## Architecture Onboarding

- **Component map**: Training data partitioner -> Prefix tokenizer -> SFT trainer -> Inference sampler -> Gradient clustering module (optional)
- **Critical path**: 1) Identify or discover modes in training data, 2) Annotate each example with mode label, 3) Train with prefix conditioning (prepend mode token) or train separate specialists, 4) At inference, sample k/C per mode and aggregate
- **Design tradeoffs**: Separate models vs. prefixes (guarantee isolation vs. enable knowledge sharing), number of modes (coverage vs. sample reduction), gradient clustering vs. manual labels (scalability vs. potential noise)
- **Failure signatures**: Mode collapse (prefixes fail to induce distinct behaviors), dominant mode bias (model ignores prefix), gradient clustering noise (clusters don't improve Pass@k)
- **First 3 experiments**: 1) Reproduce Countdown DFS/BFS experiment on adversarial test set, 2) Ablate prefix vs. separate models on NumimaMath, 3) Validate gradient clustering on unlabeled data

## Open Questions the Paper Calls Out
- How does ModC scale to a larger number of modes beyond two? The authors suggest extending to richer behavioral dimensions like reasoning depth.
- Can adaptive allocation policies learn to optimally divide compute across modes at test time rather than using fixed equal allocation?
- How can prefix-mode binding be maintained during reinforcement learning? The authors note this requires prefix-following rewards.

## Limitations
- All experiments use exactly two modes; scaling to more modes may introduce capacity limits and coordination challenges.
- Prefix-based advantages over separate models are demonstrated primarily on math tasks; generalization to non-math domains is untested.
- Gradient clustering's effectiveness on truly unlabeled data (without ground truth) is less validated than on teacher-labeled data.

## Confidence
- **High confidence**: Mathematical proof that Pass@k^ModC > Pass@k^std under mode complementarity, and the core claim that diversity collapse hurts test-time scaling.
- **Medium confidence**: Effectiveness of gradient clustering for mode discovery in fully unsupervised settings, and the consistent magnitude of 4x efficiency gains on OpenThoughts.
- **Low confidence**: Claims about prefix advantages over separate models generalizing beyond math tasks, and the robustness of gradient clustering to different projection dimensions.

## Next Checks
1. Apply gradient clustering to a dataset without teacher labels (e.g., OpenThoughts), compare discovered modes against manual inspection, and validate Pass@k improvements against random partitioning baselines.
2. Test prefix-based ModC on non-math reasoning tasks (e.g., coding or commonsense reasoning) to verify if knowledge sharing advantages persist outside NumimaMath.
3. Systematically vary the number of discovered modes via gradient clustering and measure Pass@k degradation as modes become noisier or less distinct, establishing practical limits of the approach.