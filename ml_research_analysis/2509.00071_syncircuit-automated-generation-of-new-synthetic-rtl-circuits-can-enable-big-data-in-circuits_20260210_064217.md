---
ver: rpa2
title: 'SynCircuit: Automated Generation of New Synthetic RTL Circuits Can Enable
  Big Data in Circuits'
arxiv_id: '2509.00071'
source_url: https://arxiv.org/abs/2509.00071
tags:
- graph
- circuit
- design
- circuits
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the scarcity of open-source circuit design
  data by proposing SynCircuit, a framework for automatically generating synthetic
  RTL circuits. The method employs a diffusion-based generative model to create directed
  cyclic graphs representing circuits, followed by probability-guided post-processing
  to ensure circuit validity and Monte Carlo tree search to optimize logic redundancy.
---

# SynCircuit: Automated Generation of New Synthetic RTL Circuits Can Enable Big Data in Circuits

## Quick Facts
- **arXiv ID:** 2509.00071
- **Source URL:** https://arxiv.org/abs/2509.00071
- **Reference count:** 35
- **Primary result:** A diffusion-based generative model can synthesize directed cyclic graphs (DCGs) representing circuits with structural properties more similar to real designs, improving ML model performance in RTL-level PPA prediction tasks.

## Executive Summary
This paper addresses the scarcity of open-source circuit design data by proposing SynCircuit, a framework for automatically generating synthetic RTL circuits. The method employs a diffusion-based generative model to create directed cyclic graphs representing circuits, followed by probability-guided post-processing to ensure circuit validity and Monte Carlo tree search to optimize logic redundancy. Experiments demonstrate that SynCircuit generates circuits with structural properties more similar to real designs, as measured by Wasserstein distance metrics (W1=0.236 vs 0.598-1.31 for baselines). The generated circuits also show improved register preservation ratios (SCPR) through optimization and enhance ML model performance in RTL-level PPA prediction tasks, reducing MAPE by up to 10% compared to models trained only on real designs.

## Method Summary
The framework generates synthetic RTL circuits through three phases: (1) A diffusion-based generative model creates initial probability matrices for circuit connectivity, using an asymmetric decoder to handle directed cyclic graph structure; (2) Probability-guided post-processing converts these probabilities into valid circuit graphs while enforcing design constraints like avoiding combinational loops; (3) Monte Carlo Tree Search optimizes the circuits to reduce logic redundancy by maximizing Sequential Cell Preservation Ratio through graph modifications. The method is trained on 22 open-source RTL designs and tested for structural similarity metrics and downstream PPA prediction performance.

## Key Results
- SynCircuit achieves Wasserstein distance W1=0.236 for degree and orbit distributions, significantly better than baselines (0.598-1.31)
- Generated circuits show improved Sequential Cell Preservation Ratio (SCPR) through MCTS optimization
- ML models trained on SynCircuit data achieve up to 10% lower MAPE in RTL-level PPA prediction compared to models trained only on real designs
- The framework successfully addresses data scarcity in circuit design while maintaining structural validity

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A diffusion-based generative model can synthesize directed cyclic graphs (DCGs) that reflect circuit topology better than autoregressive or undirected methods.
- **Mechanism:** The framework employs a denoising network with a Message Passing Neural Network (MPNN) encoder and a specialized asymmetric decoder. Standard decoders often use symmetric operators (e.g., dot product), which cannot distinguish edge direction ($i \to j$ vs $j \to i$). This architecture uses a learnable relation embedding $r^{(t)}$ added to the source node embedding, effectively translating the source to match the target, thereby modeling directionality.
- **Core assumption:** The structural distribution of valid circuits can be learned by gradually reversing a noise process applied to adjacency matrices.
- **Evidence anchors:**
  - [abstract]: Proposes "a customized diffusion-based generative model for directed cyclic graph (DCG) generation."
  - [section]: Section IV.D details the asymmetric decoder using $MLP((H_i + r) \odot H_j)$ to resolve directionality.
  - [corpus]: Corpus neighbors focus on GNNs for static characteristics (DynamicRTL) or LLM-based generation; direct evidence for diffusion-based DCG generation specifically for RTL is missing in the provided corpus.
- **Break condition:** If the generated graphs consistently fail to capture the cyclic dependencies (feedback loops) required for sequential logic, the asymmetric decoding strategy is insufficient.

### Mechanism 2
- **Claim:** Probability-guided post-processing converts "fuzzy" generative outputs into valid circuit graphs without blindly violating design constraints.
- **Mechanism:** The initial diffusion output is a probability matrix $P_E$. Rather than simple thresholding, this method sequentially assigns parent nodes based on descending probability. Crucially, it validates each assignment against a constraint checker to prevent combinational loops (cycles without registers) and enforce fan-in limits before finalizing the edge.
- **Core assumption:** The diffusion model provides a sufficiently distinct probability signal for valid connections versus invalid ones, allowing a greedy search with safety checks to succeed.
- **Evidence anchors:**
  - [abstract]: Mentions "probability-guided postprocessing to enforce circuit constraints."
  - [section]: Section V describes the sequential parent selection process that checks for loops and fan-in constraints.
  - [corpus]: Neighbors like "Automating Hardware Design" validate the difficulty of ensuring validity, but do not address this specific probability-guided constraint mechanism.
- **Break condition:** If the model assigns high probabilities to structurally invalid connections (e.g., creating unavoidable loops), the post-processor will be forced to discard high-probability edges, resulting in a valid but low-quality or nonsensical graph.

### Mechanism 3
- **Claim:** Monte Carlo Tree Search (MCTS) optimizes synthetic circuits to reduce logic redundancy, making them functionally useful for downstream tasks like PPA prediction.
- **Mechanism:** Raw generated circuits often contain logic that synthesis tools would optimize away (redundancy). This phase uses MCTS to explore graph modifications (swapping parent connections). It uses a reward metric—Post-synthesis Circuit Size (PCS)—to guide the search toward structures that survive logic synthesis, effectively "hardening" the design.
- **Core assumption:** Logic redundancy is a local topological property that can be minimized via discrete graph swaps while preserving overall validity.
- **Evidence anchors:**
  - [abstract]: Highlights "MCTS-based optimization to reduce logic redundancy."
  - [section]: Section VI.B defines the atomic swap operation and the PCS reward used to maximize sequential cell preservation.
  - [corpus]: "AUTOCIRCUIT-RL" supports the use of RL/search for topology, but specific evidence for MCTS-based redundancy reduction in RTL is not present in the corpus.
- **Break condition:** If the PCS reward does not correlate with the actual utility of the circuit for training AI models (e.g., the model generates "hard-to-optimize" junk logic), the optimization serves no downstream purpose.

## Foundational Learning

- **Concept: Directed Cyclic Graphs (DCG) vs. DAGs**
  - **Why needed here:** Standard graph generation often assumes Directed Acyclic Graphs (DAGs) (e.g., in neural architecture search) or undirected graphs. Circuits contain feedback loops (registers), making them DCGs. Understanding this distinction is vital for grasping why standard decoders fail and why the paper proposes an asymmetric decoder.
  - **Quick check question:** Why would a standard "dot product" decoder fail to distinguish a signal flowing from Node A to Node B versus B to A?

- **Concept: Logic Synthesis and Redundancy**
  - **Why needed here:** The paper's "quality" metric relies on how much of the generated circuit remains after synthesis. You must understand that synthesis tools aggressively remove useless logic. If a generated circuit has low "Sequential Cell Preservation Ratio" (SCPR), it is considered low quality because it is mostly redundant.
  - **Quick check question:** If a generated circuit is logically valid but has a very low SCPR (e.g., 10%), why is it bad for training a PPA predictor?

- **Concept: Diffusion Models in Graphs**
  - **Why needed here:** Unlike GANs or VAEs, diffusion models work by adding noise (forward) and learning to denoise (reverse). The paper adapts this to graphs by corrupting the adjacency matrix. Understanding this helps explain the "Phase 1" mechanism of recovering structure from noise.
  - **Quick check question:** In the context of this paper, what represents the "noise" that the model learns to remove?

## Architecture Onboarding

- **Component map:** Input (Node attributes, count) -> Phase 1 (Diffusion: MPNN Encoder + Asymmetric Decoder) -> Probability Matrix -> Phase 2 (Validator: Probability-guided search + Loop/Fan-in Checker) -> Valid Graph -> Phase 3 (Optimizer: MCTS + PCS Reward Model) -> Optimized Graph -> Output (HDL Code)
- **Critical path:** The **MCTS optimization (Phase 3)** is likely the computational bottleneck because it requires simulating the effect of synthesis (or a proxy discriminator) repeatedly to calculate rewards for graph swaps.
- **Design tradeoffs:**
  - **Validity vs. Diversity**: Phase 2 enforces strict validity, which might prune diverse but slightly "messy" structures generated by the diffusion model.
  - **Optimization Speed vs. Quality**: The paper uses a trained discriminator to approximate Post-synthesis Circuit Size (PCS) instead of running slow synthesis tools during MCTS. This speeds up generation but introduces approximation error.
- **Failure signatures:**
  - **"Empty" Circuits**: High structural validity but very low SCPR (redundancy), meaning the circuit disappears upon synthesis.
  - **Mode Collapse**: Generated circuits look structurally identical (low diversity) despite different noise inputs.
- **First 3 experiments:**
  1. **Ablation on Decoder**: Run the model with a symmetric decoder vs. the proposed asymmetric decoder to verify if DCG generation improves (measured by W1 distance).
  2. **Redundancy Visualization**: Generate circuits *with* and *without* Phase 3 (MCTS), synthesize them, and compare the Sequential Cell Preservation Ratio (SCPR).
  3. **Downstream Utility Test**: Train a PPA predictor on (Real Data + Synthetic Data) vs. (Real Data Only) to verify if the synthetic data actually reduces prediction error (MAPE).

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions, but the introduction suggests potential applications beyond circuit design, including neural architecture search and Bayesian optimization.

## Limitations
- The HDL-to-Graph parser implementation details are not fully specified, making faithful reproduction difficult
- The MCTS reward proxy (discriminator) replaces actual synthesis tools, introducing approximation error that may limit optimization quality
- The framework's scalability to larger, more complex industrial designs is not demonstrated
- The absolute PPA prediction error rates (30% vs 40% MAPE) indicate significant room for improvement even with synthetic data augmentation

## Confidence
- **High Confidence:** The core diffusion-based DCG generation framework and the asymmetric decoder mechanism for handling edge directionality are well-specified and theoretically sound.
- **Medium Confidence:** The probability-guided post-processing for enforcing validity is clearly described, but its effectiveness depends on the quality of the probability matrix from the diffusion model, which is not fully characterized.
- **Medium Confidence:** The MCTS-based optimization for reducing logic redundancy is conceptually valid, but the effectiveness of the PCS proxy reward (replacing actual synthesis) is an unverified approximation.
- **Low Confidence:** The absolute performance of the generated circuits for downstream PPA prediction and the generalization to unseen, complex designs are promising but require further validation.

## Next Checks
1. **Decoder Ablation Study:** Implement and compare the proposed asymmetric decoder against a standard symmetric decoder on the same training data to quantitatively measure improvements in W1 distance on degree/orbit distributions for generated DCGs.
2. **Redundancy Before/After MCTS:** Generate a set of circuits without Phase 3 (MCTS) and a set with it. Synthesize both sets and directly compare their Sequential Cell Preservation Ratios (SCPR) to validate the optimization's impact on logic redundancy.
3. **Downstream PPA Prediction with Real Test Set:** Train the PPA predictor model on (Real Data + Synthetic Data) and (Real Data Only) using the held-out 7 real test designs from the paper. Report the absolute MAPE values and the reduction achieved to verify the claimed performance improvement.