---
ver: rpa2
title: Question Generation for Assessing Early Literacy Reading Comprehension
arxiv_id: '2507.22410'
source_url: https://arxiv.org/abs/2507.22410
tags:
- question
- generation
- reading
- comprehension
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces YourBench4Edu, a framework for generating
  comprehension questions tailored to K-2 English learners. The system adapts YourBench
  to early literacy education by processing learning materials, summarizing them,
  and generating diverse question types at varying difficulty levels.
---

# Question Generation for Assessing Early Literacy Reading Comprehension

## Quick Facts
- arXiv ID: 2507.22410
- Source URL: https://arxiv.org/abs/2507.22410
- Reference count: 0
- Key outcome: YourBench4Edu achieves 0.573 MAP@10 (Rouge-L) on FairytaleQA with QwQ-32B, surpassing prior work in lexical matching

## Executive Summary
This paper introduces YourBench4Edu, a framework for generating comprehension questions tailored to K-2 English learners. The system adapts YourBench to early literacy education by processing learning materials, summarizing them, and generating diverse question types at varying difficulty levels. It uses language models to create single-shot or multi-hop questions based on segmented text chunks. Evaluation on the FairytaleQA dataset using Llama-3.3-70B-Instruct, Qwen3-235B-A22B, and QwQ-32B models showed strong performance, with MAP@N scores surpassing prior work in Rouge-L F1 and maintaining competitive results in BERTScore F1. The approach supports autonomous AI-driven English instruction by enabling educators to quickly generate assessment materials and adapt content to learner proficiencies.

## Method Summary
The framework processes raw story materials through a four-stage pipeline: ingestion normalizes inputs to markdown format, summarization creates hierarchical summaries by chunking text and integrating summaries, segmentation generates question-ready chunks by length or sentence similarity, and question generation produces single-shot or multi-hop questions with specified types and difficulty levels. For evaluation on FairytaleQA, the system first identifies the most relevant chunk for each ground-truth Q&A pair, then includes the answer as an additional instruction in the generation prompt to anchor question relevance.

## Key Results
- QwQ-32B achieved 0.573 MAP@10 with Rouge-L F1, surpassing prior work (PFQS: 0.566)
- QwQ-32B achieved 0.9107 MAP@10 with BERTScore F1, competitive with PFQS (0.9198)
- The framework demonstrates strong performance in generating diverse question types at various difficulty levels for K-2 learners

## Why This Works (Mechanism)

### Mechanism 1
Hierarchical text processing (ingestion → summarization → segmentation) enables comprehensive question coverage of source material. Raw materials are normalized to markdown, then chunked and summarized at multiple levels, creating both granular segments for targeted questions and integrated summaries for context. This ensures no story content is missed during question generation. The approach assumes that chunking by length or sentence similarity preserves semantic coherence sufficient for generating meaningful comprehension questions.

### Mechanism 2
Providing ground-truth answers and relevant chunks as constraints improves question-answer alignment. During evaluation, the system first identifies the most relevant chunk for each ground-truth Q&A pair, then includes the answer as additional instruction in the generation prompt. This anchors the LLM to produce questions that target specific comprehension points. The method assumes that constraining generation with answers improves relevance without overly limiting diversity.

### Mechanism 3
Modern instruction-tuned LLMs can modulate question type and difficulty when explicitly prompted for K-2 proficiency levels. Prompts specify question types (true-false, factual, analytical), difficulty level, and target learner proficiency. The LLM generates both single-hop (one chunk) and multi-hop (multiple chunks) questions based on these constraints. This relies on the assumption that instruction-tuned models reliably follow difficulty and type constraints without extensive fine-tuning.

## Foundational Learning

- **Concept: Dialogic Reading**
  - Why needed here: The framework aims to replicate this pedagogical approach where adults ask questions during storybook reading to help children become storytellers.
  - Quick check question: How does asking a child "What do you think happens next?" during reading differ from testing their recall after reading?

- **Concept: MAP@N (Mean Average Precision at N)**
  - Why needed here: This is the primary evaluation metric; understanding it is essential for interpreting the 0.573 MAP@10 result and comparing to prior work.
  - Quick check question: Why might a system achieve high MAP@10 but low MAP@1, and what would this indicate about question ranking?

- **Concept: Single-hop vs Multi-hop Question Generation**
  - Why needed here: The system generates both types; single-hop draws from one chunk while multi-hop requires synthesizing information across multiple chunks.
  - Quick check question: A story has three paragraphs describing a character's morning, afternoon, and evening. What type of question would require the reader to connect all three?

## Architecture Onboarding

- **Component map**: Ingestion → Summarization → Segmentation → Question Generation
- **Critical path**: 1) Input raw story material → 2) Normalize to markdown text → 3) Generate hierarchical summaries → 4) Segment into question-ready chunks → 5) Produce question-answer pairs with specified parameters
- **Design tradeoffs**: The system excels at Rouge-L (0.573 MAP@10 with QwQ-32B) but trails PFQS on BERTScore (0.9107 vs 0.9198), suggesting stronger lexical matching than semantic similarity. Model selection shows QwQ-32B achieved best Rouge-L scores; Llama-3.3-70B-Instruct may offer different tradeoffs for production deployment.
- **Failure signatures**: High BERTScore with low Rouge-L indicates questions semantically appropriate but lexically divergent from expected phrasings. MAP scores dropping sharply from Top-10 to Top-1 suggests good questions generated but poorly ranked. Difficulty mismatch with K-2 level indicates model's difficulty calibration needs prompt engineering adjustment.
- **First 3 experiments**: 1) Replicate Table 1 results with QwQ-32B on FairytaleQA to establish baseline (target: Rouge-L F1 MAP@10 ≥ 0.57). 2) Ablate the summarization step to measure its contribution to question quality. 3) Test difficulty calibration by having educators rate generated questions against stated difficulty levels.

## Open Questions the Paper Calls Out

### Open Question 1
How does the framework perform in a live deployment within an autonomous AI-driven English instructor regarding student engagement and reading comprehension outcomes? The paper claims the approach "has the potential to become an important part of autonomous AI-driven English instructors" and cites dialogic reading benefits, but the current evaluation relies solely on automated similarity metrics rather than measuring actual pedagogical efficacy or student interaction.

### Open Question 2
To what extent do the automated similarity metrics (Rouge-L, BERTScore) correlate with human judgments of pedagogical quality and developmental appropriateness for K-2 learners? The paper claims "state-of-the-art quality" based entirely on n-gram and embedding overlap scores without validating if the generated questions are actually suitable for young children.

### Open Question 3
How effectively does the system adapt question difficulty to learner proficiency when generating questions without the constraint of pre-provided ground-truth answers? While the abstract claims the method offers "adaptation to the learner's specific proficiencies," the validation methodology modifies the pipeline to provide the model with ground-truth answers, reflecting the model's ability to formulate a question given an answer rather than its ability to autonomously select appropriate difficulty levels.

## Limitations

- The framework's reliance on LLM-based text segmentation by length or sentence similarity lacks empirical validation for early literacy materials and may not preserve semantic coherence.
- The effectiveness of difficulty calibration for K-2 learners remains theoretical without direct educator validation of question appropriateness or calibration accuracy.
- The system's generalizability to non-narrative text types or learners with different language backgrounds is unaddressed.

## Confidence

- **High Confidence**: The four-stage pipeline architecture and MAP@N evaluation methodology are clearly specified.
- **Medium Confidence**: The claim that ground-truth answers improve question-answer alignment is supported by methodology but lacks empirical validation comparing answer-constrained vs. unconstrained generation.
- **Low Confidence**: The assertion that the system reliably generates questions at appropriate K-2 difficulty levels without extensive fine-tuning, given that this requires validation against actual learner responses and educator assessment.

## Next Checks

1. Conduct educator review of 50 randomly sampled questions from each difficulty level to assess accuracy of difficulty calibration against K-2 standards.
2. Perform ablation study comparing MAP@N scores with and without the summarization step to quantify its contribution to question quality and coverage.
3. Test the system on a non-narrative text corpus (e.g., science or social studies materials) to evaluate generalization beyond story-based comprehension.