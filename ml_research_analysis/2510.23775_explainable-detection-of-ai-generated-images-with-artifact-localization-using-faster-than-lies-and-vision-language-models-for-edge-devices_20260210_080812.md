---
ver: rpa2
title: Explainable Detection of AI-Generated Images with Artifact Localization Using
  Faster-Than-Lies and Vision-Language Models for Edge Devices
arxiv_id: '2510.23775'
source_url: https://arxiv.org/abs/2510.23775
tags:
- images
- image
- artifacts
- detection
- artifact
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of detecting and explaining
  AI-generated images, particularly in low-resolution (32x32) formats, with adversarial
  robustness for edge device deployment. The proposed solution, "Faster-Than-Lies,"
  combines a lightweight convolutional classifier with a Vision-Language Model (Qwen2-VL-7B)
  to classify images as real or fake, localize artifacts, and generate human-understandable
  explanations.
---

# Explainable Detection of AI-Generated Images with Artifact Localization Using Faster-Than-Lies and Vision-Language Models for Edge Devices

## Quick Facts
- arXiv ID: 2510.23775
- Source URL: https://arxiv.org/abs/2510.23775
- Reference count: 13
- Primary result: 96.5% accuracy on extended CiFAKE with adversarial perturbations; 175ms inference on 8-core CPUs

## Executive Summary
This paper presents a system for detecting and explaining AI-generated images, particularly at low resolution (32x32), with adversarial robustness for edge deployment. The approach combines a lightweight convolutional classifier ("Faster-Than-Lies") with a Vision-Language Model (Qwen2-VL-7B) to classify images, localize artifacts, and generate human-understandable explanations. The system achieves 96.5% accuracy on perturbed datasets while maintaining real-time performance on CPU, demonstrating the feasibility of interpretable authenticity detection at the edge.

## Method Summary
The method uses a binary CNN classifier trained on real and AI-generated images, augmented with synthetic perturbations to improve robustness. When an image is classified as fake, an autoencoder trained on real images generates reconstruction error heatmaps to localize artifact regions. A CLIP encoder categorizes detected artifacts into one of eight semantic groups, and the top three categories are passed to Qwen2-VL-7B with the heatmap to generate textual explanations. The entire pipeline is optimized for edge deployment with a target of 175ms inference time on 8-core CPUs.

## Key Results
- 96.5% accuracy on extended CiFAKE dataset with adversarial perturbations
- 175ms inference time on 8-core CPUs
- Artifact localization via reconstruction error heatmaps
- Explainable text generation for detected artifacts using VLM

## Why This Works (Mechanism)

### Mechanism 1: Reconstruction Error-Based Artifact Localization
Autoencoders trained exclusively on real images produce higher reconstruction error in synthetically generated regions, enabling spatial localization of artifacts. The autoencoder learns compressed representations of natural image statistics, and AI-generated content containing artifacts resists accurate reconstruction because it falls outside the learned distribution. Pixel-wise reconstruction error is visualized as a heatmap. Core assumption: AI-generated artifacts produce spatially correlated reconstruction errors rather than uniform noise.

### Mechanism 2: Perturbation-Augmented Adversarial Robustness
Systematic identification and synthetic replication of perturbation types in training data improves classifier robustness against adversarial detection-evasion attempts. The authors analyzed test images using frequency analysis, Local Binary Patterns, and edge detection to categorize perturbation types, then synthetically injected similar perturbations into training data. Core assumption: deployment perturbations are bounded by training augmentation types; novel perturbation types will degrade performance.

### Mechanism 3: Hierarchical Artifact Categorization via CLIP-Guided VLM Prompting
Pre-filtering images through a CLIP encoder to identify top artifact categories before VLM analysis improves explanation relevance and reduces computational overhead. CLIP ViT-B/32 encodes the image and compares it against eight semantic artifact groups, then passes the top three scoring categories as targeted prompts to Qwen2-VL-7B. Core assumption: CLIP embeddings contain sufficient semantic information to correctly prioritize artifact categories before detailed VLM analysis.

## Foundational Learning

- **Autoencoder Reconstruction for Anomaly Detection**: Understanding how autoencoders learn to compress and reconstruct "normal" data, and why anomalies resist reconstruction. Quick check: Given an autoencoder trained only on real faces, would you expect higher or lower reconstruction error on a face with artificially duplicated eyes? Why?

- **Vision-Language Model Prompting and Multimodal Alignment**: Understanding how VLMs generate coherent explanations from visual input conditioned on text prompts. Quick check: If a VLM receives an image with the prompt "Describe any lighting inconsistencies," what visual features would it likely prioritize compared to the prompt "Describe any texture artifacts"?

- **Adversarial Perturbations and Data Augmentation for Robustness**: Understanding the gap between augmentation and real adversarial attacks. Quick check: Why might training with Gaussian noise augmentation fail to protect against gradient-based adversarial examples crafted specifically for your model?

## Architecture Onboarding

- **Component map**: Input (32x32 image) -> [Optional: Upscaling via SwinIR] -> Faster-Than-Lies CNN (29.8M params) -> Real/Fake classification (175ms CPU) -> (if Fake) Autoencoder (trained on real images) -> Reconstruction error map -> Heatmap overlay -> CLIP ViT-B/32 -> Top-3 artifact category scores (from 8 groups) -> Qwen2-VL-7B <- (receives: image + error heatmap + top-3 categories) -> Textual explanation of detected artifacts

- **Critical path**: Classification (FTL): 175ms — gate for all downstream processing; Localization (Autoencoder): ~1s — bottleneck for CPU deployment; Categorization (CLIP): fast, but accuracy depends on image quality; Explanation (VLM): ~5.2s on GPU; CPU timing not reported — primary latency concern

- **Design tradeoffs**: FTL achieves 96.5% accuracy vs. EfficientNet-B0's 81% (two-class), but with 29.8M vs. ~5M params. Qwen2-VL-7B vs. 4-bit quantized: 4-bit achieves 88% vs. 90% accuracy but "very fast" inference. Upscaling (SwinIR): improves VLM performance on 32x32 images but introduces edge artifacts that confuse localization.

- **Failure signatures**: False negatives: highly perturbed real images may be misclassified; false positives: adversarial perturbations on real images may trigger fake classification; localization drift: heatmaps may highlight perturbation noise rather than generative artifacts; VLM hallucination: explanations are "dependent on VLM" and cannot be automatically validated against ground truth.

- **First 3 experiments**: Ablate the CLIP pre-filter: Pass images directly to Qwen2-VL-7B without category constraints and compare explanation relevance. Stress-test perturbation robustness: Apply PGD or FGSM adversarial attacks specifically targeting the FTL classifier. Profile localization-via-reconstruction on unperturbed images: Measure reconstruction error distributions on held-out real vs. fake images without perturbations.

## Open Questions the Paper Calls Out
- Can the FTL-VLM system maintain comparable accuracy (≥96%) and inference speed (<200ms) when extended to variable-resolution inputs (e.g., 64x64, 128x128, 256x256)?
- Does diffusion-based reconstruction improve artifact localization precision compared to the current autoencoder-based reconstruction error maps?
- How well does the system generalize to AI-generated images from generators not included in training (e.g., DALL-E 3, Midjourney v6, Stable Diffusion 3)?
- What is the ground-truth accuracy of the VLM-generated explanations, and does it correlate with human expert judgment?

## Limitations
- VLM-generated explanations are unverifiable without manual annotation, making explainability a black box dependent on model behavior
- Perturbation augmentation strategy may not generalize to novel adversarial attacks, particularly gradient-based methods
- System accuracy degrades with compression and quality loss, limiting real-world applicability

## Confidence
- **High confidence**: Classification accuracy (96.5%) and inference time (175ms) are well-supported by reported metrics on extended CiFAKE with perturbations
- **Medium confidence**: Artifact localization via reconstruction error is plausible given autoencoder anomaly detection literature, but localization quality on real adversarial examples is untested
- **Low confidence**: VLM-generated explanations are unverifiable without human annotation; their correctness depends entirely on VLM behavior and prompt engineering quality

## Next Checks
1. Ablate the CLIP pre-filter: Pass images directly to Qwen2-VL-7B without category constraints and compare explanation relevance
2. Stress-test perturbation robustness: Apply PGD or FGSM adversarial attacks specifically targeting the FTL classifier
3. Profile localization-via-reconstruction on unperturbed images: Measure reconstruction error distributions on held-out real vs. fake images without perturbations