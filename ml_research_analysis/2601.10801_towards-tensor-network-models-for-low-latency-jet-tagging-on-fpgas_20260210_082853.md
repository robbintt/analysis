---
ver: rpa2
title: Towards Tensor Network Models for Low-Latency Jet Tagging on FPGAs
arxiv_id: '2601.10801'
source_url: https://arxiv.org/abs/2601.10801
tags:
- tensor
- latency
- each
- trigger
- particle
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work explores Tensor Network (TN) models as low-latency alternatives
  to deep neural networks for jet tagging in high-energy physics. Motivated by the
  HL-LHC Level-1 trigger's strict timing and resource constraints, the authors implement
  and deploy Matrix Product States (MPS) and Tree Tensor Networks (TTN) on FPGAs for
  real-time jet substructure classification.
---

# Towards Tensor Network Models for Low-Latency Jet Tagging on FPGAs

## Quick Facts
- arXiv ID: 2601.10801
- Source URL: https://arxiv.org/abs/2601.10801
- Reference count: 0
- This work explores Tensor Network (TN) models as low-latency alternatives to deep neural networks for jet tagging in high-energy physics.

## Executive Summary
This study investigates Tensor Network models, specifically Matrix Product States (MPS) and Tree Tensor Networks (TTN), as low-latency alternatives to deep neural networks for jet tagging in high-energy physics. The research focuses on deploying these models on FPGAs for real-time jet substructure classification under the strict timing constraints of the HL-LHC Level-1 trigger system. Jets are represented using low-level particle features, and tensor network models are trained to classify jets originating from light quarks, gluons, W/Z bosons, or top quarks. The authors systematically evaluate model performance under quantization and compare it to state-of-the-art deep learning classifiers, demonstrating competitive results. Both architectures are synthesized for FPGA deployment, achieving sub-microsecond inference latency and efficient resource usage.

## Method Summary
The study implements MPS and TTN architectures for jet tagging on FPGAs, using JetNet100 dataset with jets represented by low-level particle features. The models are trained to classify jets from light quarks, gluons, W/Z bosons, or top quarks. Both architectures undergo quantization-aware training and are synthesized for deployment on Alveo U280 FPGA. The performance is evaluated against deep learning baselines, with particular attention to latency, resource utilization, and accuracy trade-offs. Post-training quantization is applied to further optimize hardware requirements.

## Key Results
- Competitive accuracy achieved compared to state-of-the-art deep learning classifiers
- Sub-microsecond inference latency demonstrated on FPGA implementation
- Resource-efficient deployment with significant reduction in hardware requirements through quantization
- Both MPS and TTN architectures successfully synthesized for real-time deployment

## Why This Works (Mechanism)
Tensor Network models offer a unique balance between expressiveness and computational efficiency by exploiting the entanglement structure of quantum states. The linear chain topology of MPS naturally captures sequential correlations in jet substructure, while TTN's hierarchical structure enables efficient representation of multi-scale features. The matrix product decomposition reduces parameter count while maintaining sufficient expressivity for jet classification. The inherent interpretability of tensor networks, through visualization of singular values and bond dimensions, provides insights into feature importance and decision-making processes.

## Foundational Learning
1. **Matrix Product States (MPS)**
   - Why needed: Efficient representation of 1D quantum states with limited entanglement
   - Quick check: Bond dimension controls the maximum entanglement captured

2. **Tree Tensor Networks (TTN)**
   - Why needed: Hierarchical structure for multi-scale feature representation
   - Quick check: Tree depth determines the range of correlations captured

3. **Jet Substructure Classification**
   - Why needed: Distinguish between different jet origins (quarks, gluons, bosons)
   - Quick check: Particle-level features capture essential kinematic information

4. **FPGA Deployment Constraints**
   - Why needed: Real-time processing requires strict timing and resource limitations
   - Quick check: Latency < 1 μs and resource utilization under hardware limits

5. **Quantization-aware Training**
   - Why needed: Reduce precision for hardware efficiency without accuracy loss
   - Quick check: Post-training quantization maintains competitive performance

6. **Level-1 Trigger Systems**
   - Why needed: Ultra-fast decision making in high-energy physics experiments
   - Quick check: Processing rate > 40 MHz with minimal dead time

## Architecture Onboarding

**Component Map**: Jet features -> MPS/TTN layers -> Classification output
- MPS: Particle features → Bond matrices → Classification
- TTN: Particle features → Hierarchical tensor products → Classification

**Critical Path**: Input feature extraction → Tensor network inference → Classification decision
- Primary bottleneck: Tensor contraction operations
- Secondary constraint: Data movement between FPGA blocks

**Design Tradeoffs**: Model complexity vs. hardware resource utilization
- Higher bond dimensions improve accuracy but increase resource usage
- Quantization reduces precision but enables faster inference

**Failure Signatures**: Performance degradation under high jet multiplicity
- Accuracy drops when jet features exceed model capacity
- Latency increases with larger bond dimensions

**3 First Experiments**:
1. Compare MPS and TTN performance on JetNet100 benchmark
2. Evaluate quantization impact on accuracy and hardware resources
3. Measure FPGA resource utilization and inference latency

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to single dataset (JetNet100) and specific jet tagging task
- Performance gap with deep learning models suggests potential limitations in complex scenarios
- FPGA implementation specific to Alveo U280 platform, may not generalize
- No systematic hyperparameter optimization for tensor network architectures
- Interpretability advantages mentioned but not quantitatively evaluated
- Assumes perfect preprocessing and feature extraction

## Confidence
- High confidence in reported FPGA implementation details and resource utilization metrics
- Medium confidence in generalization of performance claims across different jet tagging tasks
- Medium confidence in the claimed advantages of tensor networks for interpretability
- Low confidence in potential performance improvements through hyperparameter optimization

## Next Checks
1. Evaluate tensor network models on additional jet tagging datasets and physics tasks to assess generalization
2. Compare performance across different FPGA platforms and resource constraints to verify implementation portability
3. Conduct systematic hyperparameter optimization studies for both MPS and TTN architectures to establish performance bounds