---
ver: rpa2
title: Am I Being Treated Fairly? A Conceptual Framework for Individuals to Ascertain
  Fairness
arxiv_id: '2504.02461'
source_url: https://arxiv.org/abs/2504.02461
tags:
- fairness
- systems
- decisions
- decision
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of individuals lacking a direct
  means to verify if they are being treated fairly by automatic decision-making (ADM)
  systems. The authors propose a conceptual framework for "ascertainable fairness"
  that shifts focus from practitioner tools to empowering end-users to actively validate,
  contest, and ensure fairness in decisions affecting them.
---

# Am I Being Treated Fairly? A Conceptual Framework for Individuals to Ascertain Fairness

## Quick Facts
- **arXiv ID**: 2504.02461
- **Source URL**: https://arxiv.org/abs/2504.02461
- **Reference count**: 37
- **Primary result**: Proposes a conceptual framework enabling end-users to verify, contest, and ensure fairness in ADM decisions affecting them through four components: FOP tool, FOR tool, contestation mechanism, and audit mechanism.

## Executive Summary
This paper addresses the gap between technical fairness tools for practitioners and the ability of individuals to verify if they are being treated fairly by ADM systems. The authors propose a conceptual framework for "ascertainable fairness" that empowers end-users to actively validate fairness concepts based on their personal and group identity, identify potential discrimination sources, and obtain justifications through contestation. The framework integrates elements from algorithmic fairness, explainable AI, contestability, and accountability to bridge the procedural dimension of fairness, allowing individuals to understand, challenge, and verify the fairness of ADM decisions.

## Method Summary
The framework consists of four conceptual components: (1) a tool for checking fairness of predictions (FOP) that allows users to query an ADM system with their defined protected attributes, (2) a tool for checking fairness of recourses (FOR) that evaluates the effort required to change decisions across groups, (3) a contestation mechanism enabling bidirectional dialogue for obtaining justifications, and (4) a call for audit mechanism to trigger third-party conformity assessments. The approach shifts from practitioner-centric bias mitigation to user-centered accountability, operationalizing procedural fairness through active verification and contestation.

## Key Results
- The framework enables individuals to authenticate fairness concepts based on their personal and group identity rather than organizational definitions
- Combines prediction fairness and recourse fairness tools to detect discrimination that either metric alone would miss
- Establishes a procedural fairness mechanism through contestation dialogue that converts explanations into justifications

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Users can verify fairness against their self-defined collective identity rather than organization-imposed fairness definitions.
- Mechanism: The framework allows individuals to specify a subset of protected attributes PA ∈ A that encapsulate their group identity, then applies fairness metrics against this personalized set rather than relying solely on the organization's chosen attributes and metrics.
- Core assumption: Individuals can accurately identify which attributes meaningfully represent their collective identity and potential discrimination sources.
- Evidence anchors:
  - [abstract]: "enabling users to authenticate fairness concepts based on their personal and group identity"
  - [section 4.1]: "The collective identity of a person is unique; the attributes that are considered important to an individual may not be the attributes considered by the organizations"
  - [corpus]: Weak direct evidence; related work on fairness-aware systems (FAReL) addresses trade-offs but not user-defined identity attributes.
- Break condition: If users cannot articulate which attributes define their identity, or if the ADM system does not expose which attributes were used in the decision.

### Mechanism 2
- Claim: Contestation dialogue converts unidirectional explanations into bidirectional justifications that can reveal discriminatory practices.
- Mechanism: Rather than accepting explanations passively, users engage in an argumentation process where the system must provide justifications (not merely explanations). This exchange can expose internal rules, post-processing modifications, or metric choices that introduce unfairness.
- Core assumption: Organizations will implement contestation mechanisms in good faith and provide legitimate justifications rather than stonewalling.
- Evidence anchors:
  - [abstract]: "obtain justifications through contestation"
  - [section 4.2]: "The result of a contestation process to ascertain fairness should be a legitimate justification that convinces the individual of the fairness of the treatment received"
  - [section 2.2.3]: Leofante et al. (2024) proposed computational argumentation for contestation with ground generator methods.
- Break condition: If the organization lacks accountability structures or the user lacks literacy to formulate effective contestation arguments.

### Mechanism 3
- Claim: Combining prediction fairness (FOP) and recourse fairness (FOR) tools detects discrimination that either metric alone would miss.
- Mechanism: FOP checks if predictions are biased against protected groups. FOR checks if the effort required to change a decision is equitable across groups. A system could pass FOP while imposing unfair burdens on certain groups through difficult recourses.
- Core assumption: Both tools have sufficient access to query the ADM system and build parallel models that accurately simulate system behavior.
- Evidence anchors:
  - [section 2.2.1]: "discrimination can go unnoticed: if an end-user faces more difficulty obtaining a different decision due to group affiliation... they experience discrimination, even if the decision appears fair by standard metrics"
  - [section 5.1]: Table 1 classifies tools by FOP, FOR, and fairness assistance categories.
  - [corpus]: "Fair Recourse for All" (arXiv:2601.20449) directly addresses ensuring individual and group fairness in counterfactual explanations, supporting this dual-check approach.
- Break condition: If the ADM system is fully opaque and cannot be queried for counterfactuals, or if recourse cost functions are undefined.

## Foundational Learning

- Concept: **Counterfactual explanations**
  - Why needed here: The framework relies on counterfactuals for both understanding decisions and generating recourses. Without grasping what counterfactuals are (hypothetical "what if" scenarios showing minimal changes to alter outcomes), users cannot interpret FOR tools.
  - Quick check question: Can you explain why "if your income were $5,000 higher, you would be approved" is a counterfactual explanation and not a guarantee?

- Concept: **Procedural vs. substantive fairness**
  - Why needed here: The framework explicitly bridges these dimensions. Substantive = fair outcomes; procedural = fair process including contestation and redress. Engineers must understand this distinction to implement C3/C4 correctly.
  - Quick check question: If an ADM produces statistically equitable outcomes across groups but offers no way to challenge individual decisions, which dimension is violated?

- Concept: **Protected attributes and collective identity**
  - Why needed here: The framework's innovation is allowing users to define their own PA set rather than accepting organizational definitions. This requires understanding legal protected attributes (race, sex, age) and how identity extends beyond them.
  - Quick check question: Why might an organization's fairness metric focused on gender fail a user whose primary identity concern is age-based discrimination?

## Architecture Onboarding

- Component map:
  - **C1 (FOP Tool)**: Queries ADM system → builds parallel simulation model → applies user-specified fairness metrics → outputs prediction fairness assessment
  - **C2 (FOR Tool)**: Takes ADM outputs + explanations → generates counterfactual recourses → evaluates recourse difficulty across groups → outputs recourse fairness assessment
  - **C3 (Contestation Mechanism)**: Accepts user arguments (potentially via ground generator) → routes to ADM organization → facilitates justification exchange → may trigger redress
  - **C4 (Audit Mechanism)**: Logs unresolved contestations → interfaces with regulatory entity → initiates third-party conformity assessment

- Critical path:
  1. ADM system must expose: all attributes used, decision output, explanations/recourses, fairness metrics used
  2. User accesses C1/C2 tools (ideally provided by regulatory entity, not organization)
  3. If results suggest unfairness → initiate C3 contestation dialogue
  4. If contestation fails to resolve → trigger C4 audit request

- Design tradeoffs:
  - **Tool provenance**: C1/C2 could be built by organizations (convenient but conflicts of interest) or regulatory entities (trusted but slower to standardize). Paper suggests regulatory entity ownership.
  - **Explanation vs. justification**: C3 requires justifications (normative reasons tied to rules) not just explanations (mechanistic descriptions). Justifications are harder to automate but more meaningful for fairness.
  - **Metric flexibility vs. standardization**: Allowing user-selected metrics increases agency but creates incommensurability across cases.

- Failure signatures:
  - **Stalemate loop**: Contestation produces no resolution because organization provides only explanations, not justifications
  - **Tool opacity**: C1/C2 tools themselves become black boxes users cannot trust
  - **Regulatory gap**: C4 is invoked but no designated conformity assessment body exists (current state in many jurisdictions)
  - **Identity mismatch**: User's self-defined PA does not intersect with attributes the ADM actually uses

- First 3 experiments:
  1. **Implement C1 prototype** using existing FOP tools (e.g., CERTIFAI or those from Table 1) against a loan approval ADM. Test whether users can successfully query fairness against self-defined protected attributes.
  2. **Design C3 ground generator** using computational argumentation framework. Evaluate whether generated arguments help non-expert users formulate contestations that produce justifications rather than rejections.
  3. **Map regulatory landscape** for C4 in your jurisdiction. Identify which entities could serve as "notifying authorities" and "conformity assessment bodies" per AI Act terminology, even if not yet designated.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can a computational contestation mechanism (Component 3) be effectively designed to support non-expert users in formulating arguments and receiving appropriate justifications?
- **Basis in paper:** [explicit] Section 5.2 states that Component 3 "is still unexplored to support users in formulating contestation requests and receiving appropriate justifications."
- **Why unresolved:** Current argumentation frameworks are theoretical or designed for experts; a method for automating "ground generation" for end-users remains undeveloped.
- **What evidence would resolve it:** A functional prototype of a "ground generator" tool and user studies demonstrating that lay users can successfully navigate the contestation dialogue.

### Open Question 2
- **Question:** Which institutional stakeholder should be responsible for developing and maintaining the fairness verification tools (Components 1 and 2)?
- **Basis in paper:** [explicit] Section 5.1 notes, "it is unclear which stakeholder will be responsible for building components 1 and 2," suggesting regulatory entities or third parties.
- **Why unresolved:** There is a tension between the technical capability of organizations developing ADM and the need for independent, standardized verification tools.
- **What evidence would resolve it:** Policy proposals or frameworks defining the governance model for "regulatory sandboxes" that provide standardized fairness verification tools.

### Open Question 3
- **Question:** How can the ascertainable fairness framework be adapted for hybrid systems where human oversight interacts with ADM (human-in-the-loop)?
- **Basis in paper:** [explicit] Section 6 lists "Human-In-The-Loop Considerations" as a limitation, stating, "Future research should adapt it to hybrid systems."
- **Why unresolved:** The current framework assumes "human-out-of-the-loop" automation; procedural fairness becomes more complex when a human operator modifies or approves the algorithm's output.
- **What evidence would resolve it:** An extended version of the framework that includes mechanisms to contest human overrides or verify the fairness of the human-AI collaboration process.

## Limitations

- The contestation mechanism (C3) remains speculative with undefined ground generator logic for converting user grievances into computational arguments
- The framework assumes organizations will implement contestation in good faith and provide legitimate justifications rather than mere explanations
- Regulatory gaps exist for implementing the audit mechanism (C4), as many jurisdictions lack designated conformity assessment bodies

## Confidence

- **High confidence**: The integration of prediction fairness (FOP) and recourse fairness (FOR) as complementary checks is well-supported by existing literature and addresses a real gap in current fairness assessment approaches
- **Medium confidence**: The mechanism for allowing users to define their own protected attributes (PA) is conceptually clear but faces practical challenges in implementation and user literacy
- **Low confidence**: The contestation dialogue mechanism (C3) and the specific implementation of the ground generator for argumentation are the weakest links, with the paper itself noting these are "still unexplored"

## Next Checks

1. **Implement C1/C2 prototype**: Build working prototypes using existing FOP and FOR tools against a standard ADM system, testing whether users can effectively query fairness against self-defined protected attributes and whether the dual-check approach reveals discrimination missed by single metrics.

2. **Design C3 argumentation framework**: Develop and evaluate a ground generator system that can translate user grievances into computational arguments, testing whether this improves the quality of contestations and increases the likelihood of receiving justifications rather than rejections.

3. **Regulatory landscape mapping**: Conduct a systematic survey of current regulatory frameworks in multiple jurisdictions to identify which entities could serve as notifying authorities and conformity assessment bodies for C4, and assess the practical feasibility of implementing the audit mechanism.