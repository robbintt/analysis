---
ver: rpa2
title: 'UrduLLaMA 1.0: Dataset Curation, Preprocessing, and Evaluation in Low-Resource
  Settings'
arxiv_id: '2502.16961'
source_url: https://arxiv.org/abs/2502.16961
tags:
- language
- urdu
- dataset
- data
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents UrduLLaMA 1.0, an instruction-tuned LLM for
  the Urdu language, developed by continual pretraining Llama-3.1-8B-Instruct on 128M
  Urdu tokens, followed by instruction fine-tuning on 41k Urdu instructions and MT
  fine-tuning on 50k English-Urdu pairs. The model achieves BLEU scores of 28.01 (in-house),
  13.12 (TICO-19), and 15.16 (Tatoeba) in Urdu MT tasks, outperforming the base Llama-3.1-8B-Instruct
  and other SOTA models.
---

# UrduLLaMA 1.0: Dataset Curation, Preprocessing, and Evaluation in Low-Resource Settings

## Quick Facts
- arXiv ID: 2502.16961
- Source URL: https://arxiv.org/abs/2502.16961
- Authors: Layba Fiaz; Munief Hassan Tahir; Sana Shams; Sarmad Hussain
- Reference count: 11
- Primary result: UrduLLaMA 1.0 achieves BLEU scores of 28.01 (in-house), 13.12 (TICO-19), and 15.16 (Tatoeba) in Urdu MT tasks, outperforming the base Llama-3.1-8B-Instruct and other SOTA models

## Executive Summary
This paper presents UrduLLaMA 1.0, an instruction-tuned LLM for the Urdu language, developed through a three-stage pipeline: continual pretraining on 128M Urdu tokens, instruction fine-tuning on 41k Urdu instructions, and translation fine-tuning on 50k English-Urdu pairs. The model achieves strong performance on in-house domain-specific datasets while showing more modest results on established benchmarks. Human evaluation by native Urdu linguists confirms superior translation quality compared to baseline models, establishing a new benchmark for Urdu LLMs despite hardware constraints limiting the pretraining corpus size.

## Method Summary
The authors developed UrduLLaMA 1.0 through a three-stage process: (1) continual pretraining of Llama-3.1-8B-Instruct on 128 million Urdu tokens using full parameter updates, (2) instruction fine-tuning with LoRA (rank=64, alpha=128) on 41,000 Urdu instructions from translated Alpaca/Dolly datasets, and (3) translation fine-tuning on 50,000 English-Urdu parallel pairs from banking, law, and agriculture domains. The approach leverages LoRA to efficiently adapt the model while preserving pretrained knowledge, addressing the challenge of limited high-quality Urdu data in low-resource settings.

## Key Results
- BLEU score of 28.01 on in-house domain-specific dataset, outperforming base Llama-3.1-8B-Instruct and seamless-m4t-v2-large
- BLEU scores of 13.12 (TICO-19) and 15.16 (Tatoeba) on established benchmarks, competitive with other SOTA models
- Human evaluation by native Urdu linguists confirms superior translation quality compared to baseline models
- Strong performance on domain-specific data while maintaining reasonable general translation capabilities

## Why This Works (Mechanism)

### Mechanism 1: Continual Pretraining on Urdu Tokens
UrduLLaMA 1.0 uses full parameter updates on 128M Urdu tokens to build Urdu-specific representations. This causal language modeling forces the model to learn Urdu syntactic structures and token distributions underrepresented in the original multilingual training. The approach assumes the base Llama-3.1-8B-Instruct has sufficient multilingual priors to build upon without catastrophic forgetting.

### Mechanism 2: LoRA-Based Instruction Fine-Tuning
The model employs Low-Rank Adaptation (LoRA) with rank=64 and alpha=128 to efficiently convert the next-token predictor into an instruction-following model. By updating only adapter matrices, embeddings, and LM heads, LoRA creates task-specific pathways while preserving pretrained knowledge. This assumes instruction-following behaviors transfer across languages when instruction datasets are properly localized.

### Mechanism 3: Domain-Specific Translation Fine-Tuning
Fine-tuning on 50,000 parallel sentence pairs from specialized domains (banking, law, agriculture) anchors cross-lingual mappings in domain-specific terminology. This approach assumes translation quality gains transfer when test domains overlap with fine-tuning domains, creating strong associations for domain-specific idiomatic expressions.

## Foundational Learning

- Concept: Causal Language Modeling (CLM)
  - Why needed here: Core pretraining objective that UrduLLaMA uses to learn Urdu token distributions
  - Quick check question: Can you explain why a model trained only on CLM might generate fluent text but fail to follow instructions?

- Concept: LoRA (Low-Rank Adaptation)
  - Why needed here: The paper uses LoRA for efficient fine-tuning with specific rank and alpha parameters
  - Quick check question: If LoRA rank=64 and alpha=128, what is the effective scaling factor applied to LoRA outputs during forward pass?

- Concept: BLEU Score
  - Why needed here: Primary evaluation metric measuring n-gram overlap between model output and reference translations
  - Quick check question: Why might BLEU understate translation quality for Urdu compared to English, given Urdu's agglutinative morphology?

## Architecture Onboarding

- Component map: Llama-3.1-8B-Instruct -> Continual pretraining (128M Urdu tokens) -> LoRA instruction tuning (41K instructions) -> Translation fine-tuning (50K pairs) -> Evaluation (BLEU + human)

- Critical path: Data curation → preprocessing (language filtering, standardization, quality filtering, deduplication) → Continual pretraining (3 epochs, 2 weeks on 3×L40 48GB GPUs) → Instruction tuning (41K samples, A100 40GB) → Translation fine-tuning (50K pairs) → Evaluation (BLEU + human)

- Design tradeoffs:
  - Token budget vs. compute: Used only 128M tokens due to hardware constraints; larger budget likely improves results but requires more compute
  - Full fine-tuning vs. LoRA for pretraining: Full fine-tuning for continual pretraining but LoRA for downstream tasks—trades compute for potential forgetting risk
  - In-domain vs. general translation: Strong in-house performance but weaker on out-of-domain (TICO-19) vs. specialized models (seamless-m4t-v2-large)

- Failure signatures:
  - Low BLEU on out-of-domain test sets: Indicates overfitting to fine-tuning domains
  - Catastrophic forgetting: If English/general capabilities degrade significantly after Urdu pretraining
  - Poor instruction following: If LoRA rank is insufficient or instruction dataset quality is low
  - Hallucination on cultural/literary content: Gaps in Urdu culture/literature due to limited data

- First 3 experiments:
  1. Baseline comparison: Evaluate Llama-3.1-8B-Instruct on Urdu MT datasets to quantify gap before any adaptation
  2. Ablation on pretraining token budget: Train variants with 32M, 64M, 128M Urdu tokens to identify saturation point
  3. LoRA rank sensitivity: Test rank=[8, 16, 32, 64, 128] on instruction-following to find minimum sufficient rank

## Open Questions the Paper Calls Out

### Open Question 1
How does performance scale when increasing continual pretraining corpus from 128 million to the full 1.14 billion curated tokens? The authors state the model's "full potential can only be unlocked with access to a more extensive dataset," but computational constraints prevented utilizing the full curated dataset.

### Open Question 2
How does UrduLLaMA 1.0 perform on Urdu-centric NLP tasks beyond machine translation? The study focused primarily on translation datasets due to lack of standardized benchmarks for general Urdu NLP tasks like summarization, sentiment analysis, and question answering.

### Open Question 3
What is the trade-off between model safety and linguistic capability when applying detoxification processes to Urdu LLMs? The paper lists that "detoxification processes were not incorporated," leaving the model "uncensored and potentially prone to generating harmful or offensive content."

## Limitations
- Hardware constraints limited continual pretraining to only 128 million Urdu tokens out of a curated 1.14 billion tokens
- Strong in-house BLEU performance (28.01) contrasts with more modest results on established benchmarks (13.12 TICO-19, 15.16 Tatoeba), raising concerns about domain overfitting
- Human evaluation lacks statistical rigor with no reported inter-annotator agreement, sample size, or systematic evaluation protocols
- The model remains uncensored without detoxification processes, potentially generating harmful content

## Confidence
- **High Confidence**: The fundamental methodology of continual pretraining followed by instruction and translation fine-tuning is sound and follows established patterns in low-resource LLM adaptation
- **Medium Confidence**: Claims of outperforming SOTA models are supported by reported metrics but limited by evaluation concerns and performance discrepancies across datasets
- **Low Confidence**: Assertions about "superior translation quality" based on human evaluation are poorly supported without methodological details or statistical validation

## Next Checks
1. **Ablation Study on Pretraining Token Budget**: Train three model variants with 32M, 64M, and 128M Urdu tokens respectively, then evaluate all on the same benchmark (TICO-19 or Tatoeba) to determine if 128M tokens represent diminishing returns

2. **Independent Dataset Evaluation**: Request authors to release their in-house evaluation dataset or provide detailed descriptions of its composition, domain distribution, and size to validate the exceptionally high in-house BLEU score (28.01)

3. **Comprehensive Human Evaluation Protocol**: Conduct systematic human evaluation with at least 30 native Urdu speakers, 5-way pairwise comparisons between UrduLLaMA 1.0 and baseline models, inter-annotator agreement metrics (Cohen's kappa), and statistical significance testing on results