---
ver: rpa2
title: Augmented Relevance Datasets with Fine-Tuned Small LLMs
arxiv_id: '2504.09816'
source_url: https://arxiv.org/abs/2504.09816
tags:
- llms
- relevance
- gemma
- dataset
- ranking
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the use of small, fine-tuned large language
  models (LLMs) to automate relevance assessment for query-document pairs, aiming
  to improve ranking model performance by augmenting training datasets. The authors
  fine-tune small LLMs (7-9B parameters) such as Llama 3.1, Gemma 2, and Qwen 2.5
  for a Web search labeling task and apply these models to adjust relevance labels
  in semi-automatically generated ranking datasets.
---

# Augmented Relevance Datasets with Fine-Tuned Small LLMs

## Quick Facts
- arXiv ID: 2504.09816
- Source URL: https://arxiv.org/abs/2504.09816
- Reference count: 34
- Key outcome: Fine-tuned small LLMs (7-9B) can compete with GPT-4 in relevance annotation and improve ranking model performance

## Executive Summary
This paper explores the use of small, fine-tuned large language models (LLMs) to automate relevance assessment for query-document pairs in web search. The authors fine-tune models like Llama 3.1, Gemma 2, and Qwen 2.5 (7-9B parameters) for a web search labeling task and apply them to adjust relevance labels in semi-automatically generated ranking datasets. The approach demonstrates that these fine-tuned small LLMs can achieve competitive performance with much larger models like GPT-4 in relevance annotation tasks, leading to measurable improvements in NDCG and MRR metrics for dense re-rankers trained on the adjusted datasets.

## Method Summary
The authors employ a two-stage process: first, they fine-tune small LLMs (7-9B parameters) on labeled web search data to create specialized relevance annotation models. Then, they use these fine-tuned models to re-label or adjust relevance scores in existing semi-automatically generated ranking datasets. The adjusted datasets are subsequently used to train dense re-rankers. The fine-tuning process leverages a combination of human-labeled data and synthetic data generated by larger language models to create high-quality training sets for the small LLMs.

## Key Results
- Fine-tuned small LLMs (7-9B) achieve competitive performance with GPT-4 in relevance annotation tasks
- The approach consistently improves ranking model quality, with measurable gains in NDCG and MRR metrics
- Fine-tuned small models demonstrate effective automation of relevance assessment, reducing reliance on human annotation

## Why This Works (Mechanism)
The effectiveness stems from the combination of targeted fine-tuning and the inherent capabilities of small LLMs. By focusing the fine-tuning process on domain-specific relevance assessment, the models learn to capture nuanced relationships between queries and documents that generic models might miss. The smaller size of these models allows for efficient fine-tuning with limited computational resources while maintaining sufficient capacity to represent complex relevance patterns.

## Foundational Learning
- Relevance assessment in information retrieval: Understanding how relevance is defined and measured is crucial for evaluating the effectiveness of automated labeling approaches
- Fine-tuning methodologies for LLMs: Knowledge of parameter-efficient fine-tuning techniques and data selection strategies is essential for replicating and extending this work
- Dense re-ranking architectures: Familiarity with dense retrieval and re-ranking techniques helps in understanding how the adjusted datasets impact final ranking performance

## Architecture Onboarding
Component map: Web queries -> Document corpus -> Semi-automatic labeling -> Small LLM fine-tuning -> Dataset adjustment -> Dense re-ranker training -> Improved ranking

Critical path: The most critical components are the fine-tuning process for small LLMs and the quality of the initial semi-automatically generated datasets. The effectiveness of the approach depends on the ability of the fine-tuned models to accurately identify and correct labeling errors or inconsistencies in the original datasets.

Design tradeoffs: The primary tradeoff is between model size and annotation quality. While larger models like GPT-4 might provide more accurate annotations, they are computationally expensive to use at scale. The approach balances this by using smaller, more efficient models that are fine-tuned for the specific task.

Failure signatures: Potential failure modes include overfitting during fine-tuning, where the small LLMs might learn dataset-specific biases rather than general relevance patterns. Another risk is the propagation of errors from the initial semi-automatic labeling into the fine-tuned models, which could amplify rather than correct labeling inconsistencies.

First experiments:
1. Fine-tune a small LLM on a subset of human-labeled web search data and evaluate its performance on a held-out test set
2. Apply the fine-tuned model to adjust relevance labels in a semi-automatically generated dataset and measure the distribution of label changes
3. Train a dense re-ranker on the adjusted dataset and compare its performance against a model trained on the original dataset using standard IR metrics

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions for future research. However, several areas remain unexplored that could be valuable for extending this work, including the generalizability of the approach to different search domains, the impact on result diversity and fairness, and the potential for combining multiple small models to achieve even better performance.

## Limitations
- The approach is primarily validated on web search tasks, with unclear generalization to other domains
- The specific choice of small LLMs (Llama 3.1, Gemma 2, Qwen 2.5) may limit applicability to other model architectures
- The paper doesn't address potential biases introduced by the fine-tuning process or impacts on result diversity
- No investigation of computational efficiency comparisons between fine-tuned small models and larger pre-trained models
- Limited discussion of the scalability of the approach to very large datasets

## Confidence
- High confidence in the core claim that fine-tuned small LLMs can improve ranking model performance
- Medium confidence in the claim that small LLMs can compete with larger models like GPT-4 for relevance annotation tasks
- Low confidence in the generalizability of the approach to domains beyond web search
- Low confidence in the computational efficiency claims without detailed cost analysis

## Next Checks
1. Conduct experiments to test the approach's effectiveness on different types of search tasks (e.g., e-commerce, academic) and compare results across domains.
2. Perform a detailed analysis of the computational costs and efficiency gains (if any) of using fine-tuned small models versus larger pre-trained models in real-world deployment scenarios.
3. Investigate the impact of the augmented datasets on the diversity and fairness of search results by analyzing the distribution of relevant documents across different categories or demographics.
4. Examine the robustness of the approach to adversarial queries and documents designed to exploit potential weaknesses in the fine-tuned models.
5. Study the long-term effectiveness of the approach by evaluating model performance over extended periods with evolving query distributions and document corpora.