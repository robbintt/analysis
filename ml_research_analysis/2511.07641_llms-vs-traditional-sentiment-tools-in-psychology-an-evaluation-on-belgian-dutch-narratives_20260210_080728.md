---
ver: rpa2
title: 'LLMs vs. Traditional Sentiment Tools in Psychology: An Evaluation on Belgian-Dutch
  Narratives'
arxiv_id: '2511.07641'
source_url: https://arxiv.org/abs/2511.07641
tags:
- valence
- pattern
- dutch
- liwc
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated the performance of three Dutch-tuned LLMs
  (ChocoLlama-8B-Instruct, Reynaerde-7B-chat, and GEITje-7B-ultra) against traditional
  lexicon-based tools (LIWC and Pattern) for valence prediction in Flemish narratives.
  Using a dataset of 24,854 spontaneous textual responses from 102 Dutch-speaking
  participants with self-assessed valence ratings, the researchers found that Pattern.nl
  outperformed all other methods, achieving Pearson correlations of 0.31 with human
  ratings.
---

# LLMs vs. Traditional Sentiment Tools in Psychology: An Evaluation on Belgian-Dutch Narratives

## Quick Facts
- **arXiv ID:** 2511.07641
- **Source URL:** https://arxiv.org/abs/2511.07641
- **Reference count:** 40
- **Primary result:** Pattern.nl lexicon tool outperformed Dutch-tuned LLMs for valence prediction in Flemish narratives

## Executive Summary
This study evaluated the performance of three Dutch-tuned LLMs (ChocoLlama-8B-Instruct, Reynaerde-7B-chat, and GEITje-7B-ultra) against traditional lexicon-based tools (LIWC and Pattern) for valence prediction in Flemish narratives. Using a dataset of 24,854 spontaneous textual responses from 102 Dutch-speaking participants with self-assessed valence ratings, the researchers found that Pattern.nl outperformed all other methods, achieving Pearson correlations of 0.31 with human ratings. While the Dutch-tuned LLMs showed moderate performance on processed subsets (ChocoLlama: r=0.35, GEITje: r=0.35), they suffered from limited coverage (69.9% and 38.0% respectively) and underperformed compared to Pattern's near-universal coverage.

## Method Summary
The researchers evaluated three Dutch-tuned LLMs and two traditional lexicon-based tools on their ability to predict valence in spontaneous Dutch-language narratives. The dataset consisted of 24,854 textual responses from 102 Dutch-speaking participants who provided self-assessed valence ratings. All models were tested on their ability to predict these human ratings using Pearson correlation as the primary metric. The evaluation included both full dataset analysis and processed subsets where LLMs had better coverage.

## Key Results
- Pattern.nl achieved the highest correlation with human ratings (r=0.31) among all tested methods
- Dutch-tuned LLMs showed limited coverage issues (38-70%) despite moderate performance on processed subsets
- ChocoLlama-8B-Instruct and GEITje-7B-ultra achieved r=0.35 on processed subsets but underperformed overall

## Why This Works (Mechanism)
The Pattern.nl lexicon tool's superior performance likely stems from its comprehensive coverage of Dutch linguistic patterns and idiomatic expressions specific to Flemish narratives. The lexicon-based approach benefits from carefully curated word-emotion associations that may capture cultural and dialect-specific nuances better than the LLMs, which showed limited coverage despite being Dutch-tuned. The traditional approach appears to handle the specific characteristics of spontaneous, self-reported Flemish text more effectively than the more general-purpose LLMs.

## Foundational Learning
The study highlights important considerations for sentiment analysis in low-resource languages and dialects. It demonstrates that traditional lexicon-based approaches can outperform more sophisticated LLMs when the lexicon is well-tailored to the specific language variety and text type. This suggests that for specialized domains like psychological research using regional language variants, carefully constructed linguistic resources may provide more reliable performance than larger, general-purpose models. The findings challenge the assumption that more complex models always yield better results in sentiment analysis tasks.

## Architecture Onboarding
The evaluation employed a straightforward comparative methodology where all tools were applied to the same dataset of Flemish narratives. The lexicon-based tools (LIWC and Pattern.nl) processed all texts without coverage limitations, while the LLMs required preprocessing to handle their tokenization constraints. The researchers used Pearson correlation between model predictions and human valence ratings as the primary evaluation metric, allowing direct comparison across methods. The inclusion of both full dataset and processed subset analyses provided insights into how coverage limitations affected LLM performance.

## Open Questions the Paper Calls Out
The paper raises several important questions about the generalizability of these findings to other languages, dialects, and psychological text types. It questions whether the observed performance differences would persist in different narrative styles or sentiment dimensions beyond valence. The study also highlights the need to understand why Dutch-tuned LLMs showed such limited coverage for Flemish narratives despite being specifically developed for Dutch language processing. These open questions point to the need for further research on model selection for specialized linguistic tasks.

## Limitations
- The study's findings may not generalize beyond Flemish narratives to other languages or psychological text types
- Moderate correlation coefficients (r=0.31-0.35) indicate substantial unexplained variance in valence prediction
- Limited coverage issues with LLMs raise questions about model appropriateness for the specific dialect and narrative style
- The relatively small sample size of 102 participants may limit statistical power
- Self-reported valence ratings could introduce bias and measurement error
- The study focused only on valence prediction, leaving other psychological dimensions unexplored

## Confidence
- High confidence in relative performance ranking within this specific dataset and task
- Medium confidence in conclusions about LLM superiority for low-resource languages
- Low confidence in direct generalizability to other languages, domains, or sentiment dimensions

## Next Checks
1. Replicate evaluation using a completely held-out test set excluded from model development
2. Test additional sentiment dimensions (arousal, dominance) and other psychological text types
3. Conduct ablation studies on preprocessing strategies and tokenization approaches