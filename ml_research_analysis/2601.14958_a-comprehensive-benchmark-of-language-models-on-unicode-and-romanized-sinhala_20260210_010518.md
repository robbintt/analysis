---
ver: rpa2
title: A Comprehensive Benchmark of Language Models on Unicode and Romanized Sinhala
arxiv_id: '2601.14958'
source_url: https://arxiv.org/abs/2601.14958
tags:
- sinhala
- arxiv
- romanized
- unicode
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper benchmarks modern language models on Sinhala, a low-resource
  morphologically rich language with two scripts: Unicode and Romanized. The authors
  evaluate open-source models using perplexity and closed-source models via qualitative
  sentence completion scoring.'
---

# A Comprehensive Benchmark of Language Models on Unicode and Romanized Sinhala

## Quick Facts
- arXiv ID: 2601.14958
- Source URL: https://arxiv.org/abs/2601.14958
- Authors: Minuri Rajapakse; Ruvan Weerasinghe
- Reference count: 40
- Primary result: Mistral-Nemo-Base-2407 achieved lowest perplexity for Unicode Sinhala (2.19), while Mistral-7B-v0.3 excelled at Romanized Sinhala (74.76)

## Executive Summary
This paper benchmarks modern language models on Sinhala, a low-resource morphologically rich language with two scripts: Unicode and Romanized. The authors evaluate open-source models using perplexity and closed-source models via qualitative sentence completion scoring. The study reveals significant performance variations across models and scripts, with Mistral-Nemo-Base-2407 achieving the lowest perplexity for Unicode Sinhala (2.19), while Mistral-7B-v0.3 performed best for Romanized Sinhala (74.76). Llama-3.1-8B showed strong all-around performance across both scripts. For closed-source models, Gemini-1.5-pro and DeepSeek excelled at Unicode text, whereas Claude-3.5-Sonnet was superior for Romanized text.

## Method Summary
The authors evaluated 15 language models on a curated 200-sentence Sinhala evaluation set, derived from a 1000-sentence parallel corpus using LaBSE embeddings and K-Means clustering. Open-source models were assessed using perplexity, while closed-source models underwent qualitative sentence completion scoring by a native Sinhala speaker on a 3-point scale for coherence and grammar/readability. The evaluation covered both Unicode and Romanized scripts, with separate analysis for each. The dataset is publicly available at https://huggingface.co/datasets/Minuri/sinhala-perplexity-test-dataset.

## Key Results
- Mistral-Nemo-Base-2407 achieved lowest perplexity for Unicode Sinhala (2.19)
- Mistral-7B-v0.3 performed best for Romanized Sinhala (74.76)
- Llama-3.1-8B showed strong all-around performance across both scripts
- Gemini-1.5-pro and DeepSeek excelled at Unicode text for closed-source models
- Claude-3.5-Sonnet was superior for Romanized text in closed-source evaluation

## Why This Works (Mechanism)

### Mechanism 1: Training Data Composition Drives Script-Specific Performance
- Claim: A model's relative strength on Unicode vs. Romanized Sinhala reflects the composition of its pre-training corpus, not just architecture or scale.
- Mechanism: Models exposed to more formal web text (news, Wikipedia) develop stronger Unicode Sinhala representations, while models trained on informal social media data better capture Romanized Sinhala patterns.
- Core assumption: Pre-training corpora differ systematically in their Unicode/Romanized Sinhala proportions across model families.
- Evidence anchors:
  - [abstract] "highlight the critical role of training data in handling script variations"
  - [section V] "The divergence in performance between models implies that their respective pre-training corpora have various concentrations of Unicode versus Romanized Sinhala text"
  - [corpus] "Script Gap" paper (arxiv:2512.10780) confirms similar script-variation effects in Indian clinical LLM deployments
- Break condition: If training data composition were not causal, we would expect consistent Unicode/Romanized performance ratios across model families regardless of their known data sources.

### Mechanism 2: Tokenization Efficiency Determines Morphological Competence
- Claim: For morphologically rich languages like Sinhala, tokenization quality mediates how well parameter scale translates into perplexity improvements.
- Mechanism: Efficient tokenizers produce fewer tokens per Sinhala word, allowing attention mechanisms to capture longer-range morphological dependencies within fixed context windows; poor tokenization fragments morphology, wasting capacity.
- Core assumption: The evaluated models use different tokenizers with varying Sinhala Unicode coverage (not explicitly verified in paper).
- Evidence anchors:
  - [section V] "tokenization efficiency and architectural optimizations play a crucial role in a model's proficiency with a morphologically rich, low-resource language like Sinhala"
  - [section I] "As a morphologically rich and agglutinative language, it features a vast number of potential word forms from a single root"
  - [corpus] No direct corpus evidence on tokenizer-specific mechanisms; this remains inferred
- Break condition: If tokenizer vocabulary sizes for Sinhala were published and showed no correlation with perplexity, the mechanism would weaken.

### Mechanism 3: Parameter Scale Has Non-Monotonic Returns for Low-Resource Languages
- Claim: Larger parameter count does not guarantee better low-resource language performance; data quality and architectural choices can outweigh scale.
- Mechanism: Models optimized for high-resource languages may allocate capacity disproportionately to frequent languages, while smaller models with targeted multilingual training can achieve better low-resource representations per parameter.
- Core assumption: The phi-4 and Llama-3.1-8B training objectives and data mixtures differ in ways relevant to Sinhala.
- Evidence anchors:
  - [section V] "phi-4 model, despite having 14 billion parameters, scored 3.19 [vs] Llama-3.1-8B... 2.37 on Unicode text"
  - [section IV] Table I shows bloom-560m (560M) outperforms Seed-X-PPO-7B (7B) on Unicode (8.88 vs 668.37)
  - [corpus] No corpus papers directly address scale/performance inversions; this pattern appears underexplored
- Break condition: If controlled experiments with identical training data but varying sizes showed monotonic improvement, scale would be causal.

## Foundational Learning

- Concept: **Perplexity as intrinsic language model evaluation**
  - Why needed here: The paper uses perplexity as its primary metric for open-source models; understanding what it measures (negative log-likelihood normalized by sequence length) is essential to interpret the 2.19 vs 74.76 gap between Unicode and Romanized.
  - Quick check question: Can you explain why lower perplexity indicates better language modeling, and why perplexity alone doesn't guarantee generation quality?

- Concept: **Morphologically rich, agglutinative languages**
  - Why needed here: Sinhala's challenge stems from complex word formation; a single root can generate hundreds of surface forms, making vocabulary coverage and subword tokenization critical failure points.
  - Quick check question: How does agglutinative morphology differ from fusional morphology, and why does this matter for subword tokenizers?

- Concept: **Script variation and code-mixing in digital communication**
  - Why needed here: The Unicode/Romanized distinction is not just transliteration—Romanized Sinhala has inconsistent spellings ("me" vs "meh"), informal conventions, and code-mixed usage that models must handle.
  - Quick check question: Why might a model trained primarily on Unicode Sinhala struggle with Romanized Sinhala even if transliteration is "trivial" for humans?

## Architecture Onboarding

- Component map: Tokenizer (vocabulary coverage) -> Transformer architecture (attention patterns) -> Perplexity computation / Qualitative scoring -> Evaluation pipeline
- Critical path: Script detection → Model selection (Mistral-Nemo-Base-2407 for Unicode; Mistral-7B-v0.3 for Romanized; Llama-3.1-8B for dual-script) → Perplexity evaluation on curated benchmark → Qualitative validation for production readiness (subject-verb agreement, sentence structure)
- Design tradeoffs:
  - Single model (Llama-3.1-8B) vs. hybrid pipeline (script-specific models): Single model simpler but suboptimal per-script performance; hybrid adds latency and complexity
  - Perplexity vs. task-specific metrics: Perplexity measures intrinsic capability but doesn't guarantee downstream task performance
  - Manual vs. automated transliteration for data creation: Manual ensures quality but doesn't scale
- Failure signatures:
  - Subject-verb agreement errors (e.g., GPT-4o's "...barida kiyala hitene" failing to match expected verb forms)
  - High perplexity gap between Unicode and Romanized suggests inadequate Romanized training data
  - Inconsistent Romanized spelling handling ("sh" vs "s", "me" vs "meh") without normalization
- First 3 experiments:
  1. **Tokenizer ablation**: Evaluate the same model architecture with different tokenizers (e.g., SentencePiece vs. BPE with Sinhala-extended vocabulary) to isolate tokenization effects on perplexity
  2. **Script-specific fine-tuning**: Fine-tune Llama-3.1-8B on balanced Unicode+Romanized corpus and measure perplexity reduction; compare to single-script fine-tuning
  3. **Cross-script transfer test**: Prompt models to transliterate between scripts and evaluate accuracy; this probes whether low perplexity reflects genuine understanding vs. surface pattern matching

## Open Questions the Paper Calls Out

- Question: How do language models perform on code-mixed Sinhala text containing both Unicode and Romanized scripts within the same utterance?
  - Basis in paper: [explicit] The authors state: "The evaluation is based on monolingual Sinhala in either Unicode or Romanized scripts and does not assess the cross-scripting, which is also common in digital communication."
  - Why unresolved: The benchmark deliberately separated scripts rather than evaluating naturally occurring mixed-script text common in social media.
  - What evidence would resolve it: Perplexity and qualitative scores on a corpus containing intra-sentence code-mixed Sinhala.

- Question: What specific training data compositions (proportions of Unicode vs. Romanized Sinhala, data sources) lead to optimal performance across both scripts?
  - Basis in paper: [inferred] The authors attribute performance divergence to "their respective pre-training corpora have various concentrations of Unicode versus Romanized Sinhala text" but do not empirically verify this claim or quantify optimal ratios.
  - Why unresolved: Training data for closed-source models is unknown; open-source model training compositions were not analyzed or correlated with performance.
  - What evidence would resolve it: Controlled experiments fine-tuning models on varying Unicode/Romanized ratios and measuring resulting perplexity changes.

- Question: How reliable are single-evaluator qualitative assessments for Sinhala text generation, and what is the inter-rater agreement among native speakers?
  - Basis in paper: [explicit] The authors acknowledge: "Inter-rater reliability was not evaluated because only one rater was involved, which is acknowledged as a limitation."
  - Why unresolved: No validation of whether the 3-point scoring schema produces consistent results across multiple native speaker evaluators.
  - What evidence would resolve it: Crowd-sourced evaluation with multiple native speakers calculating inter-rater reliability metrics (e.g., Cohen's kappa).

- Question: Can fine-tuning on a balanced parallel corpus enable a single model to achieve strong performance on both scripts simultaneously?
  - Basis in paper: [inferred] The conclusion suggests "fine-tuning a single model on a balanced corpus covering both Unicode and Romanized Sinhala" as a potential approach, but this remains untested.
  - Why unresolved: No fine-tuning experiments were conducted; only pre-trained model evaluations were performed.
  - What evidence would resolve it: Fine-tuning experiments on Llama-3.1-8B (identified as promising for dual-script use) with balanced parallel data, comparing pre- and post-fine-tuning perplexity on both scripts.

## Limitations
- The dataset contains only 200 sentences, which may not capture full language diversity
- Qualitative evaluation relies on a single native speaker's judgment, introducing potential subjectivity
- Tokenizer-specific details across models were not verified, leaving open whether performance differences stem from architectural advantages or better vocabulary coverage

## Confidence

**High Confidence**: The claim that model performance varies significantly by script (Unicode vs. Romanized) is well-supported by the perplexity and qualitative scoring results. The relative rankings of models (Mistral-Nemo-Base-2407 for Unicode, Mistral-7B-v0.3 for Romanized, Llama-3.1-8B for dual-script) are directly observable from the reported metrics.

**Medium Confidence**: The inference that training data composition drives script-specific performance is reasonable but indirect. While the performance divergence suggests different data concentrations, the paper does not analyze actual training corpora or provide evidence of systematic Unicode/Romanized proportions across model families.

**Low Confidence**: The non-monotonic relationship between parameter scale and performance (e.g., phi-4 outperforming larger models) is intriguing but based on a limited sample of three models. Without controlled experiments varying only scale while holding data constant, this pattern remains suggestive rather than established.

## Next Checks

1. **Tokenizer Vocabulary Analysis**: Extract and compare tokenizer vocabulary files for all evaluated models, specifically counting Sinhala Unicode characters and Romanized variants. Correlate tokenizer coverage with perplexity scores to determine if tokenization efficiency explains performance differences.

2. **Multi-Rater Qualitative Validation**: Re-run the closed-source model evaluation with 3-5 native Sinhala speakers scoring the same 50 sentence completions. Calculate inter-rater reliability (e.g., Fleiss' kappa) and check if relative model rankings remain consistent across raters.

3. **Script-Transfer Experiment**: Fine-tune Llama-3.1-8B on a balanced 50/50 Unicode-Romanized Sinhala corpus (separate from evaluation set) and measure perplexity changes for both scripts. Compare against single-script fine-tuning to isolate the impact of balanced training data on dual-script competence.