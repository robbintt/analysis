---
ver: rpa2
title: 'OmniV-Med: Scaling Medical Vision-Language Model for Universal Visual Understanding'
arxiv_id: '2504.14692'
source_url: https://arxiv.org/abs/2504.14692
tags:
- medical
- arxiv
- images
- video
- zhang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of developing a unified medical
  vision-language model capable of processing diverse multimodal inputs including
  2D/3D images and videos, which existing models typically handle with separate encoders.
  The authors propose OmniV-Med, a unified framework featuring three key technical
  contributions: construction of a comprehensive 252K-sample instruction-following
  dataset (OmniV-Med-Instruct) spanning 14 medical image modalities and 11 clinical
  tasks, a rotary position-adaptive encoder that processes multi-resolution 2D/3D
  images and videos within a single architecture, and a medical-aware token pruning
  mechanism that exploits spatial-temporal redundancy to reduce visual tokens by 60%
  without performance degradation.'
---

# OmniV-Med: Scaling Medical Vision-Language Model for Universal Visual Understanding

## Quick Facts
- arXiv ID: 2504.14692
- Source URL: https://arxiv.org/abs/2504.14692
- Authors: Songtao Jiang, Yuan Wang, Sibo Song, Yan Zhang, Zijie Meng, Bohan Lei, Jian Wu, Jimeng Sun, Zuozhu Liu
- Reference count: 20
- Primary result: Unified medical vision-language model achieving state-of-the-art performance on 2D/3D imaging and video understanding benchmarks

## Executive Summary
This paper addresses the challenge of developing a unified medical vision-language model capable of processing diverse multimodal inputs including 2D/3D images and videos, which existing models typically handle with separate encoders. The authors propose OmniV-Med, a unified framework featuring three key technical contributions: construction of a comprehensive 252K-sample instruction-following dataset spanning 14 medical image modalities and 11 clinical tasks, a rotary position-adaptive encoder that processes multi-resolution 2D/3D images and videos within a single architecture, and a medical-aware token pruning mechanism that exploits spatial-temporal redundancy to reduce visual tokens by 60% without performance degradation. Empirical evaluations demonstrate that OmniV-Med-7B achieves state-of-the-art performance on seven benchmarks across 2D/3D medical imaging and video understanding tasks. The lightweight variant (OmniV-Med-1.5B) attains comparable performance while requiring only 8 RTX3090 GPUs for training and supporting efficient long-video inference.

## Method Summary
OmniV-Med is a unified medical vision-language model built on the Qwen2.5 LLM backbone with a modified SigLIP vision encoder. The model processes 2D images, 3D volumetric data, and medical videos using a single rotary position-adaptive encoder that replaces fixed positional embeddings with rotary position embeddings (RoPE). A medical-aware token pruning mechanism removes 60% of visual tokens by computing L1 distance between consecutive slices/frames and pruning those below a 0.1 threshold. The model is trained through a progressive three-stage strategy: first aligning 2D images with text, then fine-tuning on instruction-following pairs, and finally incorporating 3D and video data. The comprehensive OmniV-Med-Instruct dataset contains 252K samples across 14 modalities and 11 clinical tasks.

## Key Results
- Achieves state-of-the-art performance on seven benchmarks spanning 2D/3D medical imaging and video understanding tasks
- OmniV-Med-7B outperforms specialized models on VQA-RAD, SLAKE, PathVQA, M3D-VQA, Cholec80-VQA, EndoVis18-VQA, and PSI-AVA-VQA
- Medical-aware token pruning reduces visual tokens by 60% without performance degradation
- Lightweight OmniV-Med-1.5B variant achieves comparable performance while requiring only 8 RTX3090 GPUs for training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A single unified visual encoder using rotary position embeddings can process 2D, 3D, and video modalities by extrapolating learned 2D spatial relationships.
- Mechanism: The model replaces fixed positional embeddings in SigLIP with Rotary Position Embeddings (RoPE). The rotary embedding is applied to query and key vectors, allowing the model to learn relative positional relationships between patches in 2D images. These learned relationships are then extrapolated to a third dimension: 3D images are treated as multi-view slices with spatial position encoding, and videos are modeled as multi-frame sequences with temporal position encoding.
- Core assumption: The learned relative positional relationships from 2D data are sufficiently generalizable to be extrapolated to 3D spatial and temporal dimensions without requiring a dedicated 3D or video encoder pre-trained from scratch.
- Evidence anchors:
  - [abstract] "rotary position-adaptive encoder that processes multi-resolution 2D/3D images and videos within a unified architecture."
  - [section 2.2] "This approach enhances the model's ability to capture relative positional relationships, improving its adaptability to diverse medical visual inputs."
  - [corpus] Corpus does not contain direct evidence for this specific RoPE adaptation for medical imaging.
- Break condition: The mechanism fails if the spatial correlations in 2D medical images do not transfer effectively to the temporal dynamics of videos or the volumetric structure of 3D scans, leading to poor performance on non-2D tasks.

### Mechanism 2
- Claim: A medical-aware token pruning mechanism can reduce visual tokens by 60% without performance degradation by exploiting spatial-temporal redundancy.
- Mechanism: The model computes the L1 distance between patches of consecutive video frames or adjacent 3D slices. Tokens with an L1 distance below a predefined threshold (0.1) are identified as redundant and pruned. This encourages the model to focus on significant frame-to-frame and cross-view variations.
- Core assumption: The information content in consecutive slices of 3D scans and adjacent frames of medical videos is highly redundant, and only the most changed regions are diagnostically relevant.
- Evidence anchors:
  - [abstract] "...medical-aware token pruning mechanism that exploits spatial-temporal redundancy... reducing 60% of visual tokens without performance degradation."
  - [section 2.2] "We found that consecutive slices in 3D medical images and adjacent frames in medical videos often contain repetitive or similar information."
  - [corpus] No direct evidence for this specific pruning heuristic was found in the corpus.
- Break condition: This mechanism fails if diagnostically critical information is present in the "unchanged" or low-delta regions between frames or slices, causing it to be erroneously pruned.

### Mechanism 3
- Claim: A progressive training strategy (2D â†’ complex modalities) yields superior performance over joint training by establishing a robust foundational understanding first.
- Mechanism: Training proceeds in three stages. The model is first pretrained on a large corpus of 2D image-caption pairs (Stages 1 & 2) to establish a strong foundational understanding of 2D medical imagery. Only after this foundation is set is the model introduced to the more complex 3D and video data (Stage 3).
- Core assumption: A robust understanding of 2D medical imagery is a prerequisite for effectively generalizing to 3D spatial and temporal video modalities.
- Evidence anchors:
  - [abstract] Notes the model is a "unified framework" built on these contributions.
  - [section 3.3, Progressive Training Yields the Best Performance] "the best performance is achieved when 2D image understanding is robustly established before extending to 3D images and videos."
  - [corpus] No direct evidence was found in the provided corpus excerpts.
- Break condition: The mechanism fails if the features learned from 2D data do not transfer, or if joint training allows for beneficial cross-modal co-adaptation that is lost by the sequential approach.

## Foundational Learning

- Concept: Rotary Position Embeddings (RoPE)
  - Why needed here: The paper's core architectural innovation is the rotary position-adaptive encoder. RoPE allows the model to handle variable sequence lengths (different image resolutions, video lengths) and capture relative positional information, which fixed positional embeddings cannot do effectively.
  - Quick check question: How does RoPE differ from absolute positional embeddings in its handling of variable sequence lengths and relative positions?

- Concept: Vision-Language Model (VLM) Alignment
  - Why needed here: The entire model is a Med-VLM, and the first two stages of the training pipeline are dedicated to vision-language alignment. Understanding how visual features are projected into the language model's embedding space is crucial.
  - Quick check question: In a VLM, what is the role of the projector (e.g., an MLP) that sits between the visual encoder and the Large Language Model?

- Concept: Visual Tokens
  - Why needed here: Processing 3D volumes and long videos is computationally prohibitive without optimization. The paper's pruning mechanism operates directly on visual tokens, so understanding what they represent is essential.
  - Quick check question: In a Vision Transformer (ViT)-based encoder, what do "visual tokens" represent, and why would pruning them be useful?

## Architecture Onboarding

- Component map:
  - LLM Backbone: Qwen2.5 (1.5B or 7B parameters) -> Reasoning engine
  - Vision Encoder: Modified SigLIP with Rotary Position Embeddings (RoPE) -> Processes all 2D, 3D, and video inputs
  - Projector: MLP -> Projects visual tokens into LLM embedding space
  - Pruning Module: L1 distance computation -> Removes redundant tokens below threshold (0.1)

- Critical path:
  1. Input Processing: Raw medical image/video is divided into patches
  2. Encoding: Patches are converted to tokens and passed through the rotary position-adaptive SigLIP encoder
  3. Pruning: (For 3D/video) Visual tokens are pruned based on L1 distance to the previous slice/frame
  4. Projection: The surviving visual tokens are projected by the MLP into the LLM's embedding space
  5. LLM Reasoning: The combined projected visual tokens and text instructions are processed by the Qwen2.5 LLM to generate a response

- Design tradeoffs:
  - Unified vs. Specialized Encoders: The paper chooses a unified encoder for all modalities, trading the potential peak performance of a specialized encoder for greater simplicity, flexibility, and easier cross-modal alignment
  - Efficiency vs. Information Retention: The token pruning mechanism trades a potential loss of information for a 60% reduction in computational cost. The 0.1 L1 distance threshold is a key hyperparameter balancing this tradeoff

- Failure signatures:
  - Loss of Fine-Grained Detail: If the token pruning threshold is too aggressive, the model might miss subtle, critical changes in temporal or volumetric data
  - Poor Resolution Adaptation: If RoPE implementation is flawed, the model may fail to correctly interpret images at resolutions not seen during training
  - Hallucination: As a VLM, the model may generate plausible-sounding but medically incorrect answers not grounded in the image

- First 3 experiments:
  1. Validate 2D Encoder: Train the model only through Stage 1 & 2 on 2D data. Evaluate on standard 2D medical VQA benchmarks (e.g., VQA-RAD) to establish a baseline and ensure the rotary position-adaptive encoder works as intended
  2. Ablate Token Pruning Threshold: Train the full model (Stage 3) with different pruning thresholds (e.g., 0, 0.1, 0.3) on a video VQA task. Plot accuracy vs. computational cost to find the optimal balance point and verify the 60% token reduction claim
  3. Test Cross-Modal Generalization: Using the model trained only to Stage 2 (2D-only), perform a zero-shot evaluation on a video VQA benchmark (e.g., Cholec80-VQA). This validates the paper's claim that 2D positional understanding transfers to the temporal domain

## Open Questions the Paper Calls Out
None

## Limitations
- The rotary position-adaptive encoder's ability to extrapolate 2D positional understanding to 3D and video domains remains theoretical without ablation studies comparing RoPE against fixed positional embeddings or specialized 3D encoders
- The medical-aware token pruning mechanism assumes that low-L1-distance tokens are redundant, but this heuristic may discard diagnostically relevant subtle changes in medical imagery
- The progressive training strategy's superiority over joint training lacks direct comparison on identical datasets, leaving open the possibility that observed gains stem from dataset differences rather than training methodology

## Confidence
- High Confidence: The 2D medical VQA performance improvements over existing models, the dataset construction methodology, and the basic architectural framework are well-supported by empirical results and standard practices in the field
- Medium Confidence: The unified encoder's ability to process all modalities effectively is demonstrated but lacks ablation studies showing why the rotary position-adaptive approach outperforms alternatives. The 60% token reduction claim is reported but requires verification across different 3D volumes and video lengths
- Low Confidence: Claims about the mechanism by which 2D positional understanding transfers to 3D spatial and temporal domains are speculative without quantitative analysis of feature similarity across modalities. The medical-aware pruning threshold of 0.1 lacks sensitivity analysis showing its optimality

## Next Checks
1. **Ablation Study of Positional Embeddings**: Train identical models with fixed positional embeddings, rotary position embeddings, and specialized 3D encoders, then compare performance across 2D, 3D, and video benchmarks to isolate the contribution of each approach
2. **Token Pruning Sensitivity Analysis**: Systematically vary the L1 distance threshold (0.05, 0.1, 0.2, 0.3) and measure both computational savings and performance degradation across different 3D/CT and video datasets to validate the 60% reduction claim
3. **Cross-Modal Feature Analysis**: Extract and compare visual feature representations from 2D, 3D, and video inputs using the unified encoder to quantify the degree of cross-modal similarity and identify whether RoPE successfully bridges the modality gap