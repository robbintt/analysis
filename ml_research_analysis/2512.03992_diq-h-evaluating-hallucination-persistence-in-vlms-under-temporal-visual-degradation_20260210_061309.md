---
ver: rpa2
title: 'DIQ-H: Evaluating Hallucination Persistence in VLMs Under Temporal Visual
  Degradation'
arxiv_id: '2512.03992'
source_url: https://arxiv.org/abs/2512.03992
tags:
- degradation
- visual
- temporal
- hallucination
- benchmark
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DIQ-H, the first benchmark designed to evaluate
  hallucination persistence in Vision-Language Models (VLMs) under temporal visual
  degradation. Existing benchmarks focus on static images and ignore how transient
  visual corruption leads to hallucinations that persist across subsequent frames.
---

# DIQ-H: Evaluating Hallucination Persistence in VLMs Under Temporal Visual Degradation

## Quick Facts
- **arXiv ID**: 2512.03992
- **Source URL**: https://arxiv.org/abs/2512.03992
- **Reference count**: 27
- **Primary result**: First benchmark for hallucination persistence in VLMs under temporal visual degradation, revealing substantial robustness gaps between state-of-the-art models

## Executive Summary
This paper introduces DIQ-H, a benchmark designed to evaluate hallucination persistence in Vision-Language Models (VLMs) when processing temporally degraded visual inputs. The key insight is that transient visual corruption can induce hallucinations that persist across subsequent clean frames, a phenomenon not captured by existing static-image benchmarks. DIQ-H applies physics-based degradations (motion blur, sensor noise, compression artifacts) and measures hallucination persistence, error recovery, and temporal consistency through multi-turn question-answering tasks. To enable scalable annotation, the authors propose Uncertainty-Guided Iterative Refinement (UIR), which generates reliable pseudo-ground-truth using lightweight VLMs with uncertainty filtering, achieving a 15.3% accuracy improvement.

## Method Summary
DIQ-H combines physics-based visual degradation simulation with temporal consistency evaluation. The benchmark applies motion blur (6-DOF PSF convolution), sensor noise (ISO-dependent Poisson-Gaussian), and compression artifacts (H.265 at variable bitrates) to video frames. A task designer generates context-aware questions conditioned on frame content, object trajectories, and memory buffers. The Difficulty Calibrator adaptively adjusts degradation intensity based on real-time performance feedback. UIR generates annotations through K perturbed forward passes with input noise and dropout, filtering outputs by Jensen-Shannon divergence and aggregating with Hodges-Lehmann estimation. This approach enables evaluation of hallucination rates, recovery rates, and temporal consistency across 16 state-of-the-art VLMs.

## Key Results
- Hallucination rate increases from 16.9% to 27.5% under degraded vs. clean conditions
- Even advanced models like GPT-4o achieve only 78.5% recovery rate after degradation
- Open-source models struggle with temporal consistency at less than 60% accuracy
- UIR reduces hallucination error by up to 15.3% compared to direct lightweight VLM annotation
- Demonstrated correlation between model scale and recovery performance

## Why This Works (Mechanism)

### Mechanism 1: Temporal Error Propagation via Latent State Contamination
- Claim: Transient visual degradation induces hallucinations that persist in model responses even after input quality recovers.
- Mechanism: The paper formalizes this as error propagation in multimodal latent space. When frames t≤k undergo degradation D(It*), the model generates responses {At} that encode incorrect beliefs. For frames t>k where It = It* (clean), responses still reflect earlier errors because corrupted latent representations contaminate subsequent reasoning.
- Core assumption: VLMs maintain implicit temporal state across frames that is not reset between inputs.
- Evidence anchors:
  - [abstract] "...transient visual corruption induces hallucinations that persist across subsequent frames"
  - [section III.B] "We formalize this phenomenon as latent error propagation in the model's multimodal latent space"
- Break condition: Models with explicit state-reset mechanisms or frame-independent processing may not exhibit this failure mode.

### Mechanism 2: Uncertainty-Guided Pseudo-Ground-Truth Generation
- Claim: Aggregating multiple perturbed inferences via divergence filtering produces more reliable annotations than single-pass generation.
- Mechanism: UIR performs K perturbed forward passes with input noise (blur, contrast variation) and stochastic dropout. Jensen-Shannon divergence quantifies output variability; low divergence indicates stable beliefs. Hodges-Lehmann estimation robustly aggregates embeddings. Outputs below threshold τ are retained as pseudo-GT.
- Core assumption: High inter-run consistency correlates with factual correctness.
- Evidence anchors:
  - [abstract] "achieving a 15.3 percent accuracy improvement"
  - [section III.C] "UIR reduces hallucination error by up to 15.3% compared to directly using lightweight VLM outputs"
- Break condition: Systematic biases that are consistent across perturbations would evade detection.

### Mechanism 3: Closed-Loop Degradation Calibration
- Claim: Adaptive degradation based on real-time model feedback exposes failure modes more efficiently than fixed perturbation schedules.
- Mechanism: Difficulty Calibrator computes λt = α·EPIt-1 + β·HRt-1, mapping this to degradation parameters (blur kernel size, ISO level, bitrate). Poor performance triggers stronger corruption; stable performance reduces it.
- Core assumption: Stress-testing at failure boundaries reveals weaknesses that uniform testing misses.
- Evidence anchors:
  - [section III.B.3] "This mechanism effectively transforms degradation scheduling into a closed-loop control problem"
- Break condition: Overfitting to specific degradation trajectories if calibration converges prematurely.

## Foundational Learning

- Concept: **Jensen-Shannon Divergence**
  - Why needed here: Core metric for quantifying output uncertainty across perturbed inferences in UIR.
  - Quick check question: Given two probability distributions p and q, what does DJS(p||q) = 0 indicate?

- Concept: **Physics-Based Image Degradation Models**
  - Why needed here: Understanding how motion blur (PSF convolution), sensor noise (Poisson-Gaussian), and compression (HEVC) corrupt visual information.
  - Quick check question: Why model sensor noise as Poisson-Gaussian rather than pure Gaussian?

- Concept: **Temporal Consistency Constraints**
  - Why needed here: Evaluating whether model outputs satisfy logical constraints across time (oj |= ϕj).
  - Quick check question: In a video sequence, what temporal constraint should hold for a static object's color attribute?

## Architecture Onboarding

- Component map: Clean frame → Degradation (λt) → Task generation → VLM inference → Response evaluation → Metric computation → λt+1 update
- Critical path: Clean frame → Degradation (λt) → Task generation → VLM inference → Response evaluation → Metric computation → λt+1 update
- Design tradeoffs:
  - Higher K in UIR improves uncertainty estimation but increases annotation cost
  - Aggressive α, β in calibration exposes failures faster but may prevent recovery assessment
  - Strict τ threshold reduces false positives but may reject valid annotations
- Failure signatures:
  - Hallucination persistence despite clean frames (cognitive inertia)
  - High JS divergence on unambiguous inputs (model instability)
  - Recovery rate <60% indicates poor self-correction (open-source models in Table I)
- First 3 experiments:
  1. Replicate clean vs. degraded comparison on a held-out VLM to validate hallucination rate increase (expected: +5-15 percentage points)
  2. Ablate UIR components: test JS-only vs. JS+HL filtering to measure contribution of each
  3. Plot hallucination rate vs. degradation severity λ to reproduce the performance gap widening shown in Figure 7c

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can parameter-efficient fine-tuning (PEFT) techniques effectively bridge the performance gap in recovery rates and temporal consistency between smaller open-source VLMs and large proprietary models?
- **Basis in paper:** [explicit] The conclusion notes a "demonstrated correlation between model scale and recovery performance" and suggests this points to "promising directions for parameter-efficient adaptation techniques."
- **Why unresolved:** The current study evaluates models across different scales but does not experiment with specific adaptation methods to see if smaller models can acquire the temporal robustness of larger ones without full retraining.
- **What evidence would resolve it:** Experiments applying specific PEFT methods (e.g., LoRA) to open-source models (e.g., LLaVA-7B) on temporal recovery tasks to determine if they can match the >75% recovery rates of models like GPT-4o.

### Open Question 2
- **Question:** Does the reliance on a lightweight model (Qwen-VL-7B) for Uncertainty-Guided Iterative Refinement (UIR) impose a ceiling on ground-truth quality that limits the evaluation of more capable state-of-the-art models?
- **Basis in paper:** [inferred] The methodology proposes UIR to replace expensive GPT-4o annotations. While UIR improves accuracy over direct annotation by 15.3%, it uses a weaker model to generate pseudo-ground-truth, potentially introducing label noise or failing to capture complex reasoning that stronger models might perform correctly.
- **Why unresolved:** If the teacher model (Qwen-VL-7B) cannot solve the hardest temporal reasoning cases, the generated ground truth may incorrectly penalize a smarter student model, a limitation not addressed by the accuracy improvement metric alone.
- **What evidence would resolve it:** A comparison between UIR-generated labels and high-confidence GPT-4o/Human labels on the subset of data where advanced models disagree with the UIR annotation.

### Open Question 3
- **Question:** How does the "cognitive inertia" identified in DIQ-H manifest in dynamic, real-time multimodal interactions compared to the pre-recorded, offline video sequences used in the current benchmark?
- **Basis in paper:** [explicit] The conclusion states: "Future work will extend this evaluation to dynamic multi-modal interactions and real-time learning scenarios."
- **Why unresolved:** The current benchmark processes static video files with simulated degradation. It is unclear if the hallucination persistence mechanisms observed are amplified or mitigated when models must process streams in real-time with active feedback loops.
- **What evidence would resolve it:** Deploying VLMs in an interactive simulation environment where visual degradation occurs in real-time, requiring the model to actively correct its internal state to succeed in a task.

## Limitations
- Temporal state assumptions remain theoretical rather than empirically validated through neural network interpretability
- UIR's uncertainty filtering may not detect systematic biases that produce consistently wrong but low-variance outputs
- Physics-based degradations may not capture all real-world failure modes like multipath interference or variable lighting

## Confidence
- **High confidence**: Empirical findings on hallucination rate increases and recovery rate variations across models are well-supported by experimental results
- **Medium confidence**: UIR methodology and 15.3% accuracy improvement are validated through ablation studies
- **Low confidence**: Theoretical mechanism of latent error propagation through multimodal latent space lacks direct neural network interpretability evidence

## Next Checks
1. **State-reset validation**: Test whether explicitly resetting model state between frames eliminates hallucination persistence, directly validating or falsifying the temporal contamination hypothesis.

2. **Human annotation comparison**: Have human annotators validate a subset of UIR-generated pseudo-ground-truth to measure actual accuracy improvement versus introducing systematic biases.

3. **Cross-domain degradation testing**: Apply DIQ-H methodology to domains beyond the current dataset (e.g., autonomous driving scenarios, medical imaging) to test generalizability of the temporal hallucination phenomenon.