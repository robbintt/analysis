---
ver: rpa2
title: 'Best-of-Q: Improving VLM agents with Q-function Action Ranking at Inference'
arxiv_id: '2601.22701'
source_url: https://arxiv.org/abs/2601.22701
tags:
- action
- q-function
- agent
- data
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Best-of-Q, a method to improve VLM-based web
  agents without retraining the policy. The approach uses a frozen VLM to generate
  multiple candidate actions, then applies a lightweight, offline-trained Q-function
  to rerank and select the best action at inference time.
---

# Best-of-Q: Improving VLM agents with Q-function Action Ranking at Inference

## Quick Facts
- **arXiv ID**: 2601.22701
- **Source URL**: https://arxiv.org/abs/2601.22701
- **Reference count**: 40
- **Primary result**: Q-function reranking improves VLM agents 16.9-6.4% on WebVoyager without retraining the VLM

## Executive Summary
This paper introduces Best-of-Q, a method that significantly improves VLM-based web agents without modifying the underlying VLM policy. The approach generates multiple action candidates from a frozen VLM, then uses a lightweight, offline-trained Q-function to select the highest-value action at inference time. Experiments demonstrate substantial performance gains across multiple VLM models on the WebVoyager benchmark, with a Qwen2.5-VL-7B agent improving from 38.8% to 55.7% success rate and GPT-4.1 improving from 82.4% to 88.8%. The method is model-agnostic and shows better cost-performance trade-offs than alternatives.

## Method Summary
Best-of-Q decouples action proposal from selection by generating N candidate actions from a frozen VLM policy, then using a lightweight Q-function to score and select the best candidate at each step. The Q-function is trained offline using Implicit Q-Learning (IQL) on state-action embeddings extracted from the same VLM. Data collection uses ε-greedy exploration to build a mixed-quality dataset, followed by iterative cycles of training and exploitative data collection. The approach achieves significant performance gains while remaining computationally efficient and avoiding costly VLM fine-tuning.

## Key Results
- Qwen2.5-VL-7B agent success rate improves from 38.8% to 55.7% (+16.9%)
- GPT-4.1 agent success rate improves from 82.4% to 88.8% (+6.4%)
- VLM-based action selectors underperform trained Q-function despite using VLM reasoning
- Primary bottleneck is VLM's action-proposal ability, not Q-function's selection accuracy
- Performance degrades when inference N differs from training N

## Why This Works (Mechanism)

### Mechanism 1: Proposal-Selection Decoupling with Best-of-N Reranking
The frozen VLM generates N diverse candidate actions per state. A lightweight Q-function scores each candidate by expected future return. The agent executes the highest-scoring action. This bypasses the VLM's flawed greedy selection while retaining its perception and proposal capabilities. Core assumption: The VLM's candidate set contains at least one high-value action that its greedy selection would miss. Evidence shows random action baseline drops GPT-4.1 to 72.1%, proving gains come from informed selection.

### Mechanism 2: Implicit Q-Learning for Stable Offline Value Estimation
IQL trains a state-value function V(s) via expectile regression (pushing toward maximum in-sample Q-values), then uses V(s') as a stable target for Q-updates. This avoids querying out-of-distribution actions during training, stabilizing learning in sparse-reward settings. Core assumption: The offline dataset contains sufficient coverage of state-action pairs to enable multi-step value propagation. Evidence anchors show expectile τ lowered to 0.7 for Qwen datasets with higher failure rates to prevent overconfident Q-values.

### Mechanism 3: Iterative Self-Improvement Loop via ε-Greedy Exploration
Alternating between explorative data collection and exploitative Q-guided rollouts creates progressively better training data. Initial ε-greedy policy (ε=0.5) explores diverse actions from VLM proposals, creating a mixed-quality seed dataset. The trained Q-function then guides exploitative rollouts, generating higher-quality trajectories added to training data. This cycle repeats. Core assumption: The Q-function improves sufficiently each iteration to guide meaningfully better exploitation. Evidence shows performance curves with steep initial gains and plateauing, confirming sample efficiency.

## Foundational Learning

- Concept: **Markov Decision Processes (MDPs) with Sparse Rewards**
  - Why needed here: WebVoyager tasks are framed as MDPs where reward=1 only upon task completion, 0 otherwise. Understanding sparse-reward RL is essential to grasp why IQL's multi-step stitching matters.
  - Quick check question: Can you explain why sparse rewards make temporal credit assignment difficult, and how expectile regression helps?

- Concept: **Offline Reinforcement Learning and Distributional Shift**
  - Why needed here: The Q-function is trained on static, pre-collected data. Standard off-policy RL extrapolates erroneously to OOD actions; IQL's in-sample constraint prevents this.
  - Quick check question: What happens if an offline Q-learning algorithm queries actions not present in the training dataset?

- Concept: **Best-of-N Sampling and Reranking**
  - Why needed here: The core inference mechanism generates N candidates and selects via Q-value argmax. Understanding the tradeoff between N and computational cost is critical.
  - Quick check question: If N=8 at inference but the Q-function was trained on N=3 data, why might performance degrade (Section 5, Table 4)?

## Architecture Onboarding

- Component map: VLM Policy → VLM Embedder → Q-Network → Action Execution
- Critical path: 1. Prompt VLM policy with multi-action system prompt 2. Extract embeddings via frozen VLM embedder for state and each candidate action 3. Feed concatenated embeddings to Q-network; select argmax action 4. Execute action via browser automation; observe next state/reward 5. Log transition to dataset for future Q-function retraining cycles
- Design tradeoffs: N=3 vs higher N shows matching inference N to training N is critical. VLM embedder size shows minimal gain from larger embedders; bottleneck is VLM proposal quality. IQL expectile τ: Higher τ (0.8) suits high-success-rate datasets; lower τ (0.7) prevents overconfidence on failure-heavy data.
- Failure signatures: VLM proposal bottleneck: 50.3% of failures occur when correct action never appears in candidate set. Q-function selection error: 36.2% of failures when correct action was proposed but not selected. CAPTCHA/environment blocking: Domain-specific failures where no successful training paths exist.
- First 3 experiments: 1. Baseline comparison: Run prompting, random-action, and Best-of-Q agents on 50 WebVoyager tasks. Confirm >10% absolute improvement for smaller VLMs. 2. N ablation: Train Q-function on N=3 data, test inference with N=3,5,8. Verify performance degradation when N mismatched. 3. Iterative data scaling: Plot success rate vs. number of training trajectories. Confirm early plateau and identify minimum viable dataset size.

## Open Questions the Paper Calls Out

### Open Question 1
How can VLM proposal diversity be systematically improved to mitigate the "hard ceiling" where the correct action is never generated? The paper identifies that 50.3% of failures occur when the correct "golden" action is never proposed, fundamentally constraining what the Q-function can select.

### Open Question 2
How does Best-of-Q compare to VLM fine-tuning methods like AWR or FBC when controlling for the total computational budget? Direct comparison with fine-tuning baselines was not feasible due to computational constraints, leaving the trade-off between training a lightweight Q-function versus fine-tuning the massive policy unresolved.

### Open Question 3
Can curriculum learning strategies close the sample efficiency gap when generalizing Q-functions to new domains? Cross-domain generalization experiments showed that training on out-of-domain data resulted in lower performance than in-domain data, even when successful trace counts were matched.

## Limitations

- The primary bottleneck is VLM's action-proposal ability rather than Q-function's selection accuracy, with 50.3% of failures occurring when the correct action is never proposed
- Performance gains show diminishing returns when scaling up Q-function training data beyond certain thresholds
- Cost-performance tradeoffs analysis is limited to two models, lacking broader VLM size comparisons

## Confidence

- **High Confidence**: Claims about Q-function improving VLM agent performance through inference-time reranking (Section 4.1-4.3 results are statistically significant and consistent across multiple VLM models)
- **Medium Confidence**: Claims about IQL's superiority for offline value estimation and the effectiveness of the iterative data collection loop (fewer ablation studies on alternative offline RL methods)
- **Low Confidence**: Claims about cost-performance tradeoffs compared to larger VLMs (analysis limited to two models; broader VLM size comparisons would strengthen this claim)

## Next Checks

1. **Proposal Coverage Analysis**: Quantify the overlap between VLM-generated candidate sets and optimal actions across task domains to confirm whether improving proposal quality or selection accuracy would yield greater returns

2. **Q-Function Architecture Scaling**: Test whether increasing Q-function capacity beyond 11M parameters provides meaningful gains, particularly for larger VLMs like GPT-4.1

3. **Cross-Domain Generalization**: Evaluate whether Q-functions trained on one web domain transfer to others, or if domain-specific Q-functions are necessary for robust performance