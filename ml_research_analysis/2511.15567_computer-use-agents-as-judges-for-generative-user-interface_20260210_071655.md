---
ver: rpa2
title: Computer-Use Agents as Judges for Generative User Interface
arxiv_id: '2511.15567'
source_url: https://arxiv.org/abs/2511.15567
tags:
- task
- tasks
- feedback
- coder
- success
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces AUI-Gym, a benchmark for evaluating automatic
  GUI development, and proposes a Coder-CUA collaboration framework to optimize interfaces
  for agent navigation. AUI-Gym contains 52 applications across six domains, paired
  with 1560 tasks synthesized and validated via language models and rule-based verifiers.
---

# Computer-Use Agents as Judges for Generative User Interface

## Quick Facts
- arXiv ID: 2511.15567
- Source URL: https://arxiv.org/abs/2511.15567
- Reference count: 40
- Key outcome: Introduces AUI-Gym benchmark and Coder-CUA collaboration framework that improves GUI functional completeness from 67.9% to 81.5% and CUA success rates up to 31.5% through agent-centric feedback-driven UI refinement

## Executive Summary
This paper introduces AUI-Gym, a benchmark for evaluating automatic GUI development, and proposes a Coder-CUA collaboration framework to optimize interfaces for agent navigation. The framework positions a Coder as UI Designer and a Computer-Use Agent as Judge, using navigation success and task solvability as feedback signals. A CUA Dashboard compresses long interaction traces into concise, interpretable images to guide iterative redesign. Experiments show that Coder-CUA collaboration improves both functional completeness (up to 81.5% overall) and CUA success rates (up to 31.5% overall), demonstrating that agent-centric feedback-driven UI refinement can substantially improve agent usability and robustness.

## Method Summary
The method establishes a Markov Design Process where a Coder (LLM) generates HTML interfaces from user queries, which are then evaluated by a CUA (Computer-Use Agent) executing synthesized tasks. The evaluation combines rule-based verifiers for task solvability and visual dashboard compression of CUA navigation traces. Feedback from both sources guides iterative UI refinement. The AUI-Gym benchmark provides 52 applications across six domains with 1,560 tasks validated through language models and rule-based verifiers. The framework supports multiple Coder models (GPT-5, GPT-4o, Qwen3-Coder) and CUAs (UI-TARS, Operator), with the dashboard achieving 76.2% token reduction while preserving essential visual cues.

## Key Results
- Coder-CUA collaboration improves functional completeness from 67.9% to 81.5% overall
- CUA success rates increase up to 31.5% overall with integrated feedback
- Dashboard compression reduces visual tokens by 76.2% while maintaining diagnostic capability
- Agent-centric redesigns (de-stylization, simplified layouts) improve CUA navigation beyond functional fixes alone

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Task solvability feedback provides a sparse but reliable signal for identifying missing UI functionality
- **Mechanism:** The Verifier programmatically checks whether each task is executable within the generated interface. Tasks marked unsolvable are collected into a failure set T_fail and returned to the Coder as precise indicators of missing features. The Coder aggregates these failures into a language-based summary R_task, which guides functional additions without requiring full trajectory analysis
- **Core assumption:** The rule-based verifier can accurately determine task feasibility from HTML structure and element states; language summarization preserves enough signal for targeted code patches
- **Evidence anchors:**
  - [abstract] "GPT-5 also generates a customized rule-based functional checker for individual task, determining whether the task is feasible within the given interface"
  - [section 4.2] "A task is deemed solvable if and only if V(E_t, τ) = 1; otherwise it is labeled a functionality failure"
  - [corpus] Weak direct corpus support; related work (MCPWorld, WebSTAR) focuses on GUI automation benchmarks but not on solvability gating as a feedback mechanism
- **Break condition:** If the verifier generates false negatives (rejecting feasible tasks) or false positives (accepting infeasible ones), the feedback signal degrades

### Mechanism 2
- **Claim:** CUA Dashboard compression preserves task-relevant visual cues while reducing token load for the Coder's revision process
- **Mechanism:** The dashboard crops and tiles only interactive regions touched by the CUA, allocating dynamic region sizes based on step order. This produces a single 1920×1080 image that shows the task goal, actions, intermediate states, and outcome. The dashboard is then passed to a VLM Commenter to generate language feedback R_nav
- **Core assumption:** The cropped regions contain sufficient information to diagnose failure modes; the VLM Commenter can reliably translate visual patterns into actionable language suggestions
- **Evidence anchors:**
  - [abstract] "The CUA Dashboard compresses long navigation trajectories into a single, informative image, reducing visual tokens by 76.2% while preserving essential cues for redesign"
  - [table 2] Shows 76.2% token reduction (from 6×1280×720 to 1×1950×975) with failure diagnosis example
  - [section 5.3] Dashboard ablation shows FC improves from 62.1% to 70.8% and CUA SR from 18.7% to 25.7% compared to text-only baseline for GPT-5
  - [corpus] No direct corpus evidence; compression for agent feedback is a novel contribution
- **Break condition:** If the dashboard cropping excludes critical visual context or if the VLM Commenter misinterprets the visual trace, the feedback quality degrades

### Mechanism 3
- **Claim:** Agent-centric UI redesign (de-stylization, higher contrast, simplified layouts) improves CUA navigation success beyond what functional fixes alone achieve
- **Mechanism:** CUA navigation feedback R_nav identifies interaction failures such as missed affordances, hidden state, or ambiguous labels. The Coder applies design principles prioritizing visibility, predictable behavior, and input permissiveness. This yields interfaces with clear boundaries, visible state indicators, and direct controls
- **Core assumption:** CUAs fail on human-oriented UI patterns due to perception or action noise rather than fundamental task understanding; simplifying the interface reduces these failure modes
- **Evidence anchors:**
  - [section 5.4] "Revisions based on the CUA Test consistently yield more significant visual modifications geared towards agent accessibility"
  - [figure 6] Shows qualitative examples: festival-lights-show adds increase/reduce buttons as complements to sliders; csv-to-charts adds delimiter selector button
  - [corpus] Related work (Just Do It!?) notes CUAs exhibit "blind goal-directedness" regardless of feasibility, which agent-centric design may partially mitigate by making affordances more visible
- **Break condition:** If de-stylization removes cues that humans rely on for task understanding, the interface may become less interpretable for human oversight

## Foundational Learning

- **Concept: Markov Design Process for UI Optimization**
  - **Why needed here:** The paper formalizes UI revision as a Markov Design Process where state = current UI E_t, action = Coder's design update, and reward = CUA feedback metrics (SR, FC). Understanding this framing is necessary to interpret the iterative loop and the role of the discount factor γ
  - **Quick check question:** Can you explain why the paper treats the UI E_t as state rather than the CUA's internal state?

- **Concept: Rule-Based Verifiers for Automated GUI Testing**
  - **Why needed here:** The Verifier generates JavaScript checkers (e.g., `#dailyMealCount >= 5`) that determine task success without human judgment. This enables scalable, human-free evaluation but requires understanding how programmatic checks map to natural language task descriptions
  - **Quick check question:** Given a task "Create a habit named 'Meditate 5 min'", what element selector would a reasonable verifier check?

- **Concept: Visual Token Compression via Adaptive Cropping**
  - **Why needed here:** The dashboard reduces visual tokens by 76.2% by cropping to interactive regions rather than storing full screenshots. This tradeoff balances information preservation against context window limits for the VLM Commenter
  - **Quick check question:** Why does the paper allocate dynamic region sizes based on step order rather than fixed-size crops?

## Architecture Onboarding

- **Component map:** Coder (Designer) -> HTML generation -> Verifier -> Task solvability check -> CUA (Judge) -> Navigation execution -> Dashboard compression -> VLM Commenter -> Language feedback -> Coder (Reviser) -> UI iteration

- **Critical path:**
  1. User query Q → Coder generates initial HTML E_0
  2. Verifier checks each task τ; unsolvable tasks → R_task feedback
  3. For solvable tasks, CUA executes navigation → trajectory H
  4. Dashboard compresses H → VLM Commenter generates R_nav
  5. Coder revises E_t → E_{t+1} based on R_task and R_nav
  6. Repeat until convergence or max revision rounds

- **Design tradeoffs:**
  - **Verifier reliability vs. coverage:** Rule-based checkers are deterministic but may miss edge cases or generate false negatives for unconventional implementations
  - **Dashboard compression vs. information loss:** 76.2% token reduction preserves key cues but excludes non-interactive visual context that may inform failure diagnosis
  - **Lightweight CUA (UI-TARS) vs. strong CUA (Operator):** UI-TARS surfaces more failure cases (driving function revisions) but has lower raw success rates; Operator is stronger but may mask UI issues

- **Failure signatures:**
  - **Verifier false negative:** Valid tasks rejected → Coder adds unnecessary features → regression risk
  - **CUA infinite loop:** Dashboard shows repeated actions on same element → typically indicates hidden state or missing affordance
  - **VLM Commenter hallucination:** Actionable changes reference non-existent selectors → Coder fails to apply patch
  - **Revision regression:** FC improves but SR drops (observed for GPT-5 after multiple rounds) → overfitting to specific failure modes

- **First 3 experiments:**
  1. **Baseline verification:** Run Verifier on 10 held-out apps without CUA feedback; manually inspect false positive/negative rates to establish trust in the solvability signal
  2. **Dashboard ablation:** Compare text-only, screenshot-only, and full dashboard variants on 5 apps; measure both FC and SR to quantify the value of visual+textual compression
  3. **Single-round vs. multi-round revision:** Run the full pipeline with max 1 revision vs. 3 revisions; track whether FC gains saturate and whether SR degrades for strong coders

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can CUA navigation success rates be substantially improved beyond the current ~26% ceiling observed in AUI-Gym?
- Basis in paper: [explicit] The paper states "CUA navigation is the main bottleneck" and reports that even with integrated revision, overall CUA success rate only reaches 26.0% for GPT-5
- Why unresolved: The framework improves functional completeness (67.9% → 81.5%) but CUA navigation gains remain modest, indicating fundamental challenges in multi-step GUI navigation that redesign alone cannot address
- What evidence would resolve it: Experiments with alternative CUA architectures, training paradigms, or intermediate feedback mechanisms that push success rates above 50% while keeping the environment fixed

### Open Question 2
- Question: Can the Coder-CUA collaboration framework generalize to non-web platforms (mobile apps, desktop applications, operating system interfaces)?
- Basis in paper: [inferred] AUI-Gym exclusively evaluates single-page HTML web applications with 52 apps across six domains; no experiments test platform diversity beyond web
- Why unresolved: The benchmark, verifier infrastructure, and CUA Dashboard are all designed for HTML-rendered environments; the approach assumes DOM accessibility and coordinate-based interactions that may not transfer
- What evidence would resolve it: Extending AUI-Gym to native mobile or desktop applications and reporting whether similar functional completeness and navigation improvements hold

### Open Question 3
- Question: What mechanisms can prevent revision improvement saturation observed with stronger coders like GPT-5?
- Basis in paper: [explicit] The paper notes "the CUA success rate of GPT-5 coder may drop after repeated revision" while weaker models (Qwen3-Coder-30B, GPT-4o) show consistent gains across iterations
- Why unresolved: The paper documents this saturation phenomenon but does not investigate whether it stems from overfitting to CUA feedback, conflicting design signals, or inherent limits of iterative code revision
- What evidence would resolve it: Ablation studies varying feedback granularity, introducing diversity in revision objectives, or analyzing which UI properties degrade under repeated revision for strong coders

### Open Question 4
- Question: How do agent-centric redesigns (de-stylization, simplified layouts) affect human usability, and can interfaces satisfy both agents and humans simultaneously?
- Basis in paper: [inferred] The paper explicitly shifts from human-oriented aesthetics to agent-native efficiency but provides no evaluation of how these redesigns impact human task performance or satisfaction
- Why unresolved: Real-world deployment would require interfaces usable by both autonomous agents and human operators; the trade-off between agent efficiency and human experience remains unexplored
- What evidence would resolve it: User studies comparing human task completion times, error rates, and subjective ratings on original versus agent-centric redesigned interfaces

## Limitations

- The framework's success heavily depends on Coder model quality, with significant performance variation between models (Qwen3-Coder: FC 81.5%, GPT-4o: FC 59.7%, GPT-5: FC 55.5%)
- Dashboard compression may exclude contextual visual information that could be relevant for failure diagnosis
- The study focuses on single-page HTML applications, limiting generalizability to complex multi-page or mobile interfaces
- The 52 apps and 1,560 tasks in AUI-Gym may not fully represent real-world GUI development diversity

## Confidence

- **High confidence:** The feedback mechanism (task solvability + navigation compression) demonstrably improves FC and SR metrics across all Coder models tested
- **Medium confidence:** The dashboard compression preserves sufficient information for effective feedback (supported by ablation but lacks direct human evaluation)
- **Medium confidence:** Agent-centric design principles improve CUA success (supported by qualitative examples but not systematically quantified)

## Next Checks

1. **Generalization test:** Apply the Coder-CUA framework to 10+ apps from external GUI benchmarks (e.g., WebVoyager, OSWorld) to assess domain transferability beyond AUI-Gym
2. **Human usability study:** Evaluate whether agent-centric revisions (de-stylization, simplified layouts) maintain or improve human comprehension and task completion rates
3. **Failure analysis:** Conduct a systematic study of FC vs. SR trade-offs across revision rounds to identify optimal stopping criteria and prevent functional regression