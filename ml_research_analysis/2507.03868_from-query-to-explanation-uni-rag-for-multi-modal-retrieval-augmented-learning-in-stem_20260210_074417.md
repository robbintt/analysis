---
ver: rpa2
title: 'From Query to Explanation: Uni-RAG for Multi-Modal Retrieval-Augmented Learning
  in STEM'
arxiv_id: '2507.03868'
source_url: https://arxiv.org/abs/2507.03868
tags:
- retrieval
- prompt
- uni-rag
- learning
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of retrieving and explaining
  educational content in STEM domains using diverse query styles (text, images, sketches,
  audio). The authors propose Uni-RAG, a unified retrieval-augmented generation system
  that combines a multi-style retrieval module (Uni-Retrieval) with a compact instruction-tuned
  language model.
---

# From Query to Explanation: Uni-RAG for Multi-Modal Retrieval-Augmented Learning in STEM

## Quick Facts
- **arXiv ID:** 2507.03868
- **Source URL:** https://arxiv.org/abs/2507.03868
- **Reference count:** 40
- **Key outcome:** Uni-RAG achieves R@1 scores up to 85.1% on multi-modal STEM retrieval while generating pedagogically grounded explanations using under 50M activated parameters.

## Executive Summary
Uni-RAG addresses the challenge of retrieving and explaining educational content in STEM domains using diverse query styles (text, images, sketches, audio). The system combines a multi-style retrieval module (Uni-Retrieval) with a compact instruction-tuned language model. Uni-Retrieval uses prototype learning to extract query-style embeddings and a MoE-LoRA-based Prompt Bank to adaptively retrieve style-relevant prompts, enabling robust retrieval across heterogeneous inputs. The RAG module then generates human-readable explanations grounded in retrieved educational materials, achieving state-of-the-art performance on the SER dataset while maintaining low computational cost.

## Method Summary
Uni-RAG processes multi-modal STEM queries through a unified pipeline: first, query-style prototypes are extracted using modality-specific embedding extractors and mapped to a shared latent space. These prototypes retrieve the top-n most similar prompts from a continually updated Prompt Bank via cosine similarity. The MoE-LoRA mechanism routes each query to relevant expert adapters, generating style-aware representations while activating only a subset of parameters. Retrieved evidence items are then formatted with system prompts and user queries into a structured context string, which is fed to a frozen instruction-tuned LLM (Qwen3-0.6B) to generate explanations. The entire system is trained end-to-end with triplet loss and Prompt Bank regularization, using only the Prompt Bank components as trainable parameters.

## Key Results
- Uni-RAG achieves R@1 scores up to 85.1% across multi-modal retrieval tasks on the SER dataset.
- The system outperforms baseline models in both retrieval accuracy and generation quality while maintaining parameter efficiency (under 50M activated parameters).
- Audio queries remain the weakest modality (R@1 = 53.7%), indicating room for improvement in audio processing.

## Why This Works (Mechanism)

### Mechanism 1: Prototype-Guided Style Unification
The prototype learning module maps diverse input modalities into a shared d-dimensional latent space using modality-specific embedding extractors. These prototypes retrieve the top-n most similar prompt tokens from the Prompt Bank via cosine similarity, enabling cross-modal retrieval without full-parameter fine-tuning.

### Mechanism 2: MoE-LoRA Sparse Expert Routing
The Mixture-of-Experts Low-Rank Adaptation mechanism enables parameter-efficient specialization across stylistic axes while activating only a subset of experts per query. Each prompt entry has K low-rank expert adapters and a routing network that computes softmax weights over experts.

### Mechanism 3: RAG-Evidence Grounding for Generation
Structured concatenation of system prompts, retrieved evidence, and user queries grounds LLM generation in domain-specific STEM knowledge. The generation input is formatted as [PROMPT: P*; EVIDENCE: Ex; QUERY: x], ensuring the compact LLM receives agent-aware guidance and grounding context.

## Foundational Learning

- **Cross-Modal Embedding Alignment:** Understanding how different modalities map to a shared latent space is essential for grasping why prototype learning enables style-diversified retrieval.
  - Quick check: Given a sketch of a chemical flask and a text query "laboratory glassware," would you expect their embeddings to be close or far in the shared space? Why?

- **Low-Rank Adaptation (LoRA):** The MoE-LoRA mechanism assumes familiarity with how low-rank matrix decomposition enables parameter-efficient fine-tuning without modifying frozen backbone weights.
  - Quick check: If a LoRA adapter has rank r=8 and operates on a d=768 embedding, how many trainable parameters does it add per expert?

- **Retrieval-Augmented Generation (RAG):** The paper's core contribution integrates retrieval with generation; understanding the baseline RAG paradigm (retrieve → condition → generate) is prerequisite.
  - Quick check: In standard RAG, what are the two main failure modes that could cause the LLM to generate incorrect information despite retrieving relevant documents?

## Architecture Onboarding

- **Component map:** Input Query → Prototype Learning Module → Prompt Bank + MoE-LoRA → Feature Extractor → Retrieval → Generation Input Formation → LLM Generator
- **Critical path:** Prototype embedding → Prompt retrieval → Feature extraction → Evidence retrieval → Generation. The Prompt Bank is the only trainable module; errors cascade through the pipeline.
- **Design tradeoffs:** 16 Prompt Bank entries optimal; deep prompt insertion (all layers) outperforms shallow by 9.46% but increases computation; 4 tokens per layer balances performance and efficiency.
- **Failure signatures:** Audio queries underperform (R@1 = 53.7%), indicating speech-to-text conversion may not preserve nuance; unseen styles at test time may degrade retrieval if clusters are sparse.
- **First 3 experiments:** 1) Ablation on Prompt Bank size (N=4, 8, 16, 32) with VGG vs. ResNet feature extractors. 2) Cross-dataset zero-shot evaluation on DomainNet/SketchCOCO. 3) Deep vs. shallow prompt insertion comparison to verify the 9.46% performance gain.

## Open Questions the Paper Calls Out

- **Extension to Video Modality:** The authors plan to extend Uni-RAG to video for educational tasks like summarization and captioning, but the current framework lacks temporal modeling mechanisms.
- **Unified Audio Encoder Integration:** The system requires a unified audio encoder to align audio features with visual-text semantic space, as current transcription-based approach struggles with accents and speaking rates.
- **Out-of-Distribution Query Styles:** The MoE-LoRA Prompt Bank's handling of significantly out-of-distribution or hybrid query styles remains untested, as experiments are restricted to predefined categories.

## Limitations

- Audio query performance remains substantially weaker (R@1 = 53.7%) compared to other modalities, suggesting the text-based embedding approach may not preserve semantic richness needed for accurate STEM content retrieval.
- Generation quality claims are primarily validated against retrieval accuracy rather than direct pedagogical effectiveness, with no reported human evaluation of explanation quality or learning outcome metrics.
- The MoE-LoRA mechanism's effectiveness lacks external validation, with no reported expert utilization metrics or routing entropy to assess whether learned prompt routing captures distinct stylistic domains.

## Confidence

- **High Confidence:** Prototype learning mechanism for embedding alignment across modalities is well-supported by ablation studies and clear mathematical formulation.
- **Medium Confidence:** Parameter efficiency claims are verifiable but scalability to larger Prompt Banks or more diverse query styles hasn't been demonstrated.
- **Low Confidence:** Generalization claims to zero-shot domain transfer are based on limited auxiliary datasets without systematic evaluation of transfer failure modes.

## Next Checks

1. **Expert Routing Analysis:** Instrument the MoE-LoRA module to log expert selection frequencies during inference on SER test sets to verify proportional utilization across query styles.
2. **Pedagogical Quality Validation:** Conduct human evaluation of generated explanations using domain experts to rate explanation accuracy, completeness, and pedagogical appropriateness.
3. **Audio Retrieval Investigation:** Compare Whisper transcription quality metrics against audio retrieval performance and test whether directly embedding audio features improves retrieval accuracy for STEM content.