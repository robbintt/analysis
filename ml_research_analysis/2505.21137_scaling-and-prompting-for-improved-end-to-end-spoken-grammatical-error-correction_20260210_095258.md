---
ver: rpa2
title: Scaling and Prompting for Improved End-to-End Spoken Grammatical Error Correction
arxiv_id: '2505.21137'
source_url: https://arxiv.org/abs/2505.21137
tags:
- data
- sgec
- feedback
- whisper
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The authors explore how to improve end-to-end spoken grammatical
  error correction (SGEC) systems by addressing the challenge of limited labelled
  training data. They propose two main approaches: (1) a pseudo-labelling process
  that expands training data from 77 to approximately 2500 hours by leveraging unlabelled
  audio, and (2) prompting Whisper-based SGEC models with fluent transcriptions to
  provide additional contextual guidance.'
---

# Scaling and Prompting for Improved End-to-End Spoken Grammatical Error Correction

## Quick Facts
- arXiv ID: 2505.21137
- Source URL: https://arxiv.org/abs/2505.21137
- Reference count: 0
- The authors improve end-to-end spoken grammatical error correction (SGEC) by combining pseudo-labelling with model prompting, achieving state-of-the-art results.

## Executive Summary
This paper addresses the challenge of limited labeled training data for end-to-end spoken grammatical error correction (SGEC) systems. The authors propose two main approaches: a pseudo-labelling process that expands training data from 77 to approximately 2500 hours, and prompting Whisper-based SGEC models with fluent transcriptions to provide additional contextual guidance. Experimental results show that fine-tuning Whisper gec with pseudo-labelled data yields a 4.0% relative WER improvement over cascaded systems on Linguaskill, while prompting further enhances performance. The work demonstrates that combining pseudo-labelling, model scaling, and prompting effectively advances both SGEC and feedback tasks.

## Method Summary
The authors employ a two-stage approach to improve E2E SGEC. First, they use a pseudo-labelling process where a cascaded system (ASR → Disfluency Detection → GEC) generates "pseudo-GEC" transcriptions for unlabelled audio, expanding the training data from 77 to approximately 2500 hours. Second, they fine-tune Whisper models with a triple input format (Audio, Fluent Transcription) → GEC Transcription, where the fluent transcription acts as a contextual prompt. The models are fine-tuned in stages: first on pseudo-labelled data, then on high-quality labeled data. They evaluate using WER for SGEC quality and F0.5 scores (via MaxMatch + ERRANT) for feedback evaluation.

## Key Results
- Fine-tuning Whisper gec with pseudo-labelled data yields a 4.0% relative WER improvement over cascaded systems on Linguaskill
- Prompting further enhances performance, with Whisper large-v2 achieving 11.04% WER on Linguaskill and 13.08% WER on Speak & Improve
- Feedback generation F0.5 scores approach those of cascaded systems, with prompted models showing particular improvement
- Scaling model size to large-v2 consistently improves performance, though pseudo-labelled data is less effective for larger models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pseudo-labelling with a cascaded system can expand training data for E2E SGEC models, but its effectiveness is conditional on model size.
- Mechanism: A cascaded pipeline (ASR → Disfluency Detection → GEC) generates "pseudo-GEC" transcriptions for unlabelled audio. This data is used to pre-train the E2E model, which is then fine-tuned on high-quality labeled data, helping the model overcome data scarcity.
- Core assumption: The pseudo-labels are of sufficient quality for initial learning, and subsequent fine-tuning can correct residual errors from the pseudo-data.
- Evidence anchors:
  - [abstract] "This work introduces a pseudo-labelling process to address the challenge of limited labelled data, expanding the training data size from 77 hours to approximately 2500 hours, leading to improved performance."
  - [section 4.1] "Further fine-tuning with LNGlbl after training with LNG unl significantly boosts performance, reducing the WER to 12.72% and outperforming the cascaded system by 4.0% relatively."
- Break condition: This mechanism is less effective when pseudo-labels are generated by a significantly smaller model than the target E2E model, as seen with the large-v2 model.

### Mechanism 2
- Claim: Prompting an E2E SGEC model with fluent transcriptions provides beneficial context, enhancing both correction and feedback generation.
- Mechanism: The model is trained with a triple input: (Audio, Fluent Transcription) → GEC Transcription. The fluent transcription acts as a "bridge," helping the model focus on linguistic content and grammatical structure rather than expending capacity on disfluency processing.
- Core assumption: The model can learn to use the provided fluent transcription as a conditional guide, and this extra information is more beneficial than confusing, even if the prompt is imperfect.
- Evidence anchors:
  - [section 2.4] "Specifically, we fine-tune a Whisper model on a dataset that includes both fluent transcriptions and their corresponding speech input... This extra guidance helps the model better understand the structure of the spoken language."
  - [section 4.2] "This model (Whisper gec+text-flt) achieves 13.32% WER... slightly outperforming the non-prompted Whisper gec model."
- Break condition: The benefit of prompting may diminish if the prompt's quality is low or if the model is already so large and capable that it can infer the necessary context without explicit prompting.

### Mechanism 3
- Claim: Scaling model architecture yields consistent performance gains, but the relative benefit of pseudo-labelled data decreases for larger models.
- Mechanism: Larger models (e.g., Whisper large-v2) have greater capacity and more robust pre-trained representations, allowing them to perform the complex SGEC task more effectively. However, their higher baseline capability makes them more sensitive to noise in pseudo-labels generated by smaller models.
- Core assumption: The task complexity of SGEC benefits from increased parameters, and performance is not solely due to better feature extraction but also improved task-specific reasoning.
- Evidence anchors:
  - [abstract] "Scaling model size to large-v2 consistently improves performance, though pseudo-labelled data is less effective for larger models."
  - [section 4.1] "Increasing the model size to large-v2 improves performance on both cascaded and E2E models... However, pseudo-labeled data does not show the same effectiveness with the large-v2 model."
- Break condition: Gains from scaling will plateau when model capacity exceeds task complexity or when training data quality becomes the primary bottleneck.

## Foundational Learning

- Concept: Cascaded vs. End-to-End (E2E) Architectures
  - Why needed here: The paper compares traditional cascaded systems (ASR → DD → GEC) with modern E2E approaches. Understanding their trade-offs (error propagation vs. data hunger) is critical for interpreting the results.
  - Quick check question: What are the two main problems of a cascaded system that an E2E model aims to solve?

- Concept: Pseudo-labelling / Self-training
  - Why needed here: This is the primary data augmentation technique used. Understanding how a strong model labels unlabeled data to train a potentially stronger model is key to Mechanism 1.
  - Quick check question: What is the risk of using pseudo-labels generated by a small model to train a much larger model?

- Concept: Model Prompting / Conditioning
  - Why needed here: The paper introduces a novel prompting method by providing a fluent transcription alongside the audio. Understanding how models can be conditioned on auxiliary information is crucial for Mechanism 2.
  - Quick check question: In this paper's prompting setup, what two inputs does the model receive to generate a GEC transcription?

## Architecture Onboarding

- Component map:
  - Audio from L2 learner -> E2E SGEC Model (Whisper gec) -> Grammatically corrected transcript and feedback edits
  - Prompting Module (Optional): Separate Whisper model (Whisperflt) provides fluent transcription as text prompt during training and inference
  - Training Data Pipeline: Combines small gold-standard labeled data (LNGlbl) with large pseudo-labeled data (LNGunl) generated by cascaded system

- Critical path:
  1. Data Preparation: Run pseudo-labelling pipeline on unlabelled audio to create large training set
  2. Model Fine-Tuning: Initialize Whisper model, fine-tune on pseudo-labelled data, then continue fine-tuning on gold-standard data. If using prompting, format input to include fluent transcription
  3. Inference: Pass audio through final fine-tuned model. If using prompting, generate fluent prompt first and pass it in along with audio

- Design tradeoffs:
  - Performance vs. Simplicity: Best performing model (large-v2 with prompting) is most complex to train and run
  - Data Quality vs. Quantity: Large-v2 model shows high-quality gold data is more valuable than large quantity of pseudo-data
  - Cascaded vs. E2E: Cascaded systems are easier to develop with modular data but suffer from error propagation; E2E models are more elegant but require massive data

- Failure signatures:
  - Pseudo-label Mismatch: If E2E model performance degrades after adding pseudo-labelled data, likely because pseudo-labels are from much weaker model
  - Prompt Misalignment: If prompting yields no gain, check quality of generated prompts; SpecAugment can help align training prompts with inference conditions
  - Feedback Performance Gap: If WER is good but F0.5 score for feedback is poor, model is making corrections but not in way that aligns with standard edit-based metrics

- First 3 experiments:
  1. Baseline E2E: Fine-tune small.en Whisper model on only labeled data to establish baseline WER and F0.5 score
  2. Pseudo-labelling Impact: Continue training baseline model on pseudo-labelled data; repeat with large-v2 to observe interaction between model scale and pseudo-data quality
  3. Prompting Ablation: For best setup from experiment 2, retrain with input formatted to include fluent transcription; compare WER and F0.5 scores

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can pseudo-labelling improve performance for larger Whisper models if the teacher model generating the labels matches the student model's capacity?
- Basis in paper: [inferred] Section 4.1 notes that pseudo-labelled data was ineffective for the large-v2 model, hypothesizing that the small.en teacher model produced lower-quality labels unsuitable for the larger student.
- Why unresolved: The authors used a smaller model to generate labels for the larger one, creating a potential quality ceiling; they did not test if a larger teacher model could generate effective pseudo-labels for the large-v2 student.
- What evidence would resolve it: An experiment where pseudo-labels are generated using a model of equivalent size (e.g., a large cascaded system) and used to train the large-v2 E2E model.

### Open Question 2
- Question: Why does integrating GPT-4o corrections into the fluent transcription pipeline fail to yield performance gains for SGEC?
- Basis in paper: [explicit] Section 4.2 states, "We also explored using GPT-4o to correct grammar errors... but this did not improve GEC or feedback performance."
- Why unresolved: The paper reports the negative result but does not provide an analysis of whether the failure was due to stylistic mismatches, hallucination, or error types not covered by the GEC evaluation metrics.
- What evidence would resolve it: A qualitative error analysis comparing GPT-4o corrections against ground truth GEC labels to identify systematic discrepancies or style issues.

### Open Question 3
- Question: How can end-to-end models be optimized to generate accurate feedback for lower-proficiency (A2/B1) speakers?
- Basis in paper: [inferred] Section 4.3 and Figure 3 indicate that while the E2E model matches cascaded performance at level C, it still lags behind at lower proficiency levels (B1/B2).
- Why unresolved: The proposed prompting methods improved average scores but did not specifically address the performance gap for lower-level learners where errors are likely more frequent or complex.
- What evidence would resolve it: Targeted experiments using data balancing or proficiency-specific prompts, with results reported specifically for A2 and B1 proficiency subsets.

## Limitations
- Dataset Dependency: Results rely heavily on proprietary Linguaskill and S&I datasets, limiting independent verification
- Pseudo-labelling Quality: Degradation for large-v2 models raises questions about scalability across model sizes
- Feedback Metrics: F0.5 scores for feedback generation still lag behind cascaded systems without deep analysis of linguistic meaningfulness

## Confidence
- High: The core finding that pseudo-labelling expands training data and improves SGEC for small models is well-supported by ablation results and clear performance gains
- Medium: The benefit of prompting for feedback generation is demonstrated, but analysis of why prompting specifically helps feedback is less detailed
- Medium: The claim that scaling model size consistently improves performance is supported, but interaction with pseudo-labelled data quality for large models introduces uncertainty

## Next Checks
1. Dataset Replication: Attempt to reproduce results on publicly available SGEC dataset (e.g., BEA or CoNLL-2014) to test generalization beyond Linguaskill
2. Pseudo-labelling Quality Analysis: Analyze quality of pseudo-labels (e.g., via human evaluation or comparison to gold edits) to understand why they degrade large-v2 performance
3. Feedback Output Analysis: Conduct qualitative analysis of feedback edits generated by prompted vs. non-prompted models to assess linguistic correctness and pedagogical value beyond F0.5 scores