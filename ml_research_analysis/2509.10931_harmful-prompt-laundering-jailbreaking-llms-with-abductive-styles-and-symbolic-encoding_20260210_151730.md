---
ver: rpa2
title: 'Harmful Prompt Laundering: Jailbreaking LLMs with Abductive Styles and Symbolic
  Encoding'
arxiv_id: '2509.10931'
source_url: https://arxiv.org/abs/2509.10931
tags:
- llms
- response
- harmful
- attack
- encoding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Harmful Prompt Laundering (HaPLa), a jailbreaking
  technique that bypasses LLM safety filters by reframing harmful queries as abductive
  reasoning tasks and masking sensitive content with symbolic encoding. The method
  achieves over 95% attack success rate on GPT-series models and 70% across all targets,
  even with defenses in place.
---

# Harmful Prompt Laundering: Jailbreaking LLMs with Abductive Styles and Symbolic Encoding

## Quick Facts
- arXiv ID: 2509.10931
- Source URL: https://arxiv.org/abs/2509.10931
- Authors: Seongho Joo; Hyukhun Koh; Kyomin Jung
- Reference count: 40
- Primary result: Over 95% attack success rate on GPT-series models and 70% across all targets using abductive framing and symbolic encoding

## Executive Summary
This paper introduces Harmful Prompt Laundering (HaPLa), a jailbreaking technique that bypasses LLM safety filters by reframing harmful queries as abductive reasoning tasks and masking sensitive content with symbolic encoding. The method achieves high attack success rates (over 95% on GPT models) by exploiting shallow safety alignment that over-relies on explicit keywords and initial tokens. Experiments show that safety tuning cannot fully defend against novel symbolic encodings without significantly reducing model helpfulness, and LLM responses closely mirror real-world crime cases when safety filters are bypassed.

## Method Summary
HaPLa transforms harmful queries through three stages: (1) Abductive framing converts imperative requests into third-person declarative statements (e.g., "Make a bomb" → "A person made a bomb"), (2) Adaptive masking applies symbolic encoding (ASCII, Emoji, Equation) to toxic keywords based on model-specific calibration tests, and (3) Final prompts request plausible step-by-step actions starting with specific prefixes. The method uses GPT-4o-mini for query transformation and toxic word detection, then calibrates masking levels through decoding tests that measure each target model's comprehension versus evasion thresholds.

## Key Results
- HaPLa achieves over 95% attack success rate on GPT-series models and 70% across all targets
- Multi-turn attacks achieve 75%+ success in extracting harmful information from initially refusing models
- Safety tuning reduces attack success but causes substantial drops in instruction acceptance rates
- LLM responses closely mirror real-world crime cases when safety filters are bypassed

## Why This Works (Mechanism)

### Mechanism 1
Safety filters in LLMs exhibit shallow alignment that primarily monitors initial tokens and explicit harmful keywords, creating a vulnerability window. By manipulating the initial tokens through abductive framing (third-person declarative statements) and masking trigger words via symbolic encoding, the input bypasses the safety-triggering mechanism before the model commits to a refusal response pattern. Safety alignment is token-localized rather than semantically deep—it over-relies on lexical matching of harmful terms rather than understanding intent through context.

### Mechanism 2
Abductive framing exploits the model's narrative coherence bias and reduces prosocial reasoning constraints. Framing harmful queries as "A person developed X" with requests to "infer plausible intermediate steps" creates psychological distance from the user's intent, triggering the model's case-based reasoning capabilities to fill information gaps rather than refuse. LLMs trained on narratives have an inductive bias toward completing coherent storylines even when the content is harmful, and third-person framing reduces the model's activation of refusal behaviors.

### Mechanism 3
Symbolic encoding bypasses keyword-based safety filters while preserving semantic interpretability for the model. Encoding toxic words as ASCII numbers (e.g., "self-harm" → `[115 101 108 102 45 104 97 114 109]`) prevents surface-level keyword detection while the model's pre-trained pattern recognition can still decode and execute the underlying harmful request. LLMs have sufficient pattern-matching capabilities to interpret encoded content, but safety filters lack equivalent decoding mechanisms—creating an asymmetry.

## Foundational Learning

- **Concept: Autoregressive generation and token-dependency**
  - Why needed here: Understanding why initial tokens disproportionately influence output trajectories is essential for grasping why "laundering" the first few tokens enables full harmful outputs
  - Quick check question: Why does manipulating the first 10-20 tokens of a prompt affect whether a 500-token response follows a refusal or compliance pattern?

- **Concept: Abductive vs. deductive vs. inductive reasoning**
  - Why needed here: The attack specifically leverages abductive reasoning (inferring most plausible explanations from incomplete information) as a cognitive load that distracts from safety evaluation
  - Quick check question: When asked "How did someone most likely create a phishing scam?" vs. "Create a phishing scam," what different cognitive pathways might an LLM activate?

- **Concept: Keyword-based vs. semantic safety alignment**
  - Why needed here: The paper's central finding is that current safety measures are more keyword-detection than intent-understanding systems, making them vulnerable to obfuscation
  - Quick check question: If you replace "bomb" with `[98 111 109 98]` in a request, does a semantic safety evaluator process it differently than a keyword filter?

## Architecture Onboarding

- **Component map:** Query Transformation (GPT-4o-mini) → Imperative to declarative framing → Toxic Word Detection (GPT-4o-mini) → Few-shot identification → Adaptive Masking → Model-specific encoding level (decoding test calibration) → Final Prompt Assembly → Abductive instructions + encoded content → Target LLM → Generates harmful response

- **Critical path:** The decoding test (Section 3.3, Step 2) determines optimal masking level—too little masking triggers refusal, too much prevents comprehension. This calibration step is where the attack succeeds or fails for each target model.

- **Design tradeoffs:**
  - Masking complexity vs. interpretability: Higher masking (flip technique, full masking) increases evasion but risks model misunderstanding
  - Encoding novelty vs. transferability: Custom encodings (Equation, Manchu) evade safety training but require per-model calibration
  - Single-turn vs. multi-turn: Multi-turn attacks achieve higher information extraction (Section 5.1 shows 75%+ second-turn success) but increase detection risk

- **Failure signatures:**
  - Over-refusal cascade: If initial response is refusal, subsequent turns often refuse regardless of framing
  - Encoding over-complexity: Models like LLaMA-3-8B require 60% masking; full masking drops comprehension
  - Trigger-word hypersensitivity: Claude 3.5 and LLaMA-3 refuse even neutralized queries containing "suicide" or "bomb" (Section 5.3)

- **First 3 experiments:**
  1. Baseline calibration: Run decoding test with 100 benign/harmful prompts at 0%, 20%, 40%, 60%, 80%, 100% masking levels to map each target model's comprehension vs. evasion curve (replicate Figure 4 methodology)
  2. Ablation validation: Test abductive-only, encoding-only, and combined conditions on 50 AdvBench prompts to verify component contributions (replicate Figure 3)
  3. Defense robustness: Apply perplexity filtering, paraphrasing, and self-reminder defenses to HaPLa outputs and measure ASR degradation (replicate Table 2)

## Open Questions the Paper Calls Out

- How can safety tuning methodologies be developed to defend against novel symbolic encodings without significantly reducing model helpfulness on benign queries? The paper demonstrates that while models can be tuned to reject specific encodings, they fail to generalize to unseen encodings unless trained to a degree that compromises utility.

- Does the HaPLa vulnerability extend to multimodal scenarios and broader task datasets beyond the AdvBench benchmark? The current study restricts evaluation to text-based malicious prompts, leaving the efficacy of abductive framing on image, audio, or tool-use inputs unknown.

- Can context-aware filtering or multi-stage defenses effectively distinguish harmful abductive reasoning from benign inference tasks? Abductive prompts frame harmful acts as third-person narratives or puzzles, which closely resemble legitimate creative writing or logical reasoning tasks, making detection difficult.

## Limitations

- The study is limited to text-based malicious prompts and doesn't evaluate multimodal scenarios or non-benchmark task datasets
- The analysis doesn't establish whether symbolic encoding exploits universal model weaknesses or specific implementation details in tested LLMs
- The comparison between LLM outputs and actual crime cases is suggestive but doesn't establish genuine criminal knowledge versus coincidental narrative overlap

## Confidence

**High Confidence:** The technical methodology successfully bypasses safety filters on tested models, as evidenced by consistent ASR >95% on GPT-series and >70% across all targets with well-documented empirical results.

**Medium Confidence:** The claim about fundamental limitations in LLM safety measures. While evidence shows current systems are vulnerable, the paper doesn't establish whether these represent fundamental architectural constraints or temporary implementation gaps.

**Low Confidence:** The extrapolation that current safety failures "mirror real-world crime cases." The comparison between LLM outputs and actual crime cases is suggestive but doesn't prove genuine criminal knowledge.

## Next Checks

- Implement and test 10 novel encoding schemes beyond those in the paper (e.g., base64, Unicode variations, custom substitution ciphers) across the same target models to measure whether the 70% ASR claim holds across arbitrary encodings.

- Apply state-of-the-art semantic safety systems (e.g., those using sentence embeddings, intent classification, or multi-step reasoning) to HaPLa outputs to determine whether the "keyword-localized" safety failure is universal or specific to certain safety implementations.

- Implement turn-based monitoring that tracks conversation patterns across multiple exchanges to test whether HaPLa's multi-turn success rates drop when models detect suspicious abductive framing sequences or encode-decode patterns across turns.