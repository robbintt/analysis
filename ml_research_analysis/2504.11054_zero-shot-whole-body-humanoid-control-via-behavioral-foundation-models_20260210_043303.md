---
ver: rpa2
title: Zero-Shot Whole-Body Humanoid Control via Behavioral Foundation Models
arxiv_id: '2504.11054'
source_url: https://arxiv.org/abs/2504.11054
tags:
- fb-cpr
- learning
- tasks
- dataset
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FB-CPR, a novel algorithm that trains behavioral
  foundation models (BFMs) for humanoid control by grounding unsupervised reinforcement
  learning with imitation of unlabeled behaviors. The key innovation is a latent-conditional
  discriminator that encourages learned policies to "cover" the states present in
  a motion-capture dataset, while retaining zero-shot generalization to downstream
  tasks.
---

# Zero-Shot Whole-Body Humanoid Control via Behavioral Foundation Models

## Quick Facts
- arXiv ID: 2504.11054
- Source URL: https://arxiv.org/abs/2504.11054
- Authors: Andrea Tirinzoni; Ahmed Touati; Jesse Farebrother; Mateusz Guzek; Anssi Kanervisto; Yingchen Xu; Alessandro Lazaric; Matteo Pirotta
- Reference count: 40
- Primary result: FB-CPR introduces behavioral foundation models for humanoid control, achieving competitive performance with task-specific methods while producing more human-like behaviors

## Executive Summary
This paper presents FB-CPR, a novel algorithm that trains behavioral foundation models (BFMs) for whole-body humanoid control by grounding unsupervised reinforcement learning with imitation of unlabeled behaviors. The key innovation is a latent-conditional discriminator that encourages learned policies to "cover" the states present in a motion-capture dataset, while retaining zero-shot generalization to downstream tasks. When trained on the AMASS dataset, FB-CPR produces Meta Motivo, the first BFM for whole-body humanoid control that outperforms state-of-the-art unsupervised RL and model-based baselines.

## Method Summary
FB-CPR trains behavioral foundation models by combining unsupervised reinforcement learning with behavior cloning from unlabeled motion capture data. The method uses a latent-conditional discriminator to encourage policies to cover the state distribution of the training dataset while maintaining generalization capabilities. The approach grounds the learning process by incorporating a dataset of motion capture trajectories, which biases the policy toward natural, human-like movements. This enables zero-shot transfer to various downstream tasks including reward optimization, goal reaching, and motion tracking without additional fine-tuning.

## Key Results
- FB-CPR achieves competitive performance with task-specific methods on reward optimization, goal reaching, and motion tracking tasks
- Outperforms state-of-the-art unsupervised RL and model-based baselines on AMASS dataset
- Human evaluation shows FB-CPR generates more human-like behaviors than pure reward-optimization approaches
- Demonstrates first successful behavioral foundation model for whole-body humanoid control

## Why This Works (Mechanism)
FB-CPR works by combining unsupervised RL with behavior cloning through a novel latent-conditional discriminator. This discriminator encourages the policy to visit states that are likely under the motion capture dataset distribution, effectively "covering" the behavioral space of human movements. The latent conditioning allows the model to capture diverse behaviors while the unsupervised RL component provides the flexibility to adapt to new tasks. By grounding the learning process with real human motion data, the method biases the policy toward natural movements while maintaining the ability to optimize for specific objectives.

## Foundational Learning
- **Behavioral Foundation Models**: Why needed - to capture diverse, reusable behaviors for robotics; Quick check - can generate multiple distinct movement styles from same input
- **Unsupervised Reinforcement Learning**: Why needed - to learn without task-specific rewards; Quick check - policy improves performance over random exploration
- **Latent-Conditional Discriminators**: Why needed - to match policy distribution to dataset; Quick check - policy visits states similar to motion capture data
- **Motion Capture Datasets**: Why needed - provide real human movement examples; Quick check - dataset covers diverse movements and scenarios
- **Zero-Shot Generalization**: Why needed - apply learned behaviors to new tasks; Quick check - policy performs well on unseen tasks without fine-tuning

## Architecture Onboarding

**Component Map**: Motion Capture Data -> BFM Trainer -> Latent-Conditional Discriminator -> Policy Network -> Environment

**Critical Path**: The most important sequence is Motion Capture Data → BFM Trainer → Policy Network → Environment, where the discriminator shapes the policy to match the dataset distribution while maintaining task performance.

**Design Tradeoffs**: The method trades computational complexity for generalization capability - training BFMs requires more resources than task-specific methods but enables zero-shot transfer. The latent-conditional discriminator adds complexity but provides better behavioral coverage than unconditional approaches.

**Failure Signatures**: Poor performance indicates either insufficient motion capture data diversity, discriminator training instability, or mismatch between simulation and real-world dynamics. The policy may collapse to repetitive behaviors if the discriminator dominates training.

**First Experiments**: 1) Validate discriminator can match policy distribution to dataset; 2) Test zero-shot transfer to simple navigation tasks; 3) Compare human-likeness against pure reward optimization baselines.

## Open Questions the Paper Calls Out
None identified in the provided information.

## Limitations
- Potential challenges with sim-to-real transfer to physical humanoid robots
- Reliance on high-quality motion capture data may limit applicability in data-scarce domains
- Computational cost of training BFMs with latent-conditional discriminator may be prohibitive for resource-constrained applications

## Confidence

**High Confidence**:
- Competitive performance with task-specific methods on reward optimization, goal reaching, and motion tracking tasks
- Superiority over unsupervised RL and model-based baselines

**Medium Confidence**:
- Human evaluation showing more human-like behaviors than pure reward-optimization approaches
- Zero-shot generalization to downstream tasks

## Next Checks
1. **Sim-to-Real Transfer Validation**: Conduct experiments transferring FB-CPR policies from simulation to physical humanoid robots to assess real-world performance and robustness to hardware variations.

2. **Cross-Dataset Generalization**: Test FB-CPR's performance when trained on one motion capture dataset and evaluated on a different, unseen dataset to quantify true zero-shot generalization capabilities.

3. **Scalability and Efficiency Analysis**: Measure the computational requirements (training time, memory usage) of FB-CPR as dataset size and humanoid model complexity increase to establish practical deployment constraints.