---
ver: rpa2
title: 'UniHOI: Unified Human-Object Interaction Understanding via Unified Token Space'
arxiv_id: '2511.15046'
source_url: https://arxiv.org/abs/2511.15046
tags:
- generation
- detection
- unified
- interaction
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes UniHOI, the first unified framework for joint
  human-object interaction (HOI) detection and generation. It introduces a unified
  token space that allows bidirectional mapping between images and interaction semantics,
  enabling effective knowledge sharing across both tasks.
---

# UniHOI: Unified Human-Object Interaction Understanding via Unified Token Space

## Quick Facts
- arXiv ID: 2511.15046
- Source URL: https://arxiv.org/abs/2511.15046
- Authors: Panqi Yang; Haodong Jing; Nanning Zheng; Yongqiang Ma
- Reference count: 9
- Key outcome: First unified framework for joint HOI detection and generation, achieving 4.9% improvement on long-tailed HOI detection and 42.0% boost on interaction metrics for generation tasks.

## Executive Summary
UniHOI introduces the first unified framework for joint human-object interaction (HOI) detection and generation by introducing a unified token space that allows bidirectional mapping between images and interaction semantics. The method employs a symmetric interaction-aware attention module and semi-supervised learning via cycle consistency in the shared token space, enabling effective knowledge sharing across both tasks with limited annotations. Extensive experiments show state-of-the-art performance on both detection and generation tasks.

## Method Summary
UniHOI operates on a massively expanded vocabulary merging visual codebook tokens and text tokens into a single unified space. The framework uses Llama3-8B as backbone with VQ-GAN (Chameleon) tokenizer for images and BPE tokenizer for text. Interaction-Aware Attention (IAA) modules with modality-type embeddings are inserted into the backbone to enable bidirectional attention between visual and semantic tokens. The model is trained using a semi-supervised approach combining 10% strongly-supervised data (HICO-DET, V-COCO), 25% weakly-supervised data (LAION-SG with scene graph triplets), and 65% unsupervised data (LAION-400M with captions only). Cycle consistency losses enforce semantic-visual alignment in the unified token space.

## Key Results
- Achieves 4.9% improvement on long-tailed HOI detection accuracy compared to state-of-the-art methods
- Boosts interaction metrics by 42.0% on open-vocabulary generation tasks
- Demonstrates effective semi-supervised learning, with performance increasing from 0.38 to 0.64 HOI Score when increasing unsupervised data from 30% to 65%
- Shows strong performance on both detection benchmarks (HICO-DET, V-COCO) and generation metrics (FID, CLIP Score, HOI Score, Interaction Accuracy)

## Why This Works (Mechanism)

### Mechanism 1: Unified Token Space
- **Claim:** A unified token space enables bidirectional knowledge transfer between detection and generation.
- **Mechanism:** By merging visual codebook ($V_{code}$) and text vocabulary ($V_{sem}$) into a single vocabulary $V$, the model maps both interaction semantics and visual features into discrete token indices, allowing bidirectional vision-language modeling within the same sequence-to-sequence framework.
- **Core assumption:** HOI triplets can be adequately compressed into the same discrete space as low-level visual tokens without catastrophic information loss.
- **Evidence anchors:** Method section defines joint vocabulary as $V = V_{code} \cup V_{sem}$ with shared embeddings; ablation study shows removing IAA drops performance.
- **Break condition:** If codebook size is too small to disambiguate fine-grained interactions (e.g., "holding" vs. "carrying"), the unified space may collapse into generic representations.

### Mechanism 2: Symmetric Interaction-Aware Attention
- **Claim:** Symmetric IAA captures relational structures for both tasks using a single parameter set.
- **Mechanism:** IAA module injects modality-type embeddings into cross-attention, using symmetric design where visual tokens act as queries during detection and semantic tokens act as queries during generation, forcing shared parameters to learn consistent mappings.
- **Core assumption:** A single shared cross-attention block is sufficient to model inverse mappings without requiring task-specific branches.
- **Evidence anchors:** Method section describes parameter-shared symmetric cross-attention; Table 4 ablation shows IAA removal degrades performance.
- **Break condition:** If generation and detection gradients conflict, shared attention weights may oscillate or converge to sub-optimal local minimum.

### Mechanism 3: Cycle Consistency Learning
- **Claim:** Cycle consistency in token space enables effective semi-supervised learning with limited annotations.
- **Mechanism:** Model enforces dual-cycle constraint: $Sem \to Code \to Sem$ and $Code \to Sem \to Code$, minimizing reconstruction error to maintain consistent latent representation even when ground truth is missing.
- **Core assumption:** Token space is robust enough that reconstruction serves as reliable proxy for ground-truth supervision.
- **Evidence anchors:** Method defines dual cycle-consistency objective; Table 5 shows increasing unsupervised data from 30% to 65% boosts HOI Score from 0.38 to 0.64.
- **Break condition:** If VQ-GAN produces low-fidelity images, reconstruction loss may penalize valid semantic predictions due to visual differences.

## Foundational Learning

- **Concept: Discrete Visual Tokenization (VQ-GAN)**
  - **Why needed here:** UniHOI treats images as sequences of tokens, requiring mapping from continuous images to discrete codebook indices before LLM processing.
  - **Quick check question:** Can you explain why a codebook is necessary to bridge a continuous CNN encoder and a discrete LLM?

- **Concept: Cross-Attention Mechanisms**
  - **Why needed here:** IAA module is modified cross-attention layer; understanding Queries ($Q$), Keys ($K$), Values ($V$) interaction and modality embeddings is critical for debugging.
  - **Quick check question:** In IAA module, if text tokens are Queries, what modality are Keys and Values?

- **Concept: Cycle Consistency Loss**
  - **Why needed here:** Core training paradigm is semi-supervised via cycle reconstruction; understanding how $A \to B \to A$ provides supervision without explicit labels is essential.
  - **Quick check question:** If you have an unlabeled image, which cycle path ($Code \to Sem \to Code$ or $Sem \to Code \to Sem$) can be utilized, and what is the target?

## Architecture Onboarding

- **Component map:** VQ-GAN (Image → $V_{code}$) → BPE Tokenizer (Text → $V_{sem}$) → Unified Vocabulary $V = V_{code} \cup V_{sem}$ → Llama3-8B Backbone with IAA → Logits over $V$ → VQ-GAN Decoder (for images) or Text Detokenizer

- **Critical path:**
  1. Input modality detection (Image vs. Text)
  2. Tokenization into unified indices + Modality-type embedding addition
  3. Transformer processing with IAA (Queries switch based on task)
  4. Logits over Unified Vocabulary
  5. Detokenization (VQ-GAN Decoder or Text Detokenizer)

- **Design tradeoffs:**
  - **Unified vs. Separate Heads:** Single unified head (softmax over $V_{code} \cup V_{sem}$) saves parameters but forces balance between visual and semantic token frequencies, potentially causing modality imbalance.
  - **Symmetric Attention:** Sharing parameters for detection and generation improves data efficiency but risks catastrophic forgetting where generation fine-tuning degrades detection accuracy.

- **Failure signatures:**
  - **Modality Collapse:** Model generates text when asked for image (or vice versa) → Modality-type embeddings likely missing or incorrectly weighted.
  - **Low Interaction Accuracy:** Generation looks good but hand doesn't touch object → IAA module may not attend to spatial constraints; cycle loss weight may be too low.
  - **Training Divergence:** Loss spikes during semi-supervised training → Unsupervised data contains out-of-distribution images causing high reconstruction error.

- **First 3 experiments:**
  1. **Vocabulary Integrity Test:** Feed images and verify VQ-GAN decoder can reconstruct them purely from unified tokens generated by backbone (bypassing LLM logic initially).
  2. **IAA Ablation (Visual):** Run inference on detection sample with and without modality-type embeddings ($e_t$). Visualize attention maps to confirm removing $e_t$ causes looking at irrelevant background regions.
  3. **Cycle Consistency Validity:** Take weakly supervised data (captions only). Train 1 epoch. Measure $Code \to Sem \to Code$ reconstruction loss to ensure decreasing, indicating model grounds text to visual tokens effectively.

## Open Questions the Paper Calls Out

- **Open Question 1:** Would fine-grained modality-type embeddings (distinguishing human, object, and action tokens) improve cross-modal alignment precision compared to current coarse-grained approach?
  - **Basis:** Authors state coarse-grained embeddings exploit structured HOI triplet input to implicitly align semantic slots, but don't test explicit fine-grained differentiation.
  - **What evidence would resolve it:** Ablation studies replacing binary modality-type embeddings with role-specific embeddings and measuring resulting mAP on HICO-DET and HOI Score on generation tasks.

- **Open Question 2:** Does unification of visual and semantic tokens into single vocabulary introduce token collision or semantic interference limiting representational capacity?
  - **Basis:** Method defines modality-aware unified token space with shared embedding matrix, but doesn't analyze if union of distinct vocabularies causes semantic ambiguity.
  - **What evidence would resolve it:** Visualization of embedding space (t-SNE plots) to assess separation of visual and semantic clusters, or ablations comparing merged vs. separate vocabularies with alignment layers.

- **Open Question 3:** Can unified token space and symmetric attention mechanism be effectively extended to temporal domains like video HOI detection and generation?
  - **Basis:** Conclusion suggests framework provides insights for bridging recognition and generation in broader multimodal contexts, but current evaluation is strictly limited to static images.
  - **What evidence would resolve it:** Experiments applying UniHOI framework to video datasets using video tokenizer, evaluating temporal consistency of detected or generated interactions.

## Limitations
- Token Space Ambiguity: Unified vocabulary assumes semantic triplets can be adequately represented alongside visual tokens without information loss, potentially causing semantic collapse in complex HOI scenarios.
- Symmetric Attention Trade-off: Shared IAA module improves parameter efficiency but limited evidence that single attention mechanism can effectively model both vision-to-semantics and semantics-to-vision mappings.
- Cycle Consistency Validity: Semi-supervised approach relies on token-space reconstruction as proxy for supervision, but doesn't validate whether reconstruction loss correlates with actual HOI understanding.

## Confidence
- **High Confidence:** Detection performance improvements on HICO-DET and V-COCO benchmarks - methodology and evaluation are standard with clear ablation evidence.
- **Medium Confidence:** Token space unification mechanism - framework is well-specified but assumption of adequate representation without information loss needs more rigorous validation.
- **Low Confidence:** Semi-supervised cycle consistency effectiveness - demonstrates data efficiency improvements but lacks evidence that token-space reconstruction genuinely captures HOI semantics.

## Next Checks
1. **Fine-grained Interaction Discrimination Test:** Create diagnostic dataset with visually similar HOI triplets (e.g., "holding cup" vs. "carrying cup") and evaluate whether unified token space can distinguish them, measuring semantic token divergence between similar interactions.

2. **Task-specific Attention Ablation:** Implement separate detection and generation attention modules and compare against unified IAA, measuring not just final performance but also gradient stability and convergence speed to determine if parameter sharing introduces optimization conflicts.

3. **Cycle Consistency Ablation with Weak Supervision:** Train with and without cycle consistency on controlled subset of weakly supervised data, evaluating whether cycle loss improves HOI understanding or merely improves visual reconstruction quality using both automated metrics and human evaluation of generated interaction correctness.