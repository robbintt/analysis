---
ver: rpa2
title: 'Model Merging is Secretly Certifiable: Non-Vacuous Generalisation Bounds for
  Low-Shot Learning'
arxiv_id: '2505.15798'
source_url: https://arxiv.org/abs/2505.15798
tags:
- bound
- error
- learning
- pac-bayes
- generalisation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that model merging methods can achieve
  non-vacuous generalization bounds for low-shot learning with large models, providing
  a first-time certification for models like ViT-B and Mistral-7B with only 100 training
  examples. The key insight is that model merging learns a small number of merging
  parameters regardless of the base model size, making the certified generalization
  gap tiny and independent of network size.
---

# Model Merging is Secretly Certifiable: Non-Vacuous Generalisation Bounds for Low-Shot Learning

## Quick Facts
- arXiv ID: 2505.15798
- Source URL: https://arxiv.org/abs/2505.15798
- Reference count: 40
- Demonstrates non-vacuous generalization bounds for ViT-B and Mistral-7B with only 100 training examples via model merging

## Executive Summary
This paper reveals that model merging methods, when viewed through a PAC-Bayes lens, can provide non-vacuous generalization bounds for low-shot learning. By learning only a handful of merging parameters regardless of base model size, these methods overcome the typical barrier where PAC-Bayes bounds become vacuous for large neural networks. The authors show that existing merging algorithms like Task Arithmetic and AdaMerging can be "secretly" certified without modification, and propose optimizing the PAC-Bayes bound directly as the learning objective to further tighten guarantees.

## Method Summary
The approach reinterprets model merging algorithms as PAC-Bayes inference with Gaussian posteriors over merging weights. The method computes generalization bounds by taking the learned merging weights as posterior means and calculating KL divergence against a prior. For challenging cases with high-dimensional merging parameters, the paper proposes optimizing the PAC-Bayes bound directly as the training objective and using data-dependent priors constructed from a subset of training data. This framework applies to both vision transformers and large language models.

## Key Results
- Achieves non-vacuous bounds for ViT-B/32 with 100 examples (PB Bound ~0.70)
- Extends certification to 7B-parameter LLMs (Mistral-7B) on TweetEval
- Bound optimization improves certification in challenging cases, with data-dependent priors further tightening bounds
- Demonstrates certified generalization gaps within 5% of training error for 1k-4k examples

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Model merging enables non-vacuous generalization bounds because the learnable parameter count depends on the number of source models, not base network size.
- **Mechanism:** Standard fine-tuning optimizes millions-billions of parameters, making PAC-Bayes KL-divergence terms explode. Model merging learns only M–M×L weights for M models with L layers. The certified generalization gap becomes "tiny and independent of the base network size."
- **Core assumption:** The merging weight space contains good solutions for the downstream task—i.e., source models collectively encode transferable knowledge.
- **Evidence anchors:** [abstract] "the certified generalisation gap becomes tiny and independent of the base network size"; [Section 2] "the learnable parameters for the downstream tasks are handful of weighting factors compared to billions of parameters"

### Mechanism 2
- **Claim:** Existing merging algorithms can be re-interpreted as PAC-Bayes inference with Gaussian posteriors over merging weights, enabling immediate certification.
- **Mechanism:** Set prior to uniform weights N(1/M, σ²I). After any merging procedure produces weights µQ, interpret as posterior mean N(µQ, σ²I). The KL-divergence KL(Q||P) then depends only on ||µQ - µP||² and d (merging parameter count), both small.
- **Core assumption:** Gaussian distributions with fixed variance adequately capture uncertainty over merging weights.
- **Evidence anchors:** [Section 4.1] "existing merging method can be 'secretly' providing a non-vacuous PAC-Bayesian bound without any modification"

### Mechanism 3
- **Claim:** Directly optimizing the PAC-Bayes bound (not just training loss) tightens guarantees, especially combined with data-dependent priors.
- **Mechanism:** Standard merging minimizes empirical loss L̂. Bound optimization minimizes L̂ + √((KL(Q||P) + log(n/δ))/(2(n-1))). Data-dependent priors use subset of training data to initialize prior mean near good solutions, reducing KL divergence.
- **Core assumption:** Splitting already-scarce training data doesn't catastrophically hurt performance; prior construction on subset generalizes.
- **Evidence anchors:** [Table 2] Data-dependent prior reduces PB Bound from 1.000 (vacuous) to 0.483 on eurosat for layer-wise AdaMerging

## Foundational Learning

- **PAC-Bayes Framework:**
  - Why needed here: All certification relies on PAC-Bayes bounds (Theorems 1–2). Must understand prior/posterior distributions, KL-divergence role in bound tightness, and how randomized classifiers work (sample θ~Q per prediction).
  - Quick check question: Given prior P and posterior Q over parameters, what happens to the generalization bound as KL(Q||P) increases?

- **Model Merging Methods (Task Arithmetic, AdaMerging variants):**
  - Why needed here: These are the base algorithms being certified. Need to understand what parameters they learn (scalar weight, task-wise weights, layer-wise weights) to anticipate bound tightness.
  - Quick check question: How many learnable parameters does layer-wise AdaMerging have when merging 7 models with 28 layers each?

- **Non-Vacuous vs. Vacuous Bounds:**
  - Why needed here: A bound >1.0 (error rate >100%) provides no information. Goal is bounds <1.0 that are close to actual test error.
  - Quick check question: If PAC-Bayes bound is 0.75 and training error is 0.30, what's the guaranteed upper bound on test error?

## Architecture Onboarding

- **Component map:**
  - Pre-trained models -> Merging weights (1, M, or M×L parameters) -> PAC-Bayes posterior N(μ_Q, σ²I) -> Empirical loss + KL divergence -> Seeger bound

- **Critical path:**
  1. Assemble source model pool (pre-trained + fine-tuned variants)
  2. Initialize prior (uniform or data-dependent using subset)
  3. Run merging algorithm (optionally with bound optimization objective)
  4. Compute empirical loss on remaining training data
  5. Compute KL(Q||P) and solve Seeger bound numerically

- **Design tradeoffs:**
  - More merging parameters (layer-wise > task-wise > scalar) → potentially better performance but looser/vacuous bounds
  - Bound optimization → tighter guarantees but may underfit if prior is uninformative
  - Data-dependent prior → tighter bounds but requires data splitting; optimal split varies with total n

- **Failure signatures:**
  - Vacuous bounds (PB Bound ≥ 1.0) when merging parameter count is high without data-dependent priors
  - Underfitting when bound optimization forces posterior too close to uninformative prior
  - Merge learning itself fails (test error > zero-shot baseline) when source models lack relevant knowledge—no success to certify

- **First 3 experiments:**
  1. Reproduce CLIP-ViT-B/32 results: Take 7 fine-tuned models, apply Task Arithmetic with 100 examples on held-out task (e.g., EuroSAT). Compute off-the-shelf PAC-Bayes bound—should be non-vacuous (~0.70).
  2. Test bound optimization: Compare vanilla Task Arithmetic vs. bound-optimized version on same task. Measure tradeoff between test error and bound tightness.
  3. Scale to higher-dimensional merging: Try layer-wise AdaMerging (196 params). Expect vacuous bounds initially, then add data-dependent prior to recover non-vacuous certification.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the PAC-Bayesian model merging framework be extended to provide non-vacuous generalization bounds for generative tasks (e.g., open-ended text generation) rather than just classification?
- **Basis in paper:** [explicit] The authors state the results could provide "immediate impact in facilitating the certification of existing AI systems" and explicitly mention LLMs, but all empirical evaluations (BBH, TweetEval) are restricted to classification tasks using 0-1 loss.
- **Why unresolved:** The theoretical bounds rely on empirical risk measures (like 0-1 loss) that do not translate trivially to the complexity of evaluating open-ended generation quality, where ground truth is stochastic or stylistic.
- **What evidence would resolve it:** A demonstration of non-vacuous bounds on a generative benchmark using a suitable proxy for empirical risk (e.g., distinct N-gram overlap or embedding-based similarity scores) within the PAC-Bayes framework.

### Open Question 2
- **Question:** How does the diversity or quality of the source model pool impact the tightness of the generalization bound?
- **Basis in paper:** [inferred] The paper excludes results where the underlying merging algorithm failed to improve upon the base model, noting these are "failures of the base learning algorithm." This implies the method's success is contingent on the source models, but the theoretical sensitivity to this "prior" quality is unexplored.
- **Why unresolved:** While the mechanism reduces dependency on *base network* size, it introduces a dependency on the *source model* landscape. It is unclear if high interference or low relevance among source models degrades the bound tightness even if parameter count remains low.
- **What evidence would resolve it:** An ablation study analyzing the correlation between source model diversity (e.g., task distinctiveness) and the certified generalization gap on a fixed target task.

### Open Question 3
- **Question:** Can the bound optimization process be scaled efficiently to higher-dimensional merging spaces (e.g., layer-wise merging) using gradient-based methods?
- **Basis in paper:** [inferred] The authors note that for the 0-1 loss optimization, they "resort to gradient-free methods (e.g., evolutionary search)" because the objective is non-differentiable. This limits scalability compared to standard gradient descent.
- **Why unresolved:** Gradient-free methods like CMA-ES struggle with high dimensionality. As merging schemes become more complex (parameterizing more layers), the computational cost of finding the optimal PAC-Bayes posterior may become prohibitive.
- **What evidence would resolve it:** A study comparing gradient-free optimization against differentiable surrogates (e.g., using a smoothed loss or relaxation) to verify if bounds remain non-vacuous and training remains tractable for merging spaces with >1000 parameters.

## Limitations
- Vacuous bounds occur with layer-wise merging (196 parameters) without data-dependent priors
- Underfitting risk when bound optimization forces posterior too close to uninformative prior
- Success depends critically on source models collectively encoding relevant knowledge

## Confidence
- **High:** That model merging reduces learnable parameters vs full fine-tuning, enabling non-vacuous bounds in principle (Mechanism 1 supported by parameter count analysis and empirical results)
- **Medium:** That existing merging algorithms can be directly interpreted as PAC-Bayes inference (Mechanism 2 has theoretical justification but limited empirical validation of the re-interpretation itself)
- **Medium:** That bound optimization and data-dependent priors consistently improve certification (Mechanism 3 supported by specific examples but tradeoff sensitivity not fully characterized)

## Next Checks
1. **Parameter Scaling Test:** Systematically vary the number of merging parameters (scalar → task-wise → layer-wise) across different M values, measuring when bounds become vacuous even with data-dependent priors
2. **Source Model Diversity:** Test merging across source models with decreasing task similarity (e.g., natural images → medical imaging → synthetic data) to quantify when knowledge transfer fails
3. **Prior Distribution Sensitivity:** Compare Gaussian assumptions against alternative distributions (e.g., Laplace, mixture models) for the merging weights to test robustness of the PAC-Bayes re-interpretation