---
ver: rpa2
title: Reliable Evaluation Protocol for Low-Precision Retrieval
arxiv_id: '2508.03306'
source_url: https://arxiv.org/abs/2508.03306
tags:
- evaluation
- range
- retrieval
- scoring
- fp32
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses the instability in retrieval evaluation caused
  by spurious ties when using low-precision scoring functions. To resolve this, the
  authors propose a reliable evaluation protocol combining High-Precision Scoring
  (HPS), which upcasts the final scoring operation to FP32 to collapse ties, and Tie-aware
  Retrieval Metrics (TRM), which compute expected scores, range, and bias to quantify
  ordering uncertainty.
---

# Reliable Evaluation Protocol for Low-Precision Retrieval

## Quick Facts
- arXiv ID: 2508.03306
- Source URL: https://arxiv.org/abs/2508.03306
- Authors: Kisu Yang; Yoonna Jang; Hwanseok Jang; Kenneth Choi; Isabelle Augenstein; Heuiseok Lim
- Reference count: 40
- Low-precision evaluation instability is addressed via upcasting final scoring and tie-aware metrics.

## Executive Summary
This paper tackles the evaluation instability in retrieval models using low-precision inference, where spurious ties in scoring functions lead to inconsistent and unreliable metric scores. The authors propose a two-part solution: High-Precision Scoring (HPS), which upcasts the final scoring operation to FP32 to collapse ties, and Tie-aware Retrieval Metrics (TRM), which quantify the uncertainty and bias introduced by ties. Experiments demonstrate that HPS dramatically reduces tie-induced variability, while TRM exposes systematic overestimation by standard metrics, enabling more consistent and reliable evaluation.

## Method Summary
The proposed protocol combines HPS, which upcasts logits to FP32 before the final scoring function, and TRM, which computes expected metric values, ranges, and biases over all possible tie permutations. HPS targets the final scoring step to minimize ties with minimal overhead, while TRM provides analytic expectations and uncertainty quantification instead of relying on arbitrary tie-breaking. Together, they recover near-FP32 stability while keeping most computation in low precision.

## Key Results
- HPS dramatically reduces tie-induced variability (e.g., MRR@10 range reduced by 36.82 percentage points).
- TRM exposes systematic overestimation by tie-oblivious metrics, with non-zero bias in BF16 evaluation.
- HPS+TRM recovers near-FP32 stability with negligible computational overhead.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Upcasting the final scoring operation to FP32 substantially reduces spurious ties caused by low-precision inference, improving evaluation stability.
- Mechanism: Lower-precision floating-point formats (e.g., BF16, FP16) have coarser mantissa resolution, which quantizes probability scores into fewer distinct buckets, causing tied scores among distinct logits. By upcasting the logits from BF16/FP16 to FP32 immediately before applying the scoring function (softmax, sigmoid, or pairwise product), the final scores are computed with FP32's much finer mantissa (23-bit vs. 7-bit for BF16), collapsing large tie groups and restoring discriminative ranking order.
- Core assumption: Distinct low-precision logits are preserved exactly when cast to FP32, and applying FP32 scoring functions to these upcast logits yields distinct scores for most distinct inputs.
- Evidence anchors:
  - [abstract] "High-Precision Scoring (HPS), which upcasts the final scoring step to higher precision to resolve tied candidates with minimal computational cost."
  - [section] §2.2: "HPS upcasts only the final scoring operation to FP32, leaving other layers unchanged… This significantly reduces the probability of tie collisions while preserving latency."
  - [corpus] "Pychop: Emulating Low-Precision Arithmetic in Numerical Methods and Neural Networks" discusses low-precision emulation and quantization effects broadly but does not provide direct, method-specific evidence on this exact HPS protocol.
- Break condition: If upstream low-precision operations (e.g., low-precision attention, quantized embeddings) introduce distortions that alter relative logit magnitudes in ways that affect ranking, HPS alone may not recover the true FP32 order.

### Mechanism 2
- Claim: Tie-aware Retrieval Metrics (TRM) expose hidden variance and systematic bias in tie-oblivious evaluation by computing expected scores and their range across all possible tie permutations.
- Mechanism: When multiple candidates share the same score, tie-oblivious evaluation (e.g., standard MRR, nDCG) arbitrarily orders them (e.g., by index), leading to stochastic or implementation-dependent outcomes. TRM computes the expectation of the metric analytically over all permutations within each tie group, along with max/min achievable scores (range) and deviation of tie-oblivious scores from this expectation (bias), quantifying uncertainty and systematic over- or under-estimation.
- Core assumption: The set of tied items and their relevance labels are known; the tie-breaking policy of existing systems is deterministic but arbitrary.
- Evidence anchors:
  - [abstract] "Tie-aware Retrieval Metrics (TRM), which report expected scores, range, and bias to quantify order uncertainty of tied candidates."
  - [section] §2.3: "If multiple candidates receive the same score, they are ordered arbitrarily before truncation, affecting which items are included in the top-k set… TRM supplies exact expectations, range, and bias."
  - [corpus] "Average Precision at Cutoff k under Random Rankings: Expectation and Variance" discusses analytic properties of AP under random tie-breaking, supporting the expectation-based approach, though not the specific TRM formulation.
- Break condition: If the evaluation framework or downstream application does not accept probabilistic metric outputs, the richer TRM outputs (expectation and range) may not fit existing tooling without interface changes.

### Mechanism 3
- Claim: Combining HPS with TRM recovers near-FP32 stability in evaluation while keeping the bulk of inference in low precision.
- Mechanism: HPS reduces the number and size of tie groups, shrinking the range of possible metric values, while TRM explicitly reports residual uncertainty and bias. Together they yield deterministic, discriminative metric values close to full FP32 baselines with minimal added latency.
- Core assumption: The final scoring step is the dominant source of tie-related instability; other sources (e.g., retrieval stage, first-stage ranking) are either stable or out of scope.
- Evidence anchors:
  - [abstract] "This combination enables a more consistent and reliable evaluation system for lower-precision retrievals."
  - [section] §3.2: "HPS recovers near-FP32 stability and ordering with negligible time and space overhead."
  - [corpus] "Structured Relevance Assessment for Robust Retrieval-Augmented Language Models" addresses robustness in retrieval evaluation broadly, suggesting tie-aware methods can integrate into larger pipelines, but does not directly evaluate HPS+TRM.
- Break condition: If upstream ranking or retrieval components are also low-precision and introduce large tie groups, HPS applied only at the final stage may not fully stabilize the end-to-end pipeline.

## Foundational Learning
- Concept: Floating-point precision and mantissa granularity.
  - Why needed here: Understanding how BF16, FP16, and FP32 differ in representable values explains why low-precision formats cause tie collisions in scoring functions that output probabilities in (0,1).
  - Quick check question: If a scoring function outputs probabilities in (0,1) and is computed in BF16 (7 mantissa bits), what happens to distinct logits whose probabilities would otherwise differ by less than the BF16 ULP in that range?
- Concept: Rank-based evaluation metrics and tie-breaking.
  - Why needed here: Metrics like MRR, nDCG, and MAP assume a strict ordering; ties must be resolved, and different resolutions can change scores. Understanding this is essential to see why TRM's expectation and range matter.
  - Quick check question: In MRR@10, if the first relevant document appears in a tie group of size 5 at ranks 4–8, how does arbitrary tie-breaking affect the reported MRR?
- Concept: Expectation over permutations in tied groups.
  - Why needed here: TRM computes analytic expectations instead of sampling over tie permutations. Grasping this concept is needed to implement and interpret TRM outputs correctly.
  - Quick check question: Why does an analytic expectation over permutations avoid the variance introduced by repeated random tie-breaking?

## Architecture Onboarding
- Component map: Low-precision model backbone -> Logits extraction -> Upcast to FP32 -> FP32 scoring function -> Ranking and tie group identification -> TRM computation -> Reporting interface
- Critical path:
  1. Run forward pass in low precision; obtain logits z_i.
  2. Upcast logits to FP32: upcast(z_i).
  3. Apply scoring function in FP32: ŝ_i = φ(upcast(z_i)).
  4. Sort by ŝ and identify tie groups.
  5. Compute TRM: expectation, range (max/min), bias.
  6. Report (E[M], Range(M)) and auxiliary values.
- Design tradeoffs:
  - Precision vs. latency: HPS adds negligible overhead but depends on efficient upcasting; full FP32 inference is more stable but costlier.
  - Metric richness vs. tooling compatibility: TRM outputs more information than single scalars; existing leaderboards may need updates.
  - Scope of HPS: Currently targets only the final scoring step; extending to other components (e.g., attention softmax) may yield further stability but requires additional engineering.
- Failure signatures:
  - Large residual range after HPS suggests persistent large tie groups (possibly from upstream low-precision components or dataset-specific artifacts).
  - Non-zero bias with TRM indicates systematic over- or under-estimation by the existing tie-oblivious implementation.
  - Mismatch between HPS+TRM and full FP32 baselines beyond expected noise may indicate implementation bugs in upcasting or TRM formulas.
- First 3 experiments:
  1. Reproduce Table 1: Compare BF16, FP32, and BF16→FP32 (HPS) with tie-oblivious M, E[M], Range, and Bias on MIRACLReranking for at least two models (one softmax, one sigmoid).
  2. Sensitivity to precision: Run the same evaluation under BF16, FP16, and FP32 (with and without HPS) to visualize how range and bias shrink as mantissa bits increase.
  3. TRM-only on BF16 baseline: Compute TRM without HPS to quantify how much uncertainty (range) and bias exist in standard BF16 evaluation; then add HPS and confirm range reduction.

## Open Questions the Paper Calls Out
None

## Limitations
- The protocol assumes tie-related instability is primarily introduced at the final scoring stage, but upstream low-precision operations may also contribute to tie formation in real-world retrieval pipelines.
- While TRM provides richer metric outputs, existing evaluation frameworks and leaderboards may not readily accommodate probabilistic metrics, potentially limiting adoption.
- The study focuses on reranking tasks, leaving open questions about how well this protocol generalizes to end-to-end retrieval systems where low-precision affects multiple stages.

## Confidence
**High Confidence**: The core mechanism of HPS (upcasting final scoring to FP32) is technically sound and directly addresses the stated problem of tie-induced instability. The analytic approach of TRM for computing expectations and ranges over tie permutations is mathematically valid.

**Medium Confidence**: The claim that HPS+TRM recovers "near-FP32 stability" holds for the reranking tasks studied, but may not fully extend to full retrieval pipelines where multiple low-precision components interact. The generalizability to other scoring functions or architectures remains to be tested.

**Low Confidence**: The assertion that HPS has "negligible computational overhead" needs empirical validation across different hardware backends and batch sizes, as upcasting and FP32 scoring costs may vary significantly with scale.

## Next Checks
1. **End-to-end pipeline validation**: Apply HPS+TRM to a complete retrieval pipeline (first-stage + reranker) in low precision to assess whether upstream tie formation in earlier stages undermines the final-stage stabilization.

2. **Cross-architecture stress test**: Evaluate HPS+TRM on architectures beyond rerankers (e.g., dense retrievers with softmax similarity, cross-encoders) and with additional scoring functions to confirm robustness.

3. **Integration benchmark**: Measure actual latency overhead of HPS on production hardware (GPU/TPU) across varying batch sizes and sequence lengths to validate the "negligible overhead" claim and identify scaling thresholds.