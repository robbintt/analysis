---
ver: rpa2
title: Towards Robust Process Reward Modeling via Noise-aware Learning
arxiv_id: '2601.12748'
source_url: https://arxiv.org/abs/2601.12748
tags:
- qwen2
- reasoning
- step
- arxiv
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper tackles the challenge of noisy supervision in process
  reward modeling (PRM), where Monte Carlo Estimation (MCE) labels are policy-dependent
  and introduce false positives (incorrect steps rewarded due to later self-correction)
  and false negatives (correct steps penalized due to later failures). To address
  this, the authors propose a two-stage framework: (1) reflection-aware label correction,
  using an LLM judge to filter out trajectories where downstream steps explicitly
  correct earlier ones, thereby reducing false positives; and (2) noise-aware iterative
  training (NAIT), which refines labels based on model confidence to correct residual
  noise.'
---

# Towards Robust Process Reward Modeling via Noise-aware Learning

## Quick Facts
- **arXiv ID**: 2601.12748
- **Source URL**: https://arxiv.org/abs/2601.12748
- **Reference count**: 19
- **Primary result**: Up to 27% absolute gain in step-level F1 over PRMs trained with noisy supervision, and 3.2-5.6% improvement in downstream test-time scaling accuracy

## Executive Summary
This paper addresses the challenge of noisy supervision in process reward modeling (PRM), where Monte Carlo Estimation (MCE) generates policy-dependent labels that introduce false positives and false negatives. The authors propose a two-stage framework: first, a reflection-aware label correction mechanism uses an LLM judge to filter out trajectories where downstream steps explicitly correct earlier ones, reducing false positives; second, noise-aware iterative training (NAIT) refines labels based on model confidence to correct residual noise. Experiments demonstrate substantial improvements in step-level correctness discrimination and downstream scaling accuracy, with the approach scaling well with more training data.

## Method Summary
The framework consists of two complementary stages. First, reflection-aware label correction filters out successful trajectories where downstream steps explicitly correct earlier reasoning, reducing false positive rewards. Second, noise-aware iterative training (NAIT) progressively refines noisy labels based on the PRM's own confidence, with label updates occurring when model predictions strongly disagree with current labels. The method is trained on MCRD datasets constructed from GSM8K, MATH-500, and PRM800K, using Qwen2.5-Math-7B as the generator policy and various Qwen2.5-LLM judges for reflection detection.

## Key Results
- Up to 27% absolute gain in average F1 score over PRMs trained with noisy supervision
- 3.2-5.6% improvement in downstream test-time scaling accuracy (Best-of-N)
- Consistent performance gains across math reasoning tasks (GSM8K, MATH-500)
- Improvements scale well with increased training data volume

## Why This Works (Mechanism)

### Mechanism 1: Reflection-Aware Label Correction
- **Claim**: Filtering trajectories where downstream steps explicitly correct earlier errors reduces false positive labels in MCE-generated data
- **Mechanism**: LLM judge examines each successful trajectory to detect whether the final correct answer relied on self-correction of the current step
- **Core assumption**: Self-correction language in reasoning trajectories is a reliable signal that the corrected step was genuinely incorrect
- **Evidence anchors**: Abstract, Section 4.1, related work on reflection detection
- **Break condition**: Low precision in self-correction detection leads to over-penalizing valid steps

### Mechanism 2: Noise-Aware Iterative Training (NAIT)
- **Claim**: Iteratively refining labels using model confidence can recover from residual label noise
- **Mechanism**: Train initial PRM, then iteratively compare model prediction with current label and replace when disagreement exceeds threshold
- **Core assumption**: After reflection filtering, strong model-label disagreement indicates label noise rather than model error
- **Evidence anchors**: Abstract, Section 4.2, Figure 4c,d showing monotonic F1 improvements
- **Break condition**: High initial noise causes model confidence to track noisy labels, amplifying errors

### Mechanism 3: Policy-Dependent Label Noise Diagnosis
- **Claim**: MCE labels conflate step correctness with sampling policy capability
- **Mechanism**: MCE defines step rewards based on probability that policy reaches correct answer, making labels policy-dependent
- **Core assumption**: Step correctness is an intrinsic property invariant to policy choice
- **Evidence anchors**: Abstract, Figure 1 showing MCE deviation from human ground truth
- **Break condition**: If step correctness depends on continuation strategies, MCE noise is irreducible

## Foundational Learning

- **Monte Carlo Estimation (MCE) for Process Supervision**
  - Why needed: Paper's diagnosis hinges on understanding how MCE generates labels and why they become noisy
  - Quick check: Given a step with 3 of 8 sampled continuations reaching correct answer, what label would Soft vs Hard Estimation produce?

- **Cross-Entropy Loss for Binary Classification**
  - Why needed: PRMs are trained as discriminative classifiers; understanding loss function is essential
  - Quick check: If model outputs r_t = 0.7 for ground truth y_t = 0, what is cross-entropy loss contribution?

- **Self-Correction in Reasoning Trajectories**
  - Why needed: First mechanism specifically targets self-correction as source of false positives
  - Quick check: In trajectory "I calculated x=5. Wait, that's wrong—I forgot to divide by 2. The answer is x=2.5," which step would reflection-aware correction flag?

## Architecture Onboarding

**Component map:**
Data Construction Pipeline: Generator Policy → Trajectories → LLM Judge (reflection detection) → MCE Aggregation → MCRD Dataset
Training Pipeline (NAIT): MCRD Dataset → Initial PRM (Stage 0) → [Iteration Loop: Evaluate → Refine labels → Retrain] × N stages → Final PRM

**Critical path:**
1. LLM judge quality determines FP reduction effectiveness
2. Threshold δ in NAIT controls label refinement aggressiveness
3. Number of NAIT stages (3-4 iterations shown effective before saturation)

**Design tradeoffs:**
- Judge model size vs. annotation cost: 7B judge matches 14B performance
- Threshold δ: Higher values preserve more original labels; lower values allow more refinement but risk error propagation
- Sampling budget K: Paper uses K=8; diminishing returns with more samples if noise source is structural

**Failure signatures:**
- FP over-correction: MCRD labels show high precision but low recall
- NAIT divergence: Performance degrades across iterations
- Domain mismatch: Strong math performance but poor generalization

**First 3 experiments:**
1. Validate reflection detection precision: Manually annotate 100 flagged vs. 100 not flagged trajectories
2. Ablate NAIT stages: Train Stage 0 only, Stage 0+1, Stage 0+1+2, etc.
3. Stress test noise tolerance: Artificially inject 10%, 20%, 30% false negative rates into MCRD data

## Open Questions the Paper Calls Out
- Performance in reinforcement learning training loops vs. test-time scaling
- Benefits when applied to significantly larger PRM models (70B+)
- Robustness to errors or limitations in the LLM judge used for reflection detection

## Limitations
- Computational limitations prevented extension to reinforcement learning training
- Results may not generalize to larger-scale models beyond 7B parameters
- Does not analyze the impact of judge failures on reflection-aware correction effectiveness

## Confidence
- **High**: Two-stage framework improves F1 scores and test-time scaling accuracy; NAIT converges within 3-4 iterations; reflection detection reduces false positive rates
- **Medium**: Mechanism of reflection filtering improving robustness; low-noise regime assumption enabling NAIT; claim that MCE produces policy-dependent noise
- **Low**: Generalization to non-mathematical domains; optimal threshold schedule for different noise levels; completeness of reflection detection prompt

## Next Checks
1. **Judge Precision Validation**: Manually annotate 200 trajectories where reflection judge flagged self-correction vs. 200 where it did not; calculate precision, recall, and F1 for reflection detection task
2. **Noise Injection Stress Test**: Systematically inject 10%, 20%, and 30% false negative noise into training data; measure at which noise threshold NAIT begins to degrade rather than improve performance
3. **Domain Transfer Experiment**: Apply full framework to non-mathematical domain (e.g., code generation or fact verification); compare whether reflection patterns are domain-specific and whether same threshold schedule remains effective