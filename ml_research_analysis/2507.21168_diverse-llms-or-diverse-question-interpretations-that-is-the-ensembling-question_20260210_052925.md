---
ver: rpa2
title: Diverse LLMs or Diverse Question Interpretations? That is the Ensembling Question
arxiv_id: '2507.21168'
source_url: https://arxiv.org/abs/2507.21168
tags:
- question
- rain
- answer
- interpretation
- gauge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper investigates whether diversity in question interpretation
  can serve as an effective alternative to model diversity in LLM ensembles. Two approaches
  are compared: using multiple models to answer the same question versus using a single
  model to generate multiple interpretations of the question, then answering each.'
---

# Diverse LLMs or Diverse Question Interpretations? That is the Ensembling Question

## Quick Facts
- arXiv ID: 2507.21168
- Source URL: https://arxiv.org/abs/2507.21168
- Authors: Rafael Rosales; Santiago Miret
- Reference count: 39
- Primary result: Question interpretation diversity consistently outperforms model diversity in LLM ensembles for binary QA

## Executive Summary
This paper investigates whether diversity in question interpretation can serve as an effective alternative to model diversity in LLM ensembles. The authors compare two approaches: using multiple models to answer the same question versus using a single model to generate multiple interpretations of the question, then answering each. Both methods employ majority voting for final answer selection. Experiments on boolq, strategyqa, and pubmedqa datasets with GPT and LLaMA models show that question interpretation diversity consistently leads to better ensemble accuracy compared to model diversity, with minimal overlap in accuracy improvements. The results suggest that generating diverse interpretations of questions can be more effective than simply ensembling different models.

## Method Summary
The study compares two ensemble strategies for binary question answering: interpretation diversity (single model answers multiple question interpretations) and model diversity (multiple models answer the same question). Both use majority voting for final answer selection. The methodology employs a two-step prompting approach: first generating multiple interpretations of each question using GPT-3.5-turbo, then having models answer each interpretation. Answer extraction uses keyword matching or regex patterns to identify final answers from model outputs. Experiments were conducted on three datasets (BoolQ, StrategyQA, PubMedQA) using various model combinations including GPT-3.5-turbo, LLaMA-2-7B, and LLaMA-3-8B.

## Key Results
- Question interpretation diversity consistently outperforms model diversity across all tested datasets
- Model diversity ensembles typically produce results between the best and worst ensemble members without clear improvement
- Minimal overlap in accuracy improvements between the two ensemble strategies
- Interpretation diversity shows particular advantage on PubMedQA, suggesting domain relevance

## Why This Works (Mechanism)
The paper doesn't explicitly detail the mechanism behind why interpretation diversity works better, but the results suggest that generating multiple semantic interpretations of a question captures different aspects of ambiguity or underspecification that a single model might miss. By having a single model generate diverse interpretations, the approach ensures consistent understanding of the question's intent across all interpretations, potentially avoiding compounding errors from multiple models with different baseline understandings.

## Foundational Learning
- **Majority voting in ensembles**: Why needed - to aggregate predictions from multiple sources; Quick check - verify voting logic correctly handles ties and edge cases
- **Semantic interpretation generation**: Why needed - to create diverse perspectives on the same question; Quick check - validate that generated interpretations are genuinely distinct and valid
- **Answer extraction from model outputs**: Why needed - to convert free-form model responses into structured binary answers; Quick check - test regex patterns and keyword matching on sample outputs

## Architecture Onboarding
- **Component map**: Question -> Interpretation Generator -> Answer Extractor -> Voter -> Final Answer
- **Critical path**: The two-step prompting process (interpretation generation â†’ answer generation) is the bottleneck, as it requires two LLM calls per interpretation
- **Design tradeoffs**: Single model generating interpretations ensures consistency but may miss model-specific strengths; multiple models capture diversity but risk compounding different understanding errors
- **Failure signatures**: Poor interpretation quality (too similar or invalid) leads to redundant answers; answer extraction failures prevent voting; majority voting fails when interpretations/models disagree strongly
- **First experiments**: 1) Test interpretation generation with temperature=0 vs. default settings; 2) Validate answer extraction heuristics on sample outputs; 3) Run ensemble on small subset to verify voting logic

## Open Questions the Paper Calls Out
- Does increasing the size of a model ensemble eventually yield greater benefits than adding additional interpretations? The study fixed ensemble size at three for both methods, leaving scaling behavior unknown.
- Can interpretation diversity improve performance for open-ended questions using semantic similarity validation? The current methodology relies on keyword matching, inapplicable to unbounded answer sets.
- Do sophisticated consensus heuristics, such as weighting interpretation validity, outperform simple majority voting? The paper treats all generated interpretations as equal voters.

## Limitations
- Study focuses only on binary question answering tasks, limiting generalizability to multi-class or open-ended scenarios
- Relatively small validation sets used (3,270 for BoolQ, 2,290 for StrategyQA, 445 for PubMedQA) may not capture full performance variability
- Key implementation details like generation parameters and answer extraction configuration remain unspecified

## Confidence
- **High confidence**: Experimental setup is reproducible with provided prompts and datasets; general pattern of results is clearly demonstrated
- **Medium confidence**: Conclusion that interpretation diversity is viable alternative to model diversity is supported but magnitude of improvement needs further investigation
- **Medium confidence**: Claim about minimal overlap in accuracy improvements is supported but would benefit from more systematic analysis

## Next Checks
1. Test both answer extraction heuristics (EFirst vs. EPattern) across all datasets to determine which configuration was used for results
2. Implement temperature=0 for deterministic interpretation generation and compare with default temperature settings
3. Conduct experiments with larger validation sets and additional question answering tasks to evaluate generalizability beyond binary questions