---
ver: rpa2
title: Exploiting the English Vocabulary Profile for L2 word-level vocabulary assessment
  with LLMs
arxiv_id: '2506.02758'
source_url: https://arxiv.org/abs/2506.02758
tags:
- level
- cefr
- words
- vocabulary
- proficiency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of fine-grained, context-dependent
  vocabulary assessment for second language learners. It introduces a novel approach
  that combines large language models (LLMs) with the English Vocabulary Profile (EVP)
  to predict the CEFR proficiency level of individual words as they appear in learner
  writing.
---

# Exploiting the English Vocabulary Profile for L2 word-level vocabulary assessment with LLMs

## Quick Facts
- arXiv ID: 2506.02758
- Source URL: https://arxiv.org/abs/2506.02758
- Authors: Stefano Bannò; Kate Knill; Mark Gales
- Reference count: 36
- Key outcome: Qwen 2.5 32B achieves 87.0% accuracy in word-level CEFR prediction, outperforming PoS baseline (80.7%)

## Executive Summary
This paper introduces a novel approach to fine-grained vocabulary assessment for second language learners by combining large language models with the English Vocabulary Profile. The method uses LLMs to identify the intended meaning of polysemous words in context and assign corresponding CEFR proficiency levels. Experiments on the OneStopEnglish corpus demonstrate that Qwen 2.5 32B achieves 87.0% accuracy in word-level CEFR prediction, significantly outperforming both a PoS-based baseline and a random baseline. When applied to EFCAMDAT and ELLIPSE datasets, the LLM-based approach shows stronger correlations between predicted vocabulary levels and essay-level proficiency scores than the PoS-based method.

## Method Summary
The approach maps words in learner writing to CEFR proficiency levels using the English Vocabulary Profile. For each word, the system retrieves all possible definitions and their associated CEFR levels from EVP, then presents these as multiple-choice options to an LLM along with the sentence context. The LLM selects the definition that best matches the contextual meaning. To reduce positional bias, the system generates multiple permutations of the option orderings, extracts logits for option tokens, applies softmax, and averages probabilities across permutations before selecting the final answer. The method handles polysemy by allowing the LLM to choose the contextually appropriate sense rather than defaulting to the lowest common denominator.

## Key Results
- Qwen 2.5 32B achieves 87.0% accuracy on word-level CEFR prediction (test set)
- Outperforms PoS-based baseline (80.7%) and random baseline (61.6%) on OneStopEnglish corpus
- Shows stronger correlation with essay-level holistic scores (PCC 0.771 vs 0.734 for PoS baseline)
- Performs better on ambiguous words (75% vs 66% for PoS baseline)

## Why This Works (Mechanism)

### Mechanism 1
Contextual semantic analysis enables accurate assignment of proficiency levels to polysemous words. The LLM receives a target word and its surrounding sentence, evaluates potential definitions from EVP, and selects the one that semantically aligns with context. This maps the word to a specific CEFR level rather than the lowest common denominator. Core assumption: LLM's internal representation of context is sufficient to distinguish between fine-grained lexical senses. Break condition: If sentence context is too short or grammatically erroneous beyond LLM's error tolerance, semantic resolution fails.

### Mechanism 2
Logit-based probability averaging over option permutations reduces positional bias. Instead of single inference, the system generates prompt variations with shuffled definitions, extracts logits for option tokens, applies softmax, and averages probabilities across permutations. Core assumption: Positional bias is a dominant source of error in zero-shot classification, and averaging cancels it out. Break condition: If number of definitions is large (>6), computational constraints limit permutations, potentially leaving residual bias.

### Mechanism 3
Granular word-sense features provide stronger signal for essay-level proficiency than context-independent baselines. The system aggregates word-level predictions into essay-level features (e.g., proportion of C1 words). By correctly identifying when common words are used in advanced senses, LLM-based features correlate better with holistic human scores than PoS-baseline. Core assumption: Appropriate usage of specific, high-level word senses indicates higher proficiency more reliably than mere presence of complex word forms. Break condition: If learner uses advanced vocabulary incorrectly, LLM might still match it to advanced EVP definition, falsely inflating proficiency estimate.

## Foundational Learning

- **Word Sense Disambiguation (WSD)**: Moving from "What word is this?" to "What does this word mean here?" You must understand that a single lemma (e.g., "run") can map to multiple entries (run a company vs. run a mile) with different difficulty levels. Quick check: If "bank" appears in a sentence about finance, do you know to select the financial definition over the river-edge definition?

- **CEFR Levels (Common European Framework of Reference)**: This is the target variable. The system does not invent difficulty scores; it maps inputs to a standard 6-level scale (A1–C2). Quick check: Can you explain why "get" is likely A1 while "obtain" or "acquire" might be B2?

- **Logits and Softmax Probabilities**: The mechanism relies on extracting raw model confidence (logits) for option tokens (A, B, C) rather than just asking for text generation. This allows for probability averaging. Quick check: How do you access the log-probabilities of specific tokens in an LLM API response?

## Architecture Onboarding

- **Component map:** Input sentence -> spaCy tokenization/lemmatization -> EVP retrieval -> Prompt construction (with shuffled options) -> LLM inference (extract logits) -> Aggregator (softmax + average probabilities) -> Mapper (return CEFR level)

- **Critical path:** The Retrieval-to-Prompt link. If spaCy lemmatizer fails (common with L2 errors) or EVP lookup misses the word, system defaults to "N/A," breaking the assessment chain for that token.

- **Design tradeoffs:** PoS-baseline vs. LLM: PoS is fast and stateless (O(1) lookup) but fails on semantic ambiguity. LLM is expensive (inference per word) but captures nuance. Permutation count: Full permutations (3! = 6) are cheap; partial sampling (10 of 15!) introduces noise but saves compute.

- **Failure signatures:** "None" over-selection: Model frequently selects "None of the other options" fallback (Section 4.5), artificially lowering accuracy on non-ambiguous words. Lemma mismatch: L2 spelling errors (e.g., "recieve") break lemmatizer, causing valid EVP lookups to fail.

- **First 3 experiments:** Semantic validation: Test LLM on EVP's own example sentences to confirm >85% accuracy in selecting correct definition. Ablation on permutations: Compare accuracy with single fixed prompt vs. averaged permutation approach. Ambiguity stress test: Run specifically on ambiguous subset of OneStopEnglish to verify LLM outperforms PoS-baseline (>75% vs 66%).

## Open Questions the Paper Calls Out

None

## Limitations

- Relies on lemma-based lookups, so spelling errors and unconventional word forms in learner writing can break the EVP mapping chain, causing valid words to be marked as "N/A"
- Uses fixed 10 permutations for sampling option orderings, which may not fully eliminate positional bias when number of definitions exceeds 4-6
- Assumes LLM's internal representation of context is sufficient to distinguish between fine-grained lexical senses, which may not hold for highly ambiguous contexts or sentences with grammatical errors

## Confidence

**High Confidence:** Experimental results showing Qwen 2.5 32B achieving 87.0% accuracy on OneStopEnglish test set and outperforming PoS baseline in essay-level correlations (PCC 0.771 vs 0.734).

**Medium Confidence:** Claim that logit-based probability averaging across permutations effectively reduces positional bias. While mechanism is described clearly and implemented, paper does not provide direct evidence comparing performance with vs without permutation averaging on same test data.

**Low Confidence:** Generalizability of method to other learner corpora and languages. Evaluation is limited to British English EVP and specific L2 writing datasets, with no testing on non-English languages or different learner populations.

## Next Checks

1. **Robustness to L2 Errors:** Test pipeline on artificially corrupted versions of OneStopEnglish corpus where common spelling and morphological errors are introduced. Measure how often lemma mismatches cause valid EVP words to be excluded.

2. **Permutation Ablation Study:** Run same test set predictions using single fixed prompt ordering versus 10-permutation averaging approach. Quantify actual reduction in positional bias and determine if full permutations yield additional gains.

3. **Cross-Lingual Transfer:** Apply exact methodology to parallel corpus of L2 writing in another language with available CEFR-aligned vocabulary profile. Measure whether LLM-based approach maintains its advantage over context-independent baselines.