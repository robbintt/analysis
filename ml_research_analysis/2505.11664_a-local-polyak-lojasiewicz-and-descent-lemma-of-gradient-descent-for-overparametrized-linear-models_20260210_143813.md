---
ver: rpa2
title: A Local Polyak-Lojasiewicz and Descent Lemma of Gradient Descent For Overparametrized
  Linear Models
arxiv_id: '2505.11664'
source_url: https://arxiv.org/abs/2505.11664
tags:
- lemma
- convergence
- equation
- step
- rate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper establishes local Polyak-\u0141ojasiewicz (PL) and Descent\
  \ Lemma conditions for gradient descent (GD) on overparametrized two-layer linear\
  \ networks. Unlike non-overparametrized models, these conditions hold only locally\
  \ with constants depending on network weights, step size, and loss."
---

# A Local Polyak-Lojasiewicz and Descent Lemma of Gradient Descent For Overparametrized Linear Models

## Quick Facts
- **arXiv ID**: 2505.11664
- **Source URL**: https://arxiv.org/abs/2505.11664
- **Reference count**: 40
- **Primary result**: Local PL and Descent Lemma constants for overparametrized GD remain bounded, enabling linear convergence with adaptive step sizes.

## Executive Summary
This paper establishes local Polyak-Łojasiewicz (PL) and Descent Lemma conditions for gradient descent on overparametrized two-layer linear networks. Unlike non-overparametrized models where these conditions hold globally, the paper proves they hold locally with constants depending on network weights, step size, and loss. The authors show these constants remain bounded along GD trajectories under appropriate step size constraints, enabling linear convergence. By designing an adaptive step size scheduler that leverages the decreasing local smoothness constant, the method achieves faster convergence than prior approaches.

## Method Summary
The method trains two-layer linear networks via gradient descent with adaptive step sizes. The key innovation is computing local PL and smoothness constants that depend on a weight-dependent operator T mapping the gradient of the non-overparametrized loss to the overparametrized gradient. The local PL constant equals the global PL constant scaled by σ²_min(T_t), while the local smoothness constant decreases as loss decreases. An adaptive step size schedule η_t = min((1+η²₀)^t/(2η₀), 1/K̄_t) is proposed, where η₀ is chosen to balance step growth and smoothness constraints. The method requires proper initialization to ensure σ²_min(T_0) > 0 and width h ≥ m+n.

## Key Results
- Local PL constant μ_t = μ·σ²_min(T_t) remains bounded when σ_min(T_0) > 0 and D(t) stays close to D(0)
- Local smoothness constant K_t decreases as loss decreases, containing terms proportional to √L(t)
- Width h affects convergence rate: larger h improves conditioning ratio α₁/α₂ → 1
- Adaptive step size scheduler achieves faster convergence than Xu et al. (2023) and backtracking line search
- Imbalance D(t) = W₁^T W₁ - W₂^T W₂ remains close to D(0) under proper step size control

## Why This Works (Mechanism)

### Mechanism 1: Local PL Condition via Operator-Modified Gradient Structure
- Claim: Overparametrized models satisfy the Polyak-Łojasiewicz condition locally, with the local PL constant equal to the global PL constant scaled by σ²_min(T_t).
- Mechanism: Overparametrization introduces a weight-dependent linear operator T that acts on ∇ℓ(W), producing ∇L(W₁,W₂) = T(∇ℓ(W)). The gradient norm is lower-bounded by σ²_min(T_t)||∇ℓ||²_F, yielding local PL constant μ_t = μ·σ²_min(T_t).
- Core assumption: The underlying non-overparametrized loss ℓ(W) satisfies Assumption 2.1 (K-smooth, μ-strongly convex).
- Evidence anchors:
  - [abstract] "the local PL constant equals the global constant scaled by the minimum singular value of a weight-dependent operator"
  - [Theorem 3.1] "μ_t = μσ²_min(T_t)"
  - [corpus] Related work (Arora et al., 2018; Xu et al., 2023) also derives local PL conditions but with looser bounds; corpus lacks direct replication of this specific operator-singular-value relationship.
- Break condition: If σ_min(T_t) → 0 during training, the local PL constant vanishes and linear convergence cannot be guaranteed.

### Mechanism 2: Loss-Dependent Local Smoothness Enables Adaptive Step Schedules
- Claim: The local smoothness constant K_t decreases as loss decreases, containing terms proportional to √L(t) that vanish near optima.
- Mechanism: The Hessian bound in Lemma D.1-D.2 shows K_t = Kσ²_max(T_t) + √(2KL(t)) + O(η_t·√L(t)). As L(t) → 0, the loss-dependent terms vanish, leaving only Kσ²_max(T_t).
- Core assumption: Step size η_t satisfies η_t·K_t < 2 to ensure descent.
- Evidence anchors:
  - [Theorem 3.1, eq. 12] Explicit decomposition of K_t showing √(2KL(t)) term
  - [Section 3.2] "K̄_t monotonically decrease to K·exp(√η₀)α₂ w.r.t. t"
  - [corpus] Limited corpus evidence; prior work (Xu et al., 2023) uses fixed smoothness bounds from non-overparametrized model.
- Break condition: If step size grows too fast (η_t > (1+η²₀)^t/2η₀), the imbalance cannot be controlled and K_t bounds fail.

### Mechanism 3: Imbalance Preservation Enables Uniform Condition Number Bounds
- Claim: The imbalance D(t) = W₁^T W₁ - W₂^T W₂ remains close to D(0) throughout training when step size is controlled, enabling bounded κ(T_t).
- Mechanism: Lemma E.2 shows ||D(k+1)-D(k)||_F ≤ 2Kη²_k·σ²_max(T_k)·L(k). Summing yields ||D(t)-D(0)||_F ≤ O(η²₀/(1-Δ)) where Δ < 1 under proper initialization. This bounds σ²_min(T_t) ≥ α₁ + 2α₂(1-exp(η₀^c)) and σ²_max(T_t) ≤ α₂·exp(η₀^c).
- Core assumption: Initialization satisfies α₁ > 0 (guaranteed by Lemma 3.3-3.4 for h ≥ m+n or Gaussian initialization).
- Evidence anchors:
  - [Lemma 3.1] "α₁ + 2α₂(1-exp(√η₀)) ≤ σ²_min(T_t) ≤ σ²_max(T_t) ≤ α₂·exp(√η₀)"
  - [Theorem 3.2] Explicit convergence rate dependent on α₁/α₂
  - [corpus] Prior work (Min et al., 2021; Xu et al., 2023) establishes related imbalance invariance but with more restrictive step-size regimes.
- Break condition: If α₁ ≤ 0 at initialization (e.g., degenerate initialization), σ²_min(T_t) has no positive lower bound and convergence guarantees fail.

## Foundational Learning

- Concept: **Polyak-Łojasiewicz (PL) Condition**
  - Why needed here: The paper's entire convergence proof builds on local PL inequalities; understanding PL is essential to follow Theorem 3.1.
  - Quick check question: Given ||∇f(x)||² ≥ 2μ(f(x) - f*), what convergence rate does gradient descent achieve with step size 1/K?

- Concept: **Condition Number of Linear Operators**
  - Why needed here: The ratio κ(T_t) = σ_max(T_t)/σ_min(T_t) directly determines how much overparametrization slows convergence versus the non-overparametrized baseline.
  - Quick check question: If κ(T_0) = 10, what is the maximum slowdown factor for the overparametrized model's local convergence rate compared to (1 - μ/K)?

- Concept: **Imbalance in Two-Layer Networks**
  - Why needed here: D(t) = W₁^T W₁ - W₂^T W₂ is a training-invariant quantity (up to O(η²) perturbations) that controls the conditioning of T_t.
  - Quick check question: Why does balanced initialization (D(0) ≈ 0) NOT guarantee well-conditioned T_t?

## Architecture Onboarding

- Component map:
  - W₁ ∈ ℝ^(n×h), W₂ ∈ ℝ^(m×h): Weight matrices (n inputs, h hidden, m outputs)
  - W = W₁W₂^T ∈ ℝ^(n×m): Effective weight (product representation)
  - T(·; W₁,W₂): Operator mapping ∇ℓ(W) → [∇ℓ·W₂; ∇ℓ^T·W₁] (skewed gradient)
  - D = W₁^T W₁ - W₂^T W₂: Imbalance matrix (key invariant)
  - α₁, α₂: Initialization-dependent bounds on σ²_min(T_0), σ²_max(T_0)

- Critical path:
  1. Initialize W₁, W₂ to ensure α₁ > 0 (use Gaussian N(0, 1/h^(2p)) with 1/4 < p < 1/2)
  2. Compute initial bounds α₁, α₂, β₁, β₂ from Table 2 definitions
  3. Select η₀ ≤ η_max (solve equations 101-102)
  4. Run GD with adaptive η_t per equation 19: η*_t = min((1+η²₀)^t/2η₀, 1/K̄_t)

- Design tradeoffs:
  - **Width vs. conditioning**: Larger h improves α₁/α₂ → 1 (Theorem 3.3), but increases memory
  - **Initialization variance**: Smaller p (larger variance) speeds convergence but may destabilize early training
  - **Step size growth**: Adaptive schedule (eq. 19) converges faster than constant/decaying, but requires computing K̄_t each iteration

- Failure signatures:
  - Loss plateau with non-decreasing gradient norm → σ_min(T_t) approaching zero
  - Exploding step sizes → η_t violating constraint η_t ≤ 1/K_t
  - Divergence in first few iterations → η₀ > η_max or α₁ ≤ 0

- First 3 experiments:
  1. **Validate α₁/α₂ → 1 with width**: Train identical models with h ∈ {500, 1000, 4000}, plot κ(T_0) and convergence rate; expect κ(T_0) → 1 as h increases (Section 4.1, Figure 1)
  2. **Compare step size schedules**: Implement adaptive schedule (eq. 19) vs. Xu et al. (2023) vs. backtracking line search; expect adaptive to achieve fastest convergence (Section 4.2, Figure 2)
  3. **Test initialization sensitivity**: Vary p ∈ {0.275, 0.375, 0.475} with fixed h; expect smaller p yields better-conditioned T_0 but may require smaller η₀ (Appendix G.1)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the local PL condition and Descent Lemma analysis be extended to multi-layer linear networks with depth L > 2?
- Basis in paper: [explicit] The paper states "In the case L = 2, which is the one we will analyze in this paper" and discusses the overparametrized loss as "using a two-layer linear network to solve the regression problem."
- Why unresolved: The analysis fundamentally relies on the structure of the operator T for two layers. The gradient structure in equation (8) and the singular value relationships in Lemma A.2 are specific to two-layer architectures.
- What evidence would resolve it: Derivation of local PL/smoothness constants for L-layer networks showing whether similar bounds hold, or identification of fundamental obstacles that prevent extension.

### Open Question 2
- Question: Do the local PL condition and convergence results extend to nonlinear activation functions?
- Basis in paper: [explicit] The paper notes: "One line of work... studies the convergence of GD... under the assumption that the width and initialization... are sufficiently large, which is also known as the neural tangent kernel (NTK) regime. However... the NTK regime prohibits feature learning, and the performance of neural networks in this regime degrades substantially."
- Why unresolved: The analysis exploits the linearity of the network to derive explicit forms for the operator T and its singular values. Nonlinearities would fundamentally change the gradient structure and the relationship between overparametrized and non-overparametrized gradients.
- What evidence would resolve it: Counterexample showing local PL fails for certain nonlinear activations, or derivation of modified local conditions for nonlinear networks.

### Open Question 3
- Question: How does the adaptive step size scheduler compare empirically to standard adaptive methods like Adam or RMSprop on practical deep learning tasks?
- Basis in paper: [inferred] The paper proposes an adaptive scheduler (equation 19) and compares only to Xu et al. (2023) and backtracking line search. No comparison to widely-used adaptive optimizers is provided.
- Why unresolved: The theoretical framework assumes specific structure (two-layer linear, strongly convex loss) that differs from practical deep learning scenarios. The computational overhead of computing κ(T_t) at each iteration may be prohibitive.
- What evidence would resolve it: Empirical comparison on standard benchmarks showing whether the theoretically-motivated scheduler provides practical benefits over existing adaptive methods.

## Limitations

- The local PL and smoothness constants depend on weight-dependent operators whose singular values may degrade during training, potentially violating convergence guarantees
- The adaptive step size scheduler requires solving nonlinear equations (101-102) for η₀, which may be numerically unstable for ill-conditioned problems
- The theoretical framework assumes squared loss; extension to other loss functions requires re-deriving all operator bounds

## Confidence

- **High Confidence**: Local PL condition derivation (Theorem 3.1) and its relationship to operator singular values; width-dependent convergence improvement (Theorem 3.3)
- **Medium Confidence**: Adaptive step size scheduler performance claims; imbalance preservation bounds (Lemma E.2)
- **Low Confidence**: Extension to non-squared loss functions; numerical stability of η₀ selection across different problem scales

## Next Checks

1. **Numerical validation of imbalance bounds**: Implement Lemma E.2 for various initialization schemes and monitor ||D(t) - D(0)||_F throughout training to verify O(η²₀) growth prediction

2. **Cross-dataset generalization test**: Apply the method to non-synthetic datasets (e.g., MNIST with linear classifier) and measure whether κ(T_t) remains bounded as predicted

3. **Alternative loss robustness**: Test the framework with logistic loss and Huber loss to verify whether the local PL and smoothness bounds extend beyond squared loss with minimal modification