---
ver: rpa2
title: A Systematic Assessment of Language Models with Linguistic Minimal Pairs in
  Chinese
arxiv_id: '2411.06096'
source_url: https://arxiv.org/abs/2411.06096
tags:
- chinese
- pairs
- minimal
- length
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ZhoBLiMP, the largest linguistic minimal
  pair benchmark for Chinese with over 100 paradigms covering diverse syntactic phenomena.
  The authors train a suite of Chinese language models (Zh-Pythia) from scratch with
  varying parameters and tokenizers to study learning patterns.
---

# A Systematic Assessment of Language Models with Linguistic Minimal Pairs in Chinese

## Quick Facts
- **arXiv ID**: 2411.06096
- **Source URL**: https://arxiv.org/abs/2411.06096
- **Reference count**: 22
- **Key outcome**: ZhoBLiMP benchmark shows anaphor, quantifiers, and ellipsis are challenging for LMs even at 32B parameters; SLLN-LP metric reduces length bias.

## Executive Summary
This paper introduces ZhoBLiMP, the largest linguistic minimal pair benchmark for Chinese, covering 15 syntactic phenomena across 118 paradigms. The authors train a suite of Chinese language models (Zh-Pythia) from scratch with varying parameters to study learning patterns. They propose a new metric, sub-linear length normalized log-probabilities (SLLN-LP), to mitigate biases from unequal sentence lengths in minimal pairs. Using SLLN-LP, they find that certain syntactic phenomena like anaphor and quantifiers remain difficult even for large models, while others saturate at small scales. The work demonstrates that careful metric design is crucial for fair evaluation of linguistic competence.

## Method Summary
The study generates 35k Chinese minimal pairs using template-based generation with a vocabulary system, relaxing the equal-length constraint from previous benchmarks. They train Pythia-based models (14M-1.4B params) from scratch on Chinese text corpora (10M-3B tokens) using various tokenizers. Evaluation employs SLLN-LP, a sub-linear length normalization of log-probabilities that interpolates between raw and mean log-probabilities. The optimal exponent α≈0.5 is determined empirically for Chinese. Learning curves are analyzed across different phenomena to identify which structures are intrinsically difficult versus those that saturate quickly.

## Key Results
- SLLN-LP successfully mitigates length bias across Chinese, English, Dutch, and Japanese benchmarks
- Anaphor, quantifiers, and ellipsis phenomena remain challenging even for 32B parameter models
- Simple syntactic phenomena (argument structure, control) plateau at 160M parameters with modest data
- U-shaped learning curves observed for certain phenomena like control/raising

## Why This Works (Mechanism)

### Mechanism 1: Sub-linear Length Normalization (SLLN-LP)
- **Claim:** Using a sub-linear exponent for length normalization reduces evaluation bias in minimal pair benchmarks where sentences differ in token count.
- **Mechanism:** Standard log-probability favors short sequences, while mean LP over-penalizes short ones. SLLN-LP calculates LP(x) / |x|^α where α∈(0,1), finding a sweet spot (optimal α≈0.5 for Chinese) where length disparity minimally impacts accuracy.
- **Core assumption:** Primary noise stems from length mismatch rather than semantic implausibility.
- **Evidence anchors:** Length bias reduction shown across four languages; optimal α empirically determined.

### Mechanism 2: Targeted Syntactic Data Scaling
- **Claim:** Syntactic competence plateaus at small model sizes for most phenomena, with complex dependencies requiring massive scale.
- **Mechanism:** Simple syntactic rules saturate quickly via next-token prediction. Long-distance binding and pragmatic inference (e.g., ziji reflexives) scale poorly, suggesting models lack inductive bias for non-local dependencies.
- **Core assumption:** Difficulty is intrinsic to linguistic structure, not template artifacts.
- **Evidence anchors:** 160M models achieve 87% accuracy on easy phenomena, comparable to 7B models; anaphor/quantifiers remain difficult at 32B.

### Mechanism 3: Template-based Minimal Pair Generation
- **Claim:** Vocabulary-driven templates efficiently generate validated minimal pairs for diverse paradigms.
- **Mechanism:** Linguists define Good/Bad templates using Lexical, Direct, and Matched rules. A vocabulary with tagged properties fills slots, creating controlled contrasts without hand-writing sentences.
- **Core assumption:** Native speaker intuition is reliable, and templates maintain naturalness.
- **Evidence anchors:** 93.9% agreement with gold labels; validated methodology consistent with BLiMP approach.

## Foundational Learning

- **Concept: Minimal Pair Paradigm (MPP)**
  - **Why needed here:** Fundamental unit of evaluation; forces choice between grammatical and ungrammatical sentences based on probability.
  - **Quick check question:** If Model A assigns P("He likes cheese") = 0.1 and P("He like cheese") = 0.2, does it pass the minimal pair test for subject-verb agreement?

- **Concept: Linking Functions**
  - **Why needed here:** Core contribution is a new linking function (SLLN-LP) that converts raw model scores to predictions.
  - **Quick check question:** Why is raw log-probability insufficient when comparing a 5-word sentence to a 10-word sentence?

- **Concept: Tokenization Granularity**
  - **Why needed here:** Paper varies tokenizers and defines "length" in tokens; bias SLLN-LP fixes stems from tokenization differences.
  - **Quick check question:** Does a character-level tokenizer result in longer or shorter sequence lengths (in tokens) compared to a BPE tokenizer for the same Chinese sentence?

## Architecture Onboarding

- **Component map:** ZhoBLiMP Generator (GUI + Vocabulary DB) -> Generates 35k pairs -> Zh-Pythia Suite (14M-1.4B params) -> Trained on Chinese corpus -> Evaluator (SLLN-LP) -> Accuracy score

- **Critical path:** 1. Define linguistic paradigm 2. Generate pairs using ZhoBLiMP templates 3. Train/Fine-tune Zh-Pythia 4. Evaluate using SLLN-LP (α=0.5) 5. Analyze specific failures

- **Design tradeoffs:** Relaxed equal-length constraint for linguistic breadth vs. evaluation simplicity; SLLN-LP simplicity vs. ignoring token frequency

- **Failure signatures:** Length bias (high accuracy on D_< but low on D_>); U-shaped learning (performance dips mid-training)

- **First 3 experiments:**
  1. Evaluate existing LM on ZhoBLiMP using raw LP vs. SLLN-LP to reproduce length-bias finding
  2. Sweep α (0.0 to 1.0) for SLLN-LP to verify optimal α≈0.4-0.6 holds for different architectures
  3. Train small (14M) and medium (160M) models on provided corpus and compare accuracy curves on Anaphor vs. Argument Structure

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What specific factors drive U-shaped learning curves in Chinese syntax acquisition for phenomena like FCI LICENSING?
- **Basis:** Authors document U-shaped learning but state future work should investigate causes of temporary performance degradation.
- **Why unresolved:** Current study identifies trajectories but not internal model dynamics causing over-generalization errors.
- **Evidence needed:** Probing classifiers or error analysis on intermediate checkpoints to identify incorrectly applied linguistic rules during dip phase.

### Open Question 2
- **Question:** How do LMs differentially process Chinese bare reflexives (ziji) versus compound reflexives (ta-ziji) compared to human competence?
- **Basis:** ANAPHOR is uniquely difficult in Chinese; authors note need for detailed comparison between reflexive types.
- **Why unresolved:** ZhoBLiMP relies on gender-marked compound reflexives, leaving bare reflexive handling largely unexplored.
- **Evidence needed:** Targeted evaluation suite testing binding domains and locality constraints for both ziji and ta-ziji.

### Open Question 3
- **Question:** To what extent do semantic implausibility and syntactic ill-formedness independently contribute to model errors in ZhoBLiMP?
- **Basis:** Authors acknowledge templates conflate semantic anomaly with syntactic violation in some paradigms.
- **Why unresolved:** Template-based generation makes it difficult to determine if low accuracy stems from lack of syntactic knowledge or semantic processing failures.
- **Evidence needed:** Controlled experiment varying semantic plausibility and syntactic well-formedness factorially to isolate individual effects.

## Limitations

- SLLN-LP optimal α=0.5 may not generalize across languages or model architectures without retuning
- Template-based generation may produce unnatural sentences that confound acceptability judgments
- Learning curve conclusions based on from-scratch training rather than fine-tuning, which may not reflect real-world practices
- Saturation points for easy phenomena could be influenced by corpus size or architecture limitations rather than inherent difficulty

## Confidence

- **High Confidence**: Length bias demonstration using SLLN-LP is well-supported across four languages
- **Medium Confidence**: Claim that anaphor/quantifiers are intrinsically difficult is plausible but could be influenced by template quality
- **Low Confidence**: Generalizability of α=0.5 optimal value across different languages and tokenizers is asserted but not thoroughly validated

## Next Checks

1. **Cross-Architectural Alpha Tuning**: Test SLLN-LP with α=0.5 on non-Pythia architectures (Llama, Mistral) and different Chinese tokenizers to verify generalizability or need for per-architecture tuning

2. **Template Quality Analysis**: Conduct targeted native speaker evaluations on sentences from most challenging paradigms (Anaphor, Quantifiers) to identify whether errors stem from template limitations vs. true linguistic difficulty

3. **Fine-tuning vs. From-Scratch Comparison**: Train medium-sized model (160M-1B) using both from-scratch and fine-tuning approaches on same corpus, then compare learning curves on easy vs. difficult phenomena to assess architecture dependence of observed saturation points