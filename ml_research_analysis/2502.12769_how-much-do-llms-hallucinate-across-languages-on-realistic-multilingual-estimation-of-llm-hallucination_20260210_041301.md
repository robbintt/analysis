---
ver: rpa2
title: How Much Do LLMs Hallucinate across Languages? On Realistic Multilingual Estimation
  of LLM Hallucination
arxiv_id: '2502.12769'
source_url: https://arxiv.org/abs/2502.12769
tags:
- hallucination
- languages
- language
- tokens
- silver
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work quantifies LLM hallucination rates across 30 languages
  in realistic knowledge-intensive open-domain QA settings. A multilingual hallucination
  detection model is trained by translating an English benchmark and fine-tuning a
  large LLM.
---

# How Much Do LLMs Hallucinate across Languages? On Realistic Multilingual Estimation of LLM Hallucination

## Quick Facts
- **arXiv ID:** 2502.12769
- **Source URL:** https://arxiv.org/abs/2502.12769
- **Reference count:** 40
- **Primary result:** Smaller LLMs and those supporting more languages hallucinate significantly more; silver annotations can proxy for gold in rate estimation.

## Executive Summary
This paper quantifies hallucination rates across 30 languages by training a multilingual hallucination detection model via translation-train transfer from an English benchmark. The authors estimate rates by adjusting raw detector outputs with precision and recall estimates from silver (synthetic) and gold (human-annotated) evaluation data. They find that smaller models and those with broader language support hallucinate disproportionately, and that synthetic silver annotations yield similar rate estimates to gold data for five high-resource languages.

## Method Summary
The authors train a multilingual hallucination detection model by translating an English hallucination benchmark (FAVA) into 30 languages using NLLB-200, then fine-tuning a bidirectional version of Llama-3-8B-base with QLoRA adapters. They generate synthetic evaluation data (MFAVA-Silver) using GPT-4 and collect gold human annotations for five languages. Hallucination rates are estimated by adjusting detector outputs with precision and recall metrics, applied across 11 LLMs on 51,133 prompts grounded in filtered Wikipedia articles.

## Key Results
- Smaller LLMs show disproportionate hallucination rate increases with broader language support (β=1.33, p<0.001).
- Response length correlates with absolute hallucinations but not per-token rates.
- Silver (synthetic) annotations yield similar rate estimates to gold (human) annotations (r=0.83, p=1.26e-04) for five high-resource languages.
- Binary detection F1 scores are significantly higher than category detection across all languages (e.g., Arabic Gold: Binary 61.6 vs. Category 37.2).

## Why This Works (Mechanism)

### Mechanism 1
Adjusting raw hallucination detections by detector precision and recall yields reliable cross-lingual hallucination rate estimates. The formula HRest,l = (Pl × H_det,l) / (Rl × Nl) normalizes for false positives and false negatives using TP/FP/FN decomposition. Assumes detector performance generalizes from evaluation data to real-world outputs.

### Mechanism 2
Smaller LLMs exhibit disproportionately higher hallucination rates when supporting many languages or generating longer responses. Limited capacity forces compression of multilingual knowledge, increasing interference. Linear mixed-effects modeling confirms Model Size × Language Support (β=1.33, p<0.001) and Model Size × Response Length (β=-1.02, p=0.001) interactions.

### Mechanism 3
Synthetic silver annotations can proxy for gold human annotations in hallucination rate estimation. While silver F1 scores exceed gold (80.7 vs. 64.7 average), the Pl/Rl ratio remains consistent, yielding correlated HRest,l estimates. Assumes GPT-4-generated hallucinations are realistic enough for detector Pl/Rl ratios to transfer.

## Foundational Learning

- **Span-level token classification with I-O labeling**: Hallucination detection is framed as discriminative token classification requiring bidirectional context. Why needed: to identify hallucinated spans at token level. Quick check: Can you explain why I-O labeling was chosen over BIO for hallucinated spans?

- **Translation-train cross-lingual transfer**: The multilingual detector is trained on machine-translated English hallucination data for 30 languages. Why needed: to scale detection across languages without collecting new annotations. Quick check: What noise does translation-train introduce, and why is it acceptable here?

- **Bidirectional fine-tuning of decoder-only LLMs**: Converting Llama-3-8B (decoder) to an encoder via future-token mask removal improves span detection. Why needed: bidirectional context helps identify hallucinated spans. Quick check: Why does bidirectional context help hallucination span detection?

## Architecture Onboarding

- **Component map**: Wikipedia articles (depth ≥5, length ≥2000 chars) -> 51,133 queries -> LLM responses -> Hallucination Detection Model (Llama-3-8B-base, bidirectional, I-O classification) -> H_det,l -> Pl/Rl from MFAVA -> HRest,l rate

- **Critical path**: Silver validation (5 gold languages) -> Pl/Rl estimation (30 languages) -> Rate computation (11 LLMs × 30 languages)

- **Design tradeoffs**:
  - Bidirect vs. Causal: Bidirect improves F1 but requires modifying decoder architecture
  - Silver vs. Gold: Gold is expensive ($4,581 for 5 languages); silver scales but may not capture realistic patterns
  - Binary vs. Category detection: Category provides finer-grained insight but F1 is much lower (e.g., Arabic Gold: Binary 61.6, Category 37.2)

- **Failure signatures**:
  - Low Pl/Rl ratio stability → HRest,l estimates diverge between silver and gold
  - High repetitive/don't-know responses → reduced effective sample size (10% observed)
  - Short reference articles → may not support knowledge-intensive queries

- **First 3 experiments**:
  1. Replicate silver-gold correlation analysis on your target language(s) before relying on silver-based estimates
  2. Ablate bidirectional vs. causal fine-tuning on your hallucination span detection task
  3. Test whether Pl/Rl ratios hold when detector is applied to a different domain (e.g., non-Wikipedia references)

## Open Questions the Paper Calls Out

- **Open Question 1**: How can fine-grained hallucination type taxonomies and detection models be improved for multilingual settings? The study found low Inter-Annotator Agreement (IAA) for hallucination types and poor model F1 scores, rendering fine-grained classification unreliable.

- **Open Question 2**: Does the correlation between broader language support and higher hallucination rates stem from training data dilution or limited model capacity? The analysis identifies the correlation but does not isolate specific architectural or training data factors driving the increased hallucination rate.

- **Open Question 3**: Can hallucination detectors trained on synthetic data (generated by GPT-4) generalize to "natural" failure modes of other LLMs? There is a distribution shift between synthetic training data and human-annotated Gold data; it is unclear if synthetic data captures the full spectrum of natural hallucinations.

## Limitations

- Silver-gold proxy validity: The assumption that silver GPT-4-generated hallucinations provide reliable rate estimates for languages without gold validation remains untested beyond five high-resource languages.

- Detector generalization: The precision/recall adjustment mechanism assumes detector performance on evaluation data generalizes to real-world LLM outputs across domains and architectures.

- Translation-train quality: NLLB-200 translation introduces potential noise, particularly for lower-resource languages, but the impact on detector performance and rate estimates is not fully characterized.

## Confidence

- **High Confidence**: The mathematical framework for rate adjustment is internally consistent and well-justified; the empirical finding that smaller models hallucinate more is robust across 11 LLMs and 30 languages.

- **Medium Confidence**: The silver-gold correlation analysis is methodologically sound, but generalization beyond five validated languages requires further testing; bidirectional fine-tuning shows clear F1 improvements but long-term stability is uncertain.

- **Low Confidence**: The claim that response length correlates with absolute hallucinations but not per-token rates requires more granular analysis; underlying mechanisms for model size interactions with language support remain speculative.

## Next Checks

1. **Cross-Domain Detector Validation**: Apply the hallucination detection model to LLM outputs from a different domain (e.g., scientific literature, news articles) and measure whether precision/recall estimates remain stable.

2. **Lower-Resource Language Analysis**: Manually validate hallucination rates for 3-5 lower-resource languages where silver-gold correlation hasn't been established and compare against silver-based estimates.

3. **Training Data Composition Analysis**: For a subset of smaller multilingual models, analyze actual language distribution in training data rather than relying on declared language support and recompute model size interaction effects.