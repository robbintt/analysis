---
ver: rpa2
title: 'Preference Alignment on Diffusion Model: A Comprehensive Survey for Image
  Generation and Editing'
arxiv_id: '2502.07829'
source_url: https://arxiv.org/abs/2502.07829
tags:
- diffusion
- preference
- image
- reward
- alignment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides the first comprehensive survey of preference
  alignment techniques for diffusion models (DMs) in image generation and editing.
  It systematically reviews optimization methods like RLHF and DPO, explores applications
  across medical imaging, robotics, and autonomous driving, and identifies key challenges
  such as computational efficiency and preference modeling.
---

# Preference Alignment on Diffusion Model: A Comprehensive Survey for Image Generation and Editing

## Quick Facts
- arXiv ID: 2502.07829
- Source URL: https://arxiv.org/abs/2502.07829
- Reference count: 9
- This paper provides the first comprehensive survey of preference alignment techniques for diffusion models in image generation and editing.

## Executive Summary
This survey systematically reviews preference alignment methods for diffusion models (DMs), categorizing approaches by sampling strategy, training method, reward feedback, and modality. It introduces RLHF and DPO optimization frameworks, explores applications across medical imaging, robotics, and autonomous driving, and identifies key challenges including computational efficiency and preference modeling. The work highlights how integrating human feedback with DMs enhances alignment with user preferences while addressing issues like text-image misalignment and aesthetic quality.

## Method Summary
The survey examines three primary preference alignment approaches: RLHF with PPO policy gradient optimization using clipped surrogate objectives, DPO which directly optimizes preference pairs without explicit reward models, and direct reward backpropagation methods like DRaFT and AlignProp that propagate gradients through differentiable reward models. Implementation requires a pretrained text-to-image diffusion model (typically Stable Diffusion), preference datasets with human/AI annotations, and reward model training. Methods can be online (sampling during training) or offline (using pre-collected data), with parameter-efficient techniques like LoRA offering memory advantages.

## Key Results
- Introduces a taxonomy categorizing preference alignment approaches by sampling strategy, training method, reward feedback, and modality
- Identifies computational efficiency and preference modeling as primary challenges in diffusion model alignment
- Highlights the tradeoff between online adaptation (higher compute, better adaptation) and offline methods (faster, limited by dataset coverage)

## Why This Works (Mechanism)

### Mechanism 1
Reframing the diffusion denoising process as a Markov Decision Process enables policy optimization for preference alignment. Each denoising timestep becomes a state transition, the noise prediction acts as a stochastic policy, and the reward is computed on the final generated image. The policy gradient flows through the entire denoising chain.

### Mechanism 2
Direct Preference Optimization bypasses explicit reward modeling by reparameterizing the RL objective into a supervised classification loss on preference pairs. Given a preference dataset with winning/losing samples, DPO directly optimizes the likelihood ratio between preferred and dispreferred outputs without training a separate reward model.

### Mechanism 3
Direct reward backpropagation through differentiable reward models provides faster convergence than RL-based approaches but risks over-optimization. Instead of sampling and policy gradient estimation, gradients flow directly from the reward model through the entire denoising process.

## Foundational Learning

- **Concept: Markov Decision Processes and Policy Gradients**
  - Why needed here: The entire RLHF framing depends on understanding states, actions, policies, and how policy gradients propagate rewards back through sequential decisions.
  - Quick check question: Can you explain why the denoising timestep formulation satisfies the Markov property, and what the advantage function At would represent in this context?

- **Concept: Variational Lower Bounds and ELBO**
  - Why needed here: Diffusion model training fundamentally minimizes a variational bound on negative log-likelihood; understanding this clarifies what preference optimization is modifying.
  - Quick check question: How does adding a reward term to the diffusion objective affect the ELBO, and what does this imply about the tradeoff between preference alignment and sample diversity?

- **Concept: KL Divergence Regularization**
  - Why needed here: Both DPO and regularized RLHF (DPOK) use KL penalties to prevent the fine-tuned model from deviating too far from the pretrained reference.
  - Quick check question: If the KL penalty coefficient βreg is set too high or too low, what failure modes would you expect in generated image quality?

## Architecture Onboarding

- **Component map:** Prompt Input → Text Encoder → Latent Diffusion Model → VAE Decoder → Generated Image

- **Critical path:** Start with a pretrained text-to-image diffusion model (Stable Diffusion recommended). Implement reward model training on preference pairs first—this is required for RLHF and useful for evaluation even if you ultimately use DPO. Then implement PPO-based fine-tuning before attempting DPO, as PPO exposes more debugging surface area.

- **Design tradeoffs:**
  - Online vs. Offline: Online methods (DDPO, DPOK) require sampling during training (higher compute, better adaptation); offline methods (Diffusion-DPO) use pre-collected data (faster, limited by dataset coverage)
  - Human vs. AI annotation: Human feedback is higher quality but expensive; AI feedback (LLM/LVLM judges) scales but may propagate model biases
  - Full fine-tuning vs. LoRA: LoRA reduces memory and enables mixing multiple preference-tuned adapters; full fine-tuning may achieve higher peak performance

- **Failure signatures:**
  - Reward hacking: Images score high on reward model but look worse to humans (check by held-out human evaluation)
  - Mode collapse: Diversity drops sharply; all outputs converge to a narrow style (monitor CLIP diversity metrics)
  - Prompt forgetting: Model loses ability to follow prompts outside the preference fine-tuning distribution (test on held-out prompts)
  - Training instability: PPO specifically can diverge; watch for surrogate objective spikes

- **First 3 experiments:**
  1. Reproduce DDPO or Diffusion-DPO on a small-scale preference dataset (e.g., 1000 ranked image pairs) with a single reward dimension (aesthetic score only). Validate that the reward improves without catastrophic forgetting of prompt following.
  2. Compare online vs. offline data collection: train with fixed preference pairs vs. continuously collecting new samples and rankings. Measure sample efficiency and final reward.
  3. Ablate KL penalty strength: systematically vary βreg and plot the tradeoff curve between reward achievement and KL divergence from the reference model. Identify the operating point where quality degradation begins.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can diffusion models maintain semantic consistency while simultaneously optimizing for multiple conflicting objectives, such as image quality, text alignment, and aesthetic appeal?
- Basis in paper: [explicit] Section 5 states that "maintaining semantic consistency while optimizing for multiple diverse objectives—such as image quality, text alignment, and aesthetic appeal—remains difficult."
- Why unresolved: Optimization algorithms often face trade-offs where maximizing a specific reward (e.g., aesthetics) leads to hallucinations or degradation in text alignment.
- What evidence would resolve it: Development of a multi-objective optimization framework that achieves Pareto efficiency across all metrics without sacrificing semantic fidelity.

### Open Question 2
- Question: Can parameter-efficient fine-tuning techniques (e.g., LoRA) be effectively adapted to reduce the high computational burden of RLHF without sacrificing alignment performance?
- Basis in paper: [explicit] Section 5 identifies the "computational burden of preference fine-tuning" as a primary challenge and suggests "exploring parameter-efficient approaches like LoRA" as a future direction.
- Why unresolved: While LoRA reduces storage, it is unclear if low-rank updates capture the complex gradient dynamics required for robust reinforcement learning from human feedback.
- What evidence would resolve it: Benchmarks demonstrating that parameter-efficient RLHF achieves comparable reward scores and convergence speeds to full-parameter fine-tuning with significantly lower GPU memory usage.

### Open Question 3
- Question: How can reward functions be designed to reliably capture and reproduce the subjective and diverse nature of human preferences at scale?
- Basis in paper: [explicit] Section 5 notes that the "subjective nature of human preferences also makes it challenging to design effective reward functions that can reliably capture and reproduce these preferences in scale."
- Why unresolved: Scalar reward functions often fail to represent the nuanced, multi-modal distribution of human aesthetic values, leading to "reward hacking" or generic outputs.
- What evidence would resolve it: A reward modeling architecture capable of representing diverse preference distributions rather than single scalar values, validated against varied human demographic groups.

### Open Question 4
- Question: How can Large Language Models (LLMs) be better integrated to interpret and implement complex human preferences in multimodal image generation?
- Basis in paper: [explicit] Section 5 highlights "improving the integration of multiple modalities" and suggests "taking the use of LLMs to better interpret and implement human preferences in image generation."
- Why unresolved: Effectively translating abstract linguistic nuances from LLMs into precise visual guidance signals for the diffusion process remains an alignment challenge.
- What evidence would resolve it: A unified multimodal framework where an LLM provides explicit intermediate reasoning or visual grounding that directly improves the alignment score of the generated image.

## Limitations
- The survey lacks detailed implementation specifications for many approaches, with critical hyperparameters inconsistently reported across surveyed methods
- Evaluation metrics used in preference alignment remain heterogeneous across studies, with no standardized benchmark established for comparing different alignment strategies
- While computational efficiency is identified as a key challenge, quantitative comparisons of training/inference costs across methods are sparse in the surveyed literature

## Confidence
- **High confidence**: The core taxonomy categorizing approaches by sampling strategy, training method, reward feedback, and modality is well-supported by the surveyed literature
- **Medium confidence**: Claims about computational efficiency advantages of DPO over RLHF are supported by cited work but would benefit from direct head-to-head benchmarks
- **Low confidence**: Assertions about specific failure modes like reward hacking and mode collapse lack systematic empirical characterization across the surveyed papers

## Next Checks
1. **Empirical hyperparameter sweep**: Systematically vary KL penalty coefficient βreg across a range (0.1 to 10.0) for DPO fine-tuning on a standardized preference dataset to identify optimal operating points and quantify the tradeoff between reward achievement and model drift.

2. **Cross-method benchmark**: Implement 2-3 representative approaches (e.g., Diffusion-DPO, DDPO, and DRaFT) on the same base model and preference dataset, measuring convergence speed, final reward achievement, and diversity preservation under identical computational constraints.

3. **Robustness to dataset size**: Evaluate preference alignment performance using progressively smaller subsets of the same preference dataset (100, 500, 1000, 5000 pairs) to quantify sample efficiency and identify minimum viable dataset sizes for each approach.