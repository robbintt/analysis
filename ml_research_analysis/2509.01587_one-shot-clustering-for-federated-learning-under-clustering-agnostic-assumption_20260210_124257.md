---
ver: rpa2
title: One-Shot Clustering for Federated Learning Under Clustering-Agnostic Assumption
arxiv_id: '2509.01587'
source_url: https://arxiv.org/abs/2509.01587
tags:
- clustering
- learning
- local
- federated
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel clustering method for Federated Learning
  that can automatically detect when to cluster without any hyperparameters. The key
  idea is to use the cosine distance between local gradients and a temperature measure
  to detect convergence and trigger clustering.
---

# One-Shot Clustering for Federated Learning Under Clustering-Agnostic Assumption

## Quick Facts
- arXiv ID: 2509.01587
- Source URL: https://arxiv.org/abs/2509.01587
- Authors: Maciej Krzysztof Zuziak; Roberto Pellungrini; Salvatore Rinzivillo
- Reference count: 12
- One-line primary result: OCFL with density-based clustering achieves Rand Index ~0.95-0.96 on MNIST/FMNIST and F1-scores 0.96-0.65 vs 0.24-0.60 for baselines

## Executive Summary
This paper proposes OCFL (One-Shot Clustering for Federated Learning), a method that automatically detects when to cluster clients in federated learning without requiring prior knowledge of cluster count. The key innovation is a temperature-based convergence detection mechanism that triggers clustering when the global model begins to diverge due to data heterogeneity. The approach combines cosine distance between local gradients with a temperature measure to identify optimal clustering points, achieving superior clustering performance and personalization compared to state-of-the-art methods while maintaining generalization capability.

## Method Summary
OCFL monitors the cosine distance between local gradients to build a Divergence Matrix, then computes a temperature function as the p-norm of this matrix to detect convergence. When temperature rises (indicating the global model has exhausted shared learning capacity), OCFL triggers one-shot clustering using density-based algorithms like HDBSCAN or Mean-Shift. The method operates in two phases: initial global training until the temperature trigger, then cluster-specific training without further global aggregation. This approach eliminates the need for hyperparameters like cluster count while providing personalized models that outperform global baselines.

## Key Results
- OCFL achieves Rand Index scores of 0.95-0.96 on MNIST and FMNIST datasets
- On CIFAR10, OCFL achieves Rand Index of 0.80 with personalized F1-scores between 0.96-0.65
- Baseline methods achieve Rand Index of 0.24-0.60 and F1-scores 0.24-0.60
- OCFL provides more cohesive and interpretable local explanations compared to non-personalized models

## Why This Works (Mechanism)

### Mechanism 1: Temperature-Based Convergence Detection
The "Clustering Temperature Function" detects optimal partitioning by signaling when the global model diverges due to data heterogeneity. The system computes a Divergence Matrix containing pairwise cosine distances between client gradients and calculates a scalar "Temperature" using the p-norm. When temperature rises after initial descent, it indicates the global model has exhausted shared learning capacity and is fitting local noise or conflicting distributions, triggering clustering.

### Mechanism 2: Gradient Cosine Distance as Distribution Proxy
Cosine distance between flattened parameter updates serves as a sufficient proxy for underlying data distribution similarity. Instead of sharing data, clients share model updates, and cosine distance quantifies alignment of learning directions. Low distance implies clients are climbing the same loss surface, while high distance indicates different optimization objectives.

### Mechanism 3: Density-Based Cluster Identification
Density-based clustering algorithms applied to the Divergence Matrix identify cohesive client cohorts without requiring cluster count as a prior hyperparameter. Once triggered by the Temperature function, OCFL treats the Divergence Matrix as feature space and uses HDBSCAN to identify dense regions of similar clients, automatically determining cluster numbers and isolating outliers.

## Foundational Learning

- **Clustered Federated Learning (CFL)**
  - Why needed: OCFL is a specific instantiation of CFL. Understanding the goal of moving beyond single global models to personalized models for heterogeneous data silos is crucial.
  - Quick check: Why does a standard global FedAvg model fail when data is Non-IID across clients?

- **Cosine Similarity vs. Euclidean Distance**
  - Why needed: The paper uses cosine distance to build the Divergence Matrix. Understanding that cosine measures directional alignment (angle) rather than magnitude is crucial for interpreting why gradients with different norms but similar updates are clustered together.
  - Quick check: If Client A updates weights by ×1000 and Client B by ×1, but in the exact same direction, what would their Cosine Distance be?

- **Model Convergence & Divergence Dynamics**
  - Why needed: OCFL's core logic relies on observing the "U-curve" or rise in the Temperature function. Understanding why shared learning converges first (low temperature) before personalized needs diverge (rising temperature) is essential.
  - Quick check: In a heterogeneous system, why might gradient disagreement increase after the model learns the basic "shared" features of the dataset?

## Architecture Onboarding

- **Component map:** Client Side: Local Trainer (SGD/Adam) → Delta Calculator (Δθ). Server Side: Divergence Matrix Builder (Cosine Distance) → Temperature Monitor (L-p norm) → Clustering Engine (HDBSCAN/Mean-Shift) → Cluster-wise Aggregator

- **Critical path:**
  1. Initialization: Start with all clients in single cluster
  2. Warm-up: Run standard FL updates while monitoring Temperature function
  3. Trigger: Detect first rise in Temperature (Tt ≥ Tt-1)
  4. One-Shot Clustering: Compute Divergence Matrix on current round's gradients; run HDBSCAN to assign cluster IDs
  5. Bifurcation: Cease global aggregation; aggregate models only within newly defined clusters for remaining rounds

- **Design tradeoffs:**
  - Trigger Sensitivity: Uses simple switch (Tt ≥ Tt-1) to trigger; moving average would be more robust to noise but slower to react
  - One-Shot vs. Iterative: OCFL clusters once to save compute/communication; cannot handle concept drift or dynamic client entry effectively without restarting
  - Full Gradient vs. Last Layer: OCFL flattens entire model rather than just output layer, increasing communication cost slightly but improving signal fidelity

- **Failure signatures:**
  - Premature Clustering: High learning rate may cause temperature to oscillate early, triggering clustering before shared features are learned
  - Model Collapse: On complex datasets, temperature might drop to 0, causing gradients to become indistinguishable and clustering to fail

- **First 3 experiments:**
  1. Visualize the Trigger: Plot Temperature Function over 50 rounds on MNIST (Non-overlapping split) to verify local minimum and subsequent rise
  2. Clustering Algorithm Ablation: Run OCFL using K-Means vs. HDBSCAN on "Overlapping Balanced" split to observe how density-based methods handle ambiguous boundaries
  3. Explainability Validation: Generate GradCAM heatmaps for personalized model vs. Global Baseline on BloodMNIST dataset to confirm personalization yields more cohesive attention maps

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the asymptotic drop of the Temperature Function to zero observed in CIFAR-10 experiments indicate a total network collapse where gradients become uninformative?
- Basis: Section 4.6.2 theorizes that temperature dropping to 0 implies either identical local models despite data incongruence or "total network collapse" where gradients are uninformative
- Why unresolved: Paper observes behavior empirically but exact theoretical cause remains unclear and requires further investigation
- What evidence would resolve it: Theoretical analysis or ablation study monitoring gradient magnitude and variance in deep networks under high data incongruence to confirm if backpropagation fails to escape local minima

### Open Question 2
- Question: Would employing a moving average of the Temperature Function over N rounds provide more robust clustering triggers than single-iteration switch condition?
- Basis: Section 6.3 notes that while current one-iteration switch works, moving average might be "even more beneficial," particularly for datasets with irregular temperature patterns or noise
- Why unresolved: Authors implemented static switch condition to prove concept, leaving moving average as limitation and future direction
- What evidence would resolve it: Comparative experiments running OCFL with sliding window temperature trigger against current method on datasets known for volatile gradient convergence

### Open Question 3
- Question: How does personalization delivered by OCFL impact susceptibility to privacy risks, such as Membership Inference Attacks (MIA)?
- Basis: Section 6.1 states "subsequent studies should focus on that particular problem, addressing issues such as the impact of personalisation on the susceptibility to privacy risks"
- Why unresolved: Paper focused on personalization performance and explainability, explicitly noting privacy-related issues were not primary concern
- What evidence would resolve it: Execute standard privacy attacks (e.g., MIA) on personalized cluster models versus global baseline to measure if clustering increases risk of singling out specific clients

## Limitations
- Temperature function reliability is inconsistent across datasets, dropping to zero on CIFAR-10 which potentially breaks the entire clustering pipeline
- One-shot nature means method cannot adapt to concept drift or dynamic client populations without complete retraining
- Method's claim of being "clustering-agnostic" is somewhat overstated since it still requires careful tuning of divergence matrix computation

## Confidence
- **High Confidence:** Experimental results on MNIST and FMNIST are robust and well-documented with convincing Rand Index scores around 0.95-0.96
- **Medium Confidence:** Method's generalization to PathMNIST and BloodMNIST is reasonable but less thoroughly analyzed, with qualitative explainability claims based on GradCAM visualizations
- **Low Confidence:** Temperature function behavior on CIFAR-10 raises serious concerns about method's reliability on complex, real-world datasets

## Next Checks
1. **Temperature Function Robustness Test:** Reproduce temperature curve on CIFAR-10 with varying learning rates and model architectures to identify conditions causing temperature collapse; test whether gradient clipping or alternative normalization schemes prevent temperature from dropping to zero

2. **Dynamic Client Scenario:** Implement streaming client scenario where new clients join after initial clustering; measure how OCFL's static clustering performs compared to adaptive methods like FLUX, particularly focusing on degradation of personalized model performance

3. **Overlapping Distribution Challenge:** Design synthetic dataset with highly overlapping client distributions and measure OCFL's clustering accuracy; compare against DPMM-CFL which explicitly handles nonparametric clustering without relying on gradient divergence signals