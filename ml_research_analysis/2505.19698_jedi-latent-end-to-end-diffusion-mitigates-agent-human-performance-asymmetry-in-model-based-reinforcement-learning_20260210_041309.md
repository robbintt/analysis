---
ver: rpa2
title: 'JEDI: Latent End-to-end Diffusion Mitigates Agent-Human Performance Asymmetry
  in Model-Based Reinforcement Learning'
arxiv_id: '2505.19698'
source_url: https://arxiv.org/abs/2505.19698
tags:
- uni00000014
- uni00000015
- uni00000016
- uni00000013
- uni00000018
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies a significant performance asymmetry in pixel-based
  MBRL agents across the Atari100k benchmark, where agents dramatically outperform
  humans on tasks humans perform poorly (agent-optimal tasks) but underperform on
  tasks where humans excel (human-optimal tasks). The authors hypothesize this asymmetry
  arises from the lack of temporally-structured latent space in pixel-based methods,
  which forces agents to learn representations solely through temporal-difference
  updates without the self-consistency objective present in latent-based approaches.
---

# JEDI: Latent End-to-end Diffusion Mitigates Agent-Human Performance Asymmetry in Model-Based Reinforcement Learning

## Quick Facts
- arXiv ID: 2505.19698
- Source URL: https://arxiv.org/abs/2505.19698
- Authors: Jing Yu Lim; Zarif Ikram; Samson Yu; Haozhe Ma; Tze-Yun Leong; Dianbo Liu
- Reference count: 40
- Primary result: JEDI mitigates agent-human performance asymmetry in pixel-based MBRL by introducing a latent end-to-end diffusion world model trained with self-consistency objective

## Executive Summary
JEDI addresses a significant performance asymmetry in pixel-based model-based reinforcement learning agents on the Atari100k benchmark, where agents dramatically outperform humans on tasks humans perform poorly but underperform on tasks where humans excel. The authors hypothesize this asymmetry arises from the lack of temporally-structured latent space in pixel-based methods, which forces agents to learn representations solely through temporal-difference updates without the self-consistency objective present in latent-based approaches. JEDI proposes a novel latent diffusion world model trained end-to-end with a self-consistency objective inspired by Joint Embedding Predictive Architecture (JEPA), predicting compressed latent representations directly through diffusion denoising without reconstruction or hidden state extraction.

## Method Summary
JEDI is a latent end-to-end diffusion world model that trains an encoder to map observations to compressed latents, then uses a diffusion model to predict the next latent state conditioned on the current latent and action. The system employs a self-consistency objective where the loss minimizes the distance between the prediction and a stop-gradient encoded target, forcing the encoder to learn representations that contain only the information necessary for predicting dynamics. This approach enables action-temporal reasoning while maintaining the expressiveness of diffusion models, operating in a compressed latent space (12× compression) rather than pixels to achieve 3× faster inference and 43% lower memory than pixel-based diffusion baselines.

## Key Results
- Achieves state-of-the-art performance on human-optimal tasks, outperforming baselines by over 2×
- Maintains competitive performance on agent-optimal tasks while mitigating performance asymmetry
- Runs 3× faster with 43% lower memory than the latest pixel-based diffusion baseline
- Successfully avoids "self-sabotaging behaviour" like self-destruction in BankHeist that plagues pixel-based agents

## Why This Works (Mechanism)

### Mechanism 1: End-to-End Latent Self-Consistency
The system uses a Joint Embedding Predictive Architecture (JEPA) style objective where an encoder maps observations to latents, and a diffusion model predicts the next latent state conditioned on the current latent and action. The loss minimizes the distance between the prediction and a stop-gradient encoded target, forcing the encoder to learn representations that contain only the information necessary for predicting dynamics. This trains the encoder while training the latent diffusion model without any reconstruction loss, addressing the hypothesis that performance gaps stem from lack of "temporal structure" in latent spaces learned via TD-errors alone.

### Mechanism 2: Asymmetric Task Generalization
By explicitly training the encoder to predict future states, the agent learns a causal model of action consequences that allows it to handle complex action spaces requiring long-term reasoning. The approach argues this enables the agent to handle tasks like avoiding self-destruction in BankHeist that purely reactive pixel-based agents (like DIAMOND) cannot manage due to lack of explicit temporal prior. This mechanism addresses the performance asymmetry by providing the right inductive bias for different task types.

### Mechanism 3: Latent Diffusion for Efficiency
Operating in a compressed latent space (rather than pixels) allows for significant computational gains and stability. JEDI compresses the observation space (12× compression to 16×8×8 latents) before applying the diffusion process, reducing memory usage by 43% and inference speed by 3×. This efficiency gain comes from denoising abstract vectors rather than high-fidelity images, while maintaining necessary information for decision-making through the encoder's learned representation.

## Foundational Learning

- **World Models & Self-Supervised Learning**: JEDI is not just an RL algorithm; it is a world model trained via self-supervision. You must understand the difference between learning a policy (RL) and learning environmental dynamics (Prediction). Quick check: Can you explain why predicting the next frame (or latent) might help an agent plan, even if no reward is given for that prediction?

- **Joint Embedding Predictive Architecture (JEPA)**: The paper explicitly draws from JEPA/Yann LeCun's work. The core idea is predicting representations from representations, rather than reconstructing pixels. Quick check: Why does the paper use a stop-gradient (sg) on the target latent z_{t+1} during the diffusion loss calculation?

- **Diffusion Models (Score-based)**: JEDI uses a diffusion model as its dynamics predictor (predicting the direction toward the clean latent). You need to grasp the basics of adding noise and learning to denoise. Quick check: In JEDI, does the diffusion model predict the noise, or the clean latent representation directly?

## Architecture Onboarding

- **Component map**: Env step → Encoder Eφ → Store in Buffer → Sample batch → Encode current/target → Stop-Grad on target → Train Diffusion & Reward models → Imagine rollouts → Update Actor-Critic

- **Critical path**: 1) Data Collection: Env step → Encoder → Store in Buffer. 2) World Model Training: Sample batch → Encode current/target → Apply Stop-Grad to target → Train Diffusion & Reward models. 3) Policy Training: Imagine rollouts in latent space using trained diffusion model → Update Actor-Critic via REINFORCE.

- **Design tradeoffs**: Latent vs. Pixel Diffusion - latent is faster but requires tuning encoder to ensure it doesn't discard task-relevant details. Reconstruction-Free - authors remove decoder to focus on "utility" for agent, meaning no decoded images can be visualized, only latent states.

- **Failure signatures**: Representation Collapse - if encoder learning rate is too high or stop-gradient is missing, latents may converge to constant vector. Reward Hacking (Baseline) - pixel-based agents often learn "brittle" policies (e.g., suicide for points). If JEDI exhibits this, temporal structure has failed to enforce long-term reasoning.

- **First 3 experiments**: 1) Encoder Sanity Check - verify Stop-Gradient implementation by training on small dataset; if loss drops but latent variance goes to zero, stop-grad is likely missing. 2) Overfit Single Transition - test diffusion dynamics model on single (s_t, a_t, s_{t+1}) pair; it should perfectly predict s_{t+1} to ensure architecture capacity. 3) Asymmetry Benchmark - run JEDI vs baseline (like DreamerV3) specifically on BankHeist task; monitor for "self-bombing" behavior to see if mechanism holds.

## Open Questions the Paper Calls Out

### Open Question 1
Can the JEDI architecture and the proposed Agent-Optimal/Human-Optimal classification generalize effectively to continuous control domains or 3D environments? The authors explicitly limit their scope to the Atari100k benchmark, validating the method solely on discrete action spaces with 2D visual inputs under a strict interaction limit (100k steps).

### Open Question 2
What underlying factors prevent JEDI from fully closing the performance gap on Human-Optimal tasks, where it still underperforms humans by roughly an order of magnitude? Even though JEDI achieves SOTA performance in Human-Optimal tasks, the performance is still around one order of magnitude smaller than the performance on Agent-Optimal tasks.

### Open Question 3
Is it possible to reduce the inference latency of latent diffusion world models to match single-step latent predictors without sacrificing the temporal reasoning capabilities? The authors acknowledge a limitation: they are slower than existing latent MBRL baselines due to the nature of the multi-step denoising inference with every next state sampled.

## Limitations
- Claims about performance asymmetry mitigation rely heavily on ablation studies and comparisons against a single strong baseline (DIAMOND)
- Analysis does not fully disentangle whether improvements stem from latent representation, diffusion architecture, or self-consistency objective
- Paper doesn't explore alternative explanations such as differences in model capacity, optimization stability, or inductive biases beyond temporal prediction objective

## Confidence
- **High Confidence**: Computational efficiency claims (3× faster, 43% lower memory) are well-supported by architecture design and directly verifiable
- **Medium Confidence**: Performance asymmetry mitigation is credible given ablation studies, but causal mechanism could benefit from additional controlled experiments
- **Medium Confidence**: Qualitative evidence of improved long-term reasoning is persuasive but not systematically quantified across all Human-Optimal tasks

## Next Checks
1. **Ablation Isolation**: Systematically ablate the self-consistency objective, latent space compression, and diffusion architecture separately to quantify each component's contribution to asymmetry mitigation
2. **Temporal Reasoning Benchmark**: Design a synthetic task that explicitly requires long-horizon reasoning (e.g., delayed reward scenarios) to quantitatively measure whether JEDI genuinely improves temporal reasoning versus the baseline
3. **Representation Analysis**: Visualize and quantify the latent space structure learned by JEDI versus pixel-based baselines using techniques like t-SNE or latent traversal to empirically verify the "temporal structure" hypothesis