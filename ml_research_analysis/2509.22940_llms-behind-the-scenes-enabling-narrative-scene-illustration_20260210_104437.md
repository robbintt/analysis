---
ver: rpa2
title: 'LLMs Behind the Scenes: Enabling Narrative Scene Illustration'
arxiv_id: '2509.22940'
source_url: https://arxiv.org/abs/2509.22940
tags:
- story
- scene
- image
- fragment
- criteria
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a pipeline that uses large language models
  (LLMs) as an interface for generating scene illustrations from story text, employing
  text-to-image models to create visual depictions. The pipeline includes story fragmentation,
  scene description generation via LLM, and image generation.
---

# LLMs Behind the Scenes: Enabling Narrative Scene Illustration

## Quick Facts
- arXiv ID: 2509.22940
- Source URL: https://arxiv.org/abs/2509.22940
- Reference count: 40
- Primary result: Human annotators preferred LLM-generated scene captions (73-78% win rate) over raw story text for generating narrative scene illustrations.

## Executive Summary
This paper introduces a pipeline using large language models (LLMs) to generate scene illustrations from narrative text by leveraging text-to-image models. The pipeline includes story fragmentation, scene description generation via LLM, and image generation. Applied to the ROCStories corpus, it produces the SceneIllustrations dataset with 2,990 annotated illustration pairs. Human annotators consistently preferred illustrations generated using LLM-generated captions over those using raw story text, with win rates around 73-78%. Additionally, LLMs can articulate visual scene characteristics useful for evaluating illustration quality, outperforming baseline approaches by about 11% in accuracy. The results demonstrate that LLMs effectively verbalize implicit scene knowledge from text, enhancing both generation and evaluation of narrative illustrations.

## Method Summary
The pipeline processes narrative text through three stages: (1) Story fragmentation using an LLM to segment stories into visualizable fragments, (2) Scene description generation where another LLM creates detailed visual captions from these fragments, and (3) Image generation using text-to-image models like Midjourney and FLUX-1. For evaluation, an LLM generates atomic criteria describing visual characteristics, which a vision-language model uses to score illustrations. The approach was applied to ROCStories corpus, creating 2,990 illustration pairs evaluated by human annotators and automated raters.

## Key Results
- Human annotators preferred LLM-generated CAPTION illustrations over baseline scene descriptions with win rates of 73-78%
- Inter-annotator agreement was higher for pairs with different scene descriptions (κu=0.483) vs. different image generators (κu=0.364)
- Criterial raters achieved ~70% accuracy vs. ~59% for baseline raters in predicting human illustration preferences
- Scene description quality was the dominant factor affecting illustration quality, more than image generator choice

## Why This Works (Mechanism)

### Mechanism 1
LLMs can verbalize implicit visual scene knowledge from narrative text more effectively than raw story text serves as prompts. Story text often lacks detailed visual descriptions (e.g., physical appearance, spatial relationships). An LLM "scene captioner" explicitly infers and articulates these missing details—entities, settings, poses, lighting, atmosphere—transforming abstract narrative into concrete visual specifications that text-to-image models can render. Core assumption: Text-to-image models require explicit visual descriptors to produce high-quality illustrations; implicit or abstract story text is insufficient as direct prompts.

### Mechanism 2
LLMs can identify scene boundaries in narrative text, segmenting stories into visualizable fragments. Using few-shot prompting with bracket annotations, an LLM identifies where one "distinctly illustratable" scene ends and another begins. This leverages the LLM's narrative understanding to recognize event boundaries and causally linked action units. Core assumption: Stories decompose naturally into discrete events or scenes, each sufficient for a single image; LLMs trained on narrative corpora have implicit knowledge of event segmentation.

### Mechanism 3
LLM-generated atomic evaluation criteria enable VLMs to predict human illustration quality preferences more accurately than unguided holistic rating. A text-only LLM generates a checklist of atomic scene characteristics (e.g., "image shows a ring," "woman's expression conveys joy"). A VLM then scores each illustration by checking each criterion. The sum of satisfied criteria predicts human preference. This decomposes the evaluation into verifiable visual propositions rather than a single subjective judgment. Core assumption: Illustration quality can be approximated by the count of correctly rendered scene characteristics; each characteristic is independently assessable by a VLM.

## Foundational Learning

- **Meta-prompting / Prompt Optimization**: Why needed here: The pipeline relies on an LLM generating prompts for another model (text-to-image). Understanding how to structure meta-prompts—task framing, exemplars, output constraints—is essential for effective scene captioning. Quick check question: Can you write a prompt that instructs an LLM to generate a text-to-image caption with specific visual detail requirements (entities, setting, lighting) from an abstract story fragment?

- **Text-to-Image Model Capabilities and Limitations**: Why needed here: The paper's premise is that raw story text is suboptimal for text-to-image models. Understanding why (lack of visual descriptors, spatial ambiguity) helps diagnose when LLM captioning will help most. Quick check question: Given a prompt "Alice called her mother and apologized profusely," what visual information is missing that a text-to-image model would need?

- **Atomic Criteria Decomposition**: Why needed here: The evaluation mechanism depends on breaking scene descriptions into independently verifiable propositions. This requires understanding how to write criteria that are neither too coarse nor too fine. Quick check question: Given "A woman receives a gold locket in a hospital room," decompose this into at least five atomic criteria suitable for VLM verification.

## Architecture Onboarding

- Component map: Story Text → [Fragmentation LLM] → Fragments + Context → [Scene Captioner LLM] → Visual Caption → [Text-to-Image Model] → Illustration
- Critical path: Scene captioning quality (LLM prompt design) → caption quality → illustration quality. Human evaluation (Table 3) shows this is the dominant factor over image generator choice.
- Design tradeoffs:
  - Caption detail vs. hallucination risk: More detailed captions may improve visual specificity but risk introducing information not grounded in the story text.
  - Criteria granularity: More atomic criteria improve interpretability but increase VLM inference cost and potential response noise.
  - Fragment size: Larger fragments preserve more context but may compress multiple visual events into one image; smaller fragments may lose narrative coherence.
- Failure signatures:
  - Low human preference for CAPTION over baselines → check prompt quality, LLM choice, or story corpus complexity.
  - Low criterial rater accuracy vs. baseline → criteria may be non-atomic or VLM may struggle with specific visual attributes (e.g., emotions).
  - High inter-annotator disagreement → pairs may be too similar in quality; consider more distinct comparison conditions.
- First 3 experiments:
  1. Ablate the captioner: Compare illustrations generated from raw fragments (NC-FRAGMENT, VC-FRAGMENT, SC-FRAGMENT) vs. LLM-generated CAPTION on a held-out story set. Replicate Table 3 win rates.
  2. Swap captioner LLMs: Test CLAUDE-3.5, GPT-4O, and LLAMA-3.1 as scene captioners. Measure caption length, descriptive detail, and resulting illustration win rates.
  3. Perturb criteria atomicity: Generate criteria sets at varying granularity (1-5, 10-15, 20+ items). Measure VLM rater accuracy and human agreement to identify optimal criteria density.

## Open Questions the Paper Calls Out

### Open Question 1
Does the scene illustration pipeline maintain effectiveness when applied to complex narratives outside the ROCStories corpus? The authors acknowledge they have "not yet fully assessed whether our scene illustration pipeline generalizes to more complex narratives" due to the constrained nature of the ROCStories corpus. The current dataset uses simple, five-sentence stories with basic causal links, whereas complex narratives involve nuanced subtext and longer context windows that may challenge the current fragmentation and captioning prompts. What evidence would resolve it: Benchmarking the pipeline on a corpus of complex literary fiction or long-form stories and comparing human preference scores against the ROCStories baseline.

### Open Question 2
How can the pipeline be modified to ensure visual consistency of characters and settings across sequential scene illustrations? The Conclusion states the intent to "consider recent approaches addressing... image-image alignment, such as ensuring visual consistency... to extend our illustration pipeline to generate multi-scene image sequences." The current pipeline treats scenes independently (text-image alignment), and it is unclear how to integrate consistency modules without breaking the modular "plug-and-play" architecture described in the paper. What evidence would resolve it: Extending the SceneIllustrations dataset to include full stories and measuring the visual consistency (e.g., via CLIP embeddings) of character appearances across generated images.

### Open Question 3
Can automated prompt optimization techniques improve the performance of the fragmentation and scene captioning modules beyond manual design? The Limitations section notes that prompts were designed via "vibe-based" engineering and that "it leaves open the possibility of further prompt optimization" using quantitative approaches. Manual prompt engineering may have overfitted to the specific style of ROCStories or the specific LLMs used (Claude, GPT), limiting transferability to other models or domains. What evidence would resolve it: Implementing an automated optimizer (e.g., DSPy) to tune the system prompts and measuring the resulting win rate of generated illustrations against the human-annotated ground truth.

## Limitations

- The pipeline relies on proprietary LLMs and image generators with unknown capability bounds
- Evaluation is confined to ROCStories—a dataset with simple, everyday narratives that may not stress-test the pipeline's generalization
- Human evaluation was conducted on Prolific without demographic diversity reporting, limiting generalizability of preference patterns

## Confidence

- **High confidence**: LLMs can segment narrative text into illustratable fragments (validated on 50 stories with ~96% correct sizing)
- **High confidence**: Human annotators prefer illustrations from LLM-generated captions over raw story text (73-78% win rate with κu=0.483 agreement)
- **Medium confidence**: LLM-generated atomic criteria enable VLMs to predict human preferences ~11% more accurately than unguided rating (requires assumptions about criteria decomposition quality)
- **Low confidence**: Claims about LLMs "effectively verbalizing implicit scene knowledge" are descriptive rather than mechanistic—the paper doesn't establish what knowledge is implicit vs. hallucinated

## Next Checks

1. **Ablate the captioner**: Compare illustrations generated from raw fragments (NC-FRAGMENT, VC-FRAGMENT, SC-FRAGMENT) vs. LLM-generated CAPTION on a held-out story set. Replicate Table 3 win rates to verify scene description quality is the dominant factor.

2. **Swap captioner LLMs**: Test CLAUDE-3.5, GPT-4O, and LLAMA-3.1 as scene captioners. Measure caption length, descriptive detail, and resulting illustration win rates to identify whether improvements generalize across models or depend on specific prompt engineering.

3. **Perturb criteria atomicity**: Generate criteria sets at varying granularity (1-5, 10-15, 20+ items). Measure VLM rater accuracy and human agreement to identify optimal criteria density and test the assumption that more atomic criteria improve evaluation accuracy.