---
ver: rpa2
title: 'SAKE: Towards Editing Auditory Attribute Knowledge of Large Audio-Language
  Models'
arxiv_id: '2510.16917'
source_url: https://arxiv.org/abs/2510.16917
tags:
- editing
- knowledge
- audio
- auditory
- type
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SAKE, the first benchmark for editing auditory
  attribute knowledge in Large Audio-Language Models (LALMs). Unlike previous work
  focused on factual or visual knowledge, SAKE targets abstract auditory attributes
  like speaker gender, emotion, language, and animal sounds.
---

# SAKE: Towards Editing Auditory Attribute Knowledge of Large Audio-Language Models

## Quick Facts
- arXiv ID: 2510.16917
- Source URL: https://arxiv.org/abs/2510.16917
- Reference count: 40
- Primary result: First benchmark for editing auditory attribute knowledge in LALMs, revealing high reliability but poor generality and locality

## Executive Summary
This paper introduces SAKE, the first benchmark for editing auditory attribute knowledge in Large Audio-Language Models (LALMs). Unlike previous work focused on factual or visual knowledge, SAKE targets abstract auditory attributes like speaker gender, emotion, language, and animal sounds. The benchmark evaluates seven editing methods across four dimensions: reliability (edit success), generality (generalization to equivalent inputs), locality (preservation of unrelated knowledge), and portability (propagation to related knowledge). Experiments on two LALMs show that while most methods achieve high reliability, they struggle with generality, especially for auditory inputs, and fail to preserve unrelated knowledge within the same attribute.

## Method Summary
SAKE introduces a benchmark for editing auditory attribute knowledge in LALMs, evaluating seven methods (fine-tuning LLM backbone, fine-tuning audio encoder, key-value editing, MEND, UnKE, I-IKE, and IE-IKE) across four dimensions: reliability, generality, locality, and portability. The benchmark uses data from the SAKURA benchmark (CommonVoice, CREMA-D, ESC-50, Animal-Sound Dataset) with 4,000 training instances and 1,200 testing instances. Evaluation is performed using GPT-5 mini as a judge with 98.10% human agreement. The study tests both single and sequential editing scenarios, with sequential editing revealing significant forgetting effects.

## Key Results
- Most editing methods achieve high reliability (>90% edit success) but struggle with generality, particularly for auditory inputs
- Methods fail to preserve unrelated knowledge within the same attribute, with Type 2 audio locality showing the poorest performance
- Sequential editing reveals rapid forgetting, with edited knowledge degrading after a few additional updates
- Fine-tuning the LLM backbone or modality connector remains the most robust baseline across all evaluation dimensions

## Why This Works (Mechanism)
The paper demonstrates that auditory attribute knowledge in LALMs is entangled within shared representations, making selective editing challenging. When editing one attribute label, the model's representation of other labels within the same attribute (e.g., different emotions) is also affected. This intra-attribute entanglement explains why locality preservation is difficult and why sequential editing leads to forgetting - updates to shared representations compound over time.

## Foundational Learning
- **Auditory attribute knowledge**: Abstract concepts like speaker gender, emotion, language, and animal sounds that LALMs need to recognize - needed to understand what knowledge is being edited
- **Intra-attribute entanglement**: Shared representations among related labels within the same attribute - needed to explain why editing one label affects others
- **Catastrophic forgetting**: Degradation of previously learned knowledge when updating models - needed to understand sequential editing challenges
- **Knowledge editing evaluation dimensions**: Reliability, generality, locality, portability - needed to assess editing effectiveness comprehensively
- **GPT-based judge evaluation**: Using language models to evaluate editing quality - needed to understand the evaluation methodology
- **Speech-to-text vs speech-to-speech models**: Different LALM architectures with varying output modalities - needed to understand model differences

## Architecture Onboarding

**Component Map**: SAKURA datasets -> Preprocessing -> Editing methods (FT-LLM, FT-Audio, KE, MEND, UnKE, I-IKE, IE-IKE) -> EasyEdit framework -> DeSTA2.5-Audio/Qwen2-Audio models -> GPT-5 mini judge evaluation

**Critical Path**: Data preprocessing → Editing method application → Model inference → GPT-5 judge evaluation → Metric computation

**Design Tradeoffs**: Parameter-efficient methods (KE, MEND) vs. robust fine-tuning approaches; computational cost vs. editing effectiveness; generality vs. locality preservation

**Failure Signatures**: Degenerated outputs in sequential editing (repetitive generations), poor generality scores indicating overfitting, low locality scores showing attribute entanglement

**Three First Experiments**:
1. Run single editing on D_edit split to verify basic reliability scores
2. Test MEND method on emotion attribute to observe locality preservation failure
3. Conduct sequential editing with 5-edit sequence to replicate forgetting patterns

## Open Questions the Paper Calls Out
- How can knowledge editing methods disentangle specific target labels from the "intra-attribute entanglement" observed in LALMs to preserve unrelated labels within the same attribute?
- Can specialized editing techniques be developed to sustain auditory knowledge updates over sequential edits without succumbing to rapid catastrophic forgetting?
- How do the challenges of editing auditory attributes generalize to speech-to-speech Large Audio-Language Models?

## Limitations
- Benchmark relies on GPT-5 mini judge evaluation, introducing potential subjectivity despite high human agreement
- Study focuses on four specific auditory attributes, limiting generalizability to all types of auditory knowledge
- Sequential editing protocol may not fully represent real-world editing scenarios with less frequent updates

## Confidence
- **High Confidence**: Reliability results showing successful edits across all methods with objective accuracy metrics
- **Medium Confidence**: Generality and locality findings showing method limitations with GPT-5 judge evaluation
- **Medium Confidence**: Sequential editing results demonstrating forgetting patterns, though specific degradation may vary

## Next Checks
1. Replicate sequential editing with varied edit intervals (beyond 0-5) to understand long-term stability of edited knowledge
2. Cross-attribute editing validation to test whether editing one attribute affects unrelated attributes
3. Judge reliability assessment through human evaluation on a subset of 100-200 instances to validate GPT-5 judge consistency