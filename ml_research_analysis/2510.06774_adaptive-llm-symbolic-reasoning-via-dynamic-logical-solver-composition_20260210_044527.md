---
ver: rpa2
title: Adaptive LLM-Symbolic Reasoning via Dynamic Logical Solver Composition
arxiv_id: '2510.06774'
source_url: https://arxiv.org/abs/2510.06774
tags:
- reasoning
- solver
- problems
- problem
- routing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel adaptive neuro-symbolic reasoning
  framework that automatically identifies formal reasoning strategies from natural
  language problems and dynamically selects specialized logical solvers. The framework
  decomposes problems, classifies reasoning types (LP, FOL, CSP, SMT), and composes
  appropriate solvers via autoformalization interfaces.
---

# Adaptive LLM-Symbolic Reasoning via Dynamic Logical Solver Composition
## Quick Facts
- arXiv ID: 2510.06774
- Source URL: https://arxiv.org/abs/2510.06774
- Reference count: 40
- Key outcome: Framework achieves 98% accuracy in strategy prediction and outperforms GPT-4o by 27% on reasoning tasks

## Executive Summary
This paper presents a novel adaptive neuro-symbolic reasoning framework that dynamically identifies formal reasoning strategies from natural language problems and composes specialized logical solvers accordingly. The system decomposes problems, classifies reasoning types (LP, FOL, CSP, SMT), and leverages autoformalization interfaces to integrate appropriate solvers. The approach demonstrates significant performance improvements over existing methods, particularly in sequential multi-paradigm reasoning tasks, establishing foundations for unifying material and formal inferences on heterogeneous reasoning challenges.

## Method Summary
The framework employs a three-stage pipeline: problem decomposition using large language models, reasoning strategy classification into logical paradigms (linear programming, first-order logic, constraint satisfaction, satisfiability modulo theories), and dynamic solver composition through autoformalization interfaces. The system uses frontier models for strategy prediction with over 98% accuracy, enabling flexible integration of specialized solvers. The approach is evaluated across multiple reasoning tasks including mathematical problems, planning scenarios, and symbolic reasoning benchmarks, with comparisons against zero-shot, chain-of-thought, and symbolic chain-of-thought baselines.

## Key Results
- 98% accuracy in reasoning strategy prediction using frontier models
- 27% improvement over GPT-4o and 6% over DeepSeek-V3.1 on benchmark tasks
- 54.4% accuracy on sequential multi-paradigm reasoning vs 27.3% for pure LLM methods
- Fine-tuned smaller models achieve up to 59.6% accuracy compared to 29.2% zero-shot

## Why This Works (Mechanism)
The framework succeeds by addressing the fundamental limitation of single-solver approaches through dynamic strategy identification and solver composition. By decomposing problems and classifying reasoning types, it matches each subproblem with the most appropriate formal reasoning paradigm. The autoformalization interfaces enable seamless integration between natural language understanding and formal logical systems, while the adaptive selection mechanism ensures optimal solver choice based on problem characteristics. This multi-paradigm approach captures the complementary strengths of different reasoning frameworks that monolithic methods miss.

## Foundational Learning
**Logical Reasoning Paradigms** (why needed: different problem types require different formal reasoning approaches; quick check: identify which paradigm applies to basic math vs planning vs constraint problems)
**Autoformalization** (why needed: bridges natural language to formal logic representation; quick check: verify conversion accuracy from problem statement to formal specification)
**Solver Composition** (why needed: enables combining multiple specialized solvers for complex problems; quick check: test that composed solutions maintain correctness across subproblems)

## Architecture Onboarding
**Component Map**: Natural Language Problem -> Problem Decomposition -> Strategy Classification -> Autoformalization -> Solver Selection -> Solution Generation
**Critical Path**: Problem Decomposition -> Strategy Classification -> Autoformalization -> Solver Selection (the adaptive core that determines overall performance)
**Design Tradeoffs**: Single unified solver (simplicity) vs multiple specialized solvers (performance); static solver selection (predictability) vs dynamic adaptation (flexibility)
**Failure Signatures**: Misclassification of reasoning strategies leading to solver mismatch; autoformalization errors causing invalid problem representations; decomposition failures breaking problem into unsolvable subproblems
**3 First Experiments**: 1) Test strategy classification accuracy on diverse problem types 2) Validate autoformalization conversion for each logical paradigm 3) Evaluate solver selection accuracy on simple benchmark problems

## Open Questions the Paper Calls Out
None

## Limitations
- Lack of detailed error analysis for strategy prediction misclassifications
- No statistical significance testing for comparative performance metrics
- Unclear scalability to real-world problems with larger search spaces
- Potential overfitting of fine-tuned smaller models to benchmark domains

## Confidence
- High confidence in methodology description and technical implementation details
- Medium confidence in reported accuracy improvements due to limited statistical validation
- Medium confidence in framework's adaptability claims without extensive real-world testing
- Low confidence in long-term stability of fine-tuned smaller models beyond evaluated benchmarks

## Next Checks
1. Conduct statistical significance testing with multiple runs and confidence intervals for all comparative performance metrics
2. Evaluate framework on substantially larger-scale problems (10x-100x current benchmark size) to assess scalability
3. Perform ablation studies removing specific components to quantify individual contributions to performance gains