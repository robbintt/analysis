---
ver: rpa2
title: LLM-based Query Expansion Fails for Unfamiliar and Ambiguous Queries
arxiv_id: '2505.12694'
source_url: https://arxiv.org/abs/2505.12694
tags:
- query
- knowledge
- retrieval
- queries
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates when LLM-based query expansion fails in
  information retrieval, focusing on two failure modes: when the LLM lacks knowledge
  about the query and when the query is ambiguous. The authors conduct controlled
  experiments across multiple datasets (NQ, TriviaQA, MS MARCO, BioASQ, AmbigDocs,
  AmbigQA, TREC Web) using different retrieval models (BM25, Contriever, E5-base-v2)
  and query expansion methods (Q2E, Q2D, GaQR).'
---

# LLM-based Query Expansion Fails for Unfamiliar and Ambiguous Queries

## Quick Facts
- arXiv ID: 2505.12694
- Source URL: https://arxiv.org/abs/2505.12694
- Reference count: 40
- Primary result: LLM-based query expansion degrades retrieval when LLMs lack query knowledge or when queries are ambiguous

## Executive Summary
This paper investigates when LLM-based query expansion fails in information retrieval, focusing on two failure modes: when the LLM lacks knowledge about the query and when the query is ambiguous. Through controlled experiments across multiple datasets (NQ, TriviaQA, MS MARCO, BioASQ, AmbigDocs, AmbigQA, TREC Web) using different retrieval models (BM25, Contriever, E5-base-v2) and query expansion methods (Q2E, Q2D, GaQR), the authors demonstrate that LLM-based QE can significantly degrade retrieval effectiveness when LLM knowledge is insufficient, particularly for declarative queries with proper nouns. The study also reveals that ambiguous queries suffer from popularity bias, where QE favors dominant interpretations while reducing recall for less popular ones.

## Method Summary
The study evaluates three query expansion methods: Q2E (keyword generation), Q2D (answer generation), and GaQR (query rewriting). Retrievers tested include BM25, Contriever, and E5-base-v2. Queries are classified by LLM knowledge availability using exact-match or LLM-as-judge evaluation, and by ambiguity using interpretation counts or subtopic thresholds. The experiments measure NDCG@10 and Recall@100 across datasets, with statistical significance assessed via paired t-tests.

## Key Results
- LLM-based QE degrades retrieval when LLMs lack query knowledge, especially for declarative queries with proper nouns
- Query rewriting (GaQR) is more resilient to knowledge gaps than keyword or answer generation methods
- Ambiguous queries show popularity bias in QE, favoring dominant interpretations while reducing recall for less popular ones

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Answer-based query expansion (Q2D) degrades retrieval when the LLM lacks knowledge, especially for declarative queries containing proper nouns.
- Mechanism: Q2D generates pseudo-answer documents that expand the query. When the LLM lacks knowledge, it hallucinates incorrect proper nouns, which sparse retrievers (BM25) weight heavily, causing misalignment with relevant documents.
- Core assumption: Retrieval models assign disproportionate importance to named entities in expanded text.
- Evidence anchors:
  - [abstract] "LLM-based QE can significantly degrade the retrieval effectiveness when knowledge in the LLM is insufficient"
  - [section 4.1] "Q2D failed to outperform 'No Exp' on declarative queries, yet they still improved procedural queries"
  - [corpus] Related work on LLM alignment for QE (arXiv:2507.11042) suggests similar knowledge-dependency issues
- Break condition: When queries are procedural (how-to) rather than declarative (what/who), incorrect expansions are less harmful since they rarely contain proper nouns.

### Mechanism 2
- Claim: Query rewriting methods (GaQR) are more resilient to knowledge gaps than keyword or answer generation methods.
- Mechanism: GaQR paraphrases queries for better retrieval effectiveness rather than generating new content (keywords or pseudo-answers). Paraphrasing requires less factual knowledge about entities, reducing dependency on accurate information.
- Core assumption: Query paraphrasing relies more on linguistic patterns than factual knowledge.
- Evidence anchors:
  - [section 4.1] "GaQR is the most effective QE method when the LLM lacks sufficient knowledge"
  - [section 4.1] "As paraphrasing may not require knowledge about the query when compared to generating keywords or answers, GaQR is considered less susceptible"
  - [corpus] Query rewriting survey (arXiv:2308.07107) categorizes this as "question" category with different failure modes
- Break condition: When queries require domain-specific terminology that paraphrasing cannot infer without knowledge.

### Mechanism 3
- Claim: LLM-based QE exhibits popularity bias, favoring dominant interpretations of ambiguous queries while reducing recall for less popular interpretations.
- Mechanism: LLMs privilege popular entities due to training data distribution. When expanding ambiguous queries, they generate terms aligned with popular interpretations, narrowing search coverage and excluding documents relevant to alternative interpretations.
- Core assumption: LLM training data skew toward popular entities transfers to query expansion behavior.
- Evidence anchors:
  - [abstract] "when the query is ambiguous, causing biased refinements that narrow search coverage"
  - [section 4.2] "LLM-based QE was effective in retrieving relevant documents for popular query interpretations, but not for less popular ones"
  - [section 4.2] Figure 2 shows QE effectiveness diminishes for rarer interpretations across BM25 and Contriever
  - [corpus] Weak corpus evidence; related papers don't directly address popularity bias in QE
- Break condition: GaQR with BM25 shows consistent improvement regardless of popularity—rewriting strategy may mitigate this bias.

## Foundational Learning

- Concept: Query Expansion (QE) categories
  - Why needed here: Understanding whether a method generates answers (Q2D), keywords (Q2E), or rewrites (GaQR) predicts its knowledge dependency and failure modes.
  - Quick check question: Given a biomedical query outside typical LLM training, which QE method would likely perform best?

- Concept: Sparse vs. Dense Retrieval sensitivity
  - Why needed here: BM25 (sparse) weights proper nouns heavily in expanded text; dense retrievers (Contriever, E5) may be more sensitive to semantic drift from incorrect expansions.
  - Quick check question: Why might Q2D harm BM25 more than E5-base-v2 on declarative queries with unknown knowledge?

- Concept: Recall vs. NDCG@10 trade-offs
  - Why needed here: Ambiguous queries show recall degradation (coverage narrows) even when NDCG@10 improves (popular interpretations ranked well).
  - Quick check question: If QE improves NDCG@10 but degrades Recall@100 for high-ambiguity queries, what does this indicate about coverage?

## Architecture Onboarding

- Component map:
  Query → [LLM Expansion Module] → Expanded Query → [Retriever] → Documents
           ↓
  [Knowledge Evaluator] → Confidence Score (optional gating)

- Critical path:
  1. Query classification: Assess knowledge availability (exact-match for short answers, LLM-as-judge for long)
  2. Ambiguity detection: Count interpretations or subtopics
  3. QE method selection: GaQR for unknown/ambiguous, Q2D/Q2E for known/unambiguous
  4. Retrieval with expanded query

- Design tradeoffs:
  - **Q2D vs. GaQR**: Q2D gives larger gains when knowledge is sufficient (+7.42 NDCG@10 on NQ with BM25) but larger losses when insufficient. GaQR offers safer, smaller improvements.
  - **Sparse vs. Dense**: High-performing dense retrievers (E5-base-v2) are more adversely affected by QE; Table 3 shows consistent negative impacts on BioASQ.
  - **Knowledge evaluation cost**: Exact-match is cheap but limited to short-answer datasets; LLM-as-judge (GPT-4) adds latency and cost.

- Failure signatures:
  - **Unknown knowledge + Q2D/Q2E on declarative queries**: NDCG drops 5-15 points (Table 1, Contriever on BioASQ)
  - **High ambiguity + any QE on dense retrievers**: Recall drops 10-30 points (Table 4, E5 on AmbigDocs)
  - **Rare interpretations**: Recall consistently lower for popularity groups 3-4 (Figure 2)

- First 3 experiments:
  1. **Knowledge gating**: Implement a lightweight knowledge check (exact-match on entity extraction). Compare QE-with-gating vs. QE-always on NQ/TriviaQA. Expected: Gating prevents degradation on unknown subset.
  2. **Method switching**: Route unknown queries to GaQR, known queries to Q2D. Evaluate on MS MARCO split by declarative/procedural. Expected: Hybrid approach outperforms single-method baseline.
  3. **Ambiguity-aware expansion**: For high-ambiguity queries, generate multiple expansions (one per interpretation) and aggregate retrieval results. Evaluate recall on AmbigDocs/AmbigQA. Assumption: Multiple expansions may recover coverage lost to popularity bias.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the identified failure patterns persist when using advanced retrieval pipelines such as pseudo-relevance feedback, chain-of-thought prompting, or multiple query expansions?
- Basis in paper: [explicit] Authors state: "Our future work is as follows: (i) test whether these issues persist with advanced pipelines (e.g., pseudo-relevance feedback, chain-of-thought prompting, multiple expansions)"
- Why unresolved: The current experiments only use zero-shot single-pass query expansion without iterative refinement or auxiliary retrieval techniques.
- What evidence would resolve it: Experiments applying PRF, CoT prompting, or multi-expansion aggregation to the same datasets (NQ, AmbigDocs, etc.) with knowledge/ambiguity stratification.

### Open Question 2
- Question: Can a practical module be developed to accurately assess LLM knowledge sufficiency and query ambiguity in real-time for adaptive query expansion?
- Basis in paper: [explicit] Authors state: "(ii) develop a module that assesses LLM's knowledge and query ambiguity for adaptive QE"
- Why unresolved: Current knowledge evaluation requires ground-truth answers (exact-match or GPT-4 assessment), which are unavailable at inference time; ambiguity thresholds are dataset-specific and manually set.
- What evidence would resolve it: A trained classifier or prompting strategy that predicts knowledge/ambiguity labels without ground truth, validated against held-out queries with retrieval performance as downstream metric.

### Open Question 3
- Question: How does LLM-based query expansion effectiveness vary across distinct types of query ambiguity (entity reference vs. answer type vs. temporal ambiguity)?
- Basis in paper: [inferred] The paper notes AmbigQA contains only 23% entity-reference ambiguity, with other queries ambiguous due to answer types or time, and hypothesizes document diversity differs by ambiguity type.
- Why unresolved: Results were inconclusive for AmbigQA on Recall metrics; the hypothesis about document diversity across ambiguity types was not directly tested.
- What evidence would resolve it: Controlled experiments stratifying queries by specific ambiguity type within a single dataset, measuring both retrieval diversity and per-interpretation recall.

## Limitations
- Findings based on zero-shot LLM usage without knowledge grounding or ambiguity resolution mechanisms
- Knowledge evaluation relies on LLM-as-judge, introducing potential circularity
- Popularity bias analysis uses Wikipedia page views as proxy, which may not capture user intent

## Confidence
- **High confidence**: LLM-based QE degrades performance when knowledge is insufficient, particularly for declarative queries with proper nouns
- **Medium confidence**: GaQR's resilience to knowledge gaps through paraphrasing
- **Medium confidence**: Popularity bias in ambiguous query handling

## Next Checks
1. **Knowledge Grounding Experiment**: Implement knowledge grounding for Q2D/Q2E and measure whether this mitigates degradation on declarative queries with unknown knowledge
2. **Ambiguity Resolution Test**: For high-ambiguity queries, generate multiple expansions targeting different interpretations and compare recall@100 against single-expansion baseline
3. **Cross-Lingual Generalization**: Evaluate the same QE methods on a non-English dataset to assess whether knowledge gaps and popularity bias patterns hold across languages