---
ver: rpa2
title: 'Llama Nemoretriever Colembed: Top-Performing Text-Image Retrieval Model'
arxiv_id: '2507.05513'
source_url: https://arxiv.org/abs/2507.05513
tags:
- retrieval
- vidore
- arxiv
- embedding
- nvidia
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Llama Nemoretriever Colembed introduces a state-of-the-art text-image
  retrieval model leveraging bidirectional attention and ColBERT-style late interaction.
  The 3B variant achieves NDCG@5 of 91.0 on ViDoRe V1 and 63.5 on ViDoRe V2, ranking
  first on both benchmarks as of June 27, 2025.
---

# Llama Nemoretriever Colembed: Top-Performing Text-Image Retrieval Model

## Quick Facts
- arXiv ID: 2507.05513
- Source URL: https://arxiv.org/abs/2507.05513
- Authors: Mengyao Xu; Gabriel Moreira; Ronay Ak; Radek Osmulski; Yauhen Babakhin; Zhiding Yu; Benedikt Schifferer; Even Oldridge
- Reference count: 34
- Primary result: 3B model achieves NDCG@5 of 91.0 on ViDoRe V1 and 63.5 on ViDoRe V2

## Executive Summary
Llama Nemoretriever Colembed introduces a state-of-the-art text-image retrieval model leveraging bidirectional attention and ColBERT-style late interaction. The 3B variant achieves NDCG@5 of 91.0 on ViDoRe V1 and 63.5 on ViDoRe V2, ranking first on both benchmarks as of June 27, 2025. The model is built on NVIDIA Eagle2 VLM with modifications for fine-grained multimodal retrieval. A two-stage training strategy combining text-only pretraining and text-image fine-tuning enhances performance. While the late-interaction mechanism improves accuracy, it introduces trade-offs in storage and efficiency compared to bi-encoder approaches. The 1B variant also outperforms several larger models, demonstrating strong parameter efficiency.

## Method Summary
The method modifies NVIDIA Eagle2 VLM by replacing causal attention with bidirectional attention and integrating ColBERT-style late interaction for fine-grained multimodal retrieval. Training follows a two-stage approach: Stage 1 uses text-only retrieval corpora (HotpotQA, MIRACL, Natural Questions, Stack Exchange, SQuAD, Tiger Math/Stack) with InfoNCE contrastive loss and hard negative mining. Stage 2 fine-tunes on text-image pairs (ColPali train set, Wiki-SS-NQ, VDR, VisRAG-Ret-Train-Synthetic/Indomain, Docmatix). Two model variants are produced: 1B (2.42B parameters, 2048-dim embeddings) and 3B (4.41B parameters, 3072-dim embeddings). The architecture uses dynamic tiling (max 2 tiles for training, 6 for inference) and employs a MaxSim operator for query-document relevance scoring.

## Key Results
- 3B model achieves NDCG@5 of 91.0 on ViDoRe V1 and 63.5 on ViDoRe V2
- 1B variant demonstrates strong parameter efficiency, outperforming several larger models
- Late interaction achieves higher accuracy than bi-encoder approaches but requires 300x more storage (179-10,311 GB vs 2.9-3.8 GB per million images)
- Dimensionality reduction (3072→512) maintains accuracy with 88% storage reduction

## Why This Works (Mechanism)

### Mechanism 1
Bidirectional attention improves retrieval by enabling full context visibility across all tokens. By removing causal masking constraints, every token embedding can incorporate information from the entire sequence, producing representations better suited for similarity matching where query-document alignment isn't sequential.

### Mechanism 2
ColBERT-style late interaction achieves finer-grained matching than single-vector pooling by preserving all token-level embeddings. Each query token computes maximum similarity against all document tokens, then these maxima are summed. This preserves fine-grained semantic alignment but requires 300x more storage than bi-encoders.

### Mechanism 3
Two-stage training transfers text-only retrieval capabilities to multimodal settings more effectively than joint training from scratch. Stage 1 establishes robust semantic similarity representations using abundant text-only data, then Stage 2 aligns these learned representations with visual inputs via text-image pairs.

## Foundational Learning

- **Contrastive Learning (InfoNCE Loss)**: Essential for understanding the training objective that maximizes similarity between query-positive pairs while minimizing similarity to negatives. Quick check: Given a query embedding with similarity 0.8 to its positive document and 0.75 to a hard negative, would the InfoNCE loss consider this a difficult or easy example?

- **ColBERT Late Interaction vs. Bi-Encoder Pooling**: The core architectural trade-off. Without understanding MaxSim vs. mean pooling, you cannot reason about storage costs, latency budgets, or when to choose each approach. Quick check: If a document has 1,000 tokens and embedding dimension 128, how many floating-point values must be stored per document for late interaction vs. mean pooling?

- **Hard Negative Mining**: Critical for understanding the training strategy. The paper uses NV-Retriever's "top-k with percentage to positive threshold" strategy (threshold 0.95, K=2). Quick check: Why would selecting negatives with similarity 0.99 to the query be problematic for training, even if they're technically negative examples?

## Architecture Onboarding

- **Component map**: Vision Encoder (from Eagle2) -> LLM Backbone (Llama-based) -> Embedding Projection -> Late Interaction Scorer
- **Critical path**: Image input → Dynamic tiling → Vision encoder → Token embeddings; Text input → Tokenizer → LLM backbone (bidirectional) → Token embeddings; At retrieval time: Query embeddings × Document embeddings → MaxSim per query token → Sum for final score
- **Design tradeoffs**: Accuracy vs. Storage (late interaction: 91.0 NDCG@5, 10,311 GB/million images vs. bi-encoders: 3.8 GB); Dimensionality vs. Accuracy (3072→512 dims reduces storage by 88% with only 0.4% NDCG drop); Bi-encoder + Reranker vs. Late Interaction (similar accuracy, 3.8 GB storage, 2,368ms reranking latency)
- **Failure signatures**: Storage exhaustion (check embedding dimension and sequence lengths); Query latency spikes (verify max_input_tiles and consider dimensionality reduction); Accuracy degradation after dimensionality reduction (test on held-out set); Multilingual failures (check MIRACL-VISION results by language)
- **First 3 experiments**: 1) Establish baseline with 3B model at full dimension on 10K document subset; measure indexing time, storage, and query latency. 2) Test 512-dim projection on same subset; compare NDCG@5 degradation vs. storage savings. 3) Compare 1B model with 25-candidate reranker against 3B late interaction; evaluate if reranker approach meets latency SLA while matching accuracy.

## Open Questions the Paper Calls Out

1. Does binary quantization effectively maintain retrieval accuracy for late-interaction models when the embedding dimension is reduced to 128? The paper notes this requires further testing.

2. Can methods like MUVERA or late-pooling close the massive storage gap between late-interaction and bi-encoder models without sacrificing fine-grained accuracy advantage? The paper introduces these as potential solutions but does not evaluate them.

3. Does increasing the `max_input_tiles` during training beyond 2 yield performance gains, or does the current training-inference discrepancy act as a beneficial regularizer? The paper notes no gains at 4 tiles but hasn't tested maximum inference configuration.

## Limitations

- Massive storage requirements for late-interaction embeddings (300x larger than bi-encoders)
- Unknown training hyperparameters (learning rates, batch sizes, epoch counts)
- Hard negative mining implementation details not fully specified
- Dataset mixing strategy across training stages not detailed

## Confidence

**High Confidence Claims**:
- 3B model achieves NDCG@5 of 91.0 on ViDoRe V1 and 63.5 on ViDoRe V2
- Late interaction improves accuracy compared to bi-encoder approaches at the cost of storage
- 1B variant demonstrates strong parameter efficiency

**Medium Confidence Claims**:
- Two-stage training strategy enhances performance
- Bidirectional attention improves retrieval by enabling full context visibility
- Text-only pretraining capabilities transfer effectively to multimodal retrieval

**Low Confidence Claims**:
- None identified

## Next Checks

1. Validate storage-efficiency tradeoff by testing 3072→512 dimensionality reduction on representative document subset to verify claimed 0.4% NDCG degradation with 88% storage reduction.

2. Test hard negative mining effectiveness by comparing embedding similarity distributions and retrieval discrimination metrics between models trained with and without the specified hard negative mining strategy.

3. Verify multilingual performance by evaluating on MIRACL-VISION across multiple languages, particularly low-performing ones like Telugu at 0.0853, to identify if language-specific fine-tuning is necessary.