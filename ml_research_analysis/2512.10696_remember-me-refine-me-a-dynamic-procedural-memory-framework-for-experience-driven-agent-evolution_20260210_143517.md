---
ver: rpa2
title: 'Remember Me, Refine Me: A Dynamic Procedural Memory Framework for Experience-Driven
  Agent Evolution'
arxiv_id: '2512.10696'
source_url: https://arxiv.org/abs/2512.10696
tags:
- experience
- task
- reme
- memory
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ReMe addresses the static memory limitation in LLM agents by introducing
  a dynamic procedural memory framework that extracts fine-grained experiences from
  both successes and failures, adapts them to new contexts via scenario-aware indexing,
  and refines the experience pool through utility-based addition and deletion. Experiments
  on BFCL-V3 and AppWorld show ReMe achieves state-of-the-art performance, with Qwen3-8B
  + ReMe outperforming larger Qwen3-14B without memory (avg gains of 8.83% in Avg@4
  and 7.29% in Pass@4).
---

# Remember Me, Refine Me: A Dynamic Procedural Memory Framework for Experience-Driven Agent Evolution

## Quick Facts
- arXiv ID: 2512.10696
- Source URL: https://arxiv.org/abs/2512.10696
- Reference count: 18
- Qwen3-8B + ReMe outperforms Qwen3-14B without memory (avg gains of 8.83% in Avg@4 and 7.29% in Pass@4)

## Executive Summary
ReMe introduces a dynamic procedural memory framework to address the static memory limitations of LLM agents by extracting fine-grained experiences from both successes and failures. It adapts these experiences to new contexts through scenario-aware indexing and refines the memory pool using utility-based addition and deletion mechanisms. Experiments on BFCL-V3 and AppWorld demonstrate that ReMe achieves state-of-the-art performance, enabling efficient lifelong learning and narrowing the performance gap between different model scales.

## Method Summary
ReMe extracts fine-grained experiences from task executions, distinguishing between successes and failures to build a rich procedural memory. These experiences are indexed in a scenario-aware manner, allowing adaptation to new contexts. The framework continuously refines the memory pool through utility-based addition of beneficial experiences and deletion of obsolete ones, enabling the agent to evolve dynamically based on accumulated knowledge.

## Key Results
- ReMe achieves state-of-the-art performance on BFCL-V3 and AppWorld benchmarks.
- Qwen3-8B + ReMe outperforms Qwen3-14B without memory (avg gains of 8.83% in Avg@4 and 7.29% in Pass@4).
- The framework demonstrates efficient lifelong learning and narrows performance gaps between model scales.

## Why This Works (Mechanism)
ReMe's effectiveness stems from its ability to dynamically extract, adapt, and refine procedural memories. By capturing both successful and failed experiences, it builds a comprehensive knowledge base. Scenario-aware indexing ensures experiences are contextually relevant, while utility-based memory management keeps the pool optimized. This self-evolving mechanism allows agents to improve over time without requiring retraining.

## Foundational Learning
- **Procedural Memory Extraction**: Needed to capture detailed task execution traces for future reuse. Quick check: Verify that extracted experiences contain actionable steps and context.
- **Scenario-Aware Indexing**: Ensures experiences are retrievable in relevant contexts. Quick check: Test retrieval accuracy across diverse scenarios.
- **Utility-Based Memory Management**: Maintains an optimized memory pool by adding useful experiences and removing obsolete ones. Quick check: Monitor memory pool size and performance trends over time.

## Architecture Onboarding

**Component Map**: Task Execution -> Experience Extraction -> Scenario-Aware Indexing -> Memory Pool -> Context Adaptation -> Action Selection

**Critical Path**: The core workflow involves executing tasks, extracting procedural memories, indexing them by scenario, and using the refined memory pool to inform future actions.

**Design Tradeoffs**: ReMe prioritizes memory efficiency and adaptability over raw model scale, enabling smaller models to perform competitively. The tradeoff involves computational overhead for memory management versus performance gains.

**Failure Signatures**: Performance degradation may occur if memory extraction is noisy, indexing fails to capture context, or utility-based management incorrectly prunes useful experiences.

**First Experiments**: 1) Benchmark ReMe on BFCL-V3 to measure performance gains. 2) Compare Qwen3-8B + ReMe vs. Qwen3-14B without memory. 3) Conduct ablation studies on memory addition and deletion mechanisms.

## Open Questions the Paper Calls Out
None

## Limitations
- Performance comparisons are limited to single-agent settings; multi-agent or real-world scenarios may reveal scaling limits.
- The framework assumes structured log traces and accurate tool execution metadata, which may not generalize to noisy or incomplete feedback environments.
- Benchmarks used (BFCL-V3, AppWorld) have relatively small task pools, limiting generalizability.

## Confidence
- High in observed benchmark performance improvements.
- Medium in generality of memory extraction and scenario-aware indexing mechanisms.
- Low in long-term stability and transferability beyond tested domains.

## Next Checks
1. Test ReMe on a broader suite of benchmarks with longer horizon tasks to assess memory retention and decay over extended usage periods.
2. Conduct ablation studies isolating the contributions of memory addition vs. deletion mechanisms to quantify their individual impact on performance.
3. Evaluate ReMe's robustness under noisy or incomplete feedback conditions, simulating real-world deployment scenarios where execution logs may be partial or corrupted.