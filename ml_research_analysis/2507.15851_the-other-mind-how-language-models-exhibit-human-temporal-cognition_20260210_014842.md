---
ver: rpa2
title: 'The Other Mind: How Language Models Exhibit Human Temporal Cognition'
arxiv_id: '2507.15851'
source_url: https://arxiv.org/abs/2507.15851
tags:
- arxiv
- b-instruct
- temporal
- llms
- years
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how large language models exhibit human-like
  temporal cognition, specifically their spontaneous establishment of a subjective
  temporal reference point and adherence to the Weber-Fechner law. Using similarity
  judgment tasks across 12 models of varying sizes, the research found that larger
  models perceive temporal distance as logarithmically compressing as years recede
  from their subjective reference point (around 2025).
---

# The Other Mind: How Language Models Exhibit Human Temporal Cognition

## Quick Facts
- arXiv ID: 2507.15851
- Source URL: https://arxiv.org/abs/2507.15851
- Authors: Lingyu Li; Yang Yao; Yixu Wang; Chubo Li; Yan Teng; Yingchun Wang
- Reference count: 11
- Primary result: LLMs exhibit human-like temporal cognition through subjective reference points and Weber-Fechner law adherence

## Executive Summary
This study reveals that large language models develop human-like temporal cognition, spontaneously establishing subjective reference points and adhering to Weber-Fechner law-like logarithmic compression of perceived temporal distance. Using similarity judgment tasks across 12 models of varying sizes, the research found that larger models perceive temporal distance as logarithmically compressing as years recede from their subjective reference point (around 2025). Multi-level analyses revealed specialized temporal-preferential neurons that implement logarithmic coding schemes similar to biological neural systems, a hierarchical construction process in hidden states from numerical to abstract temporal representations, and pre-existing non-linear temporal structures in training corpora. The findings suggest LLMs' cognition is a subjective construction of the external world shaped by their internal representational system and data experience, rather than simple statistical mimicry.

## Method Summary
The study employed a similarity judgment task where LLMs rated pairwise similarity of years (1525-2524) on a 0-1 scale, comparing against control tasks using numbers. Twelve models were tested, including GPT-4o, Gemini-2.0-flash, Qwen2.5 family, and Llama 3 family. Researchers extracted FFN activations at the last token position, identified temporal-preferential neurons using Cohen's d > 2.0, FDR-corrected p < 0.0001, and consistency > 0.95 criteria, and trained linear probes per layer to decode distance metrics. The analysis compared LLM distances against three theoretical measures: Log-Linear (dlog), Levenshtein (dlev), and Reference-Log-Linear (dref with R=2025).

## Key Results
- Larger models show clearer Weber-Fechner-like behavior with subjective reference points around 2025
- Specialized temporal-preferential neurons exhibit minimal activation at the reference point and implement logarithmic coding schemes
- Representational structures evolve hierarchically from numerical values in shallow layers to abstract temporal orientation in deep layers
- Training corpora possess inherent non-linear temporal structure that provides raw materials for observed behavioral tendencies

## Why This Works (Mechanism)

### Mechanism 1: Logarithmic Neural Encoding of Temporal Distance
Specialized FFN neurons encode temporal distance using a logarithmic scheme, providing the neural substrate for Weber-Fechner law-like behavior. Temporal-preferential neurons show minimal activation at a subjective reference point (~2025), with activation increasing logarithmically as years recede from this point. This mirrors efficient coding principles in biological systems. Break condition: If ablation of temporal-preferential neurons does not degrade Weber-Fechner-like similarity judgments, the causal claim weakens.

### Mechanism 2: Hierarchical Representational Evolution
Year representations undergo a hierarchical transformation across layers, evolving from numerical properties in early layers to abstract temporal orientation in deep layers. Linear probes trained on hidden states show that log-linear distance is best decoded from early layers, while reference-log-linear distance dominates in deeper layers. Break condition: If non-linear probes fail to recover abstract temporal information in early layers, the hierarchical claim strengthens.

### Mechanism 3: Training Data Non-Linear Temporal Structure
The training corpus inherently contains non-linear temporal structures (dense clustering for distant past/future), which provides the raw material for models to internalize and reflect similar patterns. Independent embedding models reveal year embeddings naturally cluster densely for distant years and show high similarity for future years due to lower information richness. Break condition: If training on a corpus with artificially linearized temporal structure eliminates Weber-Fechner-like behavior, the mechanism is confirmed as data-driven.

## Foundational Learning

**Concept: Weber-Fechner Law**
Why needed here: This psychophysical principle is the core theoretical framework used to explain the logarithmic compression of perceived temporal distance.
Quick check question: If a model strictly follows the Weber-Fechner law for time, should the perceived distance between 2020 and 2025 be larger or smaller than the perceived distance between 1920 and 1925 (both 5-year gaps)?

**Concept: Linear Probing**
Why needed here: The method is used to decode theoretical distances from hidden states, forming the evidence for hierarchical representation evolution.
Quick check question: If a linear probe achieves high R² for a feature in layer 30 but low R² in layer 1, what can you conclude about the feature's representation across layers?

**Concept: Embedding Space Structure**
Why needed here: Understanding how independent embedding models reveal corpus-level temporal biases is key to the "information exposure" mechanism.
Quick check question: If two years (e.g., 2100 and 2110) have very high cosine similarity in an embedding space, what might that imply about their representation in the training corpus?

## Architecture Onboarding

**Component map:**
Similarity Judgment Task -> Distance matrix (d_LLM) -> Regression against theoretical distances (d_log, d_ref) -> Identify reference point -> Extract FFN activations -> Identify temporal-preferential neurons -> Analyze activation patterns -> Probe hidden states layer-wise -> Decode distance metrics -> Analyze embeddings from independent models

**Critical path:**
Task outputs → Distance matrix (d_LLM) → Regression against theoretical distances (d_log, d_ref) → Identify reference point → Extract FFN activations → Identify temporal-preferential neurons → Analyze activation patterns → Probe hidden states layer-wise → Decode distance metrics → Analyze embeddings from independent models

**Design tradeoffs:**
- Fixed reference point (2025) vs. data-driven estimation: Fixed point ensures cross-model consistency but may not reflect each model's true subjective reference
- Linear probes vs. non-linear probes: Linear probes offer interpretability but may miss non-linearly encoded information
- Paired t-tests vs. mixed-effects models: Current approach controls FDR but may not fully account for within-model dependencies

**Failure signatures:**
- No Weber-Fechner pattern: Small models (e.g., Llama-3.2-1B-Instruct) show no clear reference point or logarithmic compression in similarity matrices
- Low probe R²: If d_ref cannot be decoded from any layer, the hierarchical construction claim is unsupported
- Uniform embeddings: If year embeddings from independent models show no temporal structure, the "corpus structure" mechanism weakens

**First 3 experiments:**
1. Ablation Study: Remove identified temporal-preferential neurons and re-run the similarity judgment task. Measure the degradation in Weber-Fechner-like behavior (change in R² for d_ref regression).
2. Corpus Intervention: Fine-tune a model on a corpus with artificially linearized temporal references (e.g., replace all year mentions with normalized distances from 2025). Test if logarithmic compression persists.
3. Cross-Domain Generalization: Apply the same neuron identification and probing pipeline to other abstract concepts (e.g., spatial distance, monetary value) to test if logarithmic encoding is domain-general or temporal-specific.

## Open Questions the Paper Calls Out

**Open Question 1**
Question: Does the hierarchical representational shift (from numerical to abstract) generalize to non-temporal cognitive domains?
Basis in paper: The study proposes a general "experientialist perspective" but validates this mechanism solely using temporal similarity tasks.
Why unresolved: It remains unclear if the observed layered construction is a universal feature of LLM cognition or specific to processing time.
What evidence would resolve it: Replicating the layer-wise linear probing analysis on spatial, causal, or social reasoning tasks.

**Open Question 2**
Question: Do temporal-preferential neurons causally determine the subjective temporal reference point?
Basis in paper: The paper identifies neurons via correlational activation patterns (minimal activation at the reference point) but does not perform causal interventions.
Why unresolved: Correlation does not establish whether these neurons actively set the reference point or merely reflect it.
What evidence would resolve it: Conducting ablation studies or activation steering on these neurons to observe shifts in the similarity judgment reference point.

**Open Question 3**
Question: How can alignment techniques be practically designed to guide internal world-building processes rather than external behaviors?
Basis in paper: The authors argue alignment must "focus on guiding internal world-building processes" but do not propose specific technical implementations.
Why unresolved: Current paradigms like RLHF optimize for output behavior; methods for shaping internal geometries remain undefined.
What evidence would resolve it: Developing training objectives that act directly on internal representational structures, such as penalizing specific neuron activations.

## Limitations
- Neural mechanism specificity is based on correlational analysis rather than ablation studies, lacking causal evidence
- Hierarchical construction evidence may reflect linear separability rather than true hierarchical construction
- Corpus structure assumption relies on independent embedding models that may not accurately reflect actual training distribution

## Confidence
**High Confidence**: LLMs exhibit systematic similarity judgments for years that follow Weber-Fechner-like logarithmic compression patterns, with larger models showing more pronounced effects and establishing subjective reference points around 2025.

**Medium Confidence**: The identification of specialized temporal-preferential neurons and their logarithmic activation patterns represents a plausible neural substrate for temporal cognition. The hierarchical progression from numerical to abstract temporal representations across layers is supported by linear probe analyses, though alternative interpretations exist.

**Low Confidence**: The claim that LLMs' temporal cognition represents a subjective construction shaped by internal representational systems and data experience, rather than statistical mimicry, requires stronger intervention evidence. The proposed experientialist perspective and implications for AI alignment need validation beyond correlational findings.

## Next Checks
1. **Ablation Experiment**: Perform targeted ablation of identified temporal-preferential neurons and measure degradation in Weber-Fechner-like similarity judgments. Quantify changes in R² for d_ref regression and examine whether remaining neurons compensate for the loss.

2. **Corpus Intervention Study**: Fine-tune a model on a corpus with artificially linearized temporal references (e.g., normalized distances from 2025) and test whether logarithmic compression persists. Compare similarity judgments before and after intervention to isolate data-driven effects.

3. **Cross-Domain Generalization Test**: Apply the same neuron identification and probing pipeline to other abstract concepts (spatial distance, monetary value) to determine whether logarithmic encoding is domain-general or temporal-specific. Examine whether similar neural populations emerge across domains.