---
ver: rpa2
title: 'Beyond Known Facts: Generating Unseen Temporal Knowledge to Address Data Contamination
  in LLM Evaluation'
arxiv_id: '2601.13658'
source_url: https://arxiv.org/abs/2601.13658
tags:
- temporal
- facts
- yago
- knowledge
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a method to generate future temporal knowledge
  graph extraction datasets to avoid data contamination in LLM evaluation. It uses
  a temporal knowledge graph forecasting system (TLogic) to predict plausible future
  quadruples, then uses an LLM (Llama3.3-70B) to generate textual descriptions for
  these quadruples, creating contamination-free benchmarks.
---

# Beyond Known Facts: Generating Unseen Temporal Knowledge to Address Data Contamination in LLM Evaluation

## Quick Facts
- arXiv ID: 2601.13658
- Source URL: https://arxiv.org/abs/2601.13658
- Authors: Arthur Amalvy; Hen-Hsen Huang
- Reference count: 40
- The paper proposes generating future temporal knowledge graph extraction datasets to avoid data contamination in LLM evaluation.

## Executive Summary
This paper addresses data contamination in LLM evaluation for temporal knowledge graph extraction by creating a benchmark of unseen future facts. The authors use a temporal knowledge graph forecasting system (TLogic) to predict plausible future quadruples, then employ an LLM (Llama3.3-70B) to generate textual descriptions for these quadruples, creating contamination-free benchmarks. When evaluating the state-of-the-art EDC framework on this dataset versus a dataset of known facts, performance decreases significantly (2.82 strict F1 score drop in single-fact setup), confirming data contamination effects. The approach enables sustainable, long-term evaluation of temporal knowledge graph extraction systems with unseen future facts.

## Method Summary
The method involves three key stages: First, historical temporal facts from YAGO 4.5 (2000-2024) are processed and linearized to convert time intervals into start/end events. Second, TLogic learns temporal rules from this historical data through random walks, then generates plausible future quadruples for 2026 by completing partial queries with future timestamps. Third, Llama3.3-70B generates newspaper-style textual descriptions for these future facts, creating the YAGO 2026 benchmark dataset. The evaluation compares EDC framework performance on known facts (2022) versus unseen future facts (2026), revealing contamination effects and timestamp biases.

## Key Results
- Performance on unseen future facts (YAGO 2026) drops by 2.82 strict F1 score compared to known facts (YAGO 2022) in single-fact setup
- Timestamp bias observed: future timestamps (2026) reduce extraction accuracy by ~2 F1 points compared to contemporary timestamps (2022)
- YAGO 2026 dataset contains 4.2K future quadruples with corresponding textual descriptions generated by Llama3.3-70B

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Temporal knowledge graph forecasting can generate plausible future facts that existing LLMs have not seen during pre-training.
- Mechanism: TLogic learns temporal rules from historical facts via random walks, then applies these rules to complete partial quadruples with future timestamps. The forecasting system projects temporal patterns forward rather than retrieving memorized data.
- Core assumption: Temporal patterns in historical data generalize plausibly to future timeframes; rule-based forecasting avoids importing LLM training biases.
- Evidence anchors:
  - [abstract] "Temporal Knowledge Graph Forecasting (TKGF) generates plausible future quadruples, which are subsequently filtered to adhere to the original knowledge base schema"
  - [section 3.2.2] "we query TLogic with the set of queries {(e_s, r, ?, t), e_s ∈ S_r}, and return the valid generated quadruple with the highest confidence"

### Mechanism 2
- Claim: Synthetic benchmark datasets built from forecasted future facts can reveal LLM performance degradation caused by data contamination.
- Mechanism: By comparing LLM extraction performance on known facts (YAGO 2022) versus unseen future facts (YAGO 2026), the difference in F1 scores isolates contamination effects from model capability.
- Core assumption: The forecasted facts, while not verified as true, are structurally valid and linguistically processable; performance differences stem from memorization rather than factual difficulty.
- Evidence anchors:
  - [abstract] "We benchmark Extract, Define and Canonicalize (EDC)... demonstrating that LLM performance decreases when evaluated on our dataset compared to a dataset of known facts"
  - [section 5.3] "performance on the dataset of unknown future facts is lower than on the 2022 dataset of known facts in the single fact setup, with a statistically significant difference of 2.82 strict F1 using EDC"

### Mechanism 3
- Claim: LLMs exhibit timestamp bias—extracting facts with contemporary timestamps more accurately than future timestamps—independently of fact novelty.
- Mechanism: Temporal tokens in training data create implicit priors; models may associate certain date formats or year values with specific reasoning patterns. Changing timestamps while keeping facts constant isolates this bias from contamination.
- Core assumption: The timestamp bias operates through token-level associations rather than semantic reasoning.
- Evidence anchors:
  - [section 5.3.1] "we find that extraction for future timestamps is rendered more difficult... with a significant difference of −2.02 strict F1" (baseline model)

## Foundational Learning

- Concept: Temporal Knowledge Graphs (TKGs) as quadruples (subject, relation, object, timestamp)
  - Why needed here: The methodology depends on understanding that facts have temporal validity—e.g., (Bill Gates, memberOf, Microsoft, [start_date, end_date])—and that forecasting systems extend these patterns forward.
  - Quick check question: Given quadruples (A, worksAt, B, 2020-01-01) and (A, worksAt, C, 2023-06-15), what temporal inference can you make about A's employment history?

- Concept: Data contamination in LLM evaluation
  - Why needed here: The core problem motivating this work; LLMs pre-trained on web corpora may have memorized evaluation examples, inflating reported performance.
  - Quick check question: If an LLM achieves 95% accuracy on a Wikipedia-based QA dataset it was potentially trained on, what can you confidently conclude about its true generalization capability?

- Concept: Rule-based temporal forecasting vs. embedding-based methods
  - Why needed here: The authors chose TLogic (rule-learning) over neural embedding methods specifically to avoid importing LLM biases.
  - Quick check question: Why might a rule like "employment ends within 5 years of starting" be preferable to a learned embedding for generating contamination-free test facts?

## Architecture Onboarding

- Component map: YAGO 4.5 TKG (2000-2024 facts) → TLogic rule learning → Temporal rules + confidence scores → Algorithm 1: subject/relation sampling, TLogic query, schema filtering → Future quadruples (2026) → Llama3.3-70B verbalization, single-fact & multi-fact prompts → Textual descriptions → YAGO 2026 benchmark dataset

- Critical path: TLogic rule quality → future fact plausibility → description generation accuracy → benchmark validity. If TLogic's Hits@10 (43.09) is insufficient, downstream facts may be structurally valid but semantically nonsensical.

- Design tradeoffs:
  - TLogic vs. neural TKG methods: TLogic is interpretable and LLM-bias-free but struggles with rare relations; neural methods capture complex patterns but import training biases
  - Single-fact vs. multi-fact evaluation: Single-fact isolates contamination effects; multi-fact better reflects realistic extraction but has higher variance
  - Day-level granularity vs. time ranges: Linearization simplifies forecasting but loses duration semantics

- Failure signatures:
  - Skewed relation distribution in generated dataset (observed: startMemberOf over-represented, rare relations missing)
  - Multi-fact descriptions with self-consistency violations (9/50 samples in manual review)
  - TLogic returns empty candidate sets for sparse relation queries (Algorithm 1 loops until valid fact found)

- First 3 experiments:
  1. Reproduce the contamination detection: Run EDC on both YAGO 2022 and YAGO 2026 single-fact setups. Verify the ~2.82 F1 gap.
  2. Isolate timestamp bias: Take YAGO 2022 facts, replace all timestamps with 2026 dates. Compare extraction performance. Then reverse: take YAGO 2026 facts, timestamp as 2022.
  3. Extend to new relations: Attempt to generate facts for relations TLogic currently fails on (e.g., rare relations in the long tail). Document which filtering steps block generation most frequently.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can advanced temporal forecasting methods, such as TILP, be effectively extended to support fine-grained, day-level event forecasting?
- Basis in paper: [explicit] The Conclusion states, "Future work could explore advanced forecasting methods like TILP... extending them to finer day-level granularity."
- Why unresolved: Current methods like TLogic struggle with rare relationships, and alternatives like TILP focus on time ranges rather than the day-level granularity required for this benchmark.
- What evidence would resolve it: A modification of TILP or similar logic programming methods that successfully generates and evaluates temporal facts at the day-level with higher accuracy than TLogic.

### Open Question 2
- Question: Do the observed data contamination effects and timestamp biases persist consistently across a broader range of Large Language Model architectures?
- Basis in paper: [explicit] The Conclusion notes, "our analysis may be extended to a broader range of LLMs."
- Why unresolved: The experiments were limited to the Mistral-7B-Instruct-v0.2 model for extraction, leaving the generalizability of the contamination findings unconfirmed for other model scales or architectures.
- What evidence would resolve it: A replication of the extraction experiments using diverse LLMs (e.g., Llama, GPT-4, smaller open-source models) to compare performance drops on the unseen YAGO 2026 dataset.

### Open Question 3
- Question: What standardized temporal-aware metrics can be developed to provide a more accurate assessment of temporal extraction accuracy?
- Basis in paper: [explicit] The paper states, "the absence of standardized temporal-aware metrics remains a challenge... We advocate for the development of standardized temporal-aware metrics..."
- Why unresolved: The authors relied on adapting WebNLG 2020 metrics (designed for triples) to quadruples, correcting specific bugs, but a native, standard evaluation protocol for TKGE does not exist.
- What evidence would resolve it: The proposal and community adoption of a new evaluation metric specifically designed to handle the temporal dimension of quadruples without relying on adaptations of static graph metrics.

## Limitations
- TLogic's rule-based forecasting may fail to generate plausible facts for rare relations, creating coverage gaps in the benchmark
- The contamination-free claim assumes TLogic's rule-learning is genuinely independent of LLM training data, but this is difficult to verify
- The timestamp bias effect (2 F1 points) may be specific to Mistral-7B or the EDC pipeline's canonicalization steps rather than a general LLM phenomenon

## Confidence
- High confidence in the contamination detection methodology (comparing known vs. unknown facts performance)
- Medium confidence in the timestamp bias findings (limited to specific model/pipeline)
- Medium confidence in TLogic's ability to generate structurally valid future facts (schema adherence verified but semantic plausibility uncertain)

## Next Checks
1. Test contamination sensitivity across multiple LLMs (GPT-4, Claude) to verify the YAGO 2026 benchmark reveals consistent performance drops beyond Mistral-7B
2. Validate TLogic's semantic plausibility by having human raters assess a sample of generated facts for temporal coherence and real-world reasonableness
3. Stress-test the timestamp bias by systematically varying year values (2020, 2022, 2026, 2030) while keeping facts constant to map the temporal token sensitivity curve