---
ver: rpa2
title: Trajectory Entropy Reinforcement Learning for Predictable and Robust Control
arxiv_id: '2505.04193'
source_url: https://arxiv.org/abs/2505.04193
tags:
- action
- entropy
- learning
- terl
- trajectory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes Trajectory Entropy Reinforcement Learning (TERL),
  which introduces a novel inductive bias towards simple policies in reinforcement
  learning by minimizing the entropy of entire action trajectories. This approach
  addresses the problem of deep reinforcement learning policies being sensitive to
  environmental changes due to capturing intricate correlations between observations
  and actions.
---

# Trajectory Entropy Reinforcement Learning for Predictable and Robust Control

## Quick Facts
- **arXiv ID:** 2505.04193
- **Source URL:** https://arxiv.org/abs/2505.04193
- **Reference count:** 40
- **Primary result:** TERL achieved an average reward of 137 on the H1 Walk task, significantly outperforming SAC and LZ-SAC baselines while demonstrating improved robustness to perturbations.

## Executive Summary
This paper introduces Trajectory Entropy Reinforcement Learning (TERL), a novel approach that introduces an inductive bias towards simple policies in reinforcement learning by minimizing the entropy of entire action trajectories. The method addresses the sensitivity of deep RL policies to environmental changes by encouraging the policy to produce predictable, cyclical action patterns. TERL achieves this through a variational upper bound on trajectory entropy, which is integrated into the reward function as an information-regularized objective. Experimental results on high-dimensional locomotion tasks demonstrate that TERL learns policies producing more consistent and robust behaviors compared to state-of-the-art methods like SAC and LZ-SAC.

## Method Summary
TERL extends SAC by adding an encoder that maps states to lower-dimensional representations and an autoregressive action prediction model. The core innovation is introducing trajectory entropy minimization as a regularization term in the reward function. The method uses a variational upper bound on trajectory entropy, computed using an autoregressive prediction model that conditions on consecutive state representations and the previous action. This bound is then added to the reward with coefficient α, creating an information-regularized objective. The policy, encoder, and prediction model are jointly optimized through a modified SAC algorithm that maximizes expected reward while minimizing trajectory entropy.

## Key Results
- TERL achieved an average reward of 137 on the H1 Walk task, significantly outperforming SAC and LZ-SAC baselines
- The method produced more cyclical and consistent action trajectories compared to baseline methods, as evidenced by trajectory analysis and compressibility metrics
- TERL demonstrated superior robustness to mass changes, observation noise, and action noise, maintaining the highest rewards under various perturbations
- Trajectory file sizes (measured via bzip2) were significantly smaller for TERL policies, indicating more predictable and compressible action sequences

## Why This Works (Mechanism)

### Mechanism 1: Trajectory Entropy as Simplicity Regularizer
Minimizing trajectory entropy H(a₁:T₋₁|z₁:T) penalizes complex, hard-to-compress action trajectories, biasing learning toward repetitive, cyclical patterns. This assumes simpler trajectories generalize better and are more robust to perturbations, though it may harm performance on tasks requiring complex or unpredictable behaviors.

### Mechanism 2: Variational Upper Bound for Tractable Optimization
A variational autoregressive prediction model q_ψ(aₜ|zₜ, zₜ₊₁, aₜ₋₁) provides an upper bound on trajectory entropy that can be jointly optimized with the policy. This makes the otherwise intractable entropy computation tractable by exploiting the non-negativity of KL divergence.

### Mechanism 3: Information-Regularized Reward Reshaping
Adding the log-prediction-probability term to rewards creates incentives for the policy to produce actions that are predictable given recent state transitions and previous actions. The policy receives bonus rewards for actions the prediction model assigns high probability, favoring smooth, cyclical transitions.

## Foundational Learning

- **Concept: Conditional Entropy and Information Theory**
  - Why needed here: The core objective uses H(a₁:T₋₁|z₁:T)—the uncertainty in action sequences given state representations. Understanding that entropy measures description length and that conditional entropy quantifies remaining uncertainty after conditioning is essential.
  - Quick check question: If H(actions|states) is low, what does that imply about the relationship between states and actions?

- **Concept: Variational Inference and ELBO-style Bounds**
  - Why needed here: The derivation of the tractable upper bound uses the standard variational trick: introducing an approximate distribution q and exploiting that KL divergence ≥ 0. This pattern appears throughout machine learning (VAEs, etc.).
  - Quick check question: Why does minimizing -E[log q(x)] provide an upper bound on H(p(x))?

- **Concept: Soft Actor-Critic (SAC) and Maximum Entropy RL**
  - Why needed here: TERL is implemented as an extension of SAC. Understanding SAC's entropy bonus (β log π(a|s)), its Q-function targets, and the actor-critic update rules is necessary to see how the information-regularized reward plugs in.
  - Quick check question: In SAC, what role does the entropy term play, and how does TERL's trajectory entropy term differ in objective?

## Architecture Onboarding

- **Component map:** State s → Encoder e_φ → Representation z → Policy π_θ → Action a → Reward r → Q-function Q_υ → Autoregressive predictor q_ψ (conditions on zₜ, zₜ₊₁, aₜ₋₁)

- **Critical path:**
  1. Sample batch from replay buffer
  2. Encode states → zₜ, zₜ₊₁ via encoder
  3. Compute prediction log-probability log q_ψ(aₜ|zₜ,zₜ₊₁,aₜ₋₁)
  4. Form information-regularized reward r* = r + α·log q_ψ
  5. Update Q-function using standard SAC TD loss with r*
  6. Jointly update {θ, φ, ψ} via Eq. 11 objective

- **Design tradeoffs:**
  - α selection: Grid search required; too high sacrifices task reward, too low gives weak inductive bias. Paper uses 10⁻⁵ for most tasks, 10⁻⁴ for Walker.
  - Encoder dimensionality: 30-dim latent; Assumption: sufficient to capture task-relevant state info while enabling prediction.
  - Prediction model conditioning: Uses (zₜ,zₜ₊₁,aₜ₋₁); requires storing previous action, adds replay buffer complexity.

- **Failure signatures:**
  - Policy produces overly repetitive/oscillatory behaviors that fail task → α too large
  - No improvement over vanilla SAC → α too small or prediction model undertrained
  - Training instability → encoder collapsing to trivial representations; monitor z variance
  - Poor robustness despite regularized reward → encoder may not be learning useful representations

- **First 3 experiments:**
  1. Reproduce Walker Walk baseline: Train TERL vs. SAC with α=10⁻⁴. Verify TERL achieves higher average reward (~972 vs. ~962) and visually inspect trajectory periodicity.
  2. Ablate α on Walker Walk: Test α ∈ {0, 10⁻⁵, 10⁻⁴, 10⁻³, 10⁻²}. Measure both task reward and compressibility (bzip2 file size). Expect U-shaped performance curve; confirm Table VII trend.
  3. Robustness stress test on H1 Walk: Train TERL and SAC, then evaluate under mass scaling s=1.5 and action noise σ=0.3. Verify TERL maintains >80% and >90% of original performance respectively (per Tables IV-V).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the simplicity inductive bias in TERL degrade performance on tasks that inherently require complex, non-periodic, or unpredictable behaviors, such as adversarial games?
- Basis in paper: [explicit] The authors state in the conclusion that "the simplicity inductive bias can for some tasks harm the performance... namely, when the task inherently demands complex and unpredictable behaviors, e.g. playing poker."
- Why unresolved: The experimental evaluation was restricted to locomotion tasks (e.g., walking, running) which naturally benefit from periodic and consistent action sequences.
- Evidence would resolve it: Evaluating TERL on environments requiring high policy entropy or deceptive strategies (e.g., competitive multi-agent tasks) to see if the bias restricts the emergence of necessary complex behaviors.

### Open Question 2
- Question: Can the regularization coefficient α be determined automatically based on task-specific information to eliminate the need for computationally expensive grid search?
- Basis in paper: [explicit] The authors identify a limitation where they "rely on grid search to determine the value of the hyperparameter α, leading to high computational costs," and propose exploring automatic selection in future work.
- Why unresolved: The current implementation requires manually tuning α (e.g., 10⁻⁵ vs 10⁻⁴) for different tasks like Walker Walk.
- Evidence would resolve it: Developing a mechanism (e.g., based on state dimension or learning progress) that dynamically adjusts α and demonstrates comparable performance to the manually tuned baseline.

### Open Question 3
- Question: Can alternative upper bounds on trajectory entropy be derived to provide tighter estimates or more efficient optimization than the current variational autoregressive factorization?
- Basis in paper: [explicit] The authors suggest that "developing additional upper bounds of our trajectory entropy objective to further improve performance presents another avenue for future research."
- Why unresolved: The paper relies on a single specific variational approximation q(a₁:T₋₁|z₁:T), which may not be the most optimal formulation for estimating trajectory complexity.
- Evidence would resolve it: The formulation of a novel upper bound that yields higher rewards or faster convergence rates on the same high-dimensional locomotion benchmarks.

## Limitations
- The empirical evaluation is limited to locomotion tasks, leaving unclear whether trajectory entropy regularization generalizes to domains requiring diverse or adversarial behaviors.
- The choice of regularization coefficient α requires careful tuning through grid search, with no principled method for automatic selection provided.
- The paper lacks ablation studies on the autoregressive prediction model architecture, leaving open questions about the necessity of specific design choices.

## Confidence
- **High confidence:** TERL achieves superior performance on the tested locomotion tasks compared to SAC and LZ-SAC, with statistically significant improvements (e.g., Walker Walk average reward of 137 vs. SAC's lower scores).
- **Medium confidence:** The mechanism by which minimizing trajectory entropy induces simpler, more robust behaviors is theoretically sound but relies on assumptions about task complexity that may not hold universally.
- **Low confidence:** The generalizability of TERL to non-locomotion domains (e.g., robotic manipulation, game playing) remains unproven, and the robustness benefits under extreme perturbations have not been thoroughly explored.

## Next Checks
1. **Generalization Test:** Evaluate TERL on a task requiring behavioral diversity (e.g., a multi-goal navigation task) to assess whether trajectory entropy regularization harms performance in such settings.
2. **Ablation Study:** Systematically vary the prediction model architecture (e.g., remove autoregressive conditioning, reduce latent dimension) to quantify the contribution of specific design choices to TERL's performance.
3. **Extreme Perturbation Analysis:** Test TERL's robustness under extreme mass scaling (e.g., s=3.0) or high action noise (e.g., σ=0.5) to determine the limits of its robustness benefits.