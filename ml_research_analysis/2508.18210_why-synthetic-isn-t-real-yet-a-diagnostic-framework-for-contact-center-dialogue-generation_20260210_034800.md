---
ver: rpa2
title: 'Why Synthetic Isn''t Real Yet: A Diagnostic Framework for Contact Center Dialogue
  Generation'
arxiv_id: '2508.18210'
source_url: https://arxiv.org/abs/2508.18210
tags:
- stage
- call
- transcript
- generation
- synthetic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a diagnostic framework for evaluating synthetic
  dialogue generation in contact centers, where privacy and data scarcity limit access
  to real transcripts. Unlike prior work focused on open-domain or medical dialogues,
  this framework addresses goal-oriented, role-asymmetric, and behaviorally complex
  conversations featuring disfluencies, ASR noise, and compliance-driven agent actions.
---

# Why Synthetic Isn't Real Yet: A Diagnostic Framework for Contact Center Dialogue Generation

## Quick Facts
- **arXiv ID:** 2508.18210
- **Source URL:** https://arxiv.org/abs/2508.18210
- **Authors:** Rishikesh Devanathan; Varun Nathan; Ayush Kumar
- **Reference count:** 40
- **Primary result:** No synthetic dialogue generation method excels across all realism traits; diagnostic framework reveals specific gaps in disfluency, sentiment, and behavioral realism.

## Executive Summary
This paper addresses the challenge of generating realistic synthetic dialogue for contact centers, where privacy and data scarcity limit access to real transcripts. The authors introduce a diagnostic framework of 18 latent metrics to evaluate the behavioral and linguistic realism of synthetic transcripts, focusing on goal-oriented, role-asymmetric conversations with disfluencies and ASR noise. Four generation strategies—including single-stage and dual-stage pipelines—are benchmarked against real data, revealing that high fidelity to input prompts does not equate to behavioral realism. The framework exposes specific gaps in disfluency, sentiment, and behavioral realism, guiding future improvements in synthetic dialogue generation.

## Method Summary
The method leverages structured supervision signals (intent summaries, topic flows, QA forms) derived from real contact center data to guide synthetic dialogue generation. Four generation pipelines are implemented: single-stage, dual-stage (with turn count/length segmentation), and characteristic-aware approaches. The diagnostic framework evaluates 18 latent traits (e.g., disfluency, sentiment arcs, ASR noise) using LLM-based classifiers and statistical tests (JS Divergence, Chi-square) against a real data reference. Generation is optimized using DSPy to maximize a composite reconstruction score, but results show that reconstruction does not predict realism.

## Key Results
- No generation method excels across all realism traits; each has specific strengths and weaknesses.
- Dual-stage generation with turn count segmentation best captures disfluency, ASR noise, and interruptions.
- High reconstruction scores do not correlate with behavioral realism; latent trait distributions diverge from real data.
- Deficits in disfluency and ASR noise suggest a need for multimodal inputs or tighter ASR integration.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Structured supervision using intent summaries and QA forms may act as "semantic anchors," reducing hallucination in synthetic dialogue generation.
- **Mechanism:** The system conditions generation on modular, interpretable supervision signals (derived from real call attributes like intent summaries, topic flows, and QA forms) rather than open-ended prompts. This constrains the LLM's output space to align with specific business goals and compliance requirements.
- **Core assumption:** The assumption is that the structured metadata (summaries, QA scores) accurately captures the semantic and behavioral essence of the real calls they represent.
- **Evidence anchors:**
  - [abstract] "We leverage these as supervision signals to guide generation."
  - [section 2] "Intent-Specific Summaries capture the semantic backbone... act as anchors that prevent hallucinations."
  - [corpus] The paper *FECT* highlights the challenge of hallucinations in enterprise contact center AI, supporting the need for factuality constraints.
- **Break condition:** If the input structured data is noisy, incomplete, or contradictory, the generation may "faithfully" reproduce errors or fail to form a coherent dialogue.

### Mechanism 2
- **Claim:** A dual-stage generation process (planning followed by realization) appears necessary to introduce realistic noise and disfluencies without losing semantic coherence.
- **Mechanism:** A base "clean" transcript is generated first to ensure logic and intent adherence. Then, the transcript is segmented, and specific chunks are enhanced with disfluencies, interruptions, and ASR errors. This decouples structural logic from surface realization.
- **Core assumption:** This mechanism assumes that LLMs struggle to simultaneously maintain strict logical adherence to complex prompts while also generating the "imperfect" surface features of spontaneous speech.
- **Evidence anchors:**
  - [section 3.1] "By splitting the conversation into smaller, semantically coherent segments... the method reduces the risk of long-context drift... while enabling fine-grained insertion."
  - [section 5.4] "Dual Stage - Turn Count leads on disfluency, ASR noise, and interruptions."
  - [corpus] Weak corpus support; no directly comparable dual-stage architecture was found in the neighbor papers for contact centers specifically.
- **Break condition:** If the segmentation logic fails to respect semantic boundaries, the insertion of interruptions or noise may disrupt the conversational flow or logic.

### Mechanism 3
- **Claim:** Diagnostic evaluation using 18 latent metrics reveals that high fidelity to input prompts ("Reconstruction") does not equate to behavioral realism.
- **Mechanism:** The framework evaluates realism by comparing the distribution of generated traits (e.g., sentiment arcs, disfluency rates) against real data using statistical tests (Chi-square, JS Divergence), rather than just checking if the output matches the input prompt.
- **Core assumption:** The assumption is that "realism" is best defined by the statistical similarity of latent features to a reference corpus, rather than semantic correctness alone.
- **Evidence anchors:**
  - [abstract] "Results show no method excels across all traits... The framework exposes these gaps."
  - [section 5.5] "High reconstruction scores do not imply stronger realism... [they] fail to predict realism."
  - [corpus] *Strong Reasoning Isn't Enough* corroborates the idea that evaluation must move beyond simple reasoning correctness to process-oriented metrics.
- **Break condition:** If the reference "real" data is biased or if the latent metric classifiers (used for evaluation) are inaccurate, the diagnostic framework may optimize for the wrong distributional targets.

## Foundational Learning

- **Concept:** **Role-Asymmetric, Goal-Oriented Dialogue**
  - **Why needed here:** Unlike open-domain chat, contact center dialogues have strict roles (Agent vs. Customer) and specific goals (resolution, compliance). You must understand these constraints to evaluate why "open-ended" generation fails here.
  - **Quick check question:** Can you distinguish between a "chit-chat" dialogue and a "transactional" dialogue in terms of the speaker's objectives?

- **Concept:** **Speech Disfluencies and ASR Artifacts**
  - **Why needed here:** Real transcripts are messy (fillers, stuttering, misheard words). LLMs naturally generate clean text; understanding these artifacts is critical to diagnosing why synthetic data often feels "artificial."
  - **Quick check question:** What is the difference between a hesitation (planning pause) and an ASR deletion (word missed by the transcriber)?

- **Concept:** **Distributional Evaluation (JS Divergence/Chi-square)**
  - **Why needed here:** Standard metrics like BLEU measure n-gram overlap. This paper argues for measuring how well the *frequency* of behaviors (e.g., 20% of turns are "frustrated") matches reality.
  - **Quick check question:** If a generator produces the correct sentiment only 10% of the time but the real data shows it 50% of the time, why would a simple accuracy score be misleading?

## Architecture Onboarding

- **Component map:** Input Processor -> Generator (LLM) -> Perturbation Module (Dual-Stage only) -> Diagnostic Evaluator
- **Critical path:** The **Reconstruction Score** loop is critical. You must tune the generator so it adheres to the input attributes (Intent/Topic/QA) *before* attempting to fix stylistic issues, or you risk generating realistic-sounding nonsense.
- **Design tradeoffs:**
  - **Single-Stage vs. Dual-Stage:** Single-stage is cheaper and faster but lacks noise/realism. Dual-stage captures noise better but adds latency and complexity.
  - **Prompt-Conditioning vs. Post-Processing:** The paper suggests that simply prompting for "noisy speech" (Single Stage) is insufficient; explicit post-processing or dual-stage enhancement is required for disfluency fidelity.
- **Failure signatures:**
  - **The "Clean" Hallucination:** The transcript reads perfectly (no typos/noise) but describes events not found in the Intent Summary.
  - **The "Drift":** A long transcript where the agent forgets the customer's original complaint (Topic Flow breakdown).
  - **The "Positive Bias":** The synthetic agent is overly polite and lacks the "frustration" distribution seen in real customers (Sentiment Arc mismatch).
- **First 3 experiments:**
  1. **Sanity Check (Input Adherence):** Run the Single-Stage pipeline. Use the *Intent Summary* and *QA Replication* scorers to verify the LLM is actually following the provided metadata. Target >0.90 Intent Score (as per paper findings).
  2. **Realism Gap Analysis:** Generate 50 transcripts using Single-Stage. Run the 18-metric Diagnostic Framework. Identify the top 3 metrics with the highest JS Divergence (likely Disfluency and Sentiment) to confirm the "clean text" bias.
  3. **Noise Injection Validity:** Implement the "Dual Stage - Turn Count" pipeline. Compare the *Disfluency* and *ASR Noise* JS Divergence scores against the Single-Stage baseline to validate if the chunk-enhancement mechanism actually improves distributional fit.

## Open Questions the Paper Calls Out

- **Can a hybrid generation strategy that combines the semantic fidelity of single-stage prompting with the behavioral induction of characteristic-aware approaches consistently outperform individual methods?**
  - Basis in paper: [explicit] Section 7 (Limitations) states, "we do not investigate hybrid approaches that selectively combine their strengths—e.g., using single-stage prompting for semantic fidelity and characteristic-aware generation for behavioral induction."
  - Why unresolved: The paper benchmarks four pipelines in isolation, finding that no single method excels across all traits (e.g., Single Stage is best for intent, Characteristic Aware for repetition, but neither wins globally).
  - What evidence would resolve it: A study implementing a modular pipeline that switches strategies per turn or chunk, demonstrating superior aggregate scores across the 18 diagnostic metrics.

- **Do reinforcement learning or differentiable objectives provide more robust induction of complex traits like disfluency and ASR noise compared to the current rule-based supervision?**
  - Basis in paper: [explicit] Section 7 notes the pipeline "does not leverage reinforcement learning or differentiable objectives to induce traits like disfluency, noise, or emotion more robustly."
  - Why unresolved: Current methods rely on probabilistic sampling and sequential prompting, which fail to replicate the distribution of disfluencies and ASR noise found in real data (Section 5.4).
  - What evidence would resolve it: Experiments using policy-gradient methods (e.g., RLHF) to optimize generation specifically against the disfluency and noise metrics defined in the framework.

- **Does the integration of acoustic signals or phonetic modeling improve the simulation of speech-driven artifacts, specifically ASR noise and disfluencies?**
  - Basis in paper: [explicit] Section 5.4 concludes that deficits in ASR noise and disfluency "may require multimodal inputs, phonetic modeling, or tighter ASR integration," as these traits are poorly captured by text-only prompts.
  - Why unresolved: Text-only LLMs struggle to simulate the "spoken modality" artifacts like prosody and transcription errors naturally.
  - What evidence would resolve it: A comparison of text-only generation against a multimodal pipeline that conditions on or generates acoustic features, showing statistically significant alignment (higher p-values) in ASR noise metrics.

- **Can evaluation-aware tuning objectives successfully align "Reconstruction Scores" with actual behavioral realism?**
  - Basis in paper: [inferred] Section 5.5 and the Conclusion highlight a "weak correlation" where high reconstruction scores (adherence to input) do not predict realism (behavioral fidelity), suggesting a need for "evaluation-aware tuning."
  - Why unresolved: Optimizing for input adherence (topic flow, QA) does not guarantee the latent conversational properties (sentiment, emphasis) match real distributions.
  - What evidence would resolve it: A new training regime that incorporates the 18 diagnostic metrics directly into the loss function, resulting in a positive correlation between reconstruction scores and JS divergence metrics.

## Limitations
- The diagnostic framework's reliance on proprietary data limits reproducibility; surrogate datasets are suggested but exact reference distributions are unclear.
- LLM-based classifiers for the 18 latent metrics are not independently validated, introducing uncertainty about metric reliability.
- The paper does not provide a detailed error analysis for persistent gaps in disfluency and sentiment realism, leaving open whether these are fundamental or addressable.

## Confidence
- **High confidence:** The core contribution—the diagnostic framework and the finding that reconstruction score does not predict behavioral realism—is well-supported and aligns with prior work.
- **Medium confidence:** The superiority of dual-stage pipelines for noise and disfluency is demonstrated, but generalizability to other domains or languages is not established.
- **Low confidence:** The exact impact of structured supervision signals on hallucination reduction is inferred but not directly quantified.

## Next Checks
1. **Classifier reliability audit:** Independently validate the accuracy of the LLM-based classifiers used for the 18 diagnostic metrics against a held-out annotated subset of the real data.
2. **Cross-dataset generalization:** Apply the diagnostic framework to a non-proprietary contact center dataset (e.g., a public subset of MultiWOZ or Schema-Guided Dialogue) to assess whether the identified realism gaps persist across data sources.
3. **Ablation of supervision signals:** Conduct an ablation study to quantify the individual and combined impact of intent summaries, topic flows, and QA forms on hallucination and behavioral realism.