---
ver: rpa2
title: 'Ibom NLP: A Step Toward Inclusive Natural Language Processing for Nigeria''s
  Minority Languages'
arxiv_id: '2511.06531'
source_url: https://arxiv.org/abs/2511.06531
tags:
- languages
- language
- gemini
- flash
- ibom
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work introduces IBOM, a dataset for machine translation and
  topic classification in four low-resource Nigerian languages: Anaang, Efik, Ibibio,
  and Oro. The authors extend the Flores-200 and SIB-200 benchmarks to these languages,
  creating the first parallel language resource for Anaang and Oro.'
---

# Ibom NLP: A Step Toward Inclusive Natural Language Processing for Nigeria's Minority Languages

## Quick Facts
- arXiv ID: 2511.06531
- Source URL: https://arxiv.org/abs/2511.06531
- Reference count: 16
- Introduces IBOM datasets for machine translation and topic classification in four low-resource Nigerian languages

## Executive Summary
This work introduces IBOM, a dataset for machine translation and topic classification in four low-resource Nigerian languages: Anaang, Efik, Ibibio, and Oro. The authors extend the Flores-200 and SIB-200 benchmarks to these languages, creating the first parallel language resource for Anaang and Oro. They evaluate both fine-tuned baselines (M2M-100, NLLB-200) and LLM prompting (GPT-4.1, o4-mini, Gemini models) on these tasks. Results show that current LLMs perform poorly on machine translation in zero-shot settings, but few-shot prompting (5-20 shots) steadily improves topic classification performance, sometimes exceeding supervised fine-tuning baselines. A two-stage fine-tuning approach leveraging Efik religious data improves translation quality for related languages. Human evaluation confirms the weakness of automatic metrics for these languages. The IBOM datasets are made publicly available to support future research on underrepresented African languages.

## Method Summary
The study introduces IBOM-MT (3,009 parallel sentences) and IBOM-TC (1,004 labeled sentences for 200 topics) for four Nigerian languages. For machine translation, they fine-tune M2M-100 and NLLB-200 models using a two-stage approach: first on 331K Efik-English religious parallel sentences from JW300, then on 1K target language pairs from IBOM. For topic classification, they fine-tune various encoders (AfroXLMR-61L, AfroXLMR, Serengeti, Glot500, XLM-R) and evaluate LLM prompting (GPT-4.1, o4-mini, Gemini 2.0/2.5 Flash) with zero/5/10/20-shot settings. All models are evaluated on MT (ChrF++, BLEU, SSA-COMET) and TC (accuracy) metrics, with human direct assessment validating results.

## Key Results
- Current LLMs perform poorly on machine translation in zero-shot settings for these low-resource languages
- Few-shot prompting (5-20 shots) steadily improves topic classification performance, sometimes exceeding supervised fine-tuning baselines
- Two-stage fine-tuning leveraging Efik religious data improves translation quality for related languages (Anaang, Ibibio) but shows modest gains for Oro
- Human evaluation confirms the weakness of automatic metrics (ChrF++, BLEU, SSA-COMET) for these languages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Two-stage fine-tuning leveraging a related language with more data improves translation quality for low-resource related languages.
- Mechanism: Stage 1 fine-tunes on 331K Efik-English religious parallel sentences (JW300/MT560), then Stage 2 fine-tunes on 1K target language pairs. Cross-lingual transfer occurs because Ibibio, Efik, and Anaang belong to the same Efik-Ibibio sub-family.
- Core assumption: Linguistic closeness (shared phonology, morphology, SVO syntax) enables positive transfer; representations learned for Efik partially generalize.
- Evidence anchors:
  - [abstract]: "A two-stage fine-tuning approach leveraging Efik religious data improves translation quality for related languages."
  - [section 5.1.1]: "Since the fine-tuning data is limited, we leverage the effectiveness of cross-lingual transfer to improve performance on related Ibom languages that are not covered in existing multilingual models."
  - [corpus]: No direct corpus evidence addresses this specific mechanism for Akwa Ibom languages.
- Break condition: Linguistic distance matters—Oro (different NsíN Oro sub-family) showed modest improvement vs. Anaang/Ibibio. Transfer degrades when target language diverges significantly from source.

### Mechanism 2
- Claim: Few-shot prompting improves topic classification but has inconsistent effects on machine translation for low-resource languages.
- Mechanism: In-context examples provide task structure and label space for classification (discrete outputs), helping models leverage latent multilingual knowledge. For MT (open-ended generation), examples cannot compensate for missing vocabulary/representation learning.
- Core assumption: Proprietary LLMs (especially Gemini) have some latent representation of these languages from undisclosed training data.
- Evidence anchors:
  - [abstract]: "few-shot prompting (5-20 shots) steadily improves topic classification performance, sometimes exceeding supervised fine-tuning baselines."
  - [section 6.1]: "We find that 5-shots improve performance across all LLMs... However, in the xx→en direction, 20-shots lead to a significant drop in performance."
  - [corpus]: FormosanBench paper notes similar patterns—LLMs struggle on low-resource language tasks despite few-shot prompting.
- Break condition: More shots ≠ better performance. Gemini 2.5 Flash 20-shot collapsed in xx→en direction (ChrF++ dropped from 31.0 to 18.2). Diminishing or negative returns beyond optimal shot count.

### Mechanism 3
- Claim: Automatic MT metrics (ChrF++, BLEU, SSA-COMET) misalign with human judgment for truly low-resource languages.
- Mechanism: Metrics trained on higher-resource languages don't capture tonal nuances, agglutinative morphology, or proper noun handling specific to these languages. Different metrics also disagree with each other.
- Core assumption: Human native speaker judgment is ground truth; metrics should approximate human DA scores.
- Evidence anchors:
  - [abstract]: "Human evaluation confirms the weakness of automatic metrics for these languages."
  - [section 6.2]: "For the others, they are between 0.5 and 0.65 [spearman correlation]... Out of the four languages, only Oro did not meet this criteria (<0.2), so we excluded it."
  - [corpus]: No corpus evidence directly addresses metric reliability for these specific languages.
- Break condition: When inter-annotator agreement is low (Oro <0.2), human evaluation itself fails. No reliable ground truth exists in this regime.

## Foundational Learning

- Concept: Cross-lingual Transfer via Language Families
  - Why needed here: Understanding when transfer works requires knowing linguistic relationships—Efik/Ibibio/Anaang share sub-family; Oro does not.
  - Quick check question: If you have 100K parallel sentences for Efik but none for Ibibio, what conditions must hold for positive transfer?

- Concept: Agglutinating Morphology and Tonal Systems
  - Why needed here: These languages have high/low/downstepped/contour tones and agglutinative morphology—tokenization and character-level metrics matter.
  - Quick check question: Why might ChrF++ (character n-grams) outperform BLEU (word-level) for agglutinating languages?

- Concept: Curse of Multilinguality vs. Targeted Adaptation
  - Why needed here: AfroXLMR-61L (61 African languages) outperforms Serengeti (500 languages) despite covering fewer languages.
  - Quick check question: When does adding more languages to pre-training hurt per-language performance?

## Architecture Onboarding

- Component map:
  - Data: IBOM-MT (3,009 parallel sentences: 1K train, 997 dev, 1,012 test); IBOM-TC (1,004 labeled for 200 topics)
  - Fine-tuned encoders: AfroXLMR-61L, AfroXLMR, Serengeti, Glot500, XLM-R (classification)
  - Encoder-decoders: M2M-100 (418M), NLLB-200 (600M) with 2-stage fine-tuning (MT)
  - LLM prompting: GPT-4.1, o4-mini, Gemini 2.0 Flash, Gemini 2.5 Flash (0/5/10/20-shot)

- Critical path:
  1. Stage 1: Fine-tune M2M-100/NLLB-200 on 331K Efik-English religious data
  2. Stage 2: Fine-tune on 1K IBOM training pairs per language
  3. Evaluate on devtest with ChrF++, BLEU, SSA-COMET; validate with human DA

- Design tradeoffs:
  - **AfroXLMR vs Serengeti**: Targeted African adaptation (61 langs) beats broad coverage (500 langs) for these languages
  - **Fine-tuning vs Few-shot**: Fine-tuning wins for MT; few-shot approaches fine-tuning for classification with 20 shots
  - **Efik-only pre-training data**: Only Efik appears in Glot500/AfroXLMR-61L/Serengeti; Anaang and Oro are completely unseen

- Failure signatures:
  - LLMs: ChrF++ <25 on en→xx for zero-shot; catastrophic drop with 20-shot in xx→en
  - Metrics: ChrF++ and BLEU disagree; both disagree with SSA-COMET; none correlate perfectly with human DA
  - Annotation: Inter-annotator agreement <0.2 (Oro) invalidates human evaluation

- First 3 experiments:
  1. **Replicate 2-stage baseline**: Fine-tune M2M-100 on Efik religious data → fine-tune on IBOM-MT; measure ChrF++ gap between Anaang (close) and Oro (distant)
  2. **Few-shot scaling curve**: Test Gemini 2.5 Flash with 5/10/15/20 shots on IBOM-TC; plot accuracy vs shot count to find plateau/collapse point
  3. **Metric correlation audit**: On 50 samples, collect human DA from 3 bilingual annotators; compute spearman correlation between ChrF++, BLEU, SSA-COMET, and human scores

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why do "thinking" models (o4-mini, Gemini 2.5 Flash thinking) perform worse than their non-thinking counterparts on machine translation for the Ibom languages?
- Basis in paper: [explicit] Authors state: "Surprisingly, thinking models such as o4-mini and Gemini 2.5 Flash performed worse than their non-thinking counterparts."
- Why unresolved: The paper observes this phenomenon but does not investigate whether it stems from token budget issues, lack of language-specific reasoning patterns in training, or other factors.
- What evidence would resolve it: Ablation studies varying thinking budgets and analyzing intermediate reasoning outputs for these specific languages.

### Open Question 2
- Question: How can evaluation metrics be developed or adapted to better align with human judgment for truly low-resource languages like Anaang, Efik, Ibibio, and Oro?
- Basis in paper: [explicit] Authors state: "This evaluation highlights the weakness of the current evaluation metrics for many low-resource languages" and call for investment in "metric development alongside MT development."
- Why unresolved: ChrF++, BLEU, and SSA-COMET show misalignment with human direct assessment; e.g., Gemini 2.5 Flash scored higher on ChrF++ but humans preferred M2M-100 outputs by 20+ points for Anaang and Efik.
- What evidence would resolve it: Creation of COMET-style metrics with source language support and human-annotated training data for these specific languages.

### Open Question 3
- Question: Does the two-stage cross-lingual transfer approach using Efik as a pivot generalize to minority Nigerian languages from different language families?
- Basis in paper: [inferred] Authors note Oro showed "more modest improvement compared to the other languages due to being linguistically farther away" and state in Limitation (A) that results "may not generalize to other languages."
- Why unresolved: The transfer effectiveness appears tied to linguistic proximity within the Lower Cross branch; applicability to unrelated minority languages remains untested.
- What evidence would resolve it: Applying the methodology to minority languages from different Nigerian language families (e.g., Adamawa, Plateau) and comparing transfer gains.

### Open Question 4
- Question: Why do few-shot examples substantially improve Gemini's topic classification but yield minimal gains for GPT-4.1 and o4-mini on these languages?
- Basis in paper: [explicit] Authors state: "This suggests that Gemini is likely more multilingual than the OpenAI models, although further investigation is needed."
- Why unresolved: The differential few-shot benefit is documented but its cause—training data coverage, architecture, or tokenization—remains unidentified.
- What evidence would resolve it: Comparative analysis of multilingual representations and training data composition across model families for these specific languages.

## Limitations
- Data representativeness: Only 1,000 training sentences per language may be insufficient for robust generalization across diverse topics and domains
- Model accessibility: LLM prompting results depend on proprietary models with undisclosed training data composition and capabilities
- Linguistic distance measurement: Specific linguistic features enabling transfer are not quantified, making it difficult to predict transfer success for other language pairs

## Confidence
- Cross-lingual transfer effectiveness (High): Well-supported by experimental results showing consistent improvements for related languages
- Few-shot prompting benefits for classification (High): Clear evidence of steady improvement with 5-20 shots
- Automatic metric unreliability (Medium): Supported by human evaluation, but Oro inter-annotator agreement <0.2 invalidates those results
- LLM MT performance limits (Medium): Performance limits are documented but inconsistent few-shot effects suggest complex underlying mechanisms

## Next Checks
1. **Two-stage transfer replication**: Replicate the M2M-100 fine-tuning pipeline with Efik religious data → IBOM-MT pairs, measuring ChrF++ gap between Anaang (close) and Oro (distant)
2. **Few-shot scaling curve validation**: Test Gemini 2.5 Flash with 5/10/15/20 shots on IBOM-TC, plotting accuracy vs shot count to identify the optimal range
3. **Metric correlation audit**: Collect human direct assessment (DA) scores from 3 bilingual annotators on 50 MT samples, computing spearman correlation between ChrF++, BLEU, SSA-COMET, and human scores