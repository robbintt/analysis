---
ver: rpa2
title: 'Cognitive Maps in Language Models: A Mechanistic Analysis of Spatial Planning'
arxiv_id: '2511.13371'
source_url: https://arxiv.org/abs/2511.13371
tags:
- spatial
- layer
- training
- foraging
- context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates how large language models solve spatial\
  \ navigation tasks by comparing models trained on exploratory random walks (Foraging\
  \ Model) versus goal-directed shortest-path finding (SP Models). Using behavioral,\
  \ representational, and mechanistic analyses on 4\xD74 grid environments, the authors\
  \ uncover two fundamentally different algorithms."
---

# Cognitive Maps in Language Models: A Mechanistic Analysis of Spatial Planning

## Quick Facts
- **arXiv ID**: 2511.13371
- **Source URL**: https://arxiv.org/abs/2511.13371
- **Reference count**: 22
- **Key outcome**: Exploratory training data enables language models to develop robust allocentric spatial representations, while goal-directed data produces brittle path-dependent algorithms.

## Executive Summary
This study investigates how transformer models solve spatial navigation tasks by comparing models trained on exploratory random walks versus goal-directed shortest-path finding. Using behavioral, representational, and mechanistic analyses on 4×4 grid environments, the authors uncover two fundamentally different algorithms. The Foraging Model develops a robust, map-like spatial representation that consolidates into a self-sufficient coordinate system by layer 8, enabling 98.3% accuracy on 5×5 grids and perfect Hamiltonian cycle completion. The model adaptively switches between local heuristics for short contexts and global map-based reasoning for longer ones. In contrast, SP models learn brittle, path-dependent algorithms relying continuously on explicit directional inputs, with the Hamiltonian-trained model failing on most generalization tasks (3.6% accuracy on 5×5 grids).

## Method Summary
The study trains GPT-2 small models on 4×4 grid navigation tasks with three paradigms: Foraging Model (predict next direction-node pair from 120-step random walks), SP-Hamiltonian (generate shortest paths given Hamiltonian context walks), and SP-Random Walk (fine-tune SP-H with random walk contexts). Models use custom two-letter node IDs and cardinal direction tokens. Training uses AdamW optimizer with specified batch sizes and learning rates. Evaluation includes next-step prediction accuracy, shortest-path accuracy, and generalization to 5×5/6×6 grids. Mechanistic analysis employs linear probe R² for coordinate decoding and layer-wise direction token ablations to measure recovery curves.

## Key Results
- Foraging Model achieves 98.3% accuracy on 5×5 grids and perfect Hamiltonian cycle completion through internal map representation
- SP-Hamiltonian model fails on most generalization tasks (3.6% accuracy on 5×5 grids) due to path-dependent algorithm
- Foraging Model shows phase transition at Layer 8 where spatial representation becomes independent of directional tokens, evidenced by linear probe R² ≈ 0.93

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The Foraging Model implements a two-stage spatial processing algorithm, evolving from local updates to a global coordinate system.
- **Mechanism**: In early layers (Layer 1), attention heads attend to the penultimate node to execute local directional updates. By Layer 8, the model consolidates sequential information into a linearly decodable, allocentric coordinate system.
- **Core assumption**: Linear probe accuracy (R² ≈ 0.93) reflects an internalized Cartesian coordinate system rather than statistical correlation.
- **Evidence anchors**: Sharp phase transition where reliance on historical direction tokens vanishes by middle layers; Layer 1 attention to penultimate node; Layer 8 coordinate consolidation.
- **Break condition**: Direction-token ablation at Layer 8+ causing performance to drop to chance levels falsifies self-sufficiency claim.

### Mechanism 2
- **Claim**: Goal-directed (Shortest Path/SP) models learn continuous, path-dependent integration rather than map-based representation.
- **Mechanism**: SP models rely on continuous access to explicit directional tokens across all 12 layers to compute position, functioning as path integrators rather than map references.
- **Core assumption**: "Horizontal mirroring effect" in PCA implies brittle compression strategy rather than robust geometric understanding.
- **Evidence anchors**: Continuous dependence on directional tokens throughout all layers; ongoing path integration for position computation.
- **Break condition**: SP models maintaining high accuracy when historical direction tokens are ablated at middle layers.

### Mechanism 3
- **Claim**: Foraging Model dynamically arbitrates between low-level heuristic and global reasoning based on context length.
- **Mechanism**: For short contexts (2-3 steps), model defaults to "reverse bias" heuristic. As context increases (>30 steps), model shifts to global map-based strategy, mitigating Clever Hans effects.
- **Core assumption**: Performance dip at mid-range contexts (11 steps) represents handoff period between heuristic and map representation.
- **Evidence anchors**: 100% accuracy at 2-3 steps with high reverse bias; dropping to 71% around 11 steps; recovering to >96% beyond 30 steps.
- **Break condition**: Reverse bias persisting at high context lengths invalidates adaptive switching claim.

## Foundational Learning

- **Concept: Allocentric vs. Egocentric Representation**
  - **Why needed here**: The paper hinges on model transitioning from egocentric (relative direction updates) to allocentric (world-centered coordinates) representations.
  - **Quick check question**: Does model's representation of a node change if arrival direction changes? (Answer should be "No" for allocentric map).

- **Concept: Causal Intervention (Ablation/Patching)**
  - **Why needed here**: Study moves beyond correlation to causation by zeroing out direction tokens to test necessity.
  - **Quick check question**: If accuracy is 100% when ablating directions at Layer 8 but 0% at Layer 2, where is the "map" located?

- **Concept: Phase Transition in Neural Networks**
  - **Why needed here**: "Sharp phase transition" is primary evidence for discrete change in algorithm (local integration to global map).
  - **Quick check question**: Does performance degrade linearly or drop off a cliff? (Linear implies continuous dependence; cliff implies discrete structural change).

## Architecture Onboarding

- **Component map**: Input (Node IDs + Direction Tokens) → Layer 1 Attention (Penultimate node, "reverse heuristic") → Layers 3-7 Integration (Noisy spatial encoding) → Layer 8 Consolidation (Cognitive Map emerges, R² ≈ 0.93) → Layers 9-12 Functional (Navigational affordances clustering)

- **Critical path**: Flow of spatial information from Layer 1 Attention (directional update) → Layer 8 Residual Stream (coordinate consolidation)

- **Design tradeoffs**:
  - Foraging: High generalization (98.3% on 5×5), robust internal map, requires exploratory training data
  - SP Models: Perfect performance on trained distribution, but brittle (3.6% generalization) and path-dependent (no internal map)

- **Failure signatures**:
  - Foraging: Performance dip at ~11 steps (transition uncertainty)
  - SP Models: Failure on paths > Manhattan Distance 6 (out-of-distribution), "mirroring" errors confusing symmetric positions
  - General: Reliance on "Clever Hans" heuristics (reverse bias) when context insufficient for map inference

- **First 3 experiments**:
  1. Run Foraging Model on 4-hop loop; zero out direction tokens at input to Layer 8. Verify if accuracy remains high (proving self-sufficiency).
  2. Train linear classifier on Layer 8 hidden states to predict (x,y) coordinates. Confirm R² > 0.90.
  3. Patch Layer 1 output from "NORTH" context into "WEST" context in Foraging Model. Confirm output direction changes to "NORTH" (proving directional encoding is localized and transferable).

## Open Questions the Paper Calls Out

- **Open Question 1**: Does emergence of cognitive maps in Foraging Model scale to larger transformer architectures and more complex, non-grid environments? (Unresolved: study restricted to 124M parameter model and simple grid topologies)
- **Open Question 2**: What are specific computational circuits that transform local directional updates in early layers into global coordinate system? (Unresolved: processing flow between Layer 1 and Layer 8 remains black box)
- **Open Question 3**: What is mechanistic basis for brittleness and failure of generalization in Shortest-Path models? (Unresolved: mechanistic interventions focused on Foraging Model; SP model failures not isolated)

## Limitations
- Dataset construction uncertainty regarding custom BTP tokenizer training and sequence regeneration frequency
- Generalization task heterogeneity making direct algorithmic comparison challenging
- Mechanistic intervention granularity not fully exploring intermediate layers or alternative intervention points

## Confidence
- **High Confidence**: Foraging Model develops robust internal representations enabling generalization beyond training distribution (supported by multiple independent metrics)
- **Medium Confidence**: Two-stage algorithm evolution from local updates to global coordinate system (ablation evidence compelling but alternative explanations possible)
- **Medium Confidence**: SP models' path-dependent algorithm characterization (continuous reliance on direction tokens documented but "horizontal mirroring" interpretation speculative)

## Next Checks
1. Systematically ablate direction tokens at every layer (1-12) for Foraging Model on 4-hop loops, measuring performance at each point to test whether phase transition is truly sharp or gradual.
2. Train new Foraging Model on 3×3 grids and test performance on 4×4, 5×5, and 6×6 grids to determine whether model learns scalable map representation or memorizes grid-specific patterns.
3. Train Foraging Model, then during testing randomly permute meaning of direction tokens (swap NORTH/SOUTH, rotate all directions). If accuracy remains high, it proves model has internalized allocentric map rather than relying on token semantics.