---
ver: rpa2
title: Jointly Reinforcing Diversity and Quality in Language Model Generations
arxiv_id: '2509.02534'
source_url: https://arxiv.org/abs/2509.02534
tags:
- diversity
- cited
- page
- zhang
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of diversity collapse during post-training
  of large language models, where improvements in response quality often come at the
  expense of reduced output diversity, limiting usefulness in creative tasks. The
  authors propose Diversity-Aware Reinforcement Learning (Darling), which jointly
  optimizes for response quality and semantic diversity.
---

# Jointly Reinforcing Diversity and Quality in Language Model Generations

## Quick Facts
- **arXiv ID:** 2509.02534
- **Source URL:** https://arxiv.org/abs/2509.02534
- **Reference count:** 40
- **Primary result:** Proposes DARLING to jointly optimize LLM generations for both quality and semantic diversity, preventing diversity collapse during post-training.

## Executive Summary
This paper addresses the problem of diversity collapse during post-training of large language models, where improvements in response quality often come at the expense of reduced output diversity, limiting usefulness in creative tasks. The authors propose Diversity-Aware Reinforcement Learning (Darling), which jointly optimizes for response quality and semantic diversity. Darling uses a learned semantic classifier to partition responses into equivalence classes, capturing diversity beyond surface-level variations. It then combines this diversity signal with a quality reward during online reinforcement learning, amplifying the advantage of high-quality and diverse responses. Experiments across multiple model families and sizes demonstrate Darling's effectiveness in both non-verifiable tasks (instruction following and creative writing) and verifiable tasks (competition math).

## Method Summary
DARLING modifies the GRPO objective by multiplying the quality reward with a normalized semantic diversity score. The method uses a learned semantic equivalence classifier to partition responses into equivalence classes, then computes diversity based on pairwise distances between these classes. This approach prevents reward hacking through superficial lexical variations while maintaining meaningful diversity. The training procedure involves generating multiple rollouts per prompt, computing the DARLING reward for each, and updating model weights using modified GRPO with token-level averaging and no standard deviation normalization.

## Key Results
- DARLING consistently outperforms quality-only RL baselines across five benchmarks
- In competition math, DARLING achieves higher pass@1 (+3.51% on 4B model) and pass@k scores
- Explicitly optimizing for diversity catalyzes exploration in online RL, manifesting as higher-quality responses
- DARLING maintains semantic diversity while improving quality, unlike standard RL which sacrifices variety for correctness

## Why This Works (Mechanism)

### Mechanism 1: Semantic Partitioning for Reward Hacking Prevention
- **Claim:** Partitioning outputs into semantic equivalence classes prevents the model from exploiting superficial lexical variations (reward hacking) while ignoring meaningful differences in content.
- **Mechanism:** The authors train a classifier (ModernBERT for general tasks, Qwen3-Embedding for math) to group responses by semantic meaning rather than token overlap. The diversity score for a specific response is calculated based on its distance from the centroids of other semantic clusters in the batch.
- **Core assumption:** Assumes that a binary classifier can reliably approximate "semantic equivalence" and that distinct clusters represent "useful" diversity rather than noise.
- **Evidence anchors:**
  - [abstract] Mentions "learned partition function to measure diversity beyond surface-level lexical variations."
  - [section 3.1] Describes the formalization of diversity $Div_d$ using pairwise distance and the classification process.
  - [corpus] Neighbors (e.g., "Diverse Preference Optimization") frequently cite the failure of standard RL to maintain output variety, validating the need for partitioning.
- **Break condition:** If the semantic classifier has low accuracy (fails to distinguish meaning), or if the task requires subtle stylistic variations that are semantically "equivalent" but pragmatically distinct, the reward signal will be noisy or zeroed out.

### Mechanism 2: Multiplicative Advantage Amplification
- **Claim:** Multiplying the quality reward by the diversity score amplifies the gradient updates for responses that are high-quality *and* distinct, forcing the model to explore high-reward regions outside the current mode.
- **Mechanism:** In the GRPO loss, the Advantage $A_{i,t}$ is calculated using $r_{darling} = r_{quality} \times Norm(Div)$. If a response is high quality but not diverse (Div $\approx$ 0), the effective reward drops. This selectively reinforces "novel" high-quality paths.
- **Core assumption:** Assumes that quality and diversity rewards scale comparably when normalized, and that a multiplicative interaction is superior to a weighted sum (which might allow high quality to offset low diversity).
- **Evidence anchors:**
  - [section 3.2] Defines the reward aggregation: $r_{darling} := r(x, y_i) \times \text{Norm}(\text{Div}_d)$.
  - [section 6.1] Shows ablation where additive aggregation performs worse on AlpacaEval 2.0 compared to multiplicative.
- **Break condition:** If the "quality" signal is sparse (e.g., binary math reward) and the model fails to find a correct answer, the diversity term has nothing to amplify, potentially leading to flat gradients.

### Mechanism 3: Exploration as a Catalyst for Quality
- **Claim:** Explicitly optimizing for diversity acts as an exploration strategy, increasing the probability of discovering high-quality solutions that standard "quality-only" RL would miss due to premature convergence.
- **Mechanism:** By maintaining a wider policy distribution (preventing sharpening), the model samples from a larger solution space. In verifiable tasks (math), this increases the likelihood of finding the correct answer (Pass@1) because the model is less "stuck" in a local suboptimal reasoning pattern.
- **Core assumption:** Assumes that the quality landscape is rugged and that local modes (found by standard RL) are suboptimal.
- **Evidence anchors:**
  - [abstract] States: "Most strikingly, explicitly optimizing for diversity catalyzes exploration in online RL, which manifests itself as higher-quality responses."
  - [section 5.2] Reports Darling outperforms GRPO on Pass@1 (+3.51% on 4B model), suggesting diversity aided in finding the single best solution more often.
- **Break condition:** In tasks where the "correct" answer is unique and narrow (e.g., strict factual recall), high diversity might lower the probability mass on the single correct answer, reducing accuracy.

## Foundational Learning

- **Concept: Group Relative Policy Optimization (GRPO)**
  - **Why needed here:** DARLING is implemented as a modification to the GRPO objective. Understanding how GRPO uses group-based advantages (comparing a response against others in the same batch) is essential to see how the diversity multiplier distorts this relative ranking.
  - **Quick check question:** How does GRPO calculate the advantage $A_i$ for a specific response $y_i$ relative to the other responses $y_j$ generated for the same prompt?

- **Concept: Semantic Equivalence vs. Lexical Diversity**
  - **Why needed here:** The paper's central critique of prior work is that they rely on n-gram overlap (lexical). Understanding the difference is key to justifying the computational cost of the classifier.
  - **Quick check question:** Why would two responses with 0% 4-gram overlap still be considered "redundant" or "semantically equivalent" in the context of a math solution?

- **Concept: Reward Hacking**
  - **Why needed here:** The authors show that using n-gram diversity causes the model to generate irrelevant self-reflection text (Appendix G) to maximize the length/variety metric.
  - **Quick check question:** In the context of Appendix G, why did the model generate text describing "Identifying Tough Parts" after solving the math problem?

## Architecture Onboarding

- **Component map:** Rollout Engine -> Partitioner -> Reward Calculator -> Trainer
- **Critical path:** The **Partitioner** is the new bottleneck. Unlike standard reward models, this requires pairwise classification across all rollouts in a batch. Efficient batching of the classifier inference is crucial to prevent training stalling.
- **Design tradeoffs:**
  - **Multiplicative vs. Additive:** Multiplicative (DARLING) is stricter (requires both quality AND diversity); Additive allows trade-offs.
  - **Classifier Choice:** A heavier classifier (e.g., Llama-70B) might be more accurate but too slow for the RL loop. The authors use fine-tuned BERT/Embedding models for speed.
- **Failure signatures:**
  - **Gibberish Generation:** If using lexical diversity (4-gram), model generates random text/reflections (see Appendix G).
  - **Mode Collapse:** If the classifier is too strict (everything is "equivalent"), diversity scores flatline, and performance reverts to standard GRPO.
  - **Reward Collapse:** If the quality reward is 0 (wrong answer), the total reward is 0 regardless of diversity.
- **First 3 experiments:**
  1. **Classifier Validation:** Replicate the classifier accuracy test (Appendix A.1) on a held-out set to ensure the "semantic equivalence" judgment aligns with human labels before starting RL.
  2. **Reward Function Ablation:** Run a small-scale training (1k steps) comparing `Reward = Quality + Diversity` vs. `Reward = Quality * Diversity` to replicate the results in Table 2.
  3. **Over-optimization Check:** Monitor "Distinct-4" vs. "Semantic Distinct" over training steps to ensure the model isn't hacking the metric (Distinct-4 rising while Semantic Distinct falls or flatlines).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What are the precise theoretical mechanisms by which enforcing semantic diversity improves single-sample quality (pass@1), rather than just variety?
- **Basis in paper:** [explicit] The authors note in Section 5.2 that "explicitly optimizing for diversity catalyzes exploration in online RL, which manifests itself as higher-quality responses," treating the quality gain as a observed phenomenon rather than a fully explained mechanism.
- **Why unresolved:** It is unclear if the improvement stems solely from better coverage of the solution space (avoiding local optima) or if the diversity signal acts as a specific regularizer against the "sharpening" of output distributions mentioned in Section 1.
- **What evidence would resolve it:** A detailed analysis of the training dynamics, specifically visualizing the loss landscape or the entropy of the policy gradients, to show if diversity prevents premature convergence.

### Open Question 2
- **Question:** Can a single, universal semantic equivalence classifier generalize across disparate domains (e.g., creative writing vs. formal math) without requiring task-specific fine-tuning?
- **Basis in paper:** [inferred] Appendix A describes training two separate classifiers: one for non-verifiable tasks (based on NoveltyBench) and another specifically for math (based on DeepscaleR), suggesting a single model may not suffice.
- **Why unresolved:** The reliance on domain-specific annotations implies the "semantic similarity" definition is brittle and context-dependent, potentially limiting the framework's scalability to new, unlabeled domains.
- **What evidence would resolve it:** Experiments applying a cross-domain classifier (trained on a mixture of data) to both the creative writing and math benchmarks to observe if performance degrades.

### Open Question 3
- **Question:** Is multiplicative aggregation of quality and diversity rewards theoretically optimal compared to dynamic or constrained fusion methods?
- **Basis in paper:** [explicit] Section 6.1 states the authors "opt for multiplicative aggregation due to its simplicity" to avoid hyperparameter tuning and scale balancing, leaving the search for an optimal fusion function unexplored.
- **Why unresolved:** While multiplication effectively filters for the intersection of high-quality and high-diversity, it may overly penalize high-quality responses that are semantically similar to a previous "winning" generation, potentially discarding valid strategies.
- **What evidence would resolve it:** A comparison against a Pareto-optimization approach or a learnable reward fusion layer that adapts the trade-off during training.

## Limitations

- **Classifier dependency:** The method's effectiveness heavily depends on the semantic equivalence classifier's accuracy, which hasn't been validated across all target domains
- **Computational overhead:** The pairwise classification required for semantic partitioning adds significant computational cost compared to standard RL approaches
- **Reward sparsity sensitivity:** When quality rewards are sparse (as in math verification), the multiplicative combination may lead to flat gradients if the model fails to find correct answers

## Confidence

- **High confidence:** The core observation that standard RL leads to diversity collapse is well-established in literature and supported by ablation studies. The multiplicative reward mechanism and its implementation details (GRPO modifications) are clearly specified and experimentally validated.
- **Medium confidence:** The claim that diversity catalyzes exploration and improves quality in verifiable tasks (math) is supported by Pass@1 improvements but requires additional validation across different math problem distributions and model scales.
- **Medium confidence:** The semantic classifier's ability to capture meaningful diversity beyond surface variations is demonstrated on NoveltyBench but hasn't been validated across the full range of tasks where DARLING is applied.

## Next Checks

1. **Classifier generalization test:** Evaluate the semantic equivalence classifier on held-out prompts from each target domain (instruction following, creative writing, math) to measure cross-domain performance degradation before running full RL experiments.

2. **Diversity-quality tradeoff analysis:** Systematically vary the temperature parameter during generation across DARLING and baseline models to determine whether observed diversity improvements come from policy sharpening or genuine semantic variation.

3. **Long-term stability evaluation:** Monitor semantic distinctness metrics over extended training periods (50+ epochs) to verify that DARLING maintains diversity improvements without gradual mode collapse that might emerge from classifier drift or reward saturation.