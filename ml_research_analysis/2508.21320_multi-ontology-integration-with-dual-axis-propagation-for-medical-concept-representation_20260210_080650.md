---
ver: rpa2
title: Multi-Ontology Integration with Dual-Axis Propagation for Medical Concept Representation
arxiv_id: '2508.21320'
source_url: https://arxiv.org/abs/2508.21320
tags:
- concept
- medical
- ontology
- linko
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of limited medical concept
  representation in electronic health records (EHRs) by integrating multiple medical
  ontologies (diseases, drugs, procedures) into a unified learning framework. The
  proposed LINKO method uses a large language model (LLM) for graph-augmented initialization
  of concept embeddings, then performs dual-axis knowledge propagation: horizontal
  message passing across ontologies at each hierarchical level and vertical message
  passing within each ontology.'
---

# Multi-Ontology Integration with Dual-Axis Propagation for Medical Concept Representation

## Quick Facts
- **arXiv ID:** 2508.21320
- **Source URL:** https://arxiv.org/abs/2508.21320
- **Reference count:** 40
- **Primary result:** LINKO achieves AUPRC scores of 32.38% on MIMIC-IV (vs. 30.21% for the best baseline) and 31.79% on MIMIC-III (vs. 29.38% for the best baseline)

## Executive Summary
This paper addresses the challenge of limited medical concept representation in electronic health records (EHRs) by integrating multiple medical ontologies (diseases, drugs, procedures) into a unified learning framework. The proposed LINKO method uses a large language model (LLM) for graph-augmented initialization of concept embeddings, then performs dual-axis knowledge propagation: horizontal message passing across ontologies at each hierarchical level and vertical message passing within each ontology. Extensive experiments on MIMIC-III and MIMIC-IV datasets demonstrate that LINKO significantly outperforms state-of-the-art baselines, achieving AUPRC scores of 32.38% on MIMIC-IV (vs. 30.21% for the best baseline) and 31.79% on MIMIC-III (vs. 29.38% for the best baseline). LINKO also shows enhanced robustness in scenarios involving limited data availability and rare disease prediction, and can function as a plug-in encoder to improve existing EHR predictive models.

## Method Summary
LINKO constructs a Meta-Knowledge Graph from multiple medical ontologies (ICD-9/10 for diagnoses/procedures, ATC for drugs) and EHR co-occurrence statistics. It initializes concept embeddings using OpenAI's text-embedding-3-small with ancestor-augmented prompts, then performs dual-axis knowledge propagation: horizontal message passing (HMP) across ontologies using graph attention networks, and vertical message passing (VMP) within ontologies using hierarchical graph information propagation (HGIP) and graph recurrent aggregation model (GRAM). The method can be plugged into existing EHR predictive models as an encoder, with final embeddings fed into a Transformer backbone for sequential diagnosis prediction.

## Key Results
- LINKO achieves AUPRC scores of 32.38% on MIMIC-IV and 31.79% on MIMIC-III, outperforming state-of-the-art baselines by 2.17% and 2.41% respectively
- The method demonstrates enhanced robustness with limited data availability, showing relative performance improvements of 11.06% and 9.92% on MIMIC-IV and MIMIC-III when training data is reduced to 25%
- LINKO shows superior performance on rare disease prediction, with relative AUPRC improvements of 24.57% and 19.07% on MIMIC-IV and MIMIC-III for infrequent labels

## Why This Works (Mechanism)

### Mechanism 1: Semantic Anchoring via Graph-Augmented LLM Initialization
Initializing ontology node embeddings with an LLM creates a semantically rich starting state that accelerates convergence and improves rare concept representation compared to random initialization. The method constructs a specific prompt containing the concept code, its description, and its ancestors, then uses the LLM to encode this context into a dense vector, bypassing the "cold start" problem for sparse medical codes.

### Mechanism 2: Cross-Ontology Contextualization (Horizontal Propagation)
Horizontal Message Passing allows a concept to refine its representation by aggregating features from related concepts in other ontologies at the same hierarchical level. The system constructs edges based on co-occurrence statistics derived from EHR visits, using GAT or HAT to aggregate neighbor information and enable concepts to learn from related medical domains.

### Mechanism 3: Hierarchical Knowledge Distillation (Vertical Propagation)
Vertical Message Passing stabilizes representation learning for sparse or rare codes by enforcing consistency with their general ancestors. The process uses a two-round approach: HGIP aggregates child information to parents, and GRAM refines children using parent information, allowing rare leaf nodes to borrow statistical strength from more frequent parent categories.

## Foundational Learning

- **Concept: Graph Attention Networks (GAT) & Hypergraphs**
  - *Why needed here:* The core engine of LINKO is the GNN, using GAT for parent levels and HAT for the leaf level to handle high-order relationships
  - *Quick check question:* Can you explain why a Hypergraph is preferred over a standard graph for the leaf level (visits) in this specific architecture?

- **Concept: Medical Ontologies (ICD-9, ATC)**
  - *Why needed here:* The "Vertical" axis relies entirely on the existence of a tree structure (parent-child), making understanding that "250.7" is a child of "Diabetes" prerequisite to understanding the VMP mechanism
  - *Quick check question:* How does the model map a specific ICD-9 code to its ancestors in the Meta-KG construction phase?

- **Concept: LLM Embeddings as Priors**
  - *Why needed here:* The paper posits that LLMs are used not for generation, but as "knowledge bases" to initialize vectors
  - *Quick check question:* Why does the paper freeze the LLM after initialization rather than keeping it active during the training of the EHR model?

## Architecture Onboarding

- **Component map:** Meta-KG Constructor -> LLM Initializer -> Horizontal Encoder -> Vertical Encoder -> Plug-in Encoder
- **Critical path:** The Meta-KG construction is the most fragile dependency. If the co-occurrence matrix is too sparse or the ontology mapping is missing codes, the graph structure (and thus HMP/VMP) collapses
- **Design tradeoffs:**
  - GAT vs. HAT at Leaf Level: The paper shows HAT performs better on MIMIC-IV (larger dataset) while GAT is better for MIMIC-III (smaller). Default to HAT for large, sparse code spaces; use GAT for denser, smaller datasets
  - LLM Freezing: The paper explicitly warns against freezing embeddings after initialization (Table 3, row "w/ LLM-emb-freezed" fails). The embeddings must be refined by the GNN
- **Failure signatures:**
  - Performance Collapse on Rare Codes: If the "Vertical" propagation is removed or the LLM initialization is skipped, performance on the 0-25% frequency band degrades significantly
  - Over-smoothing: If the horizontal graph edges (co-occurrence) are too dense (low threshold τ), distinct medical concepts may lose their discriminative features
- **First 3 experiments:**
  1. Initialization Validity: Reproduce the "w/o LLM" vs. "w/ LLM" experiment on a subset of MIMIC-III to verify that the embedding space separation improves before training the full model
  2. Ablation of Axes: Run (HMP only) vs. (VMP only) vs. (Full LINKO) to quantify the contribution of cross-ontology vs. hierarchical information
  3. Robustness Check: Train with reduced data (e.g., 25% of training set) to confirm the paper's claim that LINKO's relative performance improves as data availability decreases

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** To what extent does the choice of Large Language Model (LLM) for initialization impact the robustness of the framework?
- **Basis in paper:** The methodology (Section 2.3) relies exclusively on the OpenAI text-embedding-3-small model, without comparing it to other domain-specific or open-source alternatives
- **Why unresolved:** The paper demonstrates that LLM initialization is superior to random initialization, but it does not disentangle the contribution of the specific GPT model's knowledge from the general prompting strategy
- **What evidence would resolve it:** A comparative analysis using clinical domain-specific encoders (e.g., ClinicalBERT, BioBERT) versus general-purpose models for initialization

### Open Question 2
- **Question:** Can alternative graph construction methods mitigate the redundancy issues observed in parent-level horizontal message passing?
- **Basis in paper:** The authors explicitly avoid using hypergraphs for ancestral levels (Section 2.4) because they cause embeddings to become "overly similar," restricting the architecture to regular graphs at higher levels
- **Why unresolved:** The paper does not explore if modified hypergraph mechanisms (e.g., weighted edges) or semantic-edge constructions could preserve high-order relationships without the noted redundancy
- **What evidence would resolve it:** Ablation studies testing weighted hypergraph attention mechanisms or semantic-similarity-based edges at non-leaf hierarchy levels

### Open Question 3
- **Question:** How does the model's performance scale in outpatient settings where clinical visits are less frequent and co-occurrence data is sparser?
- **Basis in paper:** Experiments are limited to the MIMIC-III and MIMIC-IV datasets (Section 3), which consist of high-density inpatient records where strong co-occurrence patterns are likely
- **Why unresolved:** The Horizontal Message Passing module relies on co-occurrence probabilities to form edges; the framework's effectiveness in sparse data regimes typical of outpatient care remains unverified
- **What evidence would resolve it:** Evaluation on outpatient datasets (e.g., eICU or insurance claims data) to analyze performance degradation relative to visit density

## Limitations
- Critical implementation details such as the co-occurrence threshold τ and specific training hyperparameters are not fully specified in the text
- The reliance on OpenAI's text-embedding-3-small raises reproducibility concerns due to API costs and potential version changes
- The paper does not thoroughly explore edge cases where ontology hierarchies might be contradictory or where co-occurrence statistics might capture spurious correlations

## Confidence
- **High Confidence:** The core mechanism of dual-axis knowledge propagation is well-founded and supported by ablation studies
- **Medium Confidence:** The specific implementation details are partially specified but require access to code for complete reproduction
- **Medium Confidence:** The superiority over baselines is statistically significant, though the paper could provide more rigorous statistical testing across multiple random seeds

## Next Checks
1. **Threshold Sensitivity Analysis:** Systematically vary the co-occurrence threshold τ and measure its impact on rare code prediction performance to identify optimal values and failure modes
2. **Ontology Hierarchy Robustness:** Test LINKO's performance when injecting synthetic errors into the medical ontology hierarchies to quantify sensitivity to hierarchical structure quality
3. **Cross-Dataset Generalization:** Evaluate LINKO trained on MIMIC-III and tested on a completely different EHR dataset (e.g., eICU) to assess whether the learned representations transfer beyond the training domain