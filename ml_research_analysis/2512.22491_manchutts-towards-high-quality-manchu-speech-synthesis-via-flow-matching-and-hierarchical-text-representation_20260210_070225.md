---
ver: rpa2
title: 'ManchuTTS: Towards High-Quality Manchu Speech Synthesis via Flow Matching
  and Hierarchical Text Representation'
arxiv_id: '2512.22491'
source_url: https://arxiv.org/abs/2512.22491
tags:
- speech
- manchu
- synthesis
- data
- manchutts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ManchuTTS tackles low-resource speech synthesis for the endangered
  Manchu language, addressing data scarcity and complex agglutinative morphology.
  It introduces a three-tier hierarchical text representation (phoneme, syllable,
  prosodic) combined with cross-modal attention for multi-granular alignment, integrated
  into a conditional flow matching framework.
---

# ManchuTTS: Towards High-Quality Manchu Speech Synthesis via Flow Matching and Hierarchical Text Representation

## Quick Facts
- arXiv ID: 2512.22491
- Source URL: https://arxiv.org/abs/2512.22491
- Reference count: 13
- Primary result: Achieves MOS of 4.52 on 5.2 hours Manchu data, outperforming baselines by large margin

## Executive Summary
ManchuTTS addresses low-resource speech synthesis for the endangered Manchu language by combining flow matching with hierarchical text representation. The method tackles challenges of data scarcity and complex agglutinative morphology through a three-tier linguistic decomposition (phoneme, syllable, prosodic) that conditions the generation process. Experimental results on a newly constructed 6.24-hour dataset demonstrate state-of-the-art performance with MOS of 4.52, while ablation studies confirm the importance of hierarchical guidance for pronunciation accuracy and prosodic naturalness.

## Method Summary
The approach uses conditional flow matching with a three-tier text representation where input text is decomposed into phoneme, syllable, and prosodic conditions. A DiT backbone with three-layer cross-modal attention (self-attention → cross-attention → self-attention) processes these conditions, with the flow matching framework learning to predict mel-spectrograms. The model is trained with a hierarchical contrastive alignment loss that enforces acoustic-linguistic consistency across all three linguistic granularities, achieving improved performance on the challenging Manchu language task.

## Key Results
- MOS of 4.52 on Manchu dataset with only 5.2 hours of training data
- 31% improvement in agglutinative word pronunciation accuracy (AWPA) with hierarchical guidance
- WER reduced to 12.4% compared to 15.6% for F5-TTS and 28.7% for Tacotron2
- Real-time synthesis capability with 0.12 RTF on edge devices

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Text Representation for Agglutinative Morphology
Decomposing text into phoneme, syllable, and prosody tiers improves pronunciation accuracy for agglutinative words by 31%. Each tier captures different linguistic granularity—phonemes handle vowel harmony and coarticulation; syllable level models root+suffix transitions (e.g., /sung/-/ra/ energy gap narrowed by 12ms); prosody tier predicts global intonation patterns. These tiers jointly condition the flow vector field.

### Mechanism 2: Three-Layer Cross-Modal Attention for Multi-Granular Alignment
Progressive "self-attention → cross-attention → self-attention" enables implicit alignment without explicit duration modeling. Layer 1 refines intra-modal features via self-attention; Layer 2 performs bidirectional cross-attention (text→audio and audio→text) for mutual information exchange; Layer 3 consolidates fused representations. This avoids explicit alignment error rates (25–35% in low-resource settings).

### Mechanism 3: Hierarchical Contrastive Alignment Loss (HCA)
Contrastive loss at each linguistic tier enforces acoustic-linguistic consistency, reducing WER to 12.4%. For each condition level c(k), positive pairs (x1, c(k)) and negative pairs (x1, c(k)_neg) are constructed; maximizing mutual information preserves fine-grained linguistic information. The loss is weighted sum across tiers.

## Foundational Learning

- **Flow Matching / Rectified Flow**: Core generative framework; differs from diffusion (learns ODE vector field directly, not score function). Understanding xt = (1-t)·x0 + t·x1 path interpolation is essential. Quick check: Given noise x0 and target mel-spectrogram x1, what does the vector field ut(xt|c) = x1 - x0 represent?

- **Agglutinative Morphology**: Motivates entire architecture; Manchu words combine root+multiple suffixes (e.g., "sunggi-ra"), creating alignment challenges absent in isolating languages. Quick check: Why would a phoneme-only representation fail to capture stem-suffix boundaries in "sunggi-ra"?

- **Cross-Attention for Text-Audio Alignment**: Layer 2 of attention mechanism; understanding Q/K/V where queries from one modality attend to keys/values from another. Quick check: In equation F(2)_t = LayerNorm(F(1)_t + MHA(F(1)_t, F(1)_a, F(1)_a)), which modality provides keys and values?

## Architecture Onboarding

- **Component map**: Input Text → Phoneme Encoder (2-layer Conv1D, 256ch) → Three-tier Feature Extraction (phoneme/syllable/prosody) → Cross-Modal Attention (3 layers) → DiT Backbone (8 layers, 512 dim, 4 heads) → Flow Matching ODE Solver → Mel-Spectrogram (80-dim)

- **Critical path**: 1. Text preprocessing to IPA phoneme sequences (vocab: 1,024) 2. Hierarchical feature extraction (requires linguist-validated syllable/prosody annotations) 3. DiT forward pass with cross-modal conditioning 4. ODE sampling (inference: start from noise, solve vt trajectory)

- **Design tradeoffs**: Non-autoregressive vs. autoregressive (chose NAR for speed (RTF 0.12) but sacrifices some prosodic nuance); Implicit vs. explicit alignment (avoids duration predictor rigidity but requires more training data); Three-tier vs. single-tier (+0.58 MOS gain but requires annotated prosody data)

- **Failure signatures**: Missing syllable layer: high-frequency energy loss (≥4kHz), particularly frication noise in sibilants; Missing prosody layer: pitch correlation drops to 0.63 (vs. 0.91) for questions; Data < 2 hours: MOS drops to ~3.2, WER rises to 27%

- **First 3 experiments**: 1. Phoneme-only baseline: Train with only c_phon conditioning; measure AWPA and compare to 70.8% baseline 2. Ablate contrastive loss: Set λk = 0 for HCA; expect WER increase 3. Cross-lingual zero-shot: Test on Ewenki without retraining; target MOS ~3.78

## Open Questions the Paper Calls Out

- **Dialect adaptation**: Can hierarchical representation capture phonological divergence of distinct Manchu dialects without extensive retraining data? The model is limited to studio speech and accuracy decreases in dialect variations.

- **Noise robustness**: Does noise perception training improve real-world robustness while maintaining studio prosodic naturalness? Current dataset relies on high-quality audio (SNR ≥ 25 dB).

- **Long-range prosody**: How to refine prosodic guidance to eliminate stress drift and limited emotional range in long sentences? Current three-tier structure struggles with long-range semantic dependencies for consistent emotional expression.

## Limitations
- Dataset access not available at publication time, preventing independent verification
- Computational overhead of three-layer cross-modal attention not fully explored versus explicit duration predictors
- Unspecified hyperparameters (λk weights) for hierarchical contrastive loss prevent exact reproduction

## Confidence
- **High confidence**: Flow matching framework implementation and training procedure (300K steps, AdamW hyperparameters, architectural dimensions fully specified)
- **Medium confidence**: Hierarchical text representation benefits (31% AWPA improvement, but specific linguistic annotation methodology underspecified)
- **Medium confidence**: Three-layer cross-modal attention effectiveness (mechanism well-described but limited comparative analysis)
- **Low confidence**: Contrastive alignment loss contribution (HCA equation provided but without negative sampling details or λk values)

## Next Checks
1. **Phoneme-only ablation validation**: Train with only phoneme-level conditioning and measure AWPA/MOS to validate 70.8% baseline claim
2. **Prosodic feature isolation test**: Implement prosody tier with simple features and verify pitch correlation drops from 0.91 to ~0.63 when removed
3. **Cross-lingual transfer verification**: Test trained model on Ewenki without fine-tuning and measure MOS to validate 3.78 zero-shot transfer result