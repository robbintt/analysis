---
ver: rpa2
title: Self-Rewarding Rubric-Based Reinforcement Learning for Open-Ended Reasoning
arxiv_id: '2509.25534'
source_url: https://arxiv.org/abs/2509.25534
tags:
- vous
- training
- rappel
- wang
- avez
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Self-Rewarding Rubric-Based Reinforcement
  Learning for Open-Ended Reasoning, a lightweight framework that uses a model's own
  rubric-based scoring to train reasoning capabilities, eliminating the need for separate
  reward models. The approach substantially reduces resource consumption and training
  time while improving performance on hard open-ended tasks.
---

# Self-Rewarding Rubric-Based Reinforcement Learning for Open-Ended Reasoning

## Quick Facts
- arXiv ID: 2509.25534
- Source URL: https://arxiv.org/abs/2509.25534
- Reference count: 40
- A lightweight framework that uses model's own rubric-based scoring to train reasoning capabilities without separate reward models

## Executive Summary
This paper introduces a novel reinforcement learning framework that leverages rubric-based self-rewarding to train reasoning capabilities in large language models. The approach eliminates the need for separate reward models by having the model generate its own rubric-based scores, substantially reducing resource consumption and training time. The method demonstrates significant performance improvements on hard open-ended tasks, with Qwen3-32B trained on only 4000 easy samples achieving performance exceeding GPT-4 on HealthBench Hard subset. The framework maintains the model's grading ability while enabling more efficient training through transparent, rubric-based evaluation.

## Method Summary
The framework implements rubric-based self-rewarding reinforcement learning where models generate their own evaluation criteria and scores during training. Instead of relying on external reward models, the system uses the model's inherent grading capability to provide feedback signals. Training is conducted on easy samples with self-generated rubrics, and the learned reasoning patterns transfer to harder tasks. The approach is particularly effective for strong models like Qwen3-32B, which achieve best results with purely self-rewarding training, while weaker models benefit from incorporating small amounts of teacher-graded data. The method preserves and slightly improves grading ability while reducing computational overhead.

## Key Results
- Qwen3-32B trained on only 4000 easy HealthBench samples with self-rewarding achieves performance exceeding GPT-4 on HealthBench Hard subset
- Resource consumption and training time substantially reduced compared to traditional reward model-based approaches
- Self-rewarding training maintains and slightly improves the model's grading ability while enhancing reasoning performance
- Stronger models achieve best results with purely self-rewarding training, while weaker models benefit from teacher-graded data incorporation

## Why This Works (Mechanism)
The framework exploits the model's existing capability to evaluate its own outputs using rubric-based criteria, creating a closed-loop training system. By generating self-consistent evaluation signals, the approach eliminates the need for computationally expensive reward models while maintaining training effectiveness. The rubric-based evaluation provides transparent, interpretable feedback that guides reasoning improvement. The transfer of learned patterns from easy to hard tasks suggests that self-rewarding captures fundamental reasoning structures that generalize across difficulty levels. The method's efficiency stems from leveraging the model's dual role as both reasoner and evaluator, reducing dependency on external computational resources.

## Foundational Learning
- Reinforcement Learning with Self-Rewarding: Models generate their own evaluation signals rather than relying on external reward models - needed to reduce computational overhead and enable more efficient training
- Rubric-Based Evaluation: Structured criteria-based scoring instead of binary or continuous rewards - needed for transparent, interpretable feedback that guides reasoning improvement
- Knowledge Transfer from Easy to Hard Tasks: Learning patterns from simpler problems that generalize to complex ones - needed to achieve strong performance with limited training data
- Model-as-Evaluator Paradigm: Using the same model for both reasoning and evaluation - needed to eliminate separate reward model training and inference costs
- Performance Benchmarking Across Model Scales: Comparing results across different model sizes (Qwen3-8B vs 32B) - needed to understand when self-rewarding is most effective

## Architecture Onboarding

Component Map: Input Text -> Rubric Generation -> Self-Reward Calculation -> Policy Update -> Output Generation

Critical Path: The reinforcement learning loop where the model generates reasoning outputs, creates rubric-based evaluations, and uses these self-generated rewards to update its policy. This closed-loop system eliminates the need for external reward signal computation.

Design Tradeoffs: Pure self-rewarding works best for stronger models but weaker models require hybrid approaches with teacher-graded data. The quality of generated rubrics directly impacts training effectiveness. Resource efficiency gains must be balanced against potential performance limitations in domains where self-evaluation is challenging.

Failure Signatures: Poor rubric quality leading to noisy reward signals, overfitting to easy samples without proper generalization to hard tasks, and performance degradation in domains requiring subjective judgment beyond structured criteria.

First Experiments: 1) Validate rubric generation quality by comparing self-generated rubrics against expert-defined ones. 2) Test knowledge transfer by training on easy samples and evaluating on progressively harder tasks. 3) Compare training efficiency against traditional reward model-based approaches using identical computational resources.

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Evaluation primarily focused on HealthBench benchmark with Qwen3 models, limiting generalizability to other domains and model architectures
- Claims of exceeding GPT-4 performance from limited training data require careful independent validation across different model sizes and training regimes
- Reliance on model's own grading ability introduces potential bias that isn't fully addressed
- Long-term stability of performance gains and potential degradation in non-rubric-based tasks not thoroughly investigated

## Confidence
Performance claims (Medium): Results show substantial improvements, but GPT-4 comparison and dramatic gains from limited data warrant independent verification across different model sizes and benchmarks.

Methodology claims (High): Rubric-based self-rewarding approach is well-documented with clear implementation details that can be reproduced.

Resource efficiency claims (Medium): Reported reductions in resource consumption are plausible but require independent benchmarking across different hardware configurations and model scales.

## Next Checks
1. Cross-domain validation: Test the self-rewarding framework on non-medical reasoning tasks (e.g., legal reasoning, scientific problem-solving) to assess generalizability beyond HealthBench.

2. Ablation study on rubric quality: Systematically vary the quality and granularity of rubric definitions to quantify their impact on performance and identify minimum viable rubric complexity.

3. Long-term stability assessment: Conduct extended training experiments to evaluate whether self-rewarding signals maintain performance improvements over multiple training epochs and whether any degradation occurs in general capabilities.