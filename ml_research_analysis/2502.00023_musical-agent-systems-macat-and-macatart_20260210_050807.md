---
ver: rpa2
title: 'Musical Agent Systems: MACAT and MACataRT'
arxiv_id: '2502.00023'
source_url: https://arxiv.org/abs/2502.00023
tags:
- musical
- audio
- agent
- music
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This research introduces MACAT and MACataRT, two musical agent
  systems designed to support real-time music performance and improvisation through
  human-in-the-loop generative AI. MACAT employs real-time synthesis and self-listening
  for autonomous performance, while MACataRT facilitates collaborative improvisation
  using audio mosaicing and sequence-based learning.
---

# Musical Agent Systems: MACAT and MACataRT

## Quick Facts
- arXiv ID: 2502.00023
- Source URL: https://arxiv.org/abs/2502.00023
- Reference count: 36
- Primary result: Two musical agent systems (MACAT and MACataRT) for real-time human-AI musical performance and improvisation using small, personalized audio corpora

## Executive Summary
MACAT and MACataRT are musical agent systems designed to support real-time performance and improvisation through human-in-the-loop generative AI. MACAT employs real-time synthesis and self-listening for autonomous performance, while MACataRT facilitates collaborative improvisation using audio mosaicing and sequence-based learning. Both systems emphasize training on personalized, small datasets to align with individual artistic styles, ensuring ethical and transparent AI engagement. The systems demonstrate practical effectiveness in live performances, enabling musicians to explore new forms of artistic expression and fostering dynamic human-AI collaboration.

## Method Summary
The systems are implemented in Max/MSP using the MuBu library for audio feature extraction and machine learning. MACAT uses Self-Organizing Maps (SOM) for timbral clustering and Factor Oracle (FO) for temporal pattern generation, with real-time concatenative synthesis based on BMU lookup and congruence-controlled traversal. MACataRT extends CataRT with FO-based temporal modeling and audio mosaicing, supporting both reactive mode (live machine listening) and proactive mode (offline FO training on segment index sequences). Training uses small, personalized .wav corpora (16-bit, 44.1 kHz) with features including MFCCs, loudness, spectral flatness, perceptual spectral decrease, and valence/arousal.

## Key Results
- Both systems enable real-time human-AI musical collaboration through personalized training on small audio corpora
- MACAT achieves autonomous performance with self-listening capabilities using SOM clustering and FO pattern generation
- MACataRT facilitates collaborative improvisation through audio mosaicing and sequence-based learning
- The small-data approach ensures ethical and transparent AI engagement aligned with individual artistic styles

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-Organizing Maps (SOM) enable real-time retrieval of timbrally coherent audio segments from a clustered corpus.
- Mechanism: During offline training, audio is segmented and analyzed; feature vectors map to a 2D SOM topology via Best Matching Unit (BMU) competition and neighbor adjustment. At runtime, the agent navigates this topology to select audio segments clustered by similarity.
- Core assumption: Euclidean distance in the extracted feature space meaningfully corresponds to perceptual timbral similarity.
- Evidence anchors:
  - [Section 2.1]: "MACAT conducts offline machine listening to analyze the original audio data, initialize SOM nodes, and identify the Best Matching Unit (BMU) for each input vector based on Euclidean distance."
  - [Appendix B.2]: Details SOM clustering for high-dimensional input data in a 2-dimensional topology.
  - [corpus]: Weak direct evidence—neighbor papers discuss corpus-based methods but not SOM-specific mechanisms.
- Break condition: If audio features do not capture perceptually relevant dimensions, clustering will produce incoherent sonic groupings.

### Mechanism 2
- Claim: Factor Oracle (FO) provides temporal structure by learning and recombining sequence patterns from the SOM node traversal.
- Mechanism: FO is a suffix automaton that records sequences of SOM node indices during training. During generation, it navigates forward (continuation) or backward (pattern recombination) based on a congruence parameter, balancing repetition and variation.
- Core assumption: Musical structure can be meaningfully captured through the sequence of clustered audio segments without explicit higher-level symbolic representation.
- Evidence anchors:
  - [Section 2.1]: "FO identifying patterns within this sequence during real-time generation."
  - [Section 2.2]: "MACataRT integrates the factor oracle to automate the generation process... enabling the factor oracle to generate music based on these learned sequences."
  - [corpus]: Apollo paper mentions corpus-based style imitation but does not validate FO specifically.
- Break condition: If training corpus lacks repeated patterns, FO offers no structural advantage over random selection.

### Mechanism 3
- Claim: Small, artist-curated datasets yield stylistically aligned outputs with enhanced transparency and reduced ethical risk.
- Mechanism: By restricting training data to an artist's own recordings or licensed corpus, the system's output is traceable to known sources, avoiding unauthorized style appropriation inherent in large-scale models.
- Core assumption: Artistic identity is sufficiently captured in a small, high-quality corpus; generalization is not the goal.
- Evidence anchors:
  - [Abstract]: "Both systems emphasize training on personalized, small datasets, fostering ethical and transparent AI engagement."
  - [Section 1.2]: "Training on a small, high-quality audio corpus enables our musical agent systems to closely align with the specific musical nuances and stylistic preferences."
  - [Appendix A]: Explicit ethical argument for small-data approach over big-data models.
  - [corpus]: Weak validation—no comparative studies in neighbors; ethical framing appears unique to this work.
- Break condition: If corpus is too small or unrepresentative, output diversity collapses into repetitive loops.

## Foundational Learning

- Concept: Self-Organizing Maps (SOM)
  - Why needed here: Core clustering mechanism for organizing audio segments by timbral similarity; understanding BMU competition is essential for debugging retrieval behavior.
  - Quick check question: Can you explain how a BMU is selected and how neighbor nodes are updated during training?

- Concept: Factor Oracle (suffix automaton)
  - Why needed here: Provides the temporal memory for pattern learning and generation; the congruence parameter directly controls the repetitive vs. exploratory balance.
  - Quick check question: How does the factor oracle decide between forward continuation and backward jumps during generation?

- Concept: Corpus-Based Concatenative Synthesis (CBCS)
  - Why needed here: Underlying synthesis paradigm; audio output is assembled from segments selected based on feature matching, not generated from scratch.
  - Quick check question: What is the difference between CBCS and parametric synthesis in terms of output flexibility and corpus dependency?

## Architecture Onboarding

- Component map:
  - Audio Corpus → Segmentation → Feature Extraction (MFCC, loudness, spectral features, valence/arousal)
  - SOM: Clusters segments by feature similarity; visualized as 2D topology
  - Factor Oracle: Learns and generates sequences of SOM node indices
  - Machine Listening Module: Real-time audio analysis for reactive mode
  - Synthesis Engine: Concatenative playback with controls (resampling, pitch shift, reverse)
  - Interface (Max/MSP): Parameter control, visualization, corpus loading

- Critical path:
  1. Prepare audio corpus (curate recordings matching target style)
  2. Run offline training: segment audio → extract features → train SOM → build FO sequence
  3. Configure parameters: tempo, congruence, feature weights (MACataRT)
  4. Run real-time: agent generates via FO navigation, responds to live input in reactive mode

- Design tradeoffs:
  - Reactive vs. proactive improvisation: Reactive uses real-time feature matching; proactive uses learned FO sequences
  - SOM dimensionality: Smaller maps yield coarser clusters; larger maps increase granularity but require more data
  - Congruence setting: High values produce repetitive output; low values increase unpredictability

- Failure signatures:
  - Repetitive output with no variation: Congruence too high or corpus too small
  - Incoherent timbral jumps: Feature extraction not capturing relevant perceptual dimensions
  - Rhythmic instability when resampling: Tempo not recalculated to match playback speed change
  - Corpus load failure: Audio format mismatch (requires .wav, 16-bit, 44.1kHz per Appendix B.2)

- First 3 experiments:
  1. Train MACAT on a single-instrument corpus (e.g., 2-3 minutes of solo percussion); observe SOM clustering and FO pattern formation; vary congruence from 0.2 to 0.8.
  2. In MACataRT, compare reactive mode (real-time machine listening driving mosaicing) vs. proactive mode (FO-driven generation) using the same corpus; document differences in temporal coherence.
  3. Stress-test corpus size: Train on progressively smaller subsets (e.g., 60s, 30s, 15s) and identify the threshold where output degrades into repetitive or incoherent generation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can deep learning architectures successfully enable musical agents to learn and retain longer sequences of musical patterns compared to the current Factor Oracle implementation?
- Basis in paper: [explicit] The authors state, "To improve temporality, we plan to integrate deep learning architectures that enable agents to learn and retain longer sequences of musical patterns."
- Why unresolved: The current systems rely on the Factor Oracle algorithm, which is limited in its ability to manage long-term temporal dependencies in music.
- What evidence would resolve it: A comparative analysis of sequence retention length and musical coherence between the current Factor Oracle models and the proposed deep learning models.

### Open Question 2
- Question: Does incorporating a history module effectively advance the explainability of the agent's output from a note-level to a bar-level understanding?
- Basis in paper: [explicit] The authors plan to "incorporate a module that records the history of past musical patterns, thereby advancing comprehension from a note-level to a bar-level understanding."
- Why unresolved: Current explainability is limited; the system lacks a mechanism to provide higher-level structural context for the generated music.
- What evidence would resolve it: Demonstration of a module that outputs musical analysis or metadata corresponding to bar-level structures rather than just discrete note or segment events.

### Open Question 3
- Question: To what extent does a reinforcement learning (RL) feedback loop enhance the adaptability of musical agents in real-time performance contexts?
- Basis in paper: [explicit] The paper suggests that "A feedback loop using reinforcement learning may also be introduced to further enhance the adaptability of musical agents in real-time performance contexts."
- Why unresolved: The current systems utilize pre-trained models and specific probabilistic transitions (Factor Oracle), but do not yet employ RL for dynamic, goal-oriented adaptation during performance.
- What evidence would resolve it: Performance metrics or qualitative evaluations showing improved response to human input or environmental changes in agents utilizing RL versus static agents.

### Open Question 4
- Question: What quantitative metrics can effectively evaluate the artistic success and stylistic alignment of personalized musical agents?
- Basis in paper: [inferred] The authors acknowledge the "absence of well-established quantitative measures for evaluating personalized improvisation systems," leading them to prioritize qualitative showcase over quantitative analysis.
- Why unresolved: Standard quantitative measures (e.g., timing accuracy) fail to capture the subjective and co-creative dimensions of human-AI improvisation.
- What evidence would resolve it: The development and validation of a standardized evaluation framework that correlates quantitative scores with human expert assessments of musicality and collaboration.

## Limitations

- Key implementation details remain underspecified, particularly SOM hyperparameters and Factor Oracle integration specifics
- Empirical validation against larger-scale models is absent, limiting claims about the small-data approach's superiority
- The paper relies primarily on qualitative research-creation methodology rather than systematic quantitative evaluation

## Confidence

- **High confidence**: The core architectural claims regarding SOM-based clustering and FO-based temporal generation are well-grounded in established research
- **Medium confidence**: The claimed effectiveness in live performance settings relies primarily on qualitative methodology rather than systematic evaluation
- **Low confidence**: Specific parameter settings and integration details necessary for faithful reproduction are insufficient

## Next Checks

1. **SOM clustering validation**: Test whether BMU assignments based on the specified feature set (MFCC, loudness, spectral features, valence/arousal) produce perceptually coherent timbral groupings by conducting listening tests on clustered segments.

2. **FO sequence learning validation**: Verify that the congruence parameter (0.0-1.0) produces predictable changes in repetition versus variation by analyzing generated sequences from corpora with known structural properties.

3. **Corpus size threshold validation**: Systematically determine the minimum corpus size required for musically coherent output by training on progressively smaller subsets and identifying the point where generation quality degrades significantly.