---
ver: rpa2
title: A Dataset for Addressing Patient's Information Needs related to Clinical Course
  of Hospitalization
arxiv_id: '2506.04156'
source_url: https://arxiv.org/abs/2506.04156
tags:
- patient
- question
- relevance
- llama
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces ArchEHR-QA, a novel dataset of 134 real-world
  patient cases designed to benchmark question answering systems that address patient
  information needs using electronic health record (EHR) evidence. Each case includes
  a patient question, clinician-interpreted question, annotated EHR excerpts, and
  clinician-authored answers.
---

# A Dataset for Addressing Patient's Information Needs related to Clinical Course of Hospitalization

## Quick Facts
- arXiv ID: 2506.04156
- Source URL: https://arxiv.org/abs/2506.04156
- Reference count: 40
- Introduces ArchEHR-QA dataset for benchmarking clinical QA systems using real EHR evidence

## Executive Summary
This study introduces ArchEHR-QA, a novel dataset of 134 real-world patient cases designed to benchmark question answering systems that address patient information needs using electronic health record (EHR) evidence. Each case includes a patient question, clinician-interpreted question, annotated EHR excerpts, and clinician-authored answers. Three open-weight large language models (Llama 4, Llama 3, and Mixtral) were evaluated across three prompting strategies for generating answers with citations. The answer-first prompting approach consistently outperformed others, with Llama 4 achieving the highest overall score (42.3%) and essential-only micro F1 of 51.8%. Manual error analysis revealed common issues such as omitted critical evidence and hallucinated content, highlighting the need for further improvements in factual consistency and relevance in clinical contexts.

## Method Summary
The researchers developed ArchEHR-QA by collecting 134 real patient cases from clinical settings. For each case, they gathered patient questions about their clinical course, had clinicians interpret these questions to identify underlying information needs, annotated relevant EHR excerpts, and provided clinician-authored answers. They then evaluated three open-weight large language models (Llama 4, Llama 3, and Mixtral) using three different prompting strategies: answer-first, evidence-first, and integrated approaches. The models were assessed on their ability to generate accurate, relevant answers with proper citations to EHR evidence. Performance was measured using multiple metrics including overall score, essential-only micro F1, and micro F1 for evidence inclusion.

## Key Results
- Answer-first prompting consistently outperformed evidence-first and integrated approaches across all evaluation metrics
- Llama 4 achieved the highest overall score (42.3%) and essential-only micro F1 of 51.8%
- Manual error analysis revealed omitted critical evidence and hallucinated content as primary failure modes
- The essential-only micro F1 metric showed higher performance than overall micro F1, suggesting models better handle core information than peripheral details

## Why This Works (Mechanism)
The answer-first prompting strategy appears to work better because it allows models to first construct a coherent narrative response before attempting to ground it in evidence. This approach may reduce the cognitive load of simultaneously generating relevant content and identifying supporting evidence. By first establishing the answer structure, models can then selectively cite evidence that best supports their generated response, potentially leading to more natural and comprehensive answers.

## Foundational Learning
- **Clinical question interpretation**: Why needed - to bridge the gap between patient language and medical terminology; Quick check - verify clinician agreement on question interpretation
- **EHR evidence annotation**: Why needed - to provide verifiable ground truth for model evaluation; Quick check - ensure EHR excerpts directly support clinician answers
- **Citation-aware QA evaluation**: Why needed - to assess both answer quality and evidence grounding; Quick check - verify all cited evidence appears in provided EHR excerpts
- **Multi-metric evaluation framework**: Why needed - to capture different aspects of QA performance (relevance, completeness, factual accuracy); Quick check - ensure metrics are not redundant and measure distinct qualities

## Architecture Onboarding

**Component Map**: Patient Question -> Clinician Interpretation -> EHR Annotation -> Model Generation -> Evaluation Metrics

**Critical Path**: The essential workflow involves receiving a patient question, interpreting it clinically, identifying relevant EHR evidence, generating an answer with citations, and evaluating against ground truth answers using multiple metrics.

**Design Tradeoffs**: The dataset prioritizes real clinical cases over synthetic examples, accepting smaller size for higher authenticity. The evaluation focuses on open-weight models to enable reproducibility, though this may limit performance compared to proprietary models.

**Failure Signatures**: Common failure modes include omitting critical evidence from answers, generating hallucinated content not supported by EHR excerpts, and misinterpreting patient questions in ways that lead to irrelevant answers.

**First Experiments**: 
1. Test each model with all three prompting strategies on a small validation subset
2. Compare model performance on essential vs. non-essential information subsets
3. Analyze error patterns to identify whether failures are systematic or random

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but implicit areas for future research include: (1) whether the answer-first prompting advantage generalizes to larger datasets and different clinical domains, (2) how to systematically reduce hallucination rates in clinical QA systems, and (3) whether combining multiple models or using model ensembles could improve performance beyond the best single model.

## Limitations
- Dataset size of 134 cases may be insufficient for robust model training and comprehensive evaluation across diverse clinical scenarios
- Reliance on clinician annotations and assessments introduces potential subjectivity in question interpretation and answer evaluation
- Error analysis identified common issues but did not systematically quantify the prevalence of specific error types across the dataset
- The study only evaluated open-weight models, potentially missing performance improvements available from proprietary models
- The evaluation focused on English language clinical data, limiting generalizability to other languages and healthcare systems

## Confidence

**High confidence**: The dataset creation methodology and annotation process are well-documented and reproducible. The comparative evaluation of three prompting strategies provides clear evidence of answer-first prompting superiority.

**Medium confidence**: The benchmark results for model performance are internally consistent but may not generalize to other clinical contexts or larger datasets. The identified error patterns are plausible but require systematic validation.

**Low confidence**: The relative ranking of models (Llama 4 vs. Llama 3 vs. Mixtral) may be sensitive to the specific evaluation protocol and dataset composition.

## Next Checks
1. Conduct cross-validation with an independent panel of clinicians to assess inter-rater reliability for question interpretation and answer evaluation
2. Expand the dataset to 500+ cases across multiple clinical departments to evaluate model performance in diverse medical contexts and test generalization
3. Implement automated factual consistency checks using medical knowledge graphs to systematically quantify hallucination rates beyond manual error analysis
4. Test additional prompting strategies such as iterative refinement or chain-of-thought approaches
5. Evaluate whether fine-tuning the models on the ArchEHR-QA dataset improves performance compared to prompting-only approaches