---
ver: rpa2
title: Show or Tell? Effectively prompting Vision-Language Models for semantic segmentation
arxiv_id: '2503.19647'
source_url: https://arxiv.org/abs/2503.19647
tags:
- visual
- segmentation
- lisa
- text
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work systematically investigates the effectiveness of prompting
  Vision-Language Models (VLMs) for semantic segmentation, comparing text and visual
  prompts. A scalable few-shot prompted semantic segmentation (FPSS) setup is introduced,
  inspired by open-vocabulary segmentation and few-shot learning, using a single annotated
  example per class.
---

# Show or Tell? Effectively prompting Vision-Language Models for semantic segmentation

## Quick Facts
- arXiv ID: 2503.19647
- Source URL: https://arxiv.org/abs/2503.19647
- Authors: Niccolo Avogaro; Thomas Frick; Mattia Rigotti; Andrea Bartezzaghi; Filip Janicki; Cristiano Malossi; Konrad Schindler; Roy Assaf
- Reference count: 34
- Primary result: VLMs underperform specialist models by ~30% IoU on semantic segmentation

## Executive Summary
This work systematically investigates the effectiveness of prompting Vision-Language Models (VLMs) for semantic segmentation, comparing text and visual prompts. A scalable few-shot prompted semantic segmentation (FPSS) setup is introduced, inspired by open-vocabulary segmentation and few-shot learning, using a single annotated example per class. Evaluations on the MESS dataset collection reveal that VLMs significantly underperform compared to specialist models (by ~30% IoU) and that text and visual prompts are complementary—each excelling where the other fails.

## Method Summary
The paper introduces a few-shot prompted semantic segmentation (FPSS) framework that uses a single annotated example per class to evaluate VLMs on semantic segmentation tasks. The approach systematically compares text and visual prompts, revealing that these modalities are complementary. Based on this finding, the authors propose PromptMatcher, a training-free framework that combines both prompt types. The method is evaluated on the MESS dataset collection, demonstrating state-of-the-art results for prompted VLMs with improvements of 2.5% over the best text-prompted VLM and 3.5% over the best visual-prompted VLM.

## Key Results
- VLMs underperform specialist segmentation models by ~30% IoU
- Text and visual prompts are complementary, each excelling where the other fails
- Anticipating the most effective prompt modality can improve performance by 11% IoU
- PromptMatcher achieves state-of-the-art results with 2.5% improvement over best text-prompted VLM and 3.5% over best visual-prompted VLM

## Why This Works (Mechanism)
None

## Foundational Learning
- **Semantic segmentation**: Image understanding task requiring pixel-level classification
  - Why needed: Core task being evaluated
  - Quick check: VLMs' ~30% IoU gap to specialist models
- **Vision-Language Models (VLMs)**: Models trained on both visual and textual data
  - Why needed: Primary subject of investigation
  - Quick check: Underperformance on segmentation task
- **Prompt engineering**: Strategic input formulation to guide model outputs
  - Why needed: Central methodology for adapting VLMs to segmentation
  - Quick check: Complementary nature of text vs visual prompts

## Architecture Onboarding
**Component map**: Input image -> Text prompts OR Visual prompts -> VLM backbone -> Segmentation mask output
**Critical path**: Prompt selection → VLM processing → Output refinement
**Design tradeoffs**: Single annotated example per class (few-shot) vs. comprehensive training; training-free adaptation vs. fine-tuning
**Failure signatures**: ~30% IoU gap to specialist models; modality-specific failures
**First experiments**:
1. Test PromptMatcher on standard segmentation benchmarks with more training examples per class
2. Conduct ablation studies to identify specific VLM limitations for segmentation
3. Evaluate fine-tuning VLMs on segmentation tasks to reduce performance gap

## Open Questions the Paper Calls Out
None

## Limitations
- VLMs underperform specialist models by ~30% IoU, indicating fundamental architectural limitations
- Modest improvements (2.5-3.5%) from PromptMatcher may not bridge the 30% baseline gap
- Few-shot setup with single example per class may limit generalizability to data-intensive scenarios

## Confidence
- Core findings about VLM limitations: High
- Complementary nature of text and visual prompts: Medium
- PromptMatcher framework's practical utility: Medium-Low

## Next Checks
1. Test PromptMatcher on standard segmentation benchmarks with more training examples per class
2. Conduct ablation studies to determine specific aspects of VLM architecture that limit segmentation performance
3. Evaluate whether fine-tuning VLMs on segmentation-specific tasks can reduce the 30% performance gap to specialist models