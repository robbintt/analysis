---
ver: rpa2
title: ResNetVLLM -- Multi-modal Vision LLM for the Video Understanding Task
arxiv_id: '2504.14432'
source_url: https://arxiv.org/abs/2504.14432
tags:
- video
- understanding
- resnetvllm
- visual
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ResNetVLLM introduces a zero-shot video understanding framework
  that combines a ResNet-based visual encoder with a Large Language Model, avoiding
  reliance on pre-trained video models by using a non-pretrained ResNet for visual
  feature extraction. This design allows joint learning of visual and semantic representations
  within a unified architecture.
---

# ResNetVLLM -- Multi-modal Vision LLM for the Video Understanding Task

## Quick Facts
- arXiv ID: 2504.14432
- Source URL: https://arxiv.org/abs/2504.14432
- Reference count: 40
- Primary result: Achieves state-of-the-art zero-shot video understanding performance with 3.55/5 on Video-based Text Generation benchmark

## Executive Summary
ResNetVLLM introduces a zero-shot video understanding framework that combines a randomly initialized ResNet visual encoder with a Large Language Model, avoiding reliance on pre-trained video models. The architecture employs a two-stage training approach: first training the visual encoder exclusively for 150 epochs, then jointly optimizing both components for 50 epochs. The model achieves strong performance across multiple video QA benchmarks, demonstrating that trainable visual features can outperform frozen pretrained features for zero-shot video understanding tasks.

## Method Summary
The method employs a 2D ResNet (randomly initialized) as the visual encoder, extracting features from 100 uniformly sampled frames per video. These features are projected and fed into an LLaVA-based LLM for text generation and question answering. Training proceeds in two stages: a warm-up phase where only the ResNet is trained for 150 epochs using SGD, followed by joint training of both components for 50 epochs using AdamW. The architecture avoids 3D convolutions for efficiency, using frame-level 2D features with global average pooling instead.

## Key Results
- Achieves 3.55/5 average score on Video-based Text Generation benchmark (CI, DO, CU, TU, C metrics)
- Zero-shot QA accuracy: 78.3% on MSVD-QA, 63.5% on MSRVTT-QA, 59.9% on TGIF-QA, 54.8% on ActivityNet-QA
- State-of-the-art performance in zero-shot video understanding without pre-trained video models

## Why This Works (Mechanism)

### Mechanism 1
Training a randomly-initialized visual encoder jointly with an LLM enables task-adaptive visual representations that outperform frozen pretrained features for zero-shot video understanding. The non-pretrained ResNet updates continuously during training, allowing visual embeddings to align with the LLM's semantic space rather than inheriting potentially mismatched priors from video pretraining corpora. Core assumption: Frozen pretrained visual features contain domain-specific biases that limit transfer to unseen video content; trainable features can discover more generalizable representations.

### Mechanism 2
Two-stage training (visual encoder warm-up followed by joint optimization) stabilizes convergence when training a randomly-initialized visual encoder with an LLM. Stage 1 trains only the ResNet for 150 epochs using SGD to establish basic visual feature extraction before introducing the complexity of LLM joint optimization in Stage 2. Core assumption: Simultaneous end-to-end training from random initialization creates optimization instability or competing gradient signals between vision and language components.

### Mechanism 3
Frame-level 2D convolution with temporal pooling provides sufficient spatiotemporal representation for video QA while maintaining computational efficiency. Uniformly sampling 100 frames at 6-frame intervals, extracting features via 2D ResNet, and using global average pooling features allows the LLM to attend across temporal positions without 3D convolution overhead. Core assumption: Temporal dynamics are sufficiently captured through frame sequence position encoding rather than explicit 3D spatiotemporal convolutions.

## Foundational Learning

- **Zero-shot transfer in vision-language models**: Why needed here: The paper's central claim is zero-shot video understanding—answering questions about videos without training on those specific question-answer pairs. Quick check: Can you explain why a model trained on video-description pairs can answer questions it was never explicitly trained to answer?

- **Feature freezing vs. end-to-end training trade-offs**: Why needed here: ResNetVLLM's core innovation is avoiding frozen pretrained features; understanding when features should be frozen vs. trainable is essential. Quick check: What are two scenarios where frozen pretrained features would outperform trainable features initialized from scratch?

- **LLaVA architecture (CLIP encoder + Vicuna decoder)**: Why needed here: The paper builds on LLaVA as its language model base; understanding its visual instruction tuning approach clarifies how ResNetVLLM integrates. Quick check: How does LLaVA differ from a standard GPT-style LLM, and what component handles visual input?

## Architecture Onboarding

- **Component map**: 100 frames (6-frame intervals, 224×224 crops) → 2D ResNet (random init) → Global average pooling → Projection layer → LLaVA-based LLM → Text output

- **Critical path**: Frame extraction → ResNet forward pass → Feature projection → LLM token generation

- **Design tradeoffs**: Non-pretrained ResNet vs. pretrained encoders (task-specific adaptation vs. rich visual priors); 2D vs. 3D convolutions (efficiency vs. explicit motion modeling); 100 frames vs. denser sampling (computational cost vs. temporal resolution); Two-stage vs. end-to-end training (stability vs. complexity)

- **Failure signatures**: Insufficient ResNet warm-up leads to noisy visual features; mismatched learning rates cause catastrophic forgetting; missed key frames result in plausible but incorrect descriptions; projection dimension mismatch prevents visual token integration

- **First 3 experiments**: (1) Ablation on pretrained vs. random ResNet initialization; (2) Single-stage vs. two-stage training comparison; (3) Frame count sensitivity analysis (25, 50, 100, 200 frames)

## Open Questions the Paper Calls Out

- **Open Question 1**: Does the use of flattened global average pooling features result in a loss of fine-grained spatial information that could otherwise improve performance if patch-level features were used? The paper does not provide an ablation study comparing global pooled features against spatial token-based features.

- **Open Question 2**: To what extent is the performance improvement attributable to the "vanilla" (non-pretrained) initialization versus the ability to train the visual encoder jointly with the LLM, compared to simply fine-tuning a pre-trained encoder? The experiments compare against models with frozen features but fail to compare against jointly trained pretrained ResNet baselines.

- **Open Question 3**: How does ResNetVLLM's reliance on a fixed sampling of 100 frames affect its ability to understand temporal dynamics in videos significantly longer or shorter than the training distribution? The paper does not analyze performance variance relative to video duration or the model's capacity to handle temporal dependencies beyond this fixed token count.

## Limitations

- Missing architectural specifications: Exact ResNet variant and projection layer implementation are not provided, making faithful reproduction difficult
- Training configuration gaps: Batch size specifications and the "1 clip for training, 25 clips for testing" notation are unclear
- Limited generalization validation: Strong performance on benchmark datasets but lacks extensive validation on out-of-distribution video content

## Confidence

- **High confidence**: The two-stage training methodology is clearly described and theoretically sound for stabilizing training of randomly initialized visual encoders
- **Medium confidence**: Zero-shot performance claims are supported by benchmark results, but lack direct comparison with pretrained visual encoder baselines
- **Low confidence**: Exact architectural details needed for faithful reproduction (ResNet variant, projection implementation, LLaVA checkpoint version) are unspecified

## Next Checks

1. **Ablation study**: Train ResNetVLLM with both random-initialized and ImageNet-pretrained ResNet variants on the same data to quantify the impact of avoiding pretrained features

2. **Staged training validation**: Compare end-to-end training from random initialization versus the proposed two-stage approach to empirically validate the convergence stability claims

3. **Temporal sampling analysis**: Systematically vary frame counts (25, 50, 100, 200) to determine the minimum temporal resolution required for maintaining performance and identify potential hallucination thresholds