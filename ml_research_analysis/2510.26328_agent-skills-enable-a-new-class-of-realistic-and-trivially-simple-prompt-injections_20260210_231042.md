---
ver: rpa2
title: Agent Skills Enable a New Class of Realistic and Trivially Simple Prompt Injections
arxiv_id: '2510.26328'
source_url: https://arxiv.org/abs/2510.26328
tags:
- skills
- agent
- skill
- claude
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper demonstrates that Agent Skills\u2014a framework for\
  \ equipping agents with new knowledge through simple markdown files\u2014are fundamentally\
  \ vulnerable to prompt injections. The authors show how malicious instructions can\
  \ be hidden in long Agent Skill files and referenced scripts to exfiltrate sensitive\
  \ data such as internal files or passwords."
---

# Agent Skills Enable a New Class of Realistic and Trivially Simple Prompt Injections

## Quick Facts
- **arXiv ID**: 2510.26328
- **Source URL**: https://arxiv.org/abs/2510.26328
- **Reference count**: 5
- **Primary result**: Agent Skills framework vulnerable to prompt injections via malicious markdown files and referenced scripts

## Executive Summary
This paper demonstrates that Agent Skills—a framework for equipping agents with new knowledge through simple markdown files—are fundamentally vulnerable to prompt injections. The authors show how malicious instructions can be hidden in long Agent Skill files and referenced scripts to exfiltrate sensitive data such as internal files or passwords. In experiments with Claude Code, they successfully executed a prompt injection by embedding a malicious backup script in a legitimate PowerPoint editing skill, which was automatically executed after benign task approval with the "Don't ask again" option. The attack bypassed system-level guardrails, allowing unauthorized data exfiltration without further user interaction. Similar attacks were tested on the Claude Web Interface by attaching malicious URLs to model outputs.

## Method Summary
The authors conducted experiments using Claude Code and Claude Web Interface to test prompt injection vulnerabilities in Agent Skills. They created malicious Agent Skill files that appeared legitimate but contained hidden instructions to execute harmful actions. In one attack, they embedded a malicious backup script within a PowerPoint editing skill, which was automatically executed after the user approved the benign task with the "Don't ask again" option. Another attack involved attaching malicious URLs to model outputs in the Claude Web Interface. The experiments demonstrated that the attacks could bypass system-level guardrails and exfiltrate sensitive data without further user interaction.

## Key Results
- Agent Skills framework is vulnerable to prompt injections through malicious markdown files and referenced scripts
- Malicious scripts embedded in legitimate skills can be automatically executed after benign task approval
- System-level guardrails can be bypassed, allowing unauthorized data exfiltration without further user interaction
- Similar attacks demonstrated on Claude Web Interface through malicious URL attachments

## Why This Works (Mechanism)
Agent Skills frameworks allow agents to load external knowledge through markdown files, but lack proper input sanitization and context separation. Malicious actors can embed harmful instructions within seemingly legitimate skill files, which are then executed when the agent processes the context. The "Don't ask again" option further reduces user oversight, enabling automatic execution of malicious actions. The framework's design assumes trusted skill sources, but this assumption is violated when attackers can create or modify skill files.

## Foundational Learning

### Input Sanitization
- **Why needed**: Prevents malicious code or instructions from being executed when processing external inputs
- **Quick check**: Test if special characters or embedded scripts in skill files are properly escaped or filtered

### Context Separation
- **Why needed**: Ensures that malicious instructions in one context don't contaminate or override the agent's intended behavior
- **Quick check**: Verify that skill contexts are isolated and don't persist beyond their intended scope

### Guardrail Bypass
- **Why needed**: Identifies how security measures can be circumvented through creative prompt engineering
- **Quick check**: Test if system prompts and safety instructions can be overwritten by skill content

## Architecture Onboarding

### Component Map
User -> Claude Code -> Agent Skills Loader -> Skill File -> Execution Engine

### Critical Path
1. User requests task requiring new skill
2. Agent Skills Loader fetches and parses skill file
3. Skill content integrated into agent context
4. Execution Engine processes combined context
5. Agent performs task (potentially malicious)

### Design Tradeoffs
- **Flexibility vs Security**: Agent Skills prioritize ease of adding new capabilities but sacrifice input validation
- **Automation vs Oversight**: "Don't ask again" reduces friction but eliminates critical security checkpoints
- **Trust vs Verification**: Assumes skill files are benign without validation mechanisms

### Failure Signatures
- Unexpected file access or exfiltration attempts
- Execution of referenced scripts not explicitly requested by user
- URL attachments in model outputs that weren't user-initiated

### First 3 Experiments
1. Create a skill file that references an external script to read and exfiltrate a password file
2. Test if "Don't ask again" approval for benign tasks enables automatic execution of malicious scripts
3. Attempt to attach malicious URLs to model outputs in the Claude Web Interface

## Open Questions the Paper Calls Out
None

## Limitations
- Experiments limited to Claude Code and Claude Web Interface, limiting generalizability to other agent systems
- Attack scenarios focus on specific use cases (PowerPoint editing, URL attachments)
- Study does not address potential mitigations or defenses against these vulnerabilities

## Confidence

**High**: The core finding that Agent Skills are vulnerable to prompt injections due to insufficient input sanitization and lack of robust context separation.

**Medium**: The effectiveness of the demonstrated attack vectors (malicious scripts and URL attachments) in real-world scenarios, as these were tested in controlled environments.

**Low**: The scalability of these attacks across different agent frameworks and the potential for defenders to implement effective mitigations.

## Next Checks
1. Test the attack vectors across a broader range of agent frameworks and models (e.g., OpenAI, Google, or open-source agents) to assess generalizability.
2. Evaluate the effectiveness of potential mitigations, such as input sanitization, context separation, and user interaction requirements, against the demonstrated attack scenarios.
3. Investigate the feasibility of automated detection systems for identifying and blocking malicious Agent Skills or embedded instructions.