---
ver: rpa2
title: 'CDM-QTA: Quantized Training Acceleration for Efficient LoRA Fine-Tuning of
  Diffusion Model'
arxiv_id: '2504.07998'
source_url: https://arxiv.org/abs/2504.07998
tags:
- diffusion
- lora
- training
- memory
- fine-tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a hardware accelerator for LoRA fine-tuning
  of diffusion models, addressing the high computational cost and memory requirements
  of fine-tuning large diffusion models on mobile devices. The method uses a fully
  quantized training scheme with LoRA, reducing memory usage and power consumption
  while maintaining model fidelity.
---

# CDM-QTA: Quantized Training Acceleration for Efficient LoRA Fine-Tuning of Diffusion Model

## Quick Facts
- arXiv ID: 2504.07998
- Source URL: https://arxiv.org/abs/2504.07998
- Authors: Jinming Lu; Minghao She; Wendong Mao; Zhongfeng Wang
- Reference count: 16
- The paper proposes a hardware accelerator for LoRA fine-tuning of diffusion models, addressing the high computational cost and memory requirements of fine-tuning large diffusion models on mobile devices. The method uses a fully quantized training scheme with LoRA, reducing memory usage and power consumption while maintaining model fidelity. The accelerator features flexible dataflow (weight stationary and output stationary) for efficient processing of irregular tensor shapes. Experiments show up to 1.81x training speedup and 5.50x energy efficiency improvement over baseline, with minimal impact on image generation quality. The design achieves 1.64x and 1.83x improvements in energy and area efficiency compared to previous work.

## Executive Summary
This paper presents CDM-QTA, a hardware accelerator specifically designed for Low-Rank Adaptation (LoRA) fine-tuning of diffusion models. The accelerator addresses the challenge of efficiently fine-tuning large diffusion models on resource-constrained devices by combining LoRA's parameter efficiency with fully quantized training and a flexible dataflow architecture. The system achieves significant improvements in training speed and energy efficiency while maintaining model fidelity for custom concept learning tasks.

## Method Summary
CDM-QTA implements LoRA fine-tuning on Stable Diffusion's cross-attention layers using INT8 fully quantized training. The accelerator features a 64×64 systolic array with configurable weight-stationary (WS) and output-stationary (OS) dataflows to handle irregular tensor shapes. The design includes dedicated memory hierarchies (512KB weight, 512KB input, 1MB output memory) with double buffering to hide DRAM latency. The training pipeline quantizes weights per-tensor and activations/gradients per-channel, enabling integer-only MAC operations while maintaining convergence.

## Key Results
- Up to 1.81× training speedup compared to baseline
- 5.50× improvement in energy efficiency (EDP)
- 1.64× and 1.83× improvements in energy and area efficiency versus previous work
- Minimal impact on image generation quality during fine-tuning
- Flexible dataflow achieves 1.22× and 1.27× speedup over fixed WS and OS respectively

## Why This Works (Mechanism)

### Mechanism 1
LoRA substitution in cross-attention layers reduces trainable parameters to ~5% of the full model while preserving generation quality. The original weight matrix W is decomposed into two low-rank matrices A and B where r ≪ min(d₁, d₂). During fine-tuning, only A and B are updated instead of the full projection matrices for key and value mappings. Frozen weights still participate in forward/backward computation but require no gradient storage. Core assumption: The low-rank decomposition sufficiently captures the concept adaptation needed for custom diffusion without significant representational loss.

### Mechanism 2
Fully quantized INT8 training maintains convergence while reducing memory and compute requirements. Weights use per-tensor quantization; activations and gradients use per-channel/per-column schemes. Quantization scale S = X_max / (2^q-1) where q=8. All MAC operations execute in integer arithmetic, avoiding floating-point overhead. Core assumption: The quantization error introduced does not exceed the noise tolerance inherent in diffusion model training (which already operates in noisy latent space).

### Mechanism 3
Hybrid weight-stationary (WS) and output-stationary (OS) dataflow maximizes PE utilization for irregular LoRA tensor shapes. WS uses inner-product GEMM (weights pre-loaded, inputs streamed). OS uses outer-product GEMM (broadcast inputs/weights, accumulate partial sums locally). Text embeddings have short sequences (<77 tokens); image latents have long sequences (4096). LoRA matrices A and B introduce additional shape irregularity. The controller selects optimal dataflow per layer. Core assumption: The overhead of dataflow configuration and mode switching is less than the utilization gain from matching dataflow to layer shape.

## Foundational Learning

- **Concept:** Systolic Array Dataflows (Weight-Stationary vs Output-Stationary)
  - **Why needed here:** Understanding WS vs OS is essential to grasp why hybrid switching improves utilization for irregular shapes.
  - **Quick check question:** For a matrix multiplication C = A × B where A is 64×4096 and B is 4096×64, which dataflow minimizes weight reloads if B must be reused across multiple A rows?

- **Concept:** Low-Rank Adaptation (LoRA) Decomposition
  - **Why needed here:** The core algorithmic compression that enables reduced memory and compute.
  - **Quick check question:** Given W ∈ R^(1024×1024) and rank r=8, what are the shapes of A and B, and what is the parameter reduction ratio?

- **Concept:** Quantization-Aware Training vs Post-Training Quantization
  - **Why needed here:** This paper uses fully quantized training (weights, activations, and gradients quantized during backprop), which differs from inference-only quantization.
  - **Quick check question:** Why does gradient quantization require per-channel scaling more than weight quantization does?

## Architecture Onboarding

- **Component map:**
  - Controller receives instructions, configures dataflow mode (WS/OS), coordinates array and memory
  - 64×64 Systolic Array configurable PE array executing GEMM with dual dataflow support
  - Weight Memory (512KB) stores weight tensors loaded from DRAM
  - Input Activation Memory (512KB) stores input feature maps
  - Output Activation Memory (1MB) accumulates partial sums, larger due to output buffering
  - DRAM Interface off-chip memory with double-buffering to hide latency

- **Critical path:**
  1. Load LoRA matrices A and B + frozen cross-attention weights from DRAM to SRAM
  2. Configure dataflow based on current layer shape (text vs image sequence length)
  3. Execute forward pass: frozen weights + LoRA contribution (X × A × B^T)
  4. Execute backward pass: gradient computation through frozen weights to LoRA matrices
  5. Update only A and B matrices; write back to DRAM

- **Design tradeoffs:**
  - Array size (64×64): Larger array increases throughput but reduces utilization for small LoRA matrices; 64×64 balances GEMM efficiency with LoRA irregularity
  - Memory allocation (512KB/512KB/1MB): Output memory doubled to accommodate partial sum accumulation in OS mode; reduces weight buffer capacity
  - INT8 quantization: Reduces precision vs FP32 but enables integer-only MAC; convergence depends on per-channel scaling accuracy

- **Failure signatures:**
  - Low PE utilization on specific layers: Indicates mismatched dataflow selection; check layer shape vs configured mode
  - Gradient overflow/underflow during training: Quantization scale may be too coarse; verify per-channel scaling factors
  - Image quality degradation after fine-tuning: LoRA rank r may be too low, or quantization error accumulated; compare FP32 baseline
  - Stall cycles on DRAM interface: Double buffer not hiding latency; profile memory access patterns

- **First 3 experiments:**
  1. Dataflow ablation: Run fine-tuning with WS-only, OS-only, and hybrid modes; measure per-layer utilization and total latency. Compare against Fig. 6 results to validate simulator predictions.
  2. Quantization sensitivity: Train with INT8 vs mixed precision (FP16 activations, INT8 weights) vs full FP32; compare FID scores and convergence curves. Identify which layers are most sensitive to quantization.
  3. LoRA rank sweep: Test r = 4, 8, 16, 32 on multiple concept datasets (pets, objects, scenes); measure generation quality vs training time. Correlate with memory footprint to find optimal r for target edge device constraints.

## Open Questions the Paper Calls Out

### Open Question 1
Does the fully quantized INT8 training scheme require more iterations to converge compared to FP32 training, potentially offsetting the per-iteration latency gains? The paper reports "minimal impact on image generation quality" and hardware speedup, but does not plot training loss curves or compare the total number of epochs required to reach convergence against the FP32 baseline. Hardware speedup is measured per operation, but if quantization noise degrades gradient information, the model may require more training steps to achieve the same fidelity, negating the energy benefits.

### Open Question 2
Can the proposed flexible dataflow architecture maintain high utilization efficiency when applied to Diffusion Transformer (DiT) architectures, which have different attention matrix dimension characteristics than the U-Net based Stable Diffusion? The paper specifically attributes the need for flexible dataflow (WS/OS) to the irregular shapes in U-Net cross-attention (text length < 77 vs. image length 4096). It does not evaluate the architecture on emerging DiT models where attention mechanisms and tensor shapes differ.

### Open Question 3
How does the accelerator's efficiency scale with the LoRA rank (r), specifically regarding the trade-off between memory traffic for small matrices and PE array utilization? The paper assumes r ≪ min(d₁, d₂) to justify the method, and the dataflow switches to handle variable tensor shapes. However, it does not analyze performance sensitivity if r increases significantly (e.g., for complex multi-concept fine-tuning), potentially reducing the sparsity/irregularity the accelerator optimizes for.

## Limitations
- Exact LoRA rank configuration and quantization calibration procedure remain unspecified
- Image quality preservation claims are qualitative without quantitative metrics (FID scores)
- Dataflow selection algorithm and specific criteria for choosing WS vs OS per layer are not detailed
- No evaluation on emerging DiT architectures with different attention matrix characteristics

## Confidence
**High Confidence:** The fundamental LoRA mechanism and hybrid WS/OS dataflow concept are well-established and mathematically sound. The 64×64 systolic array configuration is a reasonable choice for GEMM acceleration.

**Medium Confidence:** The INT8 quantization scheme is theoretically sound but lacks empirical validation of gradient quantization stability across training steps. The claimed 1.81× speedup and 5.50× energy efficiency improvements are plausible but require independent verification.

**Low Confidence:** The image quality preservation claims are qualitative only. Without quantitative metrics or baseline comparisons, it's impossible to verify that quantization and reduced precision training maintain generation quality.

## Next Checks
1. Implement the hybrid WS/OS controller and measure PE utilization per layer during LoRA fine-tuning. Compare against fixed WS and fixed OS baselines to confirm the 1.22× and 1.27× improvements reported in Fig. 6.

2. Conduct systematic experiments varying quantization precision (INT8, INT4, mixed precision) and per-channel vs per-tensor scaling. Measure both training convergence speed and final image quality metrics to identify the minimum precision that maintains acceptable generation quality.

3. Sweep LoRA rank r from 2 to 64 on the same concept datasets and measure the relationship between parameter count, training time, and image generation quality. This will quantify the actual "5% parameter" claim and identify optimal configurations for different edge device constraints.