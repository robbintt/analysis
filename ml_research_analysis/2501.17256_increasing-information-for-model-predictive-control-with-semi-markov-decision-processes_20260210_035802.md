---
ver: rpa2
title: Increasing Information for Model Predictive Control with Semi-Markov Decision
  Processes
arxiv_id: '2501.17256'
source_url: https://arxiv.org/abs/2501.17256
tags:
- control
- learning
- information
- system
- process
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of sample complexity in Learning-Based
  Model Predictive Control (LB-MPC) by introducing temporal abstraction through Semi-Markov
  Decision Processes (SMDP). The core idea is to improve information gathering during
  exploration by allowing temporally extended actions, where the system can remain
  under a fixed control for variable durations rather than updating at every time
  step.
---

# Increasing Information for Model Predictive Control with Semi-Markov Decision Processes

## Quick Facts
- arXiv ID: 2501.17256
- Source URL: https://arxiv.org/abs/2501.17256
- Reference count: 13
- Primary result: SMDP-based exploration achieves higher Expected Information Gain and improved sample complexity versus standard MDP approaches

## Executive Summary
This paper addresses the sample complexity challenge in Learning-Based Model Predictive Control (LB-MPC) by introducing temporal abstraction through Semi-Markov Decision Processes (SMDP). The key insight is that allowing temporally extended actions—where control actions can be held constant for variable durations—enables more informative exploration within a fixed sampling budget. The authors extend the Trajectory Information Planning (TIP) algorithm by incorporating inter-decision times into the control policy, allowing the agent to select not only the control action but also how long to apply it before the next decision.

The method uses an Expected Information Gain (EIG) criterion adapted for SMDPs to guide exploration, with experimental validation on Inverted Pendulum and Lorenz attractor systems. Results show that SMDP-based exploration achieves higher EIG values compared to standard MDP-based approaches, particularly in early sampling iterations, and reaches optimal control performance in fewer iterations. The approach is especially effective when the maximal inter-decision time is moderate (2-4 steps), though performance degrades when the extension is too large due to increased model prediction error.

## Method Summary
The method extends LB-MPC by introducing SMDP-based exploration through variable inter-decision times. It combines Gaussian Process regression for dynamics modeling with MPC using the iCEM planner, and extends the action space to include control duration. The core innovation is the Expected Information Gain (EIG) criterion modified for SMDPs, which jointly selects control actions and durations to maximize information gain. The exploration strategy samples candidate (control, duration) pairs, estimates EIG via Monte Carlo bootstrapping, and selects the pair maximizing information gain. This framework increases the reachable state set within trajectory constraints while reducing sample complexity by decorrelating consecutive samples through temporal abstraction.

## Key Results
- SMDP-based exploration achieves higher EIG values than standard MDP approaches, particularly in early sampling iterations
- Experimental results on Inverted Pendulum and Lorenz attractor demonstrate improved sample complexity, reaching optimal control performance in fewer iterations
- SMDP methods are especially effective with moderate maximal inter-decision times (2-4 steps), with performance degrading for tmax=8 due to bootstrapping error
- The approach maintains trajectory-based sampling while expanding the reachable state set through temporal abstraction

## Why This Works (Mechanism)

### Mechanism 1
Temporally-extended actions through SMDP modeling increase information gain per sample compared to standard MDP approaches. By allowing variable inter-decision times, the system reaches more distant states unreachable via single-step MDP transitions, decorrelating consecutive samples and reducing redundancy when dynamics exhibit high autocorrelation or evolve slowly. This works when system dynamics have multiple time scales and informative state regions require multiple timesteps to reach. Evidence shows SMDP methods achieve higher EIG early in sampling, though the benefit degrades when dynamics are fast relative to control frequency or bootstrapping error exceeds information benefit.

### Mechanism 2
Expected Information Gain (EIG) adapted for SMDPs provides a principled acquisition function for jointly selecting control actions and inter-decision times. EIG quantifies entropy reduction about the optimal trajectory given a new transition, conditioning on both action and duration for SMDPs. This works when the GP model provides calibrated uncertainty estimates and Monte Carlo EIG estimation via bootstrapped future states is sufficiently accurate. The method measures uncertainty reduction for transitions given control held constant for duration t, though high bootstrapping error can make EIG estimation unreliable, causing suboptimal action/duration selection.

### Mechanism 3
Trajectory-constrained exploration with SMDP extension increases the reachable state set within physical realizability constraints. Real systems limit queryable state-action pairs to those along trajectories, and SMDP extension expands the reachable subset by allowing trajectory jumps via inter-decision times > 1. This works when many real-world systems are not fully controllable from arbitrary states and exploration must follow realizable trajectories. The approach yields Supp(D^SM-TIP_n) ⊇ Supp(D^TIP_n), though if the system is fully controllable, trajectory-constrained exploration is unnecessarily restrictive and SMDP provides no advantage over global exploration.

## Foundational Learning

**Gaussian Process (GP) Regression for Dynamics Modeling**
Why needed: GP regression models transition probabilities with uncertainty quantification, essential for computing entropy-based EIG
Quick check: Can you explain how a GP provides both mean prediction and uncertainty for new inputs, and why posterior covariance depends on training data distribution?

**Semi-Markov Decision Processes (SMDP)**
Why needed: The core contribution extends MDP exploration to SMDPs via variable inter-decision times. Understanding options, decision epochs, and inter-decision times is essential
Quick check: How does an SMDP differ from a standard MDP, and what does the "semi-Markov" property mean for state transitions?

**Information Theory (Entropy and Mutual Information)**
Why needed: EIG is defined via differential entropy and conditional mutual information. Understanding these is necessary to interpret exploration guidance
Quick check: What does mutual information measure between two random variables, and why is maximizing EIG equivalent to minimizing conditional mutual information here?

## Architecture Onboarding

**Component map:** GP Dynamics Model -> EIG Estimator -> MPC Controller -> SMDP Sampler

**Critical path:** Per sampling iteration: (1) Sample candidate controls/durations uniformly; (2) Bootstrap future states via GP; (3) Compute EIG^SM-TIP via Monte Carlo; (4) Select (u,t) maximizing EIG; (5) Execute control for duration t on true system; (6) Observe resulting state and update GP dataset

**Design tradeoffs:** Max inter-decision time (tmax): Larger values reach more distant states but increase bootstrapping error (tmax=8 fails for Pendulum, tmax=2,4 succeed). Monte Carlo samples (m): More samples improve EIG accuracy but increase computation. GP kernel choice: Must capture dynamics structure; inappropriate kernels yield poor uncertainty estimates and misleading EIG. Sampling budget vs. horizon: Limited budgets favor early high-information samples

**Failure signatures:** (1) EIG consistently high but model doesn't improve—GP uncertainty miscalibrated; (2) Inter-decision times always equal tmax—EIG dominated by bootstrapping noise; (3) Controller degrades with more samples—model overfitting or exploration visiting low-value regions

**First 3 experiments:**
1. Reproduce Inverted Pendulum with tmax ∈ {1, 2, 4, 8}; verify EIG^SM-TIP exceeds EIG^TIP early and tmax=8 degrades from bootstrapping error
2. Vary Monte Carlo sample counts (m ∈ {10, 50, 100}); characterize accuracy vs. computational cost tradeoff; plot EIG variance across runs
3. Test on synthetic multi-timescale system (slow/fast modes) vs. single-timescale baseline; confirm SMDP benefits specifically require varying time scales

## Open Questions the Paper Calls Out

**Open Question 1**
Can the SMDP-based exploration strategy be effectively scaled to high-dimensional dynamical systems or complex real-world robotics platforms? The conclusion states future work may extend this methodology to more complex systems, but experimental validation is currently limited to low-dimensional academic benchmarks. Demonstration on high-dimensional tasks (>10 dimensions) or physical robotic hardware would resolve this.

**Open Question 2**
How can the trade-off between temporal extension and bootstrapping error be managed to prevent performance degradation at larger inter-decision times? Results show tmax=8 fails on the Pendulum due to bootstrapping prediction error, but no solution is offered to stabilize EIG estimates for longer horizons. An adaptive mechanism regulating inter-decision time based on uncertainty estimates would resolve this.

**Open Question 3**
Is it possible to develop a causal or recursive estimator for the EIG to eliminate the reliance on non-causal bootstrapping of future states? The EIG metric is non-causal and requires looking ahead using bootstrapped states, which may not be feasible in strictly online settings. A reformulation relying only on current and past data, or theoretical bounds showing bootstrapping bias is negligible, would resolve this.

## Limitations
- Empirical validation limited to two relatively simple systems (Inverted Pendulum and Lorenz attractor) without testing on higher-dimensional robotics or real-world systems
- Theoretical analysis focuses on information gain properties but lacks formal sample complexity bounds or regret guarantees
- Monte Carlo EIG estimation introduces variance that isn't fully characterized, with sensitivity to GP hyperparameters and bootstrapping error incompletely explored

## Confidence
**High**: Core mechanism that SMDP extension increases information gain through decorrelated samples follows directly from entropy formulation
**Medium**: Empirical claims about improved sample complexity given limited experimental scope and lack of comparison to other exploration strategies
**Low**: Claims about trajectory-constrained exploration benefits requiring assumptions about system controllability not validated experimentally

## Next Checks
1. Test on a higher-dimensional system (e.g., quadrotor or 2D navigation) to verify scalability of SMDP exploration benefits
2. Characterize EIG estimation variance across multiple random seeds and MC sample counts to quantify uncertainty in action selection
3. Compare against alternative exploration strategies (e.g., UCB, Thompson sampling) to isolate the benefit of SMDP temporal abstraction specifically