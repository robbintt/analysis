---
ver: rpa2
title: 'UniAIDet: A Unified and Universal Benchmark for AI-Generated Image Content
  Detection and Localization'
arxiv_id: '2510.23023'
source_url: https://arxiv.org/abs/2510.23023
tags:
- image
- methods
- images
- detection
- localization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces UniAIDet, a comprehensive benchmark for detecting
  and localizing AI-generated image content. Unlike prior work, UniAIDet covers diverse
  image categories (photographic and artistic) and generative model types (text-to-image,
  image-to-image, inpainting, editing, and deepfake), making it the first large-scale,
  wide-coverage dataset for this task.
---

# UniAIDet: A Unified and Universal Benchmark for AI-Generated Image Content Detection and Localization

## Quick Facts
- arXiv ID: 2510.23023
- Source URL: https://arxiv.org/abs/2510.23023
- Reference count: 21
- This paper introduces UniAIDet, a comprehensive benchmark for detecting and localizing AI-generated image content across diverse categories and model types.

## Executive Summary
This paper introduces UniAIDet, a comprehensive benchmark for detecting and localizing AI-generated image content. Unlike prior work, UniAIDet covers diverse image categories (photographic and artistic) and generative model types (text-to-image, image-to-image, inpainting, editing, and deepfake), making it the first large-scale, wide-coverage dataset for this task. The benchmark includes 80k images and supports fine-grained localization through provided masks. Evaluations reveal that existing methods struggle with partial synthesis models and artistic images, highlighting significant generalization gaps. Detection and localization tasks are generally consistent, but current methods fail notably on edited and inpainted images. The study concludes that robust detection requires improved generalization across generative models and image types. UniAIDet provides a valuable resource for advancing research in AI-generated content detection.

## Method Summary
The UniAIDet benchmark construction uses Gemma3-4B for captions/editing instructions, SAM for inpainting masks, threshold-based difference detection (τ=40, min region size γ=20) for editing masks, and method-provided masks for deepfake images. The benchmark contains 80k images from 20 generative models (8 text-to-image, 1 image-to-image, 5 inpainting, 3 editing, 2 deepfake) and real images from MSCOCO, NYTimes800k, WikiArt, and Danbooru (all pre-2023). Evaluation metrics include Acc, AP, f.Acc, r.Acc for detection and mIoU for localization (partial synthesis only). The benchmark supports both binary detection and pixel-level localization tasks across photographic and artistic image domains.

## Key Results
- Existing detection methods fail on partial synthesis models (inpainting, editing, deepfake) while performing well on holistic synthesis
- Frequency-based detection features generalize better across photographic and artistic image domains than semantic features
- Detection and localization performance are positively correlated for partial synthesis images
- CLIP-based methods show 7-10% accuracy drop on artistic images compared to photographs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Frequency-based detection features generalize better across photographic and artistic image domains than semantic features.
- **Mechanism:** Frequency-domain artifacts persist across image categories while CLIP-based semantic features exhibit distribution shift on artistic styles.
- **Core assumption:** Frequency artifacts from generative models are category-agnostic and transfer across content domains.
- **Evidence anchors:** SAFE maintains 79.89% Photo / 79.15% Art accuracy; DeeCLIP drops from 68.90% to 64.25%.

### Mechanism 2
- **Claim:** Detection and localization performance are positively correlated for partial synthesis images.
- **Mechanism:** Accurate localization of generated regions naturally supports binary classification through spatial evidence.
- **Core assumption:** Generated regions contain detectable artifacts distinguishable from real content.
- **Evidence anchors:** f.Acc and mIoU exhibit nearly identical trends across methods; visual comparison shows curves tracking together.

### Mechanism 3
- **Claim:** Partial synthesis models are fundamentally harder to detect than holistic synthesis because they preserve authentic image statistics in unmodified regions.
- **Mechanism:** Holistic models generate every pixel leaving global artifacts, while partial synthesis leaves 60-90% of pixels untouched diluting detectable signals.
- **Core assumption:** Detection methods trained on fully synthetic images rely on global statistics that partial synthesis obscures.
- **Evidence anchors:** DRCT and SAFE show near-zero f.Acc on editing/inpainting models while maintaining 80%+ on T2I models.

## Foundational Learning

- **Concept:** Binary Detection vs. Pixel-Level Localization
  - **Why needed here:** The benchmark evaluates both tasks; methods optimized for one may fail on the other.
  - **Quick check question:** Given an inpainted image with 15% modified pixels, should a detector focus on global statistics or local inconsistencies?

- **Concept:** Holistic vs. Partial Synthesis Taxonomy
  - **Why needed here:** UniAIDet explicitly categorizes generative models into these two classes with different failure modes.
  - **Quick check question:** Is a face-swapped deepfake image a holistic or partial synthesis? What about style transfer?

- **Concept:** Cross-Domain Generalization (Photo → Art)
  - **Why needed here:** The paper demonstrates that models trained on photographs fail on artistic content.
  - **Quick check question:** A detector trained on SDXL outputs evaluated on anime artwork shows 20% accuracy drop. Is this a training distribution shift or a fundamental capacity limitation?

## Architecture Onboarding

- **Component map:** Image → Feature Extractor (CLIP/Frequency/Hybrid) → Detection Head (Binary Classifier) + Localization Head (Pixel-wise Mask Predictor) → Evaluation Metrics
- **Critical path:** Load image from benchmark → Pass through feature extractor → Aggregate to binary prediction (detection) or generate mask (localization) → Compute Acc/AP/mIoU
- **Design tradeoffs:** Frequency vs. semantic features (generalization vs. artifact capture), joint vs. separate detection/localization (representation sharing vs. task interference), training data composition (partial synthesis inclusion vs. annotation cost)
- **Failure signatures:** High r.Acc, low f.Acc (real-biased predictions), high holistic f.Acc, near-zero partial f.Acc (global statistic reliance), high detection Acc, low mIoU (spurious feature usage)
- **First 3 experiments:**
  1. Baseline frequency detector on Photo split only to quantify generalization gap
  2. Cross-domain transfer test (Photo/holistic → Art/partial) to identify worst-case failure modes
  3. Localization consistency check: correlation between detection confidence and mask IoU to validate detection-localization relationship

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can detection and localization methods be adapted to generalize effectively to partial synthesis models, specifically inpainting and instruction-guided editing?
- **Basis in paper:** The authors state that "generalization is still a challenging problem" and note that strong detectors like DRCT and SAFE "fail on most partial synthesis models."
- **Why unresolved:** Current methods excel at holistic synthesis (T2I) but exhibit significant performance drop when identifying images where only a region is AI-generated.
- **What evidence:** A method that achieves high mIoU and accuracy on the "Edit" and "Inpaint" splits of UniAIDet, comparable to its performance on text-to-image subsets.

### Open Question 2
- **Question:** How can the architectural bias towards "foreground distinguishable objects" be corrected in localization models to handle arbitrary editing scenarios?
- **Basis in paper:** The case study in Appendix C.2 observes that current models "tend to predict 'foreground distinguishable objects'" because they are fine-tuned on segmentation models.
- **Why unresolved:** Localization models incorrectly constrain generated regions to salient objects, failing to detect edits in backgrounds or styles.
- **What evidence:** Qualitative and quantitative improvements in localizing non-object edits (e.g., background changes) within the benchmark's editing subset.

### Open Question 3
- **Question:** What architectural or training modifications are required to overcome the "fundamental lack of overall capability" in joint detection & localization models?
- **Basis in paper:** Section 4.2.3 concludes that joint methods perform poorly across nearly all models due to a "fundamental lack of overall capability."
- **Why unresolved:** Specialized joint models (e.g., FakeShield, SIDA) currently underperform compared to detection-only baselines.
- **What evidence:** A joint model that surpasses the detection accuracy of state-of-the-art detection-only methods (like SAFE) while maintaining reliable localization mIoU.

## Limitations
- Benchmark evaluation is limited to pre-2023 generative models, potentially missing current detection challenges
- Performance metrics are reported at dataset level without individual model breakdowns, masking variability in detection difficulty
- Frequency-based detection mechanism's superiority lacks theoretical grounding for why artifacts persist across diverse content domains

## Confidence
- **High confidence:** Detection-localization correlation findings (consistent visual and numerical evidence)
- **Medium confidence:** Frequency-based generalization claims (strong empirical support but limited theoretical explanation)
- **Medium confidence:** Partial synthesis detection difficulty (well-supported by current results but may evolve with improved training methods)

## Next Checks
1. **Temporal validation:** Evaluate detection performance on images generated by post-2023 models (SD3, Midjourney v6, GPT-4o image generation) to assess benchmark relevance
2. **Adversarial robustness test:** Apply simple adversarial perturbations (small rotations, JPEG compression, contrast adjustments) to synthetic images and measure detection/localization performance degradation
3. **Theoretical investigation:** Analyze frequency spectra of successfully detected vs. undetected partial synthesis images to identify specific artifact patterns enabling detection