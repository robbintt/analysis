---
ver: rpa2
title: 'M^3Builder: A Multi-Agent System for Automated Machine Learning in Medical
  Imaging'
arxiv_id: '2502.20301'
source_url: https://arxiv.org/abs/2502.20301
tags:
- files
- dataset
- json
- data
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces M3Builder, a multi-agent system for automating
  machine learning workflows in medical imaging. The system uses four specialized
  agents (Task Manager, Data Engineer, Module Architect, and Model Trainer) that collaborate
  within a structured workspace containing datasets, code templates, and interaction
  tools.
---

# M^3Builder: A Multi-Agent System for Automated Machine Learning in Medical Imaging

## Quick Facts
- arXiv ID: 2502.20301
- Source URL: https://arxiv.org/abs/2502.20301
- Reference count: 40
- Primary result: M^3Builder achieves 94.29% task completion rate on M3Bench benchmark using Claude-3.7-Sonnet

## Executive Summary
M^3Builder introduces a multi-agent system that automates the entire machine learning pipeline for medical imaging tasks. The system employs four specialized agents—Task Manager, Data Engineer, Module Architect, and Model Trainer—that collaborate within a structured workspace containing datasets, code templates, and interaction tools. Each agent focuses on specific subtasks while leveraging the others' outputs, creating a collaborative environment that significantly outperforms single-agent approaches. The authors evaluate their system on M3Bench, a benchmark spanning four tasks across 14 datasets covering five anatomies and three imaging modalities, demonstrating the potential for fully automated machine learning in medical imaging.

## Method Summary
M^3Builder is a multi-agent system built on LangGraph that automates ML workflows in medical imaging. The system uses four specialized agents with distinct roles: Task Manager selects datasets and creates planning documents, Data Engineer generates train/test JSON indices by traversing datasets, Module Architect writes dataloaders based on templates, and Model Trainer handles training execution and debugging. The workspace provides structured resources including datacards with dataset metadata, code templates based on Transformers Trainer and nnU-Net frameworks, and an 8-function toolset (list_files, read_files, write_files, edit_files, run_script, copy_files, preview_dirs, preview_files). Agents iterate through code generation and compiler feedback loops, with the Model Trainer having authority to modify any component based on errors. The system uses Claude-3.7-Sonnet as the LLM core and limits each execution to 100 actions.

## Key Results
- M^3Builder achieves 94.29% task completion rate on M3Bench benchmark using Claude-3.7-Sonnet
- Ablation studies show removing collaboration drops success from 82.14% to 39.29% (42.85% gap)
- Removing debugging capability causes the largest performance drop (82.14% to 21.43%)
- Weaker models (Gemini-2.0-Flash, Llama-3.3-70B) achieve only 4.29% success rate

## Why This Works (Mechanism)

### Mechanism 1: Role-Specialized Multi-Agent Decomposition
Dividing the ML workflow into four specialized agents with distinct responsibilities improves task completion compared to single-agent approaches, conditional on the LLM core's reasoning and tool-use capabilities. Each agent receives role-specific system prompts, a curated toolset subset, and works on a bounded sub-problem. Task Manager selects datasets and creates planning documents; Data Engineer generates train/test JSON indices; Module Architect writes dataloaders; Model Trainer handles training execution and debugging. This reduces per-agent complexity and enables focused iteration loops with compiler feedback.

### Mechanism 2: Constrained Workspace with Templates and Toolset
Providing a structured workspace with data cards, code templates (based on Transformers Trainer and nnU-Net), and a fixed 8-function toolset improves convergence by reducing the action space and giving agents validated starting points. Data cards provide dataset metadata in natural language. Code templates offer modular packages for four task types (segmentation, detection, diagnosis, report generation). The toolset restricts operations to a predefined set, with preview_* tools handling files exceeding context windows.

### Mechanism 3: Iterative Auto-Debugging with Compiler Feedback
The self-correction loop—where agents generate code, execute via run_script, receive compiler/environment feedback, and edit until success—is critical for handling the fragility of code generation in medical imaging pipelines. Each code-generating agent follows: write/edit code → run_script → parse error traceback → modify → re-run. The Model Trainer has authority to modify any component (model, dataloader, training script) based on errors.

## Foundational Learning

- **LangGraph for Agent Orchestration**
  - Why needed: The paper explicitly uses LangGraph architecture for building agents and compiling workflow graphs (Section A.4). Understanding state management, tool binding, and conditional edges is required to modify the agent coordination logic.
  - Quick check: Can you sketch how you would add a fifth agent between Data Engineer and Module Architect in a LangGraph workflow?

- **Transformers Trainer Framework and nnU-Net Architecture**
  - Why needed: All code templates are built on these frameworks (Section 2.2). Extending or debugging templates requires understanding Trainer callbacks, data collators, and nnU-Net's self-configuring segmentation pipeline.
  - Quick check: Where would you inject a custom loss function in a Transformers Trainer-based medical imaging pipeline?

- **Tool-Calling LLM Agents with Function Schemas**
  - Why needed: The 8-function toolset is exposed via function schemas; agents select and invoke tools based on docstrings (Section A.3). Understanding parameter validation, error handling, and tool result formatting is essential for adding new capabilities.
  - Quick check: If you added a visualize_results tool, what constraints would you place on input paths to prevent filesystem access outside the workspace?

## Architecture Onboarding

- **Component map:** Workspace Layer (datacards, templates, execution directory) -> Agent Layer (Task Manager → Data Engineer → Module Architect → Model Trainer) -> Execution Layer (Python environment with compiler feedback)

- **Critical path:** User requirement → Task Manager reads datacards, selects dataset, outputs planning document → Data Engineer traverses dataset, generates train.json/test.json/label_dict.json → Module Architect reads JSON indices + template, writes dataloader.py, validates → Model Trainer copies train.py/train.sh, configures imports, executes training, debugs until completion → Trained model artifact

- **Design tradeoffs:**
  - Constrained toolset (8 tools) vs. flexibility → Reduces error surface but limits adaptability to novel workflows
  - Multi-agent coordination overhead vs. single-agent simplicity → 42.85% performance gain at cost of more complex orchestration
  - Template-based code vs. free-form generation → Faster convergence but architectural lock-in to Transformers Trainer/nnU-Net
  - 100-action ceiling vs. unlimited iteration → Prevents runaway costs but may truncate valid long-horizon solutions

- **Failure signatures:**
  - LLM core inadequacy: Gemini-2.0-Flash and Llama-3.3-70B at 4.29% success — check tool-calling reliability before deployment
  - Debug loop exhaustion: Reaching 100 actions without success — likely indicates error beyond LLM's diagnostic capability
  - Dataset structure mismatch: Data Engineer fails to generate valid JSON indices — may require datacard refinement
  - Context window overflow: preview_dirs/preview_files usage spikes — dataset may be too large for current context strategy

- **First 3 experiments:**
  1. Baseline validation: Run all 14 tasks with Claude-3.7-Sonnet, log per-agent action counts and iteration distributions to establish your own baseline against Table 1 metrics.
  2. Toolset ablation: Systematically remove one tool at a time (especially preview_dirs or run_script) to understand critical tool dependencies for your target tasks.
  3. New dataset integration: Add a 15th dataset with a custom datacard (following the JSON schema in Section A.2), run 5 trials, and measure success rate vs. datacard detail level to test workspace extensibility.

## Open Questions the Paper Calls Out

- **Do the models achieve diagnostic accuracy comparable to human-expert tuned baselines?**
  The authors define task completion as successfully training a model with performance falling within an "acceptable range," but results focus on automation success rate rather than absolute performance metrics of resulting models against state-of-the-art baselines. It is unclear if agents' code debugging translates into tuning for maximal clinical accuracy.

- **How can the system effectively incorporate visual processing to better approximate clinical expertise?**
  The Conclusion states that future work aims to "incorporate visual processing to better approximate clinical expertise." Current agent cores are LLMs that process text (code, metadata, logs) and file structures, but lack the ability to visually inspect imaging data to identify anomalies or quality issues that a clinician would spot.

- **Can the framework autonomously adapt to completely raw, unstructured datasets?**
  The Conclusion lists "implement automated dataset preparation capabilities" as future work, while the Method section describes a workspace that requires datasets to be accompanied by structured descriptions (datacards). The current system relies on human-provided or GPT-synthesized metadata to guide the Task Manager.

## Limitations

- Model dependency: System's success heavily depends on LLM's tool-calling and reasoning capabilities, with weaker models achieving only 4.29% success rate
- Template lock-in: System is architecturally constrained to Transformers Trainer and nnU-Net frameworks, limiting adaptability to novel architectures
- Dataset representation: M3Bench benchmark may not represent full diversity of medical imaging challenges, particularly rare diseases or novel acquisition protocols

## Confidence

- **High Confidence**: Multi-agent decomposition mechanism is well-validated through systematic ablation studies showing 42.85% performance gap when removing collaboration
- **Medium Confidence**: Workspace template approach shows positive effects but lacks external validation of specific constraint level and architectural choices
- **Medium Confidence**: Iterative debugging mechanism demonstrates strong empirical support but relies on assumption that error messages are sufficiently informative for LLM-based diagnosis

## Next Checks

1. **Model Generalization Test**: Evaluate M3Builder with a broader range of LLMs (GPT-4o, Gemini-1.5-Pro) across multiple random seeds to assess robustness beyond Claude-3.7-Sonnet.

2. **Out-of-Distribution Task Evaluation**: Test the system on medical imaging tasks outside current template coverage (e.g., 3D reconstruction, multi-modal fusion) to quantify architectural constraints' impact on generalizability.

3. **Human-in-the-Loop Cost Analysis**: Measure time and expertise required for users to create datacards for new datasets and modify templates for custom requirements, assessing practical adoption barrier.