---
ver: rpa2
title: 'AdaGrad Meets Muon: Adaptive Stepsizes for Orthogonal Updates'
arxiv_id: '2509.02981'
source_url: https://arxiv.org/abs/2509.02981
tags:
- muon
- adago
- arxiv
- convergence
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes AdaGO, a new optimizer that combines norm-based
  AdaGrad-type stepsizes with orthogonalized updates. AdaGO brings together the benefits
  of Muon's orthogonalized momentum and AdaGrad's adaptive stepsizes.
---

# AdaGrad Meets Muon: Adaptive Stepsizes for Orthogonal Updates

## Quick Facts
- arXiv ID: 2509.02981
- Source URL: https://arxiv.org/abs/2509.02981
- Authors: Minxin Zhang; Yuxuan Liu; Hayden Schaeffer
- Reference count: 40
- One-line primary result: AdaGO combines AdaGrad-type adaptive stepsizes with Muon's orthogonalized momentum, achieving better empirical performance on CIFAR-10 and regression tasks while preserving orthogonality and establishing optimal convergence rates.

## Executive Summary
This paper proposes AdaGO, an optimizer that combines AdaGrad-type adaptive stepsizes with Muon's orthogonalized momentum updates for matrix parameters. AdaGO preserves the orthogonality of update directions while adapting stepsizes based on accumulated gradient norms, achieving both spectral descent properties and adaptive scaling. The authors establish optimal convergence rates for nonconvex functions in both stochastic and deterministic settings, and demonstrate empirical improvements over Muon and Adam on CIFAR-10 classification and function regression tasks.

## Method Summary
AdaGO modifies the Muon optimizer by adding adaptive stepsize scaling based on accumulated squared gradient norms. At each iteration, it computes a stochastic gradient, updates momentum, accumulates clamped squared norms in a scalar accumulator, orthogonalizes the momentum via SVD or Newton-Schulz iterations, and scales the orthogonal direction by an adaptive stepsize. The stepsize is computed as α_t = max{ε, η·min{||G_t||,γ}/v_t}, where v_t is the accumulated norm. This design preserves orthogonality while adapting to the optimization landscape, requiring only minimal modification to Muon with a single additional scalar variable.

## Key Results
- AdaGO achieves optimal convergence rates O(T^{-1/4}) for nonconvex functions in both stochastic and deterministic settings
- Empirically outperforms Muon and Adam on CIFAR-10 classification and function regression tasks
- Preserves orthogonality of update directions while providing adaptive scaling, unlike other adaptive Muon variants
- Demonstrates null gradient consistency - automatic stepsize decay at convergence without manual scheduling

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Orthogonalized update directions preserve spectral descent properties while enabling adaptive scaling without directional corruption.
- **Mechanism:** At each iteration, the momentum-accumulated gradient M_t is orthogonalized via O_t = Orth(M_t) = UV^T (from reduced SVD M_t = UΣV^T). This orthogonal direction is then scaled by the adaptive stepsize α_t. Crucially, because O_t has unit norm by construction, the stepsize controls magnitude without altering direction—unlike diagonal AdaGrad variants that distort the gradient direction.
- **Core assumption:** The loss function L(Θ) is L-smooth under the nuclear norm (Assumption 3.1), which is equivalent to standard Frobenius-norm smoothness but matches the spectral geometry of orthogonalized updates.
- **Evidence anchors:**
  - [abstract]: "Unlike other adaptive variants of Muon, AdaGO preserves the orthogonality of the update direction, which can be interpreted as a spectral descent direction"
  - [Section 2]: "since the orthogonalized momentum has unit magnitude, we scale O_t by the clamped current gradient norm... ensuring that the per-iteration update decays to zero as AdaGO converges"
  - [corpus]: Related work (AdaMuon, FISMO) also combines adaptivity with orthogonalization but uses element-wise methods that alter directions; corpus lacks direct comparison of directional preservation.
- **Break condition:** If the gradient matrix becomes severely ill-conditioned, orthogonalization amplifies noise (Section 1 notes "distance between orthogonalized counterparts can be much larger when gradient matrices are ill-conditioned"), potentially destabilizing the accumulator.

### Mechanism 2
- **Claim:** Clamped gradient-norm accumulation provides noise-robust adaptive scaling that does not require bounded gradient assumptions.
- **Mechanism:** The accumulator v_t² = Σ_{τ=0}^t min{||G_τ||², γ²} clips individual gradient norms at γ before accumulation. This prevents unbounded gradients from dominating the sum while still tracking overall gradient magnitude history. The stepsize α_t = max{ε, η·min{||G_t||, γ}/v_t} then scales inversely with accumulated norm, yielding larger steps early (when gradients are large) and smaller steps near convergence.
- **Core assumption:** Stochastic gradients are unbiased with bounded variance E[||G_t - ∇L(Θ_{t-1})||²_F] ≤ κ²/b_t (Assumption 3.2); no uniform gradient norm bound required.
- **Evidence anchors:**
  - [abstract]: "adapting the stepsizes to the optimization landscape by scaling the direction with accumulated past gradient norms"
  - [Section 2]: "we remove this assumption [of uniformly bounded gradient norms] and instead accumulate the squared norms clamped by a large constant γ"
  - [corpus]: No direct corpus evidence for clamped accumulation in orthogonalized contexts; related adaptive Muon variants (AdaMuon, Variance-Adaptive Muon) use different scaling mechanisms.
- **Break condition:** If γ is set too low relative to typical gradient norms, clipping introduces systematic underestimation; Theorem 3.3 shows γ appears only in logarithmic terms, suggesting robustness but not insensitivity.

### Mechanism 3
- **Claim:** Null gradient consistency ensures automatic stepsize decay at convergence without manual scheduling.
- **Mechanism:** Because α_t ∝ min{||G_t||, γ} and v_t is monotonically increasing, the numerator shrinks as ||G_t|| → 0 near stationary points while the denominator grows, driving α_t → 0 naturally. The lower bound ε prevents complete stalling but is set to decay with T (ε = T^{-3/4} in Theorem 3.3).
- **Core assumption:** Convergence to a stationary point where gradients vanish; the optimization horizon T is known or estimated for ε selection.
- **Evidence anchors:**
  - [Section 2]: "This ensures that the per-iteration update decays to zero as AdaGO converges to a stationary point—a property known as null gradient consistency"
  - [Section 3]: Theorems 3.3–3.5 all establish convergence to stationary points measured by E[||∇L(Θ_{t-1})||_*]
  - [corpus]: Corpus papers on Muon variants do not explicitly discuss null gradient consistency; mechanism is specific to this work's design.
- **Break condition:** If ε is set too large relative to η², the lower bound dominates and prevents sufficient decay, degrading final convergence (Section 4.2 notes "effective ε satisfies ε < η²").

## Foundational Learning

- **Concept: Singular Value Decomposition (SVD) and matrix orthogonalization**
  - **Why needed here:** AdaGO's core operation is Orth(M) = UV^T from M = UΣV^T; understanding why this yields the nearest orthogonal matrix and how Newton–Schulz iterations approximate it efficiently is essential for implementation.
  - **Quick check question:** Given a 3×2 gradient matrix G with SVD G = UΣV^T where Σ = diag([3, 0.5]), what is Orth(G) and why does it discard singular values?

- **Concept: AdaGrad-Norm and adaptive stepsize theory**
  - **Why needed here:** AdaGO inherits the accumulator structure v_t² = Σ||G_τ||² from AdaGrad-Norm; understanding why this provides O(T^{-1/4}) rates and how it adapts to noise levels informs hyperparameter intuition.
  - **Quick check question:** In standard AdaGrad-Norm, if gradients have constant norm ||G_t|| = 1 for all t, what is the effective stepsize at iteration T and how does this compare to constant learning rate SGD?

- **Concept: Muon optimizer and spectral descent interpretation**
  - **Why needed here:** AdaGO modifies Muon; understanding Muon's momentum, orthogonalization, and interpretation as steepest descent under spectral norm clarifies what is preserved and what is changed.
  - **Quick check question:** Why does orthogonalizing the gradient correspond to steepest descent under the spectral norm rather than Frobenius norm, and what implicit bias might this introduce for matrix-valued parameters?

## Architecture Onboarding

- **Component map:**
  Input: Θ_{t-1}, M_{t-1}, v_{t-1}, minibatch
     │
     ├─► Stochastic gradient G_t = ∇L_t(Θ_{t-1})
     │
     ├─► Momentum update: M_t ← μM_{t-1} + (1-μ)G_t
     │
     ├─► Accumulator update: v_t² ← v_{t-1}² + min{||G_t||², γ²}
     │
     ├─► Orthogonalization: O_t ← Orth(M_t) [Newton–Schulz in practice]
     │
     ├─► Stepsize: α_t ← max{ε, η·min{||G_t||, γ}/v_t}
     │
     └─► Parameter update: Θ_t ← Θ_{t-1} - η·α_t·O_t

- **Critical path:** The orthogonalization step (O_t ← Orth(M_t)) dominates compute. In practice, this uses Newton–Schulz iterations rather than full SVD. The accumulator update (v_t² ← ...) is O(1) scalar arithmetic and is not on the critical path.

- **Design tradeoffs:**
  - **Exact vs. approximate orthogonalization:** Theory assumes exact SVD; practice uses Newton–Schulz (typically 3–5 iterations). Faster but introduces approximation error. Paper does not analyze inexact orthogonalization effects.
  - **Clipping threshold γ:** Large γ preserves gradient information but risks accumulator explosion on unbounded gradients; small γ stabilizes but may clip informative signal. Empirically robust across "wide range" but no systematic ablation provided.
  - **Lower bound ε:** Must decay with T for theory (ε = T^{-3/4}); in practice requires horizon estimate. Section 4.2 suggests ε < η² as practical guideline.

- **Failure signatures:**
  - **Oscillating loss with large initial drop then plateau:** Learning rate η too high; orthogonalized updates with excessive magnitude cause overshooting (Appendix A, Figure 3 demonstrates this for OGD).
  - **Slow convergence, higher final loss:** η too low or ε too large (lower bound dominates, preventing decay).
  - **Spiky loss curves:** Aggressive stepsizes; AdaGO exhibits this but recovers (Figure 1a). Not necessarily failure if trend is downward.
  - **Memory errors on large matrices:** Newton–Schulz iteration count too high or intermediate matrix allocations not reused.

- **First 3 experiments:**
  1. **Learning rate sweep with fixed ε, γ:** On a small MLP (2-layer, ~10K parameters), sweep η ∈ {0.01, 0.05, 0.1, 0.5, 1.0} with ε = 0.001, γ = 10.0. Plot training loss curves to identify the "oscillation zone" (too high η) vs. "slow zone" (too low η). Compare against Muon with same η values to validate adaptive benefit.
  2. **ε ablation with fixed η:** Set η to best value from (1), sweep ε ∈ {10^{-6}, 10^{-4}, 10^{-3}, 10^{-2}, η²}. Monitor final training loss and convergence speed. Verify ε < η² guideline and observe degradation when ε approaches η².
  3. **Comparison on ill-conditioned problem:** Construct a regression task with deliberately ill-conditioned design matrix (singular value ratio >100). Compare AdaGO vs. Muon vs. Adam. Hypothesis: AdaGO's adaptive scaling should handle the varying gradient magnitudes better than fixed-η Muon, but orthogonalization may amplify noise—observe which effect dominates.

## Open Questions the Paper Calls Out

- **Question:** Does AdaGO maintain its performance advantages over Muon and Adam when training large language models?
  - **Basis in paper:** [explicit] "Future work includes testing AdaGO on LLM training..."
  - **Why unresolved:** All experiments are limited to CIFAR-10 classification and small-scale function regression; no LLM benchmarks were conducted.
  - **What evidence would resolve it:** Empirical comparisons of AdaGO versus Muon and Adam on standard LLM pretraining tasks (e.g., language modeling perplexity, downstream task performance).

- **Question:** Can convergence guarantees for AdaGO be established under weaker smoothness or noise assumptions than Lipschitz continuity and bounded variance?
  - **Basis in paper:** [explicit] "Future work includes...analyzing the algorithm under relaxed assumptions..."
  - **Why unresolved:** The current analysis relies on Assumptions 3.1–3.2 (Lipschitz gradients and bounded stochastic gradient variance), which may not hold in all practical deep learning settings.
  - **What evidence would resolve it:** Theoretical proofs of convergence under relaxed conditions (e.g., generalized smoothness, heavy-tailed noise).

- **Question:** How does using approximate orthogonalization via Newton–Schulz iterations affect AdaGO's theoretical convergence guarantees?
  - **Basis in paper:** [inferred] The theoretical analysis assumes exact orthogonalization at each iteration, while Muon employs Newton–Schulz iterations for computational efficiency in practice.
  - **Why unresolved:** The gap between exact orthogonalization (theory) and approximate orthogonalization (practice) is not analyzed.
  - **What evidence would resolve it:** Convergence bounds that explicitly account for approximation error from Newton–Schulz iterations.

- **Question:** Does AdaGO effectively mitigate noise amplification in orthogonalized updates when gradient matrices are ill-conditioned?
  - **Basis in paper:** [inferred] The paper notes that "orthogonalization may amplify the effect of noise in stochastic gradients" when gradient matrices are ill-conditioned, but does not provide theoretical or empirical analysis specifically addressing this.
  - **What evidence would resolve it:** Controlled experiments varying gradient matrix condition numbers, or theoretical bounds relating condition number to convergence behavior.

## Limitations

- Convergence guarantees rely on standard Lipschitz smoothness and bounded variance assumptions that may not hold in all practical deep learning settings
- The analysis assumes exact orthogonalization while practice uses approximate Newton–Schulz iterations, creating a gap not analyzed in the paper
- Empirical evaluation is limited to small-scale CIFAR-10 classification and function regression tasks; performance on larger models or language models remains untested

## Confidence

- **High confidence** in the orthogonality preservation mechanism and its spectral descent interpretation, directly grounded in SVD theory and Muon's established framework
- **Medium confidence** in clamped accumulation providing noise robustness; theory supports stability but empirical clipping effects on signal fidelity are not systematically studied
- **Medium confidence** in empirical superiority; CIFAR-10 and regression results are promising but limited in scope and lack extensive hyperparameter sweeps or larger-scale tests

## Next Checks

1. Conduct systematic ablation on γ (clipping threshold) and Newton–Schulz iteration count to quantify trade-offs between numerical stability and approximation fidelity in orthogonalization
2. Test AdaGO on larger-scale tasks (e.g., ImageNet classification or transformer pretraining) to verify whether empirical gains scale beyond the small CNN and regression benchmarks
3. Compare AdaGO's gradient norm trajectories and spectral properties against Muon and Adam to directly validate the claimed benefits of adaptive scaling with orthogonalized directions