---
ver: rpa2
title: 'ReferGPT: Towards Zero-Shot Referring Multi-Object Tracking'
arxiv_id: '2504.09195'
source_url: https://arxiv.org/abs/2504.09195
tags:
- object
- tracking
- referring
- multi-object
- matching
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents ReferGPT, a zero-shot referring multi-object
  tracking framework for autonomous driving that eliminates the need for supervised
  training. The key innovation lies in combining a tracking-by-detection approach
  with a multi-modal large language model (MLLM) that generates spatially-aware object
  captions using 3D motion and position information.
---

# ReferGPT: Towards Zero-Shot Referring Multi-Object Tracking

## Quick Facts
- **arXiv ID:** 2504.09195
- **Source URL:** https://arxiv.org/abs/2504.09195
- **Reference count:** 40
- **Primary result:** Zero-shot referring multi-object tracking framework achieving 49.46% HOTA on Refer-KITTI using 3D LiDAR input

## Executive Summary
This paper introduces ReferGPT, a zero-shot referring multi-object tracking framework that eliminates the need for supervised training in autonomous driving scenarios. The method combines a tracking-by-detection pipeline with a multi-modal large language model (MLLM) that generates spatially-aware captions using 3D motion and position information. By employing a hybrid matching strategy that combines CLIP-based semantic encoding with fuzzy character-level matching, the framework achieves competitive performance against trained methods while maintaining strong generalization across diverse and open-set queries.

## Method Summary
ReferGPT follows a tracking-by-detection paradigm using CasA for 3D object detection and PC3T for Kalman filter-based tracking. For each tracked object, the system crops its 2D bounding box and concatenates it with flattened spatial context (position, heading, distance, motion variations over 5 frames) as input to GPT-4o-mini. The MLLM generates natural language descriptions incorporating this spatial knowledge. A hybrid matching module computes similarity scores using both CLIP text encoder embeddings and Ratcliff/Obershelp fuzzy matching, which are combined additively. Final track selection uses majority voting followed by agglomerative hierarchical clustering to dynamically identify high-similarity clusters without requiring fixed thresholds.

## Key Results
- Achieves 49.46% HOTA on Refer-KITTI using 3D LiDAR input, outperforming trained methods
- Hybrid matching (CLIP + fuzzy) improves HOTA from 47.48% to 49.46% compared to fuzzy alone
- 3D LiDAR input yields slightly better performance (49.46% HOTA) than 2D image input (46.36% HOTA)
- Strong generalization across diverse queries while maintaining high association accuracy

## Why This Works (Mechanism)

### Mechanism 1: Spatial Context Injection via Prompt Engineering
- **Claim:** Injecting 3D motion and position information into the MLLM prompt enables generation of spatially-grounded captions that support depth-dependent queries.
- **Mechanism:** The tracker outputs ego-centric coordinates (position, heading, distance traveled, motion variations) which are flattened into text and provided to the MLLM alongside cropped object images. This grounds the language model's descriptions in verifiable spatial facts.
- **Core assumption:** The MLLM can accurately interpret numeric spatial data in textual form and incorporate it into natural language descriptions without hallucination.
- **Evidence anchors:**
  - [Section 3.3]: "Ci_t consists of the current position p = (x, y, z) at t0... This compact representation is then flattened into a 1-D text sequence and provided as input to the MLLM."
  - [Abstract]: "We provide a multi-modal large language model (MLLM) with spatial knowledge enabling it to generate 3D-aware captions."
  - [Corpus]: Weak direct evidence; Spatial-LLaVA (arXiv:2505.12194) explores similar spatial grounding but in a different task context.

### Mechanism 2: Hybrid Semantic-Lexical Matching
- **Claim:** Combining CLIP semantic embeddings with fuzzy character-level matching improves query-caption alignment across both synonyms and exact lexical overlaps.
- **Mechanism:** CLIP text encoder captures semantic similarity (e.g., "vehicle" ≈ "automobile"), while Ratcliff/Obershelp fuzzy matching captures structural similarity via longest common subsequences. Final score ST = SC + SF.
- **Core assumption:** The two matching strategies are complementary and additive; neither alone suffices for open-vocabulary queries.
- **Evidence anchors:**
  - [Section 3.4]: "The best performance is achieved by combining CLIP Text with fuzzy matching... achieving 49.46% HOTA."
  - [Table 5]: CLIP alone achieves 30.04% HOTA; Fuzzy alone achieves 47.48%; Combined achieves 49.46%.
  - [Corpus]: No direct corpus validation of this specific hybrid approach.

### Mechanism 3: Adaptive Trajectory Selection via Hierarchical Clustering
- **Claim:** Agglomerative hierarchical clustering dynamically identifies relevant tracks without requiring a fixed similarity threshold, adapting to query-specific score distributions.
- **Mechanism:** After majority voting filters frame-level matches, clustering groups trajectories by similarity scores, selecting the high-score cluster as final output.
- **Core assumption:** True matches cluster together in similarity space, separable from non-matches.
- **Evidence anchors:**
  - [Section 3.5]: "We employ an agglomerative hierarchical clustering algorithm to dynamically identify the subset of tracks having the highest similarity scores."
  - [Table 6]: Fixed threshold achieves 36.61% HOTA; Clustering alone achieves 46.50%; Majority voting + clustering achieves 49.46%.

## Foundational Learning

- **Tracking-by-Detection Paradigm:**
  - **Why needed here:** ReferGPT decouples detection (CasA) from tracking (PC3T/Kalman filter), requiring understanding of how detections propagate into trajectories across frames.
  - **Quick check question:** Can you explain why TBD methods separate detection and association optimization rather than jointly learning both?

- **CLIP Vision-Language Alignment:**
  - **Why needed here:** The matching module relies on CLIP's pre-trained text encoder to project captions and queries into a shared embedding space for cosine similarity computation.
  - **Quick check question:** Why does CLIP's contrastive pretraining enable zero-shot semantic matching without task-specific fine-tuning?

- **Kalman Filter State Estimation:**
  - **Why needed here:** The tracker predicts future object states (position, velocity) to associate detections across occlusions and maintain trajectory continuity.
  - **Quick check question:** How does a Kalman filter handle measurement noise versus process noise in trajectory prediction?

## Architecture Onboarding

- **Component map:** LiDAR (3D) or Image (2D) → CasA/PV-RCNN/Second-IOU detector → 3D bounding boxes → PC3T (Kalman filter + confidence-guided association) → Trajectories with ego-centric coordinates → GPT-4o-mini (cropped image + flattened spatial context) → Natural language description → CLIP ViT-L/14 text encoder + Ratcliff/Obershelp fuzzy matching → Similarity score → Majority voting per trajectory → Agglomerative hierarchical clustering → Final matched tracks

- **Critical path:** Detector quality → Tracking stability → Caption accuracy (depends on spatial context encoding) → Matching score separation → Clustering boundary. Detector errors propagate; weak caption spatial grounding reduces matching discriminability.

- **Design tradeoffs:**
  - **Latency vs. accuracy:** GPT-4o-mini captioning adds ~100-300ms per object per frame (inference cost not quantified in paper). Smaller MLLMs (Qwen2-VL 2B) reduce latency but drop HOTA from 49.46% to 19.97%.
  - **Fixed threshold vs. clustering:** Fixed threshold enables online operation (36.61% HOTA); clustering requires batch processing but improves to 49.46%.
  - **2D vs. 3D input:** 3D LiDAR achieves 49.46% HOTA vs. 2D image at 46.36%—modest gap suggests referring module compensates for some detection quality differences.

- **Failure signatures:**
  - **MLLM hallucination:** Incorrect color/gender attributes in captions (observed with Qwen2-VL) → false negatives in matching.
  - **Query abstraction mismatch:** If user query refers to attributes absent from prompt-guided caption (e.g., "the car with a dent"), matching fails.
  - **Sparse matches:** Uniform similarity score distribution → clustering cannot separate true from false positives.

- **First 3 experiments:**
  1. **Reproduce baseline:** Run CLIP-image-only matching (no MLLM) on Refer-KITTI to verify 21.42% HOTA baseline from Table 1.
  2. **Ablate matching components:** Disable fuzzy matching, then disable CLIP, comparing against Table 5 to validate contribution of each component.
  3. **Swap MLLM backbone:** Replace GPT-4o-mini with Qwen2-VL-Instruct and measure HOTA drop; inspect generated captions for spatial reasoning failures (color errors, gender confusion as noted in Section 5.2).

## Open Questions the Paper Calls Out
None

## Limitations
- MLLM hallucination can produce incorrect attributes (color, gender) in generated captions, leading to false negatives in matching
- Performance drops significantly with smaller MLLMs (HOTA from 49.46% to 19.97% when using Qwen2-VL 2B)
- Fixed threshold matching enables online operation but achieves lower HOTA (36.61%) compared to batch clustering approach (49.46%)

## Confidence
- **Tracking pipeline integration (CasA + PC3T):** High - Well-established components with clear implementation details
- **MLLM spatial context encoding:** Medium - Prompt format not fully specified, though spatial feature integration is clear
- **Hybrid matching strategy:** Medium - Effectiveness demonstrated empirically but theoretical justification limited
- **Clustering-based filtering:** Medium - Performance gains shown but clustering parameters not fully specified

## Next Checks
1. Verify the exact MLLM prompt template for spatial context encoding by testing with different spatial feature formats
2. Reproduce the 21.42% HOTA baseline using CLIP-image-only matching to validate the reference point
3. Test the impact of replacing GPT-4o-mini with Qwen2-VL on caption quality and downstream matching performance, specifically checking for hallucination errors in generated captions