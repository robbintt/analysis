---
ver: rpa2
title: 'Fine-Tuning LLMs on Small Medical Datasets: Text Classification and Normalization
  Effectiveness on Cardiology reports and Discharge records'
arxiv_id: '2503.21349'
source_url: https://arxiv.org/abs/2503.21349
tags:
- medical
- data
- training
- text
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the effectiveness of fine-tuning large
  language models (LLMs) on small medical datasets for text classification and named
  entity recognition (NER) tasks. Using German cardiology reports and the i2b2 Smoking
  Challenge dataset, the research demonstrates that fine-tuning small LLMs locally
  with limited training data (200-300 examples) can achieve performance comparable
  to larger models.
---

# Fine-Tuning LLMs on Small Medical Datasets: Text Classification and Normalization Effectiveness on Cardiology reports and Discharge records
## Quick Facts
- arXiv ID: 2503.21349
- Source URL: https://arxiv.org/abs/2503.21349
- Reference count: 14
- Key outcome: Fine-tuning small LLMs on limited medical datasets achieves performance comparable to larger models for text classification and NER tasks

## Executive Summary
This study investigates the effectiveness of fine-tuning large language models on small medical datasets for text classification and named entity recognition tasks. Using German cardiology reports and the i2b2 Smoking Challenge dataset, the research demonstrates that fine-tuning small LLMs locally with limited training data (200-300 examples) can achieve performance comparable to larger models. The study shows notable improvements in both tasks, with fine-tuned models outperforming zero-shot approaches and reaching parity with larger open-source models. The fine-tuning process improved models' adherence to machine-readable output formats and reduced parsing errors.

## Method Summary
The study fine-tuned small LLMs using German cardiology reports and the i2b2 Smoking Challenge dataset for text classification and named entity recognition tasks. Models were trained locally with 200-300 examples per task. The research compared fine-tuned models against zero-shot approaches and other open-source models, evaluating performance on format adherence and parsing error reduction. The methodology focused on practical implementation considerations for medical text processing.

## Key Results
- Fine-tuned small LLMs achieved performance comparable to larger models on medical text classification and NER tasks
- Models showed improved adherence to machine-readable output formats and reduced parsing errors
- Fine-tuned models outperformed zero-shot approaches across both evaluation datasets

## Why This Works (Mechanism)
Fine-tuning LLMs on domain-specific medical data allows the models to learn specialized vocabulary, context, and formatting requirements unique to clinical documentation. The limited but focused training examples help the models adapt to the specific patterns and structures found in medical text, improving their ability to correctly identify and normalize medical entities. This targeted adaptation appears particularly effective because medical text contains domain-specific terminology and formatting conventions that differ significantly from general text corpora.

## Foundational Learning
- **Medical entity recognition**: Essential for identifying clinical concepts like diagnoses, medications, and procedures in unstructured text. Quick check: Can the model accurately extract all relevant medical entities from sample clinical notes?
- **Text classification in medical contexts**: Critical for categorizing clinical documents by type, urgency, or diagnostic relevance. Quick check: Does the model correctly classify different types of medical reports?
- **Named entity normalization**: Necessary for converting clinical entities into standardized medical codes or terminologies. Quick check: Are extracted entities mapped correctly to standardized medical vocabularies?
- **Format adherence in clinical outputs**: Important for ensuring machine-readable results that integrate with existing healthcare IT systems. Quick check: Do outputs follow required JSON or other structured formats without errors?
- **Zero-shot vs. fine-tuned performance**: Understanding when pre-trained knowledge suffices versus when task-specific adaptation is needed. Quick check: Compare zero-shot and fine-tuned performance on held-out medical text samples.

## Architecture Onboarding
**Component map**: Input text -> Tokenizer -> LLM backbone -> Classification/NER head -> Post-processing -> Structured output

**Critical path**: The fine-tuning pipeline where domain-specific medical data flows through the tokenizer, into the LLM layers for adaptation, and through task-specific heads for classification or NER output.

**Design tradeoffs**: Small LLMs offer faster training and lower computational costs but may lack the broad knowledge base of larger models. Fine-tuning on limited data balances customization with resource efficiency but may miss rare edge cases.

**Failure signatures**: Zero-shot approaches show poor format adherence and higher parsing errors. Untrained models struggle with medical terminology and produce inconsistent outputs. Models may overfit to limited training examples, reducing generalization.

**Three first experiments**:
1. Compare zero-shot, few-shot, and fine-tuned performance on a held-out validation set from the same medical domain
2. Test format adherence by measuring structured output parsing success rate
3. Evaluate model robustness by introducing adversarial medical text variations and measuring performance degradation

## Open Questions the Paper Calls Out
None

## Limitations
- Study focuses exclusively on German cardiology reports and one English dataset, limiting generalizability across medical specialties and languages
- Small sample sizes (200-300 examples) may not capture full variability and complexity of real-world clinical documentation
- Comparison primarily benchmarks against zero-shot approaches and other open-source models rather than commercial medical-grade LLMs

## Confidence
- High confidence: Fine-tuning improves format adherence and reduces parsing errors
- Medium confidence: Claims about performance parity with larger models may vary with different datasets or evaluation metrics
- Medium confidence: Practical implications for clinical workflow automation require real-world implementation studies

## Next Checks
1. Test the fine-tuning approach across multiple medical specialties (radiology, pathology, oncology) and languages to assess generalizability
2. Conduct a head-to-head comparison with commercial medical LLMs using identical datasets and evaluation metrics
3. Perform error analysis on edge cases and rare clinical scenarios to identify potential failure modes and safety concerns in clinical deployment