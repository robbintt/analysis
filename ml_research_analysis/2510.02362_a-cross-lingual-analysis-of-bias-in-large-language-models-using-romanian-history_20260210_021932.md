---
ver: rpa2
title: A Cross-Lingual Analysis of Bias in Large Language Models Using Romanian History
arxiv_id: '2510.02362'
source_url: https://arxiv.org/abs/2510.02362
tags:
- language
- romanian
- large
- across
- bias
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates bias in Large Language Models (LLMs) by
  examining their responses to controversial Romanian historical questions across
  four languages. A cross-lingual approach was employed, presenting 14 contentious
  historical statements to 13 different LLMs in Romanian, English, Hungarian, and
  Russian.
---

# A Cross-Lingual Analysis of Bias in Large Language Models Using Romanian History

## Quick Facts
- arXiv ID: 2510.02362
- Source URL: https://arxiv.org/abs/2510.02362
- Reference count: 4
- Primary result: LLMs show significant inconsistency in historical responses across languages, encoding culturally specific biases from training data

## Executive Summary
This study investigates bias in Large Language Models (LLMs) by examining their responses to controversial Romanian historical questions across four languages. A cross-lingual approach was employed, presenting 14 contentious historical statements to 13 different LLMs in Romanian, English, Hungarian, and Russian. The methodology involved a three-stage questioning process: a binary yes/no answer, a numeric rating on a 1-10 scale, and a detailed essay response. An LLM-as-a-judge evaluated the essays. Results show significant inconsistency in model responses, with binary stability averaging 75-81% across languages and numeric ratings frequently diverging from initial binary choices. Russian and Hungarian languages exhibited notable deviations from consensus on specific statements, reflecting culturally specific training data biases.

## Method Summary
The research team constructed 14 controversial statements about Romanian history and presented them to 13 different LLMs in four languages: Romanian, English, Hungarian, and Russian. Each model was asked the same questions in each language using a three-stage process: first a binary yes/no answer, then a numeric rating from 1-10, and finally a detailed essay response. An LLM-as-a-judge was used to evaluate the quality and consistency of the essay responses. Temperature parameters were varied to test for consistency across different random seeds. The study measured binary stability (consistency of yes/no answers) and compared numeric ratings against initial binary choices to assess internal coherence.

## Key Results
- Binary stability averaged 75-81% across languages, indicating significant inconsistency in LLM historical responses
- Numeric ratings frequently contradicted initial binary choices, revealing internal incoherence in model reasoning
- Russian and Hungarian languages showed notable deviations from consensus on specific statements, reflecting culturally specific training data biases

## Why This Works (Mechanism)
The study works by exploiting the fact that LLMs encode historical narratives from their training data, which varies by language and cultural context. By asking the same historical questions across multiple languages, the research reveals how training data composition influences model outputs. The three-stage questioning process (binary, numeric, essay) exposes inconsistencies in how models process and articulate historical claims. Using an LLM-as-a-judge provides a scalable evaluation method, though it introduces potential circularity. The cross-lingual design allows isolation of language-specific effects on historical interpretation, revealing that LLMs are not neutral arbiters but rather reflect the biases present in their multilingual training corpora.

## Foundational Learning
- Cross-lingual analysis methodology: Why needed - to reveal language-specific training data biases; Quick check - compare model responses across languages for same content
- LLM-as-a-judge evaluation: Why needed - scalable assessment of essay quality; Quick check - validate judge consistency by re-running evaluations
- Temperature parameter tuning: Why needed - to assess consistency across random seeds; Quick check - measure response stability at different temperature settings
- Binary stability metrics: Why needed - quantitative measure of response consistency; Quick check - calculate stability rates across different model families
- Cultural historiography: Why needed - understanding how historical narratives vary by culture; Quick check - map controversial statements to known cultural perspectives

## Architecture Onboarding

**Component Map:** Historical statements -> LLMs (13 models) -> Three-stage responses -> LLM judge evaluation -> Stability metrics

**Critical Path:** Statement generation → Multi-language prompting → Response collection → Essay evaluation → Stability analysis

**Design Tradeoffs:** LLM judge evaluation provides scalability but introduces potential bias circularity; binary stability is simple but may oversimplify complex historical interpretations

**Failure Signatures:** Inconsistent binary answers across languages indicate training data bias; numeric-rating contradictions suggest internal model incoherence; LLM judge evaluations may reflect same biases being studied

**First Experiments:**
1. Test consistency by repeating same prompts with varied temperature settings
2. Compare LLM judge evaluations against human expert assessments
3. Analyze training data composition differences between languages for identified bias patterns

## Open Questions the Paper Calls Out
None

## Limitations
- Narrow geographic scope focused exclusively on Romanian history limits generalizability
- Small sample of 13 LLMs and 14 statements may not capture full spectrum of model behaviors
- LLM-as-a-judge evaluation introduces potential circularity and self-referential bias
- Does not fully disentangle influence of language, training data, and prompt framing
- Binary stability metric may oversimplify nuanced spectrum of historical interpretation

## Confidence
High: LLMs are unstable historical arbiters - directly measured response variability and documented clear inconsistencies across multiple languages and models
Medium: Russian and Hungarian languages produce culturally biased responses - deviations observed but causal mechanisms not fully established
Medium: LLMs encode dominant historiographical biases - evidence suggestive but evaluation relies on LLM judge without external historical consensus comparison

## Next Checks
1. Expand study to include historical statements and languages from other cultural contexts (African, Asian, Latin American histories)
2. Employ human experts as judges for essay evaluations to eliminate LLM-as-a-judge circularity
3. Experiment with alternative prompting strategies (chain-of-thought, role-playing, structured templates) to assess consistency improvements