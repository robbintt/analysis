---
ver: rpa2
title: How important is Recall for Measuring Retrieval Quality?
arxiv_id: '2512.20854'
source_url: https://arxiv.org/abs/2512.20854
tags:
- response
- figure
- arxiv
- score
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates recall-independent measures for evaluating
  retrieval quality in RAG systems, where the total number of relevant documents is
  unknown. The authors propose a simple retrieval quality measure "T" that does not
  require knowledge of total relevant documents.
---

# How important is Recall for Measuring Retrieval Quality?

## Quick Facts
- arXiv ID: 2512.20854
- Source URL: https://arxiv.org/abs/2512.20854
- Authors: Shelly Schwartz; Oleg Vasilyev; Randy Sawaya
- Reference count: 40
- Primary result: Proposed recall-independent measure "T" performs comparably to F-measure for RAG evaluation when total relevant documents is unknown

## Executive Summary
This paper addresses the challenge of evaluating retrieval quality in RAG systems where the total number of relevant documents is unknown. Traditional F-measure requires knowing all relevant documents, making it impractical for production systems. The authors introduce a simple measure "T" that balances precision against negative documents within the top-K selection without requiring knowledge of total relevant documents. Through extensive experiments across multiple datasets with low numbers of relevant documents (2-15), T shows comparable performance to F-measure while avoiding recall dependency. The study also reveals that for high K/Np ratios, nDCG becomes superior, and surprisingly, estimating Np from top-2K documents can outperform using true Np.

## Method Summary
The study evaluates retrieval quality measures on a combined "retrieval-response" dataset containing 535,888 graded samples from ARXIV, MSMARCO, HotpotQA, and Natural Questions. Documents are embedded using various models (MiniLM, E5) and ranked by cosine similarity. The proposed measure T = ((1-α)np - αnn) / K balances positives against negatives within top-K selection. F-measure uses harmonic mean with α weighting, while nDCG provides position-aware evaluation. GPT-4o-mini generates ideal responses from all positives and grades comparisons on a 1-5 Likert scale. Measures are evaluated by optimizing α per segment to maximize Spearman correlation with LLM quality scores, comparing performance across different K/Np ratio regimes.

## Key Results
- T performs comparably to F-measure across datasets while avoiding recall dependency
- For K/Np < 1, T shows better correlation with LLM response quality than nDCG
- For K/Np > 1, nDCG becomes superior due to document ordering importance
- Estimating Np from top-2K (Fe) can outperform true Np at high K/Np ratios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: T provides recall-independent proxy when K/Np < 1
- Mechanism: T = ((1-α)np - αnn) / K balances positives against negatives within top-K, removing unknown Np while preserving precision-negatives tradeoff via α
- Core assumption: Relevant documents within top-K are primary drivers of LLM response quality
- Evidence: Abstract states T "performs well without requiring knowledge of total number of relevant documents"; equations 3-4 define T with explicit α tradeoff
- Break condition: If exhaustive coverage is required (legal discovery), T undervalues recall

### Mechanism 2
- Claim: nDCG outperforms T at high K/Np ratios due to document order
- Mechanism: nDCG's graded relevance discounting rewards placing relevant documents earlier, helping LLMs process cleaner signal before noise
- Core assumption: LLMs exhibit position bias - earlier documents disproportionately influence generation quality
- Evidence: Section 3.3 states "order of documents within top-K becomes too important... nDCG becomes better than F or T"; Figure 13 shows nDCG correlation exceeds F for ARXIV at K/Np ≥ 1.5
- Break condition: If LLMs are robust to ordering (attention mechanisms), nDCG's advantage disappears

### Mechanism 3
- Claim: Estimating Np from top-2K (Fe) outperforms true Np at high K/Np ratios
- Mechanism: Higher-ranked relevant documents are more semantically similar to queries and thus more useful to LLMs than lower-ranked ones
- Core assumption: Relevance is not binary - documents ranked higher by similarity provide more value
- Evidence: Section 3.3 notes "relevant documents falling into top 2K are more important"; Figures 16-17 show Fe positive correlation difference vs F at high K/Np
- Break condition: If embedding quality is poor, top-2K contains mostly noise, Fe becomes unreliable

## Foundational Learning

- **F-measure (harmonic mean of precision and recall)**
  - Why needed: Paper benchmarks T against F; understanding α-weighting (precision vs recall tradeoff) is essential
  - Quick check: If α=0.7, does F weight precision or recall more heavily?

- **nDCG (Normalized Discounted Cumulative Gain)**
  - Why needed: nDCG is recall-independent alternative; must understand position-based discounting to grasp why it wins at high K/Np
  - Quick check: Why does nDCG not require Np?

- **Spearman/Pearson correlation for metric validation**
  - Why needed: Paper judges metrics by correlation with LLM quality scores; understanding correlation interpretation is critical
  - Quick check: What does Spearman correlation of 0.4 between T and response score indicate about predictive power?

## Architecture Onboarding

- **Component map**: Embedding models (MiniLM, E5) → cosine similarity ranking → top-K selection → T/F/nDCG computation → correlate with LLM quality scores (1-5 Likert) → LLM receives top-K → produces response → graded against ideal response

- **Critical path**: 1) Choose K relative to expected Np (unknown in production) 2) Retrieve and rank documents 3) Apply T for K/Np < 1; switch to nDCG for K/Np > 1 4) Monitor correlation with downstream task quality (if labels available)

- **Design tradeoffs**:
  - T vs F: T requires no Np knowledge but assumes Np is less important than top-K precision; F is theoretically complete but infeasible without labeled corpora
  - T vs nDCG: T is simpler (no position discounting); nDCG captures ordering but adds complexity
  - Fe vs F: Fe is practical (uses retrievable subset) but may misestimate if retrieval quality is poor

- **Failure signatures**:
  - T underperforms when K >> Np (too many negatives dilute signal)
  - nDCG underperforms when K << Np (ordering irrelevant with few negatives)
  - Fe underperforms when embedding model has low discriminative power (top-2K is mostly noise)

- **First 3 experiments**:
  1. Implement T on your corpus: Compute T for sample queries with K=5, 10, 20; compare against human-judged relevance if available
  2. Calibrate α for your domain: Run grid search on α ∈ [0.3, 0.7] to maximize correlation with your downstream task metric
  3. Identify your K/Np regime: Estimate typical Np via pooling or LLM annotation; if K/Np > 1 is common, add nDCG to your evaluation pipeline

## Open Questions the Paper Calls Out
None

## Limitations
- Focus on low-Np settings (2-15 relevant documents) may not generalize to domains with richer relevance distributions
- Correlation-based validation relies on single LLM grader without human judgment validation
- Assumption that higher-ranked relevant documents are more useful remains empirically unverified
- Paper does not explore impact of varying document lengths or semantic similarity thresholds

## Confidence

- **High Confidence**: Mathematical formulation of T and advantage over F in Np-unknown settings; empirical observation that nDCG outperforms T at high K/Np ratios
- **Medium Confidence**: Claim that Fe can outperform F by upweighting higher-ranked relevant documents; generalizability across different embedding models and document types
- **Low Confidence**: Assumption that LLM position bias drives nDCG's superiority; assertion that top-2K relevant documents are inherently more useful than lower-ranked ones

## Next Checks

1. **Cross-domain validation**: Apply T, F, nDCG, and Fe to dataset with higher Np values (50-100) to test measure robustness beyond studied range

2. **Human judgment validation**: Compare LLM-graded correlations with human-annotated relevance assessments to verify T's correlation with LLM scores translates to human perception

3. **Ablation on document order**: Create controlled experiments where relevant documents are artificially reordered within top-K to isolate impact of position on LLM response quality, directly testing position bias assumption underlying nDCG's advantage