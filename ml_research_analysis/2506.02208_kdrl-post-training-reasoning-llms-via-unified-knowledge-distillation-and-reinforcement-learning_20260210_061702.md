---
ver: rpa2
title: 'KDRL: Post-Training Reasoning LLMs via Unified Knowledge Distillation and
  Reinforcement Learning'
arxiv_id: '2506.02208'
source_url: https://arxiv.org/abs/2506.02208
tags:
- training
- kdrl
- teacher
- grpo
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work introduces KDRL, a unified post-training framework that\
  \ combines knowledge distillation (KD) and reinforcement learning (RL) to enhance\
  \ reasoning capabilities in large language models (LLMs). KDRL leverages policy\
  \ gradient optimization to jointly minimize the reverse Kullback\u2013Leibian divergence\
  \ (RKL) between student and teacher models while maximizing expected rule-based\
  \ rewards."
---

# KDRL: Post-Training Reasoning LLMs via Unified Knowledge Distillation and Reinforcement Learning

## Quick Facts
- arXiv ID: 2506.02208
- Source URL: https://arxiv.org/abs/2506.02208
- Reference count: 40
- Primary result: KDRL improves mathematical reasoning accuracy by up to 2.6% over GRPO alone while reducing reasoning token usage

## Executive Summary
This work introduces KDRL, a unified post-training framework that combines knowledge distillation (KD) and reinforcement learning (RL) to enhance reasoning capabilities in large language models (LLMs). KDRL leverages policy gradient optimization to jointly minimize the reverse Kullback–Leibian divergence (RKL) between student and teacher models while maximizing expected rule-based rewards. Through systematic exploration of KL approximations, coefficient scheduling, and reward-guided masking strategies, KDRL achieves superior performance and efficiency. Empirical results on mathematical reasoning benchmarks show that KDRL outperforms both standalone GRPO and KD baselines, improving accuracy by up to 2.6% and achieving better reasoning token efficiency. The framework also demonstrates effectiveness in R1-Zero-like training scenarios. These findings indicate that integrating KD and RL offers a promising, resource-efficient approach to scaling reasoning LLMs post-training.

## Method Summary
KDRL implements a unified post-training framework that integrates knowledge distillation and reinforcement learning through policy gradient optimization. The method uses the unbiased k2 KL estimator to minimize RKL divergence between student and teacher models while maximizing rule-based rewards via GRPO. Key innovations include KL coefficient annealing to transition from imitation to exploration, response-level reward-guided masking to focus teacher supervision on failure modes, and systematic exploration of different KL approximations. The framework operates through on-policy sampling where the student model generates reasoning trajectories that are evaluated by both the teacher model and rule-based verifier, enabling joint optimization of imitation and reward objectives.

## Key Results
- KDRL improves mathematical reasoning accuracy by up to 2.6% over GRPO alone across multiple benchmarks
- Response-level reward-guided masking reduces reasoning token usage by 10% without sacrificing performance
- Linear KL coefficient annealing provides the best trade-off between accuracy and efficiency compared to constant schedules
- KDRL achieves better performance than standalone KD or RL approaches when using appropriate teacher models
- The framework demonstrates effectiveness in R1-Zero-like training scenarios with strong zero-shot generalization

## Why This Works (Mechanism)

### Mechanism 1: Gradient-Unified Policy Optimization via k2 KL Estimation
- **Claim:** Using the unbiased k2 KL gradient estimator in a joint loss provides stable, superior training dynamics for combining GRPO and KD.
- **Mechanism:** The k2 estimator has a gradient that is an unbiased estimate of the RKL gradient, allowing direct gradient combination in the joint loss. This avoids the high variance and instability introduced by reward shaping approaches.
- **Core assumption:** The joint loss gradient behaves predictably across varied KL coefficient schedules without conflicting gradient directions between GRPO and KD terms.
- **Evidence anchors:** k2 outperforms commonly used k3 and Top-K approximations; the unbiased gradient ensures consistent credit assignment across RL rewards and KD signals.
- **Break condition:** Training divergence or gradient variance explosion despite the unbiased estimator.

### Mechanism 2: KL Coefficient Annealing for Imitation-to-Exploration Transition
- **Claim:** Linearly annealing the KL coefficient β enables a curriculum from strong teacher imitation to reward-driven exploration, improving final performance.
- **Mechanism:** High β early prioritizes alignment with the teacher to bootstrap reasoning patterns, while low β later allows GRPO rewards to drive self-exploration and generalization.
- **Core assumption:** Teacher reasoning patterns do not conflict with the reward landscape later; the transition is smooth without policy collapse.
- **Evidence anchors:** Linearly annealed β schedule achieves best overall performance, suggesting more effective reward exploitation.
- **Break condition:** Validation performance drops sharply during β decay or final policy deviates excessively without improving rewards.

### Mechanism 3: Response-Level Reward-Guided KL Masking
- **Claim:** Masking KD loss for correct responses focuses teacher supervision on failure modes, improving token efficiency.
- **Mechanism:** By zeroing KD gradient for tokens in responses with positive rewards, the method prevents penalizing deviations from the teacher when the student's self-explored path is already correct.
- **Core assumption:** Correct responses do not contain suboptimal reasoning patterns that would benefit from teacher correction; reward signal is a sufficient proxy for trajectory quality.
- **Evidence anchors:** Response-level masking achieves better training and reasoning token efficiency with comparable performance.
- **Break condition:** Masked training leads to reward hacking or correct-reward responses develop problematic patterns harming generalization.

## Foundational Learning

- **Concept: Policy Gradient Methods (GRPO)**
  - **Why needed here:** KDRL builds on GRPO using group-relative advantages for RL updates; understanding policy gradients is essential to integrate the KD loss correctly.
  - **Quick check question:** Can you explain how GRPO's advantage function differs from PPO's and why it avoids a value model?

- **Concept: Reverse KL Divergence (RKL) vs. Forward KL**
  - **Why needed here:** KDRL minimizes RKL (student||teacher) via on-policy sampling, avoiding exposure bias; this contrasts with SFT's forward KL.
  - **Quick check question:** Why does minimizing RKL require on-policy sampling from the student, and how does this reduce exposure bias?

- **Concept: On-Policy vs. Off-Policy Distillation**
  - **Why needed here:** The paper's KD-RKL uses on-policy student samples with teacher logits; this is computationally costlier but improves performance.
  - **Quick check question:** What is the exposure bias problem in off-policy KD, and how does on-policy KD-RKL address it?

## Architecture Onboarding

- **Component map:** GRPO Engine -> KD-RKL Module -> Reward-Guided Masker -> KL Scheduler -> Unified Optimizer

- **Critical path:**
  1. Batch prompts → GRPO samples G rollouts per prompt using current policy
  2. Teacher model computes logits for each rollout (requires teacher inference, throughput bottleneck)
  3. Rule-based verifier assigns binary rewards
  4. KD-RKL module computes token-level ratios, applies reward-guided masking, aggregates into KL loss
  5. GRPO computes advantages and policy loss
  6. Scheduler provides current β; losses are combined and optimizer updates policy
  7. Repeat, periodically evaluating on held-out benchmarks

- **Design tradeoffs:**
  - k2 vs. k3 estimator: k2 is unbiased in gradient but biased in value; k3 is unbiased in value but biased in gradient. Evidence favors k2 for downstream performance
  - Response vs. Group-level masking: Response-level masking balances efficiency and performance; group-level masking underutilizes teacher signals
  - Teacher inference overhead: On-policy KD requires teacher forward passes for all rollouts, increasing training time ~1.5–2x vs. pure GRPO

- **Failure signatures:**
  - Training collapse (rewards → 0, loss spikes): Typically from reward shaping or excessive β; switch to joint loss and reduce initial β
  - Length explosion (>15K tokens, high clip ratios): Over-imitation from high β; lower β or accelerate annealing
  - No improvement over GRPO baseline: Teacher may be too weak or distribution mismatch; verify teacher quality and data filtering
  - Instability with Top-K approximation: High variance from out-of-top-K tokens; prefer k2 estimator

- **First 3 experiments:**
  1. **Ablation on KL estimator:** Compare k2 vs. k3 vs. Top-K on a small dataset (10K samples) monitoring training stability, validation accuracy, and response length. Expect k2 to outperform k3 by ~1–2% in accuracy and Top-K to destabilize.
  2. **KL coefficient sensitivity:** Sweep β in {1e-3, 2e-3, 5e-3} with and without linear annealing (β_init=5e-3, β_min=1e-3, δ=5e-5) on a medium dataset (30K samples). Expect annealing to yield the best accuracy-length trade-off.
  3. **Masking strategy validation:** Compare no masking, response-level, and group-level masking on the same medium dataset. Expect response-level masking to reduce average response length by ~10% without accuracy loss, while group-level masking underperforms.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does KDRL generalize effectively to non-mathematical reasoning domains such as code generation, logical deduction, or multi-hop question answering?
- Basis in paper: All experiments are conducted exclusively on mathematical reasoning benchmarks (AIME, MATH, AMC, Minerva, OlympiadBench). The paper does not validate KDRL on other reasoning tasks, leaving domain generality untested.
- Why unresolved: Mathematical reasoning may have unique properties (verifiability, structured outputs) that make KDRL particularly well-suited, but the unified objective assumes broad applicability.
- What evidence would resolve it: Apply KDRL to code reasoning benchmarks, logical reasoning tasks, and scientific reasoning datasets, comparing against GRPO and KD baselines.

### Open Question 2
- Question: How does KDRL scale to significantly larger student and teacher models (e.g., 30B–70B parameters)?
- Basis in paper: Experiments are limited to small models (1.5B student with 7B teacher; 3B student with 7B teacher in Zero-RL). The computational trade-offs and performance gains may differ at larger scales.
- Why unresolved: Larger models may have different optimization dynamics, and the relative contribution of KD versus RL could shift as base model capability increases.
- What evidence would resolve it: Conduct KDRL training with larger student models (e.g., 32B, 70B) and report training dynamics, final performance, and efficiency comparisons with baselines.

### Open Question 3
- Question: What are the necessary conditions on the teacher model for KDRL to be effective—specifically, must the teacher itself be RL-tuned, and what accuracy gap between teacher and student is tolerable?
- Basis in paper: The ablation study shows that using R1-Distill-Qwen-7B as a weaker teacher provides only modest gains over GRPO (+1.1%), compared to the stronger Skywork-OR1-Math-7B. The authors attribute this to both lower accuracy and lack of RL-based tuning.
- Why unresolved: It remains unclear whether the teacher's RL-based tuning or its higher raw accuracy is the dominant factor, and whether any strong off-the-shelf model can serve as an effective teacher.
- What evidence would resolve it: Systematically vary teacher properties (RL-tuned vs. non-RL-tuned, controlled accuracy differences) and measure KDRL performance to isolate each factor's contribution.

### Open Question 4
- Question: Is there a theoretically motivated or adaptive KL coefficient schedule that outperforms the empirical linear annealing used in KDRL?
- Basis in paper: The paper empirically selects a linear decay schedule (β from 5e-3 to 1e-3) and constant values through experimentation. While annealing works well, the optimal schedule may depend on task difficulty, model scale, or training stage.
- Why unresolved: The current schedule is heuristic. A principled approach (e.g., based on KL divergence dynamics, reward progress, or uncertainty estimation) could potentially improve the imitation-to-exploration transition.
- What evidence would resolve it: Develop adaptive KL scheduling strategies (e.g., performance-based, gradient-norm-based, or divergence-threshold-based) and compare against fixed and linear annealing schedules across multiple benchmarks.

## Limitations
- Computational overhead from teacher inference required for every rollout increases training time by 1.5–2x compared to pure GRPO
- Effectiveness depends on teacher model quality; weaker teachers provide only modest improvements over standalone GRPO
- Assumes rule-based rewards are reliable indicators of reasoning quality, which may not hold for all tasks
- Scaling behavior to very large models (100B+ parameters) remains untested

## Confidence
- **High Confidence**: The core mechanism of using the unbiased k2 KL estimator in a joint loss (not reward shaping) for stable gradient combination is well-supported by ablation studies and theoretical grounding
- **Medium Confidence**: The effectiveness of linear KL coefficient annealing for the imitation-to-exploration transition is supported by empirical results, but generalizability to other domains is uncertain
- **Low Confidence**: The assumption that correct-reward responses do not benefit from teacher correction (justifying response-level masking) lacks direct validation

## Next Checks
1. **Teacher Quality Sensitivity**: Run KDRL with teachers of varying quality (e.g., smaller models, different pre-training regimes) on a fixed dataset to quantify the impact of teacher strength on final performance and training stability
2. **Reward Function Ablation**: Replace the binary rule-based rewards with a learned reward model and compare KDRL's performance and training dynamics to assess sensitivity to reward function design
3. **Scaling Benchmark**: Implement KDRL on a larger model (e.g., 30B+ parameters) and measure training time, memory usage, and final performance against GRPO and standalone KD baselines to provide concrete data on computational overhead at scale