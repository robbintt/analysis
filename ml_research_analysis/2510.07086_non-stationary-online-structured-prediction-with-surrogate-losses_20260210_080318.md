---
ver: rpa2
title: Non-Stationary Online Structured Prediction with Surrogate Losses
arxiv_id: '2510.07086'
source_url: https://arxiv.org/abs/2510.07086
tags:
- loss
- surrogate
- learning
- bound
- young
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles online structured prediction in non-stationary
  environments. While prior work achieved finite surrogate regret bounds, these guarantees
  fail when the data distribution shifts over time.
---

# Non-Stationary Online Structured Prediction with Surrogate Losses

## Quick Facts
- **arXiv ID**: 2510.07086
- **Source URL**: https://arxiv.org/abs/2510.07086
- **Authors**: Shinsaku Sakaue, Han Bao, Yuzhou Cao
- **Reference count**: 40
- **Primary result**: Achieves cumulative target loss bound F_T + C(1 + P_T) in non-stationary online structured prediction, where F_T is comparator surrogate loss and P_T is comparator path length

## Executive Summary
This paper addresses online structured prediction in non-stationary environments where data distributions shift over time. Prior work achieved finite surrogate regret bounds, but these guarantees break down when F_T = Ω(T) due to distribution shifts. The authors introduce a new "small-surrogate-loss + path-length" bound that depends on the time horizon T only through F_T and P_T, yielding stronger guarantees in non-stationary settings. The key innovation is a Polyak-style learning rate that systematically offers target-loss guarantees while exhibiting promising empirical performance.

## Method Summary
The method uses Online Gradient Descent with a novel Polyak-style learning rate that exploits the surrogate gap between target and surrogate losses. At each round, the algorithm computes a linear estimator W_t, derives a score vector θ_t = W_t x_t, samples a prediction ŷ_t from a decoding distribution π(θ_t), and updates using gradients computed from surrogate loss. The learning rate η_t = min(2(L_t - E[ℓ])/||G_t||², η_{t-1}) ensures systematic target-loss guarantees while adapting to changing environments. The approach is extended to structured prediction problems where prediction and label sets differ via convolutional Fenchel-Young losses.

## Key Results
- Achieves cumulative target loss bound F_T + C(1 + P_T) that depends on T only through F_T and P_T
- Introduces Polyak-style learning rate that systematically offers target-loss guarantees
- Extends approach to structured prediction with different prediction/label sets via convolutional Fenchel-Young loss
- Proves lower bound showing dependence on F_T and P_T is tight
- Demonstrates effectiveness on synthetic and real datasets, particularly in highly non-stationary environments

## Why This Works (Mechanism)

### Mechanism 1: Surrogate Gap Creates Target-Loss Cancellation
The cumulative target loss is bounded by F_T + C(1 + P_T) because the surrogate loss terms cancel during summation. Assumption 3.2 provides E[ℓ(ŷ, y)] ≤ (1-α)L(θ, y), creating a gap between target and surrogate losses. Combined with the self-bounding property (||G_t||_F² ≤ 2M L_t), this guarantees a non-empty learning rate range η_t ∈ [α/M, 2(L_t - E[ℓ])/||G_t||_F²]. The upper bound enables canceling Σ_t L_t(W_t) terms in the OGD regret bound, leaving only comparator surrogate loss F_T.

### Mechanism 2: Polyak-Style Learning Rate Enables Adaptive Non-Stationarity Tracking
The learning rate η_t = min(2(L_t - E[ℓ])/||G_t||², η_{t-1}) provides systematic target-loss guarantees while adapting to changing environments. This rate satisfies theoretical bounds while being data-adaptive. The non-increasing constraint ensures η_T ≥ α/M, controlling the O((1+P_T)/η_T) term. The Polyak-style form automatically increases η_t when the surrogate gap is large (confident predictions) and decreases it near decision boundaries.

### Mechanism 3: Convolutional Fenchel-Young Loss Extends to Ŷ ≠ Y
The approach handles problems where prediction and label sets differ (e.g., ranking with NDCG). Lemma 4.3 establishes E[ℓ(ŷ, y)] = L^Ω_τ(θ, y) - L^Ω(θ + L_ρπ(θ), y), decomposing target loss into surrogate minus a regularized term. Lemma 4.4 bounds this negative term below by λ/2||∇L^Ω_τ||², providing an alternative to Assumption 3.2's surrogate gap condition.

## Foundational Learning

- **Online Convex Optimization with Dynamic Regret**: Why needed - Extends static comparator analysis to time-varying U_1, ..., U_T with path length P_T measuring non-stationarity. Quick check - Can you explain why dynamic regret O(√(T(1+P_T))) is meaningful when P_T = o(T) but vacuous when P_T = Θ(T)?

- **Surrogate Loss Framework**: Why needed - Enables continuous optimization for discrete prediction spaces via the (ρ, ℓ_ρ)-decomposition of target losses. Quick check - For multiclass classification with 0-1 loss, what are ρ(y), ℓ_ρ(ŷ), and the resulting dimension d?

- **Strong Convexity ↔ Smoothness Duality**: Why needed - If Ω is λ-strongly convex, then L^Ω is 1/λ-smooth, enabling the self-bounding property ||G||_F² ≤ 2M L. Quick check - Why does larger λ reduce the regret term D(1+P_T)/λ but potentially increase F_T?

## Architecture Onboarding

- **Component map**: Input x_t → Linear estimator W_t → Score θ_t = W_t x_t → Decoding distribution π(θ_t) → Sample prediction ŷ_t → Compute surrogate gap E_π[ℓ(ŷ_t, y_t)] → Polyak-style η_t = min(2(L_t - E[ℓ])/||G_t||², η_{t-1}) → OGD update: W_{t+1} = Proj_W(W_t - η_t G_t)

- **Critical path**: The learning rate computation requires evaluating E_{π(θ_t)}[ℓ(ŷ_t, y_t)] before observing y_t; this is feasible since decoding distribution is computed from θ_t alone (the expectation uses known target loss structure).

- **Design tradeoffs**:
  - Constant vs. Polyak-style η: Constant (η = α/M) is simpler but conservative; Polyak adapts to data but requires computing E[ℓ]
  - λ selection: Larger λ shrinks (1+P_T)/λ term but may increase surrogate loss scale
  - Fenchel-Young vs. smooth hinge: FY losses are smooth; smooth hinge is non-smooth in W but still self-bounding

- **Failure signatures**:
  - Linear regret growth (Σ ℓ = Θ(T)) with small F_T → η range may be violated or α too small
  - Dominated by path length term → comparator sequence changing too rapidly; consider shorter horizons
  - Decoding returns no gap → verify Assumption 3.2 conditions; use convolutional extension if Ŷ ≠ Y

- **First 3 experiments**:
  1. Binary classification with periodic label flips (replicate Figure 1): Compare constant η, AdaGrad, and Polyak-style under {1, 10, 100} flips over T=10,000 rounds using logistic loss
  2. Multiclass classification (K=10): Validate surrogate gap α=1/K with Fenchel-Young logistic loss; measure cumulative 0-1 loss vs. comparator F_T
  3. Label ranking with NDCG (Appendix A.3): Test convolutional Fenchel-Young extension; verify efficient decoding via Birkhoff polytope Frank-Wolfe with controlled support size

## Open Questions the Paper Calls Out
- **Question**: Can meaningful "small-surrogate-loss + path-length" bounds be achieved in non-stationary online structured prediction under bandit feedback?
- **Basis in paper**: Section 6 explicitly states the work is restricted to full-information feedback and notes that obtaining meaningful guarantees in settings like bandit feedback, where existing bounds depend on T, is a challenge left for future work.
- **Why unresolved**: The current theoretical guarantees rely on the full observation of ground-truth labels to compute gradients and exploit the surrogate gap; bandit feedback obscures the necessary information to link target loss to surrogate loss dynamically.
- **What evidence would resolve it**: An algorithm capable of achieving a cumulative target loss bound of the form F_T + C(1 + P_T) under bandit feedback, or a formal proof demonstrating that such guarantees are impossible in this setting.

## Limitations
- Bounds critically depend on surrogate gap parameter α > 0; when α is small, learning rate range becomes very restrictive
- Lower bound proof assumes adversarial target loss sequence bounded away from zero, which may not reflect practical scenarios
- Method requires full-information feedback, limiting applicability to bandit feedback settings

## Confidence
- **High confidence**: The mechanism connecting surrogate gap to learning rate bounds (Mechanism 1) is mathematically rigorous and the self-bounding property is well-established in OCO literature
- **Medium confidence**: The Polyak-style learning rate's practical effectiveness (Mechanism 2) is supported by theory but relies on efficient computation of expected target loss
- **Medium confidence**: The convolutional Fenchel-Young extension (Mechanism 3) introduces novel technical tools but requires careful implementation of the decoding distribution π(θ)

## Next Checks
1. **Gap sensitivity analysis**: Systematically vary α (e.g., 0.1, 0.5, 0.9) in synthetic experiments to quantify the impact on learning rate adaptivity and regret bounds
2. **Computational complexity verification**: Measure the actual support size of π(θ_t) in structured prediction tasks and benchmark against the theoretical O(d) bound
3. **Path-length dependency stress test**: Design experiments where P_T grows at different rates (O(1), O(√T), O(T)) to empirically validate the bound's dependence on non-stationarity