---
ver: rpa2
title: 'CLASS-IT: Conversational and Lecture-Aligned Small-Scale Instruction Tuning
  for BabyLMs'
arxiv_id: '2510.25364'
source_url: https://arxiv.org/abs/2510.25364
tags:
- instruction
- tuning
- data
- both
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether BabyLM-scale models can benefit
  from instruction tuning. It compares conversational and question-answering instruction
  tuning datasets, applied either merged or sequentially, using decoder-only models
  with 100M and 140M parameters.
---

# CLASS-IT: Conversational and Lecture-Aligned Small-Scale Instruction Tuning for BabyLMs

## Quick Facts
- arXiv ID: 2510.25364
- Source URL: https://arxiv.org/abs/2510.25364
- Reference count: 11
- Small-scale instruction tuning improves fine-tuning but not zero-shot performance for BabyLM models

## Executive Summary
This paper investigates whether BabyLM-scale models can benefit from instruction tuning. It compares conversational and question-answering instruction tuning datasets, applied either merged or sequentially, using decoder-only models with 100M and 140M parameters. Evaluation spans both fine-tuning (SuperGLUE) and zero-shot (BLiMP, EWoK, WUGs, entity tracking, and psycholinguistic correlation) settings. Results show that instruction tuning yields small but consistent gains in fine-tuning scenarios, with sequential curricula outperforming merged data; however, improvements do not consistently transfer to zero-shot tasks, suggesting a trade-off between interaction-focused adaptation and broad linguistic generalization.

## Method Summary
The study evaluates instruction tuning on BabyLM-scale decoder-only models (100M and 140M parameters) using conversational and question-answering datasets. Two approaches are tested: merging datasets before fine-tuning versus applying them sequentially in a curriculum. Models are evaluated on both fine-tuning tasks (SuperGLUE) and zero-shot linguistic capabilities (BLiMP, EWoK, WUGs, entity tracking, and psycholinguistic correlation). The sequential curriculum consistently outperforms merged data in fine-tuning, but gains do not reliably transfer to zero-shot scenarios.

## Key Results
- Sequential instruction tuning curricula outperform merged datasets in fine-tuning settings
- Small but consistent gains in fine-tuning performance with instruction tuning
- No reliable improvement in zero-shot linguistic tasks from instruction tuning

## Why This Works (Mechanism)
Instruction tuning helps models adapt to interaction-focused tasks by teaching them to follow structured input-output patterns. The sequential curriculum allows models to first learn conversational patterns before moving to more structured Q&A formats, creating a smoother learning trajectory. However, this specialization appears to come at the cost of general linguistic capabilities, as zero-shot performance does not improve. The mechanism likely involves the model developing task-specific heuristics rather than broad linguistic understanding.

## Foundational Learning
- **Instruction Tuning**: Why needed - enables models to follow structured prompts and improve task performance. Quick check - compare performance on instruction-following benchmarks before/after tuning.
- **Curriculum Learning**: Why needed - sequential presentation of tasks can improve learning efficiency and generalization. Quick check - measure learning curves for sequential vs. merged training approaches.
- **Zero-shot Evaluation**: Why needed - assesses whether models can apply learned capabilities to novel tasks. Quick check - test models on held-out tasks not seen during training.
- **Fine-tuning vs. Zero-shot Trade-off**: Why needed - understanding when adaptation helps or hurts generalization. Quick check - compare performance across both evaluation paradigms.

## Architecture Onboarding
**Component Map**: Dataset -> Tokenizer -> Instruction Tuner -> Fine-tuning Evaluator -> Zero-shot Evaluator

**Critical Path**: Instruction datasets → model fine-tuning → evaluation on SuperGLUE (fine-tuning) and linguistic benchmarks (zero-shot)

**Design Tradeoffs**: Sequential curriculum vs. merged data presents a tradeoff between learning efficiency and potential task interference. Smaller models may benefit more from structured curricula due to limited capacity.

**Failure Signatures**: 
- Poor performance on both fine-tuning and zero-shot tasks indicates insufficient instruction tuning
- Strong fine-tuning but weak zero-shot suggests over-specialization
- Degradation in base capabilities indicates catastrophic forgetting

**First Experiments**:
1. Compare sequential vs. merged curriculum on a held-out instruction-following task
2. Test zero-shot performance on a novel linguistic benchmark
3. Measure retention of base capabilities after instruction tuning

## Open Questions the Paper Calls Out
None

## Limitations
- Lack of cross-linguistic validation limits generalizability beyond English
- Focus on small models (100M-140M) leaves uncertainty about scalability
- Heavy reliance on English benchmarks restricts insights into multilingual contexts
- Absence of mechanistic explanations for sequential curriculum advantages

## Confidence
- High confidence: Sequential instruction tuning curricula outperform merged datasets in fine-tuning settings
- Medium confidence: Instruction tuning does not reliably improve zero-shot performance
- Low confidence: Applicability of findings to larger models or other languages

## Next Checks
1. Cross-linguistic validation: Replicate sequential instruction tuning with multilingual BabyLM-scale models
2. Mechanistic analysis: Conduct probing experiments to identify preserved/enhanced linguistic capabilities
3. Scaling study: Extend sequential curriculum to larger decoder-only models (1B-10B parameters)