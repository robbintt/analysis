---
ver: rpa2
title: 'MoE-Compression: How the Compression Error of Experts Affects the Inference
  Accuracy of MoE Model?'
arxiv_id: '2509.07727'
source_url: https://arxiv.org/abs/2509.07727
tags:
- layer
- experts
- expert
- errors
- inference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores how compression errors in different expert
  layers of a Mixture-of-Experts (MoE) model affect inference accuracy. The authors
  introduce error-bounded lossy compression algorithms (SZ3 and CuSZp) to compress
  non-activated experts, reducing data transfer overhead during MoE inference under
  GPU memory constraints.
---

# MoE-Compression: How the Compression Error of Experts Affects the Inference Accuracy of MoE Model?

## Quick Facts
- **arXiv ID:** 2509.07727
- **Source URL:** https://arxiv.org/abs/2509.07727
- **Reference count:** 40
- **Primary result:** Layer-wise sensitivity analysis reveals middle MoE experts are critical for reasoning, while shallow/deep layers tolerate compression errors

## Executive Summary
This paper investigates how compression errors in different expert layers affect MoE model inference accuracy under GPU memory constraints. Using error-bounded lossy compression (SZ3 and CuSZp) to compress non-activated experts, the authors systematically analyze error tolerance across different expert layers in the Moonlight model. Their experiments reveal a clear stratification: middle-layer experts crucial for reasoning cannot tolerate errors without significant accuracy degradation, while shallow and deep layers can handle bounded errors with minimal impact. Interestingly, bounded errors in deep-layer experts sometimes improve instruction compliance accuracy, suggesting these layers have inherent redundancy that can be leveraged for compression.

## Method Summary
The authors simulate error-bounded lossy compression by injecting Gaussian noise with standard deviation proportional to expert weights' L1 norm ($\hat{e} = \text{rate} \times ||\theta||_1 / n_{params}$). Experiments are conducted on the Moonlight model (26 layers, 64 experts per layer, top-6 routing) using GSM8K and Math datasets. They measure two metrics: Pure Inference Accuracy (correct answers) and Instruction Compliance Accuracy (formatting adherence). The study systematically tests single experts, top-k experts, and layer groups across different error bounds (0-80% relative to L1 norm) to map the sensitivity landscape of the MoE architecture.

## Key Results
- Middle-layer experts (layers 9-18) suffer severe accuracy drops when compressed, with Layer 13 showing lowest tolerance
- Shallow-layer experts (handling attention and token transformation) tolerate 80% error bounds with minimal accuracy loss
- Deep-layer experts (instruction following) sometimes benefit from bounded errors, improving instruction compliance while maintaining semantic accuracy
- Error tolerance follows a clear depth-dependent pattern that enables asymmetric compression strategies

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Inference accuracy exhibits non-uniform sensitivity to compression errors depending on layer depth.
- **Mechanism:** The authors observe that middle-layer experts are critical for reasoning and cannot tolerate errors, whereas shallow-layer experts (attention/token transformation) and deep-layer experts (instruction following) are robust or even benefit from bounded noise. By constraining errors strictly in the middle layers while allowing higher compression in shallow/deep layers, one can maximize compression ratios while preserving accuracy.
- **Core assumption:** The functional role of experts is stratified by depth, with reasoning concentrated in the middle layers, and error propagation does not linearly destroy semantic integrity in non-reasoning layers.
- **Evidence anchors:**
  - [abstract] "Middle-layer experts, crucial for model reasoning, suffer significant accuracy drops... deep-layer experts... can sometimes improve inference accuracy."
  - [section 3.5] "Layer 13 resulted in the lowest inference accuracy... middle layers play a critical role in problem analysis."
  - [corpus] Weak direct links; adjacent papers (e.g., Sub-MoE) discuss merging, but this specific depth-wise error sensitivity profile is isolated to this work's simulation.
- **Break condition:** If an MoE architecture distributes reasoning uniformly across all layers or lacks distinct "shallow/middle/deep" functional specialization, this asymmetric compression strategy would likely degrade performance unpredictably.

### Mechanism 2
- **Claim:** Bounded errors in deep-layer experts may function as an implicit ensemble or regularization mechanism, improving instruction compliance.
- **Mechanism:** The paper suggests that introducing specific magnitudes of noise ($\hat{e}$) into the parameters of deep-layer experts creates a "diverse ensemble" effect during the output integration phase. This appears to enhance the model's ability to follow formatting instructions (Instruction Compliance Accuracy) without degrading the semantic truth (Pure Inference Accuracy).
- **Core assumption:** The output generation phase operates in a high-dimensional feature space where small perturbations do not shift the semantic peak but smooth the decision boundary for output formatting.
- **Evidence anchors:**
  - [abstract] "Introducing bounded errors in the deep-layer experts... can sometimes lead to improvements in inference accuracy."
  - [section 3.2.3] "Adding noise to parameters in deeper layers can enhance task execution... implicit integration effect."
  - [corpus] No direct corpus confirmation of this specific "noise-benefit" in neighbors; assumes generalization beyond the Moonlight model used in experiments.
- **Break condition:** If the task requires strict numerical precision (e.g., code generation) rather than semantic instruction following, noise injection in deep layers will likely cause hallucinations or syntax errors.

### Mechanism 3
- **Claim:** Error-bounded lossy compression provides a superior trade-off for offloading compared to low-bit quantization because it controls the worst-case error magnitude.
- **Mechanism:** Unlike quantization, which often introduces "uncontrollable and unpredictable" errors that can trigger collapse in sensitive experts, error-bounded compressors (like SZ3) guarantee that parameter distortion stays within a defined norm ($\hat{e}$). This predictability allows the system to stay within the "robustness threshold" of the shallow and deep layers.
- **Core assumption:** The reconstruction error of lossy compressors approximates the random normal distribution used in the paper's simulations, and the PCIe bandwidth saving outweighs the computational cost of on-the-fly decompression.
- **Evidence anchors:**
  - [section 1] "Quantization frequently leads to significant degradation... due to the uncontrollable and unpredictable errors."
  - [section 3.1] "To simulate the compression errors... we randomly generated n errors which follows the normal distributions $N \sim (0, \hat{e})$."
  - [corpus] [Section 1] contrasts with neighbors like "MoEQuant" or "MC#" which focus on quantization/merging.
- **Break condition:** If the decompression latency of algorithms like SZ3/CuSZp exceeds the time required to simply transfer uncompressed data over PCIe (e.g., on systems with extremely high interconnect bandwidth), the utility of this compression collapses.

## Foundational Learning

- **Concept:** **Expert Offloading & PCIe Bottlenecks**
  - **Why needed here:** The entire premise relies on the fact that moving experts from CPU RAM to GPU VRAM is slow (I/O bound). You must understand that inference speed is limited by the 32 GB/s PCIe bandwidth vs. 300 GB/s internal GPU bandwidth to see why compression is necessary.
  - **Quick check question:** In an offloading setup, is the bottleneck the GPU compute speed or the time it takes to load the expert weights from main memory?

- **Concept:** **L1 Norm & Error Bounds ($\hat{e}$)**
  - **Why needed here:** The paper defines error tolerance relative to the L1 norm of expert parameters ($\hat{e} = 80\% \times ||\theta||_1 / n$). You need to grasp that "80% error" doesn't mean randomizing 80% of values, but rather perturbing values within a bound relative to the weight magnitude.
  - **Quick check question:** Does a higher compression ratio require a tighter or looser error bound $\hat{e}$?

- **Concept:** **Instruction Compliance Accuracy (ICA) vs. Pure Inference Accuracy (PIA)**
  - **Why needed here:** The paper decouples "getting the right answer" (PIA) from "formatting it correctly" (ICA). This distinction is critical because deep-layer errors hurt ICA less than PIA, and sometimes improve it.
  - **Quick check question:** If a model solves a math problem correctly but outputs the answer in prose instead of the requested JSON format, did it fail in PIA, ICA, or both?

## Architecture Onboarding

- **Component map:** Input Token -> Router Decision -> Check VRAM -> (Miss) Load from RAM -> Decompress/Inject Error -> Expert Compute -> Output
- **Critical path:** Input Token → **Router Decision** → Check VRAM → (Miss) Load from RAM → **Decompress/Inject Error** → **Expert Compute** → Output
- **Design tradeoffs:**
  - **Aggressive Compression (High $\hat{e}$):** maximizes bandwidth savings but risks "non-instructional errors" and reasoning collapse in middle layers
  - **Layer-Skipping:** You can compress shallow/deep layers aggressively but must handle middle layers (Layers 9-18 in this architecture) with high fidelity
- **Failure signatures:**
  - **Reasoning Collapse:** Accuracy drops to ~0.38 (ICA) when middle layers (e.g., Layer 13) are compressed at 80% error bounds
  - **Format Violation:** Model outputs correct semantic content but fails to box the answer or follow syntax (low ICA with high PIA)
  - **Model Silence:** Outputting no tokens when error bounds exceed 80% across multiple grouped layers
- **First 3 experiments:**
  1. **Single Expert Sensitivity:** Randomize the weights of *one* expert in Layer 1 to confirm if the model fails completely or stays robust (validates redundancy)
  2. **Depth-Wise Error Injection:** Inject identical error bounds ($\hat{e} = 80\%$) into the top experts of Layer 1, 13, 20, and 26 respectively to map the "Reasoning Core"
  3. **Grouped Collapse Test:** Inject errors into the top experts of a 10-layer group (e.g., Layers 9-18) to verify if errors accumulate non-linearly and cause system failure

## Open Questions the Paper Calls Out
- How can pipeline algorithms be designed to effectively overlap compression/decompression operations with offloading tasks to minimize latency in MoE inference?
- Do the observed error sensitivity patterns (robust shallow layers, sensitive middle layers, accuracy gains in deep layers) generalize to other MoE architectures like Mixtral or DeepSeek?
- Does the simulation of errors via Normal distribution accurately reflect the error characteristics and impact of actual lossy compression algorithms like SZ3 and CuSZp?

## Limitations
- The beneficial effect of bounded errors in deep layers remains unexplained mechanistically, appearing to work empirically but without theoretical grounding
- The study focuses exclusively on the Moonlight model architecture, limiting generalizability to other MoE designs with different expert counts and routing strategies
- The paper uses simulated Gaussian noise injection rather than actual SZ3/CuSZp compression-decompression, introducing uncertainty about real-world behavior

## Confidence
- **High Confidence:** The asymmetric sensitivity of MoE layers to compression errors (middle layers critical, shallow/deep layers robust) is well-supported by controlled experiments
- **Medium Confidence:** The claim that deep-layer errors can improve instruction compliance requires further validation and may be specific to the Moonlight architecture
- **Low Confidence:** The assertion that error-bounded lossy compression is superior to quantization for MoE offloading is based on theoretical arguments rather than direct empirical comparison

## Next Checks
1. Replace the Gaussian noise injection with actual SZ3 and CuSZp compression-decompression pipelines to verify that simulated errors accurately predict real compression behavior
2. Test the same layer-wise error sensitivity analysis on a different MoE architecture (e.g., Mixtral-8x7B) to determine if stratification is architecture-dependent or general
3. Conduct ablation studies on deep-layer noise benefits by varying noise magnitudes systematically while measuring both semantic accuracy and instruction compliance separately, coupled with feature visualization