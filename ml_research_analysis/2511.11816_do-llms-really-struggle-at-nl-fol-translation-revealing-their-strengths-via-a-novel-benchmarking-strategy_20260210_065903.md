---
ver: rpa2
title: Do LLMs Really Struggle at NL-FOL Translation? Revealing their Strengths via
  a Novel Benchmarking Strategy
arxiv_id: '2511.11816'
source_url: https://arxiv.org/abs/2511.11816
tags:
- translation
- logical
- formula
- task
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper critiques current NL-FOL translation benchmarks, showing\
  \ they conflate ontology extraction with logical translation and use flawed metrics\
  \ like LE and BLEU that fail to capture logical equivalence. To address this, the\
  \ authors propose a novel benchmarking strategy that: (1) provides a fixed FOL signature\
  \ and ontology upfront to isolate logical translation; (2) generates random logical\
  \ perturbations on the fly to create candidate sets, reducing memorization and pattern-matching;\
  \ and (3) evaluates through three tasks\u2014logical translation (using SMT solver\
  \ equivalence), most similar (select closest semantic match), and ranking (order\
  \ by semantic similarity, including negations and equivalents)."
---

# Do LLMs Really Struggle at NL-FOL Translation? Revealing their Strengths via a Novel Benchmarking Strategy

## Quick Facts
- **arXiv ID:** 2511.11816
- **Source URL:** https://arxiv.org/abs/2511.11816
- **Reference count:** 19
- **Key outcome:** Novel benchmark shows strong LLMs understand FOL semantics well when evaluated properly; embedding-centric models lag behind.

## Executive Summary
This paper critiques current NL-FOL translation benchmarks, showing they conflate ontology extraction with logical translation and use flawed metrics like LE and BLEU that fail to capture logical equivalence. To address this, the authors propose a novel benchmarking strategy that: (1) provides a fixed FOL signature and ontology upfront to isolate logical translation; (2) generates random logical perturbations on the fly to create candidate sets, reducing memorization and pattern-matching; and (3) evaluates through three tasks—logical translation (using SMT solver equivalence), most similar (select closest semantic match), and ranking (order by semantic similarity, including negations and equivalents). Experiments on two datasets with dialogue-oriented models (GPT-4O-MINI, O3-MINI, Qwen3) show strong logical understanding, especially in O3-MINI, while embedding-centric models (Qwen3-Embedding-8B, Gemini-Embedding-001) perform markedly worse. The results demonstrate that strong logical translation scores reflect genuine semantic grasp, not superficial pattern matching.

## Method Summary
The authors propose a new benchmark for evaluating LLMs on Natural Language to First-Order Logic (NL-FOL) translation that isolates logical translation from ontology extraction. They provide a fixed FOL signature and ontology upfront for each instance, generate random logical perturbations on the fly to create candidate sets, and evaluate through three tasks: logical translation (checking semantic equivalence via SMT solver), most similar (selecting the closest semantic match from candidates), and ranking (ordering candidates by semantic similarity). The approach uses zero-shot inference with automatic chain-of-thought prompting, and results are reported with standard deviation over multiple seeds for dialogue models.

## Key Results
- Dialogue-oriented models (O3-MINI, GPT-4O-MINI) achieve high accuracy on logical translation (0.79) and most similar (0.91) tasks, with O3-MINI outperforming others significantly
- Embedding-centric models (Qwen3-Embedding-8B, Gemini-Embedding-001) show much lower performance (0.40-0.43 on most similar, 0.20-0.30 on ranking-both)
- The ranking task proves more discriminative than most similar, revealing shallow heuristics in models that succeed at discrimination but fail at fine-grained ordering
- Results hold across two datasets (D_FOLIO and D_Stanford) with consistent patterns, demonstrating robustness of the benchmarking approach

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Providing a fixed FOL signature and ontology upfront isolates logical translation ability from ontology extraction capability.
- **Mechanism**: By decoupling these two subtasks, the benchmark eliminates a major confounding variable that plagued previous evaluations (FOLIO and MALLS both collapsed these into one task). With a fixed signature, SMT solver verification becomes straightforward—you can directly compare the generated formula to ground truth via logical equivalence rather than proxy measures.
- **Core assumption**: That logical translation is the core competency worth measuring independently, and that in many real-world domains (e.g., formal verification), experts can predefine an ontology once.
- **Evidence anchors**:
  - [Page 2]: "keeping OE and LT separate allows to determine whether a model fails at extracting a signature or at translating logic"
  - [Page 4]: "Providing Ω marks a deliberate departure from earlier studies, which asked the model to derive φ′ from p alone."
  - [Corpus]: Weak/no direct corpus support—this decomposition is novel to this paper.
- **Break condition**: If your application requires the model to invent its own ontology (e.g., open-domain reasoning), this isolation becomes a limitation, not a feature.

### Mechanism 2
- **Claim**: On-the-fly random perturbations of FOL formulas create evaluation sets resistant to memorization and dataset contamination.
- **Mechanism**: By generating candidate sets (F_ms, F_r) dynamically—through single edits like connective swaps, quantifier flips, or negation insertions—the benchmark produces novel test cases not present in any training corpus. This forces models to rely on semantic understanding rather than surface-level pattern matching.
- **Core assumption**: That perturbations preserve sufficient difficulty while remaining semantically distinct; and that models cannot game the task by syntactic proximity alone.
- **Evidence anchors**:
  - [Page 4]: "the most similar and ranking tasks are far less vulnerable than the logical translation one to typical LLM evaluation issues such as memorisation and dataset leakage, since their candidate sets F_ms, T_ms, F_r, T_r are generated on the fly"
  - [Page 5-6]: Describes perturbation types and notes that syntactically distant formulas (like NNF-transformed negations) are not easier for models, suggesting semantic processing.
  - [Corpus]: Corpus papers on FOL evaluation (e.g., "Assessing the Sensitivity and Alignment of FOL Closeness Metrics") highlight metric flaws but don't propose perturbation-based solutions.
- **Break condition**: If perturbations are too trivial (e.g., constant renaming), models may solve via syntax; the paper notes they excluded such easy cases after pilot testing.

### Mechanism 3
- **Claim**: Multi-task evaluation (translation + most similar + ranking) with graded difficulty reveals genuine logical competence better than single-metric approaches.
- **Mechanism**: The three tasks probe different aspects: translation tests generative ability; most similar tests discrimination among candidates; ranking tests fine-grained semantic ordering (including recognizing negations and equivalents). The ranking task proves more discriminative—models that succeed at most similar often fail at ranking, exposing shallow heuristics.
- **Core assumption**: That the ranking task's requirements (top positions for φ and φ_eq; bottom for negations) correctly operationalize "semantic grasp."
- **Evidence anchors**:
  - [Page 7-8]: "For example, with GPT-4O-MINI on D_FOLIO (FOL), the model succeeds in most similar but fails in ranking-equivalence in 31% of the instances, whereas the opposite occurs in only 4% of the cases."
  - [Page 8]: "models misrank ¬φ and (¬φ)_nnf at similar rates, despite the fact that (¬φ)_nnf is, in principle, syntactically more distant from φ... offering additional evidence that the models do not rely on purely syntactic information."
  - [Corpus]: Corpus papers largely use single-metric evaluation; this multi-task approach is distinctive.
- **Break condition**: If your model cannot handle structured output constraints (e.g., returning ranked lists), the ranking task becomes infeasible.

## Foundational Learning

- **Concept: First-Order Logic (FOL) syntax and semantics**
  - Why needed here: The entire benchmark assumes fluency with quantifiers (∀, ∃), logical connectives (∧, ∨, →, ↔, ¬), predicates, constants, and variables. Without this, you cannot interpret the perturbation types or understand why LE score is flawed.
  - Quick check question: Given "Every cube is small," can you distinguish ∀x(Cube(x) → Small(x)) from ∃x(Cube(x) ∧ Small(x)) and explain why they're not equivalent?

- **Concept: SMT solvers and logical equivalence checking**
  - Why needed here: The benchmark's "logical translation" task relies on Z3 to verify equivalence between candidate and ground-truth formulas. Understanding that equivalence reduces to unsatisfiability of ¬(φ ↔ φ′) is essential.
  - Quick check question: If Z3 returns "unsat" on ¬(φ ↔ φ′), what does that mean for the relationship between φ and φ′?

- **Concept: Ontology Extraction vs. Logical Translation decomposition**
  - Why needed here: This is the paper's core conceptual contribution. You must understand why previous benchmarks conflate these, and how providing a fixed signature (σ) and glossary (γ) isolates pure logical translation.
  - Quick check question: If you're given the phrase "A musician loves music" and must translate to FOL, what's the difference between (a) inventing predicate symbols yourself vs. (b) being told to use Musician(x) and Love(x, y)?

## Architecture Onboarding

- **Component map**: Dataset layer -> Perturbation engine -> Task layer -> Verification layer -> Scoring layer
- **Critical path**:
  1. Define ontology Ω for each dataset story/instance (manual work)
  2. Implement perturbation generator (algorithmic; see paper's descriptions)
  3. Build prompt templates for each task (paper provides examples in Appendix H)
  4. Integrate Z3 for equivalence checking (or embedding similarity for embedding-centric models)
  5. Run 5-seed repetition for dialogue models; single-run for deterministic embedding models

- **Design tradeoffs**:
  - Providing Ω upfront vs. requiring model to infer: reduces realism but increases measurement precision
  - Number of perturbations (k=8 for most similar, k=3 for ranking): more perturbations increase difficulty but also computational cost
  - Zero-shot vs. few-shot prompting: paper uses zero-shot with automatic chain-of-thought; results are lower bounds
  - SMT solver vs. embedding distance: SMT is rigorous but only works for dialogue models generating valid FOL; embeddings work for all but are noisier

- **Failure signatures**:
  - Model generates syntactically invalid FOL → Z3 parsing fails (exclude from scoring)
  - Model selects correct formula in most similar but fails ranking → indicates shallow heuristics (observed in GPT-4O-MINI)
  - Model performs well on NL variants but poorly on FOL variants → embedding-centric models show this pattern, suggesting poor FOL semantic encoding
  - Ranking-Equivalence succeeds but Ranking-Negation fails → model struggles with negation semantics (common across models)

- **First 3 experiments**:
  1. **Replicate translation task on a small subset** (e.g., 50 instances from D_FOLIO training set) with a dialogue model (GPT-4O-MINI or open-source equivalent). Verify Z3 integration works and scoring matches paper's methodology.
  2. **Perturbation stress test**: Manually inspect 20 perturbation sets (F_ms and F_r) to confirm semantic distinctness. Check that models aren't solving via trivial syntactic cues (e.g., all perturbations share same predicate names but differ only in one connective).
  3. **Embedding model baseline**: Run QWEN-EMB-PLAIN or GEMINI-EMB on most similar and ranking tasks. Confirm performance gap vs. dialogue models (expected: ~0.4-0.5 on most similar, ~0.2-0.3 on ranking-both). This establishes your evaluation pipeline captures the reported capability difference.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do current state-of-the-art LLMs perform on Ontology Extraction (OE) when it is rigorously decoupled from Logical Translation (LT)?
- Basis in paper: [explicit] The authors state that a "key next step is to determine how difficult extracting an ontology is for current models" and explicitly exclude OE from the current study to focus on LT.
- Why unresolved: The study deliberately assumes access to a predefined, correct ontology to isolate logical translation capabilities, leaving the difficulty of the first step of the pipeline unmeasured.
- What evidence would resolve it: An extension of the proposed benchmark that requires the model to first generate the signature and glossary from the NL sentence before translation, evaluated against human-curated ground truths.

### Open Question 2
- Question: Can alternative embedding-similarity metrics or instruction-tuning strategies significantly improve the performance of embedding-centric models on the proposed "Most Similar" and "Ranking" tasks?
- Basis in paper: [explicit] The authors note that embedding-centric models performed markedly worse and suggest "future work could include a deeper analysis, e.g., exploring alternative embedding-similarity metrics."
- Why unresolved: The study only utilized cosine similarity for embedding evaluation, which may not capture the logical semantic proximity required by the benchmark's tasks as effectively as it captures general semantic similarity.
- What evidence would resolve it: A comparative analysis of the benchmark results using various distance metrics (e.g., Manhattan, Euclidean) or learned similarity measures on the same model outputs.

### Open Question 3
- Question: To what extent does explicit handling of linguistic ambiguity and polysemy in datasets affect LLM performance on the Logical Translation task?
- Basis in paper: [explicit] The authors conclude that progress requires "high-quality, carefully curated datasets... explicitly accounting for polysemy."
- Why unresolved: The datasets used ($D_{Stanford}$ and $D_{FOLIO}$) were pre-processed or selected for specific logical complexity, but the specific impact of polysemy on the models' ability to map NL to FOL was not isolated in the experiments.
- What evidence would resolve it: A controlled experiment using a new dataset version where specific sentences are modified to introduce polysemy, comparing translation accuracy against the unambiguous baseline.

### Open Question 4
- Question: How does the "lower bound" performance of dialogue-oriented models change when advanced prompt engineering (e.g., few-shot exemplars) is applied?
- Basis in paper: [inferred] The authors explicitly limit their scope by stating they "performed no prompt engineering," and thus treat their reported scores as "lower bounds" on the models' capabilities.
- Why unresolved: It remains unclear if the strong performance of models like O3-MINI is close to the ceiling for this task architecture or if specific prompting strategies could resolve the remaining errors.
- What evidence would resolve it: Re-evaluating the Logical Translation task using optimized few-shot or Chain-of-Thought prompts tailored to FOL syntax to observe score shifts.

## Limitations
- Ontology definition variability: The paper relies on manually defined ontologies (Ω) for each dataset instance, but the process and criteria for constructing these are not fully specified, introducing potential variability across implementations.
- Dataset access constraints: While D_FOLIO is public, D_Stanford requires private access to the Grade Grinder Corpus, limiting reproducibility for one of the two datasets.
- Limited prompt engineering: The study uses zero-shot inference with automatic chain-of-thought prompting, treating results as lower bounds and leaving potential improvements from advanced prompting unexplored.

## Confidence
- **High confidence**: The technical methodology for logical equivalence checking via Z3 and the perturbation generation process are well-specified and verifiable.
- **Medium confidence**: The superiority of O3-MINI in logical translation is supported, but the exact margin depends on ontology definitions and may vary across implementations.
- **Medium confidence**: The claim that embedding-centric models fundamentally struggle with FOL semantics is well-supported, but the distinction from dialogue models could be sharpened with more systematic probing of encoding mechanisms.

## Next Checks
1. **Perturbation semantic distinctness validation**: Manually inspect 20 perturbation sets (F_ms and F_r) to verify that generated formulas are semantically distinct from ground truth and not trivially solvable via syntax alone.
2. **Ontology reproducibility test**: Implement the benchmark on a small subset of D_FOLIO using only the publicly available ontology definitions, comparing results to those reported with full ontology access.
3. **Embedding model encoding probe**: Test whether fine-tuning embedding models on FOL logic improves performance in most similar and ranking tasks, helping determine if the gap reflects architectural limitations or lack of training data.