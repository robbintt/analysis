---
ver: rpa2
title: Wavelet Mixture of Experts for Time Series Forecasting
arxiv_id: '2508.08825'
source_url: https://arxiv.org/abs/2508.08825
tags:
- time
- series
- data
- wavelet
- prediction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes WaveTS, a lightweight time series forecasting
  model that leverages wavelet transforms to decompose data into high- and low-frequency
  components. The method employs a wavelet-based filtering strategy combined with
  a mixture-of-experts framework to capture both periodic and non-stationary patterns
  in the data.
---

# Wavelet Mixture of Experts for Time Series Forecasting

## Quick Facts
- **arXiv ID:** 2508.08825
- **Source URL:** https://arxiv.org/abs/2508.08825
- **Reference count:** 40
- **Primary result:** Achieves state-of-the-art performance on 8 datasets with only 69K-157K parameters

## Executive Summary
WaveTS introduces a novel time series forecasting model that combines wavelet transforms with a mixture-of-experts framework. The approach decomposes time series data into high-frequency and low-frequency components using wavelet filtering, allowing the model to separately capture periodic patterns and non-stationary trends. By leveraging a lightweight architecture with only 69K-157K parameters, WaveTS achieves superior forecasting accuracy compared to much larger models while maintaining computational efficiency. The method demonstrates particular effectiveness on multi-channel datasets through its innovative channel clustering strategy.

## Method Summary
WaveTS employs a wavelet-based filtering strategy to decompose time series data into high-frequency and low-frequency components, which are then processed through separate mixture-of-experts networks. The model uses the Maximal Overlap Discrete Wavelet Transform (MODWT) to extract wavelet coefficients, followed by a threshold-based denoising step that preserves important signal features while removing noise. The high-frequency component captures rapid fluctuations and periodic patterns, while the low-frequency component models long-term trends and non-stationary behavior. These components are processed through distinct expert networks and combined using a gating mechanism to produce final forecasts. The architecture is designed to be lightweight, requiring only 69K parameters for the base model (WaveTS-B) and 157K for the medium variant (WaveTS-M), yet achieves state-of-the-art performance across multiple real-world datasets.

## Key Results
- Achieves state-of-the-art performance on 8 real-world time series datasets
- Requires only 69K parameters for WaveTS-B and 157K for WaveTS-M models
- Demonstrates superior accuracy and computational efficiency, particularly excelling on multi-channel datasets

## Why This Works (Mechanism)
WaveTS works by exploiting the natural separation of time series signals into different frequency components. The wavelet transform effectively decomposes complex time series into high-frequency components that capture periodic patterns and low-frequency components that represent long-term trends. By processing these components separately through specialized expert networks and then combining them through a gating mechanism, the model can capture both rapid fluctuations and gradual changes more effectively than traditional single-path architectures. The threshold-based denoising preserves important signal features while removing noise, and the channel clustering strategy for multi-channel datasets enables the model to learn shared representations across related time series while maintaining individual channel characteristics.

## Foundational Learning

**Wavelet Transform (MODWT)**
- *Why needed:* Decomposes time series into frequency components for separate processing
- *Quick check:* Verify MODWT implementation correctly separates high and low frequency components

**Mixture-of-Experts Framework**
- *Why needed:* Enables specialized processing of different signal components through dedicated experts
- *Quick check:* Confirm gating mechanism properly weights expert contributions

**Threshold-based Denoising**
- *Why needed:* Preserves important signal features while removing noise from wavelet coefficients
- *Quick check:* Validate that thresholding doesn't eliminate significant signal patterns

**Channel Clustering Strategy**
- *Why needed:* Enables effective modeling of relationships across multiple related time series
- *Quick check:* Ensure clustering properly groups related channels without losing individual channel information

## Architecture Onboarding

**Component Map:**
Wavelet Transform -> Threshold-based Denoising -> Mixture-of-Experts (High/Low Frequency) -> Gating Mechanism -> Forecast Output

**Critical Path:**
MODWT decomposition → Denoising → Separate expert processing for high/low frequency components → Gating combination → Final prediction

**Design Tradeoffs:**
The model prioritizes parameter efficiency and computational speed over raw capacity, accepting the tradeoff of potentially missing very complex patterns that larger models might capture. The wavelet decomposition adds preprocessing complexity but enables more effective signal processing.

**Failure Signatures:**
Potential failures include: (1) poor decomposition if MODWT parameters are improperly configured, (2) gating mechanism failure to properly combine expert outputs, (3) thresholding that removes important signal features, and (4) channel clustering that groups unrelated channels or separates related ones.

**First Experiments:**
1. Verify MODWT correctly decomposes a known periodic signal into appropriate frequency components
2. Test gating mechanism with synthetic data where expert contributions are known
3. Validate channel clustering on multi-channel data with obvious relationships

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions for future research.

## Limitations

- Limited evaluation on extremely large-scale datasets with millions of time series
- Performance may degrade on time series with very irregular patterns that don't decompose well with wavelets
- The model's effectiveness on non-stationary time series with rapidly changing characteristics is not extensively explored

## Confidence

- **State-of-the-art performance claims:** High - validated across 8 diverse datasets
- **Parameter efficiency claims:** High - explicit parameter counts provided and compared to baselines
- **Computational efficiency:** Medium - claimed but not extensively benchmarked against all competitors
- **Generalization to unseen patterns:** Low - limited discussion of model behavior on novel time series patterns

## Next Checks

1. Replicate results on at least 2-3 additional time series datasets not included in the original evaluation
2. Benchmark computational performance (inference time, memory usage) against comparable models
3. Test model robustness by introducing synthetic noise patterns and evaluating recovery capability