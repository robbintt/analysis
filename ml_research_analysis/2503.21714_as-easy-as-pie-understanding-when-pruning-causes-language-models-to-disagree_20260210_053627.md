---
ver: rpa2
title: 'As easy as PIE: understanding when pruning causes language models to disagree'
arxiv_id: '2503.21714'
source_url: https://arxiv.org/abs/2503.21714
tags:
- uni00000013
- uni00000056
- uni00000048
- uni00000003
- uni00000011
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PIEs (Pruned Identified Exemplars), a subset
  of data points where pruned and unpruned language models disagree in their predictions.
  The authors find that these PIEs are more frequent and impactful in BERT compared
  to BiLSTM, and that they contain harder, longer, and more semantically complex text
  than the average data point.
---

# As easy as PIE: understanding when pruning causes language models to disagree
## Quick Facts
- arXiv ID: 2503.21714
- Source URL: https://arxiv.org/abs/2503.21714
- Reference count: 40
- Key outcome: Pruning disproportionately harms the most influential and challenging examples, undermining model reliability despite stable average accuracy.

## Executive Summary
This paper introduces PIEs (Pruned Identified Exemplars), a subset of data points where pruned and unpruned language models disagree in their predictions. The authors find that these PIEs are more frequent and impactful in BERT compared to BiLSTM, and that they contain harder, longer, and more semantically complex text than the average data point. Importantly, PIEs include many of the most influential examples for model generalization, meaning pruning disproportionately affects the data points most critical for performance. These findings highlight a hidden cost of pruning: while average accuracy may remain stable, pruning significantly harms the accuracy of the most influential and challenging examples, potentially undermining model reliability in high-stakes applications.

## Method Summary
The authors introduce PIEs (Pruned Identified Exemplars) as data points where pruned and unpruned language models disagree in predictions. They compare BERT and BiLSTM models, analyzing PIE frequency, characteristics (length, complexity), and influence on generalization using influence function approximations. The study focuses on in-distribution examples to assess pruning's impact on model reliability.

## Key Results
- PIEs are more frequent and impactful in BERT than BiLSTM.
- PIEs contain harder, longer, and more semantically complex text than average data points.
- PIEs include many of the most influential examples for model generalization, showing pruning disproportionately harms critical data points.

## Why This Works (Mechanism)
Pruning reduces model capacity, which can lead to prediction disagreements on challenging or influential examples. Influence functions approximate the impact of individual training examples on model predictions, allowing identification of PIEs. However, these approximations may be unreliable for deep models, and the observed differences between architectures may conflate pruning effects with structural differences.

## Foundational Learning
- **Influence Functions**: Estimate how much each training example influences model predictions. Needed to identify PIEs. Quick check: Verify influence estimates correlate with actual leave-one-out retraining.
- **Pruning**: Removing model parameters to reduce size/compute. Needed to create pruned models for comparison. Quick check: Confirm pruning maintains baseline accuracy.
- **Semantic Complexity**: Measures linguistic difficulty (e.g., vocabulary diversity). Needed to characterize PIEs. Quick check: Validate complexity proxies align with human judgments.

## Architecture Onboarding
- **Component Map**: Training data -> Unpruned model -> Pruned model; Unpruned model -> Influence function estimator; Both models -> Prediction disagreement detector -> PIE identification
- **Critical Path**: Influence estimation → PIE identification → Characterization of hardness/complexity → Comparison of pruning effects
- **Design Tradeoffs**: Using influence functions trades computational efficiency for potential approximation error; focusing on in-distribution examples limits generalizability
- **Failure Signatures**: Disagreement analysis may conflate architectural differences with pruning effects; proxy measures for hardness may not reflect true difficulty
- **First Experiments**:
  1. Replicate PIE identification using a different influence function estimator
  2. Test hardness proxies against human judgments of difficulty
  3. Analyze pruning effects on out-of-distribution examples

## Open Questions the Paper Calls Out
None

## Limitations
- Influence function approximations may not accurately reflect true influence in deep models
- Hardness proxies (length, complexity) may not directly correlate with model difficulty or practical relevance
- Comparison between BERT and BiLSTM may conflate architectural differences with pruning effects

## Confidence
- **High**: PIEs can be identified and pruning causes prediction disagreements
- **Medium**: PIEs are harder/longer/more complex than average (proxy measures)
- **Low**: Influence functions reliably identify most critical examples; differences due to pruning not architecture

## Next Checks
1. Replicate PIE identification and disagreement analysis using a different influence function estimator or a ground-truth influence method on smaller models
2. Test whether the hardness proxies (length, complexity) correlate with human judgments of difficulty or downstream task performance on PIEs
3. Extend the analysis to out-of-distribution or adversarially selected examples to assess whether pruning effects generalize beyond in-distribution data