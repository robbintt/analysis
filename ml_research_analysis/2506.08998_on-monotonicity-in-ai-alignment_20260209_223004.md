---
ver: rpa2
title: On Monotonicity in AI Alignment
arxiv_id: '2506.08998'
source_url: https://arxiv.org/abs/2506.08998
tags:
- loss
- monotonicity
- preference
- learning
- pairwise
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates monotonicity properties in comparison-based
  preference learning frameworks used for AI alignment, particularly in language model
  fine-tuning. The authors formalize various flavors of monotonicity (pairwise, individual,
  local/global, score/probability, minimum/gradient-descent) and prove that general
  frameworks like DPO, GPO, and GBT satisfy local pairwise monotonicity under mild
  assumptions.
---

# On Monotonicity in AI Alignment

## Quick Facts
- arXiv ID: 2506.08998
- Source URL: https://arxiv.org/abs/2506.08998
- Reference count: 40
- This paper investigates monotonicity properties in comparison-based preference learning frameworks used for AI alignment.

## Executive Summary
This paper investigates monotonicity properties in comparison-based preference learning frameworks used for AI alignment, particularly in language model fine-tuning. The authors formalize various flavors of monotonicity (pairwise, individual, local/global, score/probability, minimum/gradient-descent) and prove that general frameworks like DPO, GPO, and GBT satisfy local pairwise monotonicity under mild assumptions. They show that adding a preference comparison or intensifying existing comparisons leads to increased score differences between preferred and less-preferred responses. The paper also identifies sufficient conditions for other monotonicity forms, though stronger versions like global pairwise or individual-score monotonicity require more restrictive assumptions. Experimental observations confirm that naive gradient descent may violate monotonicity expectations, motivating this theoretical investigation.

## Method Summary
The authors formalize monotonicity properties in preference learning frameworks and prove theoretical guarantees using the implicit function theorem. They analyze conditions under which various monotonicity properties hold, including local pairwise monotonicity for general frameworks like DPO, GPO, and GBT. The theoretical analysis examines both minimum-finding and gradient-descent optimization procedures, identifying sufficient conditions for different monotonicity variants. The paper also provides empirical observations using Llama 3.1/3.2 models fine-tuned on UltraFeedback, demonstrating instances where preferred response scores decrease despite the monotonic preference signal.

## Key Results
- General preference learning frameworks like DPO, GPO, and GBT satisfy local pairwise monotonicity under mild assumptions
- Adding a preference comparison or intensifying existing comparisons leads to increased score differences between preferred and less-preferred responses
- Stronger monotonicity forms (global pairwise, individual-score) require restrictive assumptions unlikely to hold in practice
- Naive gradient descent can violate individual-score monotonicity, causing preferred response scores to decrease

## Why This Works (Mechanism)

### Mechanism 1: Local Pairwise Monotonicity via Implicit Function Theorem
- **Claim:** If a preference learning model (like DPO or GBT) is at a strict local minimum, adding an infinitesimal preference for response $y$ over $z$ guarantees the score difference $s_{yz}$ increases.
- **Mechanism:** This relies on the Implicit Function Theorem applied to the loss gradient $\nabla \text{LOSS}(\theta) = 0$. The score difference shift is approximated by $-\epsilon (\partial_s \ell) \nabla s_{yz}^T [\nabla^2 \text{LOSS}]^{-1} \nabla s_{yz}$. Because the Hessian $\nabla^2 \text{LOSS}$ is positive definite and $\partial_s \ell < 0$ (Assumption 2), the resulting quadratic form is strictly positive.
- **Core assumption:** The loss Hessian is positive definite (strict convexity locally) and the per-data loss derivative is negative ($\partial_s \ell(s, \text{max } C) < 0$).
- **Evidence anchors:**
  - [abstract] "Under mild assumptions, we prove that such methods still satisfy what we call local pairwise monotonicity."
  - [section 4.2] Theorem 1 establishes the sufficient conditions for local pairwise monotonicity using the implicit function theorem.
  - [corpus] Related work "Generalizing while preserving monotonicity" supports the focus on score differences rather than individual probabilities.
- **Break condition:** Fails if the model is not at a strict local minimum (Hessian not positive definite) or if the loss function derivative is non-negative (violating Assumption 2).

### Mechanism 2: Individual-Score Non-Monotonicity (Likelihood Displacement)
- **Claim:** Guaranteeing that the individual score $s_y$ (or probability) of the preferred response increases requires significantly stronger matrix constraints than pairwise monotonicity.
- **Mechanism:** An increase in the score difference $s_{yz}$ does not imply $s_y$ increases and $s_z$ decreases individually. Mathematically, individual monotonicity depends on the matrix $\nabla s^T [\nabla^2 \text{LOSS}]^{-1} \nabla s$ being "max-diagonally dominant." In high-dimensional LLMs, interference between response gradients makes this dominance unlikely.
- **Core assumption:** The embedding space allows for "max-diagonal dominance," meaning the gradient of a specific response's score dominates cross-response interference terms.
- **Evidence anchors:**
  - [abstract] "The model may actually decrease the probability (and reward) of generating $y$."
  - [section 5.2] Theorem 4 cites "max-diagonal dominance" as a necessary condition, noting it is "very demanding" for large matrices.
  - [corpus] "ComPO" addresses likelihood displacement, confirming this is a recognized failure mode in alignment.
- **Break condition:** Fails when response embeddings are correlated (non-orthogonal), causing the gradient update for the preferred response $y$ to negatively impact its own score due to coupling with $z$.

### Mechanism 3: Gradient Descent Instability
- **Claim:** A single step of gradient descent on a preference pair guarantees pairwise score increase, but does **not** guarantee individual score increases without specific gradient alignments.
- **Mechanism:** The score difference change is proportional to $\|\nabla s_{yz}\|^2$ (always positive). However, the individual score change depends on the alignment $\nabla s_y^T \nabla s_{yz}$. If the gradient for the preferred response is not aligned with the difference gradient, the individual score may drop.
- **Core assumption:** Nil regularization ($R=0$) and specific gradient geometry ($\nabla s_{yz}^T \nabla s_y > 0$).
- **Evidence anchors:**
  - [section 5.4] Theorem 5 formally derives the conditions for gradient-descent monotonicity.
  - [section 1] Figure 1 shows empirical evidence where chosen response scores decrease during gradient steps.
  - [corpus] Explicit corpus signals on "likelihood displacement" validate that this is an empirical reality, not just theory.
- **Break condition:** Fails if the gradients for the chosen and rejected responses are not geometrically aligned with their difference, often due to complex loss landscapes or network architecture specifics.

## Foundational Learning

- **Concept: Bradley-Terry Model & DPO Loss**
  - **Why needed here:** The paper generalizes these specific loss functions ($\ell(s, c) = -\log \sigma(cs)$). Understanding that loss is a function of the *score difference* ($s_{yz}$) is critical to grasping why pairwise monotonicity holds naturally while individual monotonicity does not.
  - **Quick check question:** Does the Bradley-Terry loss optimize the absolute score of a response or the relative difference between two responses?

- **Concept: Implicit Function Theorem**
  - **Why needed here:** The paper's main proofs (Theorems 1 & 2) use this theorem to model how the optimal parameters $\theta^*$ shift infinitesimally when the dataset changes. It connects loss landscape geometry (Hessian) to monotonicity guarantees.
  - **Quick check question:** Why is the positive definiteness of the Hessian matrix required to apply the implicit function theorem in this context?

- **Concept: Diagonal Dominance**
  - **Why needed here:** This is the key mathematical property distinguishing pairwise monotonicity (easy) from individual monotonicity (hard). It describes whether a system's variables affect themselves more than they affect neighbors.
  - **Quick check question:** Why is max-diagonal dominance difficult to achieve in the large response space ($A$) of Large Language Models?

## Architecture Onboarding

- **Component map:** Scoring Function ($s_{y|x}(\theta)$) -> Loss Function ($\ell(s_{yz}, c)$) -> Optimization Loop
- **Critical path:**
  1. Select a preference pair $(x, y, z)$.
  2. Compute score difference $s_{yz}$.
  3. Apply loss $\ell$ and backpropagate.
  4. Verify if $s_y$ increased (not guaranteed) vs $s_{yz}$ increased (guaranteed locally).
- **Design tradeoffs:**
  - **Pairwise vs. Individual:** Algorithms optimize relative ordering (pairwise) efficiently. Optimizing absolute individual probabilities requires restrictive architectural constraints (diagonal dominance) that typically conflict with generalization.
  - **Global vs. Local:** Global monotonicity guarantees require strong convexity (Assumption 4), which is restrictive for deep nets. Local monotonicity is the realistic guarantee for DPO/GBT.
- **Failure signatures:**
  - **Likelihood Displacement:** The preferred response $y$ becomes *less* likely after training on a preference saying $y > z$. (Identified in Abstract & Fig 1).
  - **Gradient Misalignment:** $\nabla s_y \cdot \nabla s_z$ is large and positive, causing the optimization to suppress $z$ while inadvertently suppressing $y$ as well.
- **First 3 experiments:**
  1. **Replicate Figure 1:** Run DPO on Llama models with single pairs; plot histograms of $\Delta s_y$ and $\Delta s_z$ to confirm non-monotonicity in individual scores.
  2. **Pairwise Verification:** On the same run, plot $\Delta s_{yz}$ to confirm it is strictly positive (verifying Theorem 1).
  3. **Hessian Analysis:** For a small model, inspect the matrix $\nabla s^T [\nabla^2 \text{LOSS}]^{-1} \nabla s$ to check for max-diagonal dominance violations when individual monotonicity fails.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can novel loss functions or model architectures be designed to provably guarantee individual-score monotonicity in deep language models?
- **Basis in paper:** [inferred] The paper concludes that "local individual-score monotonicity is highly unlikely to hold" for current algorithms due to the restrictive max-diagonal dominance condition required by Theorem 4.
- **Why unresolved:** The authors prove local pairwise monotonicity but find that individual scores (e.g., for the preferred response) can still decrease, which current theory does not prevent.
- **What evidence would resolve it:** A formal proof showing a new algorithm satisfies individual-score monotonicity without requiring impractical matrix properties.

### Open Question 2
- **Question:** Under what specific conditions does gradient descent optimization (as opposed to theoretical minima) satisfy monotonicity?
- **Basis in paper:** [explicit] The authors "document an empirical setting where individual gradient-descent monotonicity fails" (Figure 1) and provide sufficient conditions in Theorem 5.
- **Why unresolved:** The paper's main theoretical results focus on the properties of local minima (using the implicit function theorem) rather than the trajectory of finite-step gradient descent used in practice.
- **What evidence would resolve it:** Identification of learning rate bounds or gradient properties that guarantee monotonic behavior during the training trajectory.

### Open Question 3
- **Question:** Do non-monotonic learning dynamics systematically incentivize annotators to misreport their true preferences?
- **Basis in paper:** [explicit] The introduction states that counter-intuitive decreases in preferred response probabilities "may even incentivize annotators to misreport their true preferences, in high-stakes applications."
- **Why unresolved:** The paper investigates the root causes of non-monotonicity but does not model the game-theoretic interaction between the model's learning dynamics and annotator strategy.
- **What evidence would resolve it:** A game-theoretic analysis or user study demonstrating that non-monotonicity reduces the utility of truthful reporting.

## Limitations

- Theoretical guarantees for stronger monotonicity forms (global, individual, probability-based) rely on restrictive conditions unlikely to hold in practice
- Experimental validation focuses on observing monotonicity violations rather than quantifying their frequency or severity
- Analysis assumes idealized loss landscapes that may not fully capture complex optimization dynamics in large-scale models

## Confidence

- **High confidence:** Theoretical guarantees for local pairwise monotonicity are well-supported with clear assumptions and proofs
- **Low-Medium confidence:** Stronger monotonicity forms (global, individual, probability-based) rely on restrictive conditions unlikely to hold in practice
- **Medium confidence:** Experimental observations demonstrate monotonicity violations but don't quantify their frequency or severity

## Next Checks

1. **Empirical Frequency Analysis**: Quantify how often individual-score monotonicity violations occur during standard DPO fine-tuning across different model scales and datasets.
2. **Regularization Impact Study**: Test whether specific regularization schemes (beyond nil regularization) can enforce stronger monotonicity properties without sacrificing alignment quality.
3. **Alternative Architecture Evaluation**: Compare monotonicity properties across different scoring architectures (MLP vs. cross-attention based) to identify structural factors that promote or inhibit individual monotonicity.