---
ver: rpa2
title: LLM Routing with Dueling Feedback
arxiv_id: '2510.00841'
source_url: https://arxiv.org/abs/2510.00841
tags:
- perf
- cost
- excel
- learning
- mpnet
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of dynamically selecting the best
  large language model for each query, balancing user satisfaction, model expertise,
  and inference cost. It introduces a new formulation using contextual dueling bandits
  with preference feedback instead of absolute scores, making the approach label-efficient
  and adaptive to changing environments.
---

# LLM Routing with Dueling Feedback
## Quick Facts
- arXiv ID: 2510.00841
- Source URL: https://arxiv.org/abs/2510.00841
- Authors: Chao-Kai Chiang; Takashi Ishida; Masashi Sugiyama
- Reference count: 40
- This paper tackles the challenge of dynamically selecting the best large language model for each query, balancing user satisfaction, model expertise, and inference cost. It introduces a new formulation using contextual dueling bandits with preference feedback instead of absolute scores, making the approach label-efficient and adaptive to changing environments. The key technical contribution is Category-Calibrated Fine-Tuning (CCFT), which learns model embeddings from offline data using contrastive fine-tuning with categorical weighting to better capture model expertise. These embeddings enable the practical use of Feel-Good Thompson Sampling for Contextual Dueling Bandits (FGTS.CDB), a theoretically grounded posterior-sampling algorithm. Four variants of categorical weighting are proposed to integrate performance and cost. Experiments on the RouterBench and MixInstruct datasets show that the proposed methods achieve lower cumulative regret and faster convergence than strong baselines using OpenAI embeddings, demonstrating better robustness and a favorable performance-cost balance.

## Executive Summary
This paper addresses the challenge of dynamically routing user queries to the most appropriate large language model while balancing satisfaction, expertise, and cost. The authors introduce a novel formulation using contextual dueling bandits with preference feedback, which is more label-efficient than traditional absolute-score approaches. The key innovation is Category-Calibrated Fine-Tuning (CCFT), a method that learns model embeddings from offline preference data using contrastive fine-tuning with categorical weighting. These embeddings enable the use of Feel-Good Thompson Sampling for Contextual Dueling Bandits (FGTS.CDB), a theoretically grounded posterior-sampling algorithm. Experiments on two benchmark datasets demonstrate superior performance compared to strong baselines, achieving lower cumulative regret and faster convergence while maintaining a favorable performance-cost balance.

## Method Summary
The proposed approach tackles LLM routing as a contextual dueling bandit problem, where the agent must select among multiple LLMs for each query based on contextual information. Instead of requiring absolute quality scores, the method learns from pairwise preference feedback, making it more label-efficient. The core innovation is Category-Calibrated Fine-Tuning (CCFT), which generates model embeddings by contrastive fine-tuning on offline preference data with categorical weighting schemes. These embeddings capture model expertise across different query categories while accounting for performance and cost trade-offs. The method then employs Feel-Good Thompson Sampling for Contextual Dueling Bandits (FGTS.CDB), which uses these embeddings to make probabilistic routing decisions that balance exploration and exploitation. Four variants of categorical weighting are proposed to integrate different aspects of model performance and cost into the embedding learning process.

## Key Results
- The proposed methods achieve lower cumulative regret and faster convergence than strong baselines using OpenAI embeddings on RouterBench and MixInstruct datasets
- CCFT-generated embeddings demonstrate better robustness and adaptability to changing environments compared to static embedding approaches
- The preference-based feedback mechanism enables label-efficient learning while maintaining competitive performance
- The approach demonstrates a favorable performance-cost balance by effectively integrating cost considerations into the routing decisions

## Why This Works (Mechanism)
The method works by learning rich model embeddings that capture the nuanced expertise of different LLMs across various query categories. By using contrastive fine-tuning with categorical weighting on offline preference data, the embeddings encode not just absolute performance but relative strengths and weaknesses of models. The dueling bandit formulation allows the system to learn from pairwise comparisons rather than absolute scores, reducing labeling requirements while still capturing meaningful quality differences. FGTS.CDB then leverages these embeddings to make principled probabilistic routing decisions that balance exploration of potentially better models against exploitation of known good choices, adapting to changing query distributions and model performance over time.

## Foundational Learning
- **Contextual Dueling Bandits**: A reinforcement learning framework for sequential decision-making where feedback comes in the form of pairwise preferences rather than absolute scores; needed to handle the label-efficient nature of preference feedback and model the relative quality of LLM choices.
- **Contrastive Fine-Tuning**: A self-supervised learning technique that learns representations by contrasting similar and dissimilar pairs; needed to create meaningful embeddings that capture model expertise from pairwise preference data.
- **Thompson Sampling**: A Bayesian approach to balancing exploration and exploitation in sequential decision problems; needed to make principled probabilistic routing decisions that adapt to uncertainty.
- **Categorical Weighting**: A method for incorporating multiple factors (performance, cost) into a single embedding space; needed to balance the trade-off between model quality and inference cost.
- **Posterior Sampling in Bandits**: A technique for maintaining and updating beliefs about arm quality in bandit problems; needed to enable the algorithm to adapt to changing environments and model performance.

## Architecture Onboarding
- **Component Map**: Offline preference data -> CCFT model -> Model embeddings -> FGTS.CDB routing algorithm -> LLM selection -> User query -> Preference feedback (loop)
- **Critical Path**: User query → Context extraction → Embedding lookup → FGTS.CDB decision → LLM API call → Response generation → Preference collection
- **Design Tradeoffs**: Pairwise preferences vs absolute scores (label efficiency vs information richness), exploration vs exploitation (learning vs performance), cost integration (simplicity vs optimality)
- **Failure Signatures**: High regret indicates poor embedding quality or bandit algorithm miscalibration; slow convergence suggests insufficient exploration or poor initial embeddings; cost overruns indicate suboptimal weighting of cost factors
- **3 First Experiments**: 1) Compare CCFT vs OpenAI embeddings on a small query set with known ground truth preferences, 2) Ablation study removing cost weighting to quantify its impact, 3) Stress test with distribution shift to evaluate adaptation speed

## Open Questions the Paper Calls Out
None

## Limitations
- The approach assumes access to offline preference data for model embeddings, which may not be available in all scenarios
- The focus on pairwise preference feedback, while label-efficient, may not capture the full spectrum of user satisfaction metrics that could be ordinal or continuous
- Experimental validation is primarily limited to synthetic routing scenarios using established benchmarks, with limited exploration of real-world deployment challenges

## Confidence
**High Confidence**: The theoretical framework for contextual dueling bandits with preference feedback is well-established and the proposed CCFT method for learning model embeddings is technically sound and reproducible.

**Medium Confidence**: The empirical results showing improved regret and convergence rates are convincing on the tested benchmarks, but generalizability to production environments with different query distributions and model pools remains uncertain.

**Low Confidence**: The claim about "favorable performance-cost balance" lacks comprehensive cost analysis across different deployment scales and does not account for computational overhead of the embedding generation process.

## Next Checks
1. **Ablation on embedding quality**: Systematically evaluate the impact of different embedding methods (OpenAI vs CCFT) on routing performance across varying amounts of offline preference data to quantify the value proposition of the proposed fine-tuning approach.

2. **Real-world deployment study**: Conduct a field deployment study measuring actual user satisfaction, cost savings, and system latency in a production setting with dynamic model availability and heterogeneous query patterns.

3. **Robustness to distribution shift**: Test the algorithm's performance when the query distribution changes significantly over time, evaluating whether the preference-based learning approach adapts more quickly than absolute-score baselines under concept drift scenarios.