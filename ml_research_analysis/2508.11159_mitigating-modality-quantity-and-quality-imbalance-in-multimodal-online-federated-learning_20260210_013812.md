---
ver: rpa2
title: Mitigating Modality Quantity and Quality Imbalance in Multimodal Online Federated
  Learning
arxiv_id: '2508.11159'
source_url: https://arxiv.org/abs/2508.11159
tags:
- modality
- data
- imbalance
- quality
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses modality quantity and quality imbalance in
  Multimodal Online Federated Learning (MMO-FL) within IoT environments. Due to unstable
  sensors, some modalities may be missing or degraded in quality during data collection,
  negatively impacting learning performance.
---

# Mitigating Modality Quantity and Quality Imbalance in Multimodal Online Federated Learning

## Quick Facts
- arXiv ID: 2508.11159
- Source URL: https://arxiv.org/abs/2508.11159
- Reference count: 36
- This paper proposes the Modality Quantity and Quality Rebalanced (QQR) algorithm to address modality imbalance in Multimodal Online Federated Learning (MMO-FL), achieving sublinear regret and higher test accuracy under imbalance conditions.

## Executive Summary
This paper addresses modality quantity and quality imbalance in Multimodal Online Federated Learning (MMO-FL) within IoT environments. Due to unstable sensors, some modalities may be missing or degraded in quality during data collection, negatively impacting learning performance. To address this, the authors propose the Modality Quantity and Quality Rebalanced (QQR) algorithm, which uses prototype learning to compensate for missing data and correct low-quality modality representations in real time. Theoretical analysis shows the proposed method achieves sublinear regret under imbalance conditions. Experiments on two real-world datasets (UCI-HAR and MVSA-Single) demonstrate that QQR outperforms baseline methods, achieving higher test accuracy under both quantity and quality imbalance scenarios. Quantized prototype uploads further reduce communication overhead with minimal performance loss.

## Method Summary
The QQR algorithm addresses modality imbalance in MMO-FL through three components: Online Global Prototype Construction (OGPC) creates cumulative class-conditional prototypes from high-quality local samples; Prototypical Quantity Rebalancing (PNR) substitutes missing modality features with their corresponding class prototypes; and Prototypical Quality Rebalancing (PLR) regularizes low-quality modality features toward their prototypes via Prototype Cross Entropy (PCE) loss. The method operates in an online federated learning setting where clients perform local training with E iterations before uploading quantized prototypes or model weights to a central server for aggregation. The algorithm maintains theoretical guarantees of sublinear regret while improving practical performance under various imbalance scenarios.

## Key Results
- QQR achieves higher test accuracy than baseline methods under both quantity and quality imbalance scenarios on UCI-HAR and MVSA-Single datasets
- Theoretical analysis proves sublinear regret bounds O(√T + T·ρ_max) where ρ_max is bounded by imbalance ratios
- Communication overhead reduced by 64%-93% through quantized prototype uploads (b=2-8 bits) with minimal performance loss
- PNR and PLR components individually improve performance, with combined QQR showing synergistic effects

## Why This Works (Mechanism)

### Mechanism 1: Prototypical Quantity Rebalancing (PNR)
- Claim: Replacing missing modality feature embeddings with cumulative global prototypes recovers learning performance under quantity imbalance.
- Mechanism: When `pt,m_k,n = 0` (modality missing), substitute the encoder output with the class-matched cumulative global prototype `v̄t,m_c(k,n)`. This ensures all samples contribute to gradient updates regardless of modality availability.
- Core assumption: Cumulative global prototypes approximate the true class-conditional feature distributions sufficiently to serve as meaningful surrogates.
- Evidence anchors:
  - [abstract]: "prototypical quantity rebalancing" addresses "differences in sensor reliability or operation result in unequal amounts of data collected for each modality"
  - [Section V-C]: Regret bound includes term `T·γ_max·(N-N_min)/N`; Theorem 3 commentary states "a more practical approach is to compensate for missing data to raise N_min toward N"
  - [Section VII-F]: PNR outperforms Zero Padding and Incomplete Subset baselines on both datasets
  - [corpus]: Related work FedMobile (arXiv:2502.15839) similarly uses prototype-based knowledge transfer for incomplete modalities, suggesting cross-validation of the approach
- Break condition: When imbalance ratio λ_p is severe (e.g., 0.5), performance degrades substantially even with PNR (Figure 5b shows MVSA-Single drop), indicating prototype quality degrades when too few samples inform their construction.

### Mechanism 2: Prototypical Quality Rebalancing (PLR)
- Claim: Regularizing low-quality modality features toward their corresponding cumulative global prototypes reduces the effective noise term ρ_max in the regret bound.
- Mechanism: When `qt,m_k = 0` (low quality detected), augment the standard cross-entropy loss with Prototype Cross Entropy (PCE): `L_PCE = -log[exp(-d(z̃, v̄_c)) / Σ_c' exp(-d(z̃, v̄_c'))]`. This pulls noisy representations toward their expected semantic positions.
- Core assumption: Quality status can be detected (via sensor monitoring), and cumulative prototypes remain relatively uncontaminated by low-quality samples due to the filtering condition Φ_g in Equation 14.
- Evidence anchors:
  - [abstract]: "prototypical quality rebalancing" compensates for "low-quality data through online global prototype construction"
  - [Section V-B]: Theorem 2 shows regret bound O(√T + T·ρ_max); commentary states "if appropriate algorithms are designed such that ρ_max decreases with T, the overall regret bound remains sublinear"
  - [Section VII-F]: PLR achieves higher test accuracy than Bare Quality baseline, with gap widening over rounds as prototypes stabilize
  - [corpus]: Weak direct evidence—corpus papers address quality imbalance primarily in offline settings (e.g., "Rebalanced Multimodal Learning" arXiv:2503.03792)
- Break condition: If quality imbalance ratio δ_r is very high (>0.7), cumulative prototypes may become unreliable if too few high-quality samples are available for construction.

### Mechanism 3: Cumulative Global Prototype Construction (OGPC)
- Claim: Exponentially-weighted averaging of per-round global prototypes provides stable, bias-resistant reference representations for both PNR and PLR.
- Mechanism: Server computes `v̄t,m_c = [(t-1)·v̄t-1,m_c + vt,m_c] / t`, where `vt,m_c` is the mean of local prototypes from clients who have high-quality samples for class c. Law of large numbers ensures convergence toward true class centers.
- Core assumption: Sufficient rounds will include at least some clients with high-quality data for each class-modality pair; local prototypes are normalized for cross-modality comparability.
- Evidence anchors:
  - [Section VI-A]: "According to the law of large numbers, as the number of samples increases, the sample mean tends to converge to the true population mean"
  - [Section VI-A]: Two principles enforced—"unified structure" across modalities and "normalized" magnitudes for stability
  - [Section VII-F]: Quantization experiments (b=2,4 bits) show minimal performance loss, indicating prototypes are robust to precision reduction
  - [corpus]: Cross-modal prototype learning in MFCPL (referenced in Related Work) validates prototype-based knowledge transfer, though in offline FL setting
- Break condition: If certain class-modality combinations are never observed with high quality, their prototypes never form; clients cannot rebalance those missing/low-quality modalities.

## Foundational Learning

- Concept: **Online Convex Optimization and Regret Analysis**
  - Why needed here: MMO-FL optimizes cumulative loss over streaming data; regret bounds quantify how imbalance affects theoretical performance guarantees. Theorems 1-3 derive explicit bounds that motivate algorithm design.
  - Quick check question: Given a sublinear regret bound O(√T + T·ρ_max), under what condition on ρ_max does average regret still approach zero as T→∞?

- Concept: **Prototype Learning in Metric Space**
  - Why needed here: PNR and PLR both operate by comparing features to class prototypes in Euclidean space. Understanding distance-based classification and representation alignment is essential.
  - Quick check question: Why must prototypes from different modalities share a "unified structure" and be normalized before aggregation?

- Concept: **Federated Averaging with Local Iterations**
  - Why needed here: The MMO-FL workflow uses multiple local gradient descent steps (E > 1) before aggregation. Theoretical analysis accounts for gradient drift across local iterations (φ term in bounds).
  - Quick check question: How does increasing the number of local iterations E affect the regret bound in Theorem 1, and what tradeoff does this imply?

## Architecture Onboarding

- Component map:
  - Client side: Modality encoders (θ_1...θ_M) → Feature embeddings (Z) → Head encoder (θ_0) → Predictions. Each client also computes local prototypes when data meets quality criteria (Φ_g).
  - Server side: Aggregates local prototypes into instantaneous global prototypes (Equation 15), maintains cumulative prototypes (Equation 16), and distributes prototype collection V̄^t to clients.
  - PNR module: Intercepts feature embeddings; replaces missing modalities with prototype-matched substitutes before head encoder.
  - PLR module: Modifies loss function to include PCE term when quality indicator q_k^t,m = 0.

- Critical path:
  1. Client collects multimodal data with availability (p) and quality (q) indicators
  2. Client computes local prototypes for high-quality samples only (Equation 13)
  3. Client uploads local prototypes (optionally quantized)
  4. Server updates cumulative prototype collection
  5. Client downloads V̄^t, applies PNR (feature substitution) and PLR (loss modification)
  6. Client performs E iterations of online gradient descent
  7. Client uploads model weights; server aggregates

- Design tradeoffs:
  - **Quantization level (b)**: Lower bits reduce communication (Figures 8-9) but introduce approximation error in prototype representations
  - **PCE weight (β)**: Higher β more aggressively corrects low-quality data but may over-regularize and reduce model flexibility
  - **Prototype update frequency**: Every-round updates provide fresher prototypes but increase communication; could batch updates at cost of staleness

- Failure signatures:
  - Sudden accuracy drop mid-training: Check if quality imbalance ratio δ_r increased (prototype quality degraded) or if certain classes stopped contributing high-quality samples
  - PNR underperforms Zero Padding: Indicates prototypes may not be properly normalized or unified across modalities—verify encoder outputs have consistent dimensionality and magnitude
  - PLR shows no improvement over BQ baseline: Quality indicator q may not be correctly signaling low-quality rounds, or β may be set too low
  - Communication costs unexpectedly high: Verify clients are only uploading prototypes when Φ_g condition is satisfied, not every round regardless of data quality

- First 3 experiments:
  1. **Ablation on λ_p and λ_r separately**: Fix quality parameters (δ_r=0, β=0.5), vary quantity imbalance ratios (λ_p ∈ {0.5, 0.7, 0.9}, λ_r ∈ {0.3, 0.5, 0.7}) to isolate PNR effectiveness and identify breaking points.
  2. **PCE loss weight (β) sensitivity**: Fix quantity parameters, vary β ∈ {0.1, 0.3, 0.5, 0.7, 1.0} under moderate quality imbalance (δ_r=0.5) to find optimal regularization strength without over-constraining.
  3. **Quantization robustness test**: Run PNR and PLR with b ∈ {2, 4, 8, 16, 32} bits, plot accuracy vs. communication cost curves to determine practical operating points for bandwidth-constrained IoT deployment.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but the following limitations and future work directions are implied:

## Limitations
- Theoretical regret bounds assume oracle knowledge of imbalance parameters and convex loss functions, which may not hold in practice
- Prototype construction requires sufficient high-quality samples for each class-modality pair, which may not be available under extreme imbalance conditions
- Quality detection mechanism for q_{t,m}^k is assumed but implementation details are not specified
- Experiments only validate performance on two-modality datasets, leaving generalization to higher-dimensional modality spaces unverified

## Confidence

- **High confidence**: Regret bound derivations and their dependence on imbalance parameters; PNR and PLR algorithmic structures; communication efficiency gains from quantization
- **Medium confidence**: Empirical performance improvements over baselines; prototype aggregation convergence properties; practical effectiveness under moderate imbalance conditions
- **Low confidence**: Theoretical guarantees under extreme imbalance ratios (λ_p < 0.5); prototype quality maintenance when class coverage is highly uneven; exact mechanism for quality indicator q_{t,m}^k in real deployments

## Next Checks

1. Test QQR under extreme quantity imbalance (λ_p = 0.3) to identify breaking points where prototype-based compensation fails
2. Implement and evaluate practical quality detection mechanisms (e.g., SNR-based thresholds) to replace assumed oracle quality indicators
3. Analyze prototype coverage statistics across rounds to verify sufficient class-modality representation for stable cumulative prototype construction