---
ver: rpa2
title: Pardon? Evaluating Conversational Repair in Large Audio-Language Models
arxiv_id: '2601.12973'
source_url: https://arxiv.org/abs/2601.12973
tags:
- audio
- repair
- semantic
- conversational
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the gap in evaluation of large audio-language
  models (LALMs), which focus on answer accuracy and robustness to acoustic perturbations
  but ignore conversational repair behavior when inputs become semantically unanswerable.
  To fill this gap, the authors introduce a repair-aware evaluation setting that explicitly
  distinguishes answerable from unanswerable spoken inputs using a semantic-acoustic
  masking protocol, where answer-critical semantic content is selectively removed.
---

# Pardon? Evaluating Conversational Repair in Large Audio-Language Models

## Quick Facts
- arXiv ID: 2601.12973
- Source URL: https://arxiv.org/abs/2601.12973
- Reference count: 17
- This work addresses the gap in evaluation of large audio-language models (LALMs) by introducing a repair-aware evaluation setting and EAR score that jointly assesses task competence and conversational repair behavior.

## Executive Summary
This work addresses a significant gap in large audio-language model (LALM) evaluation by introducing a framework that explicitly assesses conversational repair behavior when inputs become semantically unanswerable. The authors develop a semantic-acoustic masking protocol to create answerable and unanswerable inputs, and propose the Evaluability Awareness and Repair (EAR) score as a non-compensatory metric. Experiments on spoken QA benchmarks reveal that while many models achieve high accuracy on answerable inputs, most fail to recognize semantic unanswerability and do not initiate appropriate repair, exposing a substantial gap between accuracy-centric evaluation and real-world conversational reliability.

## Method Summary
The authors introduce a repair-aware evaluation framework for LALMs that distinguishes between answerable and unanswerable spoken inputs through semantic-acoustic masking. This protocol selectively removes answer-critical semantic content while preserving acoustic signals. They propose the EAR score as a non-compensatory metric that jointly evaluates task competence under answerable conditions and conversational repair under unanswerable conditions. The framework is tested on two spoken QA benchmarks, revealing that most current models fail to recognize when inputs are semantically unanswerable and do not initiate appropriate repair behaviors, despite achieving high accuracy on answerable inputs.

## Key Results
- Most LALMs achieve high accuracy on answerable inputs but fail to recognize semantic unanswerability
- Models that exhibit repair behavior (like DeSTA2.5-Audio and Audio Flamingo 3) achieve substantially higher EAR scores
- Current accuracy-centric evaluation practices substantially overestimate real-world reliability of LALMs
- There is a significant gap between task competence and conversational repair capabilities in existing models

## Why This Works (Mechanism)
The framework works by explicitly creating a controlled environment where models must distinguish between answerable and unanswerable inputs, forcing them to demonstrate conversational repair capabilities rather than simply answering every question. The semantic-acoustic masking protocol creates a clear binary distinction that models cannot resolve through sophisticated reasoning alone, requiring them to recognize unanswerability and respond appropriately.

## Foundational Learning
- **Semantic-acoustic masking**: Why needed - to create controlled unanswerable inputs while preserving acoustic structure; Quick check - verify masking preserves prosodic features while removing semantic content
- **Non-compensatory evaluation metrics**: Why needed - to ensure both competence and repair are assessed independently; Quick check - test metric behavior when one dimension is high and the other is low
- **Conversational repair theory**: Why needed - to define what constitutes appropriate repair behavior; Quick check - validate repair response categories against human judgments

## Architecture Onboarding

**Component map**: Masking protocol -> LALM input -> Answer generation -> Repair detection -> EAR score calculation

**Critical path**: Masking protocol creation → Model response generation → Answerability assessment → Repair behavior evaluation → EAR score computation

**Design tradeoffs**: The framework trades comprehensiveness for specificity - focusing narrowly on repair behavior rather than general conversational ability, which provides clearer evaluation but may miss other important aspects of conversational competence.

**Failure signatures**: Models may: 1) Answer unanswerable questions with fabricated content, 2) Remain silent without acknowledging unanswerability, 3) Produce generic responses that don't address the unanswerability, or 4) Fail to recognize semantic gaps while preserving acoustic information.

**3 first experiments**:
1. Test model responses to progressively more severely masked inputs to establish a repair capability gradient
2. Compare EAR scores across models with different pretraining objectives (audio-only vs. audio-language)
3. Evaluate whether fine-tuning on repair-aware data improves EAR scores without degrading answerable performance

## Open Questions the Paper Calls Out
None specified in the source material.

## Limitations
- The semantic-acoustic masking protocol may not fully capture the complexity of real-world conversational repair scenarios
- EAR score may still be influenced by factors beyond conversational repair capabilities, such as general language proficiency
- The evaluation focuses on spoken QA benchmarks, which may not represent all conversational contexts where repair is crucial

## Confidence

High confidence: The general approach of introducing repair-aware evaluation and EAR score is well-founded and addresses a real gap in current practices

Medium confidence: The effectiveness of semantic-acoustic masking in eliciting genuine repair behavior requires further validation

Medium confidence: Results showing accuracy-reliability gaps are compelling but may be influenced by specific benchmark choices

## Next Checks

1. Expand evaluation to include diverse conversational contexts and tasks beyond spoken QA to assess EAR score generalizability

2. Conduct human evaluations to validate the semantic-acoustic masking protocol's effectiveness in eliciting genuine repair behavior

3. Investigate the relationship between EAR scores and real-world performance in conversational AI systems to establish practical relevance