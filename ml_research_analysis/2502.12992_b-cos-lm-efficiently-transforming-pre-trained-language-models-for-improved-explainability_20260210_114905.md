---
ver: rpa2
title: 'B-cos LM: Efficiently Transforming Pre-trained Language Models for Improved
  Explainability'
arxiv_id: '2502.12992'
source_url: https://arxiv.org/abs/2502.12992
tags:
- b-cos
- explanations
- explanation
- language
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces B-cos LM, a method for transforming pre-trained
  language models into inherently explainable models by removing bias terms and enforcing
  input-weight alignment through B-cos transformations. Unlike post-hoc explanation
  methods, B-cos LMs generate explanations that are both more faithful (e.g., +14.63
  Comp score over strongest post-hoc baselines) and more interpretable to humans,
  while maintaining task performance comparable to conventional fine-tuning.
---

# B-cos LM: Efficiently Transforming Pre-trained Language Models for Improved Explainability

## Quick Facts
- arXiv ID: 2502.12992
- Source URL: https://arxiv.org/abs/2502.12992
- Authors: Yifan Wang; Sukrut Rao; Ji-Ung Lee; Mayank Jobanputra; Vera Demberg
- Reference count: 40
- Pre-trained language models transformed into inherently explainable models via B-cos transformations achieve +14.63 Comp score over post-hoc baselines while maintaining task performance

## Executive Summary
B-cos LM introduces a method for converting pre-trained language models into inherently explainable architectures by removing bias terms and enforcing input-weight alignment through B-cos transformations. Unlike post-hoc explanation methods, B-cos LMs generate explanations that are both more faithful (e.g., +14.63 Comp score over strongest post-hoc baselines) and more interpretable to humans, while maintaining task performance comparable to conventional fine-tuning. The method is shown to be effective on encoder-only models for classification tasks and extended to decoder-only models for generation tasks, demonstrating improved explainability across different architectures.

## Method Summary
B-cos LM transforms pre-trained language models by removing all bias terms from transformer components (attention, layer normalization, prediction heads) and replacing linear layers with B-cos transformations that enforce input-weight alignment. The method applies architectural modifications (bias removal, B-cos layers, remove prediction head activations) to frozen PLM weights, then fine-tunes jointly on downstream tasks using BCE loss. This leverages pre-trained representations while learning alignment under task supervision, achieving both improved explainability and task performance.

## Key Results
- B-cos LM achieves +14.63 Comp score improvement over strongest post-hoc explanation baselines
- Maintains task performance comparable to conventional fine-tuning (within 3% on IMDB, AG News)
- Converges faster than conventional fine-tuning (4.33K vs 6.67K steps)
- Effectively extended to decoder-only models (GPT-2, Llama) for generation tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: B-cos transformations enforce input-weight alignment, producing explanations that faithfully reflect model reasoning rather than post-hoc approximations.
- Mechanism: Replace standard linear layers f(x;w,b) = w^Tx + b with B-cos(x;w) = ŵ^Tx × |cos(x,ŵ)|^(B-1). This bounds output magnitude by ||x|| and forces weights to align with task-relevant patterns to maximize activation. The alignment pressure parameter B > 1 exponentially amplifies alignment differences.
- Core assumption: Higher input-weight correlation correlates with more faithful attribution to relevant features.
- Evidence anchors:
  - [abstract] "+14.63 Comp score over strongest post-hoc baselines"
  - [section 3.1] "the weight w must align closely with task-relevant patterns to achieve a high cosine similarity and strong activation"
  - [corpus] "Gender Bias in Explainability" paper confirms post-hoc methods have performance disparity issues, supporting the need for inherent explainability
- Break condition: Setting B too large (>2.0) over-constrains capacity, degrading task performance and producing overly sparse explanations.

### Mechanism 2
- Claim: Removing all bias terms from transformer components ensures W(x)x completely summarizes the model's computation.
- Mechanism: Eliminate bias from attention blocks, layer normalization, and prediction heads. Without b(x), f(x) = W(x)x exactly, so attribution sum equals the prediction output.
- Core assumption: Bias terms introduce unaccounted variance that cannot be cleanly attributed to input features.
- Evidence anchors:
  - [abstract] "removing bias terms and promoting input-weight alignment"
  - [section 3.2.1] "For completeness and faithfulness of explanations, we...remove all bias terms in models"
  - [corpus] No direct corpus evidence on bias removal specifically for NLP
- Break condition: Removing prediction head activations (tanh/sigmoid) without B-cos compensation causes numerical instability when x ≈ 0.

### Mechanism 3
- Claim: Converting pre-trained LMs via combined B-cos adaptation + task fine-tuning is more efficient than separate pre-training or "task then B-cos" approaches.
- Mechanism: Apply architectural modifications (bias removal, B-cos layers, remove prediction head activations) to frozen PLM weights, then fine-tune jointly on downstream task. This leverages pre-trained representations while learning alignment under task supervision.
- Core assumption: Pre-trained weights provide sufficient initialization; joint optimization does not destabilize alignment learning.
- Evidence anchors:
  - [section 5, Table 4] B-cos LM converges in 4.33K steps vs 6.67K for conventional fine-tuning
  - [section 5] "combining task fine-tuning and B-cos conversion is the most efficient approach"
  - [corpus] Weak evidence; corpus neighbors don't address training efficiency
- Break condition: For complex/diverse language tasks, larger B values cause overfitting to spurious correlations (e.g., label bias in HateXplain at B=2.5).

## Foundational Learning

- Concept: Dynamic Linear Models
  - Why needed here: The paper frames neural networks as piecewise affine transformations where W(x) changes per input; understanding this enables comprehension of why B-cos explanations are "complete."
  - Quick check question: Given f(x) = W(x)x + b(x), what makes attribution to W(x)x incomplete when b(x) ≠ 0?

- Concept: Post-hoc vs Inherent Explainability
  - Why needed here: The core motivation is that post-hoc methods (LIME, SHAP, gradients) approximate reasoning rather than extract it, leading to faithfulness failures.
  - Quick check question: Why does perturbing top-k tokens and measuring probability drop (Comprehensiveness) not guarantee the original explanation reflected model reasoning?

- Concept: Transformer Component Linearization
  - Why needed here: The paper claims all transformer components can be expressed as or converted to dynamic linear modules; verifying this requires understanding attention, normalization, and activations as matrix operations.
  - Quick check question: How can softmax attention Att(X) = softmax(X^TQ^TKX)VX be written as W(X)X for some weight matrix W(X)?

## Architecture Onboarding

- Component map: Load PLM → Remove biases → Convert linear→B-cos → Remove prediction activations → Fine-tune
- Critical path: 1. Load pre-trained PLM (BERT/DistilBERT/RoBERTa or GPT-2/Llama) 2. Remove all bias parameters from attention, layer norm, and head 3. Convert all linear layers to B-cos with weight normalization 4. Remove prediction head activations for classification 5. Initialize B-cos weights from original PLM weights (not random) 6. Fine-tune with BCE loss on downstream task
- Design tradeoffs:
  - B ∈ [1.0, 2.0]: Lower preserves capacity; higher increases faithfulness but risks overfitting to spurious correlations
  - BCE vs CE loss: BCE explicitly maximizes logit magnitude, strengthening alignment
  - Combined vs staged training: Combined is faster; staged (B-cos pre-train then task fine-tune) achieves slightly better performance with more compute
- Failure signatures:
  - Numerical instability in W(x) when using sigmoid/tanh without removal (|x| ≈ 0 causes division issues)
  - Over-sparse explanations at B > 2.0: most tokens receive near-zero attribution, failing Comp/SeqPG
  - Bias amplification: On imbalanced datasets, high B increases prediction positive rate and relies on non-semantic tokens ([CLS]/[SEP])
  - Spurious correlation overfitting: e.g., "black" → higher toxic attribution at B=2.5
- First 3 experiments:
  1. **Sanity check**: Convert BERT-base to B-cos with B=1.25, train on IMDB (binary sentiment). Verify accuracy within 3% of baseline and Comp score improves over gradient-based methods.
  2. **B-value sweep**: Train B-cos BERT on HateXplain with B ∈ {1.0, 1.25, 1.5, 2.0, 2.5}. Plot accuracy, Comp, and explanation entropy to identify task-specific optimal B.
  3. **Ablation of components**: Remove each B-cos adaptation (bias removal, BCE loss, prediction head activation removal, alignment pressure) independently on HateXplain to measure faithfulness degradation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the inherent explainability of B-cos LMs be leveraged to actively detect and mitigate spurious correlations or societal biases learned from training data?
- Basis in paper: [explicit] The authors note in Section 6 and the Conclusion that B-cos explanations facilitate the detection of spurious correlations but leave the exploration of bias mitigation to future work.
- Why unresolved: While the paper demonstrates that B-cos LMs highlight potentially spurious correlations (e.g., specific identity terms), it does not test methods for using these signals to update the model or reduce bias.
- What evidence would resolve it: Experiments showing that a debiasing algorithm applied to B-cos LMs reduces bias metrics (e.g., fairness disparity) more effectively than when applied to conventional models.

### Open Question 2
- Question: Can B-cosification be integrated directly into the pre-training phase of Large Language Models to eliminate the need for separate post-hoc conversion training?
- Basis in paper: [explicit] Section 7 states that future work could explore integrating B-cosification into the pre-training phase to reduce the training overhead required by the current pipeline.
- Why unresolved: The current method requires applying B-cos adaptations and then fine-tuning/conversion, which adds computational steps compared to using a pre-trained model directly.
- What evidence would resolve it: A study training a B-cos model from scratch or modifying the pre-training objective to include alignment pressure, comparing the final performance and training cost against the proposed conversion method.

### Open Question 3
- Question: How can B-cos architectures be modified to better capture compositional semantics, such as negation phrases, where current token-level attributions are inconsistent?
- Basis in paper: [explicit] Section 10 (Limitations) states that B-cos explanations do not consistently capture token interactions within multi-token phrases (e.g., "not good"), treating tokens individually.
- Why unresolved: The current dynamic linear mechanism aligns weights to individual input features, struggling to assign a cohesive attribution score to a phrase whose meaning differs from its constituent words.
- What evidence would resolve it: An architectural extension that allows for sub-word or phrase-level alignment, resulting in consistent attributions for standard negation benchmarks.

## Limitations
- Optimal B value appears task-specific rather than universal, requiring hyperparameter tuning
- Limited evidence for complex language understanding tasks or reasoning-intensive applications
- Implementation complexity in computing W(x) through all transformer layers without numerical instability
- Token-level explanations struggle with compositional semantics like negation phrases

## Confidence
- **High Confidence**: B-cos transformations enforce input-weight alignment and improve faithfulness over post-hoc methods; removing bias terms makes explanations more complete and faithful; method maintains comparable task performance at moderate B values; converges faster than conventional fine-tuning
- **Medium Confidence**: B-cos explanations are more interpretable to humans than post-hoc explanations; method generalizes from encoder-only to decoder-only architectures; combined fine-tuning is more efficient than staged approaches
- **Low Confidence**: B-cos LM eliminates the need for post-hoc explanations entirely; method scales to complex, diverse language tasks without degradation; B-cos transformations preserve all useful representational capacity of pre-trained models

## Next Checks
1. **Ablation Study with Individual B-cos Components**: Perform a comprehensive ablation where each B-cos adaptation (bias removal, BCE loss, prediction head activation removal, alignment pressure B) is removed independently across all three classification tasks. This would quantify the relative contribution of each component to faithfulness improvements and identify which modifications are essential versus beneficial.

2. **Extended Decoder Model Evaluation**: Implement and evaluate B-cos LM on Llama-3.2-1B for both classification (IMDB) and generation tasks (BLiMP, IOI) with comprehensive faithfulness metrics. This would validate the method's generalizability to larger decoder architectures and assess whether explainability gains transfer from encoder to decoder models.

3. **Cross-task B-value Optimization**: Conduct a systematic study across diverse tasks (sentiment analysis, topic classification, NLI, summarization) to identify patterns in optimal B values. This would test whether B-cos LM requires task-specific tuning or if there are principled guidelines for selecting B based on task characteristics like label distribution or input length.