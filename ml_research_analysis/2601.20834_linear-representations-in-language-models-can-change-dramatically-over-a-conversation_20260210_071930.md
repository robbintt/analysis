---
ver: rpa2
title: Linear representations in language models can change dramatically over a conversation
arxiv_id: '2601.20834'
source_url: https://arxiv.org/abs/2601.20834
tags:
- representations
- conversation
- questions
- factuality
- linear
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work explores how language model representations of high-level
  concepts, such as factuality and ethics, evolve over the course of a conversation.
  By identifying linear dimensions in model representations that correspond to these
  concepts, the authors analyze how these dimensions change as conversations progress.
---

# Linear representations in language models can change dramatically over a conversation

## Quick Facts
- arXiv ID: 2601.20834
- Source URL: https://arxiv.org/abs/2601.20834
- Reference count: 39
- Key finding: Linear representations of high-level concepts like factuality and ethics in LLMs can flip dramatically over conversations, even reversing from factual to non-factual, while generic knowledge remains stable.

## Executive Summary
This paper demonstrates that language models' internal representations of high-level concepts undergo significant, systematic changes during conversations. Using linear probes to track "factuality" and "ethics" dimensions, the authors show that these representations can flip from positive to negative margins (indicating inversion of meaning) when exposed to role-cuing contexts like opposite-day prompts or sustained conversations on specific topics. Critically, this flipping occurs for conversation-relevant concepts while generic factual knowledge remains stable, suggesting selective adaptation rather than general degradation. The findings challenge interpretability methods that assume fixed meaning of internal representations across contexts.

## Method Summary
The authors extract residual stream activations at answer token positions across all layers, then fit L2-regularized logistic regression probes to detect linear directions corresponding to concepts like factuality. They train probes on generic yes/no questions in empty contexts (plus robust probes including opposite-day variants), select the best layer via holdout performance, and track margin scores (separation between factual and non-factual answers) over conversation turns. Conversations are replayed turn-by-turn, with representations extracted after each turn to monitor how margin scores evolve for both generic and context-relevant questions.

## Key Results
- Representations of conversation-relevant concepts (e.g., factuality about consciousness) can flip from positive to negative margins over the course of a conversation, while generic representations remain stable
- Steering interventions along these representation directions can have opposite effects at different points in a conversation—improving factuality early but worsening it later
- These dynamics occur across multiple model families and layers, and persist even when replaying conversations written by other models

## Why This Works (Mechanism)

### Mechanism 1: Role-Playing Driven Representation Restructuring
Models dynamically restructure internal representations to align with conversational roles they adopt, rather than maintaining fixed conceptual mappings. When a conversation cues a particular role (e.g., arguing for consciousness), attention mechanisms prioritize role-consistent patterns, causing the linear direction that previously correlated with "factuality" to now correlate with "role-appropriate assertions"—even when those conflict with ground truth. This is supported by findings that end-of-conversation correction prompts partially reverse shifts, suggesting rapid role adaptation rather than permanent change.

### Mechanism 2: Selective Content-Dependent Adaptation
Representations adapt selectively—conversation-relevant concepts shift while generic/world-knowledge representations remain preserved. The model maintains parallel representation streams: a stable "core knowledge" subsystem for context-independent facts, and a context-sensitive "conversational state" subsystem that tracks the current discourse frame. The linear probe primarily captures directions in the latter for topic-relevant questions, explaining why generic factuality remains intact while conversation-specific factuality flips.

### Mechanism 3: In-Context Emergence of Alternative Linear Structures
Linear representation directions don't simply "rotate"—new linear structures emerge through in-context learning from conversation patterns. Per Ravfogel et al., linear truth encodings emerge from co-occurrence statistics. In a conversation where the model consistently makes non-factual claims, these claims co-occur with each other, creating a new local linear direction that the probe (trained on generic data) misclassifies. This suggests representations reorganize based on abstract co-occurrence patterns rather than just surface tokens.

## Foundational Learning

- **Linear Representation Hypothesis**: Why needed: The entire paper assumes concepts like "factuality" can be captured as directions in activation space. Quick check: Can you explain why a logistic regression on activations can predict whether an answer is factual?

- **Residual Stream Architecture**: Why needed: The paper extracts representations from "the residual stream (after each layer block)" and analyzes layerwise emergence. Quick check: At what position in the sequence does the paper extract activations, and why does this matter for measuring representations of both factual and non-factual answers?

- **Construct Validity in Interpretability**: Why needed: The core argument is that a direction labeled "factuality" in one context may not measure factuality in another. Quick check: If a probe trained on context A achieves 90% accuracy on context B, does this guarantee the probe measures the same concept in both contexts? (Hint: the paper says no.)

## Architecture Onboarding

- **Component map**: Input (conversation + question) → Tokenizer → Transformer layers (residual stream after each block) → Target layer selection (via holdout validation) → Activation extraction at answer token position → Logistic regression probe → Margin score
- **Critical path**: Layer selection is critical—the paper finds factuality becomes "reliably decodable" around layer 24-26 in Gemma 27B (roughly 1/3 through). Earlier layers are transparent in plots because decodability is too weak.
- **Design tradeoffs**:
  - Extracting at answer token vs. before: Answer token lets you measure how both factual and non-factual answers are represented (model processes both). Pre-answer extraction enables causal steering but only measures the path the model was going to take.
  - Robust probe training (including opposite-day) vs. simple probe: Robust probes disentangle "what the model thinks is correct behavior" from "what is factually true," but still flip in conversations—tradeoff doesn't solve the core problem.
  - Generic vs. context-relevant question sets: Generic questions establish baseline; context-relevant questions reveal dynamics. Both are needed.
- **Failure signatures**:
  - Probe trained on empty-context data achieves high accuracy on held-out generic questions but negative margin (worse than chance) on conversation-specific questions → representation has flipped.
  - Steering intervention produces expected behavioral change in empty context but opposite change after conversation → intervention direction has inverted meaning.
  - High variance across CCS runs or sub-90% accuracy even on generic questions → representation direction is not robustly recoverable.
- **First 3 experiments**:
  1. Reproduce opposite-day flip: Train factuality probe on generic questions in empty context. Test on same questions after opposite-day prompt (3 turns). Verify margin flips negative.
  2. Test conversation replay: Take a pre-written conversation (e.g., consciousness or chakras). Replay onto model. Extract representations for both generic and context-relevant questions at each turn. Plot margin over time to confirm selective adaptation pattern.
  3. Layerwise emergence check: At conversation end, extract activations from all layers for both question types. Verify that once factuality becomes decodable (~layer 24+), the flip pattern is consistent across remaining layers.

## Open Questions the Paper Calls Out
- **Mechanistic processes**: The authors explicitly state they have not established the mechanisms by which these representational changes occur. What specific attention patterns or circuit-level processes drive the reorganization of linear representations during context accumulation?
- **Generalizability to other concepts**: While the study focused on factuality and ethics, the authors suggest future work should explore whether similar dynamics occur for other high-level concepts or in non-conversational contexts like large codebases.
- **Impact on SAE methods**: The paper argues that SAEs fundamentally assume consistent representation meaning across contexts, a hypothesis their results challenge. How do dynamic context-dependent shifts impact the assumption of feature consistency in methods like Sparse Autoencoders?

## Limitations
- Generalizability to non-QA tasks remains untested—all analyses focus on yes/no questions where binary classification is natural
- The underlying cause (role-playing vs. genuine belief updating) cannot be definitively distinguished from behavioral data alone
- The linear probe methodology assumes that "factuality" is a coherent direction in activation space, which may not hold for more nuanced or context-dependent factual claims

## Confidence

- **High Confidence**: That representations of conversation-relevant concepts can flip systematically during conversations; that generic representations remain relatively stable while context-relevant ones shift; that steering interventions have opposite effects at different conversation points
- **Medium Confidence**: That these dynamics are primarily driven by role-playing rather than belief updating; that selective preservation of generic information reflects separable representation streams
- **Low Confidence**: That these findings generalize to non-QA tasks, multi-turn reasoning, or continuous generation scenarios; that the robust probe fully disentangles "model's belief about correctness" from "ground truth"

## Next Checks

1. **Cross-Task Generalization Test**: Apply the same representation analysis methodology to a non-QA task such as multi-turn reasoning or open-ended generation. Track whether representation directions for core concepts like logical consistency or task completion also flip over the course of the task, or whether the phenomenon is specific to binary QA contexts.

2. **Intervention Timing Experiment**: Systematically vary when steering interventions are applied during conversations (early, middle, late turns) and measure both immediate behavioral effects and whether the intervention itself causes the representation to flip. This would help distinguish between representations responding to role cues versus responding to intervention history.

3. **Representation Trajectory Analysis**: Instead of only measuring final margins, track the full trajectory of representation evolution turn-by-turn using techniques like dimensionality reduction or probing for intermediate states. This could reveal whether flips occur gradually, suddenly, or through identifiable intermediate patterns, providing stronger evidence for specific mechanisms like role adaptation versus random drift.