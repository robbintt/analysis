---
ver: rpa2
title: 'Beyond Task-Oriented and Chitchat Dialogues: Proactive and Transition-Aware
  Conversational Agents'
arxiv_id: '2511.08835'
source_url: https://arxiv.org/abs/2511.08835
tags:
- dialogue
- chitchat
- intent
- user
- transition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TACT, a dataset designed to support transition-aware
  conversational agents that can handle fluid switches between task-oriented dialogue
  and chitchat. Unlike existing datasets, TACT includes structurally diverse flows
  with multiple mode transitions and recoverable structures, enabling agents to learn
  proactive and context-sensitive dialogue control.
---

# Beyond Task-Oriented and Chitchat Dialogues: Proactive and Transition-Aware Conversational Agents

## Quick Facts
- arXiv ID: 2511.08835
- Source URL: https://arxiv.org/abs/2511.08835
- Reference count: 35
- Achieved 75.74% joint mode-intent accuracy and 70.1% win rate against GPT-4o in human evaluation

## Executive Summary
This paper introduces TACT, a dataset designed to support transition-aware conversational agents that can handle fluid switches between task-oriented dialogue and chitchat. Unlike existing datasets, TACT includes structurally diverse flows with multiple mode transitions and recoverable structures, enabling agents to learn proactive and context-sensitive dialogue control. The authors propose two new metrics—Switch and Recovery—to measure an agent's ability to initiate and successfully return from mode transitions. Experiments show that models trained on TACT outperform baselines in intent detection and transition handling. Applying Direct Preference Optimization (DPO) further improves response quality and naturalness, achieving 75.74% joint mode-intent accuracy and a 70.1% win rate against GPT-4o in human evaluation. These results demonstrate that pairing diverse, transition-rich data with preference-based fine-tuning enhances both task accuracy and conversational fluency.

## Method Summary
The paper constructs the TACT dataset by combining MultiWOZ 2.2 (7,199 dialogues) and SLURP (9,936 dialogues), creating two flow patterns: TCT (TOD→Chitchat→TOD) and CTC (Chitchat→TOD→Chitchat). The FnCTOD architecture is used, which employs function-calling for unified intent prediction and response generation in a single auto-regressive pass. The model is initialized from LLAMA-3.1-8B-INSTRUCT and trained using supervised fine-tuning (SFT) with 3 epochs, learning rate 1×10^-5, batch size 256, and DeepSpeed ZeRO-3 bf16. DPO is then applied using 3,009 preference pairs judged by GEMINI-2.5-PRO on four criteria: sensibleness, specificity, interestingness, and transition naturalness.

## Key Results
- Achieved 75.74% joint mode-intent accuracy on TACT
- DPO fine-tuning improved win rate to 70.1% against GPT-4o in human evaluation
- Switch and Recovery metrics demonstrate effective transition handling in TCT flows

## Why This Works (Mechanism)
The paper's approach works by providing structurally diverse dialogue flows with explicit transition signals, enabling models to learn when and how to switch between task-oriented and chitchat modes. The introduction of recoverable structures allows agents to practice returning to task completion after digression, while the Switch and Recovery metrics provide targeted evaluation of transition capabilities.

## Foundational Learning
- **Dialogue Mode Classification**: Understanding how to distinguish between task-oriented and chitchat modes is essential for appropriate response generation.
- **Intent Detection**: Accurate intent recognition enables proper task completion in TOD mode.
- **Transition Detection**: Identifying natural breakpoints for mode switches ensures smooth conversational flow.
- **Recoverable Structures**: Learning to return to TOD after chitchat digression maintains task completion while allowing natural conversation.
- **Preference-Based Fine-Tuning**: Using human preference judgments to improve response quality and naturalness.

## Architecture Onboarding
- **Component Map**: FnCTOD (Function-Calling + TOD) -> Intent Prediction -> Response Generation
- **Critical Path**: Intent detection → Mode selection → Response generation → Transition handling
- **Design Tradeoffs**: Unified auto-regressive approach simplifies training but may limit specialized optimization for each mode.
- **Failure Signatures**: Over-triggering transitions (high attempts, low success) or failure to recover to TOD after chitchat.
- **First Experiments**: 1) Train on TACT without transitions to establish baseline performance. 2) Train with TCT flows only to test structured transitions. 3) Apply DPO to evaluate preference-based improvements.

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset construction relies heavily on automated generation with GPT-4O-MINI validation, with undocumented quality thresholds.
- Experimental evaluation limited to two flow patterns (TCT and CTC) and two base datasets, lacking testing on more complex or realistic dialogue scenarios.
- DPO fine-tuning process omits key hyperparameters including beta values, number of epochs, and learning rate schedules.

## Confidence
- **High Confidence**: Core dataset construction methodology, Switch and Recovery metrics, and basic SFT training procedure.
- **Medium Confidence**: Performance improvements (75.74% joint accuracy, 70.1% win rate) depend on undocumented DPO aspects.
- **Low Confidence**: Claims about real-world applicability not substantiated by testing on diverse, realistic dialogue scenarios.

## Next Checks
1. Re-run DPO fine-tuning with explicit specification of beta values, number of epochs, and learning rate schedules to verify reproducibility of performance gains.
2. Test trained model on dialogues from datasets not used in TACT construction (e.g., Schema-Guided Dialogue) to evaluate robustness to unseen conversational patterns.
3. Re-analyze human evaluation results by calculating inter-annotator agreement to quantify reliability of the 70.1% win rate against GPT-4o.