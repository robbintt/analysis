---
ver: rpa2
title: Emergence of Minimal Circuits for Indirect Object Identification in Attention-Only
  Transformers
arxiv_id: '2510.25013'
source_url: https://arxiv.org/abs/2510.25013
tags:
- head
- attention
- layer
- token
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates the minimal computational mechanisms required
  for solving the Indirect Object Identification (IOI) task in transformers. The author
  trains attention-only transformer models from scratch on a simplified, symbolic
  version of IOI, where the model must predict the indirect object token given a sequence
  containing two names and a subject marker.
---

# Emergence of Minimal Circuits for Indirect Object Identification in Attention-Only Transformers

## Quick Facts
- arXiv ID: 2510.25013
- Source URL: https://arxiv.org/abs/2510.25013
- Authors: Rabin Adhikari
- Reference count: 19
- Key outcome: Single-layer, two-head attention-only transformers can solve IOI via interpretable additive-contrastive circuits.

## Executive Summary
This paper investigates the minimal computational mechanisms required for solving the Indirect Object Identification (IOI) task in transformers. The author trains attention-only transformer models from scratch on a simplified, symbolic version of IOI, where the model must predict the indirect object token given a sequence containing two names and a subject marker. Surprisingly, a single-layer model with just two attention heads achieves perfect accuracy, demonstrating that IOI can be solved without MLPs or normalization layers. Mechanistic analysis reveals that the two heads specialize into distinct roles: one performs additive aggregation of the name tokens, while the other executes contrastive suppression of the incorrect name. This additive-contrastive circuit enables clean, interpretable IOI resolution. Additionally, a two-layer, single-head model achieves similar performance by composing information across layers through query-value interactions, suggesting a hierarchical mechanism. The results demonstrate that task-specific training induces highly interpretable, minimal circuits, offering a controlled testbed for understanding the computational foundations of transformer reasoning.

## Method Summary
The study trains attention-only transformer models from scratch on a symbolic IOI task with a tiny vocabulary (6 names + 2 special tokens). Models are evaluated on their ability to predict the indirect object token given a sequence containing two names and a subject marker. Training uses cross-entropy loss with AdamW optimizer and OneCycle learning rate scheduler. The analysis focuses on residual stream decomposition and spectral analysis of QK/OV circuits to understand how different architectures solve the task.

## Key Results
- A single-layer, two-head attention-only transformer achieves 100% accuracy on symbolic IOI.
- The two heads specialize into additive (aggregation) and contrastive (subtraction) subcircuits that jointly implement IOI resolution.
- Two-layer, single-head models achieve similar performance through query-value composition across layers, with Q-composition being most critical.
- Single-head models fail at ~50% accuracy due to uniform attention that averages out name distinctions.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Two attention heads can implement a complete IOI circuit through additive-contrastive specialization.
- Mechanism: Head 0 aggregates both name tokens additively (output aligned with "correct + incorrect" direction). Head 1 computes the contrastive difference (output aligned with "correct - incorrect" direction). When summed in the residual stream, the incorrect token's logit contribution cancels, amplifying the correct token.
- Core assumption: The embedding space permits linear decomposition along semantically meaningful directions (sum and difference of name embeddings).
- Evidence anchors:
  - [abstract] "the two heads specialize into additive and contrastive subcircuits that jointly implement IOI resolution"
  - [Section 3.2.2] "the first head's output is aligned closest with the sum direction... the second head's output aligns closest with the direction of the token difference"
  - [corpus] Related IOI work (Wang et al. 2023) found multi-hop circuits in GPT-2; this shows single-layer sufficiency under task-constrained training.
- Break condition: If embeddings are not approximately linearly separable, or if the vocabulary grows beyond the capacity of two orthogonal directions to encode all pairwise contrasts.

### Mechanism 2
- Claim: Single-head models fail because "reference detection" and "copying" are functionally incompatible within one attention mechanism.
- Mechanism: A single head attends uniformly to both names (≈50/50). The OV circuit shows each name contributes positively to its own logit and negatively to the other's. Uniform attention causes these to average out, yielding indistinguishable logits.
- Core assumption: Attention softmax normalization forces competition; a single head cannot simultaneously encode positional role distinctions and perform selective copying.
- Evidence anchors:
  - [Section 3.1] "The model attends roughly equally to both the names in the dependent clause... indicating that a single attention head cannot jointly encode the information required"
  - [Section 3.1, Figure 1] QK circuit shows uniform attention; OV circuit shows symmetric self-boosting with weak cross-suppression.
  - [corpus] No direct corpus corroboration; this functional incompatibility claim is specific to this paper's analysis.
- Break condition: If a different positional encoding scheme or non-softmax attention allows sharper discrimination, this incompatibility may not hold.

### Mechanism 3
- Claim: Two-layer composition enables IOI resolution through query-value (Q-V) interactions across layers.
- Mechanism: Layer 0 aggregates information to the S2 token position. Layer 1's query attends based on Layer 0's output, enabling context-dependent retrieval. Ablation shows Q-composition is most critical (100% drop), followed by V-composition (93% drop).
- Core assumption: Layer 0 writes information that Layer 1's query can access; without this cross-layer communication, each layer operates independently.
- Evidence anchors:
  - [Section 3.3] "we observe a drop in accuracy in the following order: Q composition (≈100% drop), V composition (≈93.33% drop), and K composition (≈26.67% drop)"
  - [Section 3.3] "the first head is not solely positional, but aggregates information to S2 token to be used by the latter head"
  - [corpus] Elhage et al. (2021) introduced Q/K/V composition terminology; this paper operationalizes it for IOI.
- Break condition: If layers are prevented from reading each other's residual stream contributions, or if gradients cannot propagate through composition paths during training.

## Foundational Learning

- Concept: **QK and OV circuits** (Elhage et al. 2021)
  - Why needed here: The entire analysis framework depends on decomposing attention into "what to attend to" (QK) and "what to output given attention" (OV). Spectral analysis of these matrices reveals functional specialization.
  - Quick check question: Given an attention head, can you explain why high positive eigenvalues in the OV matrix indicate "copying" behavior?

- Concept: **Residual stream decomposition**
  - Why needed here: The paper decomposes final logits into contributions from each component (embeddings, each head) and projects onto interpretable directions (correct/incorrect/sum/difference).
  - Quick check question: If Head 0 outputs vector v₀ and Head 1 outputs v₁, what does it mean if v₀ · (e_correct + e_incorrect) is large but v₀ · (e_correct - e_incorrect) is near zero?

- Concept: **Eigendecomposition of attention matrices**
  - Why needed here: The paper uses eigenvalue spectra to classify heads as "additive" (positive eigenvalues → copying) vs "contrastive" (mixed/negative eigenvalues → subtraction with rotation).
  - Quick check question: Why would a matrix with all positive eigenvalues behave differently from one with large negative eigenvalues when applied to token embeddings?

## Architecture Onboarding

- Component map: `<BOS> IO S1 S2 <MID>` sequence → embedding layer → attention-only layers → unembedding → logits
- Critical path:
  1. Name tokens embed into residual stream
  2. Attention heads read from residual stream via QKV projections
  3. Head outputs write additively back to residual stream
  4. Final unembedding projects residual stream at `<MID>` to logits
  5. Cross-entropy loss against the IO token

- Design tradeoffs:
  - **One layer, two heads**: Parallel specialization; cleaner interpretability; requires sufficient head dimension for orthogonal subspaces
  - **Two layers, one head**: Enables cross-layer composition; more flexible but requires analyzing layer-to-layer information flow
  - **Positional embeddings**: With them → 100% accuracy, interpretable patterns. Without → ~70% accuracy, less interpretable but still functional

- Failure signatures:
  - **Single-head model**: ~50% accuracy; uniform attention to both names; logit difference near zero
  - **No positional embeddings**: Heads attend similarly; accuracy drops to ~70%
  - **Compositional ablation (Q or V)**: Near-complete accuracy collapse in two-layer model

- First 3 experiments:
  1. **Replicate single-head failure**: Train one-layer, one-head model. Verify ~50% accuracy and uniform attention pattern. Visualize QK/OV circuits to confirm the averaging behavior.
  2. **Two-head specialization analysis**: Train one-layer, two-head model. Use residual stream decomposition to project each head's output onto sum/difference directions. Confirm Head 0 → sum, Head 1 → difference.
  3. **Composition ablation**: Train two-layer, one-head model. Systematically ablate Q, K, and V composition by subtracting Layer 0's output from Layer 1's input to each projection. Quantify accuracy drops to verify Q > V > K importance hierarchy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the additive-contrastive circuits discovered in symbolic IOI generalize to explain the natural language IOI task in pretrained models?
- Basis in paper: [explicit] The paper studies "a symbolic version of the IOI task" that "abstracts away all linguistic and tokenization complexities," and states findings offer "insight into the primitive computational motifs that may underlie reasoning in larger, pretrained language models."
- Why unresolved: The symbolic setting removes tokenization, vocabulary variation, and syntactic structure present in natural language IOI. Whether the same two-head additive-contrastive mechanism operates in pretrained models like GPT-2 remains untested.
- What evidence would resolve it: Apply the same residual stream decomposition and spectral analysis to GPT-2 small on natural language IOI examples; test whether analogous additive and contrastive head specialization exists.

### Open Question 2
- Question: Does multi-task training causally produce more complex circuits than single-task training for the same capability?
- Basis in paper: [explicit] The paper argues that "circuits in large, broadly pre-trained models may be overly complex due to multi-task pressures, whereas task-constrained training can reveal more parsimonious mechanisms."
- Why unresolved: This claim is inferred from comparing the paper's minimal circuits to Wang et al.'s complex GPT-2 circuit, but no controlled experiment compares single-task vs. multi-task training directly.
- What evidence would resolve it: Train models of equivalent capacity on IOI alone versus IOI combined with other language modeling objectives, then measure circuit complexity (number of heads involved, composition depth, interpretability scores).

### Open Question 3
- Question: Are the discovered additive-contrastive circuits the unique solution to symbolic IOI, or do multiple circuit architectures exist?
- Basis in paper: [inferred] The paper reports one successful circuit configuration but acknowledges the residual stream analysis "is also not foolproof," and does not investigate whether different random seeds or architectures yield different solutions.
- Why unresolved: Neural network optimization can find equivalent functions via different internal mechanisms. The paper shows one solution works but doesn't establish uniqueness or characterize the space of valid circuits.
- What evidence would resolve it: Train multiple models with different initializations and head counts; systematically analyze whether alternative circuit structures emerge that achieve perfect accuracy through different mechanistic strategies.

## Limitations

- The symbolic setup uses an extremely small vocabulary (6 names + 2 special tokens), limiting generalizability to larger vocabularies.
- The task is binary in nature, and it's unclear whether the same minimal circuits would scale to more complex reasoning tasks.
- The absence of MLPs and LayerNorm means the circuits are fundamentally different from those in standard pretrained transformers.
- The training procedure converges quickly due to the tiny dataset, raising questions about whether the same specialization patterns would emerge under more challenging data regimes.

## Confidence

- **High Confidence**: The failure of single-head models to solve IOI, and the success of two-head models with additive-contrastive specialization.
- **Medium Confidence**: The two-layer composition mechanism and the Q > V > K ablation hierarchy.
- **Medium Confidence**: The claim that multi-task training in large models leads to unnecessarily complex circuits.

## Next Checks

1. **Robustness to vocabulary scaling**: Extend the symbolic IOI task to 10+ names and retrain the minimal circuits. Verify whether two heads can still encode all pairwise contrasts through orthogonal directions, or whether head count must scale.

2. **Compositional limits**: Train a three-layer, one-head model and perform Q/K/V composition ablations. Determine if the compositional hierarchy (Q > V > K) holds, and whether deeper layers introduce new functional roles.

3. **Training dynamics analysis**: Instrument the training loop to record head specialization trajectories. Track when additive vs contrastive specialization emerges, and whether it coincides with accuracy milestones. This would clarify whether the circuit is a stable attractor or an early-convergence artifact.