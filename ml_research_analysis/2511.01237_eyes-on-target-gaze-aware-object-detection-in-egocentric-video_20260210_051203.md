---
ver: rpa2
title: 'Eyes on Target: Gaze-Aware Object Detection in Egocentric Video'
arxiv_id: '2511.01237'
source_url: https://arxiv.org/abs/2511.01237
tags:
- gaze
- attention
- dataset
- object
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a gaze-aware object detection framework for
  egocentric video that integrates human gaze signals into a Vision Transformer (ViT)
  to improve detection accuracy by biasing attention toward viewer-prioritized regions.
  The method uses gaze position, depth, pupil diameter, and direction to modify self-attention
  weights and dynamically scale bounding boxes for tighter localization.
---

# Eyes on Target: Gaze-Aware Object Detection in Egocentric Video

## Quick Facts
- arXiv ID: 2511.01237
- Source URL: https://arxiv.org/abs/2511.01237
- Authors: Vishakha Lall; Yisi Liu
- Reference count: 0
- Primary result: Gaze-aware object detection framework integrating human gaze signals into Vision Transformer to improve detection accuracy by biasing attention toward viewer-prioritized regions.

## Executive Summary
This paper presents a gaze-aware object detection framework for egocentric video that integrates human gaze signals into a Vision Transformer to improve detection accuracy. The method uses gaze position, depth, pupil diameter, and direction to modify self-attention weights and dynamically scale bounding boxes for tighter localization. Evaluations on custom and public datasets show consistent gains in classification accuracy over baselines like YOLOv7 and DETR, with classification accuracy reaching 0.92-0.94 and mAP@0.5 up to 0.61.

## Method Summary
The framework uses a DETR backbone with ResNet-50/101, modifying the encoder self-attention to incorporate gaze signals. The gaze-modified attention adds a GazeBias term to standard attention weights, with optimal bias strength α=0.7. Dynamic bounding box scaling uses attention scores, inverse pupil diameter, and inverse depth to adjust box size. Training uses bipartite matching loss with CE, L1, and GIoU components. The approach was evaluated on the custom Egocentric Maritime Simulator Dataset (~10 hours, 1M frames), Ego Motion, and Ego-CH-Gaze datasets.

## Key Results
- Classification accuracy reached 0.92-0.94 across datasets, significantly outperforming YOLOv7 and standard DETR baselines
- mAP@0.5 achieved up to 0.61 on maritime dataset with gaze guidance
- Ablation study confirmed all gaze components (position, direction, depth, pupil) contribute to performance, with pupil dilation and depth particularly aiding localization
- Novel gaze-aware attention head importance metric revealed how gaze modulates transformer attention dynamics

## Why This Works (Mechanism)
The framework works by integrating multiple gaze signals into the attention mechanism of a Vision Transformer. By biasing attention toward regions where the viewer is looking (gaze position), considering how close objects are (depth), accounting for focus intensity (pupil diameter), and tracking viewing direction, the model better aligns its attention with human visual priorities. This alignment improves classification accuracy while dynamic box scaling tightens localization around attended objects.

## Foundational Learning
- Vision Transformer self-attention: The core mechanism that processes image patches through learned attention patterns. Why needed: Forms the basis for incorporating gaze signals into the detection pipeline. Quick check: Verify attention weights can be modified and tracked.
- DETR architecture: End-to-end object detection framework using transformers. Why needed: Provides the baseline detection model to enhance with gaze signals. Quick check: Confirm bipartite matching loss implementation works.
- Gaze signal normalization: Converting raw eye tracker measurements to standardized ranges. Why needed: Ensures consistent behavior across different subjects and videos. Quick check: Verify all gaze features fall within [0,1] after normalization.

## Architecture Onboarding

Component Map: Input Video -> Gaze Extraction -> DETR Backbone -> Gaze-Modified Attention -> Dynamic Box Scaling -> Detection Output

Critical Path: Gaze signals (position, depth, pupil, direction) → GazeBias modification of self-attention → Attention-weighted feature maps → Classification and bounding box prediction

Design Tradeoffs: Using multiple gaze signals increases model complexity but improves alignment with human attention; dynamic box scaling may reduce mAP slightly but provides tighter localization around attended objects.

Failure Signatures: Over-concentration of attention at high α values (>0.7) degrades classification; dynamic box scaling may reduce overlap with ground truth despite tighter gaze-aligned boxes.

Three First Experiments:
1. Implement gaze-modified attention with varying α (0.3, 0.5, 0.7, 0.9) and measure classification accuracy vs attention alignment
2. Test dynamic box scaling on public dataset with pseudo-gaze, comparing IoU with gaze-salient regions vs standard DETR
3. Validate pupil diameter normalization across multiple subjects to ensure consistent [0,1] scaling

## Open Questions the Paper Calls Out
None

## Limitations
- Custom Egocentric Maritime Simulator Dataset not publicly available, limiting reproducibility
- Gaze depth extraction method underspecified - unclear if from eye tracker hardware or computed from scene
- Several key training hyperparameters (learning rate, optimizer settings) not explicitly stated
- Input image resolution unspecified, which could significantly impact performance

## Confidence

High Confidence:
- Classification accuracy improvements (0.92-0.94) and overall mAP gains over baselines are well-supported

Medium Confidence:
- Gaze-aware attention head importance metric interpretation depends on normalization details
- Claim that all four gaze components contribute meaningfully is supported by ablation results

## Next Checks

1. Implement gaze-aware attention modification with varying α values and verify classification accuracy peaks around α=0.7 while tracking attention alignment trade-offs.

2. Replicate dynamic bounding box scaling on Ego Motion dataset using pseudo-gaze annotations, evaluating IoU with gaze-salient regions vs full ground truth.

3. Test pupil diameter normalization procedure across multiple subjects to ensure consistent [0,1] ranges and optimal box scaling parameters.