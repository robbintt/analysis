---
ver: rpa2
title: A Simple and Reproducible Hybrid Solver for a Truck-Drone VRP with Recharge
arxiv_id: '2509.18162'
source_url: https://arxiv.org/abs/2509.18162
tags:
- drone
- truck
- alns
- search
- makespan
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces a hybrid solver for the truck-drone vehicle
  routing problem with explicit battery management, where a single drone operates
  at twice the truck speed and must recharge after each delivery. The core method
  combines an Adaptive Large Neighborhood Search (ALNS) for optimizing the truck tour
  with a learned pointer/attention policy for scheduling drone sorties.
---

# A Simple and Reproducible Hybrid Solver for a Truck-Drone VRP with Recharge

## Quick Facts
- arXiv ID: 2509.18162
- Source URL: https://arxiv.org/abs/2509.18162
- Reference count: 23
- One-line primary result: Hybrid RL-ALNS solver achieves 2.73% lower makespan than ALNS on 50-node Euclidean truck-drone routing problems.

## Executive Summary
This study introduces a hybrid solver for the truck-drone vehicle routing problem with explicit battery management, where a single drone operates at twice the truck speed and must recharge after each delivery. The core method combines an Adaptive Large Neighborhood Search (ALNS) for optimizing the truck tour with a learned pointer/attention policy for scheduling drone sorties. The policy decodes launch-serve-rendezvous triplets using feasibility masks for endurance and recharge constraints, evaluated via a fast timeline simulator that computes the true makespan. Experiments on Euclidean instances (N=50, endurance E=0.7, recharge R=0.1) show the hybrid RL solver achieves an average makespan of 5.203±0.093, outperforming ALNS (5.349±0.038) by 2.73% and matching or improving over nearest neighbor heuristics (5.208±0.124). Per-seed analysis confirms the learned scheduler never underperforms ALNS and ties or beats NN on two of three seeds, demonstrating consistent gains. The results highlight the effectiveness of integrating learned drone scheduling with strong classical truck routing heuristics.

## Method Summary
The hybrid solver decomposes the truck-drone VRP into two sub-problems: ALNS optimizes the truck tour to minimize travel distance, while a pointer/attention network schedules drone sorties conditioned on the fixed truck route. The policy decodes sequences of launch-serve-rendezvous triplets, using feasibility masks to enforce endurance (E=0.7) and recharge (R=0.1) constraints. A deterministic timeline simulator computes the exact makespan, which serves as the reward in Self-Critical Sequence Training (SCST). The method assumes a single drone, one-customer sorties, drone speed twice the truck speed, and a depot at the origin. Evaluation uses N=50 random Euclidean instances with 10 random seeds.

## Key Results
- Hybrid RL solver achieves 5.203±0.093 makespan vs ALNS's 5.349±0.038 (2.73% improvement, p<0.05)
- Outperforms nearest neighbor heuristic (5.208±0.124) and matches ALNS in per-seed comparisons
- Learned scheduler never underperforms ALNS and beats NN on two of three seeds
- Strong performance attributed to feasibility-guided decoding and exact simulation for gradient alignment

## Why This Works (Mechanism)

### Mechanism 1: Hybrid Problem Decomposition
Separating spatial routing (truck) from temporal scheduling (drone) allows the solver to apply the strongest available heuristics to the truck tour while reserving the learning capacity for the complex synchronization constraints. The architecture decouples the problem into two sub-problems. First, ALNS optimizes the truck tour to minimize travel distance. Second, the RL policy schedules drone sorties (launch-serve-rendezvous) *conditioned* on that fixed tour. This reduces the RL search space from combinatorial routing to feasible assignment under constraints.

### Mechanism 2: Feasibility-Guided Decoding
Applying hard feasibility masks during the decoding phase prevents the accumulation of invalid actions, ensuring that the learned policy only explores the subspace of physically possible drone sorties. The policy outputs probabilities over potential next-nodes, but a validity mask nullifies probabilities for actions violating endurance or recharge constraints. The mechanism relies on a fast timeline simulator to dynamically check these constraints at each decoding step.

### Mechanism 3: Exact Simulation for Gradient Alignment
Using a deterministic, exact timeline simulator to compute the reward (negative makespan) ensures that the optimization objective perfectly matches the physical reality, avoiding the "surrogate gap" often found in learned value functions. Instead of learning a critic to estimate future cost, the system calculates the exact makespan after a full schedule is decoded. This exact scalar serves as the baseline/reward in Self-Critical Sequence Training (SCST), anchoring gradients in true operational performance.

## Foundational Learning

- **Concept: Adaptive Large Neighborhood Search (ALNS)**
  - **Why needed here:** You cannot understand the truck routing backbone without understanding ruin-and-recreate heuristics. ALNS iteratively destroys parts of the solution (removing customers) and repairs them (re-inserting) to escape local minima.
  - **Quick check question:** If ALNS uses a "Shaw removal" heuristic, what attribute of customers is it likely exploiting to decide which ones to remove? (Answer: Relatedness/proximity).

- **Concept: Pointer Networks / Attention Mechanisms**
  - **Why needed here:** The drone scheduler uses this architecture to output a sequence of nodes. You must understand that attention allows the model to weigh the importance of specific customers relative to the current truck state and drone battery level.
  - **Quick check question:** In a Pointer Network, how does the output distribution differ from a standard classification layer? (Answer: It uses the attention score over input embeddings as the output probability distribution).

- **Concept: Self-Critical Sequence Training (SCST)**
  - **Why needed here:** The paper uses this specific RL algorithm to train the scheduler. SCST uses the reward of the greedy decoded sequence as a baseline to reduce variance in gradient estimates, which is crucial for structured prediction like routing.
  - **Quick check question:** Why is the "greedy rollout" used as a baseline in SCST? (Answer: To reduce variance by comparing the sampled result against the deterministic policy behavior).

## Architecture Onboarding

- **Component map:** Input -> Timeline Simulator -> Truck Solver (ALNS) -> Drone Scheduler (RL Policy) -> Timeline Simulator -> Trainer (SCST)
- **Critical path:** The interaction between the **Policy** and the **Timeline Simulator**. The policy proposes a sortie, the simulator validates it and updates the state (time, battery). If this hand-off is slow or buggy, the entire system fails.
- **Design tradeoffs:**
  - ALNS vs. NN: ALNS yields better tours but is slower. The hybrid method tries to get the best of both.
  - Learned vs. Heuristic Scheduler: A learned scheduler requires training data/time but generalizes better to new instances. A greedy scheduler is instant but myopic.
- **Failure signatures:**
  - Deadlock: The drone is never launched because the policy learned it reduces makespan to keep the drone on the truck.
  - Sync Drift: Truck wait times explode, indicating the drone is consistently arriving late, suggesting the endurance constraint is too tight or the policy ignores recharge time.
- **First 3 experiments:**
  1. Sanity Check (NN+LS+Drone): Run the pure heuristic baseline on a single seed to reproduce the makespan.
  2. Ablation (Truck Only): Run ALNS with a *greedy* drone scheduler to isolate the value of the RL component.
  3. Constraint Stress: Increase recharge time R to 0.5 and observe if the RL policy adapts or performance degrades compared to heuristics.

## Open Questions the Paper Calls Out
- The paper explicitly states the method assumes a single drone and one-customer sorties, noting it is suitable as a baseline for richer variants with multiple drones or multi-stop sorties.

## Limitations
- Hyperparameter dependence: The 2.73% improvement relies on undisclosed training parameters (batch size, learning rate, episode count).
- Timeline simulator precision: Edge cases like overlapping recharge and truck movement may introduce subtle bugs affecting policy learning.
- Limited generalization: Experiments are restricted to N=50 Euclidean instances with fixed E=0.7 and R=0.1 parameters.

## Confidence
- **High Confidence:** The hybrid decomposition approach is sound and well-supported by mechanism description.
- **Medium Confidence:** The 2.73% improvement is statistically significant but depends on unreported training hyperparameters.
- **Low Confidence:** Claims about generalization to different endurance/recharge parameters are speculative without additional experiments.

## Next Checks
1. **Hyperparameter Sensitivity:** Test RL policy with learning rates 0.001, 0.0005, 0.0001 and batch sizes 16, 32, 64 to assess robustness of the 2.73% improvement.
2. **Simulator Unit Tests:** Create test cases for edge scenarios (simultaneous arrivals, overlapping recharge) to verify timeline simulator handles all constraints correctly.
3. **Constraint Stress Test:** Evaluate solver on instances with R=0.5 and E=0.5 to assess whether learned policy degrades more gracefully than ALNS baseline.