---
ver: rpa2
title: Playing Non-Embedded Card-Based Games with Reinforcement Learning
arxiv_id: '2504.04783'
source_url: https://arxiv.org/abs/2504.04783
tags:
- game
- dataset
- learning
- detection
- reinforcement
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a non-embedded reinforcement learning approach
  for playing the card-based RTS game Clash Royale. The authors develop a complete
  pipeline including real-time image capture, object detection, optical character
  recognition, and decision-making on mobile devices.
---

# Playing Non-Embedded Card-Based Games with Reinforcement Learning

## Quick Facts
- arXiv ID: 2504.04783
- Source URL: https://arxiv.org/abs/2504.04783
- Authors: Tianyang Wu; Lipeng Wan; Yuhang Wang; Qiang Wan; Xuguang Lan
- Reference count: 16
- Primary result: 10% win rate against built-in AI using non-embedded RL approach for Clash Royale

## Executive Summary
This paper presents a non-embedded reinforcement learning approach for playing the card-based RTS game Clash Royale. The authors develop a complete pipeline including real-time image capture, object detection, optical character recognition, and decision-making on mobile devices. A generative dataset was created for object detection training due to the lack of existing datasets. The decision model combines spatial and temporal attention mechanisms with delayed action prediction to handle the sparse action problem. Using offline reinforcement learning with expert data, the agent achieves a 10% win rate against built-in AI opponents.

## Method Summary
The approach involves capturing game screenshots in real-time, processing them through a custom-trained object detection model, performing OCR on game elements, and feeding this information into a reinforcement learning decision model. The system uses a combination of spatial and temporal attention mechanisms with delayed action prediction to address the sparse action problem inherent in card-based games. The RL agent is trained offline using expert data, and the entire pipeline is designed to run on mobile devices with minimal latency.

## Key Results
- Achieved 10% win rate against built-in AI opponents
- Decision time of 120ms on mobile devices
- Perception fusion time of 240ms
- Open-sourced all code and datasets

## Why This Works (Mechanism)
The system leverages real-time computer vision to extract game state information, then uses attention mechanisms to focus on relevant spatial and temporal features for decision-making. The delayed action prediction helps address the sparse reward problem common in card games, where meaningful outcomes occur infrequently. Offline reinforcement learning with expert data provides a foundation for learning effective strategies without requiring extensive online interaction.

## Foundational Learning
1. **Object Detection**: Why needed - to identify game elements from screenshots; Quick check - validation on held-out test images
2. **Optical Character Recognition**: Why needed - to read card names and game statistics; Quick check - character-level accuracy metrics
3. **Reinforcement Learning**: Why needed - to learn optimal card-playing strategies; Quick check - reward curves during training
4. **Attention Mechanisms**: Why needed - to focus on relevant game regions and temporal patterns; Quick check - attention weight visualizations
5. **Offline RL**: Why needed - to learn from expert demonstrations without extensive online interaction; Quick check - performance comparison with online RL
6. **Mobile Optimization**: Why needed - to ensure real-time performance on resource-constrained devices; Quick check - latency measurements under different conditions

## Architecture Onboarding

**Component Map**: Screenshot Capture -> Object Detection -> OCR -> State Representation -> Attention Model -> Decision Prediction

**Critical Path**: Game screenshot → Object detection → OCR → State vector → Attention layers → Action selection

**Design Tradeoffs**: The system trades off between model complexity and real-time performance, opting for simpler but faster models that can run on mobile devices. The use of offline RL trades exploration for exploitation of expert knowledge.

**Failure Signatures**: Poor object detection leading to incorrect state representation, OCR errors causing misclassification of cards, attention mechanisms focusing on irrelevant features, or latency exceeding game time constraints.

**3 First Experiments**:
1. Validate object detection performance on a held-out test set of game screenshots
2. Measure OCR accuracy on various game text elements
3. Test end-to-end latency from screenshot capture to action selection

## Open Questions the Paper Calls Out
None

## Limitations
- 10% win rate represents modest performance that may not generalize to human opponents
- Reliance on generated dataset may introduce domain-specific biases
- System performance claims lack comprehensive comparison with other approaches

## Confidence
- High confidence in technical implementation and mobile deployment aspects
- Medium confidence in overall approach and methodology
- Low confidence in generalizability of results to other card-based RTS games

## Next Checks
1. Test the agent against multiple difficulty levels of built-in AI and human opponents to establish robustness
2. Conduct ablation studies to quantify the individual contributions of spatial attention, temporal attention, and delayed action prediction mechanisms
3. Evaluate performance and latency under varying network conditions and device specifications to verify real-world applicability claims