---
ver: rpa2
title: 'Learning What Matters Now: A Dual-Critic Context-Aware RL Framework for Priority-Driven
  Information Gain'
arxiv_id: '2506.06786'
source_url: https://arxiv.org/abs/2506.06786
tags:
- priority
- information
- exploration
- ca-miq
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CA-MIQ (Context-Aware Max-Information Q-learning) is a dual-critic
  RL framework that dynamically adapts to shifting mission priorities in SAR environments.
  It pairs an extrinsic critic for task rewards with an intrinsic critic that combines
  state-novelty, information-location awareness, and priority alignment.
---

# Learning What Matters Now: A Dual-Critic Context-Aware RL Framework for Priority-Driven Information Gain

## Quick Facts
- **arXiv ID:** 2506.06786
- **Source URL:** https://arxiv.org/abs/2506.06786
- **Reference count:** 25
- **Primary result:** CA-MIQ achieves 65.9% mission success after single priority shift in SAR grid-world, outperforming baseline failures.

## Executive Summary
CA-MIQ is a dual-critic reinforcement learning framework designed for search-and-rescue environments where mission priorities shift during operation. It combines an extrinsic critic for task rewards with an intrinsic critic that balances novelty, information-location awareness, and priority alignment. A shift detector triggers transient exploration boosts and selective critic resets when priorities change, enabling rapid adaptation to piecewise-stationary information-value distributions. In simulated SAR grid-worlds, CA-MIQ achieved 100% recovery success while baselines failed to adapt.

## Method Summary
CA-MIQ implements dual-critic tabular Q-learning in a 4×4 grid-world SAR environment. The extrinsic critic Q_E handles task rewards, while the intrinsic critic Q_I combines three components: novelty (30%), information-location awareness (40%), and priority alignment (30%). A shift detector triggers ε-boosting and selective Q-value resets when priorities change. The ε-MaxInfoRL policy selects actions based on the intrinsic critic during exploration and extrinsic critic during exploitation. The framework was evaluated across 100 independent runs with single and multiple priority shifts.

## Key Results
- CA-MIQ achieved 65.9% mission success after a single priority shift, compared to baseline failures
- With multiple priority shifts, CA-MIQ maintained 50.2% mission success
- Recovery success rate reached 100% while baselines failed to adapt
- Adaptation following priority shifts required approximately 600-800 episodes

## Why This Works (Mechanism)
The framework's effectiveness stems from its ability to detect priority shifts and dynamically adjust exploration-exploitation balance. When a shift is detected, ε-boosting temporarily increases exploration while selective resets of Q-values for collection actions allow the agent to unlearn outdated strategies. The intrinsic critic's three-component reward structure ensures the agent maintains awareness of novel states, information locations, and current mission priorities simultaneously.

## Foundational Learning
- **Dual-critic RL:** Uses separate critics for extrinsic task rewards and intrinsic motivation signals. Needed to balance mission completion with adaptive exploration.
- **Priority alignment bonuses/penalties:** Adjusts rewards based on current priority order. Quick check: Verify priority sequence correctly influences reward calculation.
- **Shift detection mechanisms:** Identifies when mission priorities change. Quick check: Confirm shift triggers ε-boosting and resets.
- **Selective critic resetting:** Resets only collection-action Q-values rather than entire table. Quick check: Ensure reset targets correct action subset.
- **Tabular Q-learning with ε-greedy policy:** Baseline RL method for discrete state-action spaces. Quick check: Verify Q-table updates with proper learning rate and discount.

## Architecture Onboarding
- **Component map:** Grid-world environment → Dual-critic Q-learning → Shift detector → ε-MaxInfoRL policy → Action execution
- **Critical path:** Environment state → Q-value calculation (Q_E + Q_I) → Action selection → Reward calculation → Q-update
- **Design tradeoffs:** Tabular representation enables exact novelty counting but limits scalability; fixed intrinsic weights are simple but may not generalize.
- **Failure signatures:** Agent fails to recover after shifts if ε-boosting doesn't trigger or resets target wrong actions; intrinsic reward dominance prevents convergence if β values mis-scaled.
- **First experiments:**
  1. Verify baseline Q-learning works in static environment without priority shifts
  2. Test shift detection triggers correct ε-boosting and reset behavior
  3. Validate intrinsic reward components contribute as expected through ablation studies

## Open Questions the Paper Calls Out
- Can CA-MIQ be extended to continuous state-action spaces while preserving priority-adaptation mechanisms?
- Can adaptive reset scheduling or parameterized exploration boosts reduce 600-800 episode adaptation time?
- Would Bayesian-based or distribution-shift detection methods improve robustness without explicit priority-change signals?
- Can an adaptive weighting mechanism automatically tune intrinsic reward coefficients across diverse mission characteristics?

## Limitations
- Implementation is inherently limited to discrete state-action spaces
- Fixed intrinsic reward coefficients may not generalize to different environments
- Current shift detector requires explicit signals or performance-drop thresholds
- 600-800 episode adaptation time may be too slow for real-world applications

## Confidence
- **Mission success rates (65.9%/50.2%):** Medium - depends on exact parameter choices and controlled environment
- **100% recovery success claim:** Medium - validated only in simulation, not physical SAR platforms
- **Adaptive exploration mechanism (ε-boost + selective reset):** High - procedural description complete and testable
- **Prioritization framework necessity:** Medium - demonstrated in controlled grid-world setting

## Next Checks
1. Run ablation studies varying β₁, β₂, β₃, β₄ to identify sensitivity and optimal scaling
2. Implement and test alternative shift-detection methods (performance drop threshold vs. external signal)
3. Validate the algorithm in a larger grid-world or continuous-state SAR scenario to assess scalability