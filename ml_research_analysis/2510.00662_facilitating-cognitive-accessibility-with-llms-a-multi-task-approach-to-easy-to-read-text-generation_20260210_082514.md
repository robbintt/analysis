---
ver: rpa2
title: 'Facilitating Cognitive Accessibility with LLMs: A Multi-Task Approach to Easy-to-Read
  Text Generation'
arxiv_id: '2510.00662'
source_url: https://arxiv.org/abs/2510.00662
tags:
- text
- etr-fr
- dataset
- information
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of generating cognitively accessible
  text for individuals with disabilities by introducing ETR-fr, the first high-quality,
  paragraph-aligned dataset compliant with European Easy-to-Read (ETR) guidelines.
  The authors propose a multi-task learning (MTL) approach that combines text summarization,
  simplification, and ETR generation, evaluated using two strategies: multi-task retrieval-augmented
  generation (RAG) for in-context learning and MTL-LoRA for parameter-efficient fine-tuning.'
---

# Facilitating Cognitive Accessibility with LLMs: A Multi-Task Approach to Easy-to-Read Text Generation

## Quick Facts
- arXiv ID: 2510.00662
- Source URL: https://arxiv.org/abs/2510.00662
- Authors: François Ledoyen; Gaël Dias; Jeremie Pantin; Alexis Lechervy; Fabrice Maurel; Youssef Chahir
- Reference count: 40
- Key outcome: Introduces ETR-fr dataset and multi-task learning approach for cognitively accessible text generation, showing MTL-LoRA outperforms single-task baselines

## Executive Summary
This paper addresses the challenge of generating cognitively accessible text for individuals with disabilities by introducing ETR-fr, the first high-quality, paragraph-aligned dataset compliant with European Easy-to-Read (ETR) guidelines. The authors propose a multi-task learning (MTL) approach that combines text summarization, simplification, and ETR generation, evaluated using two strategies: multi-task retrieval-augmented generation (RAG) for in-context learning and MTL-LoRA for parameter-efficient fine-tuning. Experiments on Mistral-7B and LLaMA-3-8B show that MTL-LoRA consistently outperforms single-task baselines in in-domain settings, achieving the highest SRB score of 39.60 and compression ratio of 56.11%, while RAG-based approaches demonstrate better generalization on out-of-domain political texts. Human evaluation confirms the benefits of multi-task learning across ETR criteria and text quality dimensions, though illustration generation remains challenging.

## Method Summary
The authors introduce a multi-task learning framework for Easy-to-Read text generation that combines three tasks: text summarization, text simplification, and ETR generation. They evaluate two approaches: a RAG-based in-context learning method that retrieves similar examples for prompt construction, and an MTL-LoRA fine-tuning approach that adds low-rank adapter layers to pre-trained models. The evaluation uses both in-domain datasets (French parliamentary speeches) and out-of-domain political texts. Models are tested on Mistral-7B and LLaMA-3-8B architectures, with human evaluation covering six ETR criteria and four text quality dimensions. The ETR-fr dataset, comprising 24,251 paragraph-aligned samples, serves as the primary evaluation resource.

## Key Results
- MTL-LoRA achieves highest SRB score of 39.60 and compression ratio of 56.11% on in-domain evaluation
- RAG-based approach shows better generalization performance on out-of-domain political texts compared to MTL-LoRA
- Multi-task learning consistently outperforms single-task baselines across all evaluation metrics in in-domain settings
- Human evaluation confirms multi-task learning benefits across ETR criteria and text quality dimensions
- Illustration generation remains challenging despite overall improvements in other ETR components

## Why This Works (Mechanism)
The multi-task learning approach works by leveraging shared representations across related tasks (summarization, simplification, and ETR generation) that all require reducing complexity while preserving meaning. By training on multiple related tasks simultaneously, the model learns more robust and generalizable representations that capture the core principles of cognitive accessibility. The parameter-efficient LoRA fine-tuning allows adaptation of large pre-trained models without full fine-tuning, preserving general language understanding while specializing in ETR generation. The RAG approach benefits from retrieving similar examples during inference, providing contextual guidance that helps with out-of-domain generalization. The combination of these techniques addresses the challenge of balancing simplification with preservation of essential information, which is critical for cognitive accessibility.

## Foundational Learning

**Easy-to-Read (ETR) Guidelines**: Standardized accessibility guidelines for creating cognitively accessible text for people with intellectual and developmental disabilities. Why needed: Provides the framework and criteria for evaluating cognitive accessibility. Quick check: Verify compliance with European ETR guidelines for target audience.

**Multi-Task Learning (MTL)**: Training a single model on multiple related tasks simultaneously to improve generalization and performance. Why needed: Leverages shared representations across summarization, simplification, and ETR generation tasks. Quick check: Ensure tasks are sufficiently related to benefit from shared learning.

**Low-Rank Adaptation (LoRA)**: Parameter-efficient fine-tuning method that adds low-rank adapter layers instead of full fine-tuning. Why needed: Enables adaptation of large models without extensive computational resources. Quick check: Verify adapter dimensions and training stability.

**SRB (Simplification and Readability Benchmark)**: Evaluation metric specifically designed for assessing cognitively accessible text quality. Why needed: Provides standardized assessment of ETR text quality beyond general language metrics. Quick check: Calculate SRB scores for generated vs reference texts.

**Retrieval-Augmented Generation (RAG)**: Approach that retrieves relevant examples during inference to provide contextual guidance. Why needed: Helps with out-of-domain generalization by providing task-relevant examples. Quick check: Measure retrieval relevance and impact on generation quality.

## Architecture Onboarding

**Component Map**: Input Text -> RAG/MTL-LoRA Processor -> Generated ETR Text -> Human Evaluation + Automated Metrics

**Critical Path**: The most critical path is the text generation pipeline, where the model must balance simplification with information preservation. The RAG approach retrieves similar examples, while MTL-LoRA fine-tunes the model parameters. Both paths converge on generating cognitively accessible text that meets ETR guidelines.

**Design Tradeoffs**: The main tradeoff is between in-domain performance (favoring MTL-LoRA) and out-of-domain generalization (favoring RAG). MTL-LoRA provides superior performance on the target domain but may overfit to specific patterns, while RAG offers better adaptability but with potentially lower absolute quality. The choice of model size (Mistral-7B vs LLaMA-3-8B) also affects the balance between computational efficiency and generation quality.

**Failure Signatures**: Common failure modes include over-simplification that removes essential information, under-simplification that fails to meet cognitive accessibility needs, and generation of grammatically incorrect or incoherent text. The model may also struggle with illustration generation, producing irrelevant or missing visual elements that are important for ETR compliance.

**First Experiments**: 1) Compare single-task vs multi-task performance on in-domain ETR-fr dataset. 2) Evaluate RAG-based in-context learning vs MTL-LoRA fine-tuning on out-of-domain political texts. 3) Conduct ablation studies removing individual tasks from the multi-task framework to measure their contribution.

## Open Questions the Paper Calls Out
None

## Limitations

**Dataset Limitations**: The ETR-fr dataset is relatively small (24,251 samples) and may not capture the full diversity of cognitively accessible text needs across different disabilities and contexts. The focus on French parliamentary speeches may limit generalizability to other domains and languages.

**Model Limitations**: The evaluation relies primarily on technical metrics and general human judgments rather than direct assessment of cognitive accessibility improvements for users with disabilities. The approach shows limited success with illustration generation, a key component of ETR compliance.

**Methodological Limitations**: The comparison between RAG and MTL-LoRA does not explore other parameter-efficient fine-tuning methods or more sophisticated RAG implementations. The out-of-domain evaluation uses a limited test set that may not represent true generalization challenges.

## Confidence

**High Confidence**: Multi-task learning approach consistently outperforms single-task baselines in in-domain settings; ETR-fr dataset represents meaningful contribution to cognitively accessible text generation.

**Medium Confidence**: Relative performance of RAG versus MTL-LoRA for out-of-domain generalization; human evaluation results showing multi-task learning benefits across ETR criteria.

**Low Confidence**: Actual impact on cognitive accessibility for end users with disabilities; scalability of these approaches to larger, more diverse datasets and different languages.

## Next Checks

1. Conduct user studies with individuals having cognitive disabilities to validate whether the generated ETR texts actually improve comprehension and accessibility, rather than relying solely on automated metrics and general human evaluations.

2. Expand the evaluation to include additional text domains (e.g., healthcare, education, news) and assess cross-domain generalization performance to determine the robustness of the multi-task approach beyond political content.

3. Test the approach with larger foundation models (e.g., LLaMA-3-70B, GPT-4) and compare against additional parameter-efficient fine-tuning methods (e.g., prefix tuning, prompt tuning) to establish the optimal balance between model size and fine-tuning strategy.