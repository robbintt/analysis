---
ver: rpa2
title: Bidirectional Representations Augmented Autoregressive Biological Sequence
  Generation
arxiv_id: '2510.08169'
source_url: https://arxiv.org/abs/2510.08169
tags:
- peptide
- sequence
- should
- decoder
- mass
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CROSSNOVO is a hybrid architecture for de novo peptide sequencing
  that combines autoregressive (AR) and non-autoregressive (NAR) transformers. It
  uses a shared spectrum encoder with an AR decoder for sequential peptide generation
  and a NAR decoder for bidirectional context learning.
---

# Bidirectional Representations Augmented Autoregressive Biological Sequence Generation

## Quick Facts
- arXiv ID: 2510.08169
- Source URL: https://arxiv.org/abs/2510.08169
- Reference count: 40
- CROSSNOVO achieves amino acid precision of 0.811 and peptide recall of 0.654 on 9-species benchmark

## Executive Summary
CROSSNOVO introduces a hybrid architecture that combines autoregressive (AR) and non-autoregressive (NAR) transformers for de novo peptide sequencing. The model employs a shared spectrum encoder with an AR decoder for sequential peptide generation and a NAR decoder for bidirectional context learning. A cross-decoder attention module transfers bidirectional knowledge from the NAR to the AR decoder, while gradient blocking prevents interference between the two components. The architecture demonstrates superior performance compared to both AR and NAR baselines, achieving 0.811 amino acid precision and 0.654 peptide recall on a 9-species benchmark dataset.

## Method Summary
The CROSSNOVO architecture integrates AR and NAR transformers through a shared spectrum encoder, enabling bidirectional context learning while maintaining sequential generation capabilities. The NAR decoder captures bidirectional dependencies and transfers this knowledge to the AR decoder via cross-decoder attention, with gradient blocking preventing interference. A multitask training strategy with importance annealing balances the competing objectives of both components. The model employs a fixed scaffold approach for peptide generation, constraining the search space while enabling efficient exploration of novel peptide structures.

## Key Results
- Achieves amino acid precision of 0.811 on 9-species benchmark
- Attains peptide recall of 0.654, surpassing both AR and NAR baselines
- Demonstrates strong performance on antibody data and post-translational modification tasks

## Why This Works (Mechanism)
The architecture leverages the complementary strengths of AR and NAR transformers: AR provides sequential generation capabilities while NAR captures bidirectional context. The cross-decoder attention mechanism effectively transfers bidirectional knowledge to improve sequential generation, while gradient blocking maintains the stability of the multitask learning process. The importance annealing strategy gradually balances the competing objectives during training, allowing the model to converge to an optimal solution that leverages both autoregressive and non-autoregressive representations.

## Foundational Learning

**Autoregressive vs Non-autoregressive Transformers**
- Why needed: Different approaches to sequence generation with distinct trade-offs
- Quick check: AR provides sequential generation but struggles with bidirectional context; NAR captures bidirectional dependencies but lacks sequential generation capability

**Cross-decoder Attention Mechanisms**
- Why needed: Enables knowledge transfer between AR and NAR components
- Quick check: Allows bidirectional context from NAR to enhance sequential generation in AR decoder

**Gradient Blocking in Multitask Learning**
- Why needed: Prevents interference between competing objectives
- Quick check: Maintains stability of multitask training while enabling selective knowledge transfer

**Importance Annealing in Multitask Training**
- Why needed: Balances competing objectives during training
- Quick check: Gradually adjusts the relative importance of AR and NAR objectives to optimize performance

## Architecture Onboarding

**Component Map**
Spectrum Encoder -> AR Decoder -> Peptide Generation
              -> NAR Decoder -> Cross-decoder Attention -> AR Decoder Enhancement

**Critical Path**
Spectrum encoder output → NAR decoder → cross-decoder attention → AR decoder → peptide generation

**Design Tradeoffs**
- Increased architectural complexity vs. performance gains
- Gradient blocking limits bidirectional knowledge transfer but prevents interference
- Fixed scaffold approach constrains search space but enables efficient exploration

**Failure Signatures**
- Degraded performance on rare amino acid sequences
- Sensitivity to hyperparameter settings in importance annealing
- Limited generalization to highly modified peptide sequences

**First Experiments**
1. Ablation study on gradient blocking effectiveness
2. Analysis of cross-decoder attention contribution to performance
3. Evaluation of fixed scaffold approach on novel peptide structures

## Open Questions the Paper Calls Out
None

## Limitations
- Multitask learning with importance annealing introduces hyperparameter sensitivity
- Gradient blocking may limit full transfer of bidirectional knowledge
- Fixed scaffold approach constrains search space and may miss novel peptide structures

## Confidence
- Core architecture design and integration: High
- Benchmark performance metrics: High
- Cross-domain generalization claims: Medium
- Multitask training stability: Medium
- Biological interpretation of learned representations: Low

## Next Checks
1. Ablation studies on gradient blocking effectiveness and bidirectional knowledge transfer efficiency
2. Extended testing on diverse PTM datasets with varying modification densities
3. Analysis of architectural complexity vs. performance trade-offs compared to simpler autoregressive baselines