---
ver: rpa2
title: Towards integration of Privacy Enhancing Technologies in Explainable Artificial
  Intelligence
arxiv_id: '2507.04528'
source_url: https://arxiv.org/abs/2507.04528
tags:
- attack
- privacy
- explanations
- training
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper evaluates Privacy Enhancing Technologies (PETs) integration\
  \ in feature-based Explainable AI (XAI) to defend against attribute inference attacks.\
  \ The study empirically evaluates three PETs categories\u2014synthetic data generation,\
  \ differentially private training, and noise addition\u2014across three AI pipeline\
  \ stages (pre-model, in-model, post-model) using two XAI subcategories (backpropagation\
  \ and perturbation-based)."
---

# Towards integration of Privacy Enhancing Technologies in Explainable Artificial Intelligence

## Quick Facts
- arXiv ID: 2507.04528
- Source URL: https://arxiv.org/abs/2507.04528
- Authors: Sonal Allana; Rozita Dara; Xiaodong Lin; Pulei Xiong
- Reference count: 0
- Primary result: Post-model noise addition most effective PET for mitigating attribute inference attacks in XAI

## Executive Summary
This paper systematically evaluates Privacy Enhancing Technologies (PETs) for defending against attribute inference attacks in Explainable AI systems. The study investigates three PET categories—synthetic data generation, differentially private training, and noise addition—across three pipeline stages (pre-model, in-model, post-model) using two XAI subcategories (backpropagation and perturbation-based). Through empirical testing on four tabular datasets, the research identifies post-model noise addition as the most effective defense, reducing attack success by up to 49.47% while maintaining model utility and explanation quality. The findings provide actionable recommendations for integrating PETs into XAI systems to balance privacy protection with explainability requirements.

## Method Summary
The study evaluates PETs integration in feature-based Explainable AI to defend against attribute inference attacks. Three PET categories—synthetic data generation, differentially private training, and noise addition—are tested across three AI pipeline stages (pre-model, in-model, post-model) using two XAI subcategories (backpropagation and perturbation-based). Attribute inference attacks are launched on explanations to measure privacy effectiveness while evaluating explanation quality, model accuracy, and performance overhead. The evaluation uses four tabular datasets and measures attack success reduction, faithfulness metrics, and model utility preservation.

## Key Results
- Post-model noise addition most effective, reducing attack success by up to 49.47% while maintaining model utility
- Synthetic data and differentially private training show limited mitigation against attribute inference
- Gaussian noise most effective for backpropagation methods (IG, SmoothGrad, SHAP), Laplace noise best for perturbation methods (LIME)
- 93% of 352 total attacks showed mitigation with post-model noise addition

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Post-model noise addition on explanations provides the most effective defense against attribute inference attacks while preserving model utility and explanation quality.
- Mechanism: Noise is added to explanation outputs after model inference, introducing adversarial perturbations that mislead the attack classifier without requiring model retraining. The noise disrupts the mapping that adversaries use to infer sensitive attributes.
- Core assumption: Attack models rely on precise explanation vectors; sufficient noise degrades their classification boundary without destroying explanation faithfulness.
- Evidence anchors:
  - [abstract] "In the best case, PETs integration in explanations reduced the risk of the attack by 49.47%, while maintaining model utility and explanation quality."
  - [section 5.1] "Post-model noise methods consistently displayed reduction in attack success, bringing the attack success closer to random guess... 93% of 352 total attacks showed mitigation of the attack."
- Break condition: If noise magnitude exceeds explanation signal variance, faithfulness metrics degrade significantly, breaking the utility-privacy balance.

### Mechanism 2
- Claim: Synthetic training data provides unreliable protection against attribute inference and may increase attack success in some configurations.
- Mechanism: Synthetic data generators approximate training distributions without one-to-one mapping. However, the attack exploits explanation patterns rather than raw data membership, so distributional similarity preserves exploitable signal.
- Core assumption: Attribute inference attacks target explanation-output relationships, not training data provenance.
- Evidence anchors:
  - [section 5.1] "The results from pre-model indicate that synthetic data is ineffective at mitigating attribute inference... 48% cases [showed mitigation] while remaining cases showing a boost in attack success."
- Break condition: If synthetic data perfectly preserves feature-attribution relationships while removing sensitive correlations, protection may emerge—but current generators show mixed results.

### Mechanism 3
- Claim: Differentially private training (DP-SGD) is poorly suited for defending against attribute inference from explanations, despite effectiveness against membership inference.
- Mechanism: DP-SGD clips gradients and adds noise during training to obscure individual record presence. Attribute inference exploits explanation feature patterns rather than membership signals, so DP's guarantee does not translate to feature-level privacy.
- Core assumption: Attribute inference and membership inference have fundamentally different attack surfaces in XAI systems.
- Evidence anchors:
  - [section 5.1] "Lower values of ε, in the strong privacy guarantee tier, which are expected to boost privacy, failed to strengthen the resilience of the system against the attack."
  - [section 6.1] "This can be due to the mechanics of DP, which effectively hides the presence of a single record in a dataset, making it more suitable for mitigating membership inference than attribute inference."
- Break condition: If sensitive attributes are directly encoded in training gradients with high magnitude, DP-SGD clipping could incidentally reduce their influence—but the paper shows this does not occur reliably.

## Foundational Learning

- Concept: **Feature-based XAI categories (Backpropagation vs. Perturbation)**
  - Why needed here: Defense effectiveness varies by XAI method; paper evaluates IG/SmoothGrad (backpropagation) and SHAP/LIME (perturbation-based) with different resilience profiles.
  - Quick check question: Given a tabular classification model requiring feature importance, which XAI category would you start with for privacy-sensitive deployment?

- Concept: **Attribute Inference Attack Model**
  - Why needed here: The threat model assumes black-box access to explanations, auxiliary dataset possession, and an MLP attack classifier trained to reverse-map explanations to sensitive attributes.
  - Quick check question: An adversary has query access to your model's SHAP explanations but not training data. What additional resource does the attack model require?

- Concept: **Faithfulness Metrics for Explanation Quality**
  - Why needed here: Privacy defenses must be evaluated against explanation degradation; faithfulness correlation and sufficiency measure alignment between explanations and model behavior.
  - Quick check question: After adding noise to explanations, faithfulness correlation remains stable but sufficiency drops. What does this indicate about the noise's impact?

## Architecture Onboarding

- Component map:
  Pre-model: Synthetic data generation (CTGAN/Gaussian Copula/TVAE) → replaces training data
  In-model: DP-SGD training with ε tuning → modifies training process
  Post-model: Noise addition (random/calibrated, Gaussian/Laplace) → perturbs explanation outputs
  Attack evaluation: MLP classifier trained on explanations to infer binary sensitive attributes

- Critical path:
  1. Train baseline model on original data
  2. Generate explanations using IG, SmoothGrad, SHAP, or LIME
  3. Apply post-model noise (recommended: Gaussian for IG/SG/SHAP, Laplace for LIME)
  4. Evaluate attack success, faithfulness metrics, and model accuracy
  5. Tune noise parameters based on privacy-utility tradeoff

- Design tradeoffs:
  - Gaussian vs. Laplace noise: Gaussian more effective for IG/SG/SHAP; Laplace better for LIME
  - Random vs. calibrated noise: Random has negligible overhead; calibrated provides formal DP guarantee but adds computation
  - Pre-model synthetic data: Faster generation with Gaussian Copula, better accuracy retention with TVAE—but neither provides reliable privacy

- Failure signatures:
  - Attack success increases above baseline (observed in 52% of pre-model, 56% of in-model configurations)
  - Faithfulness metrics drop significantly (indicates noise destroying explanation signal)
  - Model accuracy degrades >10% (DP-SGD at low ε, synthetic data with Gaussian Copula)

- First 3 experiments:
  1. **Baseline attack measurement**: Train model on Adult/Credit/Compas/Hospital datasets, generate SHAP explanations, run attribute inference attack to establish attack success baseline for your data.
  2. **Post-model noise tuning**: Apply random Gaussian noise to SHAP explanations at varying scales, measure attack success reduction vs. faithfulness retention to find optimal noise level.
  3. **Cross-XAI comparison**: Repeat experiment 2 with LIME and Laplace noise to validate method-specific recommendations from the paper on your target dataset.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do hybrid combinations of PETs (e.g., synthetic data combined with noise addition) perform in balancing the trade-off between attribute inference defense and system utility?
- Basis in paper: [explicit] The authors state in the recommendations that "Multiple PETs can be implemented in an XAI system due to their complimentary nature" and suggest that "hybrid privacy preservation approaches can benefit the privacy of the overall system."
- Why unresolved: The study design explicitly introduced "one privacy preservation method at a time in the pipeline to enable inter-stage comparison," leaving the interaction effects of combining methods untested.
- What evidence would resolve it: Empirical results measuring privacy-utility trade-offs when applying multiple PETs simultaneously (e.g., pre-model synthetic data with post-model noise).

### Open Question 2
- Question: How does the integration of PETs, particularly noise addition, affect XAI evaluation metrics beyond faithfulness, such as explanation robustness and complexity?
- Basis in paper: [explicit] Section 5.5 (Limitations) explicitly states that "other XAI metrics such as complexity and robustness, are not evaluated" despite their importance in evaluating explanation quality.
- Why unresolved: The study focused primarily on faithfulness metrics (correlation, estimate, sufficiency) to align explanations with predictive behavior, leaving other dimensions of explanation quality unexplored.
- What evidence would resolve it: A comparative analysis of explanation scores using robustness and complexity metrics on models treated with post-model noise.

### Open Question 3
- Question: Can post-model noise addition effectively mitigate attribute inference attacks in non-tabular data domains (e.g., images) or with non-binary sensitive attributes?
- Basis in paper: [explicit] Section 5.5 (Limitations) notes the study is constrained to "tabular datasets" and "inference of binary sensitive attributes."
- Why unresolved: The experiments utilized four specific tabular datasets (Adult, Credit, Compas, Hospital), and it is unclear if the ~49% attack reduction via noise translates to more complex data structures like images or continuous sensitive variables.
- What evidence would resolve it: Experimental replication using image datasets (e.g., facial recognition) or datasets with continuous sensitive attributes to test if the noise mitigation strategy remains effective.

## Limitations
- Limited dataset diversity (only 4 tabular datasets tested) constrains generalizability across domains
- Fixed attack model architecture (MLP) may not represent all adversarial capabilities
- Gaussian/Laplace noise assumptions may not hold for non-Gaussian explanation distributions
- No evaluation of computational overhead impact on real-time systems

## Confidence
- Post-model noise effectiveness: **High** (strong empirical support across 93% of attacks)
- Synthetic data unreliability: **Medium** (consistent negative results but limited exploration of advanced generators)
- DP-SGD ineffectiveness: **Medium** (mechanistic explanation aligns with results but narrow parameter sweep)
- Method-specific noise recommendations: **Medium** (based on aggregate results, requires domain validation)

## Next Checks
1. **Cross-domain validation**: Test the post-model noise approach on image and time-series datasets beyond tabular data to verify generalizability
2. **Adaptive attack evaluation**: Implement a white-box attack variant that leverages knowledge of the defense mechanism to assess robustness
3. **Multi-attribute inference**: Extend the attack to target multiple sensitive attributes simultaneously and evaluate defense scalability