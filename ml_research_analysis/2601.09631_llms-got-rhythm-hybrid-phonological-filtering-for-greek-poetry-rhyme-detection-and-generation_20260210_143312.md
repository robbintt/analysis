---
ver: rpa2
title: LLMs Got Rhythm? Hybrid Phonological Filtering for Greek Poetry Rhyme Detection
  and Generation
arxiv_id: '2601.09631'
source_url: https://arxiv.org/abs/2601.09631
tags:
- rhyme
- miss
- claude
- llama
- imperfect
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We present a hybrid neuro-symbolic system that combines LLMs with
  deterministic phonological algorithms to achieve accurate rhyme identification and
  generation in Modern Greek poetry. Our approach implements a comprehensive taxonomy
  of Greek rhyme types and employs an agentic generation pipeline with phonological
  verification.
---

# LLMs Got Rhythm? Hybrid Phonological Filtering for Greek Poetry Rhyme Detection and Generation

## Quick Facts
- arXiv ID: 2601.09631
- Source URL: https://arxiv.org/abs/2601.09631
- Authors: Stergios Chatzikyriakidis; Anastasia Natsina
- Reference count: 7
- Primary result: Hybrid neuro-symbolic system achieves 73.1% valid poem generation vs under 4% for pure LLMs

## Executive Summary
This paper presents a hybrid neuro-symbolic approach to Greek poetry rhyme detection and generation that combines LLMs with deterministic phonological algorithms. The system implements a comprehensive taxonomy of Greek rhyme types and employs an agentic generation pipeline with phonological verification. Experimental results show that pure LLM generation fails catastrophically (under 4% valid poems), while the hybrid verification loop restores performance to 73.1%. The approach achieves 100% verification accuracy compared to 54% for pure LLM approaches.

## Method Summary
The system combines a phonological engine with LLMs through a generate-verify-refine loop. The phonological engine performs syllabification, stress detection, rhyme domain extraction, and phonetic transcription based on Greek phonological rules. A Rhyme Classification Module analyzes rhyme pairs using features from Topintzi et al. (2019). The LLM integration layer supports multiple models (Claude, GPT-4o, Gemini, Llama, Mistral) with four prompting strategies. The agentic pipeline generates text, verifies against phonological constraints, provides structured feedback, and retries up to 15 times. The system is evaluated on a corpus of 40,000+ rhymes and a test set of 40 poems.

## Key Results
- Pure LLM generation achieves under 4% valid poem performance
- Hybrid verification loop restores validity to 73.1%
- Symbolic phonological verification achieves 100% accuracy vs 54% for best LLM-only approach
- F3 (proparoxytone) rhymes are most challenging for LLMs (<22% accuracy)
- Complex multi-feature constraints fail even with verification (0% success)

## Why This Works (Mechanism)

### Mechanism 1: Generate-Verify-Refine Loop
Iterative phonological verification restores generation validity from under 4% to 73.1%. The deterministic phonological engine provides exact rhyme-domain feedback that LLMs cannot self-generate. Structured error messages (e.g., "Stress mismatch: Expected F3, found F2") guide LLM correction. Up to 15 retry attempts allow progressive refinement. The core assumption is that LLMs can interpret phonological error feedback and adjust vocabulary selection without internal phonological representation.

### Mechanism 2: Chain-of-Thought Elicits Latent Phonological Reasoning
Reasoning-heavy models (Claude 4.5) require explicit CoT prompting to achieve peak performance (26.9% → 53.8%). CoT forces explicit decomposition of phonological analysis into steps (stress detection → domain extraction → feature matching). This externalizes reasoning that would otherwise remain implicit and error-prone in single-pass token prediction. The core assumption is that models have latent phonological knowledge from training data requiring structured elicitation.

### Mechanism 3: Symbolic Decoupling of Phonology from Semantics
Separating phonological verification from semantic generation allows each component to operate within its competence boundary. The phonological engine provides 100% verification accuracy (ground truth) while the LLM handles lexical selection and thematic coherence. This "sandwich architecture" compensates for BPE tokenization's failure to align with phonological units. The core assumption is that phonological rules are complete and correct for the target language.

## Foundational Learning

- **Greek Stress-Based Rhyme Taxonomy (M/F2/F3)**: All classification and verification depends on correct stress position identification. Greek rhyme domains extend from the stressed syllable to word end; the three-syllable rule constrains stress position. *Quick check*: Given "παράπονο" (parápono), which syllable bears stress and what rhyme type is this? (Answer: antepenultimate → F3 proparoxytone)

- **Tokenization-Phonology Misalignment**: Understanding why pure LLMs fail requires recognizing that BPE/WordPiece tokenize by statistical co-occurrence, not phonological units. This obscures stress patterns and rhyme domains. *Quick check*: Why might a tokenizer split "παράπονο" in a way that prevents the model from identifying stress position? (Answer: Subword splits like [παρ, άπ, ονο] break syllable boundaries)

- **Agentic Generate-Verify-Refine Pattern**: This is the architectural pattern that enables constrained generation. Understanding the feedback loop structure is essential for extending or debugging the system. *Quick check*: What three components must the verifier output for effective refinement? (Answer: Error type, location, and structured feedback the LLM can parse)

## Architecture Onboarding

- **Component map**: Input poem/prompt → Phonological Engine (ground truth) ↔ LLM Analysis → Classification Module (comparison) → Verification output. For generation: Theme + constraints → LLM generate → Engine verify → If errors: format feedback → update prompt → regenerate.

- **Critical path**: The phonological engine provides 100% accurate ground truth verification, which is compared against LLM analysis to identify phonological violations. The verifier then formats structured error messages that guide the LLM's next generation attempt.

- **Design tradeoffs**: Computational overhead increases with iterative loop vs. single-pass inference; coverage vs. precision as symbolic rules may miss valid edge cases; model selection as reasoning-heavy models perform best but cost more while smaller open models fail to leverage CoT effectively.

- **Failure signatures**: F3 (proparoxytone) rhymes fail most often (most models under 22% accuracy); MOSAIC rhymes undetected by any model in automated evaluation (0% across all models); complex multi-feature constraints (IDV+MOSAIC+PURE) fail even with verification (0% success); hallucinated rhymes occur when the LLM proposes pairs with zero phonetic overlap.

- **First 3 experiments**:
  1. Run the phonological engine on the 40-poem test set against human-annotated ground truth to confirm 100% verification accuracy claim.
  2. Test generation validity with max_attempts ∈ {1, 5, 10, 15} to quantify the marginal value of additional iterations.
  3. Categorize verifier feedback by error type (stress mismatch, feature violation, non-rhyme) across models to identify which phonological constraints each model struggles with most.

## Open Questions the Paper Calls Out

### Open Question 1
Can the hybrid architecture be extended to encompass metrical verification (e.g., iambic 15-syllable verse) alongside rhyme detection? The authors state this is the natural next step. This remains unresolved because the current system focuses exclusively on rhyme, whereas Greek poetry relies heavily on stress-timed constituent meters requiring analysis of the entire line's prosodic structure.

### Open Question 2
Can phonological constraints be internalized by smaller models via Reinforcement Learning from Phonological Feedback (RLPF) to replace the inference-time verification loop? The paper proposes exploring this to fine-tune smaller models like Llama 8B. This is unresolved because it's unknown if deterministic feedback can be distilled into model parameters to allow single-pass generation.

### Open Question 3
Can this neuro-symbolic approach be generalized to other low-resource languages to create a "Universal Rhyme Engine"? The authors aim to extend beyond Greek to other languages. This remains unresolved because the current phonological engine relies on Greek-specific rules that may not map to languages with different prosodic systems or shallower orthographies.

## Limitations
- Performance generalizability is limited as the system's effectiveness for other languages with different stress patterns remains untested
- Heavy dependency on reasoning-capable models creates deployment constraints, as open-weight models fail to leverage CoT effectively
- Symbolic rule completeness depends on Topintzi et al. (2019) algorithms, which have acknowledged precision limitations

## Confidence
- **High Confidence**: Pure LLM approaches fail catastrophically (<4% validity); hybrid verification loop improves performance to 73.1%; symbolic verification achieves 100% accuracy
- **Medium Confidence**: CoT doubles Claude 4.5 performance (26.9% → 53.8%); F3 rhymes are most challenging (<22% accuracy); complex constraints fail even with verification (0% success)
- **Low Confidence**: Approach generalizes to other stress-based languages; open-weight models can achieve similar performance with modified prompting; computational overhead is acceptable for practical applications

## Next Checks
1. Apply the hybrid architecture to another stress-based language (e.g., Spanish or Italian) with a small test set (10-20 poem pairs) to assess cross-linguistic generalizability.

2. Modify CoT prompts with explicit phonological instruction templates and test whether Llama 3.1 8B or Mistral 7B can achieve >40% performance on identification tasks.

3. Measure generation latency and API costs for the 15-iteration loop versus single-pass generation on 100 test cases to calculate the trade-off between validity improvement and computational expense.