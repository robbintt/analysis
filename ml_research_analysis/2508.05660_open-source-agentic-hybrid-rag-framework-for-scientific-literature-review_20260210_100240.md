---
ver: rpa2
title: Open-Source Agentic Hybrid RAG Framework for Scientific Literature Review
arxiv_id: '2508.05660'
source_url: https://arxiv.org/abs/2508.05660
tags:
- retrieval
- literature
- arxiv
- agentic
- review
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduces an open-source agentic hybrid RAG framework
  for scientific literature review that dynamically selects between graph-based (GraphRAG)
  and vector-based (VectorRAG) retrieval methods for each query. The framework integrates
  a Neo4j knowledge graph with citation relationships and a FAISS vector store using
  all-MiniLM-L6-v2 embeddings, orchestrated by a Llama-3.3-70B agent that adapts retrieval
  strategies based on query type.
---

# Open-Source Agentic Hybrid RAG Framework for Scientific Literature Review

## Quick Facts
- arXiv ID: 2508.05660
- Source URL: https://arxiv.org/abs/2508.05660
- Reference count: 40
- Introduces open-source agentic hybrid RAG framework for scientific literature review with dynamic GraphRAG/VectorRAG selection

## Executive Summary
This paper presents an open-source agentic hybrid RAG framework that dynamically selects between graph-based (GraphRAG) and vector-based (VectorRAG) retrieval methods for scientific literature review. The framework integrates a Neo4j knowledge graph with citation relationships and a FAISS vector store using all-MiniLM-L6-v2 embeddings, orchestrated by a Llama-3.3-70B agent that adapts retrieval strategies based on query type. Instruction tuning with Direct Preference Optimization (DPO) refines the response generator, while bootstrapped evaluation provides uncertainty quantification. On synthetic benchmarks mimicking real-world queries, the Instruction-Tuned Agent with DPO achieved substantial performance gains over the baseline.

## Method Summary
The framework ingests bibliometric data from PubMed, arXiv, and Google Scholar APIs, building a Neo4j citation-based knowledge graph and embedding full-text PDFs into a FAISS vector store. A Llama-3.3-70B agent dynamically routes queries to either GraphRAG (Cypher translation for KG queries) or VectorRAG (BM25 + semantic + reranking for full-text queries). The response generator uses Mistral-7B-Instruct-v0.3 fine-tuned with DPO using 15 human-annotated preference pairs. Evaluation employs bootstrapped resampling (12 samples) with RAGAS metrics including faithfulness, answer relevance, and context precision/recall.

## Key Results
- Instruction-Tuned Agent with DPO improved VS Context Recall by 0.63 and VS Faithfulness by 0.24 over baseline
- KG Answer Relevance increased by 0.12 and VS Precision by 0.12 with instruction tuning
- Dynamic retrieval mode selection provided complementary signals across heterogeneous scientific information
- System achieved robust performance with uncertainty quantification via bootstrapping

## Why This Works (Mechanism)

### Mechanism 1
Dynamic retrieval mode selection improves relevance over static pipelines. A Llama-3.3-70B agent classifies each query and routes it to either GraphRAG (Cypher over Neo4j) or VectorRAG (BM25 + semantic search + reranking). The agent uses 10 few-shot examples to learn routing logic, with GraphRAG handling structured/metadata queries and VectorRAG handling full-text semantic queries. This approach assumes query intent can be reliably inferred from natural language with few-shot prompting, and that optimal retrieval mode is query-dependent rather than universal.

### Mechanism 2
Hybrid storage (knowledge graph + vector store) provides complementary retrieval signals. The Neo4j KG stores structured metadata (authors, citations, keywords, publication year) with explicit relationships, enabling precise multi-hop queries via Cypher. The FAISS vector store embeds full-text chunks using all-MiniLM-L6-v2, enabling semantic similarity search. This assumes structured metadata and unstructured full-text provide non-redundant information, and combining both yields better coverage than either alone.

### Mechanism 3
Direct Preference Optimization (DPO) with limited human feedback improves faithfulness and context grounding. DPO is applied to the Mistral-7B-Instruct-v0.3 response generator using 15 human-annotated preference pairs. The model learns to prefer answers grounded in retrieved context over hallucinated content, directly aligning outputs with human judgment without full RLHF. This assumes a small set of high-quality preference pairs can shift model behavior toward context-grounded generation, and the preference signal generalizes to unseen queries.

## Foundational Learning

- **Concept: Retrieval-Augmented Generation (RAG)**
  - Why needed: The entire framework builds on RAG—understanding how retrieval conditions generation is essential for grasping why hybrid retrieval and DPO matter.
  - Quick check: Can you explain how a retrieved context passage influences the probability distribution of an LLM's next token during generation?

- **Concept: Graph Databases and Cypher Query Language**
  - Why needed: GraphRAG requires translating natural language to Cypher queries over Neo4j; understanding graph schemas and traversal is prerequisite to debugging or extending the KG component.
  - Quick check: Given a knowledge graph with (Paper)-[:CITES]->(Paper) relationships, write a Cypher query to find all papers cited by a given author's publications.

- **Concept: Direct Preference Optimization (DPO)**
  - Why needed: The response generator is fine-tuned with DPO; understanding how preference pairs optimize policy without explicit reward models clarifies why only 15 pairs suffice.
  - Quick check: How does DPO differ from RLHF in terms of reward model training and policy optimization steps?

## Architecture Onboarding

- **Component map:**
  Data Ingestion -> Preprocessing -> Graph Store -> Vector Store -> Orchestrator Agent -> Retrieval Tools -> Response Generator -> Evaluation

- **Critical path:**
  1. Ingest and deduplicate bibliometric data (DOI, title, abstract, authors, year, PDF URL)
  2. Extract keywords via TF-IDF, compute cosine similarity to query, filter to Q3
  3. Build Neo4j KG (metadata + citations) and FAISS VS (full-text chunks)
  4. Route query via Llama-3.3-70B agent using few-shot prompts
  5. Execute GraphRAG (Cypher) or VectorRAG (BM25 + L2 + rerank)
  6. Generate response with DPO-tuned Mistral-7B, return with uncertainty estimates

- **Design tradeoffs:**
  - Latency vs quality: ~2 min/query on consumer hardware vs ~10 sec with GPU server
  - Small DPO data (15 pairs) vs generalization risk—works here but may not scale
  - Synthetic benchmark vs real-world complexity—controlled evaluation but limited ecological validity
  - Open-source stack (Neo4j, FAISS, Llama, Mistral) vs proprietary APIs (Cohere reranker remains external)

- **Failure signatures:**
  - KG Precision drops (-0.04) after DPO → Cypher translation errors on complex queries
  - VS Faithfulness slightly decreases (-0.02) after DPO → over-reliance on context, missing broader knowledge
  - Routing errors if query intent ambiguous → agent defaults to wrong retrieval mode
  - Empty or sparse KG if source APIs rate-limited or missing metadata

- **First 3 experiments:**
  1. **Routing robustness test:** Manually label 50 queries with ground-truth optimal retrieval mode; measure agent routing accuracy and correlation with final metrics. Identify failure patterns in misclassified queries.
  2. **DPO scaling ablation:** Train response generator with 15, 50, and 150 preference pairs; compare faithfulness and context recall across splits. Determine if small-data gains persist or saturate.
  3. **Hybrid vs single-mode baseline:** Run same benchmark with GraphRAG-only and VectorRAG-only pipelines; quantify contribution of dynamic selection vs fixed modality. Validate claim that hybrid selection is necessary, not just beneficial.

## Open Questions the Paper Calls Out

- **Open Question 1:** Would fine-tuning a dedicated model on curated (natural language query, Cypher) pairs significantly reduce Cypher translation errors and improve multi-hop recall compared to the current few-shot prompting approach? The current implementation relies on few-shot prompting with 30 examples, which the authors acknowledge can fail on complex queries; no fine-tuning experiments were conducted.

- **Open Question 2:** Can reinforcement learning (RLHF or reward-model fine-tuning) learn an optimal retrieval policy that outperforms the current agent's dynamic selection between GraphRAG and VectorRAG? The current system uses an LLM-based agent with few-shot examples for tool selection, but no RL-based policy optimization was implemented or evaluated.

- **Open Question 3:** To what extent does the synthetic benchmark fail to capture the complexity of real-world scientific literature review queries, particularly multi-modal reasoning over figures and tables? The benchmark was synthetically generated from a single search query about MLLMs in healthcare, with questions designed to require only one specific retrieval tool; real queries may require hybrid approaches or multimodal understanding.

- **Open Question 4:** What specific modifications to DPO training would address the observed performance degradation in GraphRAG-specific faithfulness and precision metrics? DPO improved most metrics but decreased KG Precision (−0.04) and KG Faithfulness (−0.03) compared to baseline, yet only 15 preference pairs were used with unclear distribution across retrieval modes.

## Limitations
- Synthetic benchmark of 40 controlled queries cannot capture full complexity and ambiguity of actual scientific literature searches
- Small DPO dataset (15 preference pairs) may not generalize well beyond specific domain and query patterns tested
- Evaluation assumes access to structured metadata and full-text content, which may not be available for all scientific domains or publication types

## Confidence
- **High Confidence:** Framework architecture and implementation details are clearly specified with reproducible code components. Performance improvements on synthetic benchmark are statistically significant with bootstrapping.
- **Medium Confidence:** Mechanisms by which dynamic retrieval selection and DPO improve performance are plausible but require further validation on real-world queries and diverse scientific domains.
- **Low Confidence:** Scalability to larger corpora, different scientific fields, and production deployment scenarios has not been demonstrated.

## Next Checks
1. **Real-world query validation:** Test the framework on 50+ actual user queries from literature review workflows, comparing against expert human results and measuring practical utility beyond benchmark metrics.
2. **Domain transferability assessment:** Evaluate performance when applied to non-biomedical domains (e.g., computer science, social sciences) where citation patterns and document structures differ significantly from the PubMed/arXiv corpus.
3. **DPO scaling study:** Systematically vary the number of preference pairs (15, 50, 150, 300) and measure the relationship between training data size and generalization performance across multiple query types and domains.