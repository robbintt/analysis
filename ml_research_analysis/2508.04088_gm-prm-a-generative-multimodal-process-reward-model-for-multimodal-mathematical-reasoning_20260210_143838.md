---
ver: rpa2
title: 'GM-PRM: A Generative Multimodal Process Reward Model for Multimodal Mathematical
  Reasoning'
arxiv_id: '2508.04088'
source_url: https://arxiv.org/abs/2508.04088
tags:
- reasoning
- arxiv
- step
- gm-prm
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GM-PRM, a generative multimodal process reward
  model that transforms traditional PRMs from passive verifiers into active reasoning
  collaborators. The key innovation is training the model to not only identify errors
  in reasoning steps but also generate corrected versions of erroneous steps, providing
  fine-grained analysis across step intent, visual alignment, and logical soundness.
---

# GM-PRM: A Generative Multimodal Process Reward Model for Multimodal Mathematical Reasoning

## Quick Facts
- arXiv ID: 2508.04088
- Source URL: https://arxiv.org/abs/2508.04088
- Authors: Jianghangfan Zhang; Yibo Yan; Kening Zheng; Xin Zou; Song Dai; Xuming Hu
- Reference count: 10
- One-line primary result: Achieves state-of-the-art on five multimodal math benchmarks using a 20K-sample training dataset with a generative correction mechanism

## Executive Summary
GM-PRM transforms traditional Process Reward Models from passive verifiers into active reasoning collaborators by generating corrected versions of erroneous reasoning steps. This generative approach provides fine-grained analysis across step intent, visual alignment, and logical soundness, enabling a Refined Best-of-N inference framework that actively enhances solution quality. The model achieves state-of-the-art results across multiple multimodal math benchmarks while demonstrating remarkable data efficiency with only 20K training samples.

## Method Summary
GM-PRM is a generative Process Reward Model trained via supervised fine-tuning on a 19,614-sample dataset created using GPT-4o for textual analysis and Monte Carlo estimation for filtering. The model outputs structured critiques (step intent, image alignment, reasoning logic) and generates corrected versions of the first erroneous step it identifies. During inference, Refined-BoN generates N/2 initial solutions, uses GM-PRM to identify and correct errors, then continues generation from the corrected prefix to complete the solution pool before selecting the best answer.

## Key Results
- Achieves state-of-the-art results on five multimodal math benchmarks using only 20K training samples
- GM-PRM improves accuracy by 1.9% to 4.5% over self-consistency baselines across six different MLLMs
- Particularly effective on plane geometry problems where visual alignment errors are common
- Shows consistent improvements across multiple datasets with Refined-BoN outperforming standard Best-of-N

## Why This Works (Mechanism)

### Mechanism 1: Generative Critique and Correction
Replacing binary classification with generative text for error analysis and correction enables active reasoning collaboration rather than passive verification. The model is trained via supervised fine-tuning to output structured textual critiques and a refined version of the first erroneous step, transforming the PRM from a scalar outputter to a generative corrector. The core assumption is that the policy model can effectively utilize the corrected step as a prefix to generate superior reasoning trajectories.

### Mechanism 2: Refined Best-of-N (Refined-BoN)
Actively refining flawed candidate solutions at test-time scaling improves reasoning diversity and accuracy more effectively than passive selection. Instead of simply selecting the best of N static solutions, the framework uses the PRM to identify the first error in initial solutions, generates a correction, feeds the correct prefix back to the policy model to regenerate the rest of the solution, and then selects the best. The core assumption is that the corrected prefix provides a stronger signal than re-sampling from the start.

### Mechanism 3: Visual Alignment Evaluation
Explicitly evaluating "image alignment" prevents cascading errors caused by visual misinterpretation in geometry problems. The model evaluates whether reasoning steps correctly utilize visual information rather than just checking logical consistency, forcing reasoning to be grounded in the provided image. The core assumption is that the model has sufficient visual grounding capabilities to detect discrepancies between textual reasoning and image content.

## Foundational Learning

- **Concept: Process Reward Models (PRMs) vs. Outcome Reward Models (ORMs)**
  - Why needed: GM-PRM is a specific instantiation of a PRM that grades the *process* rather than just the *outcome*
  - Quick check: Does an ORM reward a correct answer derived from flawed logic? (Yes. GM-PRM aims to fix this)

- **Concept: Test-Time Scaling (TTS) / Best-of-N**
  - Why needed: Refined-BoN is a modification of standard TTS strategies that increases compute at inference
  - Quick check: How does Refined-BoN differ from standard Best-of-N? (Standard BoN selects from static pool; Refined-BoN modifies pool dynamically)

- **Concept: Supervised Fine-Tuning (SFT) with Synthetic Data**
  - Why needed: GM-PRM is trained on dataset generated by GPT-4o and filtered by Monte Carlo estimation
  - Quick check: How was the "ground truth" for error analysis generated? (By prompting GPT-4o and filtering with MC estimation)

## Architecture Onboarding

- **Component map:** Policy Model (MLLM) -> GM-PRM (Critic) -> Refined-BoN (Inference Loop)
- **Critical path:**
  1. Data Curation: Select geometry/function data -> GPT-4o generates critiques/corrections -> MC estimation filters -> Result: 20K SFT dataset
  2. Training: SFT of Qwen2.5-VL-7B on 20K dataset with frozen ViT and all other parameters trainable
  3. Inference: Policy Model generates N/2 solutions -> GM-PRM critiques -> If error, GM-PRM generates corrected step -> Policy Model continues from corrected step to generate remaining N/2 solutions

- **Design tradeoffs:** Data Quality vs. Cost (synthetic data vs. human annotation), Complexity vs. Interpretability (generative vs. binary classifier)
- **Failure signatures:** Error propagation from bad MC filtering, Policy Model stubbornness to ignore corrections, Visual misalignment from weak visual encoder
- **First 3 experiments:**
  1. Baseline Validation: Evaluate base policy model on MathVista/MathVerse using standard Best-of-N with self-consistency
  2. Ablation on Aggregation: Compare different aggregation strategies (Average vs. Min vs. Max) to verify "Average" superiority
  3. Refined-BoN vs. BoN: Implement full Refined-BoN loop (N=8) and compare Pass@8 and final accuracy against standard BoN

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several implications emerge from the methodology and results regarding the generalizability and limitations of the generative correction approach.

## Limitations
- Heavy reliance on GPT-4o for both training data generation and Monte Carlo filtering creates potential for systematic bias
- 20K sample size, while data-efficient, may not generalize robustly across diverse mathematical domains beyond geometry/function focus
- Framework effectiveness depends critically on policy model's ability to utilize corrected prefixes, which may not generalize to all MLLM architectures

## Confidence
- High Confidence: Effectiveness of Refined-BoN over standard Best-of-N for improving solution quality
- Medium Confidence: Data efficiency claim (20K samples sufficient) given limited cross-domain validation
- Medium Confidence: Superiority of average step scoring over other aggregation methods
- Low Confidence: Visual alignment evaluation mechanism's specific contribution due to lack of independent visual grounding benchmarking

## Next Checks
1. Cross-Domain Robustness Test: Evaluate GM-PRM on non-visual mathematical domains to determine generalizability beyond geometry/function problems
2. Correction Quality Analysis: Conduct human evaluation of generated corrections to measure hallucination rates and assess genuine improvement versus new errors
3. Visual Grounding Isolation: Implement ablation where GM-PRM's visual encoder is disabled to determine specific contribution of visual alignment component versus correction capability alone