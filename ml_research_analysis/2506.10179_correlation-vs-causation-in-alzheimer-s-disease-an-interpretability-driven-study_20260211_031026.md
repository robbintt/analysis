---
ver: rpa2
title: 'Correlation vs causation in Alzheimer''s disease: an interpretability-driven
  study'
arxiv_id: '2506.10179'
source_url: https://arxiv.org/abs/2506.10179
tags:
- features
- disease
- correlation
- cognitive
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study explored the relationship between correlation and causation\
  \ in Alzheimer's disease by integrating machine learning feature importance with\
  \ classical correlation analysis. Using XGBoost classification and SHAP interpretability,\
  \ the research identified key features\u2014such as CDRSB, DIGITSCOR, and APOE4\u2014\
  that strongly influence disease classification."
---

# Correlation vs causation in Alzheimer's disease: an interpretability-driven study

## Quick Facts
- arXiv ID: 2506.10179
- Source URL: https://arxiv.org/abs/2506.10179
- Authors: Hamzah Dabool; Raghad Mustafa
- Reference count: 9
- Primary result: XGBoost identified CDRSB, DIGITSCOR, and APOE4 as key features for AD classification, but correlation ≠ causation distinction remains unresolved

## Executive Summary
This study investigates the relationship between correlation and causation in Alzheimer's disease using machine learning interpretability tools. By combining XGBoost classification, SHAP value decomposition, and correlation analysis on ADNI datasets, the research identifies clinically relevant features while explicitly distinguishing predictive importance from causal mechanisms. The work demonstrates that while features like CDRSB and APOE4 strongly influence disease classification, their role as causal drivers versus downstream markers remains an open question requiring formal causal inference approaches.

## Method Summary
The study merged ADNI1, ADNIGo, and ADNI2 datasets, resulting in 11,603 samples with 41 features after removing duplicates and records with missing class labels. XGBoost classification with 5-fold cross-validation achieved 75.96% accuracy. Feature importance was extracted using gain-based ranking, correlation matrices computed across all features, and SHAP values calculated for class-specific attribution using TreeExplainer. The analysis explicitly distinguishes between predictive importance and causal mechanisms, acknowledging that high correlation or importance scores do not establish causation.

## Key Results
- XGBoost identified CDRSB, DIGITSCOR, and APOE4 as top features for AD classification accuracy
- Correlation matrices revealed clusters of interrelated clinical and biomarker variables (TAU, PTAU, ABETA, Ecog scores)
- SHAP decomposition showed class-specific feature contributions, with CDRSB and DIGITSCOR consistently impacting all diagnostic classes
- The study explicitly demonstrated that high correlation does not imply causation in AD feature relationships

## Why This Works (Mechanism)

### Mechanism 1: XGBoost Feature Importance as Diagnostic Signal Extraction
Gradient boosting identifies clinical and cognitive features that maximize classification accuracy across AD diagnostic categories. XGBoost constructs sequential decision trees where each split optimizes information gain; features appearing more frequently and earlier in trees receive higher importance scores. CDRSB dominates because it directly encodes disease severity. The core assumption is that features with high predictive utility for classification are candidates for further causal investigation, but predictive importance ≠ causal mechanism. Break condition: If feature importance were interpreted as causation directly, the mechanism would fail—importance reflects correlation with outcome, not mechanistic influence.

### Mechanism 2: SHAP Decomposition for Class-Specific Attribution
SHAP values decompose model predictions into additive feature contributions, revealing how each variable influences specific diagnostic classes differently. SHAP computes Shapley values from cooperative game theory, assigning each feature a marginal contribution averaged across all possible feature orderings. This yields class-specific attribution scores. The core assumption is that the model's decision boundary approximates clinically meaningful distinctions; SHAP attributions reveal heterogeneous feature roles across disease stages. Break condition: If the underlying model is misspecified or classes are poorly defined, SHAP attributions become uninterpretable regardless of mathematical validity.

### Mechanism 3: Correlation Structure Analysis for Feature Redundancy Detection
Pairwise correlation matrices reveal clusters of co-varying features, identifying redundancy and potential confounding structures. Pearson correlation coefficients quantify linear relationships between feature pairs; hierarchical clustering in the heatmap exposes functional groupings (e.g., Ecog scores, TAU/PTAU/ABETA biomarkers). The core assumption is that high correlation indicates shared variance, which may reflect common underlying pathology but does not establish directionality or causation. Break condition: If nonlinear relationships dominate, Pearson correlations miss important structure; correlation also cannot detect confounding from unmeasured variables.

## Foundational Learning

- **Concept: Correlation ≠ Causation**
  - Why needed here: The paper's central thesis is that high correlation among features (e.g., MMSE and MOCA) does not establish that one causes the other or that either causes disease progression
  - Quick check question: If two cognitive scores correlate at r=0.85, can you conclude that improving one would improve the other? (Answer: No—both may reflect a common underlying process.)

- **Concept: Feature Importance vs. Causal Influence**
  - Why needed here: XGBoost importance and SHAP values measure how much a feature helps prediction, not whether intervening on that feature would change the outcome
  - Quick check question: A feature has the highest SHAP value. Does this mean it is a causal driver of Alzheimer's disease? (Answer: No—it may be a downstream consequence or proxy.)

- **Concept: SHAP Additive Attribution**
  - Why needed here: Understanding that SHAP decomposes predictions into sum of feature contributions enables interpretation of class-specific effects
  - Quick check question: If CDRSB has mean |SHAP| = 0.4 and DIGITSCOR has 0.25, what does this tell you about their relative roles? (Answer: CDRSB contributes more to model output magnitude on average, but this reflects predictive utility, not clinical importance.)

## Architecture Onboarding

- **Component map:** Data cleaning -> XGBoost classifier -> Feature importance extraction -> Correlation matrix computation -> SHAP value calculation
- **Critical path:** Data cleaning → duplicate removal, missing label handling → Model training with tuned hyperparameters → Feature importance extraction from trained model → Correlation matrix computation across all features → SHAP value calculation and class-wise decomposition
- **Design tradeoffs:** XGBoost handles missing values internally vs. imputation may preserve more information for other model types; 5-fold CV provides robust accuracy estimate vs. held-out test set would enable independent evaluation; SHAP is model-agnostic and theoretically grounded vs. computationally expensive for large feature sets
- **Failure signatures:** Interpreting high feature importance as causal mechanism (explicitly warned against in paper); assuming correlation clusters reflect causal pathways without further analysis; over-relying on SHAP for clinical decision-making without domain validation
- **First 3 experiments:** 
  1. Replicate correlation analysis with nonlinear measures: Replace Pearson with Spearman or mutual information to capture nonlinear dependencies that may reveal different feature clusters
  2. Add confounding analysis: Introduce partial correlation or adjustment for key covariates (e.g., age, APOE4) to test whether observed correlations are spurious
  3. Pilot causal inference method: Apply a basic causal discovery algorithm (e.g., PC algorithm or simple DAG specification) to the top 10 features to explore potential causal structures, acknowledging this remains exploratory and requires domain expert validation

## Open Questions the Paper Calls Out

### Open Question 1
Which of the identified high-importance features (e.g., CDRSB, DIGITSCOR, APOE4) act as causal drivers of Alzheimer's disease progression versus correlated downstream markers? The authors state "the question of which factors are drivers versus passengers" remains open, and that "many features may serve as markers or results of underlying pathology rather than direct causes." This remains unresolved because correlation analysis and SHAP-based feature importance cannot distinguish causal mechanisms from associative patterns; XGBoost identifies predictive power, not mechanistic influence. Evidence would require causal inference methods such as Mendelian randomization, longitudinal intervention studies, or structural causal models applied to these features.

### Open Question 2
Can integrating causal inference frameworks with the identified feature set disentangle direct effects from confounding in AD classification? The authors explicitly call for "integrating these findings with domain knowledge and applying causal inference frameworks" as essential next steps. This remains unresolved because the current study deliberately stops at identifying associations and predictive importance without implementing formal causal methods. Evidence would require application of directed acyclic graphs, do-calculus interventions, or instrumental variable approaches to quantify direct causal pathways.

### Open Question 3
Why do some features show divergence between correlation strength and SHAP-based predictive importance? The paper notes that highly correlated features (e.g., MMSE, MOCA) may reflect downstream consequences, while features with moderate correlation but high importance may capture subtler causal signals. This divergence is not investigated; the paper documents the phenomenon without explaining it. Evidence would require stage-stratified analysis examining how feature importance and correlation evolve across disease progression (CN → EMCI → LMCI → AD).

### Open Question 4
How does the high missingness rate (~43%) in the combined ADNI dataset affect the reliability of identified feature relationships? The methodology acknowledges ~250,480 missing values (43% of data) but relies on XGBoost's internal handling rather than systematic missingness analysis. No sensitivity analysis is conducted to assess whether missing data patterns bias the correlation and importance rankings. Evidence would require comparison of results across multiple imputation strategies or explicit modeling of missing data mechanisms.

## Limitations
- Purely observational design cannot establish causation without experimental intervention or formal causal inference methods
- Feature importance rankings may be sensitive to model hyperparameters and data preprocessing choices
- 75.96% accuracy leaves substantial room for misclassification, particularly in intermediate disease stages

## Confidence

**High Confidence:** Correlation ≠ causation distinction, feature importance methodology, SHAP value computation

**Medium Confidence:** Interpretation of SHAP class-wise contributions, correlation structure interpretation, clinical relevance of identified features

**Low Confidence:** Causal implications, generalizability to clinical practice, robustness across different ADNI cohorts

## Next Checks
1. Apply partial correlation analysis adjusting for age and APOE4 status to test whether observed feature correlations persist after controlling for key confounders
2. Replicate the analysis on an independent AD cohort to assess model generalizability and stability of feature importance rankings
3. Implement a simple causal discovery algorithm (e.g., PC or FCI) on the top 10 features to explore potential causal structures, followed by domain expert validation of any proposed causal relationships