---
ver: rpa2
title: Testing Transformer Learnability on the Arithmetic Sequence of Rooted Trees
arxiv_id: '2512.01870'
source_url: https://arxiv.org/abs/2512.01870
tags:
- sequence
- words
- prime
- tokens
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper tests whether a transformer network (GPT-2) can learn
  the internal grammar of a deterministic sequence of rooted trees derived from the
  prime factorizations of natural numbers. Each integer is mapped to a rooted planar
  tree, which is encoded as a Dyck word, forming an arithmetic text with measurable
  statistical structure.
---

# Testing Transformer Learnability on the Arithmetic Sequence of Rooted Trees

## Quick Facts
- arXiv ID: 2512.01870
- Source URL: https://arxiv.org/abs/2512.01870
- Reference count: 12
- Primary result: A transformer (GPT-2) trained on an arithmetic sequence of rooted trees (derived from prime factorizations) achieves moderate accuracy on next-word prediction and masked language modeling, outperforming a Markov chain baseline but struggling with long-range arithmetic dependencies, especially for primes.

## Executive Summary
This paper investigates whether transformer networks can learn the internal grammar of a deterministic sequence of rooted trees derived from the prime factorizations of natural numbers. Each integer is mapped to a rooted planar tree, encoded as a Dyck word, forming an arithmetic text with measurable statistical structure. A GPT-2 model is trained from scratch on the first 10^11 such encoded integers and evaluated on next-word prediction (NWP) and masked language modeling (MLM) tasks. The results show that the model outperforms a Markov chain baseline, achieving word-level accuracy around 0.3 and KL divergence around 0.02–0.03 on NWP, and token-level accuracy up to 0.4 on MLM at low temperature and masking rates. While the model captures some non-trivial statistical regularities, it struggles with long-range arithmetic dependencies, especially for prime numbers, indicating only partial mastery of the sequence grammar.

## Method Summary
The authors construct an arithmetic sequence by mapping each natural number to a rooted planar tree via its prime factorization. Each tree is encoded as a Dyck word (a balanced parentheses string), forming an arithmetic text. A GPT-2 model is trained from scratch on the first 10^11 such encoded integers. The model is evaluated on two tasks: next-word prediction (NWP) and masked language modeling (MLM). Performance is compared to a Markov chain baseline. The study focuses on the model's ability to predict primes, square-free numbers, and capture the underlying arithmetic structure.

## Key Results
- GPT-2 outperforms a Markov chain baseline on both NWP (word-level accuracy ~0.3, KL divergence ~0.02–0.03) and MLM (token-level accuracy up to 0.4 at low temperature).
- The model correctly predicts primes and square-free numbers with moderate precision and recall (~0.3), but struggles with long-range arithmetic dependencies.
- On the MLM task, performance is better at low temperature and masking rates, but this may be due to local context rather than deep structural understanding.

## Why This Works (Mechanism)
The transformer's attention mechanism allows it to capture local and some long-range dependencies in the Dyck word encoding of rooted trees. The Dyck structure provides a clear grammatical scaffold, which the model can partially exploit. However, the multiplicative nature of the underlying arithmetic (prime factorization) introduces dependencies that are harder to learn, especially for primes and numbers with complex factorizations. The model's partial success suggests it can learn some statistical regularities, but not the full arithmetic grammar.

## Foundational Learning
- **Prime factorization**: Maps integers to trees; needed for the sequence structure; check: verify trees match factorizations.
- **Dyck words**: Encode trees as balanced parentheses; needed for sequence encoding; check: validate Dyck word generation.
- **Attention mechanism**: Enables context capture in transformers; needed for sequence modeling; check: assess attention patterns.
- **Next-word prediction**: Standard language modeling task; needed for evaluation; check: compare to baseline.
- **Masked language modeling**: Tests local context use; needed for robustness; check: vary temperature/masking.
- **Markov chains**: Baseline for sequence modeling; needed for comparison; check: confirm baseline setup.

## Architecture Onboarding
- **Component map**: Natural numbers → Prime factorization → Rooted trees → Dyck words → Transformer (GPT-2) → NWP/MLM tasks.
- **Critical path**: Data generation (factorization → trees → Dyck) → Model training (GPT-2) → Evaluation (NWP/MLM).
- **Design tradeoffs**: Dyck encoding simplifies tree structure but may obscure arithmetic details; model size vs. data scale; sequence length vs. performance.
- **Failure signatures**: Low accuracy on primes and long-range dependencies; failure to generalize complex multiplicative structures; modest precision/recall (~0.3).
- **Exactly 3 first experiments**: 1) Test on held-out complex numbers (e.g., semiprimes). 2) Train on subsets (e.g., only primes). 3) Vary sequence length to assess performance degradation.

## Open Questions the Paper Calls Out
None provided.

## Limitations
- The model struggles with long-range arithmetic dependencies, especially for prime numbers.
- Performance metrics (e.g., 0.3 accuracy for primes) are modest, raising questions about depth of learned grammar.
- The study does not explore the impact of varying sequence length, model size, or training data scale on performance.
- The Dyck word encoding may obscure or simplify some structural features of the rooted trees.

## Confidence
- **High confidence**: Transformers outperform a Markov chain baseline on both NWP and MLM tasks.
- **Medium confidence**: The model captures "non-trivial statistical regularities," as demonstrated but with significant gaps, especially for long-range dependencies.
- **Low confidence**: The claim that the model "correctly predicts primes and square-free numbers with moderate precision and recall" is only modestly supported by the reported precision/recall (~0.3).

## Next Checks
1. Test the model's performance on a held-out subset of numbers with known complex multiplicative structures (e.g., highly composite numbers, semiprimes) to better characterize its grasp of arithmetic relationships.
2. Conduct ablation studies by training on subsets of the data (e.g., only primes, only square-free numbers) to isolate the model's ability to learn specific arithmetic patterns.
3. Evaluate the model's robustness to sequence length by training and testing on progressively longer segments of the arithmetic sequence, measuring how performance degrades (if at all) with increased depth.