---
ver: rpa2
title: Predicting Oscar-Nominated Screenplays with Sentence Embeddings
arxiv_id: '2511.05500'
source_url: https://arxiv.org/abs/2511.05500
tags:
- script
- summary
- dataset
- work
- title
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Movie-O-Label, a new dataset combining movie
  scripts from MovieSum with curated Oscar records, enabling prediction of Oscar-nominated
  screenplays using modern text embeddings. Scripts, summaries, and titles were encoded
  with E5 embeddings and classified using logistic regression.
---

# Predicting Oscar-Nominated Screenplays with Sentence Embeddings

## Quick Facts
- arXiv ID: 2511.05500
- Source URL: https://arxiv.org/abs/2511.05500
- Reference count: 3
- This paper introduces Movie-O-Label, a new dataset combining movie scripts from MovieSum with curated Oscar records, enabling prediction of Oscar-nominated screenplays using modern text embeddings. Scripts, summaries, and titles were encoded with E5 embeddings and classified using logistic regression. The best model combining all three features achieved a macro F1 score of 0.66, precision-recall AP of 0.445, and ROC-AUC of 0.79. Results show that embeddings alone can effectively distinguish nominated screenplays, outperforming random baselines and rivaling earlier feature-engineered approaches. The approach is lightweight, reproducible, and runs on modest hardware, suggesting practical utility for screenwriters and industry professionals. Future work includes using larger models, video signals, multilingual datasets, and incorporating non-textual features.

## Executive Summary
This paper introduces Movie-O-Label, a new dataset combining movie scripts from MovieSum with curated Oscar records, enabling prediction of Oscar-nominated screenplays using modern text embeddings. Scripts, summaries, and titles were encoded with E5 embeddings and classified using logistic regression. The best model combining all three features achieved a macro F1 score of 0.66, precision-recall AP of 0.445, and ROC-AUC of 0.79. Results show that embeddings alone can effectively distinguish nominated screenplays, outperforming random baselines and rivaling earlier feature-engineered approaches. The approach is lightweight, reproducible, and runs on modest hardware, suggesting practical utility for screenwriters and industry professionals. Future work includes using larger models, video signals, multilingual datasets, and incorporating non-textual features.

## Method Summary
The method involves merging the MovieSum dataset with Oscar nomination records to create the Movie-O-Label dataset (2,200 entries, ~19% positive). Text fields (title, Wikipedia summary, script_clean) are chunked (400 words, 80 overlap) to handle long scripts exceeding transformer context limits. Each chunk is encoded using E5-base-v2 embeddings with a "query: " prefix. Pooling strategies (mean, max, mean-max) are applied to aggregate chunk embeddings per field, followed by L2 normalization and concatenation. Logistic regression (L2 penalty, C=1.0, class_weight="balanced", max_iter=5000) classifies the fused embeddings. Threshold tuning on validation data maximizes F1 for the positive class.

## Key Results
- Best model (Script+Summary+Title) achieves ROC-AUC 0.790, PR-AUC 0.455, and Macro-F1 0.664
- Script-only model achieves ROC-AUC 0.742, PR-AUC 0.415, outperforming random baseline (PR-AUC 0.19)
- Embedding-based approach rivals earlier feature-engineered methods without manual feature engineering

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Chunked embeddings with Mean-Max pooling capture screenplay-level semantic signals relevant to nomination prediction.
- Mechanism: Long scripts exceeding transformer context limits are split into 400-word overlapping chunks, each encoded independently by E5, then aggregated via mean pooling (global semantic summary) and max pooling (strongest localized signals). The concatenation preserves both averaged meaning and salient features before L2 normalization.
- Core assumption: The semantic patterns distinguishing nominated screenplays are distributed across the text and recoverable through this pooling strategy.
- Evidence anchors:
  - [section 3.2]: "Mean pooling calculates the averages of all embeddings of a document... Max pooling takes, for each embedding dimension, the maximum value across all chunk embeddings."
  - [section 4]: Table 6 shows Script-only achieves ROC-AUC 0.742, PR-AUC 0.415—substantially above the 0.19 random baseline.
  - [corpus]: Weak corpus evidence for screenplay-specific embedding benchmarks; related work focuses on summarization or dialogue analysis, not award prediction.
- Break condition: If salient nomination signals require cross-scene dependencies longer than 400 words, or are concentrated in structural features (e.g., dialogue ratio) not captured by semantic embeddings, performance ceiling is limited.

### Mechanism 2
- Claim: Combining script, summary, and title embeddings yields complementary predictive signals.
- Mechanism: Scripts provide scene-level detail (median ~32k tokens), summaries offer condensed plot synopses (median ~826 tokens), and titles contribute minimal but non-zero signal. Concatenating their pooled representations allows the classifier to leverage both granular narrative content and high-level descriptors.
- Core assumption: Wikipedia summaries accurately reflect the screenplay content and nomination-relevant qualities; titles carry weak semantic associations with award-worthy material.
- Evidence anchors:
  - [section 4]: "Script+Summary+Title achieved ROC-AUC 0.790, PR-AUC 0.455, Macro-F1 0.664" vs. Script-only ROC-AUC 0.742.
  - [section 5]: "Combining script and summary already show much better results... This suggests that plot descriptions and detailed scene-level text contain complementary signals."
  - [corpus]: STAGE benchmark confirms screenplays are "rich long-form narratives"—supporting the need for multi-granularity representations.
- Break condition: If summaries are noisy (variable length/quality as noted in limitations) or if title signal is spurious correlation, fusion gains diminish with noisier data.

### Mechanism 3
- Claim: A linear classifier suffices because predictive power resides primarily in pre-trained embeddings.
- Mechanism: E5 embeddings, pre-trained on large corpora, already encode semantic structure. Logistic regression only learns a decision boundary in this space without feature engineering.
- Core assumption: The embedding space is well-calibrated for screenplay quality signals; no complex non-linear interactions are required.
- Evidence anchors:
  - [section 3.2]: "This model was assumed to be fully sufficient... because the main semantic and structural information is already captured by the pre-trained text embeddings."
  - [section 5]: "Most of the predictive power probably comes from the text embeddings and not from the classifier itself."
  - [corpus]: Chiu et al. (2020) achieved ~0.75 ROC-AUC with handcrafted features; this approach rivals it without feature engineering.
- Break condition: If nomination decisions involve complex non-linear interactions (e.g., genre × budget × release timing), linear classifiers on embeddings alone are insufficient.

## Foundational Learning

- **Sentence/Text Embeddings (E5)**
  - Why needed here: Core representation learning mechanism; understanding what E5 encodes (semantic similarity, not structural features) clarifies why dialogue ratio or pacing require alternative approaches.
  - Quick check question: Can you explain why mean pooling might dilute rare but important signals that max pooling would preserve?

- **Class Imbalance Metrics (PR-AUC vs. ROC-AUC)**
  - Why needed here: Dataset is 19% positive; ROC-AUC can be misleadingly optimistic. PR-AUC and macro-F1 are the reported "most informative metrics."
  - Quick check question: Given a 19% positive rate, what PR-AUC would a random classifier achieve? (Answer: 0.19, the baseline cited in the paper.)

- **Pooling Strategies for Long Documents**
  - Why needed here: Scripts exceed E5's context window; chunking + pooling is the architectural response. Understanding tradeoffs (mean vs. max vs. mean-max) is critical for extension work.
  - Quick check question: Why might mean-max pooling outperform mean alone for screenplays where key scenes (e.g., climax) are sparse but predictive?

## Architecture Onboarding

- **Component map:**
  - Movie-O-Label dataset → title, summary, script_clean fields
  - Chunking: Word-based splits (400 words, 80 overlap) for summary and script; title as single chunk
  - Encoding: E5-base-v2 (768-dim) with "query: " prefix
  - Pooling: Mean + Max per field → concatenate → L2 normalize
  - Fusion: Vertical stack of title, summary, script vectors
  - Classification: Logistic regression (L2, C=1.0, class_weight="balanced", max_iter=5000)
  - Threshold optimization: Scan 0.05–0.95 on validation set for max positive-class F1

- **Critical path:**
  1. Data preparation: Merge MovieSum with Oscar records via IMDb ID → 2,200 entries
  2. Chunking long texts (script median 77 chunks; summary median 2.5 chunks)
  3. Embedding computation (~1 hour on RTX 3060)
  4. Threshold tuning on validation set (critical for imbalanced data)

- **Design tradeoffs:**
  - Chunk size 400 words balances context preservation vs. transformer limits; smaller chunks lose cross-chunk dependencies.
  - E5-base-v2 chosen for hardware constraints (12GB VRAM); larger models (e5-large-v2) showed no improvement in pre-tests, suspected due to dataset size limitations.
  - 60/20/20 split trades training data for robust validation; stratification preserves 19% positive rate.

- **Failure signatures:**
  - Low recall on positive class: Threshold not optimized; check validation curve.
  - PR-AUC near baseline (0.19): Embeddings not capturing nomination signals; inspect failure cases for systematic patterns (e.g., genre bias).
  - Convergence warnings: Increase max_iter beyond 5000 or check feature scaling.

- **First 3 experiments:**
  1. **Ablation by feature:** Train separate models on script, summary, and title alone; compare to fusion model to quantify complementarity.
  2. **Chunk size sensitivity:** Test 200-word and 800-word chunks to probe whether 400 words captures sufficient context or loses cross-scene dependencies.
  3. **Alternative pooling:** Compare mean-only, max-only, and attention-weighted pooling to assess whether mean-max is optimal or if attention over chunks improves salient-scene focus.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would integrating video signals from trailers or released films improve prediction accuracy beyond text-only embeddings?
- Basis in paper: [explicit] Future work section states: "integrating video-to-LLM signals, e.g. from trailers or released films. This could improve the model's predictive ability by providing complementary information that pure text can not capture."
- Why unresolved: Current model uses only screenplay text, summaries, and titles. Human voters watch films, so visual and auditory signals may contain nomination-relevant information absent from scripts.
- What evidence would resolve it: Train multimodal models combining script embeddings with video features extracted from trailers/films; compare PR-AUC and macro F1 against text-only baseline on same test split.

### Open Question 2
- Question: Can LLM personas simulate human Oscar voting behavior reliably enough to improve nomination predictions?
- Basis in paper: [explicit] Future work proposes: "an artificial jury could be created from LLM personas. So, the human voting process can probably be simulated and improve model predictions," citing Liu et al. (2024) on reliability concerns.
- Why unresolved: LLM-as-judge approaches show promise but have known reliability issues; it is unclear whether persona-based simulation captures the subjective, social, and strategic factors in Academy voting.
- What evidence would resolve it: Construct LLM persona ensembles mimicking voter demographics; compare their aggregated predictions against actual nomination outcomes and against the logistic regression baseline.

### Open Question 3
- Question: Would incorporating non-textual features (budget, genre, studio, marketing signals) capture nomination factors that text embeddings miss?
- Basis in paper: [explicit] Discussion notes: "Oscar nominations for screenplays depend on many non-textual factors... such as production campaigns, studio reputation, or release timing, which were not considered here." Future work adds: "combining other non-text features... might capture important nomination factors beyond the script itself."
- Why unresolved: Text-only models cannot capture industry dynamics, campaign spending, or timing strategies that influence voter attention and preference.
- What evidence would resolve it: Augment the embedding-based model with metadata features; measure whether combined model significantly outperforms text-only model on held-out years.

### Open Question 4
- Question: Do larger embedding models outperform e5-base-v2 given more training data, or does the 400-word chunking strategy inherently limit their advantage?
- Basis in paper: [inferred] The paper reports: "Pre-tests with e5-large-v2 embeddings could not outperform the best-performance model... It is suspected that the used dataset (only 2200 scripts) is too small for the larger embedding space and that chunking long scripts into 400-word segments may have limited the advantage."
- Why unresolved: It remains unclear whether performance saturation stems from dataset size, chunking granularity, or model capacity mismatch; the interaction between these factors is untested.
- What evidence would resolve it: Systematic experiments varying (1) training set size, (2) chunk size/overlap strategies, and (3) embedding model scale; identify conditions under which larger models yield gains.

## Limitations
- Dataset size: Only 2,200 entries, with 19% positive rate, may limit model generalizability and larger model performance
- No visual/audio features: Model relies solely on text, missing non-textual nomination factors like production campaigns or release timing
- Unreported details: Final threshold value, exact script_clean regex rules, and random seed not specified

## Confidence
- Method reproducibility: High — method is clearly specified with concrete hyperparameters and implementation details
- Claims about embedding-based approach: Medium — strong baseline performance but limited ablation studies and dataset size concerns
- Future work proposals: Medium — reasonable but untested extensions with unclear practical gains

## Next Checks
1. Verify stratified 60/20/20 split preserves 19% positive rate; compare training/validation/test distributions
2. Run ablation experiments: script-only, summary-only, title-only, script+summary, and all three to quantify feature complementarity
3. Test alternative pooling strategies (mean-only, max-only, attention-weighted) to assess whether mean-max is optimal for screenplay prediction