---
ver: rpa2
title: Robustness Test for AI Forecasting of Hurricane Florence Using FourCastNetv2
  and Random Perturbations of the Initial Condition
arxiv_id: '2512.05323'
source_url: https://arxiv.org/abs/2512.05323
tags:
- noise
- initial
- weather
- conditions
- added
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The study tests the robustness of the AI weather forecasting model\
  \ FourCastNetv2 by introducing varying levels of Gaussian noise and fully random\
  \ initial conditions to simulate Hurricane Florence (2018). Under low to moderate\
  \ noise, the model accurately tracks the hurricane\u2019s trajectory, though it\
  \ consistently underestimates storm intensity and persistence."
---

# Robustness Test for AI Forecasting of Hurricane Florence Using FourCastNetv2 and Random Perturbations of the Initial Condition

## Quick Facts
- **arXiv ID:** 2512.05323
- **Source URL:** https://arxiv.org/abs/2512.05323
- **Reference count:** 12
- **Primary result:** FourCastNetv2 accurately tracks hurricane trajectory under moderate noise but underestimates storm intensity; random initializations converge to coherent forecasts after several timesteps.

## Executive Summary
This study evaluates the robustness of the AI weather forecasting model FourCastNetv2 (FCNv2) by injecting Gaussian noise and fully random initial conditions to simulate Hurricane Florence (2018). Under low to moderate noise, FCNv2 maintains accurate hurricane tracking, though it systematically underestimates storm intensity and persistence. With high noise levels, trajectory accuracy degrades but general storm direction is preserved. When given fully random initial conditions, the model quickly produces smooth, coherent forecasts, demonstrating strong regularization tendencies. However, some variables like surface pressure retain unrealistic randomness, and humidity can exceed physical bounds. These results indicate that FCNv2 is resilient to imperfect inputs and tends to generate stable predictions, but may lack full physical consistency. The findings suggest potential benefits for ensemble forecasting, while highlighting the need for integrating physical constraints into purely data-driven models for operational use.

## Method Summary
The study uses the pretrained FourCastNetv2 model to forecast Hurricane Florence from September 13-16, 2018. The authors perturb initial conditions with zero-mean Gaussian noise at seven levels (0-50% of variable standard deviation) and run 30 trials per level with different random seeds. They also test fully random initial conditions sampled from normal distributions. For each scenario, they generate 84-hour forecasts (15 timesteps × 6 hours) and evaluate hurricane tracking via minimum MSL pressure identification. The analysis includes mean trajectory error (MTE), pixelwise MSL error, and statistical moment analysis of forecast fields.

## Key Results
- FCNv2 accurately tracks hurricane trajectory with up to 10-15% noise injection, with MTE increasing beyond this threshold
- The model consistently underestimates storm intensity, with MSL pressure biased high near the eye
- Fully random initial conditions converge to spatially coherent fields within 84 hours, demonstrating strong implicit regularization
- Some variables (surface pressure, relative humidity) retain unphysical values or randomness even after multiple timesteps

## Why This Works (Mechanism)

### Mechanism 1: Noise Resilience through Learned Large-Scale Pattern Prioritization
- **Claim:** FCNv2 maintains hurricane tracking accuracy under moderate noise because it prioritizes statistically dominant atmospheric patterns over fine-scale perturbations.
- **Mechanism:** Training on ERA5 reanalysis data teaches the model to extract coherent atmospheric signals from inherently noisy observations. The model's learned representations emphasize large-scale flow patterns (which govern hurricane motion) while treating small-scale perturbations as less consequential for dynamics. Zero-mean Gaussian noise primarily distorts fine structure while preserving the large-scale gradients that steer storms.
- **Core assumption:** Perturbations that preserve zero-mean statistics keep inputs within the learned distribution manifold.
- **Evidence anchors:**
  - [abstract]: "FCNv2 accurately preserves hurricane features under low to moderate noise injection. Even under high levels of noise, the model maintains the general storm trajectory and structure"
  - [section 3a, Fig 5]: "For up to 10% noise added, the predicted hurricane trajectory tends to accurately follow the true storm path...As noise levels rise to 20% or above, the predicted trajectories begin to increasingly deviate"
  - [corpus]: Arnoldi Singular Vector perturbations study (arxiv:2506.22450) similarly examines ML weather model sensitivity to initial condition errors, though using structured perturbations
- **Break condition:** Biased perturbations (α≠0) cause complete forecast divergence within a few timesteps because input standardization produces non-zero-centered data that propagates through the network. Zero-mean noise exceeding ~20% of standard deviation degrades positional accuracy significantly.

### Mechanism 2: Implicit Regularization Drives Convergence from Random States
- **Claim:** FCNv2 transforms fully random initial conditions into spatially coherent forecasts through iterative projection onto learned atmospheric manifolds.
- **Mechanism:** The Spherical Fourier Neural Operator architecture, trained exclusively on smooth ERA5 fields, imposes strong structural priors. Each 6-hour prediction step applies learned dynamics that progressively smooth and restructure the state. After multiple autoregressive steps, random noise is suppressed as the model projects inputs toward physically plausible configurations that minimize learned loss functions.
- **Core assumption:** The learned dynamics manifold is broad enough to accommodate arbitrary initial conditions given sufficient timesteps.
- **Evidence anchors:**
  - [abstract]: "With full random initial conditions, the model generates smooth and cohesive forecasts after a few timesteps, implying the model's tendency towards stable, smoothed outputs"
  - [section 3b, Fig 9]: "across each tested distribution the initial conditions were sampled from, the MSL pressure forecasts evolve into spatially coherent fields within 84 hours"
  - [corpus]: Weak/missing - No directly comparable fully-random initialization studies found in corpus; adversarial perturbation work (arxiv:2512.08832, arxiv:2504.15942) explores related but distinct robustness questions
- **Break condition:** Variable-specific failures occur: surface pressure (SP) retains input randomness; relative humidity (RH) develops spatial patterns but preserves unphysical values (negative or >100%) because the model lacks hard physical constraints.

### Mechanism 3: Training Distribution Bias Causes Systematic Intensity Underestimation
- **Claim:** FCNv2 consistently underestimates hurricane intensity because loss minimization during training favors statistically typical states over rare extremes.
- **Mechanism:** Extreme weather events represent tail distributions in training data. The model learns to minimize average prediction error across all samples, which creates implicit bias toward mean atmospheric conditions. Hurricane cores—intense, localized pressure minima—are systematically "filled in" toward more common pressure values, reducing both peak intensity and storm persistence in forecasts.
- **Core assumption:** The model optimizes for average-case accuracy rather than tail-event fidelity.
- **Evidence anchors:**
  - [abstract]: "FCNv2 consistently underestimates storm intensity and persistence across all levels of injected noise"
  - [section 3a]: "MSL pressure is consistently overestimated near the eye of Florence, reinforcing a tendency to underestimate storm intensity, aligning with what Charlton-Perez (2024) and Shi et al. (2025) presented"
  - [corpus]: Corpus confirms this is a general AI weather model limitation—multiple papers (referenced in section 1) document similar underestimation in Pangu-Weather, GraphCast, and FourCastNet for extreme events
- **Break condition:** This is a systematic architectural bias, not a threshold effect. Mitigation requires physics-informed constraints during training (Kashinath et al. 2021, referenced in section 4) or post-hoc calibration for extreme events.

## Foundational Learning

- **Concept: Ensemble Forecasting via Initial Condition Perturbations**
  - Why needed here: The paper's methodology generates forecast uncertainty estimates by running 30 perturbed simulations per noise level. Understanding why deterministic models require external ensemble methods—and how perturbation magnitude relates to forecast spread—is essential for interpreting results.
  - Quick check question: Why does the same noise level with different random seeds produce different hurricane tracks, and what does the spread of these tracks tell you about forecast uncertainty?

- **Concept: ERA5 Reanalysis as Ground Truth**
  - Why needed here: ERA5 provides both training data and evaluation baseline. It's a gridded reconstruction combining observations with numerical model assimilation—neither pure observation nor pure simulation. This affects what "truth" means in the paper's context.
  - Quick check question: If ERA5 already blends observations with model physics, what are the implications for a purely data-driven model trained on it?

- **Concept: Spherical Fourier Neural Operators (SFNOs)**
  - Why needed here: FCNv2's architecture learns mappings between function spaces in the spectral (Fourier) domain rather than operating purely on discrete grids. This architectural choice inherently promotes smooth solutions and explains the regularization behavior observed with random inputs.
  - Quick check question: How might learning atmospheric dynamics in spectral space versus physical space affect the model's ability to represent sharp gradients like hurricane eyewalls?

## Architecture Onboarding

- **Component map:** Retrieve ERA5 data -> Format to match FCNv2 variable ordering -> Load pretrained model -> Apply perturbations -> Run 6-hour timestep inference loop -> Extract features
- **Critical path:**
  1. Retrieve ERA5 data for target timeframe (Copernicus CDS API, 0.25° resolution)
  2. Format data to match FCNv2's expected variable ordering and dimensions
  3. Load pretrained model via Earth-2 MIP (NVIDIA GitHub repository)
  4. Apply perturbations to initial conditions if testing robustness
  5. Run inference iteratively for desired forecast horizon
  6. Extract features (e.g., minimum MSL for storm tracking) from outputs
- **Design tradeoffs:**
  - **Deterministic vs. probabilistic:** Single FCNv2 run gives one forecast; uncertainty quantification requires external ensemble generation (computationally expensive but tractable)
  - **Robustness vs. extreme fidelity:** Strong regularization helps noise tolerance but systematically underrepresents intensity of rare events
  - **Speed vs. physical consistency:** Purely data-driven inference is fast but can violate physical bounds (RH outside 0-100%, SP retaining noise)
- **Failure signatures:**
  - **Exploding/divergent forecasts within 1-3 timesteps:** Input standardization failure—likely mean shift in input data (α≠0 perturbation or wrong preprocessing statistics)
  - **Unphysical humidity values:** RH predictions outside [0, 100%] range indicate missing physical constraints; requires post-hoc clamping or physics-informed training
  - **Persistent randomness in surface pressure:** SP-specific architectural limitation; variable does not converge from random initializations
  - **Accurate track but weak intensity:** Expected behavior from training distribution bias; not a failure but a systematic limitation
- **First 3 experiments:**
  1. **Baseline reproduction:** Run FCNv2 on unperturbed ERA5 for Hurricane Florence (Sept 13-16, 2018). Track storm position via minimum MSL. Verify ~100-200km MTE and confirm intensity underestimation (~5-10 hPa pressure bias at storm center).
  2. **Noise sensitivity calibration:** Test 5%, 10%, 20%, 35% zero-mean Gaussian noise (α=0, β varied). Run 10 trials each. Plot MTE vs. noise percentage to identify the stability threshold (~10-15% based on paper). Document when track direction degrades vs. when positional error increases.
  3. **Variable-specific convergence audit:** Initialize with normal-distributed random fields (scaled to ERA5 statistics). Run 84-hour forecast. For each variable, measure time to spatial coherence (gradient smoothness) and check physical bounds violations. Flag SP and RH as known outliers; verify whether other variables (T, u, v, z) converge cleanly.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Do the robustness behaviors observed in FourCastNetv2 generalize to other AI weather forecasting models and different storm events?
- **Basis in paper:** [explicit] The conclusion states future work could "extend these experiments beyond FCNv2 and Hurricane Florence to assess whether the same behaviors appear across other AI weather forecasting models and different storm events."
- **Why unresolved:** This study restricted its scope to a single model (FourCastNetv2) and a single event (Hurricane Florence).
- **What evidence would resolve it:** Applying the same Gaussian noise and random initialization protocols to other architectures (e.g., GraphCast, Pangu-Weather) across multiple distinct case studies.

### Open Question 2
- **Question:** Does the integration of physical constraints improve the robustness of AI models to imperfect initial conditions compared to purely data-driven models?
- **Basis in paper:** [explicit] The authors ask if "integrating physical laws into data-driven AI weather forecasting models... may help maintain physicality and further improve robustness to imperfect initial conditions."
- **Why unresolved:** The paper demonstrates that purely data-driven models produce unphysical outputs (e.g., humidity out of bounds) under random initialization, but it does not test physics-informed alternatives.
- **What evidence would resolve it:** A comparative study benchmarking a physics-informed neural network against FourCastNetv2 using the paper's defined noise-injection metrics.

### Open Question 3
- **Question:** Can simple safeguards, such as clamping variables to physical bounds, ensure physical consistency without degrading forecast skill?
- **Basis in paper:** [explicit] The authors suggest "testing simple safeguards such as clamping variables with physical bounds (for instance, restricting RH to the 0–100% range) could help ensure physical consistency."
- **Why unresolved:** While the paper identifies unphysical outputs (e.g., negative relative humidity), it does not validate whether post-hoc corrections or in-line clamping would destabilize the model's learned dynamics.
- **What evidence would resolve it:** Experiments enforcing physical bounds on variables like Relative Humidity during inference, followed by an analysis of the trade-off between physical validity and trajectory error.

## Limitations

- **Single-event scope:** The robustness analysis focuses exclusively on Hurricane Florence, limiting generalizability to other storm types and meteorological conditions.
- **Physical constraint absence:** The model produces unphysical outputs (humidity out of bounds, persistent randomness in surface pressure) that are not addressed within the current architecture.
- **No comparison to physics-based ensembles:** The study does not benchmark FCNv2's perturbation-based ensemble against traditional physics-based ensemble forecasting systems in terms of computational cost or forecast skill.

## Confidence

- **High confidence:** The observed trajectory accuracy under moderate noise and the convergence from random initial conditions are well-supported by experimental results and reproducible with the described methodology.
- **Medium confidence:** The systematic intensity underestimation is consistent with literature and observed across multiple variables, but quantifying the exact magnitude of bias requires calibration against physical models.
- **Low confidence:** The paper's claim that FCNv2 is suitable for operational ensemble forecasting is largely aspirational, as the integration of physical constraints and comparison with existing methods are not demonstrated.

## Next Checks

1. **Scale sensitivity audit:** Repeat the noise injection experiments using ERA5 data for a small mesoscale convective system (e.g., a single thunderstorm complex) to determine if the 10-15% noise tolerance threshold holds across spatial scales.
2. **Physical constraint benchmark:** Implement post-hoc clamping of RH to [0,100%] and SP to physically plausible ranges. Re-run the fully random initial condition experiment and measure the impact on forecast coherence and skill metrics.
3. **Computational overhead analysis:** Run 30-member FCNv2 ensembles with varying noise levels and compare the total GPU hours required to a single run of a physics-based ensemble forecasting system (e.g., ECMWF EPS) for the same forecast horizon and resolution.