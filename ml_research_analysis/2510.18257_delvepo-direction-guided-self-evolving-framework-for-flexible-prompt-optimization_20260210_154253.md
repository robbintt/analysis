---
ver: rpa2
title: 'DelvePO: Direction-Guided Self-Evolving Framework for Flexible Prompt Optimization'
arxiv_id: '2510.18257'
source_url: https://arxiv.org/abs/2510.18257
tags:
- prompt
- role
- task
- prompts
- component
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces DelvePO, a direction-guided self-evolving
  framework for flexible prompt optimization that addresses the instability and local
  optima issues in existing methods. The core idea involves decoupling prompts into
  functional components analogous to genetic loci, then using a working memory mechanism
  (component memory and prompt memory) to guide evolutionary operations through task-evolution,
  solution-evolution, and memory-evolution modules.
---

# DelvePO: Direction-Guided Self-Evolving Framework for Flexible Prompt Optimization

## Quick Facts
- arXiv ID: 2510.18257
- Source URL: https://arxiv.org/abs/2510.18257
- Reference count: 40
- Primary result: Consistently outperforms baselines by 4.93-8.51 percentage points across 11 datasets and three LLMs

## Executive Summary
DelvePO introduces a direction-guided self-evolving framework that addresses instability and local optima issues in prompt optimization by decoupling prompts into functional components analogous to genetic loci. The framework uses working memory (component and prompt memory) to guide evolutionary operations through task-evolution, solution-evolution, and memory-evolution modules. Experiments show DelvePO consistently outperforms manual prompts, CoT methods, APE, PromptBreeder, and EvoPrompt across diverse task types while demonstrating better optimization speed.

## Method Summary
DelvePO operates through component-based prompt decomposition with working memory guidance. The framework breaks prompts into functional components (role, task_description, action, requirements, etc.) using HTML-like markup tags, then evolves these components independently while preserving overall structure. Two memory systems operate jointly: Component Memory stores before/after value pairs per component ordered by performance, while Prompt Memory stores complete prompts ordered by performance. The three-stage evolution process separates "what to evolve" (Task-Evolution) from "how to evolve" (Solution-Evolution), with LLMs extracting insights from memory to guide mutation and crossover operations. The framework initializes with 10 candidate values per component, randomly combines them into 10 initial prompts, then iteratively evolves through roulette wheel selection, insight extraction, and memory updates.

## Key Results
- Outperforms manual prompts by 8.51 percentage points on average across all tasks
- Beats CoT methods by 8.51 percentage points and outperforms specialized prompt optimization methods (APE, PromptBreeder, EvoPrompt) by 4.93-7.72 percentage points
- Shows consistent improvements across multiple task types: classification (3.78-8.14 pp), question answering (6.09-7.01 pp), sentiment analysis (2.53-6.43 pp), summarization (3.54-4.05 pp)
- Demonstrates better optimization speed than PromptBreeder while maintaining superior accuracy

## Why This Works (Mechanism)

### Mechanism 1: Component-Based Prompt Decomposition
- Claim: Decomposing prompts into modular functional components enables targeted optimization and prevents accidental loss of critical elements during evolution.
- Mechanism: The framework breaks prompts into components (role, task_description, action, requirements, etc.) using HTML-like markup tags. These components are independently evolved while preserving overall structure, analogous to genetic loci and alleles.
- Core assumption: Different prompt components have semi-independent effects on task performance, and targeted modification is more effective than whole-prompt mutation.
- Evidence anchors: [abstract] "we decouple prompts into different components that can be used to explore the impact that different factors may have on various tasks"; [section] Page 2: "due to the stochastic nature of the mutation process, the stochastic mutation process may accidentally remove this component. Once discarded, it cannot be reintegrated into subsequent evolutionary iterations"; [corpus] Weak direct evidence; LangGPT (cited) supports structured prompt design.
- Break condition: If component dependencies are strong (modifying one invalidates others), targeted optimization may still converge to local optima.

### Mechanism 2: Working Memory for Directional Guidance
- Claim: Maintaining explicit memory of evolutionary history reduces random exploration by providing directional insights to LLMs.
- Mechanism: Two memory systems operate jointly: Component Memory stores before/after value pairs per component ordered by performance; Prompt Memory stores complete prompts ordered by performance. Task-Evolution uses component memory to select mutation targets; Solution-Evolution uses prompt memory to generate variants.
- Core assumption: Historical performance patterns contain learnable regularities that LLMs can extract and apply.
- Evidence anchors: [abstract] "we introduce working memory, through which LLMs can alleviate the deficiencies caused by their own uncertainties and further obtain key insights to guide the generation of new prompts"; [section] Page 7, Table 3: Ablation shows removing either memory drops performance (SAMSum: 35.3→28.4-29.4); [corpus] SCOPE paper addresses similar agent evolution challenges.
- Break condition: If the search landscape is highly non-smooth, historical patterns may not extrapolate.

### Mechanism 3: Three-Stage Evolution with Separated Concerns
- Claim: Decoupling "what to evolve" (Task-Evolution) from "how to evolve" (Solution-Evolution) enables more controlled optimization.
- Mechanism: Task-Evolution: LLM extracts insights from component memory, selects promising component(s) to mutate; Solution-Evolution: LLM extracts insights from prompt memory, performs mutation (Sub-solution I) or mutation+crossover (Sub-solution II); Memory-Evolution: Updates both memories with new results; evaluates on dev set.
- Core assumption: LLMs can reliably extract actionable insights from structured memory and apply them consistently.
- Evidence anchors: [section] Page 5: "The main goal of solution evolution is to utilize the insights (derived from the prompts memory) and direction (received from the task-evolution)"; [section] Page 3-4: Figure 1 shows full workflow; Algorithm 1 in Appendix C formalizes the loop; [corpus] Self-evolving agent concepts appear in related work (SEW, wireless networks).
- Break condition: If insight extraction quality degrades (e.g., memory grows beyond LLM's reasoning capacity), guidance collapses to near-random mutation.

## Foundational Learning

- **Evolutionary Algorithms (mutation, crossover, selection)**
  - Why needed here: DelvePO uses roulette wheel selection and genetic operators. Understanding selection pressure, exploration vs. exploitation, and convergence is essential.
  - Quick check question: Why does roulette wheel selection maintain population diversity better than elitist selection?

- **Prompt Engineering Components**
  - Why needed here: The framework assumes components like role, task_description, and constraints have distinct effects. You need intuition for how each influences LLM behavior.
  - Quick check question: What is the expected behavioral difference between assigning role="Expert Translator" vs. role="Casual Paraphraser"?

- **LLM In-Context Learning and Context Limits**
  - Why needed here: Working memory is fed as context to guide evolution. Understanding context window constraints and in-context generalization is critical.
  - Quick check question: What happens if component memory grows to contain 100+ value pairs per component?

## Architecture Onboarding

- **Component map:**
  - Initialization: Task-agnostic template generates 10 candidate values per component → random combination → N=10 initial prompts → evaluate → store in prompt memory
  - Sampling Module: Roulette wheel selection picks 1 (mutation-only) or 2 (mutation+crossover) prompts
  - Task-Evolution: LLM reads component memory → extracts insights → outputs target component(s)
  - Solution-Evolution: LLM reads prompt memory → applies insights to mutate/crossover target components → outputs new prompt(s)
  - Memory-Evolution: Evaluate new prompts → update component memory (value pairs) and prompt memory (full prompts with scores)
  - Update: Merge evolved + current population → keep Top-N

- **Critical path:**
  1. Initialize population and prompt memory
  2. Loop (m epochs × n steps): Sample → Task-Evolve → Solution-Evolve → Evaluate → Memory-Evolve
  3. Return highest-scoring prompt

- **Design tradeoffs:**
  - Discrete vs. continuous prompt memory: Discrete = interpretable; continuous = preserves semantic context
  - Memory size vs. token cost: More memory improves guidance but increases API costs (Limitations acknowledges this)
  - Component value count: Table 4 shows 10–50 values yield similar initial performance, suggesting diminishing returns

- **Failure signatures:**
  - Premature convergence: Component memory lacks diversity → repetitive insights
  - Component loss during crossover: Without explicit tracking, crossover can discard useful elements (motivation cited for EvoPrompt issues)
  - Token overflow: Memory as input tokens can exceed context limits for long runs
  - High variance: Single-seed evaluation may not capture instability

- **First 3 experiments:**
  1. **Reproduce ablation** (Table 3): Run w/ both memories, w/o component memory, w/o prompt memory, w/o both on SAMSum or SQuAD to verify contribution
  2. **Component value sensitivity** (Table 4): Test #values ∈ {10, 20, 30, 40, 50} on SST-5 to confirm stability
  3. **Single-iteration trace**: Run one full evolution cycle on sentence simplification task; manually inspect Task-Evolution output (selected components), Solution-Evolution output (new prompt), and memory updates to validate pipeline correctness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can prompt compression techniques be integrated into the DelvePO framework to reduce token overhead from memory module inputs while preserving optimization quality?
- Basis in paper: [explicit] In Section 4.3 Cost Analysis, the authors state: "The primary factor is that the content stored in the memory module is included as part of the input provided to the target LLMs. In future work, we plan to integrate prompt compression techniques into the framework to reduce this overhead."
- Why unresolved: The current framework achieves strong performance but at higher token cost than baselines (Table 6); compression-accuracy trade-offs are unexplored.
- What evidence would resolve it: Experiments comparing DelvePO with and without prompt compression (e.g., LLMLingua, Selective Context) on same datasets, reporting both accuracy and token consumption metrics.

### Open Question 2
- Question: How does DelvePO performance scale with significantly larger LLMs (e.g., 70B+ parameters) and across multilingual or multi-modal domains?
- Basis in paper: [inferred] The Limitations section acknowledges: "Due to substantial computational costs, we cannot comprehensively evaluate all models and domains" and evaluated only three mid-sized LLMs on primarily English tasks.
- Why unresolved: Scaling behavior to larger models and non-English/multi-modal contexts remains unknown; component types may need task-specific adaptation.
- What evidence would resolve it: Benchmarks on 70B+ models (Llama-3-70B, GPT-4) and multilingual datasets (XNLI, mMARCO) or multi-modal tasks (VQA, image captioning).

### Open Question 3
- Question: What is the optimal configuration and capacity for Component Memory and Prompt Memory to balance guidance quality against memory size and retrieval efficiency?
- Basis in paper: [inferred] The ablation study (Table 3) demonstrates both memory types contribute substantially, but does not explore memory size limits, decay strategies, or whether ordered vs. sampled retrieval affects convergence.
- Why unresolved: Memory mechanisms are critical to performance, yet their scaling properties and optimal configurations are uncharacterized.
- What evidence would resolve it: Systematic experiments varying memory capacity (k entries), comparing FIFO vs. performance-weighted retention, and measuring convergence speed vs. final accuracy.

## Limitations

- Critical parameters unspecified: Number of epochs (m) and iterations per epoch (n) in Algorithm 1, final population size N after each update cycle, and memory capacity limits for component and prompt memory.
- Token cost concerns: Working memory inclusion significantly increases token costs without providing cost metrics or discussing memory compression strategies.
- Limited evaluation scope: Single random seeds (5, 10, 15) without variance reporting, and evaluation limited to three mid-sized LLMs on primarily English tasks.
- Missing scalability analysis: Claims about optimization speed advantages over PromptBreeder lack quantitative validation, and paper does not address potential scalability issues as memory grows across many generations.

## Confidence

**High confidence**: The core architectural design (component decomposition + working memory + three-stage evolution) is clearly specified and the ablation studies (Table 3) provide strong evidence for the memory mechanism's contribution. The comparison baselines are appropriate and well-established in prompt optimization literature.

**Medium confidence**: Performance improvements over baselines are substantial and consistent across multiple task types, but single-seed evaluation and lack of variance metrics reduce confidence in generalizability. The discrete vs. continuous prompt memory distinction is theoretically justified but empirical comparison is limited.

**Low confidence**: Claims about optimization speed advantages over PromptBreeder lack quantitative validation, and the paper does not address potential scalability issues as memory grows across many generations.

## Next Checks

1. **Ablation reproducibility test**: Run the memory ablation study (Table 3) on SAMSum or SQuAD with both memories, without component memory, without prompt memory, and without both to verify the 6.9-8.9 percentage point performance drop.

2. **Component value sensitivity analysis**: Test the impact of varying component value counts (10, 20, 30, 40, 50) on SST-5 performance to confirm the paper's claim that 10-50 values yield similar results.

3. **Single-iteration pipeline validation**: Execute one complete evolution cycle on a sentence simplification task, manually inspecting Task-Evolution output (selected components), Solution-Evolution output (new prompt), and memory updates to verify the framework's internal logic and component tracking mechanisms.