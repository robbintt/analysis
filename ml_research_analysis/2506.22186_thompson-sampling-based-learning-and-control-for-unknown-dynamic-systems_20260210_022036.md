---
ver: rpa2
title: Thompson Sampling-Based Learning and Control for Unknown Dynamic Systems
arxiv_id: '2506.22186'
source_url: https://arxiv.org/abs/2506.22186
tags:
- control
- function
- learning
- system
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses active learning control (ALC) for unknown
  nonlinear dynamic systems by proposing a Thompson sampling-based approach that learns
  control laws directly from online data without requiring system identification.
  The method treats the control law as an element in a reproducing kernel Hilbert
  space, parameterized using an initial control law to form a structured function
  space.
---

# Thompson Sampling-Based Learning and Control for Unknown Dynamic Systems

## Quick Facts
- arXiv ID: 2506.22186
- Source URL: https://arxiv.org/abs/2506.22186
- Reference count: 40
- Proposes Thompson sampling-based active learning control for unknown nonlinear dynamic systems without system identification

## Executive Summary
This paper introduces a Thompson sampling-based approach for active learning control of unknown nonlinear dynamic systems. The method directly learns optimal control laws from online data without requiring system identification, treating the control law as an element in a reproducing kernel Hilbert space. By parameterizing the control law using an initial control law to form a structured function space, the approach balances exploration and exploitation through Thompson sampling. The method demonstrates exponential convergence of the learned cost function to a neighborhood of the true cost function and provides regret bounds, with closed-loop stability guarantees after sufficient exploration.

## Method Summary
The approach represents control laws in a reproducing kernel Hilbert space (RKHS) parameterized by an initial control law, creating a structured function space for learning. Thompson sampling is employed to explore and exploit potential optimal control laws within this space, allowing direct policy learning from online data without system identification. The method alternates between sampling candidate control laws from a posterior distribution and evaluating their performance to update the belief distribution. Theoretical analysis establishes exponential convergence of the learned cost function and derives regret bounds, while also providing high-probability stability guarantees after sufficient exploration.

## Key Results
- Thompson sampling approach enables direct learning of control laws without system identification
- Learned cost function converges exponentially to a neighborhood of the true cost function
- Numerical experiments show improved control performance compared to baseline methods

## Why This Works (Mechanism)
The method works by leveraging the structure of reproducing kernel Hilbert spaces to parameterize control laws, allowing Thompson sampling to efficiently explore the policy space while exploiting promising candidates. By avoiding system identification, the approach eliminates the bias and error propagation associated with intermediate modeling steps, directly optimizing control performance from online data.

## Foundational Learning

**Reproducing Kernel Hilbert Spaces (RKHS)**: Function spaces with inner products defined by positive definite kernels, enabling representation of complex nonlinear functions. Needed to provide a structured space for representing control laws with desirable mathematical properties. Quick check: Verify that the chosen kernel is positive definite and that the representer theorem applies.

**Thompson Sampling**: A Bayesian sequential decision-making approach that samples from posterior distributions to balance exploration and exploitation. Needed to efficiently navigate the policy space without exhaustive search. Quick check: Confirm that the posterior updates correctly incorporate new data and that sampling remains computationally tractable.

**Active Learning Control (ALC)**: Control paradigm where the controller actively explores the system to improve performance over time. Needed to frame the problem as one of online learning rather than offline optimization. Quick check: Ensure that the exploration-exploitation trade-off is properly tuned for the specific system dynamics.

## Architecture Onboarding

Component map: Initial Control Law -> RKHS Parameterization -> Thompson Sampling Posterior -> Control Policy -> System Dynamics -> Cost Evaluation -> Posterior Update

Critical path: The learning loop follows: (1) sample control law from posterior, (2) apply to system, (3) observe cost, (4) update posterior distribution, (5) repeat. Each iteration refines the belief about optimal control laws.

Design tradeoffs: Direct policy learning avoids system identification bias but requires careful kernel selection and exploration scheduling. The method trades computational complexity for avoiding modeling errors.

Failure signatures: Poor kernel choice leads to slow convergence or suboptimal policies; insufficient exploration results in premature convergence to local optima; overly aggressive exploration degrades control performance during learning.

First experiments: 1) Implement on a simple linear system to verify theoretical convergence rates. 2) Test on a benchmark nonlinear system (e.g., inverted pendulum) to validate performance improvements. 3) Conduct sensitivity analysis on kernel parameters and exploration rates.

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical guarantees rely on smoothness and boundedness assumptions that may not hold for all practical systems
- Exponential convergence rate depends critically on kernel selection, with poor choices degrading performance
- Regret bounds derived under idealized conditions may not fully capture real-world disturbances and uncertainties

## Confidence

High: The Thompson sampling-based approach for active learning control is mathematically sound and provides a novel framework for direct policy learning without system identification.

Medium: The theoretical convergence and regret bounds hold under the stated assumptions, but practical performance may vary depending on system characteristics and kernel selection.

Low: The closed-loop stability guarantees with high probability after sufficient exploration may be sensitive to the exploration-exploitation trade-off parameters and the specific structure of the initial control law.

## Next Checks

1. Implement the algorithm on a benchmark nonlinear control problem (e.g., cart-pole or quadrotor) and compare regret performance against established baselines under realistic noise conditions.

2. Conduct a systematic sensitivity analysis of the kernel choice and initial control law structure on convergence rates and final control performance across different system classes.

3. Design and execute a robustness test where the true system dynamics deviate from the assumed smoothness and boundedness conditions to quantify the degradation of theoretical guarantees.