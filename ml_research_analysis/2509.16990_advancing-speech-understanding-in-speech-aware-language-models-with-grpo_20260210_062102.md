---
ver: rpa2
title: Advancing Speech Understanding in Speech-Aware Language Models with GRPO
arxiv_id: '2509.16990'
source_url: https://arxiv.org/abs/2509.16990
tags:
- grpo
- speech
- arxiv
- bleu
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes using Group Relative Policy Optimization (GRPO)
  with BLEU reward to improve Speech-Aware Large Language Models (SALLMs) on open-format
  speech understanding tasks. The authors address the challenge of improving SALLMs
  on tasks like Spoken Question Answering (SQA) and Automatic Speech Translation (AST),
  where open-ended responses are required rather than multiple-choice answers.
---

# Advancing Speech Understanding in Speech-Aware Language Models with GRPO

## Quick Facts
- **arXiv ID**: 2509.16990
- **Source URL**: https://arxiv.org/abs/2509.16990
- **Reference count**: 0
- **Primary result**: GRPO with BLEU reward improves SALLMs on SQA and AST, achieving 61.8% BLEU improvement on LibriSQA and 8.2% on CoV oST2 vs SFT baselines

## Executive Summary
This paper addresses the challenge of improving Speech-Aware Large Language Models (SALLMs) on open-ended speech understanding tasks like Spoken Question Answering (SQA) and Automatic Speech Translation (AST). The authors propose using Group Relative Policy Optimization (GRPO) with BLEU reward to train SALLMs, demonstrating empirically that this approach surpasses standard supervised fine-tuning across several key metrics. They show significant improvements on LibriSQA and CoV oST2 datasets, with the 2B model achieving 61.8% BLEU improvement on LibriSQA and 8.2% on AST. The paper also explores mixed-policy GRPO incorporating ground truth references as off-policy samples, finding mixed results depending on task familiarity.

## Method Summary
The authors train SALLMs using GRPO with BLEU as the reward signal. The method samples G=8 responses per prompt from the model, computes BLEU scores against ground truth references, and estimates advantages via group normalization. The policy is updated using DAPO loss with a KL penalty (β=0.02). They test on Granite Speech 2B/8B models using LibriSQA and CoV oST2 datasets, comparing against SFT baselines across BLEU, BERTScore, ROUGE, and METEOR metrics.

## Key Results
- GRPO improved BLEU scores by 61.8% over baseline and 9.8% over SFT for the 2B model on LibriSQA
- For the 8B model on LibriSQA, GRPO achieved 151% improvement over baseline and 6% over SFT
- On CoV oST2 English to German translation, GRPO achieved 8.2% BLEU improvement over baseline and 3.2% over SFT for the 2B model
- Mixed-Policy GRPO showed inconsistent results: improved AST performance but degraded SQA performance

## Why This Works (Mechanism)

### Mechanism 1: Continuous Reward Signal
Using BLEU as a continuous, differentiable reward signal allows SALLMs to optimize for open-ended generative tasks more effectively than binary rewards. GRPO samples multiple responses, computes BLEU rewards against references, normalizes within groups to calculate advantages, and updates the policy to increase probability of high-advantage samples. This assumes BLEU is a meaningful proxy for response quality.

### Mechanism 2: Group-Based Advantage Estimation
GRPO's group-based advantage estimation eliminates the need for a separate value model, making it more memory-efficient for SALLMs. Instead of training a critic, GRPO estimates sample advantage by comparing its reward to the mean and standard deviation of other samples in the group. This assumes group rewards provide stable baselines for policy updates.

### Mechanism 3: Mixed-Policy with Off-Policy Samples
Incorporating ground-truth references as off-policy samples can stabilize training and steer the policy, particularly for tasks where the base model has pre-existing competency. The group is augmented with the ground truth reference as a high-quality anchor with perfect reward. This assumes the off-policy sample is of sufficient quality to guide policy updates effectively.

## Foundational Learning

- **Speech-Aware Large Language Models (SALLMs)**: Core architecture combining speech encoder, projector, and text LLM. Understanding this setup is crucial for applying GRPO.
  - Quick check: What are the three main components of a SALLM, and what is the function of the projector?

- **Group Relative Policy Optimization (GRPO)**: On-policy RL method updating model based on relative performance of generated sample groups.
  - Quick check: In GRPO, how is the "advantage" of a generated sample calculated without a separate value model?

- **Reward Functions for Generative Tasks**: BLEU, ROUGE, METEOR, BERTScore quantify similarity between model output and reference.
  - Quick check: Why is BLEU considered suitable for open-format speech tasks compared to binary rewards?

## Architecture Onboarding

- **Component map**: Speech-and-text input -> GRPO sampler -> generated samples -> BLEU-based reward calculation -> advantage estimation -> policy update
- **Critical path**: The loop from input through GRPO sampling, BLEU reward calculation, advantage estimation, and policy update must function correctly
- **Design tradeoffs**: GRPO requires 10-100× more compute than SFT but yields superior generative results; reward function choice matters; mixed-policy can help or hurt depending on task familiarity
- **Failure signatures**: Training divergence with β=0; no improvement with poor reward functions; high compute cost without sufficient gains
- **First 3 experiments**: 1) Baseline SFT reproduction on LibriSQA, 2) GRPO reward ablation with different metrics, 3) Mixed-policy testing on familiar vs novel tasks

## Open Questions the Paper Calls Out

### Open Question 1
Can GRPO with generative rewards be effectively adapted for ASR tasks with single valid outputs? The paper notes preliminary experiments yielded only minor improvements due to ASR's unique valid response constraint, requiring further investigation into modified frameworks or reward structures.

### Open Question 2
What causes MP-GRPO to degrade SQA while improving AST? The authors hypothesize task familiarity differences affect off-policy training stability but haven't validated this mechanism or defined boundary conditions for success.

### Open Question 3
Does incorporating multiple reference translations improve MP-GRPO performance? The current study used single references; the impact of providing multiple diverse ground truth examples remains untested.

### Open Question 4
Can neural-based rewards (BERTScore) or combined metrics outperform BLEU? The authors excluded these due to computational constraints, leaving potential performance gains from semantic or neural-based reward signals unexplored.

## Limitations

- Limited to two specific domains (literature audiobooks and multilingual speech translation), restricting generalizability claims
- Relies on BLEU and related metrics as proxies for response quality, which have known limitations in capturing semantic equivalence
- High computational cost (4×H100 GPUs for up to 24 hours) may limit practical adoption despite performance gains
- Mixed-policy GRPO showed inconsistent results between tasks, with underlying mechanism unclear

## Confidence

- **High Confidence**: GRPO with BLEU reward improves SALLM performance over SFT baselines on tested datasets, supported by quantitative results and multiple metrics
- **Medium Confidence**: Group-based advantage estimation provides memory efficiency benefits, though direct measurements weren't provided
- **Low Confidence**: Mixed-policy mechanism's effectiveness and the claim that ground-truth off-policy samples stabilize training, given contradictory results between tasks

## Next Checks

1. **Reward Ablation Study**: Implement GRPO training with alternative reward functions (ROUGE-1, ROUGE-L, BERTScore) on same datasets to validate BLEU's optimality

2. **Cross-Dataset Generalization Test**: Apply best-performing GRPO models to completely different speech understanding datasets (SpeechStew or MuST-C) to assess generalization beyond tested domains

3. **Resource Efficiency Analysis**: Compare GRPO training time and GPU memory usage against SFT for same model sizes and datasets, measuring BLEU score achieved per GPU-hour to quantify whether performance gains justify computational overhead