---
ver: rpa2
title: Safety Monitoring for Learning-Enabled Cyber-Physical Systems in Out-of-Distribution
  Scenarios
arxiv_id: '2504.13478'
source_url: https://arxiv.org/abs/2504.13478
tags:
- safety
- prediction
- learning
- conformal
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a direct safety monitoring approach for learning-enabled
  cyber-physical systems (LE-CPS) in out-of-distribution (OOD) scenarios, arguing
  that OOD detection alone is insufficient since OOD inputs do not necessarily lead
  to safety violations. The method predicts violations of signal temporal logic (STL)
  safety specifications by predicting future trajectories using adaptive conformal
  prediction (ACP) combined with incremental learning (IL).
---

# Safety Monitoring for Learning-Enabled Cyber-Physical Systems in Out-of-Distribution Scenarios

## Quick Facts
- arXiv ID: 2504.13478
- Source URL: https://arxiv.org/abs/2504.13478
- Reference count: 40
- Primary result: ACP+IL achieves 0.94-0.98 recall, 4.75-4.93 steps ahead timeliness, and 0.59-0.76 precision for STL safety monitoring in OOD scenarios

## Executive Summary
This paper addresses safety monitoring for learning-enabled cyber-physical systems (LE-CPS) in out-of-distribution (OOD) scenarios. The authors argue that detecting OOD inputs alone is insufficient for safety since OOD does not necessarily imply unsafe behavior. Instead, they propose a direct safety monitoring approach that predicts violations of signal temporal logic (STL) safety specifications by forecasting future trajectories using adaptive conformal prediction (ACP) combined with incremental learning (IL). The method provides probabilistic guarantees without distribution assumptions while preventing overly conservative predictions on OOD data through IL. Evaluated on two case studies—an F1Tenth car with static obstacles and a race car with dynamic obstacles—the approach achieves high recall, timeliness, and competitive precision while outperforming alternatives.

## Method Summary
The approach monitors safety by predicting future trajectories and evaluating them against STL specifications. A trajectory predictor forecasts H-step future states from h-step history, which are evaluated to compute robustness values indicating safety margins. Adaptive conformal prediction provides probabilistic guarantees by maintaining online non-conformity score (NCS) sets and adaptively updating significance levels. Incremental learning collects high-error trajectories, clusters them via K-means, and dynamically selects predictors to reduce conservatism on OOD data. The method raises an alarm when predicted robustness falls below the ACP prediction threshold. The combination of ACP and IL is essential for balancing recall-precision trade-offs in safety monitoring.

## Key Results
- Achieves high recall (0.94-0.98) and timeliness (4.75-4.93 steps ahead) for STL safety monitoring
- Maintains competitive precision (0.59-0.76) through incremental learning
- Outperforms robust conformal prediction and other alternatives in OOD settings
- Provides theoretical guarantees where other methods fail under distribution shift

## Why This Works (Mechanism)

### Mechanism 1: Direct Safety Monitoring via STL Robustness Prediction
Predicting safety violations directly is more useful than detecting OOD inputs because OOD does not necessarily imply unsafe behavior. A trajectory predictor forecasts future system states over horizon H, which are evaluated against an STL specification to compute robustness values. This mechanism works when the safety property can be expressed as STL and system state is observable. Break condition occurs if STL doesn't capture all failure modes or state observation is noisy.

### Mechanism 2: Adaptive Conformal Prediction for Distribution-Free Guarantees
ACP maintains probabilistic coverage guarantees without assuming exchangeability or bounded distribution shift. It adaptively updates significance level δₜ based on whether true robustness falls outside prediction intervals. This works when non-conformity scores are computable online and learning rate is properly tuned. Break condition occurs if prediction horizon is too large or predictor is systematically biased.

### Mechanism 3: Incremental Learning to Reduce Conservatism on OOD Data
ACP alone becomes overly conservative on OOD inputs; incremental learning recovers precision by adapting the trajectory predictor. High-error trajectory samples are collected, clustered, and used to train new predictors that are dynamically selected. This works when sufficient high-error samples are collected and OOD distribution is not too far from existing clusters. Break condition occurs when few samples exceed error threshold, limiting fine-tuning data.

## Foundational Learning

- **Signal Temporal Logic (STL) and Robustness Values**: STL formalizes safety properties and robustness provides quantitative safety margins. Quick check: Given a trajectory, can you compute the robustness value for a "always within bounds" specification?

- **Conformal Prediction Variants (CP, RCP, ACP)**: Understanding why standard CP fails under distribution shift and how ACP adapts is essential. Quick check: What assumption does CP require that ACP relaxes, and what tradeoff does ACP incur?

- **Incremental Learning and Catastrophic Forgetting**: The IL mechanism relies on maintaining a predictor set and selecting dynamically. Quick check: Why does the method use a set of predictors with K-means selection instead of continuously fine-tuning a single model?

## Architecture Onboarding

- **Component map**: State History -> Predictor Selection (K-means) -> Trajectory Prediction -> STL Robustness Calculation -> ACP Interval Generation -> Alarm Decision

- **Critical path**: Observe h-step state history → Select predictor from DP → Predict H-step trajectory → Compute robustness ρ̂ → ACP generates interval Cₜ → If ρ̂ < Cₜ, raise alarm; else continue → After H steps, compute true robustness, update NCS and optionally W

- **Design tradeoffs**: Recall vs. Precision (ACP maximizes recall but lowers precision; IL recovers precision at cost of more predictors), Horizon H (larger H gives earlier warnings but degrades accuracy), Error threshold τ (lower τ collects more IL samples but may include noisy data)

- **Failure signatures**: Constant false alarms (ACP intervals too wide), Missed violations (predictor systematically wrong on OOD), Coverage collapse (CP/RCP coverage << 1-δ)

- **First 3 experiments**: 1) Baseline coverage test: Run ACP on in-distribution data; verify empirical coverage converges to 1-δ, 2) OOD stress test: Compare CP vs. RCP vs. ACP coverage and recall under known distribution shift, 3) IL ablation: Enable/disable incremental learning on OOD scenarios; measure precision recovery

## Open Questions the Paper Calls Out

- How does the performance of the ACP+IL safety monitor change when applied to systems trained online rather than offline? The authors state that evaluating the method on "models trained online" could provide insights into resilience, as these models continually influence the distribution of system states.

- Can alternative methods for selecting fine-tuning data at runtime improve the precision of the safety monitor? The authors propose that "alternative methods for selecting fine-tuning data at runtime may allow larger and richer datasets to be collected, improving the precision."

- How can the safety monitoring approach be adapted to handle errors in state estimation and obstacle detection? The authors explicitly assume the monitor has "error-free knowledge of the environment map, system states, and obstacles" to isolate safety monitoring from perception problems.

## Limitations

- Performance on thin-tailed NCS distributions where calibration and online data means are close remains theoretically characterized but not extensively empirically tested
- Long-term stability of adaptive δₜ update mechanism under persistent OOD conditions lacks theoretical bounds beyond asymptotic coverage
- Computational overhead of maintaining multiple predictors and performing K-means clustering at runtime has not been quantified

## Confidence

- **High**: STL-based safety monitoring mechanism and its theoretical grounding; empirical coverage results for ACP under distribution shift
- **Medium**: Incremental learning's effectiveness in precision recovery; comparative performance against baselines in the two case studies
- **Low**: Long-term stability of adaptive calibration under persistent OOD; computational scalability with increasing predictor set size

## Next Checks

1. Test the method on thin-tailed NCS distributions where calibration and online data means are close to verify IL's precision recovery limits
2. Evaluate computational overhead (runtime, memory) of maintaining multiple predictors and performing online K-means clustering in resource-constrained CPS
3. Assess long-term stability of adaptive δₜ updates under persistent OOD conditions by monitoring coverage and alarm rates over extended deployment periods