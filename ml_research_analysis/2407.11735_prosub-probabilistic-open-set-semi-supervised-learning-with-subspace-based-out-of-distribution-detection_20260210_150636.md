---
ver: rpa2
title: 'ProSub: Probabilistic Open-Set Semi-Supervised Learning with Subspace-Based
  Out-of-Distribution Detection'
arxiv_id: '2407.11735'
source_url: https://arxiv.org/abs/2407.11735
tags:
- data
- prosub
- learning
- training
- unlabeled
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes ProSub, a new framework for open-set semi-supervised
  learning that addresses the challenge of handling unlabeled data containing unknown
  classes. The core method introduces a subspace-based score that measures the angle
  between data features and an ID subspace, combined with a probabilistic approach
  for ID/OOD classification using estimated Beta distributions.
---

# ProSub: Probabilistic Open-Set Semi-Supervised Learning with Subspace-Based Out-of-Distribution Detection

## Quick Facts
- arXiv ID: 2407.11735
- Source URL: https://arxiv.org/abs/2407.11735
- Authors: Erik Wallin; Lennart Svensson; Fredrik Kahl; Lars Hammarstrand
- Reference count: 40
- Primary result: ProSub achieves state-of-the-art performance in open-set semi-supervised learning by combining subspace-based scoring with probabilistic ID/OOD classification using Beta distributions

## Executive Summary
ProSub addresses the challenge of open-set semi-supervised learning by introducing a novel framework that can effectively handle unlabeled data containing both known (in-distribution, ID) and unknown (out-of-distribution, OOD) classes. The method combines a subspace-based scoring mechanism that measures the angle between data features and an ID subspace with a probabilistic approach for ID/OOD classification using estimated Beta distributions. Through extensive experiments on standard image benchmarks, ProSub demonstrates significant improvements over existing methods in both closed-set accuracy and the ability to detect and separate ID from OOD samples.

## Method Summary
ProSub introduces a subspace-based scoring mechanism that measures the angle between data features and an ID subspace to detect OOD samples. The method employs self-supervision to learn from all unlabeled data while using a subspace loss to enhance separation between ID and OOD distributions. A probabilistic approach based on estimated Beta distributions enables robust ID/OOD classification. The framework integrates these components within a semi-supervised learning pipeline, allowing the model to leverage both labeled ID data and unlabeled data containing potential OOD samples.

## Key Results
- Achieves state-of-the-art performance on CIFAR-10, CIFAR-100, SVHN, and TinyImageNet benchmarks
- Demonstrates significant improvements in closed-set accuracy compared to existing open-set semi-supervised methods
- Shows superior AUROC scores for ID/OOD classification, indicating better separation between known and unknown classes

## Why This Works (Mechanism)
ProSub works by exploiting the geometric properties of feature space to distinguish between ID and OOD data. The subspace-based scoring measures how well data points align with the ID subspace, with OOD samples typically exhibiting larger angles. By fitting Beta distributions to the ID and OOD scores, the method can probabilistically classify samples while maintaining uncertainty estimates. The self-supervision component ensures that all unlabeled data contributes to representation learning, while the subspace loss explicitly encourages separation between ID and OOD distributions in feature space.

## Foundational Learning
- **Semi-supervised learning**: Why needed - to leverage unlabeled data for improved performance when labeled data is scarce; Quick check - verify understanding of self-training and consistency regularization approaches
- **Out-of-distribution detection**: Why needed - to identify samples from unknown classes in unlabeled data; Quick check - understand distance-based and density-based OOD detection methods
- **Subspace representation learning**: Why needed - to capture the intrinsic dimensionality of ID data; Quick check - verify understanding of PCA and other dimensionality reduction techniques
- **Beta distribution modeling**: Why needed - to represent uncertainty in ID/OOD classification scores; Quick check - understand Beta distribution properties and parameter estimation
- **Self-supervision**: Why needed - to learn meaningful representations from unlabeled data; Quick check - understand contrastive learning and other self-supervised approaches

## Architecture Onboarding

**Component Map**: Raw data -> Feature extractor -> Subspace scoring -> Beta distribution estimation -> Probabilistic classification -> Final prediction

**Critical Path**: The core computational path involves feature extraction from input data, computation of subspace-based scores measuring alignment with ID subspace, estimation of Beta distributions for ID and OOD scores, and probabilistic classification based on these distributions.

**Design Tradeoffs**: The method trades computational complexity (due to Beta distribution fitting and subspace computations) for improved OOD detection and semi-supervised learning performance. The reliance on Beta distributions assumes that ID and OOD scores follow specific distributional patterns.

**Failure Signatures**: Potential failure modes include poor Beta distribution fitting when score distributions deviate significantly from Beta assumptions, degradation in performance when OOD data is not well-separated in feature space, and computational bottlenecks with very large datasets.

**First 3 Experiments**:
1. Validate the Beta distribution fitting on synthetic data with known ID/OOD score distributions
2. Test the subspace scoring mechanism on a simple 2D dataset to visualize separation between ID and OOD samples
3. Evaluate the impact of the subspace loss on feature space separation using t-SNE visualizations

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Effectiveness depends on the assumption that OOD data exhibits distinct directional properties in feature space relative to ID subspaces
- Beta distribution fitting approach may be sensitive to hyperparameter choices and may not generalize well to extremely imbalanced or high-dimensional datasets
- Experimental validation is primarily limited to image classification benchmarks, with unverified performance on other data modalities

## Confidence
- High confidence in the methodological innovation of combining subspace-based scoring with probabilistic ID/OOD classification
- Medium confidence in the empirical superiority claims, given the limited scope of benchmark datasets
- Medium confidence in the scalability and generalization of the approach to diverse real-world applications

## Next Checks
1. Cross-domain generalization test: Evaluate ProSub's performance on non-image datasets (e.g., text, audio, or tabular data) to assess its robustness across different data modalities and feature spaces.

2. Scalability analysis: Conduct experiments to quantify the computational overhead introduced by the subspace loss and Beta distribution estimation, particularly for larger datasets or more complex model architectures.

3. Robustness to feature space assumptions: Design controlled experiments to test the method's sensitivity to violations of the assumption that OOD data exhibits distinct directional properties in feature space, including scenarios where OOD data is close to the ID subspace.