---
ver: rpa2
title: 'HFedATM: Hierarchical Federated Domain Generalization via Optimal Transport
  and Regularized Mean Aggregation'
arxiv_id: '2508.05135'
source_url: https://arxiv.org/abs/2508.05135
tags:
- hfedatm
- domain
- layer
- learning
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces HFedDG, a hierarchical federated learning
  framework designed to handle domain shifts in multi-station setups. It proposes
  HFedATM, which combines Filter-wise Optimal Transport Alignment to align convolutional
  filters semantically across stations, followed by Shrinkage-aware Regularized Mean
  Aggregation for data-free linear layer merging.
---

# HFedATM: Hierarchical Federated Domain Generalization via Optimal Transport and Regularized Mean Aggregation

## Quick Facts
- arXiv ID: 2508.05135
- Source URL: https://arxiv.org/abs/2508.05135
- Reference count: 34
- Primary result: HFedATM improves accuracy over baselines in federated domain generalization with tighter generalization bounds

## Executive Summary
HFedATM introduces a hierarchical federated learning framework for domain generalization that combines Filter-wise Optimal Transport (FOT) alignment with Shrinkage-aware Regularized Mean (RegMean) aggregation. The method addresses the challenge of training models across distributed clients with different data distributions while maintaining strong generalization to unseen domains. HFedATM achieves this by aligning semantic features across models before aggregation and using a data-free linear layer merging technique based on local Gram matrices. Theoretical analysis shows the approach yields tighter generalization error bounds than standard hierarchical averaging.

## Method Summary
HFedATM operates in a three-tier hierarchical architecture (Client → Station → Server) where clients train local models using standard FedDG methods (FedSR, FedIIR, FedAvg, FedProx), compute Gram matrices during the final forward pass, and send weights and Gram matrices to their stations. Stations aggregate local Gram matrices, align convolutional filters using Filter-wise Optimal Transport with the Sinkhorn algorithm, and forward aligned weights to the server. The server computes the final global model using RegMean aggregation, which solves a closed-form optimization problem using the aggregated Gram matrices. The method handles heterogeneous data distributions through a partitioning scheme controlled by parameter λ and employs domain generalization techniques at the client level to minimize initial divergence.

## Key Results
- HFedATM consistently improves accuracy over baselines across vision (PACS, Office-Home, TerraIncognita) and NLP (Amazon Reviews) datasets
- On PACS dataset, HFedATM with FedSR achieves 87.7% accuracy versus 84.1% for baseline FedSR
- Theoretical analysis shows HFedATM yields tighter generalization error bounds than naive averaging
- The method adds minimal computational overhead (<10% latency increase) while maintaining robustness under privacy constraints

## Why This Works (Mechanism)

### Mechanism 1
Filter-wise Optimal Transport (FOT) resolves the semantic permutation mismatch of convolutional filters across different stations. Independent training results in arbitrary filter indices (e.g., the "vertical edge detector" is at index 1 for Station A but index 10 for Station B). FOT solves a discrete Optimal Transport problem using the Sinkhorn algorithm to find a permutation matrix that aligns filters by minimizing the Euclidean distance between their weights, ensuring index correspondence before averaging. Core assumption: The Euclidean distance between normalized filter weights correlates with semantic similarity. Break condition: If the filter space is highly non-convex or weights do not reflect functional similarity, the alignment will create a "franken-model" with conflicting features.

### Mechanism 2
Shrinkage-aware Regularized Mean (RegMean) Aggregation merges linear layers by optimizing for the geometry of activation spaces rather than just weight magnitudes. Instead of simple weight averaging, this method solves for global weights $W$ that minimize the difference between the global model's output and local models' outputs using local Gram matrices ($G = X^\top X$). It applies diagonal shrinkage to these matrices to improve stability and invertibility. Core assumption: Gram matrices sufficiently capture the second-order statistics of local data distributions without leaking raw data. Break condition: If local batch sizes are too small, Gram matrices become poor estimators of the true activation covariance, leading to unstable merging.

### Mechanism 3
The combination of FOT and RegMean provides a theoretically tighter generalization error bound than standard hierarchical averaging. Theoretical analysis suggests that FOT reduces "breadth" terms (inter-station divergence) and RegMean reduces "inner divergence" terms. This contraction of divergence terms lowers the upper bound of the target domain error. Core assumption: Local training has already achieved a minimal domain generalization risk using methods like FedSR or FedIIR. Break condition: If local client models are highly heterogeneous and local DG is weak, the initial divergence terms are too large for HFedATM to effectively tighten the bound.

## Foundational Learning

### Concept: Optimal Transport (Sinkhorn Algorithm)
Why needed here: You must understand how the cost matrix is constructed from filter weights and how the entropy-regularized Sinkhorn iteration finds the optimal permutation to align models. Quick check question: Can you explain why simple weight averaging fails if the "indices" of features are shuffled in one of the models?

### Concept: Gram Matrices in Linear Layers
Why needed here: RegMean relies on $X^\top X$. You need to grasp why this captures the "shape" of the data distribution and how it acts as a preconditioner for merging weights without raw data. Quick check question: If two local models have identical weights but vastly different data distributions (activations), would RegMean treat them the same as simple averaging?

### Concept: Domain Generalization (DG) Bounds
Why needed here: The paper justifies HFedATM by tightening theoretical bounds based on H-divergence. Understanding the difference between "empirical risk" and "generalization bound" is crucial. Quick check question: Does minimizing the local training loss guarantee a lower generalization bound in the presence of domain shift?

## Architecture Onboarding

### Component map:
Client -> Station -> Server

### Critical path:
FOT alignment must happen before linear layer merging. If filters are not aligned, the linear layers receiving inputs from those filters cannot be meaningfully merged via RegMean.

### Design tradeoffs:
- **Latency vs. Accuracy:** FOT adds computational overhead (Sinkhorn iterations) and RegMean increases communication (sending Gram matrices), but experiments show <10% latency increase
- **Privacy vs. Utility:** Sharing Gram matrices enables better merging but theoretically leaks second-order statistics. The paper argues they are non-invertible, but DP noise can be added if strict privacy is required

### Failure signatures:
- **Accuracy Collapse:** If FOT aligns filters incorrectly (e.g., due to dissimilar architectures or extreme weight noise), the merged model performs worse than FedAvg
- **Singular Matrix Error:** If the shrinkage parameter α is too low and Gram matrices are rank-deficient, the inverse required for RegMean will fail

### First 3 experiments:
1. **Ablation (Table 4):** Run HFedATM with only FOT and only RegMean to verify the contribution of each component on PACS
2. **Privacy Stress Test:** Attempt to reconstruct input data from the uploaded Gram matrices to validate the Theorem 3 claim of non-invertibility
3. **Heterogeneity Sweep:** Vary the λ parameter (1.0 → 0.0) to observe performance degradation as the domain shift assumption is stressed

## Open Questions the Paper Calls Out

### Open Question 1
How can HFedATM be adapted to accommodate clients with heterogeneous model architectures? The paper states that "Exploring adaptations to accommodate heterogeneous models remains a compelling future direction," noting the current reliance on model homogeneity. Filter-wise Optimal Transport requires matching filter dimensions across stations to calculate cost matrices and permutations.

### Open Question 2
Can the transmission of intermediate statistics (Gram matrices) be secured or avoided to prevent potential reconstruction attacks without relying on differential privacy noise? While Theorem 3 claims exact inversion is impossible, the authors acknowledge potential risks and currently rely on noise injection (DP), which degrades utility.

### Open Question 3
Can the HFedATM framework maintain robustness when local client training produces highly heterogeneous representations (large initial divergence)? Section 5.2 observes that HFedATM's effectiveness is limited when local baselines produce large initial divergences, relying heavily on strong local DG algorithms.

## Limitations
- The privacy claim for RegMean (non-invertibility of Gram matrices) is stated but not rigorously tested through empirical stress-testing
- Performance gains over baselines are consistent but relatively modest (e.g., +3.6% on PACS), suggesting HFedATM may be complementary to rather than a replacement for strong local DG methods
- The method's robustness to extreme domain shifts (λ ≤ 0.1) without strong local regularizers is not fully explored

## Confidence

### High confidence:
- The hierarchical aggregation framework is clearly defined and the theoretical generalization bound tightening is logically derived from standard DG theory

### Medium confidence:
- The FOT and RegMean mechanisms are sound in principle, but their robustness to real-world heterogeneity (e.g., dissimilar architectures, noisy gradients) is not fully explored

### Low confidence:
- The claim that Gram matrices are "non-invertible" for privacy is a strong assertion that requires empirical stress-testing to validate

## Next Checks

1. **Ablation Study:** Run HFedATM with only FOT and only RegMean to verify the individual contribution of each component on PACS

2. **Privacy Stress Test:** Attempt to reconstruct input data from the uploaded Gram matrices to validate the Theorem 3 claim of non-invertibility

3. **Heterogeneity Sweep:** Vary the λ parameter (1.0 → 0.0) to observe performance degradation as the domain shift assumption is stressed