---
ver: rpa2
title: Can SAEs reveal and mitigate racial biases of LLMs in healthcare?
arxiv_id: '2511.00177'
source_url: https://arxiv.org/abs/2511.00177
tags:
- race
- patient
- black
- arxiv
- latent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates the utility of Sparse Autoencoders (SAEs)
  for revealing and mitigating racial bias in large language models (LLMs) used in
  healthcare. The authors first identify SAE latents predictive of patient race using
  clinical discharge summaries.
---

# Can SAEs reveal and mitigate racial biases of LLMs in healthcare?

## Quick Facts
- **arXiv ID:** 2511.00177
- **Source URL:** https://arxiv.org/abs/2511.00177
- **Reference count:** 37
- **Primary result:** SAEs effectively reveal racial bias in clinical LLMs but show limited utility for mitigating bias in realistic healthcare tasks.

## Executive Summary
This paper evaluates Sparse Autoencoders (SAEs) for revealing and mitigating racial bias in large language models used in healthcare. The authors identify SAE latents predictive of patient race using clinical discharge summaries, finding that these latents activate on both explicit race mentions and problematic associations. Steering experiments demonstrate that these latents causally influence model predictions, such as increasing perceived risk of belligerence for Black patients. While SAE-based interventions effectively reduce racial stereotyping in simple vignette generation tasks (~30% reduction), they have minimal impact on bias reduction in more complex and realistic clinical risk prediction tasks.

## Method Summary
The study uses Gemma-2 models with GemmaScope SAEs to identify race-correlated latents through L1-regularized logistic regression probes trained on MIMIC-III discharge summaries. Race-predictive latents are validated via activation steering, where intervening on these latents causes measurable changes in model outputs. Bias mitigation is evaluated through zero-ablation of race-related latents, comparing effectiveness against anti-bias prompting on both simplified vignette generation and realistic clinical risk prediction tasks. The analysis focuses on Black vs. white patient groups due to data availability.

## Key Results
- Race-correlated latents achieve AUROC of 0.63-0.72 in predicting patient race from clinical notes
- Steering with Black latent increases belligerence risk prediction positive rate by Δ=0.51 (2B) and Δ=0.80 (9B)
- SAE-based interventions reduce racial stereotyping by ~30% in vignette tasks but show <5% FLDD in realistic clinical risk prediction

## Why This Works (Mechanism)

### Mechanism 1: Linear Probe Identification of Race-Correlated Latents
- Claim: SAE latents can reveal where race information is encoded in LLMs, including problematic associations.
- Mechanism: Train an L1-regularized logistic regression probe on max-aggregated SAE activations from patient discharge summaries to predict race. The highest-coefficient latents are "race-predictive."
- Core assumption: Race information is sufficiently localized in specific SAE latents to be detectable via linear probing.
- Evidence anchors:
  - [abstract] "identify SAE latents in Gemma-2 models which appear to correlate with Black individuals... this latent activates on reasonable input sequences (e.g., 'African American') but also problematic words like 'incarceration'"
  - [section 2.2] "The AUROCs computed using the Black latent's max-aggregated activations are 0.63 and 0.72 for gemma-2-2B and gemma-2-9B"
  - [corpus] Limited direct corpus support; related work on unsupervised concept extraction (arXiv:2502.19721) shows FMR=0.0
- Break condition: When race information is highly distributed across hundreds of latents with weak individual correlations.

### Mechanism 2: Activation Steering for Causal Validation
- Claim: Race-correlated latents causally influence downstream predictions, not merely correlating with inputs.
- Mechanism: Intervene on hidden states by computing z' = z + α·z_max for the target latent index r, then reconstruct h' = Wz' + b before continuing forward pass.
- Core assumption: Modifying a single latent's activation produces measurable, interpretable changes in model behavior.
- Evidence anchors:
  - [abstract] "activating the Black latent increases the risk assigned to the probability that a patient will become 'belligerent'"
  - [section 3, Table 3] Steering with Black latent increases positive rate for belligerence prediction by Δ=0.51 (2B) and Δ=0.80 (9B); 100% and 78% of outputs explicitly identify patient as Black post-steering
  - [corpus] Weak corpus support for this specific steering methodology in healthcare
- Break condition: When steering factor α is too high, causing perplexity degradation and incoherent outputs (see Appendix C plot).

### Mechanism 3: Zero-Ablation for Bias Mitigation
- Claim: Ablating race-correlated latents can reduce biased outputs, but effectiveness varies dramatically by task complexity.
- Mechanism: Set race latent activations to zero (z[r] = 0), reconstruct hidden states, and measure change in output bias via fractional logit difference decrease (FLDD).
- Core assumption: Race information can be disentangled from clinically-relevant features without degrading task performance.
- Evidence anchors:
  - [abstract] "SAE-based interventions are effective in simple vignette generation tasks, reducing racial stereotyping by ~30% compared to ~18% with anti-bias prompting. However, for more complex... tasks, SAE interventions have minimal impact"
  - [section 4.1, Table 4] Vignette generation: cocaine abuse ratio drops from 0.88 → 0.46 with SAE ablation
  - [section 4.2.3, Table 7] Realistic tasks: FLDD ≤3% across all conditions
  - [corpus] Related work on bias control (arXiv:2502.19721) but no direct replications
- Break condition: When race representations are entangled with legitimate clinical concepts in complex tasks.

## Foundational Learning

- **Concept: Sparse Autoencoders (SAEs)**
  - Why needed here: Core interpretability technique that decomposes dense LLM activations into sparse, interpretable latents via encoder-decoder architecture with L1 sparsity penalty.
  - Quick check question: Given hidden state h ∈ R^D and SAE width W, what does the reconstruction loss term enforce?

- **Concept: Activation Steering/Intervention**
  - Why needed here: Method for causal validation—demonstrating that a latent doesn't just correlate with behavior but causes it.
  - Quick check question: If steering factor α=0.5 and max activation z_max=3.0, what value do you add to the target latent?

- **Concept: Linear Probing with L1 Regularization**
  - Why needed here: Identifies which SAE latents carry demographic information; L1 induces sparsity for interpretability.
  - Quick check question: Why prefer L1 over L2 regularization when you want to identify a small subset of predictive features?

## Architecture Onboarding

- **Component map:**
  - Tokenization → LLM → Hidden states → SAE encoder → Sparse latents → SAE decoder → Reconstructed states
  - SAE latents → Linear probe → Race prediction
  - SAE latents → Steering/ablating module → Modified latents → Reconstruction → Downstream prediction

- **Critical path:**
  1. Tokenize input, run through LLM to layer ℓ → hidden states {h_1, ..., h_n}
  2. Pass each h_j through SAE → activations z_j ∈ R^W
  3. Max-aggregate across tokens: z_i = max(z_1, ..., z_n) for each latent
  4. For steering: modify z[r], reconstruct h' = W_dec·z' + b, continue forward pass
  5. For ablation: zero out z[r], reconstruct, measure output change

- **Design tradeoffs:**
  - Single latent vs. multiple race latents: paper expanded to 7 (2B) / 9 (9B) latents with minimal improvement
  - Middle layer vs. multi-layer intervention: ablating 5 layers showed no significant FLDD improvement (Appendix D.3)
  - Steering factor α: balance between effect size and output coherence (paper used α ∈ [0.01, 5])

- **Failure signatures:**
  - High perplexity post-steering → α too large
  - FLDD near 0% → race entangled with clinical features, ablation ineffective
  - CoT doesn't mention race → unfaithful explanations (100% of reasoning chains omitted race terms despite causal influence)

- **First 3 experiments:**
  1. **Probe replication:** Train L1-regularized logistic regression on MIMIC-III discharge summaries using GemmaScope SAE activations (layer 12 for 2B, layer 20 for 9B); verify top latent AUROC ≥0.60.
  2. **Steering validation:** On 100 BHC samples, steer with Black latent using α=0.03 (2B) or α=0.06 (9B); confirm >70% of outputs explicitly identify patient as Black.
  3. **Ablation comparison:** Test zero-ablation vs. anti-bias prompting on both vignette generation (expect ~30% reduction) and risk prediction (expect <5% FLDD); document the gap.

## Open Questions the Paper Calls Out

- **Question:** Is it possible to ablate racial bias using SAEs without simultaneously removing valid clinical correlations and degrading performance on complex tasks?
  - Basis in paper: [explicit] Conclusion: "If race and clinical concepts are entangled, then it is unclear how problematic associations can be removed... without ablating clinical concepts and compromising downstream performance."
  - Why unresolved: The study demonstrates that SAE interventions fail in realistic tasks, but it does not isolate whether this failure is due to the entanglement of race with necessary clinical features.
  - What evidence would resolve it: An analysis of activation overlap between race-correlated latents and latents responsible for essential clinical reasoning in complex tasks.

- **Question:** Do SAE-based bias detection and mitigation findings generalize to non-Black demographic groups and model architectures other than Gemma-2?
  - Basis in paper: [explicit] Limitations: "Future work might extend this analysis to other racial groups... other models may encode racial associations differently."
  - Why unresolved: The study was restricted to Black vs. white patient groups and specific Gemma-2 variants due to data availability and existing SAE tooling.
  - What evidence would resolve it: Replicating the probing and ablation pipeline on diverse model families (e.g., Llama, Mistral) and datasets featuring Asian or Hispanic patient cohorts.

- **Question:** Why does latent ablation effectively reduce bias in simplified vignette generation but fail in realistic clinical risk prediction?
  - Basis in paper: [inferred] Results show high efficacy in vignette tasks (~30% reduction) but minimal effect in risk tasks; authors hypothesize race is "more dispersed and entangled" in complex tasks.
  - Why unresolved: The paper identifies the performance gap but provides no definitive mechanistic explanation for why the intervention loses efficacy as task complexity increases.
  - What evidence would resolve it: Layer-wise analysis of latent distribution or experiments with wider SAEs to determine if race is represented more sparsely in complex reasoning contexts.

## Limitations
- SAE-based bias mitigation shows minimal effectiveness in realistic clinical tasks where race information is entangled with clinically-relevant features
- Study limited to Black vs. white patient groups due to data availability, limiting generalizability to other demographic groups
- Steering interventions produce unfaithful explanations, with CoT chains omitting race terms despite causal influence

## Confidence
- **Low:** Generalizability of SAE-based bias mitigation beyond specific clinical tasks tested
- **Medium:** Causal claims about race latents influencing model predictions (steering effects measurable but explanations unfaithful)
- **Medium:** Identification of race-correlated latents (reasonable AUROC but assumes race is sufficiently localized)

## Next Checks
1. **Test SAE intervention effectiveness on multi-modal clinical data** - Evaluate whether the same race latents identified in discharge summaries remain predictive and mitigatable when applied to radiology reports, lab results, or clinical images.
2. **Compare SAE ablation to alternative bias mitigation techniques** - Implement and evaluate counterfactual data augmentation or adversarial debiasing approaches on the same tasks to determine whether SAE-based methods offer unique advantages.
3. **Assess long-term model performance after SAE intervention** - Measure not just immediate bias reduction but also downstream task performance, hallucination rates, and clinical reasoning quality after ablating race latents.