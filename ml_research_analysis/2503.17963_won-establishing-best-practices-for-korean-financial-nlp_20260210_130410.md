---
ver: rpa2
title: 'Won: Establishing Best Practices for Korean Financial NLP'
arxiv_id: '2503.17963'
source_url: https://arxiv.org/abs/2503.17963
tags:
- arxiv
- financial
- preprint
- language
- korean
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Won: Establishing Best Practices for Korean Financial NLP

## Quick Facts
- arXiv ID: 2503.17963
- Source URL: https://arxiv.org/abs/2503.17963
- Reference count: 37
- Key outcome: None

## Executive Summary
This paper presents ₩ON, the first reasoning model for Korean financial domain, developed through systematic analysis of 1,119 submissions to the Woori-Eval financial NLP competition. The authors establish best practices for Korean financial NLP by examining training methodologies across top-performing teams, demonstrating that a two-stage training pipeline (CPT+SFT+DPO) with reasoning-focused fine-tuning achieves state-of-the-art performance on financial benchmarks.

## Method Summary
The authors developed ₩ON by implementing a two-stage training pipeline on Qwen2.5-Math-7B-Instruct. They first collected 200k instruction samples from government/non-profit sources, filtered to 86k high-quality instances, then generated Deepseek-R1 reasoning traces for each prompt. The model underwent Supervised Fine-Tuning (SFT) on this corpus, followed by Direct Preference Optimization (DPO) using preference pairs constructed from R1 outputs (chosen) and SFT model outputs (rejected). Training used 8x H100 80GB GPUs for 25 hours total with Axolotl framework and DeepSpeed-Zero1 optimization.

## Key Results
- Achieved state-of-the-art performance on Korean financial benchmark across 5 MCQA categories
- Demonstrated that CPT+SFT outperforms SFT-only by 2.7 points on average
- Showed reasoning-focused training improves Finance & Accounting tasks (0.78) but underperforms on factual knowledge domains (0.66 on Financial Markets)
- Established that DPO effectively corrects SFT-induced behavioral issues like overthinking on simple prompts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reasoning-focused training with explicit thought traces improves performance on complex financial tasks requiring multi-step deduction
- Mechanism: Training on Deepseek-R1 generated responses with reasoning enclosed in special tags (``...`` and `<solution>...</solution>`) teaches the model to decompose problems before answering
- Core assumption: Financial reasoning benefits from explicit intermediate steps rather than direct answer generation
- Evidence anchors:
  - [abstract] "after regenerating responses for each instance using Deepseek-R1... we release ₩ON, the first reasoning model for the Korean financial domain"
  - [section 5.1] "₩ON demonstrates strong results in the Finance & Accounting category... tasks that benefit from robust mathematical and logical reasoning"
  - [corpus] Limited direct corpus evidence on reasoning mechanisms in financial NLP; related work focuses on task performance rather than reasoning processes
- Break condition: When tasks require primarily factual recall (e.g., regulatory knowledge), reasoning-oriented training may not help and could hurt performance

### Mechanism 2
- Claim: Two-stage training (SFT → DPO) addresses domain specialization while preserving general usability
- Mechanism: SFT on domain-specific instruction data creates financial competence but introduces behavioral artifacts; DPO with preference pairs (R1 outputs as "chosen", SFT model outputs as "rejected") corrects overfitting behaviors like excessive CoT on simple prompts
- Core assumption: The SFT-only model exhibits correctable behaviors that don't require full retraining
- Evidence anchors:
  - [section 5.1] "Post-SFT, the model struggled with everyday prompts, tended to overthink... and occasionally displayed formatting issues"
  - [section 4.2] "submissions that have combined SFT with preference optimization techniques such as DPO or KTO have successfully adapted models"
  - [corpus] "Fine-tuning of lightweight large language models for sentiment classification" confirms SFT+preference optimization pipelines are common but doesn't isolate mechanism
- Break condition: If domain data distribution differs dramatically from general data, DPO may not fully recover general capabilities

### Mechanism 3
- Claim: Continual Pre-Training (CPT) before SFT provides measurable gains for domain adaptation in Korean finance
- Mechanism: CPT on domain corpus (3.7GB text in one team's case) builds domain-specific token representations and knowledge before task-specific SFT
- Core assumption: Domain vocabulary and concepts are underrepresented in base model pre-training
- Evidence anchors:
  - [section 4.2] "CPT+SFT scores an average of 2.7 points higher than plain SFT"
  - [section 4.2] "a well-structured continued pre-training approach can benefit LLMs in Korean finance"
  - [corpus] "LLMs Meet Finance" paper also employs SFT+DPO pipeline but doesn't compare with/without CPT
- Break condition: Effectiveness depends on CPT data quality and size; the paper notes "further research is required to establish what marks a good continual pre-training"

## Foundational Learning

- Concept: Supervised Fine-Tuning (SFT) on instruction-response pairs
  - Why needed here: Core technique used by ALL top-10 teams; foundational for domain adaptation
  - Quick check question: Can you explain why instruction tuning differs from continued pre-training in terms of objective function and data format?

- Concept: Direct Preference Optimization (DPO) and KTO
  - Why needed here: Used by top main-round teams to refine SFT models; critical for fixing post-SFT behavioral issues
  - Quick check question: How does DPO differ from RLHF in terms of reward model requirements?

- Concept: LLM-as-a-Judge evaluation
  - Why needed here: Used for open-ended QA evaluation and for creating preference data during training
  - Quick check question: What biases might GPT-4o-as-judge introduce when evaluating Korean financial responses?

## Architecture Onboarding

- Component map: Qwen2.5-Math-7B-Instruct -> CPT (optional) -> SFT -> DPO/KTO -> ₩ON
- Critical path:
  1. Collect license-free financial corpus from government/non-profit sources
  2. Convert to instruction format using GPT-4o or Qwen2.5-72B-Instruct
  3. Apply quality filters (deduplication, time-bound query removal)
  4. Generate R1 reasoning traces for filtered prompts
  5. Train SFT on ~400k instances (2 epochs, lr=4e-5)
  6. Train DPO on preference pairs (1 epoch, lr=5e-6)
  7. Evaluate on closed benchmark (5 MCQA categories + open-ended)
- Design tradeoffs:
  - **Reasoning vs. factual knowledge**: ₩ON excels at F&A (0.78) but underperforms on Market knowledge (0.66) — reasoning training may not equally benefit all categories
  - **Data quality vs. quantity**: 200k collected → 86k after filtering; aggressive filtering prioritizes quality
  - **Specialization vs. general capability**: SFT-heavy training caused "overthinking" on everyday prompts, requiring DPO correction
- Failure signatures:
  - **MCQA contamination**: SFT model "treating some queries as if they were MCQA tasks" due to heavy MCQA data weighting
  - **Overthinking**: Excessive CoT on simple prompts
  - **Format drift**: Models not outputting valid options during evaluation (mitigated by logit processor)
- First 3 experiments:
  1. Establish baseline by evaluating Qwen2.5-7B-Instruct on the ₩ON benchmark categories without any domain training
  2. Run ablation comparing SFT-only vs. SFT+DPO on a held-out validation set to quantify DPO's correction of overthinking behavior
  3. Test CPT+SFT vs. SFT-only on your own financial corpus to validate the +2.7 point improvement claim on your specific domain data

## Open Questions the Paper Calls Out
None

## Limitations
- Domain Generalization Trade-offs: Reasoning-focused training improves complex financial tasks but underperforms on factual knowledge domains, suggesting category-dependent effectiveness
- Data Quality vs. Quantity Tension: Aggressive filtering reduced data from 200k to 86k instances, with unexplored tradeoffs between dataset size and filtering strictness
- Reproducibility Constraints: Key implementation details remain unspecified, including exact R1 dataset URLs and specific GPT-4o filtering criteria

## Confidence
- High Confidence: The overall training pipeline (CPT+SFT+DPO) achieving state-of-the-art results on the Korean financial benchmark
- Medium Confidence: The mechanism by which reasoning traces improve financial task performance, though category-dependent effects exist
- Low Confidence: The generalizability of these findings beyond Korean finance to other languages or financial subdomains

## Next Checks
1. Run controlled experiments comparing SFT-only vs. SFT+DPO models specifically on the Financial Markets category where ₩ON underperformed, to determine if reasoning-focused training systematically hurts factual knowledge tasks
2. Systematically vary the filtering threshold to create datasets at 50k, 86k, and 150k instances, then measure the impact on both reasoning-intensive and factual knowledge tasks to quantify the quality-quantity tradeoff
3. Apply the exact ₩ON training pipeline (CPT+SFT+DPO) to a non-Korean financial corpus and evaluate on a different financial benchmark to test whether the "best practices" generalize beyond the specific domain and language