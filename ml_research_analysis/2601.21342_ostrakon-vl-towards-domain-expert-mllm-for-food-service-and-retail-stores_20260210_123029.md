---
ver: rpa2
title: 'Ostrakon-VL: Towards Domain-Expert MLLM for Food-Service and Retail Stores'
arxiv_id: '2601.21342'
source_url: https://arxiv.org/abs/2601.21342
tags:
- data
- multimodal
- visual
- arxiv
- fsrs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces Ostrakon-VL, the first MLLM tailored for
  Food-Service and Retail Stores (FSRS) environments. The authors address two main
  obstacles: the noise and heterogeneity of real-world FSRS data and the lack of fine-grained
  evaluation benchmarks for such domains.'
---

# Ostrakon-VL: Towards Domain-Expert MLLM for Food-Service and Retail Stores

## Quick Facts
- arXiv ID: 2601.21342
- Source URL: https://arxiv.org/abs/2601.21342
- Reference count: 30
- Key outcome: Ostrakon-VL achieves 60.1 avg score on ShopBench, outperforming same-scale models by 4.8 points and larger models like Qwen3-VL-235B by 0.7 points.

## Executive Summary
This paper introduces Ostrakon-VL, the first MLLM tailored for Food-Service and Retail Stores (FSRS) environments. The authors address two main obstacles: the noise and heterogeneity of real-world FSRS data and the lack of fine-grained evaluation benchmarks for such domains. To overcome these, they propose QUAD, a multi-stage data curation pipeline, and ShopBench, a specialized benchmark spanning single-image, multi-image, and video inputs. Ostrakon-VL, built on Qwen3-VL-8B, achieves an average score of 60.1 on ShopBench, outperforming both larger models like Qwen3-VL-235B (59.4) and same-scale models like Qwen3-VL-8B (55.3). QUAD reduces the instruction corpus by over 95% while improving performance by 2.5 points. The model demonstrates strong parameter efficiency and robust domain-specific reasoning. All resources will be publicly released to support reproducible research.

## Method Summary
Ostrakon-VL builds on Qwen3-VL-8B and employs a three-stage training pipeline: Caption Bootstrapping (CB) on 4.23M curated captions, Offline Curriculum Learning (OCL) on 3.40M tiered VQA instructions, and Mixed Preference Optimization (MPO) on 3.21M preference pairs. QUAD—a four-stage data curation pipeline—filters raw FSRS data via reward model scoring, foundation model referenced filtering, semantic deduplication, and capability redistribution. ShopBench, the evaluation benchmark, spans single-image, multi-image, and video inputs across five subtasks. Training uses AdamW, lr=2×10⁻⁶, cosine decay, and DeepSpeed ZeRO-2 with BF16 precision.

## Key Results
- Ostrakon-VL-8B achieves 60.1 avg score on ShopBench, outperforming Qwen3-VL-8B (55.3) by 4.8 points and Qwen3-VL-235B (59.4) by 0.7 points.
- QUAD reduces the VQA instruction corpus by over 95% (69.25M → 3.40M) while improving performance by 2.5 points.
- MPO adds +0.6–1.2 points when combined with CB+OCL, demonstrating the value of preference optimization.
- On public benchmarks, Ostrakon-VL-8B scores 66.7 (avg) versus Qwen3-VL-8B's 72.4, showing domain specialization trade-offs.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Aggressive data filtering with quality-learnability scoring improves instruction-tuning efficiency.
- Mechanism: QUAD's four-stage pipeline removes samples that rely on linguistic priors (vision-ablated check), lack marginal utility (foundation model reference gap), or are semantically redundant (embedding-space deduplication), then redistributes by capability taxonomy.
- Core assumption: The reward model (Skywork-VL-Reward) reliably ranks visual grounding quality and that reference model performance gaps approximate learnability.
- Evidence anchors:
  - [abstract] "QUAD reduces the instruction corpus by over 95% while improving performance by 2.5 points."
  - [section 3.3] Equations 4 and 6 define the joint filtering criteria with explicit thresholds.
  - [corpus] No direct corpus validation; related work (Information Density Principle for MLLM Benchmarks) discusses benchmark reliability but not QUAD-style curation.
- Break condition: If reward model scores correlate poorly with downstream task performance, or if capability taxonomy misaligns with target task distribution, filtering may discard useful edge cases.

### Mechanism 2
- Claim: Curriculum-ordered instruction tuning stabilizes domain adaptation under heterogeneous inputs.
- Mechanism: Offline Curriculum Learning stratifies samples by difficulty (K reference models' success rate against ground truth), then trains in tiers—single-image first, then dedicated video-only stage, then multi-image/text—avoiding mixed-batch gradient interference.
- Core assumption: Difficulty estimated by reference model ensembles correlates with training utility and that separate video stages prevent computational bottlenecks.
- Evidence anchors:
  - [section 4.2] Equation 10 defines difficulty score s via voting across reference models.
  - [section A.1.3] Table 9 shows tiered data volumes with dedicated video stage.
  - [corpus] No direct corpus evidence; Traj-MLLM and multi-image reasoning papers address generalization but not curriculum scheduling.
- Break condition: If difficulty proxy misranks samples (e.g., "hard" samples are actually noisy), curriculum may amplify poor signals.

### Mechanism 3
- Claim: Preference optimization with plausible-but-incorrect contrasts sharpens fine-grained discrimination.
- Mechanism: MPO combines preference loss (DPO-style), quality loss (BCO-style), and generation loss; preference pairs are constructed from high-temperature sampling where model alternates correct/incorrect on same input, explicitly contrasting subtle errors.
- Core assumption: Rule-based rewards correctly identify plausible-but-incorrect responses, and the three-loss combination prevents DPO's probability-collapse failure mode.
- Evidence anchors:
  - [section 4.3] Equation 12 defines weighted MPO loss; ablation (Table 6) shows MPO adds +0.6–1.2 points when combined with CB+OCL.
  - [corpus] InternVL and Skywork R1V2 (cited in paper) validate MPO effectiveness in other domains.
- Break condition: If preference pair construction captures spurious patterns rather than true reasoning gaps, model may overfit to contrast artifacts.

## Foundational Learning

- Concept: **Vision-Language Alignment via Caption Pretraining**
  - Why needed here: Caption Bootstrapping stage requires understanding how dense, evidence-rich captions establish visual grounding before task-specific instruction tuning.
  - Quick check question: Can you explain why caption pretraining might improve OCR and spatial grounding in downstream VQA?

- Concept: **Preference Optimization (DPO/BCO family)**
  - Why needed here: MPO combines DPO-style preference learning with BCO-style quality scoring; understanding these primitives is essential for debugging alignment failures.
  - Quick check question: What failure mode does standard DPO exhibit that MPO's generation loss term addresses?

- Concept: **Curriculum Learning Scheduling**
  - Why needed here: OCL's offline difficulty stratification and tiered training require understanding how data ordering affects convergence stability.
  - Quick check question: Why might mixing video and image data in the same batch degrade training efficiency?

## Architecture Onboarding

- Component map:
  - Qwen3-VL-8B base model -> QUAD pipeline (Skywork-VL-Reward, GME-Qwen2VL-2B, capability classifier) -> CB -> OCL (5 tiers + video stage) -> MPO -> ShopBench evaluation

- Critical path:
  1. Raw FSRS data -> QUAD stages 1-4 -> 3.40M curated samples
  2. Caption corpus (4.23M) -> CB training
  3. VQA corpus (3.40M) -> difficulty scoring -> OCL tiers
  4. MPO preference pairs (3.21M) -> final alignment

- Design tradeoffs:
  - 95%+ data reduction trades coverage for signal-to-noise ratio; assumes reward model captures task-relevant quality.
  - Separate video stage trades unified training for batch efficiency and convergence stability.
  - MPO over GRPO trades online sampling richness for ~10× lower compute overhead (per paper).

- Failure signatures:
  - Low VNR with high VIF suggests model ignoring visual input; check vision encoder integration.
  - Sharp drop on specific L3 capability categories after redistribution indicates taxonomy misalignment.
  - MPO stage degrading general benchmarks suggests overfitting to preference pairs; reduce MPO weight w1.

- First 3 experiments:
  1. **QUAD ablation**: Train on raw 69.25M vs. QUAD-filtered 3.40M on ShopBench subset; verify 2.5-point gain replicates.
  2. **Curriculum collapse test**: Shuffle OCL tiers randomly; compare convergence stability and final ShopBench score.
  3. **MPO pair quality audit**: Manually inspect 100 preference pairs; verify "plausible-but-incorrect" samples contain subtle errors rather than obvious failures.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can uncertainty modeling be effectively integrated into domain-specific MLLMs to quantify prediction reliability in high-stakes FSRS compliance workflows?
- Basis in paper: [explicit] The conclusion states that "future work will... incorporate stricter auditing protocols and uncertainty modeling to further enhance reliability."
- Why unresolved: Current evaluation focuses on accuracy metrics (VNR, VIF, MG) but does not provide calibrated confidence scores that auditors could use to flag low-certainty predictions for human review.
- What evidence would resolve it: An uncertainty-aware variant of Ostrakon-VL evaluated on ShopBench with metrics like Expected Calibration Error or Brier scores across FSRS subtasks.

### Open Question 2
- Question: Can the trade-off between domain specialization and general multimodal competence be mitigated through architectural or training modifications?
- Basis in paper: [inferred] Table 4 shows Ostrakon-VL-8B achieves 66.7 average on public benchmarks versus Qwen3-VL-8B's 72.4, with notable drops on MMVet (36.4 vs 61.2) and DocVQA (85.7 vs 94.4), suggesting catastrophic forgetting of some general capabilities.
- Why unresolved: The paper demonstrates the trade-off exists but does not explore whether techniques like elastic weight consolidation, parameter-efficient fine-tuning, or mixture-of-experts could preserve general capabilities during domain adaptation.
- What evidence would resolve it: Ablation experiments comparing full fine-tuning against parameter-isolating methods while tracking both ShopBench and general benchmark performance.

### Open Question 3
- Question: What mechanisms can address the intrinsic scarcity of complex interaction categories (e.g., Social Relation, Physical Property) that remain underrepresented even after Capability Coverage Redistribution?
- Basis in paper: [explicit] Section A.1.2 notes that "a residual long-tail persists, largely due to intrinsic data constraints: categories requiring complex interactions (e.g., Social Relation, Physical Property) remain naturally scarce even after redistributing."
- Why unresolved: QUAD's redistribution balances frequency but cannot synthesize genuinely scarce interaction types from the raw data pool.
- What evidence would resolve it: Evaluation of targeted data augmentation or synthetic generation pipelines for long-tail categories, measuring performance gains on ShopBench's sparsely populated L4 categories.

### Open Question 4
- Question: What is the compositional effect of temporal evaluation complexity and explicit operational rule adherence on MLLM performance in video-based FSRS inspection?
- Basis in paper: [explicit] The conclusion commits to extending "stronger temporal and rule-centric evaluations" as future work.
- Why unresolved: ShopBench's video subset currently scores lowest among subtasks (53.3 for Ostrakon-VL), and the benchmark does not yet systematically test whether models can apply explicit operational rules consistently across video sequences.
- What evidence would resolve it: An extended ShopBench-Video benchmark with rule-grounded temporal reasoning questions (e.g., "Does the staff member follow the required handwashing sequence?") and baseline evaluations.

## Limitations
- Data quality assumptions: QUAD's effectiveness relies on the Skywork-VL-Reward model's ability to reliably rank instruction quality, but no direct human evaluation of filtered vs. raw data quality is reported.
- Taxonomy completeness: The L1-L4 capability taxonomy and redistribution priors are determined by validation/manual auditing, but specific thresholds and category definitions are not disclosed, limiting reproducibility.
- Generalization boundaries: Strong ShopBench performance on retail/food-service tasks does not guarantee robustness to domain shift or novel FSRS scenarios not represented in training data.

## Confidence
- **High confidence**: Ostrakon-VL achieves superior ShopBench scores versus same-scale models; QUAD pipeline structure and training stages are clearly specified; architectural design choices are technically coherent.
- **Medium confidence**: Claims about QUAD's 2.5-point performance gain from 95%+ data reduction; MPO's +0.6–1.2 point contribution over baselines; curriculum ordering benefits from difficulty stratification.
- **Low confidence**: Generalizability of QUAD filtering thresholds across different FSRS datasets; long-term stability of MPO preference pair quality; sensitivity to capability taxonomy choices.

## Next Checks
1. **QUAD filtering ablation**: Train Ostrakon-VL on raw 69.25M VQA instructions versus QUAD-filtered 3.40M set; verify the claimed 2.5-point ShopBench performance gain reproduces and inspect qualitative differences in retained samples.
2. **Curriculum ordering stress test**: Shuffle OCL tiers randomly versus following the published difficulty stratification; compare convergence curves and final ShopBench scores to isolate curriculum ordering effects.
3. **MPO pair audit**: Randomly sample 100 preference pairs from the MPO stage; manually classify whether "plausible-but-incorrect" examples truly contain subtle errors versus obvious failures, validating the contrast quality assumption.