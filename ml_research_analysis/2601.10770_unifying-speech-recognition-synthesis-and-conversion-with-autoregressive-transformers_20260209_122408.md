---
ver: rpa2
title: Unifying Speech Recognition, Synthesis and Conversion with Autoregressive Transformers
arxiv_id: '2601.10770'
source_url: https://arxiv.org/abs/2601.10770
tags:
- speech
- wang
- zhang
- autoregressive
- chen
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces General-Purpose Audio (GPA), a unified autoregressive
  transformer model that integrates text-to-speech (TTS), automatic speech recognition
  (ASR), and voice conversion (VC) within a single framework. GPA operates on a shared
  discrete audio token space, enabling seamless task switching through instruction-driven
  conditioning without requiring task-specific architectures.
---

# Unifying Speech Recognition, Synthesis and Conversion with Autoregressive Transformers

## Quick Facts
- **arXiv ID**: 2601.10770
- **Source URL**: https://arxiv.org/abs/2601.10770
- **Reference count**: 40
- **Primary result**: A unified autoregressive transformer model (GPA) integrates TTS, ASR, and VC in a single framework with competitive performance across tasks.

## Executive Summary
This paper introduces GPA (General-Purpose Audio), a unified autoregressive transformer model that integrates text-to-speech (TTS), automatic speech recognition (ASR), and voice conversion (VC) within a single framework. GPA operates on a shared discrete audio token space, enabling seamless task switching through instruction-driven conditioning without requiring task-specific architectures. The model leverages joint multi-task training across ASR, TTS, and VC tasks using a mixture of the Emilia dataset and proprietary data, and employs a dual-tokenizer scheme (GLM and BiCodec) to enhance semantic fidelity. A key contribution is the purely autoregressive formulation, which simplifies inference, supports high concurrency and throughput, and enables streaming applications.

## Method Summary
GPA uses a Qwen3-based autoregressive Transformer decoder trained from scratch with joint multi-task learning on ASR, TTS, and VC. The model processes audio through a dual-tokenizer scheme (GLM for linguistic alignment, BiCodec for acoustic/semantic tokens) into a unified discrete token space. Task behavior emerges from input-output token composition and instruction prompts rather than specialized modules. The framework supports both 0.3B (edge) and 3B (server) model sizes, trained on ~1M hours of pretraining data and ~200K hours of supervised fine-tuning. Audio preprocessing includes normalization, noise suppression, VAD/diarization, and multi-ASR transcript consensus with forced-alignment punctuation.

## Key Results
- TTS achieves character error rates as low as 0.85% (Chinese) and word error rates of 1.51% (English)
- ASR achieves word error rates of 8.88% (Librispeech) and 4.50% (AISHELL-1) with the 0.3B model
- Unified design reduces system complexity and improves cross-task knowledge transfer
- Supports scalable, low-latency deployment in both server and resource-constrained environments

## Why This Works (Mechanism)

### Mechanism 1: Shared Discrete Token Space Enables Cross-Task Transfer
Representing speech and text in a unified discrete token space allows a single autoregressive model to perform ASR, TTS, and VC without architectural modification. The dual-tokenizer scheme maps heterogeneous modalities into a common vocabulary, enabling the model to transfer acoustic-linguistic knowledge across recognition, synthesis, and conversion.

### Mechanism 2: Joint Multi-Task Training Provides Implicit Regularization
Simultaneously training on ASR, TTS, and VC within a shared objective improves individual task performance compared to isolated training. ASR enforces acoustic-to-linguistic alignment while TTS/VC emphasize acoustic realization and speaker variability, encouraging linguistically grounded yet acoustically expressive representations.

### Mechanism 3: Purely Autoregressive Formulation Simplifies Inference
A fully autoregressive architecture achieves favorable throughput and latency compared to hybrid multi-stage systems. All tasks are reduced to sequential token prediction without encoder-decoder handoffs, enabling straightforward batching, streaming via incremental token emission, and integration with standard LLM serving infrastructure.

## Foundational Learning

- **Neural Audio Codecs (e.g., EnCodec, SoundStream)**: GPA relies on discretizing continuous audio into tokens; understanding codec quantization, codebook design, and reconstruction tradeoffs is essential. Quick check: Can you explain how residual vector quantization compresses audio while preserving perceptual quality?

- **Autoregressive Language Modeling (Transformer Decoder)**: The entire GPA framework is built on next-token prediction; fluency with causal masking, KV-cache inference, and temperature sampling is prerequisite. Quick check: How does KV-cache reuse improve inference efficiency in autoregressive models?

- **Multi-Task Learning and Gradient Interference**: Joint ASR/TTS/VC training assumes beneficial shared representations; understanding task balancing, gradient conflicts, and negative transfer informs debugging. Quick check: What is negative transfer in multi-task learning, and how might it manifest in a speech model trained on both recognition and synthesis?

## Architecture Onboarding

- **Component map**: Audio normalization -> VAD/diarization -> multi-ASR transcription consensus -> BiCodec + GLM tokenization -> Qwen3 autoregressive Transformer decoder -> task-specific output tokens -> BiCodec detokenization

- **Critical path**: Verify tokenizer output shapes and vocabulary alignment across GLM/BiCodec; confirm instruction templates correctly route tasks (ASR vs TTS vs VC); profile streaming inference latency and KV-cache memory for target concurrency

- **Design tradeoffs**: 0.3B vs 3B (edge deployment friendliness vs recognition accuracy); dual-tokenizer complexity (richer semantics vs increased preprocessing/compute overhead); pure AR vs hybrid (simpler serving vs potential quality gap on complex prosody)

- **Failure signatures**: High WER/CER on ASR with low TTS error (insufficient acoustic-linguistic alignment capacity); speaker similarity collapse in TTS/VC (acoustic token degradation or insufficient speaker conditioning); latency spikes under concurrency (KV-cache memory pressure or batching inefficiency)

- **First 3 experiments**: 1) Tokenizer sanity check: Encode/decode audio samples through BiCodec and GLM tokenizers; measure reconstruction SNR and semantic token alignment with transcription. 2) Single-task baseline: Train GPA-0.3B on ASR only; compare WER to joint-trained model to quantify multi-task benefit. 3) Streaming latency profile: Benchmark TTFC and RTF for TTS at concurrency 1, 10, 40; identify KV-cache or batching bottlenecks using profiler.

## Open Questions the Paper Calls Out

### Open Question 1
**Question**: Can reinforcement learning (RL) on discrete audio tokens effectively optimize subjective metrics like perceptual quality and instruction following in a unified model?
**Basis in paper**: Section 5 states that the unified formulation "opens the door to metric-driven post-training" via RL, but this has not yet been implemented or validated.
**Why unresolved**: The authors propose RL as a method to resolve credit assignment issues inherent in heterogeneous pipelines, but they do not provide evidence that RL on discrete tokens remains stable or effective for multi-task speech generation.
**What evidence would resolve it**: Empirical results showing improvements in Mean Opinion Score (MOS) or instruction-following accuracy after applying RL fine-tuning to the GPA backbone.

### Open Question 2
**Question**: To what extent does the shared modeling bottleneck in a unified token space restrict peak performance on specialized tasks compared to task-specific architectures?
**Basis in paper**: Section 5 acknowledges that the shared formulation "may limit peak performance on highly specialized tasks when compared to purpose-built architectures."
**Why unresolved**: While the paper demonstrates competitive average performance, it does not quantify the "performance ceiling" or trade-offs incurred by forcing all tasks into a single discrete autoregressive pathway.
**What evidence would resolve it**: A comparative analysis on extreme out-of-distribution tasks (e.g., rare vocal styles or noisy ASR) showing the performance gap between GPA and state-of-the-art specialized models.

### Open Question 3
**Question**: Does the purely autoregressive formulation require disproportionately more parameters to match the ASR efficiency of discriminative models?
**Basis in paper**: Section 5 notes the weaker ASR performance of the GPA-0.3B model is due to "capacity constraints," and Section 3.3 shows larger gaps in ASR metrics for smaller models.
**Why unresolved**: It is unclear if the autoregressive generative objective is fundamentally less data/capacity-efficient for discriminative tasks (recognition) than for generative tasks (synthesis).
**What evidence would resolve it**: Scaling curves comparing ASR Word Error Rate (WER) against parameter count for GPA versus standard discriminative baselines (e.g., Whisper) to see if the scaling coefficient differs significantly.

## Limitations
- ASR performance degrades significantly in smaller models (0.3B variant shows much higher WER than 3B)
- No speaker similarity metrics reported for voice conversion or assessment of prosody controllability
- Training data composition beyond Emilia dataset is unspecified, limiting reproducibility
- No ablations provided on the dual-tokenizer design to quantify marginal benefit

## Confidence

- **High Confidence**: The core mechanism of shared discrete token space enabling unified task execution is well-supported by architecture description and task induction experiments. The autoregressive formulation's inference advantages (streaming, simplicity) are clearly demonstrated through reported TTFC/RTF metrics.

- **Medium Confidence**: Joint multi-task training improves performance over isolated training, but lacks direct ablation comparisons in the paper. The degree of regularization benefit versus capacity overhead is not quantified.

- **Low Confidence**: Claims about cross-task knowledge transfer (e.g., ASR improving TTS quality) are plausible but not empirically validated in the paper. Related work (VoiceCraft-X) suggests feasibility but does not confirm this specific transfer.

## Next Checks

1. **Capacity Sensitivity Analysis**: Train GPA-1B and GPA-0.1B variants; compare ASR/TTS/VC performance curves to identify optimal model size and detect capacity saturation or interference thresholds.

2. **Dual-Tokenizer Ablation**: Replace BiCodec+GLM with either tokenizer alone; measure impact on TTS/WER/CER to quantify semantic fidelity gains from the hybrid approach.

3. **Streaming Stress Test**: Benchmark GPA-0.3B TTS at 1, 10, 40 concurrent streams with varying audio lengths (1s, 10s, 60s); profile KV-cache growth, memory pressure, and TTFC stability to identify deployment bottlenecks.