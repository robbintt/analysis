---
ver: rpa2
title: 'RAISE: A Unified Framework for Responsible AI Scoring and Evaluation'
arxiv_id: '2510.18559'
source_url: https://arxiv.org/abs/2510.18559
tags:
- fairness
- robustness
- responsible
- evaluation
- framework
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work introduces RAISE, a unified framework for evaluating
  deep learning models across four responsibility dimensions: explainability, fairness,
  robustness, and sustainability. The framework uses normalized metrics and aggregates
  them into a holistic Responsibility Score.'
---

# RAISE: A Unified Framework for Responsible AI Scoring and Evaluation

## Quick Facts
- arXiv ID: 2510.18559
- Source URL: https://arxiv.org/abs/2510.18559
- Reference count: 23
- Multi-dimensional responsible AI evaluation framework for tabular deep learning models

## Executive Summary
This work introduces RAISE, a unified framework for evaluating deep learning models across four responsibility dimensions: explainability, fairness, robustness, and sustainability. The framework uses normalized metrics and aggregates them into a holistic Responsibility Score. Three deep learning models—Multilayer Perceptron, Tabular ResNet, and Feature Tokenizer Transformer—were evaluated on structured datasets from finance, healthcare, and socioeconomic domains. The analysis revealed that no single model dominates all dimensions: the MLP excelled in sustainability and robustness, the Transformer in explainability and fairness at high environmental cost, and the Tabular ResNet offered a balanced profile. These findings emphasize the importance of multi-dimensional evaluation for responsible model selection in high-stakes applications.

## Method Summary
RAISE provides a systematic evaluation framework that aggregates 21 metrics across four responsibility dimensions into normalized Dimension Scores and an optional holistic Responsibility Score. Three deep learning architectures (MLP, Tabular ResNet, Feature Tokenizer Transformer) were trained on German Credit, Diabetes 130-Hospitals, and ACSIncome datasets to comparable F1-Score thresholds. SHAP-based feature attributions were evaluated for faithfulness and complexity using Quantus, fairness was measured via group disparities with AIF360/Fairlearn, robustness was assessed using ART against adversarial attacks, and sustainability was quantified through carbon emissions and computational metrics. All metrics were normalized to [0,1] scales with lower-is-better values inverted before aggregation.

## Key Results
- No single model dominates all responsibility dimensions; each architecture exhibits distinct trade-offs
- MLP excels in sustainability and robustness but lags in explainability and fairness
- Transformer achieves top explainability and fairness scores at high environmental cost
- Tabular ResNet offers balanced performance across all four dimensions
- Performance-controlled evaluation (equal F1-Score) reveals architectural responsibility profiles independent of predictive capacity

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Normalizing heterogeneous metrics to a unified scale enables cross-dimensional comparison and aggregation.
- **Mechanism:** Raw metrics are collected across four dimensions (21 total), each normalized to [0,1] with lower-is-better metrics inverted so that 1 always represents ideal behavior. These are then averaged within dimensions to produce Dimension Scores (DS), which can be further aggregated into a holistic Responsibility Score (RS).
- **Core assumption:** Metrics from conceptually distinct dimensions (e.g., carbon emissions vs. explanation faithfulness) are commensurable once normalized, permitting meaningful aggregation and comparison.
- **Evidence anchors:** [abstract]: "normalized metrics and aggregates them into a holistic Responsibility Score"; [section 3.3]: "Each raw metric is first normalized to a scale, with lower-is-better values inverted to ensure a score of 1 represents ideal behavior. These are then averaged to produce a Dimension Score (DS) for each of our four pillars."; [corpus]: Weak direct evidence—corpus papers address individual dimensions but do not validate cross-dimensional normalization schemes.
- **Break condition:** If context-specific stakeholder weights make uniform averaging misleading; or if normalization obscures practical interpretability.

### Mechanism 2
- **Claim:** Performance-controlled evaluation (normalizing for F1-Score) isolates architectural responsibility profiles from raw predictive capacity.
- **Mechanism:** By training all models to a comparable F1-Score threshold per dataset, the framework attributes observed differences in responsibility scores to architectural inductive biases rather than accuracy advantages.
- **Core assumption:** Models at similar F1-Score thresholds have comparable predictive "budgets," so residual variation in responsibility dimensions reflects inherent architectural properties.
- **Evidence anchors:** [abstract]: "Three deep learning models... were evaluated... The analysis revealed that no single model dominates all dimensions"; [section 4.1]: "all models were trained to a comparable F1-Score threshold on each dataset"; [section 4.2]: "large differences in responsibility were hidden by the fact that all models reached similar F1 scores"; [corpus]: No direct validation of this control methodology in neighbor papers.
- **Break condition:** If F1-Score equivalence does not imply comparable model maturity; or if thresholding introduces hidden selection biases.

### Mechanism 3
- **Claim:** Different architectures encode systematic, inherent trade-offs across responsibility dimensions that persist despite equivalent predictive performance.
- **Mechanism:** Architectural inductive biases (e.g., parameter count, attention mechanisms, residual connections) differentially affect sustainability (compute/energy), robustness (sensitivity to perturbations), explainability (feature attribution stability), and fairness (group-wise error distributions).
- **Core assumption:** Trade-offs are structural to architecture class, not artifacts of a specific training run or hyperparameter configuration.
- **Evidence anchors:** [abstract]: "MLP excelled in sustainability and robustness, the Transformer in explainability and fairness at a very high environmental cost, and the Tabular ResNet offered a balanced profile"; [section 4.2]: "key trade-offs are built into each architecture"; [corpus]: Indirect support—FairSense-AI notes sustainability/fairness tensions; DICE emphasizes interpretable evaluation trade-offs.
- **Break condition:** If architectural modifications substantially alter the trade-off surface; or if dataset-specific effects dominate architectural effects.

## Foundational Learning

- **Concept: SHAP (SHapley Additive exPlanations)**
  - **Why needed here:** RAISE uses SHAP to generate feature attributions, which are then evaluated for faithfulness, complexity, robustness, and randomization resistance.
  - **Quick check question:** Can you explain why SHAP values are considered "model-agnostic" and what the "faithfulness" of an explanation means?

- **Concept: Group Fairness Metrics (Demographic Parity, Equalized Odds)**
  - **Why needed here:** The framework quantifies fairness via performance disparities across sensitive subgroups; understanding these metrics is essential to interpret the Fairness Dimension Score.
  - **Quick check question:** What is the difference between Demographic Parity and Equalized Odds, and when might optimizing one harm the other?

- **Concept: Adversarial Robustness (FGSM, CLEVER-u, Loss Sensitivity)**
  - **Why needed here:** RAISE evaluates robustness using attack-based (FGSM Accuracy Gap) and attack-independent metrics (CLEVER-u, Loss Sensitivity) to quantify vulnerability.
  - **Quick check question:** What does a high CLEVER-u score indicate about a model's local robustness, and how does it differ from measuring accuracy drop under FGSM?

## Architecture Onboarding

- **Component map:** Structured dataset + trained model (MLP, TabResNet, Transformer) + sensitive attribute → 21 metrics across four dimensions → normalization to [0,1] → Dimension Scores → optional Responsibility Score → multi-dimensional profile visualization

- **Critical path:** 1. Train/prepare models to comparable F1 thresholds on target dataset. 2. Run SHAP to generate feature attributions. 3. Compute all 21 metrics using Quantus (explainability), AIF360/Fairlearn (fairness), ART (robustness), and Lacoste score + compute metrics (sustainability). 4. Normalize and invert metrics as needed. 5. Aggregate into Dimension Scores and (optionally) Responsibility Score. 6. Analyze trade-offs via profile visualization rather than relying solely on RS.

- **Design tradeoffs:** Profile vs. Score (multi-dimensional profile more actionable than aggregate RS); Metric selection (21 metrics provide breadth but may not be exhaustive); Performance control (equalizing F1 enables architectural comparison but may not reflect real-world deployment).

- **Failure signatures:** Metric saturation (all models score near 1.0 on a dimension); Normalization artifacts (extreme outliers skew normalized scores); Context mismatch (applying uniform dimension weights where regulatory priorities differ).

- **First 3 experiments:** 1. Reproduce baseline comparison: Run RAISE on German Credit dataset for all three architectures; verify F1 near reported values and compare dimension scores against Table 1. 2. Ablate a dimension: Remove fairness metrics and observe how RS and profiles shift; assess whether architectural rankings change. 3. Stress-test normalization: Introduce a synthetic outlier model (extremely high parameter count) and examine its effect on max-norm scaled sustainability scores; identify whether normalization remains meaningful.

## Open Questions the Paper Calls Out

- **Open Question 1:** Do non-neural "classic" models (e.g., boosted trees) exhibit significantly different responsibility trade-offs compared to the deep learning architectures evaluated in this study? Basis: [explicit] The authors state in the Discussion that they "will expand the framework to include the classic, non-neural models like boosted trees that are still workhorses in many industries." Why unresolved: Current experimental scope is strictly limited to three deep learning architectures. What evidence: Comparative evaluation of XGBoost or Random Forest models on the same datasets using the RAISE metric suite.

- **Open Question 2:** How can a quantifiable "privacy" dimension be successfully integrated into the RAISE aggregation methodology? Basis: [explicit] The authors list as a key direction for future work: "we will add privacy as a core dimension, measuring how well a model protects sensitive data." Why unresolved: Current framework lacks metrics for data privacy or leakage. What evidence: Formulation of a normalized privacy score that correlates with membership inference attacks or data extraction risks.

- **Open Question 3:** Does the application of the RAISE framework by practitioners lead to more defensible model selection decisions in real-world workflows? Basis: [explicit] The Conclusion notes the need to "conduct usability studies to validate effectiveness in real-world workflows," building on the Discussion's aim to "move beyond the lab." Why unresolved: Paper presents results from controlled experiments on public datasets rather than human-subject studies or live deployments. What evidence: Usability studies or field trials where stakeholders utilize the Responsibility Score to justify specific model choices in high-stakes scenarios.

## Limitations
- The framework assumes commensurability of heterogeneous responsibility metrics through uniform normalization, lacking validation that this aggregation meaningfully reflects stakeholder priorities or legal standards
- Performance-controlled evaluation via F1-Score thresholding may not generalize if architectures exhibit different learning dynamics at equivalent accuracy levels
- The unified Responsibility Score's practical interpretability across domains and regulatory contexts remains largely theoretical

## Confidence
- **High:** Cross-dimensional metric normalization methodology and aggregation mechanics are clearly specified and reproducible
- **Medium:** Architecture-level trade-off findings are well-documented but require external validation given dataset-specific effects
- **Low:** The unified Responsibility Score's practical interpretability across domains and regulatory contexts remains largely theoretical

## Next Checks
1. Conduct ablation studies removing individual dimensions to assess whether architectural rankings and trade-off patterns persist
2. Replicate the evaluation on a new tabular dataset from a different domain to test generalizability of observed trade-offs
3. Compare normalized Responsibility Scores against domain-expert weighted scoring to evaluate alignment with stakeholder priorities