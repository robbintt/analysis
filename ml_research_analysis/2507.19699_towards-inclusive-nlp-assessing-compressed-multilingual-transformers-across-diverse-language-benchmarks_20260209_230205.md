---
ver: rpa2
title: 'Towards Inclusive NLP: Assessing Compressed Multilingual Transformers across
  Diverse Language Benchmarks'
arxiv_id: '2507.19699'
source_url: https://arxiv.org/abs/2507.19699
tags:
- arabic
- arxiv
- multilingual
- language
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates the performance of multilingual and monolingual
  Large Language Models (LLMs) across Arabic, English, and Kannada languages, focusing
  on the effects of model compression strategies like pruning and quantization. The
  study benchmarks six open-access LLMs (BLOOMZ, AceGPT, Jais, LLaMA-2, XGLM, and
  AraGPT) on standard datasets including ArabicMMLU, EnglishMMLU, and Kannada-ARC-C-2.5K.
---

# Towards Inclusive NLP: Assessing Compressed Multilingual Transformers across Diverse Language Benchmarks

## Quick Facts
- **arXiv ID:** 2507.19699
- **Source URL:** https://arxiv.org/abs/2507.19699
- **Reference count:** 30
- **Primary result:** Multilingual models outperform monolingual ones in low-resource languages when compressed via quantization (4-bit/8-bit) rather than aggressive pruning.

## Executive Summary
This paper evaluates the performance of multilingual and monolingual Large Language Models (LLMs) across Arabic, English, and Kannada languages, focusing on the effects of model compression strategies like pruning and quantization. The study benchmarks six open-access LLMs (BLOOMZ, AceGPT, Jais, LLaMA-2, XGLM, and AraGPT) on standard datasets including ArabicMMLU, EnglishMMLU, and Kannada-ARC-C-2.5K. Results show that multilingual models outperform monolingual ones, especially in low-resource settings. Quantization (4-bit and 8-bit) maintains model accuracy with minimal loss, while aggressive pruning significantly degrades performance, particularly in larger models. The findings highlight the importance of model size, training diversity, and compression techniques for developing scalable and fair multilingual NLP solutions.

## Method Summary
The study evaluates six open-access LLMs (BLOOMZ, AceGPT, Jais, LLaMA-2, XGLM, and AraGPT) across Arabic, English, and Kannada languages using standard benchmarks. Compression strategies include L1 unstructured pruning at 20%, 40%, and 80% sparsity levels, and post-training quantization at 4-bit and 8-bit precision. Models are tested on ArabicMMLU, EnglishMMLU, and Kannada-ARC-C-2.5K datasets using zero-shot evaluation with subject-specific prompting. Performance is measured in accuracy percentage, comparing compression effects across languages and model sizes.

## Key Results
- Multilingual models (BLOOMZ) outperform monolingual models (AceGPT, AraGPT) in low-resource languages (Arabic, Kannada) by up to 10% accuracy.
- 4-bit and 8-bit quantization maintains model accuracy with <2% degradation across all languages and model sizes.
- Aggressive pruning (80% sparsity) causes catastrophic accuracy collapse, particularly in larger models, with Kannada accuracy dropping 7.9% compared to 3.1% on English.
- Model size matters: BLOOMZ-7.1B outperforms BLOOMZ-560M by 8.3% on Kannada-ARC-C-2.5K, demonstrating scale advantages in low-resource settings.

## Why This Works (Mechanism)

### Mechanism 1: Precision Reduction via Quantization
Reducing model precision to 4-bit or 8-bit integers preserves cross-lingual reasoning accuracy significantly better than removing parameters via pruning. Quantization maps floating-point weights to lower-bit integers while retaining the network topology and relative magnitude of weights, maintaining structural pathways required for multilingual reasoning while drastically cutting memory usage. The semantic knowledge required for multilingual tasks is distributed across the network's structure rather than concentrated in the precise numeric values of individual weights.

### Mechanism 2: Cross-Lingual Transfer via Shared Embeddings
Multilingual models outperform monolingual counterparts in low-resource settings due to cross-lingual knowledge transfer. Models like BLOOMZ pre-train on diverse languages simultaneously, creating a shared embedding space that allows the model to apply reasoning patterns learned from high-resource data (English) to low-resource languages (Kannada), compensating for data scarcity. Linguistic features and logical reasoning patterns are partially language-agnostic and can be abstracted across distinct linguistic families.

### Mechanism 3: Pruning-Induced Forgetting in Low-Resource Languages
Aggressive weight pruning degrades performance in low-resource languages faster than in high-resource languages. Weights associated with low-resource languages are often fewer and less redundantly encoded, making them vulnerable to magnitude-based pruning criteria. Important weights for low-resource languages may have lower magnitude than those for dominant languages, making them disproportionately affected by L1 unstructured pruning.

## Foundational Learning

- **Post-Training Quantization (PTQ):** The study relies heavily on PTQ as a primary efficiency lever. Understanding the difference between FP16 and INT4 is essential to interpret the "minimal loss" results. How does the scale factor (S) and zero point (Z) map a continuous floating-point weight to a discrete integer in Eq. (2)?

- **Unstructured vs. Structured Pruning:** The paper utilizes L1 unstructured pruning. Knowing that this removes individual weights globally (rather than entire neurons/heads) explains why the model size shrinks but hardware acceleration can be inconsistent. Why does removing 80% of individual weights (unstructured) lead to a larger accuracy drop in larger models compared to smaller ones?

- **Zero-Shot Cross-Lingual Transfer:** To understand why models like AceGPT (English-tuned) or BLOOMZ (Multilingual) are being tested on Kannada without specific fine-tuning. What property of the Transformer architecture allows a model trained on English reasoning tasks to answer questions in Kannada?

## Architecture Onboarding

- **Component map:** Model -> Compression (Quantization/Pruning) -> Prompting -> Evaluation
- **Critical path:** 1. Model Load: Initialize model (e.g., BLOOMZ-7.1B) 2. Compression: Apply 20%, 40%, or 80% pruning OR 4-bit/8-bit quantization 3. Prompting: Inject subject/meta-data into context window 4. Inference: Extract logits for answer tokens (A, B, C, D)
- **Design tradeoffs:** The paper clearly favors Quantization over Pruning. Use 4-bit for memory saving with <2% accuracy cost. Avoid Pruning >40% unless model size is the only constraint and accuracy is secondary. Larger models (13B) resist moderate pruning better than smaller ones initially, but suffer catastrophic collapse at 80% sparsity.
- **Failure signatures:** The "Pruning Cliff": Accuracy remains stable up to ~20-40% sparsity, then collapses at 80%. Low-Resource Drift: If Kannada accuracy drops significantly more than English accuracy under the same compression, you are observing "Pruning-Induced Cross-Lingual Drift."
- **First 3 experiments:** 1. Baseline Profiling: Run BLOOMZ-560M vs. BLOOMZ-7.1B on EnglishMMLU to establish the "scale advantage" delta 2. Safe Compression Test: Apply 8-bit quantization to BLOOMZ-7.1B and verify that accuracy delta is < 1% 3. Stress Test: Apply 80% pruning to AceGPT-13B and confirm the "collapse" phenomenon

## Open Questions the Paper Calls Out
- **Advanced Compression Techniques:** Do movement pruning, knowledge distillation, and enhanced quantization preserve linguistic richness of low-resource languages better than standard L1 pruning?
- **Dialectal and Code-Switching Impact:** How does model compression impact performance on dialectal Arabic and code-switching scenarios compared to Modern Standard Arabic?
- **Few-Shot Prompting Mitigation:** Can few-shot prompting effectively mitigate accuracy degradation caused by aggressive pruning in low-resource languages?

## Limitations
- The evaluation focuses primarily on knowledge-based question answering, potentially missing other critical NLP tasks like generation, translation, or conversation.
- The analysis may not capture language-specific pruning vulnerabilities beyond the three studied languages (Arabic, English, Kannada).
- The paper doesn't explore the interaction between different compression techniques or their effects on model training dynamics.

## Confidence
- **High Confidence:** Quantization maintains cross-lingual accuracy with minimal degradation (<2%). The empirical evidence strongly supports this mechanism.
- **High Confidence:** Cross-lingual transfer benefits are robustly demonstrated. Multilingual models consistently outperform monolingual ones in low-resource settings.
- **Medium Confidence:** Pruning-induced degradation is well-documented, but the exact mechanisms of cross-lingual drift remain somewhat speculative.

## Next Checks
1. **Cross-Task Generalization Test:** Validate whether compression behaviors hold across additional NLP tasks beyond knowledge-based question answering.
2. **Language Family Expansion:** Test pruning vulnerability hypothesis on additional low-resource languages from different families to determine universality.
3. **Compression Combination Study:** Investigate effects of combining quantization with moderate pruning to determine if synergistic benefits exist.