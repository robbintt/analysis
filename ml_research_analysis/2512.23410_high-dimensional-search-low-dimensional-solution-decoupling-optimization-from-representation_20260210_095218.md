---
ver: rpa2
title: 'High-Dimensional Search, Low-Dimensional Solution: Decoupling Optimization
  from Representation'
arxiv_id: '2512.23410'
source_url: https://arxiv.org/abs/2512.23410
tags:
- solution
- projection
- random
- learning
- subspace
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates the relationship between model width and
  intrinsic dimensionality in deep learning. While state-of-the-art models are extremely
  wide, evidence suggests their representations live in much lower-dimensional solution
  manifolds.
---

# High-Dimensional Search, Low-Dimensional Solution: Decoupling Optimization from Representation

## Quick Facts
- arXiv ID: 2512.23410
- Source URL: https://arxiv.org/abs/2512.23410
- Authors: Yusuf Kalyoncuoglu; Ratmir Miftachov
- Reference count: 9
- Primary result: Trained deep models retain >97% accuracy under 16× random projection compression, confirming solution manifolds are intrinsically low-dimensional

## Executive Summary
This work demonstrates that state-of-the-art deep learning models, despite their extreme width, operate on intrinsically low-dimensional solution manifolds. Through extensive experiments with ResNet, ViT, and BERT models, the authors show that random Johnson-Lindenstrauss projections can compress representations by up to 16× while preserving over 97% of classification accuracy. The key insight is that high-dimensional width serves optimization (search) rather than representation (solution), as random projections match the performance of data-dependent PCA and learned methods. This establishes Subspace-Native Distillation as a promising paradigm where student models directly target this intrinsic manifold.

## Method Summary
The method involves fine-tuning pre-trained backbones on target tasks, freezing their weights, then projecting final representations into random low-dimensional subspaces using Johnson-Lindenstrauss projections. A linear classifier is trained on these compressed features with higher learning rates than the original fine-tuning. The JL projection matrix R ∈ R^(k×d) is generated with entries ~ N(0,1) and scaled by 1/√k. The approach is tested across multiple architectures (ResNet-50, ViT-B/16, BERT-base) and datasets (CIFAR-100, ImageNet-100, MNLI) with compression ratios up to 16×.

## Key Results
- ResNet-50 on CIFAR-100: 2048→128 projection retains 80.19% vs 82.40% baseline
- ViT-B/16 on ImageNet-100: 768→64 projection retains 93.98% vs 93.69% baseline  
- BERT-base on MNLI: 768→64 projection retains 83.64% vs 83.74% baseline
- Random projections achieve parity with PCA and learned projections at all compression levels tested

## Why This Works (Mechanism)

### Mechanism 1
Random projections preserve task-relevant geometry because the solution manifold is intrinsically low-dimensional and flat. The discriminative signal dominates noise, so linear projections don't need to "find" structure, only preserve distances. If class manifolds were highly curved or entangled, random linear projections would cause overlap and performance collapse—this does not occur, validating flatness.

### Mechanism 2
High-dimensional width serves optimization (search), not representation (solution). Over-parameterization smooths the loss landscape, making gradient descent tractable. Small networks lack this scaffolding—their landscapes remain chaotic despite having representational capacity to hold solutions. The final solution geometry is decoupled from the optimization trajectory that discovered it.

### Mechanism 3
Heavy-tailed spectral structure makes precise eigen-alignment redundant. Well-trained weight matrices concentrate information in few dominant singular values; most dimensions are noise bulk. Since signal dominates noise, even "blind" random projections preserve separability—PCA-level precision is unnecessary.

## Foundational Learning

- **Johnson-Lindenstrauss Lemma**: Core theoretical guarantee that random projections preserve pairwise distances for N points in O(ε⁻² log N) dimensions. Enables the paper's claim that geometric structure survives random contraction.
- **Neural Collapse**: Explains why final-layer representations naturally contract to low-dimensional structures—class means converge to simplex vertices, implying separability requires only C-dimensional subspace for C classes.
- **Heavy-Tailed Self-Regularization**: Explains the spectral sparsity that makes compression possible—most singular values are noise-like bulk, few carry signal. Justifies why SVD/LoRA work and why random projections suffice.

## Architecture Onboarding

- **Component map**: Teacher backbone f_θ → Fixed random projection Φ → Linear classifier g_W
- **Critical path**: 1) Fine-tune backbone on target task, 2) Freeze backbone weights, 3) Generate random projection matrix R with fixed seed, 4) Train linear head on projected features
- **Design tradeoffs**: Lower k → more compression but risk of information loss; Higher LR for linear head vs lower LR for full fine-tuning; Fixed seed ensures reproducibility
- **Failure signatures**: >2% accuracy drop suggests backbone wasn't properly fine-tuned or projection is misconfigured; Learned projections underperforming JL suggests overfitting; Class collapse in t-SNE indicates projection destroyed separability
- **First 3 experiments**: 1) Baseline: Fine-tune backbone, freeze, train linear head on full d-dimensional features, 2) Compression sweep: Project to k ∈ {d/2, d/4, d/8, d/16}, train linear head each time, 3) Projection ablation: Compare JL vs PCA vs Learned at same k values

## Open Questions the Paper Calls Out

1. Can "Subspace-Native Distillation" successfully train student models to match teacher accuracy by targeting the low-dimensional manifold directly, bypassing the high-dimensional optimization bottleneck?

2. Does the "optimization scaffolding" hypothesis apply to intermediate layers, or must hidden states remain high-dimensional to facilitate gradient flow and feature disentanglement?

3. Is the robustness to random geometric contraction preserved in tasks with massive output vocabularies (e.g., LLMs) or continuous output spaces (e.g., generative models)?

## Limitations
- Minimal direct evidence that Johnson-Lindenstrauss distance preservation specifically explains the results
- Claims about optimization serving as search rather than solution representation are largely theoretical
- Heavy-tailed spectral density enabling compression is cited from external work but not directly validated

## Confidence

- **High confidence**: Empirical demonstration that random projections achieve parity with PCA/learned methods
- **Medium confidence**: General mechanism that solution manifolds are low-dimensional and flat
- **Low confidence**: Claims about optimization serving as search rather than solution representation, and heavy-tailed spectral structure being the mechanism for JL success

## Next Checks

1. Directly measure pairwise distance distortion between projected and original features to confirm Johnson-Lindenstrauss guarantees hold empirically for these models

2. Train ResNet-18 on CIFAR-100 from scratch to test whether optimization difficulty in small networks validates the "search vs solution" hypothesis

3. Compute and visualize singular value distributions of trained weight matrices to verify heavy-tailed structure and assess whether JL projections preserve the signal-to-noise ratio