---
ver: rpa2
title: 'Towards Personalized Deep Research: Benchmarks and Evaluations'
arxiv_id: '2509.25106'
source_url: https://arxiv.org/abs/2509.25106
tags:
- task
- research
- user
- deep
- persona
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first benchmark for evaluating personalization
  in deep research agents (DRAs). The Personalized Deep Research Bench (PDR-Bench)
  pairs 50 diverse research tasks across 10 domains with 25 authentic user profiles,
  yielding 250 realistic user-task queries.
---

# Towards Personalized Deep Research: Benchmarks and Evaluations

## Quick Facts
- arXiv ID: 2509.25106
- Source URL: https://arxiv.org/abs/2509.25106
- Reference count: 40
- Primary result: First benchmark for evaluating personalization in Deep Research Agents (DRAs)

## Executive Summary
This paper introduces the Personalized Deep Research Bench (PDR-Bench), the first benchmark for evaluating personalization in Deep Research Agents. The benchmark pairs 50 diverse research tasks across 10 domains with 25 authentic user profiles, yielding 250 realistic user-task queries. To assess performance, the authors introduce the PQR Evaluation Framework that jointly measures Personalization Alignment, Content Quality, and Factual Reliability. Experiments reveal that while open-source agents excel in personalization, commercial systems provide better reliability and balanced quality.

## Method Summary
The PDR-Bench consists of 50 tasks across 10 domains paired with 25 user profiles to create 250 user-task queries. The PQR Evaluation Framework uses LLM-based judges to assess three orthogonal dimensions: Personalization Alignment (P-Score) using dynamic dimension weighting and sub-criterion generation, Content Quality (Q-Score) with hierarchical weighted averages, and Factual Reliability (R-Score) combining claim extraction, citation verification, and two metrics (FA, CC). GPT-5 serves as the primary judge for P and Q metrics, with GPT-5-Mini used for the R metric to balance cost and quality.

## Key Results
- Open-source agents achieve stronger personalization (OAgents P-Score: 6.78) compared to commercial systems (Gemini-2.5-Pro: 4.70)
- Commercial systems provide better factual reliability and citation coverage despite lower personalization scores
- Explicit user personas significantly outperform context-only settings, with P-Scores improving by 10-20% when personas are provided
- Memory systems can partially bridge the gap between implicit context and explicit personas but still underperform direct persona provision

## Why This Works (Mechanism)

### Mechanism 1
The PQR evaluation framework decomposes personalized research quality into three orthogonal dimensions that can be measured independently via LLM-based judges. Personalization Alignment (P-Score) uses dynamic dimension weighting + sub-criterion generation per task-persona pair, then weighted aggregation. Content Quality (Q-Score) follows a similar hierarchical weighted average. Factual Reliability (R-Score) combines claim extraction, citation verification, and two metrics (FA, CC) averaged. The core assumption is that LLM judges can produce weights and sub-criteria that meaningfully reflect user- and task-specific priorities, and claim-level verification via retrieved sources approximates human fact-checking.

### Mechanism 2
Providing richer, more structured user information (task → context → explicit persona) causally improves personalization scores across all tested systems. Experiments under three settings show monotonic P-Score increases: OAgents (6.17→6.53→6.78), O3 (5.13→5.48→5.46), Gemini-2.5-Pro w/Search (3.96→4.55→4.70). Explicit personas provide a structured, accessible signal that agents can directly leverage; implicit context requires extraction/inference, which introduces noise and information loss. The core assumption is that measured P-Score reflects genuine alignment with user needs rather than evaluator bias toward longer outputs.

### Mechanism 3
Memory systems can partially bridge the gap between implicit context and explicit personas by extracting and structuring user information, but current implementations underperform compared to directly provided personas. Memory systems (Mem0, Memory OS, O-Mem) process conversational/interaction context to generate user profiles. O-Mem outperforms baselines (P-Score 4.26 vs. 3.69 No Memory) but remains below Task w/Persona (4.58). The gap indicates difficulty in synthesizing high-level user models from raw context. The core assumption is that evaluated memory systems are representative of current capability.

## Foundational Learning

- **Concept: Deep Research Agents (DRAs)**
  - Why needed here: DRAs are the target system class; understanding their workflow (retrieval, reasoning, synthesis) is prerequisite to evaluating personalization.
  - Quick check question: Can you name three capabilities that distinguish DRAs from standard LLMs with search?

- **Concept: Personalization Dimensions (Goal, Content, Presentation, Actionability)**
  - Why needed here: These dimensions operationalize personalization; evaluators and developers must internalize their definitions to design criteria and interpret scores.
  - Quick check question: For a travel-planning task for a budget-conscious student, which dimension would you weight highest and why?

- **Concept: LLM-as-Judge Evaluation with Dynamic Weighting**
  - Why needed here: PQR relies on LLM-generated weights and sub-criteria; understanding this paradigm is critical for reproducibility and improving evaluation.
  - Quick check question: What could go wrong if the judge LLM is biased toward verbose outputs when assigning weights?

## Architecture Onboarding

- **Component map**: PDR-Bench (50 tasks × 25 profiles) → DRA generates report → PQR pipeline (P-Score dynamic weights → sub-criteria → scoring, Q-Score similar, R-Score claim extraction → verification → FA/CC) → GPT-5 judges → S_overall aggregation

- **Critical path**: 1. Task + Persona input → 2. DRA generates report → 3. Judge generates weights & sub-criteria per dimension → 4. Scoring against criteria → 5. R-Score claim verification → 6. Aggregate S_overall

- **Design tradeoffs**: Explicit personas vs. inferred from context (higher P-Score but requires structured data); open-source DRAs (better personalization) vs. commercial (better reliability, citation coverage); judge model size (larger models improve human alignment but increase cost)

- **Failure signatures**: Low P-Score with high Q-Score (report well-written but not user-tailored); high FA but low CC (citations accurate but many claims lack sources); context-only settings underperforming Task Only (noisy or irrelevant context)

- **First 3 experiments**: 1. Run baseline evaluation on your DRA using the 150-query subset with Task w/Persona setting; report P/Q/R sub-scores. 2. Ablate personalization signal: compare Task Only vs. Task w/Context vs. Task w/Persona to quantify information value. 3. Integrate a memory system (e.g., O-Mem) and measure reduction in gap to Task w/Persona performance.

## Open Questions the Paper Calls Out

### Open Question 1
How can memory systems be designed to transform unstructured conversational context into explicit, actionable personas that match the performance of directly provided user profiles? This gap highlights the need for future research on memory systems that combine factual retrieval with higher-level reasoning and abstraction, moving beyond storage toward constructing dynamic, persona-like models of users. Evidence would be a memory architecture achieving P-Score within 10% of Task w/Persona baseline on PDR-Bench.

### Open Question 2
Can the trade-off between personalization alignment and factual reliability be resolved, or is it inherent to current DRA architectures? Open-source agents achieve the strongest personalization while commercial systems achieve slightly lower personalization but higher reliability. Evidence would be a DRA architecture achieving P-Score >6.5 and Factual Accuracy >8.0 simultaneously on PDR-Bench.

### Open Question 3
How do personalization capabilities and evaluation metrics transfer across languages and cultural contexts? The collection of user personas and context annotations was conducted in Chinese, making the underlying content linguistically and culturally constrained. Evidence would be comparative evaluation showing consistent P-Score rankings across Chinese, English, and at least one additional language/cultural context.

### Open Question 4
What mechanisms enable agents to better extract user preferences from implicit, unstructured context data versus explicit persona descriptions? Agents struggle to fully extract user preferences from unstructured, implicit data. Explicit personas provide a stronger and more accessible personalization signal. Evidence would be ablation studies identifying which context modalities contribute most to personalization, combined with architectures that weight them appropriately.

## Limitations

- The benchmark relies on GPT-5 and GPT-5-Mini for evaluation, which were not publicly available at the time of writing, creating significant reproducibility challenges
- The full benchmark dataset (250 queries across 25 personas) was not released due to file size constraints, limiting independent validation
- The human alignment validation showing GPT-5's 0.43 correlation with human judgment cannot be independently verified without access to the same judge model

## Confidence

- **High Confidence**: The multi-dimensional PQR framework design and its decomposition into orthogonal evaluation criteria is methodologically sound and well-supported by existing literature
- **Medium Confidence**: The empirical findings about open-source vs. commercial DRAs' relative strengths are robust within the tested sample, though generalizability remains uncertain
- **Low Confidence**: The specific numerical thresholds and performance gaps depend heavily on the unavailable judge models and full dataset, making exact replication uncertain

## Next Checks

1. Implement the PQR evaluation framework using accessible judge models (GPT-4o/Claude-3.5-Sonnet) on a smaller sample of 10-15 task-persona pairs to verify the framework's sensitivity to personalization signals
2. Conduct ablation studies on the weight generation mechanism by comparing static vs. dynamic weighting approaches across different judge model capabilities to assess robustness
3. Test the claim verification pipeline (R-Score) on a controlled set of verifiable vs. unverifiable claims to measure false positive/negative rates and identify failure modes in the Jina Reader API integration