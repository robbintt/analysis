---
ver: rpa2
title: 'KNARsack: Teaching Neural Algorithmic Reasoners to Solve Pseudo-Polynomial
  Problems'
arxiv_id: '2509.15239'
source_url: https://arxiv.org/abs/2509.15239
tags:
- neural
- capacity
- item
- table
- algorithmic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces KNARsack, a neural algorithmic reasoning
  (NAR) approach for solving the Knapsack problem, a pseudo-polynomial optimization
  problem omitted from standard NAR benchmarks. The authors develop a two-phase pipeline
  that first constructs the dynamic programming (DP) table and then reconstructs the
  solution, achieving better generalization to larger problem instances than direct-prediction
  baselines.
---

# KNARsack: Teaching Neural Algorithmic Reasoners to Solve Pseudo-Polynomial Problems

## Quick Facts
- arXiv ID: 2509.15239
- Source URL: https://arxiv.org/abs/2509.15239
- Reference count: 40
- Introduces a neural algorithmic reasoning approach for solving the Knapsack problem with strong out-of-distribution performance

## Executive Summary
This paper introduces KNARsack, a neural algorithmic reasoning (NAR) approach for solving the Knapsack problem, a pseudo-polynomial optimization problem omitted from standard NAR benchmarks. The authors develop a two-phase pipeline that first constructs the dynamic programming (DP) table and then reconstructs the solution, achieving better generalization to larger problem instances than direct-prediction baselines. Key innovations include edge length encoding, which enables the model to identify correct past states influencing the current one, and a homogeneous processor that enforces scale invariance with respect to item values.

## Method Summary
The KNARsack approach employs a two-phase pipeline: a DP table constructor that builds the dynamic programming table row by row, and a solution reconstructor that traces back through the table to determine which items to include. The DP table constructor uses an edge length encoding mechanism where the MLP predicts how many steps back to look in the table, combined with a homogeneous processor that normalizes item values for scale invariance. During training, the model is supervised on intermediate DP table values and the final solution, enabling it to learn the underlying algorithmic structure rather than just memorizing solutions.

## Key Results
- Achieves micro-F1 scores of 0.917 and exact-match accuracy of 0.495 on instances with 16 items and capacity 64
- Outperforms no-hint baseline (micro-F1 0.785, exact-match 0.438) by significant margins
- DP tables produced by homogeneous processor show significantly better adherence to fundamental properties like item-wise monotonicity and optimal substructure

## Why This Works (Mechanism)
The approach succeeds by explicitly modeling the dynamic programming process rather than attempting to directly predict the final solution. By constructing the DP table step-by-step, the model learns to reason about optimal substructure - the principle that optimal solutions contain optimal sub-solutions. The edge length encoding allows the model to identify which previous states influence the current state, mimicking the recursive nature of dynamic programming. The homogeneous processor normalizes values to ensure the model learns invariant relationships rather than memorizing specific scales.

## Foundational Learning
- Dynamic Programming: Required for understanding optimal substructure and how to build solutions incrementally through overlapping subproblems. Quick check: Can explain why storing intermediate results is more efficient than recomputing.
- Graph Neural Networks: Needed to understand how the processor propagates information across the DP table. Quick check: Can describe message passing between table cells.
- Attention Mechanisms: Essential for understanding how the model selects relevant past states. Quick check: Can explain how edge length encoding relates to attention over time steps.

## Architecture Onboarding

**Component Map:** Input items -> Edge Length Encoder -> Homogeneous Processor -> DP Table -> Solution Reconstructor

**Critical Path:** The DP table construction phase is the critical path, as errors propagate forward and affect the final solution quality.

**Design Tradeoffs:** The quadratic complexity of DP table construction versus direct prediction creates a fundamental tradeoff between computational cost and generalization capability. The homogeneous processor adds normalization overhead but enables better scale invariance.

**Failure Signatures:** Poor edge length predictions lead to incorrect state selection, causing the DP table to deviate from optimal substructure. Value normalization that's too aggressive can lose important information about relative item values.

**First 3 Experiments:**
1. Compare edge length encoding against direct attention over all previous states
2. Test homogeneous processor against standard MLP with and without normalization
3. Evaluate performance degradation as problem size scales beyond training distribution

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions beyond the immediate scope of the work.

## Limitations
- Computational complexity grows quadratically with capacity and linearly with items, potentially limiting scalability
- Evaluation focuses on synthetic uniform distributions, with unclear generalization to real-world skewed distributions
- Strong performance on out-of-distribution testing within bounds doesn't establish guarantees for arbitrary scaling factors

## Confidence
- **High confidence**: The two-phase pipeline architecture and edge length encoding mechanisms are well-validated through ablation studies and comparison with baselines
- **Medium confidence**: The claim that this approach generalizes better than direct-prediction methods to larger instances, though supported by experiments, needs validation on problems significantly larger than the training distribution
- **Medium confidence**: The assertion that the homogeneous processor enforces meaningful scale invariance, as this depends on the specific normalization scheme and problem characteristics

## Next Checks
1. Test scalability on Knapsack instances with 32+ items and capacity 128+ to verify the quadratic complexity doesn't become prohibitive and that performance remains robust
2. Evaluate generalization to non-uniform distributions, including correlated value-weight pairs and skewed distributions common in real-world scenarios
3. Apply the edge length encoding and homogeneous processor concepts to other dynamic programming problems (e.g., sequence alignment, coin change) to assess transferability of the approach