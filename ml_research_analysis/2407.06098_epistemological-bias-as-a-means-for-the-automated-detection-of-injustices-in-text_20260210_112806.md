---
ver: rpa2
title: Epistemological Bias As a Means for the Automated Detection of Injustices in
  Text
arxiv_id: '2407.06098'
source_url: https://arxiv.org/abs/2407.06098
tags:
- word
- meghan
- kate
- bias
- stereotype
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors propose a framework that uses NLP models to detect
  implicit injustices in text by leveraging epistemic bias detection, stereotype generation,
  and semantic analysis. They fine-tune a BERT-based tagger to identify biased words,
  use CO-STAR and Social Bias Frames models to generate associated stereotypes and
  concepts, and provide explainability through lexicon lookups.
---

# Epistemological Bias As a Means for the Automated Detection of Injustices in Text

## Quick Facts
- **arXiv ID:** 2407.06098
- **Source URL:** https://arxiv.org/abs/2407.06098
- **Reference count:** 40
- **Primary result:** Framework uses BERT-based tagger, stereotype generation, and semantic analysis to detect implicit injustices in text, validated by human survey and comparative headline analysis.

## Executive Summary
This paper presents a framework for detecting implicit testimonial, character, and framing injustices in text by identifying and analyzing epistemic biases. The system leverages a BERT-based sequence tagger to detect potentially biased words, integrates lexicon lookups for bias categorization, and employs CO-STAR and Social Bias Frames models to generate associated stereotypes and concepts. The approach provides explainability by mapping detected biases to human-understandable categories and ranking them by semantic similarity. Validation on news headlines about Meghan Markle and Kate Middleton shows moderate alignment with human judgment on bias detection and sentiment.

## Method Summary
The framework detects epistemic biases as indicators of injustice by combining NLP models and semantic analysis. A BERT-based sequence tagger is fine-tuned on the Wikipedia Neutrality Corpus to identify biased words, incorporating expert features (POS tags, assertive verbs) and a TMI feature (flagging sentences with >2 adjectives/adverbs). Detected words are categorized via lexicon lookup (compiled from social science research) and further analyzed using CO-STAR and Social Bias Frames models to generate stereotypes and concepts. These are ranked by semantic similarity (>0.3 threshold) to highlight potential injustices. The system provides explainability by mapping biases to categories like framing, character, or testimonial. Validation includes human surveys and qualitative analysis of headline comparisons.

## Key Results
- BERT-based tagger achieves ~74-75% accuracy on bias detection task.
- Human survey shows 52% agreement on word-level bias choices and 66% on sentiment judgments.
- Framework identifies and ranks potential injustices in headlines, with qualitative results aligning with human perceptions in many cases.

## Why This Works (Mechanism)
The framework works by detecting epistemic biases—words or phrases that reveal the author's stance—as proxies for injustice. These biases often manifest implicitly through word choice, framing, or testimonial implications. By fine-tuning a BERT-based tagger on labeled bias data, the system learns to identify such words. Lexicon lookups categorize the bias type, while stereotype generation models (CO-STAR, Social Bias Frames) provide context by generating associated concepts. Semantic similarity ranking prioritizes the most relevant injustices, making the system's reasoning transparent and actionable for journalists.

## Foundational Learning
- **Epistemic bias detection:** Identifying words/phrases that reveal author stance or bias. *Why needed:* Core input for injustice detection. *Quick check:* Verify BERT tagger correctly flags known biased terms.
- **Expert feature engineering:** Combining POS tags, assertive verbs, and TMI feature with BERT embeddings. *Why needed:* Enhances model's ability to detect subtle biases. *Quick check:* Ensure feature vector dimensions match BERT output for concatenation.
- **Lexicon-based categorization:** Mapping detected words to bias types (framing, character, testimonial, regular). *Why needed:* Provides interpretable bias labels. *Quick check:* Test lexicon lookup on sample biased words; check categorization accuracy.
- **Stereotype generation:** Using CO-STAR/SBF models to generate context around detected biases. *Why needed:* Expands understanding of bias implications. *Quick check:* Verify model outputs relevant stereotypes for input biases.
- **Semantic similarity ranking:** Ranking generated stereotypes by similarity to input text. *Why needed:* Prioritizes most relevant injustices. *Quick check:* Ensure threshold (>0.3) effectively filters meaningful stereotypes.

## Architecture Onboarding

**Component Map:** BERT tagger -> Lexicon Lookup -> CO-STAR/SBF Models -> Semantic Ranking -> UI Output

**Critical Path:** The pipeline's core flow is BERT-based bias detection → lexicon categorization → stereotype generation → semantic ranking. Early stopping at epoch 2 is critical to avoid overfitting.

**Design Tradeoffs:** Uses expert features alongside BERT to capture linguistic nuances, but this increases model complexity. Relies on external lexicons and models (CO-STAR/SBF), which may vary in availability and quality.

**Failure Signatures:** Overfitting after epoch 2 (evaluation loss spikes). Lexicon mismatches leading to high "regular" bias returns. Incorrect stereotype generation due to model/API differences.

**Three First Experiments:**
1. Train BERT tagger on WNC with expert features; monitor loss and save at epoch 2.
2. Test lexicon lookup on tagged words; measure "regular" return rate.
3. Run pipeline on sample headlines; compare outputs to Table 6 for qualitative validation.

## Open Questions the Paper Calls Out
None

## Limitations
- **Reproducibility gaps:** Exact expert features and lexicon sources are not fully specified, making exact replication difficult.
- **Moderate human agreement:** System's judgments align with human perception only 52-66% of the time, especially for word-level bias choices.
- **External dependencies:** Relies on unspecified versions of CO-STAR and Social Bias Frames models, affecting consistency.

## Confidence

**High Confidence:**
- BERT-based tagger architecture and training regimen (early stopping at epoch 2) are clearly described and reproducible.
- Overall pipeline flow (tagger → lexicon → stereotype generation → ranking) is methodologically sound.

**Medium Confidence:**
- Framework's ability to detect implicit injustices is validated by qualitative analysis and moderate human agreement.
- Performance metrics (~74-75% accuracy, 52-66% human alignment) are reported but may vary with implementation differences.

**Low Confidence:**
- Exact mechanism for stereotype ranking (0.3 threshold) and specific lexicon entries used are not fully transparent.
- Integration with external models (CO-STAR/SBF) depends on unspecified API versions.

## Next Checks
1. **Feature Extraction Verification:** Reconstruct expert features (assertive verbs, additional cues) from cited papers. Validate BERT tagger's feature vector dimensions and content.
2. **Lexicon Reconstruction and Testing:** Identify and implement bias lexicons from Pryzant et al. [16]. Test lexicon lookup on tagged words; measure categorization accuracy and "regular" return rate.
3. **Pipeline End-to-End Validation:** Run complete pipeline on Meghan Markle vs. Kate Middleton headlines. Compare detected biases and ranked stereotypes to Table 6 outputs.