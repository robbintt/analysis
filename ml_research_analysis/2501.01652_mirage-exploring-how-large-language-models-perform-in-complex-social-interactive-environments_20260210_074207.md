---
ver: rpa2
title: 'MIRAGE: Exploring How Large Language Models Perform in Complex Social Interactive
  Environments'
arxiv_id: '2501.01652'
source_url: https://arxiv.org/abs/2501.01652
tags:
- game
- mystery
- script
- players
- gpt-4
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'MIRAGE is a comprehensive framework designed to evaluate large
  language models'' proficiency in complex social interactions through murder mystery
  games. The framework includes eight unique scripts with diverse themes and styles,
  and employs four evaluation metrics: Trust Inclination Index (TII), Clue Investigation
  Capability (CIC), Interactivity Capability Index (ICI), and Script Compliance Index
  (SCI).'
---

# MIRAGE: Exploring How Large Language Models Perform in Complex Social Interactive Environments

## Quick Facts
- **arXiv ID:** 2501.01652
- **Source URL:** https://arxiv.org/abs/2501.01652
- **Reference count:** 40
- **Primary result:** LLMs struggle with trust dynamics, clue investigation, and role-playing in complex murder mystery games

## Executive Summary
MIRAGE is a comprehensive framework designed to evaluate large language models' proficiency in complex social interactions through murder mystery games. The framework includes eight unique scripts with diverse themes and styles, and employs four evaluation metrics: Trust Inclination Index (TII), Clue Investigation Capability (CIC), Interactivity Capability Index (ICI), and Script Compliance Index (SCI). Experimental results show that even state-of-the-art models like GPT-4 struggle with the complexities of MIRAGE, demonstrating significant challenges in navigating trust dynamics, clue investigation, and role-playing. The framework reveals that most LLMs tend to trust other characters excessively and have difficulty identifying critical information necessary for solving mysteries.

## Method Summary
The MIRAGE framework simulates murder mystery games through a three-phase loop: Open Conversation, Interaction, and Voting, running for five iterations. Agents interact using scripted prompts while the system tracks their trust dynamics, clue discovery, and adherence to character roles. The framework employs a neutral LLM (GPT-4-Turbo) as judge to evaluate interativity and script compliance. Key components include context summarization to manage long scripts, suspicion and trust modules to track social dynamics, and a rerun mechanism for output parsing failures.

## Key Results
- GPT-4 and other state-of-the-art models show significant struggles with MIRAGE's complex social interactions
- Most LLMs demonstrate excessive trust in other characters even when deception is involved
- Models show difficulty identifying key clues versus general environmental exploration
- Performance varies significantly across different script complexities and model architectures

## Why This Works (Mechanism)

### Mechanism 1: Narrative-Driven Context Stress Testing
Complex, open-ended narrative structures expose limitations in LLMs that rigid decision trees do not. By requiring agents to navigate "intricately crafted scripts" with diverse themes, the system forces the model to maintain long-horizon context and perform social reasoning without the safety net of limited action spaces.

### Mechanism 2: The Trust-Suspicion Decoupling
The framework quantifies a specific failure mode where models optimize for social agreeableness over logical rigor. Models generate high volumes of interaction while maintaining excessive trust in potentially adversarial agents, effectively masking their inability to identify the culprit.

### Mechanism 3: Salience Detection vs. Action Generation
LLMs demonstrate a disconnect between the capacity to investigate and the competence to identify critical information. The CIC metric tracks general clue discovery, while the distinction between "Clues" and "Key Clues" isolates the model's ability to prioritize high-value information.

## Foundational Learning

- **Concept: Theory of Mind (ToM) in Agents**
  - Why needed here: The MIRAGE environment is fundamentally a game of deception and hidden states. An agent cannot succeed without modeling the beliefs and intentions of other characters.
  - Quick check question: Can you explain why an agent might deliberately reveal a true clue to lower suspicion, and how a model might fail to model this second-order intentionality?

- **Concept: Context Window & "Lost in the Middle" Phenomenon**
  - Why needed here: The paper notes that scripts can exceed 60k words. The "Summarization Module" is critical because models often neglect information in the middle of long contexts.
  - Quick check question: If an agent investigates a clue early in the game, how does the system ensure that clue is still "active" in the model's context window during the final voting phase?

- **Concept: Evaluation Bias in LLM-as-a-Judge**
  - Why needed here: The ICI and SCI metrics rely on a "neutral LLM" (GPT-4-Turbo) to score performance. Understanding the bias of the judge model is essential for interpreting the results.
  - Quick check question: Why does the paper choose GPT-4-Turbo over GPT-4o for evaluation, and what risk does using a single model family as the "judge" introduce to the leaderboard?

## Architecture Onboarding

- **Component map:** [Open Conversation] -> [Suspicion/Trust Module Updates] -> [Interaction (Ask/Investigate)] -> [Memory Summarization] -> [Final Voting]
- **Critical path:** The **Summarization Module** is the bottleneck. This module must compress context without losing critical details required for the "Interaction with Environment" phase.
- **Design tradeoffs:** 
  - Summarization Granularity vs. Cost: Aggressive summarization saves cost but risks the "Lost in the Middle" issue where key clues are dropped.
  - Open vs. Closed Source Evaluation: Using GPT-4-Turbo for scoring provides consistency but introduces a potential "self-preference" bias.
- **Failure signatures:**
  - The "Reticence" Failure: GPT-3.5 produces significantly fewer "User Tokens" (speech) than GPT-4o, failing to engage in the social dynamic.
  - The "Over-Trust" Failure: A model maintaining a high TII (>70) even when another character has been explicitly exposed as a culprit.
  - Instruction Refusal: Safety filters causing the model to refuse to role-play a "culprit" character involved in a murder.
- **First 3 experiments:**
  1. Calibration Run: Run a "Single & Orthodox" script to baseline token costs and verify the "Summarization Module" does not truncate the final voting prompt.
  2. Trust Ablation: Force a "Self-Exposure" event to test if your specific model version updates its TII score or suffers from the reported "excessive trust" bias.
  3. Key Clue Retrieval: Run a "Multi" stage script and track the specific "Key Clues" found per round to visualize the "bumpy rise" curve.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can specific training interventions or prompt engineering mitigate the "excessive trust" bias (high Trust Inclination Index) LLMs exhibit in deceptive social environments?
- Basis in paper: [explicit] The paper explicitly states that "most LLMs tend to trust other characters excessively" and maintain trust even when characters are forced to self-disclose criminal identities.
- Why unresolved: The paper identifies the high trust inclination as a significant failure mode but does not propose or test methods to correct this behavioral bias.

### Open Question 2
- Question: How does context compression via summarization specifically alter LLM decision-making trajectories in long-horizon social simulations?
- Basis in paper: [explicit] The Limitations section notes that context summarization was necessary due to token limits, and "such summarization can impact the decision-making to a certain extent."
- Why unresolved: The paper acknowledges the constraint but does not quantify the trade-off between context retention and cost/efficiency, or how information loss affects strategic reasoning.

### Open Question 3
- Question: What architectural or prompting mechanisms are required to improve the early identification of "Key Clues" rather than general environmental exploration?
- Basis in paper: [explicit] The analysis notes that while general CIC rises steeply, the CIC for "Key Clues" has a "bumpy rise," indicating LLMs "struggle to identify critical information... at an earlier stage."
- Why unresolved: The results show a discrepancy between the enthusiasm for exploration and the efficacy of finding plot-critical information, but the paper offers no solution for prioritizing critical evidence.

## Limitations
- Reliance on LLM-as-a-judge introduces potential scoring bias
- "Key Clue" definitions across scripts may contain subjectivity
- Specific algorithmic triggers for the Summarization Module remain underspecified
- Context window overflow risks losing critical information

## Confidence
- **High Confidence:** LLMs struggle with complex social interactions (supported by TII/CIC/ICI/SCI metric breakdown)
- **Medium Confidence:** Models exhibit "excessive trust" behavior (supported by data but may be influenced by prompt phrasing)
- **Medium Confidence:** Script complexity correlates with performance degradation (plausible but needs controlled ablation studies)

## Next Checks
1. Run the same scripts with multiple judge models (including Claude-3, Gemini-1.5) to quantify scoring variance and identify potential bias in the GPT-4-Turbo evaluations.
2. Systematically vary the phrasing of Suspicion/Trust prompts to determine whether the observed "over-trust" behavior is a genuine reasoning limitation or a prompt adherence issue.
3. Create synthetic scripts with controlled complexity (varying length but identical clue distribution) to isolate whether performance degradation is due to context window limitations or genuine reasoning challenges.