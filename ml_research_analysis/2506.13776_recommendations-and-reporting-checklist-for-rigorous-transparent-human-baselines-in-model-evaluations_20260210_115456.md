---
ver: rpa2
title: Recommendations and Reporting Checklist for Rigorous & Transparent Human Baselines
  in Model Evaluations
arxiv_id: '2506.13776'
source_url: https://arxiv.org/abs/2506.13776
tags:
- human
- baselines
- baseline
- https
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper highlights the need for more rigorous and transparent
  human baselines in foundation model evaluations. Existing baselines often lack methodological
  rigor and transparency, hindering reliable comparisons between human and AI performance.
---

# Recommendations and Reporting Checklist for Rigorous & Transparent Human Baselines in Model Evaluations

## Quick Facts
- arXiv ID: 2506.13776
- Source URL: https://arxiv.org/abs/2506.13776
- Reference count: 40
- Existing human baselines lack methodological rigor and transparency, hindering reliable AI-human comparisons

## Executive Summary
This paper addresses critical gaps in how human baselines are conducted and reported in AI evaluations. Through a systematic review of 115 human baselines, the authors found pervasive methodological weaknesses including small sample sizes (median N=8), inconsistent test sets, and inadequate documentation. To address these issues, they provide evidence-based recommendations and a comprehensive checklist grounded in measurement theory. The framework covers the full human baseline lifecycle from design through documentation, emphasizing test set equivalence, population sampling, and uncertainty quantification. By standardizing these practices, the authors aim to enable more rigorous, interpretable, and trustworthy comparisons between human and AI performance.

## Method Summary
The authors conducted a systematic review of 115 human baselines from 65 papers published between 2020-2024, extracting data on design choices, recruitment methods, execution practices, analysis techniques, and documentation quality. They identified common failure modes and developed methodological recommendations organized around measurement theory principles. The recommendations were structured into a lifecycle framework covering five stages: design and implementation, recruitment, execution, analysis, and documentation. The authors then created a detailed reporting checklist (Appendix B) with 27 specific items across these five stages to guide researchers in conducting and reporting rigorous human baselines.

## Key Results
- Median human baseline sample size was only 8 participants, with 1.74% conducting power analyses
- 41.74% of baselines used unspecified or non-representative populations
- Only 28.70% used identical test sets for humans and AI models
- The provided checklist addresses critical gaps in test set equivalence, sampling strategy, method effects, statistical power, and comprehensive documentation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** If test sets and sampling strategies are standardized, the comparison between AI and human performance becomes a valid measure of capability rather than a measure of data contamination or selection bias.
- **Core assumption:** AI models and humans have similar data contamination risks (train/test leakage vs. prior exposure).
- **Evidence anchors:** [Section 4.1] "Robust comparisons of human vs. AI performance require comparing performance on the same test set."
- **Break condition:** If the human baseline is conducted on a subset of the AI test set but AI results are reported on the full set.

### Mechanism 2
- **Claim:** If method effects (instructions, UI, tool access) are controlled, performance differences can be attributed to cognitive capability rather than interface friction.
- **Core assumption:** It is possible to design an interface that is equally "native" or optimal for both human cognition and current AI processing.
- **Evidence anchors:** [Section 4.3] "Method effects are variations in item response attributable to data collection methods... use the same tasks for both human and AI responses."
- **Break condition:** If the task relies on sensory modalities humans possess but models lack.

### Mechanism 3
- **Claim:** If statistical uncertainty is quantified (e.g., via power analysis), "super-human" claims become statistically significant rather than anecdotal.
- **Core assumption:** The distribution of human performance follows standard statistical distributions where standard tests apply.
- **Evidence anchors:** [Section 4.1] "A rule of thumb is that a sample of 1,000 respondents is needed to represent the U.S. adult population."
- **Break condition:** If sample sizes are too small to support standard Central Limit Theorem approximations.

## Foundational Learning

- **Concept: Measurement Theory & Validity**
  - **Why needed here:** The paper explicitly draws on measurement theory (psychometrics) to fix AI evaluation.
  - **Quick check question:** Does this baseline measure the human's ability to reason, or their ability to understand my confusing survey interface?

- **Concept: Selection Bias & Sampling**
  - **Why needed here:** The paper highlights that crowd-workers (MTurk/Prolific) are not representative of the general population.
  - **Quick check question:** If my baseline uses only US-based crowdworkers, am I claiming my AI is "super-human" or just "super-MTurker"?

- **Concept: Statistical Power**
  - **Why needed here:** The median baseline size found was 8.
  - **Quick check question:** With a sample size of 10, what is the margin of error for my "human baseline" estimate?

## Architecture Onboarding

- **Component map:** Design & Implementation -> Recruitment -> Execution -> Analysis -> Documentation
- **Critical path:** The most fragile step is Design (4.1). If you do not define the "Population of Interest" and perform "Power Analysis" before recruiting, the entire downstream baseline is invalid.
- **Design tradeoffs:** The paper frames this primarily as Rigor vs. Cost.
  - *High Rigor:* N=1,000, stratified sampling, lab setting (High cost).
  - *Low Rigor:* N=10, convenience sampling (Low cost).
  - *Guidance:* If cost constrains rigor, the paper explicitly advises narrowing the interpretation of results rather than faking rigor.
- **Failure signatures:**
  - "The N=2 Baseline": Citing "human performance" based on two research assistants.
  - "The Subset Trap": Evaluating AI on a full dataset but humans on a random subset of 10 questions without recalculating AI scores.
  - "The Instruction Mismatch": Giving the AI 5-shot examples but the human none.
- **First 3 experiments:**
  1. **Audit:** Apply the checklist (Appendix B) to the last 3 human baselines you read or wrote. Did they report "Unknown" for key items?
  2. **Power Analysis:** For your next baseline, calculate the required sample size before recruiting. If N=500 is required but you only have budget for N=50, explicitly document this limitation.
  3. **Equivalence Test:** Run a pilot where you give humans the exact JSON/Prompt context you give the model. Measure if performance changes.

## Open Questions the Paper Calls Out
None identified in the paper.

## Limitations
- The framework assumes construct validity can be achieved through standardization, but some human cognitive abilities may not be fully capturable through standardized testing paradigms.
- Achieving true population representativeness across diverse global populations remains challenging and costly.
- The recommendations assume normal distributions and standard statistical power calculations apply to human performance distributions.

## Confidence
- **High Confidence:** The need for standardized test sets and sampling strategies
- **Medium Confidence:** The ability to control method effects across human and AI interfaces
- **Medium Confidence:** Statistical power calculations for human baseline claims

## Next Checks
1. **Empirical Validation:** Conduct a controlled experiment comparing human performance when given AI-native vs. human-native interfaces for the same task.
2. **Population Generalization Study:** Apply the checklist framework to human baselines from multiple countries/regions to assess whether recommendations hold across cultural contexts.
3. **Statistical Robustness Test:** Analyze human performance distributions across multiple baselines to verify whether standard statistical assumptions hold for different cognitive task types.