---
ver: rpa2
title: 'IndicGEC: Powerful Models, or a Measurement Mirage?'
arxiv_id: '2511.15260'
source_url: https://arxiv.org/abs/2511.15260
tags:
- task
- languages
- output
- language
- error
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates the effectiveness of large language models
  (LLMs) for grammatical error correction (GEC) in five Indian languages: Hindi, Telugu,
  Tamil, Malayalam, and Bangla. The study evaluates zero-shot and few-shot prompting
  of models ranging from 4B to large proprietary models, focusing on understanding
  their capabilities without task-specific fine-tuning.'
---

# IndicGEC: Powerful Models, or a Measurement Mirage?
## Quick Facts
- arXiv ID: 2511.15260
- Source URL: https://arxiv.org/abs/2511.15260
- Reference count: 7
- Primary result: Small multilingual models achieved near-perfect GLEU scores for Malayalam and Bangla GEC, but qualitative analysis revealed significant data quality issues

## Executive Summary
This paper investigates the effectiveness of large language models (LLMs) for grammatical error correction (GEC) in five Indian languages without task-specific fine-tuning. The study evaluates zero-shot and few-shot prompting approaches using models ranging from 4B to large proprietary models. While the methodology achieved strong rankings in the BHASHA-Task 1 shared task, particularly for Hindi and Telugu, subsequent qualitative analysis uncovered significant data quality issues including incorrect gold outputs and context-dependent corrections. The research highlights both the potential of small multilingual models for Indian language GEC and the critical need for high-quality datasets, better evaluation metrics, and more robust methodologies tailored to Indian languages.

## Method Summary
The study employed zero-shot and few-shot prompting techniques to evaluate LLM performance on GEC tasks across five Indian languages. Models ranging from 4B to large proprietary models were tested without task-specific fine-tuning. The evaluation focused on understanding baseline capabilities and ranking performance in the BHASHA-Task 1 shared task. The approach extended experiments to Hindi, Telugu, Tamil, Malayalam, and Bangla, with particular attention to GLEU score measurement and qualitative analysis of correction outputs.

## Key Results
- Achieved rank 2 in Hindi and rank 4 in Telugu in BHASHA-Task 1 with GLEU scores of 84.31 and 83.78, respectively
- Small multilingual models achieved near-perfect GLEU scores for Malayalam and Bangla GEC tasks
- Qualitative analysis revealed significant data quality issues including incorrect gold outputs, context-dependent corrections, and formatting errors

## Why This Works (Mechanism)
The success of zero-shot and few-shot prompting for Indian language GEC stems from the strong multilingual capabilities of modern LLMs and the transfer learning potential from high-resource to low-resource languages. Small multilingual models can leverage shared linguistic features across Indian languages while maintaining computational efficiency. The few-shot approach provides sufficient context for models to understand task requirements without extensive fine-tuning.

## Foundational Learning
1. **Zero-shot vs Few-shot Learning** - Why needed: Different prompting strategies require different understanding of task context and examples. Quick check: Can the model perform the task with minimal examples?
2. **Grammatical Error Correction (GEC)** - Why needed: Understanding the specific challenges of language correction across different scripts and grammatical structures. Quick check: Does the model correctly identify and fix grammatical errors?
3. **GLEU Metric** - Why needed: Evaluation metric must be appropriate for the language scripts and error types being measured. Quick check: Does the metric accurately reflect correction quality across languages?
4. **Indian Language Scripts** - Why needed: Character encoding and script-specific variations affect model performance and evaluation. Quick check: Can the model handle language-specific orthographic and grammatical features?

## Architecture Onboarding
**Component Map:** LLM Model -> Prompt Template -> Input Text -> Output Text -> GLEU Evaluation
**Critical Path:** Input text → Model → Generated corrections → GLEU scoring
**Design Tradeoffs:** Zero-shot (no examples, faster but potentially less accurate) vs Few-shot (requires examples, potentially more accurate)
**Failure Signatures:** High GLEU scores with incorrect corrections, inconsistent scores for semantically equivalent outputs, model failure on context-dependent corrections
**First Experiments:**
1. Test model performance with varying numbers of few-shot examples to find optimal prompting strategy
2. Compare GLEU scores against human evaluation for a subset of corrections
3. Evaluate model performance on independently-curated datasets for each language

## Open Questions the Paper Calls Out
The paper identifies several open questions regarding the validity of current evaluation metrics for Indian languages, the generalizability of zero-shot/few-shot results to real-world deployment, and the impact of data quality issues on reported performance. The study also questions whether the observed high performance for Malayalam and Bangla reflects true model capabilities or measurement artifacts.

## Limitations
- Data quality issues including incorrect gold outputs and context-dependent corrections may have inflated GLEU scores
- GLEU metric shows clear limitations for Indian scripts, potentially producing inconsistent results for semantically equivalent corrections
- Reliance on zero-shot and few-shot prompting without fine-tuning makes real-world generalizability uncertain
- Absence of human evaluation limits confidence in claimed performance levels

## Confidence
- **High confidence**: Identification of data quality issues and metric limitations is well-supported by qualitative analysis
- **Medium confidence**: Reported performance rankings are likely accurate for specific dataset conditions but may not reflect real-world capabilities
- **Medium confidence**: Conclusion that small multilingual models can achieve competitive results is plausible but requires independent validation

## Next Checks
1. Conduct human evaluation studies comparing model outputs against gold standards to verify if GLEU scores accurately reflect correction quality
2. Test model performance on additional, independently-curated datasets for each language to assess generalizability beyond the BHASHA-Task corpus
3. Implement and evaluate alternative metrics specifically designed for Indian language scripts that account for character variations and semantic equivalence