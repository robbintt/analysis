---
ver: rpa2
title: Multi-Agent Teams Hold Experts Back
arxiv_id: '2602.01011'
source_url: https://arxiv.org/abs/2602.01011
tags:
- expert
- expertise
- teams
- team
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates whether multi-agent LLM teams can achieve\
  \ \"strong synergy\"\u2014matching or exceeding the performance of their best individual\
  \ member\u2014when coordination is unconstrained. Across human-inspired psychology\
  \ tasks and frontier ML benchmarks, teams consistently fail to match expert performance,\
  \ with synergy gaps ranging from 8.1% to 37.6%."
---

# Multi-Agent Teams Hold Experts Back

## Quick Facts
- arXiv ID: 2602.01011
- Source URL: https://arxiv.org/abs/2602.01011
- Reference count: 40
- Multi-agent teams systematically underperform their best individual member, with synergy gaps of 8.1-37.6%

## Executive Summary
This paper reveals a fundamental limitation of multi-agent LLM teams: even when coordination is unconstrained, teams consistently fail to match or exceed their best individual member's performance. Across human psychology ranking tasks and frontier ML benchmarks, teams exhibit "integrative compromise" behavior—averaging expert and non-expert views rather than appropriately weighting expertise. The synergy gap ranges from 8.1% to 37.6%, with larger teams performing increasingly worse. This consensus-seeking behavior, while providing robustness to adversarial agents, prevents teams from leveraging expertise effectively. The findings suggest that current alignment procedures may need to enable contextual expertise utilization without sacrificing robustness.

## Method Summary
The study employs a four-phase deliberation protocol where models first provide individual opinions privately, then engage in open discussion across four rounds with randomized speaking order, and finally produce a consensus answer. Teams of four heterogeneous agents are evaluated across psychology ranking tasks (NASA Moon Survival, Lost at Sea, Student Body President) and ML benchmarks (MMLU Pro, SimpleQA, GPQA Diamond, HLE, MATH-500). Expertise is manipulated through ground-truth information partitioning and explicit "Reveal Expert" prompts. The primary metric is relative synergy gap: (Best Individual Error - Team Error) / Best Individual Error, distinguishing strong synergy (matching best member) from weak synergy (beating average).

## Key Results
- Multi-agent teams consistently underperform their best member, with synergy gaps of 8.1-37.6% across all tasks
- Larger teams perform increasingly worse relative to the expert, even when explicitly told who the expert is
- Consensus-seeking behavior correlates negatively with performance but provides robustness to adversarial agents
- Expertise leveraging is the bottleneck—teams struggle to use expertise even when correctly identified

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-organizing LLM teams systematically underperform their best member due to integrative compromise behavior.
- Mechanism: Non-expert agents treat expert opinion as evidence to integrate rather than authority to defer to, producing middle-ground positions that dilute expert knowledge.
- Core assumption: Assumes RLHF alignment procedures inadvertently train consensus-seeking behavior over expertise recognition.
- Evidence anchors:
  - [abstract] "Conversational analysis reveals a tendency toward integrative compromise—averaging expert and non-expert views rather than appropriately weighting expertise—which increases with team size and correlates negatively with performance."
  - [section 4.5] "When non-experts yield to expert authority (ED), teams perform better (NASA: r=−0.44, p=0.007)... Conversely, integrative compromise (IC)—treating expert opinions as evidence to average—correlates positively with the synergy gap (NASA: r=0.55, p<0.001)."
  - [corpus] Related work on multi-agent coordination (CausalPlan, SwarmSys) focuses on structured planning rather than emergent expertise recognition—limited direct corroboration for the compromise mechanism specifically.
- Break condition: If models are trained with explicit expertise-deference objectives or structured role hierarchies are imposed, the mechanism would likely not manifest.

### Mechanism 2
- Claim: Expertise dilution intensifies with team size even when the expert is explicitly identified.
- Mechanism: Each additional non-expert voice increases "compromise pressure," pulling the consensus toward the average rather than the expert position.
- Core assumption: Assumes deliberation dynamics follow a social combination model where each voice receives roughly equal weight regardless of expertise.
- Evidence anchors:
  - [section 4.3] "Larger teams perform increasingly worse relative to the expert... we find statistically significant positive correlations between team size and synergy gap across all tasks and information conditions (all p<0.05)."
  - [section E.4] "This effect persists even in the Reveal Expert condition where teams are explicitly told who the expert is."
  - [corpus] CodeCRDT and FLEET address coordination scaling through structural mechanisms, not deliberation dynamics—indirect support only.
- Break condition: If explicit voting权重 or formal hierarchies override deliberation, dilution would be structurally prevented rather than emergently occurring.

### Mechanism 3
- Claim: The same consensus-seeking behavior that impairs expertise leveraging provides robustness against adversarial manipulation.
- Mechanism: When adversarial input deviates substantially from the majority position, the compromise-averaging process naturally dilutes its signal.
- Core assumption: Assumes adversaries take positions substantially different from the correct answer (rather than subtle perturbations near the expert position).
- Evidence anchors:
  - [section 4.4] "Teams with one agent instructed to sabotage performance show minimal degradation—the same mechanism preventing teams from leveraging expertise also filters adversarial input."
  - [abstract] "Interestingly, this consensus-seeking behavior improves robustness to adversarial agents, suggesting a trade-off between alignment and effective expertise utilization."
  - [corpus] Related work on adversarial robustness in multi-agent debate (Chern et al., 2024, cited in paper) supports dilution of adversarial signals—corpus papers do not directly address this trade-off.
- Break condition: If adversaries produce plausible-but-wrong positions close to expert reasoning, consensus-seeking may not filter effectively and could instead amplify errors.

## Foundational Learning

- Concept: **Strong vs. Weak Synergy**
  - Why needed here: The paper's core metric distinguishes matching best member (strong synergy) from beating the average (weak synergy). Without this distinction, one might conclude teams "work" when they merely avoid disaster.
  - Quick check question: If a 4-person team averages 70% accuracy individually and achieves 72% as a group, but the best member scored 90%, has strong synergy been achieved?

- Concept: **Epistemic Deference vs. Evidence Integration**
  - Why needed here: The mechanism analysis hinges on whether expert testimony should preempt reasoning (replace it) or supplement reasoning (be weighed alongside other views). This philosophical distinction maps directly to the coding scheme.
  - Quick check question: When an expert says "X is correct," should a non-expert adopt X as their belief, or should they combine their own reasoning with X to form a new position?

- Concept: **Expertise Identification vs. Leveraging Gap**
  - Why needed here: The paper decomposes team failure into two stages: knowing who knows (identification) and using what they know (leveraging). Experiments show leveraging is the bottleneck—fixing identification alone doesn't help.
  - Quick check question: If you tell a team "Agent 3 is the expert" and they still average all opinions equally, is the problem identification or leveraging?

## Architecture Onboarding

- Component map: Individual Opinion Collection -> Discussion Initialization -> Collaborative Discussion -> Final Answer Elicitation
- Critical path:
  1. Define task and ground truth expertise source (e.g., NASA rankings)
  2. Select heterogeneous model team ensuring performance variance
  3. Partition expertise information across agents per experimental condition
  4. Execute 4-phase deliberation protocol with logging
  5. Compute synergy gap: (Team Error − Best Individual Error) / Best Individual Error
  6. Run conversational analysis using Gemini 3.0 Pro with coding rubric (ED/IC/SP/EF)

- Design tradeoffs:
  - **Heterogeneous vs. Homogeneous Teams**: Heterogeneous needed to test genuine expertise differences but introduces confounds; paper tests both
  - **Structured vs. Unstructured Deliberation**: Unstructured reveals emergent behavior but cannot guarantee convergence; paper accepts this as ecologically valid
  - **Reveal Expert Prompts**: Aggressively tuned to maximize deference—creates upper bound on leveraging potential but may not reflect real deployments

- Failure signatures:
  - **Integrative Compromise**: Non-experts proposing "middle ground" rankings explicitly ("I propose a compromise where...")
  - **Expert Flexibility**: Experts modifying positions based on non-expert feedback despite having ground truth
  - **Synergy Gap > 15%**: Team substantially underperforming best individual despite Reveal Expert condition
  - **Positive IC-to-Gap Correlation**: More compromise language correlates with worse outcomes (r > 0.4)

- First 3 experiments:
  1. **Replicate NASA Moon Survival with 4-agent team**: Run all four information conditions (No Info, Expert Not Mentioned, Reveal Expert, Best Individual) with 10 seeds each to establish baseline synergy gap. Expected: 60-80% relative synergy gap in Reveal Expert condition.
  2. **Vary team size (2, 4, 8 agents)**: Test expertise dilution hypothesis by measuring correlation between team size and synergy gap. Expected: Significant positive correlation (r > 0.4, p < 0.05) even in Reveal Expert.
  3. **Adversarial robustness test**: Introduce one agent with worst-possible ranking instructed to sabotage. Compare performance degradation. Expected: Minimal degradation (<5%) due to consensus-seeking dilution.

## Open Questions the Paper Calls Out

None

## Limitations

- Expertise manipulation relies on induced ground-truth partitioning rather than naturally occurring expertise, potentially limiting ecological validity
- "Reveal Expert" prompts were "aggressively tuned" to maximize deference, creating an upper bound scenario that may overestimate real-world achievability
- Study focuses on self-organizing teams without structural mechanisms like voting or hierarchical roles, limiting generalizability to coordinated systems

## Confidence

- **High Confidence**: The core empirical finding that teams underperform their best member across multiple tasks (8.1-37.6% synergy gap) is well-supported with multiple benchmarks and seeds
- **Medium Confidence**: The mechanism explanation (integrative compromise) is supported by conversational analysis but could benefit from alternative coding schemes or experimental controls
- **Medium Confidence**: The expertise-leveraging vs. adversarial-robustness trade-off is demonstrated but only tested with one type of adversarial manipulation

## Next Checks

1. **Real-World Expertise Test**: Replicate with naturally occurring expertise differences (e.g., domain-specific models on their specialized tasks) rather than induced ground-truth partitioning to test ecological validity

2. **Structured Coordination Intervention**: Implement formal voting or hierarchical role assignment to test whether structural mechanisms can prevent integrative compromise while preserving adversarial robustness

3. **Subtle Adversarial Test**: Evaluate team performance against adversarial agents that produce plausible-but-wrong positions near expert reasoning, testing whether consensus-seeking remains effective against more sophisticated attacks