---
ver: rpa2
title: Probability-Flow ODE in Infinite-Dimensional Function Spaces
arxiv_id: '2503.10219'
source_url: https://arxiv.org/abs/2503.10219
tags:
- function
- diffusion
- infinite-dimensional
- equation
- solution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work derives the first probability-flow ODE (PF-ODE) in infinite-dimensional
  Hilbert spaces, extending the concept from finite-dimensional diffusion models.
  The PF-ODE is rigorously formulated using measure-valued Fokker-Planck-Kolmogorov
  equations and functional derivatives, enabling faster sampling without compromising
  quality.
---

# Probability-Flow ODE in Infinite-Dimensional Function Spaces

## Quick Facts
- **arXiv ID**: 2503.10219
- **Source URL**: https://arxiv.org/abs/2503.10219
- **Reference count**: 40
- **Primary result**: Derives the first probability-flow ODE in infinite-dimensional Hilbert spaces, enabling faster sampling for diffusion models without compromising quality.

## Executive Summary
This work establishes the theoretical foundation for probability-flow ODEs (PF-ODEs) in infinite-dimensional Hilbert spaces, a crucial extension for diffusion models operating on function spaces. By replacing probability densities with measure-theoretic constructs like the Fomin derivative and logarithmic gradient, the authors derive a deterministic ODE whose solution preserves the same probability law as the original stochastic process. This enables faster sampling via ODE solvers while maintaining or improving sample quality compared to traditional SDE-based approaches.

## Method Summary
The method extends probability-flow ODEs to infinite-dimensional Hilbert spaces by using measure-theoretic tools instead of probability densities. It employs the Fomin derivative (logarithmic gradient) of the probability measure along the Cameron-Martin space, derived from the Fokker-Planck-Kolmogorov equation. The approach uses a Fourier Neural Operator to learn the score function, with inference performed by solving the derived PF-ODE using deterministic solvers like Euler's method. The method is validated on synthetic 1D functions and 2D PDEs, demonstrating superior efficiency compared to SDE-based sampling.

## Key Results
- ODE sampling with 20 NFE outperforms SDE sampling across all NFEs for 1D function generation
- For PDE tasks, ODE samples show lower sliced Wasserstein distances and Lp-distances to ground truth solutions compared to SDE samples with same NFE
- The deterministic ODE approach achieves comparable or superior generation quality while requiring significantly fewer function evaluations

## Why This Works (Mechanism)

### Mechanism 1: Measure-Theoretic Substitution for Probability Densities
Instead of calculating $\nabla \log p_t(x)$ which requires a density $p_t$, the method utilizes the Fomin derivative (logarithmic gradient $\rho_{\mu_t}^{H_Q}$) of the law $\mu_t$ along the Cameron-Martin space $H_Q$. This allows construction of a deterministic ODE whose solution $Y_t$ preserves the same marginal probability law $\mu_t$ as the original SDE solution $X_t$ at every time step $t$.

### Mechanism 2: Deterministic Smoothing via ODE Solvers
Replacing the stochastic sampling process with a deterministic ODE solver significantly reduces the number of function evaluations required to achieve high sample fidelity. The ODE formulation removes the stochastic diffusion term $G(t)dW_t$ present in the reverse SDE, and empirical results suggest that deterministic solvers converge to the data manifold more efficiently in this setting.

### Mechanism 3: Single-Step Discretization Error Accumulation (Hypothesized)
ODE sampling may outperform SDE sampling in infinite dimensions because it isolates discretization error to the initial noise approximation rather than propagating it through the trajectory. In the SDE setting, the infinite-dimensional noise $\xi \sim N(0, Q)$ must be approximated repeatedly during integration steps, while the ODE method only incurs this approximation error once at the start.

## Foundational Learning

- **Concept: Cameron-Martin Space / Reproducing Kernel Hilbert Space (RKHS)**
  - **Why needed here:** In infinite dimensions, "noise" is not isotropic; it lives in the Cameron-Martin space $H_Q$ associated with the covariance operator $Q$.
  - **Quick check question:** If $Q$ is an integral operator defined by a kernel $k$, can you describe the space of functions that constitute $H_Q$?

- **Concept: Functional (Fréchet/Gâteaux) Derivatives**
  - **Why needed here:** The paper generalizes the score function ($\nabla \log p$) to the logarithmic gradient, which is defined via functional derivatives of test functions on the Hilbert space.
  - **Quick check question:** How does the Fréchet derivative differ from a standard partial derivative when the input variable is a function $u \in L^2(\Omega)$?

- **Concept: Fokker-Planck-Kolmogorov (FPK) Equation**
  - **Why needed here:** The paper derives the ODE by analyzing the time evolution of the probability measure $\mu_t$ using the weak formulation of the FPK equation.
  - **Quick check question:** In finite dimensions, the FPK equation describes the evolution of the density $p_t(x)$. What mathematical object does it evolve in infinite dimensions?

## Architecture Onboarding

- **Component map:** Gaussian Process prior (N(0, Q)) -> Fourier Neural Operator (FNO) -> Score Matching Loss -> Logarithmic Gradient Network $S_\theta$ -> Euler Solver (ODE) or Euler-Maruyama Solver (SDE)

- **Critical path:**
  1. Define Prior: Implement $Q$ (e.g., via FFT for Bessel prior or RBF kernel matrix)
  2. Approximate Gradient: Train $S_\theta$ to minimize the score-matching loss between network output and perturbed data
  3. Inference: Solve PF-ODE: $dY = \frac{\alpha(T-t)}{2}[Y + S_\theta(Y)]dt$ starting from noise

- **Design tradeoffs:**
  - Noise Resolution: You must approximate the infinite-dimensional noise $N(0, Q)$ on a finite grid. A coarse grid loses high-frequency details; a fine grid increases computational cost
  - ODE vs. SDE: ODE is faster (low NFE) but deterministic (less diverse samples for same seed?). SDE is slower but might explore the manifold differently
  - Choice of $Q$: The paper uses Bessel priors for PDEs (inductive bias for smoothness) and RBF for 1D tasks

- **Failure signatures:**
  - Mode Collapse / Blurry Outputs: Likely indicates a mismatch between the chosen covariance operator $Q$ and the data structure
  - Divergence at low NFE: If the ODE solver diverges at NFE < 10, check the step size or the stiffness of the learned vector field

- **First 3 experiments:**
  1. Sanity Check (1D): Train on the synthetic `Quadratic` dataset. Verify that ODE NFE=20 beats SDE NFE=100 using the kernel two-sample test (Power metric)
  2. PDE Consistency (2D): Train on Reaction-Diffusion data. Generate samples at NFE=10 and compute Sliced Wasserstein distance against the test set
  3. Ablation on Discretization: For the Heat Equation, vary the resolution of the initial noise approximation (e.g., 16x16 vs 64x64) to observe the impact of the "single discretization error" hypothesis

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can advanced numerical solvers (e.g., DPM-Solver) or consistency distillation techniques be effectively adapted to the infinite-dimensional PF-ODE framework to further reduce sampling steps?
- **Basis in paper:** The conclusion states, "We leave extending our work to faster sampling (Lu et al., 2022b) and knowledge distillation (Song et al., 2023)) as future work."
- **Why unresolved:** The current work focuses on the rigorous mathematical derivation of the PF-ODE itself and validates it using basic Euler integration, leaving advanced acceleration techniques unexplored.
- **What evidence would resolve it:** Successful implementation of high-order ODE solvers or consistency models on the infinite-dimensional PF-ODE that maintains sample quality with significantly fewer NFEs than the Euler method.

### Open Question 2
- **Question:** What are the theoretical bounds on discretization error for infinite-dimensional diffusion models, and do they mathematically explain the empirical superiority of ODE solvers over SDE solvers?
- **Basis in paper:** The authors identify "a rigorous investigation into the discretization error of infinite-dimensional diffusion models, both SDE and our PF-ODE" as a "fruitful direction."
- **Why unresolved:** While empirical results show ODE outperforming SDE, a formal analysis of the error accumulation in function space remains to be conducted.
- **What evidence would resolve it:** A theorem quantifying the trade-offs in discretization error between the stochastic and deterministic paths in Hilbert spaces, aligning with the observed experimental data.

### Open Question 3
- **Question:** Is the superior performance of the PF-ODE over SDE strictly due to the single discretization error of the initial noise approximation versus repeated errors in the stochastic path?
- **Basis in paper:** The authors hypothesize that the effectiveness is "because the ODE method only incurs a single discretization error from the initial approximation...; in contrast, for the SDE method, repeated discretization error... occurs."
- **Why unresolved:** This is presented as a belief/hypothesis ("we believe") to explain the results rather than a proven property.
- **What evidence would resolve it:** An ablation study or theoretical proof isolating the error contribution of the initial noise approximation versus the iterative integration errors in both methods.

## Limitations
- The theoretical framework relies heavily on the Fomin differentiability assumption and specific structure of Gaussian priors with covariance operators
- The method primarily validates on synthetic 1D functions and relatively simple 2D PDEs, leaving open questions about performance on real-world data
- While the ODE is shown to require fewer function evaluations, the computational cost of evaluating functional derivatives and implementing infinite-dimensional noise sampler is not fully characterized

## Confidence

- **High Confidence**: The theoretical derivation of the PF-ODE in infinite dimensions using measure-theoretic tools (Fokker-Planck-Kolmogorov equation, Fomin derivatives, Cameron-Martin space) is mathematically rigorous and well-supported by the proofs in Section 3.
- **Medium Confidence**: The experimental results demonstrating that ODE sampling achieves comparable or superior quality to SDE with fewer function evaluations are convincing for the specific synthetic datasets and PDEs tested.
- **Low Confidence**: The hypothesized mechanism that ODE sampling outperforms SDE specifically because it avoids repeated discretization of the infinite-dimensional noise is plausible but not definitively proven.

## Next Checks

1. **Ablation on Noise Discretization Resolution**: For the Heat Equation experiment, systematically vary the spatial resolution of the initial noise approximation (e.g., 16x16, 32x32, 64x64 grids) and measure the impact on ODE and SDE sampling quality at fixed NFE.

2. **Real-World PDE Benchmark**: Apply the method to a real-world PDE dataset (e.g., Navier-Stokes flow data or subsurface flow from PDEBench) to evaluate whether the theoretical advantages translate to practical, complex problems beyond synthetic functions.

3. **Diversity Analysis**: Generate multiple samples from both ODE and SDE solvers with the same random seed for a fixed PDE initial condition. Analyze the diversity and mode coverage of the generated solutions to determine if the deterministic nature of the ODE introduces any biases compared to the stochastic SDE.