---
ver: rpa2
title: Large language models are not about natural language
arxiv_id: '2512.13441'
source_url: https://arxiv.org/abs/2512.13441
tags:
- language
- llms
- human
- languages
- impossible
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The commentary argues that Large Language Models (LLMs) are fundamentally
  unsuited for understanding human language because they operate as probabilistic
  models requiring vast amounts of training data, while human language is generated
  through a recursive, hierarchical, mind-internal computational system. Unlike human
  children who can develop language with minimal input and distinguish possible from
  impossible languages, LLMs cannot make this distinction and process language through
  linear pattern detection.
---

# Large language models are not about natural language

## Quick Facts
- arXiv ID: 2512.13441
- Source URL: https://arxiv.org/abs/2512.13441
- Authors: Johan J. Bolhuis; Andrea Moro; Stephen Crain; Sandiway Fong
- Reference count: 0
- Key outcome: LLMs cannot understand human language because they process linear patterns rather than hierarchical structures, failing to distinguish possible from impossible languages.

## Executive Summary
This commentary argues that Large Language Models (LLMs) fundamentally differ from human language processing because they operate as probabilistic models analyzing linear word sequences rather than hierarchical structures. While LLMs require vast training data, human children acquire language with minimal input through innate computational systems. Recent studies show LLMs perform equally well on both normal and impossible languages, failing to capture the cognitive architecture underlying human language acquisition. The authors conclude that despite their capabilities, LLMs cannot inform our understanding of the human language faculty.

## Method Summary
The paper synthesizes findings from cited empirical studies rather than conducting original experiments. Key referenced paradigms include comparing LLM performance on forward vs. backward English text (Luo et al., 2024; Ziv et al., 2025) and testing learning on possible vs. impossible language structures (Kallini et al., 2024). The authors evaluate whether LLMs show selective difficulty with impossible languages, using neuroimaging evidence from Musso et al. (2003) and Tettamanti et al. (2002) as a human benchmark. The commentary critically assesses whether LLM biases align with linguistic cognition or merely reflect statistical pattern matching.

## Key Results
- LLMs process language as linear probabilistic sequences rather than hierarchical structures
- LLMs perform equally well on both normal and impossible languages, unlike humans
- LLMs require trillion-parameter models and massive corpora while humans learn with minimal input

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs process language as linear probabilistic sequences rather than hierarchical structures
- Mechanism: LLMs operate as probabilistic models analyzing "flattened" strings of words using pattern detection across training corpora. They detect statistical regularities in linear word order rather than computing recursive hierarchical structures.
- Core assumption: The authors assume that human language comprehension necessarily requires hierarchical structure building, and that linear processing is fundamentally incompatible with linguistic competence.
- Evidence anchors:
  - [abstract] "LLMs are probabilistic models that require a vast amount of data to analyse externalized strings of words"
  - [section] "LLMs are probabilistic models that analyse externalized—'flattened'—strings of words. As such, they are not new: already in 1913, Markov (1913/2006) published a probabilistic analysis"
  - [corpus] Related work on symbolic NLU systems notes LLMs' "reliance on probabilistic inference makes them vulnerable to errors such as hallucination"
- Break condition: If LLMs were shown to implicitly construct hierarchical representations that mirror human syntactic processing, this mechanism would weaken.

### Mechanism 2
- Claim: Human language acquisition operates through domain-specific constraints that exclude "impossible" languages
- Mechanism: The human language faculty incorporates innate structural constraints that allow discrimination between possible human languages (hierarchical, recursive) and impossible languages (linear-only rules). This is evidenced by selective activation in Broca's area only for impossible language processing.
- Core assumption: The distinction between possible and impossible languages reflects hard cognitive constraints rather than learned statistical patterns.
- Evidence anchors:
  - [abstract] "[human language] can readily distinguish between real language and impossible languages"
  - [section] "convergent experiments, exploiting both actual languages (Musso et al., 2003) and invented languages (Tettamanti et al., 2002)... showing a selective inhibition of a network involving Broca's area for impossible languages only"
  - [corpus] Weak direct corpus support for neuroimaging claims; no neighboring papers reference Broca's area or impossible language paradigms
- Break condition: If LLMs demonstrated systematic difficulty with impossible languages matching human patterns, the distinction claim would be less diagnostic.

### Mechanism 3
- Claim: LLMs learn through data scale while humans learn through structural priors
- Mechanism: LLMs require trillion-parameter models trained on massive corpora, whereas human children build syntactic structures "even without any relevant input." LLMs cannot simulate the developmental stages children traverse, which include structures unattested in adult input.
- Core assumption: The Poverty of the Stimulus argument is valid—children's linguistic knowledge exceeds what could be derived from input alone.
- Evidence anchors:
  - [abstract] "The language system grows with minimal external input"
  - [section] "human infants... can build syntactic structures in their mind even without any relevant input (Crain et al., 2017; Yang et al., 2017; Bolhuis et al., 2023)—the 'Poverty of the Stimulus' argument"
  - [corpus] Neighboring paper "How Linguistics Learned to Stop Worrying and Love the Language Models" directly contests this, arguing LLMs' success obviates need for innate priors
- Break condition: If LLMs matched human sample efficiency or replicated child developmental stages through different architectures, the data-scale critique would weaken.

## Foundational Learning

- Concept: **Recursive hierarchical structure**
  - Why needed here: The paper's central claim depends on understanding that human language generates meaning through nested, hierarchical structures (not linear word sequences). Without this, the critique of LLMs as "flattened" pattern matchers is unintelligible.
  - Quick check question: Can you explain why "the dog that chased the cat barked" requires hierarchical structure rather than linear word-pair associations to parse correctly?

- Concept: **Poverty of the Stimulus**
  - Why needed here: The paper invokes this argument to explain why humans can acquire language with minimal input while LLMs require massive corpora. Understanding this is essential to evaluate the authors' claims about fundamental differences in learning.
  - Quick check question: What linguistic knowledge do children demonstrate that could not plausibly be extracted from the input they receive?

- Concept: **Possible vs. impossible languages**
  - Why needed here: The paper uses this as a diagnostic test: humans cannot learn impossible languages (linear-only rules), while LLMs learn them equally well. This is presented as evidence that LLMs lack the cognitive architecture underlying human language.
  - Quick check question: What makes a rule structure "impossible" in the sense used here, and why would humans selectively fail to learn such rules?

## Architecture Onboarding

- Component map:
  - LLM path: Training corpus → Tokenization → Probabilistic next-token prediction over linear sequences → Output generation
  - Human language faculty (per authors): Minimal input → Domain-specific computational system → Recursive hierarchical structure generation → Externalization (optional)
  - Diagnostic comparator: Possible/impossible language discrimination task (Broca's area activation as biomarker)

- Critical path:
  1. Identify task that distinguishes hierarchical from linear processing
  2. Test whether system shows selective difficulty with impossible (linear-only) languages
  3. Compare sample efficiency and developmental trajectory against human benchmarks

- Design tradeoffs:
  - Scale vs. cognitive plausibility: Larger models improve output quality but diverge further from human learning mechanisms
  - Linear pattern detection vs. structure building: LLMs excel at the former; authors claim only the latter constitutes language understanding
  - Engineering capability vs. scientific informativeness: Successful output generation does not imply correct cognitive modeling

- Failure signatures:
  - Equal performance on forward and reversed text (Luo et al. 2024, Ziv et al. 2025)
  - Minimal performance difference between natural and impossible languages (Kallini et al. 2024 critique)
  - No analogous developmental stages to child language acquisition
  - Energy consumption orders of magnitude beyond biological systems (~20W brain vs. 70MW GPU clusters)

- First 3 experiments:
  1. Replicate Luo et al. (2024) paradigm: Compare model perplexity on forward vs. backward English text; humans should fail backward, LLMs should show equivalent performance
  2. Test hierarchical dependency processing: Use sentences with center-embedding requiring long-distance dependencies; compare error patterns to human working memory constraints
  3. Developmental trajectory analysis: Train model incrementally and compare error types/structures produced to documented stages in child language acquisition (e.g., structure-agnostic to structure-dependent stages)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can computational architectures be designed to replicate the human ability to selectively inhibit or fail to acquire "impossible" languages (e.g., linear-structure rules)?
- Basis in paper: [inferred] The authors cite studies (Luo et al., 2024; Ziv et al., 2025) showing LLMs process impossible languages (like reversed text) as effectively as natural language, failing to exhibit the selective inhibition seen in human neural circuits (Broca's area).
- Why unresolved: Current probabilistic models lack the specific "inductive biases" that restrict human acquisition to hierarchical structures, treating all patterned data as equally learnable regardless of neurocognitive validity.
- What evidence would resolve it: Demonstration of an artificial system that, like humans, shows significant impairment in learning impossible languages compared to natural ones, specifically due to architectural constraints rather than data volume.

### Open Question 2
- Question: Is it possible for a language model to acquire complex syntax from the limited input available to human children (Poverty of the Stimulus) rather than requiring massive datasets?
- Basis in paper: [explicit] The paper contrasts the "enormous amount of training" required for trillion-parameter LLMs with the human infant's ability to build syntactic structures "even without any relevant input."
- Why unresolved: There is a fundamental mismatch between the statistical learning mechanisms of current LLMs, which rely on data volume, and the generative computational system underlying human language, which grows with minimal external input.
- What evidence would resolve it: A model that successfully acquires the full range of grammatical structures found in adult human language using only the quantity and quality of linguistic input available to a child.

### Open Question 3
- Question: Can an LLM be constructed to operate on hierarchical, recursive structures directly, rather than analyzing "flattened" linear strings of words?
- Basis in paper: [explicit] The authors argue that human language uses a "recursive function" to generate hierarchical thought structures, whereas LLMs are fundamentally probabilistic models of "externalized—‘flattened’—strings."
- Why unresolved: The core architecture of dominant LLMs (transformers) processes tokens sequentially or via attention mechanisms over linear sequences, lacking an inherent mechanism for generating the proposed mind-internal hierarchical syntax.
- What evidence would resolve it: An architecture that internally represents and manipulates nested hierarchical trees (thought structures) before linearization, resulting in outputs that reflect semantic meaning derived from structure rather than statistical correlation.

### Open Question 4
- Question: Do the "inductive biases" of LLMs meaningfully align with the constraints of human linguistic cognition, or are they merely computational shortcuts for statistical pattern matching?
- Basis in paper: [explicit] The authors dispute Futrell & Mahowald's claim that LLM biases align with linguistic structure, referencing Bowers (2025) to argue that differences in performance are due to "computational complexity" rather than linguistic similarity.
- Why unresolved: It is unclear whether the successes of LLMs in linguistic tasks stem from simulating the cognitive architecture of language or from exploiting vast statistical regularities that are inaccessible to humans.
- What evidence would resolve it: Disentangling computational complexity from linguistic validity in learning tasks, showing that models fail or succeed for the same structural reasons humans do, distinct from raw processing power.

## Limitations

- Key empirical claims rely on cited studies whose methodologies are not fully specified in the commentary
- Neuroimaging evidence for human inability to process impossible languages is based on limited literature
- The paper does not provide original experimental data, relying entirely on synthesis of prior work

## Confidence

- High confidence: Characterization of LLMs as probabilistic models operating on linear sequences; contrast between LLMs' data requirements and human sample efficiency
- Medium confidence: Claims about LLMs' inability to distinguish possible from impossible languages; methodological weaknesses acknowledged in some cited work
- Low confidence: Neuroimaging evidence definitively showing human brains cannot process impossible languages

## Next Checks

1. Replicate the forward/backward text paradigm with controlled conditions: Generate matched corpora of normal and reversed English text, evaluate multiple LLM architectures (varying size and training data), and compare perplexity scores with statistical significance testing.

2. Test hierarchical dependency processing: Create stimuli with center-embedding requiring long-distance syntactic dependencies, measure error patterns in both LLMs and human subjects, and compare working memory constraints.

3. Investigate developmental trajectories: Train models incrementally on increasing amounts of linguistic data, catalog error types at each stage, and compare against documented stages in child language acquisition research to assess whether LLM learning trajectories mirror human developmental patterns.