---
ver: rpa2
title: Exploring the Impact of Instruction-Tuning on LLM's Susceptibility to Misinformation
arxiv_id: '2507.18203'
source_url: https://arxiv.org/abs/2507.18203
tags:
- misinformation
- llms
- question
- option
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how instruction-tuning affects large language
  models' susceptibility to misinformation. Through experiments on two proprietary
  and four open-source LLMs, the authors find that instruction-tuned models are significantly
  more likely to accept misinformation when it is presented in the user role compared
  to the assistant role.
---

# Exploring the Impact of Instruction-Tuning on LLM's Susceptibility to Misinformation

## Quick Facts
- arXiv ID: 2507.18203
- Source URL: https://arxiv.org/abs/2507.18203
- Reference count: 27
- Primary result: Instruction-tuning increases LLMs' susceptibility to user-provided misinformation, shifting susceptibility from assistant-role to user-role inputs.

## Executive Summary
This paper investigates how instruction-tuning affects large language models' susceptibility to misinformation. Through experiments on two proprietary and four open-source LLMs, the authors find that instruction-tuned models are significantly more likely to accept misinformation when it is presented in the user role compared to the assistant role. The effect is amplified when misinformation is presented as a separate user turn. Comparing instruction-tuned models to their base versions shows that instruction-tuning shifts models from being most susceptible to assistant-provided misinformation to being most susceptible to user-provided misinformation. Additional factors like misinformation length and warning messages in system prompts also influence susceptibility, with longer misinformation and simple warnings showing varied effectiveness across models.

## Method Summary
The study uses the Farm dataset with "logical" misinformation type, evaluating models through three scenarios: Single-Turn (STQ), Assistant-Provided Document (APD), and User-Provided Document (UPD). Models are first tested in closed-book mode to establish parametric knowledge, then re-evaluated with misinformation injected via the three scenarios. MSR (Misinformation Susceptibility Rate) is calculated as the percentage of questions correctly answered in closed-book but answered incorrectly (aligned with misinformation) when the document is provided. The study compares base and instruction-tuned variants across multiple model families including Llama-3, Mistral, Qwen, and GPT-4o.

## Key Results
- Instruction-tuned models show highest susceptibility to user-provided misinformation (UPD scenario) compared to assistant-provided (APD) or embedded (STQ) misinformation.
- The UPD vs. STQ MSR gap ranges from 5-22 percentage points across all tested models and datasets.
- Comparing base to instruction-tuned models reveals a shift from assistant-role to user-role susceptibility patterns.
- Longer misinformation contexts reduce the user-role effect, causing models to behave more like their base counterparts.
- System prompt warnings reduce MSR by 65-70% for proprietary models but have negligible effect on open-source models.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Instruction-tuning shifts model attention from assistant-role toward user-role inputs, increasing susceptibility to user-provided misinformation.
- **Mechanism:** Base models exhibit highest susceptibility when misinformation appears in the assistant-role (APD scenario). After instruction-tuning, 3 of 4 tested models flipped to show highest susceptibility in the user-role (UPD scenario). The authors hypothesize this reflects training on instruction-following data that conditions models to weight user inputs more heavily during generation.
- **Core assumption:** The role-based susceptibility shift is caused by instruction-tuning itself, not by pre-training dynamics or model architecture.
- **Evidence anchors:**
  - [abstract] "A comparison with base models shows that instruction-tuning increases reliance on user-provided information, shifting susceptibility from the assistant role to the user role."
  - [section 4.3, Figure 5] Visualizes ranking changes: base models show APD as highest susceptibility; after instruction-tuning, UPD becomes highest for Llama-3-8B, Llama-3.1-8B, and Mistral-7B.
  - [corpus] Weak direct support; related work on "sycophancy" (Perez et al., 2023) is cited but not Mechanism-validated.
- **Break condition:** Models with different instruction-tuning configurations (e.g., Qwen2.5-7B in the paper) may not exhibit the same shift, suggesting training data or method variance matters.

### Mechanism 2
- **Claim:** Presenting misinformation as a separate user-role turn amplifies its influence on model outputs compared to embedding it in a single-turn query.
- **Mechanism:** In the UPD scenario, misinformation appears as its own conversational turn with user-role markers. Across all tested models and datasets, UPD produced higher MSR scores than STQ (differences ranging 5–22 percentage points). The authors hypothesize that role-separation makes misinformation more "prominent" in the generation process, though the attention mechanism underlying this is not directly probed.
- **Core assumption:** The observed effect is due to role-based prominence, not simply increased token count or context position.
- **Evidence anchors:**
  - [abstract] "susceptibility increasing when misinformation is presented as a separate user-role turn"
  - [section 4.2] "Across all models and datasets, UPD consistently recorded higher MSR scores than STQ. For most models, the difference ranged between 5%p and 8%p, while Mistral-7B-Instruct-v0.3 showed a particularly large gap, averaging 22%p."
  - [corpus] No direct corpus evidence on turn-separation effects.
- **Break condition:** If the effect is driven by context position rather than role markers, then repositioning misinformation within the same turn should produce similar results (untested in this paper).

### Mechanism 3
- **Claim:** Longer misinformation contexts reduce the user-role susceptibility effect, causing models to behave more like their base counterparts.
- **Mechanism:** As misinformation length increased from 1 to 3 paragraphs, the MSR gap between UPD and APD narrowed in most models. The authors suggest this indicates "the influence of instruction-tuning diminishes as misinformation length grows," reverting models toward base-model patterns (which favored assistant-role susceptibility), but they do not propose a specific attentional or processing explanation.
- **Core assumption:** Length-dependent behavior reflects diminished instruction-tuning influence rather than other factors such as context window saturation or attention dilution.
- **Evidence anchors:**
  - [section 4.4, Figure 6] "in most cases, as the misinformation length increases, the MSR score gap between UPD and APD gradually decreases."
  - [section 4.4] "In particular, in Llama-3-8B-Instruct, this gap steadily narrowed, and in some models, APD's MSR score even surpassed UPD's."
  - [corpus] No direct corpus evidence on length-susceptibility relationships.
- **Break condition:** GPT-4o and Mistral-7B-Instruct-v0.3 did not show the same length-dependent pattern, indicating model-specific factors (possibly long-context handling or instruction-tuning method) may interfere.

## Foundational Learning

- **Concept: Instruction-tuning objective**
  - **Why needed here:** The paper's central claim is that instruction-tuning causes the susceptibility shift; understanding what instruction-tuning optimizes for (instruction-following, helpfulness, safety alignment) is prerequisite.
  - **Quick check question:** What behavioral change distinguishes an instruction-tuned model from its base pre-trained counterpart?

- **Concept: Knowledge conflict in LLMs**
  - **Why needed here:** The experimental setup assumes LLMs have parametric knowledge that can conflict with external misinformation; MSR measures override of this knowledge.
  - **Quick check question:** When a model's parametric knowledge contradicts provided context, what three behaviors might it exhibit?

- **Concept: Chat template roles (user/assistant/system)**
  - **Why needed here:** The paper's three scenarios (STQ, APD, UPD) manipulate which conversational role contains the misinformation; understanding how models process role markers is essential.
  - **Quick check question:** In a standard chat template, which role typically carries user instructions vs. model responses?

## Architecture Onboarding

- **Component map:**
  - Input layer: Chat template formatter (adds role tokens for different model families)
  - Context assembly: Three scenarios construct different conversation histories
  - Evaluation: MSR metric calculation based on closed-book vs. misinformation-exposed performance

- **Critical path:**
  1. Filter dataset to questions the model answers correctly in closed-book setting (establishes parametric knowledge baseline)
  2. Inject misinformation via one of three scenarios
  3. Measure whether model selects the misinformation-aligned incorrect option
  4. Compare MSR across scenarios and between base/instruction-tuned variants

- **Design tradeoffs:**
  - Proprietary vs. open-source: Proprietary models (GPT-4o) showed strong response to system-prompt warnings; open-source models did not. Warning-based mitigation may not transfer.
  - Length vs. susceptibility granularity: Short misinformation maximizes user-role effect but may not reflect real-world attack scenarios; longer contexts dilute the effect but are more realistic.
  - Dataset scope: Farm dataset uses MCQ format; results may not generalize to open-ended generation tasks.

- **Failure signatures:**
  - Unexpectedly low UPD vs. STQ gap: Check chat template formatting for role markers
  - Base and instruction-tuned models showing identical patterns: Verify correct model checkpoints are loaded
  - Warning having no effect on proprietary models: Check that warning text is in system prompt position, not user prompt

- **First 3 experiments:**
  1. **Reproduce STQ vs. UPD gap:** Run both scenarios on Llama-3-8B-Instruct with Farm dataset subset; expect UPD MSR > STQ MSR by 5–8 percentage points.
  2. **Base vs. instruction-tuned comparison:** Run APD scenario on Llama-3-8B (base) and Llama-3-8B-Instruct; expect base to show higher APD susceptibility than instruction-tuned.
  3. **Warning effectiveness test:** Add the paper's warning to system prompt for GPT-4o mini (or available proprietary model); expect MSR reduction of 15–25 percentage points on NQ dataset.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why do some models (GPT-4o, Mistral-7B-Instruct-v0.3) fail to show the base-model-like susceptibility shift as misinformation length increases, while other instruction-tuned models do?
- Basis in paper: [explicit] Appendix C.3 states "The reasons behind this deviation remain unclear and require further investigation" and suggests potential factors include "differences in instruction-tuning methodologies, long-context processing capabilities, and other architectural distinctions."
- Why unresolved: Most models shifted toward base-model patterns with longer misinformation, but GPT-4o and Mistral-7B-Instruct-v0.3 showed divergent behavior; no controlled comparison of long-context handling across architectures was conducted.
- What evidence would resolve it: Controlled experiments comparing attention patterns and context-processing mechanisms across models as document length varies.

### Open Question 2
- Question: What mitigation techniques beyond system-prompt warnings can effectively reduce misinformation susceptibility in open-source instruction-tuned LLMs?
- Basis in paper: [explicit] Conclusion calls for "techniques that balance instruction-following abilities with stronger misinformation resistance"; [inferred] Figure 7 shows warnings reduced MSR by ~65-70% for proprietary models but had negligible effect (<5% change) for open-source models.
- Why unresolved: The authors tested only one mitigation approach (simple warnings), which worked for proprietary models but not open-source ones, suggesting open-source models may process system prompts differently.
- What evidence would resolve it: Testing alternative strategies (fine-tuning, contrastive learning, dedicated discriminator modules) specifically on open-source models to identify what works.

### Open Question 3
- Question: How does model scale affect the relationship between instruction-tuning and misinformation susceptibility?
- Basis in paper: [explicit] Limitations section states "Future research should include models across a broader range to systematically examine the relationship between model size and misinformation susceptibility."
- Why unresolved: Experiments were limited to 7B-8B parameter models due to resource constraints; no larger open-source models were tested.
- What evidence would resolve it: Replicating the three experimental scenarios (STQ, APD, UPD) across multiple model sizes (e.g., 1B, 7B, 13B, 70B) within the same model family.

### Open Question 4
- Question: What specific instruction-tuning configurations cause Qwen2.5-7B-Instruct to exhibit higher susceptibility to assistant-role misinformation (APD > UPD), contrary to other models?
- Basis in paper: [explicit] Section 4.2-4.3 and Limitations note this deviation and that "models do not disclose details about their instruction-tuning methods or training data, making it difficult to determine the fundamental reasons for these variations."
- Why unresolved: Without transparency into training procedures, the cause of model-specific behavioral differences cannot be isolated.
- What evidence would resolve it: Training models from the same base using systematically varied instruction-tuning configurations to identify which factors drive role-specific susceptibility.

## Limitations
- Narrow dataset scope (Farm dataset with "logical" misinformation type only) limits generalizability to real-world scenarios.
- Proprietary model results cannot be independently verified due to API access constraints.
- Warning mitigation mechanism tested was simple and may not generalize to more sophisticated safety interventions.

## Confidence
- **High confidence:** The observed MSR differences between STQ and UPD scenarios across multiple open-source models are robust and reproducible based on the specified methodology.
- **Medium confidence:** The claim that instruction-tuning shifts susceptibility from assistant-role to user-role is supported by cross-model comparisons, though the mechanism remains heuristic without direct attention analysis.
- **Low confidence:** The effectiveness of warning messages in system prompts is only demonstrated on proprietary models and shows high variability, making generalization to other models or warning types uncertain.

## Next Checks
1. **Attention mechanism validation:** Run attention visualization on instruction-tuned vs. base models during UPD scenario to verify whether user-role inputs receive disproportionately higher attention weights.
2. **Cross-dataset generalization:** Test the STQ vs. UPD gap on a second misinformation dataset (e.g., TruthfulQA) to confirm findings are not dataset-specific.
3. **Instruction-tuning method variation:** Compare susceptibility patterns across different instruction-tuning approaches (e.g., supervised fine-tuning vs. reinforcement learning) using the same base model to isolate the training method effect.