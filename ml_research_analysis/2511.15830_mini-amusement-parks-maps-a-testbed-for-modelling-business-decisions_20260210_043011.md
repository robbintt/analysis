---
ver: rpa2
title: 'Mini Amusement Parks (MAPs): A Testbed for Modelling Business Decisions'
arxiv_id: '2511.15830'
source_url: https://arxiv.org/abs/2511.15830
tags:
- park
- react
- gpt-5
- guests
- cost
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Mini Amusement Parks (MAPs) is a benchmark designed to evaluate
  AI systems on the integrated challenges of real-world decision-making, such as business
  management. It combines open-ended optimization, long-horizon planning, active world-model
  learning, spatial reasoning, and stochastic reasoning under uncertainty.
---

# Mini Amusement Parks (MAPs): A Testbed for Modelling Business Decisions

## Quick Facts
- arXiv ID: 2511.15830
- Source URL: https://arxiv.org/abs/2511.15830
- Reference count: 40
- Primary result: AI agents underperform humans by 6.5× on easy and 9.8× on medium difficulty in amusement park management

## Executive Summary
Mini Amusement Parks (MAPs) is a benchmark designed to evaluate AI systems on integrated real-world decision-making challenges, specifically business management tasks. The environment simulates running an amusement park where agents must maximize park value through managing rides, shops, staff, and guest behavior. The benchmark combines open-ended optimization, long-horizon planning, active world-model learning, spatial reasoning, and stochastic reasoning under uncertainty. Evaluation against state-of-the-art LLMs shows humans significantly outperform AI, with analysis revealing specific weaknesses in long-horizon planning, sample-efficient learning, spatial reasoning, and robust decision-making under stochasticity.

## Method Summary
The MAPS testbed uses a Python-based simulator that provides JSON state observations and accepts string-formatted Python function calls as actions. The primary baseline agent uses the ReAct framework, implementing a loop of thought, action, and observation. Agents receive a 15-page game manual and history of the last 5 turns as context. Evaluation involves running agents on held-out layouts for 50 (Easy) or 100 (Medium) steps, with performance measured as final Park Value (Money + Asset Sell Price + Intellectual Property). The system includes a sandbox mode for active world-model learning and supports testing with oracle world models via Model Predictive Control.

## Key Results
- Human agents outperform AI by 6.5× on easy and 9.8× on medium difficulty settings
- AI struggles with long-horizon planning, often pursuing greedy strategies without building sustainable early-game foundations
- Spatial reasoning failures are common, with agents frequently constructing inaccessible rides or placing attractions in sub-optimal locations
- World modeling with learned models fails, while oracle models yield up to 4× improvement
- Sandbox mode for active world-model learning provides minimal benefit

## Why This Works (Mechanism)
MAPS works by creating a rich, stochastic environment that requires agents to integrate multiple cognitive capabilities simultaneously. The benchmark forces agents to learn from limited interactions, reason about spatial constraints, plan over long horizons, and adapt to uncertainty in guest behavior. By providing a sandbox learning mode and supporting world model testing, the environment can diagnose specific failure modes in AI decision-making. The text-based interface makes it accessible to LLMs while maintaining the complexity of real-world business decisions.

## Foundational Learning
- **ReAct (Reasoning + Acting) Framework**
  - Why needed here: The primary baseline agent for MAPS is implemented using the ReAct framework. It's essential to understand how this loop of thought, action, and observation is used to structure the agent's decision-making process.
  - Quick check question: In the ReAct prompt (Section J.1), what is the required format for an agent to take an action?

- **World Model (in Model Predictive Control)**
  - Why needed here: A key part of the paper's analysis is testing whether a learned or oracle world model, combined with MPC, can improve planning. Understanding what a world model does—predicting future states—is critical for interpreting these results.
  - Quick check question: What is the primary function of a world model within a Model Predictive Control (MPC) loop, as described in Section 3.1?

- **Stochasticity (Aleatoric Uncertainty)**
  - Why needed here: A core challenge in MAPS is reasoning under uncertainty. The paper demonstrates that identical actions can lead to different outcomes due to probabilistic guest behavior. Grasping this concept is key to understanding why the task is hard.
  - Quick check question: According to the paper, what is the main source of non-deterministic transitions in MAPS?

## Architecture Onboarding
- **Component map:** Simulator (Mini Amusement Parks) -> JSON state observation -> ReAct agent wrapper -> LLM -> String-formatted Python function calls -> Simulator
- **Critical path:** Agent wrapper (ReAct loop) -> LLM (generates action) -> Simulator (executes action, returns new state). In the learning condition, the path is Sandbox Agent wrapper -> LLM (generates action & learning summary) -> Simulator.
- **Design tradeoffs:** The evaluation focuses on LLM-based agents using text-based JSON state representation, making it accessible to LLMs but not testing other paradigms like traditional reinforcement learning. The human baseline is from an online leaderboard representing an "unlimited resource" setting.
- **Failure signatures:**
  - Myopic planning: Agents pursuing greedy strategies without building sustainable early game
  - Ineffective world-model learning: Generated learnings being overly specific or failing to identify causal relationships
  - Spatial reasoning failures: Constructing inaccessible rides or placing attractions in sub-optimal locations
- **First 3 experiments:**
  1. Run the provided ReAct agent baseline on the "easy" difficulty to establish a performance baseline and familiarize yourself with the state/action format.
  2. Execute the sandbox learning experiment as described in Section 3.1. Inspect the generated "Learnings" text file to see what, if any, causal relationships the agent extracted.
  3. Implement the simple spatial reasoning heuristic described in Section 3.1 and Table 5. Modify the agent to use this heuristic for all placement actions and measure the performance gain.

## Open Questions the Paper Calls Out
- **How can agents be designed to perform effective active world-model learning from limited sandbox interactions?** Agents currently generate "learnings" that are overly specific, ignore causality, or parrot the manual, leading to performance degradation rather than improvement. An agent architecture capable of extracting generalizable, actionable rules from sandbox interactions that result in a statistically significant performance increase on held-out evaluation layouts would resolve this.

- **Can LLM-based agents develop spatial reasoning capabilities that surpass simple heuristic baselines?** Models struggle with path connectivity and clustering, frequently constructing inaccessible rides or failing to utilize water tiles effectively. An agent achieving higher scores than the heuristic-augmented baseline by demonstrating robust reasoning about path topology and adjacency constraints would resolve this.

- **Can learned world models bridge the performance gap between reactive agents and planning with oracle world models?** Current learned models struggle to accurately predict stochastic transitions and require expensive rollouts. A learned world model that, when combined with a planning algorithm, improves performance over the standard ReAct baseline while maintaining computational efficiency would resolve this.

## Limitations
- The use of unreleased or proprietary models (GPT-5, GPT-5 Nano, Grok 4) prevents exact reproduction of reported scores
- The human baseline from an online leaderboard may not represent resource-constrained settings and could overestimate the AI performance gap
- The text-based JSON representation limits comparisons with other AI paradigms like traditional reinforcement learning

## Confidence
**High Confidence:** The architectural design of MAPs as a benchmark combining multiple real-world decision-making challenges is well-specified and reproducible. The identification of specific AI weaknesses (long-horizon planning, spatial reasoning, world-model learning) is supported by concrete experimental evidence.

**Medium Confidence:** The relative performance comparisons between humans and AI agents are credible, though the exact numerical gaps depend on the specific model versions used. The analysis of failure modes is well-documented but may be influenced by the particular LLM implementations tested.

**Low Confidence:** The exact performance scores against the specific proprietary models cannot be verified without access to those model checkpoints. The effectiveness of the sandbox learning mode and spatial heuristics would benefit from more extensive ablation studies.

## Next Checks
1. **Reproduce with Current Models:** Implement the ReAct agent using accessible frontier LLMs (GPT-4o, Claude 3.5) and measure performance on the same layouts to establish comparable baselines and verify the identified failure modes.

2. **Variance Analysis Under Controlled Stochasticity:** Run multiple trials with fixed random seeds to quantify the impact of stochastic guest behavior on agent performance, particularly focusing on whether agents can learn to hedge against uncertainty.

3. **Ablation of World Model Components:** Systematically test the MPC framework by isolating the contributions of the world model versus the planning horizon, and compare oracle versus learned models across different complexity levels to validate the claimed 4× improvement.