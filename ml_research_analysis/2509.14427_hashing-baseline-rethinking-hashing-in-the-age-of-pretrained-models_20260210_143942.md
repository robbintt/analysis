---
ver: rpa2
title: 'Hashing-Baseline: Rethinking Hashing in the Age of Pretrained Models'
arxiv_id: '2509.14427'
source_url: https://arxiv.org/abs/2509.14427
tags:
- hashing
- retrieval
- audio
- binary
- embeddings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces Hashing-Baseline, a training-free hashing\
  \ method that leverages powerful pre-trained embeddings for efficient retrieval.\
  \ The method combines classical techniques\u2014principal component analysis (PCA),\
  \ random orthogonal projection, and threshold binarization\u2014with frozen embeddings\
  \ from state-of-the-art vision and audio encoders."
---

# Hashing-Baseline: Rethinking Hashing in the Age of Pretrained Models

## Quick Facts
- arXiv ID: 2509.14427
- Source URL: https://arxiv.org/abs/2509.14427
- Reference count: 0
- Key outcome: Training-free hashing method achieves up to 91.0% mAP on CIFAR-10 using frozen embeddings and classical hashing techniques

## Executive Summary
This paper introduces Hashing-Baseline, a training-free hashing method that leverages powerful pre-trained embeddings for efficient retrieval. The method combines classical techniques—principal component analysis (PCA), random orthogonal projection, and threshold binarization—with frozen embeddings from state-of-the-art vision and audio encoders. This approach produces competitive retrieval performance without additional learning or fine-tuning. The authors evaluate Hashing-Baseline on standard image retrieval benchmarks (CIFAR-10, Flickr25K, COCO, NUS-WIDE) and a newly introduced audio hashing benchmark (GTZAN, ESC-50, CREMA-D, VocalSound). Results show that Hashing-Baseline achieves high mAP scores, with 16-bit codes reaching up to 91.0% on CIFAR-10 and 84.4% on Flickr25K, demonstrating strong performance with minimal bit lengths.

## Method Summary
Hashing-Baseline is a training-free hashing method that transforms frozen pre-trained embeddings into compact binary codes using classical techniques. The pipeline consists of four stages: (1) extract frozen embeddings from pre-trained vision or audio encoders, (2) apply PCA dimensionality reduction fitted on training set, (3) project onto random orthogonal matrix generated via QR decomposition, and (4) threshold with sigmoid to produce binary codes for database items while keeping queries continuous. The method uses asymmetric Hamming retrieval, computing similarity as the sum of absolute differences between continuous query probabilities and binary database codes. This approach achieves competitive retrieval performance without any learning or fine-tuning, with 16-bit codes reaching up to 91.0% mAP on CIFAR-10 and 84.4% on Flickr25K.

## Key Results
- 16-bit codes achieve 91.0% mAP on CIFAR-10, 84.4% on Flickr25K, and 80.7% on NUS-WIDE
- Performance improves with bit length: 64-bit codes reach 94.4% (CIFAR-10), 93.4% (Flickr25K), and 92.5% (NUS-WIDE)
- Audio benchmarks show CLAP encoder outperforms others, with 16-bit codes reaching 93.8% mAP on GTZAN and 92.2% on ESC-50
- Dataset-specific PCA outperforms global PCA fitted on ImageNet-1K, validating the importance of domain-specific dimensionality reduction

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-trained embeddings contain substantial redundancy that can be compressed via PCA while preserving retrieval-relevant structure.
- Mechanism: Truncated SVD projects normalized embeddings onto the top-k principal directions, retaining dimensions with highest variance while discarding noise and redundant information. This concentrated representation then feeds into binarization.
- Core assumption: Pre-trained encoders distribute semantic information across many dimensions, with principal components capturing the retrieval-critical structure.
- Evidence anchors:
  - [abstract] "We revisit classical, training-free hashing techniques: principal component analysis, random orthogonal projection, and threshold binarization"
  - [section 3.1] "A feature x is then projected into the reduced k-dimensional space: z = V^T x"
  - [corpus] Related work on foundation model embeddings for retrieval (arXiv:2510.27584) similarly leverages dimensionality reduction for compact representations
- Break condition: If pre-trained embeddings are already near-orthogonal with minimal redundancy, PCA may discard discriminative information rather than noise.

### Mechanism 2
- Claim: Random orthogonal projection redistributes variance across bits and preserves geometric relationships via the Johnson-Lindenstrauss lemma.
- Mechanism: QR decomposition of a random Gaussian matrix produces an orthogonal transformation R. This spreads concentrated variance from PCA across all dimensions before binarization, preventing any single bit from dominating.
- Core assumption: The reduced PCA space contains information that benefits from isotropic redistribution before thresholding.
- Evidence anchors:
  - [section 3.2] "A random orthogonal matrix R is generated by sampling a Gaussian matrix followed by QR decomposition"
  - [section 3.4] "Random orthogonal projections redistribute the concentrated variance across bits"
  - [corpus] Corpus evidence is limited; neighboring papers focus on learned rather than random projections
- Break condition: If the PCA-reduced space is already well-balanced, random rotation may introduce unnecessary variance without benefit.

### Mechanism 3
- Claim: Asymmetric Hamming retrieval reduces quantization error by keeping queries continuous while binarizing only the database.
- Mechanism: Query embeddings pass through the full pipeline (PCA → R → sigmoid) but are not thresholded. Similarity is computed as the sum of absolute differences between continuous query probabilities p_q and binary database codes b_i.
- Core assumption: Quantization error is more harmful for queries (processed repeatedly) than for database items (indexed once).
- Evidence anchors:
  - [section 3.3] "Rather than binarizing the query, we use asymmetric hamming retrieval"
  - [section 3.3] "Asymmetric Hamming allows to reduce quantization loss on the query-level"
  - [corpus] Weak direct corpus support; related hashing work does not emphasize asymmetric schemes
- Break condition: If query and database distributions differ significantly, asymmetric scoring may introduce systematic bias.

## Foundational Learning

- Concept: **Principal Component Analysis (PCA)**
  - Why needed here: Core dimensionality reduction step; understanding eigenspace projection is essential for debugging bit allocation and variance retention.
  - Quick check question: Can you explain why PCA on pre-trained embeddings differs from PCA on raw pixel/waveform data?

- Concept: **Johnson-Lindenstrauss Lemma**
  - Why needed here: Theoretical justification for random projections preserving pairwise distances; frames why the method works without learning.
  - Quick check question: What does JL guarantee about distance preservation after random projection?

- Concept: **Hamming Distance and Binary Retrieval**
  - Why needed here: Fundamental to understanding why compact codes enable fast search; XOR operations and popcount are the practical primitives.
  - Quick check question: Why does Hamming distance between binary codes approximate angular similarity in the original space?

## Architecture Onboarding

- Component map: Frozen Encoder → PCA Module → Random Projection → Sigmoid + Threshold → Asymmetric Scorer
- Critical path:
  - Offline: Fit PCA on training embeddings → generate R → binarize entire database
  - Online: Encode query → apply PCA → apply R → sigmoid → compute asymmetric similarity against all database codes
- Design tradeoffs:
  - Dataset-specific vs. global PCA: Dataset-specific PCA outperforms global PCA fitted on ImageNet-1K, but global PCA enables zero-shot deployment
  - Bit length vs. accuracy: 16-bit codes lose 2-10% mAP vs. 64-bit; diminishing returns above 64 bits
  - Encoder choice: DINOv2 and DFN outperform SimDINOv2 on vision; CLAP outperforms Dasheng on audio
- Failure signatures:
  - mAP drops sharply at 16 bits with global PCA: Indicates domain mismatch between PCA training data and target distribution
  - Near-random performance without random projection: Suggests PCA variance is concentrated in few dimensions
  - Audio retrieval underperforms baseline: May indicate encoder pre-training objective mismatches retrieval needs
- First 3 experiments:
  1. Sanity check: Replicate 16-bit CIFAR-10 result with DFN encoder; target 91.0±0.7 mAP. Verify PCA is fit on CIFAR-10 training set, not global.
  2. Ablation isolation: Run (a) PCA-only, (b) random-projection-only, (c) full pipeline on Flickr25K. Confirm both components contribute.
  3. Cross-domain test: Apply vision-trained PCA to audio embeddings (or vice versa) to quantify domain specificity.

## Open Questions the Paper Calls Out

- Question: Can Hashing-Baseline be extended to training-free cross-modal retrieval (e.g., audio–text, image–text) while maintaining competitive performance?
- Basis in paper: [explicit] Authors state in conclusion: "Finally, extending Hashing-Baseline to training-free cross-modal retrieval (e.g., audio–text, image–text) could unlock scalable, high-performance multimodal retrieval systems, maintaining the benefits of compact binary codes without the need for full end-to-end training."
- Why unresolved: Current method is evaluated only on single-modality retrieval; cross-modal alignment requires handling different embedding spaces that may not share geometric properties suitable for the same projection pipeline.
- What evidence would resolve it: Successful application to cross-modal benchmarks with mAP scores competitive against supervised cross-modal hashing methods.

- Question: What performance gains can lightweight parameter-efficient fine-tuning (e.g., LoRA, adapters) provide over the fully training-free Hashing-Baseline while maintaining low computational overhead?
- Basis in paper: [explicit] Authors state: "Future work could explore lightweight hashing via parameter-efficient fine-tuning, allowing pre-trained models to adapt to different bit constraints with minimal computational overhead."
- Why unresolved: The paper demonstrates training-free is competitive, but the trade-off between minimal fine-tuning cost and retrieval quality improvement remains unexplored.
- What evidence would resolve it: Benchmarks comparing Hashing-Baseline against parameter-efficient variants showing mAP improvements relative to added training time/parameters across different bit lengths.

- Question: How much retrieval accuracy improvement can be achieved by jointly optimizing the feature extractor and hashing module in an end-to-end framework?
- Basis in paper: [explicit] Authors state: "End-to-end representation learning jointly optimized with hashing also presents an exciting opportunity. Co-training the feature extractor and hashing module could better align the latent space with quantization constraints, further enhancing retrieval accuracy while preserving efficiency."
- Why unresolved: Current approach uses frozen embeddings not optimized for binarization; it remains unclear whether learned hash-aware representations would significantly outperform the training-free baseline.
- What evidence would resolve it: A co-trained model achieving substantially higher mAP at equivalent bit lengths compared to Hashing-Baseline.

## Limitations
- Limited comparative analysis against existing audio retrieval methods on the new audio hashing benchmark
- Asymmetric Hamming retrieval lacks strong empirical validation against symmetric alternatives
- Claims of generalizability need more rigorous testing with cross-domain experiments showing performance degradation

## Confidence
- High confidence in the general approach's validity and mechanism descriptions
- Medium confidence in specific quantitative claims due to limited comparative analysis and cross-domain testing
- Medium confidence in asymmetric retrieval contribution without stronger ablation studies

## Next Checks
1. Conduct cross-domain experiments by applying vision-trained PCA to audio embeddings and vice versa to quantify domain specificity and potential performance degradation.
2. Implement a symmetric vs. asymmetric retrieval ablation study on all benchmarks to isolate the contribution of asymmetric scoring.
3. Evaluate retrieval performance when database and query distributions differ significantly, such as using CIFAR-10 PCA on Flickr25K embeddings, to test robustness to domain shift.