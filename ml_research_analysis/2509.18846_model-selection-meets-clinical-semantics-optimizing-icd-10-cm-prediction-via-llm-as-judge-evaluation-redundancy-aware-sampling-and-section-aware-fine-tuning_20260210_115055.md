---
ver: rpa2
title: 'Model selection meets clinical semantics: Optimizing ICD-10-CM prediction
  via LLM-as-Judge evaluation, redundancy-aware sampling, and section-aware fine-tuning'
arxiv_id: '2509.18846'
source_url: https://arxiv.org/abs/2509.18846
tags:
- coding
- section
- performance
- chapter
- codes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a modular framework for ICD-10-CM coding using
  open-source LLMs, addressing challenges in base model selection, data redundancy,
  and contextual input modeling. The authors propose an LLM-as-judge evaluation protocol
  with Plackett-Luce aggregation to rank candidate models, a redundancy-aware sampling
  strategy using embedding similarity to remove duplicate summaries, and section-aware
  prompting leveraging structured discharge summaries.
---

# Model selection meets clinical semantics: Optimizing ICD-10-CM prediction via LLM-as-Judge evaluation, redundancy-aware sampling, and section-aware fine-tuning

## Quick Facts
- arXiv ID: 2509.18846
- Source URL: https://arxiv.org/abs/2509.18846
- Authors: Hong-Jie Dai; Zheng-Hao Li; An-Tai Lu; Bo-Tsz Shain; Ming-Ta Li; Tatheer Hussain Mir; Kuang-Te Wang; Min-I Su; Pei-Kang Liu; Ming-Ju Tsai
- Reference count: 40
- Primary result: BioMistral selected via LLM-as-judge framework achieved F1=0.78 on internal test and 0.63 on external validation for ICD-10-CM coding

## Executive Summary
This paper presents a modular framework for ICD-10-CM coding using open-source LLMs, addressing challenges in base model selection, data redundancy, and contextual input modeling. The authors propose an LLM-as-judge evaluation protocol with Plackett-Luce aggregation to rank candidate models, a redundancy-aware sampling strategy using embedding similarity to remove duplicate summaries, and section-aware prompting leveraging structured discharge summaries. Experiments on two Taiwanese hospital datasets show that BioMistral, selected via the proposed framework, achieved the best performance (F1 = 0.78 on internal test, 0.63 on external validation). The redundancy-aware sampling improved F1 by 0.01, and incorporating more clinical sections consistently enhanced performance. The approach demonstrates robustness across institutions and offers a scalable solution for real-world automated medical coding.

## Method Summary
The framework integrates three key components: (1) LLM-as-judge evaluation using pairwise comparisons and Plackett-Luce aggregation to rank open-source LLMs based on ICD-10-CM code comprehension, (2) redundancy-aware sampling with embedding similarity to remove duplicate summaries while preserving code diversity, and (3) section-aware prompting using structured discharge summaries. The process involves preparing datasets with 8:1:1 stratified splits, cleaning invalid ICD-10 codes, normalizing text, and extracting five clinical sections. Redundancy-aware sampling encodes summaries using all-MiniLM-L6-v2, builds FAISS indexes, and removes semantically similar summaries with identical codes. Model selection uses Atla Selene Mini for pairwise comparisons of candidate models on top-50 ICD codes, aggregating results via iterative Luce spectral ranking. The top-ranked model (BioMistral-7B) is fine-tuned using QLoRA with section-aware prompts, handling missing sections with "Nil" placeholders and truncating when exceeding 2048 tokens.

## Key Results
- BioMistral selected via LLM-as-judge framework achieved F1=0.78 on internal test and 0.63 on external validation
- Redundancy-aware sampling improved F1 by 0.01 while reducing training set from 100,656 to 85,820 summaries
- Section-aware prompting with DischgDiag+MedHist achieved PRF-scores of 0.803/0.786/0.794, outperforming DischgDiag-only baseline (0.798/0.776/0.787)

## Why This Works (Mechanism)

### Mechanism 1
LLM-as-judge pairwise comparison with Plackett-Luce aggregation provides a computationally efficient proxy for downstream fine-tuning performance. Candidate models generate ICD-10-CM code definitions; a lightweight judge model (Atla Selene Mini) performs pairwise assessments; win-rate matrices are aggregated via iterative Luce spectral ranking (ILSR) to produce stationary distribution scores that rank models by intrinsic code comprehension. Core assumption: A model's ability to accurately describe an ICD code's clinical meaning correlates with its capacity to predict that code from discharge summaries after fine-tuning. Evidence: Abstract states framework integrates LLM-as-judge with Plackett-Luce aggregation; section explains ILSR produces stationary distribution ranking. Break condition: If judge model exhibits systematic bias toward certain output styles over clinical accuracy, rankings may not reflect downstream utility.

### Mechanism 2
Embedding-based semantic deduplication reduces training redundancy while preserving code diversity, improving both efficiency and generalization. Discharge summaries encoded via all-MiniLM-L6-v2; FAISS index computes L2 distances; summaries sharing identical ICD codes with similarity >0.9 flagged as redundant; perplexity comparison retains linguistically diverse samples; longer summaries kept when perplexity similar. Core assumption: Semantically similar summaries with identical code assignments provide diminishing learning signal and may promote overfitting to frequent phrasing patterns. Evidence: Abstract states redundancy-aware sampling improved F1 by 0.01; section reports reduction from 100,656 to 85,820 summaries with consistent PRF-score improvements. Break condition: If threshold too aggressive, rare code presentations may be lost; if too permissive, redundancy benefits diminish.

### Mechanism 3
Incorporating multiple structured clinical sections, particularly medical history alongside discharge diagnosis, improves ICD-10-CM prediction by providing contextual diagnostic cues. Section-wise prompting with explicit "### [Section Name]" headers; universal model trained with all sections (missing sections filled with "Nil"); section-specific models trained on subset; priority-based truncation when exceeding 2,048 tokens. Core assumption: Coders cross-reference multiple sections; models benefit from same structured context, with MedHist containing diagnostic cues not explicit in DischgDiag alone. Evidence: Abstract states incorporating more clinical sections consistently enhanced performance; section shows DischgDiag+MedHist model elevated PRF-scores from 0.798/0.776/0.787 to 0.803/0.786/0.794. Break condition: If token limits force aggressive truncation of high-priority sections, performance gains may reverse; universal model underperforms section-specific when only sparse sections available.

## Foundational Learning

- **Plackett-Luce ranking model**: Aggregates pairwise comparison outcomes into global model rankings even with partial matchup data. Quick check: Given win rates A beats B (0.7), B beats C (0.6), A beats C (0.8), which has highest stationary probability?
- **QLoRA (Quantized Low-Rank Adaptation)**: Enables fine-tuning 7B parameter models on single GPU by quantizing base weights and training only low-rank adapters. Quick check: What is the trade-off between 4-bit quantization and full 16-bit fine-tuning for domain adaptation?
- **Multi-label stratified sampling**: Preserves ICD code distribution across train/validation/test splits when each summary has multiple codes. Quick check: Why does random split risk creating validation sets with unseen code combinations?

## Architecture Onboarding

- **Component map**: Preprocessing (character normalization → pattern matching → redundancy-aware sampling) -> Model Selection (candidate LLMs → code definition generation → judge pairwise comparison → ILSR ranking) -> Fine-tuning (section-aware prompt construction → QLoRA adaptation → autoregressive loss) -> Inference (universal model with Nil-padding OR section-specific model selection)
- **Critical path**: 1) Identify top-50 frequent ICD codes for judge evaluation 2) Run pairwise comparisons across candidate models 3) Apply ILSR to select base model (BioMistral in study) 4) Apply redundancy-aware sampling to training data 5) Fine-tune with section-aware prompts 6) Evaluate on internal test set then external hospital data
- **Design tradeoffs**: Universal vs section-specific (Universal handles variable input structure but underperforms when sections sparse ~0.02 F1 drop on DischgDiag-only); Token limit (2,048) covers ~85% of samples; priority-based truncation risks information loss for long summaries; Judge model selection (Lightweight Atla Selene Mini vs larger judges—speed vs assessment reliability)
- **Failure signatures**: Judge outputs not following "A"/"B" format → implement regex fallbacks, record ties; External validation F1 drops significantly (0.78→0.63) without retraining → indicates institutional documentation drift; PubMedGPT2 underperforms despite domain pretraining → parameter size (774M) may limit capacity more than domain adaptation helps
- **First 3 experiments**: 1) Validate LLM-as-judge correlation: Run pairwise comparisons on 50 codes, compute ILSR ranking, compare against full fine-tuning results to verify ranking correlates with downstream F1 2) Ablate redundancy threshold: Test similarity thresholds (0.85, 0.90, 0.95) measuring training time reduction vs F1 impact on held-out set 3) Section contribution analysis: Train models incrementally adding sections (DischgDiag → +MedHist → +OpNote → +PathRep → +TreatCous), measure F1 delta per section on matched subsets

## Open Questions the Paper Calls Out

### Open Question 1
Can the proposed modular framework be extended to support multi-type medical coding, specifically ICD-10-PCS procedure codes? Basis: The authors explicitly state the study focuses exclusively on ICD-10-CM diagnostic codes and future work should explore extending the pipeline to support procedure coding. Unresolved because current input features and fine-tuning objectives are optimized for diagnostic codes found in discharge diagnoses and medical histories, whereas procedure codes require reasoning over operative notes and different terminology. Evidence: Applying the framework to a dataset annotated with ICD-10-PCS codes and comparing the performance of the section-aware model against a diagnostic-only baseline.

### Open Question 2
Do retrieval-augmented generation (RAG) or federated learning approaches offer a more resource-efficient solution to coding drift and institutional heterogeneity than static fine-tuning? Basis: The authors identify static retraining as a resource-intensive limitation and explicitly suggest future work consider alternative paradigms like RAG or federated learning to handle evolving guidelines. Unresolved because the current study relies on static fine-tuning which requires periodic, costly retraining to maintain accuracy as clinical guidelines shift. Evidence: Longitudinal experiments comparing the maintenance cost and accuracy retention of RAG-based systems against the fine-tuned baseline as clinical coding guidelines are updated.

### Open Question 3
Does the section-aware prompting strategy maintain its effectiveness when applied to non-Taiwanese healthcare datasets with different documentation standards? Basis: The authors acknowledge that all data was collected within Taiwan and the generalizability to other geographic regions with different documentation styles remains unverified. Unresolved because the model's section-aware prompting relies on specific headers (e.g., "Discharge Diagnosis"); it is unclear if the model can adapt to Western datasets like MIMIC-IV which often lack such rigid sectioning or use different header conventions. Evidence: External validation of the universal model on English-language discharge summaries (e.g., MIMIC-IV) to test if the semantic understanding of sections transfers across languages and documentation formats.

## Limitations
- LLM-as-judge correlation with downstream performance lacks direct validation
- External validation F1 drop from 0.78 to 0.63 suggests institutional documentation drift not addressed
- Section-aware prompting relies on structured discharge summaries with specific headers, limiting generalizability

## Confidence

- **High Confidence**: BioMistral's superior performance on both internal and external validation datasets; token limit coverage (~85% of samples); section contribution analysis showing MedHist and DischgDiag as most impactful
- **Medium Confidence**: LLM-as-judge correlation with downstream performance; 0.01 F1 improvement from redundancy-aware sampling; universal model vs section-specific performance tradeoff
- **Low Confidence**: External validation F1 drop explanation; judge model selection impact on rankings; threshold sensitivity of redundancy-aware sampling

## Next Checks

1. **Judge Correlation Validation**: Run full fine-tuning on all candidate models and compute Pearson/Spearman correlation between ILSR rankings and final F1 scores to validate the LLM-as-judge proxy
2. **Redundancy Threshold Sensitivity**: Systematically test similarity thresholds (0.85, 0.90, 0.95) measuring training time reduction vs F1 impact across multiple random seeds to determine optimal tradeoff
3. **Institutional Adaptation Test**: Fine-tune the best model on the external validation dataset for 1-2 epochs and measure F1 improvement to assess whether documentation drift explains the performance gap