---
ver: rpa2
title: '"Yeah Right!" -- Do LLMs Exhibit Multimodal Feature Transfer?'
arxiv_id: '2501.04138'
source_url: https://arxiv.org/abs/2501.04138
tags:
- basic
- zero-shot
- few-shot
- gpt-4o
- gpt-4-turbo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether large language models exhibit multimodal
  feature transfer when detecting covert deceptive communication, such as sarcasm,
  irony, and condescension. The authors compare speech+text models (GPT-4o) against
  text-only models (GPT-4-Turbo) and conversation-tuned models (Llama-2-70B-conversational)
  against standard text models (Llama-2-70B-chat).
---

# "Yeah Right!" -- Do LLMs Exhibit Multimodal Feature Transfer?

## Quick Facts
- arXiv ID: 2501.04138
- Source URL: https://arxiv.org/abs/2501.04138
- Authors: Benjamin Reichman; Kartik Talamadupula
- Reference count: 8
- Primary result: Speech+text and conversation-tuned LLMs outperform their text-only counterparts in detecting covert deceptive communication, suggesting multimodal feature transfer

## Executive Summary
This paper investigates whether large language models can transfer multimodal features learned from speech or conversation data to improve text-only tasks, specifically detecting covert deceptive communication like sarcasm, irony, and condescension. The authors compare four model variants across four datasets: speech+text models (GPT-4o) against text-only models (GPT-4-Turbo), and conversation-tuned models (Llama-2-70B-conversational) against standard text models (Llama-2-70B-chat). Using different prompt types, they find that speech+text models achieve 2.2% better accuracy and conversation-tuned models achieve 12.7% better accuracy than their counterparts. These results suggest that multimodal features learned from non-text modalities can transfer to improve performance on purely textual tasks.

## Method Summary
The study employs a comparative approach using four model variants: GPT-4o (speech+text), GPT-4-Turbo (text-only), Llama-2-70B-conversational (conversation-tuned), and Llama-2-70B-chat (standard text). Four datasets containing covert deceptive communication are used to test model performance. Four prompt types are evaluated: basic, speech-features, conversational-features, and chain-of-thought. Models are evaluated on accuracy, precision, and recall metrics. The key comparison is between models trained on multimodal or conversation data versus their text-only counterparts to determine if features learned from non-text modalities transfer to improve text-based deception detection tasks.

## Key Results
- Speech+text models (GPT-4o) outperform text-only models (GPT-4-Turbo) by 2.2% accuracy, 1.9% precision, and 3.3% recall on basic prompting
- Conversation-tuned models (Llama-2-70B-conversational) achieve 12.7% better accuracy and 11.8% better precision than standard models, though with lower recall
- The performance gains suggest that multimodal features learned from speech or conversation data transfer to text-only tasks for detecting covert deceptive communication

## Why This Works (Mechanism)
The study suggests that LLMs can leverage multimodal features learned during pretraining or fine-tuning to improve performance on text-only tasks. When models are exposed to speech or conversation data, they may learn subtle cues like tone, pacing, and conversational context that can inform their understanding of textual communication. These learned features appear to transfer to text-only tasks, allowing models to better detect nuanced forms of deceptive communication that might be missed by models trained only on text.

## Foundational Learning
- Multimodal pretraining: Why needed - Models learn representations from multiple data types; Quick check - Compare models with different multimodal pretraining strategies
- Feature transfer learning: Why needed - Understanding how knowledge transfers across modalities; Quick check - Analyze which specific features contribute to performance gains
- Deception detection in NLP: Why needed - Specialized task requiring nuanced understanding; Quick check - Validate results across diverse deception types
- Prompt engineering impact: Why needed - Different prompts may activate different capabilities; Quick check - Test prompt variations systematically

## Architecture Onboarding
**Component map:** Input -> Text/Multimodal Model -> Feature Extraction -> Deception Classification -> Output
**Critical path:** Data input → Model processing → Feature extraction → Classification → Performance evaluation
**Design tradeoffs:** Model complexity vs. performance gains, computational cost of multimodal processing vs. benefits
**Failure signatures:** Performance degradation on datasets with different communication styles, sensitivity to prompt variations
**First experiments:** 1) Test statistical significance of performance differences, 2) Conduct ablation studies to isolate feature contributions, 3) Validate generalizability across additional datasets

## Open Questions the Paper Calls Out
None

## Limitations
- Performance differences, while statistically present, are relatively modest (2.2% accuracy gain for speech+text models)
- Datasets used are not described in detail, raising questions about diversity and quality
- Larger performance gaps (12.7% accuracy) may reflect architectural differences beyond just multimodal feature transfer
- Paper lacks analysis of how different prompt strategies interact with multimodal capabilities

## Confidence
- Multimodal feature transfer exists in LLMs: Medium
- Speech features improve covert deception detection: Medium
- Conversation data enhances text-only deception detection: Medium
- Prompt engineering significantly impacts multimodal transfer: Low

## Next Checks
1. Conduct statistical significance testing on performance differences between model variants to determine if observed improvements exceed random variation
2. Perform ablation studies isolating the contribution of speech/conversation features versus other architectural differences (model size, fine-tuning objectives)
3. Test the models on additional datasets with varied domains and communication styles to assess generalizability of the multimodal transfer effects