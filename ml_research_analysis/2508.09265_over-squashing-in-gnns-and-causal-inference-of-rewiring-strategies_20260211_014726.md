---
ver: rpa2
title: Over-Squashing in GNNs and Causal Inference of Rewiring Strategies
arxiv_id: '2508.09265'
source_url: https://arxiv.org/abs/2508.09265
tags:
- over-squashing
- rewiring
- graph
- datasets
- node
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the over-squashing problem in Graph Neural
  Networks (GNNs), where long-range information is exponentially compressed during
  message passing, limiting expressivity. It proposes a topology-focused method to
  measure pairwise over-squashing using the decay rate of mutual sensitivity between
  node pairs, then extends this to four graph-level metrics (prevalence, intensity,
  variability, extremity).
---

# Over-Squashing in GNNs and Causal Inference of Rewiring Strategies

## Quick Facts
- **arXiv ID**: 2508.09265
- **Source URL**: https://arxiv.org/abs/2508.09265
- **Authors**: Danial Saber; Amirali Salehi-Abari
- **Reference count**: 40
- **Primary result**: Graph classification datasets suffer substantial over-squashing that rewiring mitigates; node classification shows minimal over-squashing where rewiring often increases it without improving performance

## Executive Summary
This paper addresses the over-squashing problem in Graph Neural Networks where long-range information becomes exponentially compressed during message passing. The authors propose a topology-focused measurement framework using decay rates of mutual sensitivity between node pairs, then evaluate four rewiring strategies' causal effects on mitigating over-squashing across graph and node classification benchmarks. The key finding is that rewiring effectiveness depends critically on baseline over-squashing severity—substantial in graph classification where it helps, minimal in node classification where it often harms. The study reveals that restrained rewiring (FoSR, BORF) correlates better with performance gains than aggressive approaches (DIGL), suggesting a trade-off between connectivity and structural preservation.

## Method Summary
The method quantifies over-squashing by computing the decay rate $k_{vu}$ of normalized sensitivity between node pairs using the graph's augmented adjacency matrix powers $\tilde{A}^\ell$ for depths $\ell \in [D, 2D-1]$. These pairwise decay rates are aggregated into four graph-level metrics: prevalence (fraction of positive decays), intensity (average decay magnitude), variability (standard deviation), and extremity (maximum decay). The causal effect of rewiring is measured using Individual Treatment Effect (ITE) for node classification and Average Treatment Effect (ATE) for graph classification by comparing these metrics before and after applying rewiring strategies like DIGL, FoSR, SDRF, GTR, and BORF. Performance correlations are assessed via Spearman correlation between treatment effects and accuracy improvements.

## Key Results
- Graph classification datasets exhibit substantial over-squashing with high prevalence and intensity metrics, while node classification datasets show minimal over-squashing
- Rewiring effectively mitigates over-squashing in graph classification but often increases it in node classification without performance gains
- DIGL shows strongest mitigation metrics but weakest performance correlation; FoSR and BORF show weaker mitigation but stronger performance correlations
- The causal framework confirms rewiring's effects are not merely correlational but mechanistically linked to over-squashing reduction

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Exponential decay rate of pairwise sensitivity serves as reliable over-squashing proxy
- **Mechanism**: Normalized Jacobian norm $\tilde{J}_\ell(v, u)$ approximated via $\tilde{A}^\ell$, with decay rate $k_{vu}$ from linear regression of $\ln \tilde{J}_\ell$ vs $\ell$. Positive $k_{vu}$ indicates information compression
- **Core assumption**: Linear GNN approximation isolates topological effects from model parameters
- **Evidence anchors**: [abstract] "topology-focused method... using the decay rate of mutual sensitivity"; [section 3.1] "A positive $k_{vu}$ indicates over-squashing"; [corpus] "On Vanishing Gradients..." supports decay modeling
- **Break condition**: If linear approximation fails or $R^2$ values are low, metric becomes unreliable noise

### Mechanism 2
- **Claim**: Rewiring mitigates over-squashing only when baseline is substantial (graph classification), fails when minimal (node classification)
- **Mechanism**: Rewiring alters edge sets to reduce commute time/effective resistance. In graph classification (high metrics), added connectivity relieves bottlenecks. In node classification (low metrics), disrupts local structure and creates new compression pathways
- **Core assumption**: Performance bottlenecked by long-range information flow requiring large receptive fields
- **Evidence anchors**: [abstract] "Rewiring is most beneficial when over-squashing is both substantial and corrected with restraint"; [section 4.3] "rewiring more often increases over-squashing... opposite of graph-classification trend"; [corpus] "Structural Invariance Matters" warns rewiring distorts topology-dependent inductive biases
- **Break condition**: If performance gains stem from regularization rather than information flow, causal link breaks

### Mechanism 3
- **Claim**: Aggressive rewiring maximizes mitigation metrics but fails to translate to performance gains; restrained rewiring correlates better with accuracy
- **Mechanism**: DIGL adds edges aggressively, drastically reducing over-squashing metrics (high negative ATE) but potentially erasing topological inductive biases or inducing over-smoothing. FoSR (moderate addition) shows stronger correlation between metric reduction and performance gain
- **Core assumption**: Over-smoothing and loss of local structure are negative side effects not captured by over-squashing metric
- **Evidence anchors**: [abstract] DIGL most effective at mitigation, FoSR/BORF stronger performance correlations; [section 4.2] "DIGL... fails to improve performance—likely because aggressive edge additions erase topological information"; [corpus] "GNNs Getting ComFy" suggests maximizing spectral gap isn't always beneficial for generalization
- **Break condition**: If specific GNN architecture (e.g., Transformers) is immune to over-smoothing, aggressive rewiring might be purely beneficial

## Foundational Learning

- **Concept**: **Jacobian Norm & Sensitivity**
  - **Why needed here**: Measurement framework rests on estimating how sensitive target node's representation is to source node's input features ($\partial h_v / \partial h_u$)
  - **Quick check question**: If Jacobian norm decays to near-zero as depth increases, what does that imply about information flow from node $u$ to $v$?

- **Concept**: **Causal Inference (ATE/ITE)**
  - **Why needed here**: Paper uses Average Treatment Effect to prove rewiring causes changes in over-squashing rather than just correlating with it
  - **Quick check question**: Why is "within-graph" design (comparing Graph $G$ to Rewired Graph $R(G)$) crucial for satisfying "Exchangeability" assumption in causal inference?

- **Concept**: **Graph Topology Metrics (Diameter, Curvature)**
  - **Why needed here**: Over-squashing is topological problem linked to graph diameter and curvature. Understanding these helps interpret why "long-range" dependencies are hard
  - **Quick check question**: Does increasing edges (rewiring) generally increase or decrease graph diameter, and how does that relate to over-squashing?

## Architecture Onboarding

- **Component map**: Graph $G=(V, E)$ -> Rewiring Layer (Optional) -> Metric Calculator -> Aggregator -> Causal Estimator
- **Critical path**: Calculation of decay rate $k_{vu}$ via linear regression on $\ln(\tilde{J}_\ell(v, u))$ against depth $\ell$. If regression is unstable, diagnostics fail
- **Design tradeoffs**:
  - Approximation Accuracy vs. Cost: Linear approximation $\tilde{A}^\ell$ is $O(n^3)$ (or better with sparsity) but ignores non-linearities
  - Mitigation vs. Smoothing: Aggressive rewiring (DIGL) solves bottlenecks but introduces over-smoothing. Conservative rewiring (FoSR) preserves structure but may under-mitigate
- **Failure signatures**:
  - Counter-Responsiveness: Rewiring increases over-squashing metrics (common in node classification)
  - Translation Gap: Large reduction in over-squashing (negative ATE) but zero or negative performance gain (common with DIGL)
- **First 3 experiments**:
  1. Baseline Diagnostic: Run decay rate calculator on dataset. If Prevalence/Intensity is "Low" (like Citeseer), do not apply rewiring
  2. Translation Check: If metrics are "High," apply FoSR and check correlation between metric reduction and validation accuracy (ensure not just over-smoothing)
  3. Ablation on Depth: Vary layer range $[D, 2D-1]$ for fitting decay rates to ensure exponential model fits well (high $R^2$) before trusting diagnosis

## Open Questions the Paper Calls Out

- **Open Question 1**: How does measurement framework perform when applied to dynamic rewiring methods that alter graph topology during training?
  - **Basis in paper**: [explicit] Authors list "extending experiments to dynamic rewiring methods" as future work
  - **Why unresolved**: Current study focuses exclusively on static pre-processing rewiring techniques (FoSR, DIGL, SDRF) and excludes methods that adapt topology dynamically
  - **What evidence would resolve it**: Empirical results showing ATE of dynamic methods (e.g., DREW) on proposed over-squashing metrics compared to static baselines

- **Open Question 2**: Does negative decay rate in sensitivity metric reliably distinguish mitigation of over-squashing from onset of over-smoothing?
  - **Basis in paper**: [explicit] Authors call for "exploring relationship between negative decay rates and over-smoothing"
  - **Why unresolved**: Study observed DIGL significantly reduced over-squashing metrics but failed to improve performance, hypothesizing heavy edge addition induced over-smoothing, but this link remains unquantified
  - **What evidence would resolve it**: Theoretical or empirical correlation showing how negative decay rates align with established over-smoothing metrics (e.g., Dirichlet energy) across different architectures

- **Open Question 3**: Can rewiring strategy explicitly designed to minimize proposed graph-level metrics (e.g., extremity) outperform existing heuristics?
  - **Basis in paper**: [explicit] Conclusion proposes "designing novel rewiring methods guided by our over-squashing measurement framework"
  - **Why unresolved**: Existing methods optimize proxies like effective resistance or curvature rather than direct decay rate metrics proposed in this work
  - **What evidence would resolve it**: Development and benchmarking of learning-based rewiring algorithm that directly minimizes sensitivity decay rates, demonstrating superior performance gains over FoSR or BORF

## Limitations

- Linear approximation of sensitivity decay via $\tilde{A}^\ell$ may not fully capture non-linear GNN dynamics, potentially limiting metric accuracy in deeper networks
- Causal framework assumes stable rewiring effects across random seeds, but performance variability isn't extensively quantified
- Task-specific performance gains may conflate over-squashing mitigation with regularization effects from edge addition

## Confidence

- **High**: Graph classification over-squashing is substantial and rewiring provides measurable mitigation (Section 4.2, 4.3 results are consistent across datasets)
- **Medium**: Node classification over-squashing is minimal and rewiring often increases it without performance gains (Citeseer, Cora results align with pattern)
- **Medium**: Correlation between restrained rewiring (FoSR, BORF) and performance gains is stronger than aggressive rewiring (DIGL), though mechanisms aren't fully isolated

## Next Checks

1. **Ablation on non-linear effects**: Compare sensitivity decay metrics computed from linear vs. non-linear GNN implementations to quantify approximation error
2. **Seed sensitivity analysis**: Run 5-fold experiments on same datasets with different random seeds to assess stability of rewiring performance effects
3. **Mechanistic isolation**: Apply FoSR-style moderate rewiring to graph classification dataset and measure both over-squashing metrics and over-smoothing indicators to disentangle effects