---
ver: rpa2
title: 'CEQuest: Benchmarking Large Language Models for Construction Estimation'
arxiv_id: '2508.16081'
source_url: https://arxiv.org/abs/2508.16081
tags:
- construction
- llms
- dataset
- cequest
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CEQuest, a benchmark dataset for evaluating
  large language models (LLMs) in construction estimation tasks, specifically construction
  drawing interpretation and quantity takeoff. The dataset includes 164 questions
  covering five subject areas, with both multiple-choice and true/false formats.
---

# CEQuest: Benchmarking Large Language Models for Construction Estimation

## Quick Facts
- arXiv ID: 2508.16081
- Source URL: https://arxiv.org/abs/2508.16081
- Reference count: 40
- Primary result: Even best-performing LLM (GPT-4.1) achieves only 75.37% accuracy on construction estimation tasks

## Executive Summary
This paper introduces CEQuest, a benchmark dataset for evaluating large language models (LLMs) in construction estimation tasks, specifically construction drawing interpretation and quantity takeoff. The dataset includes 164 questions covering five subject areas, with both multiple-choice and true/false formats. Five state-of-the-art LLMs—Gemma 3, Phi4, LLaVA, Llama 3.3, and GPT-4.1—were evaluated on accuracy, execution time, and model size. Results showed that even the best-performing model, GPT-4.1, achieved only 75.37% accuracy, highlighting the need for domain-specific knowledge integration. The authors also identified challenges in maintaining structured responses and domain-specific reasoning. CEQuest is open-sourced to support future research and development of specialized LLMs for the construction domain.

## Method Summary
The study evaluates five LLMs on CEQuest, a dataset of 164 construction estimation questions covering general knowledge, drawing element identification, drawing organization, spatial reasoning, and quantity takeoff. Questions are formatted as multiple-choice (62%) and true/false (38%). Models are queried via standardized prompts requiring single-letter answers, with accuracy computed as percentage correct across five runs. Open-source models are deployed via Ollama, while GPT-4.1 uses OpenAI API. The evaluation framework measures accuracy, execution time, and model size to assess performance gaps and domain-specific reasoning challenges.

## Key Results
- GPT-4.1 achieved highest accuracy at 75.37%, while open-source models ranged from 61.83% to 64.02%
- Phi4 (14B) outperformed LLaVA (34B), demonstrating that model scale alone doesn't guarantee superior performance
- LLMs frequently failed to apply construction-specific rounding conventions, rounding 5.33 cubic yards down to 5 instead of up to 6
- Format compliance issues occurred when models output full explanations instead of single letters
- Domain-specific knowledge gaps were evident in spatial reasoning and quantity takeoff calculations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Domain-specific benchmarks reveal performance gaps that general benchmarks fail to capture.
- Mechanism: CEQuest targets construction-specific reasoning (spatial interpretation, quantity takeoff conventions, cross-referencing drawings) that requires embedded domain knowledge absent from general pretraining corpora. General benchmarks like MMLU lack this specificity.
- Core assumption: Construction domain knowledge is not sufficiently represented in general LLM training data.
- Evidence anchors:
  - [abstract] "current LLMs exhibit considerable room for improvement, highlighting the importance of integrating domain-specific knowledge"
  - [Section 1] "general LLM benchmarks like MMLU [14] or HellaSwag [37] are insufficient for construction-specific tasks"
  - [corpus] Weak direct evidence—neighbor papers focus on other domains (quantum code, legal QA, safety), suggesting benchmark specialization is a cross-domain pattern but not directly validated for construction.

### Mechanism 2
- Claim: Larger model size does not guarantee superior accuracy on specialized tasks.
- Mechanism: Architecture and training data quality mediate performance. Phi4 (14B) outperformed LLaVA (34B), suggesting that parameter count alone is insufficient without domain-relevant training.
- Core assumption: Model architecture and training corpus composition influence domain performance more than scale alone.
- Evidence anchors:
  - [Section 3] "Phi-4 (14B) outperforms the larger LLaVA (34B) in terms of accuracy, demonstrating that factors such as model architecture and training data quality can be as important as model scale"
  - [Section 3] Table 2 shows Gemma3 (4B, 61.83%), Phi4 (14B, 64.02%), LLaVA (34B, 62.56%)
  - [corpus] No direct corpus evidence on construction-specific model scaling; neighbor papers do not address this.

### Mechanism 3
- Claim: Domain-specific rounding and estimation conventions are not captured by default LLM reasoning.
- Mechanism: Construction estimation requires rounding up (not nearest integer) to ensure material sufficiency. LLMs default to mathematical rounding, producing incorrect estimates that appear numerically plausible.
- Core assumption: Practical domain conventions are learned through exposure to domain-specific training data or explicit instruction.
- Evidence anchors:
  - [Section 3] "in Q23, the calculated volume of the concrete slab is 5.33 cubic yards... most LLMs incorrectly selected Option B (5 cy), favoring the closest integer. However, in professional construction estimating practice, such quantities are typically rounded up"
  - [Section 3] "This misunderstanding indicates a lack of embedded domain knowledge and practical reasoning"
  - [corpus] No corpus evidence directly addresses construction-specific rounding conventions.

## Foundational Learning

- Concept: **Quantity Takeoff Methodology**
  - Why needed here: CEQuest's "Quantity Takeoff and Estimating" subject area requires interpreting dimensional data and computing measurements; understanding rounding conventions is essential.
  - Quick check question: Given a slab volume of 5.33 cubic yards, should an estimator order 5, 5.33, or 6 cubic yards? (Answer: Round up to ensure sufficient material.)

- Concept: **Drawing View Types and Cross-Referencing**
  - Why needed here: The dataset tests spatial reasoning across plan, section, and detail views. Understanding how drawings relate is prerequisite to answering cross-referencing questions.
  - Quick check question: What information would you need to match a wall shown in plan view to its corresponding section detail? (Answer: Drawing reference symbols, scale, and section cut indicators.)

- Concept: **Structured Output Enforcement**
  - Why needed here: LLMs evaluated on CEQuest frequently failed to output only the option letter (e.g., "A" vs. "A. Room finish schedule"). Prompt engineering for constrained outputs is a practical skill.
  - Quick check question: If a model outputs "The answer is C because..." instead of just "C", how would you modify the prompt to enforce strict formatting? (Answer: Add explicit constraints: "Output only the single letter. No explanation.")

## Architecture Onboarding

- Component map:
  CEQuest dataset (JSON) -> Evaluation framework (OpenAI API/Ollama) -> Metrics computation (accuracy, time, size) -> Five subject categories (General Knowledge, Drawing Element Identification, Drawing Organization, Spatial Reasoning, Quantity Takeoff)

- Critical path:
  1. Clone CEQuest from GitHub (https://github.com/mlsysx/CEQuest)
  2. Load JSON dataset and configure LLM endpoints (OpenAI API or local Ollama)
  3. Run evaluation loop with constrained prompting (e.g., "Output only the letter")
  4. Parse responses, compute accuracy, log execution time

- Design tradeoffs:
  - Question format: Multiple-choice (62%) enables automated grading; True/False (38%) tests binary understanding but may introduce guessing bias
  - Model selection: Open-source models (Gemma, Phi4, LLaVA, Llama) allow local deployment but show 10-14% accuracy gap vs. GPT-4.1
  - Prompt strictness: Lenient prompts (allowing explanations) risk unparseable outputs; strict prompts may suppress useful reasoning

- Failure signatures:
  - Format deviation: Model outputs full answer text instead of letter (e.g., LLaVA on Q10)
  - Verbose reasoning without conclusion: Model explains but omits final choice (e.g., Llama 3.3 on Q25)
  - Domain-incorrect rounding: Selecting mathematically nearest integer instead of rounding up (e.g., Phi4, LLaVA, GPT-4.1 on Q23)

- First 3 experiments:
  1. Baseline replication: Run all five evaluated models on CEQuest to reproduce reported accuracy ranges (61.83%–75.37%) and validate your evaluation pipeline.
  2. Prompt engineering ablation: Test strict vs. lenient prompts (e.g., "Output only A, B, C, D, E, or F" vs. "Explain your reasoning then answer") to quantify format deviation rates.
  3. Domain-specific RAG augmentation: Retrieve construction estimation rules (e.g., rounding conventions) at inference time and measure accuracy improvement on Quantity Takeoff questions.

## Open Questions the Paper Calls Out

- To what extent can Retrieval-Augmented Generation (RAG) or fine-tuning close the performance gap between open-source models and proprietary models like GPT-4.1 on construction estimation tasks?
- Does multi-modal integration (processing actual visual drawings) significantly improve performance on construction interpretation tasks compared to the text-based methodology used in this pilot?
- Can embedding specific domain heuristics, such as industry-standard rounding rules for material waste, correct the systematic reasoning errors observed in quantity takeoff calculations?

## Limitations

- Dataset construction process lacks transparency in question generation methodology and validation against domain experts
- Accuracy gap between open-source models and GPT-4.1 is substantial but without detailed error analysis by question type
- Architectural vs. training data quality tradeoff lacks corpus evidence and broader model scaling comparisons

## Confidence

- High Confidence: Core observation that general-purpose LLMs struggle with construction-specific tasks is well-supported by the 75.37% maximum accuracy and documented format compliance failures
- Medium Confidence: Claim that domain-specific knowledge integration is essential relies on assumption that CEQuest questions genuinely require construction expertise beyond general reasoning
- Low Confidence: Assertion that larger models don't guarantee better performance based on only four open-source models and doesn't establish clear scaling law relationship

## Next Checks

1. Have three licensed construction estimators independently review 20 randomly selected CEQuest questions to confirm they require genuine domain knowledge beyond general reasoning abilities.

2. Categorize model errors by type (format compliance, domain reasoning, calculation) to determine whether the accuracy gap stems from fundamental knowledge gaps or technical compliance issues.

3. Implement retrieval-augmented generation with construction estimation guidelines (e.g., rounding conventions) and measure accuracy improvement specifically on Quantity Takeoff questions to validate the domain knowledge integration hypothesis.