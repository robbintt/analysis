---
ver: rpa2
title: 'Lost in Tokenization: Context as the Key to Unlocking Biomolecular Understanding
  in Scientific LLMs'
arxiv_id: '2510.23127'
source_url: https://arxiv.org/abs/2510.23127
tags:
- protein
- sequence
- context
- function
- biomolecular
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the fundamental challenge of integrating biomolecular
  sequences into Scientific Large Language Models (Sci-LLMs), identifying the "tokenization
  dilemma" where treating sequences as language destroys functional motifs or treating
  them as modality introduces semantic misalignment. The authors propose a context-driven
  approach that bypasses raw sequence interpretation by providing Sci-LLMs with high-level
  structured biological context derived from established bioinformatics tools like
  InterProScan and BLASTp.
---

# Lost in Tokenization: Context as the Key to Unlocking Biomolecular Understanding in Scientific LLMs

## Quick Facts
- arXiv ID: 2510.23127
- Source URL: https://arxiv.org/abs/2510.23127
- Reference count: 40
- Context-driven approaches outperform sequence-based methods for protein function prediction in Sci-LLMs

## Executive Summary
This paper addresses the fundamental challenge of integrating biomolecular sequences into Scientific Large Language Models (Sci-LLMs), identifying the "tokenization dilemma" where treating sequences as language destroys functional motifs or treating them as modality introduces semantic misalignment. The authors propose a context-driven approach that bypasses raw sequence interpretation by providing Sci-LLMs with high-level structured biological context derived from established bioinformatics tools like InterProScan and BLASTp. Through systematic evaluation on protein function prediction tasks, they demonstrate that context-only input consistently outperforms sequence-only and sequence+context configurations, with context-driven approaches achieving near-perfect functional separation (ARI 0.958) compared to sequence-as-language models (ARI 0.492-0.690) and sequence-as-modality models (ARI 0.809).

## Method Summary
The proposed approach treats Sci-LLMs as reasoning engines over expert knowledge rather than sequence decoders. The method extracts structured biological context from raw biomolecular sequences using established bioinformatics tools (InterProScan for protein families, BLASTp for similarity-based context), then provides this high-level context as input to Sci-LLMs. The framework bypasses the tokenization dilemma by avoiding raw sequence interpretation altogether. Evaluation was conducted on protein function prediction tasks using metrics including ARI (Adjusted Rand Index) for clustering quality, ROC-AUC for classification performance, and prompt retrieval success rates. The approach was tested across different protein families and compared against sequence-as-language and sequence-as-modality baselines.

## Key Results
- Context-only input outperforms both sequence-only and sequence+context configurations
- Context-driven approaches achieve ARI 0.958 for functional separation vs. 0.492-0.690 for sequence-as-language models
- Adding raw sequences to context degrades performance, suggesting sequences act as informational noise
- Superior robustness demonstrated compared to existing paradigms for protein function prediction

## Why This Works (Mechanism)
The approach works by leveraging the inherent semantic gap between natural language and biomolecular sequences. Traditional methods either tokenize sequences as language (destroying functional motifs) or treat them as modality (introducing semantic misalignment). The context-driven method circumvents this by providing structured biological knowledge extracted from sequences using established tools. This allows Sci-LLMs to reason over high-level biological concepts rather than struggling with raw sequence interpretation. The degradation when adding sequences to context suggests that raw sequences introduce noise that interferes with the reasoning process, confirming that Sci-LLMs are better suited for knowledge integration than sequence decoding.

## Foundational Learning

**Tokenization Dilemma**: The challenge of representing biomolecular sequences in LLMs where language-based tokenization destroys functional motifs and modality-based approaches create semantic misalignment. Needed because standard tokenization methods fail to preserve biological meaning. Quick check: Compare sequence preservation in language vs. modality tokenization approaches.

**Structured Biological Context**: High-level biological information (protein families, functional domains, similarity relationships) extracted from raw sequences using bioinformatics tools. Needed because it provides semantic meaning without raw sequence complexity. Quick check: Verify context extraction accuracy using established bioinformatics benchmarks.

**Bioinformatics Tool Integration**: Using tools like InterProScan and BLASTp to derive expert knowledge from sequences. Needed because these tools provide validated biological annotations. Quick check: Compare tool-derived context with gold standard annotations.

## Architecture Onboarding

**Component Map**: Biomolecular Sequence -> Bioinformatics Tools (InterProScan/BLASTp) -> Structured Context -> Sci-LLM Reasoning Engine

**Critical Path**: Context extraction -> Context encoding -> Sci-LLM processing -> Function prediction output

**Design Tradeoffs**: Context-only vs. sequence+context input (context-only performs better but loses sequence detail); choice of bioinformatics tools (tradeoff between comprehensiveness and computational cost); context representation format (structured vs. natural language encoding).

**Failure Signatures**: Poor performance when context extraction tools fail to identify relevant features; degradation when raw sequences are added to context; reduced effectiveness on novel protein families with limited existing annotations.

**First Experiments**: 1) Test context extraction accuracy on benchmark protein datasets; 2) Evaluate context-only vs. sequence+context performance on protein function prediction; 3) Measure robustness across diverse protein families with varying annotation quality.

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Evaluation focuses primarily on protein function prediction, leaving generalization to DNA/RNA analysis untested
- Comparison against domain-specific protein models (ESM-2, AlphaFold2) not extensively explored
- Claims about sequences acting as "informational noise" require further investigation of underlying mechanisms

## Confidence

**High confidence**: Context-only approaches outperforming sequence-based methods on protein function prediction
**Medium confidence**: Claims about sequences acting as "informational noise" - mechanism requires further investigation
**Medium confidence**: Generalization claims to novel protein families - demonstrated on limited examples

## Next Checks

1. Evaluate context-driven approaches on DNA/RNA sequence analysis tasks to assess cross-biomolecule generalization
2. Test robustness across diverse protein families including membrane proteins, intrinsically disordered proteins, and rare structural folds
3. Compare performance against domain-specific protein models (ESM, AlphaFold) in zero-shot and few-shot settings for structure prediction and function annotation tasks