---
ver: rpa2
title: 'Federated Distillation on Edge Devices: Efficient Client-Side Filtering for
  Non-IID Data'
arxiv_id: '2508.14769'
source_url: https://arxiv.org/abs/2508.14769
tags:
- data
- proxy
- conv2d
- client
- edgefd
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: EdgeFD introduces a resource-efficient federated distillation method
  for edge devices using KMeans-based density ratio estimation to filter in-distribution
  and out-of-distribution proxy data. By eliminating server-side filtering and reducing
  client-side complexity, EdgeFD achieves up to 99% accuracy on MNIST, 89% on FashionMNIST,
  and 86% on CIFAR10 across strong non-IID, weak non-IID, and IID data distributions.
---

# Federated Distillation on Edge Devices: Efficient Client-Side Filtering for Non-IID Data

## Quick Facts
- **arXiv ID**: 2508.14769
- **Source URL**: https://arxiv.org/abs/2508.14769
- **Reference count**: 27
- **Primary result**: EdgeFD achieves up to 99% accuracy on MNIST, 89% on FashionMNIST, and 86% on CIFAR10 across strong non-IID, weak non-IID, and IID distributions using only 20% proxy data and client-side KMeans-based filtering.

## Executive Summary
EdgeFD introduces a resource-efficient federated distillation method for edge devices using KMeans-based density ratio estimation (KMeans-DRE) to filter in-distribution (ID) and out-of-distribution (OOD) proxy data. By eliminating server-side filtering and reducing client-side complexity, EdgeFD achieves high accuracy across challenging non-IID scenarios while maintaining computational efficiency. The method replaces computationally expensive statistical density ratio estimators with a centroid-based approach that scales linearly with data dimensions, making it suitable for deployment on resource-constrained edge devices.

## Method Summary
EdgeFD implements a two-phase federated distillation workflow: initialization and training rounds. During initialization, clients contribute 20% of their private data as proxy data, aggregate it into a global proxy set, and train a KMeans model on their local data to learn centroid positions. In each training round, the server broadcasts proxy sample indices to clients, who use their local KMeans-DRE to filter ID/OOD proxy samples based on Euclidean distance to nearest centroid. Only ID predictions are uploaded to the server, which averages them to create a global soft label distribution. Clients then perform local training using knowledge distillation from the averaged logits combined with private data. The method handles heterogeneous client architectures and operates effectively under strong non-IID, weak non-IID, and IID data distributions.

## Key Results
- Achieves up to 99% accuracy on MNIST, 89% on FashionMNIST, and 86% on CIFAR10
- Maintains performance with only 20% proxy data contribution
- Outperforms state-of-the-art methods in challenging non-IID scenarios
- Reduces computational complexity from O(m³) to O(k·n·c·d) compared to KuLSIF-DRE

## Why This Works (Mechanism)

### Mechanism 1: KMeans-based Density Ratio Estimation (KMeans-DRE)
Replaces statistical density ratio estimators (e.g., KuLSIF) with a centroid-based KMeans approach, significantly reducing computational complexity while maintaining filtering accuracy for ID/OOD proxy data. The KMeans model learns K centroid positions from a client's private data distribution, and samples with distance below a threshold are classified as ID.

### Mechanism 2: Two-Stage Client-Side Filtering
Performs ID/OOD filtering entirely on the client side, eliminating server-side filtering latency and preventing misleading knowledge from corrupting the global model. Only predictions for ID samples are uploaded, removing the need for a second filtering stage.

### Mechanism 3: Efficient Knowledge Transfer via Minimal Proxy Data
Achieves effective collaborative learning by exchanging soft logits on a small, shared proxy dataset comprising only 20% of clients' private data. This decouples the learning process from sharing full model parameters, accommodating heterogeneous model architectures.

## Foundational Learning

- **Density Ratio Estimation (DRE)**: Needed to computationally determine if a proxy sample belongs to a client's local data distribution without access to the full private dataset. Quick check: How does KMeans-DRE's use of Euclidean distance to centroids differ from KuLSIF's use of kernel matrices for estimating density ratios?

- **Knowledge Distillation in Federated Settings**: Core method for collaborative learning, transferring "dark knowledge" via soft logits instead of raw gradients or parameters. Quick check: In EdgeFD, what acts as the "teacher" and what acts as the "student" during the local model update step?

- **Non-IID Data Challenges**: Primary problem EdgeFD solves; non-IID data causes client drift and degrades global model performance in traditional FL. Quick check: Why does sharing predictions on a common proxy dataset help mitigate the model divergence caused by non-IID data?

## Architecture Onboarding

- **Component map**: Clients (hold private data, local models, KMeans-DRE module) -> Proxy Dataset (aggregated from client contributions) -> Server (aggregates ID predictions, distributes averaged logits)
- **Critical path**: Initialize (clients contribute 20% data → proxy set, train KMeans) → Training Round (server broadcasts proxy indices → clients filter ID samples → clients upload ID logits → server averages logits → clients distill from average)
- **Design tradeoffs**: Higher ID threshold → more data shared but more OOD noise; more KMeans centroids (c) → better distribution capture but higher compute; larger proxy % → better potential coverage but higher privacy risk and bandwidth
- **Failure signatures**: OOD Leakage (accuracy degrades at high thresholds); Data Starvation (very low thresholds yield few aggregated predictions); Cluster Misconfiguration (using too few centroids on complex data reduces filter effectiveness)
- **First 3 experiments**:
  1. Threshold Sensitivity Scan: Run EdgeFD on MNIST/FashionMNIST (Strong Non-IID) while varying the ID threshold. Plot test accuracy vs. threshold.
  2. Proxy Data Ablation: Compare EdgeFD performance using 10%, 20%, 40%, and 60% proxy data on CIFAR-10 under weak non-IID conditions.
  3. Computational Micro-benchmark: Profile KMeans-DRE learning and estimation phases against KuLSIF-DRE on a resource-constrained device to verify complexity claims.

## Open Questions the Paper Calls Out

### Open Question 1
How does EdgeFD performance degrade when clients contribute non-uniform amounts of proxy data due to bandwidth or privacy constraints? The current evaluation assumes consistent 20% contributions, which doesn't reflect real-world scenarios with vastly different client data availability or willingness to share.

### Open Question 2
Can EdgeFD maintain its linear complexity and accuracy on complex datasets (e.g., CIFAR-100) without relying on pre-trained external feature extractors? The current dependency on transfer learning (ResNet18) shifts the computational burden rather than solving it natively on the edge.

### Open Question 3
What is the trade-off between model utility and privacy when replacing direct proxy data sharing with synthetic data generation or differential privacy? The authors currently use subsets of private data as proxy, creating a privacy vulnerability. It's unclear if KMeans-DRE filtering logic would function effectively on synthetic/noisy data distributions.

## Limitations

- Performance on high-dimensional data depends critically on pre-trained feature extractors
- Assumes clients can contribute consistent proxy data portions, which may not reflect real-world constraints
- Requires careful threshold calibration to balance data starvation against OOD noise inclusion

## Confidence

- KMeans-DRE computational efficiency claim: **High** (complexity analysis is explicit and theoretically sound)
- 20% proxy data sufficiency: **Medium** (supported by ablation in paper but not independently verified in corpus)
- Elimination of server-side filtering benefits: **Medium** (mechanism is clear but end-to-end system benefits not benchmarked against alternatives)

## Next Checks

1. **Hyperparameter Sensitivity**: Systematically vary local epochs, learning rate, and distillation loss weight across all three datasets to identify stable performance regions and confirm reported accuracy ranges.

2. **Proxy Data Sufficiency Test**: Replicate the proxy data ablation study (10%, 20%, 40%, 60%) on CIFAR-10 under weak non-IID conditions to independently verify the 20% sufficiency claim.

3. **True Edge Deployment Benchmark**: Profile EdgeFD's memory and computation time on a Raspberry Pi or similar constrained device, comparing against a KuLSIF-DRE baseline to validate the claimed efficiency gains in practice.