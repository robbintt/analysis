---
ver: rpa2
title: Do Latent Tokens Think? A Causal and Adversarial Analysis of Chain-of-Continuous-Thought
arxiv_id: '2512.21711'
source_url: https://arxiv.org/abs/2512.21711
tags:
- reasoning
- coconut
- latent
- tokens
- experiments
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the reliability of latent reasoning tokens
  in the COCONUT framework, finding that they function as uninterpretable placeholders
  rather than encoding genuine reasoning. The authors conduct steering experiments,
  where perturbations to COCONUT tokens show minimal impact on model predictions (less
  than 5% perturbation success rate) compared to explicit CoT tokens (up to 50% success
  rate).
---

# Do Latent Tokens Think? A Causal and Adversarial Analysis of Chain-of-Continuous-Thought

## Quick Facts
- arXiv ID: 2512.21711
- Source URL: https://arxiv.org/abs/2512.21711
- Authors: Yuyi Zhang; Boyu Tang; Tianjie Ju; Sufeng Duan; Gongshen Liu
- Reference count: 17
- Primary result: COCONUT latent tokens function as uninterpretable placeholders rather than encoding genuine reasoning

## Executive Summary
This paper investigates whether latent reasoning tokens in the COCONUT framework encode genuine reasoning processes or merely serve as uninterpretable placeholders. Through a series of causal and adversarial analyses, the authors demonstrate that COCONUT tokens have minimal impact on model predictions when manipulated, in stark contrast to explicit chain-of-thought tokens. The study reveals that COCONUT models exploit dataset shortcuts and superficial patterns rather than engaging in true reasoning, particularly in multiple-choice and open-ended tasks. These findings challenge the interpretability of latent reasoning in COCONUT and suggest that the framework may mask shortcut learning behaviors while appearing structured.

## Method Summary
The authors conduct a comprehensive evaluation of COCONUT's reasoning capabilities through three main experimental approaches. First, they perform steering experiments where they perturb COCONUT tokens and explicit CoT tokens to measure their impact on model predictions, finding that COCONUT perturbations have minimal success rates (<5%) compared to CoT perturbations (up to 50%). Second, they conduct token-swapping experiments to assess the influence of individual tokens on outputs, demonstrating that COCONUT accuracy remains stable after swapping while CoT accuracy drops substantially. Third, they design shortcut experiments across multiple tasks to identify whether COCONUT models exploit dataset biases, revealing strong shortcut dependence in tasks like CSQA and CSQA-OE. The experiments are conducted primarily using FLAN-T5 XXL as the base model.

## Key Results
- COCONUT token perturbations show minimal impact on predictions (less than 5% success rate) compared to explicit CoT tokens (up to 50% success rate)
- Token-swapping experiments confirm COCONUT tokens have limited influence on outputs, with accuracy remaining stable while CoT accuracy drops substantially
- Shortcut experiments reveal COCONUT models exploit dataset biases and superficial patterns rather than true reasoning, particularly in multiple-choice and open-ended tasks

## Why This Works (Mechanism)
COCONUT's latent reasoning tokens function as uninterpretable placeholders because the model learns to associate these continuous representations with correct answers through pattern matching rather than genuine reasoning. The framework's architecture allows the model to exploit dataset shortcuts by encoding superficial features into these latent tokens, which appear structured but lack meaningful semantic content. When perturbed, these tokens fail to disrupt the model's predictions because they don't encode the causal reasoning steps that explicit CoT tokens represent. The model's reliance on shortcuts rather than reasoning is revealed through systematic adversarial testing that shows minimal performance degradation when COCONUT tokens are manipulated.

## Foundational Learning

**Chain-of-Thought reasoning** - Explicit step-by-step reasoning demonstrated in model outputs
*Why needed:* Provides baseline for comparing reasoning quality against latent approaches
*Quick check:* Can you distinguish between explicit reasoning steps and final answer generation?

**Latent variable models** - Models that encode intermediate reasoning as continuous vectors rather than discrete tokens
*Why needed:* COCONUT uses continuous latent tokens instead of explicit reasoning
*Quick check:* How do continuous representations differ from discrete token sequences?

**Adversarial testing** - Systematic perturbation of model inputs to test robustness and reasoning quality
*Why needed:* Essential method for revealing whether models rely on genuine reasoning vs. shortcuts
*Quick check:* What perturbation strategies would effectively test reasoning robustness?

**Dataset shortcut exploitation** - Models learning superficial patterns that correlate with correct answers without understanding
*Why needed:* Central finding that COCONUT relies on shortcuts rather than reasoning
*Quick check:* How can you distinguish between pattern matching and genuine reasoning?

## Architecture Onboarding

**Component map:** Input -> Explicit CoT Generator -> COCONUT Encoder -> Latent Tokens -> Answer Decoder -> Output
**Critical path:** Input → Encoder → Latent Tokens → Decoder → Output (reasoning path)
**Design tradeoffs:** COCONUT trades interpretability for potentially more efficient reasoning representation, but this paper shows the efficiency gain comes at the cost of genuine reasoning
**Failure signatures:** Minimal prediction changes when latent tokens are perturbed, accuracy stability under token swapping, strong dependence on dataset shortcuts
**First experiments:** 1) Compare steering success rates between COCONUT and explicit CoT perturbations 2) Measure accuracy changes after systematic token swapping 3) Test model performance on adversarially modified datasets designed to break shortcut patterns

## Open Questions the Paper Calls Out
None

## Limitations
- Experiments focus primarily on FLAN-T5 XXL, limiting generalizability across different architectures and scales
- Perturbation experiments don't fully establish whether alternative latent representations could encode meaningful reasoning
- Shortcut analysis identifies exploitation but doesn't conclusively prove absence of any latent reasoning capabilities
- The distinction between genuine reasoning and effective shortcut learning remains philosophically and empirically challenging

## Confidence

**High confidence:** The empirical findings that COCONUT token perturbations have minimal impact on predictions compared to explicit CoT tokens

**Medium confidence:** The conclusion that COCONUT models exploit dataset shortcuts rather than reasoning, given the consistent shortcut vulnerability across multiple tasks

**Medium confidence:** The interpretation that COCONUT tokens function as uninterpretable placeholders rather than encoding genuine reasoning

## Next Checks

1. Replicate the steering and token-swapping experiments across multiple model families (e.g., Llama, GPT, Claude) to test architecture dependence

2. Design ablation studies that systematically remove shortcut features while preserving task-relevant information to better isolate reasoning vs. pattern-matching

3. Conduct human evaluation studies where annotators assess the plausibility of reasoning in COCONUT outputs versus explicit CoT to complement the quantitative analysis