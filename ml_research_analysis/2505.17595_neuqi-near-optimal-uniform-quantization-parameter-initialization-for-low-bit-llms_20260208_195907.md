---
ver: rpa2
title: 'NeUQI: Near-Optimal Uniform Quantization Parameter Initialization for Low-Bit
  LLMs'
arxiv_id: '2505.17595'
source_url: https://arxiv.org/abs/2505.17595
tags:
- neuqi
- quantization
- gptq
- initialization
- uniform
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of efficient deployment of large
  language models (LLMs) on consumer-grade hardware by improving post-training quantization
  (PTQ) initialization. The authors identify limitations in the conventional Min-Max
  initialization formula, which restricts both the scale and zero-point parameters
  and forces the zero-point to be an integer.
---

# NeUQI: Near-Optimal Uniform Quantization Parameter Initialization for Low-Bit LLMs

## Quick Facts
- arXiv ID: 2505.17595
- Source URL: https://arxiv.org/abs/2505.17595
- Authors: Li Lin; Xinyu Hu; Xiaojan Wan
- Reference count: 40
- Primary result: NeUQI achieves superior perplexity and accuracy compared to existing PTQ initialization methods across various low-bit quantization settings and model families

## Executive Summary
This paper addresses the challenge of efficient deployment of large language models (LLMs) on consumer-grade hardware through improved post-training quantization (PTQ) initialization. The authors identify limitations in conventional Min-Max initialization that restrict both scale and zero-point parameters, forcing the zero-point to be an integer. They propose NeUQI, which simplifies joint optimization by deriving the zero-point for a given scale, reducing the problem to single-variable scale optimization. Experiments on LLaMA and Qwen model families show NeUQI consistently outperforms existing methods across various settings and tasks, achieving superior performance even compared to more resource-intensive fine-tuning methods.

## Method Summary
NeUQI addresses two key limitations in conventional Min-Max initialization: the coupling of scale and zero-point to extreme values, and the integer constraint on zero-point. The method reformulates quantization to allow floating-point zero-points and derives the optimal zero-point for a fixed scale analytically, reducing joint optimization to single-variable scale optimization. This is achieved through piecewise quadratic optimization with O(n log n) complexity using transition-point reduction, combined with coarse-to-fine scale search to reduce computational cost. The approach is integrated with GPTQ-style sequential layer quantization using diagonal Hessian approximation for activation importance weighting.

## Key Results
- NeUQI achieves 17.14 perplexity on WikiText2 and 17.50 on C4 for LLaMA-2-7B with 2-bit channel-wise quantization, compared to 2592.02/2592.02 for Min-Max
- Outperforms existing PTQ methods (GPTQ, AWQ, AQT, EFB) across multiple model families (LLaMA, Qwen) and quantization schemes
- Combined with lightweight distillation, NeUQI surpasses PV-tuning while using 2000x fewer training tokens
- Floating-point zero-points add only ~0.11 bits average overhead while providing significant accuracy gains

## Why This Works (Mechanism)

### Mechanism 1: Floating-Point Zero-Point Relaxation
Allowing zero-point to take floating-point values rather than being restricted to k-bit integers improves quantization quality at negligible memory cost. The conventional Min-Max formula forces z to be a k-bit unsigned integer in [0, 2^k - 1]. NeUQI reformulates the quantization function to impose no integer constraint, enabling z to take any real value. This expands the searchable parameter space without changing the underlying quantization grid.

### Mechanism 2: Decoupled Scale-Zero-Point Optimization via Analytical Derivation
Deriving the optimal zero-point for a fixed scale analytically reduces joint optimization to single-variable scale optimization. Given the loss function L(s, z) = Σᵢ Hᵢᵢ(Q_{s,z}(wᵢ) - wᵢ)², fixing scale s makes L(z) a piecewise quadratic function with 2^k intervals per sample. Algorithm 1 efficiently finds the global minimum by tracking transition points and updating the quadratic incrementally, achieving O(n log n) complexity.

### Mechanism 3: Coarse-to-Fine Scale Search with Bounded Search Space
Coarse-to-fine search over scale candidates reduces computational cost while maintaining near-optimal solutions. Rather than exhaustively evaluating all T=2048 scale candidates, NeUQI performs coarse search with T_c = √T candidates to localize the optimal region, then refines within a neighborhood. This reduces zero-point computations from O(T) to O(√T).

## Foundational Learning

- **Uniform (Asymmetric Affine) Quantization**: LLMs use this quantization scheme where quantized values are q_i = s·(i - z). Why needed: NeUQI optimizes parameters (scale s, zero-point z) for this specific scheme.
  - Quick check: Given s=0.5, z=2.5, and k=2 bits, what is the quantized value for index i=1?

- **Hessian-Based Loss for Quantization**: The loss function uses H = E[X^T X] (proxy Hessian) to weight quantization error by activation importance, following GPTQ's formulation. Why needed: Enables importance-aware quantization that minimizes impact on model performance.
  - Quick check: Why does the diagonal Hessian approximation simplify the loss to a sum of per-weight terms?

- **Piecewise Quadratic Functions**: Understanding that L(z) consists of quadratic segments separated by transition points enables the efficient O(n log n) optimization. Why needed: Critical for understanding why Algorithm 1 achieves near-optimal results efficiently.
  - Quick check: For a 2-bit quantizer, how many transition points does a single sample's loss function L_i(z) have?

## Architecture Onboarding

- **Component map**: Calibration data (128 C4 samples) -> Hessian computation -> Scale search -> Zero-point optimization -> Weight quantization -> Sequential layer processing

- **Critical path**: 
  1. Compute diagonal Hessian H from calibration activations
  2. For each quantization group: run scale search → zero-point optimization loop
  3. Quantize weights using final (s*, z*)
  4. Proceed to next layer with updated activations from quantized preceding layers

- **Design tradeoffs**:
  - **Accuracy vs. Speed**: Larger T (scale candidates) improves search quality but increases runtime linearly without coarse-to-fine
  - **Memory vs. Flexibility**: Floating-point zero-points add ~0.11 bits average overhead; double quantization can mitigate
  - **Approximation vs. Precision**: Full Hessian is more accurate but computationally prohibitive; diagonal is practical

- **Failure signatures**:
  - **Exploded perplexity (>1000)**: Scale search range may be insufficient; check if Min-Max upper bound is reasonable
  - **No improvement over Min-Max**: Verify diagonal Hessian is being computed correctly; check transition-point reduction isn't over-pruning
  - **Runtime exceeds baseline**: Coarse-to-fine not activating; verify T_c is set to √T (e.g., T=2048, T_c=64)

- **First 3 experiments**:
  1. **Sanity check**: Quantize LLaMA-2-7B W2 channel-wise with NeUQI vs. Min-Max; expect perplexity drop from ~2592 to ~18 on C4
  2. **Ablation on zero-point constraint**: Compare NeUQI vs. Int-Search (integer-constrained variant) on same setting; expect ~4-5 point accuracy gain from floating-point z
  3. **Distillation integration**: Apply lightweight distillation (256 samples, 1 epoch) to NeUQI-initialized model; expect to match or exceed PV-tuning with 2000x fewer tokens

## Open Questions the Paper Calls Out

### Open Question 1
Can fine-tuning strategies be developed that explicitly operate beyond the first Min-Max constraint (extreme-value dependency) to further improve post-initialization performance under uniform quantization?
Basis: The conclusion states: "For future work, we will explore fine-tuning strategies beyond the first constraint to further enhance post-initialization performance under uniform quantization." This remains unresolved because subsequent fine-tuning stages in methods like EfficientQAT may still implicitly rely on constraint-based formulations.

### Open Question 2
Does the diagonal Hessian approximation cause significant suboptimality compared to using the full Hessian matrix for zero-point and scale optimization?
Basis: Section 4.1 notes that "incorporating [the full Hessian] into optimization is often hard to handle in practice" and adopts the diagonal approximation without empirical comparison. The performance gap from this approximation remains unknown without direct comparison.

### Open Question 3
Can NeUQI be effectively combined with non-uniform quantization schemes (e.g., vector quantization, learnable codebooks) to achieve further compression beyond uniform quantization limits?
Basis: The paper focuses exclusively on uniform quantization and notes in Section 3.1 that the formulation "imposes no integer constraint on z," but does not explore whether the optimization framework could extend to non-uniform representations. Non-uniform quantization methods operate under fundamentally different constraints.

## Limitations

- **Floating-point zero-point deployment reality**: Benefits depend on hardware/software support for floating-point zero-points, which may not be universally available in existing quantized inference stacks
- **Diagonal Hessian approximation validity**: The decoupling assumption may introduce systematic bias for certain architectural patterns or extremely wide layers where cross-weight interactions are significant
- **Limited architectural diversity**: Experiments focus primarily on decoder-only transformers, leaving uncertainty about performance on encoders, hybrids, or architectures with different attention patterns

## Confidence

- **High confidence** in the core technical contribution: The analytical derivation of optimal zero-point for fixed scale and the resulting O(n log n) complexity improvement are mathematically sound and empirically validated
- **Medium confidence** in deployment benefits: While accuracy improvements are clear in controlled settings, practical advantages depend on factors not fully explored (kernel support, memory constraints, inference engine compatibility)
- **Medium confidence** in distillation integration claims: The lightweight distillation results are promising but the methodology details are limited, and comparisons to other fine-tuning methods would strengthen the argument

## Next Checks

1. **Deployment stack validation**: Implement NeUQI initialization in a real quantized inference pipeline (e.g., vLLM or TensorRT-LLM) and measure actual memory bandwidth, latency, and accuracy on consumer hardware, comparing against Min-Max initialization under identical deployment conditions

2. **Architectural generalization study**: Apply NeUQI to diverse model architectures including BERT-style encoders, T5 hybrids, and models with rotary embeddings to measure whether initialization quality degrades for architectures with different weight distributions than LLaMA/Qwen decoders

3. **Scaling behavior analysis**: Systematically evaluate NeUQI's relative performance across model scales (1B→70B) and bit-widths (2→8 bits) to identify where the floating-point zero-point advantage is most pronounced versus where integer constraints become less limiting