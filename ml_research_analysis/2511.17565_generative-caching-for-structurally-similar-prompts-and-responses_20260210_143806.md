---
ver: rpa2
title: Generative Caching for Structurally Similar Prompts and Responses
arxiv_id: '2511.17565'
source_url: https://arxiv.org/abs/2511.17565
tags:
- cache
- prompt
- prompts
- response
- program
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GenCache is a generative caching technique for structurally similar
  prompts that identifies reusable response patterns and synthesizes variation-aware
  responses. It clusters prompts by structural similarity, generates programs encoding
  common response patterns, and validates them before caching.
---

# Generative Caching for Structurally Similar Prompts and Responses

## Quick Facts
- arXiv ID: 2511.17565
- Source URL: https://arxiv.org/abs/2511.17565
- Reference count: 40
- Key outcome: GenCache achieves 83% cache hit rate with minimal incorrect hits on datasets without prompt repetition, improving cache hit rate by ~20% in agentic workflows and reducing end-to-end execution latency by ~34% compared to standard prompt matching.

## Executive Summary
GenCache is a generative caching technique for structurally similar prompts that identifies reusable response patterns and synthesizes variation-aware responses. It clusters prompts by structural similarity, generates programs encoding common response patterns, and validates them before caching. On a cache hit, it executes the cached program to produce customized responses. The system achieves high cache hit rates while minimizing incorrect hits through a dual-embedding clustering approach and program validation pipeline.

## Method Summary
GenCache operates by clustering structurally similar prompts using combined prompt and response embeddings, then synthesizing executable Python programs that generalize the prompt-to-response mappings. When a new prompt arrives, it's embedded and matched to existing clusters using similarity thresholds. If a match is found, a cached program is executed locally to generate the response. The system validates programs before caching using a separate LLM to ensure correctness, with reflection-based retries for failed validations. The approach targets agentic workflows where responses follow predictable patterns that can be encoded as programs.

## Key Results
- Achieves 83% cache hit rate with minimal incorrect hits on datasets without prompt repetition
- Improves cache hit rate by ~20% in agentic workflows compared to standard prompt matching
- Reduces end-to-end execution latency by ~34% compared to standard prompt matching
- Saves 35-73% in LLM token usage compared to no caching

## Why This Works (Mechanism)

### Mechanism 1: Dual-Embedding Structural Clustering
Clustering prompts by combining prompt and response embeddings improves cluster quality over prompt-only similarity. For each incoming prompt-response pair, compute prompt embeddings via SentenceTransformer and response embeddings by averaging embeddings of JSON value fields. Assign to clusters only if both similarity scores exceed thresholds (Tp=0.8, Tr=0.75). Final cluster selection maximizes combined similarity. Core assumption: System messages in prompts dominate embeddings and cause false similarity matches; response structure provides discriminative signal. Evidence anchors: Section 3.2 notes considering both similarities ensures more accurate clustering; Table 5 shows moderate thresholds yield 79-84% hit rates.

### Mechanism 2: Program Synthesis via In-Context Pattern Extraction
An LLM (CodeGenLLM) can synthesize executable Python programs that generalize prompt-to-response mappings from few exemplars. Provide CodeGenLLM with ν in-context examples (default ν=4) from a cluster. The LLM identifies static phrases (a1, a2), variable extraction patterns via regex (f(x)), and response templates (b1, b2). Output program takes arbitrary prompts as command-line input and produces formatted responses. Core assumption: Structurally similar prompts exhibit consistent transformation patterns expressible as regex extraction and template filling. Evidence anchors: Section 3.3 shows GenCache automatically identifies patterns from multiple examples using regex matching.

### Mechanism 3: Validation-Gated Caching with Reflection Retry
Pre-cache validation using a separate LLM (ValidLLM) filters unreliable programs, and reflection-based retry recovers from initial failures. ValidLLM receives program-generated responses and ground-truth exemplars, returning boolean correctness per example. If <γ (default 50%) match, retry with reflection—incorporating ValidLLM's failure justifications into CodeGenLLM prompt. Cap retries at ρ=30. Core assumption: ValidLLM can reliably detect semantic equivalence between program output and expected responses. Evidence anchors: Figure 3 illustrates validation workflow; Section 4.3 shows only 10.4% of FLASH cache creation attempts produced valid caches.

## Foundational Learning

- **Concept: Structural vs. Semantic Similarity**
  - Why needed here: GenCache's core innovation is recognizing that exact and semantic matching fail for prompts with identical structure but different parameters (e.g., "buy AAA batteries" vs. "buy USB-C cable").
  - Quick check question: Given prompts "Diagnose connection loss in useast1" and "Diagnose connection loss in uswest2," would GPTCache return a correct cached response? (Answer: Likely yes on similarity, but wrong if it returns verbatim response without region adaptation.)

- **Concept: In-Context Learning with Pattern Induction**
  - Why needed here: CodeGenLLM must infer regex patterns from exemplars without fine-tuning.
  - Quick check question: Why does increasing ν improve program quality? (Answer: More examples reduce overfitting to idiosyncrasies and reveal consistent patterns across variations.)

- **Concept: ReAct-Style Prompting**
  - Why needed here: GenCache targets agentic workflows where responses are actions (JSON) rather than free-form text.
  - Quick check question: Why does GenCache disable caching for rationale generation in Laser? (Answer: Rationales are structurally diverse and lack predictable response patterns.)

## Architecture Onboarding

- **Component map:**
  API Interface -> Cluster Database -> Cache Store -> CodeGenLLM -> ValidLLM -> Runtime (Python Interpreter)

- **Critical path:**
  1. New prompt arrives → embedding generation (~42ms)
  2. Cluster similarity search → identify nearest cluster (~9ms)
  3. Regex validation → check structural conformity
  4. If hit: retrieve program, execute locally (~64ms total hit latency)
  5. If miss: LLM call (~3.5s), store in cluster database, trigger background cache generation if ν threshold met

- **Design tradeoffs:**
  - ν (exemplars): Lower = faster cache creation but noisier patterns; higher = better generalization but delayed caching
  - γ (validation threshold): Lower = more caches stored but higher negative hit rate; higher = fewer caches but better precision
  - Tp/Tr thresholds: Lower = larger clusters with diverse prompts; higher = smaller, purer clusters with lower hit rates
  - ρ (retry cap): Higher = more robust programs but increased cost

- **Failure signatures:**
  - Overfitted regex: Program hardcodes item names from exemplars; fails on new prompts. Detection: regex patterns containing specific values rather than capture groups
  - Multi-sentence fragmentation: Cached regex like `r'buy|need|purchase|get (item name)'` misfires on "want headphone. need it in black" → returns "it in black"
  - Validation false positives: ValidLLM approves semantically mismatched responses; downstream task fails
  - Cluster drift: New prompts assigned to stale cluster with outdated program; regex validation fails → cache miss despite cluster match

- **First 3 experiments:**
  1. **Param-Only baseline**: Run 10,000 synthetic prompts with only parameter variations (item, price). Expect ~98% hit rate with <2% negative hits. Verify ν=4 yields stable programs by comparing against ν=10.
  2. **Threshold sweep**: Vary Tp ∈ {0.75, 0.8, 0.85} and Tr ∈ {0.7, 0.75, 0.85} on Param-w-Synonym dataset. Measure hit rate vs. negative hit rate tradeoff. Identify optimal operating point.
  3. **Feedback integration test**: Implement GenCache-feedback where downstream errors trigger cache deletion. Run on WebShop with Laser agent for 200 requests. Measure reduction in negative hits vs. hit rate loss.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can GenCache be adapted to maintain high hit rates for structurally diverse prompts where sentence syntax varies significantly?
- Basis in paper: The authors explicitly state future work includes "supporting structurally diverse prompts," noting that current regex-based caching fails when syntax changes (e.g., hit rate drops to 4.23% on structural datasets).
- Why unresolved: The current architecture relies on regex patterns derived from consistent sentence structures; it cannot generalize when the order of entities (like item and price) changes.
- What evidence would resolve it: A mechanism that maintains >80% hit rates on datasets with high structural variance (like the "Structural" dataset in the appendix) without sacrificing correctness.

### Open Question 2
- Question: How can the system ensure safety and correctness for non-reversible agent workflows where an incorrect cache hit executes a damaging action?
- Basis in paper: The paper explicitly identifies "non-reversible agent workflows" as a limitation and area for future work, restricting current applicability to reversible tasks like adding items to a cart.
- Why unresolved: The current validation allows for small error rates (e.g., 1-2% negative hits), which are acceptable only if the action can be undone.
- What evidence would resolve it: A validation or confidence-scoring layer that reduces incorrect actions to near-zero or a rollback mechanism for irreversible operations.

### Open Question 3
- Question: What adaptive strategies can dynamically modify cached programs to accommodate new logical branches in incoming prompts?
- Basis in paper: The Appendix notes that GenCache "may fail when the number of branches is large" and explicitly calls for an "adaptive strategy to modify the cached program dynamically whenever a new branch is detected."
- Why unresolved: Currently, programs are generated based on fixed exemplars; if a prompt requires a new `if-else` branch not present in the initial cluster, the cached program fails.
- What evidence would resolve it: A system that successfully updates a cached Python program in real-time to handle a new conditional logic path without requiring a full cache regeneration.

## Limitations
- High validation failure rates: Only 10.4% of FLASH cache creation attempts produced valid caches, with 66% of Laser attempts failing validation entirely
- Corpus diversity concerns: All evaluation datasets share structural similarities (JSON responses, parameterized templates), may not generalize to more diverse response types
- Resource overhead: Requires multiple LLM invocations per cache creation (CodeGenLLM, ValidLLM, reflection retries up to ρ=30)

## Confidence
- **High confidence** in the core mechanism: Dual-embedding clustering and program synthesis via in-context learning are technically sound approaches that align with established LLM capabilities
- **Medium confidence** in the claimed performance: While cache hit rates of 83% are reported, high validation failure rates and lack of negative hit rate reporting for agentic workflows suggest actual effectiveness may be lower
- **Low confidence** in generalizability: Paper doesn't adequately address performance on more diverse prompt types, different response formats, or complex response patterns

## Next Checks
1. **Negative hit rate validation**: Re-run the agentic workflow experiments (Laser on WebShop) and explicitly measure negative hit rates across different γ thresholds to reveal the true tradeoff between hit rate and correctness
2. **Response diversity stress test**: Create a benchmark dataset with increasingly complex response structures (from simple JSON to nested branching logic with multiple conditions) to measure how program synthesis success rates degrade as complexity increases
3. **Cost-benefit analysis**: Calculate the total computational cost (LLM tokens, API calls, execution time) of the full GenCache pipeline versus the savings from reduced LLM invocations, including the cost of failed cache creation attempts and reflection retries