---
ver: rpa2
title: 'CASTELLA: Long Audio Dataset with Captions and Temporal Boundaries'
arxiv_id: '2511.15131'
source_url: https://arxiv.org/abs/2511.15131
tags:
- audio
- castella
- captions
- dataset
- moments
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CASTELLA, a large-scale human-annotated dataset
  for audio moment retrieval (AMR), addressing the lack of reliable real-world benchmarks
  in the field. The dataset consists of 1,862 audio recordings (1-5 minutes long)
  with 3,881 captions and temporal boundaries, making it 24 times larger than the
  previous AMR dataset.
---

# CASTELLA: Long Audio Dataset with Captions and Temporal Boundaries

## Quick Facts
- arXiv ID: 2511.15131
- Source URL: https://arxiv.org/abs/2511.15131
- Authors: Hokuto Munakata; Takehiro Imamura; Taichi Nishimura; Tatsuya Komatsu
- Reference count: 0
- Primary result: CASTELLA is 24× larger than prior AMR datasets, and synthetic pre-training + fine-tuning improves Recall1@0.7 by 10.4 points

## Executive Summary
This paper introduces CASTELLA, a large-scale human-annotated dataset for audio moment retrieval (AMR), addressing the lack of reliable real-world benchmarks in the field. The dataset consists of 1,862 audio recordings (1-5 minutes long) with 3,881 captions and temporal boundaries, making it 24 times larger than the previous AMR dataset. The authors establish a baseline AMR model using DETR-based architectures and demonstrate that fine-tuning on CASTELLA after pre-training on synthetic data significantly improves performance (10.4 points in Recall1@0.7) compared to training solely on synthetic data. The experiments also show that model architectures effective in video moment retrieval perform well in AMR, though short moments remain challenging. The dataset is publicly available and includes both global and local captions with precise temporal boundaries.

## Method Summary
The authors construct CASTELLA by collecting audio from YouTube via the AudioCaps subset, downsampling to 32 kHz, and applying a 1-second sliding window for feature extraction. Local and global captions are collected through crowdsourcing, with temporal boundaries annotated at 1-second resolution. The dataset is split into 1009/213/640 train/valid/test recordings. For evaluation, DETR-based architectures (QD-DETR, Moment-DETR, UVCOM) are fine-tuned on CASTELLA after pre-training on the synthetic Clotho-Moment dataset. The model uses CLAP embeddings for audio and text, with DETR predicting temporal boundaries from learned queries.

## Key Results
- CASTELLA contains 1,862 audio recordings (1-5 minutes each) with 3,881 captions and temporal boundaries
- Fine-tuning on CASTELLA after pre-training on Clotho-Moment improves Recall1@0.7 by 10.4 points compared to training solely on synthetic data
- DETR-based architectures effective in video moment retrieval transfer well to AMR, with UVCOM achieving the best performance (R1@0.7=20.3)
- Short moments (<10 seconds) remain challenging, with near-zero performance in R1@0.5

## Why This Works (Mechanism)

### Mechanism 1: Synthetic pre-training + fine-tuning
Pre-training on synthetic audio data followed by fine-tuning on real-world annotated data improves audio moment retrieval performance more than either training strategy alone. Synthetic data provides large-scale exposure to diverse audio-text patterns and overlay combinations, establishing general cross-modal alignment. Fine-tuning on CASTELLA then adapts representations to natural audio distributions, real acoustic complexity, and human annotation patterns.

### Mechanism 2: DETR architecture transfer
DETR-based architectures effective for video moment retrieval transfer well to audio moment retrieval with feature encoder substitution. The Detection Transformer's object detection framework (encoder-decoder with learned queries) models temporal relationships and predicts boundary segments regardless of modality. Replacing visual features with CLAP-extracted audio features preserves the temporal reasoning machinery while adapting to acoustic input.

### Mechanism 3: Sliding window feature extraction
Sliding window feature extraction with CLAP enables processing of long audio while maintaining temporal localization capability. CLAP encodes audio-text similarity at the clip level. A 1-second sliding window with 1-second hop converts variable-length audio (1-5 minutes) into fixed-dimension frame-level features. The DETR decoder then attends across these frames to predict moment boundaries.

## Foundational Learning

- **Audio Moment Retrieval (AMR) vs. Sound Event Detection (SED)**: AMR handles long audio (1-5 minutes), free-form text queries, and variable-length moments—fundamentally different from SED's short clips and fixed class labels. Quick check: Given a 3-minute podcast, would you use SED or AMR to find "the segment where two speakers start arguing"? (Answer: AMR—SED outputs class labels, not temporal segments from natural language.)

- **Detection Transformer (DETR) for Temporal Localization**: DETR uses learned object queries to predict variable numbers of bounding boxes; in AMR, these become temporal moment predictions. Quick check: Why can DETR predict a variable number of moments without explicit moment count prediction? (Answer: Learned queries compete via attention; confidence scores filter low-quality predictions.)

- **CLAP (Contrastive Language-Audio Pretraining)**: CLAP provides the shared audio-text embedding space that enables text queries to retrieve semantically similar audio segments. Quick check: If CLAP was trained only on 10-second clips, how might it perform on 3-minute audio? (Answer: May struggle with long-range temporal dependencies; sliding window mitigates but doesn't fully solve this.)

## Architecture Onboarding

- **Component map**: Raw Audio (1-5 min) → Resample to 32kHz → Sliding Window (1s window, 1s hop) → CLAP Audio Encoder → Frame-level features → Text Query → CLAP Text Encoder → DETR Encoder (cross-attention: audio + text) → DETR Decoder (learned moment queries) → Moment predictions (start, end, confidence)

- **Critical path**: 
  1. Audio preprocessing (resampling, windowing)—incorrect settings propagate through entire pipeline
  2. CLAP feature extraction—must match pretraining distribution (32kHz, expected normalization)
  3. DETR fine-tuning—learning rate and early stopping critical; paper used 1e-4, batch 32, early stopping on validation

- **Design tradeoffs**: 
  - Window size: Larger windows capture more context but reduce temporal precision; 1s chosen for balance
  - Synthetic vs. real training: Synthetic provides scale; real provides distribution alignment. Paper recommends both.
  - Architecture selection: UVCOM best but most complex; QD-DETR offers simplicity-accuracy tradeoff

- **Failure signatures**:
  - Short moment underperformance: R1@0.5 for moments <10 seconds is near zero. If your use case involves brief events (e.g., door slams, notification sounds), current architecture will struggle.
  - Visual bias leakage: Despite review process, captions may describe sounds inferred from visual context. Listen to audio without video to verify caption validity.
  - Overlapping moment confusion: Multiple local captions can describe overlapping segments; model may merge or incorrectly separate them.

- **First 3 experiments**:
  1. Reproduce baseline: Train QD-DETR on Clotho-Moment only, then fine-tune on CASTELLA training split. Verify R1@0.7 improves by ~10 points. This validates your data pipeline.
  2. Short moment analysis: Filter test set for moments <10 seconds and >30 seconds. Compare performance gap. If gap is smaller than reported, your preprocessing may differ.
  3. Architecture sweep: Train UVCOM, Moment-DETR, and QD-DETR with identical hyperparameters on CASTELLA. Confirm UVCOM > QD-DETR > Moment-DETR ranking holds, validating that VMR architectural insights transfer.

## Open Questions the Paper Calls Out

### Open Question 1
How can model architectures be improved to accurately retrieve short audio moments (less than 10 seconds)? The current paper shows that accurately retrieving short moments remains a significant challenge, with near-zero performance in R1@0.5 for moments under 10 seconds. This is particularly problematic since short moments are statistically dominant in the dataset.

### Open Question 2
Can CASTELLA support the simultaneous localization and captioning of salient events in a single model? The current paper only establishes baselines for the retrieval task (moment detection given a query), not the generative task of describing detected moments. Developing technologies for simultaneous localization and captioning would be a valuable extension.

### Open Question 3
What methods are effective for retrieving specific moments from a collection of multiple long audio recordings? The current experimental setup and evaluation metrics are designed for searching within a single known audio recording rather than across a database. New evaluation pipelines are needed to demonstrate retrieval across large corpora of audio files.

## Limitations
- The synthetic pre-training data (Clotho-Moment) requires external access for validation
- The 1-second sliding window resolution may be suboptimal for applications requiring sub-second temporal precision
- Performance on short moments (<10 seconds) is significantly worse than longer moments, limiting applicability to brief audio events
- The review process did not explicitly confirm captions were generated without visual information leakage

## Confidence

- **High confidence**: Dataset construction methodology and size claims (1,862 recordings, 3,881 captions, 24× larger than prior AMR dataset). The R1@0.7 improvement of 10.4 points from synthetic pre-training + fine-tuning on CASTELLA is directly supported by experimental results.
- **Medium confidence**: The architectural transfer claim that VMR-effective models (UVCOM, QD-DETR) perform well in AMR. While the experimental ranking is provided, direct comparison with VMR literature requires verification.
- **Medium confidence**: The claim that 1-second resolution is sufficient for real-world AMR applications. This is stated but not empirically validated against higher-resolution alternatives.

## Next Checks

1. Verify caption quality by listening to random audio samples without visual context to detect visual information leakage that may have occurred during annotation.
2. Conduct an ablation study on window resolution by extracting features with 0.5-second windows and comparing performance to the reported 1-second results.
3. Evaluate model performance specifically on moments <10 seconds using R1@0.5 and R1@0.7 to quantify the short-moment limitation and determine if it affects your target use case.