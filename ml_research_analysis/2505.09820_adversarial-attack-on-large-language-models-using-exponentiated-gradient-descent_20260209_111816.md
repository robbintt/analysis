---
ver: rpa2
title: Adversarial Attack on Large Language Models using Exponentiated Gradient Descent
arxiv_id: '2505.09820'
source_url: https://arxiv.org/abs/2505.09820
tags:
- arxiv
- adversarial
- language
- gradient
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of jailbreaking aligned large
  language models (LLMs) through adversarial attacks. The authors propose a novel
  attack method based on exponentiated gradient descent (EGD) that directly optimizes
  the relaxed one-hot encoding of adversarial suffixes, avoiding the need for extrinsic
  projection techniques.
---

# Adversarial Attack on Large Language Models using Exponentiated Gradient Descent

## Quick Facts
- arXiv ID: 2505.09820
- Source URL: https://arxiv.org/abs/2505.09820
- Reference count: 40
- Key outcome: EGD achieves up to 60% attack success rate on Mistral-7B-v0.3 for MaliciousInstruct dataset, outperforming GCG, PGD, and SoftPromptThreats while being computationally faster

## Executive Summary
This paper introduces a novel adversarial attack method for jailbreaking aligned large language models using Exponentiated Gradient Descent (EGD). The approach directly optimizes relaxed one-hot encodings of adversarial suffixes on the probability simplex, eliminating the need for extrinsic projection techniques required by traditional methods like PGD. By enforcing constraints intrinsically through the simplex structure, EGD efficiently generates discrete adversarial tokens that can bypass safety alignment. Experiments on five open-source LLMs across four datasets demonstrate superior attack success rates compared to three state-of-the-art baselines while maintaining computational efficiency.

## Method Summary
The EGD method addresses LLM jailbreaking by optimizing adversarial suffixes appended to harmful prompts. It represents suffixes as continuous probability distributions over vocabulary tokens (relaxed one-hot encodings) and uses exponentiated gradient updates with multiplicative weights to traverse the probability simplex directly. The loss function combines cross-entropy with entropic regularization and KL-divergence terms to encourage discrete token selection. After optimization (~200 epochs), the continuous representation is discretized via argmax to produce the final adversarial suffix. The method is validated using dual model-based evaluators and shows improved attack success rates across multiple LLMs and datasets.

## Key Results
- EGD achieves attack success rates up to 60% for Mistral-7B-v0.3 on MaliciousInstruct dataset
- Outperforms GCG, PGD, and SoftPromptThreats baselines across all five tested LLMs
- Demonstrated computational efficiency with log-runtime complexity compared to baselines
- Consistent performance across four datasets: AdvBench, HarmBench, JailbreakBench, and MaliciousInstruct

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** EGD maintains valid token constraints intrinsically through probability simplex optimization, eliminating external projection layers
- **Mechanism:** Multiplicative update rule (Ẋt = PKL(Ẋt-1 ⊙ exp(-η∇F))) followed by row normalization keeps relaxed one-hot matrix on probability manifold without L2 projection
- **Core assumption:** Multiplicative updates effectively traverse LLM token probability landscape without getting trapped at constraint boundaries
- **Evidence anchors:** Abstract states "enforcing constraints intrinsically via probability simplex structure"; Section III.B/C shows update equations
- **Break condition:** Exploding gradients or high learning rates can push probabilities to zero, freezing token choices prematurely

### Mechanism 2
- **Claim:** Continuous relaxation enables simultaneous optimization of all tokens, reducing search complexity versus discrete substitution
- **Mechanism:** Suffix represented as continuous matrix where each position is probability distribution over vocabulary, allowing holistic evolution through gradient flow
- **Core assumption:** Best discrete token reliably recoverable from continuous relaxation via argmax
- **Evidence anchors:** Abstract mentions "directly optimizes relaxed one-hot encoding"; Section III.A defines Ẋ ∈ [0,1]^(L×|T|)
- **Break condition:** If relaxation remains soft with evenly split probabilities, discretization introduces significant error causing attack failure

### Mechanism 3
- **Claim:** Entropic regularization and KL-divergence force relaxed probabilities to become peaky, ensuring close mapping to discrete tokens
- **Mechanism:** Loss augmented with -τH(X) and KL-divergence terms, penalizing uncertain distributions to push optimizer toward specific tokens
- **Core assumption:** High confidence in relaxed space correlates with lower loss in discrete space after projection
- **Evidence anchors:** Appendix A.2/A.3 defines modified loss; Figure 3 shows increased mean maximal probabilities with regularization
- **Break condition:** High τ reduces search space too aggressively, preventing optimal adversarial suffix discovery

## Foundational Learning

- **Concept: Probability Simplex**
  - **Why needed here:** Paper formulates attack as optimization constrained to probability simplex rather than Euclidean space; understanding sum-to-1 constraint is critical for grasping EGD vs standard GD
  - **Quick check question:** Does standard Gradient Descent guarantee vector remains on probability simplex?

- **Concept: One-Hot Encoding & Relaxation**
  - **Why needed here:** Core technical move treats discrete tokens (one-hot vectors) as continuous variables; must understand mapping between vocabulary index and vector representation
  - **Quick check question:** If relaxed one-hot vector is [0.1, 0.7, 0.2], what is resulting discrete token?

- **Concept: Auto-regressive LLM Loss (Cross-Entropy)**
  - **Why needed here:** Attack optimizes suffix to minimize loss associated with generating harmful target string; understanding gradient flow backward through model is essential
  - **Quick check question:** In this context, is optimizer trying to maximize or minimize likelihood of target harmful sequence?

## Architecture Onboarding

- **Component map:** User Prompt + Adversarial Suffix (relaxed one-hot) + Target String -> LLM Forward Pass (logits) -> Loss Calculation (Cross-Entropy + Entropy/KL) -> EGD Update (Multiplicative weights) + Adam Optimizer -> Bregman Projection (Row normalization) -> Discrete adversarial suffix (argmax)

- **Critical path:** Interaction between EGD Update and Bregman Projection is the critical novelty; if projection doesn't correctly normalize exponentiated gradients, representation ceases to represent valid probabilities

- **Design tradeoffs:**
  - Efficiency vs. Stability: EGD faster (log-runtime complexity) than GCG but requires careful Adam and learning rate tuning to stabilize multiplicative updates
  - Specificity: White-box attack requiring full model weight access, trading generality of black-box attacks for gradient-based optimization efficiency

- **Failure signatures:**
  - High Entropy Loop: Loss decreases but maximal probability per token remains low (< 0.3), making "best" token ambiguous and discretization yields garbage
  - Refusal Loop: Model loss decreases but output is repetitive refusal phrase, indicating target "harmful" string not effectively aligned with model's generation path

- **First 3 experiments:**
  1. Reproduce Convergence (Fig 1): Run EGD on Llama2-7B-chat with single harmful prompt to verify median cross-entropy loss drops to near 0 within 200 epochs
  2. Ablation on Regularization (Fig 3): Run optimization with/without entropic regularization term (τ) to observe difference in "mean maximal probabilities" (sparsity)
  3. Baseline Comparison (Table II): Compare ASR of EGD against standard PGD implementation on same prompt to verify "intrinsic constraint" advantage

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can adversarial suffixes generated by EGD transfer effectively across different LLM architectures?
- **Basis in paper:** Section VI states "we would like to prove transferability of our attack by optimizing adversarial tokens for specific harmful behavior on one model and applying them to attack a different one"
- **Why unresolved:** Current study focuses on white-box attacks with full weight access; cross-model transfer not tested
- **What evidence would resolve it:** Experiments showing successful jailbreaks on Model B using suffixes optimized exclusively on Model A

### Open Question 2
- **Question:** Does EGD method remain effective in black-box attack environments?
- **Basis in paper:** Section V notes "Whether this technique will be effective in a black-box attack environment is being investigated"
- **Why unresolved:** Method relies on calculating gradients directly from target model's weights, unavailable in black-box scenarios
- **What evidence would resolve it:** Successful jailbreak rates using EGD via API-only access or surrogate models without direct gradient access to target

### Open Question 3
- **Question:** Can theoretical convergence guarantees be established for EGD with Adam or non-smooth models?
- **Basis in paper:** Remark 1 notes Theorem 1 requires differentiable loss with Lipschitz gradient, "not satisfied for... ReLU-based models" or Adam variant used
- **Why unresolved:** Provided convergence proof applies only to smooth models using standard EGD, excluding Adam optimizer and ReLU activations
- **What evidence would resolve it:** Formal proof extending convergence to Adam update rule or empirical analysis confirming optimization stability on ReLU-based LLMs

## Limitations

- Reproducibility Constraints: Exact implementation details for evaluator prompt templates, exponential annealing schedule for τ, and target sequence formatting not fully specified in paper
- Black-box Generalization: Requires white-box access to model weights and gradients, limiting practical applicability to real-world scenarios with only API access
- Dataset Specificity: Attack success rates vary significantly across datasets (up to 60% for Mistral on MaliciousInstruct), suggesting method effectiveness may be highly prompt-dependent

## Confidence

- **High Confidence:** EGD achieves higher attack success rates than baselines (GCG, PGD, SoftPromptThreats) well-supported by experimental results in Table II with robust dual evaluator validation
- **Medium Confidence:** Claim about EGD's computational efficiency being superior to GCG supported by Figure 2 but needs more detailed runtime analysis across different suffix lengths
- **Low Confidence:** Assertion that EGD "avoids need for extrinsic projection techniques" while being "computationally faster" requires more nuanced analysis as Bregman projection is still a projection step

## Next Checks

1. **Baseline Implementation Verification:** Implement PGD baseline with identical hyperparameters and evaluation protocol to verify EGD's superiority is due to intrinsic constraint mechanism rather than implementation advantages

2. **Black-box Transferability Assessment:** Test whether adversarial suffixes generated by EGD on one model maintain effectiveness when applied to different models without gradient access, quantifying transfer rates across five model families

3. **Regularization Ablation Study:** Systematically vary entropic regularization coefficient τ across multiple orders of magnitude to quantify impact on both attack success rate and computational convergence time, determining optimal schedule and sensitivity