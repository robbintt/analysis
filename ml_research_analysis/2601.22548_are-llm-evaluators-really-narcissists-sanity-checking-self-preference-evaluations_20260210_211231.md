---
ver: rpa2
title: Are LLM Evaluators Really Narcissists? Sanity Checking Self-Preference Evaluations
arxiv_id: '2601.22548'
source_url: https://arxiv.org/abs/2601.22548
tags:
- judge
- self-preference
- bias
- quality
- response
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates self-preference bias in LLM evaluators,
  where models favor their own outputs when judging. The authors identify a key methodological
  confound: evaluator uncertainty on hard problems can inflate self-preference measurements.'
---

# Are LLM Evaluators Really Narcissists? Sanity Checking Self-Preference Evaluations

## Quick Facts
- **arXiv ID:** 2601.22548
- **Source URL:** https://arxiv.org/abs/2601.22548
- **Reference count:** 28
- **Key outcome:** Evaluator Quality Baseline reduces measured self-preference by 89.6% on average across 37,448 queries

## Executive Summary
This paper challenges the widespread assumption that large language models (LLMs) exhibit genuine self-preference bias when evaluating their own outputs. The authors demonstrate that much of the observed self-preference in prior studies stems from evaluator uncertainty on difficult problems rather than true narcissism. By introducing an Evaluator Quality Baseline that compares preferences for a judge's own incorrect responses against preferences for equally incorrect responses from other models, they reveal that only 51% of initial self-preference findings remain statistically significant after correction. The study shows that LLM evaluators' uncertainty on hard problems inflates self-preference measurements, and their proposed method enables cleaner measurement of true self-preference signals.

## Method Summary
The authors developed an Evaluator Quality Baseline to address the confound of evaluator uncertainty in self-preference measurements. This baseline compares a judge's preference for its own incorrect responses against preferences for equally incorrect responses from other models, effectively measuring whether judges show self-preference even when wrong. The methodology was tested across 37,448 queries spanning 16 different models and 9 datasets, providing robust statistical power to validate the approach. The baseline calculation isolates the self-preference signal by controlling for the uncertainty inherent in judging difficult problems.

## Key Results
- Measured self-preference bias reduced by 89.6% on average after applying Evaluator Quality Baseline
- Only 51% of initial self-preference findings retain statistical significance after correction
- 44% of cases show no or negative self-preference after baseline adjustment
- Results validated across 37,448 queries, 16 models, and 9 datasets

## Why This Works (Mechanism)
The methodology works by recognizing that evaluator uncertainty on difficult problems creates artificial inflation in self-preference measurements. When judges struggle to distinguish between incorrect responses of similar quality, they may show spurious preference for their own outputs. The Evaluator Quality Baseline controls for this by measuring self-preference specifically when the judge is wrong, revealing whether the bias persists even in cases of clear error. This isolates the true self-preference signal from uncertainty-driven artifacts.

## Foundational Learning

**LLM Evaluation Fundamentals** - Understanding how models assess outputs quality and preference judgments
*Why needed:* Forms the basis for understanding self-preference bias mechanisms
*Quick check:* Can you explain the difference between preference judgments and absolute quality scoring?

**Statistical Significance Testing** - Methods for determining whether observed biases are meaningful
*Why needed:* Critical for validating whether self-preference signals persist after correction
*Quick check:* Can you calculate confidence intervals for preference proportions?

**Experimental Design in AI Research** - Principles for controlling confounds in model evaluation studies
*Why needed:* Essential for understanding how uncertainty affects measurement validity
*Quick check:* Can you identify potential confounds in a given evaluation setup?

## Architecture Onboarding

**Component Map:** Evaluator Models -> Query Generation -> Response Collection -> Preference Judgment -> Baseline Calculation -> Statistical Analysis

**Critical Path:** Query Generation → Response Collection → Preference Judgment → Baseline Calculation → Statistical Analysis

**Design Tradeoffs:** The study prioritizes measurement accuracy over simplicity, accepting increased computational complexity through the baseline calculation to achieve more reliable self-preference measurements.

**Failure Signatures:** Over-reliance on self-preference measurements without uncertainty correction, misinterpretation of preference judgments on difficult problems, and failure to account for response quality equivalence when calculating baselines.

**First 3 Experiments:**
1. Implement the Evaluator Quality Baseline on a small dataset to verify baseline calculation mechanics
2. Test baseline effectiveness on synthetic data with known self-preference properties
3. Apply baseline to human evaluators to establish ground truth comparison

## Open Questions the Paper Calls Out
None

## Limitations
- Assumes removing evaluator uncertainty through baseline fully addresses self-preference bias
- Methodology depends on availability of multiple incorrect responses of similar quality
- Focus on preference judgments limits generalizability to other evaluation metrics

## Confidence

**High Confidence:**
- Statistical methodology and baseline construction are sound with robust testing across 37,448 queries
- Multiple datasets and models provide strong validation

**Medium Confidence:**
- Interpretation that reduced self-preference primarily reflects evaluator uncertainty
- Generalizability across different types of evaluation tasks given focus on preference judgments only

## Next Checks
1. Test the Evaluator Quality Baseline across additional evaluation metrics beyond preference judgments
2. Conduct ablation studies varying difficulty distribution of evaluation tasks
3. Implement cross-validation using human evaluators on subset of tasks for ground truth comparison