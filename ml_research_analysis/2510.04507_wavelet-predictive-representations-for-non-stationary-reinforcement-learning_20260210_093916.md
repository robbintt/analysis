---
ver: rpa2
title: Wavelet Predictive Representations for Non-Stationary Reinforcement Learning
arxiv_id: '2510.04507'
source_url: https://arxiv.org/abs/2510.04507
tags:
- wavelet
- task
- non-stationary
- tasks
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Wavelet Predictive Representations for Non-Stationary Reinforcement Learning

## Quick Facts
- arXiv ID: 2510.04507
- Source URL: https://arxiv.org/abs/2510.04507
- Authors: Min Wang; Xin Li; Ye He; Yao-Hui Li; Hasnaa Bennis; Riashat Islam; Mingzhong Wang
- Reference count: 34
- Primary result: Achieves average success rate of 95.4% on Meta-World ML10 benchmark

## Executive Summary
This paper introduces WISDOM, a novel approach for non-stationary reinforcement learning that leverages wavelet transforms to track and predict task evolution in multi-task RL environments. The key innovation is a wavelet representation network that performs multi-scale decomposition of latent task representations, capturing both global trends and fine-grained variations. The method combines discrete wavelet transforms with temporal difference learning to create a wavelet TD operator that theoretically guarantees convergence. Experimental results demonstrate significant improvements over state-of-the-art methods across Meta-World, MuJoCo, and Type-1 Diabetes environments.

## Method Summary
WISDOM is a three-module architecture built on context-based meta-RL. First, an MLP context encoder maps transitions to a latent task representation z. Second, a wavelet representation network Y_φ applies discrete wavelet transform (DWT) using learnable filters to decompose z into approximation (y₀) and detail (y₁) coefficients across M levels, then reconstructs a refined representation ẑ. Third, SAC-based policy and critics operate on (s, ẑ). The model is trained jointly with four losses: KL loss for encoder, wavelet TD loss (contraction mapping proof), autoregressive loss for stability, and SAC losses. Hyperparameters include z_dim=5, M=2-5 depending on task, and α_Y balancing TD vs AR loss.

## Key Results
- Achieves 95.4% average success rate on Meta-World ML10 benchmark, outperforming PEARL (90.2%) and VariBAD (85.6%)
- Demonstrates superior adaptation to both slow global trends and rapid task variations in Walker-Vel and Hopper-Vel environments
- Shows stable performance across decomposition levels M=2-5 with diminishing returns beyond M=3

## Why This Works (Mechanism)

### Mechanism 1: Multi-Scale Wavelet Decomposition for Task Evolution Tracking
Wavelet transforms separate task evolution trends from fine-grained variations more effectively than time-domain or Fourier-only methods. The discrete wavelet transform iteratively decomposes task representation sequences via learnable low-pass (y₀) and high-pass (y₁) filters. Approximation coefficients capture global trends while detail coefficients capture rapid local changes. Selective downsampling of detail coefficients preserves rapid task changes while removing noise. This works because task evolution in NSRL exhibits multi-scale temporal patterns—both slow global trends and rapid local variations that single-resolution methods cannot capture.

### Mechanism 2: Wavelet Temporal Difference Update Operator
A TD-style update in the wavelet domain provides stable convergence for tracking MDP evolution. The wavelet TD operator F_W(z_t) = z_t + ΓW(z_{t+1}) where W is the low-pass filter network is proven to be a contraction mapping with γ ∈ [0,1), guaranteeing convergence via Banach fixed-point theorem. This works because wavelet features W(z_t) satisfy Bellman-like structure similar to successor features, allowing TD-style updates to track MDP evolution effectively.

### Mechanism 3: Wavelet-Domain Policy Performance Bounds
Policy performance differences are bounded by wavelet-domain feature differences, providing theoretical grounding. Under Taylor-expandable reward R(z), the sup-norm difference of wavelet transforms of z^(b) sequences bounds |J_π1 - J_π2|. This works because the wavelet representation captures the essential information about task evolution that affects policy performance, making it an informative indicator of policy quality.

## Foundational Learning

- **Concept: Discrete Wavelet Transform (DWT)**
  - Why needed here: Core transformation enabling multi-resolution analysis of task evolution sequences
  - Quick check question: Given sequence [2, 4, 6, 8], compute one level of Haar decomposition (approximation: averages, detail: differences)

- **Concept: Contraction Mapping**
  - Why needed here: Required to understand Theorem 1's convergence guarantee for wavelet TD operator
  - Quick check question: If T satisfies ||T(x) - T(y)|| ≤ γ||x - y|| with γ = 0.9, why does iterating T converge to a unique fixed point?

- **Concept: Context-Based Meta-RL**
  - Why needed here: WISDOM builds on PEARL-style frameworks where latent task representations z are inferred from trajectory context
  - Quick check question: In context-based meta-RL, what does the latent variable z represent and how is it used by π(a|s, z)?

## Architecture Onboarding

- **Component map**: Context Encoder (e_η) -> Wavelet Representation Network (Y_φ) -> W Network -> Policy π_θ, Critics Q_υ

- **Critical path**: 1) Encode transitions → z sequence via e_η 2) Transform z → wavelet domain via Y_φ (M decomposition levels) 3) Apply y₀ (low-pass) and y₁ (high-pass) with downsampling 4) Transform coefficients back → restored representation ẑ 5) Condition π(a|s, ẑ); optimize jointly: KL + Wavelet TD + AR + SAC losses

- **Design tradeoffs**: Decomposition level M: Higher captures longer trends but risks losing low-freq info. Paper uses M=2–5 by environment. Coefficient α_Y: Balances TD vs AR loss (0.1–0.9). Filter initialization: Haar wavelets; learnable filters adapt with "relaxed orthogonality."

- **Failure signatures**: Representation collapse when clustering complex distributions; error accumulation with AR-only training (no wavelet TD); over-decomposition losing vital low-frequency components; variable-wise encoding (VWE) disrupts cross-variate dependencies

- **First 3 experiments**: 1) Ablate Y network and individual losses to isolate contributions 2) Sweep M ∈ {1, 2, 3, 4, 5} on Walker-Vel 3) Vary non-stationarity degree (T - T_h)/T

## Open Questions the Paper Calls Out

- **Open Question 1**: How can adaptive mechanisms be developed to dynamically adjust decomposition levels (M) in the wavelet representation network to eliminate the need for environment-specific hyperparameter tuning? The paper notes that performance is sensitive to these levels and requires manual tuning based on specific non-stationary environments.

- **Open Question 2**: What strategies can be employed for the effective selection of detail coefficients (g_m) to maximize rapid adaptation to fine-grained task variations? While the paper downsamples and preserves detail coefficients, the optimal method for filtering or selecting these coefficients remains unoptimized.

- **Open Question 3**: Does the theoretical performance bound derived in Theorem 2 hold when the reward function is non-smooth or discontinuous, violating the Taylor expansion assumption? The paper does not analyze the tightness of the performance bound when reward landscapes lack required smoothness properties.

- **Open Question 4**: To what extent does the performance of WISDOM depend on the choice of the initial wavelet basis function (e.g., Haar) versus the learned adaptive kernels? The paper initializes with Haar wavelets but does not ablate sensitivity to this initialization.

## Limitations

- The method requires careful tuning of decomposition level M for different environments, with no adaptive mechanism for automatic adjustment
- Performance bounds rely on Taylor-expandable reward functions, which may not hold for sparse or discontinuous rewards common in RL
- Implementation details remain incomplete, particularly regarding downsampling mechanisms for detail coefficients and W network architecture specifications

## Confidence

- **High Confidence**: The discrete wavelet transform implementation using learnable filters is technically sound and the multi-scale decomposition approach is well-established for time series analysis
- **Medium Confidence**: The contraction mapping proof for the wavelet TD operator appears mathematically valid but its practical convergence behavior in non-stationary RL remains unproven
- **Low Confidence**: The policy performance bounds claim lacks empirical validation and the specific conditions under which the Taylor expansion assumption holds are unclear

## Next Checks

1. Implement the wavelet TD operator with varying γ values (0.5, 0.9, 0.99) and measure convergence behavior on a controlled non-stationary task with known evolution patterns

2. Test the AR loss effectiveness by training with and without wavelet TD loss on a simple non-stationary environment, monitoring representation variance across distinct tasks to detect collapse

3. Verify the downsampling mechanism by implementing the "most recent detail coefficients" preservation and measuring information retention across decomposition levels M ∈ {1, 2, 3, 4, 5}