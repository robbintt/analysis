---
ver: rpa2
title: "Curi\xF3-Edu 7B: Examining Data Selection Impacts in LLM Continued Pretraining"
arxiv_id: '2512.12770'
source_url: https://arxiv.org/abs/2512.12770
tags:
- curi
- data
- pretraining
- language
- portuguese
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work investigates continued pretraining as a strategy for\
  \ adapting large language models to underrepresented languages, focusing on Portuguese.\
  \ By leveraging the ClassiCC-PT corpus, the authors train two 7-billion-parameter\
  \ models derived from LLaMA-2: Curi\xF3 7B on 100 billion uncurated tokens and Curi\xF3\
  -Edu 7B on a 10 billion token subset filtered for educational and STEM content."
---

# Curió-Edu 7B: Examining Data Selection Impacts in LLM Continued Pretraining

## Quick Facts
- arXiv ID: 2512.12770
- Source URL: https://arxiv.org/abs/2512.12770
- Authors: Thales Sales Almeida; Rodrigo Nogueira; Hélio Pedrini
- Reference count: 28
- One-line result: Curió-Edu 7B trained on 10B filtered tokens outperforms full 100B token model across all PoETa V2 categories

## Executive Summary
This work investigates continued pretraining as a strategy for adapting large language models to underrepresented languages, focusing on Portuguese. By leveraging the ClassiCC-PT corpus, the authors train two 7-billion-parameter models derived from LLaMA-2: Curió 7B on 100 billion uncurated tokens and Curió-Edu 7B on a 10 billion token subset filtered for educational and STEM content. Despite using only 10% of the data and 20% of the computation, Curió-Edu 7B outperforms the full-corpus model in evaluations on PoETa V2, a comprehensive benchmark for Portuguese language tasks. The performance advantage is consistent across multiple domains, including reasoning, ethics, and general knowledge, not just the STEM-aligned tasks expected from the filtering. This suggests that high-quality, targeted data can be more effective than larger, uncurated datasets, particularly for models with limited prior exposure to the target language. The benefits are more pronounced at larger model scales, with smaller models showing more mixed results, likely due to representational constraints.

## Method Summary
The authors continue pretraining from LLaMA-2 7B using the ClassiCC-PT corpus (120B Portuguese tokens from Common Crawl). They filter this corpus using an educational/STEM classifier with threshold ≥2.5, yielding ~10B tokens. Two models are trained: Curió 7B on 100B uncurated tokens and Curió-Edu 7B on 10B filtered tokens for 2 epochs (20B tokens seen). Training uses TPU v2-256 with T5x framework, Adafactor optimizer (lr=10⁻³, cosine decay), sequence length 4096, and batch size 256. Models are evaluated on PoETa V2 benchmark (40+ tasks) using Normalized Preferred Metric (NPM).

## Key Results
- Curió-Edu 7B achieves ~28% relative gain over LLaMA 2 using only 0.01% of the pretraining tokens
- Despite 5× less data, Curió-Edu 7B outperforms Curió 7B across all PoETa V2 categories including non-STEM tasks
- Benefits of data curation scale with model capacity: consistent improvements at 7B, mixed results at 1.1B

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Semantically filtered data can outperform much larger unfiltered datasets in continued pretraining for underrepresented languages.
- Mechanism: Educational/STEM filtering reduces noisy or low-quality documents that impede learning efficiency, concentrating the training signal on structured, pedagogically valuable content.
- Core assumption: The quality signal from curated data transfers across linguistic tasks, not just domain-specific ones.
- Evidence anchors:
  - [abstract] "Despite using only 10% of the data and 20% of the computation, Curió-Edu 7B surpasses the full-corpus model in our evaluations"
  - [section 4.3] Under data-constrained conditions, the educational model outperforms the 100B-token model across all categories despite 5× fewer tokens
  - [corpus] Weak direct evidence—neighbors focus on alignment and evaluation, not data quality mechanisms
- Break condition: If noise in the corpus is actually beneficial for generalization, filtering would harm performance.

### Mechanism 2
- Claim: The benefits of data curation scale with model capacity—larger models better exploit structured, filtered data.
- Mechanism: Higher-capacity models have sufficient representational power to internalize and generalize from reduced-diversity but higher-quality corpora, while smaller models become capacity-constrained.
- Core assumption: Smaller models require broader data diversity to avoid overfitting to narrow signals.
- Evidence anchors:
  - [section 4.2] "In the 7B setting, the educationally filtered mixture leads to consistent improvements... the 1.1B models exhibit a more uneven pattern"
  - [section 4.3] 7B models show consistent gains across all categories; 1.1B models show mixed results with some regressions
  - [corpus] No direct corpus evidence on scale-dependent curation effects
- Break condition: If capacity isn't the limiting factor, improvements should scale similarly across model sizes.

### Mechanism 3
- Claim: Continued pretraining efficiently adapts models with minimal prior exposure to a target language.
- Mechanism: Leveraging already-learned representations from base pretraining, continued pretraining specializes the model to new linguistic distributions without full retraining cost.
- Core assumption: The base model's representations transfer sufficiently to enable efficient linguistic adaptation.
- Evidence anchors:
  - [abstract] Curió-Edu 7B achieves ~28% relative gain over LLaMA 2 using "just 0.01% of the 2 trillion tokens used in LLaMA 2's original pretraining"
  - [section 3.1] LLaMA 2 contains only 0.05% Portuguese (~10B tokens), making it suitable for studying adaptation under minimal prior exposure
  - [corpus] Neighbor "Memorization in Large Language Models in Medicine" discusses continued pretraining for domain adaptation, supporting transfer mechanism
- Break condition: If base model has no relevant linguistic foundation, continued pretraining would require substantially more data.

## Foundational Learning

- Concept: **Continued pretraining vs. fine-tuning distinction**
  - Why needed here: The paper's core intervention is continued pretraining (not task-specific fine-tuning), which operates on next-token prediction over raw corpora rather than instruction-response pairs.
  - Quick check question: Can you explain why continued pretraining might be preferred over instruction tuning for linguistic adaptation?

- Concept: **Semantic data filtering / quality scoring**
  - Why needed here: The ClassiCC-PT corpus uses classifier-based scores (threshold ≥2.5) to identify educational/STEM content—understanding this pipeline is essential for reproducing results.
  - Quick check question: How would you validate that a quality classifier isn't inadvertently filtering out valuable linguistic diversity?

- Concept: **Compute- vs. data-constrained training regimes**
  - Why needed here: The paper explicitly disentangles these scenarios (fixed token budget vs. unlimited data access), which yield different optimal strategies.
  - Quick check question: In a compute-constrained setting, would you prefer 20B tokens of filtered data or 20B randomly sampled tokens? Why?

## Architecture Onboarding

- Component map: LLaMA-2 7B -> ClassiCC-PT corpus -> educational/STEM filtering (score ≥2.5) -> Adafactor training -> PoETa V2 evaluation
- Critical path:
  1. Obtain LLaMA-2 7B checkpoint
  2. Prepare ClassiCC-PT with educational scoring
  3. Filter to documents with score ≥2.5 (~10B tokens)
  4. Train for 2 epochs (20B tokens seen), sequence length 4096, batch size 256
  5. Evaluate on PoETa V2

- Design tradeoffs:
  - Filtered data (10B unique, 20B seen) vs. full corpus (100B): 5× less compute, higher quality but reduced diversity
  - Adafactor vs. AdamW: Memory efficiency for large-scale training
  - Two epochs on filtered data vs. one pass on larger data: Risk of overfitting vs. data efficiency

- Failure signatures:
  - Small models (1.1B) showing regressions in Ethics, Code, General Knowledge—likely capacity limits
  - Plateauing at ~80B tokens for full-corpus model—diminishing returns from low-quality data

- First 3 experiments:
  1. **Baseline replication**: Train Curió-Edu 7B from LLaMA-2 on filtered 10B tokens, verify NPM ~37.6 on PoETa V2
  2. **Ablation on filter threshold**: Test thresholds 2.0, 2.5, 3.0 to validate quality-quantity tradeoff curve
  3. **Scale test**: Apply same pipeline to a smaller model (e.g., 3B) to confirm scale-dependence of filtering benefits

## Open Questions the Paper Calls Out

### Open Question 1
- Question: At what minimum model size does semantic filtering begin to consistently outperform unfiltered data in continued pretraining?
- Basis in paper: [explicit] The authors note that "smaller models may lack the representational capacity required to fully benefit from more selective data" and that "the influence of data quality is strongly tied to model capacity."
- Why unresolved: Only two model scales (1.1B and 7B) were tested, showing mixed results at 1.1B but consistent benefits at 7B. The threshold where benefits become reliable remains unknown.
- What evidence would resolve it: Systematic experiments across intermediate model sizes (e.g., 2B, 3B, 5B) using identical data regimes.

### Open Question 2
- Question: How does the optimal educational/STEM filtering threshold vary with corpus size and model capacity?
- Basis in paper: [inferred] The paper selected a threshold of 2.5 without systematic exploration, yielding 10B tokens from 120B. The authors acknowledge this was due to "computational budget constraints."
- Why unresolved: Only one threshold value was tested, leaving unclear whether a different cutoff would yield better results or how the threshold should scale with available compute.
- What evidence would resolve it: Ablation studies testing multiple threshold values (e.g., 2.0, 2.5, 3.0, 3.5) across different model sizes.

### Open Question 3
- Question: Do the observed benefits of educational filtering generalize to other underrepresented languages beyond Portuguese?
- Basis in paper: [inferred] The methodology was applied exclusively to Portuguese using ClassiCC-PT corpus. The authors state Portuguese has "minimal prior exposure" in LLaMA-2, but this condition exists for many languages.
- Why unresolved: Portuguese has relatively abundant web presence compared to truly low-resource languages. Whether similar filtering strategies work for languages with different characteristics (e.g., morphologically complex, different scripts) is unknown.
- What evidence would resolve it: Replicating the same experimental design on corpora for other underrepresented languages (e.g., Indonesian, Vietnamese, Swahili).

## Limitations
- The educational/STEM classifier used for filtering is not described in sufficient detail, creating uncertainty about potential biases in the filtering process
- Results are based entirely on PoETa V2 benchmark, which may not capture all dimensions of model quality despite showing consistent advantages across categories
- Findings may not transfer to models with stronger multilingual foundations or different base architectures

## Confidence
- High Confidence (90-100%): Curió-Edu 7B achieves superior performance to Curió 7B on PoETa V2 using 5× less data (20B vs 100B tokens seen); the performance advantage is consistent across multiple task categories including Reasoning, Ethics, and General Knowledge; the benefits of data curation are more pronounced at larger model scales (7B vs 1.1B)
- Medium Confidence (70-89%): The educational filtering mechanism is the primary driver of performance gains rather than other training factors; the results generalize beyond the specific PoETa V2 benchmark to indicate broader linguistic adaptation benefits; the scale-dependent benefits observed (7B vs 1.1B) reflect fundamental capacity constraints rather than implementation specifics
- Low Confidence (0-69%): The exact threshold of educational/STEM score ≥2.5 is optimal (could be different values yield similar or better results); the findings extend directly to languages other than Portuguese without modification; the computational efficiency gains (20% of compute for superior results) would hold across different hardware or training frameworks

## Next Checks
1. **Classifier Ablation Study**: Systematically vary the educational/STEM filtering threshold (2.0, 2.5, 3.0) and measure the tradeoff between data quality and quantity on final performance to validate that the observed benefits are indeed due to quality rather than specific threshold choice.

2. **Cross-Linguistic Generalization**: Apply the identical continued pretraining pipeline to a different low-resource language (e.g., Indonesian or Hindi) using appropriate filtered corpora to test whether the data curation benefits transfer across linguistic contexts.

3. **External Benchmark Validation**: Evaluate both Curió and Curió-Edu models on at least two external Portuguese language benchmarks not included in PoETa V2 (such as XSum Portuguese summarization or other established tasks) to confirm the generalization of performance improvements beyond the primary evaluation set.