---
ver: rpa2
title: 'VisJudge-Bench: Aesthetics and Quality Assessment of Visualizations'
arxiv_id: '2510.22373'
source_url: https://arxiv.org/abs/2510.22373
tags:
- data
- visualization
- color
- design
- chart
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces VISJUDGE-BENCH, the first benchmark for evaluating
  MLLMs' capabilities in assessing visualization aesthetics and quality. The benchmark
  is based on a three-dimensional framework of Fidelity, Expressiveness, and Aesthetics,
  containing 3,090 expert-annotated samples across 32 chart types.
---

# VisJudge-Bench: Aesthetics and Quality Assessment of Visualizations

## Quick Facts
- **arXiv ID**: 2510.22373
- **Source URL**: https://arxiv.org/abs/2510.22373
- **Reference count**: 40
- **Primary result**: First benchmark for MLLMs' visualization quality assessment with 3,090 expert-annotated samples across 32 chart types

## Executive Summary
This paper introduces VISJUDGE-BENCH, the first comprehensive benchmark for evaluating MLLMs' capabilities in assessing visualization aesthetics and quality. Based on a three-dimensional framework of Fidelity, Expressiveness, and Aesthetics, the benchmark contains 3,090 expert-annotated samples across 32 chart types. Systematic testing reveals significant gaps between even the most advanced MLLMs (including GPT-5) and human experts, with MAE of 0.553 and correlation of only 0.428. To address this, the authors propose VISJUDGE, a model specifically designed for visualization quality assessment. Experimental results demonstrate substantial improvements over baseline models, reducing MAE to 0.421 (23.9% reduction) and increasing consistency with human experts to 0.687 (60.5% improvement).

## Method Summary
The VISJUDGE-BENCH benchmark contains 3,090 expert-annotated visualization samples across 32 chart types, annotated across six dimensions (Data Fidelity, Semantic Readability, Insight Discovery, Design Style, Visual Composition, Color Harmony) using a 1-5 scoring system. The VisJudge model is trained using GRPO reinforcement learning on 4 base models (Qwen2.5-VL 3B/7B, InternVL3-8B, LLaVA-v1.6-mistral-7B) with LoRA adapters (rank=128, alpha=128). The training uses a composite reward function combining accuracy (exponential decay based on distance from human scores) and format rewards, trained for 5 epochs at LR 1e-5 with AdamW optimizer.

## Key Results
- VisJudge achieves MAE of 0.421 and correlation of 0.687 with human experts, significantly outperforming baseline MLLMs
- The model reduces score inflation issues present in general models (mean score correction from 3.89 to 3.11)
- VisJudge demonstrates effective generalization to real-world visualization generation and recommendation systems

## Why This Works (Mechanism)

### Mechanism 1: Multi-Dimensional Alignment via Reward Optimization
Specialized fine-tuning with reinforcement learning (GRPO) aligns the model's latent quality assessment with the multi-dimensional "Fidelity, Expressiveness, and Aesthetics" (FEA) framework, correcting the "surface-level" biases of general MLLMs. The composite reward function combines an accuracy reward (exponential decay based on distance from human scores) and a format reward, penalizing "score inflation" where general models prioritize visual appeal over data truthfulness.

### Mechanism 2: Hierarchical Decomposition of Assessment Tasks
Decomposing visualization quality into six sub-dimensions enables the model to isolate specific failure modes rather than producing a conflated "good/bad" judgment. By generating distinct questions and scoring criteria for each dimension, the model is forced to attend to specific visual features, mitigating the "halo effect" where visually pleasing but misleading charts receive high overall scores.

### Mechanism 3: Distribution Alignment and Bias Correction
Domain-specific training corrects systematic distribution biases (like right-skewed score inflation) prevalent in general MLLMs, producing scoring distributions that mirror human statistical variance. General models tend to concentrate scores around a high mean (e.g., 3.8-4.0), lacking discriminative power. VisJudge training exposes the model to a balanced range of low-to-high quality samples and penalizes deviations from the human mean.

## Foundational Learning

- **MLLM-as-a-Judge & Score Inflation**: You must understand that general MLLMs act as "lenient graders," often conflating "clean layout" with "high quality." Quick check: Would a general MLLM likely rate a visually stunning but data-distorted chart higher or lower than a human expert?

- **Visualization Literacy Dimensions (Fidelity vs. Aesthetics)**: The core innovation is separating "does it look good?" from "is it true?". You need to distinguish between encoding accuracy (Fidelity) and visual design (Aesthetics). Quick check: In this framework, if a chart uses a dual-axis to mislead but has perfect color harmony, which dimension fails?

- **GRPO (Group Relative Policy Optimization)**: This is the training engine. Unlike standard supervised fine-tuning which might just mimic text, GRPO uses a reward signal to optimize the *relative* quality of outputs. Quick check: What two components make up the reward function used to train VisJudge?

## Architecture Onboarding

- **Component map**: Input (Image + Metadata) -> Adaptive Question Generator -> Vision-Language Model (Qwen2.5-VL-7B with LoRA) -> Output (6 dimension scores + Rationale)

- **Critical path**: 1. Data Curation: Filtering 300k images → 3k diverse samples, 2. Annotation: Crowd annotation → Expert adjudication, 3. Training: GRPO fine-tuning using Accuracy Reward + Format Reward, 4. Inference: Generating JSON scores with rationales

- **Design tradeoffs**: Generalist vs. Specialist (7B specialist outperforms massive generalists on domain metrics), Crowd vs. Expert (crowd data scales but requires heavy filtering), Fixed vs. Adaptive Criteria (adaptive questions improve relevance but add inference overhead)

- **Failure signatures**: Score Inflation (model outputs cluster between 3.5-4.5 regardless of input quality), Aesthetic Bias (high Fidelity scores to charts with good aesthetics but flawed data), Semantic Hallucination (critiques non-existent elements)

- **First 3 experiments**: 1. Bias Probe: Run base model vs. VisJudge on "misleading but pretty" charts, 2. Dimension Ablation: Evaluate VisJudge on "Single View" vs. "Dashboard" inputs, 3. Generative Loop: Use VisJudge as feedback agent for visualization generator and verify quality improvement over iterations

## Open Questions the Paper Calls Out

1. How can visualization quality assessment be extended to capture dynamic evolution and interactive exploration? (The authors note current benchmarks focus on static visualizations with limited coverage of dynamic evolution and interactive exploration)

2. How can data fidelity be comprehensively verified in web-sourced visualizations where the raw data is unavailable? (Since samples are collected from the web and raw data is often unavailable, the evaluation framework aligns with real-world human assessment by primarily relying on visual presentation)

3. How can evaluation methods move beyond single-point scores to capture the distribution of human aesthetic preferences? (The authors suggest future work should focus on investigating distribution-based evaluation methods to better capture the diversity of human preferences)

## Limitations
- Performance gap remains substantial even after specialized training (MAE of 0.421 vs human experts)
- Benchmark expert annotations represent subjective judgments that may not generalize to all cultural contexts
- Model shows particular difficulty with complex dashboard evaluations, suggesting framework may need adaptation for multi-view scenarios

## Confidence
- **High Confidence**: Systematic superiority of VisJudge over baseline MLLMs on VISJUDGE-BENCH metrics (MAE reduction of 23.9%, correlation improvement of 60.5%)
- **Medium Confidence**: Generalization claims to real-world visualization generation tasks (limited scope demonstration)
- **Medium Confidence**: Attribution of performance gains specifically to multi-dimensional FEA framework versus other training factors

## Next Checks
1. **Cross-Cultural Validation**: Test VisJudge on visualization datasets annotated by experts from different cultural backgrounds to assess generalizability of aesthetic judgments
2. **Longitudinal Stability**: Evaluate whether VisJudge's performance degrades over time as visualization design trends evolve, requiring periodic retraining
3. **Error Analysis**: Conduct detailed case studies on VisJudge's systematic errors, particularly in the Aesthetics dimension where human-model correlation remains lowest