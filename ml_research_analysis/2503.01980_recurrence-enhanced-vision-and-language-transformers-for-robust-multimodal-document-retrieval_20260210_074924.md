---
ver: rpa2
title: Recurrence-Enhanced Vision-and-Language Transformers for Robust Multimodal
  Document Retrieval
arxiv_id: '2503.01980'
source_url: https://arxiv.org/abs/2503.01980
tags:
- layer
- input
- visual
- clip
- document
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses multimodal document retrieval where both queries
  and documents consist of paired images and text. The proposed method, ReT, employs
  a novel Transformer-based recurrent cell that integrates features from multiple
  layers of visual and textual backbones, using sigmoidal gates inspired by LSTMs
  to control information flow across layers and modalities.
---

# Recurrence-Enhanced Vision-and-Language Transformers for Robust Multimodal Document Retrieval

## Quick Facts
- arXiv ID: 2503.01980
- Source URL: https://arxiv.org/abs/2503.01980
- Reference count: 40
- Primary result: Achieves 82.0 R@5 on OVEN, outperforming state-of-the-art multimodal retrieval approaches

## Executive Summary
This paper introduces ReT, a novel Transformer-based recurrent cell that processes multi-layer features from frozen vision and language backbones to improve multimodal document retrieval. By integrating features across backbone layers using LSTM-inspired gates, ReT captures both low-level visual details and high-level semantic concepts that single-layer approaches miss. The method demonstrates superior performance on the M2KR benchmark and shows strong generalization to the M-BEIR benchmark, while also enhancing retrieval-augmented generation for knowledge-based visual question answering.

## Method Summary
ReT employs a recurrent cell that processes frozen CLIP backbone features layer-by-layer, using cross-attention to fuse textual and visual information at each step. The cell uses learnable sigmoidal gates (forget, input-text, input-visual) to control information flow across layers and modalities. Multi-layer features are extracted from frozen backbones, passed through the recurrent cell sequentially, and projected to 128-dimensions for fine-grained late interaction scoring. The model is trained end-to-end using symmetric InfoNCE loss with MaxSim-style token-level matching.

## Key Results
- Achieves 82.0 R@5 on OVEN, significantly outperforming state-of-the-art approaches
- Demonstrates 73.4 R@10 on WIT dataset using fine-grained late interaction
- Shows superior performance on M-BEIR benchmark with competitive latency (3.5ms per query)
- Consistently retrieves more accurate documents in retrieval-augmented generation for visual question answering

## Why This Works (Mechanism)

### Mechanism 1
Multi-layer feature extraction captures both low-level visual details and high-level semantic concepts that single-layer approaches miss. The recurrent cell processes backbone features sequentially from shallow to deep layers, accumulating information via the hidden state. Shallow layers retain task-relevant low-level information that deeper layers progressively attenuate.

### Mechanism 2
LSTM-inspired gating enables adaptive, query-specific control over which layers and modalities contribute to the final representation. Three sigmoid gates per layer (forget, input-text, input-visual) modulate information flow based on current cross-attention outputs, allowing the model to balance visual and textual contributions dynamically.

### Mechanism 3
Fine-grained late interaction via token-level similarity aggregation produces more robust relevance scores than single-vector matching. Each query and document produces k=32 latent tokens, with relevance scores computed by aggregating best-matching document tokens per query token, enabling local semantic matching.

## Foundational Learning

- **Transformer self-attention and cross-attention**
  - Why needed here: The recurrent cell uses self-attention for candidate state computation and cross-attention for fusing multimodal features at each layer.
  - Quick check question: Can you explain how cross-attention differs from self-attention in terms of query/key/value sources?

- **LSTM gating dynamics (forget, input gates)**
  - Why needed here: ReT's gating mechanism directly borrows the sigmoidal forget/input gate concept from LSTMs to control information flow.
  - Quick check question: What is the functional role of the forget gate in an LSTM cell, and how does ReT adapt this concept?

- **Late interaction retrieval (ColBERT paradigm)**
  - Why needed here: ReT uses MaxSim-style scoring where each query token finds its best-matching document token.
  - Quick check question: How does late interaction differ from bi-encoder cosine similarity in terms of computational cost and granularity?

## Architecture Onboarding

- **Component map:**
  Frozen CLIP backbones (24 visual layers, 12 text layers) → multi-layer feature extraction → recurrent cell (LayerNorm → parallel branches → gating → MLP) → k=32 tokens → 128-dim projection → fine-grained late interaction

- **Critical path:**
  1. Extract multi-layer features E^m_l from frozen backbones
  2. Initialize h_0 as learnable vectors
  3. For each layer l=1..L: compute cross-attention outputs, compute gates, update state via Eq. 3
  4. Project final state h_L to 128-dim; compute fine-grained scores via Eq. 9

- **Design tradeoffs:**
  - Token count k=32 vs. retrieval speed: More tokens improve granularity but increase index size and scoring cost
  - Frozen vs. fine-tuned backbones: Frozen reduces training cost (80 GPU-hrs vs. 864 for PreFLMR) but may limit domain adaptation
  - Layer sampling strategy: Uniform subsampling for mismatched backbone depths vs. learned selection

- **Failure signatures:**
  - Gate collapse: All gates converge to constant values → recurrence behaves like single-layer fusion
  - Token collapse: h_L tokens become near-identical → late interaction degrades to single-vector matching
  - Modality imbalance: One input gate consistently dominates → underutilization of one modality

- **First 3 experiments:**
  1. Ablate recurrence depth: Train with recurrence only on last 4 layers vs. all layers
  2. Visualize gate activations: Plot forget and input gate distributions across layers on validation set
  3. Benchmark retrieval latency: Measure forward pass + scoring time with k=32 tokens vs. PreFLMR's 320+ tokens

## Open Questions the Paper Calls Out

### Open Question 1
Why does increasing the visual encoder size (e.g., from ViT-L to ViT-H) lead to performance degradation in multimodal retrieval tasks despite the expectation of improved feature representation? The authors observe that OpenCLIP ViT-H performs lower than CLIP ViT-L, aligning with previous work but lacking definitive explanation. This suggests potential overfitting, data scarcity relative to model capacity, or specific incompatibilities within the recurrent fusion mechanism for very deep visual backbones.

### Open Question 2
Does the presence of overlapping document images between different datasets in a training corpus bias the retrieval model against datasets with unique or sparsely represented images (e.g., OKVQA)? The authors hypothesize that OKVQA document images in OVEN and InfoSeek training splits cause the model to associate those images with larger datasets, effectively "ignoring" OKVQA. While performance drops correlate with data overlap percentages, the causal link remains unconfirmed without deduplication experiments.

### Open Question 3
What is the optimal strategy for selecting feature layers from backbones of unequal depth (e.g., 12 text layers vs. 24 visual layers) to maximize the efficiency of the recurrent fusion process? The current approach uses uniform sampling or manual adjustment to handle unequal depths, suggesting a lack of theoretical grounding for how to best align semantic levels between modalities of different depths.

## Limitations
- Evaluation relies heavily on the M2KR benchmark, which may not represent diverse real-world document structures
- Comparison is limited to a relatively small set of baselines, potentially missing stronger alternatives
- Computational overhead compared to simpler approaches is not thoroughly characterized beyond single forward-pass measurements

## Confidence

**High confidence** in the architectural design and its theoretical foundation - the integration of multi-layer features through recurrent gating is well-motivated by the need to capture both low-level visual details and high-level semantics that single-layer approaches miss.

**Medium confidence** in the empirical superiority claims - while results show consistent improvements across multiple datasets and metrics, evaluation is constrained to the M2KR benchmark and could be more comprehensive.

**Low confidence** in the scalability and practical deployment implications - the paper does not address performance on larger document collections or strict latency requirements beyond single forward-pass measurement.

## Next Checks

1. **Generalization test on diverse document structures**: Evaluate ReT on a benchmark that includes documents with varying layouts (news articles, scientific papers, product catalogs) to verify that multi-layer recurrence provides consistent benefits across different document types.

2. **Computational efficiency analysis**: Measure end-to-end system performance including indexing time, storage requirements, and query throughput under realistic loads. Compare these metrics against both simple baselines and other state-of-the-art approaches to determine if accuracy gains justify computational overhead.

3. **Ablation study on backbone layer selection**: Systematically vary the number and specific layers used in the recurrence (beyond uniform subsampling) to identify whether certain layers consistently contribute more to performance across different query types and document categories.