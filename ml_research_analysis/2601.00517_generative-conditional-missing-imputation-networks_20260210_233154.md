---
ver: rpa2
title: Generative Conditional Missing Imputation Networks
arxiv_id: '2601.00517'
source_url: https://arxiv.org/abs/2601.00517
tags:
- missing
- data
- imputation
- values
- gcmi
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Generative Conditional Missing Imputation
  Networks (GCMI), a deep learning approach for handling missing data in statistical
  analysis. The method leverages generative conditional networks to impute missing
  values under both MCAR and MAR mechanisms, incorporating a multiple imputation framework
  using chained equations to enhance stability and accuracy.
---

# Generative Conditional Missing Imputation Networks

## Quick Facts
- arXiv ID: 2601.00517
- Source URL: https://arxiv.org/abs/2601.00517
- Reference count: 38
- Primary result: GCMI achieves 22% lower RMSE than state-of-the-art methods on MIMIC-III lab data across 10-50% missingness levels

## Executive Summary
Generative Conditional Missing Imputation Networks (GCMI) introduces a deep learning approach for handling missing data using generative adversarial networks with chained equations. The method trains individual generator-discriminator pairs for each feature, imputing values conditioned on observed data while minimizing Pearson χ² divergence between true and imputed distributions. Extensive experiments demonstrate GCMI's superior performance compared to traditional methods like MICE and MissForest, particularly in maintaining inter-variable correlations while achieving robust imputation across diverse missingness mechanisms.

## Method Summary
GCMI decomposes multivariate imputation into P independent conditional generation tasks, training separate GANs for each feature. Generators take the incomplete data and noise vector as input to produce imputations, while discriminators distinguish between observed and imputed values using least-squares loss. The method incorporates chained equations, iteratively updating imputations across features until convergence criteria are met. An accuracy penalty term handles mixed data types by switching between MSE and cross-entropy based on whether features are numerical or categorical. The approach theoretically minimizes Pearson χ² divergence when discriminators reach optimal states.

## Key Results
- GCMI achieved 22% lower RMSE (0.058-0.068) than best baseline on MIMIC-III laboratory data
- Similar improvements observed on eICU data (15-25% lower RMSE)
- Method excels at maintaining inter-variable correlations while providing robust imputation across MCAR and MAR mechanisms
- Performance validated across missingness levels from 10-50%

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The defined loss function theoretically minimizes the Pearson $\chi^2$ divergence between the true conditional distribution and the generated distribution.
- **Mechanism:** The authors define specific least-squares loss functions for the discriminator ($L_D$) and generator ($L_G$). Theoretical derivation shows that minimizing $L_G$ is equivalent to minimizing $\frac{1}{2} D_{\chi^2}(p_{xj} + p_{Gj} \| 2p_{Gj})$, forcing the generator distribution $p_{Gj}$ to match the true data distribution $p_{xj}$.
- **Core assumption:** The discriminator must be able to reach its optimal state $D^*_j$ for the generator loss to effectively approximate the $\chi^2$ divergence.
- **Evidence anchors:**
  - [abstract] Mentions theoretical analysis establishing GCMI minimizes Pearson $\chi^2$ divergence.
  - [page 5] Explicit derivation of $L_G(G_j) = \frac{1}{2} D_{\chi^2}$ and Theorem 1.
  - [corpus] Weak direct support; neighbors like MissARF focus on Random Forests rather than $\chi^2$ divergence proofs in GANs.
- **Break condition:** If the discriminator is under-capacity or fails to converge to $D^*$, the gradient provided to the generator no longer corresponds to minimizing $\chi^2$ divergence, potentially leading to mode collapse.

### Mechanism 2
- **Claim:** Decomposing a multivariate imputation problem into $P$ independent conditional generation tasks improves handling of mixed data types.
- **Mechanism:** Instead of one giant generator outputting a full row, GCMI trains $P$ separate generator-discriminator pairs. Generator $G_j$ imputes feature $j$ conditioned solely on the rest of the row ($X_{(-j)}$). This allows for specific handling of binary vs. continuous variables via an accuracy penalty term.
- **Core assumption:** The conditional distribution $p(x_j | X_{(-j)})$ captures the necessary dependencies for valid imputation, and features can be predicted accurately based on available counterparts.
- **Evidence anchors:**
  - [page 4] "GCMI is an ensemble of generative conditional networks consisting of $P$ pairs of conditional generators and discriminators."
  - [page 5] Details the accuracy penalty $L_{acc}$ switching between MSE and Cross-Entropy based on data type.
  - [corpus] Impute-MACFM uses flow matching for similar structure handling, validating the generative approach to tabular data.
- **Break condition:** If inter-variable dependencies are extremely sparse or non-existent, the conditioning signal $X_{(-j)}$ is noise, reducing the generator to a random sampler.

### Mechanism 3
- **Claim:** Iteratively updating imputed values in a chained equation framework stabilizes the joint distribution of the imputed dataset.
- **Mechanism:** The algorithm sorts variables by missingness and iterates through the dataset multiple times (Chained Equations). In each iteration, the previously imputed values of other features serve as conditioning inputs for the current feature's generator, refining the consistency across variables until a convergence criterion is met.
- **Core assumption:** The "chained" process converges to a stationary distribution where the imputations are consistent across the multivariate structure (Gibbs sampling assumption).
- **Evidence anchors:**
  - [page 7] "The GCIN imputation procedure is repeated until a stopping criterion is met... dual convergence criterion that monitors both numerical and categorical variables."
  - [abstract] "Incorporating a multiple imputation framework using chained equations to enhance stability."
  - [corpus] Markov Missing Graph also leverages conditional independence for local decomposition, supporting the iterative/gibbs-like approach.
- **Break condition:** If initial imputations are extremely poor (e.g., mean imputation on highly skewed data) and the learning rate is too low, the chained equations may oscillate or converge to a local optimum that preserves the initial bias.

## Foundational Learning

- **Concept: Missing Mechanisms (MCAR vs. MAR)**
  - **Why needed here:** The paper explicitly claims theoretical guarantees only for MCAR and MAR. If data is Missing Not At Random (MNAR), the conditional distribution $p(x_j | X_{(-j)})$ learned by the generator is biased, and the core theoretical foundation breaks.
  - **Quick check question:** Does the probability of a value being missing depend on the value itself (MNAR), or only on observed data (MAR)?

- **Concept: f-Divergence (specifically Pearson $\chi^2$)**
  - **Why needed here:** Standard GANs often minimize Jensen-Shannon divergence. GCMI modifies the loss to target Pearson $\chi^2$. Understanding this helps explain why the loss functions are $(D-2)^2$ and $(D)^2$ rather than the standard log-likelihood.
  - **Quick check question:** How does the gradient behavior of $\chi^2$ divergence differ from JS divergence when the generator distribution is far from the true distribution?

- **Concept: Chained Equations (MICE)**
  - **Why needed here:** GCMI is not a single-shot generation; it is an iterative process. One must understand that the input "observed data" changes every iteration as neighbors get imputed, turning the training/inference loop into a dynamic system.
  - **Quick check question:** In a chained equation system, why is the ordering of variables (e.g., by missingness proportion) important for convergence speed?

## Architecture Onboarding

- **Component map:**
  Input Layer (X + z) -> GCIN Core (P independent GANs: G_j(X_(-j), z) -> x_j, D_j(X_(-j), x_j) -> score) -> MICE Wrapper (Iterative controller with convergence monitoring)

- **Critical path:**
  The stability of the **Discriminator ($D_j$)** is the critical path. Because the generator loss relies on the discriminator's gradient, if $D_j$ overfits to the small observed sample (common in high missingness), it provides garbage gradients to $G_j$, causing the chained loop to diverge.

- **Design tradeoffs:**
  - **Ensemble vs. Single Model:** Training $P$ GANs is computationally expensive ($O(P \times N \times I)$) compared to single matrix completion, but allows specific handling of categorical/continuous variables and avoids error accumulation in a single giant decoder.
  - **Capacity vs. Overfitting:** The paper prescribes smaller architectures for small datasets ($<20k$ samples) to prevent the generator from memorizing the limited observed rows.

- **Failure signatures:**
  - **Discriminator Dominance:** Loss $L_D$ goes to 0 immediately, $L_G$ explodes. *Fix:* Increase $D$ regularization or lower its learning rate ($\eta_D < \eta_G$).
  - **Oscillating Convergence:** $\gamma$ metrics bounce without decreasing. *Fix:* Reduce the number of training epochs per iteration or lower learning rates.
  - **Categorical Mode Collapse:** Binary variables all impute to 0 or 1. *Fix:* Check weighting of Cross-Entropy term in $L_{acc}$.

- **First 3 experiments:**
  1. **Hyperparameter Validation:** Reproduce the synthetic data experiment (Page 11) with $\eta_G=0.001$ and $\eta_D=0.0005$. Verify that RMSE drops below Mean Imputation at 20% missingness.
  2. **Ablation on Accuracy Loss:** Run GCMI with $L_{acc}$ removed vs. included on the UCI dataset. This tests if the "supervised" penalty is the driver of performance or if the GAN loss alone suffices.
  3. **Convergence Stress Test:** Apply GCMI to a dataset with 60% missingness (beyond the paper's standard 50%) to observe if the dual convergence criteria fail or if imputation quality degrades sharply.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can GCMI be adapted to ensure robust performance on small datasets where GAN training is inherently unstable?
- Basis in paper: [explicit] The conclusion states that "training of GAN-based algorithms like GCMI can be challenging, particularly with small datasets."
- Why unresolved: The paper validates performance on large ICU datasets ($N=5000$) but does not propose specific stabilizers for small sample regimes.
- What evidence would resolve it: Empirical analysis of GCMI stability and convergence rates on datasets with $N < 1000$ samples.

### Open Question 2
- Question: Can the theoretical guarantee of minimizing Pearson $\chi^2$ divergence be extended to the Missing Not at Random (MNAR) mechanism?
- Basis in paper: [inferred] The theoretical analysis (Section 2.1.1) explicitly establishes properties for MCAR and MAR, while excluding MNAR from the formal proof despite including MNAR in experimental simulations.
- Why unresolved: The paper provides no mathematical justification for GCMI's behavior when missingness depends on unobserved data.
- What evidence would resolve it: A formal extension of Theorem 1 covering the MNAR setting or an identifiability analysis for the model.

### Open Question 3
- Question: To what extent does the observed reduction in imputation RMSE translate to improved accuracy in downstream clinical tasks?
- Basis in paper: [inferred] The introduction states the goal is to "enhance subsequent performance" in prediction, but the Results section evaluates success solely via imputation error (RMSE).
- Why unresolved: Lower reconstruction error does not guarantee better utility for specific tasks like mortality prediction.
- What evidence would resolve it: A comparative study measuring downstream classification metrics (e.g., AUROC) using GCMI-imputed datasets versus baselines.

## Limitations

- The theoretical guarantees only apply to MCAR and MAR mechanisms, not MNAR
- Computational complexity increases linearly with the number of features (P separate GANs)
- Performance on small datasets (<1000 samples) is not well-established
- No evaluation of downstream task performance despite stated goals

## Confidence

- **High Confidence:** The RMSE improvement results (22% on MIMIC-III, 15-25% on eICU) and the theoretical framework for $\chi^2$ divergence minimization
- **Medium Confidence:** The chained equation convergence properties and the practical benefits of decomposing the problem into P independent GANs
- **Low Confidence:** The method's performance under MNAR mechanisms and its scalability to extremely high-dimensional datasets (>1000 features)

## Next Checks

1. **MNAR Mechanism Test:** Systematically evaluate GCMI on synthetic data with varying MNAR mechanisms (e.g., missingness depends on the true value being high/low) to quantify performance degradation when theoretical assumptions break.

2. **Architecture Sensitivity Analysis:** Conduct ablation studies varying network depth, width, and noise dimensions across datasets of different sizes to identify when GCMI's complexity provides marginal gains over simpler methods like MICE.

3. **Scalability Benchmark:** Test GCMI on high-dimensional genomic or imaging datasets (>10,000 features) to evaluate computational feasibility and whether the per-feature GAN approach remains superior to joint matrix completion methods at scale.