---
ver: rpa2
title: 'Reading the unreadable: Creating a dataset of 19th century English newspapers
  using image-to-text language models'
arxiv_id: '2502.14901'
source_url: https://arxiv.org/abs/2502.14901
tags:
- arxiv
- text
- visited
- page
- ncse
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that pre-trained image-to-text language
  models can perform OCR on historical documents with high accuracy. Using Pixtral
  12B, the study achieved a median character error rate of 1% on 19th-century English
  newspapers, five times lower than the next best model.
---

# Reading the unreadable: Creating a dataset of 19th century English newspapers using image-to-text language models

## Quick Facts
- arXiv ID: 2502.14901
- Source URL: https://arxiv.org/abs/2502.14901
- Authors: Jonathan Bourne
- Reference count: 40
- Pre-trained image-to-text models achieve 1% median CER on 19th-century newspapers

## Executive Summary
This paper demonstrates that pre-trained image-to-text language models can perform optical character recognition (OCR) on historical documents with high accuracy, specifically on 19th-century English newspapers. Using the Pixtral 12B model, the study achieved a median character error rate of 1% across the dataset, representing a five-fold improvement over previous approaches. The resulting NCSE v2.0 dataset contains 1.4 million entries, 321 million words, and enables new historical and sociological research opportunities by making previously unreadable archival newspapers accessible.

The research addresses the challenge of extracting text from historical documents, where traditional OCR methods struggle due to degradation, printing variations, and lack of labeled training data. By leveraging the capabilities of modern image-to-text models, the study provides a cost-effective solution at approximately $3.7 per 1000 pages, making it suitable for low-resource projects seeking to digitize historical archives.

## Method Summary
The study employed a multi-stage pipeline to process 19th-century English newspapers from the British Library collection. First, images were preprocessed using thresholding and scaling techniques to optimize input quality for the OCR models. Four different image-to-text language models were then applied: Pixtral 12B, InternVL2-6.6B, Reka-Core-1.0, and GPT-4o. The outputs were evaluated using multiple metrics including character error rate (CER), word error rate (WER), and token edit rate (TER), compared against ground truth transcriptions from the British Library's dataset.

The processed outputs were organized into a structured dataset (NCSE v2.0) that includes article-level segmentation, text classification into types and topics, and metadata enrichment. The study employed a hybrid approach where different models were selected based on their relative performance for different document types, optimizing both accuracy and computational efficiency. The final dataset provides researchers with searchable, analyzable text from historical newspapers that were previously inaccessible due to OCR limitations.

## Key Results
- Pixtral 12B achieved a median character error rate of 1% across the dataset, five times lower than the next best model
- The NCSE v2.0 dataset contains 1.4 million entries and 321 million words from 19th-century English newspapers
- Computational cost was approximately $3.7 per 1000 pages, making the approach viable for low-resource digitization projects

## Why This Works (Mechanism)
Image-to-text language models succeed on historical documents because they integrate visual feature extraction with contextual language understanding in a unified architecture. Unlike traditional OCR pipelines that separate layout analysis, character recognition, and post-processing, these models process the entire image holistically. This allows them to leverage contextual clues from surrounding text to disambiguate degraded or ambiguous characters, handle unusual typefaces and printing artifacts common in historical documents, and maintain consistency across connected text regions.

The models' large-scale pretraining on diverse text corpora enables them to recognize historical language patterns, spelling variations, and vocabulary that would be absent from modern OCR training data. The attention mechanisms can focus on relevant regions while maintaining awareness of the broader document context, helping to resolve cases where visual features alone are insufficient for accurate character identification.

## Foundational Learning

**Optical Character Recognition (OCR)**: The process of converting images of text into machine-encoded text. Needed to understand the core problem being addressed. Quick check: Traditional OCR pipelines typically achieve CERs of 5-15% on historical documents.

**Character Error Rate (CER)**: A metric measuring the minimum edit distance between predicted and ground truth text at the character level. Needed to quantify OCR accuracy. Quick check: CER = (Substitutions + Insertions + Deletions) / Total characters in ground truth.

**Image-to-Text Models**: Neural architectures that take images as input and generate text descriptions, capable of performing both visual understanding and language generation. Needed to understand the technological approach. Quick check: These models combine vision encoders with language decoders through cross-attention mechanisms.

**Historical Newspaper Characteristics**: Specific challenges including ink bleed, paper degradation, varied typefaces, multi-column layouts, and archaic language. Needed to contextualize the technical challenges. Quick check: 19th-century newspapers often used letterpress printing with inconsistent inking.

## Architecture Onboarding

Component Map: Image Preprocessing -> Vision Encoder -> Cross-Attention -> Language Decoder -> Text Output

Critical Path: The vision encoder processes the newspaper image into visual features, which are then attended to by the language decoder through cross-attention layers. This allows the model to iteratively refine its understanding of characters and words based on both visual evidence and linguistic context.

Design Tradeoffs: The study prioritized using pre-trained models over fine-tuning to reduce computational costs and avoid the need for labeled training data. This sacrificed some potential accuracy gains that could be achieved through domain-specific fine-tuning but enabled practical deployment on historical archives without extensive annotation efforts.

Failure Signatures: The models struggle with highly degraded text where visual features are insufficient, unusual typefaces not well-represented in pretraining data, and cases requiring domain-specific knowledge (historical names, archaic terms). Errors often cluster in regions with poor image quality or complex layouts.

First Experiments:
1. Evaluate CER on a held-out validation set with varying image quality levels
2. Test model performance on different newspaper typefaces and printing qualities
3. Compare processing speed and accuracy tradeoffs between the four evaluated models

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Evaluation relies on a subset of the British Library collection, potentially limiting generalizability to other historical newspaper archives
- Comparison to GT4HistOCR uses a model trained on European books rather than British newspapers, introducing potential domain mismatch
- Computational cost estimates are specific to the implementation and hardware used, which may vary significantly with different infrastructure choices

## Confidence
High confidence in the core finding that Pixtral 12B outperforms other image-to-text models for 19th-century newspaper OCR, supported by multiple error rate metrics and comparison with a previously published baseline.

Medium confidence in the scalability and cost-effectiveness claims, as these depend on implementation details and infrastructure choices that may vary across different computational environments.

Medium confidence in the generalizability of results to other historical newspaper collections, given the single-source evaluation dataset.

## Next Checks
1. Evaluate the same models on newspapers from different collections (e.g., American, European, non-English) to assess cross-collection generalization of the observed performance improvements.

2. Conduct human evaluation studies on sample outputs to validate automated metrics, particularly for the article segmentation and classification components that rely on heuristic rules.

3. Replicate the computational cost analysis on different hardware configurations and with alternative model optimization approaches (e.g., quantization, distillation) to verify the reported $3.7 per 1000 pages cost estimate.