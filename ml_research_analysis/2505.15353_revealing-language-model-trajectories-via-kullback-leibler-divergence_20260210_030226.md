---
ver: rpa2
title: Revealing Language Model Trajectories via Kullback-Leibler Divergence
arxiv_id: '2505.15353'
source_url: https://arxiv.org/abs/2505.15353
tags:
- divergence
- arxiv
- preprint
- language
- trajectories
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes a consistent scale for measuring Kullback-Leibler
  divergence across diverse language models and settings using log-likelihood vectors.
  By representing model probabilities in a shared coordinate system, the authors enable
  unified comparisons across pretraining checkpoints, quantized models, fine-tuned
  models, and intermediate layers.
---

# Revealing Language Model Trajectories via Kullback-Leibler Divergence

## Quick Facts
- arXiv ID: 2505.15353
- Source URL: https://arxiv.org/abs/2505.15353
- Reference count: 40
- This paper establishes a consistent scale for measuring Kullback-Leibler divergence across diverse language models and settings using log-likelihood vectors.

## Executive Summary
This paper establishes a consistent scale for measuring Kullback-Leibler divergence across diverse language models and settings using log-likelihood vectors. By representing model probabilities in a shared coordinate system, the authors enable unified comparisons across pretraining checkpoints, quantized models, fine-tuned models, and intermediate layers. Their experiments on Pythia models reveal that KL divergence between consecutive checkpoints decreases during training, with characteristic scales per setting: pretraining (0.01-0.1 bits/byte), quantization (0.44-0.49 bits/byte), fine-tuning (0.40 bits/byte), and across layers (median 3.0 bits/byte). Critically, diffusion analysis shows that while weight trajectories exhibit near-Brownian diffusion, log-likelihood trajectories are strongly subdiffusive (exponent ≈0.2), indicating that model behavior stabilizes early despite continued weight changes.

## Method Summary
The method computes log-likelihood vectors for each model over a fixed text set, then applies double-centering to create a shared coordinate system where squared Euclidean distance approximates KL divergence. The log-likelihood matrix L is formed by computing log p(x_i) for each text x_i and model, then row-centered (texts) and column-centered (models) to produce Q. KL(p_i, p_j) ≈ ||q_i - q_j||²/(2N B̄ log 2) gives divergence in bits/byte. This framework enables comparing pretraining trajectories, quantization effects, fine-tuning impacts, and intermediate layer representations through logit lens approximation. Diffusion exponents are estimated via least-squares regression on log(distance) vs. log(time) over sliding windows.

## Key Results
- Characteristic KL divergence scales per transformation: pretraining (0.01-0.1 bits/byte), quantization (0.44-0.49 bits/byte), fine-tuning (0.40 bits/byte), layer-wise (median 3.0 bits/byte)
- Log-likelihood trajectories show strongly subdiffusive behavior (exponent ≈0.2) while weight trajectories remain near-Brownian (exponent ≈1)
- Early training checkpoints exhibit higher KL divergence (0.05-0.1 bits/byte) than later ones (0.01-0.05 bits/byte)
- Double-centering enables cross-model comparison without architectural alignment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Log-likelihood vectors computed over a fixed text set create a unified coordinate system where squared Euclidean distance approximates KL divergence between any two language models.
- Mechanism: Given a log-likelihood matrix L over N texts for K models, double-centering produces coordinates Q where ||q_i - q_j||²/N ≈ 2KL(p_i, p_j). This allows heterogeneous models (different architectures, checkpoints, layers) to occupy the same geometric space.
- Core assumption: The reference text set captures behavioral differences relevant to downstream tasks and model comparison.
- Evidence anchors:
  - [abstract]: "By representing model probabilities in a shared coordinate system, the authors enable unified comparisons across pretraining checkpoints, quantized models, fine-tuned models, and intermediate layers."
  - [section 2]: The relationship 2KL(p_i, p_j) ≈ ||q_i - q_j||²/N is derived; double-centering enables cross-model comparison without architectural alignment.
  - [corpus]: The related paper "Mapping 1,000+ Language Models via the Log-Likelihood Vector" (Oyama et al., 2025) provides the foundational methodology and validates the approximation across 1,018 models.
- Break condition: If text set domain shifts substantially from training distribution, KL estimates may not reflect functional similarity; domain-specific text sets would project trajectories onto different aspects.

### Mechanism 2
- Claim: Training trajectories in log-likelihood space exhibit strongly subdiffusive behavior (exponent ≈0.2) despite near-Brownian diffusion in weight space (exponent ≈1), indicating early behavioral stabilization.
- Mechanism: The mapping from weight space to log-likelihood space involves many-to-one folding due to permutation symmetries and flat minima. As weights drift through functionally equivalent regions, log-likelihood coordinates remain constrained to a narrower region, producing fractal-like trajectories with higher effective dimension (D_q ≈ 10 vs D_w ≈ 2).
- Core assumption: Diffusion exponents estimated over 10k-step windows capture intrinsic dynamics rather than checkpoint discretization artifacts.
- Evidence anchors:
  - [abstract]: "diffusion analysis shows that while weight trajectories exhibit near-Brownian diffusion, log-likelihood trajectories are strongly subdiffusive (exponent ≈0.2)"
  - [section 4.2]: Diffusion exponents are estimated via least-squares regression on squared distance vs. time; c_w ≈ 1 for weights, c_q ≈ 0.2 for log-likelihood; fractal dimensions derived under fractional Brownian motion interpretation.
  - [corpus]: No direct corpus corroboration of this specific subdiffusion finding; related work on anomalous diffusion in weight space (Chen et al. 2022, Kunin et al. 2024) is cited but not extended to log-likelihood space.
- Break condition: If checkpoint spacing were much finer, the apparent subdiffusion exponent might change; the analysis is limited to ≥1k-step resolution.

### Mechanism 3
- Claim: Different model transformation types produce characteristic KL divergence scales that serve as interpretable reference points for model comparison.
- Mechanism: Each transformation (quantization, fine-tuning, pretraining step) perturbs the output distribution in consistent ways. Quantization acts as structured perturbation with low variance; fine-tuning produces moderate shifts within model type; layer-wise transitions show high variance due to noise in shallow layer logits via logit lens.
- Core assumption: The characteristic scales generalize beyond the Pythia family and specific quantization/fine-tuning methods tested.
- Evidence anchors:
  - [abstract]: "characteristic scales per setting: pretraining (0.01-0.1 bits/byte), quantization (0.44-0.49 bits/byte), fine-tuning (0.40 bits/byte), and across layers (median 3.0 bits/byte)"
  - [section 3]: Table 1a provides summary statistics; quantization shows tight distributions (8-bit median 0.44, 4-bit 0.49); fine-tuning (median 0.40) is smaller than intra-type random pairs (0.95).
  - [corpus]: No corpus corroboration of these specific numerical scales; this is a novel contribution.
- Break condition: If applied to models with substantially different architectures or training regimes, scale values may shift; domain-specific text sets would also affect absolute values.

## Foundational Learning

- Concept: **KL divergence as a pseudo-metric between probability distributions**
  - Why needed here: The entire framework rests on interpreting KL values as distances; 0.1 bits/byte is "small" while 3+ bits/byte is "large" requires understanding the scale.
  - Quick check question: If KL(P||Q) = 0.5 bits/byte and KL(Q||R) = 0.3 bits/byte, can you conclude KL(P||R) < 0.8 bits/byte?

- Concept: **Logit lens and intermediate layer interpretation**
  - Why needed here: The paper extends log-likelihood vectors to intermediate layers by treating each layer's logits as a pseudo-model, enabling layer-wise trajectory analysis.
  - Quick check question: Why might shallow layer logits produce noisier KL estimates than final layer outputs?

- Concept: **Diffusion exponents and anomalous diffusion**
  - Why needed here: Understanding c_w ≈ 1 (Brownian) vs. c_q ≈ 0.2 (subdiffusive) requires grasping that exponent values indicate how mean-squared displacement scales with time.
  - Quick check question: If a trajectory has diffusion exponent c = 0.5, what does this imply about its confinement over time compared to c = 1?

## Architecture Onboarding

- Component map: Text set → Log-likelihood computation → Double-centering → Q coordinates → Pairwise KL → Trajectory analysis
- Critical path: Text set selection → Log-likelihood matrix L → Double-centering → Q coordinates → Pairwise KL → Trajectory analysis
- Design tradeoffs:
  - **Text set size vs. compute**: More texts reduce variance but increase forward-pass cost; 10,000 provides SE ≈ 0.001-0.01 bits/byte
  - **Text domain vs. generalization**: Domain-specific text sets project onto different behavioral aspects
  - **Checkpoint resolution**: 1k-step spacing limits analysis to coarse temporal scales
- Failure signatures:
  - **High variance in KL**: Insufficient text count; increase N or filter outlier texts
  - **Anomalous checkpoints**: Detect via sudden KL spikes; exclude as outliers (e.g., Pythia 1B step 116k)
  - **Noisy layer-wise KL**: Shallow layers via logit lens have high noise; consider tuned lens instead
- First 3 experiments:
  1. **Scale validation**: Replicate KL scale analysis on a different model family (e.g., Llama checkpoints) to test generalization of characteristic values.
  2. **Text set sensitivity**: Compute KL estimates using different random subsets of the Pile and measure correlation with original estimates (paper reports r=0.99 for same-domain subsets).
  3. **Early training dynamics**: Apply to finer checkpoint spacing (e.g., 100-step intervals) during warmup to characterize the high-KL phase more precisely.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent does the domain of the reference text set influence the absolute scale and trajectory dynamics of the KL divergence?
- Basis in paper: [explicit] The authors state that the measured KL divergence depends on the text set and that "the impact of domain shifts across text sets has not been examined."
- Why unresolved: The experiments rely exclusively on 10,000 texts from the Pile; it is unknown if the established scales (e.g., 0.01-0.1 bits/byte for pretraining) generalize to other corpora like code or domain-specific data.
- What evidence would resolve it: Replicating the analysis using text sets from diverse domains (e.g., programming code, biomedical literature) and comparing the resulting KL scales and diffusion exponents.

### Open Question 2
- Question: Does the observed subdiffusive behavior (exponent $\approx 0.2$) persist at training time scales finer than 1,000 steps?
- Basis in paper: [explicit] The authors note that Pythia checkpoints are available only at 1k-step intervals and "finer-scale behavior below this resolution is not examined."
- Why unresolved: The characteristic "early stabilization" might be an artifact of the coarse measurement interval; behavior could differ significantly at the 10-step or 100-step level.
- What evidence would resolve it: Running new pretraining experiments with checkpointing enabled at much higher frequencies (e.g., every 10 steps) and recalculating the diffusion exponent $c_q$.

### Open Question 3
- Question: How does the estimated KL divergence across intermediate layers change when using a tuned lens instead of the logit lens?
- Basis in paper: [explicit] The authors identify "substantial noise" in shallow layers using the logit lens as a limitation, suggesting the "tuned lens" as a solution limited only by current availability.
- Why unresolved: The high variance and median KL (3.0 bits/byte) observed across layers may be inflated by the noise inherent to the logit lens method, obscuring the true trajectory of the model's internal representations.
- What evidence would resolve it: Training tuned lenses for the analyzed models and comparing the layer-wise KL trajectories and magnitudes against the current logit lens baselines.

### Open Question 4
- Question: What are the precise geometric or optimization factors that cause the log-likelihood trajectory to become subdiffusive while the weight trajectory remains Brownian?
- Basis in paper: [explicit] The authors state their analysis of the underlying causes of subdiffusion "remains preliminary and has not yet been quantitatively established," despite offering hypotheses regarding permutation symmetries and flat minima.
- Why unresolved: It is unclear whether the phenomenon is driven primarily by the "folding" of the loss landscape due to symmetries or by the specific dynamics of stochastic gradient descent.
- What evidence would resolve it: A quantitative analysis measuring the correlation between weight permutation shifts and KL divergence changes, or interventionist experiments that restrict weight changes to non-degenerate subspaces.

## Limitations
- Single text set dependency: The methodology relies exclusively on 10,000 texts from The Pile, potentially limiting generalization across domains.
- Pythia-specific scales: Characteristic KL divergence values were established only on Pythia models and may not hold for other architectures.
- Logit lens noise: Intermediate layer analysis using logit lens introduces substantial noise, particularly in shallow layers.
- Coarse checkpoint resolution: 1k-step intervals limit analysis of fine-grained training dynamics and early warmup behavior.

## Confidence
- **High confidence**: The mathematical framework for double-centering log-likelihood vectors to approximate KL divergence is well-established and reproducible (Mechanism 1).
- **Medium confidence**: The characteristic KL divergence scales for different transformations are empirically robust within Pythia but require validation across model families.
- **Low confidence**: The diffusion exponent c_q ≈ 0.2 for log-likelihood trajectories is novel and needs independent verification, particularly given the lack of corpus corroboration.

## Next Checks
1. **Cross-model validation**: Replicate the characteristic KL scale analysis on Llama or GPT-2 checkpoints to test whether quantization, fine-tuning, and pretraining trajectories produce similar divergence magnitudes.
2. **Text set sensitivity analysis**: Compute KL estimates using multiple random subsets of The Pile and measure correlation with original estimates; test whether domain-specific text sets (e.g., code, medical) produce consistent or divergent scale values.
3. **Fine-grained checkpoint analysis**: Apply the methodology to finer checkpoint spacing (e.g., 100-step intervals) during the high-KL warmup phase to characterize early training dynamics more precisely and validate the subdiffusive exponent estimates.