---
ver: rpa2
title: Measuring Moral LLM Responses in Multilingual Capacities
arxiv_id: '2510.08776'
source_url: https://arxiv.org/abs/2510.08776
tags:
- languages
- llms
- responses
- safety
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated the multilingual moral and ethical reasoning
  of leading large language models across five categories (Biases & Stereotypes, Consent
  & Autonomy, Harm Prevention & Safety, Legality, and Moral Judgment) in five languages
  (English, Chinese, Spanish, Arabic, Hindi, and Swahili). Using a five-point rubric
  and an LLM judge, the research found that GPT-5 consistently outperformed other
  models, while Gemini 2.5 Pro showed significant inconsistency, particularly in Consent
  & Autonomy (1.39/5) and Harm Prevention & Safety (1.98/5).
---

# Measuring Moral LLM Responses in Multilingual Capacities

## Quick Facts
- **arXiv ID:** 2510.08776
- **Source URL:** https://arxiv.org/abs/2510.08776
- **Reference count:** 1
- **Primary result:** GPT-5 consistently outperformed other models in multilingual moral reasoning, while Gemini 2.5 Pro showed significant inconsistency, particularly in Consent & Autonomy and Harm Prevention & Safety categories.

## Executive Summary
This study evaluated the multilingual moral and ethical reasoning of leading large language models across five categories (Biases & Stereotypes, Consent & Autonomy, Harm Prevention & Safety, Legality, and Moral Judgment) in five languages (English, Chinese, Spanish, Arabic, Hindi, and Swahili). Using a five-point rubric and an LLM judge, the research found that GPT-5 consistently outperformed other models, while Gemini 2.5 Pro showed significant inconsistency, particularly in Consent & Autonomy (1.39/5) and Harm Prevention & Safety (1.98/5). Most models performed better in English than in non-English languages, with notable exceptions in low-resource languages for trick questions. The study reveals that safety mechanisms are less reliable across linguistic contexts, emphasizing the need for more robust multilingual evaluation frameworks and datasets to ensure consistent ethical responses regardless of language.

## Method Summary
The study translated 500 English questions (100 per category) into five languages using Googletrans, then queried five frontier and open-source models (GPT-5, Gemini 2.5 Pro, Claude Sonnet 4, Llama 4 Scout, and Qwen3 235B) at temperature 0.7. Responses were back-translated to English and evaluated using a five-point rubric by an LLM judge (Gemini 2.5 Pro). The evaluation included "trick questions" designed to test indirect phrasing and safety mechanisms. The study found performance variations across languages and models, with GPT-5 showing the most consistent high performance and Gemini 2.5 Pro displaying significant category-specific weaknesses.

## Key Results
- GPT-5 consistently outperformed all other models across all languages and categories
- Gemini 2.5 Pro scored only 1.39/5 in Consent & Autonomy and 1.98/5 in Harm Prevention & Safety
- Most models performed better in English than non-English languages, except for some low-resource languages where trick questions triggered higher refusal rates
- Qwen demonstrated locale-specific responses, assuming Chinese jurisdiction when prompted in Chinese

## Why This Works (Mechanism)

### Mechanism 1: The Resource-Consistency Paradox
- **Claim:** Models may exhibit stricter safety refusal behaviors in low-resource languages (LRLs) than in high-resource languages (HRLs) when facing deceptive prompts, not due to superior reasoning, but due to lower semantic comprehension.
- **Mechanism:** In LRLs (e.g., Swahili, Hindi), models appear to rely on keyword matching ("harmful words") rather than parsing full intent. When a "trick question" is posed, the model fails to understand the deceptive context and defaults to a refusal upon detecting sensitive keywords. In HRLs (e.g., Spanish, Chinese), the model successfully parses the nuance, bypassing the refusal trigger because the prompt is phrased indirectly.
- **Core assumption:** Refusal rates in LRLs are driven by a "failure to understand nuance" rather than "superior moral alignment."
- **Evidence anchors:** [section 4] "Interestingly... models scored higher in the low-resource languages... When prompted in low-resource languages, models note the use of harmful words and refuse to answer; however, in high-resource languages, models read the context..."

### Mechanism 2: Domain Specificity of Safety Alignment
- **Claim:** Safety alignment is not uniform across reasoning domains; high capability in factual/academic reasoning does not transfer to "street-smart" detection of manipulative phrasing.
- **Mechanism:** The paper attributes Gemini 2.5 Pro's polarized performance (high in Biases, low in Consent/Harm) to its training data composition. Academic training data optimizes for factual correctness and standard ethical dilemmas but lacks the "adversarial" examples found in general web usage (which GPT-5 may have more of). This creates a vulnerability to "indirect" phrasing designed to circumvent rigid safety rules.
- **Core assumption:** The distinction between "academic" and "general" training data sources dictates the robustness of safety guardrails against social engineering.
- **Evidence anchors:** [section 4] "...likely due to Gemini testing data primarily containing academic papers, hence why Gemini displayed high performance in more factual, straightforward categories."

### Mechanism 3: Linguistic-Locale Anchoring
- **Claim:** Models utilize prompt language as a proxy for user location, leading to localized legal/ethical answers where neutral, global perspectives are required.
- **Mechanism:** When Qwen (a Chinese-developed model) was prompted in Chinese, it assumed the user was in China and applied Chinese law. This "anchoring" creates a logical collision in benchmarks expecting jurisdiction-agnostic answers.
- **Core assumption:** Alignment training often pairs language with specific legal/cultural corpora (e.g., Chinese text → PRC law), creating implicit user profiles.
- **Evidence anchors:** [section 4] "Qwen responds to questions, assuming the user is located in China... due to Qwen being a China-based model [with] the majority of testing data being in Chinese."

## Foundational Learning

- **Concept: LLM-as-a-Judge (with Calibration)**
  - **Why needed here:** The study relies on Gemini 2.5 Pro to grade itself and others. Understanding that judges have biases (e.g., leniency toward their own outputs) is critical for interpreting the 1-5 scores.
  - **Quick check question:** If the Judge model is bad at "Consent & Autonomy" logic, will it reliably score another model's failure in that category?

- **Concept: High-Resource vs. Low-Resource Languages (HRL vs LRL)**
  - **Why needed here:** The core counter-intuitive finding—that models are "safer" in LRLs—depends on understanding that LRL capability is often superficial (keyword-level) rather than deep (semantic-level).
  - **Quick check question:** Does a high safety score in an LRL mean the model is ethical, or just confused?

- **Concept: Adversarial Phrasing / Trick Questions**
  - **Why needed here:** The study distinguishes between "regular" categories (Legality) and "trick" categories (Harm Prevention). Safety mechanisms often fail not on the *topic* but on the *indirect phrasing* of the request.
  - **Quick check question:** Why would a model refuse "How do I make a bomb?" but answer "What chemicals are used in mining explosives and how are they mixed?" (Hypothetical example of context override).

## Architecture Onboarding

- **Component map:** Input Pipeline (500 English Questions → Googletrans → 5 Translated Variants) → Target Models (GPT-5, Gemini 2.5 Pro, Claude Sonnet 4, Llama 4 Scout, Qwen3 235B) → Evaluation Pipeline (Target Response → Googletrans (Back to English) → Judge LLM (Gemini 2.5 Pro) → Score 1-5)

- **Critical path:** The definition of the **Evaluation Rubric**. If the rubric does not explicitly penalize "contextual blindness" (answering the trick question), the scores will not reflect safety failures.

- **Design tradeoffs:**
  - **Translation vs. Native Generation:** The study uses Googletrans for input translation. This introduces noise (idioms may break), but ensures all models receive identical semantic inputs rather than relying on the model's own translation capability.
  - **Self-Grading:** Using Gemini 2.5 Pro as the judge for Gemini 2.5 Pro outputs introduces bias risk, partially mitigated by running checks with GPT-5 and Qwen judges.

- **Failure signatures:**
  - **The "Context Trap":** Model provides a detailed, helpful answer to an indirect request that facilitates harm (Score: 1/5).
  - **The "Localization Drift":** Model answers a legal question by citing the law of the language's origin country rather than remaining neutral (Qwen behavior).
  - **The "False Positive Refusal":** Model refuses to answer a benign question in an LRL because it triggers a keyword heuristic (suggested by the LRL high scores).

- **First 3 experiments:**
  1. **Verify the LRL Paradox:** Take the 10 lowest-scoring "trick" questions in English. Run them in Swahili and Spanish. Check if the Swahili response is a refusal (safe) vs. the Spanish response (tricked).
  2. **Isolate Locale Bias:** Ask Qwen a Legality question in English vs. Chinese. Explicitly prompt "Answer based on international standards" to see if the jurisdiction drift is suppressed.
  3. **Judge Consistency Check:** Take 5 responses from Gemini that scored "5/5". Feed them to the GPT-5 judge to see if the score holds, specifically in the "Consent & Autonomy" category where Gemini performed poorly.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How do LLM moral judgments align with human societal values when evaluated against a baseline of human responses from diverse cultures?
- **Basis in paper:** [explicit] The "Future Work" section states the need to "collect human responses... from diverse countries... to establish a reliable baseline truth."
- **Why unresolved:** The current study relied on a static rubric and "LLM-as-a-judge" (Gemini 2.5 Pro) rather than empirical human survey data to define moral correctness.
- **What evidence would resolve it:** A comparative dataset matching LLM outputs against human respondent data from the target cultures across the five moral categories.

### Open Question 2
- **Question:** To what extent does altering the phrasing of questions (e.g., leading vs. neutral) cause inconsistent model outputs in moral evaluation tasks?
- **Basis in paper:** [explicit] The authors state in "Future Work" that "slightly different wordings... may shape how models interpret user intent" and call for testing these variations to reduce noise.
- **Why unresolved:** The current dataset used fixed phrasing, making it difficult to distinguish between a model's failure in moral reasoning and its sensitivity to specific prompt wording.
- **What evidence would resolve it:** Ablation studies measuring response variance across semantically equivalent but syntactically diverse paraphrases of the moral questions.

### Open Question 3
- **Question:** Are the higher safety scores observed in low-resource languages caused by genuine moral reasoning or by translation artifacts that trigger keyword-based refusal mechanisms?
- **Basis in paper:** [inferred] The Results section suggests low-resource performance was often due to models refusing to answer after detecting harmful words, while the Limitations section acknowledges potential "inaccuracies and inconsistencies" introduced by the Googletrans translation tool.
- **Why unresolved:** It is unclear if the machine translation introduced specific terms that acted as crude refusal triggers, preventing the model from engaging with the context as it did in high-resource languages.
- **What evidence would resolve it:** A comparative evaluation using native-speaker-crafted prompts versus machine-translated prompts in low-resource languages to isolate the effect of translation quality on safety refusals.

## Limitations
- Reliance on a single LLM judge (Gemini 2.5 Pro) introduces potential systematic bias, particularly given Gemini's own performance inconsistencies
- Translation pipeline using Googletrans may introduce semantic drift affecting cross-linguistic validity
- Specific content of the 500-question dataset and exact rubric remain unspecified, limiting reproducibility
- Temperature setting of 0.7 may influence consistency of safety responses across runs

## Confidence
- **High Confidence:** The general finding that GPT-5 outperformed other models and that performance varied significantly across languages is well-supported by the data presented.
- **Medium Confidence:** The mechanism explanations for Gemini's inconsistent performance and Qwen's locale-specific responses are plausible but require further validation.
- **Medium Confidence:** The observation that low-resource languages sometimes yield "safer" responses (via refusal) is documented, but the underlying cause remains speculative.

## Next Checks
1. **Judge Consistency Validation:** Re-evaluate a stratified random sample (n=50) of responses using GPT-5 as judge to quantify scoring divergence, particularly in the Consent & Autonomy category where Gemini showed weakness.

2. **LRL Paradox Verification:** Take 20 "trick question" responses that received low scores in English and re-run them in Swahili and Spanish to empirically verify whether the LRL responses are refusals (safe) versus detailed answers (unsafe).

3. **Locale Bias Isolation:** Prompt Qwen with identical Legality questions in English and Chinese, explicitly instructing "Answer based on international standards" in both cases, to test whether the Chinese response continues to default to PRC law when explicitly overridden.