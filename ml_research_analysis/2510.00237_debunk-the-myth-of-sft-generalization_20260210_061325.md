---
ver: rpa2
title: Debunk the Myth of SFT Generalization
arxiv_id: '2510.00237'
source_url: https://arxiv.org/abs/2510.00237
tags:
- training
- arxiv
- data
- instruction
- generalization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper challenges the prevailing belief that supervised fine-tuning\
  \ (SFT) inherently memorizes training data and fails to generalize, while reinforcement\
  \ learning (RL) is more robust. The authors systematically evaluate SFT on two decision-making\
  \ benchmarks, Sokoban and General Points, and identify that much of SFT's perceived\
  \ generalization failure stems from \"frozen-prompt artifacts\" \u2014 when trained\
  \ on fixed instruction templates, SFT models overfit to training semantics rather\
  \ than adapting to new ones."
---

# Debunk the Myth of SFT Generalization

## Quick Facts
- **arXiv ID:** 2510.00237
- **Source URL:** https://arxiv.org/abs/2510.00237
- **Authors:** Xiaofeng Lin; Hejian Sang; Zhipeng Wang; Xuezhou Zhang
- **Reference count:** 40
- **Primary result:** Combining prompt diversity with chain-of-thought supervision enables vanilla SFT to match or surpass RL baselines on instruction-following decision-making tasks while generalizing to unseen instruction variants and strictly harder tasks.

## Executive Summary
This paper challenges the prevailing belief that supervised fine-tuning (SFT) inherently memorizes training data and fails to generalize, while reinforcement learning (RL) is more robust. The authors systematically evaluate SFT on two decision-making benchmarks, Sokoban and General Points, and identify that much of SFT's perceived generalization failure stems from "frozen-prompt artifacts" — when trained on fixed instruction templates, SFT models overfit to training semantics rather than adapting to new ones. The core insight is that introducing prompt diversity during training breaks this shortcut, yielding strong generalization to unseen instruction variants without harming in-distribution performance. Additionally, chain-of-thought (CoT) supervision provides an algorithmic scaffold that markedly improves transfer to strictly harder tasks, such as larger Sokoban grids with additional boxes and arithmetic with out-of-distribution values or five-card compositions that increase combinatorial complexity. The primary result is that combining prompt diversity with CoT achieves the best of both worlds: robust generalization across both instruction-variant and difficulty-variant settings, matching or surpassing RL baselines on their benchmarks while retaining SFT's simplicity and stability. This demonstrates that with appropriately curated demonstrations, vanilla SFT can generalize as strongly as RL, challenging the narrative that SFT is inherently inferior to RL and supporting a data-centric perspective on model generalization.

## Method Summary
The authors evaluate SFT on two synthetic decision-making benchmarks (Sokoban and General Points) to investigate generalization capabilities. They systematically vary instruction templates during training (prompt diversity) and introduce chain-of-thought supervision to provide algorithmic scaffolding. The experimental design compares standard SFT with variants incorporating prompt diversity and CoT, measuring performance on both in-distribution test sets and generalization sets featuring unseen instruction variants and strictly harder task instances. RL baselines are included for comparison, with SFT variants evaluated against these established approaches.

## Key Results
- Prompt diversity during training breaks frozen-prompt artifacts, improving generalization to unseen instruction variants without harming in-distribution performance
- Chain-of-thought supervision provides algorithmic scaffolding that significantly improves transfer to strictly harder tasks with increased complexity
- Combined prompt diversity and CoT achieve performance matching or surpassing RL baselines on both instruction-variant and difficulty-variant test sets

## Why This Works (Mechanism)
The mechanism underlying the improved generalization centers on breaking the frozen-prompt artifacts that cause SFT models to overfit to specific training instruction semantics. When models are trained on fixed instruction templates, they learn shortcuts tied to those particular phrasings rather than developing robust semantic understanding. Prompt diversity forces the model to learn instruction-invariant representations by exposing it to varied ways of expressing the same underlying task. Chain-of-thought supervision provides intermediate reasoning steps that serve as an algorithmic scaffold, helping the model develop generalizable problem-solving strategies rather than memorizing solution patterns. Together, these techniques enable SFT to learn more flexible, transferable representations that can handle both novel instruction formulations and increased task complexity.

## Foundational Learning
- **Frozen-prompt artifacts**: When models overfit to specific instruction templates during SFT, failing to generalize to semantically equivalent but syntactically different instructions. Why needed: This explains the core limitation the paper addresses. Quick check: Does the model perform worse on semantically identical instructions with different phrasing?
- **Prompt diversity**: Training on varied instruction templates to force learning of instruction-invariant representations. Why needed: Provides the solution to frozen-prompt artifacts. Quick check: Does performance on unseen instruction variants improve with increased training template diversity?
- **Chain-of-thought supervision**: Providing intermediate reasoning steps during training to scaffold algorithmic problem-solving. Why needed: Enables transfer to harder tasks by teaching generalizable reasoning strategies. Quick check: Does CoT improve performance on tasks with increased combinatorial complexity?

## Architecture Onboarding
- **Component map**: Training data (diverse prompts + CoT demonstrations) -> SFT model -> Test on in-distribution and generalization sets (instruction-variant and difficulty-variant)
- **Critical path**: Prompt diversity generation → SFT training → Evaluation on unseen instruction variants → Difficulty scaling with CoT supervision
- **Design tradeoffs**: Prompt diversity vs. data efficiency (more templates require more data), CoT supervision vs. annotation cost (requires generating reasoning steps)
- **Failure signatures**: Overfitting to training templates, inability to handle increased task complexity, poor performance on semantically equivalent but syntactically different instructions
- **3 first experiments**: 1) Compare standard SFT vs. prompt diversity on instruction-variant generalization, 2) Evaluate CoT supervision impact on difficulty scaling, 3) Test combined prompt diversity + CoT against RL baselines

## Open Questions the Paper Calls Out
None

## Limitations
- Findings are based on synthetic decision-making benchmarks that may not directly transfer to more complex, open-ended domains
- Focus is on language-conditioned decision-making rather than general language understanding or generation tasks
- Mechanisms by which prompt diversity and CoT transfer benefits across different task types and model scales remain underexplored

## Confidence
- **High confidence**: Prompt diversity breaks frozen-prompt artifacts and improves instruction-variant generalization in controlled benchmarks
- **Medium confidence**: Chain-of-thought supervision provides algorithmic scaffolding that transfers to strictly harder tasks within the studied domains
- **Medium confidence**: Combined prompt diversity and CoT achieve the "best of both worlds" relative to RL baselines on these specific benchmarks

## Next Checks
1. Test prompt diversity and CoT techniques on open-ended language tasks (e.g., question answering, summarization) to assess generalizability beyond instruction-following decision-making
2. Evaluate performance scaling with model size (1B → 10B+ parameters) and across different instruction distributions to identify robustness boundaries
3. Conduct ablation studies on prompt diversity variants (e.g., random vs. structured variation) and CoT supervision granularity to isolate which aspects drive generalization improvements