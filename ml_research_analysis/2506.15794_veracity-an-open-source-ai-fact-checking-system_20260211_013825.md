---
ver: rpa2
title: 'Veracity: An Open-Source AI Fact-Checking System'
arxiv_id: '2506.15794'
source_url: https://arxiv.org/abs/2506.15794
tags:
- system
- misinformation
- veracity
- fact-checking
- claim
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Veracity, an open-source AI fact-checking
  system that combines Large Language Models with web retrieval agents to combat misinformation.
  The system allows users to submit claims and receive evidence-based veracity assessments
  with intuitive explanations, displaying sources and providing a numerical reliability
  score from 0% (completely false) to 100% (completely true).
---

# Veracity: An Open-Source AI Fact-Checking System

## Quick Facts
- arXiv ID: 2506.15794
- Source URL: https://arxiv.org/abs/2506.15794
- Reference count: 4
- Primary result: Open-source AI fact-checking system combining LLMs with web retrieval for evidence-based veracity assessments with numerical scoring and explanations

## Executive Summary
Veracity is an open-source AI fact-checking system that combines Large Language Models with web retrieval agents to combat misinformation. The system allows users to submit claims and receive evidence-based veracity assessments with intuitive explanations, displaying sources and providing a numerical reliability score from 0% (completely false) to 100% (completely true). Veracity supports multilingual claims, presents LLM reasoning for decisions, and features an interactive interface inspired by messaging applications. The system was designed with transparency and accessibility in mind, targeting both the general public and expert users through a dedicated expert dashboard.

## Method Summary
Veracity leverages the synergy between Large Language Models and web retrieval agents to analyze user-submitted claims and provide grounded veracity assessments. The system retrieves relevant sources using a web search engine, then uses an LLM to synthesize evidence into a numerical reliability score (0-100%) with textual justification. The architecture combines a Next.js frontend with TypeScript, a FastAPI backend on Kubernetes, and a PostgreSQL database. The system employs a multi-agent approach where the LLM determines if retrieval is needed, web agents fetch relevant sources, and the LLM produces a final assessment. No model training is described—the system uses inference with off-the-shelf LLM APIs and web search services.

## Key Results
- Provides evidence-based veracity assessments with numerical reliability scores (0-100%)
- Displays retrieved sources with credibility rankings alongside LLM verdicts
- Supports multilingual claims and presents LLM reasoning for decisions
- Features dual interfaces: general public chat interface and expert dashboard for claim trend analysis
- Open-sources the entire system to empower individuals and advance AI-powered fact-checking research

## Why This Works (Mechanism)

### Mechanism 1: LLM-Web Retrieval Agent Synergy
- Claim: Combining LLMs with web retrieval agents produces more grounded fact-checking than LLMs alone.
- Mechanism: User submits claim → LLM determines if retrieval is needed → Web agent fetches relevant sources → LLM synthesizes evidence into veracity assessment with explanation.
- Core assumption: External evidence retrieval grounds LLM outputs in verifiable sources, reducing hallucination and improving accuracy.
- Evidence anchors: [abstract]: "Veracity leverages the synergy between Large Language Models (LLMs) and web retrieval agents"; [section]: "Many studies have shown the benefits of retrieving information from online sources to improve the performance of fact-checking"; [corpus]: FMR=0.56 paper supports retrieval-augmented fact-checking approaches.
- Break condition: When claims are too recent for indexed sources, or when authoritative sources don't exist for niche topics.

### Mechanism 2: Transparent Source Attribution
- Claim: Displaying retrieved sources with credibility rankings enables user verification and builds trust.
- Mechanism: System retrieves sources → assigns credibility scores based on documented rankings → displays to user alongside LLM verdict.
- Core assumption: Users will engage with source lists and can meaningfully evaluate credibility when presented.
- Evidence anchors: [abstract]: "provide grounded veracity assessments with intuitive explanations"; [section]: "the user is also shown a list of sources, as well as their documented credibility"; [corpus]: Weak direct evidence for transparency's causal effect on user trust.
- Break condition: When credibility rankings are outdated, when niche sources lack ratings, or when users ignore source panels.

### Mechanism 3: Numerical Scoring with LLM Justification
- Claim: A 0-100% reliability score accompanied by textual reasoning improves interpretability over binary labels.
- Mechanism: LLM evaluates claim against evidence → assigns numerical score → generates free-text justification explaining the score.
- Core assumption: LLMs can reliably quantify veracity on a continuous scale and articulate reasoning that matches the score.
- Evidence anchors: [abstract]: "numerical scoring of claim veracity"; [section]: "This is the first tool of its kind to present a reliability score that represents the factuality of a user's claim"; [corpus]: No corpus papers directly validate numerical scoring vs. categorical alternatives.
- Break condition: When claims contain mixed true/false elements that resist single-score representation, or when LLM reasoning doesn't align with assigned score.

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: Core mechanism depends on augmenting LLM knowledge with external web evidence before reasoning.
  - Quick check question: Can you trace how a user claim triggers retrieval, how sources are selected, and how they're incorporated into the LLM prompt?

- Concept: Source Credibility Assessment
  - Why needed here: System must both use and display credibility rankings; understanding these ratings is essential for debugging unexpected verdicts.
  - Quick check question: What would cause a high-credibility source to disagree with a low-credibility source on the same claim?

- Concept: Prompt Engineering for Explainable Output
  - Why needed here: LLM must produce both a numerical score and coherent justification; prompt structure determines output quality.
  - Quick check question: How would you design a prompt that forces the LLM to cite specific retrieved passages in its explanation?

## Architecture Onboarding

- Component map: Frontend (Next.js + TypeScript + Sass + Chart.js) -> Backend API (FastAPI on Kubernetes) -> Database (PostgreSQL on Cloud SQL) -> External dependencies (Web search API + LLM API)
- Critical path: User input → API endpoint → LLM decision on retrieval necessity → Web agent query → Source retrieval → LLM evidence synthesis → Score + explanation generation → Database persistence → Frontend rendering of sources, score, and reasoning
- Design tradeoffs:
  - Open-source accessibility vs. hosting complexity: Users can self-host for privacy but must manage GCP/Vercel deployment
  - Numerical granularity vs. interpretability: 0-100% scores provide precision but may overstate certainty on ambiguous claims
  - General vs. expert interfaces: Dual dashboards increase maintenance burden but serve broader user base
- Failure signatures:
  - Empty sources panel: Web agent failed or LLM skipped retrieval; check API connectivity and LLM decision logic
  - Score without supporting sources: May indicate LLM relied on parametric knowledge rather than retrieved evidence
  - Low average source credibility: Claim may originate from domains outside mainstream credibility datasets
  - Explanation contradicts score: Prompt alignment issue between scoring and justification generation
- First 3 experiments:
  1. Submit a clearly false claim with established debunking (e.g., "Vaccines cause autism") and verify: (a) sources retrieved are authoritative, (b) score is low, (c) explanation cites specific evidence.
  2. Test multilingual support with a non-English claim; confirm retrieval works across languages and explanation is coherent.
  3. Submit a partially true claim with nuance (e.g., "Electric cars produce zero emissions") to observe how score and explanation handle complexity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the system's architecture be enhanced to accurately handle complex claims that contain a mixture of true and false information?
- Basis in paper: [explicit] The conclusion explicitly lists "improving the handling of complex claims" as a primary direction for future work.
- Why unresolved: Misinformation often intertwines facts with fabrications, and current binary or single-score systems may struggle to provide nuanced assessments for these compound statements.
- What evidence would resolve it: A new evaluation methodology or system module that can decompose complex claims into sub-claims and assess their veracity independently.

### Open Question 2
- Question: What interface designs and explanation styles most effectively foster user trust and media literacy?
- Basis in paper: [explicit] The introduction notes there is "much less research on how to make a high-accuracy system into a helpful and trustworthy one," and the conclusion lists "improving user interaction features" as future work.
- Why unresolved: While the system provides explanations and scores, the specific interaction patterns that maximize user comprehension and reliance without over-trust remain undetermined.
- What evidence would resolve it: User studies comparing different explanation formats (e.g., numerical scores vs. textual reasoning) to measure improvements in user media literacy and decision-making accuracy.

### Open Question 3
- Question: How can source credibility measurement be advanced beyond current rankings to better account for context and bias?
- Basis in paper: [explicit] The conclusion identifies "more advanced credibility measurement techniques" as a necessary area for development.
- Why unresolved: The current system relies on existing credibility rankings, which may not capture the nuanced reliability of a source for specific claims or contexts.
- What evidence would resolve it: The integration of a dynamic credibility assessment model that outperforms static rankings on standard fact-checking benchmarks.

## Limitations

- Technical Architecture Gaps: Specific LLM model, prompt templates, and source credibility scoring methodology remain unspecified, limiting precise reproduction capabilities.
- Evaluation Methodology: No formal user studies or quantitative accuracy metrics are presented to validate the system's effectiveness in real-world fact-checking scenarios.
- Scalability Considerations: The paper does not address rate limits, cost implications, or performance under high query volumes for the web retrieval + LLM pipeline.

## Confidence

- Mechanism Claims (High): The synergy between LLMs and web retrieval agents for fact-checking is well-supported by existing literature and the described architecture.
- User Experience Claims (Medium): The chat interface and expert dashboard concepts are straightforward, but usability benefits require empirical validation.
- Impact Claims (Low): Assertions about combating misinformation and empowering users lack quantitative evidence or user study results.

## Next Checks

1. **Accuracy Validation**: Test Veracity on a benchmark set of known true/false claims (e.g., Snopes database) to measure precision, recall, and F1-score.
2. **Source Credibility Audit**: Analyze a sample of retrieved sources across different claim domains to verify the system's source selection and credibility ranking effectiveness.
3. **User Study**: Conduct a small-scale study with diverse participants to assess whether the numerical scores and explanations improve users' ability to identify misinformation compared to traditional fact-checking methods.