---
ver: rpa2
title: The Karp Dataset
arxiv_id: '2501.14705'
source_url: https://arxiv.org/abs/2501.14705
tags:
- reductions
- independent
- problem
- cover
- vertex
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Karp dataset, the first dataset composed
  of detailed proofs of NP-completeness reductions. The dataset contains 90 reductions
  ranging from simple undergraduate exercises to challenging academic proofs, spanning
  over 170 pages and averaging approximately 2000 characters per reduction.
---

# The Karp Dataset

## Quick Facts
- arXiv ID: 2501.14705
- Source URL: https://arxiv.org/abs/2501.14705
- Reference count: 40
- Primary result: First dataset of detailed NP-completeness reduction proofs; Strawberry model outperforms Llama and fine-tuned variants on both test and challenge sets

## Executive Summary
This paper introduces the Karp dataset, the first dataset composed of detailed proofs of NP-completeness reductions. The dataset contains 90 reductions ranging from simple undergraduate exercises to challenging academic proofs, spanning over 170 pages and averaging approximately 2000 characters per reduction. Experiments compare state-of-the-art models including OpenAI's Strawberry, Llama 70B-Instruct, and a fine-tuned LlamaReduce model on two problem sets: a test set of 8 undergraduate-level reductions and a challenge set of 8 more difficult reductions. Strawberry achieves the highest performance with average scores of 1.5 on the test set and 0.875 on the challenge set, outperforming base Llama (0.875 and 0.375 respectively) and LlamaReduce (1.25 and 0.5 respectively). The results demonstrate that reductions are challenging for LLMs, with fine-tuning providing benefits for easier problems but limited improvement for harder ones.

## Method Summary
The study constructs a dataset of 90 NP-completeness reduction proofs from literature sources, formatted in LaTeX with a strict template structure (Theorem, Reduction, Proof of Correctness). Three models are evaluated: base Llama 70B-Instruct, OpenAI's Strawberry (o1), and LlamaReduce (fine-tuned on 82 reductions via QLoRA). All models are prompted using a structured template requiring explicit definition of mappings and biconditional proofs. Human experts manually score outputs on a 0-2 scale for correctness. The evaluation uses two test sets: 8 undergraduate-level reductions and 8 more challenging reductions not present in the training data.

## Key Results
- Strawberry achieves average scores of 1.5 on test set and 0.875 on challenge set
- Llama 70B-Instruct scores 0.875 on test set and 0.375 on challenge set
- LlamaReduce scores 1.25 on test set and 0.5 on challenge set
- Fine-tuning improves performance on easier problems but shows limited benefit for harder reductions

## Why This Works (Mechanism)

### Mechanism 1: Inference-time compute scaling
- **Claim:** Inference-time compute scaling (repeated prompting/self-evaluation) appears to improve performance on complex reduction tasks more effectively than standard instruction tuning.
- **Mechanism:** OpenAI's Strawberry (o1) model outperformed Llama 70B-Instruct significantly on the test set (1.5 vs 0.875). The authors attribute this to Strawberry's design philosophy of scaling inference-time compute, allowing the model to "think" longer or self-correct before outputting the final reduction.
- **Core assumption:** Assumption: Higher scores correlate directly with the model's internal self-correction capabilities rather than simply having seen similar proofs in pre-training data (though data contamination is a known concern mentioned in Section 1.2).
- **Evidence anchors:**
  - [Section 3] "The impressive performance of Strawberry provides additional evidence that prompt engineering has a significant effect... design philosophy of Strawberry is to scale-up inference-time compute with repeated prompting and self-evaluation."
  - [Section 3] Table 2 shows Strawberry achieving a perfect score (2) on 5 out of 8 test set problems, whereas Llama only achieved 2 on 3 out of 8.
  - [Corpus] Weak corpus evidence; neighbors focus on equivalence checking (EquivaMap) rather than inference-time scaling mechanisms for proofs.

### Mechanism 2: Fine-tuning generalization gap
- **Claim:** Fine-tuning on domain-specific reduction data improves performance on problems of similar structure/difficulty but fails to generalize to novel, harder constructions.
- **Mechanism:** LlamaReduce (fine-tuned Llama 70B) improved over the base model on the test set (1.25 vs 0.875) but failed to show significant improvement on the challenge set (0.5 vs 0.375). This suggests the model learned surface-level patterns or "tricks" for standard undergraduate reductions (e.g., Independent Set to Vertex Cover) rather than acquiring a generalized reasoning capability for NP-completeness.
- **Core assumption:** Assumption: The "Challenge set" represents a true shift in reasoning complexity rather than just a data distribution shift that could be solved with more fine-tuning data.
- **Evidence anchors:**
  - [Section 3] "Fine-tuning was beneficial in improving performance... [but] appear to be less effective for improving performance for the harder reductions such as those in the challenge dataset."
  - [Abstract] "...fine-tuning providing modest improvements for easier problems but limited benefit for harder ones."
  - [Corpus] "Atomic Thinking of LLMs" (neighbor) suggests current reasoning models rely heavily on scaling datasets, which aligns with LlamaReduce struggling when simple pattern matching fails on harder problems.

### Mechanism 3: Structured template guidance
- **Claim:** Highly structured templates decouple the "construction" of the reduction from the "verification" of correctness, partially guiding model reasoning.
- **Mechanism:** The Karp dataset enforces a strict template: Definition of mapping → Proof of Correctness (⟹ and ⟸). By forcing the model to generate the "Proof of Correctness" explicitly after the "Reduction," the prompt structure compels the model to attempt verification logic it might otherwise omit.
- **Core assumption:** Assumption: The explicit structure in the prompt causes the model to generate more logically sound reductions than it would with a free-form "prove X" prompt.
- **Evidence anchors:**
  - [Section 2] "Reductions in the dataset adhere to a highly structured template... A precise definition of the mapping followed by a proof of correctness."
  - [Section 4] Shows the prompt template explicitly asking for "Proof of Correctness... It remains to prove that x is 'yes' iff y is 'yes'".
  - [Corpus] Weak corpus evidence; no direct comparison of structured vs. unstructured prompting found in neighbors.

## Foundational Learning

- **Concept: Polynomial-Time Reductions**
  - **Why needed here:** The entire dataset is built on the concept of mapping one NP problem to another to prove hardness. Without understanding that A ≤p B means "if we can solve B efficiently, we can solve A efficiently," the model outputs are just text generation, not mathematical proofs.
  - **Quick check question:** Given two problems A and B, does a reduction from A to B prove that B is hard, or that A is hard? (Answer: It proves B is at least as hard as A).

- **Concept: Biconditional Proofs (If and only if)**
  - **Why needed here:** The "Proof of Correctness" in the Karp dataset requires proving x ∈ A ⟺ y ∈ B. Models often fail on the "backwards" direction (⟸), as seen in the Clique to Balanced Biclique example where "misaligned" bicliques break the logic.
  - **Quick check question:** In a reduction proof, why is proving "If y is a yes-instance, then x is a yes-instance" necessary? (Answer: To ensure the solution to the new problem actually maps back to a valid solution of the original problem).

- **Concept: NP-Completeness vs. NP-Hardness**
  - **Why needed here:** The paper mentions "omitted details" regarding polynomial-time verification. To evaluate the models, one must distinguish between a reduction proving NP-Hardness and the full requirements for NP-Completeness (which also requires membership in NP).
  - **Quick check question:** If a reduction proves a problem is NP-Hard, is it automatically NP-Complete? (Answer: No, it must also be shown to be in NP).

## Architecture Onboarding

- **Component map:** Dataset (90 LaTeX reductions) → Models (Base Llama 70B, Strawberry/o1, LlamaReduce) → Prompt Interface (Structured LaTeX template) → Evaluator (Human Expert scoring 0-2)

- **Critical path:** The path from Prompt Construction to Manual Evaluation. The prompt must precisely define the source and destination problems. The model then generates the mapping. The human evaluator must parse the LaTeX, check for "superficial bugs," and then verify the biconditional logic (as seen in Section 4).

- **Design tradeoffs:**
  - **Natural Language vs. Formal Code:** The dataset uses natural language (LaTeX) for proofs, making it accessible for LLMs but difficult to automatically verify (requiring human graders). A code-based approach (like the "Karp" Racket framework mentioned in Related Work) allows automatic testing but is harder for standard LLMs to generate.
  - **Verbosity vs. Clarity:** The authors chose clarity/verbosity (170 pages) over compressed textbook proofs to reduce ambiguity, increasing token costs for training/inference.

- **Failure signatures:**
  - **"Naive" Reductions:** Models may output convincing but logically flawed reductions (e.g., Clique to Balanced Biclique) that fail on edge cases (misaligned sets) while sounding correct.
  - **Missing Constraints:** Models may define a mapping but fail to prove the "only if" direction (⟸), resulting in a "significant yet fixable flaw" (Score of 1).
  - **Template Hallucination:** Models might generate valid LaTeX that doesn't actually define a function or skips the reduction step entirely.

- **First 3 experiments:**
  1. **Baseline Assessment:** Run Llama 70B-Instruct on the 8-problem "Test Set" using the strict prompt template to establish a baseline score (expected ~0.875).
  2. **Ablation on Difficulty:** Evaluate the baseline model on the "Challenge Set" to confirm the performance drop (expected ~0.375) and identify if failures are due to construction errors or proof logic errors.
  3. **Fine-Tuning Loop:** Fine-tune Llama 70B using QLoRA (as per Appendix B) on the remaining 82 reductions. Re-evaluate on the Test Set to verify the expected uplift (~1.25) and check for degradation on the Challenge Set to confirm the generalization gap.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can generative AI utilize this dataset to discover novel reductions or simplify known NP-completeness proofs?
- **Basis in paper:** [explicit] The authors state that using the Karp dataset "to discover new reductions and simplify known NP-completeness proofs is an exciting future direction."
- **Why unresolved:** The current study focuses on reproducing existing proofs from literature rather than generating new mathematical insights.
- **What evidence would resolve it:** An LLM outputting a valid, previously unknown reduction or a shorter proof for an existing reduction.

### Open Question 2
- **Question:** How can the bottleneck of manual evaluation be overcome to enable automatic verification of natural language proofs?
- **Basis in paper:** [explicit] The paper identifies that "The lack of automatic verification for natural language proofs of NP-completeness is a bottleneck" and suggests transformation to structured representations like the Karp language.
- **Why unresolved:** Current models failed to judge correctness reliably, forcing the authors to rely on time-consuming manual scoring by human experts.
- **What evidence would resolve it:** A reliable translator that converts natural language proofs into code or formal logic for automated verification.

### Open Question 3
- **Question:** Can the dataset be extended to include reductions establishing the hardness of approximation for NP-hard optimization problems?
- **Basis in paper:** [explicit] The conclusion suggests, "Future work could examine extending the dataset with additional reductions (e.g., reductions establishing hardness of approximation...)."
- **Why unresolved:** The current dataset is restricted to standard NP-completeness decision problems and does not cover approximation-preserving reductions.
- **What evidence would resolve it:** The creation of a benchmark subset specifically for hardness of approximation proofs.

## Limitations
- The dataset requires manual expert evaluation for scoring, creating a scalability bottleneck
- Dataset access requires direct email request rather than public availability
- Paper does not address potential data contamination issues
- Limited comparison to only three model variants without exploring intermediate approaches

## Confidence
- **High Confidence:** The dataset construction methodology and template structure are well-documented and reproducible. The observation that Strawberry outperforms Llama significantly on test set reductions (1.5 vs 0.875) is directly supported by Table 2 and the explicit scoring methodology. The claim that fine-tuning helps on easier problems but not harder ones is clearly demonstrated through the comparative scores (1.25 vs 0.5 on test vs challenge sets).
- **Medium Confidence:** The attribution of Strawberry's performance to inference-time compute scaling is plausible but not definitively proven, as the paper does not conduct controlled experiments isolating this mechanism. The assertion that the challenge set represents genuinely harder reasoning problems (rather than just distributional differences) is reasonable but not empirically validated through additional difficulty metrics.
- **Low Confidence:** The generalization gap observed in fine-tuning (good on test set, poor on challenge set) could be due to multiple factors beyond the proposed mechanism of learning surface patterns versus deep reasoning. Without exploring alternative fine-tuning approaches or larger datasets, the specific cause remains uncertain.

## Next Checks
1. **Automatic Verification Protocol Development:** Implement a preliminary automatic checker using symbolic execution or constraint solving to verify the logical correctness of reductions. Start with a subset of problems (e.g., Partition→Knapsack) and measure agreement with human scores to establish reliability metrics.

2. **Controlled Inference-Time Experiment:** Create a Llama-based model with explicit self-correction loops (multiple passes through the same reduction problem) and compare its performance to Strawberry on the challenge set. This would isolate whether inference-time compute scaling or other architectural differences drive the performance gap.

3. **Fine-Tuning Data Scaling Study:** Fine-tune LlamaReduce on progressively larger subsets of the 90 reductions (e.g., 30, 60, 90 problems) and evaluate on both test and challenge sets. This would quantify whether the generalization gap persists with more training data or whether it's fundamentally a capacity issue.