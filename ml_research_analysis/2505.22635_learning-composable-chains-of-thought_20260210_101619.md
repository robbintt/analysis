---
ver: rpa2
title: Learning Composable Chains-of-Thought
arxiv_id: '2505.22635'
source_url: https://arxiv.org/abs/2505.22635
tags:
- compositional
- letter
- data
- atomic
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of compositional generalization
  in large language models, where the goal is to combine atomic reasoning skills to
  solve unseen, more complex tasks. The authors propose a data augmentation scheme
  called Composable Chain-of-Thought (CoT), which modifies the format of atomic CoT
  training data to include explicit prefix and suffix tags, enabling models to generate
  composable reasoning traces.
---

# Learning Composable Chains-of-Thought

## Quick Facts
- arXiv ID: 2505.22635
- Source URL: https://arxiv.org/abs/2505.22635
- Reference count: 40
- Primary result: Composable CoT format improves compositional reasoning through data augmentation and multitask training

## Executive Summary
This paper addresses compositional generalization in large language models by proposing a data augmentation scheme called Composable Chain-of-Thought (CoT). The key insight is that atomic reasoning skills can be combined to solve unseen, more complex tasks if trained with explicit compositional structure. By modifying atomic CoT training data with prefix and suffix tags, the approach enables models to generate composable reasoning traces that can be combined via multitask learning or model merging. The method demonstrates significant improvements in zero-shot compositional reasoning performance across string operation and natural language skill composition tasks.

## Method Summary
The authors introduce Composable Chain-of-Thought (CoT), a data augmentation technique that modifies standard atomic CoT training data by adding explicit prefix and suffix tags. These tags indicate where reasoning chains should begin and end, allowing atomic reasoning components to be more easily combined. The approach involves training atomic CoT models with this augmented format, then combining them through either multitask learning (joint training on multiple atomic skills) or model merging techniques. For further improvement, the authors employ rejection sampling fine-tuning on limited compositional data, generating multiple reasoning traces per example and selecting the best-performing ones for training. This multi-stage approach enables models to better handle unseen compositional tasks by leveraging learned atomic reasoning skills.

## Key Results
- Zero-shot exact match accuracy reaches 95.4% on string operation tasks
- Significant performance gains over standard CoT formats on compositional reasoning benchmarks
- Strong improvements with limited supervision through rejection sampling fine-tuning
- Demonstrated effectiveness of multitask learning and model merging for combining atomic reasoning skills

## Why This Works (Mechanism)
The approach works by explicitly structuring atomic reasoning traces to be composable. By adding prefix/suffix tags to CoT training data, the model learns to identify reasoning boundaries and can more easily combine atomic skills. The multitask learning and model merging approaches then leverage these structured skills to handle novel compositional tasks. The rejection sampling fine-tuning stage further refines the model's ability to select appropriate reasoning chains for complex problems.

## Foundational Learning
**Chain-of-Thought Reasoning**: Why needed - provides step-by-step explanations for complex problems; Quick check - model can generate intermediate reasoning steps
**Compositional Generalization**: Why needed - ability to combine learned skills for novel tasks; Quick check - performance on unseen task combinations
**Data Augmentation**: Why needed - increases training diversity without manual annotation; Quick check - improved performance on test set
**Multitask Learning**: Why needed - enables simultaneous learning of multiple related skills; Quick check - joint training improves individual task performance
**Model Merging**: Why needed - combines strengths of separately trained models; Quick check - merged model outperforms individual components
**Rejection Sampling**: Why needed - selects highest-quality generations for fine-tuning; Quick check - improved performance after fine-tuning

## Architecture Onboarding
**Component Map**: Input data -> Prefix/Suffix tagging -> Atomic CoT training -> Multitask learning/Merging -> Rejection sampling fine-tuning -> Compositional reasoning
**Critical Path**: Data augmentation with tags → Atomic skill training → Skill combination → Fine-tuning on compositional data
**Design Tradeoffs**: Prefix/suffix tagging adds structure but requires careful prompt engineering; multitask learning vs model merging offers flexibility in skill combination approaches
**Failure Signatures**: Poor performance on compositional tasks indicates insufficient atomic skill training or ineffective tag utilization
**First Experiments**: 1) Test prefix/suffix tagging effectiveness on simple compositional tasks; 2) Compare multitask learning vs model merging performance; 3) Evaluate rejection sampling impact on limited compositional data

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focuses primarily on synthetic tasks, limiting real-world applicability assessment
- Data augmentation strategy may not generalize well to diverse task formats
- Computational overhead of rejection sampling fine-tuning could limit scalability
- Performance gains measured against narrow task sets, uncertain broader generalization

## Confidence
- Core technical contribution: High confidence
- Generalization claims: Medium confidence
- Practical utility: Medium confidence

## Next Checks
1. Evaluate Composable CoT approach on broader reasoning tasks from established benchmarks like BIG-Bench
2. Conduct ablation studies to isolate contributions of prefix/suffix tagging, multitask learning, and rejection sampling
3. Test approach's robustness across different model scales and architectures, comparing smaller models to larger frontier models