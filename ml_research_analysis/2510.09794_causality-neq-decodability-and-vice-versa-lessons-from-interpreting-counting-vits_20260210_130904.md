---
ver: rpa2
title: 'Causality $\neq$ Decodability, and Vice Versa: Lessons from Interpreting Counting
  ViTs'
arxiv_id: '2510.09794'
source_url: https://arxiv.org/abs/2510.09794
tags:
- object
- patching
- layers
- information
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work demonstrates that decodability and causality are distinct
  and complementary measures in mechanistic interpretability. Using activation patching
  to assess causal influence and linear probing to assess decodability in a vision
  transformer fine-tuned for object counting, the authors find systematic mismatches:
  middle-layer object tokens exert strong causal influence despite weak decodability,
  while final-layer object tokens support high decodability but are functionally inert.'
---

# Causality $\neq$ Decodability, and Vice Versa: Lessons from Interpreting Counting ViTs
## Quick Facts
- arXiv ID: 2510.09794
- Source URL: https://arxiv.org/abs/2510.09794
- Reference count: 22
- Primary result: Decodability and causality are distinct measures in mechanistic interpretability; both are necessary for comprehensive analysis.

## Executive Summary
This paper investigates the relationship between decodability (how well a representation can be read) and causality (how much a representation influences model behavior) in vision transformers trained for object counting. Using activation patching to measure causal influence and linear probing to measure decodability, the authors find systematic mismatches: middle-layer object tokens have strong causal effects but weak decodability, while final-layer object tokens show the opposite pattern. CLS tokens also demonstrate decodability before acquiring causal power. These findings reveal that probing and patching capture different aspects of representation use, and both are necessary for comprehensive interpretability analysis.

## Method Summary
The authors analyze a vision transformer fine-tuned for object counting using two complementary approaches. For decodability, they train linear probes on intermediate representations to measure how well they predict the count target. For causality, they employ activation patching to assess how much removing or replacing individual token representations affects the model's output. They systematically compare these measures across different token types (CLS, object tokens) and layers, revealing systematic mismatches between what can be decoded and what is causally important for the model's behavior.

## Key Results
- Middle-layer object tokens exert strong causal influence on model outputs despite weak decodability
- Final-layer object tokens support high decodability but are functionally inert (low causal influence)
- CLS tokens show decodability before acquiring causal power, indicating temporal decoupling
- Decodability and causality are complementary measures that capture different aspects of representation use

## Why This Works (Mechanism)
The paper demonstrates that linear probing measures representational information content while activation patching measures functional contribution. This decoupling occurs because representations can encode information that the model doesn't use (high decodability, low causality) or the model can use information that's hard to decode (high causality, low decodability). The vision transformer's architecture and counting task create conditions where these patterns emerge systematically across layers and token types.

## Foundational Learning
- Activation patching: intervention method to measure causal influence by replacing activations and observing output changes; needed to assess functional importance beyond passive information content
- Linear probing: training linear classifiers on intermediate representations to measure decodability; needed as a passive measure of information content
- Mechanistic interpretability: approach to understanding model behavior by analyzing internal representations and their causal roles; needed as the conceptual framework connecting the analysis
- Vision transformer architecture: transformer models adapted for vision tasks with CLS and object tokens; needed as the model family being analyzed
- Object counting task: supervised learning task predicting the number of objects in an image; needed as the specific task revealing these patterns

## Architecture Onboarding
- Component map: CLS token -> Layer 0 -> Layer 1 -> ... -> Layer N -> Output; Object tokens flow in parallel through layers
- Critical path: CLS token through all layers for final prediction, with object tokens providing intermediate representations
- Design tradeoffs: Linear probes are computationally efficient but may miss non-linear relationships; activation patching is more expensive but captures functional roles
- Failure signatures: High decodability with low causality suggests unused information; high causality with low decodability suggests opaque but important representations
- First experiments: 1) Compare linear probe accuracy across layers for CLS vs object tokens, 2) Apply activation patching to remove CLS token at different layers, 3) Plot causality vs decodability for all token-layer combinations

## Open Questions the Paper Calls Out
None

## Limitations
- Analysis focused on a specific task (object counting) and model architecture (ViT)
- Linear probing may underestimate decodability if non-linear readouts are needed
- Activation patching assumes discrete interventions capture continuous information flow
- Results may not generalize to other vision tasks or model families

## Confidence
- Causality and decodability are distinct: High
- Both measures are necessary for comprehensive interpretability: High
- Systematic mismatches exist in ViTs for counting: Medium
- Findings generalize beyond counting task: Low

## Next Checks
- Replicate findings on object detection or classification tasks to test generalizability
- Test non-linear probes (small MLPs) to see if decodability estimates change
- Apply intervention methods with continuous interpolation rather than discrete patching