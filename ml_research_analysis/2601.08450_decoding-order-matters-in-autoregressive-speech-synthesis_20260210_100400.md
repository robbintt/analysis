---
ver: rpa2
title: Decoding Order Matters in Autoregressive Speech Synthesis
arxiv_id: '2601.08450'
source_url: https://arxiv.org/abs/2601.08450
tags:
- decoding
- speech
- order
- diffusion
- autoregressive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how decoding order impacts autoregressive
  speech synthesis by integrating the masked diffusion model framework with text-to-speech
  synthesis. Through experiments on the LJSpeech dataset, the authors demonstrate
  that adaptive decoding strategies, such as Top-K and duration-guided decoding, consistently
  outperform fixed orders like left-to-right (l2r) or right-to-left (r2l), with duration-guided
  decoding achieving the best MCD of 3.98, logF0 of 5.67, and UTMOSv2 of 3.98.
---

# Decoding Order Matters in Autoregressive Speech Synthesis

## Quick Facts
- arXiv ID: 2601.08450
- Source URL: https://arxiv.org/abs/2601.08450
- Reference count: 0
- Key outcome: Adaptive decoding strategies (Top-K, duration-guided) outperform fixed orders in speech synthesis, with duration-guided achieving MCD 3.98, logF0 5.67, and UTMOSv2 3.98

## Executive Summary
This paper investigates how decoding order impacts autoregressive speech synthesis by integrating the masked diffusion model framework with text-to-speech synthesis. Through experiments on the LJSpeech dataset, the authors demonstrate that adaptive decoding strategies, such as Top-K and duration-guided decoding, consistently outperform fixed orders like left-to-right (l2r) or right-to-left (r2l), with duration-guided decoding achieving the best MCD of 3.98, logF0 of 5.67, and UTMOSv2 of 3.98. The authors also show that even 1-bit quantization of Mel-spectrograms preserves partial intelligibility, and that varying the randomness in decoding order affects synthesis quality, with intermediate randomness yielding the best pitch preservation. These findings highlight that left-to-right decoding is suboptimal and that adaptive strategies can significantly improve speech synthesis quality.

## Method Summary
The authors propose a framework that combines masked diffusion models with autoregressive text-to-speech synthesis, where the model learns to predict speech features in a specific decoding order. They experiment with various decoding strategies including left-to-right, right-to-left, Top-K (selecting tokens based on probability), and duration-guided decoding (aligning predictions with phoneme durations). The model is trained on the LJSpeech dataset and evaluated using MCD, logF0, and UTMOSv2 metrics. The framework allows for flexible decoding orders during inference, enabling comparison between fixed and adaptive strategies. The authors also explore quantization effects by reducing Mel-spectrogram precision to 1-bit and analyze how randomness in decoding order impacts pitch preservation.

## Key Results
- Duration-guided decoding achieves the best performance with MCD 3.98, logF0 5.67, and UTMOSv2 3.98
- Adaptive strategies (Top-K, duration-guided) consistently outperform fixed orders (l2r/r2l)
- 1-bit quantization of Mel-spectrograms preserves partial intelligibility
- Intermediate randomness in decoding order yields optimal pitch preservation

## Why This Works (Mechanism)
The paper demonstrates that decoding order is not merely a technical detail but a fundamental factor affecting synthesis quality. Adaptive strategies work better because they can leverage contextual information more effectively - for instance, duration-guided decoding aligns predictions with phoneme boundaries, providing structural guidance that fixed orders lack. The observation that intermediate randomness produces optimal pitch preservation suggests there's a sweet spot between deterministic and highly stochastic decoding that balances stability with natural variation. The 1-bit quantization results indicate that even highly compressed representations retain enough information for partial intelligibility, suggesting robustness in the underlying speech representation.

## Foundational Learning

**Autoregressive models**: Models that generate output sequentially, where each prediction depends on previous outputs. Why needed: Core architecture for the speech synthesis approach. Quick check: Does the model generate one token at a time based on previous predictions?

**Masked diffusion models**: Models trained to predict missing or masked portions of data. Why needed: Provides the framework for flexible decoding orders. Quick check: Can the model predict tokens in arbitrary positions given partial context?

**Mel-spectrograms**: Time-frequency representations of audio commonly used in speech synthesis. Why needed: Standard intermediate representation between text and waveform. Quick check: Are the speech features represented as 2D matrices over time and frequency?

**MCD (Mel-Cepstral Distortion)**: Metric measuring spectral distance between synthesized and ground truth speech. Why needed: Standard objective evaluation metric for speech quality. Quick check: Lower MCD values indicate better spectral similarity.

**F0 (Fundamental frequency)**: The pitch of speech signals. Why needed: Critical for naturalness and intelligibility of synthesized speech. Quick check: Does the model preserve pitch contours and intonation patterns?

## Architecture Onboarding

**Component map**: Text input -> Phoneme duration prediction -> Masked diffusion model with flexible decoding order -> Mel-spectrogram generation -> Waveform synthesis

**Critical path**: The autoregressive generation loop where each token prediction depends on the decoding order strategy. The critical path involves: token selection based on decoding strategy → context update → next token prediction → repeat until completion.

**Design tradeoffs**: Fixed orders (l2r/r2l) offer simplicity and computational efficiency but lack adaptability. Adaptive strategies (Top-K, duration-guided) provide better performance but add complexity in implementation and may increase inference time. The tradeoff between randomness and determinism affects both quality and controllability.

**Failure signatures**: Fixed orders may struggle with long-range dependencies and produce unnatural prosody. Too much randomness can lead to unstable pitch and reduced intelligibility. Poor alignment between decoding order and linguistic structure can cause artifacts in synthesized speech.

**First experiments to run**:
1. Compare duration-guided decoding vs. l2r on a held-out test set using MCD and logF0
2. Vary quantization levels (4-bit, 2-bit, 1-bit) to establish intelligibility thresholds
3. Test different levels of randomness in decoding order to find the optimal balance for pitch preservation

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation conducted exclusively on LJSpeech dataset, limiting generalizability to diverse speech corpora
- Absolute improvements in MCD and logF0, while statistically significant, represent modest practical gains
- 1-bit quantization results show partial intelligibility preservation but lack detailed perceptual analysis
- Analysis of why intermediate randomness yields optimal pitch preservation is empirical rather than theoretically grounded

## Confidence
- High confidence: The demonstration that adaptive decoding strategies outperform fixed orders on LJSpeech
- Medium confidence: The claim that decoding order is a critical factor in speech synthesis quality (limited to single dataset)
- Low confidence: The generalizability of findings to other speech datasets or real-world applications

## Next Checks
1. Replicate experiments on diverse datasets (multispeaker, noisy, multilingual) to test generalization of decoding order effects
2. Conduct large-scale human preference studies comparing adaptive decoding vs. standard left-to-right synthesis across multiple naturalness dimensions
3. Analyze synthesis failure cases systematically to understand when and why specific decoding orders degrade quality