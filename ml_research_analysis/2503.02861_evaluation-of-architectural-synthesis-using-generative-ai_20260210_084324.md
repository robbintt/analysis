---
ver: rpa2
title: Evaluation of Architectural Synthesis Using Generative AI
arxiv_id: '2503.02861'
source_url: https://arxiv.org/abs/2503.02861
tags:
- architectural
- design
- parts
- step
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a comparative evaluation of two multimodal
  generative AI systems, GPT-4o and Claude 3.5 Sonnet, for architectural 3D synthesis.
  Using two Palladian buildings as case studies, the authors assess the systems' ability
  to interpret architectural drawings and generate CAD scripts in OpenSCAD.
---

# Evaluation of Architectural Synthesis Using Generative AI

## Quick Facts
- arXiv ID: 2503.02861
- Source URL: https://arxiv.org/abs/2503.02861
- Reference count: 4
- Claude 3.5 Sonnet achieved 73.12% average performance for Villa Rotonda and 73.43% for Palazzo Porto, outperforming GPT-4o's 61.56% and 65% respectively

## Executive Summary
This paper presents a comparative evaluation of two multimodal generative AI systems, GPT-4o and Claude 3.5 Sonnet, for architectural 3D synthesis. Using two Palladian buildings as case studies, the authors assess the systems' ability to interpret architectural drawings and generate CAD scripts in OpenSCAD. The evaluation methodology involves sequential text and image prompting, with a binary scoring matrix assessing part generation, positioning, proportion, orientation, and spatial relationships. Results show Claude 3.5 Sonnet achieved higher overall performance and better self-correction capabilities, while both systems struggled with accurate spatial assembly of parts.

## Method Summary
The study evaluates GPT-4o and Claude 3.5 Sonnet using a five-step sequential prompting protocol with custom architectural drawing sheets. Buildings are decomposed into three parts each, with the AI systems generating OpenSCAD code for individual parts (Steps 1-3), whole assembly (Step 4), and iterative refinement using rendered output images (Steps 5+). A binary scoring matrix evaluates part generation, positioning, proportion, orientation, and spatial relationships. The process uses temperature=1.0 and continues refinement until outputs degrade or repeat.

## Key Results
- Claude 3.5 Sonnet outperformed GPT-4o on both case studies (Villa Rotonda: 73.12% vs 61.56%; Palazzo Porto: 73.43% vs 65%)
- Both systems successfully generated individual parts with correct positions and orientations
- Both systems struggled to accurately assemble parts into desired spatial relationships
- Claude 3.5 demonstrated better self-correction capabilities during iterative refinement

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sequential decomposition of complex spatial synthesis into discrete steps improves part-level generation accuracy.
- Mechanism: The methodology separates individual part generation (Steps 1-3) from whole assembly (Step 4), reducing cognitive load on the model by constraining each task to simpler geometries before combining them. The binary scoring matrix explicitly tracks part generation, positioning, proportion, and orientation as independent dimensions.
- Core assumption: Architectural wholes can be meaningfully decomposed into describable geometric primitives that retain semantic relationships when reassembled.
- Evidence anchors:
  - [abstract] "Both systems successfully generated individual parts with correct positions and orientations, they struggled to accurately assemble parts into desired spatial relationships"
  - [section 3.3] "The fact that the AI systems show better performance when dealing with individual component parts suggests that current AI models generally perform better with less complex tasks"
  - [corpus] CAD-Coder (arXiv:2505.08686) explores text-guided CAD generation but does not address decomposition strategies for complex assemblies
- Break condition: When spatial relationships between parts cannot be captured through sequential independent descriptions—e.g., interlocking geometries where part boundaries are mutually dependent.

### Mechanism 2
- Claim: Visual feedback loops enable limited self-correction of generated CAD outputs, with system-dependent effectiveness.
- Mechanism: Step 5+ provides the model with four-view renderings of its own OpenSCAD output, allowing comparison against reference drawings. The model attempts iterative refinement until outputs stabilize or degrade. Claude 3.5 demonstrated more consistent refinement; GPT-4o showed earlier degradation.
- Core assumption: The vision-language model can meaningfully compare its rendered output against reference drawings and identify discrepancies in a way that translates to corrected code.
- Evidence anchors:
  - [abstract] "Claude 3.5 demonstrated better self-correction capabilities"
  - [section 2.2] "Step 5 was repeated as necessary until satisfactory results were achieved or the outputs became repetitive or began to deteriorate, indicating the AI system has reached its capacity for self-refinement"
  - [corpus] Human-in-the-Loop (arXiv:2509.07010) proposes quantitative evaluation frameworks for LLM-generated 3D models but does not address autonomous self-correction loops
- Break condition: When visual comparison fails to detect systematic errors (e.g., consistent proportion misestimation) or when corrections introduce new errors faster than resolving existing ones.

### Mechanism 3
- Claim: Structured visual input sheets combining multiple representation types (plan, section, axonometric, part-to-whole diagrams) support multimodal interpretation more effectively than original historical drawings.
- Mechanism: Custom high-level models replaced Palladio's original detailed drawings after pilot studies yielded unsatisfactory results. The prepared drawing sheets standardize visual vocabulary and coordinate systems (red dot origin), reducing ambiguity in interpretation.
- Core assumption: Abstracted, consistent representations reduce interpretation variance compared to historically authentic but visually heterogeneous source material.
- Evidence anchors:
  - [section 2.1] "After obtaining unsatisfactory results from a preliminary pilot study that used Palladio's own detailed drawings, new custom high-level models were created"
  - [section 2.2] "Specific requirements for the task included utilising the red dot as the origin in the shared coordinate system for all parts"
  - [corpus] Weak direct evidence—neighbor papers focus on symbol spotting and information extraction rather than representation standardization effects
- Break condition: When abstraction eliminates critical geometric detail needed for accurate reconstruction, or when standardized representations fail to capture idiosyncratic architectural features.

## Foundational Learning

- Concept: **Constructive Solid Geometry (CSG) in OpenSCAD**
  - Why needed here: The target output format relies on programmatic 3D construction through boolean operations (union, difference, intersection) on primitives, not mesh-based modeling.
  - Quick check question: Can you explain how a complex architectural form like a domed hall with four loggias would be decomposed into CSG operations?

- Concept: **Multimodal vision-language grounding**
  - Why needed here: The systems must map 2D visual features from drawings to 3D spatial predicates in code—understanding that a plan view rectangle at position (x,y) corresponds to a 3D extrusion requires cross-modal alignment.
  - Quick check question: Given a floor plan and section drawing, how would you verify that an AI system correctly inferred the z-height of a volume?

- Concept: **Spatial relationship taxonomies**
  - Why needed here: The evaluation framework distinguishes position, orientation, proportion, and spatial relationships as separate scored dimensions—understanding what "correct spatial relationship" means distinct from correct positioning is essential.
  - Quick check question: If two building parts are individually positioned correctly but the entrance loggia is rotated 45° relative to the main hall, which evaluation dimension(s) would be marked incorrect?

## Architecture Onboarding

- Component map: Drawing sheets -> Vision encoder -> Multimodal model -> OpenSCAD script -> Rendered model images -> Visual comparison -> Refinement prompts

- Critical path: Accurate coordinate system interpretation -> Correct part dimensions -> Correct part positioning -> Correct spatial relationships between parts

- Design tradeoffs:
  - Decomposition depth: More granular parts improve individual accuracy but increase assembly complexity
  - Iteration limits: More self-correction cycles may improve results or trigger degradation
  - Drawing abstraction: Cleaner inputs reduce ambiguity but may omit necessary detail

- Failure signatures:
  - Proportion drift: Individual parts generated with systematically incorrect scale ratios
  - Spatial relationship collapse: Parts positioned correctly in isolation but misaligned when assembled
  - Iteration degradation: Self-correction cycles produce increasingly distorted outputs rather than refinement

- First 3 experiments:
  1. Replicate the single-part generation test (Steps 1-3) with a simpler geometric primitive (e.g., a rectangular mass) to establish baseline part-generation accuracy before attempting complex architectural forms.
  2. Test coordinate system grounding explicitly: provide a drawing with marked origin point and query the model for the coordinates of labeled features—verify alignment before any 3D generation.
  3. Isolate the spatial relationship failure mode: generate two simple parts (e.g., two cubes) with a specified spatial relationship (e.g., "cube B is centered 10 units above cube A") and assess whether assembly accuracy degrades with increasing relationship complexity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would further decomposition of complex architectural parts into simpler geometric primitives improve spatial assembly accuracy in multimodal AI systems?
- Basis in paper: [explicit] Authors state: "one may further decompose complex architectural parts into simpler describable geometric shapes... or employ few-shot fine-tuning" as potential improvements not yet tested.
- Why unresolved: The study tested only a three-part decomposition per building; finer-grained decomposition strategies were not evaluated.
- What evidence would resolve it: A controlled experiment comparing current decomposition levels against finer-grained geometric decomposition using the same evaluation framework.

### Open Question 2
- Question: Does GPT-4o genuinely perform better on linear spatial arrangements while Claude 3.5 excels at symmetrical/repetitive features?
- Basis in paper: [explicit] Authors hypothesize: "this may be due to GPT-4o's ability to better process linear spatial arrangements" and that "Claude 3.5 may have a stronger capability in handling repetitive and symmetrical architectural features."
- Why unresolved: Only two buildings were tested, with one symmetrical (Villa Rotonda) and one linear (Palazzo Porto); insufficient data to confirm the pattern.
- What evidence would resolve it: Systematic testing across multiple buildings with controlled symmetrical vs. linear spatial organizations.

### Open Question 3
- Question: To what extent do findings generalize beyond Palladian architecture to other architectural styles and spatial logics?
- Basis in paper: [inferred] The study deliberately selected Palladio for "comprehensive documentation of design principles," but this controlled context may not represent broader architectural diversity in complexity, style, and spatial reasoning demands.
- Why unresolved: Only Palladian buildings were tested; the authors acknowledge the "spatial complexity characteristic of a mature architectural design language" may differ elsewhere.
- What evidence would resolve it: Replication of the methodology across diverse architectural styles (e.g., Gothic, Modernist, parametric) and building typologies.

### Open Question 4
- Question: What modifications to training data or model architecture would specifically enhance spatial reasoning capabilities for assembly tasks?
- Basis in paper: [explicit] Authors conclude: "current off-the-shelf Generative AI technologies lack advanced spatial reasoning and perception" impacting "ability to fully integrate geometric parts into a cohesive whole."
- Why unresolved: The study evaluated off-the-shelf systems without investigating causal factors or potential architectural improvements.
- What evidence would resolve it: Ablation studies or fine-tuning experiments targeting spatial reasoning components, measured against the same evaluation matrix.

## Limitations
- Binary scoring matrix introduces subjective thresholds for "correct" spatial relationships
- Small sample size (two buildings, three parts each) limits generalizability across architectural typologies
- Study focuses exclusively on OpenSCAD format, not reflecting capabilities with other CAD systems

## Confidence
- **High Confidence**: Individual part generation accuracy (correct positions and orientations) - consistently achieved by both systems across both buildings
- **Medium Confidence**: Self-correction capabilities - demonstrated but with system-dependent effectiveness and degradation patterns
- **Medium-Low Confidence**: Spatial relationship assembly - identified as the primary failure mode, with inconsistent results suggesting fundamental limitations in current multimodal models

## Next Checks
1. **Cross-architectural-type validation**: Test the same methodology on non-Palladian buildings with different geometric characteristics (e.g., Gothic or Baroque) to assess whether the observed performance patterns generalize beyond the studied case studies.

2. **Output format comparison**: Replicate the experiment using an alternative CAD format (such as parametric mesh-based modeling) to determine if the limitations are specific to CSG-based OpenSCAD or represent broader spatial reasoning challenges.

3. **Human expert baseline**: Conduct the same evaluation with trained architects to establish human performance benchmarks for the same tasks, providing context for the AI systems' capabilities and identifying whether certain spatial reasoning tasks remain uniquely human.