---
ver: rpa2
title: 'Enhancing LLM-Based Code Generation with Complexity Metrics: A Feedback-Driven
  Approach'
arxiv_id: '2505.23953'
source_url: https://arxiv.org/abs/2505.23953
tags:
- code
- complexity
- metrics
- pass
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates the correlation between code complexity
  metrics and the effectiveness of LLM-generated code. It finds that certain complexity
  metrics, such as Halstead Length and Lines of Code, are predictive of code correctness
  (Pass@1).
---

# Enhancing LLM-Based Code Generation with Complexity Metrics: A Feedback-Driven Approach

## Quick Facts
- arXiv ID: 2505.23953
- Source URL: https://arxiv.org/abs/2505.23953
- Reference count: 36
- Primary result: Complexity-aware feedback improves Pass@1 by 35.71% vs 12.5% baseline on HumanEval for GPT-3.5 Turbo

## Executive Summary
This paper investigates how code complexity metrics can predict and improve the success rate of LLM-generated code. Using logistic regression and Shapley values, the authors identify which complexity metrics (like Halstead Length and Lines of Code) are most predictive of code correctness. They then implement an iterative feedback method where LLMs regenerate code by adjusting these metrics when initial outputs fail. Experiments across multiple datasets and models show consistent improvements, particularly for smaller models. The approach also enhances agent-based frameworks like Reflexion.

## Method Summary
The method trains logistic regression models on 53 complexity metrics to predict code correctness (Pass@1), using Shapley values to identify the top-5 most predictive metrics per dataset. When code fails, the system prompts the LLM to adjust these specific metrics and regenerates code (up to 5 iterations). Internal test cases are generated by GPT-4o to avoid data leakage. The approach is evaluated on HumanEval, MBPP-sanitized, LeetCode, and BigCodeBench using multiple LLMs including GPT-4o, GPT-3.5 Turbo, Llama 3.1-8B, and GPT-o3 mini.

## Key Results
- Pass@1 improved by 35.71% vs 12.5% baseline on HumanEval for GPT-3.5 Turbo
- GPT-4o + Reflexion + complexity feedback achieved 0.36 Pass@1 vs 0.33 baseline
- Complexity metrics like Halstead Length and LOC were consistently predictive across datasets
- Effectiveness varies by dataset correlation strength (0.60-0.74 for MBPP, 0.76-0.92 for HumanEval)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Complexity metrics of generated code are predictive of Pass@1 success rates
- Mechanism: Logistic regression models trained on 53 complexity metrics can predict whether generated code will pass test cases. Shapley values identify the most influential metrics per dataset.
- Core assumption: The relationship between complexity metrics and correctness is learnable and consistent within a dataset
- Evidence anchors:
  - [abstract] "Using logistic regression models, we identify which complexity metrics are most predictive of code correctness"
  - [section IV-A, Table II] GPT-4o on HumanEval achieved 0.921 accuracy using Shapley values for feature selection
  - [corpus] Weak external validation—neighbor papers focus on feedback-driven approaches but not complexity metrics specifically
- Break condition: On datasets with weaker correlation (e.g., MBPP with 0.60-0.74 accuracy), complexity-based feedback shows minimal improvement over naive regeneration

### Mechanism 2
- Claim: Feedback-driven regeneration using complexity metrics improves code correctness more than naive iteration
- Mechanism: When code fails test cases, the system extracts values of the top-5 most influential complexity metrics (via Shapley) and prompts the LLM to "ensure that your generated code has different values for the following complexity metrics"
- Core assumption: LLMs can meaningfully adjust specific complexity characteristics when explicitly prompted
- Evidence anchors:
  - [abstract] "Pass@1 increased by 35.71% compared to the baseline's improvement of 12.5% on the HumanEval dataset" (GPT-3.5 Turbo)
  - [section IV-C, Table III] Consistent improvements across models; GPT-4o: 6.74% vs 2.24% baseline; Llama 3.1: 10.29% vs 4.41%
  - [corpus] Aligned with TestForge, ReFuzzer, CodeContests-O—all show feedback-driven iteration improves generation quality
- Break condition: Models near performance ceiling (e.g., GPT-4o on LeetCode at 91% initial Pass@1) show minimal improvement

### Mechanism 3
- Claim: Complexity-aware feedback compounds with agent-based frameworks (e.g., Reflexion)
- Mechanism: Apply Reflexion's verbal reinforcement learning first, then layer complexity-based feedback on intermediate outputs to further refine code
- Core assumption: Complexity-based feedback addresses error modes orthogonal to execution-based reflection
- Evidence anchors:
  - [abstract] "Integrating this method into Reflexion also boosts performance"
  - [section IV-D, Table IV] GPT-4o + Reflexion + complexity: 0.36 Pass@1 vs 0.33 baseline; GPT-o3 mini + Reflexion + complexity: 0.48 vs 0.45
  - [corpus] RECODE-H confirms iterative feedback is realistic for code workflows; no direct corpus validation of complexity+agent combination
- Break condition: On datasets with low complexity-Pass@1 correlation (BigCodeBench: 0.59-0.72 accuracy), improvements over baseline are marginal

## Foundational Learning

- Concept: **Pass@1**
  - Why needed here: The primary evaluation metric; measures probability that first-generated solution passes all test cases
  - Quick check question: If a model generates 3 solutions and at least 1 passes, does Pass@1 capture this? (No—use Pass@k for that scenario)

- Concept: **Shapley Values**
  - Why needed here: Identifies which complexity metrics contribute most to predicting code correctness; enables targeted feedback
  - Quick check question: If Halstead Length has a Shapley value of 7.5 and loops has 2.5, which metric should feedback prioritize? (Halstead Length)

- Concept: **Halstead Complexity Metrics**
  - Why needed here: Among the most predictive metrics identified; includes Length (total operators+operands), Volume, Effort, Difficulty
  - Quick check question: If code has many unique operators but few total operations, which Halstead metric increases? (Vocabulary, not Length)

## Architecture Onboarding

- Component map:
  1. Code Generator (LLM) -> Complexity Extractor (Radon + custom) -> Test Case Generator (GPT-4o) -> Importance Detector (Logistic Regression + Shapley) -> Feedback Composer -> Evaluator

- Critical path:
  1. Training phase → Generate code → Extract metrics → Train logistic regression → Compute Shapley values → Store top-5 metrics
  2. Evaluation phase → Generate code → Test with LLM-generated cases → If fail: extract metrics → Prompt with feedback → Regenerate (max 5 iterations) → Final evaluation with ground-truth tests

- Design tradeoffs:
  - 5 metrics vs more: Using only top-5 prevents overfitting but may miss secondary signals
  - LLM-generated test cases vs ground truth: Avoids data leakage but risks misaligned evaluation if tests hallucinate
  - Max 5 iterations: Balances improvement potential vs API costs; diminishing returns observed after iteration 3-4
  - Dataset-specific Shapley values: More accurate than global feature selection but requires re-training per dataset

- Failure signatures:
  - Minimal improvement over baseline: Indicates weak complexity-Pass@1 correlation for that dataset (observed in MBPP, BigCodeBench)
  - Metric adjustments ignored: LLM produces similar code despite feedback (more likely in simpler models with weaker instruction-following)
  - Over-correction: Code becomes more complex when simpler solution needed (observed in GPT-3.5 failures where correct code was more complex than initial attempt)

- First 3 experiments:
  1. Replicate HumanEval baseline: Run zero-shot generation with GPT-4o, compute Pass@1, verify complexity extraction pipeline produces 53 metrics per sample
  2. Validate metric predictiveness: Train logistic regression on 4 folds, test on held-out fold; confirm Shapley values identify Halstead Length and LOC as top predictors (per paper)
  3. A/B test feedback loop: Compare (a) naive regeneration with "this code is incorrect, try again" vs (b) complexity-aware feedback with top-5 metrics; target 3x improvement ratio (6.74% vs 2.24% per paper)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can developing more sophisticated complexity metrics, specifically tailored to machine learning code characteristics, provide more precise feedback and higher Pass@1 improvements than the standard metrics used in this study?
- Basis in paper: [explicit] The conclusion proposes "developing more sophisticated metrics tailored to machine learning code characteristics could provide more precise feedback."
- Why unresolved: The current study relied solely on standard complexity metrics like Halstead and Cyclomatic complexity.
- What evidence would resolve it: A comparative experiment where ML-specific metrics are substituted for standard metrics in the feedback loop.

### Open Question 2
- Question: How does the complexity-aware feedback approach perform on diverse datasets, newer LLMs, and non-Python programming languages?
- Basis in paper: [explicit] The Threats to Validity section states "Future work with diverse datasets and newer LLMs could improve generalizability."
- Why unresolved: The study focused only on Python-based datasets (HumanEval, MBPP, LeetCode, BigCodeBench) and specific models.
- What evidence would resolve it: Replicating the experiment across multi-language benchmarks (e.g., MultiPL-E) or with state-of-the-art models released after the study.

### Open Question 3
- Question: To what degree do hallucinations or misalignments in the LLM-generated internal test cases impact the reliability of the complexity-based feedback loop?
- Basis in paper: [inferred] The authors list reliance on LLM-generated test cases as a threat to validity, noting the need to reduce hallucinations, yet the method depends on these tests for iterative feedback.
- Why unresolved: The study assumed the generated test cases were sufficient for guiding feedback but did not quantify the error introduced by potential test case failures.
- What evidence would resolve it: An ablation study measuring the correlation between the accuracy of generated tests and the final Pass@1 score after feedback.

### Open Question 4
- Question: Does guiding the LLM to specifically increase or decrease complexity (directional feedback) result in better code correction rates than simply asking for "different values"?
- Basis in paper: [inferred] The authors use a prompt asking for "different values" but observe that different models (GPT-3.5 vs GPT-4) adjust complexity in opposite directions when failing.
- Why unresolved: The prompt design did not explicitly control for the direction of complexity change, potentially missing optimization opportunities.
- What evidence would resolve it: An experiment comparing "increase complexity," "decrease complexity," and "change complexity" prompts for failing solutions.

## Limitations

- Dataset dependency: Effectiveness varies significantly across datasets, with weaker correlation on MBPP and BigCodeBench
- Metric selection sensitivity: Using only top-5 metrics may miss important secondary signals
- Test case quality: Reliance on LLM-generated internal test cases introduces potential evaluation misalignment
- Diminishing returns: Effectiveness drops significantly after 2-3 iterations of feedback

## Confidence

- Pass@1 improvements: High confidence in measured improvements on HumanEval (35.71% vs 12.5% baseline), Medium-High on other datasets
- Shapley value selection: High confidence that the methodology correctly identifies predictive metrics per dataset
- Integration with Reflexion: Medium confidence—the absolute improvements are modest (0.33→0.36) and may not justify added complexity
- Generalizability: Low-Medium confidence—effectiveness varies significantly by dataset and model size

## Next Checks

1. Dataset transferability test: Apply the trained HumanEval logistic regression model directly to MBPP and BigCodeBench without retraining to assess whether complexity-metric relationships transfer across datasets

2. Baseline refinement: Compare against a "random metric feedback" baseline where 5 random metrics are selected instead of top-5 Shapley metrics to quantify the value of informed selection

3. Metric importance stability: Track how Shapley values shift across training folds and datasets to assess whether the same metrics consistently emerge as important, or if the approach requires dataset-specific recalibration for each new domain