---
ver: rpa2
title: 'Scaling Patterns in Adversarial Alignment: Evidence from Multi-LLM Jailbreak
  Experiments'
arxiv_id: '2511.13788'
source_url: https://arxiv.org/abs/2511.13788
tags:
- harm
- adversarial
- attacker
- alignment
- refusal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study systematically explores how adversarial jailbreaks\
  \ scale across large language models (LLMs) by simulating 6000 multi-turn attacker-target\
  \ exchanges across a range of model sizes (0.6B\u2013120B parameters). Using standardized\
  \ adversarial prompts from JailbreakBench, the researchers measured harm scores\
  \ and refusal behavior to assess the impact of relative model size on adversarial\
  \ potency."
---

# Scaling Patterns in Adversarial Alignment: Evidence from Multi-LLM Jailbreak Experiments

## Quick Facts
- arXiv ID: 2511.13788
- Source URL: https://arxiv.org/abs/2511.13788
- Reference count: 40
- Larger attacker models more effectively elicit harmful responses from smaller targets in proportion to their size ratio

## Executive Summary
This study systematically explores how adversarial jailbreaks scale across large language models (LLMs) by simulating 6000 multi-turn attacker-target exchanges across a range of model sizes (0.6B–120B parameters). Using standardized adversarial prompts from JailbreakBench, the researchers measured harm scores and refusal behavior to assess the impact of relative model size on adversarial potency. Results show a strong positive correlation between mean harm and the logarithm of the attacker-to-target size ratio (Pearson r = 0.51, p < 0.001), indicating that larger attacker models more effectively elicit harmful responses from smaller targets. Harm variance is higher across attackers than targets, and attacker refusal frequency is strongly and negatively correlated with harm (ρ = -0.93, p < 0.001), suggesting that attacker-side alignment and refusal behavior are key protective mechanisms.

## Method Summary
The researchers conducted a controlled adversarial alignment study using 6000 multi-turn dialogues between 20 distinct LLMs (sizes 0.6B–120B parameters) paired in all possible attacker-target combinations. Each exchange used standardized adversarial prompts from JailbreakBench across three domains, with up to 5 turns and early stopping on attacker refusal. Three independent LLM judges evaluated harm scores (1-5) and refusal flags for each response. The study measured mean harm and variance components across attacker and target models, examining correlations with relative size ratios and refusal behaviors.

## Key Results
- Strong positive correlation between mean harm and log attacker-to-target size ratio (Pearson r = 0.51, p < 0.001)
- Harm variance is higher across attackers (0.180) than across targets (0.097)
- Attacker refusal frequency is strongly and negatively correlated with harm (ρ = -0.93, p < 0.001)

## Why This Works (Mechanism)

### Mechanism 1: Relative Scale Asymmetry
- **Claim:** Larger attacker models elicit higher harm from smaller targets in proportion to their size ratio.
- **Mechanism:** Attacker models with greater parameter counts appear to possess higher persuasive/expressive capacity, enabling more effective framing strategies (roleplay, hypotheticals, educational framing) that bypass target alignment safeguards. The logarithmic relationship suggests diminishing returns at extreme ratios.
- **Core assumption:** Parameter count serves as a proxy for persuasive capability; the effect holds across different training regimes and architectures.
- **Evidence anchors:**
  - [abstract] "strong and statistically significant correlation between mean harm and the logarithm of the attacker-to-target size ratio (Pearson r = 0.510, p < 0.001)"
  - [Section 4] Correlations remain stable across leave-one-family-out robustness tests (r = 0.486-0.562)
  - [corpus] Neighbor papers (GSAE, AdvChain) confirm latent-space and CoT-based adversarial vectors exploit capability asymmetries, but do not directly test size ratios
- **Break condition:** If training data quality or alignment procedures (not size) are the true drivers, the correlation would disappear in controlled ablations holding tuning constant.

### Mechanism 2: Attacker-Side Behavioral Dominance
- **Claim:** Harm variance is driven more by attacker characteristics than target susceptibility.
- **Mechanism:** Attacker family and prompt strategy account for larger variance shares (0.180 vs. 0.097 target-side), suggesting that attacker behavioral diversity—particularly refusal tendencies and persuasive strategy selection—determines outcomes more than target defensiveness.
- **Core assumption:** Variance decomposition reflects causal contribution rather than measurement artifact; attacker behavioral diversity is not itself a function of unmeasured target properties.
- **Evidence anchors:**
  - [abstract] "Variance in harm is higher across attackers (0.180) than across targets (0.097)"
  - [Section 5] Mixed-effects analysis shows attacker family (R² = 0.086) and size ratio (R² = 0.055) explain more variance than target family (R² = 0.030)
  - [corpus] CARES and Security Assessment papers emphasize target-side robustness evaluation; limited corpus evidence on attacker-side variance decomposition
- **Break condition:** If target refusal behavior is systematically under-measured (early stopping hides target variability), attacker dominance could be an artifact of experimental design.

### Mechanism 3: Refusal as Primary Safety Gate
- **Claim:** Attacker-side refusal frequency is strongly negatively correlated with harm outcomes.
- **Mechanism:** When attacker models' own alignment triggers cause them to refuse generating adversarial content, the attack terminates early regardless of target vulnerability. This acts as a protective bottleneck in the adversarial pipeline.
- **Core assumption:** Refusal detection is accurate; refusal represents genuine alignment behavior rather than prompt formatting artifacts.
- **Evidence anchors:**
  - [abstract] "Attacker refusal frequency is strongly and negatively correlated with harm (rho = -0.927, p < 0.001)"
  - [Section 5] "Refusal frequency was strongly and negatively correlated with mean harm (ρ = −0.927, p < 0.001), confirming that alignment-driven refusal acts as a key protective mechanism"
  - [corpus] AdvChain and GSAE focus on target-side refusal mechanisms; corpus does not provide direct evidence on attacker-side refusal as primary gate
- **Break condition:** If refusal is prompt-dependent (some attack formulations trigger refusal, others don't, regardless of model capability), the correlation may not generalize across attack strategies.

## Foundational Learning

- **Concept: Scaling Laws in Language Models**
  - **Why needed here:** The paper extends scaling-law reasoning from capability to adversarial vulnerability; understanding baseline scaling helps contextualize why relative size ratios matter.
  - **Quick check question:** If you double parameter count, what happens to loss/capability under classic scaling laws? (Answer: predictable power-law improvement)

- **Concept: Correlation vs. Causation in Observational Studies**
  - **Why needed here:** The paper explicitly acknowledges its correlational nature; parameter count confounds with training data, alignment procedures, and architecture.
  - **Quick check question:** The paper finds r = 0.510 between size ratio and harm. Name two confounders that could explain this without direct causation. (Answer: alignment tuning intensity, training data quality)

- **Concept: Multi-Turn Jailbreak Dynamics**
  - **Why needed here:** Experiments use up to 5-turn adversarial exchanges with adaptive retry; understanding multi-turn attack patterns is essential for interpreting the results.
  - **Quick check question:** Why might a 5-turn adversarial exchange produce different results than single-turn jailbreak attempts? (Answer: iterative persuasion, rapport-building, reframing after refusal)

## Architecture Onboarding

- **Component map:**
  - Attacker model -> Target model -> Judge models (3 independent evaluators)
  - Up to 5 turns with early stop on attacker refusal

- **Critical path:**
  1. Sample adversarial prompt from JailbreakBench (30 prompts across 3 domains)
  2. Initialize attacker with system prompt (directed or undirected strategy)
  3. Execute turn: attacker generates → target responds → judge evaluates
  4. Check for attacker refusal (early stop) or max turns (5)
  5. Aggregate harm scores across three judges via mean

- **Design tradeoffs:**
  - **Model-based vs. human evaluation:** Scalable and consistent, but risks calibration bias and circularity
  - **Bounded turns (5) vs. unlimited:** Enables systematic comparison but may underestimate persistent adversarial dynamics
  - **Cross-family vs. single-family testing:** Broader generalization but confounds architecture with alignment regime

- **Failure signatures:**
  - Attacker refusal terminates exchange before target vulnerability is tested (4684/6000 = 78% valid runs)
  - High refusal rates in smaller models (e.g., Llama-3.1-8B-Instruct at 59%) may mask potential adversarial success
  - Judge calibration drift across model families or harm domains

- **First 3 experiments:**
  1. **Reproduce correlation with single family:** Test Llama-only attacker-target pairs to isolate size ratio from cross-family alignment differences; expect correlation to persist if size is genuine driver.
  2. **Controlled ablation on refusal:** Force attacker models to continue despite refusal triggers (override system prompt) to measure target-side vulnerability when attacker gate is removed.
  3. **Extend turn limit:** Run selected high-variance pairs with 15+ turns to test whether longer interactions amplify or attenuate the size-ratio effect.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does relative model size causally determine adversarial vulnerability, or is the observed correlation driven by confounding differences in training data quantity and alignment procedures?
- Basis in paper: [explicit] The authors explicitly state in the Limitations that "model scale is confounded with multiple other factors" and call for "controlled ablation experiments or counterfactual fine-tuning studies."
- Why unresolved: Current models vary simultaneously in size, data, and safety tuning, making it impossible to isolate parameter count as the sole driver of jailbreak success.
- What evidence would resolve it: Controlled experiments where models are trained identically except for parameter count, or ablation studies that vary alignment strength while holding scale constant.

### Open Question 2
- Question: Do adversarial scaling patterns persist, amplify, or diminish in longer-horizon interactions with persistent memory and recursive feedback loops?
- Basis in paper: [explicit] The Conclusion notes the study used a "bounded, few-turn dialogue structure" and explicitly calls to "extend this framework to longer-horizon, multi-agent... settings."
- Why unresolved: The study limited interactions to a maximum of five turns, potentially underestimating the complexity of real-world multi-agent escalation.
- What evidence would resolve it: Experiments involving extended multi-turn dialogues (e.g., 20+ turns) with persistent context and agent memory.

### Open Question 3
- Question: How does the homogeneity or heterogeneity of alignment training across model families influence the transferability of adversarial attacks?
- Basis in paper: [explicit] The authors list "Cross-family alignment tuning differences" as a central limitation, noting that "attacker and target systems differ... in the degree and nature of alignment reinforcement."
- Why unresolved: Public models use opaque, heterogeneous safety objectives (e.g., RLHF vs. constitutional tuning), making it unclear if "size ratio" is a proxy for alignment incompatibility.
- What evidence would resolve it: Testing attacks between models specifically fine-tuned with standardized alignment benchmarks versus diverse alignment methods.

### Open Question 4
- Question: To what extent does model-based evaluation introduce circularity or calibration bias when judging adversarial success?
- Basis in paper: [inferred] The Limitations section warns that using LLM judges risks "circularity: the same architectures used to generate outputs are assessing their own alignment behavior."
- Why unresolved: While multiple judges were used to mitigate variance, the absence of human evaluation limits interpretability for nuanced ethical judgments.
- What evidence would resolve it: Comparative studies measuring the correlation between automated harm scores and human expert assessments on identical adversarial outputs.

## Limitations

- Scale confounds: Parameter count correlates with training data quality, alignment procedures, and architecture
- Measurement constraints: Model-based judges may introduce circularity and calibration bias
- Generalizability boundaries: Results based on fixed prompt set and bounded 5-turn interactions

## Confidence

**High Confidence:** The empirical finding that attacker-side variance (0.180) exceeds target-side variance (0.097) in harm outcomes.

**Medium Confidence:** The correlation between size ratio and harm (r = 0.510), subject to potential confounders not controlled in experimental design.

**Medium Confidence:** Attacker refusal as a protective mechanism (ρ = -0.927), assuming refusal represents genuine alignment behavior rather than prompt-dependent artifacts.

## Next Checks

1. **Controlled Parameter Ablation:** Test models with identical training data and alignment procedures but systematically varied parameter counts to isolate size effects from training-related confounds.

2. **Refusal Override Experiment:** Force attacker models to continue exchanges despite refusal triggers to measure the magnitude of target-side vulnerability when attacker-side gating is removed.

3. **Extended Interaction Horizon:** Run selected high-variance pairs for 15+ turns to test whether the size-ratio effect amplifies, attenuates, or remains stable under prolonged adversarial engagement.