---
ver: rpa2
title: 'From What Ifs to Insights: Counterfactuals in Causal Inference vs. Explainable
  AI'
arxiv_id: '2505.13324'
source_url: https://arxiv.org/abs/2505.13324
tags:
- counterfactual
- causal
- treatment
- outcome
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper explores how counterfactuals are used in causal inference
  (CI) and explainable AI (XAI), two distinct data science fields. While both leverage
  "what if" scenarios, they differ in purpose: CI aims to estimate causal effects,
  while XAI seeks to explain model predictions.'
---

# From What Ifs to Insights: Counterfactuals in Causal Inference vs. Explainable AI

## Quick Facts
- arXiv ID: 2505.13324
- Source URL: https://arxiv.org/abs/2505.13324
- Reference count: 19
- Primary result: Proposes unified definition of counterfactuals for CI and XAI, highlighting key differences in purpose, causality vs. correlation, aggregation levels, and evaluation methods

## Executive Summary
This paper explores how counterfactuals are used in causal inference (CI) and explainable AI (XAI), two distinct data science fields. While both leverage "what if" scenarios, they differ in purpose: CI aims to estimate causal effects, while XAI seeks to explain model predictions. The paper introduces a comprehensive definition of counterfactuals that applies to both fields, highlighting differences in causality vs. correlation, aggregation levels (sample vs. individual), and what is modified (model vs. data). It also contrasts their performance evaluation methods, with CI focusing on standard errors and robustness checks, while XAI emphasizes feasibility, proximity, and sparsity. The paper identifies opportunities for cross-fertilization, such as using CI to ensure actionable XAI recommendations and leveraging counterfactual fairness in machine learning. By clarifying these distinctions, the authors aim to foster synergy between CI and XAI, enabling deeper insights and more informed decision-making.

## Method Summary
The paper employs a conceptual framework development approach through literature synthesis. No empirical experiments or code implementations are provided. The authors systematically compare CI and XAI counterfactuals across six dimensions: purpose, causality vs. correlation, aggregation level, modified object, assumptions, and evaluation. They propose a unified definition of counterfactuals that encompasses both fields and identify opportunities for cross-fertilization between the domains.

## Key Results
- Introduces a unified definition of counterfactuals applicable to both CI and XAI
- Highlights fundamental differences in purpose: CI estimates causal effects while XAI explains model predictions
- Identifies key distinctions in aggregation level (population vs. individual) and what is modified (model structure vs. input data)
- Proposes opportunities for cross-fertilization including actionable XAI recommendations and counterfactual fairness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Counterfactuals in CI estimate population-level causal effects by comparing potential outcomes, while XAI counterfactuals identify feature modifications for individual predictions.
- Mechanism: CI uses the counterfactual to estimate the THEN portion (outcome difference) through aggregations like ATE, ATT, and CATE that average across units. XAI inverts this focus to identify the IF portion—the specific input changes (x̃ᵢ) that would alter a predicted outcome for a single unit.
- Core assumption: CI assumes the identification strategy (unconfoundedness, positivity, consistency) validly approximates counterfactual outcomes from observed data.
- Evidence anchors:
  - [abstract] "CI aims to estimate causal effects, while XAI seeks to explain model predictions"
  - [section 2.3.1] "In CI the main quantity of interest, or estimand, is the difference between two outcomes... in XAI, the quantity of interest is the 'best' combination of feature values"
  - [corpus] Corpus confirms active research in counterfactual explanation generation methods but limited direct comparison with CI approaches
- Break condition: If identification assumptions in CI are violated (e.g., unmeasured confounding), or if XAI's predictive model captures spurious correlations, counterfactuals become misleading in both domains.

### Mechanism 2
- Claim: The validity of counterfactuals depends critically on whether they modify the model structure (CI) or input data (XAI).
- Mechanism: CI counterfactuals require modifying the treatment variable or structural model to simulate hypothetical interventions—demanding domain knowledge of the data-generating process. XAI counterfactuals hold the model fixed and perturb feature values, relying on the model's learned correlation structure without requiring causal knowledge.
- Core assumption: Assumption: XAI assumes the predictive model's correlation structure meaningfully reflects decision-relevant relationships, even if not causal.
- Evidence anchors:
  - [section 2.4] "In CI, the target of modification often involves changing the structural form of the model... in XAI, the target of modification is the input data to a given, fixed predictive model"
  - [section 2.2] "There is no guarantee, however, that observed correlations in the training data accurately reflect causal structures"
  - [corpus] "DiCoFlex" and related papers emphasize model-agnostic counterfactual generation, reinforcing the fixed-model assumption in XAI
- Break condition: When XAI counterfactuals suggest causally ineffective feature changes (e.g., "change your zip code to get a loan"), the explanation may be actionable only in manipulating the model, not achieving real-world outcomes.

### Mechanism 3
- Claim: Performance evaluation diverges fundamentally: CI emphasizes statistical uncertainty and robustness; XAI emphasizes feasibility, proximity, and sparsity of explanations.
- Mechanism: CI validates counterfactual estimates through standard errors, confidence intervals, and design-specific robustness checks (e.g., McCrary test for RDD, parallel trends for DiD). XAI evaluates counterfactual quality through metrics assessing whether explanations are realistic (feasibility), close to original inputs (proximity), parsimonious (sparsity), and stable across model perturbations.
- Core assumption: Assumption: XAI metrics like feasibility and sparsity proxy for human interpretability and actionability, though this connection requires empirical validation.
- Evidence anchors:
  - [section 2.6] "In CI we are interested in evaluating the quality of the estimate... in XAI we are interested in evaluating the quality of the explanation"
  - [table 1] Explicitly contrasts evaluation approaches: "Standard error, conf. interval, significance, robustness tests" vs. "Feasibility, sparseness, etc."
  - [corpus] Corpus papers on counterfactual evaluation (e.g., "From Facts to Foils") similarly emphasize feasibility and actionability metrics
- Break condition: If CI robustness checks fail (e.g., manipulation at RDD cutoff), causal claims are undermined. If XAI counterfactuals lack feasibility, explanations may be technically correct but practically useless.

## Foundational Learning

- Concept: Potential Outcomes Framework (Rubin Causal Model)
  - Why needed here: This is the foundational formalism for CI counterfactuals—the paper explicitly frames its discussion around this approach. Understanding Yᵢ(1) and Yᵢ(0) as potential (counterfactual) outcomes is prerequisite to grasping why CI aggregates and XAI individualizes.
  - Quick check question: For a given unit i, can you observe both Yᵢ(1) and Yᵢ(0) simultaneously? Why does this create the "fundamental problem of causal inference"?

- Concept: Counterfactual Explanation (in XAI sense)
  - Why needed here: The paper positions this as the XAI counterpart to CI counterfactuals. Understanding that XAI counterfactuals explain model behavior (not real-world causation) is essential for avoiding category errors.
  - Quick check question: If a counterfactual explanation says "increase income by $10K to get loan approval," does this mean increasing income causes loan approval in reality, or that the model would predict approval?

- Concept: Identification Assumptions (SUTVA, Unconfoundedness, Positivity)
  - Why needed here: The paper details these as the scaffolding enabling CI counterfactual estimation. Without understanding what must be assumed vs. tested, practitioners cannot assess when CI claims are credible.
  - Quick check question: What empirical test can definitively verify the unconfoundedness assumption? (Answer: None—it's untestable from observed data alone.)

## Architecture Onboarding

- Component map:
  - **CI pipeline**: Treatment assignment → Potential outcomes (unobservable) → Identification strategy (RCT, IV, RDD, DiD, matching) → Aggregated estimand (ATE/ATT/CATE) → Uncertainty quantification + robustness checks
  - **XAI pipeline**: Trained model (fixed) + instance features → Counterfactual generator → Candidate x̃ᵢ values → Evaluation metrics (feasibility/proximity/sparsity) → Selected explanation
  - **Cross-fertilization layer**: Causal constraints on XAI counterfactuals (actionability); counterfactual fairness diagnostics; policy counterfactuals combining prediction + causal reasoning

- Critical path:
  1. Determine whether the goal is causal estimation (CI) or model explanation (XAI)
  2. If CI: Map identification assumptions to available data/design; if violated, estimand is not recoverable
  3. If XAI: Verify model is fixed and accessible; generate counterfactuals; validate feasibility constraints
  4. For hybrid applications: Use CI to ground XAI recommendations in causal structure (e.g., ensure suggested feature changes are actionable)

- Design tradeoffs:
  - **CI**: Stronger causal claims require stronger assumptions and often experimental/Quasi-experimental designs; observational designs trade internal validity for feasibility
  - **XAI**: Proximity vs. sparsity (closer counterfactuals may require more changes); feasibility vs. optimality (realistic changes may not achieve target outcome); individual vs. aggregated explanations
  - **Hybrid**: Incorporating causal constraints into XAI may reduce explanation space but increase actionability

- Failure signatures:
  - CI failures: Parallel trends violation in DiD (treated/control diverge pre-treatment); weak instruments (F-stat < 10); manipulation at RDD cutoff; confounding in observational studies
  - XAI failures: Counterfactuals suggesting impossible changes (negative age); spurious correlations driving explanations; instability across similar instances; explanations that manipulate protected attributes
  - Cross-domain failures: Treating XAI counterfactuals as causal prescriptions; applying CI aggregations to individual decision-making without heterogeneity analysis

- First 3 experiments:
  1. **Implement a basic XAI counterfactual explainer on a tabular classifier** (e.g., loan approval), then manually evaluate whether top counterfactuals are causally plausible. Document cases where model correlations diverge from plausible causal mechanisms.
  2. **Apply a CI method (e.g., matching or DiD) to a dataset with known treatment effects**, then generate counterfactual explanations for individual predictions from a model trained on the same data. Compare what CI reveals (average effects) vs. what XAI reveals (individual-level feature importance).
  3. **Test counterfactual fairness**: Train a model on a dataset with protected attributes, generate counterfactual explanations that flip the protected attribute, and assess whether predictions change. This bridges XAI explanation with CI-style fairness auditing as discussed in Section 3.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can causal models be integrated into Explainable AI (XAI) to ensure counterfactual explanations are actionable and causally valid rather than based on spurious correlations?
- Basis in paper: [explicit] The authors state that "causal models can help ensure that returned counterfactual explanations are truly actionable and therefore useful to end users."
- Why unresolved: Current XAI methods rely on the correlation structure of the training data, which may decouple from the real-world data generating process, leading to causally ineffective recommendations.
- What evidence would resolve it: The development and validation of XAI frameworks that explicitly incorporate causal constraints to generate recommendations that reliably change outcomes in the real world.

### Open Question 2
- Question: How can counterfactual explanations be utilized to enrich the theoretical and managerial implications of traditional causal inference studies?
- Basis in paper: [explicit] The authors note "significant potential in incorporating the use of counterfactual explanations to enrich the theoretical as well as managerial implications of causal empirical research."
- Why unresolved: Traditional causal inference focuses on aggregate estimands (like ATE), leaving questions about specific feature-level changes for individuals unanswered.
- What evidence would resolve it: Empirical studies or methodological frameworks that combine aggregate causal estimates with individual-level counterfactual explanations to provide granular policy insights.

### Open Question 3
- Question: In what ways can counterfactual fairness techniques be applied to reveal the social and institutional mechanisms generating bias in training data?
- Basis in paper: [explicit] The paper identifies "counterfactual fairness" as a salient example of cross-fertilization, suggesting explanations like "if you were not a woman, you would have received the loan" can stimulate research into data-generating mechanisms.
- Why unresolved: While the potential is identified, the specific methods for using these explanations to reverse-engineer or critique the institutional mechanisms behind the data are not fully developed.
- What evidence would resolve it: A systematic approach for auditing training data using counterfactual fairness metrics to identify and categorize systemic biases.

## Limitations
- The paper's claims about cross-fertilization between CI and XAI remain largely conceptual without empirical validation
- Lacks concrete examples demonstrating how CI methods could improve XAI counterfactual explanations or vice versa
- Discussion of counterfactual fairness is introduced without specifying implementation details or evaluation metrics
- Does not address potential conflicts between CI's emphasis on causal validity and XAI's focus on prediction accuracy

## Confidence
- High confidence: The fundamental distinction between CI (estimating population-level causal effects) and XAI (explaining individual model predictions) is well-established and clearly articulated
- Medium confidence: The proposed unified definition of counterfactuals is comprehensive but requires empirical testing across diverse domains to verify its practical utility
- Low confidence: Specific claims about cross-fertilization opportunities (e.g., using CI to ground XAI recommendations) lack concrete implementation strategies and validation

## Next Checks
1. Implement a case study comparing CI-based causal effects with XAI-generated counterfactual explanations on the same dataset, documenting where explanations align or conflict with causal knowledge
2. Develop and test evaluation metrics for "actionable" counterfactuals that combine XAI's proximity/sparsity measures with CI's causal plausibility constraints
3. Design an experiment testing whether incorporating causal structure into XAI counterfactual generation improves downstream decision-making compared to purely correlative approaches