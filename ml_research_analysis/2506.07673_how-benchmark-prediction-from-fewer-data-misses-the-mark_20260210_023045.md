---
ver: rpa2
title: How Benchmark Prediction from Fewer Data Misses the Mark
arxiv_id: '2506.07673'
source_url: https://arxiv.org/abs/2506.07673
tags:
- benchmark
- data
- estimation
- prediction
- random-sampling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the problem of efficient LLM evaluation
  through benchmark prediction, where the goal is to estimate model performance on
  a full benchmark using only a small subset of data points. The authors evaluate
  11 state-of-the-art benchmark prediction methods across 19 diverse benchmarks, comparing
  them against simple baselines.
---

# How Benchmark Prediction from Fewer Data Misses the Mark

## Quick Facts
- arXiv ID: 2506.07673
- Source URL: https://arxiv.org/abs/2506.07673
- Reference count: 40
- Primary result: Simple random sampling with regression learning outperforms most sophisticated benchmark prediction methods, while all methods fail when evaluating superior models

## Executive Summary
This paper investigates efficient LLM evaluation through benchmark prediction, where the goal is to estimate model performance on a full benchmark using only a small subset of data points. The authors evaluate 11 state-of-the-art benchmark prediction methods across 19 diverse benchmarks, comparing them against simple baselines. Their findings challenge the assumption that careful core-set selection is necessary for benchmark prediction. The primary contribution demonstrates that these methods heavily rely on model similarity and struggle when evaluating novel, potentially superior models - precisely when such methods would be most valuable. This highlights the need for caution when applying these techniques, particularly at the evaluation frontier where new models are being assessed.

## Method Summary
The study evaluates benchmark prediction methods that estimate full-benchmark performance for target models using only n=50 data points (core-set), given full performance data for source models. The methods include Random-Sampling (mean on subset), Random-Sampling-Learn (Ridge regression from subset to full mean), AIPW (Ridge per target + debiasing), Anchor-Points variants, P-IRT, GP-IRT, PCA imputation, and Lasso. The evaluation uses per-data-point performance matrices S(F,D) for 19 benchmarks with 83-448 models each, testing both interpolation (75%/25% source/target split) and extrapolation (lowest 50%/top 30% split) regimes. The primary metric is estimation gap = (1/|F(t)|) Σ|true_mean - estimated_mean| averaged over target models, reported as reduction over Random-Sampling baseline.

## Key Results
- Random sampling with regression learning (Random-Sampling-Learn) outperforms most existing methods in interpolation settings
- All benchmark prediction methods show significantly reduced effectiveness when evaluating models that perform better than source models (extrapolation regime)
- Augmented Inverse Propensity Weighting (AIPW) consistently outperforms random sampling in both interpolation and extrapolation regimes, though gains are modest

## Why This Works (Mechanism)

### Mechanism 1
Random sampling with regression learning outperforms sophisticated core-set selection methods in interpolation settings. A Ridge regression model learns to predict full-benchmark performance from subset performance by exploiting correlations between subset and full-benchmark scores across source models. The regression captures systematic biases in the subset mean (e.g., if a subset is systematically harder than average). This works when target models follow similar score distributions to source models on both subset and full benchmark. When target models have substantially different accuracy distributions than source models (extrapolation regime), regression coefficients learned on sources fail to generalize.

### Mechanism 2
All benchmark prediction methods exhibit strong dependence on model similarity between target and source models. Learning-based methods train predictors using source model behavior patterns; when target models exhibit different error patterns on data points, predictions become unreliable. Model similarity measures alignment of correct/incorrect predictions between target and source distributions. Methods work best when interpolating scores among similar models. Low model similarity (<0.5 in ImageNet experiments) correlates with normalized estimation gaps 2-3× higher than high-similarity targets.

### Mechanism 3
Augmented Inverse Propensity Weighting (AIPW) maintains consistent estimation across interpolation and extrapolation regimes, though gains are modest. AIPW combines the unbiased subset mean with a debiasing correction term. It trains a per-target Ridge regression predicting point-wise performance from source model scores, then corrects for sampling bias. As a consistent estimator, it converges to true performance regardless of distribution shift, with variance reduction proportional to prediction quality. When n/N ratio is high or prediction correlation ρ is low, variance reduction becomes negligible.

## Foundational Learning

- **Consistent vs. biased estimators**: Random-Sampling is consistent (unbiased, converges to true mean) but high variance. Random-Sampling-Learn reduces variance through regression but introduces bias. AIPW maintains consistency while reducing variance. Understanding this tradeoff is essential for interpreting why AIPW works in extrapolation while other methods fail. Quick check: If you double the sample size, which method's advantage over random sampling would decrease most: Random-Sampling-Learn or AIPW?

- **Interpolation vs. extrapolation regimes in evaluation**: The paper's central finding is that benchmark prediction "fails when most needed: at the evaluation frontier." Methods optimized for interpolation (source and target from same distribution) catastrophically fail when extrapolating to better-performing models. This mirrors train/test distribution shift but in a meta-evaluation context. Quick check: If source models have 60-80% accuracy and your target has 92% accuracy, which regime are you in? Which methods would you trust?

- **Core-set selection vs. post-hoc learning**: The paper challenges the assumption that "careful subset selection is necessary." Understanding why random subsets with learning outperform k-medoids clustering or gradient-based optimization informs practical implementation choices. Quick check: Given 50 evaluations on a random subset, would you (a) compute the mean, (b) fit a regression to historical model data, or (c) re-run with 100 evaluations? What information do you need to decide?

## Architecture Onboarding

- **Component map**:
```
Benchmark Prediction Pipeline
├── Core-set Selection (n=50-200 points)
│   ├── Random (Random-Sampling, Random-Sampling-Learn, AIPW)
│   ├── Clustering (Anchor-Points-Weighted, Anchor-Points-Predictor)
│   └── Optimization (Double-Optimize, Lasso)
├── Estimation Methods
│   ├── Direct mean (Random-Sampling)
│   ├── Regression-based (Random-Sampling-Learn, PCA, Anchor-Points-Predictor)
│   ├── Weighted (Anchor-Points-Weighted, GP-IRT)
│   └── Consistent estimators (AIPW)
└── Source Model Data
    └── Requires full performance matrix S(F(s), D) for all methods except Random-Sampling
```

- **Critical path**: For implementing AIPW (the recommended method):
  1. Collect full benchmark scores for source models (M≥50 recommended)
  2. Random sample n=50-100 data points as core-set
  3. For each target model: train Ridge regression predicting point-wise scores from source scores
  4. Compute: h_AIPW = subset_mean + (N-n)/(N+n) × (mean_prediction_full - mean_prediction_subset)

- **Design tradeoffs**:
  - Random-Sampling: Zero implementation cost, no source data needed, robust to distribution shift, but requires 2× data to match AIPW
  - Random-Sampling-Learn: Simple, best interpolation performance, but fails catastrophically in extrapolation (200-500% error increase)
  - AIPW: Consistent performance across regimes, modest gains (10-30%), requires source model data and per-target training

- **Failure signatures**:
  - Random-Sampling-Learn in extrapolation: Estimation gap increases from ~3% to 13-28% depending on benchmark; positive correlation with target model accuracy (worse for better models)
  - All methods except AIPW/Random-Sampling: Negative correlation with model similarity indicates failure on novel architectures
  - AIPW with low prediction quality: When ρ<0.3, variance reduction negligible; fallback to near-random performance

- **First 3 experiments**:
  1. **Establish baseline**: Run Random-Sampling and Random-Sampling-Learn on your benchmark with n=50, measure estimation gap across available models. If gap <4% with Random-Sampling-Learn, you're likely in interpolation regime.
  2. **Test extrapolation robustness**: Hold out top 30% of models as targets, train on bottom 50%. Compare Random-Sampling vs. AIPW. If AIPW doesn't improve on random sampling, source models lack predictive signal for targets.
  3. **Calibrate sample size**: Test n∈{50,100,200} for your benchmark size N. If AIPW advantage over Random-Sampling drops below 10% at n=100, just use Random-Sampling with larger n rather than implementing AIPW.

## Open Questions the Paper Calls Out
None

## Limitations
- The evaluation focuses primarily on regression-based methods and simple baselines, potentially overlooking other promising approaches like ensemble methods or neural predictors
- Source model requirements (M≥50 models with full performance data) may be prohibitive for emerging benchmarks or specialized domains
- While AIPW shows consistent performance across regimes, the modest gains (10-30% improvement over random sampling) may not justify the implementation complexity for many practical applications

## Confidence

**High confidence**: Random sampling with regression learning outperforming sophisticated core-set selection methods in interpolation settings; catastrophic failure of learning methods in extrapolation regime; AIPW maintaining consistent performance across both regimes.

**Medium confidence**: The quantitative performance differences between methods (specific percentage improvements) may vary with different benchmark characteristics, sample sizes, or model populations not tested in this study.

**Medium confidence**: The claim that careful subset selection is unnecessary may not generalize to extremely large benchmarks (N>100k) where subset diversity becomes more critical.

## Next Checks

1. **Test on frontier models**: Evaluate the same methods on recent frontier models (GPT-4, Claude-3, Gemini) compared to the source models used in this study to verify whether the extrapolation failure pattern persists with state-of-the-art architectures.

2. **Cross-domain generalization**: Apply the benchmark prediction methods to domains outside vision and language (e.g., protein folding, robotics control) to test whether the interpolation/extrapolation distinction holds across fundamentally different task types.

3. **Ensemble approach validation**: Implement an ensemble of Random-Sampling-Learn and AIPW to test whether combining interpolation-optimized and consistent methods can capture the benefits of both regimes while mitigating their respective weaknesses.