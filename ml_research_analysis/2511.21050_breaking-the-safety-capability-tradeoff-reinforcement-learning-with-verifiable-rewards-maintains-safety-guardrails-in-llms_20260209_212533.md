---
ver: rpa2
title: 'Breaking the Safety-Capability Tradeoff: Reinforcement Learning with Verifiable
  Rewards Maintains Safety Guardrails in LLMs'
arxiv_id: '2511.21050'
source_url: https://arxiv.org/abs/2511.21050
tags:
- safety
- rlvr
- arxiv
- reasoning
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the fundamental safety-capability tradeoff
  in fine-tuning large language models, where improving task performance often degrades
  safety alignment even on benign datasets. The authors present the first comprehensive
  theoretical and empirical analysis of safety properties in reinforcement learning
  with verifiable rewards (RLVR).
---

# Breaking the Safety-Capability Tradeoff: Reinforcement Learning with Verifiable Rewards Maintains Safety Guardrails in LLMs

## Quick Facts
- arXiv ID: 2511.21050
- Source URL: https://arxiv.org/abs/2511.21050
- Reference count: 11
- This paper demonstrates that RLVR can improve reasoning capabilities while maintaining or improving safety guardrails, challenging the assumed safety-capability tradeoff.

## Executive Summary
This paper addresses the fundamental safety-capability tradeoff in fine-tuning large language models, where improving task performance often degrades safety alignment even on benign datasets. The authors present the first comprehensive theoretical and empirical analysis of safety properties in reinforcement learning with verifiable rewards (RLVR). Theoretically, they derive upper bounds on safety drift under KL-constrained optimization and prove conditions under which safety degradation is eliminated when reward and safety objectives are statistically independent. Empirically, they conduct extensive experiments across five adversarial safety benchmarks, demonstrating that RLVR can simultaneously enhance reasoning capabilities while maintaining or improving safety guardrails.

## Method Summary
The study evaluates RLVR safety properties by training Qwen2.5 models (7B and 32B) on GSM8K and MATH datasets using GRPO and REINFORCE++ algorithms with binary verifiable rewards. The KL-constrained objective J_RLVR = E[f(x,Y)] - β·D_KL(π||π_ref) is used with β = 1e-4 (7B) and 1e-3 (32B). Safety evaluation uses DeBERTa harmfulness scorer (0-4 scale) and HarmBench binary classifier across five adversarial benchmarks. Paired difference analysis compares base and fine-tuned models, with reasoning performance measured on GSM8K, MATH500, and AIME24.

## Key Results
- RLVR maintains safety with negligible harmfulness score changes (~1.5-1.6) while SFT significantly increases scores (from ~1.7 to ~2.9 on 0-4 scale)
- RLVR improves reasoning performance on GSM8K and MATH500 while preserving safety guardrails
- Theoretical analysis shows safety drift bounded by χ² divergence when independence assumption fails
- Comprehensive ablation studies confirm safety preservation across optimization algorithms, model scales, and task domains

## Why This Works (Mechanism)

### Mechanism 1: Exponential-Tilt Policy Reweighting Preserves Safety-Space Geometry
RLVR reweights token path probabilities based on task success while maintaining safety properties when rewards and safety are orthogonal. The optimal RLVR policy takes the form p*(r|x) = exp(g_x(r)/β) · p_ref(r|x) / Z(x), where g_x(r) is the success probability for path r and β is the KL regularization coefficient. This exponential tilt amplifies high-success paths without fundamentally altering the relative ordering of paths within safety-irrelevant dimensions. Core assumption: Assumption 1 (Constant Success and Safety)—success and safety are conditionally deterministic given a token path.

### Mechanism 2: KL Regularization Provides Worst-Case Safety Drift Bound
Even when independence fails, KL-constrained optimization provides provable upper bounds on safety degradation. The safety drift |E_π*[s_x(R)] - E_πref[s_x(R)]| is bounded by √(χ²(π*||πref)). Since KL divergence is part of the objective, the optimization inherently limits how far the policy can deviate, creating a hard constraint on maximum safety erosion. Core assumption: The χ² divergence between learned and reference policies remains controlled by the β coefficient.

### Mechanism 3: Verifiable Reward Orthogonality to Safety Domains
Math/code correctness rewards are empirically uncorrelated with safety-relevant token trajectories, enabling simultaneous capability gains and safety preservation. Verifiable rewards (f(x,y) ∈ {0,1} for exact correctness) target domain-specific reasoning patterns (e.g., arithmetic steps, code syntax) that have no inherent overlap with refusal behaviors or harmful content generation. This domain separation satisfies the independence condition. Core assumption: Assumption 2 (Independence of Safety and Success)—g_x(r) and s_x(r) are statistically independent.

## Foundational Learning

- Concept: **KL Divergence Regularization in Policy Optimization**
  - Why needed here: The β·D_KL(π||πref) term is the mathematical mechanism that bounds safety drift; understanding this is essential for tuning the safety-capability tradeoff.
  - Quick check question: If you increase β from 1e-4 to 1e-2, would you expect more or less capability improvement? What happens to the safety guarantee?

- Concept: **Token Trajectory / Path Formulation**
  - Why needed here: The theoretical framework models LLM generation as sampling token paths R, then tokens conditioned on R. This abstraction enables the covariance-based safety analysis.
  - Quick check question: How does Assumption 1 simplify the relationship between token paths and safety outcomes?

- Concept: **Paired Difference Statistical Testing**
  - Why needed here: The paper uses paired evaluation (base vs. fine-tuned) rather than aggregate comparisons to isolate fine-tuning effects; this methodology is critical for valid safety conclusions.
  - Quick check question: Why is paired testing more reliable than comparing mean scores across model groups?

## Architecture Onboarding

- Component map: Base models (Qwen2.5-7B/32B) -> RL algorithms (GRPO/REINFORCE++) -> Verifiable reward (binary correctness) -> KL regularization (β coefficient) -> Safety evaluation (DeBERTa/HarmBench)

- Critical path:
  1. Start with base/instruction-tuned model as π_ref
  2. Define verifiable reward for target domain (math/code correctness)
  3. Configure KL coefficient β based on model scale
  4. Run GRPO/REINFORCE++ optimization on training dataset (e.g., GSM8K)
  5. Evaluate safety on adversarial benchmarks (I-BeaverTails, I-CoNa, etc.) using paired comparison

- Design tradeoffs:
  - Lower β → more capability gain, weaker safety guarantee (higher χ² bound)
  - Higher β → stronger safety guarantee, potentially capped capability improvement
  - GRPO vs. REINFORCE++: Figure 2 shows minimal safety difference; choice driven by computational efficiency
  - 7B vs. 32B: Larger models show slightly lower baseline harmfulness (Table 3), similar RLVR safety preservation

- Failure signatures:
  - Harmfulness score increase >0.5 points post-training suggests independence assumption violated or β too low
  - High variance in paired differences across datasets indicates domain-specific safety issues
  - If capability benchmarks (GSM8K, MATH500) don't improve, reward signal may be misconfigured

- First 3 experiments:
  1. Reproduce safety preservation on Qwen2.5-7B-Instruct: Train with GRPO on GSM8K (β=1e-4), evaluate harmfulness scores on I-BeaverTails subset (n=100-500). Expect paired difference <0.1.
  2. Stress-test β coefficient: Train identical models with β ∈ {1e-5, 1e-4, 1e-3, 1e-2}, plot safety drift vs. capability gain. Identify where χ² bound becomes tight.
  3. Validate independence assumption: Compute empirical covariance between success rates on math tasks and harmfulness scores. If covariance >0.1, investigate which prompt types violate independence.

## Open Questions the Paper Calls Out

### Open Question 1
How does the magnitude of safety drift change when the statistical independence between reward success and safety (Assumption 2) is violated in the training distribution? The paper evaluates on math/code datasets (likely high independence) but lacks experiments where unsafe outputs might inadvertently maximize the verifiable reward.

### Open Question 2
Do the safety-preserving properties of KL-constrained RLVR generalize to model architectures and families other than Qwen2.5? All empirical results are derived from Qwen2.5 models; the role of the base model's pre-training data on the observed orthogonality is unclear.

### Open Question 3
Can the theoretical safety guarantees and empirical robustness of RLVR be maintained when shifting from binary verifiable rewards to continuous or model-based reward functions? The authors define the reward function as binary f(x,y) ∈ {0,1} and explicitly list "stronger reward-verification strategies" as a necessary direction for future work.

## Limitations

- Domain Applicability: The theoretical safety guarantees rely critically on statistical independence between verifiable rewards and safety-relevant token trajectories, which may not hold for other task categories.
- Reward Function Dependence: Safety preservation is fundamentally contingent on the reward function being orthogonal to safety concerns, limiting generalization to domains with complex reward structures.
- Statistical Significance vs. Practical Impact: While paired difference tests show statistically significant safety preservation, the practical significance of minor harmfulness score fluctuations requires careful interpretation.

## Confidence

- **High Confidence**: The core theoretical framework linking KL regularization to safety bounds is mathematically sound. The empirical demonstration that RLVR maintains safety while SFT degrades it is robust across multiple benchmarks and model scales.
- **Medium Confidence**: The independence assumption's validity is empirically supported but theoretically unverified beyond the specific domains tested. The worst-case χ² bound provides theoretical protection, but practical tightness depends on implementation details.
- **Low Confidence**: Generalization to non-mathematical domains, complex reward structures, or frontier model architectures remains speculative. The sensitivity of safety preservation to hyperparameters like β across diverse applications is not fully characterized.

## Next Checks

1. **Independence Assumption Stress Test**: Compute empirical covariance between success rates and harmfulness scores across the five safety benchmarks for all fine-tuned models. Target correlation coefficients below 0.1 to validate Assumption 2.

2. **β Coefficient Sensitivity Analysis**: Systematically vary β from 1e-5 to 1e-2 across multiple training runs, measuring both safety drift (harmfulness score changes) and capability gains (reasoning benchmark improvements).

3. **Domain Transfer Validation**: Apply RLVR to a non-mathematical domain (e.g., creative writing or general question answering) with a carefully designed reward function that should remain orthogonal to safety concerns.