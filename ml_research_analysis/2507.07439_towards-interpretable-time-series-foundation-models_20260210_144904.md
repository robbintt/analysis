---
ver: rpa2
title: Towards Interpretable Time Series Foundation Models
arxiv_id: '2507.07439'
source_url: https://arxiv.org/abs/2507.07439
tags:
- time
- series
- language
- zhang
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the feasibility of distilling time series reasoning
  capabilities into small, instruction-tuned language models to build interpretable
  foundation models. The authors generate a synthetic dataset of mean-reverting time
  series using the Ornstein-Uhlenbeck process, then annotate it using a large multimodal
  model to produce natural language descriptions covering trend, noise, and extrema.
---

# Towards Interpretable Time Series Foundation Models

## Quick Facts
- arXiv ID: 2507.07439
- Source URL: https://arxiv.org/abs/2507.07439
- Reference count: 6
- Distilled time series reasoning into small Qwen models via teacher annotations and synthetic data

## Executive Summary
This paper demonstrates that small language models can acquire interpretable time series reasoning capabilities through supervised fine-tuning on synthetic data annotated by a large multimodal model. The authors generate 200 Ornstein-Uhlenbeck time series, use pixtral-large to create natural language annotations covering trend, noise, and extrema, then fine-tune compact Qwen models on numerical data paired with these annotations. Post-training results show significant improvement in time series reasoning, with high agreement scores across multiple evaluation metrics, suggesting that interpretable foundation models for time series can be built using knowledge distillation techniques.

## Method Summary
The authors create a synthetic dataset of 200 Ornstein-Uhlenbeck time series with configurable mean-reversion speed, long-term mean, and noise parameters. They generate line chart images and use pixtral-large to produce structured JSON annotations describing trend, noise, and extrema. These annotations are fact-checked using natural language inference and feature-based comparisons, with corrections applied to ~5% of extrema annotations. The numerical time series data is preprocessed into integer tokenization (00-99) to ensure consistent single-token representations. Small Qwen models (1.5B and 0.5B parameters) are then fine-tuned on the numerical data paired with annotations. The method relies on knowledge distillation from the multimodal teacher to create interpretable, compact models suitable for on-device deployment.

## Key Results
- Post-training models achieve cosine similarity of 0.98 with teacher annotations
- NLI scores exceed 0.67 across all three annotation categories (trend, noise, extrema)
- Feature-based similarity exceeds 0.8 for trend and noise, and 0.67 for extrema
- Small models show significant improvement from pre-training (NLI trend ~0.7, noise ~0.5, extrema ~0.35) to post-training
- Integer tokenization successfully preserves numerical signal for time series reasoning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structured teacher annotations from a large multimodal model can transfer time series reasoning to compact language models through supervised fine-tuning.
- Mechan: The distillation pipeline converts visual and numerical time series into natural language descriptions (trend, noise, extrema) via pixtral-large, then uses these as supervision targets for smaller Qwen models. The structured three-sentence JSON format creates consistent learning signals.
- Core assumption: The teacher model's annotations accurately capture ground-truth time series features that are learnable by smaller architectures.
- Evidence anchors:
  - [abstract] "we generate natural language annotations using a large multimodal model and use these to supervise the fine-tuning of compact Qwen models"
  - [section 2.5] "we fine-tune the small Qwen models using only the numerical time series data paired with their annotated text"
  - [corpus] CaTS-Bench and Chat-TS papers confirm multi-modal reasoning over time series is an active research direction, but do not provide direct validation of this specific distillation approach.
- Break condition: If teacher annotations contain systematic errors or contradictions not caught by fact-checking, student models will inherit these flaws.

### Mechanism 2
- Claim: Integer tokenization of rescaled time series values improves numerical reasoning in small models by ensuring consistent single-token representations.
- Mechan: Floating-point numbers are rescaled, rounded to integers (00-99), and left-padded so each digit becomes a single token. This addresses the documented problem that standard tokenizers split decimals inconsistently.
- Core assumption: Precision loss from rounding to two-digit integers does not destroy the signal needed for trend, noise, and extrema identification.
- Evidence anchors:
  - [section 2.5] "we rescale and round all floating numbers obtained by the time series generation process to integers with left padding from 00 to 99, and ensure that each digit is represented as a single token"
  - [corpus] No corpus papers directly validate this tokenization strategy; the approach is theoretical based on cited tokenization challenges.
- Break condition: If tasks require finer numerical precision than two-digit integers provide, performance will degrade on those features.

### Mechanism 3
- Claim: Multi-metric evaluation (cosine similarity, NLI, feature-based) provides complementary signal for assessing annotation quality and model capability.
- Mechan: Cosine similarity captures overall relatedness; NLI detects logical contradictions; feature-based comparison grounds annotations in computable ground truth (smoothed trend direction, variance-based noise estimation, exact extrema positions).
- Core assumption: The three metrics together sufficiently capture "good" time series reasoning without requiring human evaluation.
- Evidence anchors:
  - [section 2.3] "cosine similarity score is known to measure overall relatedness... however, might not be able to capture a strong contradiction in the logical meaning"
  - [section 2.4] "For the 'trend' and 'noise' annotations, we observe agreement rates of 93% and 95%, respectively, between pixtral-large and the fact-based sentences"
  - [corpus] CaTS-Bench proposes alternative captioning benchmarks but uses different evaluation approaches; no direct validation of this specific metric combination.
- Break condition: If models learn to produce high-scoring outputs via gaming individual metrics (e.g., generic descriptions with high cosine similarity but low information content), the evaluation will overestimate true capability.

## Foundational Learning

- Concept: **Knowledge Distillation (Teacher-Student)**
  - Why needed here: The entire method depends on transferring reasoning from pixtral-large to Qwen models. Without understanding distillation principles, you cannot diagnose failure modes or improve the pipeline.
  - Quick check question: Can you explain why a smaller model might fail to learn from teacher outputs even if the teacher is accurate?

- Concept: **Mean-Reverting Stochastic Processes (Ornstein-Uhlenbeck)**
  - Why needed here: The synthetic dataset uses OU processes with configurable κ (mean-reversion speed), r̄ (long-term mean), and σ (noise). Understanding these parameters is essential for data generation and debugging.
  - Quick check question: If you increase κ in an OU process, what happens to the autocorrelation structure of the resulting time series?

- Concept: **Natural Language Inference (NLI)**
  - Why needed here: NLI scores are a core evaluation metric, used to detect contradictions between generated and reference annotations. Misinterpreting NLI outputs could lead to wrong conclusions about model quality.
  - Quick check question: What is the difference between "neutral" and "entailment" in NLI, and why might a valid annotation receive a neutral score rather than entailment?

## Architecture Onboarding

- Component map:
  - OU process generator (κ, r̄, σ) → time series → line chart images
  - pixtral-large (image + numerical input) → JSON {trend, noise, extrema}
  - Feature extraction + NLI comparison → filtered/corrected annotations
  - Rescaler + integer converter → single-token digits (00-99)
  - Qwen2.5-1.5B-Instruct, Qwen2.5-0.5B-Instruct
  - Cosine similarity (all-MiniLM-L6-v2), NLI (roberta-large-mnli), feature similarity

- Critical path:
  1. Generate OU time series with varied parameters
  2. Render as images and annotate with pixtral-large
  3. Fact-check annotations; correct 5% extrema contradictions
  4. Tokenize numerical data as two-digit integers
  5. Fine-tune Qwen models on (numerical series, annotation) pairs
  6. Evaluate on held-out test set using all three metrics

- Design tradeoffs:
  - **Dataset size (200 samples)**: Small enough for reproducibility; may limit generalization
  - **Integer tokenization**: Consistency vs. precision loss
  - **Teacher annotation without images during student training**: Reduces multimodal requirements but may lose signal
  - **Three-sentence structure**: Constrains output format; may limit expressiveness

- Failure signatures:
  - Pre-training: Models cannot produce valid JSON; trend NLI ~0.7, noise NLI ~0.5, extrema NLI ~0.35
  - Post-training: Cosine similarity should reach ~0.98; extrema NLI below 0.67 may indicate persistent localization errors
  - Fact-check failures: >5% contradictions in trend/noise categories would suggest teacher unreliability

- First 3 experiments:
  1. **Baseline reproduction**: Generate 20 OU time series with documented parameters, run through the full pipeline, verify post-training metrics match Table 2 (cosine 0.98, NLI trend 0.875, feature trend 1.0).
  2. **Tokenization ablation**: Compare integer tokenization vs. raw floating-point strings on a 20-sample test split; measure NLI and feature similarity degradation.
  3. **Out-of-distribution test**: Generate OU series with κ, σ values outside training range; evaluate whether student models generalize or if performance collapses (this addresses the authors' stated future work on OOD generalization).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of distilled small models generalize to out-of-training-distribution (OOD) time series data?
- Basis in paper: [explicit] The authors state they plan to "explore... out-of-training-distribution generalization... in greater depth" in future work.
- Why unresolved: The current study evaluates performance on a synthetic test set derived from the same Ornstein-Uhlenbeck process as the training data.
- What evidence would resolve it: Benchmark results on time series with stochastic properties or domains (e.g., chaotic systems) distinct from the mean-reverting training distribution.

### Open Question 2
- Question: Can alternative tokenization strategies improve the precision of arithmetic reasoning for floating-point numbers without manual rounding?
- Basis in paper: [explicit] The authors list "exploring alternative numerical tokenization strategies for effective floating-point representation" as future work.
- Why unresolved: The current method rescales and rounds values to integers (00–99) to mitigate known tokenization issues, which limits numerical precision.
- What evidence would resolve it: A comparative study showing high performance on raw floating-point data using a novel tokenizer versus the current integer-mapping approach.

### Open Question 3
- Question: Does incorporating visual representation learning alongside numerical input enhance the reasoning capabilities of small time series models?
- Basis in paper: [explicit] The conclusion proposes "enhancing interpretable foundation model... capabilities through visual and numerical representation learning."
- Why unresolved: The small student models were trained exclusively on numerical data, even though the teacher model utilized visual inputs to generate the annotations.
- What evidence would resolve it: Performance benchmarks comparing models trained on numerical data alone versus those trained on combined numerical and visual time series embeddings.

## Limitations

- Dataset scalability: 200 synthetic samples may not capture real-world time series complexity (seasonality, irregular sampling, multivariate structures)
- Numerical precision tradeoff: Integer tokenization (00-99) limits fine-grained value representation
- Evaluation methodology: Automated metrics without human validation may miss semantic nuances in annotation quality
- Teacher model dependency: Reliance on expensive pixtral-large limits practical accessibility despite small student models

## Confidence

**High Confidence**: The distillation mechanism works as described for the synthetic OU process domain. The three-metric evaluation framework is methodologically sound, and the reported improvements from pre-training to post-training are substantial and internally consistent.

**Medium Confidence**: The annotation quality assessment via NLI and feature similarity adequately captures model capability for this specific task. The integer tokenization strategy sufficiently preserves numerical signal for trend/noise/extrema reasoning.

**Low Confidence**: Generalization to real-world time series beyond OU processes, especially with different statistical properties or multivariate structures. The evaluation methodology's sufficiency without human validation for semantic quality assessment.

## Next Checks

1. **Generalization Test**: Generate time series using alternative stochastic processes (e.g., geometric Brownian motion, ARIMA) and evaluate whether fine-tuned models maintain performance or experience catastrophic forgetting. This directly tests the method's robustness beyond the training distribution.

2. **Precision Sensitivity Analysis**: Create a controlled experiment varying the tokenization precision (1-digit, 2-digit, 3-digit integers) and measure degradation in feature similarity metrics. This quantifies the precision-consistency tradeoff and identifies the minimum viable precision.

3. **Human Evaluation Validation**: Select 20 test samples and have human experts rate annotation quality on a 5-point scale for coherence, accuracy, and informativeness. Compare these ratings against the automated metrics to validate whether high metric scores correspond to human-perceived quality.