---
ver: rpa2
title: 'SPARTA: Evaluating Reasoning Segmentation Robustness through Black-Box Adversarial
  Paraphrasing in Text Autoencoder Latent Space'
arxiv_id: '2510.24446'
source_url: https://arxiv.org/abs/2510.24446
tags:
- text
- adversarial
- paraphrase
- segmentation
- original
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the vulnerability of reasoning segmentation
  models to adversarial paraphrasing, where semantically equivalent but misleading
  text queries degrade model performance. To tackle this, the authors introduce SPARTA,
  a black-box optimization method that operates in the semantic latent space of a
  text autoencoder, using reinforcement learning to generate adversarial paraphrases
  that preserve grammatical correctness and meaning while degrading segmentation performance.
---

# SPARTA: Evaluating Reasoning Segmentation Robustness through Black-Box Adversarial Paraphrasing in Text Autoencoder Latent Space

## Quick Facts
- arXiv ID: 2510.24446
- Source URL: https://arxiv.org/abs/2510.24446
- Reference count: 40
- Primary result: SPARTA achieves up to 2x higher success rates than baselines on adversarial paraphrasing attacks against reasoning segmentation models

## Executive Summary
This paper addresses the vulnerability of reasoning segmentation models to adversarial paraphrasing, where semantically equivalent but misleading text queries can degrade model performance. The authors introduce SPARTA, a black-box optimization method that operates in the semantic latent space of a text autoencoder, using reinforcement learning to generate adversarial paraphrases that preserve grammatical correctness and meaning while degrading segmentation performance. SPARTA significantly outperforms state-of-the-art baselines, achieving up to 2x higher success rates on the ReasonSeg and LLMSeg-40k datasets. Despite strict constraints on semantic fidelity and grammar, current reasoning segmentation models remain vulnerable to such attacks, with success rates reaching up to 68% at a 10% relative IoU drop threshold.

## Method Summary
SPARTA operates by encoding text queries into the latent space of a SONAR autoencoder, then optimizing the latent representation through reinforcement learning to maximize segmentation IoU degradation while preserving semantic meaning. The method uses Proximal Policy Optimization (PPO) to generate adversarial paraphrases, with rewards based on IoU drop and regularization terms ensuring semantic fidelity and grammatical correctness. The optimization balances three objectives: maximizing IoU degradation, maintaining similarity to the original query (cosine similarity ≥ 0.825), and ensuring grammatical correctness through LLM scoring. The approach is evaluated on two datasets (ReasonSeg and LLMSeg-40k) with 300 samples each, targeting reasoning segmentation models including LISA, LISA++, and GSVA.

## Key Results
- SPARTA achieves up to 2x higher success rates than state-of-the-art baselines on adversarial paraphrasing attacks
- Success rates reach up to 68% at a 10% relative IoU drop threshold on the tested datasets
- Despite strict constraints on semantic fidelity and grammar, reasoning segmentation models remain vulnerable to such attacks

## Why This Works (Mechanism)
SPARTA leverages the semantic structure of text autoencoder latent space to find adversarial paraphrases that preserve meaning while degrading segmentation performance. By operating in this continuous latent space rather than discrete token space, the method can explore semantic variations more efficiently. The reinforcement learning framework allows for joint optimization of multiple constraints (IoU degradation, semantic similarity, grammatical correctness) through carefully designed reward functions and regularization terms.

## Foundational Learning

**Text Autoencoder Latent Space**: Understanding how text is encoded into continuous vector representations and decoded back to text. Why needed: SPARTA operates by manipulating these latent representations. Quick check: Verify reconstruction quality by encoding-decoding sample queries and checking BLEU scores.

**Reinforcement Learning for Text Generation**: Using policy gradient methods like PPO to optimize discrete text generation through continuous latent space manipulation. Why needed: SPARTA uses PPO to optimize latent vectors that are decoded into adversarial paraphrases. Quick check: Monitor policy entropy and reward stability during training.

**Semantic Similarity Metrics**: Computing cosine similarity between embedding vectors to measure semantic fidelity. Why needed: SPARTA requires paraphrases to maintain semantic similarity to original queries. Quick check: Test cosine similarity threshold with known semantically similar and dissimilar pairs.

**Image Segmentation Metrics**: Understanding Intersection over Union (IoU) as the primary metric for segmentation quality. Why needed: SPARTA's reward function is based on IoU degradation. Quick check: Verify IoU computation on sample segmentation outputs.

**LLM-based Quality Scoring**: Using large language models to assess grammatical correctness and semantic coherence. Why needed: SPARTA filters generated paraphrases for grammatical correctness. Quick check: Test scoring consistency with sample grammatically correct and incorrect sentences.

## Architecture Onboarding

**Component Map**: Text Query → SONAR Autoencoder → Latent Space → PPO Policy → Latent Vector → Beam Search Decoding → Paraphrase → Segmentation Model → IoU → Reward → PPO Update

**Critical Path**: The most critical execution path is: (1) Latent vector sampling and decoding → (2) Segmentation prediction and IoU computation → (3) Reward calculation → (4) PPO policy update. This loop drives the adversarial optimization process.

**Design Tradeoffs**: SPARTA trades computational efficiency for effectiveness by operating in continuous latent space rather than discrete token space. The method prioritizes semantic fidelity over maximum attack success, requiring paraphrases to maintain grammatical correctness and semantic similarity. This constraint-aware approach makes the attacks more realistic but potentially less effective than unconstrained methods.

**Failure Signatures**: Common failure modes include paraphrases being too semantically distant (cosine similarity < 0.825), reward variance being too high causing unstable PPO training, or the value baseline not learning properly. These manifest as low success rates or training instability.

**First Experiments**: 1) Test SONAR autoencoder reconstruction quality on sample queries to verify baseline performance. 2) Verify semantic fidelity filtering by testing known similar and dissimilar pairs against the 0.825 cosine similarity threshold. 3) Run a single PPO iteration with synthetic rewards to verify the optimization pipeline works correctly.

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but the work raises important considerations about the robustness of reasoning segmentation models to semantic attacks and the need for defenses against such adversarial paraphrases.

## Limitations

- Critical implementation details including SONAR checkpoint identifier, value network architecture, and optimizer settings are unspecified, creating potential reproducibility gaps
- Results are based on 300 samples each from ReasonSeg and LLMSeg-40k, which may not generalize to full datasets or other domains
- The evaluation assumes full black-box access, which may not reflect all real-world scenarios with additional constraints or partial model access

## Confidence

- **High confidence**: The fundamental approach of using text autoencoder latent space for adversarial paraphrasing is sound and the overall optimization framework is well-established
- **Medium confidence**: The reported success rates and comparative performance against baselines are likely accurate given the detailed experimental setup
- **Medium confidence**: The semantic fidelity constraints are appropriate but their effectiveness depends on unspecified LLM scoring implementation details

## Next Checks

1. Validate SONAR autoencoder reconstruction quality by testing on sample queries from ReasonSeg and verifying BLEU-4 and ROUGE-L scores match reported values (0.72 and 0.88 respectively).

2. Test semantic fidelity filtering pipeline by generating paraphrases with known semantic distances and verifying the 0.825 cosine similarity threshold correctly identifies semantically similar pairs.

3. Verify PPO reward stability by monitoring value baseline loss convergence and advantage normalization statistics during training to ensure stable optimization as described in the method section.