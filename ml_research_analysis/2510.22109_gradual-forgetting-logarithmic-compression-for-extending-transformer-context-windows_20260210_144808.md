---
ver: rpa2
title: 'Gradual Forgetting: Logarithmic Compression for Extending Transformer Context
  Windows'
arxiv_id: '2510.22109'
source_url: https://arxiv.org/abs/2510.22109
tags:
- memory
- transformer
- filters
- compression
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of processing long sequences
  in transformers, which is limited by the quadratic complexity of self-attention.
  The authors propose a novel approach that applies logarithmic compression to the
  input tokens, inspired by cognitive models of human memory.
---

# Gradual Forgetting: Logarithmic Compression for Extending Transformer Context Windows

## Quick Facts
- arXiv ID: 2510.22109
- Source URL: https://arxiv.org/abs/2510.22109
- Reference count: 39
- Primary result: A scale-invariant logarithmic compression method improves transformer language modeling perplexity on WikiText-103 and PG-19 benchmarks by enabling attention to exponentially longer contexts.

## Executive Summary
This paper addresses the quadratic complexity bottleneck in transformer self-attention for processing long sequences. The authors propose a novel approach inspired by cognitive models of human memory: applying logarithmic compression to input tokens using a bank of scale-invariant temporal filters. This creates a compressed representation of input history where recent events have high resolution and distant events have low resolution. A standard transformer then processes this compressed representation concatenated with recent uncompressed tokens, achieving improved perplexity on language modeling tasks compared to uncompressed baselines, with performance increasing as compressed memory length grows.

## Method Summary
The method applies a depth-wise 1-D convolution with fixed Gamma-function-based temporal filters (SITH) to create a compressed representation of input history. The filters have geometrically spaced peak times, creating a scale-invariant logarithmic compression where resolution decreases over time. This compressed representation, along with a subset of recent uncompressed tokens, forms the input to a standard unmodified transformer. The compression is applied once as preprocessing, reducing attention complexity from O((m+M)²d) to O((m+L)²d) where m is recent tokens, M is full history, and L is compressed memory length.

## Key Results
- Best model achieved per-word perplexity of 23.56 on WikiText-103, outperforming other transformer models of similar size
- Perplexity improves as the compressed memory length (L) increases
- Computational complexity reduced from O((m+M)²d) to O((m+L)²d) where L << M

## Why This Works (Mechanism)

### Mechanism 1: Scale-Invariant Logarithmic Input Compression
Compressing input history using log-spaced temporal filters allows transformers to access exponentially longer temporal contexts while maintaining fixed attention windows. The SITH filters with geometrically spaced peaks create compressed memory where recent events have high resolution and distant events have low resolution, mirroring cognitive models of human memory. This works under the assumption that long-range dependencies in language follow power-law decay, making coarse-grained information from the distant past sufficient for prediction.

### Mechanism 2: Hybrid Attention over Concatenated Token Streams
A standard transformer attention mechanism processes long-range history by attending to a fixed-size sequence composed of recent uncompressed tokens and compressed historical slots. The self-attention treats these m+L tokens as a unified sequence without architectural modification. This works under the assumption that the self-attention mechanism can effectively learn to weight the summary statistics from compressed slots alongside high-resolution recent tokens.

### Mechanism 3: Computational Complexity Decoupling
Preprocessing history via convolution decouples effective memory length from attention complexity, reducing the quadratic bottleneck. By computing compressed history once before transformer layers (cost O(MLd)), the attention mechanism only needs to operate on sequence length m+L rather than full history m+M. This preserves the transformer's stateless batching capability while reducing computational burden.

## Foundational Learning

**Quadratic Complexity of Self-Attention:** The paper's primary motivation is overcoming the O(N²) cost of standard attention. Understanding this bottleneck explains why authors compress sequence length rather than model depth. Quick check: Why does doubling input sequence length roughly quadruple compute cost for standard transformer?

**Temporal Convolution and Impulse Responses:** The core SITH mechanism relies on convolving input embeddings with specific kernel shapes (Gamma functions). One must grasp how sliding window convolution creates smoothed summary of past. Quick check: How does shape of filter's impulse response determine smoothing/memory applied to input signal?

**Logarithmic vs. Linear Representation:** The paper argues for "gradual forgetting" where resolution decreases over time. Understanding logarithmic spacing allocates more resources to near past and fewer to distant past is key to "scale-invariant" claim. Quick check: If filter bank is log-spaced, does it have higher density of filters for events 10-20 steps ago or 1000-1010 steps ago?

## Architecture Onboarding

**Component map:** Input Embeddings -> SITH Filter Bank -> Normalizer -> Sequence Constructor -> Standard Transformer

**Critical path:** The filter bank initialization and application. If filters (τ values or width k) are misconfigured, compressed tokens may contain only noise or redundant copies of present, providing no long-range value.

**Design tradeoffs:**
- Filter Width (k): High k = narrow filters (precise but miss context between peaks); Low k = wide filters (smooths too much, losing detail)
- Number of Filters (L) vs. Window (M): Increasing L improves performance but increases attention cost; Increasing M extends history reach without increasing attention cost but increases preprocessing convolution cost
- Fixed vs. Learned Filters: Paper describes filters as fixed preprocessing step (implied "scale-invariant"), though modern implementations might fine-tune them

**Failure signatures:**
- Perplexity Stagnation: If perplexity doesn't decrease as L increases, model is likely ignoring compressed tokens (attention weights on compressed slots ≈ 0)
- Loss Instability: If normalization is omitted on compressed slots, their magnitude might differ significantly from uncompressed tokens, destabilizing attention softmax

**First 3 experiments:**
1. Baseline Comparison (Delta Pulse): Run model with "delta pulse" filters (effectively no compression, just truncation) vs. scale-invariant filters to confirm shape of compression matters, not just token count
2. Filter Ablation (L sweep): Vary number of filters (e.g., 5, 17, 53) while keeping uncompressed window m fixed to replicate logarithmic performance trend
3. Parameter k Sensitivity: Test filter sharpness (k=10, 100, 200) to observe trade-off between temporal resolution and noise smoothing on validation loss

## Open Questions the Paper Calls Out
None

## Limitations
- Assumes long-range dependencies in language follow power-law decay, which may not hold for all domains or text types
- Experimental validation limited to WikiText-103 and PG-19 language modeling benchmarks, lacking generalization to other domains
- Filter design uses fixed Gamma distributions rather than demonstrating optimality compared to learned alternatives

## Confidence
**High Confidence:** The core mechanism of logarithmic compression via temporal filters is technically sound and clearly described; experimental results on tested benchmarks show consistent perplexity improvements; hybrid attention architecture is correctly implemented and functional.

**Medium Confidence:** The claim that this approach meaningfully extends context windows for practical applications beyond tested benchmarks; assertion that computational savings are significant in real-world deployment scenarios; specific choice of Gamma-function-based SITH filters being optimal or necessary for observed performance gains.

**Low Confidence:** Generalization to domains with different dependency structures (e.g., code, legal documents, multilingual text); performance relative to emerging alternatives like local attention patterns or state-space models on same tasks; necessity of specific "scale-invariant" logarithmic property versus other compression schemes.

## Next Checks
1. Cross-Domain Dependency Structure Analysis: Test model on code generation (CodeXGLUE), multilingual text (XL-WikiText), and structured/formal text datasets to verify power-law decay assumption holds and compression remains effective across dependency patterns.

2. Filter Architecture Ablation Study: Replace fixed SITH Gamma filters with learned temporal convolution kernels or alternative compression functions (e.g., polynomial decay, exponential forgetting) while keeping overall architecture identical to determine if specific filter shape is critical to performance.

3. End-to-End Computational Benchmarking: Measure wall-clock inference time, memory consumption, and throughput on GPU/CPU for compressed model versus standard transformer with sliding window attention, both achieving similar effective context lengths, to empirically validate claimed computational advantages.