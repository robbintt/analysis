---
ver: rpa2
title: Ordinal Label-Distribution Learning with Constrained Asymmetric Priors for
  Imbalanced Retinal Grading
arxiv_id: '2509.26146'
source_url: https://arxiv.org/abs/2509.26146
tags:
- ordinal
- latent
- asymmetric
- prior
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CAP-WAE addresses the challenge of imbalanced, ordinal diabetic
  retinopathy grading by integrating a constrained asymmetric prior, ordinal label-distribution
  supervision, and geometric regularization within a Wasserstein autoencoder framework.
  The asymmetric prior preserves minority class structure in the latent space, while
  direction-aware ordinal supervision and adaptive multi-task weighting improve sensitivity
  to clinical grading errors.
---

# Ordinal Label-Distribution Learning with Constrained Asymmetric Priors for Imbalanced Retinal Grading

## Quick Facts
- arXiv ID: 2509.26146
- Source URL: https://arxiv.org/abs/2509.26146
- Reference count: 32
- Primary result: CAP-WAE achieves up to +3.2 QWK and +2.6 macro-F1 improvements over strong baselines on four public DR benchmarks.

## Executive Summary
CAP-WAE addresses the challenge of imbalanced, ordinal diabetic retinopathy grading by integrating a constrained asymmetric prior, ordinal label-distribution supervision, and geometric regularization within a Wasserstein autoencoder framework. The asymmetric prior preserves minority class structure in the latent space, while direction-aware ordinal supervision and adaptive multi-task weighting improve sensitivity to clinical grading errors. The Margin-Aware Orthogonality and Compactness (MAOC) loss ensures grade-ordered, well-separated latent clusters. On four public DR benchmarks, CAP-WAE consistently outperforms prior methods, achieving up to +3.2 QWK and +2.6 macro-F1 improvements over strong baselines, with t-SNE visualizations confirming more compact, severity-ordered latent representations. The approach offers a robust, tuning-light solution for ordinal, imbalanced medical grading tasks.

## Method Summary
CAP-WAE combines a VGG16 encoder (ImageNet pretrained) with a symmetric transposed-conv decoder and three heads: classifier, asymmetric Gaussian (AG) dispersion predictor, and ordinal regression (ORM) severity scorer. The model uses Wasserstein MMD alignment to a constrained asymmetric Generalized Gaussian prior (β=1.2) to preserve minority class structures, direction-aware ordinal soft labels with asymmetric dispersions to penalize under-grading, and Margin-Aware Orthogonality and Compactness (MAOC) loss to enforce grade-ordered latent clusters. Training uses AdamW with adaptive multi-task weighting via learned log-variances, batch size 32, and 100 epochs. The asymmetric prior parameters are fixed from training data, while AG dispersions are clamped to [0.2, 5.0] and MAOC uses margin δ=0.1 and compactness weight 0.5.

## Key Results
- CAP-WAE achieves up to +3.2 QWK and +2.6 macro-F1 improvements over strong baselines on four public DR benchmarks.
- On Zenodo-DR-7 (7-class), CAP-WAE achieves QWK of 0.847 vs. 0.788 for the best baseline (CTT-VAE).
- t-SNE visualizations show more compact, severity-ordered latent clusters compared to symmetric prior baselines.
- CAP-WAE demonstrates consistent performance across all four benchmarks (Zenodo-DR-7, IDRiD, APTOS-2019, Messidor-2) with reduced sensitivity to hyperparameter tuning.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Aligning the aggregate posterior to an asymmetric, heavy-tailed prior preserves minority class structures that symmetric Gaussian priors collapse.
- Mechanism: The model computes a factorized Asymmetric Generalized Gaussian Distribution (AGGD) prior with empirically-estimated parameters (μ, β, α_ℓ, α_r) from the data. MMD-based Wasserstein alignment matches the aggregate posterior to this prior at the distribution level (not pointwise KL), allowing non-Gaussian structures to persist.
- Core assumption: Minority class features exhibit skew and heavier tails than a Gaussian; preserving these improves their separability.
- Evidence anchors:
  - [abstract]: "The asymmetric prior preserves minority class structure in the latent space"
  - [section 3.2]: "This distribution-level matching allows the model to preserve the non-Gaussian structures of minority classes, which would otherwise be lost under the instance-level KL divergence penalty in a standard VAE."
  - [corpus]: CTTVAE (latent space structuring for imbalanced data) supports the premise that structured latent priors help under imbalance, though not DR-specific.
- Break condition: If minority features are approximately Gaussian or the dataset is near-balanced, the asymmetric prior's advantage diminishes.

### Mechanism 2
- Claim: Direction-aware ordinal soft labels with asymmetric dispersions penalize under-grading more severely than over-grading, improving clinical sensitivity.
- Mechanism: A lightweight head predicts per-instance left/right spreads (σ_ℓ, σ_r). These define an asymmetric soft label distribution around the ground-truth grade. When under-grading is clinically costlier, σ_r can exceed σ_ℓ, shifting probability mass to higher grades and penalizing low predictions more strongly via soft cross-entropy.
- Core assumption: Clinical grading errors are asymmetric; under-grading is more harmful (missed referrals) than over-grading (false alarms).
- Evidence anchors:
  - [abstract]: "direction-aware ordinal supervision... penalizing under-grading more severely"
  - [section 3.3]: Eq. (9) defines asymmetric Gaussian soft labels; Eq. (10) applies them in soft CE loss.
  - [corpus]: Ordinal Adaptive Correction (arXiv:2509.02351) also exploits ordinal structure and asymmetry in noisy labels, aligning with directional penalty intuition.
- Break condition: If clinical costs are symmetric or grades are not meaningfully ordinal, the directional advantage collapses to standard soft labels.

### Mechanism 3
- Claim: Margin-Aware Orthogonality and Compactness (MAOC) loss enforces grade-ordered, near-orthogonal latent clusters with low intra-class variance.
- Mechanism: MAOC combines (1) orthogonality: penalize cosine similarity between class prototype vectors beyond margin δ, and (2) compactness: pull each latent to its class mean. This yields severity-ordered, well-separated bands in the latent manifold.
- Core assumption: Class prototypes can be meaningfully orthogonalized in the latent space; grades have a natural ordering that aligns with prototype angles.
- Evidence anchors:
  - [abstract]: "MAOC loss ensures grade-ordered, well-separated latent clusters"
  - [section 3.4]: Eq. (12) defines L_MAOC with orthogonality and compactness terms.
  - [corpus]: Corpus lacks direct MAOC analogs; orthogonality-compactness regularization in ordinal medical grading is underexplored.
- Break condition: If class counts exceed latent dimensionality or latent redundancy is high, orthogonality constraints may conflict with reconstruction/supervision objectives.

## Foundational Learning

### Concept: Wasserstein Autoencoders and MMD matching
- Why needed here: CAP-WAE replaces pointwise KL with aggregate-level MMD alignment to support non-Gaussian priors.
- Quick check question: Can you explain why MMD-based distribution matching preserves multi-modal structures better than KL?

### Concept: Ordinal Label Distribution Learning (OLDL)
- Why needed here: The supervision uses soft distributions over ordered grades and distance-sensitive divergences (e.g., CDF-based or quadratic-form).
- Quick check question: How does a CDF-based distance (e.g., 1-Wasserstein) penalize ordinal errors differently from standard CE?

### Concept: Asymmetric Generalized Gaussian Distribution (AGGD)
- Why needed here: The latent prior uses AGGD with separate left/right scales and a tail parameter; understanding parameter semantics is essential for diagnostics.
- Quick check question: What happens to the shape of AGGD as β decreases and α_r ≫ α_ℓ?

## Architecture Onboarding

### Component map
Encoder (VGG16 + FC) -> latent z (512-dim) -> Decoder (transposed conv) for reconstruction; Heads on z: Classifier (logits), AG head (σ_ℓ, σ_r for soft labels), ORM head (scalar severity score); Regularizers: MMD(posterior, p_cap), MAOC on batch prototypes.

### Critical path
Forward: x -> encoder -> z -> (decoder, heads). Compute L_recon, L_MMD, L_MAOC, L_CE, L_AG, L_ORM. Backward: Adaptive weighting via learned log-variances s_k balances supervised losses.

### Design tradeoffs
- Prior stationarity: p_cap parameters are fixed from data; if domain shifts, re-estimate or risk misalignment.
- MAOC vs. capacity: Strong orthogonality constraints can conflict with reconstruction fidelity if latent capacity is limited.
- Tuning-light: Adaptive weighting reduces manual tuning but introduces learnable parameters that may overfit on small validation sets.

### Failure signatures
- Minority collapse: t-SNE shows severe-grade clusters absorbed into majority clusters; check if MMD weight λ_reg is too low.
- Over-regularization: Reconstruction loss plateaus high; MAOC margin δ or weight λ_maoc may be too aggressive.
- Under-grading bias: Macro-F1 low on severe classes; verify AG-soft asymmetry (σ_r vs. σ_ℓ) and loss weighting.

### First 3 experiments
1. Baseline sanity check: Train VAE-KL and WAE-MMD on Zenodo-DR-7; confirm reported QWK gaps (Table 3) before adding CAP/MAOC.
2. Ablation on prior: Compare symmetric Gaussian vs. AGGD prior with MMD fixed; visualize t-SNE and measure minority-class F1.
3. MAOC sensitivity: Sweep margin δ ∈ {0.05, 0.1, 0.2} and λ_maoc ∈ {0.01, 0.05, 0.1}; plot QWK vs. macro-F1 and inspect orthogonality (prototype cosine matrix).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the constrained asymmetric prior be adapted dynamically under domain shift, rather than remaining fixed based on training statistics?
- Basis in paper: [explicit] The conclusion states: "the approach assumes stationarity of training distributions when fixing asymmetric priors" and future work should "investigate adaptive prior learning under domain shift."
- Why unresolved: The current CAP-WAE computes prior parameters empirically from training data once, which may not generalize when test distributions diverge.
- What evidence would resolve it: Evaluation on cross-domain benchmarks (e.g., training on APTOS, testing on Messidor-2) with learned/adapted priors showing maintained or improved QWK over fixed priors.

### Open Question 2
- Question: Does integrating self-supervised or multi-modal representations reduce dependence on large, cleanly annotated ordinal datasets?
- Basis in paper: [explicit] The authors explicitly call for future work to "integrate self-supervised or multi-modal representations to reduce annotation dependence."
- Why unresolved: Current performance relies on reliable ordinal annotations, which the authors acknowledge "in clinical practice can be noisy or inconsistent."
- What evidence would resolve it: Experiments combining CAP-WAE with self-supervised pretraining (e.g., contrastive learning on unlabeled retinal images) demonstrating comparable performance with fewer labeled examples.

### Open Question 3
- Question: Can the CAP-WAE framework be extended to model longitudinal disease progression across patient visits?
- Basis in paper: [explicit] Future work should "extend to longitudinal modeling of progression" per the conclusion.
- Why unresolved: The current formulation treats each image independently without temporal modeling of how patients transition between severity grades over time.
- What evidence would resolve it: A temporal extension (e.g., recurrent or transformer-based latent dynamics) evaluated on longitudinal DR cohorts, showing improved prediction of future grade transitions.

### Open Question 4
- Question: What theoretical guarantees exist for global latent separability under the MAOC constraint?
- Basis in paper: [inferred] The authors note MAOC "enforces local alignment without theoretical guarantees of global separability."
- Why unresolved: While MAOC empirically produces well-separated clusters, it remains unclear whether orthogonality margins guarantee global manifold structure across all classes.
- What evidence would resolve it: Theoretical analysis relating MAOC margin constraints to inter-class distance bounds, or empirical stress tests with increasing class numbers showing when separability degrades.

## Limitations
- Prior parameter estimation procedure is not fully specified (warm-up pass, periodic updates, or fixed initialization unclear).
- MAOC prototype update mechanism lacks specific momentum coefficient for exponential moving average.
- MMD kernel details (RBF vs. IMQ, bandwidth selection method) are not provided.

## Confidence

### High
- The core framework combining Wasserstein Autoencoders, ordinal label-distribution learning, and asymmetric priors is theoretically sound and aligns with prior work on imbalanced learning and ordinal regression.

### Medium
- The reported performance gains (e.g., +3.2 QWK, +2.6 macro-F1) are significant, but the lack of ablation studies on individual components (e.g., AGGD prior vs. symmetric Gaussian) limits confidence in attributing improvements to specific mechanisms.

### Low
- The generalizability of the asymmetric prior to non-DR ordinal tasks (e.g., histopathology grading) is untested and may depend on dataset-specific feature distributions.

## Next Checks
1. **Ablation on Prior**: Compare CAP-WAE with a symmetric Gaussian prior (fixed β=2.0, αₗ=αᵣ) while keeping MMD alignment constant. Visualize t-SNE embeddings and measure minority-class F1 to isolate the impact of asymmetry.
2. **MAOC Sensitivity**: Systematically vary the orthogonality margin δ ∈ {0.05, 0.1, 0.2} and compactness weight λ_MAOC ∈ {0.01, 0.05, 0.1}. Plot QWK vs. macro-F1 trade-offs and compute prototype cosine similarity matrices to quantify orthogonality.
3. **Domain Shift Robustness**: Retrain CAP-WAE on IDRiD, then test on Messidor-2 (and vice versa) to evaluate cross-dataset generalization. Compare performance drops against baselines to assess prior stability under distribution shifts.