---
ver: rpa2
title: 'CQD-SHAP: Explainable Complex Query Answering via Shapley Values'
arxiv_id: '2510.15623'
source_url: https://arxiv.org/abs/2510.15623
tags:
- query
- shapley
- answer
- queries
- atom
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of explaining complex query
  answering (CQA) results in knowledge graphs, focusing on the contribution of individual
  query atoms. The authors introduce CQD-SHAP, a novel framework based on Shapley
  values from cooperative game theory, to quantify the importance of each atom in
  ranking a specific answer.
---

# CQD-SHAP: Explainable Complex Query Answering via Shapley Values

## Quick Facts
- arXiv ID: 2510.15623
- Source URL: https://arxiv.org/abs/2510.15623
- Reference count: 40
- Key outcome: Novel framework using Shapley values to quantify importance of query atoms in ranking CQA answers, significantly outperforming baselines.

## Executive Summary
CQD-SHAP introduces an explanation framework for neural complex query answering (CQA) models that quantifies the contribution of each query atom to a specific answer's ranking. The method uses Shapley values from cooperative game theory, treating query atoms as players and measuring the improvement in answer rank when each atom is executed neurally versus symbolically. Extensive experiments on FB15k-237 and NELL995 demonstrate superior performance in both necessary and sufficient explanation scenarios compared to multiple baselines.

## Method Summary
The CQD-SHAP framework decomposes complex queries into atomic link prediction tasks and uses Shapley values to quantify each atom's contribution to a target answer's ranking. A value function based on Î”Rank measures the change in rank when atoms are executed neurally versus symbolically. The method constructs partial queries by executing atoms in a subset $S$ neurally (via pre-trained link predictor) and others symbolically (via graph lookup), then computes exact Shapley values by averaging marginal contributions across all possible coalitions. The approach satisfies key Shapley axioms and provides interpretable explanations at the atom level.

## Key Results
- Achieves up to 0.999 reduction in MRR for necessary explanations
- Achieves up to 0.521 improvement for sufficient explanations
- Significantly outperforms multiple baselines across 8 query types on FB15k-237 and NELL995 datasets

## Why This Works (Mechanism)

### Mechanism 1: Shapley Value Attribution for Query Atom Importance
If query atoms are treated as players in a cooperative game with ranking change as the value function, then Shapley values can quantify the contribution of each atom to a target answer's ranking. The method defines a cooperative game where players are query atoms. For any subset of atoms executed neurally (coalition $S$), a value function $\text{val}_{e_i}(Q_S)$ computes the change in rank of a target answer $e_i$ compared to a purely symbolic execution. The Shapley value $\phi_a(e_i)$ for an atom $a$ is computed by averaging its marginal contribution across all possible coalitions. This measures the expected improvement in rank from executing atom $a$ neurally rather than symbolically.

### Mechanism 2: Partial Query Execution via Neural-Symbolic Hybridization
If a query is decomposed into atoms, each can be executed independently using either neural (link prediction) or symbolic (graph traversal) methods, allowing the construction of partial queries to evaluate atom contributions. The framework constructs partial queries $Q_S$ where atoms in set $S$ are executed neurally using a link predictor (like CQD's t-norm aggregation), and atoms not in $S$ are executed symbolically by direct lookup in the observed knowledge graph. This allows comparing the ranking of an answer under different execution strategies.

### Mechanism 3: Delta-Rank as the Value Function
If the Quantity of Interest (QoI) is defined as the change in rank of a target answer (Delta-Rank), the resulting value function satisfies Shapley axioms and provides an interpretable payoff. The value function is defined as $\text{val}_{e_i}(Q_S) = r_i(Q_{\{\}}) - r_i(Q_S)$, where $r_i$ is the rank of answer $e_i$. This represents the improvement in rank when using partial query $Q_S$ versus a fully symbolic query $Q_{\{\}}$. The empty set has value 0, satisfying a key axiom. Positive values indicate rank improvement (neural helps), negative indicate degradation.

## Foundational Learning

**Concept: Shapley Values in Game Theory**
- Why needed here: This is the core mathematical framework CQD-SHAP uses to fairly distribute "credit" for an answer's ranking among query atoms.
- Quick check question: Can you explain why Shapley values are considered a "fair" attribution method compared to, say, ablation studies?

**Concept: Neurosymbolic Query Answering (CQD)**
- Why needed here: CQD-SHAP is an explanation framework built *on top of* the CQD model. You must understand how CQD decomposes queries and uses neural link predictors before you can explain its results.
- Quick check question: How does CQD combine the results of individual atomic link predictions into a final answer score for a complex conjunctive query?

**Concept: Knowledge Graph Completeness & "Hard" Answers**
- Why needed here: The central premise is that real KGs are incomplete. The method's value lies in explaining answers that are not reachable by symbolic traversal ("hard" answers) but are inferred by the neural model.
- Quick check question: What is the difference between an "easy" answer and a "hard" answer in the context of complex query answering on an incomplete knowledge graph?

## Architecture Onboarding

**Component map:**
Query -> Atom Decomposer -> Execution Engine -> Coalition Generator -> Ranker -> Shapley Calculator -> Shapley Values

**Critical path:**
1. **Correct Query Decomposition:** The logic for converting the FOL query into a dependency graph of atoms is essential. If the dependencies are wrong, the subsequent execution will fail.
2. **Accurate Partial Query Execution:** The execution engine must correctly switch between neural and symbolic modes and aggregate scores using the specified t-norms/conorms. Any bug here propagates to all Shapley value calculations.
3. **Correct Rank Computation:** The rank calculation must use the filtered setting (Eq. 6) to exclude other hard answers from the ranking to ensure a fair metric.

**Design tradeoffs:**
- **Exact vs. Approximate Shapley:** The paper computes exact Shapley values ($2^{|A|}$ permutations), which is feasible only because complex queries have a small number of atoms (typically < 10). Scaling to larger queries would require approximation (e.g., sampling permutations), sacrificing some fidelity.
- **Choice of Value Function:** Using $\Delta Rank$ makes the explanation specific to a single answer. A different QoI (e.g., recall@k) would produce different Shapley values for the same atom, changing the explanation's focus.
- **Explanation Granularity:** The method explains the *mechanism* (neural vs. symbolic) at the *atom* level. It does not explain which *facts* in the KG or which *features* in the neural model are responsible.

**Failure signatures:**
- **Trivial Explanations:** All Shapley values are near zero. This could mean the neural model provides no benefit over symbolic execution for this query type, or that the query structure makes ranking insensitive to single-atom changes (e.g., long disjunction chains).
- **Unstable Explanations:** Small changes in model weights or data lead to drastically different Shapley values. This would suggest the underlying model's behavior is not robust.
- **Incoherent Results:** Sum of Shapley values does not equal the total rank change (violating the efficiency axiom), indicating an implementation error in the value function or rank computation.

**First 3 experiments:**
1. **Reproduce Baseline CQD Performance:** Before explaining, verify your re-implementation of the CQD execution engine matches the original paper's MRR/Hits@1 on FB15k-237 and NELL995.
2. **Validate Shapley Axioms:** Manually verify for a small query that the sum of computed Shapley values equals the total rank change between fully neural and fully symbolic execution (check the efficiency axiom from Eq. 9).
3. **Run a Necessary Explanation Test:** Pick a set of queries with top-ranked answers, compute CQD-SHAP values, and verify that symbolically executing the top-ranked atom causes a significant drop in MRR, as reported in the paper's quantitative results.

## Open Questions the Paper Calls Out

**Open Question 1:** How can evaluation benchmarks be redesigned to ensure "hard" answers genuinely require neural inference?
- **Basis in paper:** [Explicit] The authors found that some "hard" answers in current benchmarks are symbolically retrievable and suggest, "Future research could benefit from developing more carefully designed datasets... ensuring that evaluation benchmarks better capture the intended query difficulty."
- **Why unresolved:** Current noise in "hard" answer labels obscures the true performance and explanation evaluation of neural inference models.
- **What evidence would resolve it:** A new benchmark where every "hard" answer is verified to be strictly unattainable via symbolic traversal of the incomplete KG.

**Open Question 2:** Can explanation methods be extended to identify influential knowledge graph triples rather than just query atoms?
- **Basis in paper:** [Explicit] The authors suggest, "Instead of working at the query atom level, future studies could aim to identify the most influential triples in the KG that contribute to a target answer."
- **Why unresolved:** Current explanations identify *which* part of the query is important, but not *which specific facts* in the underlying graph drive that importance.
- **What evidence would resolve it:** An explanation framework that outputs a subgraph of specific triples responsible for a complex query answer.

**Open Question 3:** How can local atom-level explanations be aggregated to provide global insights into a model's behavior?
- **Basis in paper:** [Explicit] The authors state, "Future research could also focus on developing global explanations of the system, for example, by aggregating the local explanations..."
- **Why unresolved:** While CQD-SHAP explains individual predictions, understanding the holistic behavior and failure modes of the CQA model remains a manual process.
- **What evidence would resolve it:** A mechanism that summarizes Shapley values across thousands of queries to detect systematic model biases or structural weaknesses.

## Limitations

- The effectiveness of CQD-SHAP depends on the fidelity of the underlying CQD model; if the neural link predictor's errors are not well-calibrated, the Shapley attributions may be misleading.
- The method assumes atoms can be executed independently, which may not hold for all query types with complex interdependencies, potentially limiting generalizability.
- The practical utility of atom-level explanations for end-users (e.g., domain experts) is not empirically demonstrated.

## Confidence

- **High confidence:** The mathematical formulation of Shapley values and their axioms is sound and correctly applied.
- **Medium confidence:** The experimental results are promising, but the evaluation focuses on a narrow set of query types and datasets. External validation on more diverse KGs and queries is needed.
- **Low confidence:** The practical utility of atom-level explanations for end-users (e.g., domain experts) is not empirically demonstrated.

## Next Checks

1. Test the stability of Shapley values by running CQD-SHAP on the same queries with slightly perturbed model weights to check for robustness.
2. Evaluate explanations on a knowledge graph with a different schema (e.g., Wikidata) to assess generalizability beyond the FB15k-237 and NELL995 domains.
3. Conduct a user study to determine if the atom-level explanations improve user trust or understanding of CQA results compared to a no-explanation baseline.