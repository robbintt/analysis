---
ver: rpa2
title: 'Social preferences with unstable interactive reasoning: Large language models
  in economic trust games'
arxiv_id: '2505.17053'
source_url: https://arxiv.org/abs/2505.17053
tags:
- llms
- game
- trust
- responses
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates whether large language models (LLMs) exhibit\
  \ human-like social preferences and interactive reasoning in economic trust games.\
  \ Three LLMs\u2014ChatGPT-4, Claude, and Bard\u2014were tested across one-shot,\
  \ repeated, and probabilistic repeated trust games, with and without selfish or\
  \ unselfish personas."
---

# Social preferences with unstable interactive reasoning: Large language models in economic trust games

## Quick Facts
- arXiv ID: 2505.17053
- Source URL: https://arxiv.org/abs/2505.17053
- Reference count: 31
- Three LLMs tested in trust games show human-like social preferences in one-shot scenarios but unstable interactive reasoning in multi-round settings

## Executive Summary
This study investigates whether large language models exhibit human-like social preferences and interactive reasoning in economic trust games. Three LLMs—ChatGPT-4, Claude, and Bard—were tested across one-shot, repeated, and probabilistic repeated trust games, with and without selfish or unselfish personas. Without prompting, LLMs demonstrated trust behaviors comparable to humans in one-shot games, but diverged significantly in repayment and multi-round scenarios. Personas had the strongest influence on responses, overshadowing model differences or game type. ChatGPT-4 showed the most stable and human-aligned interactive reasoning when given personas. Overall, LLMs display social preferences but lack consistent interactive reasoning, suggesting current models struggle with adaptive, context-dependent social exchange behaviors.

## Method Summary
The study used three LLMs (ChatGPT-4, Claude, Bard) across three personas (unspecified, selfish, unselfish) × three game types (one-shot, repeated, probabilistic repeated). Senders chose to send 0, 4, 8, or 12 points (tripled); receivers returned any amount. Experiments ran Nov 2023–Jan 2024 via web interfaces, with Chain-of-Thought prompting to verify rule comprehension. One-shot games used ~600-700 games/LLM/persona; multi-round games used 330-500 unique games with 5 games per inquiry. LLMs played against themselves. Human baseline came from a prior study of 82 female participants.

## Key Results
- LLMs showed trust and reciprocity behaviors comparable to humans in one-shot games without persona prompting
- Persona framing produced behavioral shifts 2-2.5× larger than differences between models or game types
- Interactive reasoning in multi-round games was unstable and appeared random rather than strategically adaptive
- ChatGPT-4 with personas showed more stable reciprocal adaptation than other models

## Why This Works (Mechanism)

### Mechanism 1: Persona Framing Dominates Response Variance
- Prompting LLMs with selfish/unselfish personas produces larger behavioral shifts than switching between different models or game structures
- Persona prompts activate specific patterns in the training corpus associated with those behavioral archetypes, overriding default response tendencies
- Core assumption: Training data contains sufficiently coherent representations of "selfish" and "unselfish" behavioral patterns that can be reliably retrieved
- Evidence: "LLMs' responses varied significantly when prompted to adopt personas like selfish or unselfish players, with the impact outweighing differences between models or game types"
- Break condition: If personas were merely surface-level linguistic effects, they should not produce consistent strategic adaptation differences

### Mechanism 2: Social Preferences Emerge from Training Distribution
- LLMs exhibit trust and reciprocity behaviors in one-shot games without explicit prompting, matching human baseline levels
- Training corpora contain abundant examples of trust-based social exchanges; these patterns are retrieved as plausible continuations when LLMs encounter game scenarios
- Core assumption: Statistical distribution of human trust behavior in training data maps onto behavioral patterns humans exhibit in controlled experiments
- Evidence: "LLMs deviate from pure self-interest and exhibit trust and reciprocity even without being prompted to adopt a specific persona"
- Break condition: If this were mere imitation without representation, LLMs should fail when game mechanics change

### Mechanism 3: Interactive Reasoning Fails Under Complexity
- LLMs lack stable, reproducible adaptive behavior in multi-round interactions; apparent reasoning is context-noise rather than strategic computation
- LLMs generate plausible next-action tokens based on local context but lack persistent internal state that tracks counterpart behavior across rounds
- Core assumption: Human-like interactive reasoning requires maintaining and updating a model of the other player—capability not demonstrated here
- Evidence: "Interactive reasoning to the actions of counterparts or changing game mechanics appeared to be random rather than stable, reproducible characteristics"
- Break condition: ChatGPT-4 with personas showed more stable reciprocal adaptation—suggesting scaffolding can partially compensate

## Foundational Learning

- **Concept: Trust Game Structure**
  - Why needed: The entire methodology depends on understanding why rational self-interest predicts zero exchange, and why human deviation from this benchmark reveals social preferences
  - Quick check: If both players were perfectly rational self-interested agents, what would the sender transfer and what would the receiver return?

- **Concept: Social Preferences vs. Strategic Reasoning**
  - Why needed: The paper explicitly decomposes behavior into these two components; conflating them prevents understanding why LLMs match humans on one-shot trust but diverge on multi-round adaptation
  - Quick check: In a repeated game, does sending more in round 1 reflect social preference, strategic reasoning about reputation, or both?

- **Concept: Persona Prompting as Behavioral Scaffolding**
  - Why needed: The persona effect is the largest finding by effect size; understanding it as a framing/priming intervention is essential for experimental design
  - Quick check: Why might a "selfish" persona produce different results than simply asking the model to maximize its payoff?

## Architecture Onboarding

- **Component map:** Game Prompt → LLM (web interface) → Response → Parser → Proportion Sent/Returned → Persona Prompt (optional) → Multi-round Context Window (accumulates prior rounds)

- **Critical path:**
  1. Define game mechanics with Chain-of-Thought verification to confirm rule comprehension
  2. Apply persona framing if needed
  3. Run sufficient repetitions (600-700 for one-shot, 330-500 for multi-round) to capture variance
  4. Compare proportions, not raw amounts, across conditions

- **Design tradeoffs:**
  - Web interface vs. API: Web captures real-world user experience but loses parameter control; API gives reproducibility but may miss safety-tuned behaviors
  - Self-play vs. human-LLM pairs: Self-play controls for partner variability but may produce artifacts
  - Human comparison baseline: Using an all-female participant pool from a prior study introduces demographic limitations

- **Failure signatures:**
  - LLM returns exactly 0 in one-shot receiver role → may have collapsed to strategic reasoning without social preference activation
  - No variance across 600 repetitions → likely deterministic output (temperature too low or prompt too constraining)
  - Multi-round responses identical to one-shot → interactive reasoning not engaged

- **First 3 experiments:**
  1. Replicate one-shot unspecified persona baseline to confirm trust emerges without prompting; verify response distribution variance is lower than human baseline
  2. Test persona robustness: use multiple phrasings for "selfish" and "unselfish" to confirm effect is not prompt-artifact specific
  3. Probe interactive reasoning: vary counterpart behavior systematically (always generous, always selfish, tit-for-tat) and measure whether LLM adapts or responds independently of partner history

## Open Questions the Paper Calls Out

- What mechanisms underlie the divergence between LLMs and humans in trust repayment and multi-round interactions, when initial trust expression appears aligned?
- What types of persona specification information are necessary and sufficient to reliably elicit target behaviors from LLMs in social contexts?
- Is the observed instability in LLMs' interactive reasoning an inherent limitation of current architectures, or can it be improved through fine-tuning or prompting strategies?
- How do findings generalize to more diverse human populations beyond the exclusively female university student sample used for comparison?

## Limitations
- Web interfaces used rather than API access, leaving exact model parameters and potential platform confounds unknown
- Human comparison baseline from a prior study with a demographically homogeneous sample (all female participants)
- Persona definitions and exact phrasing were not fully specified, raising questions about whether effects are concept-specific or artifact-specific
- Multi-round games used self-play, which may produce artificial dynamics unlike human-LLM interactions

## Confidence

- **High confidence**: LLMs exhibit social preferences in one-shot games without explicit prompting; persona framing produces large behavioral shifts; LLMs lack consistent interactive reasoning in multi-round settings
- **Medium confidence**: ChatGPT-4 with personas shows more stable reciprocal adaptation; effect sizes of personas are 2-2.5× larger than model/game type differences; human-LLM alignment is stronger in one-shot than multi-round games
- **Low confidence**: The claim that interactive reasoning is "random" rather than strategic—this depends on untested partner strategies and may reflect task design rather than fundamental incapacity

## Next Checks
1. Replicate the one-shot baseline with API-controlled models (fixed temperature, no system prompts) to isolate persona effects from platform confounds
2. Test persona robustness by varying the exact phrasing of "selfish" and "unselfish" prompts to confirm effects are not artifact-specific
3. Implement systematic partner behavior variation (always generous, always selfish, tit-for-tat) in multi-round games to test whether LLMs adapt strategically or respond independently of partner history