---
ver: rpa2
title: Synthesizing Agentic Data for Web Agents with Progressive Difficulty Enhancement
  Mechanisms
arxiv_id: '2510.13913'
source_url: https://arxiv.org/abs/2510.13913
tags:
- answer
- question
- facts
- agent
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Synthesizing Agentic Data for Web Agents with Progressive Difficulty Enhancement Mechanisms

## Quick Facts
- **arXiv ID:** 2510.13913
- **Source URL:** https://arxiv.org/abs/2510.13913
- **Reference count:** 40
- **Primary result:** Synthesized dataset achieves 31.4% accuracy on GAIA benchmark, outperforming larger baselines despite being smaller.

## Executive Summary
This paper introduces ProgSearch, a novel pipeline for synthesizing high-quality multi-hop QA data to train long-horizon web agents. The core innovation is a progressive difficulty mechanism that calibrates task complexity to the failure point of a strong baseline agent, creating challenging but solvable samples in an optimal training zone. The pipeline uses a multi-role agent architecture (researcher, questioner, solver, evaluator) to generate, validate, and filter questions, producing trajectories with twice the tool-call diversity of existing datasets. Experiments show that training on this data via supervised fine-tuning yields stronger performance on benchmarks like GAIA and FRAMES compared to models trained on larger but simpler datasets.

## Method Summary
The method synthesizes challenging multi-hop QA data through a two-pronged "ProgSearch" pipeline. First, a top-down approach builds a tree-of-facts from seed entities, generating questions and progressively increasing complexity until a strong solver agent fails. Second, a bottom-up approach starts with a rare entity and iteratively obfuscates questions to reach the solver's failure threshold. A multi-role agent architecture (researcher, questioner, solver, evaluator) handles each synthesis stage, with strict filtering to ensure factuality and eliminate alternative answers. The final dataset is used to train web agents via rejection sampling and supervised fine-tuning on successful trajectories.

## Key Results
- **GAIA accuracy:** 31.4% (vs. 29.5% for Asearcher and 28.8% for Taskcraft)
- **Trajectory length:** 20.43 tool calls on average (twice as long as Asearcher, four times longer than Taskcraft)
- **Tool-call diversity:** Twice the diversity in actions compared to baselines, reducing repetitive behaviors
- **Data efficiency:** Smaller dataset (12K samples) outperforms larger baselines due to higher quality and complexity

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Generating data samples with an adaptive difficulty loop calibrated to a strong agent's failure point is more effective than using static synthesis methods.
- **Mechanism:** The ProgSearch pipeline creates QA pairs by progressively increasing task complexity (adding facts, obfuscating details) until a frontier web agent fails to answer correctly. This calibrates difficulty to the edge of the agent's capabilities, producing samples in an optimal training zone.
- **Core assumption:** The failure threshold of the calibration agent is a reliable proxy for optimal learning difficulty for a student model of similar capability.
- **Evidence anchors:** Abstract states pipeline generates pairs until solver fails; section 3.1 describes the iterative cycle ending at solver's disagreement with ground truth.
- **Break condition:** Reduced benefit if calibration agent is significantly more or less capable than the student model.

### Mechanism 2
- **Claim:** A multi-role agent architecture within the synthesis pipeline enables more rigorous self-correction and quality assurance than a single-purpose generator.
- **Mechanism:** The pipeline uses one baseline agent in multiple distinct roles: researcher (finds facts), questioner (generates questions), solver (attempts answers), and evaluator (checks quality). This creates internal feedback loops and multi-stage filtering.
- **Core assumption:** An LLM can reliably role-play these distinct functions without excessive hallucination, and inconsistencies identified by one role are valid quality signals.
- **Evidence anchors:** Abstract mentions agent playing multiple roles; section 3.3 describes discarding QA pairs when solver's response conflicts with ground truth.
- **Break condition:** Unreliable if LLM's self-evaluation is poor, leading to incorrect discards or acceptance of flawed samples.

### Mechanism 3
- **Claim:** Training agents on rejection-sampled trajectories that are longer and more diverse leads to stronger long-horizon performance than training on shorter trajectories from larger but simpler datasets.
- **Mechanism:** The synthesis pipeline yields data that, after rejection sampling, produces trajectories averaging 20.43 tool calls—roughly twice as long as comparable datasets. SFT on these long, successful trajectories allows student models to learn patterns of persistent, multi-turn reasoning.
- **Core assumption:** The skill of managing long-horizon, multi-turn interactions can be effectively transferred from teacher to student via supervised learning on successful trajectory data.
- **Evidence anchors:** Abstract mentions twice the diversity in tool-use actions; section 4.2.1 shows ProgSearch trajectories include 20.43 tool calls—twice as many as Asearcher and four times more than Taskcraft.
- **Break condition:** Limited benefit if student model has too small a context window or cannot handle the complexity of the reasoning steps.

## Foundational Learning

- **Concept: Rejection Sampling**
  - **Why needed here:** This is the training method used to convert raw QA pairs into trajectory data. A strong agent attempts each question and only keeps trajectories resulting in correct answers.
  - **Quick check question:** If you change the agent used for rejection sampling, how would that affect the final training dataset and the conclusions you could draw from an experiment?

- **Concept: Instruction Tuning / Supervised Fine-Tuning (SFT)**
  - **Why needed here:** The paper uses SFT as the controlled training setup to evaluate the quality of different synthetic datasets. Their dataset is more effective for SFT than larger baselines despite being smaller.
  - **Quick check question:** Why is SFT considered a "controlled" setup in this paper compared to Reinforcement Learning (RL)?

- **Concept: Long-horizon reasoning in Agents**
  - **Why needed here:** This is the core problem the paper aims to solve. It refers to an agent's ability to maintain context, plan, and execute a sequence of dozens of tool calls over many turns to solve a complex goal.
  - **Quick check question:** A standard multi-turn chat and a long-horizon agentic task both involve multiple turns. What is the fundamental difference in the agent's objective and process between the two?

## Architecture Onboarding

- **Component map:**
  1. Seed Corpus (40K seed questions from 2WikiMultihopQA)
  2. Baseline Web Agent (G) with search, browse, python tools acting as Researcher, Questioner, Solver, Evaluator
  3. Tree-of-Facts Builder (recursive DFS expansion from seed entities)
  4. Bottom-Up Hardener (progressive obfuscation anchored to rare entities)
  5. Progressive Difficulty Loops (iterative generation/refinement until solver fails)
  6. Consolidated Filter (factuality checks, single concrete answers, alternative answer filtering)
  7. Rejection Sampler (strong agent generates trajectories, keeps only correct ones)
  8. SFT Trainer (fine-tunes student model on successful trajectories)

- **Critical path:** The data quality hinges on the Progressive Difficulty Loops and the Consolidated Filter. The loops ensure tasks are challenging enough to fail a strong agent, and the filter ensures that failure is due to complexity, not ambiguity or error.

- **Design tradeoffs:**
  - Complexity vs. Clarity: Aggressive obfuscation can make questions unnatural or ambiguous; mitigated by alternative answer filtering step.
  - Data Volume vs. Quality: Smaller dataset of high-quality, long-horizon trajectories is more effective than larger dataset of simpler ones.
  - Role-Specialization vs. Simplicity: Single agent in multiple roles simplifies architecture but relies on LLM's ability to follow role-specific instructions.

- **Failure signatures:**
  - High discard rate: Solver solves questions too easily or evaluator rejects most questions, failing to produce enough data.
  - Low trajectory diversity: Generated trajectories are short or repetitive, preventing learning of long-horizon reasoning.
  - "Alternative Answer" Leaks: Downstream models hallucinate answers, indicating filter for alternative answers is not strict enough.

- **First 3 experiments:**
  1. Baseline Validation: Run synthesis pipeline with fixed seed set and compare average tool-call length against reported ~20.43 to validate pipeline functionality.
  2. Ablation on Filtering: Train two models—one on data with full alternative answer filtering, one without—and evaluate on GAIA to quantify impact of this quality control measure.
  3. Agent Calibration Test: Use two different baseline agents (one stronger, one weaker) for the solver role in difficulty loop and compare resulting datasets and downstream performance to test calibration assumption.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided content.

## Limitations
- Heavy reliance on proprietary, frontier models (specifically `gpt-oss-20b`) for synthesis and rejection sampling creates significant reproducibility barriers.
- The methodology's effectiveness is tied to the specific capabilities of the baseline agent used for difficulty calibration, with no clear guidelines for substitutions.
- The approach requires substantial computational resources for synthesizing high-quality data, though it may be more efficient than training on larger, lower-quality datasets.

## Confidence
- **Core claims (High):** The overall data synthesis methodology and ablation results demonstrating effectiveness of specific filtering mechanisms are well-supported.
- **Performance claims (Medium-High):** Strong benchmark results are tempered by dependence on undisclosed `gpt-oss-20b` model for training final agents.
- **Reproducibility (Low):** Major barriers exist due to reliance on proprietary models and lack of clear substitution guidelines.

## Next Checks
1. **Model Substitution Sensitivity:** Replicate synthesis pipeline using publicly available frontier model (GPT-4o or Qwen2.5-72B-Instruct) as baseline Solver and Teacher. Compare resulting trajectory lengths and tool-call diversity against reported values to quantify impact of model choice.
2. **Alternative Answer Filtering Ablation:** Train two models on same synthesized dataset—one with full alternative answer filtering, one without—and evaluate both on GAIA to directly measure contribution of this quality control mechanism.
3. **Difficulty Calibration Test:** Use two different baseline agents with varying capabilities (one significantly stronger, one significantly weaker than reported `gpt-oss-20b`) in progressive difficulty loop. Compare complexity and quality of resulting datasets and performance of models trained on them to validate calibration assumption.