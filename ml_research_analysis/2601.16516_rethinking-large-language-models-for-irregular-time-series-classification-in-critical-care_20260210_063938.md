---
ver: rpa2
title: Rethinking Large Language Models For Irregular Time Series Classification In
  Critical Care
arxiv_id: '2601.16516'
source_url: https://arxiv.org/abs/2601.16516
tags:
- time
- series
- irregular
- llms
- alignment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the effectiveness of Large Language Models
  (LLMs) for irregular time series classification in critical care. The authors conduct
  a systematic empirical study on two benchmark ICU datasets, PhysioNet 2012 and MIMIC-III,
  to evaluate the impact of encoder design and multimodal alignment strategy on LLM
  performance.
---

# Rethinking Large Language Models For Irregular Time Series Classification In Critical Care

## Quick Facts
- arXiv ID: 2601.16516
- Source URL: https://arxiv.org/abs/2601.16516
- Authors: Feixiang Zheng; Yu Wu; Cecilia Mascolo; Ting Dang
- Reference count: 0
- Primary result: Encoder architecture is more critical than alignment strategy for LLM performance on irregular ICU time series

## Executive Summary
This paper investigates Large Language Models (LLMs) for irregular time series classification in critical care, systematically evaluating encoder design and multimodal alignment strategies on PhysioNet 2012 and MIMIC-III datasets. The authors find that encoders explicitly modeling temporal irregularity (particularly mTAND) outperform regular-time encoders by 12.8% AUPRC, while alignment strategies provide modest 2.9% improvements. Despite comparable performance to supervised models, LLM-based approaches require at least 10× longer training time and underperform in few-shot learning settings, highlighting current limitations for clinical deployment.

## Method Summary
The study employs a two-stage LLM framework where irregular time series are first encoded using one of four architectures (1DCNN, Decomposition, Transformer, or mTAND), then aligned with textual representations via four strategies (Reprogramming, S²IP, Cross-modal, Graph-based). The framework is evaluated on binary mortality classification tasks using PhysioNet 2012 (11,988 samples, 85.7% missing) and MIMIC-III (24,681 samples, 96.7% missing), with semi-synthetic irregular MIT-BIH ECG for controlled missingness experiments. Performance is measured via AUPRC and AUROC, with training time serving as efficiency proxy. Implementation code is available at https://github.com/mHealthUnimelb/LLMTS.

## Key Results
- Encoders explicitly modeling irregularity achieve 12.8% AUPRC improvement over vanilla Transformer
- Best alignment strategy (S²IP) provides 2.9% AUPRC improvement over cross-attention
- LLM-based methods require 10× longer training than supervised models while delivering comparable performance
- LLMs underperform in few-shot learning settings with only 10% of training data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Encoders that explicitly model temporal irregularity outperform regular-time encoders on ICU data with high missingness.
- Mechanism: mTAND uses continuous-time encoding and multi-timescale attention to represent irregular intervals directly, rather than assuming uniform spacing. This prevents misleading temporal biases that arise when regular-time encoders (1D CNN, vanilla Transformer) interpolate or aggregate away timing information.
- Core assumption: The exact timing and gaps between observations carry predictive signal, not just the values themselves.
- Evidence anchors:
  - [abstract] "Encoders that explicitly model irregularity achieve substantial performance gains, yielding an average AUPRC increase of 12.8% over the vanilla Transformer."
  - [section 4.2] "mTAND outperforms other encoders, owing to its explicitly incorporating of irregular-aware time embeddings... 1D CNN, decomposition encoder, and Transformer encoder assume uniform temporal spacing between observations."
  - [corpus] Related work on IMTS forecasting (arXiv:2505.11250) similarly emphasizes explicit irregularity modeling as critical.
- Break condition: If irregularity is primarily noise rather than signal (e.g., random measurement timing uncorrelated with patient state), explicit modeling provides no benefit.

### Mechanism 2
- Claim: Semantic anchor retrieval aligns time series to LLM embeddings more effectively than cross-attention.
- Mechanism: S²IP retrieves top-K semantic anchors from pretrained word embeddings as prefix prompts. This grounds time series features in semantically relevant textual concepts, whereas cross-attention distributes focus across multiple keys, potentially producing diffuse representations lacking semantic focus.
- Core assumption: Pretrained word embeddings contain semantic structure relevant to clinical time series patterns.
- Evidence anchors:
  - [abstract] "The best-performing semantically rich, fusion-based strategy achieving a modest 2.9% improvement over cross-attention."
  - [section 4.2] "S²IP achieves the best performance, benefiting from semantic anchor retrieval... cross-attention-based approaches distribute attention across multiple keys, which may lead to overly diffuse or redundant representations."
  - [corpus] No direct corpus evidence on anchor retrieval specifically; related multimodal LLM time series work (arXiv:2502.01477) focuses on task reformulation rather than alignment mechanisms.
- Break condition: If time series patterns do not map to semantic concepts in the pretrained embedding space, anchor retrieval becomes noise.

### Mechanism 3
- Claim: Global attention over full sequences (without patching) is more robust to high missingness rates than local patch-based representations.
- Mechanism: CALF captures coarse-grained temporal correlations across the full sequence rather than segmenting into patches. When missingness increases, patch-based methods leave patches with insufficient valid data; global attention can still exploit remaining correlations.
- Core assumption: Useful predictive signal exists at global, coarse-grained temporal scales even when local segments are sparse.
- Evidence anchors:
  - [section 4.1] "With the missing ratio increasing from 10% to 90%, Time-LLM, S²IP and FSCA drop by 38%, 42%, and 40%... CALF exhibits a more gradual performance drop, as it does not apply patching and instead captures global representation."
  - [abstract] Not directly stated; mechanism inferred from experimental findings.
  - [corpus] Related work on CDE autoencoders for sepsis (arXiv:2506.15019) similarly uses continuous dynamics for stable representations under irregularity.
- Break condition: If task requires fine-grained local pattern detection (e.g., arrhythmia detection in ECG), global attention may smooth away critical details.

## Foundational Learning

- Concept: **Irregular time series vs. missing data**
  - Why needed here: The paper distinguishes irregularity (variable sampling intervals) from simple missingness. ICU data has both: measurements occur at non-uniform times, and some timepoints have no observations.
  - Quick check question: Can you explain why imputing irregular data to regular intervals might discard predictive signal?

- Concept: **AUPRC vs. AUROC for imbalanced data**
  - Why needed here: ICU mortality is a rare event (class imbalance). The paper prioritizes AUPRC, which is more informative than AUROC when positives are scarce.
  - Quick check question: Why might a model with high AUROC still have poor clinical utility for mortality prediction?

- Concept: **Multimodal alignment strategies**
  - Why needed here: The paper tests how to bridge numerical time series and LLM textual representations. Understanding reprogramming vs. cross-attention vs. semantic anchoring is essential for interpreting results.
  - Quick check question: What is the fundamental problem each alignment strategy tries to solve?

## Architecture Onboarding

- Component map: Time series → Encoder (1DCNN/Decomposition/Transformer/mTAND) → Alignment (Reprogramming/S²IP/Cross-modal/Graph-based) → LLM Backbone → Classification Head

- Critical path:
  1. Segment irregular time series into patches (if using patch-based encoder)
  2. Encode with irregularity-aware encoder (mTAND recommended)
  3. Align temporal embeddings with textual embeddings via semantic anchors (S²IP recommended)
  4. Pass through LLM backbone
  5. Classification head outputs predictions

- Design tradeoffs:
  - Encoder complexity vs. irregularity handling: mTAND is more complex but critical for high-missingness data
  - Alignment sophistication vs. marginal gain: S²IP best but only ~3% improvement over simpler cross-attention
  - LLM vs. supervised models: LLMs add 10× training time for comparable performance

- Failure signatures:
  - Rapid performance drop as missingness increases → patching mechanism failing (switch to global attention or reduce patch size)
  - Graph-based alignment underperforms → sparse/noisy data making node representations unreliable
  - Few-shot setting underperforms → LLM overfitting to limited data; prefer smaller domain-specific models

- First 3 experiments:
  1. **Encoder ablation**: Replace default encoder with mTAND on your ICU dataset; measure AUPRC change. Expect ~10-15% improvement if irregularity is high.
  2. **Missingness robustness test**: Synthetic dropout experiment (0% to 90% missingness) comparing patch-based vs. global attention architectures. Identify where performance degrades sharply.
  3. **Training time benchmark**: Compare mTAND+S²IP vs. Warpformer on your dataset; quantify AUPRC gain per training hour to assess practical utility.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the computational overhead of LLM-based frameworks be reduced to offer a practical advantage over lightweight supervised models for irregular time series?
- **Basis in paper:** [explicit] The paper concludes there is a "need for more efficient architecture that balances performance improvements and computational costs" after finding LLMs require "at least 10× longer training" for comparable performance.
- **Why unresolved:** The study demonstrates that current LLM methods are computationally prohibitive (e.g., taking 24 hours vs. 1.5 hours for baselines) without delivering significant predictive gains, leaving the trade-off unfavorable.
- **What evidence would resolve it:** Development of an LLM-based method that achieves SOTA performance on PhysioNet/MIMIC-III with a training time within 2× of supervised baselines like Warpformer.

### Open Question 2
- **Question:** Can specific pre-training or adaptation strategies be developed to unlock effective few-shot learning for LLMs on irregular clinical data?
- **Basis in paper:** [explicit] The authors note that their results regarding poor few-shot performance "suggests that a current limitation in adapting LLMs to irregular time series tasks," contrary to the typical few-shot strength of LLMs in other domains.
- **Why unresolved:** The paper empirically shows that LLMs underperform compared to supervised models (Warpformer) when only 10% of data is used, indicating that standard LLM pre-training does not currently transfer well to sparse, irregular clinical signals.
- **What evidence would resolve it:** Identifying a training regime where an LLM-based model significantly outperforms Warpformer in the 10% data-scarce setting on the MIMIC-III dataset.

### Open Question 3
- **Question:** Can multimodal alignment strategies be designed to have a greater impact on performance than the time series encoder?
- **Basis in paper:** [inferred] The authors observe that alignment strategies currently provide only a "modest 2.9% improvement" compared to the encoder's 12.8% contribution, suggesting "need for further development in customized alignment."
- **Why unresolved:** Current alignment techniques (e.g., cross-attention, semantic anchors) fail to bridge the modality gap as effectively as simply replacing the encoder with an irregularity-aware one.
- **What evidence would resolve it:** Demonstration of a novel alignment technique that yields >10% AUPRC improvement while keeping the encoder fixed, surpassing the gains achieved by encoder swapping.

### Open Question 4
- **Question:** Does the random dropping mechanism used to simulate irregularity in benchmarks fail to capture the informative missingness present in real-world clinical settings?
- **Basis in paper:** [inferred] The authors utilize a semi-synthetic dataset where they "randomly drop data points" to test irregularity, acknowledging this is a simulation because "no such real-world data exist."
- **Why unresolved:** Random missingness is Missing Completely At Random (MCAR), whereas clinical data is often Missing Not At Random (MNAR); the paper's findings on LLM robustness might not hold if the missingness pattern itself carries diagnostic information.
- **What evidence would resolve it:** A comparative analysis showing LLM performance trends diverge between random dropping and MNAR-based missingness simulations on the same underlying dataset.

## Limitations
- The paper relies on random missingness simulation rather than real-world MNAR clinical patterns, potentially limiting generalizability to actual clinical scenarios
- Unspecified LLM backbone architecture and exact hyperparameters create uncertainty about reproducibility and optimal configuration
- 10× training time overhead compared to supervised models remains a significant practical barrier for clinical deployment

## Confidence
- **High confidence**: Encoder architecture impact (mTAND outperforms vanilla Transformer by 12.8% AUPRC) - supported by multiple ablation experiments and clear mechanistic explanation
- **Medium confidence**: Alignment strategy ranking (S²IP > cross-attention by 2.9% AUPRC) - effect is smaller and mechanism less well-established in literature
- **Low confidence**: LLM efficiency claims (10× training time) - depends heavily on unspecified hardware, batch sizes, and exact model configurations

## Next Checks
1. **Encoder architecture ablation**: Systematically vary encoder depth and width parameters for mTAND and vanilla Transformer on PhysioNet, isolating architectural complexity effects from irregularity modeling
2. **Semantic alignment robustness**: Replace S²IP's pretrained embeddings with randomly initialized embeddings and alternative clinical ontologies to test whether semantic anchoring requires domain-specific pretraining
3. **Efficiency benchmark replication**: Replicate training time measurements on identical hardware (A100 80GB) using the same batch sizes and learning rate schedules as the original experiments