---
ver: rpa2
title: 'MSTS: A Multimodal Safety Test Suite for Vision-Language Models'
arxiv_id: '2501.10057'
source_url: https://arxiv.org/abs/2501.10057
tags:
- safety
- prompts
- test
- msts
- unsafe
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MSTS, a Multimodal Safety Test Suite for
  Vision-Language Models (VLMs), addressing the growing safety risks as VLMs become
  integrated into consumer AI applications. MSTS comprises 400 test prompts across
  40 fine-grained hazard categories, where each prompt combines a text and an image
  to reveal unsafe meaning only when both are considered together.
---

# MSTS: A Multimodal Safety Test Suite for Vision-Language Models

## Quick Facts
- arXiv ID: 2501.10057
- Source URL: https://arxiv.org/abs/2501.10057
- Reference count: 11
- MSTS comprises 400 test prompts across 40 hazard categories, revealing safety differences between commercial and open vision-language models

## Executive Summary
This paper introduces MSTS, a Multimodal Safety Test Suite designed to evaluate the safety of Vision-Language Models (VLMs) by testing their responses to 400 prompts across 40 fine-grained hazard categories. Each prompt combines text and images to reveal unsafe meanings only when both modalities are considered. The study tests ten state-of-the-art VLMs and finds significant safety performance differences, with commercial models like GPT-4o and Claude-3.5 demonstrating strong safety (fewer than 0.5% unsafe responses), while some open models like xGen-MM show concerning safety issues (14.0% unsafe responses). The research also reveals that open models often appear safe due to misunderstanding prompts rather than robust safety mechanisms, and that safety performance varies across languages, with non-English prompts showing weaker safety in some models.

## Method Summary
The authors developed MSTS by first creating a taxonomy of multimodal hazards and then designing 400 test prompts across 40 fine-grained hazard categories. Each prompt combines a text and an image, where the unsafe meaning is only revealed when both are considered together. The prompts were evaluated by 10 state-of-the-art VLMs, including both commercial models (GPT-4o, Claude-3.5) and open models (xGen-MM, Qwen-2-VL). Safety responses were classified as safe or unsafe based on whether the model provided harmful content or refused to engage with the unsafe prompt. The study also tested cross-language safety and explored automated safety assessment using VLM classifiers.

## Key Results
- Commercial models (GPT-4o, Claude-3.5) showed very strong safety performance with fewer than 0.5% unsafe responses
- Open models demonstrated significant safety disparities, with xGen-MM responding unsafely to 14.0% of prompts
- Safety performance varied across languages, with models like MiniCPM-2.6 being less safe for non-English prompts
- Automated safety classifiers performed poorly, with Gemini-1.5 achieving only 53% precision in classifying unsafe responses

## Why This Works (Mechanism)
The MSTS approach works by creating multimodal prompts where the unsafe meaning is only revealed when both text and image components are considered together. This design prevents models from relying solely on text-based safety filters and forces them to engage with the full semantic content. By testing across 40 fine-grained hazard categories, the suite can identify specific types of safety failures rather than just overall safety scores. The cross-language testing reveals that safety mechanisms may not generalize well across linguistic contexts, while the comparison between commercial and open models highlights differences in safety engineering approaches.

## Foundational Learning
- Multimodal hazards taxonomy: Understanding the 40 fine-grained hazard categories is essential for interpreting safety results and identifying specific failure modes
- Prompt design methodology: Learning how text-image combinations create unsafe meanings only when combined helps in understanding why some models fail while others succeed
- Safety evaluation metrics: Understanding the classification of safe vs unsafe responses is crucial for interpreting the 0.5% vs 14.0% safety performance differences

## Architecture Onboarding
Component map: Image processing module -> Text processing module -> Multimodal fusion layer -> Safety assessment module -> Output generation
Critical path: Input image/text → Feature extraction → Multimodal reasoning → Safety check → Response generation
Design tradeoffs: The study reveals a fundamental tradeoff between model openness/accessibility and safety robustness, with commercial models prioritizing safety through stronger filters while open models offer more flexibility but weaker safety
Failure signatures: Open models failing due to prompt misunderstanding rather than robust safety mechanisms; cross-language safety degradation indicating incomplete multilingual safety training
First experiments: (1) Replicate safety testing with additional commercial models to confirm 0.5% safety threshold, (2) Test open models with adversarial prompt engineering to determine if "safety by accident" persists, (3) Evaluate safety performance across additional languages beyond the initial test set

## Open Questions the Paper Calls Out
The paper does not explicitly call out additional open questions beyond those discussed in the limitations section.

## Limitations
- The finding that open models are "safe by accident" due to prompt misunderstanding raises concerns about whether these models would remain safe under more sophisticated adversarial inputs
- Cross-language safety findings suggest non-English safety could be substantially weaker, but the study's scope across different languages and cultural contexts remains unclear
- Automated safety classifiers perform poorly (53% precision), indicating current evaluation methods may not be reliable for real-world deployment, though the reasons for this limitation are not fully explored

## Confidence
- High: Core safety performance comparisons between commercial and open models, as methodology appears systematic and results are consistent across multiple models
- Medium: Cross-language safety findings, as the study identifies disparities but depth of linguistic and cultural testing is not fully detailed
- Low: Automated safety assessment results, given poor performance metrics and lack of exploration into why classifiers fail

## Next Checks
- Test the same MSTS prompts against commercial models using adversarial prompt engineering to determine if "safety by understanding" is more robust than "safety by accident"
- Expand cross-language testing to include more diverse linguistic and cultural contexts to better understand safety disparities
- Investigate the failure modes of automated safety classifiers by analyzing specific examples of misclassification to improve future evaluation methodologies