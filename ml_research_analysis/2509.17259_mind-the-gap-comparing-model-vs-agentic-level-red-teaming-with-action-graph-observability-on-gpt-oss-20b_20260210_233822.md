---
ver: rpa2
title: 'Mind the Gap: Comparing Model- vs Agentic-Level Red Teaming with Action-Graph
  Observability on GPT-OSS-20B'
arxiv_id: '2509.17259'
source_url: https://arxiv.org/abs/2509.17259
tags:
- agentic
- attack
- iterative
- actions
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compares model-level and agentic-level security vulnerabilities
  in GPT-OSS-20B using an observability framework called AgentSeer to deconstruct
  agentic execution into granular actions. The researchers conducted iterative red
  teaming attacks with 38 harmful objectives from HarmBench at both standalone model
  and agentic deployment levels.
---

# Mind the Gap: Comparing Model- vs Agentic-Level Red Teaming with Action-Graph Observability on GPT-OSS-20B

## Quick Facts
- arXiv ID: 2509.17259
- Source URL: https://arxiv.org/abs/2509.17259
- Reference count: 29
- Agentic execution contexts introduce vulnerabilities absent in standalone model evaluation

## Executive Summary
This study reveals that agentic AI systems face unique security vulnerabilities that don't exist at the model level. Using the AgentSeer observability framework on GPT-OSS-20B deployed in a 6-agent hierarchical system, researchers found that iterative attacks succeeded against 39.47% of harmful objectives at the model level but dropped significantly in agentic contexts. Critically, they discovered "agentic-only" vulnerabilities that emerge exclusively within agentic execution, with tool-calling scenarios showing 24% higher vulnerability than non-tool contexts. The research demonstrates that standalone model security testing fails to capture the full risk landscape of deployed agentic systems.

## Method Summary
The researchers developed AgentSeer, an observability framework that deconstructs agentic execution into granular actions using MLFlow for trace capture and knowledge graphs for visualization. They conducted iterative red teaming attacks with 38 harmful objectives from HarmBench at both standalone model and agentic deployment levels. The study compared attack success rates across different injection points (human, AI, and tool messages) and analyzed vulnerability patterns across various tool contexts. Agentic-level attacks used context-aware iterative refinement with GPT-4o-mini as the attacker model.

## Key Results
- Model-level iterative attacks achieved 39.47% ASR versus 40-57% ASR in agentic contexts
- Human message injection proved most effective (57% ASR) compared to tool message injection (40% ASR)
- Tool-calling scenarios exhibited 24% higher vulnerability than non-tool actions
- 15 objectives failed at model level but succeeded in agentic context, confirming "agentic-only" vulnerabilities

## Why This Works (Mechanism)

### Mechanism 1: Agentic Context Creates Emergent Attack Surfaces
Agentic execution contexts introduce vulnerabilities that don't exist at the model level, creating "agentic-only" attack vectors. The agentic loop with tools, memory, and multi-step reasoning creates new attack vectors through tool-calling interfaces, persistent memory states, inter-agent communications in hierarchical systems, and sequential execution that compounds vulnerabilities across actions.

### Mechanism 2: Injection Point Asymmetry Determines Attack Success
Different injection points in agentic execution show dramatically different vulnerability levels based on message type and tool context. Attack effectiveness varies by message authority level (human: 57% ASR vs AI: 42% ASR vs tool: 40% ASR), specific action context (13-87% ASR range), and tool type being invoked (agent transfer: 67% ASR vs knowledge retrieval: 27% ASR).

### Mechanism 3: Semantic Context Drives Vulnerability, Not Context Length
Vulnerability patterns correlate with semantic content and tool interactions, not with input token length. Input token length (2,000-5,500 tokens) shows no linear relationship with ASR, while specific semantic patterns in tool interactions and memory states drive vulnerability. Social engineering approaches (roleplay: 60%, authority: 40%) remain most effective while logic-based fail entirely (0%).

## Foundational Learning

- **Agentic Architecture Components** (agents, tools, memory systems)
  - Why needed: Understanding how hierarchical multi-agent systems decompose into actions and components is prerequisite to analyzing where vulnerabilities emerge
  - Quick check: In a 6-agent hierarchical system with tools and memory, can you identify at least 3 distinct component types and explain how information flows between them?

- **Iterative Red Teaming / PAIR Methodology**
  - Why needed: The paper uses iterative refinement as the primary attack methodology, requiring understanding of how prompts are progressively refined based on model responses
  - Quick check: How does iterative refinement (4-5 iterations with attacker model feedback) differ from single-shot prompt attacks in discovering vulnerabilities?

- **Attack Success Rate (ASR) and Stability Metrics**
  - Why needed: The paper relies on ASR comparisons and ASR@K stability metrics to establish that vulnerabilities are context-dependent and often unstable
  - Quick check: What does a 50-80% ASR degradation on reinjection indicate about the nature of discovered vulnerabilities?

## Architecture Onboarding

- **Component map**: AgentSeer → MLFlow (execution trace capture via spans) → Knowledge Graph (JSON schema: actions, components, edges) → ReactFlow (web visualization)
- **Target**: GPT-OSS-20B (low reasoning effort config)
- **Testbed**: LangGraph 6-agent hierarchical system (1 main + 1 manager + 4 specialists)
- **Dataset**: HarmBench (38 filtered objectives from 50 random)
- **Judge**: GPT-4o-mini (StrongREJECT-inspired evaluation)

- **Critical path**:
  1. Execute baseline agentic queries → capture 29 actions across 4 queries in MLFlow
  2. Process spans → extract actions (LLM calls) and components (agents, tools, memory)
  3. Build knowledge graph with directed edges for information flow
  4. Run model-level iterative attacks (4 iterations) → establish 39.47% baseline ASR
  5. Transfer successful prompts to agentic context → test human/AI/tool injection (40-57% ASR)
  6. Perform agentic-level iterative attacks (5 iterations) → discover agentic-only vulnerabilities
  7. Reinject successful attacks (ASR@K=1,3,5) → verify 50-80% stability degradation

- **Design tradeoffs**:
  - LangGraph dependency limits framework-agnostic applicability
  - Single use case (Shopify sales analyst) limits generalizability to other domains
  - 38 objectives may not capture full vulnerability spectrum
  - PAIR methodology represents only one attack class
  - Low reasoning effort config may not represent production deployments with stronger reasoning

- **Failure signatures**:
  - Transfer failure: model-level attack succeeds (39.47% ASR) but degrades significantly in agentic context
  - Agentic-only detection: objectives with 0% model-level ASR show >0% agentic-level ASR
  - Instability pattern: original iterative ASR >> ASR@K=1 (e.g., action_16: 67% → 20%)
  - Tool risk hierarchy: agent transfer (67%) > code execution (51%) > knowledge retrieval (27%)

- **First 3 experiments**:
  1. Replicate model-level baseline: Run 4-iteration PAIR on GPT-OSS-20B with filtered HarmBench, verify ~40% ASR with roleplay dominance
  2. Test injection asymmetry: Inject successful prompts into actions using human/AI/tool message types, confirm 40-57% ASR range and identify highest-risk actions (target: action_14, action_5, action_7)
  3. Validate agentic-only vulnerabilities: Select 15 failed model-level objectives, apply context-aware iterative attacks to tool-calling actions, confirm objectives that succeed only in agentic context and 24% higher tool-context vulnerability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the identified vulnerability hierarchies—specifically the high risk of agent transfer operations—generalize to domains outside of sales analytics, such as software development or healthcare?
- Basis in paper: The authors acknowledge the study focuses on a single "Shopify sales analyst scenario" and invite "expanding evaluation across diverse agentic use cases."
- Why unresolved: It is unclear if the 67% ASR for agent transfers is an artifact of the specific domain's tools or a systemic agentic flaw.
- What evidence would resolve it: Replicating the red teaming methodology on agents designed for distinct high-stakes domains.

### Open Question 2
- Question: What defense mechanisms can effectively mitigate "agentic-only" vulnerabilities without degrading the functional utility of tool-calling capabilities?
- Basis in paper: Section 6 states the work "does not extensively explore potential defense mechanisms" despite identifying these novel attack vectors.
- Why unresolved: The study prioritized vulnerability discovery and classification over developing remediation strategies.
- What evidence would resolve it: Benchmarking specific defensive interventions (e.g., context sanitization, permission controls) against the successful agentic-level attacks.

### Open Question 3
- Question: How does the reliance on specific technology stacks (LangGraph, MLFlow) impact the observability and vulnerability patterns observed via the AgentSeer framework?
- Basis in paper: The limitations section highlights "AgentSeer Framework Dependencies" and calls for "framework-agnostic observability approaches."
- Why unresolved: The observed attack surfaces may be partially determined by the specific implementation details of the chosen libraries.
- What evidence would resolve it: A comparative study implementing the observability layer on alternative agent frameworks (e.g., AutoGen, CrewAI).

### Open Question 4
- Question: How do vulnerability profiles evolve during extended, dynamic execution sessions compared to the static injection points analyzed?
- Basis in paper: The paper notes it "does not investigate how vulnerabilities evolve across extended agentic sessions or how they interact with dynamic context changes."
- Why unresolved: Attacks were tested at specific, isolated actions rather than over continuous, time-dependent operational flows.
- What evidence would resolve it: Longitudinal red teaming measuring Attack Success Rate (ASR) fluctuations across uninterrupted multi-step workflows.

## Limitations

- Single agentic architecture (LangGraph) and use case (Shopify sales analyst) limit generalizability
- Only 38 filtered objectives from HarmBench may not represent full vulnerability spectrum
- PAIR methodology represents just one attack class, potentially missing other vulnerability patterns
- Low reasoning effort configuration may not reflect production deployments with stronger reasoning capabilities

## Confidence

- **High confidence**: The existence of "agentic-only" vulnerabilities and that standalone model testing fails to capture full agentic risk
- **Medium confidence**: Specific ASR percentages and injection point effectiveness rankings due to single model and use case
- **Medium confidence**: Semantic context hypothesis over context length, though this represents a novel finding requiring further validation

## Next Checks

1. Replicate findings across diverse agentic architectures (different frameworks, agent counts, and memory configurations) to establish generalizability
2. Test vulnerability stability over time and across model versions to understand persistence and evolution of discovered attack vectors
3. Evaluate defensive interventions (context-aware filtering, tool confirmation requirements) to determine practical mitigation strategies for identified injection point asymmetries