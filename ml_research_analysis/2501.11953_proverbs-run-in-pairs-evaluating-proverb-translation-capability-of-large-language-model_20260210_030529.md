---
ver: rpa2
title: 'Proverbs Run in Pairs: Evaluating Proverb Translation Capability of Large
  Language Model'
arxiv_id: '2501.11953'
source_url: https://arxiv.org/abs/2501.11953
tags:
- translation
- proverb
- proverbs
- language
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates how well current machine translation (MT)
  systems, including both NMT and LLM-based models, can translate proverbs, which
  are culturally rooted expressions. The authors construct two datasets: one with
  standalone proverbs and another with proverbs used in conversations across four
  language pairs.'
---

# Proverbs Run in Pairs: Evaluating Proverb Translation Capability of Large Language Model

## Quick Facts
- arXiv ID: 2501.11953
- Source URL: https://arxiv.org/abs/2501.11953
- Authors: Minghan Wang; Viet-Thanh Pham; Farhad Moghimifar; Thuy-Trang Vu
- Reference count: 27
- Primary result: LLMs generally outperform NMT models in proverb translation, especially between languages from similar cultural backgrounds.

## Executive Summary
This paper investigates how well current machine translation systems can translate proverbs, which are culturally rooted expressions. The authors construct two datasets: one with standalone proverbs and another with proverbs used in conversations across four language pairs. They evaluate multiple models using automatic metrics (BLEU, CHRF++, COMET) and LLM-as-a-judge. Results show that LLMs generally outperform NMT models in proverb translation, especially between languages from similar cultural backgrounds. The study also reveals that existing automatic evaluation metrics are inadequate for assessing the quality of proverb translation, as they are overly sensitive to lexical differences and fail to capture semantic equivalence.

## Method Summary
The authors construct two proverb datasets: MAPS (standalone proverbs) and Proverb-in-Conversation (PiC) mined from OpenSubtitles with filtering for quality and usage validation. They evaluate NMT models (NLLB variants), instruction LLMs (Mistral, Qwen2, Gemma2, Llama-3.1, GPT-4o-mini), and a fine-tuned MT-LLM (ALMA-R) across four language pairs. Multiple prompting strategies are tested, including zero-shot, one-shot, proverb-explanation, context-via-dialogue, and context-via-concatenation. Evaluation uses automatic metrics (BLEU, CHRF++, COMET), LLM-as-a-judge (GPT-4o-mini), and data contamination analysis.

## Key Results
- LLMs generally outperform NMT models in proverb translation, especially between languages from similar cultural backgrounds.
- Incorporating conversational context significantly enhances translation performance, with dialogue format proving more effective than simple concatenation.
- Current automatic evaluation metrics such as BLEU, CHRF++, and COMET are inadequate for reliably assessing the quality of proverb translation.

## Why This Works (Mechanism)

### Mechanism 1: Cultural Proximity Advantage
- Claim: MT systems perform better on proverb translation between languages sharing cultural backgrounds.
- Mechanism: When source and target languages originate from related cultural regions (e.g., Western Europe), proverbs are more likely to have direct functional equivalents or shared etymological roots, reducing the need for cultural adaptation.
- Core assumption: Cultural proximity correlates with proverb translatability—proverbs between culturally distant pairs require more paraphrasing or cultural substitution.
- Evidence anchors:
  - [abstract] "LLMs generally outperform NMT models in proverb translation, especially between languages from similar cultural backgrounds"
  - [Section 4.1] "All models perform reasonably well in DE-EN pairs because both languages are high-resource and belong to similar cultural regions"
  - [corpus] MasalBench paper confirms cross-cultural proverb understanding remains challenging for LLMs in low-resource contexts
- Break condition: Cultural proximity advantage may diminish when proverbs are figurative (non-literal meaning diverges from wording) rather than literal, or when target culture lacks equivalent proverb entirely.

### Mechanism 2: Contextual Grounding via Dialogue History
- Claim: Providing conversational context in dialogue format improves proverb translation more than providing proverb explanations.
- Mechanism: Larger LLMs (70B+) have already encoded proverb meanings during pretraining; adding explicit explanations provides no marginal benefit. However, dialogue context offers situational cues that disambiguate how a proverb is being used pragmatically, improving translation decisions.
- Core assumption: Pretraining corpora contain sufficient proverb definitions; the bottleneck is pragmatic context, not semantic knowledge.
- Evidence anchors:
  - [Section 4.2] "we do not observe any notable improvement when providing explanations of proverbs... LLMs have already exposed to the meanings of proverbs during their pre-training"
  - [Section 4.2] "incorporating conversational context significantly enhances translation performance... framing the context as a dialogue proves to be more effective than simple concatenation"
  - [corpus] Culturally-Grounded Chain-of-Thought paper shows combining cultural context with reasoning improves culturally-specific tasks
- Break condition: Dialogue context may introduce noise when subtitle reordering misaligns source-target pairs; concatenation approach was designed to handle this but underperformed, suggesting the mechanism depends on context quality.

### Mechanism 3: Evaluation Metric Breakdown for Semantic Equivalence
- Claim: Standard MT metrics (BLEU, CHRF++, COMET) systematically undervalue valid proverb translations that paraphrase rather than match reference lexically.
- Mechanism: Proverb translation often requires conveying the same wisdom using different lexical items (e.g., "rat" vs. "mouse," "discipline brings forth filial children" vs. "spare the rod and spoil the child"). Metrics relying on n-gram overlap or learned similarity to a single reference penalize legitimate semantic equivalents.
- Core assumption: Proverb translation quality should be measured by meaning preservation, not lexical overlap.
- Evidence anchors:
  - [abstract] "current automatic evaluation metrics such as BLEU, CHRF++ and COMET are inadequate for reliably assessing the quality of proverb translation"
  - [Section 5.1] Table 6 examples: "Spare the rod and spoil the child" → "Discipline brings forth filial children" scores BLEU=0.0, COMET=60.10 despite semantic equivalence
  - [corpus] MTQE Rewards for Idioms paper directly addresses improving evaluation for non-compositional expressions
- Break condition: COMET performs better than BLEU/CHRF++ but still struggles with metaphorical rephrasing; no metric tested captures cultural appropriateness adequately.

## Foundational Learning

- **Non-compositional expressions (idioms, proverbs, metaphors)**
  - Why needed here: Proverbs are multi-word expressions where meaning cannot be derived from individual words. Understanding this is prerequisite to grasping why standard MT and evaluation fail.
  - Quick check question: Can you explain why translating "kick the bucket" word-by-word fails, and how this differs from translating "the cat sat on the mat"?

- **Automatic MT evaluation metrics (BLEU, CHRF++, COMET)**
  - Why needed here: The paper's central claim is that these metrics are inadequate for proverb evaluation. You must understand what each metric measures (lexical overlap vs. learned semantic similarity) to interpret the failure analysis.
  - Quick check question: Why would COMET (a neural metric) still fail to recognize "Discipline brings forth filial children" as equivalent to "Spare the rod and spoil the child"?

- **LLM-as-a-judge paradigm**
  - Why needed here: The paper tests whether GPT-4o-mini can reliably judge proverb translation quality as an alternative to automatic metrics. Understanding this paradigm helps interpret why it partially succeeds but doesn't fully solve the problem.
  - Quick check question: What are the potential failure modes when using an LLM to evaluate another LLM's culturally-grounded outputs?

## Architecture Onboarding

- **Component map:**
  - Dataset layer (MAPS + PiC) -> Model layer (NMT + LLMs + MT-LLM) -> Prompt layer (5 strategies) -> Evaluation layer (BLEU/CHRF++/COMET + LLM-as-a-judge + contamination analysis)

- **Critical path:**
  1. Dataset quality depends on filtering pipeline (Section 2.2.2)—LLM-based proverb usage check + quality estimation scores
  2. Prompt selection: For PiC tasks, dialogue-context prompts are default; for standalone, zero-shot or one-shot suffices for larger models
  3. Evaluation interpretation: Do NOT rely solely on automatic metrics; qualitative human inspection is required for proverb translation claims

- **Design tradeoffs:**
  - **Dataset scale vs. quality**: Strict filtering limited PiC dataset size (Table 2 shows heavy dropout from P1→P3); authors acknowledge this limits statistical power
  - **Context format**: Dialogue format better for LLM alignment but vulnerable to subtitle reordering noise; concatenation handles reordering but underperforms
  - **Model selection**: Multilingual support varies—Mistral and ALMA-R struggle on Bengali/Indonesian due to limited training coverage; NLLB-3.3B competitive on literal proverbs despite being smaller

- **Failure signatures:**
  - Low BLEU + high semantic similarity (manual inspection): Likely valid paraphrase being penalized
  - High COMET gap between literal and figurative proverbs: Model struggling with non-compositional meaning
  - Performance drops on BN-EN / EN-BN pairs: Low-resource + cultural distance compound difficulty
  - One-shot outperforms context prompts: Check for context noise/misalignment

- **First 3 experiments:**
  1. **Baseline replication**: Run NLLB-3.3B and Llama-3.1-70B on standalone proverb dataset with zero-shot and dialogue-context prompts; verify DE-EN >> BN-EN performance gap
  2. **Metric failure analysis**: Manually inspect 20 cases where semantic similarity is high but BLEU/COMET disagree; categorize failure types (lexical variation, metaphorical rephrasing, cultural substitution)
  3. **Contamination check**: Compute γ contamination rate for your target LLM on PiC dataset following Section 5.3 protocol; if γ > 0.9 for >5% samples, flag for potential memorization bias

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can automatic evaluation metrics be redesigned to reliably capture semantic equivalence and cultural nuance in proverb translation without being overly sensitive to lexical differences?
- Basis: [explicit] The authors explicitly conclude that current metrics like BLEU, CHRF++, and COMET are "inadequate for reliably assessing the quality of proverb translation" and fail to recognize valid paraphrasing.
- Why unresolved: The paper demonstrates that current metrics penalize hypothesis translations that convey the correct meaning but use different lexical choices (e.g., "rat" vs. "mouse") or significant rephrasing.
- What evidence would resolve it: The development of a new metric that correlates strongly with human judgment on figurative language test sets, specifically rewarding functional/semantic equivalence over n-gram overlap.

### Open Question 2
- Question: Can the "LLM-as-a-judge" evaluation framework be refined to overcome its current limitations and provide a consistent, bias-free assessment of translation quality?
- Basis: [explicit] The authors note in Section 5.2 that while LLM-as-a-judge follows general trends, it "cannot fully solve the limitations of traditional evaluation methods" and shows inconsistent correlation with performance gaps.
- Why unresolved: The study found that a larger difference in COMET scores did not necessarily correlate with a higher win rate in LLM-as-judge comparisons, suggesting the judge is sensitive to factors other than pure translation quality.
- What evidence would resolve it: A prompting strategy or model architecture that yields a monotonic relationship between the judge's preference and established quality differentials, validated against human annotators.

### Open Question 3
- Question: To what extent does the limited scale and variety of the current proverb datasets impact the generalizability of the finding that LLMs outperform NMT models?
- Basis: [explicit] In the "Limitation" section, the authors acknowledge the "scale of our dataset is currently relatively small" and that strict filtering led to "excessive discarding" of samples, limiting evaluation comprehensiveness.
- Why unresolved: The small sample size (e.g., only 42 samples for English-to-Bengali PiC) makes it difficult to determine if the observed performance gaps are statistically robust across broader contexts.
- What evidence would resolve it: Experiments run on a significantly expanded dataset with diverse sources, showing consistent performance gaps between LLMs and NMT models across thousands of samples.

## Limitations
- The PiC dataset filtering pipeline significantly reduced usable samples, limiting statistical power and generalizability.
- Evaluation relies heavily on GPT-4o-mini as an LLM-as-a-judge, which may introduce biases and may not fully capture cultural appropriateness.
- The cultural proximity hypothesis could be confounded by factors like resource availability and linguistic similarity that weren't fully controlled for.

## Confidence

- **High confidence**: LLMs outperform NMT models on proverb translation (Section 4.1 results are consistent across multiple language pairs and evaluation metrics)
- **Medium confidence**: Cultural proximity advantage (supported by DE-EN performance but limited data on truly distant language pairs)
- **Medium confidence**: Dialogue context superiority over explanations (mechanism is plausible but dataset filtering may have influenced results)
- **High confidence**: Automatic metrics inadequacy (clear examples provided, though COMET performs better than BLEU/CHRF++)

## Next Checks

1. **Dataset expansion validation**: Replicate the filtering pipeline on a larger mined corpus from OpenSubtitles to determine if PiC dataset size limitations affected the dialogue context findings. Compare performance differences between context strategies when sample size increases by 2-3x.

2. **Cultural distance quantification**: Develop a quantitative measure of cultural distance between language pairs (beyond Western vs. non-Western binary) and correlate this with proverb translation performance across all models. This would test whether cultural proximity advantage holds when controlling for other factors.

3. **Evaluation metric comparison with human judgments**: Conduct a human evaluation study (at least 100 proverb translations) comparing automatic metrics, LLM-as-a-judge, and human expert ratings. Calculate inter-annotator agreement and correlation coefficients to determine which evaluation approach best captures true translation quality for proverbs.