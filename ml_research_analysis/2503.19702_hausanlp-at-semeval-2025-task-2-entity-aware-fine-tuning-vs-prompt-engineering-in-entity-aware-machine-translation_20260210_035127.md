---
ver: rpa2
title: 'HausaNLP at SemEval-2025 Task 2: Entity-Aware Fine-tuning vs. Prompt Engineering
  in Entity-Aware Machine Translation'
arxiv_id: '2503.19702'
source_url: https://arxiv.org/abs/2503.19702
tags:
- translation
- named
- entities
- data
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper investigates entity-aware machine translation by comparing\
  \ supervised fine-tuning of the NLLB model against prompt engineering using Google\u2019\
  s Gemini. NLLB was fine-tuned with both task training data and Wikidata-extracted\
  \ named entity translations, while Gemini was evaluated in zero-shot and few-shot\
  \ setups using structured prompts."
---

# HausaNLP at SemEval-2025 Task 2: Entity-Aware Fine-tuning vs. Prompt Engineering in Entity-Aware Machine Translation

## Quick Facts
- arXiv ID: 2503.19702
- Source URL: https://arxiv.org/abs/2503.19702
- Reference count: 5
- Key outcome: Prompt engineering with Gemini outperforms fine-tuned NLLB in entity translation accuracy, with minimal benefit from few-shot examples.

## Executive Summary
This paper investigates entity-aware machine translation by comparing supervised fine-tuning of the NLLB model against prompt engineering using Google's Gemini. The team fine-tuned NLLB-200-distilled-600M with task training data plus Wikidata-extracted named entity translations, while evaluating Gemini in zero-shot and few-shot setups using structured prompts. Testing across 10 target languages (Arabic, German, Spanish, French, Italian, Japanese, Korean, Thai, Turkish, Chinese), results showed Gemini consistently outperformed NLLB in entity translation accuracy (M-ETA), with Spanish and Italian achieving the highest scores and Chinese performing the poorest. The study demonstrates that prompt engineering with large language models can effectively handle named entity translation without requiring extensive fine-tuning.

## Method Summary
The study compared two approaches for entity-aware machine translation: fine-tuning the NLLB-200-distilled-600M model with task training data and Wikidata-extracted entity translations, versus prompt engineering with Gemini Flash 1.5. NLLB was fine-tuned using batch size 32, sequence length 128, beam width 5, dropout 0.1, 10 epochs with early stopping, and combined task data with Wikidata entity pairs. Gemini was evaluated through zero-shot and 10-shot prompting using LangChain templates that explicitly instructed translation of named entities. The evaluation used COMET for translation quality and M-ETA for entity translation accuracy across 10 target languages, with the Overall score being the harmonic mean of both metrics.

## Key Results
- Gemini consistently outperformed NLLB in entity translation accuracy across all 10 target languages
- Minimal difference between zero-shot and 10-shot prompting for Gemini, suggesting elaborate examples are unnecessary
- Spanish and Italian achieved highest scores while Chinese performed poorest
- Entity translation accuracy varied significantly across language families, with Asian languages generally scoring lower

## Why This Works (Mechanism)
The effectiveness of prompt engineering for entity-aware translation stems from LLMs' inherent ability to recognize and handle named entities through their pretraining on diverse web data. Unlike fine-tuning, which requires substantial compute and task-specific data, prompt engineering leverages the model's existing knowledge by providing clear instructions and examples within the context window. The structured prompts explicitly instruct the model to focus on named entities while generating translations, effectively constraining the output to the desired behavior without modifying model parameters.

## Foundational Learning
- Named Entity Recognition (NER): Understanding how to identify and classify named entities in text is crucial for this task. Quick check: Verify SpaCy's entity types match the expected categories in your dataset.
- Wikidata API integration: Essential for extracting entity translations across multiple languages. Quick check: Confirm API queries return expected translations for test entities.
- COMET metric implementation: Understanding how to calculate translation quality scores. Quick check: Validate COMET scores on known good translations match expected values.

## Architecture Onboarding
Component map: SemEval data -> SpaCy NER -> Wikidata API -> Fine-tuning data/NLLB fine-tuning, Gemini prompt templates -> COMET/M-ETA evaluation -> Results

Critical path: Data extraction and preprocessing -> Model training/evaluation setup -> Prompt engineering implementation -> Metric calculation and comparison

Design tradeoffs: The paper chose to use a distilled 600M parameter NLLB model rather than larger variants due to computational constraints, which likely impacted performance compared to Gemini's larger model size.

Failure signatures: Low M-ETA scores indicate entity translation issues; poor performance on Asian languages suggests tokenization or script handling problems; zero-shot and few-shot parity indicates prompt effectiveness plateauing.

First experiments:
1. Verify Wikidata entity extraction matches SpaCy NER output for sample sentences
2. Test single-language translation with both NLLB and Gemini to establish baseline performance
3. Evaluate prompt template effectiveness by checking if generated outputs follow instructions

## Open Questions the Paper Calls Out
### Open Question 1
To what extent is the performance gap between NLLB and Gemini attributable to model scale versus the choice of fine-tuning versus prompting? The comparison uses a small, fine-tuned open model against a massive, closed prompt-based model, confounding methodology with model capabilities. A controlled experiment comparing prompt engineering and fine-tuning on a single open-weight large model of similar size to Gemini would resolve this.

### Open Question 2
Does restricting training data to task-specific named entities, rather than the broad set extracted by SpaCy, improve entity translation accuracy? The paper opted for broad entity extraction but speculates that the narrower, task-specific subset might yield better results. An ablation study comparing models fine-tuned on broad versus task-specific entities would provide evidence.

### Open Question 3
Under what conditions does few-shot prompting yield significant improvements over zero-shot prompting for entity-aware translation? Results showed minimal benefit from 10-shot examples, leading to the conclusion that elaborate examples are unnecessary. Testing various numbers of shots and varying example relevance would determine if specific prompt structures trigger few-shot benefits.

## Limitations
- Comparison between NLLB and Gemini is confounded by significant model size differences (600M vs unknown large parameters)
- Limited to 10 target languages, restricting generalizability across all language families
- Focus on named entity translation without deeper analysis of translation fluency or contextual appropriateness

## Confidence
- **High Confidence**: Gemini's superior entity translation accuracy and minimal difference between zero-shot and few-shot prompting effectiveness
- **Medium Confidence**: Claim that prompt engineering can replace extensive fine-tuning for entity-aware translation tasks
- **Low Confidence**: Assertion that prompt engineering is universally more effective than fine-tuning for all entity-aware translation scenarios

## Next Checks
1. Reproduce NLLB fine-tuning with different optimizer configurations to determine if performance gap persists across optimization strategies
2. Cross-validate prompt engineering approach using open-weight models like LLaMA or Mistral to verify if Gemini's performance is methodology-driven or model-specific
3. Evaluate both approaches on out-of-domain entity types not present in SemEval training data or Wikidata to assess generalization beyond studied entity types