---
ver: rpa2
title: 'LUNA: LUT-Based Neural Architecture for Fast and Low-Cost Qubit Readout'
arxiv_id: '2512.07808'
source_url: https://arxiv.org/abs/2512.07808
tags:
- readout
- fidelity
- design
- latency
- fpga
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LUNA addresses the challenge of resource-efficient qubit readout
  in superconducting quantum processors by integrating integrator-based preprocessing
  with LUT-mapped neural networks. It uses a low-cost integrator to compress raw I/Q
  time-series into compact features, reducing dimensionality without multipliers.
---

# LUNA: LUT-Based Neural Architecture for Fast and Low-Cost Qubit Readout
## Quick Facts
- arXiv ID: 2512.07808
- Source URL: https://arxiv.org/abs/2512.07808
- Reference count: 22
- Primary result: LUT-based neural network for qubit readout with up to 10.95× LUT reduction, 30% lower latency, and >95.9% fidelity on FPGA

## Executive Summary
LUNA introduces a low-resource neural network architecture for superconducting qubit readout, combining integrator-based preprocessing with a LUT-mapped classifier. The method compresses raw I/Q time-series using a low-cost integrator, then classifies with a LogicNet where neurons are directly mapped to FPGA LUTs, eliminating DSP usage. A differential evolution optimization jointly tunes preprocessing and classifier parameters. FPGA results on Xilinx Zynq UltraScale+ show significant reductions in area and latency while maintaining high readout fidelity, making LUNA suitable for scalable quantum processors requiring fast, low-cost measurements.

## Method Summary
LUNA integrates an integrator-based preprocessing stage with a LUT-based neural network classifier. Raw I/Q time-series data from qubit readout is first compressed by a low-cost integrator, which applies simple arithmetic without multipliers to reduce dimensionality. The compressed features are then classified by a LogicNet, where each neuron is synthesized directly into FPGA LUTs, enabling ultra-low latency and minimal area. A joint optimization using differential evolution tunes both preprocessing and classifier parameters. The complete system is implemented and tested on a Xilinx Zynq UltraScale+ FPGA, demonstrating substantial improvements in resource efficiency and speed while maintaining high readout fidelity.

## Key Results
- Up to 10.95× reduction in LUT usage compared to conventional FPGA implementations
- 30% lower classification latency while maintaining fidelities above 95.9%
- Zero DSP utilization, enabling highly scalable and low-cost deployment

## Why This Works (Mechanism)
LUNA achieves efficiency by replacing computationally heavy preprocessing and classifier components with lightweight alternatives. The integrator compresses high-dimensional I/Q data into compact features without requiring multipliers, drastically reducing the data load for the classifier. The LogicNet classifier maps each neuron directly to FPGA LUTs, avoiding DSP blocks and minimizing latency. Joint optimization via differential evolution ensures both preprocessing and classification parameters are tuned for maximal performance, resulting in a system that is both fast and resource-efficient.

## Foundational Learning
- **Integrator-based preprocessing**: Compresses I/Q time-series to reduce dimensionality; needed to lower classifier input size and computational cost; quick check: verify compression preserves key signal features.
- **LUT-based neural network (LogicNet)**: Maps neurons directly to FPGA LUTs for minimal area and latency; needed to eliminate DSP usage and reduce resource overhead; quick check: confirm LUT mapping does not degrade classification accuracy.
- **Differential evolution optimization**: Jointly tunes preprocessing and classifier parameters; needed to find optimal trade-offs between compression and classification performance; quick check: compare against grid or random search baselines.
- **Superconducting qubit readout**: Involves measuring I/Q quadratures from readout resonators; needed as the target application domain; quick check: ensure preprocessing matches signal characteristics.
- **FPGA resource constraints**: DSP blocks are scarce; needed context for why LUT-based solutions are attractive; quick check: measure LUT/DSP usage against FPGA capacity.
- **Mid-circuit measurement**: Requires fast, low-latency readout for quantum error correction; needed to justify speed requirements; quick check: benchmark latency against error correction timing constraints.

## Architecture Onboarding
- **Component map**: Raw I/Q time-series -> Integrator preprocessing -> Compressed features -> LUT-based LogicNet classifier -> Readout decision
- **Critical path**: Integrator preprocessing (feed-forward) -> LUT-based neuron evaluation (combinational) -> Classification output
- **Design tradeoffs**: LUT-based neurons minimize area and latency but may limit classifier expressiveness compared to DSP-based networks; integrator preprocessing reduces data dimensionality but may lose subtle signal details
- **Failure signatures**: Degradation in readout fidelity suggests either insufficient feature extraction by integrator or inadequate classifier capacity; excessive LUT usage indicates poor optimization or overly complex network
- **First experiments**:
  1. Run integrator preprocessing on raw I/Q data and plot compressed feature distributions to confirm meaningful signal retention.
  2. Implement a single LUT-based neuron and verify correct classification on a simple test set.
  3. Perform joint optimization with differential evolution and compare classification accuracy and resource usage against unoptimized baseline.

## Open Questions the Paper Calls Out
None

## Limitations
- Results are demonstrated on a single IBM Quantum device dataset; robustness across different noise profiles and readout frequencies is not established.
- Joint optimization may not generalize to entirely different qubit modalities or cryogenic environments without retraining.
- Exclusive reliance on LUT-based neurons could limit classifier expressiveness for highly non-linear decision boundaries compared to small embedded DSPs.

## Confidence
- **Area and latency improvements**: High — measured on actual FPGA hardware with clear quantitative comparisons
- **Preprocessing innovation**: Medium — integrator's noise suppression benefits are shown but not extensively characterized across varied experimental conditions
- **Overall scalability claim**: Low-medium — study focuses on single-qubit readout without multi-qubit or full-stack quantum processor integration

## Next Checks
1. Test LUNA on datasets from multiple superconducting qubit platforms with varying noise characteristics to verify robustness.
2. Implement a multi-qubit readout scenario on FPGA to assess scalability and resource sharing.
3. Benchmark against other low-cost FPGA readout schemes (e.g., matched filter or simple threshold methods) in realistic error-correction cycle timing.