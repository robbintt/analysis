---
ver: rpa2
title: Skewness-Robust Causal Discovery in Location-Scale Noise Models
arxiv_id: '2511.14441'
source_url: https://arxiv.org/abs/2511.14441
tags:
- noise
- causal
- skew-normal
- distribution
- cause
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses causal discovery in bivariate location-scale
  noise models (LSNMs) when noise distributions are skewed, which can cause existing
  methods to fail. The authors propose SKEWD, a likelihood-based algorithm that extends
  standard LSNMs by modeling noise using skew-normal distributions instead of normal
  distributions.
---

# Skewness-Robust Causal Discovery in Location-Scale Noise Models

## Quick Facts
- arXiv ID: 2511.14441
- Source URL: https://arxiv.org/abs/2511.14441
- Reference count: 40
- This paper proposes SKEWD, a likelihood-based algorithm for causal discovery in bivariate location-scale noise models when noise distributions are skewed, achieving perfect accuracy on highly skewed synthetic data.

## Executive Summary
This paper addresses causal discovery in bivariate location-scale noise models (LSNMs) when noise distributions are skewed, which can cause existing methods to fail. The authors propose SKEWD, a likelihood-based algorithm that extends standard LSNMs by modeling noise using skew-normal distributions instead of normal distributions. SKEWD employs a combination of Bayesian optimization, CMA-ES heuristic search, and an expectation conditional maximization (ECM) algorithm for parameter estimation. Experiments on novel synthetic datasets with varying skewness levels and established benchmarks show that SKEWD is robust under high skewness, achieving perfect accuracy on highly skewed data and performing on par with or better than state-of-the-art methods on standard benchmarks.

## Method Summary
SKEWD extends location-scale noise models by modeling the noise distribution as skew-normal rather than normal. The method uses a two-stage optimization approach: first employing CMA-ES heuristic search to find initial parameter estimates, then refining them via an ECM algorithm. The model represents functions using natural cubic B-splines with penalty parameters tuned via Bayesian optimization. For inference, SKEWD supports both independence testing using HSIC on residuals and likelihood scoring by comparing log-likelihoods in both causal directions.

## Key Results
- SKEWD achieves perfect accuracy (100%) on synthetic datasets with high skewness levels (γ₁ = 0.985 and 0.995)
- On standard benchmarks (Tübingen, UAI, CauseEffectPairs), SKEWD performs comparably to or better than state-of-the-art methods
- The ECM refinement step improves accuracy by 28-34 percentage points over CMA-ES alone on highly skewed synthetic data
- SKEWD outperforms QCCD on LSNMs with high skewness, which QCCD fails to handle due to its ANM assumptions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Modeling noise as skew-normal rather than normal enables correct causal inference when the true noise distribution exhibits skewness.
- Mechanism: The skew-normal distribution SN(ξ, ω, λ) extends the normal distribution with a shape parameter λ. When λ ≠ 0, the distribution becomes asymmetric. The conditional distribution Y|X=x in the LSNM becomes Y|X=x ~ SN(f(x), g(x), λ), allowing the model to correctly capture asymmetric residual patterns that would otherwise induce spurious dependence between cause and estimated noise.
- Core assumption: The true noise distribution is skew-normal or approximately so, with skewness bounded by |γ₁| < 0.9953.
- Evidence anchors:
  - [abstract] "SKEWD extends the usual normal-distribution framework to the skew-normal setting, enabling reliable inference under symmetric and skewed noise."
  - [section 2] Defines skew-normal LSNM with Y|X=x ~ SN(f(x), g(x), λ) and notes the skewness bound γ_max₁ ≈ 0.9953.
  - [corpus] No direct corpus validation for skew-normal specifically; related work on asymmetric distributions is limited in the neighbor papers.
- Break condition: When true noise has skewness exceeding the skew-normal bound (~0.9953), or has heavy tails not captured by skew-normal (e.g., the GNO(0,1,-0.5) distribution with γ₁ ≈ 1.750 used in experiments—SKEWD still works but may be extrapolating).

### Mechanism 2
- Claim: Refining CMA-ES heuristic estimates via ECM algorithm substantially improves causal inference accuracy, especially for LSNMs.
- Mechanism: CMA-ES provides a global heuristic search over the (p+q+1)-dimensional parameter space. The ECM algorithm then iteratively refines by: (E-step) computing conditional expectations of latent variables V|Y=y from a truncated normal distribution; (CM-step 1) updating ψ and λ analytically; (CM-step 2) updating ρ via CMA-ES since no closed form exists. This two-stage approach avoids local optima while ensuring likelihood increases monotonically.
- Core assumption: The latent variable representation of skew-normal is valid; initialization from CMA-ES is sufficiently close to a good local optimum.
- Evidence anchors:
  - [section 3] "In Appendix B, we show how the ECM algorithm improves upon the performance achieved by the initial heuristic solution in the causal discovery task on our synthetic data, demonstrating the necessity of refinement."
  - [appendix B, table 2] For LSs datasets, CMA-ES-IT achieves 52-64% accuracy vs. SKEWD-IT achieving 82-92%; improvements of 28-34 percentage points.
  - [corpus] No corpus papers discuss ECM for skew-normal specifically.
- Break condition: If CMA-ES initialization is extremely poor or the likelihood surface has pathological structure, ECM may converge to a suboptimal local maximum.

### Mechanism 3
- Claim: Independence testing is more robust across diverse benchmark settings, while likelihood scoring achieves perfect accuracy on highly skewed synthetic data but has higher variance.
- Mechanism: Independence testing extracts residuals N̂ = (Y - f̂(X))/ĝ(X) and uses HSIC to test independence with the hypothesized cause. Likelihood scoring compares ℓ_{X→Y} = log p(x) + log p(y|x) across directions. Under standardization and normal marginals, the marginal log-likelihoods cancel, reducing to comparing conditionals.
- Core assumption: For likelihood scoring, the marginal distributions of X and Y are approximately normal after standardization. For independence testing, the HSIC test has sufficient power.
- Evidence anchors:
  - [section 4] Describes both inference modes and notes likelihood scoring "assumes that the marginal distributions of X and Y are normal distributions."
  - [table 1] SKEWD-IT has mean accuracy 85.02% with std 9.71%; SKEWD-LL has 77.43% with std 20.56%, showing higher variance.
  - [figure 2] SKEWD-LL achieves 100% accuracy across all skewed datasets; SKEWD-IT shows slight degradation (92-98%).
  - [corpus] Neighbor papers on bivariate causal discovery do not directly compare these inference modes.
- Break condition: When marginals are far from normal (e.g., heavy-tailed), likelihood scoring may be biased. When independence tests have low power (small samples), both approaches may fail.

## Foundational Learning

- **Concept: Location-Scale Noise Models (LSNMs)**
  - Why needed here: LSNMs Y = f(X) + g(X)N generalize additive noise models by allowing heteroscedastic noise scaling. The paper's entire method is built on estimating LSNMs with skew-normal noise.
  - Quick check question: Given Y = X² + X·N where N ~ SN(0,1,λ), can you identify f(X), g(X), and the noise distribution?

- **Concept: Skew-Normal Distribution and Its Properties**
  - Why needed here: The skew-normal SN(ξ, ω, λ) has a bounded skewness range and requires special estimation techniques (ECM) due to the lack of closed-form MLE and potential infinite likelihood at λ=0.
  - Quick check question: If X ~ SN(0, 1, 5), what is the approximate skewness γ₁? (Hint: it's bounded by ~0.9953.)

- **Concept: EM/ECM Algorithms for Latent Variable Models**
  - Why needed here: The skew-normal distribution has a latent variable representation that enables ECM estimation. Understanding E-steps and CM-steps is essential for debugging convergence issues.
  - Quick check question: In the ECM algorithm, why is CM-step 2 solved via CMA-ES rather than analytically?

## Architecture Onboarding

- **Component map:**
  1. Bayesian Optimization (penalty parameters α, κ)
  2. CMA-ES Heuristic (initial θ estimation)
  3. ECM Algorithm (refinement)
  4. B-Spline Basis (function representation)
  5. Inference Layer (HSIC independence testing or log-likelihood comparison)

- **Critical path:**
  1. Standardize X and Y.
  2. Run Bayesian optimization to find (α, κ).
  3. For each direction (X→Y and Y→X):
     - Run CMA-ES with multiple initializations (8 from CV folds).
     - Submit best result to ECM.
     - Store fitted parameters and log-likelihood/p-value.
  4. Compare directions via likelihood ratio or p-value difference.

- **Design tradeoffs:**
  - **Spline complexity vs. overfitting**: q=14 for f, p=7 for g assumes g is less complex. Increase for highly nonlinear relationships.
  - **CMA-ES iterations vs. ECM iterations**: Paper uses 5000 max for both. Reduce for speed on large datasets (e.g., Tübingen pair 69 limited to 500 ECM iterations).
  - **Independence testing vs. likelihood scoring**: Use IT for robustness across unknown settings; use LL when you suspect highly skewed noise.

- **Failure signatures:**
  - **Skewness exceeding 0.9953**: Residuals may still show dependence; consider modeling with GNO or other distributions.
  - **ECM non-convergence**: If ||θ^(k+1) - θ^(k)||₂ never drops below 10⁻⁶, check CMA-ES initialization quality.
  - **Likelihood scoring bias**: If X or Y has heavy-tailed marginal (not normal), likelihood comparison may be unreliable.
  - **QCCD-style failure on LSNMs**: Methods assuming ANM may infer wrong direction on heteroscedastic data (Table 4: QCCD achieves 11-21% on LSs with high skewness).

- **First 3 experiments:**
  1. **Sanity check on symmetric data**: Generate LSNM with normal noise (λ=0). Verify SKEWD-LL achieves ~100% accuracy, matching LOCI baseline.
  2. **Ablation of ECM refinement**: Run on LSs(0.985) dataset with and without ECM. Expect ~30% accuracy drop without ECM (per Table 2).
  3. **Boundary skewness test**: Generate data with GNO(0,1,-0.5) noise (γ₁ ≈ 1.750, exceeding skew-normal bound). Verify SKEWD still recovers correct direction (Table 4 shows 100% accuracy), but inspect residuals for model misspecification.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the SKEWD framework be generalized to handle multivariate causal discovery beyond bivariate pairs?
- Basis in paper: [explicit] The authors explicitly identify "multivariate cause-effect inference" as a natural extension and limitation in the Conclusion section.
- Why unresolved: The current method is designed and evaluated exclusively for bivariate cause-effect pairs (X → Y vs Y → X), and the estimation pipeline has not been adapted for graph structures.
- What evidence would resolve it: An extension of the algorithm that successfully identifies causal directions within multivariate DAGs and benchmarks demonstrating its performance on such structures.

### Open Question 2
- Question: Can the noise model be extended to accommodate heavy-tailed distributions while maintaining robustness?
- Basis in paper: [explicit] The paper lists "extensions to also consider heavy tailed distributions" as a specific avenue for future work in the Conclusion.
- Why unresolved: The proposed skew-normal distribution has tail behavior similar to the normal distribution and cannot model excess kurtosis, limiting applicability for heavy-tailed real-world data.
- What evidence would resolve it: A modification of the generative model (e.g., using a skew-t distribution) that demonstrates robust performance on synthetic or real-world data exhibiting heavy tails.

### Open Question 3
- Question: Can the likelihood scoring approach be modified to remove the assumption of normally distributed marginals?
- Basis in paper: [explicit] The authors suggest "lifting the Gaussianity assumption of the marginals for the likelihood-based approach" via methods like kernel density estimation or normalizing flows.
- Why unresolved: The current likelihood scoring method assumes standardized observations follow a normal distribution to justify neglecting marginal log-likelihoods, which may introduce bias if marginals are non-Gaussian.
- What evidence would resolve it: A revised likelihood formulation that estimates marginal densities non-parametrically and maintains accuracy on data with non-Gaussian marginal distributions.

## Limitations
- **Distributional assumptions**: SKEWD assumes skew-normal noise with bounded skewness (~0.9953), potentially limiting performance on data with more extreme skewness or heavy tails.
- **Computational scalability**: The method uses population size 100 and up to 5000 iterations for CMA-ES, which may not scale efficiently to larger datasets.
- **Benchmark heterogeneity**: Performance shows mixed results with high variance across different inference modes and datasets.

## Confidence

- **High Confidence**: The fundamental mechanism that skew-normal modeling captures asymmetric noise patterns that normal models miss. The ECM refinement consistently improves over heuristic CMA-ES alone (28-34 percentage point gains on synthetic data).

- **Medium Confidence**: Performance claims on benchmark datasets due to high variance and mixed results across different inference modes. The superiority of SKEWD over specific baselines (QCCD, LOCI) is demonstrated but with notable exceptions.

- **Low Confidence**: The claim that likelihood scoring achieves "perfect accuracy" on highly skewed synthetic data, given the high variance (std 20.56%) observed on benchmarks and the assumption of normal marginals.

## Next Checks

1. **Distributional Robustness Test**: Generate synthetic data with noise distributions having skewness beyond the skew-normal bound (γ₁ > 0.9953) and evaluate SKEWD's performance to quantify the impact of distributional assumptions.

2. **Benchmark Ablation Study**: Systematically compare SKEWD-IT and SKEWD-LL on each benchmark dataset to understand which inference mode performs better under specific data characteristics (e.g., sample size, noise structure).

3. **Computational Scalability Evaluation**: Measure runtime and memory usage on increasingly large datasets to assess practical scalability, particularly the CMA-ES population and iteration parameters.