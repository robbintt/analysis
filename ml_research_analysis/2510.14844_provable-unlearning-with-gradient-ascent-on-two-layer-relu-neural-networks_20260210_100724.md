---
ver: rpa2
title: Provable Unlearning with Gradient Ascent on Two-Layer ReLU Neural Networks
arxiv_id: '2510.14844'
source_url: https://arxiv.org/abs/2510.14844
tags:
- lemma
- unlearning
- proof
- have
- point
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper provides theoretical analysis of gradient ascent as\
  \ a method for machine unlearning in neural networks. The authors formalize a new\
  \ success criterion called (\u03F5, \u03B4, \u03C4)-successful unlearning based\
  \ on Karush-Kuhn-Tucker (KKT) conditions, which are known to be satisfied by solutions\
  \ obtained through gradient descent on homogeneous models."
---

# Provable Unlearning with Gradient Ascent on Two-Layer ReLU Neural Networks

## Quick Facts
- arXiv ID: 2510.14844
- Source URL: https://arxiv.org/abs/2510.14844
- Reference count: 40
- One-line primary result: Gradient ascent provides provable approximate unlearning for linear models and two-layer ReLU networks under high-dimensional, nearly-orthogonal data assumptions.

## Executive Summary
This paper establishes theoretical foundations for gradient ascent as a method for machine unlearning in neural networks. The authors introduce a new success criterion based on Karush-Kuhn-Tucker (KKT) conditions, showing that gradient ascent can achieve approximate unlearning with bounded error parameters (ϵ, δ, τ). The method is proven effective for both linear models and two-layer ReLU networks under assumptions of high-dimensional or nearly-orthogonal data, with theoretical corrections for maintaining activation patterns in ReLU networks.

## Method Summary
The method involves taking a single gradient ascent step on the data point to be forgotten, using a theoretically-derived optimal step size that depends on dual variables from training. For linear models, this step directly removes the forgotten data's contribution from the weighted gradient sum. For two-layer ReLU networks, a theoretical correction term preserves activation patterns across retained data. The approach assumes the initial model satisfies approximate KKT conditions from implicit bias of gradient descent on homogeneous losses.

## Key Results
- Gradient ascent achieves (ϵ, δ, τ)-successful unlearning for single data points in linear models and two-layer ReLU networks
- For linear predictors, near-exact recovery of margin-maximizing predictor is achieved under near-orthogonal data assumptions
- Two-layer ReLU networks require correction terms to maintain activation patterns, with approximation error bounded by O(ϵ_d/√mn)
- Unlearning via gradient ascent does not compromise generalization performance in synthetic Gaussian-mixture settings

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Unlearning is achieved if a single gradient ascent step transforms the model parameters such that they satisfy the KKT conditions of the margin maximization problem for the retained dataset.
- **Mechanism:** Gradient descent on homogeneous networks implicitly converges to KKT points of the max-margin problem. A gradient step removing the influence of the forgotten data yields parameters that are approximate KKT points for the remaining data.
- **Core assumption:** The original model is an (ϵ₁, δ₁)-approximate KKT point for the original dataset.
- **Break condition:** If the original model is under-trained (not near a KKT point), the stationarity condition required to prove the update maps to the retained set's KKT point does not hold.

### Mechanism 2
- **Claim:** For linear predictors, a gradient ascent step removes the forgotten data's contribution from the weighted gradient sum that constitutes the model weights.
- **Mechanism:** By taking a gradient step with size β = -λ_l / ℓ'(y_l N(w, x_l)), the term λ_l y_l x_l is subtracted, leaving a sum over only the retained data.
- **Core assumption:** Data is nearly orthogonal (Assumption 2.3: |⟨x_i, x_j⟩| ≤ φ) and normalized.
- **Break condition:** If data is highly correlated (not near-orthogonal), removing one point's gradient vector significantly alters the margins of other points.

### Mechanism 3
- **Claim:** For two-layer ReLU networks, gradient ascent unlearning requires a theoretical "correction term" to preserve the activation patterns across the retained data.
- **Mechanism:** The paper constructs a theoretical correction vector using a scaling factor c to ensure that if a neuron was active on a retained point before unlearning, it remains active after the gradient step.
- **Core assumption:** The "fix" (correction term) is small (bounded by O(ϵ_d/√mn)) due to high-dimensional/nearly-orthogonal data.
- **Break condition:** If the step size is too large or data is not near-orthogonal, the required "correction" to maintain activation consistency grows, increasing the approximation error τ.

## Foundational Learning

- **Concept:** **Karush-Kuhn-Tucker (KKT) Conditions**
  - **Why needed here:** The paper redefines "successful unlearning" not as weight matching, but as matching the *optimality conditions* (KKT) of the model retrained on the retained set.
  - **Quick check question:** Can you explain the "stationarity" and "complementary slackness" conditions for a constrained optimization problem?

- **Concept:** **Implicit Bias of Gradient Descent**
  - **Why needed here:** The proof assumes the initial model is at a KKT point (max-margin solution) because of how gradient descent behaves on homogeneous losses, not because of explicit regularization.
  - **Quick check question:** Why does gradient descent on logistic loss tend to maximize the margin on separable data?

- **Concept:** **Homogeneous Networks**
  - **Why needed here:** The analysis relies on the property that scaling weights scales the output (e.g., ReLU is positive-homogeneous), which links the margin maximization problem to the training trajectory.
  - **Quick check question:** How does the property N(αθ, x) = αN(θ, x) affect the convergence direction of gradient flow?

## Architecture Onboarding

- **Component map:** Input -> Gradient Ascent Step -> KKT Error Metrics -> Validator
- **Critical path:**
  1. Verify Data Properties: Ensure Assumption 2.3 (normalized/nearly orthogonal) holds
  2. Determine Multipliers: Estimate or access the dual variables λ_i from the training process
  3. Apply Update: Execute single-step gradient ascent on the forget point (x_l, y_l)
- **Design tradeoffs:**
  - Approximate vs. Exact: The method is computationally cheap (O(1) step) but only guarantees *approximate* KKT satisfaction, not exact weight equality to retraining
  - Step Size Sensitivity: The theoretical guarantee requires a specific step size; deviating from it increases approximation error
- **Failure signatures:**
  - High Correlation: If data inner products ⟨x_i, x_j⟩ are large, the margin bounds degrade
  - Activation Flip: In ReLU nets, if the update causes neurons to flip activation states on retained data, the stationarity condition breaks
- **First 3 experiments:**
  1. Step Size Ablation: Vary step size multiplier (0.5x to 1.5x) on synthetic linear model and plot KKT approximation error
  2. Orthogonality Stress Test: Generate datasets with varying correlation and measure cosine similarity between unlearned and retrained models
  3. Activation Stability: Track percentage of neuron activation patterns that flip during unlearning step in 2-layer ReLU net

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the theoretical guarantees for gradient ascent unlearning be extended to neural network architectures with more than two layers?
- Basis in paper: [explicit] Section 7 states that "extending our results to deeper architectures... remains an important challenge."
- Why unresolved: The proofs rely specifically on the gradient structure and stationarity conditions of two-layer networks.
- What evidence would resolve it: A generalized proof demonstrating that a gradient ascent step is an (ϵ, δ, τ)-successful unlearning algorithm for an L-layer ReLU network.

### Open Question 2
- Question: Does adding a recovery phase (fine-tuning on the retained set) maintain or improve the theoretical guarantees provided for the single gradient ascent step?
- Basis in paper: [explicit] Section 7 notes it would be valuable to "analyze the effect of an additional recovery phase... under the same KKT-based framework."
- Why unresolved: The current analysis isolates the "basic building block" (A_GA) and does not analyze the multi-step procedures used in methods like NegGrad+.
- What evidence would resolve it: A theoretical analysis proving that the composite algorithm (ascent followed by descent on retained data) satisfies the (ϵ, δ, τ)-successful unlearning criterion.

### Open Question 3
- Question: What are the theoretical bounds connecting the proposed KKT-satisfaction criterion to standard privacy metrics, such as membership inference risk or differential privacy?
- Basis in paper: [explicit] Section 7 suggests "developing tighter bounds connecting approximate KKT satisfaction with practical privacy metrics."
- Why unresolved: The paper establishes a new theoretical criterion for success but does not formally map this criterion to established empirical or differential privacy guarantees.
- What evidence would resolve it: A theorem quantifying the relationship between the parameters ϵ, δ, τ and bounds on membership inference attack success rates or differential privacy ε_DP values.

## Limitations
- The theoretical analysis relies on strong assumptions of near-orthogonal, normalized data which may not hold in real-world datasets
- The unlearning guarantees are for a single data point and depend on knowing or computing optimal step sizes based on dual variables from training
- The method only achieves approximate unlearning (bounded by ϵ, δ, τ) rather than exact weight recovery

## Confidence

- **High**: The core theoretical framework linking gradient ascent to KKT conditions for unlearning is sound and well-supported by the linear model proofs
- **Medium**: The linear model unlearning guarantees are strong but rely heavily on the nearly-orthogonal data assumption; the two-layer ReLU analysis extends this but introduces more complex correction terms
- **Low**: The practical reproducibility of the exact step size calculation and the stability of the ReLU correction terms in experiments beyond the synthetic setting

## Next Checks

1. **Orthogonality Stress Test**: Generate datasets with varying degrees of correlation and measure the cosine similarity between the unlearned model and the retrained model to observe the degradation of guarantees
2. **Activation Stability**: For a 2-layer ReLU net, track the percentage of neuron activation patterns that flip on the retained set during the unlearning step to validate the need for the theoretical "correction term"
3. **Step Size Ablation**: Replicate Figure 1 by varying the step size multiplier (0.5x to 1.5x) on a synthetic linear model and plotting the resulting KKT approximation error to verify the "sharpness" of the optimum