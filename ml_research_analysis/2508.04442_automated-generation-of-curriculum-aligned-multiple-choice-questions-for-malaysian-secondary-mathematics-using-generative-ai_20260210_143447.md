---
ver: rpa2
title: Automated Generation of Curriculum-Aligned Multiple-Choice Questions for Malaysian
  Secondary Mathematics Using Generative AI
arxiv_id: '2508.04442'
source_url: https://arxiv.org/abs/2508.04442
tags:
- question
- questions
- generation
- curriculum
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Automated Generation of Curriculum-Aligned Multiple-Choice Questions for Malaysian Secondary Mathematics Using Generative AI

## Quick Facts
- arXiv ID: 2508.04442
- Source URL: https://arxiv.org/abs/2508.04442
- Reference count: 29
- Primary result: RAG-based pipelines significantly outperform non-grounded prompting methods for curriculum-aligned MCQ generation.

## Executive Summary
This paper presents a generative AI framework for creating curriculum-aligned multiple-choice questions (MCQs) for Malaysian secondary mathematics in Bahasa Melayu. The key innovation is a symbiotic RAG-QA evaluation system that verifies both semantic alignment with curriculum documents and contextual validity. Four generation pipelines were tested, with the Manual RAG approach achieving the highest performance at 96% validity and 0.96 STS scores.

## Method Summary
The system uses GPT-4o to generate MCQs from two sources: teacher notes (Nota Bab 1.pdf) and a yearly teaching plan (RPT Bab 1.pdf). Four generation methods were compared: (1) Structured Prompt with function calling, (2) Basic Prompt, (3) LangChain RAG with FAISS vector store, and (4) Manual RAG with custom PDF parsing. Questions are evaluated using Semantic Textual Similarity (STS) against the RPT and a novel RAG-QA method that checks if questions can be answered using the curriculum document.

## Key Results
- RAG-based methods (STS 0.86-0.89) significantly outperform non-grounded methods (STS 0.55)
- Manual RAG achieved highest validity (96%) through semantic chunking based on PDF layout
- RAG-QA evaluation effectively filtered out curriculum-irrelevant questions
- Method 1 (Structured Prompt) eliminated JSON formatting errors present in Method 2

## Why This Works (Mechanism)

### Mechanism 1: External Knowledge Grounding via RAG
Retrieval-Augmented Generation constrains the LLM by forcing it to attend to retrieved chunks from teacher notes before synthesizing questions. This shifts the model from relying on parametric memory to specific non-parametric context, crucial for low-resource language contexts where LLM pre-training data is sparse.

### Mechanism 2: Functional Validity via Symbiotic RAG-QA
A RAG-based Question-Answering evaluation serves as a stricter proxy for "answerability" than semantic similarity alone. The system treats generated MCQs as queries against the RPT, filtering out questions that cannot be answered from curriculum objectives.

### Mechanism 3: Structure-Preserving Chunking
Domain-aware chunking that respects semantic boundaries of educational materials yields better results than generic splitting. By parsing PDFs based on visual/layout logic, the system maintains the semantic link between problem statements and solutions, enabling procedural understanding questions.

## Foundational Learning

- **Concept: Retrieval-Augmented Generation (RAG)**
  - Why needed: Core architecture distinguishing successful pipelines from failing ones
  - Quick check: Does the system query the LLM first, or does it query a vector database first to get context?

- **Concept: Semantic Textual Similarity (STS) & Vector Embeddings**
  - Why needed: Essential for understanding how "alignment" is measured numerically
  - Quick check: If a generated question uses different synonyms than the syllabus, would an STS score capture the alignment?

- **Concept: Chunking Strategies**
  - Why needed: Critical differentiator between good and best framework implementations
  - Quick check: What happens if a text splitter cuts off the answer to a math problem from the problem statement itself?

## Architecture Onboarding

- **Component map:** Nota Bab 1.pdf (Knowledge Source) -> PDF Parser (PyMuPDF) -> Chunker -> Embedding Model -> Vector Store (FAISS) -> GPT-4o (Prompt + Retrieved Context -> JSON MCQ) -> STS Scorer (Alignment) & RAG-QA Validator (Validity) -> RPT Bab 1.pdf (Evaluation Standard)

- **Critical path:** The quality of the retrieved context is the bottleneck. If chunking logic is flawed, the LLM generates generic questions even if RAG is technically implemented.

- **Design tradeoffs:**
  - LangChain RAG: Lower development effort, standardized, "good enough" performance (92% validity)
  - Manual RAG: High development effort, but highest alignment (96% validity) and control over context quality

- **Failure signatures:**
  - High Fluency, Low Validity: Questions look correct but test concepts outside syllabus
  - JSON Formatting Errors: Basic prompting may produce invalid JSON
  - Generic Definitional Questions: Symptom of poor retrieval or non-grounded prompting

- **First 3 experiments:**
  1. Baseline Verification: Run Method 1 vs. Method 3 on a single chapter, confirm STS scores differ significantly (>0.2 delta)
  2. Chunking Sensitivity: Implement generic recursive splitter vs. custom splitter on Nota PDF, compare procedural complexity of generated questions
  3. Evaluation Stress Test: Generate 10 off-topic questions, verify if RAG-QA evaluation correctly flags them as "Invalid"

## Open Questions the Paper Calls Out

### Open Question 1
How do generated questions perform when evaluated by human experts for pedagogical value and cognitive complexity? The current study relies exclusively on automated metrics which cannot assess higher-order thinking skills or classroom suitability. Evidence needed: correlations between automated alignment scores and human teacher ratings.

### Open Question 2
Does the effectiveness of Manual RAG over LangChain implementation persist in non-STEM subjects with different textual structures? The benefit of custom chunking logic may differ for subjects that do not rely on worked examples. Evidence needed: comparative results for humanities or language subjects.

### Open Question 3
Can the automated evaluation framework be enhanced to distinguish between questions testing factual recall versus those testing problem-solving skills? Semantic similarity cannot differentiate between difficulty levels of various learning standards. Evidence needed: a classifier that categorizes generated questions by Bloom's Taxonomy level with high accuracy.

## Limitations

- Manual chunking logic for Method 4 is vaguely described without sufficient technical detail for precise replication
- RAG-QA validation mechanism lacks explicit thresholds or LLM-judge prompts
- Critical prompt templates for all four methods are omitted, leaving significant gaps in generation process specification

## Confidence

- **High:** Core finding that RAG-based methods outperform non-grounded prompting (STS scores 0.86-0.89 vs 0.55)
- **Medium:** Effectiveness of symbiotic RAG-QA evaluation as validity proxy (implementation details sparse)
- **Low:** Claim that semantic chunking based on PDF layout is critical differentiator (evidence largely anecdotal)

## Next Checks

1. **Prompt Reconstruction:** Recreate Structured Prompt and Basic Prompt based on descriptions, verify if they produce the reported STS gap (>0.3 difference) on a test topic

2. **Threshold Sensitivity:** Systematically vary RAG-QA retrieval score threshold (0.3, 0.5, 0.7) and observe effects on valid question rate and types of questions filtered

3. **Layout Consistency Test:** Apply semantic chunking approach to a different Malaysian Form 1 Mathematics PDF with different layout, assess whether method consistently produces more procedural questions than standard recursive character splitter