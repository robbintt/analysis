---
ver: rpa2
title: Reinforcement Fine-Tuning Powers Reasoning Capability of Multimodal Large Language
  Models
arxiv_id: '2505.18536'
source_url: https://arxiv.org/abs/2505.18536
tags:
- arxiv
- reasoning
- preprint
- reinforcement
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper analyzes how reinforcement fine-tuning (RFT) enhances
  the reasoning capabilities of multimodal large language models (MLLMs). The authors
  systematically summarize community progress in five areas: diverse modalities (vision,
  audio, omni-multimodal, GUI, metaverse, and agents), diverse tasks and domains (mathematical
  reasoning, academic reasoning, visual understanding, video reasoning, medical reasoning,
  embodied vision, and multimodal generation), better training algorithms (curriculum
  RL, dynamic KL regularization, noise augmentation, MCTS-guided data filtering),
  abundant benchmarks (increasingly difficult, human-like reasoning, comprehensive,
  realistic, visual-centric, and interactive benchmarks), and thriving engineering
  frameworks.'
---

# Reinforcement Fine-Tuning Powers Reasoning Capability of Multimodal Large Language Models

## Quick Facts
- arXiv ID: 2505.18536
- Source URL: https://arxiv.org/abs/2505.18536
- Reference count: 40
- Key outcome: This paper analyzes how reinforcement fine-tuning (RFT) enhances the reasoning capabilities of multimodal large language models (MLLMs) across diverse modalities and tasks.

## Executive Summary
This comprehensive survey analyzes how reinforcement fine-tuning has empowered multimodal large language models with robust reasoning capabilities across vision, audio, GUI, video, and other modalities. The authors systematically summarize community progress in five areas: diverse modalities, diverse tasks and domains, better training algorithms, abundant benchmarks, and thriving engineering frameworks. They also propose five future research directions focusing on improving generalization, combining reward paradigms, enhancing safety, exploring data augmentation, and advancing algorithms.

## Method Summary
The paper describes two main algorithm families for RFT in MLLMs: critic-model-driven approaches (PPO) using actor-critic architectures with Generalized Advantage Estimation (GAE) and clipped objectives, and critic-model-free approaches (GRPO) using group-relative advantages with KL penalties to reference models. The method involves sampling multimodal inputs, generating multiple outputs per input, computing rewards (outcome-based or step-wise), normalizing advantages within groups, and updating the policy model while preventing drift from the SFT initialization. Training can be enhanced with curriculum learning, noise augmentation, step-wise rewards, and data filtering techniques.

## Key Results
- RFT has significantly enhanced MLLMs' reasoning capabilities across diverse modalities including vision, audio, GUI, and video
- Five key algorithmic approaches have emerged: GRPO (critic-free), PPO (actor-critic), curriculum RL, noise augmentation, and MCTS-guided filtering
- Extensive benchmark development has created increasingly challenging and realistic reasoning tasks across mathematical, academic, medical, and embodied domains
- Open-source frameworks like EasyR1 and Open-R1-Multimodal have democratized RFT implementation for the community

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** RFT with simple rule-based (outcome) rewards can incentivize reasoning behaviors in MLLMs without requiring learned reward models.
- **Mechanism:** The policy model generates multiple outputs per input; rewards are computed via verifiable rules (e.g., answer correctness); group-relative advantage normalization (GRPO) provides learning signal without a critic model, while KL penalties prevent excessive drift from the SFT initialization.
- **Core assumption:** The task's ground-truth outcome is verifiable programmatically, and reasoning chains that lead to correct outcomes are learnable via policy gradient signals.
- **Evidence anchors:** GRPO objective formalizes group-relative advantages with KL penalty to reference model; simple rule-based rewards suffice without separate learned reward models.

### Mechanism 2
- **Claim:** Critic-model-driven approaches (PPO variants) provide denser learning signal via learned value functions but require additional model capacity and training stability.
- **Mechanism:** A critic model V_φ estimates state values; Generalized Advantage Estimation (GAE) computes advantages from value predictions; the policy is updated via clipped objective to prevent destabilizing updates.
- **Core assumption:** The value function can be accurately learned concurrently with policy, and its estimates meaningfully guide policy improvement.
- **Evidence anchors:** PPO objective with GAE computation from critic model; actor-critic fundamentally aim to optimize policy while concurrently learning value function.

### Mechanism 3
- **Claim:** Algorithmic enhancements (curriculum learning, noise augmentation, step-wise rewards, data filtering) can address specific failure modes of base RFT.
- **Mechanism:** Various interventions including curriculum RL adjusting reward design or data difficulty over training, noise augmentation exposing model to perturbed inputs for robustness, step-wise rewards providing denser signal by evaluating intermediate reasoning steps, and MCTS-based filtering selecting high-value training samples.
- **Core assumption:** These augmentations improve sample efficiency, stability, or generalization without introducing distribution shift or reward hacking.
- **Evidence anchors:** Curr-ReFT, NoisyRollout, VL-Rethinker (SSR), ThinkLite-VL (MCTS filtering), R1-VL (StepGRPO) described with specific algorithmic contributions.

## Foundational Learning

- **Concept: Markov Decision Process (MDP) formulation for sequence generation**
  - **Why needed here:** RFT frames language/sequence generation as sequential decision-making; understanding states (partial sequences), actions (token choices), and rewards is prerequisite.
  - **Quick check question:** Can you explain why token-by-token generation in an LLM maps to an MDP, and what the reward function would be for a reasoning task?

- **Concept: Policy gradient methods and advantage functions**
  - **Why needed here:** GRPO and PPO both rely on advantage estimation (A = Q - V or group-relative normalization); misunderstanding leads to incorrect implementation.
  - **Quick check question:** In GRPO, how is advantage computed differently from PPO's GAE, and what does this imply for training stability?

- **Concept: KL divergence constraints in policy optimization**
  - **Why needed here:** Both TRPO and GRPO use KL penalties to prevent catastrophic forgetting; understanding this trade-off is critical for balancing exploration vs. retention.
  - **Quick check question:** What happens if the KL penalty coefficient β is set too low vs. too high in GRPO training?

## Architecture Onboarding

- **Component map:** Multimodal input → Policy model (MLLM) → Multiple outputs per input → Reward computation (rule-based or learned) → Advantage normalization → Policy loss with KL penalty → Updated policy model

- **Critical path:**
  1. Sample multimodal inputs and generate multiple outputs per input (group size G)
  2. Compute rewards for each output (outcome-based or step-wise)
  3. Normalize rewards within groups to compute advantages
  4. Compute policy loss with clipping and KL penalty
  5. Update policy model; periodically sync reference model if using adaptive KL

- **Design tradeoffs:**
  - **GRPO vs. PPO:** GRPO removes critic model (lower memory, simpler) but may have higher variance; PPO provides lower-variance estimates but requires stable critic training
  - **Outcome vs. process rewards:** Outcome is simpler but sparse; process rewards are denser but require reliable step evaluation (PRM training is unstable per paper)
  - **Group size G:** Larger groups improve advantage estimation but increase compute per update

- **Failure signatures:**
  - **Reward hacking:** Model generates outputs that maximize reward signal without genuine reasoning (e.g., repeating patterns, exploiting verifier loopholes)
  - **KL collapse:** Policy drifts too far from reference, losing prior capabilities (Section 4, TO DO 3 mentions this concern)
  - **Vanishing advantages:** In GRPO, if all outputs in a group receive similar rewards, advantages approach zero; VL-Rethinker's SSR addresses this
  - **Overthinking:** Excessively long reasoning chains without improved accuracy (FAST, No-Thinking-RL address this)

- **First 3 experiments:**
  1. **Baseline GRPO on a vision-math benchmark (e.g., MathVision)** with rule-based accuracy reward; verify training stability and compare to SFT baseline
  2. **Ablation on group size G** (e.g., G=4, 8, 16) to understand impact on advantage estimation quality and compute trade-offs
  3. **KL penalty sweep** (varying β) to identify regime where policy improves without catastrophic forgetting; monitor both task accuracy and retention on held-out general tasks

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can reinforcement fine-tuning (RFT) strategies be adapted to achieve robust generalization across complex modalities (e.g., auditory, omni-multimodal) and domains (e.g., embodied AI)?
- **Basis in paper:** The paper states in "TO DO 1" that generalizable reasoning for "auditory and omni-multimodal" modalities and "embodied-specific settings" domains "remains an underexplored area."
- **Why unresolved:** Current RFT efforts are largely siloed within specific modalities (primarily vision) or limited task sets, failing to achieve the unified adaptability required for AGI.
- **What evidence would resolve it:** Demonstration of a single RFT-trained model maintaining high reasoning performance across disparate modalities (e.g., audio and video) and domains without catastrophic forgetting.

### Open Question 2
- **Question:** Can the outcome reward paradigm be effectively integrated with the process reward paradigm to stabilize Process Reward Model (PRM) training and provide dense intermediate feedback?
- **Basis in paper:** "TO DO 2" proposes that the community "consider integrating the outcome reward paradigm with the process reward paradigm" to address the instability of PRM training and the sparsity of outcome rewards.
- **Why unresolved:** Outcome rewards are efficient but sparse, while process rewards are dense but unstable; a hybrid approach that leverages the strengths of both is currently lacking.
- **What evidence would resolve it:** A training framework where outcome-based RFT successfully converges into a stable PRM that improves step-wise reasoning accuracy.

### Open Question 3
- **Question:** What specific defense mechanisms are required to mitigate reasoning-specific safety threats, such as reward hacking and jailbreak attacks, in RFT-powered MLLMs?
- **Basis in paper:** "TO DO 3" highlights that research on safety for reasoning MLLMs is "notably limited" and explicitly calls for focus on "reward hacking" and "jailbreak attacks."
- **Why unresolved:** The extended chain-of-thought reasoning trajectories introduced by RFT create a larger attack surface and new vulnerabilities (e.g., "overthinking") not present in standard MLLMs.
- **What evidence would resolve it:** The development of benchmarks and defense strategies that successfully prevent reward hacking in RFT environments where standard safety alignment fails.

### Open Question 4
- **Question:** To what extent can multimodal data augmentation techniques (e.g., noise injection, spatial transforms) during RFT enhance the perception and reasoning capabilities of MLLMs in data-scarce scenarios?
- **Basis in paper:** "TO DO 4" suggests investigating "appropriate DA methods for a broader range of visual tasks" and applying them to other modalities to address data scarcity in RFT.
- **Why unresolved:** While preliminary works like NoisyRollout show promise in math reasoning, the efficacy of diverse augmentation strategies on perception-heavy tasks (e.g., visual counting) and non-visual modalities is unverified.
- **What evidence would resolve it:** Ablation studies showing significant performance gains on perception-centric benchmarks when specific augmentation policies are applied during the RFT phase.

## Limitations

- **Reward signal reliability:** The assumption that rule-based outcome rewards can sufficiently drive reasoning behavior without learned reward models may not hold for tasks where correct answers can be reached via non-reasoning shortcuts
- **Generalization claims remain partially supported:** Many cited works demonstrate task-specific improvements without comprehensive ablation studies or cross-task transfer experiments
- **Algorithm comparison lacks systematic evaluation:** The paper positions GRPO and PPO as two main algorithmic approaches but does not provide direct comparative analysis of their effectiveness for MLLM reasoning

## Confidence

**High confidence:** The descriptive analysis of community progress across modalities, tasks, and benchmarks is well-supported by the extensive literature survey and open-source project references.

**Medium confidence:** The mechanistic claims about why RFT works (reward-based policy improvement, KL regularization preventing catastrophic forgetting) are theoretically sound but lack empirical validation specific to MLLM reasoning.

**Low confidence:** The effectiveness of specific algorithmic enhancements (curriculum learning, noise augmentation, MCTS filtering) for MLLM reasoning remains largely theoretical without systematic comparative studies.

## Next Checks

1. **Controlled algorithm comparison:** Implement GRPO and PPO variants on a standardized visual reasoning benchmark (e.g., MathVista) with identical hyperparameters except for algorithm choice. Measure both final accuracy and training stability metrics (KL divergence, reward variance) to empirically validate the claimed trade-offs.

2. **Reward signal ablation study:** Design experiments comparing outcome-only rewards versus combined outcome-process rewards on tasks with verifiable intermediate steps. Measure whether models trained with process rewards demonstrate more robust reasoning patterns (e.g., consistent chain-of-thought generation) compared to outcome-only training.

3. **Cross-task generalization evaluation:** Train RFT models on mathematical reasoning tasks, then evaluate performance degradation on general visual understanding tasks and vice versa. This would empirically test the generalization concerns raised in the future directions section and validate whether current RFT approaches maintain multimodal capabilities while enhancing reasoning.