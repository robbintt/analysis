---
ver: rpa2
title: Model-Based Reinforcement Learning in Discrete-Action Non-Markovian Reward
  Decision Processes
arxiv_id: '2512.14617'
source_url: https://arxiv.org/abs/2512.14617
tags:
- reward
- learning
- figure
- office
- state
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces QR-MAX, the first model-based reinforcement\
  \ learning algorithm for discrete-action Non-Markovian Reward Decision Processes\
  \ (NMRDPs) that explicitly factorizes Markovian environment dynamics from non-Markovian\
  \ reward handling via reward machines. This factorization enables QR-MAX to achieve\
  \ PAC-MDP convergence to \u03B5-optimal policies with polynomial sample complexity\
  \ of O(|S||A|+|S||Q|)/(\u03B5\xB3(1-\u03B3)\xB3), removing one multiplicative |Q|\
  \ term compared to applying R-MAX directly on the product MDP space."
---

# Model-Based Reinforcement Learning in Discrete-Action Non-Markovian Reward Decision Processes

## Quick Facts
- arXiv ID: 2512.14617
- Source URL: https://arxiv.org/abs/2512.14617
- Reference count: 25
- Primary result: First model-based RL algorithm for discrete-action NMRDPs with factorized sample complexity O(|S||A|+|S||Q|)/(ε³(1-γ)³), removing one |Q| multiplicative term

## Executive Summary
This paper introduces QR-MAX, a model-based reinforcement learning algorithm that factorizes Markovian environment dynamics from non-Markovian reward handling via reward machines in Non-Markovian Reward Decision Processes (NMRDPs). The key insight is that by maintaining separate transition models for the environment and the reward machine, QR-MAX achieves polynomial PAC-MDP sample complexity that scales with the sum rather than the product of environment and automaton sizes. The authors also present BUCKET-QR-MAX, which extends this factorization to continuous state spaces using SimHash-based discretization while preserving the theoretical guarantees.

## Method Summary
QR-MAX maintains two separate transition/reward models: one for environment dynamics (indexed by state-action-state triples) and one for reward machine transitions (indexed by automaton state-next state-next automaton state triples). Each model has its own visitation counts and optimistic initialization. Value iteration is triggered when either model component becomes "known" based on separate thresholds. BUCKET-QR-MAX extends this to continuous states by mapping continuous states to discrete buckets via SimHash, maintaining the same factorized structure with four separate counters. The algorithm uses optimistic initialization (Q = R_max/(1-γ)) and achieves PAC-MDP guarantees with sample complexity proportional to |S||A| + |S||Q| for discrete states.

## Key Results
- QR-MAX achieves up to two orders of magnitude better sample efficiency than state-of-the-art methods on Office gridworld benchmark
- Factorization removes one multiplicative |Q| term from sample complexity compared to applying R-MAX on product MDP
- BUCKET-QR-MAX achieves fast, stable learning on continuous state spaces without manual gridding
- Reduced memory requirements and easier hyperparameter tuning compared to baselines
- Maintains formal PAC guarantees while improving practical performance

## Why This Works (Mechanism)

### Mechanism 1
Factorizing Markovian environment dynamics from non-Markovian reward automaton dynamics improves sample complexity by removing one multiplicative |Q| term. The algorithm maintains separate transition counts n_E(s,a) for environment dynamics and n_Q(q,s') for automaton transitions. Since P(s'|s,q,a) = P(s'|s,a) (environment is Markovian regardless of automaton state), each learned environment transition is reused across all automaton states q∈Q, avoiding redundant exploration.

### Mechanism 2
Optimistic initialization with separate "knownness" thresholds provides PAC-MDP guarantees while maintaining efficient exploration. Q-values initialize to R_max/(1-γ). A state-action pair (s,a) or automaton transition (q,s') only becomes "known" after reaching threshold visits (t_E and t_Q). Unknown pairs retain optimistic values, driving exploration. Value iteration updates only when components become known.

### Mechanism 3
SimHash-based discretization extends factorization to continuous state spaces while preserving PAC-style guarantees under smoothness assumptions. Continuous states s_cont are mapped to buckets b = h(s_cont) via locality-sensitive hashing. Four separate counters (n_ET, n_ER, n_QT, n_QR) track environment transitions, environment rewards, automaton transitions, and automaton rewards independently. Value iteration operates on the induced finite bucket MDP.

## Foundational Learning

- **Markov Decision Processes (MDPs) and Bellman Equations**
  - Why needed here: QR-MAX builds on standard MDP theory; understanding Q-values, value iteration, and optimality is prerequisite to grasping the factorization.
  - Quick check question: Can you write the Bellman optimality equation for Q*(s,a)?

- **R-MAX Algorithm and PAC-MDP Framework**
  - Why needed here: QR-MAX directly extends R-MAX; the optimistic initialization, "known" state concept, and sample complexity analysis inherit from this foundation.
  - Quick check question: In R-MAX, what value is assigned to unknown state-action pairs, and why does this encourage exploration?

- **Reward Machines and NMRDPs**
  - Why needed here: The paper's core contribution requires understanding how reward machines compactly represent history-dependent rewards and how the product MDP S×Q is constructed.
  - Quick check question: Given a reward machine with states Q and environment states S, what is the product MDP state space, and how does transition P(s',q'|s,q,a) decompose?

## Architecture Onboarding

- **Component map:**
  - Environment transition estimator -> Automaton transition estimator -> Q-table -> Value Iteration module -> SimHash discretizer (Bucket variant)
  - Environment counts n_E(s,a) -> Automaton counts n_Q(q,s') -> Optimistic Q-values -> Conditional VI triggering -> Continuous state mapping

- **Critical path:**
  1. Initialize Q-table to R_max/(1-γ); zero all counters
  2. Select action a = argmax Q(s,q,a)
  3. Observe (s', q', r_E, r_A); update separate environment and automaton counters
  4. If n_E(s,a) = t_E or n_Q(q,s') = t_Q: trigger Value Iteration
  5. Repeat until all pairs known

- **Design tradeoffs:**
  - Higher thresholds t_E, t_Q → more accurate model estimates but slower convergence
  - SimHash resolution (L hash functions) → finer discretization improves approximation but increases |B| and memory
  - Discount factor γ → higher values require more VI iterations T = ⌈(1-γ)⁻¹ ln(4R_max/[ε(1-γ)])⌉

- **Failure signatures:**
  - Policy never converges: thresholds too high or environment non-Markovian
  - Inconsistent behavior across automaton states: factorization assumption violated
  - Continuous variant diverges: discretization too coarse or dynamics non-Lipschitz
  - Memory explosion in continuous case: too many unique buckets (|B| growing unbounded)

- **First 3 experiments:**
  1. Validate factorization on simple gridworld: Run QR-MAX vs. R-MAX on a 10×10 grid with a 3-state reward machine. Confirm QR-MAX reaches optimal policy in ~|S||A| + |S||Q| samples vs. ~|S||Q||A| for R-MAX.
  2. Ablate threshold sensitivity: Test t_E ∈ {10, 30, 100} with deterministic reward machine (t_Q=1). Verify that model accuracy improves with t_E but sample efficiency degrades linearly.
  3. Stress test continuous discretization: Run BUCKET-QR-MAX on continuous Office World with varying SimHash resolutions (L ∈ {4, 8, 16}). Confirm that higher L improves policy quality up to the point where |B| becomes memory-prohibitive.

## Open Questions the Paper Calls Out

### Open Question 1
Can the factorization framework of QR-MAX be extended to continuous action spaces while preserving PAC guarantees? The current algorithm assumes discrete action spaces; continuous actions would require fundamentally different exploration and optimization mechanisms that break the tabular value iteration approach. A modification of QR-MAX that handles continuous actions (e.g., via discretization or function approximation) with a formal PAC-MDP bound in the continuous setting would resolve this.

### Open Question 2
How can QR-MAX be adapted to handle stochastic reward machines while maintaining the sample complexity improvements from factorization? Stochastic RMs introduce uncertainty in the automaton transitions (P(q'|q,s') instead of deterministic η), which complicates the factorization and requires different theoretical analysis. A theoretical PAC-MDP analysis for NMRDPs with stochastic RMs, plus empirical validation showing retained sample-efficiency gains, would resolve this.

### Open Question 3
Can the factorization approach be integrated with deep RL methods to scale to high-dimensional state spaces without SimHash discretization? BUCKET-QR-MAX uses tabular counts over SimHash buckets; deep RL would require neural network function approximation, which breaks the optimism-based exploration guarantees and creates challenges for maintaining separate environment and automaton representations. A deep variant of QR-MAX with empirical sample efficiency gains on high-dimensional NMRDP benchmarks, ideally with theoretical guarantees, would resolve this.

## Limitations

- Theoretical sample complexity bound for BUCKET-QR-MAX relies on unverifiable assumption about bounded bucket diameter in high-dimensional spaces
- Experimental validation limited to one domain (Office World) with restricted variability in reward machine complexity
- No evaluation on partially observable settings where NMRDPs typically arise
- Discretization error analysis assumes Lipschitz continuity without empirical validation

## Confidence

**High confidence** in the core factorization mechanism and discrete state sample complexity improvement, as this follows directly from Markovian environment assumption and standard PAC-MDP analysis.

**Medium confidence** in the SimHash-based continuous extension, as experimental results show good sample efficiency but theoretical guarantees depend on unverifiable assumptions about bucket properties and function smoothness.

**Medium confidence** in overall performance claims, given the limited benchmark diversity and single-domain evaluation.

## Next Checks

1. **Verify factorization benefits**: Run QR-MAX vs. standard R-MAX on a suite of 5-10 synthetic NMRDPs with varying |S|, |Q|, and |A| to empirically confirm the O(|S||A| + |S||Q|) vs. O(|S||Q||A|) sample complexity scaling.

2. **Test Lipschitz assumption**: Apply the continuous variant to a benchmark with known non-smooth dynamics (e.g., gridworld with walls or discontinuous reward regions) to determine whether the discretization error bound breaks down in practice.

3. **Evaluate robustness to POMDPs**: Run QR-MAX on a partially observable variant of Office World (e.g., with noisy or partial state observations) and measure whether the algorithm still converges to near-optimal policies or whether the Markovian environment assumption becomes a critical failure point.