---
ver: rpa2
title: 'No Answer Needed: Predicting LLM Answer Accuracy from Question-Only Linear
  Probes'
arxiv_id: '2509.10625'
source_url: https://arxiv.org/abs/2509.10625
tags:
- dataset
- triviaqa
- direction
- correctness
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The authors investigate whether LLMs anticipate their own correctness\
  \ before generating an answer. They extract residual stream activations immediately\
  \ after reading a question (before any tokens are generated), and train linear probes\
  \ to predict whether the model\u2019s forthcoming answer will be correct."
---

# No Answer Needed: Predicting LLM Answer Accuracy from Question-Only Linear Probes

## Quick Facts
- **arXiv ID**: 2509.10625
- **Source URL**: https://arxiv.org/abs/2509.10625
- **Reference count**: 40
- **Primary result**: Linear probes on residual streams immediately after reading a question can predict answer correctness for knowledge-based tasks with 65-80% accuracy, but fail on mathematical reasoning.

## Executive Summary
This study investigates whether large language models can anticipate their own answer correctness before generating responses. The authors extract residual stream activations immediately after reading questions (before any tokens are generated) and train linear probes to predict whether forthcoming answers will be correct. Across three model families ranging from 7 to 70 billion parameters, they find that simple difference-of-means linear probes trained on TriviaQA generalize to diverse out-of-distribution knowledge datasets, outperforming black-box baselines and verbalized confidence. The approach demonstrates strong predictive power for factual correctness but completely fails on mathematical reasoning tasks, suggesting fundamental differences between knowledge retrieval and arithmetic computation.

## Method Summary
The methodology involves extracting residual stream activations at specific token positions immediately after the question is fully read (at position P+1 for P tokens in the question). These pre-generation activations are then used to train linear probes that predict answer correctness. The study employs two main probe architectures: a simple difference-of-means probe that separates correct and incorrect answers by their mean activations, and a two-layer multi-layer perceptron. The authors evaluate across three model families (Llama-2, Mistral, Gemma) with 7B, 13B, and 70B parameters, using TriviaQA as the primary training dataset and testing generalization across multiple knowledge domains including QuALITY, MedQA, BioASQ, and NaturalQuestions. The arithmetic reasoning failure is specifically tested on GSM8K and MATH datasets.

## Key Results
- Linear probes trained on TriviaQA achieve 65-80% accuracy in predicting answer correctness across diverse knowledge datasets (QuALITY, MedQA, BioASQ, NaturalQuestions)
- Predictive power emerges in intermediate layers (L3-L5 for 7B models, L5-L8 for 70B models) and correlates with "I don't know" responses
- The approach completely fails on mathematical reasoning tasks (GSM8K, MATH), with AUC dropping to chance levels (50%)
- Out-of-distribution generalization significantly outperforms verbalized confidence and black-box baselines

## Why This Works (Mechanism)
The predictive capability likely stems from the model's internal representation of confidence and answer plausibility encoded in the residual stream immediately after processing the question. The emergence of predictive power in intermediate layers suggests that the model forms preliminary judgments about answerability before engaging in full generation. The correlation with "I don't know" responses indicates that the direction captures genuine confidence signals rather than just question difficulty. The failure on arithmetic tasks suggests that factual knowledge and computational reasoning engage different mechanisms within the model's architecture.

## Foundational Learning
- **Residual stream activations**: The accumulated information at each token position in the transformer's computation - needed because these contain the model's current understanding before generation begins; quick check: verify activations capture sufficient context by comparing with attention patterns
- **Linear probe methodology**: Training simple classifiers on frozen model representations - needed because it isolates predictive signals without fine-tuning the base model; quick check: compare probe performance against full fine-tuning baselines
- **AUC as evaluation metric**: Area under ROC curve for binary classification - needed because it provides threshold-independent assessment of predictive power; quick check: ensure class balance doesn't artificially inflate scores
- **Out-of-distribution generalization**: Testing on datasets different from training data - needed to validate the robustness and universality of predictive directions; quick check: systematically vary domain similarity between train and test sets
- **Token position analysis**: Examining activations at specific positions relative to question length - needed because timing affects what information is available; quick check: verify that P+1 position consistently captures post-question state
- **Difference-of-means probing**: Separating classes by mean activation vectors - needed for its simplicity and interpretability in identifying predictive directions; quick check: compare against more complex probe architectures

## Architecture Onboarding

**Component Map:**
Question -> Tokenizer -> Residual Stream (L1-LN) -> Linear Probe -> Correctness Prediction

**Critical Path:**
The critical path involves question tokenization feeding into the transformer layers, where residual stream activations at position P+1 (immediately after the question) are extracted and fed to linear probes that output binary correctness predictions. The intermediate layers (L3-L5 for 7B, L5-L8 for 70B) are identified as most informative for prediction.

**Design Tradeoffs:**
The study prioritizes simplicity and interpretability over maximum predictive accuracy by using linear probes rather than fine-tuned classifiers. This choice enables zero-shot generalization and clearer interpretation of what the model "knows" before answering. The tradeoff is potentially missing more complex representations that could provide stronger signals. The focus on pre-generation activations limits the analysis to initial confidence rather than generation-time dynamics.

**Failure Signatures:**
Complete failure on mathematical reasoning tasks (AUC ~50%, chance level) indicates the approach is domain-specific to knowledge retrieval. The failure suggests that arithmetic computation engages different mechanisms than factual recall. Within knowledge domains, performance degrades on questions requiring multi-hop reasoning or when the model's knowledge is uncertain.

**Three First Experiments:**
1. Compare probe performance when trained on different token positions (during vs. after question reading) to validate that P+1 is optimal for capturing confidence signals
2. Test whether probe accuracy correlates with human-annotated question difficulty to distinguish between confidence and difficulty signals
3. Evaluate whether ensembling probes from multiple layers improves predictive accuracy compared to single-layer probes

## Open Questions the Paper Calls Out
The paper explicitly identifies the need to investigate why mathematical reasoning tasks fail completely while knowledge tasks succeed, suggesting fundamental architectural differences in how LLMs handle factual recall versus computation. It also calls for exploration of whether the predictive directions are model-agnostic or specific to each architecture, and whether dynamic probing during answer generation would provide additional insights beyond pre-generation signals.

## Limitations
- Complete failure on mathematical reasoning tasks (GSM8K, MATH) with AUC dropping to chance levels suggests the approach is domain-constrained
- Focus on single-token "I don't know" responses may not capture the full complexity of LLM answer generation and correctness
- Linear probe methodology, while simple and interpretable, may miss more complex representations that could provide stronger predictive signals

## Confidence
- **High confidence**: Core finding that linear probes on residual streams can predict answer correctness for knowledge-based tasks, supported by consistent results across three model families and multiple datasets
- **Medium confidence**: Interpretation that predictive power emerges from confidence representation rather than question difficulty, evidenced by correlations with "I don't know" responses
- **Medium confidence**: Generalization claims, given strong out-of-distribution performance on multiple datasets, though arithmetic reasoning failure suggests limitations
- **Lower confidence**: Layer-wise analysis interpretation, as the emergence of predictive power in intermediate layers requires further investigation to determine whether this reflects genuine confidence representation or training artifacts

## Next Checks
1. **Cross-model probe transfer**: Train linear probes on one model family (e.g., Llama-2) and evaluate zero-shot on different model architectures (e.g., Mistral or Gemma) to test whether predictive directions are model-agnostic or architecture-specific

2. **Multi-step reasoning analysis**: Systematically investigate whether probe performance degrades as question complexity increases within knowledge domains, distinguishing between simple fact recall and multi-hop reasoning tasks

3. **Dynamic probing during generation**: Extend the methodology to extract activations during answer generation (not just pre-generation) to determine whether predictive accuracy improves when the model has committed to an answer path, and whether this reveals different mechanisms for factual versus computational correctness