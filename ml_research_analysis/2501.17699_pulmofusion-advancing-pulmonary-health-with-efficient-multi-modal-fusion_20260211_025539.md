---
ver: rpa2
title: 'PulmoFusion: Advancing Pulmonary Health with Efficient Multi-Modal Fusion'
arxiv_id: '2501.17699'
source_url: https://arxiv.org/abs/2501.17699
tags:
- data
- thermal
- regression
- lung
- health
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents PulmoFusion, a novel multimodal approach for
  remote pulmonary health assessment that integrates RGB or thermal video data with
  patient metadata using both Spiking Neural Networks (SNNs) and CNNs. The method
  employs SNNs for classification tasks and lightweight CNNs for regression, enhanced
  with Multi-Head Attention for multimodal data fusion, K-Fold validation, and ensemble
  learning for robustness.
---

# PulmoFusion: Advancing Pulmonary Health with Efficient Multi-Modal Fusion

## Quick Facts
- **arXiv ID:** 2501.17699
- **Source URL:** https://arxiv.org/abs/2501.17699
- **Reference count:** 0
- **One-line primary result:** PulmoFusion achieves 92% accuracy for thermal breathing-cycle classification and 99.5% patient-wise accuracy using SNNs and CNNs with multi-modal fusion.

## Executive Summary
PulmoFusion presents a novel multimodal approach for remote pulmonary health assessment that integrates RGB or thermal video data with patient metadata using both Spiking Neural Networks (SNNs) and CNNs. The method employs SNNs for classification tasks and lightweight CNNs for regression, enhanced with Multi-Head Attention for multimodal data fusion, K-Fold validation, and ensemble learning for robustness. The approach achieves state-of-the-art performance with 92% accuracy (thermal) for breathing-cycle classification and 99.5% patient-wise accuracy, while regression models reach Relative RMSEs of 0.11 (thermal) and 0.26 (RGB) for Peak Expiratory Flow prediction, with 4.52% MAE for FEV1/FVC predictions.

## Method Summary
PulmoFusion processes RGB or thermal videos (30 frames, 224Ã—224 pixels) alongside patient metadata (age, height, smoking status) to perform two tasks: classification of FEV1/FVC abnormality and regression of Peak Expiratory Flow (PEF) and FEV1/FVC ratio. The system uses SNNs with LIF neurons and rate coding for classification, while employing X3D-small CNN backbone for regression, with both branches fused using Multi-Head Attention layers. The approach includes 5-fold subject-based cross-validation, data augmentation, ensemble learning, and post-processing (majority voting for classification, metric averaging for regression). The method achieves significant computational efficiency with SNNs requiring 4GB RAM versus 24GB for CNNs.

## Key Results
- 92% accuracy for thermal breathing-cycle classification and 99.5% patient-wise accuracy
- 0.11 Relative RMSE for thermal PEF regression vs 0.26 for RGB
- 4.52% MAE for FEV1/FVC predictions
- SNNs process data in 0.2 seconds vs CNN's 1.3 seconds with 4GB vs 24GB RAM requirements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Thermal imaging provides a more robust signal for respiratory volume estimation than RGB by capturing heat differentials rather than just visual motion.
- Mechanism: The paper posits that thermal sensors capture "changes in mask heat distribution, reflecting variations in exhaled air volume." This allows the model to correlate temperature fluctuations with Peak Expiratory Flow (PEF) more directly than RGB motion tracking, which suffers from contrast issues.
- Core assumption: There is a stable, monotonic relationship between the thermal signature of the exhaled air/mask and the actual volume/flow rate of the breath.
- Evidence anchors:
  - [abstract] Mentions achieving state-of-the-art performance with Relative RMSEs of 0.11 (thermal) vs 0.26 (RGB).
  - [page 4] Explicitly states thermal data outperformed RGB "due to its ability to capture changes in mask heat distribution."
  - [corpus] Neighbors like "Seeing Heat with Color" and "RGB-Thermal Infrared Fusion" generally support the premise that thermal data provides unique signal data in complex environments, though they do not validate the specific pulmonary correlation.
- Break condition: This mechanism fails if the ambient temperature fluctuates wildly or if the mask material does not retain heat long enough to correlate with flow rate, effectively decoupling the thermal signal from the respiratory volume.

### Mechanism 2
- Claim: Spiking Neural Networks (SNNs) provide a computationally efficient mechanism for temporal classification by converting continuous video frames into sparse binary events.
- Mechanism: The architecture uses Leaky Integrate-and-Fire (LIF) neurons (Eq. 1) to accumulate membrane potential over time. By processing data as spike trains, the model captures temporal dynamics of breathing cycles using sparse computations, resulting in lower memory requirements (4GB vs 24GB for CNNs).
- Core assumption: The temporal dynamics of a breathing cycle can be effectively discretized into spikes without losing the critical phase information required for the 70% FEV1/FVC classification threshold.
- Evidence anchors:
  - [page 2] Describes the LIF neuron dynamics and rate coding of frames into spike trains.
  - [page 4] Reports SNNs processed patient data in 0.2 seconds vs CNN's 1.3 seconds and required significantly less RAM.
  - [corpus] Corpus evidence is weak regarding SNNs specifically for medical video; neighbors focus on standard CNN or GAN architectures for thermal fusion.
- Break condition: If the regression target (exact flow value) is highly sensitive to small amplitude changes that get lost in the binary thresholding of the spike generation ($V_{th}$), the SNN mechanism will fail, which explains why the authors restricted SNNs to classification and not regression.

### Mechanism 3
- Claim: Fusing visual features with patient metadata via Multi-Head Attention resolves ambiguities in visual data by conditioning the inference on static patient attributes.
- Mechanism: Visual features from the X3D backbone and metadata features (height, age, smoking status) are processed separately and then merged using a Multi-Head Attention layer. This allows the model to weight visual features differently based on the patient's context (e.g., interpreting a weak flow signal differently for an athlete vs. a smoker).
- Core assumption: The dataset contains distinct subgroups where the visual signal alone is ambiguous, but the combination of (Visual + Metadata) creates a separable decision boundary.
- Evidence anchors:
  - [page 3] Table I shows Multi-Modal models consistently outperforming Single-Modal models (e.g., Thermal Patient-Wise Accuracy jumps from 94.50% to 99.50%).
  - [page 2] Describes the "Multi-Head Attention Layer" as a method to "focus on critical features and deeper correlations."
  - [corpus] "MASTER: Multimodal Segmentation" and "RGB-Thermal Infrared Fusion" support the general efficacy of attention mechanisms in fusing heterogeneous data streams.
- Break condition: If the visual features are already sufficient for high-confidence prediction (saturation), the attention mechanism may overfit to noise in the metadata, or conversely, fail if the metadata is highly correlated (redundant) with the visual features.

## Foundational Learning

- Concept: **Leaky Integrate-and-Fire (LIF) Neuron**
  - Why needed here: This is the fundamental unit of the SNN branch. Understanding Equation 1 (membrane potential decay $\beta$ and threshold $V_{th}$) is required to diagnose why the model might fail to capture long-range temporal dependencies in breathing cycles.
  - Quick check question: If the decay rate $\beta$ is set too high, will the neuron retain a memory of the start of a breath cycle by the time the cycle ends?

- Concept: **3D Convolution (X3D Architecture)**
  - Why needed here: The CNN branch relies on X3D to handle video input. Unlike 2D CNNs that process frames individually, X3D factors in the temporal dimension ($\gamma : t$), which is critical for analyzing the *rate* of expiration.
  - Quick check question: How does the X3D architecture differ from a standard ResNet when processing a video clip of a breathing cycle?

- Concept: **Spirometry Metrics (FEV1/FVC & PEF)**
  - Why needed here: The model outputs are not just classes but specific clinical markers. Understanding that FEV1/FVC < 70% indicates obstruction is necessary to interpret the "abnormal" classification logic and the regression targets.
  - Quick check question: Why would a regression model predicting Peak Expiratory Flow (PEF) be more sensitive to the *rate* of change in a video than the FEV1/FVC ratio?

## Architecture Onboarding

- Component map:
  - Inputs: Thermal/RGB Video (224x224x30 frames) + Metadata Vector (Age, Height, etc.)
  - Branch 1 (Classification/SNN): Rate Coding -> Spiking Conv Layers -> Flatten
  - Branch 2 (Regression/CNN): X3D Backbone -> Feature Vector
  - Fusion: Concatenation + Multi-Head Attention
  - Heads: FC Layer for Class (Normal/Abnormal) or Regressor (PEF Value)

- Critical path:
  1. Preprocessing: Segmentation of continuous video into single "breathing-blow cycles" (crucial step; errors here propagate to everything)
  2. Encoding: Converting normalized pixel intensities to Bernoulli spike trains (for SNN)
  3. Fusion: The attention layer combines the Video Feature Vector with the Metadata Feature Vector

- Design tradeoffs:
  - SNN vs CNN: The paper assigns Classification to SNNs (fast, low power) and Regression to CNNs (heavy, precise). Do not try to use the SNN for regression without addressing the gradient approximation issues mentioned in Section II-A.
  - Thermal vs RGB: Thermal offers higher accuracy (0.11 RMSE vs 0.26) but requires specialized hardware; RGB is accessible but requires ensemble learning to approach comparable performance.

- Failure signatures:
  - High Variance in Breathing Cycles: If the standard deviation of predictions across the 5 folds is high (>2%), check the data augmentation pipeline.
  - Regression Divergence: If the Relative RMSE stalls around 0.30, the model is likely ignoring the video features and relying only on metadata (visual backbone not training).
  - SNN "Dead" Neurons: If membrane potentials never exceed threshold $V_{th}$, the rate coding intensity may be too low for the specific lighting conditions of the input video.

- First 3 experiments:
  1. Verify Data Pipeline: Run the provided code on the "Single Model (Baseline)" for Thermal PEF regression. Confirm you can reproduce the ~0.30 RMSE from Table II before attempting the full ensemble.
  2. Ablation on Fusion: Remove the Multi-Head Attention layer and replace it with simple concatenation (Dense Layer) to measure the specific delta in accuracy gained by the attention mechanism (Table II suggests a jump from 0.15 to 0.13 RMSE).
  3. Inference Speed Test: Benchmark the SNN model vs. the CNN model on an edge device (or simulate 4GB vs 24GB VRAM constraint) to validate the 0.2s vs 1.3s latency claim.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can Spiking Neural Networks (SNNs) be effectively modified to outperform or match lightweight CNNs in pulmonary regression tasks?
- Basis in paper: [explicit] The conclusion states the "scarcity of SNN regression models restrict generalizability" and calls for "further exploration of SNNs in regression tasks."
- Why unresolved: The authors utilized lightweight CNNs for regression because SNNs remain limited for these tasks due to the challenges in gradient-based optimization with non-differentiable spikes.
- What evidence would resolve it: Demonstration of a pure SNN architecture achieving Relative RMSE scores comparable to or better than the reported 0.11 (thermal) and 0.26 (RGB) without relying on CNN backbones.

### Open Question 2
- Question: How can the reliance on manual video segmentation be removed to enable scalable, real-time pulmonary monitoring?
- Basis in paper: [explicit] The authors identify the "reliance on high-quality, manually segmented datasets" as a "critical bottleneck" and explicitly call for "automated data preprocessing techniques."
- Why unresolved: The current methodology requires "meticulous manual inspection of videos" and segmentation of breathing cycles, which limits the system's scalability and real-world applicability.
- What evidence would resolve it: The integration of an automated temporal segmentation module that processes continuous video streams with performance metrics (F1-score/Accuracy) equivalent to the manual baseline.

### Open Question 3
- Question: Does the high accuracy observed in the 60-subject cohort generalize to larger, demographically diverse populations?
- Basis in paper: [explicit] The conclusion notes that the "small participant pool... restrict generalizability" and explicitly cites the need for "larger datasets."
- Why unresolved: The study relies on a novel dataset of only 60 volunteers, which may not capture the full variance of respiratory pathologies or demographic factors needed for clinical deployment.
- What evidence would resolve it: Validation of the PulmoFusion model on a multi-center dataset containing diverse ethnicities and pulmonary conditions while maintaining the reported 99.5% patient-wise accuracy.

### Open Question 4
- Question: Can the computational efficiency of SNNs be fully leveraged on edge devices without significant loss of accuracy compared to the ensemble models?
- Basis in paper: [inferred] The introduction emphasizes the need for solutions in "low resource settings," and results show SNNs require significantly less RAM (4GB vs 24GB), but the highest performance relied on ensemble learning.
- Why unresolved: While SNNs are shown to be energy-efficient, the paper notes that the highest accuracy required ensemble learning and multi-head attention, which adds computational overhead that might negate the SNN's efficiency benefits on edge hardware.
- What evidence would resolve it: Deployment of the full multi-modal PulmoFusion pipeline on an embedded device (e.g., smartphone or Raspberry Pi) with measured latency and energy consumption metrics matching the theoretical SNN efficiency.

## Limitations
- Small dataset (60 subjects, 2,424 videos) limits generalizability
- Manual video segmentation creates scalability bottleneck
- SNN architecture restricted to classification only, not regression
- Thermal imaging requires specialized hardware not widely available

## Confidence

- **High Confidence:** The comparative performance metrics (92% accuracy for thermal breathing-cycle classification, 99.5% patient-wise accuracy, 0.11 Relative RMSE for thermal PEF regression) appear well-supported by the experimental setup and are consistent with the mechanism that thermal data provides more robust respiratory signals than RGB.
- **Medium Confidence:** The computational efficiency claims (4GB vs 24GB RAM, 0.2s vs 1.3s latency) are specific and verifiable, but the generalizability of these efficiency gains across different hardware configurations requires testing.
- **Low Confidence:** The specific architectural details for the SNN implementation (LIF decay rate, threshold values, surrogate gradient formulation) and the ensemble learning configuration are underspecified, making exact reproduction challenging.

## Next Checks
1. Verify the thermal-to-flow correlation assumption holds across different mask materials and ambient temperatures by testing on a subset of videos with controlled environmental variations.
2. Benchmark the SNN vs CNN computational efficiency on actual edge devices to confirm the 4GB vs 24GB RAM and 0.2s vs 1.3s latency claims under realistic deployment conditions.
3. Conduct an ablation study specifically isolating the Multi-Head Attention contribution by replacing it with simple concatenation and measuring the exact performance delta reported in Table II.