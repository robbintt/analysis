---
ver: rpa2
title: 'Understanding Overadaptation in Supervised Fine-Tuning: The Role of Ensemble
  Methods'
arxiv_id: '2506.01901'
source_url: https://arxiv.org/abs/2506.01901
tags:
- exex
- fine-tuning
- ensemble
- have
- equation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the problem of overadaptation in supervised
  fine-tuning of language models, where fine-tuned models tend to forget knowledge
  from pretraining while potentially overfitting to noisy downstream data. The authors
  demonstrate that ensembling pretrained and fine-tuned models mitigates overadaptation
  by balancing bias (from insufficient fine-tuning) and variance (from overfitting),
  leading to improved performance on both fine-tuning and pretraining tasks.
---

# Understanding Overadaptation in Supervised Fine-Tuning: The Role of Ensemble Methods

## Quick Facts
- **arXiv ID**: 2506.01901
- **Source URL**: https://arxiv.org/abs/2506.01901
- **Reference count**: 40
- **Primary result**: Ensembling pretrained and fine-tuned models mitigates overadaptation by balancing bias and variance, improving performance on both fine-tuning and pretraining tasks.

## Executive Summary
This paper addresses the problem of overadaptation in supervised fine-tuning of language models, where fine-tuned models tend to forget knowledge from pretraining while potentially overfitting to noisy downstream data. The authors demonstrate that ensembling pretrained and fine-tuned models mitigates overadaptation by balancing bias (from insufficient fine-tuning) and variance (from overfitting), leading to improved performance on both fine-tuning and pretraining tasks. Through theoretical analysis in an over-parameterized linear regression setting and empirical validation on instruction-following tasks with Llama-3-8B, Qwen2-7B, and Gemma-2-9B, the paper shows that ensembling consistently outperforms fine-tuning alone across different regularization strategies while reducing catastrophic forgetting on benchmarks like MMLU and Commonsense-QA.

## Method Summary
The method involves fine-tuning pretrained language models on instruction-following datasets (Dolly) with various regularization strategies (Norm-Penalty or DiffNorm-Penalty), then creating ensembles through weight interpolation between the pretrained and fine-tuned models. The ensemble model is defined as θ̂τ = (1-τ)θ̂₁ + τθ̂, where θ̂₁ is the pretrained model and θ̂ is the fine-tuned model. Training uses batch_size=16, 1 epoch, Adam optimizer, 8 GPUs with FSDP, and hyperparameter search over learning rates, regularization strengths, and ensemble weights. Evaluation measures downstream performance on MT-Bench and knowledge retention on MMLU and Commonsense-QA.

## Key Results
- Ensembling pretrained and fine-tuned models achieves better MT-Bench scores than fine-tuning alone while mitigating forgetting on MMLU and Commonsense-QA benchmarks.
- The optimal ensemble weight τ is typically greater than 0.5, indicating that retaining more pretrained knowledge improves downstream performance.
- Norm-Penalty regularization consistently outperforms DiffNorm-Penalty on Dolly tasks, though the mechanism for this counter-intuitive result remains unexplained.
- Theoretical analysis proves that ensemble models achieve better bias-variance trade-offs than either pretrained or fine-tuned models alone in the over-parameterized linear setting.

## Why This Works (Mechanism)

### Mechanism 1: Bias-Variance Tradeoff Balancing via Weight Interpolation
Ensembling achieves better test error by balancing bias (insufficient adaptation) and variance (overfitting noise). The pretrained model has low variance but high bias on downstream tasks; fine-tuning reduces bias but introduces high variance. Weight interpolation recovers pretrained information while retaining fine-tuning gains, optimizing the combined error term. Break condition: If fine-tuning data is extremely clean, variance becomes negligible and ensemble benefits shrink.

### Mechanism 2: Forgetting Mitigation Through Information Recovery
Ensembling reduces catastrophic forgetting by reintroducing pretrained knowledge that was overwritten during fine-tuning. Fine-tuning moves parameters away from the pretrained point to minimize downstream loss, destroying shared knowledge components. The ensemble's (1-τ) coefficient directly preserves this shared component proportional to the interpolation weight. Break condition: If pretrained and fine-tuning tasks are orthogonal, ensemble provides minimal forgetting benefit beyond regularization.

### Mechanism 3: Overadaptation Correction Beyond Regularization
Ensembling provides more effective overfitting control than ridge regularization alone, particularly in overparameterized regimes. Ridge regression constrains parameter movement within the fine-tuning trajectory, while ensembling creates a qualitatively different estimator by combining two distinct optimization basins. The optimal ensemble weight exceeds what ridge regularization can achieve alone. Break condition: If fine-tuning is early-stopped optimally with perfect regularization tuning, the marginal benefit of ensembling decreases.

## Foundational Learning

- **Concept: Bias-Variance Decomposition**
  - Why needed here: The paper's entire theoretical framework rests on decomposing test error into bias (systematic underfitting) and variance (overfitting noise).
  - Quick check question: Given a model with high training accuracy but poor test accuracy, which component is likely dominant?

- **Concept: Ridge vs. Ridgeless Regression in Overparameterized Settings**
  - Why needed here: The paper maps "vanilla fine-tuning" to ridgeless estimation and "early stopping/regularized fine-tuning" to ridge regression.
  - Quick check question: In an overparameterized linear model (p ≫ n), why does the minimum-norm solution still overfit noisy labels?

- **Concept: Benign Overfitting**
  - Why needed here: The paper assumes pretrained models achieve "benign overfitting" (good generalization despite fitting training data), but fine-tuning operates in a "sparse" regime where overfitting is harmful.
  - Quick check question: Under what conditions does interpolating training data lead to good test performance despite zero training error?

## Architecture Onboarding

- **Component map**: Pretrained estimator (θ̂₁) -> Fine-tuned estimator (θ̂) -> Ensemble estimator (θ̂τ)

- **Critical path**:
  1. Verify pretrained model achieves reasonable Task 1 performance (benign overfitting condition)
  2. Fine-tune with careful regularization monitoring (avoid overadaptation regime)
  3. Sweep ensemble weight τ to find optimal bias-variance balance (paper suggests τ'(λ)/2 ≤ τ < 1 for forgetting mitigation)

- **Design tradeoffs**:
  - **τ (ensemble weight)**: Higher τ prioritizes downstream performance but increases forgetting; optimal τ'(λ) depends on noise level σ̃² and task divergence ζ₂
  - **λ (ridge penalty)**: Regularization strength; paper proves benefits for λ ≤ 2σ̃²/(nζ₂)
  - **Fine-tuning epochs**: Extended training without early stopping triggers overadaptation; Figure 1 shows MT-Bench scores degrade beyond epoch 1

- **Failure signatures**:
  - **Overadaptation**: Fine-tuned model underperforms pretrained model on pretraining tasks AND fails to exceed ensemble on downstream tasks
  - **Insufficient fine-tuning**: Large gap between fine-tuned and pretrained performance on downstream tasks (high bias regime)
  - **Catastrophic forgetting**: Sharp drop in MMLU/CommonsenseQA scores after fine-tuning

- **First 3 experiments**:
  1. Replicate overadaptation curve: Train vanilla fine-tuning on Dolly for 1-5 epochs; plot MT-Bench vs. epoch to confirm performance degradation in overfitting regime
  2. τ sweep with forgetting metrics: Fix fine-tuning (DiffNorm-Penalty), sweep τ ∈ {0.1, 0.3, ..., 0.9}; measure both MT-Bench and MMLU to identify Pareto frontier
  3. Regularization comparison: Compare Norm-Penalty vs. DiffNorm-Penalty vs. vanilla; verify paper's observation that Norm-Penalty consistently outperforms DiffNorm-Penalty

## Open Questions the Paper Calls Out

- **Open Question 1**: Can the theoretical framework developed for over-parameterized linear models be extended to non-linear neural network architectures while preserving the bias-variance trade-off insights for ensemble methods? The paper uses linear regression simplifications but notes limitations in applying to full neural network fine-tuning processes.

- **Open Question 2**: What are the optimal selection strategies for the ensemble coefficient τ and regularization parameter λ across different fine-tuning scenarios? While the theoretical analysis provides bounds, practical selection remains unclear with different regularizers showing inconsistent performance.

- **Open Question 3**: How do the theoretical bounds on ensemble effectiveness scale with model size, dataset size, and task complexity beyond the tested configurations? The experiments use specific model sizes (7B-9B parameters) and assume specific data distribution conditions that may not hold universally.

## Limitations

- The theoretical analysis relies on over-parameterized linear regression with spiked covariance structure, limiting generalizability to actual neural networks.
- The paper doesn't explain why Norm-Penalty regularization outperforms DiffNorm-Penalty on Dolly tasks, which is counter-intuitive.
- Validation methodology lacks transparency with no details provided about the 600 validation samples used for hyperparameter selection.

## Confidence

- **High confidence**: The core empirical observation that ensembling mitigates catastrophic forgetting while improving downstream performance is well-supported across three model families and multiple regularization strategies.
- **Medium confidence**: The theoretical framework provides useful intuition about bias-variance trade-offs, but the mapping from linear regression to deep learning practice involves assumptions that aren't empirically validated.
- **Low confidence**: The specific mechanism by which Norm-Penalty outperforms DiffNorm-Penalty on Dolly tasks remains unexplained, making it unclear whether this finding generalizes.

## Next Checks

1. Conduct a systematic sweep of τ values (0.1 to 0.9) for each regularization strategy and compare empirically optimal weights against theoretical predictions to test framework accuracy.

2. Replicate the Norm-Penalty vs. DiffNorm-Penalty comparison on alternative instruction-following datasets (e.g., Alpaca, ShareGPT) to determine whether the performance difference is dataset-specific.

3. Perform a detailed ablation study measuring knowledge retention across different domains (MMLU subjects, Commonsense-QA categories) to identify which types of pretrained knowledge are most vulnerable to overadaptation.