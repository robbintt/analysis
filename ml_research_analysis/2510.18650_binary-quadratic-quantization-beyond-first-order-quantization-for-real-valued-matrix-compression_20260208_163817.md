---
ver: rpa2
title: 'Binary Quadratic Quantization: Beyond First-Order Quantization for Real-Valued
  Matrix Compression'
arxiv_id: '2510.18650'
source_url: https://arxiv.org/abs/2510.18650
tags:
- quantization
- binary
- matrix
- matrices
- error
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Binary Quadratic Quantization (BQQ), a novel
  matrix quantization framework that approximates real-valued matrices as linear combinations
  of binary matrix products, going beyond conventional first-order quantization methods.
  Unlike traditional approaches such as uniform quantization or binary coding quantization,
  which rely on linear combinations of binary bases, BQQ leverages the expressive
  power of binary quadratic expressions while maintaining an extremely compact data
  format.
---

# Binary Quadratic Quantization: Beyond First-Order Quantization for Real-Valued Matrix Compression

## Quick Facts
- arXiv ID: 2510.18650
- Source URL: https://arxiv.org/abs/2510.18650
- Reference count: 40
- Primary result: BQQ achieves state-of-the-art 2-bit post-training quantization on ImageNet, outperforming existing methods by up to 2.2% and 59.1% under calibration-based and data-free scenarios.

## Executive Summary
This paper introduces Binary Quadratic Quantization (BQQ), a novel matrix quantization framework that approximates real-valued matrices as linear combinations of binary matrix products. Unlike traditional approaches such as uniform quantization or binary coding quantization, which rely on linear combinations of binary bases, BQQ leverages the expressive power of binary quadratic expressions while maintaining an extremely compact data format. To handle the resulting NP-hard optimization problem, the authors develop an efficient solution based on polynomial unconstrained binary optimization (PUBO) and convex quadratic programming. Experimental results demonstrate that BQQ consistently achieves a superior trade-off between memory efficiency and reconstruction error across diverse matrix datasets. Notably, when applied to post-training quantization of Vision Transformer-based models, BQQ delivers state-of-the-art performance, outperforming existing methods by up to 2.2% and 59.1% on the ImageNet dataset under calibration-based and data-free scenarios, respectively, at a quantization level equivalent to 2 bits. These findings highlight the effectiveness of binary quadratic expressions for efficient matrix approximation and neural network compression, offering a new perspective on extreme matrix compression.

## Method Summary
BQQ represents a real-valued matrix $X$ as a linear combination of binary matrix products using an intermediate dimension $l$ and $p$ binary matrix stacks. The optimization alternates between (1) finding binary matrices $Y_i, Z_i, W_i$ using Annealed Mean Field Descent (AMFD), a gradient-based method for binary optimization that approximates the entropy term with a 2nd-order Taylor expansion to avoid numerical instability, and (2) optimizing real-valued scaling factors via convex quadratic programming. For post-training quantization, the method is applied to linear layers of Vision Transformers in a group-wise fashion, followed by a lightweight bias correction step. The approach handles the NP-hard nature of the problem by decoupling it into tractable subproblems while maintaining high reconstruction accuracy and competitive accuracy for compressed models.

## Key Results
- BQQ consistently achieves lower reconstruction error than first-order quantization methods across diverse matrix datasets including Gaussian random matrices, TSPLIB distances, SIFT features, and ImageNet image patches.
- For 2-bit post-training quantization of DeiT and Swin Transformers on ImageNet, BQQ outperforms existing methods by up to 2.2% and 59.1% under calibration-based and data-free scenarios, respectively.
- The method demonstrates superior memory efficiency while maintaining accuracy, making it particularly effective for extreme matrix compression scenarios.

## Why This Works (Mechanism)
BQQ works by leveraging binary quadratic expressions instead of linear combinations, allowing for more expressive power in matrix approximation while maintaining the same memory footprint as first-order methods. The quadratic terms capture interactions between binary variables that linear methods cannot represent, enabling better approximation of matrices with skewed singular values. The AMFD algorithm efficiently handles the binary optimization by approximating the entropy term with a Taylor expansion, making the optimization tractable while avoiding numerical instability issues common in binary optimization.

## Foundational Learning
- **Binary Quadratic Expressions**: Mathematical formulations involving products of binary variables that enable more expressive power than linear combinations.
  - *Why needed*: To capture complex matrix structures that linear methods cannot represent effectively.
  - *Quick check*: Verify that the quadratic terms in Equation (2) correctly expand to products of binary variables.

- **Polynomial Unconstrained Binary Optimization (PUBO)**: Framework for solving optimization problems with polynomial objectives over binary variables.
  - *Why needed*: To formulate the BQQ objective as a tractable optimization problem.
  - *Quick check*: Confirm that the objective function in Equation (9) is indeed quadratic in the binary variables.

- **Annealed Mean Field Descent (AMFD)**: Gradient-based algorithm for binary optimization that uses mean-field approximation and temperature annealing.
  - *Why needed*: To efficiently solve the binary optimization subproblems while avoiding local minima.
  - *Quick check*: Monitor the temperature schedule and ensure it decreases from 0.2 to 0.005 over 50,000 steps.

- **Convex Quadratic Programming**: Optimization technique for minimizing convex quadratic objectives with linear constraints.
  - *Why needed*: To efficiently solve for scaling factors once binary matrices are fixed.
  - *Quick check*: Verify that the Hessian matrix in Equation (10) is positive definite for the convex optimization to be valid.

- **Group-wise Quantization**: Strategy of partitioning large matrices into smaller submatrices for independent quantization.
  - *Why needed*: To handle memory constraints and enable parallel processing of large model layers.
  - *Quick check*: Confirm that submatrix sizes (384×384 for DeiT, 96×96 for Swin) are correctly implemented.

- **Taylor Expansion for Entropy**: Mathematical technique to approximate logarithmic entropy terms and prevent numerical overflow.
  - *Why needed*: To maintain numerical stability in the AMFD algorithm when computing entropy of binary distributions.
  - *Quick check*: Validate that the Taylor expansion coefficients correctly approximate the entropy term near x=0.

## Architecture Onboarding

**Component Map**
Input Matrix → BQQ Formulation → AMFD Binary Optimization → Convex QP Scaling Factors → Reconstructed Matrix

**Critical Path**
The critical path for BQQ implementation follows: (1) Initialize random binary matrices and scaling factors, (2) Run AMFD for 50,000 steps with temperature annealing, (3) Solve convex quadratic program for scaling factors, (4) Iterate greedy optimization across all stacks until convergence.

**Design Tradeoffs**
- **Intermediate dimension l**: Fixed as round(mn/(m+n)) for fair comparison, but optimal value may vary by matrix characteristics.
- **Stack count p**: Fixed at 2 for experiments, but joint optimization with l could improve performance.
- **Temperature schedule**: Fixed annealing from 0.2 to 0.005, but could be adaptive for better convergence.
- **Group size**: Fixed at 384×384 (DeiT) or 96×96 (Swin), balancing memory efficiency and approximation quality.

**Failure Signatures**
- **NaN values**: Typically occur in entropy calculation when x values approach 0 or 1 without proper clipping.
- **Poor convergence**: Indicated by plateauing loss or increasing MSE, often due to suboptimal learning rate or temperature schedule.
- **High reconstruction error**: May result from insufficient intermediate dimension or suboptimal binary matrix initialization.

**First Experiments**
1. **Validate AMFD Implementation**: Implement Algorithm 1 with 2nd-order Taylor expansion for entropy term. Test on a small synthetic matrix (e.g., 8×8) to verify binary matrix updates reduce MSE and entropy calculation remains stable.
2. **Test Subproblem Solver**: Integrate Algorithm 2 by combining AMFD loop with closed-form scaling factor optimization (Equation 10). Run on single DeiT-S linear layer to confirm MSE reduction and check initialization strategy for scaling factors.
3. **Full PTQ Evaluation**: Apply complete BQQ pipeline (Algorithm 3) to DeiT-S model using 1024 ImageNet samples. Measure Top-1 accuracy on ImageNet validation set and compare against first-order quantization baselines at 2 bits.

## Open Questions the Paper Calls Out

**Open Question 1**
- Question: Can a joint optimization strategy for all binary matrices and scaling factors achieve lower reconstruction errors than the proposed greedy approach?
- Basis in paper: [explicit] The authors state in Section 6 that the current greedy optimization strategy is "suboptimal from a global perspective" and that "Jointly optimizing all binary matrices and scaling factors could potentially lead to further reductions in quantization error."
- Why unresolved: The optimization problem is NP-hard, so the authors decoupled it into iterative subproblems to make the solution tractable.
- What evidence would resolve it: An algorithm capable of simultaneously optimizing variables across all stacks (i) that yields a lower Mean Squared Error (MSE) on the matrix compression benchmarks than the greedy method.

**Open Question 2**
- Question: Does adapting BQQ to minimize activation/output error directly, rather than weight reconstruction error, significantly improve post-training quantization (PTQ) accuracy?
- Basis in paper: [explicit] Section 6 notes that "it is generally more effective to minimize output quantization error" and suggests that "Adapting BQQ to optimize binary matrices with respect to output error could therefore lead to even greater PTQ accuracy."
- Why unresolved: The current method optimizes weights based on reconstruction loss, relying on a lightweight bias correction step to recover task performance, rather than integrating task-specific loss signals into the binary optimization.
- What evidence would resolve it: A modified BQQ framework that minimizes the difference in layer outputs (using calibration data) during the binary matrix search, resulting in higher ImageNet accuracy than the reported weight-reconstruction baseline.

**Open Question 3**
- Question: What is the optimal dynamic allocation ratio between the intermediate dimension (l) and the number of binary matrix stacks (p) for a fixed parameter budget?
- Basis in paper: [explicit] Section 6 and Appendix A.9 state, "In our experiments, the intermediate dimension is fixed... However, this configuration may not be optimal... Exploring the optimal ratio... could further improve approximation error."
- Why unresolved: The paper fixes the dimension l based on matrix size (round(mn/(m+n))) to ensure a fair comparison with baselines, rather than tuning the trade-off between rank (l) and ensemble size (p).
- What evidence would resolve it: A study identifying specific matrix characteristics (e.g., spectral decay rates) that dictate whether increasing the stack count (p) is more beneficial than increasing the intermediate dimension (l).

**Open Question 4**
- Question: Under what specific theoretical conditions does BQQ guarantee superior approximation capabilities compared to first-order quantization?
- Basis in paper: [explicit] Section 6 mentions that "While an upper bound on the approximation error is provided... it does not yet establish a theoretical guarantee that our method outperforms first-order quantization under specific conditions."
- Why unresolved: While the paper empirically shows better performance on matrices with skewed singular values, it lacks a formal theorem proving that the quadratic expression strictly outperforms linear combinations under those spectral constraints.
- What evidence would resolve it: A mathematical proof defining the strict bounds on singular value distributions where the BQQ error upper bound is mathematically proven to be lower than that of Binary Coding Quantization (BCQ).

## Limitations
- The optimization is sensitive to hyperparameters (temperature schedule, learning rate), and poor choices could lead to suboptimal solutions or convergence issues.
- The experimental results are based on specific model architectures (DeiT, Swin) and dataset configurations (ImageNet with 1024–2048 samples), which may limit generalizability to other domains or models.
- The paper does not provide explicit coefficients for the 2nd-order Taylor expansion of the entropy term, introducing uncertainty in achieving identical results.

## Confidence

**High Confidence**: The core theoretical framework (BQQ formulation as linear combinations of binary matrix products) and the experimental superiority over first-order quantization methods (e.g., 2.2% and 59.1% gains on ImageNet) are well-supported by the methodology and results.

**Medium Confidence**: The practical implementation details (e.g., Taylor expansion coefficients, initialization strategies) are not fully specified, introducing uncertainty in achieving identical results.

**Low Confidence**: The robustness of BQQ to different model architectures, dataset sizes, or quantization levels beyond 2 bits is not thoroughly explored.

## Next Checks

1. **Validate AMFD Implementation**: Implement Algorithm 1 with the 2nd-order Taylor expansion for the entropy term. Test on a small synthetic matrix (e.g., 8×8) to verify that the binary matrix updates reduce MSE and that the entropy calculation remains numerically stable (no NaNs).

2. **Test Subproblem Solver**: Integrate Algorithm 2 by combining the AMFD loop with the closed-form scaling factor optimization (Equation 10). Run on a single DeiT-S linear layer to confirm MSE reduction and check the initialization strategy for r, s, t, u.

3. **Full PTQ Evaluation**: Apply the complete BQQ pipeline (Algorithm 3) to a DeiT-S model using 1024 ImageNet samples. Measure Top-1 accuracy on the ImageNet validation set and compare against first-order quantization baselines (e.g., uniform quantization at 2 bits) to verify the claimed 2.2% improvement.