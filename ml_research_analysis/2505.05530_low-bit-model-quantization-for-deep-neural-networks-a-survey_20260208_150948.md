---
ver: rpa2
title: 'Low-bit Model Quantization for Deep Neural Networks: A Survey'
arxiv_id: '2505.05530'
source_url: https://arxiv.org/abs/2505.05530
tags:
- quantization
- proc
- conf
- neural
- comput
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey comprehensively reviews low-bit model quantization
  for deep neural networks over the past five years, classifying 179 papers into eight
  main categories and 24 subcategories based on their core techniques. The paper addresses
  the critical challenge of balancing model compression with accuracy preservation
  in resource-constrained environments.
---

# Low-bit Model Quantization for Deep Neural Networks: A Survey

## Quick Facts
- **arXiv ID:** 2505.05530
- **Source URL:** https://arxiv.org/abs/2505.05530
- **Reference count:** 40
- **Key outcome:** Comprehensive survey classifying 179 low-bit quantization papers into eight main categories based on core techniques

## Executive Summary
This survey provides a systematic review of low-bit model quantization techniques for deep neural networks over the past five years. The authors analyze and categorize 179 papers into eight main technique families with 24 subcategories, covering fundamental methods like improved scale factor selection and zero-point optimization, as well as advanced approaches including mixed-precision allocation, outlier redistribution, and specialized techniques for diffusion models. The paper addresses the critical challenge of balancing model compression with accuracy preservation, particularly for resource-constrained environments. It also identifies future research directions such as multimodal task applications, integration with other compression methods, and software-hardware co-optimization.

## Method Summary
The survey employs a comprehensive taxonomy approach, systematically reviewing 179 quantization papers published between 2020-2024. Methods are classified based on their core techniques into eight main categories: better scale factor and zero-point selection, mixed-precision allocation, redistribution techniques for outliers, data-free quantization, advanced quantization formats, specialized diffusion model approaches, and combinations with other compression methods. The fundamental quantization operator Q(Â·) is defined using bit-width b, scale s, and zero-point z. The survey covers both post-training quantization (PTQ) and quantization-aware training (QAT) approaches, with detailed analysis of calibration strategies including Min-Max, MSE, and Percentile methods. The paper provides theoretical foundations and practical considerations for each technique category.

## Key Results
- Comprehensive taxonomy of 179 quantization papers organized into 8 main categories and 24 subcategories
- Detailed analysis of fundamental trade-offs between quantization accuracy and computational efficiency
- Identification of critical challenges in transformer quantization, particularly outlier handling and mixed-precision allocation
- Discussion of specialized techniques for emerging model architectures including diffusion models
- Future research directions emphasizing multimodal applications and hardware-aware optimization

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Quantization accelerates inference primarily by reducing memory bandwidth bottlenecks and enabling low-cost integer arithmetic, provided hardware supports the target bit-width.
- **Mechanism:** The conversion from continuous floating-point numbers (e.g., FP32) to discrete integers (e.g., INT8 or INT4) reduces the memory footprint. This allows faster Memory I/O and leverages vectorized computing instructions (e.g., processing 64 INT8 elements in the same register space as 16 FP32 elements).
- **Core assumption:** The target hardware has specialized logic for integer operations (SIMD) and that the overhead of the quantization/de-quantization steps (Eq. 1 and 2) is lower than the speedup gained from reduced memory access.
- **Evidence anchors:**
  - [abstract] "The essence of quantization acceleration is the conversion... which significantly speeds up the memory I/O and calculation."
  - [supplement] "Memory Access Acceleration... Quantization alleviates memory bottlenecks... Vectorization... a 512-bit register natively processes 16 operands of FP32. With INT8... 64-element parallel execution."
  - [corpus] Related work (Mixed-Precision Quantization for LMs) supports the necessity of this mechanism for "alleviating memory bottlenecks."
- **Break condition:** If the hardware lacks INT4/INT8 acceleration kernels (requiring de-quantization to FP16 for computation), latency gains may be negated by the conversion overhead.

### Mechanism 2
- **Claim:** Redistribution techniques mitigate accuracy loss in transformers by mathematically transferring "quantization difficulty" from outlier-heavy activations to smoother weight distributions.
- **Mechanism:** LLMs and ViTs often exhibit extreme outliers in activation channels (e.g., from LayerNorm), which breaks standard quantization grids. Methods like SmoothQuant (Section 3.4.3) apply an invertible diagonal matrix to scale activations down and weights up (or vice versa), smoothing the activation distribution while maintaining the mathematical equivalence of the linear layer output ($Y = XW^T = (XH)(H^TW^T)$).
- **Core assumption:** Weights have a more uniform distribution than activations and can absorb the increased magnitude variance without saturating the quantization grid.
- **Evidence anchors:**
  - [abstract] Mentions "redistribution techniques for handling outliers."
  - [section 3.4.3] "SmoothQuant addresses this by scaling activations and weights channel by channel... balancing outliers between the two."
  - [corpus] (SplitQuant) discusses layer splitting as another approach to handle structural outliers, validating the "structural difficulty" assumption.
- **Break condition:** If weights are already highly irregular or sparse, pushing outliers to them (or rotating them via Hadamard matrices) may fail if the resulting matrix becomes unrepresentable in low bits.

### Mechanism 3
- **Claim:** Mixed-precision allocation preserves model capacity by assigning higher bit-widths to layers with high sensitivity (measured by Hessian spectra) and lower bits to robust layers.
- **Mechanism:** Instead of a uniform bit-width, methods like HAWQ-V3 (Section 3.3.3) analyze the top eigenvalues of the Hessian matrix (curvature of the loss landscape). Layers with high curvature (sharp minima) are sensitive to noise and are assigned higher precision (e.g., INT8), while flatter layers get lower precision (e.g., INT4).
- **Core assumption:** The Hessian trace (or similar metric) correlates strongly with the layer's impact on final task loss, and the search space can be constrained (e.g., via Integer Linear Programming) efficiently.
- **Evidence anchors:**
  - [section 3.3.1] "HAWQ focuses on the top Hessian eigenvalues... identifying sensitive weight columns."
  - [section 3.3.3] "HAWQ-V3... introduces constraints directly related to hardware resources."
  - [corpus] (MSQ: Memory-Efficient Bit Sparsification) confirms the ongoing trend of using sparsity/precision mix to balance efficiency and accuracy.
- **Break condition:** If inter-layer dependencies are ignored (optimizing layers independently), the cumulative error might diverge, or the overhead of dynamic bit-width switching might exceed the computational savings.

## Foundational Learning

- **Concept:** **Discretization Error (Rounding vs. Clipping)**
  - **Why needed here:** Section 2.2 explicitly details the trade-off. A wider quantization range increases rounding error (values fall into "bins"), while a narrower range increases clipping error (values are cut off).
  - **Quick check question:** When quantizing a tensor with a massive outlier, does Min-Max quantization increase clipping error or rounding error? (Answer: Rounding error, because the scale factor $s$ becomes too large to capture small variations).

- **Concept:** **Straight-Through Estimator (STE)**
  - **Why needed here:** Section 2.3 and 3.2.1 identify STE as the standard method for backpropagating through the non-differentiable "round" function. It assumes the gradient passes through the rounding operation unchanged ($\frac{\partial \text{Round}(x)}{\partial x} \approx 1$).
  - **Quick check question:** If a weight is oscillating between two quantization bins during training, what mechanism failure does this indicate regarding the STE? (Answer: Gradient mismatch/Misconvergence at boundaries).

- **Concept:** **Outlier Distribution in Transformers**
  - **Why needed here:** Understanding *why* standard quantization fails for LLMs (Section 3.4.3) is prerequisite to understanding redistribution. Outliers are not random noise; they are systematic features often tied to specific token channels or attention heads.
  - **Quick check question:** In a Vision Transformer (ViT), why might channel-wise quantization fail where layer-wise succeeds? (Answer: Magnitude/Correlation variance across channels, as shown in Fig 3).

## Architecture Onboarding

- **Component map:** Calibration Set -> Quantizer Operator ($Q(\cdot)$) -> Optimizer (PTQ vs. QAT)
- **Critical path:** The deployment pipeline starts with **Calibration** (feeding data to get statistics) $\to$ **Range Setting** (MSE/Percentile/Min-Max) $\to$ **Advanced Correction** (AdaRound/SmoothQuant) $\to$ **Verification**
- **Design tradeoffs:**
  - **Symmetric vs. Asymmetric:** Symmetric ($z=0$) is faster (no zero-point offset addition) but wasteful for ReLU outputs (all positive). Asymmetric is more accurate but computationally slightly heavier.
  - **Per-Tensor vs. Per-Channel:** Per-channel handles CNN weight variance better but increases memory overhead for storing multiple scale factors
- **Failure signatures:**
  - **Oscillation:** Weights jumping between bins in QAT (Section 3.2.1). *Fix:* Hysteresis quantization or freezing unstable weights
  - **Sawtooth Gradients:** Zig-zagging in loss landscapes (Section 3.2.1). *Fix:* One-step forward search (BLAQ)
  - **Accuracy Collapse in Diffusion:** Error accumulation over timesteps (Section 3.7). *Fix:* Timestep-aware calibration or specialized noise correction
- **First 3 experiments:**
  1. **Baseline Stability:** Implement Min-Max and MSE PTQ on a ResNet-18. Compare the "clipping error" vs "rounding error" trade-off by visualizing the activation histograms against the calculated $[q_{min}, q_{max}]$.
  2. **Outlier Sensitivity:** Take a small Transformer (e.g., DistilBERT) and quantize activations. Observe the accuracy drop. Apply a simple log-transform or channel-scaling (SmoothQuant style) to the activation tensors before quantization to test if "flattening" the distribution improves INT8 recovery.
  3. **Rounding Search:** Implement a naive "Round-to-Nearest" vs. "AdaRound" (optimizing rounding up vs. down based on local loss) on a single linear layer to demonstrate how small perturbations in rounding direction can minimize reconstruction error (Section 3.4.4).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can quantization techniques be refined to specifically address the distinct sensitivities of different modalities (e.g., text vs. visual generation) within large-scale multimodal models?
- Basis in paper: [explicit] Section 4.1 states that future research must focus on refining quantization for multimodal tasks (text-to-image, text-to-video) which integrate Vision Transformers and Large Language Models.
- Why unresolved: Multimodal models rely on large-scale integration of distinct architectures (ViTs and LLMs), making unified quantization strategies difficult due to varying sensitivity to precision loss across modalities.
- What evidence would resolve it: A quantization framework that maintains accuracy across distinct modalities in models like LLaVA or Stable Diffusion while reducing computational costs on resource-constrained devices.

### Open Question 2
- Question: How can software-hardware co-design be utilized to optimize non-uniform or mixed-precision quantization on edge accelerators (NPUs, FPGAs) given memory bandwidth constraints?
- Basis in paper: [explicit] Section 4.3 highlights the need for integrated toolchains that co-design quantized neural architectures with execution plans for specific hardware.
- Why unresolved: Section 2.3 notes that non-uniform quantization is difficult to accelerate on standard hardware (GPUs/CPUs), creating a gap between theoretical compression gains and practical speedups on specialized edge devices.
- What evidence would resolve it: A unified toolchain demonstrating improved energy efficiency and latency on a specific NPU or FPGA using a hardware-aware mixed-precision quantization strategy.

### Open Question 3
- Question: What are the optimal algorithms for jointly combining quantization with pruning and low-rank decomposition to achieve aggressive compression with minimal performance loss?
- Basis in paper: [explicit] Section 4.2 identifies the combination of quantization with other compression techniques as a necessary future direction to address deployment challenges.
- Why unresolved: Quantization alone often falls short of deployment requirements, but existing hybrid methods (Section 3.8) face conflicts when combining structured sparsity with low-precision constraints.
- What evidence would resolve it: A joint optimization framework that exceeds the Pareto frontier of current single-method approaches by effectively resolving the conflict between pruning masks and quantization grids.

## Limitations
- Direct performance comparisons across surveyed methods are inherently limited due to heterogeneous experimental setups (different models, datasets, and hardware configurations)
- Practical implementation details and hyperparameter sensitivities for individual methods are not fully specified in the survey text
- Some cutting-edge techniques, particularly diffusion model quantization approaches, have limited independent verification due to their novelty

## Confidence
- **High Confidence:** Core quantization formulation (scale/zero-point framework), basic calibration methods (Min-Max, MSE, Percentile), and fundamental mechanisms (STE, quantization error trade-offs)
- **Medium Confidence:** Advanced techniques like SmoothQuant and HAWQ-V3, as these build on well-established principles with consistent experimental validation across multiple papers
- **Low Confidence:** Diffusion model quantization approaches and some cutting-edge techniques due to limited independent verification and the novelty of these methods

## Next Checks
1. **Cross-method benchmarking:** Implement 2-3 representative methods from different categories (e.g., Min-Max PTQ, SmoothQuant, and HAWQ-V3) on the same model/dataset to empirically validate the claimed accuracy-efficiency trade-offs
2. **Outlier sensitivity test:** Systematically vary activation outlier magnitudes in transformers to quantify the impact on standard vs. redistribution-based quantization methods
3. **Hardware-aware validation:** Test mixed-precision allocation on target inference hardware to verify that theoretical computational savings translate to actual latency improvements, accounting for bit-width switching overhead