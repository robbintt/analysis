---
ver: rpa2
title: Jackpot! Alignment as a Maximal Lottery
arxiv_id: '2501.19266'
source_url: https://arxiv.org/abs/2501.19266
tags:
- social
- choice
- maximal
- rlhf
- preferences
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes maximal lotteries, a probabilistic social choice
  function, as a superior alternative to RLHF for aligning LLMs with human values.
  While RLHF methods based on the Bradley-Terry model fail to satisfy key properties
  like majority consistency, Condorcet consistency, and independence of irrelevant
  alternatives, maximal lotteries inherently satisfy these desirable properties.
---

# Jackpot! Alignment as a Maximal Lottery

## Quick Facts
- arXiv ID: 2501.19266
- Source URL: https://arxiv.org/abs/2501.19266
- Authors: Roberto-Rafael Maura-Rivero; Marc Lanctot; Francesco Visin; Kate Larson
- Reference count: 40
- Key outcome: Maximal lotteries, a probabilistic social choice function, provide a principled alternative to RLHF for LLM alignment that satisfies majority consistency, Condorcet consistency, and IIA.

## Executive Summary
This paper proposes maximal lotteries, a probabilistic social choice function, as a superior alternative to RLHF for aligning LLMs with human values. While RLHF methods based on the Bradley-Terry model fail to satisfy key properties like majority consistency, Condorcet consistency, and independence of irrelevant alternatives, maximal lotteries inherently satisfy these desirable properties. The authors formally prove that game-theoretic approaches like Nash Learning from Human Feedback (NLHF) and its variants approximate maximal lottery outcomes. Experimental results on synthetic datasets confirm that maximal lottery-inspired methods produce LLM outputs that better reflect aggregate human preferences compared to standard RLHF.

## Method Summary
The method trains LLMs to align with human preferences using maximal lotteries instead of traditional RLHF. Three copies of Gemma 2 2B are used: one pretrained, one RLHF-trained with PPO and a BTL reward model, and one trained with Self-Play Preference Optimization (SPO) approximating maximal lotteries. The SPO method implements a minimax objective over policies, treating the problem as a zero-sum game where the policy must perform well against all possible alternatives. Synthetic preference datasets with 2048 triplets are generated with predefined population preferences over three alternatives. The SPO algorithm trains the policy to maximize the minimum expected preference score across all alternatives, which corresponds to the maximal lottery equilibrium.

## Key Results
- RLHF with Bradley-Terry models implicitly implements Borda scoring, which can fail to elect majority or Condorcet winners
- Maximal lotteries satisfy majority consistency, Condorcet consistency, and a probabilistic form of IIA
- SPO experiments show that maximal lottery-inspired methods correctly identify majority winners and handle cyclic preferences with uniform distributions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Standard RLHF based on the Bradley-Terry model implicitly implements the Borda scoring rule, which fails to be majority-consistent or Condorcet-consistent.
- Mechanism: The Bradley-Terry model's maximum-likelihood solution orders alternatives by their summed pairwise win probabilities, which is mathematically equivalent to Borda count; Borda can elect a non-majority winner when a Condorcet winner exists.
- Core assumption: The paper's formal proof (Theorem 3.1, replicated in Appendix A.9) holds that, in the limit of infinite data, BTL reward ranking matches Borda ordering.
- Evidence anchors:
  - [abstract] "While RLHF methods based on the Bradley-Terry model fail to satisfy key properties like majority consistency, Condorcet consistency..."
  - [section] Section 3.1 and Appendix A.9 formally show RLHF implements Borda; Section 4 demonstrates violations with concrete examples.
  - [corpus] Corpus does not provide additional empirical refutation of RLHF's Borda equivalence.
- Break condition: If pairwise comparisons are not aggregated via BTL/Borda, or if strong KL regularization heavily anchors the policy to the pretrained distribution (reducing the effect of reward), the mapping to Borda may not hold.

### Mechanism 2
- Claim: Maximal lotteries, a probabilistic social choice function, satisfy majority consistency, Condorcet consistency, and a probabilistic form of independence of irrelevant alternatives (IIA).
- Mechanism: The maximal lottery is the mixed-strategy Nash equilibrium of a symmetric zero-sum margin game defined by the pairwise margin matrix M; it places probability 1 on a Condorcet winner if one exists and otherwise spreads probability over the top cycle/Smith set.
- Core assumption: The margin matrix M accurately reflects population preferences, and the equilibrium can be approximated by optimization.
- Evidence anchors:
  - [abstract] "...maximal lotteries inherently satisfy these desirable properties."
  - [section] Section 5.1 defines maximal lotteries via πᵀMπ′ ≥ 0 for all π′; Section 5.2 cites Brandl & Brandt (2020) for uniqueness and IIA properties; Appendix A.3 elaborates.
  - [corpus] Corpus does not provide direct external verification of these properties.
- Break condition: If preferences are not well-represented by pairwise margins (e.g., strong context effects beyond pairwise comparisons), or if optimization fails to find the equilibrium, properties may not hold.

### Mechanism 3
- Claim: Nash Learning from Human Feedback (NLHF) and variants like Self-Play Preference Optimization (SPO) approximate maximal lottery outcomes, inheriting their desirable social-choice properties.
- Mechanism: NLHF's minimax objective over policies against an adversarial policy mirrors the equilibrium condition of the margin game; with an indifference-aware correction (½P(a∼b)), the NLHF objective exactly matches the maximal lottery objective per Theorem 1 and Corollary 1.1.
- Core assumption: Indifferent individuals choose randomly between options in pairwise comparisons; selection probabilities can substitute for true preference probabilities.
- Evidence anchors:
  - [abstract] "The authors formally prove that game-theoretic approaches like Nash Learning from Human Feedback (NLHF) and its variants approximate maximal lottery outcomes."
  - [section] Section 5.3 derives the equivalence; Corollary 1.1 links selection probabilities to the maximal lottery.
  - [corpus] Corpus neighbors (e.g., Munos et al., 2023; Calandriello et al., 2024) report empirical NLHF advantages but do not independently verify the maximal-lottery equivalence.
- Break condition: If position bias or other systematic biases affect selection probabilities without correction, or if the algorithm fails to converge to the minimax equilibrium, the approximation may degrade.

## Foundational Learning

- Concept: Social Choice Theory and Arrow's Impossibility
  - Why needed here: To understand why no deterministic voting rule can satisfy all desirable axioms, motivating probabilistic rules like maximal lotteries.
  - Quick check question: Can you explain why Borda is not Condorcet-consistent and why that matters for LLM alignment?

- Concept: Zero-Sum Games and Mixed-Strategy Nash Equilibrium
  - Why needed here: Maximal lotteries are computed as equilibria of a margin game; grasping maximin/minimax is essential for NLHF.
  - Quick check question: How would you compute a mixed-strategy Nash equilibrium for a simple 3x3 zero-sum game given a payoff matrix?

- Concept: Pairwise Preference Models (Bradley-Terry vs. Margin-Based)
  - Why needed here: RLHF typically uses BTL reward models; maximal-lottery methods use margin matrices; understanding the difference is critical.
  - Quick check question: What is the relationship between BTL win probabilities and pairwise preference margins?

## Architecture Onboarding

- Component map: Preference Data -> Margin Matrix M -> Maximal Lottery Solver -> NLHF/SPO Policy Optimizer -> Aligned LLM
- Critical path:
  1. Collect and preprocess pairwise preference data; randomize order to mitigate position bias.
  2. Train or define the preference/selection function P̃(a≻b) or compute margin matrix M per prompt class.
  3. Run NLHF/SPO training (minimax optimization) to convergence; monitor equilibrium convergence metrics.
  4. Evaluate on held-out preference profiles for majority/Condorcet/IIA compliance.
- Design tradeoffs:
  - Computational overhead: NLHF minimax optimization is more complex than single-agent PPO on a reward model.
  - Data requirements: Pairwise data is still sufficient; no need for full rankings, but noise and bias must be managed.
  - Regularization: Strong KL regularization toward the pretrained model may dilute social-choice properties.
- Failure signatures:
  - Mode collapse to a single option when cycles exist (expected under RLHF/Borda; should not happen under maximal lotteries).
  - Sensitivity to addition of irrelevant alternatives (RLHF may flip rankings; maximal lotteries should not).
  - Non-convergence or oscillation in NLHF training indicating equilibrium not reached.
- First 3 experiments:
  1. Replicate the synthetic majority/Condorcet scenario (Figure 1) to verify RLHF selects R while NLHF/SPO selects B.
  2. Run the IIA experiment with and without an irrelevant alternative G to confirm RLHF violates IIA and maximal-lottery methods do not.
  3. Test on a cyclic-preference dataset (Rock–Paper–Scissors style) to ensure NLHF/SPO produces a near-uniform distribution over the cycle.

## Open Questions the Paper Calls Out

- Question: How can maximal lottery-based alignment methods incorporate contextual information to adjust for the varying appropriateness of responses?
  - Basis: [explicit] Section 8 states that "appropriateness of a response can vary significantly depending on the context" (e.g., comedy vs. professional setting) and urges the development of strategies to enhance context awareness.
  - Why unresolved: The current formulation relies on a prompt $x$ and alternatives $Y$ but does not explicitly model the external social or situational context in which the conversation occurs.
  - What evidence would resolve it: A modified framework where the resulting policy distribution shifts correctly based on explicit context tags, maintaining maximal lottery properties while adhering to context-specific norms.

- Question: Can an online voting mechanism be developed to approximate maximal lotteries that adapts to evolving societal preferences without succumbing to instability or exploitation?
  - Basis: [explicit] Section 8 identifies the development of an "online version of our approach" as a critical avenue for future work to prevent static models from becoming outdated.
  - Why unresolved: Human values evolve over time, but dynamic updating introduces risks of instability and potential exploitation by adversarial actors, which current static approaches avoid.
  - What evidence would resolve it: An algorithm that successfully integrates streaming preference data to update the policy in real-time while proving robust against reward hacking or drift.

- Question: Are there superior alternatives to pairwise comparisons for inferring true preferences that mitigate the ambiguity of indifference?
  - Basis: [explicit] Section 8 explicitly asks, "Are there better ways to infer preferences?" and questions the realism of estimating preferences solely by showing users pairs of sentences.
  - Why unresolved: Current methods rely on selection probability $\tilde{P}$ which conflates strict preference with indifference, potentially biasing the margin matrix $M$.
  - What evidence would resolve it: A comparative study demonstrating that alternative elicitation methods (e.g., rankings or coarse ratings) provide a more accurate estimation of the underlying utility $P(a \succ b)$ than binary selections.

## Limitations

- The theoretical equivalence between RLHF with BTL and Borda scoring may not hold in practice due to finite data, regularization, or implementation details.
- Experimental validation is limited to synthetic datasets with only 2048 samples, which may not capture the complexity of real-world preference aggregation.
- The computational overhead of minimax optimization in NLHF/SPO versus standard RLHF is acknowledged but not quantified.

## Confidence

- **High confidence**: The formal proofs connecting Borda scoring to BTL-based RLHF (Theorem 3.1), and the theoretical equivalence between NLHF with indifference correction and maximal lotteries (Theorem 1, Corollary 1.1).
- **Medium confidence**: The experimental results on synthetic datasets demonstrating the three key properties (majority consistency, Condorcet consistency, IIA).
- **Low confidence**: The practical advantages of maximal-lottery methods in real-world LLM alignment scenarios.

## Next Checks

1. Replicate the Borda-BTL equivalence: Implement RLHF with a Bradley-Terry reward model and verify that the learned reward ranking matches Borda count on synthetic datasets with known preference profiles.

2. Test IIA robustness with position bias: Construct preference datasets where position bias systematically favors alternatives appearing first, then compare RLHF and maximal-lottery methods' sensitivity to adding irrelevant alternatives.

3. Scale synthetic experiments: Increase dataset size beyond 2048 samples and test on preference profiles with more than three alternatives to verify whether the theoretical properties hold under more complex aggregation scenarios.