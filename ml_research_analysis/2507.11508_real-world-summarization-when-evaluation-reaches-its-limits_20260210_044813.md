---
ver: rpa2
title: 'Real-World Summarization: When Evaluation Reaches Its Limits'
arxiv_id: '2507.11508'
source_url: https://arxiv.org/abs/2507.11508
tags:
- human
- evaluation
- error
- overlap
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates evaluation of faithfulness in hotel highlights\u2014\
  brief LLM-generated summaries of accommodation features. Through human evaluation\
  \ campaigns using categorical error assessment and span-level annotation, the study\
  \ compares traditional metrics, trainable methods, and LLM-as-a-judge approaches."
---

# Real-World Summarization: When Evaluation Reaches Its Limits

## Quick Facts
- **arXiv ID:** 2507.11508
- **Source URL:** https://arxiv.org/abs/2507.11508
- **Authors:** Patrícia Schmidtová; Ondřej Dušek; Saad Mahamood
- **Reference count:** 21
- **Primary result:** Simple word overlap metrics surprisingly outperform complex methods for faithfulness evaluation in hotel highlights

## Executive Summary
This paper investigates the limits of evaluation metrics for LLM-generated hotel highlights—brief summaries of accommodation features. Through human evaluation campaigns using categorical error assessment and span-level annotation, the study compares traditional metrics, trainable methods, and LLM-as-a-judge approaches. Key findings reveal that simple word overlap metrics surprisingly correlate well with human judgments (Spearman correlation of 0.63), often outperforming more complex methods on out-of-domain data. The research demonstrates that while LLMs can generate high-quality highlights, they prove unreliable as evaluators, tending to either under- or over-annotate errors depending on the model used. Analysis of real-world business impacts shows incorrect and non-checkable information pose the greatest risks. The study also highlights significant challenges in crowdsourced evaluations, finding that even with careful precautions, crowdworker annotations were of poor quality, suggesting such tasks with longer inputs requiring subjective judgments are unsuitable for crowd evaluation.

## Method Summary
The study evaluates faithfulness of LLM-generated hotel highlights by comparing multiple automatic metrics against human annotations. Methods include: (1) simple word overlap metrics (form/lemma coverage, POS-specific coverage), (2) N-gram overlap (BLEU, ROUGE-L), (3) NLI entailment using ModernBERT and DeBERTa-v3, (4) text embedding similarity (BertScore, LaBSE), and (5) LLM-as-a-judge (GPT-4o, o3-mini, Gemma3). Human evaluation consists of categorical error classification (no error, hallucination, contradiction, both) and span-level error annotation (non-checkable, misleading, incorrect). The study uses 120 highlight-summary pairs for categorical annotation and 496 description-highlight pairs for span annotation.

## Key Results
- Simple word overlap metrics achieved Spearman correlation of 0.63 with human judgments, outperforming more complex methods
- ModernBERT NLI entailment reached correlation of 0.67 for referenceless evaluation, while older DeBERTa-v3 failed with correlation of 0.08
- LLMs as evaluators showed systematic biases: GPT-4o over-annotated errors while o3-mini severely under-annotated
- Crowdsourced evaluation proved unreliable even with safeguards, with only 24% of workers passing attention checks
- Incorrect and non-checkable information in highlights pose the highest business risks

## Why This Works (Mechanism)

### Mechanism 1: Word Overlap as Faithfulness Proxy
- Claim: Simple word overlap metrics can serve as effective proxies for faithfulness evaluation in domain-specific summarization tasks.
- Mechanism: When summaries must remain grounded in source text, the presence of shared vocabulary between source and summary inversely indicates hallucination risk—lower overlap suggests information not derived from the source.
- Core assumption: Hallucinations typically introduce vocabulary not present in the source text, and semantic errors manifest lexically.
- Evidence anchors:
  - [abstract] "simpler metrics like word overlap correlate surprisingly well with human judgments (Spearman correlation rank of 0.63)"
  - [section 4.1] "The simplest method we consider – overlap of word forms between the highlights and the descriptions – proved to reach the highest correlations with the human annotation"
  - [corpus] Related work (arXiv:2507.08342) confirms n-gram metrics remain indicative for summarization evaluation, though multilingual generalization is questionable
- Break condition: Tasks requiring heavy paraphrasing or synonym substitution will weaken this signal; negation handling remains a known failure mode.

### Mechanism 2: NLI Entailment for Referenceless Evaluation
- Claim: Natural Language Inference models trained on modern architectures can provide referenceless faithfulness assessment with moderate correlation to human judgment.
- Mechanism: By treating the source description as premise and the generated highlight as hypothesis, NLI models estimate entailment probability—high entailment suggests faithful summarization, while neutral or contradiction scores flag potential errors.
- Core assumption: The NLI model's training distribution transfers to the summarization domain without domain-specific fine-tuning.
- Evidence anchors:
  - [abstract] Reports correlations "up to r=0.67" for referenceless methods
  - [section 4.1] "using a newer model trained on more data, ModernBERT... proved to be helpful as we reached a correlation of 0.67 with human judgment"
  - [section 4.1] Older DeBERTa v.3 achieved only 0.08 correlation—"for the vast majority of samples, the likelihood of entailment was very close to 0 or 1"
  - [corpus] No directly comparable corpus papers validate NLI-based summarization evaluation; this mechanism relies primarily on in-paper evidence
- Break condition: Model selection is critical—older NLI models produce near-binary outputs unsuitable for graded faithfulness assessment; domain shift from NLI training data to hotel descriptions may degrade performance.

### Mechanism 3: LLM Evaluator Systematic Bias
- Claim: LLMs exhibit systematic and model-dependent biases when used as faithfulness evaluators, producing either over-annotation or under-annotation of errors.
- Mechanism: Different LLM architectures and training procedures lead to divergent calibration on error detection—some models (GPT-4o) flag excessive false positives, while others (o3-mini) miss true errors, making raw LLM judgments unreliable without validation.
- Core assumption: The bias patterns are consistent enough within a model to be predictable, but the paper does not establish this.
- Evidence anchors:
  - [abstract] "LLMs... prove unreliable for evaluation as they tend to severely under- or over-annotate"
  - [section 4.2] "GPT-4o heavily overannotates, frequently citing a nonsensical reason... o3-mini severely under-annotates"
  - [section 4.2] Table 3 shows no LLM reaches human annotator F1 baseline; Gemma3 "under-utilizes the non-checkable category"
  - [corpus] arXiv:2411.15594 (LLM-as-a-Judge survey) referenced but not retrieved; corpus provides no additional validation of this bias pattern
- Break condition: Prompt engineering attempts backfired—"prompting the models to be more lenient... led to a higher count of annotated errors"; different error types require different calibration.

## Foundational Learning

- Concept: **Natural Language Inference (NLI)**
  - Why needed here: NLI provides the theoretical foundation for referenceless faithfulness evaluation—understanding premise/hypothesis relationships is essential for interpreting why ModernBERT succeeded where older models failed.
  - Quick check question: If a summary states "pet-friendly hotel" but the source never mentions pets, what NLI label should result?

- Concept: **Spearman Rank Correlation**
  - Why needed here: The paper uses Spearman correlation (not Pearson) to validate metrics against human judgment, accounting for ordinal rather than linear relationships.
  - Quick check question: Why might rank correlation be more appropriate than Pearson correlation for comparing evaluation metrics to human judgments?

- Concept: **Hallucination Taxonomy in Summarization**
  - Why needed here: The paper distinguishes non-checkable, misleading, and incorrect errors with different business impacts—this taxonomy drives both annotation design and impact analysis.
  - Quick check question: A summary claims "15-minute drive to airport" when the source lists "17.5 km to airport"—is this non-checkable, misleading, or incorrect?

## Architecture Onboarding

- Component map:
  - **Input Layer**: Hotel description + reviews (source text) + LLM-generated highlights (candidates)
  - **Metric Layer**: (a) Statistical metrics (word overlap, BLEU, ROUGE-L), (b) Trainable metrics (NLI entailment, BERTScore, embedding similarity), (c) LLM-as-judge (GPT-4o, Gemma3, o3-mini)
  - **Validation Layer**: Human annotation (categorical + span-level) for ground truth
  - **Impact Layer**: Business risk classification (none/low/medium/high) based on error type

- Critical path:
  1. Start with word overlap and lemma overlap as baseline metrics (fast, interpretable, r=0.62-0.63)
  2. Add ModernBERT NLI entailment for higher correlation (r=0.67) if compute permits
  3. Validate any LLM-as-judge outputs against human annotations before deployment—do not use standalone

- Design tradeoffs:
  - **Simple vs. trainable metrics**: Word overlap is cheap and robust to domain shift; trainable methods (NLI) achieve higher correlation but may not generalize out-of-domain
  - **Crowdsourced vs. expert annotation**: Despite £800 spend and multiple safeguards, only 24% of crowdworkers passed attention checks—expert annotation required for high-stakes evaluation
  - **LLM generation vs. evaluation**: Models effective for highlight generation prove unreliable for evaluation; consider separating these roles

- Failure signatures:
  - **Over-annotation**: GPT-4o pattern—annotating correct highlights as errors, flagging "nonsensical reasons"; suspect when error counts significantly exceed human baseline
  - **Under-annotation**: o3-mini pattern—missing real errors; suspect when error counts are far below human baseline
  - **Category imbalance**: Gemma3 pattern—under-utilizing specific categories (non-checkable); verify distribution matches expected error profile
  - **Length-induced false positives**: Crowdworkers marked information as errors when unable to quickly locate it in longer texts despite Ctrl+F instruction

- First 3 experiments:
  1. **Metric baseline**: Implement word form overlap and ModernBERT NLI entailment on 50 held-out examples; compute Spearman correlation against human categorical judgments to verify r≈0.63 and r≈0.67 replication
  2. **LLM judge calibration**: Run GPT-4o and Gemma3 on same 50 examples; compare error count distributions and per-category F1 against human annotations; document bias direction
  3. **Length stratification test**: Split evaluation data by source text length; compare metric correlations and crowdworker agreement rates between short and long descriptions to quantify length effects

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the systematic tendencies of LLMs to over- or under-annotate errors be effectively calibrated for reliable automated faithfulness evaluation?
- **Basis in paper:** [explicit] The authors conclude that while LLMs generate high-quality highlights, they "prove unreliable for evaluation" as they tend to "severely under- or over-annotate" depending on the specific model used (e.g., GPT-4o vs. o3-mini).
- **Why unresolved:** The paper identifies these biases as a barrier to using LLMs as judges but does not propose a solution to correct the calibration or align the models with human judgment standards.
- **What evidence would resolve it:** A methodology or prompting strategy that adjusts LLM outputs to align with human span-level error distributions, validated across multiple LLM architectures.

### Open Question 2
- **Question:** What annotation protocols can successfully crowdsource complex, long-context faithfulness evaluations where standard quality controls failed?
- **Basis in paper:** [explicit] The authors state that despite financial investment and precautions (pilot studies, bonuses), "tasks with longer inputs that require potentially subjective judgments are not suitable to be evaluated through crowdworkers."
- **Why unresolved:** The study highlights a critical failure point for real-world evaluation—obtaining human ground truth is expensive and crowdsourcing proved ineffective, leaving a gap for scalable validation.
- **What evidence would resolve it:** A crowdsourcing framework specifically designed for long-text subjectivity that achieves high inter-annotator agreement and passes expert attention checks.

### Open Question 3
- **Question:** Does the superior performance of simple word overlap metrics over complex trainable methods generalize to other out-of-domain summarization tasks?
- **Basis in paper:** [inferred] The paper asks (RQ1) "How well do faithfulness metrics generalize?" and finds simple metrics surprisingly robust (r=0.63) while sophisticated methods fail on OOD data. The authors argue these simple metrics should be reported, implying a need to verify if this finding is universal.
- **Why unresolved:** This finding is counter-intuitive to the trend of using embedding-based metrics; it remains unclear if this is an artifact of the hotel domain (unique features) or a general flaw in complex metrics.
- **What evidence would resolve it:** Cross-domain benchmarking (e.g., news, medical, dialogue) showing that word overlap consistently maintains higher Spearman correlation with human judgments than BERTScore or other neural metrics on unseen data.

## Limitations

- Simple word overlap success may be domain-specific to hotel highlights rather than generalizable to other summarization tasks
- LLM-as-a-judge biases are model-dependent and cannot be reliably calibrated with current prompting approaches
- Crowdsourced evaluation proved fundamentally unsuitable for long-text faithfulness assessment despite significant safeguards

## Confidence

- **High confidence:** Word overlap metrics effectively predict faithfulness in this domain; the mechanism of lexical sharing as hallucination indicator is well-supported
- **Medium confidence:** NLI entailment works as referenceless evaluation, but model selection is critical and domain transfer remains unproven
- **Low confidence:** LLM-as-a-judge can be reliably calibrated—results show systematic but inconsistent biases across models without clear remediation strategies

## Next Checks

1. Test word overlap and NLI metrics on out-of-domain summarization tasks (news, dialogue, scientific) to verify generalization beyond hotel highlights
2. Conduct ablation studies on NLI model architectures and training data to identify which components enable successful entailment-based evaluation
3. Design prompt engineering experiments to systematically calibrate LLM evaluators, measuring whether bias patterns can be reduced or predicted across different error types