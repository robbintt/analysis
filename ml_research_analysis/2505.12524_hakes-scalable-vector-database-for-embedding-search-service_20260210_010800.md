---
ver: rpa2
title: 'HAKES: Scalable Vector Database for Embedding Search Service'
arxiv_id: '2505.12524'
source_url: https://arxiv.org/abs/2505.12524
tags:
- vectors
- search
- index
- vector
- recall
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'HAKES addresses high-recall vector search in high-dimensional
  embeddings under concurrent read-write workloads. It uses a two-stage filter-refine
  design: aggressive compression (dimensionality reduction + 4-bit quantization) in
  the filter stage, followed by exact similarity computation in the refine stage.'
---

# HAKES: Scalable Vector Database for Embedding Search Service

## Quick Facts
- **arXiv ID:** 2505.12524
- **Source URL:** https://arxiv.org/abs/2505.12524
- **Reference count:** 40
- **Primary result:** HAKES achieves up to 16× higher throughput than three distributed vector database baselines under concurrent workloads while maintaining high recall

## Executive Summary
HAKES is a scalable vector database designed for high-recall embedding search under concurrent read-write workloads. It addresses the challenge of maintaining high recall in high-dimensional vector spaces while supporting real-time updates and concurrent operations. The system uses a two-stage filter-refine approach with aggressive compression in the filter stage and exact similarity computation in the refine stage, combined with learned index parameters and a disaggregated distributed architecture to scale compute and memory independently.

## Method Summary
HAKES employs a two-stage filter-refine design to achieve high-recall vector search. In the filter stage, vectors undergo aggressive compression through dimensionality reduction and 4-bit quantization, enabling efficient approximate nearest neighbor search. The refine stage performs exact similarity computation on the filtered candidates to ensure high recall. The system learns index parameters end-to-end to preserve local similarity distributions, and uses separate insert/search parameters to prevent recall degradation during updates. A distributed disaggregated architecture separates the filter and refine stages, allowing independent scaling of compute and memory resources. This design enables HAKES to maintain high recall (recall > 0.8) while significantly improving throughput under concurrent workloads.

## Key Results
- HAKES-Index outperforms 12 state-of-the-art indexes in the high-recall region (>0.8 recall) on six deep embedding datasets
- Achieves up to 16× higher throughput than three distributed vector database baselines under concurrent read-write workloads
- Successfully handles real-time updates without significant recall degradation through separate insert/search parameters

## Why This Works (Mechanism)
The two-stage filter-refine approach works because aggressive compression in the filter stage dramatically reduces the search space while preserving enough information for high-recall retrieval. Dimensionality reduction and 4-bit quantization enable efficient indexing and search, while the exact similarity computation in the refine stage ensures that high-recall targets are met. The end-to-end learning of index parameters allows the system to adapt to the specific similarity distributions of different embedding types, optimizing the trade-off between compression and recall. The disaggregated architecture separates compute-intensive filtering from memory-intensive storage, enabling independent scaling and improved resource utilization.

## Foundational Learning
- **Vector Similarity Search**: Finding nearest neighbors in high-dimensional spaces is fundamental to many AI applications; quick check: understanding of distance metrics and their computational complexity
- **Approximate Nearest Neighbor (ANN) Indexes**: Trade-offs between accuracy and efficiency in large-scale search; quick check: familiarity with common ANN techniques like quantization and graph-based methods
- **Disaggregated Architecture**: Separating storage and compute for independent scaling; quick check: understanding of distributed systems and resource allocation
- **End-to-end Learning for Indexing**: Learning optimal index parameters from data distributions; quick check: knowledge of machine learning optimization techniques
- **Concurrent Read-Write Workloads**: Handling simultaneous updates and queries without performance degradation; quick check: understanding of concurrency control and consistency models

## Architecture Onboarding
**Component Map:** Client -> Load Balancer -> Filter Nodes -> Refine Nodes -> Storage Backend
**Critical Path:** Query enters filter stage for compression and approximate search → Candidates sent to refine stage for exact similarity computation → Results returned to client
**Design Tradeoffs:** Aggressive compression enables faster filtering but requires careful parameter learning to maintain recall; separate insert/search parameters add complexity but prevent recall degradation during updates; disaggregated architecture improves scalability but introduces coordination overhead
**Failure Signatures:** Filter node failures cause increased load on remaining nodes; refine node failures result in incomplete candidate refinement; storage backend failures lead to inability to serve queries
**First Experiments:** 1) Measure recall vs. throughput trade-off across different compression levels, 2) Test system performance under varying update rates, 3) Evaluate scalability by adding/removing filter and refine nodes

## Open Questions the Paper Calls Out
The paper doesn't explicitly call out open questions, but implicit areas for future work include: generalization to non-deep embedding types, optimization of network overhead in the disaggregated architecture, and extension to handle even higher-dimensional vectors beyond current capabilities.

## Limitations
- Evaluation focuses primarily on deep embedding datasets, limiting generalizability to other embedding types
- Throughput comparisons against specific baselines may not account for all possible optimization strategies in production systems
- Disaggregated architecture introduces complexity in distributed coordination and potential network bottlenecks that aren't fully characterized

## Confidence
- **High Confidence**: Filter-refine design performance gains in high-recall region, dimensional reduction + quantization effectiveness
- **Medium Confidence**: End-to-end learned index parameters, disaggregated architecture benefits
- **Medium Confidence**: Throughput comparisons against baselines (limited scope of alternatives)

## Next Checks
1. Test HAKES performance across diverse embedding types (non-deep embeddings, different dimensionalities) to assess generalizability
2. Characterize network overhead and coordination costs in the disaggregated architecture under varying cluster sizes
3. Evaluate the learned index parameters' performance when transferred to embedding distributions not seen during training