---
ver: rpa2
title: 'SCE: Scalable Consistency Ensembles Make Blackbox Large Language Model Generation
  More Reliable'
arxiv_id: '2503.10881'
source_url: https://arxiv.org/abs/2503.10881
tags:
- consistency
- arxiv
- responses
- samples
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes SCE, a scalable framework for reliably ensembling\
  \ outputs from multiple black-box LLMs by checking semantic consistency. SCE-CHECK\
  \ uses a novel \u201CYou Only Prompt Once\u201D (YOPO) technique to efficiently\
  \ compare responses in constant time, avoiding the quadratic complexity of pairwise\
  \ checks."
---

# SCE: Scalable Consistency Ensembles Make Blackbox Large Language Model Generation More Reliable

## Quick Facts
- arXiv ID: 2503.10881
- Source URL: https://arxiv.org/abs/2503.10881
- Reference count: 13
- Primary result: Improves truthfulness accuracy by up to 18.8% on Primality and 6.4% on HotpotQA using scalable consistency checking

## Executive Summary
This paper introduces SCE (Scalable Consistency Ensembles), a framework designed to make black-box large language model (LLM) generation more reliable by ensembling outputs and ensuring semantic consistency. SCE employs a novel "You Only Prompt Once" (YOPO) technique to efficiently compare responses in constant time, avoiding the quadratic complexity of pairwise checks. It further fuses the most consistent responses through summarization to produce enhanced, reliable outputs. Extensive experiments demonstrate that SCE outperforms single LLMs and existing ensemble baselines in both truthfulness and falseness detection accuracy, while maintaining scalability.

## Method Summary
SCE is a two-stage framework: SCE-CHECK identifies semantically consistent responses using the YOPO technique, and SCE-FUSION merges these top consistent responses to generate a final output. YOPO reduces the consistency checking complexity from O(n²) to O(n) by leveraging pairwise embeddings from a single prompt batch. SCE-FHECK first filters responses using an embedding-based consistency check with a threshold of 0.7, then applies a length-filtered consistency check. SCE-FUSION employs both majority voting and a summarization-based method to merge the selected responses. The approach is specifically designed for black-box LLM APIs, using no model parameter tuning.

## Key Results
- Improves truthfulness accuracy by up to 18.8% on the Primality dataset compared to single LLMs
- Achieves 6.4% higher truthfulness accuracy on HotpotQA versus existing ensemble baselines
- Demonstrates linear inference time growth and constant-time consistency checking via YOPO

## Why This Works (Mechanism)
SCE leverages the observation that while individual LLM outputs can be unreliable, ensembles of diverse responses can improve overall accuracy if semantic consistency is maintained. By using the YOPO technique, SCE efficiently filters inconsistent responses before fusion, avoiding the high computational cost of traditional pairwise consistency checks. The summarization-based fusion further enhances output quality by consolidating the most reliable information from consistent responses.

## Foundational Learning
- **Semantic consistency checking**: Ensures responses are semantically aligned, reducing the impact of hallucination and contradictory outputs; quick check: verify cosine similarity of embeddings exceeds threshold
- **You Only Prompt Once (YOPO)**: Enables constant-time consistency comparison by batching prompts and leveraging pairwise embeddings; quick check: confirm O(n) complexity versus O(n²) for pairwise checks
- **Black-box LLM ensemble**: Combines outputs from multiple models to boost accuracy without access to model internals; quick check: ensure all models have same temperature and context window settings
- **Summarization-based fusion**: Merges consistent responses to produce a coherent, enhanced final output; quick check: validate that majority voting and summarization yield comparable or improved results
- **Embedding-based filtering**: Uses semantic similarity of embeddings to identify and remove inconsistent responses; quick check: set and validate the consistency threshold (0.7)
- **Length-aware consistency**: Filters responses by both semantic similarity and length to avoid false positives; quick check: ensure length filtering is applied after initial semantic filtering

## Architecture Onboarding
- **Component map**: Input prompts → Batch LLM calls → YOPO consistency check → SCE-FHECK (embedding + length filtering) → SCE-FUSION (voting + summarization) → Final output
- **Critical path**: Prompt batch → YOPO → SCE-FHECK → SCE-FUSION → Output
- **Design tradeoffs**: Constant-time consistency (YOPO) vs. potential loss of pairwise nuance; summarization-based fusion vs. simple majority voting
- **Failure signatures**: High false negatives in consistency check lead to loss of useful information; overly strict thresholds may exclude valid but diverse responses
- **3 first experiments**: (1) Compare YOPO vs. naive pairwise consistency on response sets of varying size; (2) Ablate SCE-FHECK and SCE-FUSION stages to assess impact on accuracy; (3) Test consistency threshold sensitivity on benchmark datasets

## Open Questions the Paper Calls Out
None

## Limitations
- Does not explore trade-offs between consistency enforcement and diversity of outputs
- Performance on creative or open-ended tasks requiring diversity is not investigated
- Behavior on out-of-distribution data and potential bias amplification from consistency filtering is not assessed

## Confidence
- **High**: Accuracy improvements over baselines, scalability via YOPO, linear inference time
- **Medium**: Generalizability to tasks beyond factual question-answering, impact on response diversity
- **Low**: Behavior on out-of-distribution data, potential bias amplification from consistency filtering

## Next Checks
1. Evaluate SCE on open-ended creative generation tasks to assess impact on diversity and novelty
2. Test performance on adversarial examples designed to exploit consistency-based filtering
3. Measure computational overhead in production environments with rate-limited API calls and latency constraints