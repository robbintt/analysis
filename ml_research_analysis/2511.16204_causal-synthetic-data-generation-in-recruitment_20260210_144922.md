---
ver: rpa2
title: Causal Synthetic Data Generation in Recruitment
arxiv_id: '2511.16204'
source_url: https://arxiv.org/abs/2511.16204
tags:
- data
- causal
- skills
- ranking
- synthetic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents a causal generative data system for the recruitment\
  \ domain, addressing the scarcity of representative datasets due to privacy constraints.\
  \ The method uses two expert-informed causal graphs\u2014one for job offers and\
  \ one for curricula\u2014to model the generation of synthetic data."
---

# Causal Synthetic Data Generation in Recruitment

## Quick Facts
- arXiv ID: 2511.16204
- Source URL: https://arxiv.org/abs/2511.16204
- Reference count: 36
- This paper presents a causal generative data system for recruitment that enables controlled bias introduction and downstream fairness assessment.

## Executive Summary
This paper addresses the challenge of generating representative recruitment datasets under privacy constraints by developing a causal synthetic data generation (SDG) system. The method employs expert-informed causal graphs for job offers and curricula, fitting structural equations to real data to preserve causal relationships while enabling controlled bias introduction through a parameter α. Experiments demonstrate that fairness metrics (DP and rND) vary significantly with bias parameter settings and feature weights in a ranking model. The approach provides interpretable, controllable synthetic data generation that improves fairness assessment in recruitment ranking tasks, with the system released as open-source software.

## Method Summary
The system uses expert-defined Structural Causal Models (SCMs) with directed acyclic graphs (DAGs) for job offers and curricula. For each variable in the DAG, structural equations are fitted using regression for continuous variables, HistGradientBoostingClassifier for discrete variables, and conditional empirical sampling for set-valued variables like skills. A bias parameter α is introduced to shift the conditional distribution of a child variable (working hours) given a parent sensitive variable (gender). The system generates synthetic datasets that preserve causal relationships while allowing controlled introduction of bias, which is then evaluated through downstream ranking tasks using a linear pointwise model and fairness metrics (Demographic Parity and normalized Discounted Difference).

## Key Results
- Fairness metrics (DP and rND) vary significantly with bias parameter α settings, showing the system's ability to control and measure bias
- When α₀ = 4.0 (max bias favoring non-protected group), DP drops to 0.11 from 0.61 at α₀ = -4.0, demonstrating strong bias propagation to rankings
- The system successfully preserves causal relationships in synthetic data while enabling interpretable bias manipulation through expert-informed DAGs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structured causal graphs enable controlled introduction and quantification of bias in synthetic data.
- Mechanism: Expert-defined SCMs with DAG structure use parameter α to probabilistically shift the conditional distribution of child variables given sensitive parent variables, simulating how structural bias affects data generation.
- Core assumption: Expert-defined causal graph faithfully represents the underlying data-generating process for job offers and curricula.
- Evidence anchors: Abstract states the system "enables controlled bias introduction through a parameter α"; section 4.2 describes α shifting distributions for protected vs. unprotected groups.
- Break condition: If causal graph is misspecified, α parameter may not have intended causal effect, making simulated bias non-representative.

### Mechanism 2
- Claim: SDG with SCMs preserves underlying causal relationships and statistical properties of real data.
- Mechanism: System fits parametric structural equations to observational data based on predefined causal graph, enforcing factorized joint probability distribution that respects causal dependencies.
- Core assumption: Observational data is of sufficient quality and quantity to learn accurate structural equations for each variable given its parents.
- Evidence anchors: Abstract mentions "fitting structural equations to real data, the system preserves underlying causal relationships"; section 3.2 describes regression modeling with exogenous noise.
- Break condition: If observational data is biased or non-representative, learned structural equations will propagate these biases.

### Mechanism 3
- Claim: Bias introduced in data generation propagates to and can be measured in downstream ranking tasks.
- Mechanism: Controlled bias via α affects features in synthetic data, which serve as inputs to linear ranking model; changing feature distributions based on sensitive attributes alters ranking scores measured by fairness metrics.
- Core assumption: Linear pointwise ranking model is a representative proxy for evaluating how data-level bias affects real-world algorithmic decisions.
- Evidence anchors: Abstract notes fairness metrics vary with bias parameter settings and feature weights; section 5 results show DP declining significantly as α increases.
- Break condition: If downstream model is non-linear or uses different features, measured impact of data-level bias may differ.

## Foundational Learning

- **Concept: Structural Causal Models (SCMs)**
  - **Why needed here:** SCMs are the core architecture defining how each variable is generated from its causes; essential for interpreting bias injection and data synthesis.
  - **Quick check question:** Given a DAG with an edge A -> B, can you write the general form of the structural equation for B?

- **Concept: Fairness Metrics in Ranking (Demographic Parity, rND)**
  - **Why needed here:** These metrics are primary dependent variables evaluating system's generated data impact; understanding what they measure is crucial for interpreting results.
  - **Quick check question:** If a ranking has Demographic Parity (DP) value of 1, what does it signify about probability of being ranked in top-k positions for protected vs. unprotected groups?

- **Concept: Learning to Rank (LTR) – Pointwise Approach**
  - **Why needed here:** Paper uses specific LTR model (linear pointwise ranker) to assess fairness; understanding this baseline is crucial for evaluating bias impact on downstream task.
  - **Quick check question:** In a pointwise ranking approach, is the model trained on individual document scores or on pairs of documents to determine their relative order?

## Architecture Onboarding

- **Component map:** Data Preprocessing Pipeline -> Causal SDG Engine -> Downstream Task Evaluator -> Fairness Evaluator

- **Critical path:** α parameter in Causal SDG Engine alters data, which flows to Downstream Task Evaluator producing rankings, then analyzed by Fairness Evaluator to measure bias impact.

- **Design tradeoffs:**
  - Interpretability vs. Distributional Fidelity: Prioritizes interpretability and controllability via predefined causal graph but may limit capturing complex non-linear patterns
  - Expert Knowledge vs. Data-Driven Discovery: Causal graph fixed by expert elicitation ensures domain relevance but assumes experts' model is correct

- **Failure signatures:**
  - Biased Output despite α=0: If observational data contains historical bias, generated data will likely be biased even without intentional α manipulation
  - Unrealistic Synthetic Data: If causal graph is misspecified or structural equations too simple, generated samples may lack internal coherence
  - Metric Gaming: Simple linear model may show unfairness while complex real-world ranker could potentially mitigate or exacerbate it differently

- **First 3 experiments:**
  1. Sanity Check (α=0): Generate dataset with α=0, verify if fairness metrics show imbalance indicating historical bias in real data used to train structural equations
  2. Sensitivity Analysis (Varying α): Run generation across α values (-4.0 to 4.0), plot resulting DP/rND values to confirm metrics respond to increased/decreased bias
  3. Feature Weight Impact: Using fixed biased dataset, run ranking model with different weights for working hours feature, confirm fairness metrics only change when feature related to bias mechanism is weighted

## Open Questions the Paper Calls Out
None

## Limitations
- Expert-defined causal graphs represent significant structural assumption that may not capture all relevant confounding factors or non-linear relationships in real recruitment processes
- Access to underlying datasets is restricted (proprietary job offers, specialized curricula), complicating exact reproduction despite open-source code
- Downstream fairness evaluation uses linear ranking model that may not fully represent complex real-world recruitment algorithms

## Confidence
- **High confidence**: Core mechanism of using SCMs for synthetic data generation and implementation of α parameter for controlled bias introduction are well-specified and reproducible
- **Medium confidence**: Claim that structural equations preserve causal relationships is plausible but depends heavily on data quality and correctness of DAG
- **Medium confidence**: Experimental results showing fairness metric changes with α are reproducible but real-world impact on actual recruitment systems requires further validation

## Next Checks
1. **Data Quality Validation**: Generate synthetic data with α=0 and analyze whether fairness metrics indicate residual bias, confirming whether real data used for fitting contains historical inequities
2. **DAG Sensitivity Analysis**: Systematically modify causal graph structure and observe how this affects synthetic data distribution and fairness metrics to test robustness of expert-defined model
3. **Non-linear Model Testing**: Replace linear ranking model with non-linear alternative (e.g., gradient boosting) and compare how fairness metrics change to assess whether linear model is reliable proxy for real-world impact