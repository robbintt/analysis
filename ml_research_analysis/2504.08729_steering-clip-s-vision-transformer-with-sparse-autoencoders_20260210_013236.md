---
ver: rpa2
title: Steering CLIP's vision transformer with sparse autoencoders
arxiv_id: '2504.08729'
source_url: https://arxiv.org/abs/2504.08729
tags:
- features
- clip
- vision
- saes
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work trains sparse autoencoders (SAEs) on CLIP\u2019s vision\
  \ transformer and introduces a steerability metric to quantify how precisely SAE\
  \ features can influence model outputs. Analysis shows 10-15% of features are steerable,\
  \ with SAEs offering thousands more steerable elements than the base model."
---

# Steering CLIP's vision transformer with sparse autoencoders

## Quick Facts
- **arXiv ID:** 2504.08729
- **Source URL:** https://arxiv.org/abs/2504.08729
- **Reference count:** 40
- **Primary result:** SAEs trained on CLIP-ViT provide thousands more steerable features than the base model, enabling improved disentanglement (81.11% WGA on CelebA) and robustness against typographic attacks (0.72-0.88 accuracy).

## Executive Summary
This work trains sparse autoencoders (SAEs) on CLIP's vision transformer and introduces a steerability metric to quantify how precisely SAE features can influence model outputs. Analysis shows 10-15% of features are steerable, with SAEs offering thousands more steerable elements than the base model. Applying SAE-based feature suppression improves disentanglement performance on CelebA (worst-group accuracy up to 81.11%) and Waterbirds (up to 24.61%), and achieves state-of-the-art robustness against typographic attacks (0.72-0.88 accuracy across benchmarks). Findings reveal higher L0 sparsity in vision than language SAEs and suggest optimal steering occurs in middle layers.

## Method Summary
The method involves training sparse autoencoders on CLIP-ViT-B-32 residual streams with an expansion factor of 64, using ImageNet-1K for pre-training and task-specific datasets for downstream steering. SAEs decompose the vision transformer's activations into a high-dimensional sparse latent space, where features are identified and selectively ablated based on their correlation with spurious attributes. The steerability metric $S_f$ quantifies how precisely feature ablation can promote specific output concepts, and the approach is validated on disentanglement tasks (CelebA, Waterbirds) and typographic attack robustness (RTA-100, PAINT benchmarks).

## Key Results
- SAEs provide thousands more steerable features than the base model, with 10-15% of features being steerable
- SAE-based feature suppression improves worst-group accuracy to 81.11% on CelebA and 24.61% on Waterbirds
- Achieves state-of-the-art robustness against typographic attacks with 0.72-0.88 accuracy across benchmarks
- Vision SAEs show higher L0 sparsity (40-50%) than language SAEs (20-30%)
- Optimal disentanglement occurs in middle layers (Layers 7-8 for CelebA, Layer 2 for Waterbirds)

## Why This Works (Mechanism)

### Mechanism 1: Monosemantic Feature Decomposition
SAEs decompose polysemantic base model neurons into interpretable, monosemantic features through an overcomplete dictionary ($d_{SAE} \gg d_{model}$), enabling targeted manipulation of specific concepts. The core assumption is that the superposition hypothesis holds for vision transformers, meaning representations are combinations of sparsely activating features.

### Mechanism 2: Causal Disentanglement via Feature Ablation
Targeted ablation of identified SAE features causally reduces model reliance on spurious correlations by directly removing undesired concept activations during the forward pass. This works when the SAE has learned a feature direction aligning with the spurious attribute.

### Mechanism 3: Layer-wise Feature Formation Hierarchy
Disentanglement is most effective in middle layers because semantic concepts are formed but not yet irreversibly entangled, representing a "sweet spot" where concepts are well-formed but still separable from target outputs.

## Foundational Learning

- **Concept: Superposition Hypothesis**
  - Why needed here: Core theoretical justification for SAEs, positing neural networks represent more features than dimensions by encoding them in superposition
  - Quick check question: Can a single neuron in a network respond to multiple, unrelated inputs (e.g., a "car" and a "cat")?

- **Concept: Sparsity (L0 and L1)**
  - Why needed here: Critical for evaluating SAE quality; L0 quantifies true sparsity while L1 regularization enforces it during training
  - Quick check question: In a layer with 1000 SAE features, an L0 of 50 means how many features are active for a given input token?

- **Concept: Intervention via Activation Ablation**
  - Why needed here: Primary method used to control behavior through surgically setting specific feature activations to zero
  - Quick check question: What is the expected effect on an output class probability if you zero-ablate an SAE feature that is positively correlated with that class?

## Architecture Onboarding

- **Component map:** CLIP-ViT (Base Model) -> Residual Stream Hook -> SAE Encoder -> Sparse Latent Space -> SAE Decoder -> Modified Activations -> Forward Pass
- **Critical path:**
  1. Hook into specific residual stream layer of CLIP-ViT
  2. Pass activations through SAE to get sparse feature vector per token
  3. Identify features correlated with undesired attribute using labeled data
  4. Set activations of identified features to zero
  5. Reconstruct modified activations and continue forward pass
- **Design tradeoffs:**
  - Expansion factor vs. interpretability: Higher expansion creates more features, increasing monosemanticity but compute cost
  - L1 coefficient vs. reconstruction: Higher L1 penalty increases sparsity but may harm reconstruction fidelity
  - Layer selection: Earlier layers may be more general but less precise; later layers may be too late
- **Failure signatures:**
  - Dead Features: SAE features that never activate
  - Poor Reconstruction: High reconstruction loss indicates SAE not faithfully representing activations
  - Feature Splitting: Single concept split across multiple features, making clean ablation difficult
  - Polysemantic SAE Features: SAE fails to disentangle concepts, reducing steering precision
- **First 3 experiments:**
  1. Train small SAE on synthetic data with known sparse features to verify setup can recover ground-truth features and ablation works
  2. Plot layer-wise L0 sparsity to confirm vision models have higher L0 than language models
  3. Implement steerability metric $S_f$ and test on single SAE feature (e.g., "blue") to verify probability change curves

## Open Questions the Paper Calls Out

### Open Question 1
How can steerability metrics be refined to capture semantic relationships between concepts, rather than treating promoted concepts as independent? The current $S_f$ metric treats concept probability mass uniformly, lacking a mechanism to weight concepts based on semantic similarity or clustering.

### Open Question 2
To what extent does the choice of concept vocabulary $V$ limit the identification and scoring of steerable features? The reliance on a fixed 5,000-word vocabulary introduces a potential alignment bottleneck that hasn't been systematically tested against larger or domain-specific vocabularies.

### Open Question 3
Does the optimal layer for feature disentanglement depend systematically on the spatial locality or semantic level of the target attribute? The paper hypothesizes diffuse features are best disentangled in early layers while localized features are best in middle layers, but lacks formal proof or testing across diverse attributes.

## Limitations
- Interpretability is not rigorously validated beyond steerability metrics; SAE features may not correspond to coherent human concepts
- Layer-wise analysis is correlational rather than explaining underlying causal mechanisms of why middle layers are optimal
- Generalizability is limited to CLIP and specific datasets (CelebA, Waterbirds, RTA/PAINT) used

## Confidence

- **High Confidence:** SAEs provide more steerable features than base model (10-15% of features are steerable) - directly measured and reported
- **Medium Confidence:** SAE-based feature suppression improves worst-group accuracy (e.g., 81.11% on CelebA) - results reported but implementation details not fully specified
- **Medium Confidence:** Optimal disentanglement occurs in middle layers - specific layers identified but no unifying theory across tasks

## Next Checks

1. **Feature Interpretability Validation:** Conduct human evaluation where annotators label images maximally activating specific SAE features, calculating inter-annotator agreement to quantify true interpretability beyond steerability metrics.

2. **Cross-Model Generalization:** Train SAEs on different vision transformer (e.g., DeiT or DINOv2) and attempt same disentanglement and typographic attack tasks, comparing success rate and optimal layers to test universality.

3. **Causal Ablation Study:** Design experiment with synthetically injected spurious correlation, use SAE-based ablation to remove it, and measure causal effect on model's reliance on spurious feature versus true label.