---
ver: rpa2
title: 'Pie: A Programmable Serving System for Emerging LLM Applications'
arxiv_id: '2510.24051'
source_url: https://arxiv.org/abs/2510.24051
tags:
- layer
- control
- serving
- inference
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Pie is a programmable LLM serving system that enables application-specific
  optimizations by exposing fine-grained APIs and delegating control to user-provided
  programs called inferlets. Pie addresses three key limitations in existing systems:
  implicit KV cache management, inflexible generation processes, and poor workflow
  integration.'
---

# Pie: A Programmable Serving System for Emerging LLM Applications

## Quick Facts
- arXiv ID: 2510.24051
- Source URL: https://arxiv.org/abs/2510.24051
- Authors: In Gim; Zhiyao Ma; Seung-seob Lee; Lin Zhong
- Reference count: 40
- Key outcome: Pie is a programmable LLM serving system that enables application-specific optimizations by exposing fine-grained APIs and delegating control to user-provided programs called inferlets.

## Executive Summary
Pie addresses the inflexibility of monolithic LLM serving systems by decomposing the generation loop into fine-grained handlers and exposing them through APIs that user programs (inferlets) can orchestrate. This programmable approach allows applications to implement custom KV cache strategies, bespoke generation logic, and seamlessly integrate computation and I/O operations. The system uses WebAssembly for lightweight sandboxing of inferlets and demonstrates significant performance improvements on agentic workflows while maintaining competitive performance on standard tasks.

## Method Summary
Pie implements a three-layer architecture with an Application layer containing the Inferlet Lifecycle Manager and Wasm runtime, a Control layer with a Controller and Batch Scheduler for resource virtualization and API call batching, and an Inference layer with API Handlers using PyTorch/FlashInfer. User programs (inferlets) written in Rust compile to WebAssembly and orchestrate fine-grained API calls (Embed, Forward, Sample) to implement custom generation flows. The system exposes explicit KV cache management APIs and supports event-driven I/O integration within the generation loop.

## Key Results
- Matches state-of-the-art performance on standard tasks with only 3-12% latency overhead
- Achieves 1.3×-3.4× higher throughput on agentic workflows through application-specific optimizations
- Successfully implements various LLM techniques including attention variants, constrained and speculative decoding, deliberate prompting strategies, and agentic workflows

## Why This Works (Mechanism)

### Mechanism 1: Loop Decomposition
The monolithic generation loop is decomposed into fine-grained handlers (Embed, Forward, Sample) exposed via APIs, allowing applications to implement custom orchestration logic without modifying the serving system. The overhead of transitioning between these fine-grained API calls does not negate the performance gains from custom logic.

### Mechanism 2: Explicit Resource Control
Users are given explicit control over KV cache pages through APIs like `alloc_kvpage`, `export_kvpage`, and `mask_kvpage`, enabling optimization of memory usage and reuse strategies beyond system-wide LRU heuristics. The user possesses semantic knowledge of the task that the serving system lacks.

### Mechanism 3: Event-Driven I/O Integration
I/O operations are integrated into the generation loop via an event-driven runtime, allowing agentic workflows to avoid the latency of round-trips to a stateless client. The Wasm runtime provides sufficiently low-latency sandboxing so that the context-switch cost is lower than a network round-trip.

## Foundational Learning

- **Concept: PagedAttention (vLLM)**
  - Why needed here: Pie builds on the abstraction of paged KV caches (`KvPage`). Understanding that KV cache is not a contiguous tensor but a collection of blocks is required to use the `alloc`/`mask`/`copy` APIs effectively.
  - Quick check question: How does `copy_kvpage` differ from simply copying the tensor data in standard PyTorch inference?

- **Concept: WebAssembly (Wasm) Sandboxing**
  - Why needed here: Pie relies on Wasm to isolate user-submitted inferlets. You must understand that Wasm provides a memory-safe, sandboxed execution environment that prevents inferlets from accessing host memory directly.
  - Quick check question: Why is Wasm preferred over Docker containers for short-lived, high-concurrency inferlets in this architecture?

- **Concept: Event-Driven / Asynchronous Runtime**
  - Why needed here: Inferlets use `await` and futures (`future[Dist]`) to handle latency. Understanding non-blocking I/O is critical to writing inferlets that don't stall the GPU scheduler.
  - Quick check question: In an inferlet, what happens if you execute a blocking `http_get` without using the async/await pattern provided by the Pie API?

## Architecture Onboarding

- **Component map**: Application Layer (ILM, Wasm Runtime) -> Control Layer (Controller, Batch Scheduler) -> Inference Layer (API Handlers)
- **Critical path**: User submits Wasm binary → ILM loads and instantiates it → Inferlet calls `forward()` → Request enters Command Queue → Batch Scheduler groups requests → Dispatches to Inference Layer → Inference Layer executes GPU kernel → Returns logits to Control Layer → Control Layer returns result to Inferlet → Inferlet samples/loops
- **Design tradeoffs**:
  - Granularity vs. Overhead: Fine-grained API calls allow maximum flexibility but introduce IPC overhead
  - Safety vs. Performance: Wasm is safer than native code but may introduce startup latency and execution overhead
- **Failure signatures**:
  - High Latency Variance: Caused by inefficient batching strategies or inferlets holding resources too long
  - OOM (Out of Memory): Leaky inferlets that allocate `KvPage` without deallocating will crash the system
  - Deadlocks: Poorly written inferlets waiting on I/O that never completes
- **First 3 experiments**:
  1. Baseline Overhead Measurement: Run a standard text completion task comparing Pie's basic text completion inferlet against a raw vLLM instance to quantify architectural overhead
  2. KV Cache Manipulation: Write an inferlet implementing "Tree-of-Thought" by explicitly forking a `KvPage` and running two different forward passes on the copies
  3. Agentic I/O Integration: Implement a "ReAct" agent inferlet measuring end-to-end latency of a loop containing 3 tool calls in Pie vs. a Python client making sequential requests to vLLM

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the system be hardened against security threats like side-channel leakage and model extraction without compromising the low-latency benefits of user-supplied inferlets?
- Basis in paper: The Discussion section notes that delegating control "expand[s] the attack surface" and lists "hardening" and "capability-based I/O" as essential future work.
- Why unresolved: The current implementation relies on WebAssembly sandboxing but lacks the proposed advanced security policies (e.g., rate limits, logit obfuscation) necessary for commercial multi-tenant deployment.
- What evidence would resolve it: A security evaluation demonstrating defense against side-channel attacks and model extraction techniques while maintaining the reported throughput.

### Open Question 2
- Question: How can the centralized control layer be effectively distributed to support multi-node GPU clusters while managing KV cache locality?
- Basis in paper: The authors identify "Scalability of the control layer" as a limitation, stating that real deployments require "distributed coordination" and "globally-aware scheduling."
- Why unresolved: The current architecture utilizes a single controller and inference backend, which becomes a bottleneck for large-scale, distributed serving.
- What evidence would resolve it: A demonstration of a distributed Pie architecture that maintains cache locality and low latency across multiple GPU nodes.

### Open Question 3
- Question: Can advanced resource contention policies, such as CPU-GPU memory swapping or SLO-aware admission control, outperform the current First-Come-First-Served (FCFS) preemption strategy?
- Basis in paper: The Discussion mentions that the current simple FCFS policy terminates inferlets to free resources, whereas "richer policies" like "controlled overprovisioning by swapping KvPage" could improve efficiency.
- Why unresolved: The paper evaluates a simple preemption policy but does not implement or test more complex memory management techniques for handling resource exhaustion.
- What evidence would resolve it: Comparative benchmarks showing system stability and fairness under heavy load using swapping or admission control versus the baseline FCFS approach.

## Limitations

- The single-threaded Wasm runtime design creates potential scalability bottlenecks when handling multiple concurrent inferlets that perform blocking operations
- The reported 3-12% latency overhead on standard tasks represents a performance cost for the flexibility provided, though the paper argues this is acceptable
- OOM failures are mitigated through FCFS termination policies, representing a significant limitation for applications requiring long-running, memory-intensive workflows

## Confidence

**High Confidence:** The architectural decomposition of the generation loop into Embed, Forward, and Sample handlers is well-specified and technically sound. The mechanism for explicit KV cache management through `alloc_kvpage`, `export_kvpage`, and `mask_kvpage` APIs is clearly defined and implementable.

**Medium Confidence:** The performance claims showing 1.3×-3.4× improvements on agentic workflows are supported by the experimental results, but the evaluation focuses on simplified versions of Tree-of-Thought and Graph-of-Thought tasks. The real-world impact on production agentic systems remains to be validated.

**Low Confidence:** The safety guarantees of the WebAssembly sandboxing approach are not fully explored. While Wasm provides memory isolation, the paper does not comprehensively address potential security vulnerabilities in user-provided inferlets or the overhead implications of the sandboxing mechanism.

## Next Checks

1. **Architectural Overhead Quantification:** Measure the exact latency overhead of the fine-grained API calls (Embed, Forward, Sample) versus a monolithic generation loop under varying batch sizes and token lengths to validate the claimed "small" overhead relative to GPU execution time.

2. **Concurrent Inferlet Stress Test:** Evaluate system behavior under high concurrency with multiple inferlets performing I/O operations simultaneously. Measure throughput degradation, latency variance, and identify potential blocking scenarios in the single-threaded Wasm runtime.

3. **Production Agentic Workflow Benchmark:** Implement and benchmark a complete, real-world agentic workflow (e.g., ReAct agent with actual tool APIs and retrieval systems) comparing end-to-end latency against both vLLM and a naive Python orchestration approach to validate the claimed benefits beyond simplified test cases.