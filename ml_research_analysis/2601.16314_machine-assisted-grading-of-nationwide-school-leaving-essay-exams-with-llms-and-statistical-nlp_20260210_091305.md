---
ver: rpa2
title: Machine-Assisted Grading of Nationwide School-Leaving Essay Exams with LLMs
  and Statistical NLP
arxiv_id: '2601.16314'
source_url: https://arxiv.org/abs/2601.16314
tags:
- instructions
- grading
- human
- scoring
- essay
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Large language models can score Estonian school-leaving essays\
  \ with accuracy comparable to human raters, often matching or falling within human\
  \ scoring variance. In a zero-shot setting, LLM-based grading achieved mean absolute\
  \ errors of 2.66\u20134.97 points on a 0\u201327 scale, with predictions frequently\
  \ within the range of human scores."
---

# Machine-Assisted Grading of Nationwide School-Leaving Essay Exams with LLMs and Statistical NLP

## Quick Facts
- arXiv ID: 2601.16314
- Source URL: https://arxiv.org/abs/2601.16314
- Reference count: 40
- Primary result: LLM-based grading achieved mean absolute errors of 2.66–4.97 points on a 0–27 scale, comparable to human raters.

## Executive Summary
This study demonstrates that large language models can effectively score Estonian school-leaving essays in a zero-shot setting, achieving accuracy comparable to human raters. The research evaluates nine rubric categories separately using both LLM-based approaches and statistical NLP models, finding that while LLMs handle content and argumentation well, feature-based regression performs better on linguistic accuracy dimensions. The work validates the technical feasibility of automated essay scoring in small-language contexts and proposes a human-in-the-loop pipeline for implementation. Key challenges include systematic model bias and vulnerability to prompt injection attacks, which require careful mitigation.

## Method Summary
The study employs a two-path approach to automated essay scoring: zero-shot LLM prompting and supervised regression on linguistic features. For the LLM path, nine rubric categories are scored separately using carefully crafted prompts that include rubric definitions alongside essay text, evaluated on 1,559 Estonian school-leaving essays from 9th and 12th grades. The statistical NLP path extracts 108 linguistic features (grammatical, lexical, surface, and error features) using tools like Stanza and custom spellcheckers, then trains regression models with feature selection and multicollinearity removal. Both approaches are compared against human consensus scores using mean absolute error as the primary metric, with additional evaluation of whether predictions fall within human score ranges.

## Key Results
- LLM-based grading achieved MAE of 2.66–4.97 points on 0–27 scale, with predictions frequently within human score ranges
- Statistical NLP models using error and linguistic features matched or exceeded LLM accuracy on linguistic accuracy categories
- Gemini 1.5 Pro showed systematic bias of -3.66 on 9th grade, while GPT-4o showed bias of +4.68, highlighting model-specific performance differences
- LLMs were vulnerable to prompt injection attacks, with average score inflation of 6.43 points when malicious instructions were prepended

## Why This Works (Mechanism)

### Mechanism 1: Rubric-Anchored Zero-Shot Inference
LLMs can approximate human scoring when given explicit rubric criteria alongside essay text, without task-specific training. Instruction-tuned models interpret rubric descriptors as task instructions, mapping textual features to score levels through pattern matching learned during pretraining. Each of 9 rubric categories is evaluated separately via dedicated prompts.

### Mechanism 2: Feature-Based Regression for Linguistic Correctness
Supervised learning on extracted linguistic features can match or exceed LLM accuracy on mechanistically-gradable rubric categories. Surface, lexical, grammatical, and error features (108 total) are extracted via NLP tools, and regression models learn the mapping from feature vectors to human-assigned subscores.

### Mechanism 3: Human-Range Calibration via Consensus Reference
Model reliability is operationalized relative to human inter-rater variance rather than absolute accuracy. Human consensus scores serve as the reference, and model predictions are evaluated by whether they fall within the human score range, acknowledging inherent scoring ambiguity.

## Foundational Learning

- **Zero-shot in-context learning**: Models can perform tasks given only instructions and examples in the prompt, without weight updates. Quick check: Can you explain why the same model can grade essays using different rubrics without retraining?

- **Mean Absolute Error (MAE) vs. correlation metrics**: MAE is used as the primary metric because it preserves interpretability (points off on original scale) and enables direct comparison with human grader deviation. Quick check: Why might MAE be preferred over Pearson correlation for evaluating automated scoring against human raters?

- **Feature multicollinearity in regression pipelines**: The supervised learning pipeline removes features with absolute correlation >0.8 to prevent redundant predictors from destabilizing the model. Quick check: What could happen if highly correlated features (e.g., total errors and errors per word) were both included in the regression?

## Architecture Onboarding

- **Component map**: Rubric parsing layer -> LLM scoring layer -> Feature extraction layer -> Regression layer -> Calibration layer -> Human moderation layer

- **Critical path**: 1) Acquire held-out human-graded test set, 2) Run candidate models on test set, 3) Select best-performing model per rubric category based on MAE and bias metrics, 4) Deploy selected models for production scoring, 5) Route high-disagreement cases to human moderators

- **Design tradeoffs**: LLM vs. regression (LLMs handle content/argumentation but show systematic bias; regression is transparent and stable but limited to mechanistic categories), per-category vs. holistic scoring (per-category prevents "context rot" but increases API costs), open vs. closed models (open-weights offer transparency; closed models via API are convenient but limit auditability)

- **Failure signatures**: Prompt injection (malicious instructions can override grading prompts), category bleed (LLMs may conflate related categories), length bias (longer texts may receive higher scores), low-resource degradation (Estonian-specific performance may not transfer to other small languages)

- **First 3 experiments**: 1) Baseline validation: replicate zero-shot LLM evaluation on held-out subset to confirm MAE falls within reported ranges, 2) Prompt injection defense: implement delimiter-based input isolation and test against described attack, 3) Ensemble ablation: combine LLM outputs with regression predictions for language-accuracy categories using weighted averaging

## Open Questions the Paper Calls Out

- Under what conditions are students willing to trust and accept AI-generated grades and feedback? While the study focused on technical feasibility and accuracy, not the psychological acceptance or perceived fairness by examinees.

- How can grading rubrics be optimized to minimize ambiguity for both human raters and LLMs simultaneously? The paper found that vague descriptors challenge consistency for both humans and models, leaving the optimal design for dual-compatibility undefined.

- What specific role should teachers and assessors retain in AI-supported assessment to prevent the erosion of professional agency? The paper proposes a human-in-the-loop pipeline but does not define the specific boundaries where supportive technology becomes delegation of professional judgment.

## Limitations
- Data access restrictions prevent full independent validation as original essay texts and human scores are not publicly available
- Generalization to other small languages remains untested and requires validation
- Prompt injection defense is only demonstrated in one attack scenario without systematic adversarial testing
- Feature engineering relies on Estonian-specific tools whose configurations are not fully specified

## Confidence
- **High confidence**: Technical feasibility of LLM-based zero-shot grading and correlation between linguistic features and human scores
- **Medium confidence**: Claims about comparable accuracy to human raters, limited by restricted dataset and lack of broader language validation
- **Low confidence**: Generalizability to other small languages and robustness of prompt injection defense against systematic attacks

## Next Checks
1. Independent dataset validation: Replicate zero-shot LLM evaluation on comparable Estonian learner corpus to confirm MAE falls within reported ranges (2.66–4.97 on 0–27 scale)
2. Systematic adversarial testing: Expand prompt injection testing to include multiple attack vectors and measure residual score inflation to validate delimiter-based defense
3. Cross-linguistic generalization: Adapt feature extraction and LLM prompts for another small language (e.g., Finnish or Latvian) and evaluate performance to assess scalability