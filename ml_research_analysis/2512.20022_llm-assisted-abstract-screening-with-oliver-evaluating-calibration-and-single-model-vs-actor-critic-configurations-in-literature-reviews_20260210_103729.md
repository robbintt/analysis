---
ver: rpa2
title: 'LLM-Assisted Abstract Screening with OLIVER: Evaluating Calibration and Single-Model
  vs. Actor-Critic Configurations in Literature Reviews'
arxiv_id: '2512.20022'
source_url: https://arxiv.org/abs/2512.20022
tags:
- review
- screening
- abstract
- critic
- sensitivity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: OLIVER is an open-source pipeline that enables LLM-assisted abstract
  screening for systematic reviews. It uses structured prompts to enforce step-by-step
  reasoning against inclusion/exclusion criteria, requires binary decisions and confidence
  scores, and supports both single-model and actor-critic configurations.
---

# LLM-Assisted Abstract Screening with OLIVER: Evaluating Calibration and Single-Model vs. Actor-Critic Configurations in Literature Reviews

## Quick Facts
- arXiv ID: 2512.20022
- Source URL: https://arxiv.org/abs/2512.20022
- Authors: Kian Godhwani; David Benrimoh
- Reference count: 3
- Key outcome: Actor-critic framework improves LLM abstract screening calibration and discrimination, reducing false positives while maintaining high sensitivity

## Executive Summary
OLIVER is an open-source pipeline for LLM-assisted abstract screening in systematic reviews that uses structured prompts to enforce step-by-step reasoning against inclusion/exclusion criteria. Tested on two non-Cochrane reviews, single-model configurations achieved high sensitivity (up to 100%) but showed poor calibration with high false-positive rates. The actor-critic framework, where one model proposes decisions and another evaluates them, markedly improved discrimination and calibration, reducing Brier scores and false positives while maintaining high AUCs. Lightweight models performed well for recall, and the system enabled large-scale screening in minutes at low cost, though performance varied with review complexity and dataset characteristics.

## Method Summary
OLIVER uses structured prompts requiring explicit assessment of each inclusion and exclusion criterion in a fixed order, with outputs including binary decisions and confidence scores (0-1). The pipeline supports both single-model screening and an actor-critic configuration with three aggregation rules: "Any Include" (highest sensitivity, most false positives), "Critic Vetoes" (critic can override actor's inclusion), and "Agreement Required" (best specificity and calibration, lowest false positives). The system was tested on two systematic reviews using models including fictional versions like gpt-5, gpt-5-mini, gpt-5-nano, and grok-4-fast, though these would need substitution with available equivalents. Evaluation metrics included sensitivity, specificity, accuracy, precision, AUC, Brier scores, and Expected Calibration Error (ECE) against human ground-truth labels.

## Key Results
- Actor-critic screening improved discrimination and markedly reduced calibration error, yielding higher AUCs (0.76 to 0.91) and substantially lower Brier scores compared to single models
- Single models achieved high sensitivity (up to 100%) but had poor calibration (Brier scores >0.3, ECE >0.4) and high false-positive rates
- Lightweight models (gpt-5-nano, grok-4-fast) provided strong recall at low cost, while advanced reasoning models (gpt-5, Claude Sonnet) offered better specificity
- Performance varied significantly with review characteristics: older publications, shorter abstracts, and closed-access status were associated with reduced sensitivity

## Why This Works (Mechanism)

### Mechanism 1
The actor-critic configuration improves both discrimination (higher AUC) and calibration (lower Brier scores) compared to single-model screening. Two independent model signals reduce random variance in scoring, with the critic evaluating the actor's decision and providing corrective influence on overconfident or erratic probability estimates. Aggregating confidence scores further stabilizes estimates. Core assumption: the two models make independent errors that can cancel out when combined. Evidence: Brier scores at full-text stage were below 0.10 across all configurations, substantially outperforming single-model calibration. Break condition: If both models share systematic biases, the critic may reinforce rather than correct errors.

### Mechanism 2
Structured checklist prompts that enforce step-by-step criterion evaluation improve consistent application of inclusion/exclusion rules. The prompt appends a structured checklist requiring explicit assessment of each criterion in a fixed order, with strict rules (include only if ALL inclusions met AND ALL exclusions are "no"). This reduces variability in how models apply criteria across abstracts. Core assumption: Models can reliably follow multi-step instructions and output structured responses at scale. Evidence: Prior work found optimized prompts forcing step-by-step reasoning achieved better accuracy than zero-shot prompts. Break condition: If criteria are ambiguous or conflict, even structured prompts may produce inconsistent results.

### Mechanism 3
LLM performance on abstract screening depends significantly on dataset characteristics including publication era, open-access status, and abstract length. Papers published more recently, with open-access full texts, and longer abstracts are more likely to be represented in LLM pretraining data, providing stronger signals for classification. Older papers with shorter abstracts provide fewer explicit eligibility-relevant signals. Core assumption: Performance reflects both abstract-level reasoning and memorization from training data exposure. Evidence: OLIVER's poorer performance on Review 2 was driven by differences in publication era, abstract length, and open access status. Break condition: If a review predominantly involves recent, open-access literature, performance may be inflated by training data exposure rather than genuine reasoning capability.

## Foundational Learning

- **Concept: Calibration (Brier Scores, Expected Calibration Error)**
  - Why needed here: High accuracy/sensitivity does not guarantee reliable confidence estimates. Poor calibration means model confidence cannot be trusted for human-AI decision workflows.
  - Quick check question: If a model reports 90% confidence on its decisions but only 60% are correct, is it well-calibrated?

- **Concept: Actor-Critic Architecture**
  - Why needed here: This is the core ensemble configuration tested. Understanding that the actor proposes decisions and the critic evaluates them is essential for interpreting results.
  - Quick check question: In the "Agreement Required" configuration, how many models must support inclusion for an abstract to be included?

- **Concept: Sensitivity-Specificity Trade-off in Low-Prevalence Datasets**
  - Why needed here: Systematic reviews typically have 1-10% inclusion rates. High sensitivity often comes at the cost of many false positives, increasing downstream human screening workload.
  - Quick check question: Why might 100% sensitivity with low specificity still create substantial human labor burden?

## Architecture Onboarding

- **Component map:** User uploads CSV → Pipeline generates JSONL with structured prompts → Parallel API calls → Results saved incrementally → Output parsing extracts decisions + confidence → Evaluation metrics computed

- **Critical path:** 1. User uploads CSV with titles/abstracts → 2. Pipeline generates JSONL with embedded prompts → 3. Parallel API calls with retry logic → 4. Results saved incrementally to prevent data loss → 5. Output parsing extracts decisions + confidence → 6. Evaluation metrics computed against human labels

- **Design tradeoffs:**
  - Any Include = highest sensitivity, most false positives
  - Agreement Required = best specificity and calibration, lowest false positives, reduced sensitivity
  - Lightweight models (gpt-5-nano, grok-4-fast) = fast/cheap, strong recall, more false positives
  - Advanced reasoning models (gpt-5, Claude Sonnet) = better specificity, may miss true includes

- **Failure signatures:**
  - Very low sensitivity (<30%) suggests criteria may be too narrow or dataset contains many older/closed-access papers
  - High Brier scores (>0.5) indicate confidence estimates are unreliable for single-model configurations
  - API throttling failures on large datasets (>5000 abstracts) require adjusting rate limits in parallel scripts

- **First 3 experiments:**
  1. Baseline calibration check: Run single-model screening on held-out sample, compute Brier score and ECE before deploying actor-critic
  2. Aggregation rule comparison: Test all three configurations on same 200-abstract sample to quantify sensitivity/specificity tradeoffs
  3. Dataset characteristic audit: Analyze publication year distribution and open-access status before screening; if >50% pre-2010 or closed-access, expect reduced sensitivity

## Open Questions the Paper Calls Out

- **Open Question 1:** Does incorporating human feedback during an optional training phase improve LLM screening calibration and accuracy? The training phase was implemented but never tested against baseline performance.
- **Open Question 2:** What is the relationship between model-generated justifications, calibration, and accuracy in LLM screening decisions? API constraints prevented systematic collection of justifications.
- **Open Question 3:** How does LLM screening performance generalize across diverse review domains, criterion structures, and evidence types beyond non-Cochrane mental health reviews? Only two reviews were tested, limiting generalizability.
- **Open Question 4:** Can systematic prompt engineering improve the sensitivity-specificity trade-off compared to the limited prompting strategies tested? Prompt modifications had mixed effects, optimal design remains unknown.

## Limitations
- Evaluated only two non-Cochrane reviews with small inclusion sets (63 and 71 included abstracts), limiting generalizability across domains
- Fictional model names (gpt-5 series, grok-4-fast) require substitution with available equivalents, potentially affecting comparability
- Did not examine human-in-the-loop workflows or cost-benefit tradeoffs for real-world implementation

## Confidence
- **High confidence**: Actor-critic framework improves calibration and discrimination over single models
- **Medium confidence**: Lightweight models perform comparably to advanced reasoning models for recall
- **Medium confidence**: Publication era and open-access status significantly affect performance
- **Low confidence**: OLIVER can replace human screening in any systematic review

## Next Checks
1. **Domain generalizability test**: Evaluate OLIVER on reviews from multiple research domains to assess performance consistency across different terminology and methodology types
2. **Human-AI workflow validation**: Implement user study comparing traditional human screening, LLM-assisted screening, and hybrid approaches to quantify actual time savings and accuracy trade-offs
3. **Calibration robustness check**: Test actor-critic configuration across varying prevalence rates (1-20% inclusion) to determine if calibration improvements hold under different review contexts