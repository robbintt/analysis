---
ver: rpa2
title: 'AutoGeTS: Knowledge-based Automated Generation of Text Synthetics for Improving
  Text Classification'
arxiv_id: '2508.10000'
source_url: https://arxiv.org/abs/2508.10000
tags:
- data
- text
- performance
- synthetic
- class
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of improving text classification
  models when real-world training data is scarce. The authors propose AutoGeTS, a
  knowledge-based automated workflow that uses large language models (LLMs) to generate
  synthetic training data.
---

# AutoGeTS: Knowledge-based Automated Generation of Text Synthetics for Improving Text Classification

## Quick Facts
- **arXiv ID:** 2508.10000
- **Source URL:** https://arxiv.org/abs/2508.10000
- **Reference count:** 40
- **Primary result:** Demonstrates an ensemble algorithm that selects optimal LLM prompt strategies per class/metric context, significantly improving text classification performance over individual search strategies.

## Executive Summary
This paper addresses the challenge of improving text classification models when real-world training data is scarce, particularly for minority classes. The authors propose AutoGeTS, a knowledge-based automated workflow that uses large language models (LLMs) to generate synthetic training data. Through systematic experimentation across multiple datasets, they demonstrate that no single search strategy or optimization metric is universally superior. Instead, they develop an ensemble algorithm that leverages a knowledge map from systematic experimentation to select optimal strategies and metrics per class and objective, significantly improving classification performance compared to individual strategies.

## Method Summary
AutoGeTS improves text classification by generating synthetic data via LLMs when real training data is scarce. The method uses three search strategies (Sliding Window, Hierarchical Sliding Window, Genetic Algorithm) to systematically select effective example messages from the training data, which are then used as prompts for GPT-3.5 to generate synthetic text. A knowledge map is constructed by running all strategy-metric combinations on sampled data to identify which approach works best for each class-objective pair. An ensemble algorithm then consults this map to select the optimal strategy for a given optimization request, balancing exploration costs with targeted performance improvements.

## Key Results
- No single search strategy (SW, HSW, GA) or optimization metric is universally superior across all classes and objectives
- Synthetic data generated by GPT-3.5 significantly outperforms simple heuristic augmentation (EDA) in improving classification metrics
- The ensemble algorithm leveraging the knowledge map achieves better performance than any individual strategy by selecting context-appropriate approaches per class
- Cross-metric optimization can induce positive transfer effects, where improving one class benefits others or global metrics

## Why This Works (Mechanism)

### Mechanism 1: Context-Dependent Search Strategy Selection
The workflow runs systematic sampling to construct a "knowledge map" (a lookup table of performance results). When a model requires improvement, an ensemble algorithm queries this map to identify which strategy is statistically most likely to succeed for that specific class-metric context, rather than relying on a fixed heuristic. The core assumption is that the relative performance of strategies observed during the initial sampling phase remains valid for subsequent optimization phases.

### Mechanism 2: Synthetic Boundary Refinement via Semantic Diversity
Synthetic data generated by LLMs appears to improve classification performance by providing semantically diverse examples that reinforce the decision boundary for scarce classes. The selected search strategies identify "effective" example messages (prompts), and the LLM uses these to generate synthetic text that fills gaps in the feature space. This augmentation helps the classifier resolve ambiguities in minority classes without collecting real data.

### Mechanism 3: Cross-Metric Optimization Transfer
Optimizing a model for a specific objective can induce positive or negative changes in other classes or global metrics. Adding synthetic data for one class alters the global feature space, potentially leading to "collateral benefit" (improving Class T12 while targeting T13) or negative interference. The ensemble algorithm leverages this by potentially selecting a proxy class to improve the actual target class.

## Foundational Learning

**Concept: Imbalanced Classification Metrics**
- *Why needed here:* The paper optimizes for specific metrics (Balanced Accuracy, Recall, F1) that behave differently on minority vs. majority classes. Understanding why Accuracy is a poor metric for small classes is essential to grasp the optimization objectives.
- *Quick check question:* If Class A has 10 samples and Class B has 1000, why might "99% Accuracy" be a misleading metric for a model that always predicts Class B?

**Concept: Prompt Engineering & In-Context Learning**
- *Why needed here:* AutoGeTS automates the selection of "input examples" (prompts) for the LLM. You must understand that the LLM is not being retrained, but rather steered by the context provided in these examples.
- *Quick check question:* How does providing 3 specific ticket examples to an LLM change its output compared to a zero-shot request?

**Concept: Optimization Search Strategies**
- *Why needed here:* The core algorithmic comparison is between Sliding Window (local search) and Genetic Algorithms (global search). Knowing the difference is required to interpret the "Knowledge Map" results.
- *Quick check question:* Why might a Genetic Algorithm find a better solution than a Sliding Window in a high-dimensional feature space, and what is the tradeoff in computational cost?

## Architecture Onboarding

**Component map:**
Baseline Model (CatBoost) -> Search Strategies (SW, HSW, GA) -> Synthetic Generator (GPT-3.5) -> Knowledge Map -> Ensemble Algorithm

**Critical path:**
1. **Map Creation:** Execute systematic sampling (running all 180 combinations) to populate the Knowledge Map
2. **Selection:** Receive a business requirement (e.g., "Improve T13 accuracy")
3. **Execution:** The Ensemble Algorithm selects the top-k settings from the map (e.g., GA-CBA)
4. **Loop:** Generate synthetic data, retrain model Mj, evaluate, and update the model pool

**Design tradeoffs:**
- **Exploration vs. Cost:** Building the Knowledge Map is computationally expensive (running 180+ workflows) but enables cheaper, targeted optimization later
- **LLM vs. Heuristics:** Using GPT-3.5 yields higher improvements than EDA but incurs API costs and latency

**Failure signatures:**
- **Negative Transfer:** Red numbers in the results tables indicate that optimizing Class X caused Class Y's performance to drop
- **Metric Gaming:** High Recall improvements that come at the cost of unacceptable Precision (too many false positives), requiring the use of Balanced Accuracy constraints

**First 3 experiments:**
1. **Reproduce Knowledge Map (Subset):** Run the systematic sampling on just 3 classes (e.g., T1, T10, T13) to verify that the "best strategy" does indeed vary by class as claimed
2. **Validate LLM Superiority:** Compare synthetic data generated by GPT-3.5 against the Easy Data Augmentation (EDA) tool for a single minority class to measure the performance gap
3. **Test Ensemble Logic:** Implement the ensemble selector to solve a multi-objective problem (e.g., "Improve T13 recall without dropping overall F1") and compare the result against a naive "optimize only T13" approach to observe constraint satisfaction

## Open Questions the Paper Calls Out

**Open Question 1:** What are the semantic or statistical features that determine the effectiveness of specific example messages for synthetic data generation? The conclusion states a plan to "gain deep understanding of different factors that make some example messages more effective than others." The current work treats the search strategies as black-box optimizations that successfully locate effective examples, but it does not analyze *why* those specific examples lead to high-quality synthetic data.

**Open Question 2:** How dependent are the optimal search strategies on the underlying classification architecture? The authors explicitly limit their experiments to a single model type, stating, "we trained all models using CatBoost consistently with fixed hyperparameters." The "knowledge map" is generated based on the error surfaces and feature spaces of the CatBoost model. It is unclear if the optimal strategy remains superior when the classification model is changed to a deep neural network or transformer.

**Open Question 3:** Can knowledge maps be transferred between datasets or domains to avoid the computational cost of systematic sampling? The paper establishes that "there is no superior strategy" universally, requiring a knowledge map to be built for each dataset. This implies a "cold start" cost for every new application. It is unknown if a map built for one ticketing system (or domain) can predict effective strategies for another, or if meta-learning approaches could bypass this initial exploration.

## Limitations
- The systematic sampling phase is computationally expensive (180 combinations), making it impractical for rapid iteration without substantial compute resources
- The approach relies on the stability assumption that search strategy performance remains consistent across optimization phases, which may not hold if data distribution shifts
- The private industrial dataset limits reproducibility and generalizability to other domains

## Confidence

**High:** The core mechanism of using LLM-generated synthetic data to improve classification performance for minority classes is well-supported by experimental results and aligns with established techniques in imbalanced learning.

**Medium:** The ensemble algorithm's ability to select optimal strategies based on the knowledge map is logically sound, but its effectiveness depends on the stability assumption and the quality of the initial systematic sampling.

**Low:** The claim of positive transfer (improving one class benefits others) is less strongly supported in the provided corpus and requires further empirical validation across diverse datasets.

## Next Checks
1. **Knowledge Map Stability Test:** Validate that the relative performance of search strategies observed during initial sampling remains valid after retraining the model with synthetic data or when applied to a different but related dataset.
2. **Synthetic Data Quality Audit:** Compare the semantic consistency and diversity of synthetic data generated by GPT-3.5 against real data from minority classes to quantify the risk of label noise or hallucination.
3. **Multi-Objective Constraint Satisfaction:** Test the ensemble algorithm on a multi-objective problem (e.g., improve T13 recall while maintaining overall F1 above a threshold) to verify that it can effectively balance competing metrics and avoid catastrophic degradation of majority classes.