---
ver: rpa2
title: Analysis of Overparameterization in Continual Learning under a Linear Model
arxiv_id: '2502.10442'
source_url: https://arxiv.org/abs/2502.10442
tags:
- task
- learning
- where
- tasks
- forgetting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a theoretical analysis of catastrophic forgetting
  in continual learning under a linear regression model with overparameterization.
  The authors study a two-task setting where tasks are related by a random orthogonal
  transformation, motivated by permutation tasks in image classification.
---

# Analysis of Overparameterization in Continual Learning under a Linear Model

## Quick Facts
- arXiv ID: 2502.10442
- Source URL: https://arxiv.org/abs/2502.10442
- Authors: Daniel Goldfarb; Paul Hand
- Reference count: 7
- Key outcome: This paper presents a theoretical analysis of catastrophic forgetting in continual learning under a linear regression model with overparameterization.

## Executive Summary
This paper presents a theoretical analysis of catastrophic forgetting in continual learning under a linear regression model with overparameterization. The authors study a two-task setting where tasks are related by a random orthogonal transformation, motivated by permutation tasks in image classification. They show that when the model is sufficiently overparameterized (with p features, n training samples, and d latent features such that d < n < p), gradient descent with no explicit forgetting-prevention mechanism can achieve low risk on the first task even after training on the second task.

## Method Summary
The authors analyze a linear regression model in a continual learning setting with two tasks. They assume the tasks are related by a random orthogonal transformation and study the behavior of gradient descent with no explicit forgetting-prevention mechanisms. The key insight is that overparameterization (when the number of features p exceeds the number of samples n) can mitigate catastrophic forgetting. The analysis relies on minimum-norm least squares solutions and random matrix theory to derive non-asymptotic risk bounds for both single-task and sequential task settings.

## Key Results
- Overparameterization alone can mitigate catastrophic forgetting when p is large relative to n and d
- The risk on the first task remains bounded after training on the second task
- The ratio of forgetting to initial learning becomes negligible in the overparameterized regime
- The paper provides non-asymptotic risk bounds for single-task linear regression that may be of independent interest to double descent theory

## Why This Works (Mechanism)
Overparameterization provides a large hypothesis space that allows the model to find solutions that generalize well to both tasks simultaneously. When p >> n, the minimum-norm least squares solution has desirable properties that prevent catastrophic forgetting. The random orthogonal transformation between tasks creates a scenario where the solution space for the second task overlaps significantly with that of the first task, allowing the model to maintain performance on both tasks without explicit forgetting-prevention mechanisms.

## Foundational Learning

1. **Linear regression and minimum-norm solutions**
   - Why needed: Forms the basis of the theoretical analysis
   - Quick check: Can you derive the minimum-norm least squares solution?

2. **Random matrix theory**
   - Why needed: Used to analyze the spectral properties of the data matrix in overparameterized regimes
   - Quick check: Do you understand the concentration of singular values for random matrices?

3. **Catastrophic forgetting in continual learning**
   - Why needed: The primary phenomenon being analyzed
   - Quick check: Can you explain why neural networks typically forget previous tasks when trained on new ones?

4. **Double descent theory**
   - Why needed: Provides context for the risk bounds in overparameterized regimes
   - Quick check: Can you describe the double descent phenomenon in generalization error?

## Architecture Onboarding

**Component Map:** Data matrix X -> Gradient descent optimization -> Minimum-norm least squares solution -> Risk estimation

**Critical Path:** The key insight is that when p > n, the minimum-norm solution has a bias-variance tradeoff that favors both tasks simultaneously, preventing catastrophic forgetting.

**Design Tradeoffs:** The paper sacrifices realism (linear model, idealized task relationships) for analytical tractability and theoretical guarantees. This allows for rigorous mathematical analysis but limits direct applicability to complex neural network architectures.

**Failure Signatures:** The theoretical benefits of overparameterization may not translate to practical settings where task relationships are more complex, data is non-Gaussian, or the model is nonlinear.

**First Experiments:**
1. Verify the risk bounds empirically for linear regression in the overparameterized regime
2. Test if overparameterization alone provides forgetting mitigation in simple neural networks on permutation tasks
3. Compare the theoretical risk bounds with empirical performance on synthetic data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the mitigation of catastrophic forgetting via overparameterization persist in non-linear models or realistic neural network architectures?
- Basis in paper: [explicit] The Conclusion states that "the analysis of more realistic non-linear or even neural network models would provide a theoretical understanding that is more closely tied to the techniques currently used in practice."
- Why unresolved: The current theoretical results rely on the closed-form solution of minimum-norm least squares and random matrix theory specific to linear regression.
- What evidence would resolve it: Theoretical bounds or empirical scaling laws demonstrating that the ratio of forgetting to learning decreases with parameter count in non-linear settings (e.g., neural tangent kernel regime).

### Open Question 2
- Question: How does the theoretical relationship between overparameterization and forgetting change when tasks are disjoint (split tasks) rather than related by orthogonal transformations?
- Basis in paper: [explicit] The Discussion notes that "an extension to other relevant continual learning benchmarks like split tasks would be a beneficial step towards a better understanding of forgetting regimes."
- Why unresolved: The current analysis assumes tasks share labels (y) and are related by a random orthogonal matrix, which idealizes permutation tasks but does not model scenarios where output classes change between tasks.
- What evidence would resolve it: A derivation of risk bounds for a two-task setting where the ground truth parameters Î¸ or the latent features differ between tasks.

### Open Question 3
- Question: Can the linear framework presented here be extended to theoretically quantify the benefits of explicit continual learning algorithms, such as Elastic Weight Consolidation (EWC) or replay buffers?
- Basis in paper: [explicit] Section 4 suggests the "field would benefit from the direct analysis of existing continual learning methods with the goal of better understanding why they work well in practice."
- Why unresolved: This work isolates overparameterization by analyzing "gradient descent with no explicit algorithmic mechanism to prevent forgetting."
- What evidence would resolve it: An analytical comparison showing that adding a regularization term (like EWC) or rehearsal step to the loss function results in strictly tighter risk bounds than overparameterization alone.

## Limitations

- The analysis assumes a highly idealized setting with random orthogonal transformations between tasks, which may not capture the complexity of real-world continual learning scenarios.
- The linear model assumption, while analytically tractable, may not reflect the nonlinear dynamics present in practical deep learning systems.
- The study focuses on a two-task setting, limiting generalizability to longer task sequences common in continual learning applications.

## Confidence

- **High confidence** in the theoretical framework and mathematical proofs within the stated assumptions
- **Medium confidence** in the practical implications of overparameterization for mitigating catastrophic forgetting, given the idealized setting
- **Low confidence** in direct translation of results to deep learning models and complex task sequences

## Next Checks

1. Empirical validation on deep neural networks using permutation-based image classification tasks to test if overparameterization alone provides similar forgetting mitigation in nonlinear settings.

2. Extension of analysis to longer task sequences (more than two tasks) to assess the scalability of overparameterization benefits in continual learning scenarios.

3. Investigation of the trade-offs between overparameterization, model capacity, and computational resources in practical continual learning systems to provide guidance on implementation strategies.