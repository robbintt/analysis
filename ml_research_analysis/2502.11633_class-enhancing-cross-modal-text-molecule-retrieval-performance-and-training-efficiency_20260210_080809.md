---
ver: rpa2
title: 'CLASS: Enhancing Cross-Modal Text-Molecule Retrieval Performance and Training
  Efficiency'
arxiv_id: '2502.11633'
source_url: https://arxiv.org/abs/2502.11633
tags:
- training
- learning
- retrieval
- molecule
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses cross-modal text-molecule retrieval, where
  molecules are represented by SMILES and text descriptions. Existing methods overlook
  adaptive training adjustments and efficiency.
---

# CLASS: Enhancing Cross-Modal Text-Molecule Retrieval Performance and Training Efficiency

## Quick Facts
- **arXiv ID:** 2502.11633
- **Source URL:** https://arxiv.org/abs/2502.11633
- **Reference count:** 40
- **Primary result:** Improves cross-modal text-molecule retrieval Hits@1 by 1.0%-1.9% and reduces training data usage to 84.44%-86.73%

## Executive Summary
This paper addresses cross-modal text-molecule retrieval where molecules are represented by SMILES and text descriptions. The authors identify that existing methods overlook adaptive training adjustments and efficiency. CLASS introduces a curriculum learning framework that quantifies sample difficulty based on modality similarities, progressively introduces training samples from easy to difficult, and employs adaptive intensity learning to adjust training objectives dynamically. Experiments on ChEBI-20 demonstrate superior performance and efficiency gains, reducing early-stage training data while improving retrieval accuracy.

## Method Summary
CLASS is a curriculum learning framework for cross-modal text-molecule retrieval that quantifies sample difficulty through cross-modal similarity analysis, then progressively introduces samples from easy to difficult while adjusting training intensity. The method uses pre-trained encoders (SciBERT for text, Mol2Vec for molecules) to compute similarity-based difficulty scores, schedules samples via a linear pacing function λ = α + β·k, and scales losses with adaptive intensity curves γ. The framework is tested on ChEBI-20 with AMAN and ORMA backbones, achieving performance improvements while reducing training data requirements.

## Key Results
- CLASS improves Hits@1 by 1.0%-1.9% and Mean Rank by 1.76-3.35 on ChEBI-20
- Training data usage reduced to 84.44%-86.73% compared to baseline methods
- Adaptive intensity learning (γ₂ = k/(1+k)) outperforms sigmoid alternative (γ₁)
- Both text and molecule similarity modalities are necessary for optimal performance

## Why This Works (Mechanism)

### Mechanism 1: Cross-Modal Sample Difficulty Quantification
The method computes cosine similarity between each sample and all others in both text and molecule embedding spaces, then counts neighbors exceeding threshold σ=0.99. This count becomes the difficulty score—more similar neighbors means higher difficulty because the model must discriminate among confusable candidates. The assumption is that difficulty correlates with density of similar samples in joint embedding space.

### Mechanism 2: Progressive Sample Scheduling via Curriculum Pacing
Samples are sorted by difficulty and introduced progressively during training. At epoch k, only the first λ|Z| samples train, where λ = α + β·k grows linearly from initial ratio α. Early epochs use fewer, easier samples; harder samples enter progressively. This reduces early-stage sample requirements while maintaining or improving final performance.

### Mechanism 3: Adaptive Intensity Learning
Loss weights are scaled by curriculum stage using intensity curves γ. Early epochs have lower γ (gentler gradients); later epochs have higher γ (stronger signal from hard samples). The inverse proportion curve γ₂ = k/(1+k) is preferred over sigmoid γ₁ = 1/(1+e^(-k-1)) because it provides more uniform progression without premature convergence.

## Foundational Learning

- **Cross-Modal Contrastive Learning:** Why needed - The backbone models use contrastive objectives to align text and molecule embeddings; CLASS modifies how these losses are weighted, not the loss itself. Quick check - Can you explain why contrastive learning requires negative samples and how temperature affects hard negative mining?

- **Curriculum Learning Pacing Functions:** Why needed - CLASS uses linear pacing; understanding alternative pacing (logarithmic, step) helps diagnose if the chosen schedule fits the data distribution. Quick check - What happens if pacing introduces hard samples before the model has learned basic discrimination?

- **Information Retrieval Metrics (Hits@K, MRR, Mean Rank):** Why needed - Evaluation requires interpreting Hits@1 (exact match at top), Hits@10 (recall in top 10), MRR (rank-weighted score), Mean Rank (average position). Quick check - Why might Hits@1 improve while Mean Rank doesn't change proportionally?

## Architecture Onboarding

- **Component map:** [SciBERT encoder] → text embeddings ht → [Similarity Computer] → St → [Difficulty Quantifier] → Ni → [Sample Scheduler] → Dk subset → [Adaptive Intensity γ] → scaled loss Lepoch → [Backbone Model] → gradient updates
- **Critical path:** Difficulty quantification runs once before training (O(n²) similarity computation). Scheduler and intensity modules run per-epoch with O(1) overhead.
- **Design tradeoffs:**
  - σ threshold: Higher (0.99) = stricter similarity = fewer "hard" samples; may under-identify difficulty
  - α, β pacing: Small α + large β = aggressive curriculum (faster but risks skipping stages)
  - γ curve selection: γ₂ (inverse proportion) outperforms γ1 (sigmoid) per Table 4
- **Failure signatures:**
  - Hits@1 flat or decreasing: Check if σ too low or α too high
  - Training instability in later epochs: γ curve growing too fast
  - No efficiency gain: Scheduler not activating
- **First 3 experiments:**
  1. Run CLASS(AMAN) with α=40, β=3, σ=0.99 on ChEBI-20 subset (5K samples)
  2. Disable text similarity (set St=0) and re-run to confirm modality contribution
  3. Sweep β ∈ {1, 2, 3, 4} with fixed α=40 and plot Hits@1 vs. β

## Open Questions the Paper Calls Out
1. Can more sophisticated, learnable functions replace the manual sigmoid or inverse proportion curves to better adapt learning intensity across training stages?
2. Does integrating advanced molecular fingerprinting or graph-based metrics into the similarity calculation improve sample difficulty quantification over the current Mol2Vec approach?
3. How can the adaptive intensity learning mechanism be extended to dynamically weight distinct loss components in multi-objective training?
4. Does the performance and efficiency of CLASS generalize to larger, noisier chemical datasets beyond ChEBI-20?

## Limitations
- Curriculum difficulty quantification relies on static similarity thresholds that may not generalize across datasets
- Framework tested only on ChEBI-20 (33,010 pairs), limiting scalability assessment
- No comparison with other curriculum learning approaches for cross-modal retrieval

## Confidence
- **High confidence:** Training efficiency gains (84.44%-86.73% sample reduction), basic curriculum pacing mechanism
- **Medium confidence:** Performance improvements (1.0%-1.9% Hits@1), adaptive intensity learning benefits
- **Low confidence:** Cross-modal difficulty quantification correlation, generalization to other molecular datasets

## Next Checks
1. Run CLASS with varying σ thresholds (0.95, 0.99, 0.999) on ChEBI-20 and measure Hits@1 sensitivity
2. Systematically vary α and β across full range, plotting Hits@1 and sample usage efficiency to identify optimal pacing curves
3. Implement CLASS on a simpler contrastive baseline (e.g., InfoNCE with mean pooling) rather than AMAN/ORMA to isolate curriculum effects from backbone-specific optimizations