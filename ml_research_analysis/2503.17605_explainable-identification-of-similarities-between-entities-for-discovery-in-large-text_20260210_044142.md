---
ver: rpa2
title: Explainable identification of similarities between entities for discovery in
  large text
arxiv_id: '2503.17605'
source_url: https://arxiv.org/abs/2503.17605
tags:
- similarities
- text
- documents
- words
- word
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel method for identifying explainable
  similarities between text documents by analyzing n-gram patterns. The method assigns
  weights to n-grams based on their frequency in both documents while penalizing common
  words in the English language, effectively filtering out noise and highlighting
  meaningful patterns.
---

# Explainable identification of similarities between entities for discovery in large text

## Quick Facts
- arXiv ID: 2503.17605
- Source URL: https://arxiv.org/abs/2503.17605
- Authors: Akhil Joshi; Sai Teja Erukude; Lior Shamir
- Reference count: 0
- Primary result: Novel n-gram based method identifies explainable document similarities through frequency-weighted patterns, outperforming traditional approaches like Doc2Vec while providing more actionable insights than LLMs like ChatGPT.

## Executive Summary
This paper presents a novel method for identifying explainable similarities between text documents by analyzing n-gram patterns. The method assigns weights to n-grams based on their frequency in both documents while penalizing common words in the English language, effectively filtering out noise and highlighting meaningful patterns. The algorithm leverages NLP tools like SpaCy and NLTK for preprocessing and integrates named entity recognition and noun chunk analysis to enhance the quality of insights. Tested across diverse text types including biographies, classical literature, and soccer player profiles, the method successfully identifies specific and explainable similarities.

## Method Summary
The method preprocesses two documents using SpaCy (lemmatization, NER, noun chunks) and NLTK (POS tagging), then extracts unigrams, bigrams, and trigrams from each document. It computes n-gram frequencies in both documents and looks up baseline English frequencies from COCA. Weights are calculated using a logarithmic formula that emphasizes n-grams frequent in both documents but rare in general English, with additional multipliers for named entities and noun chunks. The results are visualized as word clouds for each n-gram order, providing intuitive representations of document similarities.

## Key Results
- Successfully identifies specific, explainable similarities across diverse text types including biographies, literature, and sports profiles
- Outperforms traditional approaches like Doc2Vec in identifying actionable similarities
- Provides more interpretable and actionable insights compared to large language models like ChatGPT, which tend to produce generalized summaries
- Produces intuitive visualizations through word clouds that make similarity patterns easily understandable

## Why This Works (Mechanism)

### Mechanism 1: Frequency-Based Weighting with Language Corpus Penalty
The algorithm computes weights using: Weight = f₁ × log(f₁/fₑ) + f₂ × log(f₂/fₑ), where f₁ and f₂ are n-gram frequencies in the two documents, and fₑ is the baseline frequency from COCA. Higher-order refinements add squared differences and thresholding. The core assumption is that words rare in general English but common in both documents indicate topical similarity rather than linguistic coincidence.

### Mechanism 2: Linguistic Feature Enhancement via Named Entities and Noun Chunks
A Base Multiplier = 1 + 0.5 × (Entity Presence + Noun Chunk Presence) boosts weights when SpaCy identifies the n-gram as a named entity or noun chunk. The assumption is that named entities and noun chunks carry more semantic significance than arbitrary word sequences.

### Mechanism 3: Multi-Scale Pattern Detection via N-gram Orders
The method analyzes unigrams, bigrams, and trigrams separately to capture both individual concepts and multi-word phrases that single-word analysis would fragment. This preserves compound terms like "Nobel Prize" or "general relativity" that would lose meaning when split.

## Foundational Learning

- Concept: **N-gram Extraction**
  - Why needed here: The entire method is built on n-gram generation and comparison. Without understanding contiguous word sequences, you cannot modify the core algorithm.
  - Quick check question: Given "The cat sat on the mat," what are the bigrams?

- Concept: **Lemmatization vs. Stemming**
  - Why needed here: SpaCy lemmatization normalizes word forms before comparison, so "running," "ran," and "runs" match as "run."
  - Quick check question: Why might lemmatization handle "better" differently than stemming?

- Concept: **Corpus-Based Frequency Normalization**
  - Why needed here: The penalty mechanism requires understanding how baseline English frequencies are derived and applied.
  - Quick check question: If a word appears 500 times in a 1-million-word corpus, what is its per-thousand frequency?

## Architecture Onboarding

- Component map: Preprocessing -> N-gram Generator -> Frequency Calculator -> Corpus Lookup -> Weighting Engine -> Visualization
- Critical path: 1) Preprocess both documents (lemmatize, extract entities/chunks) 2) Generate n-grams for each document 3) Identify common n-grams 4) Compute weights: frequency component × linguistic multiplier 5) Apply threshold filtering 6) Render word cloud
- Design tradeoffs: Determinism vs. semantic depth (fully explainable but cannot capture relationships requiring inference), Precision vs. recall (produces some noise alongside useful terms), Language specificity (English-only; requires equivalent tools for other languages)
- Failure signatures: Empty word clouds (insufficient n-gram overlap or over-aggressive filtering), Generic terms dominating (threshold too permissive for domain), Missing expected matches (lemmatization or NER errors), Technical jargon incorrectly penalized (high English frequency despite domain relevance)
- First 3 experiments: 1) Reproduce Einstein-Hawking comparison; verify "relativity," "Nobel prize" appear while checking noise levels 2) Test dissimilar pair (e.g., Einstein vs. appliance manual) to observe low-similarity behavior 3) Ablation: disable entity/noun chunk multipliers to measure their contribution

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the framework be expanded to identify complex semantic patterns that do not rely on exact n-gram co-occurrence?
- Basis in paper: The authors state the method is "limited to common terms" and cannot identify "complex patterns that require deep understanding of the subjects."
- Why unresolved: The current deterministic algorithm relies strictly on string matching and frequency analysis, lacking inference capabilities.
- What evidence would resolve it: Modification of the algorithm to integrate semantic embeddings, demonstrating the identification of implied similarities absent shared vocabulary.

### Open Question 2
- Question: To what extent does the strict reliance on exact n-gram matching fail to capture synonymous concepts expressed through different vocabulary?
- Basis in paper: The method identifies similarities based on shared n-grams, penalizing terms not common to both documents, potentially missing semantic equivalence (e.g., "soccer" vs. "football").
- Why unresolved: The paper does not test for recall on texts discussing the same topics using distinct terminology.
- What evidence would resolve it: A comparative analysis on a dataset with high lexical variance, measuring the method's recall against a semantic similarity ground truth.

### Open Question 3
- Question: Can the method's "actionable insights" be quantified to objectively validate its superiority over large language model summaries?
- Basis in paper: The evaluation relies on qualitative visualizations (word clouds) and anecdotal comparisons to ChatGPT rather than statistical metrics.
- Why unresolved: "Actionability" is asserted through examples, but without user studies or quantitative precision/recall scores, the claim remains subjective.
- What evidence would resolve it: A user study measuring the speed and accuracy of insight discovery using this tool versus LLM summaries.

## Limitations
- Performance highly dependent on NLP preprocessing accuracy (lemmatization, NER, noun chunk detection) without quantified impact
- No quantitative evaluation metrics provided, relying instead on qualitative comparisons
- English-only design limits domain applicability without equivalent NLP tools
- Cannot capture semantic relationships requiring inference beyond lexical overlap

## Confidence

**High Confidence:**
- The n-gram frequency weighting mechanism with corpus-based penalty is mathematically sound and implementable
- The use of named entities and noun chunks as linguistic features has face validity
- Multi-scale n-gram analysis captures different granularities of similarity

**Medium Confidence:**
- The method produces "specific and explainable" similarities based on qualitative results
- Performance claims versus ChatGPT and Doc2Vec are plausible but not rigorously validated
- The approach generalizes across diverse text types (biographies, literature, sports profiles)

**Low Confidence:**
- No quantitative evidence for superiority over existing methods
- Generalization claims to other domains lack empirical support
- The method's effectiveness with highly dissimilar documents remains untested

## Next Checks

1. **Quantitative Performance Evaluation**: Implement precision, recall, and F1-score metrics by creating a ground truth dataset of known document similarities and measuring the method's accuracy

2. **Ablation Study**: Systematically disable components (NER/noun chunk multipliers, different weighting formulas) to quantify their individual contributions to overall performance

3. **Cross-Domain Robustness Test**: Apply the method to diverse text types (technical documentation, news articles, social media) to evaluate its generalizability beyond the tested domains