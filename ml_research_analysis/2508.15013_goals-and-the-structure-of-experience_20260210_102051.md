---
ver: rpa2
title: Goals and the Structure of Experience
arxiv_id: '2508.15013'
source_url: https://arxiv.org/abs/2508.15013
tags:
- state
- telic
- learning
- states
- experience
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces telic states as a novel computational framework
  for understanding goal-directed behavior and state representation in cognitive agents.
  The framework proposes that goals and state representations co-emerge interdependently
  from experience distributions, with telic states defined as classes of goal-equivalent
  experience distributions.
---

# Goals and the Structure of Experience

## Quick Facts
- arXiv ID: 2508.15013
- Source URL: https://arxiv.org/abs/2508.15013
- Authors: Nadav Amir; Stas Tiomkin; Angela Langdon
- Reference count: 40
- Primary result: Introduces telic states as a novel computational framework for understanding goal-directed behavior through co-emergent goal and state representations

## Executive Summary
This paper introduces telic states as a novel computational framework for understanding goal-directed behavior and state representation in cognitive agents. The framework proposes that goals and state representations co-emerge interdependently from experience distributions, with telic states defined as classes of goal-equivalent experience distributions. The authors argue that this approach provides a more cognitively plausible account of purposeful behavior than traditional reinforcement learning models, which assume separate state representations and reward functions.

The telic states framework formalizes goal-directed learning in terms of statistical divergence between behavioral policies and desirable experience features. It provides a unified account of behavioral, phenomenological, and neural dimensions of purposeful behaviors across diverse substrates. The authors present a learning algorithm that can flexibly adapt to shifting goals and demonstrate its application through a dual-goal navigation task.

## Method Summary
The telic states framework formalizes goal-directed learning through statistical divergence between behavioral policies and desirable experience features. The approach defines telic states as classes of goal-equivalent experience distributions that emerge from experience rather than being predefined. The learning algorithm adapts to shifting goals by updating these telic state representations based on observed experience patterns. The framework is demonstrated through a dual-goal navigation task where an agent must balance competing objectives.

## Key Results
- Telic states framework provides unified account of behavioral, phenomenological, and neural dimensions of purposeful behaviors
- Learning algorithm demonstrates flexible adaptation to shifting goals in dual-goal navigation task
- Framework predicts that brain regions traditionally encoding state or value information may jointly encode telic states

## Why This Works (Mechanism)
The telic states framework works by reconceptualizing goal-directed behavior as emerging from the statistical structure of experience distributions rather than from separate state and reward functions. By defining goals as classes of equivalent experience distributions, the framework creates a natural coupling between what an agent experiences and what it aims to achieve. This co-emergence eliminates the need for externally imposed reward functions and allows goals to adapt organically as experience distributions shift. The statistical divergence between current and desired experience patterns provides a natural learning signal that guides behavior toward goal achievement.

## Foundational Learning
- **Telic states**: Classes of goal-equivalent experience distributions - needed to unify state representation and goal encoding; quick check: verify experience distributions cluster meaningfully around goals
- **Statistical divergence**: Measure of difference between current and desired experience distributions - needed as natural learning signal; quick check: confirm divergence decreases as goals are approached
- **Experience distribution**: Probability distribution over possible agent experiences - needed to ground goals in actual agent interactions; quick check: ensure distributions capture relevant temporal dynamics
- **Goal co-emergence**: Simultaneous development of goals and state representations - needed to avoid infinite regress in meta-reasoning; quick check: verify goals and states evolve together without external specification
- **Policy divergence**: Statistical difference between current and goal-directed behavior policies - needed to guide learning; quick check: confirm policy updates reduce divergence
- **Neural encoding**: Brain regions representing telic states - needed to bridge computational and biological accounts; quick check: identify activation patterns consistent with telic state predictions

## Architecture Onboarding
**Component map**: Experience distributions -> Telic state representations -> Policy divergence -> Learning updates -> Behavioral adaptation

**Critical path**: Experience collection → Distribution estimation → Telic state identification → Policy divergence computation → Learning signal generation → Behavioral update

**Design tradeoffs**: The framework trades computational complexity of maintaining rich experience distributions for cognitive plausibility and goal flexibility. It sacrifices the simplicity of predefined reward functions for the ability to discover goals organically. The approach requires more memory to store experience distributions but gains adaptability to changing environments.

**Failure signatures**: System may fail to form coherent telic states if experience distributions are too noisy or overlapping. Ambiguous experiences belonging to multiple telic states could cause policy divergence to become unstable. The framework may struggle with rare or novel experiences that don't fit existing telic state categories.

**3 first experiments**:
1. Test telic state formation in simple environments with clear goal distinctions
2. Evaluate policy divergence stability across varying levels of experience noise
3. Assess learning efficiency compared to baseline RL approaches in controlled tasks

## Open Questions the Paper Calls Out
None

## Limitations
- Mathematical formalization needs empirical validation through neurobiological studies
- Framework's handling of conflicting goals and ambiguous experiences not fully addressed
- Computational complexity of maintaining telic state representations across multiple goals unclear

## Confidence
- Cognitive plausibility claims: Medium - compelling theoretical arguments but need empirical validation
- Neural encoding predictions: Medium - framework is conceptually sound but requires neurobiological confirmation
- Learning efficiency claims: Low - limited comparison with established RL approaches

## Next Checks
1. Neuroimaging studies to identify neural correlates of telic states in biological agents during goal-directed tasks, comparing activation patterns with traditional state and value encoding regions
2. Computational experiments comparing the efficiency and adaptability of telic state learning versus standard RL approaches in increasingly complex environments with multiple concurrent goals
3. Behavioral experiments testing whether human subjects naturally organize experiences into goal-equivalent classes as predicted by the telic states framework, using experience-sampling methods during complex decision-making tasks