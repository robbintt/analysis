---
ver: rpa2
title: 'ZeroLM: Data-Free Transformer Architecture Search for Language Models'
arxiv_id: '2503.18646'
source_url: https://arxiv.org/abs/2503.18646
tags:
- search
- performance
- proxy
- benchmark
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes ZeroLM, a data-free training-free Transformer
  architecture search method for language models. The approach addresses the limitations
  of existing zero-cost proxy (ZCP) methods, which often require data input and underperform
  compared to simple parameter counting metrics for Transformer models.
---

# ZeroLM: Data-Free Transformer Architecture Search for Language Models

## Quick Facts
- **arXiv ID**: 2503.18646
- **Source URL**: https://arxiv.org/abs/2503.18646
- **Reference count**: 40
- **Primary result**: ZeroLM achieves Spearman's rho of 0.76 and Kendall's tau of 0.53 on FlexiBERT benchmark, outperforming parameter count baselines for Transformer NAS

## Executive Summary
ZeroLM introduces a novel data-free, training-free approach for Transformer architecture search that addresses the limitations of existing zero-cost proxy methods. By decoupling Transformer architectures into Attention and Feed-Forward Network modules and computing spectral capacity metrics via SVD, ZeroLM achieves strong ranking correlations without requiring data input or model training. The method introduces a hyperparameter α to balance the contributions of different modules, achieving competitive performance while maintaining exceptional computational efficiency at under 1 second per architecture evaluation.

## Method Summary
ZeroLM operates by extracting weight matrices from Transformer architectures, separating them into Attention (Q, K, V, O projections) and FFN (up/down projections) components. For each weight matrix, it computes the mean squared Frobenius norm via SVD as a capacity metric. These module-specific scores are aggregated and combined using a weighted sum with hyperparameter α, which can be optimized through benchmark sampling or a heuristic method. The approach claims data-free operation by working with randomly initialized or pre-trained weights without requiring training or data input for the proxy computation itself.

## Key Results
- Achieves Spearman's rho of 0.76 and Kendall's tau of 0.53 on FlexiBERT benchmark
- Outperforms parameter count baselines that fail on Transformer models
- Demonstrates computational efficiency with 0.883s average evaluation time per architecture
- Shows stability across different random seeds in extensive experiments

## Why This Works (Mechanism)

### Mechanism 1: Spectral Model Capacity Estimation via SVD
ZeroLM computes the mean squared Frobenius norm of weight matrices through SVD, treating this as a proxy for model capacity. The method calculates the square of singular values for each matrix and averages them to yield module capacity metrics. This spectral approach relates to generalization performance through approximations of VC dimension and model complexity. The core assumption is that spectral properties of untrained weights predict learning capability. The method may fail for extremely deep or sparse architectures not well-represented in benchmarks.

### Mechanism 2: Functional Decoupling of Transformer Components
The method treats Attention and FFN blocks as functionally distinct, computing separate capacity scores and combining them with weighted hyperparameter α. This accounts for differential sensitivity to pruning and non-uniform contributions to performance. The assumption is that weighted combinations of individual module metrics are more predictive than global parameter counts. This decoupling may be misleading in architectures where Attention and FFN are highly intertwined or share weights.

### Mechanism 3: Hyperparameter Optimization for Ranking Alignment
ZeroLM optimizes α through two methods: benchmark sampling that uses a mini-set of architectures with known performance to maximize Kendall's Tau, or a heuristic method that approximates optimal α using correlations from a small sample without ground truth. The assumption is that a single scalar weight suffices to balance module contributions across the search space. The method may fail if optimal α varies drastically across different regions of the search space.

## Foundational Learning

- **Singular Value Decomposition (SVD)**: Core mathematical operation to compute proxy scores from weight matrices. *Why needed*: This is the foundation of the capacity estimation method. *Quick check*: How does the mean of squared singular values relate to the Frobenius norm of a matrix?

- **Kendall's Tau and Spearman's Rho**: Primary evaluation metrics for ranking correlation. *Why needed*: The paper's claims and α optimization are entirely based on maximizing these correlations. *Quick check*: If Model A has higher proxy score and higher true performance than Model B, does this count as a concordant pair for Kendall's Tau?

- **Transformer Architecture (Attention & FFN)**: Understanding these components' distinct roles is crucial for grasping why they need separate weighting. *Why needed*: The method's main contribution is decoupling these two main components. *Quick check*: In a standard Transformer encoder layer, which component typically contains the largest number of parameters?

## Architecture Onboarding

- **Component map**: Input weights -> Decoupler (separates Attention/FFN weights) -> Spectral Scorer (computes SVD-based capacity metrics) -> Proxy Combiner (calculates S_proxy with α) -> Alpha Optimizer (optional, determines optimal α)

- **Critical path**: The Spectral Scorer is the critical path, as SVD computation (O(min(n²m, nm²)) dominates runtime. Computational efficiency depends heavily on matrix sizes in the search space.

- **Design tradeoffs**: Speed vs. Correlation (heuristic method is faster but may yield sub-optimal α), Generality vs. Specificity (single α optimized for one benchmark may not work for others)

- **Failure signatures**: Low/Random Correlation (indicates wrong α or incompatible search space), High Variance with Random Seed (indicates stability issues), Poor performance on non-standard architectures

- **First 3 experiments**:
  1. Implement Base Scorer: Pick standard Transformer architectures, compute Sattn and Sffn separately, verify SVD/norm computation
  2. Correlation Check on Known Benchmark: Use FlexiBERT subset, compute S_proxy with default α=0.5, compute Spearman's Rho against GLUE scores, compare with #params baseline
  3. Alpha Sweep: Sweep α ∈ [-1.5, 1.5] on FlexiBERT subset, plot correlation curve, identify peak and verify against paper's optimal range

## Open Questions the Paper Calls Out

### Open Question 1
How does ZeroLM's ranking performance scale when validated on larger benchmark datasets beyond current computational constraints? The paper was limited by hardware constraints to 500 architectures in FlexiBERT and 200 in GPT-2 benchmark, leaving validation on larger datasets for future work.

### Open Question 2
What determines optimal α values across different architectural paradigms and search spaces, and can this be predicted a priori? The authors observe optimal α ranges differ between benchmarks but haven't identified underlying determinants, suggesting the determining conditions may depend on specific search space characteristics rather than architectural paradigm alone.

### Open Question 3
Can ZeroLM's data-free proxy approach be enhanced to match or exceed data-dependent proxies specifically for pruning-based NAS frameworks? The method appears more effective for initialized zero-shot NAS than pruning-based approaches, as pre-trained supernet proxies likely provide closer approximations to real-world performance.

### Open Question 4
How robust is ZeroLM to different weight initialization schemes beyond random initialization? While the approach claims stability across random seeds, it hasn't been tested with alternative initialization methods like Xavier, Kaiming, or orthogonal initialization.

## Limitations

- The paper doesn't specify which weight matrices (biases, layer norms, output projections) are included in SVD computation, creating ambiguity in reproducing exact scores
- Claims of "data-free" operation require randomly initialized or pre-trained weights as input, which may not be universally available
- Assumes linear relationships between spectral properties and model capacity that may break down in non-standard architectures

## Confidence

- **High confidence**: Computational efficiency claims (0.883s average) and basic mathematical framework of using SVD-based capacity metrics
- **Medium confidence**: Specific optimal α values reported for different benchmarks, as these depend on dataset-specific characteristics
- **Low confidence**: Generalization claim to extremely deep or sparse architectures not represented in tested benchmarks

## Next Checks

1. **Ablation test on weight matrix selection**: Systematically test which weight matrices contribute to S_x and measure impact on correlation metrics
2. **Cross-benchmark α transferability**: Use optimal α from FlexiBERT directly on GPT-2 benchmark without re-optimization, measuring correlation degradation
3. **Random seed stability test**: Compute S_proxy scores for same architecture across 10 different random initializations and report variance in proxy scores