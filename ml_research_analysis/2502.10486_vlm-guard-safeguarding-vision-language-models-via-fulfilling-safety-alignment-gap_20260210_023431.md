---
ver: rpa2
title: 'VLM-Guard: Safeguarding Vision-Language Models via Fulfilling Safety Alignment
  Gap'
arxiv_id: '2502.10486'
source_url: https://arxiv.org/abs/2502.10486
tags:
- safety
- harmful
- alignment
- arxiv
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of safeguarding vision-language
  models (VLMs) from harmful queries, a problem exacerbated by the "modality gap"
  that undermines the safety alignment inherited from their language components. The
  proposed solution, VLM-Guard, is an inference-time intervention that projects the
  VLM's hidden representations onto a subspace orthogonal to a "safety steering direction"
  derived from a safety-aligned language model.
---

# VLM-Guard: Safeguarding Vision-Language Models via Fulfilling Safety Alignment Gap

## Quick Facts
- arXiv ID: 2502.10486
- Source URL: https://arxiv.org/abs/2502.10486
- Authors: Qin Liu; Fei Wang; Chaowei Xiao; Muhao Chen
- Reference count: 14
- Primary result: VLM-Guard reduces attack success rates by 66-75% while maintaining response quality as measured by perplexity

## Executive Summary
This paper addresses the critical challenge of safeguarding vision-language models (VLMs) from harmful queries, focusing on the "modality gap" that undermines safety alignment inherited from their language components. The proposed solution, VLM-Guard, is an inference-time intervention that projects VLM hidden representations onto a subspace orthogonal to a "safety steering direction" derived from a safety-aligned language model. This approach effectively steers harmful queries away from harmful responses without requiring retraining. Experimental results demonstrate significant improvements in safety across three malicious instruction settings, with attack success rates reduced by 66-75% compared to baseline methods.

## Method Summary
VLM-Guard tackles the safety alignment gap in vision-language models by implementing an inference-time intervention mechanism. The method works by first deriving a "safety steering direction" from a safety-aligned language model, then projecting the VLM's hidden representations onto a subspace orthogonal to this direction during inference. This orthogonal projection effectively steers harmful queries away from generating harmful responses while maintaining the model's ability to handle benign queries. The approach operates entirely at inference time, avoiding the computational overhead and potential performance degradation associated with fine-tuning or retraining VLMs for safety.

## Key Results
- Reduces attack success rates (ASR) by 66-75% across three malicious instruction settings
- Maintains response quality as measured by perplexity metrics
- Demonstrates effectiveness without requiring model retraining or fine-tuning
- Shows particular strength in addressing the modality gap between text and vision components

## Why This Works (Mechanism)
The mechanism works by exploiting the mathematical property of orthogonal projection to separate harmful content from the safety-aligned subspace. By projecting the VLM's hidden representations onto a subspace orthogonal to the safety steering direction, harmful query patterns are effectively redirected away from generating harmful responses. This works because the safety steering direction captures the semantic space associated with safe responses, and orthogonal projection ensures that any component of the input aligned with harmful content is removed before the final output generation.

## Foundational Learning

**Safety Alignment in LLMs** - Why needed: Understanding how language models learn safety constraints is crucial for extending these principles to multimodal systems. Quick check: Can the model consistently refuse harmful requests while maintaining task performance?

**Modality Gap** - Why needed: VLMs combine text and vision modalities, but safety alignment often doesn't transfer across modalities. Quick check: Does safety behavior differ significantly between text-only and vision-language prompts?

**Hidden Representation Spaces** - Why needed: VLMs operate in high-dimensional embedding spaces where safety-related patterns must be identified and manipulated. Quick check: Can harmful and safe query patterns be separated in the embedding space?

**Orthogonal Projection** - Why needed: Mathematical operation needed to remove harmful components while preserving useful information. Quick check: Does projection preserve task-relevant information while removing safety-violating content?

**Inference-time Interventions** - Why needed: Real-time safety measures are essential for deployed systems without retraining overhead. Quick check: Can safety interventions be applied efficiently during inference without significant latency?

## Architecture Onboarding

Component map: Input Query -> VLM Encoder -> Safety Steering Direction -> Orthogonal Projection -> Decoder -> Output Response

Critical path: The critical path involves the orthogonal projection step, which must be computed efficiently during inference. The safety steering direction is precomputed from a safety-aligned LLM, while the projection operation itself is applied to each hidden state before decoding.

Design tradeoffs: The method trades computational overhead at inference time for safety gains without retraining. The orthogonality assumption may not capture all forms of harmful content, particularly subtle or context-dependent violations.

Failure signatures: Potential failures include over-blocking benign queries (false positives), inability to handle novel harmful patterns not captured by the safety steering direction, and performance degradation on legitimate tasks requiring the projected-away semantic space.

First experiments: 1) Test orthogonal projection effectiveness on synthetic harmful/non-harmful query pairs, 2) Measure inference-time overhead compared to baseline VLM performance, 3) Evaluate safety steering direction sensitivity to different safety-aligned LLM sources.

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Heavy reliance on a safety-aligned LLM creates a single point of failure if the base LLM contains safety gaps
- Orthogonality assumption may not generalize well to all types of harmful queries, particularly subtle or context-dependent safety violations
- Evaluation framework lacks comprehensive assessment of false positives and degradation of model performance on legitimate tasks

## Confidence
- Claim: 66-75% reduction in attack success rates → Confidence: High (for specific benchmark settings tested)
- Claim: Maintains response quality as measured by perplexity → Confidence: Medium (perplexity alone may not capture nuanced aspects)
- Claim: Modality gap is primary factor undermining VLM safety → Confidence: Medium (other factors could also play significant roles)

## Next Checks
1. Test VLM-Guard's performance across diverse, real-world safety benchmarks including both image and text-based safety violations
2. Evaluate the false positive rate on benign queries across different domains and contexts
3. Conduct ablation studies to quantify the individual contributions of the safety steering direction versus other potential intervention mechanisms