---
ver: rpa2
title: 'INRet: A General Framework for Accurate Retrieval of INRs for Shapes'
arxiv_id: '2501.15722'
source_url: https://arxiv.org/abs/2501.15722
tags:
- inrs
- retrieval
- shape
- implicit
- different
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: INRet is a general framework for accurate retrieval of shape INRs
  from a data store. It addresses the challenge of determining similarity between
  INRs with different architectures (MLP, octree grids, triplanes, hash grids) and
  implicit functions (SDF, UDF, occupancy).
---

# INRet: A General Framework for Accurate Retrieval of INRs for Shapes

## Quick Facts
- arXiv ID: 2501.15722
- Source URL: https://arxiv.org/abs/2501.15722
- Authors: Yushi Guan; Daniel Kwan; Ruofan Liang; Selvakumar Panneer; Nilesh Jain; Nilesh Ahuja; Nandita Vijaykumar
- Reference count: 40
- Key outcome: 10.1% higher retrieval accuracy than existing INR retrieval methods and 12.1% higher accuracy than converting INRs to point clouds or multi-view images

## Executive Summary
INRet addresses the challenge of retrieving similar shape Implicit Neural Representations (INRs) from a data store when INRs use different architectures (MLP, octree grids, triplanes, hash grids) and implicit functions (SDF, UDF, occupancy). The framework creates unified embeddings by encoding both MLP weights and feature grid parameters, then applies regularization techniques to ensure embeddings from different implicit functions map to a shared latent space. This enables accurate retrieval across INR types without conversion overhead, achieving 82.0% mAP@1 on ShapeNet10 compared to 10% baseline accuracy.

## Method Summary
INRet creates embeddings for INRs by encoding both MLP weights and feature grid parameters through a dual-branch encoder architecture. For grid-based INRs, a Conv3D encoder samples features at fixed resolution (2N)³ and aggregates spatial information through strided convolutions. For MLP-only INRs, a separate MLP encoder processes flattened weight matrices. The embeddings are concatenated and trained with L2 regularization between same-shape, different-function INRs, plus a unified shape decoder that outputs one implicit function type regardless of input embedding. This forces embeddings across architectures and functions into a shared latent space, enabling retrieval via cosine similarity.

## Key Results
- Achieves 82.0% mAP@1 on ShapeNet10 compared to 10% baseline accuracy
- 10.1% higher retrieval accuracy than existing INR retrieval methods
- 12.1% higher accuracy than converting INRs to point clouds or multi-view images
- Retrieval speed of 0.034-0.14 seconds versus 0.98-3.05 seconds for conversion-based methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Encoding both MLP weights and feature grid parameters produces shape-semantic embeddings without explicit reconstruction
- Mechanism: Dual-branch encoder architecture — MLP encoder processes flattened weight matrices (producing 512-dim embedding for grid-based INRs, 1024-dim for MLP-only), while Conv3D encoder samples features at fixed resolution (2N)³ from spatial grids, using strided convolutions to aggregate multi-scale information. Concatenation yields unified embedding.
- Core assumption: INR weights and grid features contain recoverable shape information that correlates across architectures
- Evidence anchors: [abstract] "creates embeddings for INRs by encoding both the MLP weights and feature grid parameters"; [section 3.2] "MLP embedding and grid embedding are then concatenated to create our INR embedding"
- Break condition: If MLP weights are randomly initialized differently across INRs (not using shared initialization), the MLP encoder may produce uninformative embeddings — paper notes this is avoided by following inr2vec's shared initialization practice

### Mechanism 2
- Claim: Explicit L2 regularization between same-shape embeddings and a Unified Shape Decoder create a shared latent space across implicit function types
- Mechanism: For each training shape, generate INRs with all three implicit functions (SDF, UDF, Occ). Apply L2 loss between embeddings of same-shape different-function INRs, and train a single decoder to output one function type (e.g., UDF) regardless of input embedding type. Loss: L = Σ|f_φ(x; e_i) - d_u(x)| + λΣ||e_i - e_j||₂
- Core assumption: Different implicit functions for the same shape share sufficient structural correlation that neural networks can learn the mapping (paper shows UDF = ReLU(SDF) + ReLU(-SDF) theoretically)
- Evidence anchors: [abstract] "applies regularization techniques to ensure embeddings from different implicit functions map to a unified latent space"; [table 4] Accuracy improves from ~10% (no regularization) → 64.6% (L2 only) → 82.0% (L2 + Unified Decoder)
- Break condition: If implicit functions encode fundamentally different geometric information (e.g., UDF capturing multi-layer details vs. SDF capturing only watertight surfaces), regularization may force embeddings to lose function-specific information

### Mechanism 3
- Claim: Feature grid sampling and convolution preserves spatial structure better than global pooling of grid parameters
- Mechanism: Sample features at regular 3D grid positions, handling multi-level grids via summation (octree, triplane) or concatenation (hash grids) before Conv3D processing. Conv3D's increasing receptive field captures local-to-global spatial patterns.
- Core assumption: Spatial relationships in feature grids encode meaningful geometric information
- Evidence anchors: [section 3.2] Conv3D encoder uses "3D convolutional network that fuses discrete spatial features with gradually increasing perception fields"; [appendix 8.7] Changing summation to concatenation for NGLOD drops accuracy 14.8%; changing concatenation to summation for iNGP drops 53.8%
- Break condition: Hash collision in hash grids may corrupt spatial relationships if severe enough that concatenated features don't preserve locality

## Foundational Learning

- Concept: **Implicit Neural Representations (INRs)**
  - Why needed here: INRet operates entirely on INR parameters, not rendered outputs — understanding that INRs map coordinates to values (distances, occupancy) is essential
  - Quick check question: Given a 3D coordinate x, what does f_θ(x) output for an SDF INR? (Answer: signed distance to nearest surface)

- Concept: **Feature Grids vs. Pure MLP Architectures**
  - Why needed here: The dual-encoder design requires knowing when to use Conv3D (grid present) vs. MLP-only encoder
  - Quick check question: Why might hash grids require feature concatenation while octree grids use summation? (Answer: Hash collision mitigation vs. hierarchical detail aggregation)

- Concept: **Triplet-like Regularization for Embedding Alignment**
  - Why needed here: The L2 loss between same-shape different-function embeddings is conceptually similar to metric learning
  - Quick check question: What would happen if λ=0 in the loss function? (Answer: Embeddings for different implicit functions would not be pulled together, retrieval across functions fails)

## Architecture Onboarding

- Component map:
  - MLP Encoder (m): 4-layer MLP with batch norm + ReLU → max pool → 512-1024 dim embedding
  - Conv3D Encoder (c): 5-layer 3D conv (kernel 2×2×2, stride 2×2×2) with group norm → linear → 512 dim embedding
  - Unified Shape Decoder (f_φ): Same architecture as inr2vec decoder, takes [embedding; coordinate] → implicit function value
  - Feature Sampling: Fixed (2N)³ grid sampling, level-wise summation (octree/triplane) or concatenation (hash)

- Critical path:
  1. Train INRs for all shapes with all 3 implicit functions (SDF, UDF, Occ)
  2. Train encoders + decoder jointly with L2 regularization between same-shape embeddings
  3. At inference: encode query INR → cosine similarity search in embedding database

- Design tradeoffs:
  - Sampling resolution (N): Higher = more accurate but slower encoding (paper uses unspecified N)
  - Unified decoder output type: Paper shows UDF/SDF/Occ all work within 0.8% accuracy, but matching query type gives slight boost
  - Hash grid vs. octree: Hash grids (iNGP) achieve slightly higher accuracy (84.2 vs 82.6 mAP@1) but slower encoding (0.14s vs 0.034s) due to hash operations

- Failure signatures:
  - Cross-function retrieval ~10% accuracy → regularization not applied or decoder not unified
  - Large accuracy drop for specific grid type → feature aggregation (sum vs. concat) mismatched to grid architecture
  - Embedding distances don't correlate with shape similarity → MLP initialization not shared across INRs

- First 3 experiments:
  1. **Sanity check**: Train encoders on single implicit function (SDF only), verify retrieval accuracy matches baseline on same-function queries (~83-84% mAP@1 on ShapeNet10)
  2. **Ablation**: Disable each regularization component separately (no L2, no unified decoder) to confirm table 4 reproduction (~10% → ~65% → ~82% progression)
  3. **Cross-architecture test**: Given MLP-only query INR, use distillation to convert to hash-grid INR, then retrieve from hash-grid database — expect ~77% accuracy per table 2

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can methods be developed to allow fast and accurate Category-Chamfer retrieval?
- Basis in paper: [explicit] The authors state in the supplementary material, "We leave potential methods that would allow fast and accurate Category-Chamfer retrieval as future work."
- Why unresolved: Current hierarchical sampling reduces time but remains slower than cosine similarity, and standard similarity metrics fail the Category-Chamfer metric.
- What evidence would resolve it: A proposed mechanism that computes Category-Chamfer similarity with latency comparable to standard embedding lookup.

### Open Question 2
- Question: Can INRet be generalized to support INRs of other data modalities, such as 2D images, videos, or radiance fields?
- Basis in paper: [inferred] While the introduction lists images, videos, and scenes as applications of INRs, the method and experiments are strictly limited to 3D shape functions (SDF, UDF, Occupancy).
- Why unresolved: The current embedding encoders are specifically designed for 3D spatial grids and shape decoders; different modalities have distinct structural properties.
- What evidence would resolve it: Successful application of the framework to retrieve similar NeRF scenes or INR-encoded videos without architecture-specific redesign.

### Open Question 3
- Question: How can the computational cost of encoder training be reduced for large-scale datasets?
- Basis in paper: [inferred] Appendix 7.1 notes that training requires significant resources (e.g., 8.3 GPU days for ShapeNet10) and anticipates that "further improvements... could reduce training times for larger-scale experiments."
- Why unresolved: The current training pipeline involves generating multiple INRs per shape and jointly training encoders/decoders, which is resource-intensive.
- What evidence would resolve it: A modified training procedure that achieves comparable retrieval accuracy on a dataset significantly larger than ShapeNet10 with reduced GPU hours.

## Limitations
- The regularization mechanism's effectiveness across fundamentally different implicit functions remains theoretical and needs more empirical testing
- Cross-architecture distillation accuracy (77% for MLP→Hash) suggests residual information loss during conversion
- The framework was tested on only 4 INR types, and generalization to unseen architectures is unproven

## Confidence
- **High Confidence**: Retrieval accuracy improvements (10.1% vs INR baselines, 12.1% vs conversion methods) are directly measured on specified datasets with clear methodology
- **Medium Confidence**: The regularization mechanism's effectiveness is supported by ablation studies but lacks external validation; the theoretical basis for cross-function alignment needs more empirical testing
- **Low Confidence**: Generalization to unseen architectures is unproven—the framework was tested on only 4 INR types, and hash grid operations may introduce unpredictable behavior in different implementations

## Next Checks
1. **Ablation testing**: Disable each regularization component separately (no L2 loss, no unified decoder) to verify the progression from ~10% to ~82% accuracy matches Table 4, confirming the regularization's contribution

2. **Cross-architecture robustness**: Train INRs using architectures not in the original set (e.g., DeepSDF-style MLPs or alternative grid-based methods) and test INRet retrieval accuracy to assess generalization claims

3. **Initialization sensitivity**: Train MLP encoders with different weight initialization schemes (random vs. shared) and measure retrieval accuracy degradation to validate the shared initialization assumption