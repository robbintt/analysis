---
ver: rpa2
title: 'Large Language Models Meet Contrastive Learning: Zero-Shot Emotion Recognition
  Across Languages'
arxiv_id: '2503.21806'
source_url: https://arxiv.org/abs/2503.21806
tags:
- speech
- emotion
- audio
- multilingual
- recognition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses zero-shot multilingual speech emotion recognition
  (MSER), which is challenging due to voice variability, linguistic diversity, and
  limited data for minority languages. The authors propose a two-stage contrastive
  learning framework that aligns multilingual speech representations with emotion-aware
  language features using a large language model.
---

# Large Language Models Meet Contrastive Learning: Zero-Shot Emotion Recognition Across Languages

## Quick Facts
- arXiv ID: 2503.21806
- Source URL: https://arxiv.org/abs/2503.21806
- Authors: Heqing Zou; Fengmao Lv; Desheng Zheng; Eng Siong Chng; Deepu Rajan
- Reference count: 36
- Key outcome: Proposes a two-stage contrastive learning framework achieving competitive zero-shot multilingual speech emotion recognition with weighted accuracies of 70.3% and 68.5% on IEMOCAP four-class and seven-class datasets

## Executive Summary
This paper addresses the challenge of zero-shot multilingual speech emotion recognition (MSER) by proposing a two-stage contrastive learning framework that aligns multilingual speech representations with emotion-aware language features using large language models. The approach first trains on English datasets to learn emotion-aware, speaker-independent representations, then introduces M5SER, a large-scale synthetic multilingual dataset generated from English emotion-labeled data, to learn language-agnostic, emotion-aware speech representations. The model achieves competitive performance in both traditional SER and zero-shot MSER tasks across multiple languages.

## Method Summary
The proposed framework employs a two-stage contrastive learning approach. In the first stage, the model learns emotion-aware and speaker-independent representations from English emotion datasets. The second stage introduces M5SER, a synthetic multilingual dataset created by translating and adapting English emotion-labeled data into multiple languages. The model architecture consists of an audio encoder (Whisper), a cross-modal connector (Emotion Q-Former), and LLaMA 3 for emotion word prediction. The contrastive learning objective aligns speech representations with corresponding emotion-aware language features, enabling zero-shot cross-lingual emotion recognition.

## Key Results
- Achieved weighted accuracies of 70.3% and 68.5% on four-class and seven-class IEMOCAP datasets respectively
- Demonstrated strong zero-shot performance on previously unseen datasets and languages
- Successfully performed emotion recognition across four languages (English, Chinese, German, Spanish) without language-specific training

## Why This Works (Mechanism)
The framework leverages contrastive learning to align speech and text representations in a shared embedding space. By using synthetic multilingual data generated from English emotion labels, the model learns language-agnostic emotion patterns that transfer across languages. The large language model component provides rich semantic understanding of emotion words, while the audio encoder captures prosodic and acoustic features relevant to emotion expression. The cross-modal connector bridges the modality gap, enabling effective alignment between speech and language representations.

## Foundational Learning
1. **Contrastive Learning** - Needed to align representations from different modalities; quick check: verify that positive pairs (speech-text of same emotion) are closer than negative pairs in embedding space.
2. **Cross-Lingual Transfer** - Required for zero-shot multilingual performance; quick check: confirm that emotion patterns learned from English generalize to other languages.
3. **Synthetic Data Generation** - Essential for creating M5SER dataset; quick check: validate that translated emotion labels preserve emotional content across languages.
4. **Multi-Modal Fusion** - Critical for combining speech and language features; quick check: ensure cross-modal connector effectively bridges modality gap.

## Architecture Onboarding
- **Component Map**: Audio Encoder (Whisper) -> Cross-Modal Connector (Emotion Q-Former) -> LLM (LLaMA 3) -> Contrastive Loss
- **Critical Path**: Speech input → Audio encoder → Cross-modal connector → LLM emotion prediction → Contrastive loss computation
- **Design Tradeoffs**: Uses synthetic multilingual data instead of real data (cost-effective but potentially less authentic); leverages pre-trained models (faster training but less task-specific customization)
- **Failure Signatures**: Poor performance on spontaneous speech vs. acted speech; degraded results on languages with different emotional expression patterns; sensitivity to synthetic data quality
- **3 First Experiments**: 1) Ablation study removing synthetic data to measure its contribution, 2) Evaluation on additional languages beyond the current four, 3) Comparison with supervised multilingual baselines

## Open Questions the Paper Calls Out
None

## Limitations
- Relies entirely on synthetic data for multilingual learning, raising concerns about cross-linguistic emotion expression validity
- Limited evaluation to only four languages with 50 utterances per language for zero-shot testing
- Focuses on categorical emotion labels rather than continuous dimensional representations

## Confidence
- Model Architecture Claims: Medium Confidence - Technically sound but lacks ablation studies for architectural choices
- Zero-Shot Performance Claims: Medium Confidence - Promising results but limited test set size and lack of statistical significance testing
- Multilingual Generalization Claims: Low Confidence - Theoretical assumptions about language-agnostic representations lack sufficient validation

## Next Checks
1. Conduct ablation studies to isolate the contribution of each component in the contrastive learning framework, particularly the impact of synthetic data versus real multilingual data
2. Evaluate the model on additional languages beyond the current four, including low-resource languages with distinct phonetic and prosodic characteristics
3. Test the model on spontaneous, naturalistic speech datasets from multiple languages to evaluate real-world performance beyond acted speech scenarios