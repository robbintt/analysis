---
ver: rpa2
title: 'Federated Learning with Ad-hoc Adapter Insertions: The Case of Soft-Embeddings
  for Training Classifier-as-Retriever'
arxiv_id: '2509.16508'
source_url: https://arxiv.org/abs/2509.16508
tags:
- training
- classifier
- which
- head
- clients
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of efficiently updating retrieval-augmented
  generation (RAG) systems for new knowledge domains on resource-constrained edge
  devices. The authors propose a novel approach that uses a frozen small language
  model (SLM) with trainable adapter networks to generate soft embeddings for new
  corpora, combined with a classifier head for improved retrieval.
---

# Federated Learning with Ad-hoc Adapter Insertions: The Case of Soft-Embeddings for Training Classifier-as-Retriever

## Quick Facts
- arXiv ID: 2509.16508
- Source URL: https://arxiv.org/abs/2509.16508
- Reference count: 40
- Primary result: CaR approach with soft embeddings achieves 99.95% top-1 accuracy on SMS spam vs. 12.36% for frozen MIPS, with 1.75x-2.62x FL speedup matching centralized training

## Executive Summary
This paper addresses efficient RAG system updates for new domains on edge devices by combining frozen SLMs with trainable adapters and classifier heads. The approach uses soft embeddings generated by adapter networks to transform generic token embeddings for new corpora, paired with a classifier head that learns retrieval as a classification task rather than similarity search. Federated learning with differential privacy enables distributed training while preserving client privacy, achieving comparable accuracy to centralized training with significant speedups.

## Method Summary
The method inserts a square transformation matrix (adapter) between token embeddings and frozen transformer blocks, learning to produce domain-specific soft embeddings without modifying the base SLM weights. A classifier head replaces maximum inner-product search for retrieval, mapping query embeddings directly to document labels. Training occurs via federated averaging with gradient clipping and noise injection for differential privacy. The approach supports two modes: classifier-only training (fast, ~97% accuracy) and adapter+classifier training (slower, ~99.95% accuracy). Experiments validate performance across SMS spam, AG News, and arXiv metadata datasets using TinyLlama-1.1B and Llama-3.2-3B models.

## Key Results
- Soft embeddings + classifier achieves 99.95% top-1 accuracy on SMS spam vs. 12.36% for frozen LLM + MIPS
- Federated learning matches centralized training performance with 1.75x to 2.62x speedups depending on client count
- Differential privacy integration causes only minor accuracy drops while preserving client privacy
- Method scales to larger models (Llama-3.2-3B) on datasets with 173 classes (arXiv metadata)

## Why This Works (Mechanism)

### Mechanism 1: Soft Embeddings via Adapter Transformation
A small trainable adapter (square transformation matrix) sits between token embeddings and frozen transformer blocks, learning to transform generic embeddings into domain-specific soft embeddings. This reduces trainable parameters from billions to adapter dimension squared while preserving the frozen SLM's representational capacity.

### Mechanism 2: Classifier-as-Retriever (CaR) Replaces Similarity Search
Instead of computing dot products between query and document embeddings, a classifier head directly maps query hidden states to document labels. This transforms retrieval from a fixed similarity function into a learned task, achieving superior accuracy through learned similarity mapping.

### Mechanism 3: Federated Convergence Under Gradient Variance Bounds
FL with DP achieves O(1/T) convergence rate matching centralized training under assumptions of L_i-smooth local losses, bounded gradient variance, and gradient diversity. The theoretical analysis bounds convergence gaps by terms dependent on clipping threshold, noise variance, and gradient diversity.

## Foundational Learning

### Concept: Parameter-Efficient Fine-Tuning (PEFT)
**Why needed:** The entire approach relies on adapting frozen SLMs without full fine-tuning. Understanding why adapters work—and why LoRA was rejected (communication overhead in distributed settings)—is essential.
**Quick check:** Given the FL setting, can you explain why adding LoRA modules to every transformer block would increase communication overhead compared to a single adapter before the first block?

### Concept: Federated Averaging (FedAvg)
**Why needed:** This is the core aggregation algorithm. The paper uses weighted averaging based on local dataset sizes (n_i/n), which affects convergence behavior.
**Quick check:** If Client A has 5,000 samples and Client B has 500 samples, what weight would each receive during aggregation? How might this affect convergence if Client B's data is more representative of the target domain?

### Concept: Local Differential Privacy via Gradient Clipping and Noise Injection
**Why needed:** Privacy guarantees come from clipping gradients (bounding sensitivity) and adding calibrated noise. Understanding the privacy-utility trade-off is critical for practical deployment.
**Quick check:** If you observe that DP causes a 5% accuracy drop, which parameter would you adjust first—clipping threshold C or noise scale σ—and in which direction?

## Architecture Onboarding

### Component map:
Tokenizer (frozen) 
    ↓
Token Embeddings (frozen, φ_e)
    ↓
Adapter (trainable, θ_a) ← Square transformation matrix
    ↓
Soft Embeddings
    ↓
Transformer Blocks (frozen, φ_t) ← 22 blocks (TinyLlama) or 28 blocks (Llama-3.2-3B)
    ↓
Hidden States
    ↓
Norm Head (frozen)
    ↓
Classifier Head (trainable, θ_c) ← MLP with optional pre-classifier layer
    ↓
Top-K Predictions → Retrieved Documents

**Trainable parameters:** Only θ_a (adapter matrix) and θ_c (classifier head)—communicated in FL.

### Critical path:
1. **Training mode:** Forward pass through frozen SLM → Backprop through adapter + classifier → Gradient clipping + noise → Local parameter update → Aggregate at server
2. **Inference mode:** Query → Frozen SLM + trained adapter → Classifier → Top-K documents → RAG generator

**Key constraint:** Adapter at the model entrance requires backpropagation through all transformer blocks during training (expensive). Classifier-only training allows precomputed hidden states (fast).

### Design tradeoffs:
| Choice | Pros | Cons |
|--------|------|------|
| Classifier-only training | Fast (precompute hidden states), low memory | Lower accuracy (~97% vs ~99%) |
| Adapter + Classifier training | Higher accuracy, better domain adaptation | Slower (backprop through full SLM) |
| Mean pooling vs. EOS token | Mean pooling captures more context | EOS is faster, simpler |
| Fixed vs. Adaptive DP | Fixed is simpler to implement | Adaptive tunes to actual gradient norms, may converge better with warmup |
| More FL clients | Higher speedup (up to 2.62x with 3 clients) | Diminishing returns (Amdahl's Law), more coordination overhead |

### Failure signatures:
1. **Convergence stalls with non-decreasing loss:** Verify learning rate satisfies bounds in Theorem 5.5; check if gradient norms are hitting clipping threshold too frequently.
2. **DP causes severe accuracy drop (>2%):** Clipping threshold likely too low or noise scale too high. Try warmup (Section 6.5 uses 6 epochs) before enabling adaptive DP.
3. **Classifier overfits small document sets:** If dataset has many documents but few samples per document, consider MoE clustering (Section 4.2) or hierarchical classifiers.
4. **FL slower than centralized despite multiple clients:** Communication overhead dominates. Profile network latency vs. compute time; reduce local epochs or increase batch size.

### First 3 experiments:
1. **Classifier-only baseline on SMS spam:** Train only the classifier head (no adapter) on frozen TinyLlama. Compare centralized (10 epochs) vs. FL (5 rounds × 2 epochs) for accuracy and wall-clock time. Target: ~97% accuracy with <0.5% gap between centralized and FL, confirming FL convergence.

2. **Soft embedding ablation:** Compare classifier-only vs. adapter+classifier training on SMS spam dataset. Expect ~3% improvement (97.02% → 99.95% as reported). Measure training time increase (Section 6.3 reports ~32 min vs. <1 min for classifier-only).

3. **DP impact with warmup:** Train with adaptive DP (noise scale 0.1, clipping EMA decay 0.9, target quantile 0.9) using 6-epoch warmup on AG News with Llama-3.2-3B. Compare accuracy drop vs. non-DP baseline. Target: <1% accuracy degradation while achieving differential privacy guarantees.

## Open Questions the Paper Calls Out

### Open Question 1
**Question:** How can Mixture-of-Experts (MoE) and clustering be integrated into the Classifier-as-Retriever (CaR) framework to handle datasets with arbitrarily large document sets without violating edge device memory constraints?
**Basis:** Section 4.2 proposes using "clustering and a mixture-of-experts (MoE) solution" as a future direction to mitigate the linear growth of the classifier head size with the dataset.
**Why unresolved:** This architectural variant was identified to solve scaling issues but was not implemented or evaluated in the current work.
**What evidence would resolve it:** Experimental results demonstrating the accuracy and memory footprint of a hierarchical MoE classifier head on large-scale retrieval datasets.

### Open Question 2
**Question:** Can the frozen transformer blocks be successfully offloaded to distributed clusters while maintaining the convergence guarantees and speedups of the federated learning process?
**Basis:** Section 4.1 suggests "Distributed Load Offloading" where transformer blocks are hosted remotely to optimize resource usage, leaving the exploration of this setup for future work.
**Why unresolved:** The feasibility of this architecture regarding network latency and synchronization overhead was not tested in the experimental setup.
**What evidence would resolve it:** Convergence analysis and latency benchmarks of the training process when the SLM backbone is distributed across multiple servers.

### Open Question 3
**Question:** Does the performance advantage of training the soft embedding adapter (over the classifier head only) persist when scaling the base Small Language Model (SLM) beyond 3B parameters?
**Basis:** In Section 6.5, the authors restrict experiments on Llama-3.2-3B to "training the classifier head only" because backpropagation for soft embeddings becomes "time-consuming," leaving the full method's scalability unverified.
**Why unresolved:** It is unclear if the superior accuracy of the adapter approach (demonstrated on TinyLlama) is tractable or achievable on larger models in a federated setting.
**What evidence would resolve it:** A comparison of convergence speed and accuracy for the full adapter approach versus the head-only approach on models with 7B+ parameters.

## Limitations

- Classifier-as-retriever approach has scalability constraints when document count exceeds classifier capacity, requiring hierarchical clustering solutions
- Soft embedding adapter relies on linear transformations, which may fail when target domains require fundamentally different token-level semantics
- Resource-constrained edge deployment must account for the memory overhead of adapter-backprop (requires storing intermediate activations)

## Confidence

**High Confidence:**
- Soft embeddings + classifier architecture achieves 99.95% accuracy on SMS spam vs. 12.36% for frozen MIPS (Section 6.2, Figure 2)
- Federated learning matches centralized training performance with 1.75x-2.62x speedup across different client counts (Section 6.4, Table 2)
- DP integration causes only minor performance drops while preserving privacy (Section 6.5)

**Medium Confidence:**
- Theoretical convergence bounds for FL with DP are mathematically sound but limited by simplifying assumptions (Section 5.3, Theorem 5.5)
- Learning rate and DP parameter selections are empirically validated but not systematically explored across different datasets
- Model architecture choices (EOS token vs. mean pooling, classifier-only vs. adapter+classifier) are justified by ablation studies

**Low Confidence:**
- Performance claims on arXiv metadata dataset lack detailed ablation studies
- Communication efficiency gains relative to other parameter-efficient fine-tuning methods are not directly compared
- Long-term stability of federated training across heterogeneous client distributions is not extensively tested

## Next Checks

**Check 1: Learning Rate Sensitivity Analysis**
Systematically vary learning rates across SMS spam, AG News, and arXiv datasets to identify optimal ranges and quantify performance sensitivity. This addresses the critical unknown of SMS spam learning rate and provides guidance for hyperparameter tuning.

**Check 2: Communication Overhead Benchmarking**
Measure actual communication costs (bytes transmitted) for adapter-only vs. LoRA-based approaches under different client distributions and network conditions. This validates the claimed communication efficiency advantage and identifies practical deployment bottlenecks.

**Check 3: DP Parameter Space Exploration**
Conduct a systematic study varying noise scale (0.01-0.5) and clipping thresholds across multiple epochs and warmup schedules. This quantifies the privacy-utility trade-off space and identifies robust parameter configurations for different deployment scenarios.