---
ver: rpa2
title: Adversarial bandit optimization for approximately linear functions
arxiv_id: '2505.20734'
source_url: https://arxiv.org/abs/2505.20734
tags:
- regret
- bound
- linear
- bandit
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies adversarial bandit optimization for non-convex,\
  \ non-smooth functions that are \u01EB-approximately linear (i.e., f(x) = \u03B8\
  \u22A4x + \u03C3(x) where |\u03C3(x)| \u2264 \u01EB). The authors modify the SCRiBLe\
  \ algorithm with lifting and the \u03BD-normal barrier to handle this problem."
---

# Adversarial bandit optimization for approximately linear functions

## Quick Facts
- **arXiv ID**: 2505.20734
- **Source URL**: https://arxiv.org/abs/2505.20734
- **Reference count**: 40
- **Primary result**: This paper studies adversarial bandit optimization for non-convex, non-smooth functions that are ǫ-approximately linear (i.e., f(x) = θ⊤x + σ(x) where |σ(x)| ≤ ǫ). The authors modify the SCRiBLe algorithm with lifting and the ν-normal barrier to handle this problem. They derive both expected and high-probability regret bounds that depend on ǫ, achieving O(d√νT ln(1/δ) + dT(ν+2√ν)(1-δ)/δ·ǫ + δGDT + 2Tǫ) expected regret and similar high-probability bounds. When ǫ=0, this becomes standard bandit linear optimization, and their high-probability bound improves upon prior work. They also prove a lower bound of Ω(ǫT), showing their regret bounds are tight in terms of ǫ. The key technical innovation is a new regret decomposition that separates the linear and perturbation components and leverages properties of the ν-normal barrier.

## Executive Summary
This paper addresses adversarial bandit optimization for approximately linear functions, where the loss function f(x) = θ⊤x + σ(x) has a small perturbation |σ(x)| ≤ ǫ. The authors develop a modified SCRiBLe algorithm using lifting and the ν-normal barrier to achieve both expected and high-probability regret bounds. Their key innovation is a new regret decomposition that separates the linear component from the perturbation, yielding tighter bounds than previous work. When ǫ=0, this reduces to standard bandit linear optimization with improved high-probability bounds over prior approaches.

## Method Summary
The authors modify the SCRiBLe algorithm by incorporating lifting and the ν-normal barrier. They transform the original decision set K into a cone con(K) and use the ν-normal barrier as a regularizer in Follow-The-Regularized-Leader (FTRL). This enables gradient-based exploration through smoothed one-point estimators. The algorithm constructs gradient estimators from scalar feedback and updates actions via FTRL. A novel regret decomposition separates the regret into linear, deviation, and error terms, allowing tighter high-probability bounds, especially when ǫ=0.

## Key Results
- Achieves O(d√νT ln(1/δ) + dT(ν+2√ν)(1-δ)/δ·ǫ + δGDT + 2Tǫ) expected regret for approximately linear functions
- Provides improved high-probability regret bounds compared to prior work when ǫ=0
- Proves a lower bound of Ω(ǫT), showing regret bounds are tight in terms of ǫ
- When ǫ=0, the bounds match standard bandit linear optimization results
- The key technical innovation is a new regret decomposition that separates linear and perturbation components

## Why This Works (Mechanism)

### Mechanism 1: SCRiBLe with Lifting using ν-Normal Barrier
The algorithm achieves O(d√(νT) + dT(ν+2√ν)(1-δ)/δ·ǫ + 2Tǫ) expected regret for approximately linear functions. The lifting technique transforms the original decision set K into a cone con(K) = {0} ∪ {(x,b): x/b ∈ K}, where a ν-normal barrier R always exists. This barrier serves as a regularizer in Follow-The-Regularized-Leader (FTRL), enabling gradient-based exploration through smoothed one-point estimators. The core assumption is that the loss function f(x) = θ^T x + σ(x) where |σ(x)| ≤ ǫ, with ||θ|| ≤ G and |f(x)| ≤ 1. When the perturbation ǫ exceeds O(1/√T), the regret becomes linear in T, making optimization ineffective.

### Mechanism 2: Novel Regret Decomposition for Oblivious Adversaries
The decomposition into Reg-Term, Deviation-Term, and Error-Term yields better high-probability bounds than prior work, especially when ǫ = 0. Instead of analyzing variance between estimator g_t and true gradient θ_t, this approach exploits the oblivious adversary assumption: the algorithm only needs to bound the variance between randomized actions y_t and base points x_t, not the estimator variance itself. The core assumption is that the adversary is oblivious (chooses all loss functions before the game starts, not adaptively). For adaptive adversaries, this decomposition fails since the adversary can respond to the player's actions, invalidating the variance analysis simplification.

### Mechanism 3: Error-Term Control via Shrunk Decision Sets
The perturbation-induced error term can be bounded even when Hessian eigenvalues grow unboundedly near the boundary. By operating in the shrunk set K_δ = {x: x/(1-δ) ∈ K} and using Lemma 4, the norm ||x-y||_x is bounded by 2(1/δ - 1)(ν + 2√ν). This controls the Error-Term without requiring increasing learning rates. The barrier satisfies self-concordance and logarithmic homogeneity properties. If δ is set too small, the (1-δ)/δ factor explodes; if δ is too large, the constraint violation penalty δGDT dominates.

## Foundational Learning

- **Concept: Follow-The-Regularized-Leader (FTRL)**
  - Why needed here: The algorithm uses FTRL with the ν-normal barrier as a regularizer to stabilize decisions and enable no-regret guarantees
  - Quick check question: Can you explain how the regularizer term R(x') affects the update rule x'_{t+1} = argmin_{x'∈K'_δ} [η∑g_τ^T x' + R(x')]?

- **Concept: Self-Concordant Barriers**
  - Why needed here: The ν-normal barrier provides the geometric structure (Dikin ellipsoids, bounded local norms) essential for controlling exploration and bounding the Error-Term
  - Quick check question: Why does logarithmic homogeneity (R(tx) = R(x) - ν ln t) matter for the regret analysis?

- **Concept: Bandit Feedback Gradient Estimation**
  - Why needed here: With only scalar feedback f_t(y_t), the algorithm constructs the one-point gradient estimator g_t = d·f_t(y_t)·A_t^{-1}μ_t
  - Quick check question: When ǫ ≠ 0, why is g_t no longer an unbiased estimator of θ_t, and how does the algorithm compensate?

## Architecture Onboarding

- **Component map**:
  Input layer: Decision set K, horizon T, perturbation bound ǫ
  Lifting module: Transforms K → con(K), constructs K'_δ
  Barrier module: Computes ν-normal barrier R and its Hessian ∇²R(x'_t)
  Exploration module: Samples μ_t uniformly from S^{d+1}_1 ∩ (A_te_{d+1})^⊥, generates y_t
  Estimation module: Constructs gradient estimator g_t from scalar feedback
  Update module: FTRL update using accumulated gradient estimates
  Output: Action sequence y_1, ..., y_T

- **Critical path**:
  1. Initialize x'_1 = argmin_{x'∈K'_δ} R(x')
  2. For each round t: compute A_t = [∇²R(x'_t)]^{-1/2}
  3. Sample μ_t, compute y_t = x'_t + A_tμ_t (exploration direction)
  4. Play y_t, observe f_t(y_t), construct g_t
  5. Update x'_{t+1} via FTRL
  The regret bound depends critically on proper choice of η and δ (parameters in Theorem 1)

- **Design tradeoffs**:
  - **δ selection**: Larger δ → larger constraint violation penalty (δGDT term), smaller δ → larger error term ((1-δ)/δ · ǫ). Paper recommends δ = T^{-2} when ǫ = 0, δ = √ǫ when ǫ ≠ 0
  - **Learning rate η**: Set to η = √(ν ln(1/δ))/(2d√T) to balance exploration cost (4ηd²T) and regularization cost (ν ln(1/δ)/η)
  - **Increasing learning rates**: Deliberately NOT used (unlike Lee et al.) since Lemma 4 makes them unnecessary for bounding ||h||_{∇²R(x'_t)}

- **Failure signatures**:
  - **Linear regret growth**: If ǫ = Ω(1), the 2Tǫ term dominates, making regret linear (matches the Ω(ǫT) lower bound)
  - **Boundary explosion**: If points approach the boundary of K too closely, Hessian eigenvalues grow unboundedly, breaking the Error-Term bound
  - **Adaptive adversary**: If the adversary chooses σ_t(y_t) after seeing y_t AND θ_t after seeing x_t, the regret analysis fails (paper assumes oblivious adversary)

- **First 3 experiments**:
  1. **Baseline comparison (ǫ = 0)**: Run Algorithm 1, SCRiBLe [3], and Lee et al.'s algorithm on pure linear losses to verify the improved high-probability bound when ǫ = 0. Expected result: Algorithm 1 achieves lower cumulative loss than Lee et al.'s O(ln²(dT)·d²·ln T·√T/ln ln(dT)/γ) bound
  2. **Perturbation scaling (varying ǫ)**: Test with perturbation function σ(y_t) = ǫ·sin((y_t^T l)π) for ǫ ∈ {0, 0.25, 0.5, 0.75, 1}. Expected result: Cumulative loss scales linearly with ǫ, matching the 2Tǫ term in the regret bound
  3. **Dimension sensitivity**: Vary d ∈ {5, 10, 20, 50} while holding T = 2000 fixed. Expected result: Cumulative loss scales as O(d) consistent with the d√(νT) term, not O(d²) as in Lee et al.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the lower bound for expected regret be tightened to $\Omega(d\epsilon T)$?
- Basis in paper: Section 6 states, "We conjecture that the lower bound can be tightened to $\Omega(d\epsilon T)$, but we leave it as an open problem."
- Why unresolved: The current lower bound is $\Omega(\epsilon T)$, which matches the perturbation term in the upper bound $O(dT\epsilon)$ only if the dimension $d$ is treated as a constant.
- What evidence would resolve it: A formal proof constructing an adversarial strategy that forces regret to scale linearly with both the dimension $d$ and the perturbation magnitude $\epsilon$.

### Open Question 2
- Question: Can this analytical framework be extended to bandit convex optimization with adversarial perturbations?
- Basis in paper: Section 8 identifies future work as "extending our results to broader loss classes, such as convex losses with perturbations."
- Why unresolved: The current analysis relies heavily on the specific structure of "approximately linear" functions ($f(x) = \theta^\top x + \sigma(x)$), which does not hold for general convex functions.
- What evidence would resolve it: Deriving regret bounds for bandit convex optimization that include a term dependent on the perturbation magnitude $\epsilon$, similar to the bounds derived for the linear case.

### Open Question 3
- Question: Do the high-probability regret bounds hold when facing adaptive adversaries?
- Basis in paper: The paper explicitly limits its scope to oblivious adversaries (Section 1) and compares its results to Lee et al. [17], which handled adaptive adversaries.
- Why unresolved: The analysis assumes the loss function $f_t$ is independent of the player's current action $x_t$, an assumption that breaks down if the adversary is adaptive.
- What evidence would resolve it: A proof showing the algorithm maintains its regret guarantees when $f_t$ is allowed to depend on $x_1, \dots, x_{t-1}$.

## Limitations
- **Algorithm sensitivity**: The choice of δ parameter critically affects performance. When δ is too small, the constraint violation penalty becomes negligible but the error term explodes. Conversely, when δ is too large, constraint violations dominate.
- **Approximation regime boundary**: When ϵ = Ω(1/√T), the regret bound becomes linear in T, making the algorithm ineffective. The paper's Ω(ϵT) lower bound shows this is unavoidable, but the transition point is sharp.
- **Oblivious adversary assumption**: The regret decomposition crucially relies on the adversary being oblivious. For adaptive adversaries who respond to the player's actions, the variance analysis breaks down and the bounds no longer hold.

## Confidence
- **High confidence**: The expected regret bounds and lower bound proofs appear mathematically sound. The decomposition into linear, deviation, and error terms is clearly stated.
- **Medium confidence**: The high-probability bounds depend on the specific construction of the ν-normal barrier and the shrunk set K_δ. While the technical steps are detailed, the interplay between δ and the various terms requires careful parameter tuning.
- **Low confidence**: The practical performance implications are unclear. The paper provides theoretical bounds but doesn't report empirical results to validate the theoretical predictions.

## Next Checks
1. **Adaptive adversary simulation**: Implement the algorithm against both oblivious and adaptive adversaries on a synthetic approximately-linear loss function. Compare regret growth rates to verify whether the oblivious assumption is indeed critical in practice.

2. **Parameter sensitivity analysis**: Run experiments varying δ systematically across multiple orders of magnitude on a fixed approximately-linear problem. Measure how the actual cumulative loss varies with δ to validate the theoretical tradeoff analysis.

3. **Approximation quality transition**: Construct test instances with controlled approximation error ϵ and varying T. Plot the ratio of actual regret to T across different ϵ values to empirically verify the linear growth transition around ϵ = Θ(1/√T) predicted by the lower bound.