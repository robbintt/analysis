---
ver: rpa2
title: Confidence-Guided Error Correction for Disordered Speech Recognition
arxiv_id: '2509.25048'
source_url: https://arxiv.org/abs/2509.25048
tags:
- correction
- speech
- confidence
- error
- recognition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses error correction for automatic speech recognition
  (ASR) of disordered speech, where existing systems often overcorrect accurate outputs,
  harming performance. To mitigate this, the authors introduce confidence-guided prompting,
  incorporating word-level uncertainty estimates from entropy-based confidence scores
  into LLM fine-tuning.
---

# Confidence-Guided Error Correction for Disordered Speech Recognition

## Quick Facts
- **arXiv ID:** 2509.25048
- **Source URL:** https://arxiv.org/abs/2509.25048
- **Reference count:** 0
- **Primary result:** 10% relative WER reduction on spontaneous speech, 47% on TORGO vs naive LLM correction

## Executive Summary
This paper addresses a critical failure mode in LLM-based error correction for disordered speech recognition: overcorrection of accurate outputs. The authors introduce confidence-guided prompting, incorporating word-level uncertainty estimates from entropy-based confidence scores into LLM fine-tuning. Using LLaMA 3.1 fine-tuned on Speech Accessibility Project data, the method directs corrections toward low-confidence regions while preserving high-confidence words. Evaluations show significant WER improvements and more selective correction behavior compared to naive LLM approaches, with successful generalization across ASR architectures and datasets.

## Method Summary
The approach integrates Tsallis entropy-based confidence scores into LLM prompting for error correction. ASR hypotheses are processed with Parakeet TDT-CTC 110M to generate reference-hypothesis pairs from the Speech Accessibility Project dataset. Frame-level confidence scores are aggregated to word-level using product, mean, or min operations. These confidence values are embedded in prompts (e.g., "WORD[conf]") during fine-tuning of LLaMA 3.1 8B with LoRA (r=16, α=16). The model learns to focus corrections on low-confidence words while preserving high-confidence outputs. Evaluation includes both spontaneous speech and TORGO database, with performance measured via WER and correction behavior analysis.

## Key Results
- 10% relative WER reduction on SAP-unshared test set compared to naive LLM correction
- 47% WER reduction on TORGO database (from 20.01% to 10.56%)
- Improved selective correction: harmful correction rate <5% on high-confidence words vs 33% for naive LLM
- Generalization to Whisper ASR demonstrated with comparable improvements

## Why This Works (Mechanism)
The method works by providing the LLM with uncertainty information at the word level, allowing it to distinguish between words that are likely correct (high confidence) and those that need correction (low confidence). This prevents the LLM from indiscriminately modifying outputs, which is particularly problematic for disordered speech where ASR errors are often systematic rather than random. By conditioning corrections on confidence scores, the model learns to preserve accurate outputs while focusing its correction efforts where they're most likely to help.

## Foundational Learning

**Tsallis entropy confidence estimation:** Measures uncertainty in ASR frame-level predictions using non-extensive entropy with parameter α. Why needed: Traditional confidence measures don't capture the specific uncertainty patterns in disordered speech. Quick check: Verify Tsallis entropy produces meaningful score distributions across different α values.

**Word-level confidence aggregation:** Combines frame-level scores into per-word confidence using product, mean, or min operations. Why needed: ASR outputs are at word level, requiring aggregation of frame-level uncertainty. Quick check: Compare aggregation methods on a held-out validation set.

**LoRA fine-tuning for LLMs:** Efficient parameter-efficient fine-tuning using low-rank adaptation matrices. Why needed: Full fine-tuning of large models is computationally expensive and risks catastrophic forgetting. Quick check: Monitor training loss and validation performance during fine-tuning.

**Selective correction behavior:** The ability to distinguish when to modify outputs versus when to preserve them. Why needed: Overcorrection is a major failure mode for disordered speech error correction. Quick check: Measure helpful vs harmful correction rates across confidence levels.

## Architecture Onboarding

**Component map:** ASR -> Confidence Estimation -> Prompt Construction -> LLM Fine-tuning -> Error Correction

**Critical path:** The key workflow is: generate ASR hypotheses → compute word-level confidence scores → construct prompts with embedded confidence → fine-tune LLM → apply corrections only to low-confidence words.

**Design tradeoffs:** 
- Confidence granularity vs. computational cost (frame-level vs. word-level)
- Prompt complexity vs. model performance
- Fine-tuning duration vs. risk of overfitting
- Generalization vs. dataset-specific optimization

**Failure signatures:** 
- Overcorrection on high-confidence words (naive LLM raises TORGO WER from 10.83% to 20.01%)
- Under-correction when confidence scores are miscalibrated
- Degradation when applied to speakers with severe dysarthria not represented in training data

**First experiments:**
1. Fine-tune LLaMA 3.1 8B with confidence-guided prompts on SAP data
2. Compare WER and correction behavior against naive LLM baseline
3. Test generalization to Whisper ASR outputs on SAP data

## Open Questions the Paper Calls Out
- Does confidence-guided error correction generalize to speakers with severe dysarthria, or does performance degrade substantially compared to the mild-to-moderate cases studied?
- How robust are entropy-based confidence estimates across diverse acoustic conditions, speaker populations, and ASR architectures without requiring dataset-specific parameter tuning?
- Can confidence-guided correction improve performance on isolated word recognition tasks, where contextual information is unavailable?
- Does speaker-specific fine-tuning or personalization of the correction model provide additional gains over the speaker-independent approach presented?

## Limitations
- The method requires careful parameter tuning (α values, aggregation strategies) and may not be well-calibrated across different conditions
- The SAP dataset's focus on mild-to-moderate Parkinson's disease speakers limits generalizability to severe dysarthria and spontaneous speech
- Training data size (~130K pairs) is relatively modest for LLM fine-tuning, and scalability to larger or noisier datasets remains untested

## Confidence

**High confidence:** The reported WER reductions (10% on SAP-unshared, 47% on TORGO) and improved selective correction behavior are supported by direct comparisons with naive LLM baselines in the paper.

**Medium confidence:** The generalization claims to Whisper ASR and larger models are plausible given the method's prompt-based design, but require more extensive empirical validation.

**Low confidence:** The theoretical basis for optimal α and aggregation method selection is unclear; results may be sensitive to hyperparameter choices.

## Next Checks

1. **Reproduce key results on SAP-unshared and TORGO:** Fine-tune LLaMA 3.1 8B with the described LoRA setup and confidence-guided prompts, then compare WER and correction behavior to naive LLM baseline.

2. **Test generalization to Whisper ASR:** Apply the fine-tuned model to Whisper-generated outputs on SAP data; verify if relative WER gains persist or degrade.

3. **Validate confidence aggregation sensitivity:** Sweep α (0.3 to 0.9) and aggregation methods (product/mean/min) to assess impact on helpful/harmful correction rates and overall WER.