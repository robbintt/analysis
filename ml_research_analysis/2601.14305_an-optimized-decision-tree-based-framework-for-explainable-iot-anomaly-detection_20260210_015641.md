---
ver: rpa2
title: An Optimized Decision Tree-Based Framework for Explainable IoT Anomaly Detection
arxiv_id: '2601.14305'
source_url: https://arxiv.org/abs/2601.14305
tags:
- detection
- accuracy
- data
- decision
- tree
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of detecting anomalies in IoT
  networks while ensuring model interpretability and low computational overhead for
  resource-constrained devices. It proposes an optimized Decision Tree-based framework
  enhanced with explainable AI (XAI) techniques including SHAP values and Morris sensitivity
  analysis to provide both local and global feature importance.
---

# An Optimized Decision Tree-Based Framework for Explainable IoT Anomaly Detection

## Quick Facts
- **arXiv ID:** 2601.14305
- **Source URL:** https://arxiv.org/abs/2601.14305
- **Reference count:** 15
- **Primary result:** Decision Tree-based IoT anomaly detection achieving 99.91% accuracy with interpretability via SHAP and Morris sensitivity analysis

## Executive Summary
This paper proposes an optimized Decision Tree-based framework for explainable IoT anomaly detection that addresses the challenge of balancing high detection performance with interpretability and computational efficiency for resource-constrained edge devices. The approach combines comprehensive preprocessing, feature selection, class imbalance handling, and XAI techniques (SHAP values and Morris sensitivity analysis) to provide both local and global feature importance explanations. Tested on the WUSTL-IoT dataset, the framework achieves exceptional performance metrics (99.91% accuracy, 99.51% F1-score) while identifying SrcMac as the most influential feature for anomaly detection, making it suitable for real-time deployment in IoT environments.

## Method Summary
The framework implements a comprehensive preprocessing pipeline including duplicate removal, label encoding for categorical variables, IQR-based outlier capping with mean replacement, and z-score normalization. Feature selection employs chi-square tests (p<0.05) followed by Pearson correlation with Bonferroni correction to identify statistically significant features. The 80:20 stratified split addresses class imbalance through random oversampling of minority classes to 11,424 samples each, with Gaussian noise injection (15% of feature standard deviation) for robustness. A Decision Tree classifier (max_depth=10, min_samples_split=4) serves as the core detection model, evaluated through 5-fold cross-validation. Explainability is achieved through SHAP values for local instance explanations and Morris sensitivity analysis for global feature importance assessment.

## Key Results
- Achieved 99.91% accuracy, 99.51% F1-score, and 99.60% Cohen's Kappa on WUSTL-IoT dataset
- Cross-validation stability shows 98.93% mean accuracy with minimal variance (SD=0.0003)
- Identified SrcMac as the most influential feature for anomaly detection across both SHAP and Morris analysis
- Successfully handles class imbalance (Spoofing 6.9%, Data Alteration 5.7%) while maintaining balanced per-class performance

## Why This Works (Mechanism)
The framework's effectiveness stems from its strategic combination of preprocessing rigor, statistical feature selection, and interpretable modeling. The preprocessing pipeline ensures data quality and normalization, while the two-stage feature selection (chi-square followed by Pearson correlation with Bonferroni correction) rigorously identifies statistically significant predictors while controlling for false positives. The Decision Tree architecture provides inherent interpretability through its hierarchical structure, which is enhanced by SHAP values for local instance explanations and Morris sensitivity analysis for global feature importance. Class imbalance is addressed through oversampling with noise injection, preventing the model from being biased toward the majority Normal class while maintaining detection performance across all three classes.

## Foundational Learning
- **Chi-square feature selection**: Tests independence between categorical features and target classes; needed to filter irrelevant features and reduce dimensionality before correlation analysis; quick check: verify retained features have p<0.05
- **Pearson correlation with Bonferroni correction**: Measures linear relationships between numerical features while controlling family-wise error rate; needed to identify redundant or weakly correlated features; quick check: confirm adjusted p-values <0.05 for retained features
- **SHAP (SHapley Additive exPlanations)**: Provides local feature importance based on game theory; needed to explain individual predictions and understand feature contributions; quick check: verify SHAP values sum to model output for each instance
- **Morris sensitivity analysis**: Global sensitivity screening method using elementary effects; needed to identify influential features across the entire dataset; quick check: confirm top features align with SHAP analysis
- **Class imbalance handling via oversampling**: Addresses skewed class distributions by generating synthetic samples; needed to prevent model bias toward majority class; quick check: verify training set has balanced class counts post-augmentation
- **Gaussian noise injection**: Adds controlled randomness to training data; needed to improve model robustness and prevent overfitting; quick check: confirm noise magnitude is 15% of feature standard deviation

## Architecture Onboarding

### Component Map
Raw Data -> Preprocessing Pipeline (duplicate removal, label encoding, IQR capping, z-score normalization) -> Feature Selection (chi-square + Pearson + Bonferroni) -> Stratified Split -> Oversampling + Gaussian Noise Injection -> Decision Tree Classifier -> SHAP & Morris Analysis -> Performance Evaluation

### Critical Path
Data preprocessing → Feature selection → Class balancing → Model training → Explainability analysis → Performance evaluation

### Design Tradeoffs
- Decision Tree vs. ensemble methods: Prioritized interpretability over potential accuracy gains
- Oversampling vs. undersampling: Chose oversampling to preserve information while balancing classes
- Noise injection magnitude: Selected 15% of std as balance between robustness and preserving signal
- Max depth=10: Limited to maintain interpretability while capturing complexity

### Failure Signatures
- Class imbalance causing biased predictions toward Normal class
- Data leakage if oversampling applied before CV split
- Inconsistent CV results from non-deterministic noise injection
- Overfitting from insufficient depth constraints or excessive noise

### 3 First Experiments to Run
1. Verify preprocessing pipeline maintains feature distributions while removing outliers
2. Test feature selection pipeline independently to confirm statistical significance filtering
3. Validate class balancing by comparing class distributions before and after oversampling

## Open Questions the Paper Calls Out
### Open Question 1
**Question:** How does the proposed framework perform when deployed on physical resource-constrained edge devices in live IoT environments?
**Basis in paper:** The authors state that future efforts include "implementation of the framework in real-world settings of IoT will also be able to test its stability and capacity to be applied in practice."
**Why unresolved:** The current study evaluates computational cost theoretically and via simulation datasets, but does not provide empirical data on actual hardware resource consumption (memory, battery, processing power) during real-time operation.
**What evidence would resolve it:** Benchmarks of latency, energy consumption, and memory usage collected from deploying the model on specific edge hardware (e.g., Raspberry Pi, FPGA) within a live network.

### Open Question 2
**Question:** Can the framework maintain high detection efficacy when applied to larger, more diverse datasets beyond the specific WUSTL-EHMS-2020 IoMT dataset?
**Basis in paper:** The conclusion notes that "Larger and broader datasets will assist in better generalization."
**Why unresolved:** The model was trained and validated on a specific dataset with only three classes (Normal, Spoofing, Data Alteration), raising concerns about overfitting to the specific traffic patterns and feature distributions of that dataset.
**What evidence would resolve it:** Cross-database validation results showing accuracy and F1-scores when the model is tested on disparate IoT benchmark datasets (e.g., Bot-IoT, UNSW-NB15) or data from different IoT domains like smart cities.

### Open Question 3
**Question:** Does the integration of adaptive learning processes allow the model to effectively evolve alongside changing attack vectors?
**Basis in paper:** The authors propose "integration of adaptive learning processes" to allow "a greater sense of response to adaptive IoT threats."
**Why unresolved:** The current framework operates as a static model, which may degrade in performance over time due to concept drift as new types of anomalies emerge that were not present in the training data.
**What evidence would resolve it:** Longitudinal studies showing the model's retention of F1-score and Kappa statistics on streaming data where attack signatures mutate or shift over time.

## Limitations
- Missing random seed value for exact reproducibility despite claiming "fixed random seed"
- Morris sensitivity analysis parameters (trajectories, perturbation delta, sampling strategy) not specified
- Final feature count after combined chi-square + Pearson filtering not explicitly stated
- No empirical hardware deployment data for resource-constrained edge devices
- Limited to three-class detection without validation on larger, more diverse IoT datasets

## Confidence
- **High confidence** in overall methodology and reported metrics
- **Medium confidence** in exact implementation details for feature selection
- **Medium confidence** in CV stability claims due to underspecified noise injection

## Next Checks
1. Verify that Gaussian noise injection occurs only within training folds during cross-validation, not globally before splitting
2. Replicate the complete feature selection pipeline (chi-square + Pearson + Bonferroni) to confirm the final feature set matches the paper's top-10 ranking
3. Test reproducibility by fixing the random seed and comparing results across multiple runs with identical preprocessing and model hyperparameters