---
ver: rpa2
title: 'Autonomous Control Leveraging LLMs: An Agentic Framework for Next-Generation
  Industrial Automation'
arxiv_id: '2507.07115'
source_url: https://arxiv.org/abs/2507.07115
tags:
- heater
- control
- agent
- temperature
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper introduces a modular agentic framework integrating large\
  \ language models (LLMs) for dual autonomy in industrial control: discrete fault-recovery\
  \ planning via finite state machines (FSMs) and continuous temperature regulation\
  \ under disturbances. In the FSM case study, GPT-4o and GPT-4o-mini achieved 100\
  \ % valid-path success across 180 randomly generated FSMs (4\u201325 states, 4\u2013\
  300 transitions) within five reprompts, outperforming open-source LLMs in accuracy\
  \ and latency."
---

# Autonomous Control Leveraging LLMs: An Agentic Framework for Next-Generation Industrial Automation

## Quick Facts
- arXiv ID: 2507.07115
- Source URL: https://arxiv.org/abs/2507.07115
- Reference count: 22
- Primary result: Modular LLM-based framework achieves 100% valid-path FSM traversal and comparable PID control performance for dual-heater temperature regulation

## Executive Summary
This paper introduces a modular agentic framework integrating large language models (LLMs) for dual autonomy in industrial control: discrete fault-recovery planning via finite state machines (FSMs) and continuous temperature regulation under disturbances. In the FSM case study, GPT-4o and GPT-4o-mini achieved 100% valid-path success across 180 randomly generated FSMs (4-25 states, 4-300 transitions) within five reprompts, outperforming open-source LLMs in accuracy and latency. In the continuous control case study, the LLM-based controller regulated dual-heater temperature on a TCLab platform to maintain a setpoint under asymmetric disturbances, achieving comparable performance to PID control; ablation experiments showed the reprompting loop was critical for handling nonlinear dynamics. Key limitations identified include instruction-following lapses and coarse ODE approximations. Results demonstrate that structured feedback and modular agents enable LLMs to unify symbolic reasoning and continuous control in autonomous industrial systems.

## Method Summary
The framework employs a five-agent pipeline (monitoring, action, simulation, validation, reprompting) orchestrated via CrewAI. FSMs are encoded as Python dictionaries for interpretability. For continuous control, a digital twin simulates plant dynamics using energy balance equations with convection and radiation terms. The action agent proposes control actions, which are validated by the simulation and validation agents before execution. Invalid proposals trigger the reprompting agent, which reformulates the query based on specific violation details. The system uses a 30-second planning interval to accommodate LLM inference delays, with heater specifications including 500 J/kg-K specific heat, 0.004 kg mass, and 0.3W power limits.

## Key Results
- FSM traversal: 100% valid-path success across 180 instances with GPT-4o/mini, requiring ≤0.25 reprompts on average
- Control performance: LLM achieves comparable TW-MAE (1.01K) and RMSE (1.27K) to PID baseline
- Reprompting effectiveness: Ablation shows critical role in handling nonlinear dynamics; LLaMA requires 3.5+ reprompts with 30% success
- Prediction accuracy: LLM ODE approximations show 3.31K drift vs ground truth at 30s due to coarse finite-difference method

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The validator-reprompting loop enables LLMs to self-correct invalid plans through structured feedback
- Mechanism: When an Action Agent proposes a plan, a Simulation Agent executes it symbolically or numerically. The Validation Agent checks outcomes against constraints. If invalid, a Reprompting Agent reformulates the prompt with specific violation details, triggering a new planning iteration. This creates a bounded search where each failure narrows the solution space.
- Core assumption: LLMs can interpret structured rejection feedback and avoid previously explored failures within a short context window
- Evidence anchors:
  - [abstract] "a Validator–Reprompting loop iteratively refines invalid plans"
  - [section 4.1.3] "reprompt count remains low (≤0.25 per instance) [for GPT-4o]... while ablation of the reprompting loop reveals its critical role in handling nonlinear dynamics"
- Break condition: If LLM fails to interpret feedback or context saturates, reprompting yields diminishing returns. LLaMA required 3.5+ reprompts on hard FSMs with only 30% success.

### Mechanism 2
- Claim: Encoding FSMs as Python dictionaries improves LLM traversal accuracy over matrix or edge-list representations
- Mechanism: Dictionary format (e.g., `FSM = {0: [1, 2], 1: [2]}`) allows direct symbolic queries that align with natural language patterns. This reduces the reasoning distance between prompt and graph structure.
- Core assumption: LLMs trained on code can more reliably parse and reason over dictionary structures than adjacency matrices requiring index-based lookup
- Evidence anchors:
  - [section 4.1] "which we found to be more interpretable and reliably processed by LLMs than adjacency matrices or flat edge lists"
- Break condition: For very large FSMs, even dictionary encoding may exceed context window or introduce token-count penalties.

### Mechanism 3
- Claim: Digital twin simulation grounds LLM control proposals in physics before real-world execution
- Mechanism: The Action Agent proposes control inputs. The Simulation Agent computes predicted trajectories using ODE models. The Validation Agent checks physical constraints. Only validated actions reach the physical plant. This decouples LLM latency from real-time control loops.
- Core assumption: The digital twin's ODE model sufficiently approximates real plant dynamics; LLM approximations are caught by simulation, not by the LLM itself
- Evidence anchors:
  - [section 4.2.1] "The emulated physical system was paused using a fixed 30-second planning interval to accommodate LLM inference delays"
  - [section 4.2.2] "LLM-predicted vs. ground truth temperature over time" shows divergence up to 3.31K at 30 seconds
- Break condition: If simulation model fidelity is poor or disturbance profiles diverge from training, validated actions may still fail on physical plant.

## Foundational Learning

- Concept: **Finite State Machines (FSMs) as control abstractions**
  - Why needed here: FSMs serve as interpretable operating envelopes that constrain LLM proposals to valid state transitions, enabling symbolic reasoning without exposing raw physics
  - Quick check question: Given an FSM `{A: [B, C], B: [D], C: [D], D: []}`, can you trace all valid paths from A to D and identify which is shortest?

- Concept: **Classical PID control and error metrics (TW-MAE, RMSE)**
  - Why needed here: The paper benchmarks LLM control against PID; understanding proportional-integral-derivative behavior and error metrics is essential to interpret "similar performance" claims
  - Quick check question: If a temperature controller has steady-state error of 1.5K with RMSE of 1.27K, what does this suggest about oscillation vs. bias in the control signal?

- Concept: **Agentic orchestration patterns (monitoring → planning → simulation → validation → reprompting)**
  - Why needed here: The framework is not a single LLM call but a multi-agent pipeline with state management, tool interfaces, and fallback logic
  - Quick check question: In a 5-agent pipeline, if the Validation Agent rejects 40% of proposals and average reprompts are 0.25, what is the implied first-pass acceptance rate?

## Architecture Onboarding

- Component map: [Plant/Digital Twin] ←→ [Monitoring Agent] ↓ (anomaly detected) [Action Agent] ←→ [LLM] ↓ [Simulation Agent] ←→ [Physics Model] ↓ [Validation Agent] (constraint checks) ↓ ┌─────────┴─────────┐ ↓ [Reprompting Agent] [Execute on Plant] ↓ [Action Agent] (retry loop)

- Critical path: Monitoring Agent detects deviation → Action Agent proposes path → Simulation validates → (if invalid) Reprompting reformulates → repeat until valid or max iterations → Safety Override if exhausted. For continuous control, the 30-second planning window is the hard constraint; latency >30s causes state mismatch.

- Design tradeoffs:
  - **Cloud (GPT-4o) vs. Local (LLaMA):** Cloud offers higher accuracy (100% vs. 30-70% valid path) but introduces network latency (34s vs. 12.5s) and data sovereignty concerns
  - **Reprompt budget:** More iterations improve success but increase latency. Paper caps at 5 for FSM, adaptive for control
  - **FSM complexity:** Larger graphs (25 nodes, 300 edges) maintain 100% accuracy for GPT-4o but may strain context windows for smaller models

- Failure signatures:
  - **Instruction-following lapses:** LLMs repeat previously rejected paths during reprompting (observed with LLaMA in FSM tasks)
  - **Coarse ODE approximations:** Single-step finite difference instead of proper integration, leading to 3+K prediction error at 30s
  - **Equal power distribution heuristic:** LLM ignores current temperature asymmetry, causing overheating
  - **State drift during inference:** Physical plant evolves while LLM deliberates, causing sensor readings to mismatch LLM's assumed state

- First 3 experiments:
  1. **Reproduce FSM traversal benchmark:** Generate 20 random FSMs per complexity tier (4-6, 10-15, 20-25 nodes). Test GPT-4o-mini with 5 reprompt budget. Measure first-pass accuracy, valid-path accuracy, and reprompt count. Expect >95% valid-path within 5 reprompts for medium complexity.
  2. **Ablate reprompting in control task:** Run digital twin temperature control with and without the reprompting loop under identical disturbance profiles. Compare TW-MAE and RMSE. Expect degraded performance without reprompting.
  3. **Latency vs. accuracy tradeoff:** Run identical control scenario on (a) cloud GPT-4o, (b) local LLaMA-3.2, (c) local LLaMA-3:8b. Measure inference time, reprompt count, and control error. Expect cloud to outperform on accuracy, local to win on latency but lose on RMSE.

## Open Questions the Paper Calls Out

- **How can symbolic planning capabilities be effectively grounded in the physics and constraints of real-world plant dynamics?**
  - Basis in paper: [explicit] The authors explicitly ask this question in the introduction
  - Why unresolved: Current case studies utilize abstract FSMs or simplified digital twins, which do not capture the full variability of industrial environments
  - What evidence would resolve it: Demonstration of LLM agents successfully deriving recovery plans directly from high-fidelity physics models or real-time sensor data in a complex, multi-unit processing plant

- **Can LLM-based control frameworks generalize to unstructured, high-dimensional, or multi-modal plant state representations?**
  - Basis in paper: [inferred] The Discussion notes that the system's ability to generalize to "unstructured, high-dimensional, or multi-modal plant state representations remains untested"
  - Why unresolved: The current study relied on structured inputs and did not test reasoning over diverse data formats like raw logs, alarms, or video feeds
  - What evidence would resolve it: Successful deployment of the agentic framework in a facility utilizing heterogeneous data sources without structured pre-processing

- **How can the architectural design mitigate failures in physical modeling, such as coarse ODE approximations and instruction-following lapses?**
  - Basis in paper: [inferred] The paper analyzes failure modes where "LLMs occasionally produced physically implausible approximations" and failed to follow instructions during reprompting
  - Why unresolved: The inherent stochasticity and token-limitations of LLMs conflict with the precision required for solving differential equations or adhering to strict safety constraints
  - What evidence would resolve it: A study comparing baseline LLM performance against a hybrid system where the LLM invokes external physics solvers or formal verification tools to validate control actions

## Limitations

- LLM-based control shows 3.31K temperature prediction drift at 30 seconds due to coarse ODE approximations (single finite-difference step vs. proper integration)
- Instruction-following lapses cause LLMs to repeat rejected paths during reprompting, particularly with open-source models like LLaMA
- Physical experiments revealed 34s mean latency vs. 12.5s for digital twin due to state drift during LLM inference
- Control performance claims are based on digital twin experiments only, with physical implementation serving as a latency case study rather than performance validation

## Confidence

- FSM traversal claims (100% valid-path success): High confidence - robust across 180 randomly generated instances with clear metrics
- LLM control performance claims (comparable to PID): Medium confidence - validated only on digital twin with 3.31K prediction error
- Reprompting loop effectiveness: High confidence - ablation experiments clearly show critical role in handling nonlinear dynamics
- FSM encoding advantage: Low confidence - based on author's heuristic observation without direct comparative experiments

## Next Checks

1. **Physical plant validation:** Deploy the LLM control system on actual TCLab hardware under identical disturbance profiles to verify digital twin performance claims and measure real-world control error metrics (TW-MAE, RMSE)
2. **Open-loop trajectory prediction:** Compare LLM single-step ODE approximations against numerical integration methods across varying time horizons to quantify prediction drift and identify the source of 3.31K error at 30s
3. **Instruction-following stress test:** Design adversarial FSM traversal tasks that specifically trigger instruction-following lapses and measure whether reprompting consistently corrects these failures across multiple LLM models