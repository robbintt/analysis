---
ver: rpa2
title: A Two-Sample Test of Text Generation Similarity
arxiv_id: '2505.05269'
source_url: https://arxiv.org/abs/2505.05269
tags:
- test
- text
- data
- where
- document
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a two-sample text test for comparing similarity
  between two groups of documents by analyzing the entropy of documents using neural
  network-based language models. The test uses an estimation-and-inference framework
  where entropy is first approximated using an estimation set, followed by inference
  on the remaining data set.
---

# A Two-Sample Test of Text Generation Similarity

## Quick Facts
- **arXiv ID:** 2505.05269
- **Source URL:** https://arxiv.org/abs/2505.05269
- **Reference count:** 40
- **Primary result:** Proposes a two-sample test comparing document entropy estimates from neural language models to assess text generation similarity.

## Executive Summary
This paper introduces a novel two-sample test for comparing similarity between two groups of documents by analyzing their entropy using neural network-based language models. The test employs an estimation-and-inference framework where entropy is first approximated using a training split, followed by hypothesis testing on held-out data. To address power loss from data splitting, the method uses multiple random splits and combines resulting p-values through Cauchy combination or Multiple P-value Thresholding methods. The approach effectively handles the high-dimensional, unstructured nature of text data while maintaining Type I error control and demonstrating superior power compared to existing methods.

## Method Summary
The method first trains a neural autoregressive language model on an estimation set to learn document probability estimates. It then computes average negative log-likelihoods (entropy) for each document in the inference set for both groups. The test statistic is the normalized difference of these entropy estimates, which converges to standard normal under the null hypothesis. To enhance power, the data is split multiple times, and p-values from each split are combined using Cauchy combination or Multiple P-value Thresholding methods to produce a unified decision.

## Key Results
- Maintains nominal Type I error rate while offering greater power compared to existing methods
- Effectively handles high-dimensional, unstructured textual data
- Demonstrates robustness through multiple data-splitting strategy that combines p-values
- Validated through numerical studies and real data examples using AG News corpus

## Why This Works (Mechanism)

### Mechanism 1: Entropy as a Population-Level Fingerprint of Generative Distributions
If two document collections arise from the same probabilistic generative mapping, their expected entropy will be equivalent; measurable divergence indicates distinct underlying generators. The test estimates expected entropy per corpus and normalizes the difference by pooled variance. Under the null, the normalized statistic converges to standard normal, enabling classical hypothesis testing without permutation.

### Mechanism 2: Neural Language Models Enable Tractable High-Dimensional Probability Estimation
Neural network-based language models convert the intractable problem of estimating joint word-sequence probabilities into a tractable one by learning dense embeddings and conditional distributions over a fixed vocabulary. Words are embedded, context vectors are constructed from preceding words, and a feedforward network with softmax outputs conditional probabilities. The learned function provides log-probabilities for test statistic computation.

### Mechanism 3: Multiple Data-Splitting with P-Value Combination Mitigates Power Loss
Single estimation/inference splits sacrifice sample size for inference, reducing power; repeated random splits with aggregated p-values recover robustness while preserving Type I error. Data is split multiple times; for each split, an NNLM is trained on the estimation set and the test statistic computed on the inference set. P-values from all splits are combined via Cauchy combination or MPT methods.

## Foundational Learning

- **Concept: Shannon entropy for sequences**
  - Why needed: The entire test rests on entropy differences as the signal; misunderstanding entropy leads to misinterpreting the null hypothesis.
  - Quick check: Given a fair coin and a biased coin (p=0.9 heads), which has higher entropy, and why does entropy relate to "information content"?

- **Concept: Autoregressive language modeling and softmax**
  - Why needed: The NNLM uses autoregressive factorization; understanding how softmax produces conditional probabilities is essential to debug probability estimation.
  - Quick check: In a vocabulary of size 10,000, if the softmax output for the true next word is 0.001, what is its contribution to the log-likelihood for that token?

- **Concept: Data splitting for valid post-selection inference**
  - Why needed: Training and testing on the same data biases entropy estimates; the estimation/inference split ensures Type I error control.
  - Quick check: Why does using the same data for model training and hypothesis testing typically inflate false positive rates?

## Architecture Onboarding

- **Component map:** Tokenizer/Vocabulary -> Embedding Layer -> Context Aggregator -> Feedforward Network + Softmax -> Entropy Aggregator -> Test Statistic Calculator -> P-Value Combiner

- **Critical path:**
  1. Pool corpora A and B to build shared vocabulary
  2. For each split m = 1 to M:
     - Randomly partition into estimation/inference sets
     - Train NNLM on estimation sets (minimize cross-entropy loss)
     - Compute entropy estimates on inference sets
     - Compute test statistic and p-value
  3. Combine p-values; reject if combined statistic exceeds threshold

- **Design tradeoffs:**
  - Larger context window n: Better semantic modeling, but increased input dimension slows convergence
  - More splits M: Higher power recovery, but computational cost scales linearly
  - Truncation parameter B: Bounds log-probabilities to avoid -∞, but introduces bias if many true probabilities are very small

- **Failure signatures:**
  - Type I error inflation: Check if estimation and inference sets overlap; verify independence
  - Near-zero power: May indicate undertrained NNLM, insufficient estimation set size, or γ misconfiguration
  - Numerical instability in softmax: Log of near-zero probabilities; apply log-sum-exp trick and verify truncation B

- **First 3 experiments:**
  1. Size calibration under null: Generate two corpora from same distribution; verify empirical Type I error at α=0.05 across 1,000 replications
  2. Power under controlled divergence: Fix δ = μA - μB by varying covariance parameters; plot power vs. δ and compare single-split vs. Cauchy vs. MPT combinations
  3. Sensitivity to context window: Repeat power analysis with n ∈ {3, 5, 10}; observe tradeoff between convergence speed and semantic capture

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the estimation data excluded during inference be reused to enhance statistical power while maintaining Type I error guarantees?
  - Basis: Section 5 states future research could explore strategies for reusing the estimation data that is not utilized in the inference phase to enhance statistical power.
  - Why unresolved: The current estimation-and-inference framework necessitates data splitting to prevent overfitting, which reduces sample size for inference and thus power.
  - What evidence would resolve it: Theoretical proofs or empirical validation of a new framework that safely integrates estimation data into the inference step without inflating Type I error.

- **Open Question 2:** Can the test statistics be adapted for non-autoregressive neural network structures?
  - Basis: Section 5 suggests investigating whether the proposed test statistics can be adapted for non-autoregressive neural network structures and examining their theoretical properties.
  - Why unresolved: The current theoretical derivation relies on an autoregressive decomposition of the document likelihood, which may not hold for all generation processes.
  - What evidence would resolve it: Derivation of the asymptotic distribution of the test statistic under non-autoregressive generation assumptions.

- **Open Question 3:** How sensitive is the test to violations of the model invariance assumption across document groups?
  - Basis: Section 3.1 assumes a unique invariant function g* holds across groups.
  - Why unresolved: If the conditional probability estimates differ significantly across groups due to training instabilities or distinct vocabularies, the validity of the unified model is uncertain.
  - What evidence would resolve it: Sensitivity analysis or simulations assessing Type I error and power when the shared model assumption is violated by varying degrees of distribution shift.

## Limitations

- Method's validity critically depends on neural language model accurately estimating true document probability distributions
- Theoretical framework assumes Holder smoothness and bounded probability regions, but practical NNLM performance may deviate
- Choice of p-value combination method and its robustness to dependence between overlapping splits is not fully characterized

## Confidence

- **High Confidence**: Type I error control under ideal conditions (correctly trained NNLM, proper data splitting)
- **Medium Confidence**: Power advantages over existing methods, based on simulation studies
- **Low Confidence**: Theoretical guarantees for p-value combination methods under the specific dependence structure induced by overlapping estimation sets

## Next Checks

1. **Type I Error Validation**: Generate synthetic corpora from identical distributions and verify empirical Type I error at α=0.05 across 1,000 replications
2. **Power Sensitivity Analysis**: Systematically vary the difference between document class distributions (δ) and plot power curves, comparing single-split vs. Cauchy vs. MPT combinations
3. **Hyperparameter Robustness**: Evaluate test performance across different context window sizes (n ∈ {3, 5, 10}) to assess the tradeoff between semantic capture and convergence speed