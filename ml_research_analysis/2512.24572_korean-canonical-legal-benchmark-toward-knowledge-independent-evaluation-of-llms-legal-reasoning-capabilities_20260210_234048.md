---
ver: rpa2
title: 'Korean Canonical Legal Benchmark: Toward Knowledge-Independent Evaluation
  of LLMs'' Legal Reasoning Capabilities'
arxiv_id: '2512.24572'
source_url: https://arxiv.org/abs/2512.24572
tags:
- reasoning
- precedents
- legal
- evaluation
- korean
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces the Korean Canonical Legal Benchmark (KCL),
  designed to evaluate language models'' legal reasoning capabilities independently
  of domain-specific knowledge. KCL includes two components: KCL-MCQA, a multiple-choice
  benchmark with 283 questions and 1,103 aligned precedents, and KCL-Essay, an open-ended
  generation benchmark with 169 questions, 550 precedents, and 2,739 instance-level
  rubrics for automated evaluation.'
---

# Korean Canonical Legal Benchmark: Toward Knowledge-Independent Evaluation of LLMs' Legal Reasoning Capabilities

## Quick Facts
- arXiv ID: 2512.24572
- Source URL: https://arxiv.org/abs/2512.24572
- Reference count: 36
- Introduces KCL, a knowledge-independent legal reasoning benchmark with 283 multiple-choice and 169 open-ended questions

## Executive Summary
This paper introduces the Korean Canonical Legal Benchmark (KCL), a comprehensive evaluation framework designed to assess language models' legal reasoning capabilities independent of domain-specific knowledge. KCL consists of two components: KCL-MCQA with 283 multiple-choice questions and 1,103 aligned precedents, and KCL-Essay with 169 open-ended questions, 550 precedents, and 2,739 instance-level rubrics for automated evaluation. The benchmark provides question-level supporting precedents to disentangle reasoning ability from parameterized knowledge, enabling more faithful assessment of legal reasoning.

Systematic evaluation of 30+ models reveals significant performance gaps, with the best models achieving only 74.9% on KCL-Essay even with precedents provided. Reasoning-specialized models consistently outperform general-purpose counterparts, and the performance gap narrows when supporting precedents are available, suggesting that reasoning ability can be better assessed when domain knowledge is externally supplied. The benchmark and evaluation code are publicly released at https://github.com/lbox-kr/kcl.

## Method Summary
KCL introduces a novel approach to legal reasoning evaluation by providing question-level supporting precedents alongside test questions, allowing assessment of reasoning capabilities independent of pre-trained domain knowledge. The benchmark comprises two distinct components: KCL-MCQA for multiple-choice questions and KCL-Essay for open-ended generation tasks. KCL-Essay employs a sophisticated automated evaluation system with 2,739 instance-level rubrics designed to evaluate model outputs against expected reasoning patterns. The evaluation methodology involves comparing performance across different model families, including general-purpose and reasoning-specialized models, both with and without access to supporting precedents, to isolate the effects of reasoning capability versus knowledge retrieval.

## Key Results
- KCL-Essay performance ceiling reaches only 74.9% even with precedents provided
- Reasoning-specialized models consistently outperform general-purpose models
- Performance gap narrows when supporting precedents are provided, suggesting better isolation of reasoning ability

## Why This Works (Mechanism)
KCL works by explicitly separating reasoning capability from domain knowledge through the provision of supporting precedents for each question. This design ensures that models cannot rely solely on their pre-trained knowledge and must demonstrate actual reasoning ability to succeed. The automated evaluation system with 2,739 instance-level rubrics provides consistent and scalable assessment of open-ended legal reasoning tasks, capturing nuanced differences in reasoning quality that would be difficult to evaluate manually at scale.

## Foundational Learning
- Korean legal system fundamentals: Why needed - benchmark is built on Korean legal concepts; Quick check - understanding of Korean civil/criminal law distinctions
- Legal reasoning principles: Why needed - to evaluate whether models can apply legal logic correctly; Quick check - ability to distinguish between precedent-based and statutory reasoning
- Multiple-choice question design: Why needed - KCL-MCQA requires understanding of distractor effectiveness; Quick check - ability to identify plausible but incorrect answer choices
- Automated rubric design: Why needed - KCL-Essay relies on 2,739 rubrics for evaluation; Quick check - understanding of how rubrics capture reasoning patterns

## Architecture Onboarding

**Component map:**
KCL-Essay evaluation system -> Model output processing -> Rubric application -> Score aggregation -> Performance analysis

**Critical path:**
Model generates response -> Automated evaluation applies instance-level rubrics -> Scores aggregated across all rubrics -> Performance compared across model families

**Design tradeoffs:**
- Knowledge independence vs. realism: Providing precedents isolates reasoning but may not reflect real-world legal practice
- Automated evaluation vs. human judgment: Scalability achieved but potential loss of nuanced assessment
- Korean specificity vs. generalizability: Deep domain expertise captured but limited cross-jurisdictional applicability

**Failure signatures:**
- High variance in scores across different rubric types indicates inconsistent reasoning
- Performance improvement with precedents suggests knowledge dependency rather than pure reasoning
- Large gaps between general and reasoning-specialized models indicate benchmark successfully discriminates capabilities

**First 3 experiments:**
1. Evaluate a baseline model without any precedents to establish knowledge-dependent performance floor
2. Compare performance across model families (general vs. reasoning-specialized) with identical precedent access
3. Test rubric consistency by evaluating the same responses with different rubric instantiations

## Open Questions the Paper Calls Out
None

## Limitations
- Korean-language focus limits generalizability to other legal systems and languages
- Automated evaluation system remains unvalidated against human expert judgment
- Modest performance ceiling (74.9%) may indicate genuine difficulty or rubric stringency issues

## Confidence
- High confidence in benchmark construction methodology and precedent alignment process
- Medium confidence in the claim that KCL effectively disentangles reasoning from knowledge, pending further validation
- Medium confidence in the assertion that reasoning-specialized models demonstrate superior performance, as this depends on benchmark validity

## Next Checks
1. Conduct human evaluation of KCL-Essay rubric accuracy by having legal experts assess a subset of model outputs against both automated and manual rubrics
2. Test benchmark transferability by translating a subset of KCL problems into other legal domains (e.g., common law) and evaluating cross-jurisdictional reasoning performance
3. Perform ablation studies removing precedents from evaluation to quantify the remaining knowledge-dependent performance gaps across model families