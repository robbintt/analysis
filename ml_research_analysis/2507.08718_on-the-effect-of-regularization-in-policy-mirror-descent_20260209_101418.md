---
ver: rpa2
title: On the Effect of Regularization in Policy Mirror Descent
arxiv_id: '2507.08718'
source_url: https://arxiv.org/abs/2507.08718
tags:
- performance
- policy
- temperature
- regularization
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically analyzes the interplay between Drift
  and MDP regularization in Policy Mirror Descent (PMD), a unifying framework for
  reinforcement learning. Through over 500,000 training seeds on small environments,
  we empirically investigate how different regularization techniques affect algorithmic
  performance and robustness.
---

# On the Effect of Regularization in Policy Mirror Descent

## Quick Facts
- **arXiv ID:** 2507.08718
- **Source URL:** https://arxiv.org/abs/2507.08718
- **Reference count:** 40
- **Primary result:** Over 500,000 training seeds demonstrate that Drift and MDP regularization can partially substitute each other, but their precise combination is critical for robust performance.

## Executive Summary
This study systematically analyzes how Drift and MDP regularization interact in Policy Mirror Descent (PMD), a unifying framework for reinforcement learning. Through extensive empirical investigation across 841 temperature configurations on small environments, the authors demonstrate that regularization types can partially substitute each other in an "L-shaped" performance region, that optimal temperatures scale linearly with reward ranges, and that certain regularizer pairs (particularly max-based Bregman divergences) show superior robustness to temperature tuning. The findings highlight the critical importance of careful regularization design in balancing performance and hyperparameter sensitivity.

## Method Summary
The authors employ off-policy MDPO(h, D) with 10^6 total training steps, 256 environment steps per update, and 512 batch size across four Gymnax environments (CartPole, Acrobot, Catch, DeepSea). They systematically sweep 841 temperature configurations (29 λ values × 29 α values) with 5 seeds each, totaling 504,600 training runs. The study tests multiple regularizer pairs including negative Shannon entropy with reverse KL, Tsallis entropy with KL, and various norm and max-based Bregman divergences. Performance is normalized per environment and robustness is measured as normalized AUC of performance frequency curves.

## Key Results
- Regularization substitution effect: Drift and MDP regularizers can partially substitute each other, creating "L-shaped" performance regions where tuning one becomes less critical when the other is sufficiently strong
- Linear temperature scaling: Optimal regularization temperatures scale linearly with environment reward ranges (R² = 0.85-0.88), with minimum successful temperatures increasing proportionally with maximum return
- Regularizer pair robustness: Max-based Bregman divergences show superior robustness to temperature misspecification compared to standard entropy/KL combinations

## Why This Works (Mechanism)

### Mechanism 1: Regularization Substitution Effect
Both regularizers constrain policy optimization space—MDP regularizers reshape the optimization landscape while Drift regularizers constrain policy traversal. When one is strong, the other becomes less critical, creating viable "L-shaped" regions. This substitution holds because both mechanisms ultimately bound policy deviation from greedy behavior through different mathematical objects. The effect fails when both regularizers are weak (unstable learning) or both too strong (over-constraining exploration).

### Mechanism 2: Linear Temperature-Reward Scaling
Regularization terms are added to reward-scaled quantities in the PMD objective. Larger reward magnitudes require proportionally larger regularization temperatures to maintain the same relative influence on policy updates. This scaling holds because the optimal balance between reward signal and regularization remains constant across reward scales, but breaks under reward normalization or non-linear reward structures.

### Mechanism 3: Regularizer Pair Robustness Heterogeneity
Different regularizer pairs create different loss landscape geometries. Max-based Bregman divergences (non-smooth) may induce flatter optima or more gradual performance degradation near suboptimal temperatures, providing superior resilience compared to smooth alternatives like KL divergence.

## Foundational Learning

- **Mirror Descent Optimization**: PMD is mirror descent applied to policy optimization; understanding Bregman divergences and potential functions is prerequisite. Quick check: Can you explain why KL divergence is the Bregman divergence induced by negative entropy?
- **Policy Gradient with Trust Regions**: Drift regularization generalizes TRPO/PPO-style trust regions; understanding these algorithms provides intuition for λ's role. Quick check: What happens to policy stability when λ → 0 vs λ → ∞?
- **Entropy-Regularized RL (Soft RL)**: MDP regularization with negative Shannon entropy is the most common choice; connects to SAC and maximum entropy RL. Quick check: How does entropy regularization affect exploration vs exploitation tradeoffs?

## Architecture Onboarding

- **Component map:**
```
MDPO(h, D) Algorithm
├── Policy Network (πθ) - outputs action distributions
├── Q-Networks (Qϕ1, Qϕ2) - value function approximation
├── Target Networks - stabilized bootstrap targets
├── MDP Regularizer h - entropy, Tsallis, norm, or max function
├── Drift Regularizer D - Bregman divergence or KL
└── Temperature Controllers - α (MDP) and λ (Drift)
```

- **Critical path:**
1. Implement off-policy MDPO skeleton (Algorithm 1, lines 1-28)
2. Add regularizer functions (Appendix A for mathematical definitions)
3. Integrate temperature parameters into loss (Equation 7)
4. Validate on single environment before scaling

- **Design tradeoffs:**
- Constant vs adaptive temperatures: Paper shows constant temperatures outperform adaptive strategies for most regularizer pairs—simpler implementation, less tuning, but may require more careful initial selection.
- Regularizer pair selection: Max-based divergences offer highest robustness but non-smooth; KL/Entropy is standard but brittle to temperature misspecification.
- On-policy vs off-policy: Paper uses off-policy MDPO; on-policy may have different optimal temperature regimes.

- **Failure signatures:**
- "L-shaped" performance failure: If tuning shows performance only when one regularizer dominates, you're likely in a brittle configuration—try different (h, D) pairs.
- Scaling mismatch: If algorithm works on one reward scale but fails on another, temperature-reward scaling was not respected.
- Annealing degradation: If linear λ annealing hurts performance (contrary to Tomar et al. 2020), verify MDP regularizer is present.

- **First 3 experiments:**
1. Replicate baseline heat map (Figure 1a) on your environment: sweep α ∈ {0.001, 0.01, 0.1, 1.0} and λ ∈ {0.001, 0.1, 1.0, 10.0} to identify viable temperature region.
2. Test temperature scaling: Multiply environment rewards by factor k ∈ {0.1, 1, 10}, verify optimal temperatures scale by same factor (Figure 4 replication).
3. Compare two regularizer pairs: Test MDPO(-H, DKL) vs MDPO(-H, Bmax) with same temperature grid to quantify robustness differences for your task.

## Open Questions the Paper Calls Out

- **Do regularization dynamics persist in high-dimensional continuous control tasks and over longer training horizons?** The authors note their work was restricted to small environments to isolate effects, leaving validity of "L-shaped" substitution and linear scaling in complex domains untested.

- **Are existing temperature adaptation strategies overly specialized for Shannon Entropy and KL divergence?** Standard adaptive methods performed worse than constant temperatures for certain regularizer pairs, suggesting adaptation mechanisms may be brittle or specific to standard entropy setup.

- **What theoretical properties explain max-based Bregman divergence's superior robustness?** The paper provides empirical evidence of max function's robustness but lacks theoretical justification for why non-smooth regularizers provide better resilience than smooth alternatives.

## Limitations

- Unknown neural network architectures and optimizer configurations that could affect regularization behavior
- Study focuses exclusively on small discrete-action environments where regularization effects may differ from deep RL settings
- Robustness claims for non-smooth max-based Bregman divergences haven't been validated in continuous control domains

## Confidence

- **Empirical methodology**: High - Extensive hyperparameter sweeps with 504,600 total seeds provide strong statistical support
- **Theoretical mechanisms**: Medium - Empirical evidence is strong but theoretical explanations for regularization substitution and robustness differences are limited
- **Generalizability**: Low - Results are demonstrated only on small discrete-action environments, limiting applicability to practical deep RL systems

## Next Checks

1. Replicate the heat map structure on a held-out environment (e.g., LunarLander) with the same temperature grids to verify "L-shaped" performance regions generalize beyond CartPole/Acrobot.
2. Test the linear temperature-reward scaling hypothesis with environments of varying difficulty but similar reward scales to isolate scaling effect from task complexity.
3. Compare MDPO with max-based Bregman divergence against standard entropy-regularized SAC on continuous control benchmarks to assess whether robustness advantages transfer to practical deep RL applications.