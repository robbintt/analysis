---
ver: rpa2
title: An Adaptive Method Stabilizing Activations for Enhanced Generalization
arxiv_id: '2506.08353'
source_url: https://arxiv.org/abs/2506.08353
tags:
- adaact
- activation
- generalization
- learning
- adaptive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes AdaAct, an optimization algorithm that adapts
  learning rates based on activation variance to stabilize neuron outputs and improve
  generalization. The method scales gradient updates inversely proportional to the
  square root of activation variance, taking smaller steps with high variance and
  larger steps with low variance.
---

# An Adaptive Method Stabilizing Activations for Enhanced Generalization

## Quick Facts
- arXiv ID: 2506.08353
- Source URL: https://arxiv.org/abs/2506.08353
- Reference count: 40
- Primary result: AdaAct achieves 92.49% accuracy on CIFAR-10/ResNet-20, outperforming Adam (91.37%) while maintaining faster convergence than second-order methods

## Executive Summary
AdaAct is an optimization algorithm that adapts learning rates based on activation variance rather than gradient variance, stabilizing neuron outputs and improving generalization. The method scales gradient updates inversely proportional to the square root of activation variance, taking smaller steps with high variance and larger steps with low variance. Experiments show AdaAct achieves strong generalization performance comparable to SGD and KFAC while maintaining the fast convergence speed of Adam, outperforming other adaptive methods on vision tasks.

## Method Summary
AdaAct adapts learning rates based on per-neuron activation variance computed from minibatch statistics. For each layer, it computes the diagonal of the activation covariance matrix, maintains an exponential moving average (EMA) with β₂=0.999, and scales gradients by the inverse square root of this variance. The method uses decoupled weight decay (λ=2×10⁻³) and works with higher learning rates (η≈0.1) than Adam. The diagonal approximation reduces memory overhead compared to full covariance methods while maintaining competitive performance.

## Key Results
- ResNet-20 on CIFAR-10: AdaAct achieves 92.49% vs Adam's 91.37% and SGD's 92.63%
- CIFAR-100: AdaAct reaches 71.85% vs Adam's 69.71% and SGD's 71.48%
- ViT-S on ImageNet: AdaAct achieves 73.8% vs AdamW's 78.9%, though slower than AdamW
- Converges faster than second-order methods (FOOF, KFAC) while maintaining comparable accuracy

## Why This Works (Mechanism)

### Mechanism 1: Activation Variance-Based Learning Rate Scaling
AdaAct scales gradient updates inversely proportional to the square root of activation variance, stabilizing neuron outputs. Lower activation variance indicates more stable, generalizable representations. The square root scaling (p=0.5) is optimal for balancing adaptation strength versus stability, as shown by 92.5% accuracy versus 91.83% with full covariance and 88.59% with p=1.

### Mechanism 2: Neuron-Wise Versus Parameter-Wise Adaptivity
Neuron-wise adaptive learning rates provide a middle ground between SGD's global rate and Adam's parameter-wise rates. Parameters receiving the same input share one adaptive scaling factor, reducing over-adaptation that harms generalization. This reduces independent learning rates from O(parameters) to O(neurons).

### Mechanism 3: Diagonal Approximation of Activation Covariance
Approximating full activation covariance with diagonal elements achieves comparable performance while reducing memory and computation costs. The diagonal entries dominate the covariance structure for ReLU networks, avoiding O(m³) matrix inversion while maintaining accuracy.

## Foundational Learning

- **Exponential Moving Average (EMA) for variance estimation**: AdaTrack uses EMA (β₂=0.999) to smooth per-minibatch activation variance estimates, preventing noisy updates. Quick check: Why does β₂=0.999 produce more stable variance estimates than β₂=0.9, and what tradeoff does this create for early training iterations?

- **Gradient preconditioning versus activation-based preconditioning**: Adam scales by gradient history, AdaAct by activation history. Quick check: If gradient magnitudes are large but activation variance is low, would AdaAct take larger or smaller steps than Adam, and why might this matter for generalization?

- **Generalization gap in adaptive methods**: AdaAct addresses Adam's tendency toward faster convergence but worse test accuracy versus SGD. Quick check: What empirical observations in Wilson et al. (2017) and Zhou et al. (2020) motivated the search for methods bridging Adam-SGD generalization gaps?

## Architecture Onboarding

- **Component map**: Forward pass → Compute activations a^(ℓ) per layer → Extract diagonal variance E[ã^(ℓ-1)(ã^(ℓ-1))^T] per minibatch → Update EMA: V_t = β₂V_{t-1} + (1-β₂)diag(activation covariance) → Backward pass → Compute gradients G_t → Update gradient EMA: M_t = β₁M_{t-1} + (1-β₁)G_t → Scale: Ĝ_t = M̂_t / (√V̂_t + ε) → Parameter update → θ_t = θ_{t-1} - η_t(ĝ_t + λθ_{t-1})

- **Critical path**: Activation variance computation per layer → EMA update with bias correction → Gradient scaling by inverse sqrt of variance → Decoupled weight decay application

- **Design tradeoffs**: Learning rate η≈0.1 (comparable to SGD), weight decay λ<0.01 (lower than AdamW's 0.01), memory overhead of one additional EMA buffer per layer (neuron count, not parameter count)

- **Failure signatures**: NaN/Inf during early training (check ε numerical stability), slower convergence than Adam (learning rate may be too low), worse generalization than SGD (weight decay too low), no improvement over Adam (verify variance EMA computation)

- **First 3 experiments**: 1) Replicate LeNet-5/CIFAR10 baseline to track activation variance curves across layers for AdaAct, SGD, and Adam, 2) Learning rate sweep on ResNet-20/CIFAR10 testing η∈{0.01, 0.05, 0.1, 0.5} with cosine decay, 3) Ablation on exponent p testing p∈{0.25, 0.5, 0.75, 1.0} to replicate Figure 3 findings

## Open Questions the Paper Calls Out

- **Open Question 1**: Can AdaAct's generalization performance on Vision Transformers (ViT) be improved to match or exceed AdamW? The paper notes AdaAct (73.8%) significantly trails AdamW (78.9%) on ViT-S, attributing the difficulty to ViT's "sharper loss landscape."

- **Open Question 2**: Is the assumption of bounded activation variances (Assumption A4) strictly necessary for the convergence guarantee? The convergence proof relies on this assumption, which the authors note is based on "empirical observation" rather than structural guarantee.

- **Open Question 3**: Can AdaAct effectively replace Batch Normalization (BN) in deep networks without significant performance degradation? While AdaAct outperforms SGD without BN, there remains a large performance gap compared to using BN (81.64% vs 92.49% on CIFAR-10).

## Limitations

- The theoretical generalization bound relies on uniform stability analysis but doesn't account for adaptive method-specific phenomena that could affect constant terms
- Diagonal approximation assumption may not hold for architectures with highly correlated feature maps, particularly in transformer attention mechanisms
- Limited ablation studies on different neural network architectures leave uncertainty about method effectiveness on recurrent or graph neural networks

## Confidence

- **High Confidence**: AdaAct achieves faster convergence than second-order methods while maintaining competitive generalization performance
- **Medium Confidence**: The neuron-wise adaptation mechanism provides meaningful generalization benefits over parameter-wise adaptation
- **Medium Confidence**: Diagonal covariance approximation performs comparably to full covariance

## Next Checks

1. **Cross-Architecture Variance Stability Test**: Apply AdaAct to recurrent networks (LSTM/GRU) and graph neural networks, measuring activation variance stability curves across layers and training epochs

2. **Covariance Structure Analysis**: Compute full activation covariance matrices for transformer attention layers and convolutional layers with large receptive fields, comparing diagonal versus full approximation accuracy on synthetic data

3. **Generalization Gap Scaling**: Systematically vary model capacity and dataset size on CIFAR-10, measuring AdaAct's generalization advantage over Adam relative to model/dataset scale