---
ver: rpa2
title: 'Kvasir-VQA-x1: A Multimodal Dataset for Medical Reasoning and Robust MedVQA
  in Gastrointestinal Endoscopy'
arxiv_id: '2506.09958'
source_url: https://arxiv.org/abs/2506.09958
tags:
- dataset
- question
- clinical
- medical
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Kvasir-VQA-x1, a large-scale dataset for
  medical visual question answering in gastrointestinal endoscopy. The dataset expands
  upon the original Kvasir-VQA by incorporating 159,549 new question-answer pairs,
  designed to test deeper clinical reasoning through structured question merging and
  complexity stratification.
---

# Kvasir-VQA-x1: A Multimodal Dataset for Medical Reasoning and Robust MedVQA in Gastrointestinal Endoscopy

## Quick Facts
- **arXiv ID:** 2506.09958
- **Source URL:** https://arxiv.org/abs/2506.09958
- **Reference count:** 40
- **Primary result:** 159,549 new QA pairs for GI endoscopy VQA with complexity stratification and visual augmentations

## Executive Summary
Kvasir-VQA-x1 expands the original Kvasir-VQA dataset with 159,549 new question-answer pairs specifically designed to test deeper clinical reasoning in gastrointestinal endoscopy. The dataset introduces complexity stratification through question merging, where multiple atomic QA pairs are combined into single complex questions requiring multi-step reasoning. Visual augmentations mimicking real-world imaging artifacts are included to evaluate model robustness. The dataset is structured into two evaluation tracks—normal and transformed—and demonstrates that fine-tuned models like MedGemma-ft and Qwen2.5-VL-ft achieve 87-90% accuracy on complex medical reasoning tasks.

## Method Summary
The dataset combines 6,500 endoscopy images with 159,549 QA pairs stratified by complexity (1-3). Questions are synthesized by merging atomic QA pairs via an LLM, creating prompts requiring integration of multiple clinical cues. LoRA-based fine-tuning with frozen vision encoders adapts general VLMs (MedGemma-4B, Qwen2.5-VL-7B) to the medical domain. RandomResizedCrop, RandomRotation, RandomAffine, and ColorJitter augmentations simulate clinical imaging variations. Models are evaluated using dual-track testing (normal/transformed) with 8 NLP metrics plus LLM adjudication via Qwen3-30B-A3B.

## Key Results
- Fine-tuned MedGemma and Qwen2.5-VL achieved mean accuracies of 87% and 90% respectively
- Models trained with visual augmentations retained or slightly improved performance on clean inputs (Δ < 0.002 ROUGE-L)
- Question merging successfully stratified reasoning complexity, with Level 2 questions showing higher accuracy than Level 1 in some categories
- Polyp size estimation remained challenging at ~33% accuracy even after fine-tuning

## Why This Works (Mechanism)

### Mechanism 1: Question Merging for Multi-Step Reasoning
Merging multiple atomic QA pairs into single complex questions induces multi-step reasoning in VLMs. Questions are synthesized by combining 1-3 atomic QA pairs via an LLM, creating prompts requiring integration of multiple clinical cues. This compositional inference pattern is transferable to clinical scenarios requiring multi-hop reasoning.

### Mechanism 2: LoRA Fine-Tuning with Frozen Vision Encoders
Low-rank adaptation matrices are injected into projection layers while keeping vision backbone frozen. This preserves pre-trained visual representations while adapting cross-modal alignment to medical terminology. The approach efficiently adapts general VLMs to GI endoscopy domain.

### Mechanism 3: Weak Visual Augmentations for Robustness
RandomResizedCrop, RandomRotation, RandomAffine, and ColorJitter simulate common endoscopy variations. Models trained on transformed images maintain stable metrics across normal and perturbed validation sets, improving robustness to real-world imaging artifacts without degrading clean-image performance.

## Foundational Learning

- **Vision-Language Model Architecture**
  - Why needed here: Understanding visual feature tokenization and language embedding alignment is prerequisite for debugging cross-modal failures
  - Quick check question: Can you explain how Qwen2.5-VL's dynamic resolution handling differs from MedGemma's fixed-size SigLIP input, and why this might affect performance on heterogeneous endoscopy images?

- **Parameter-Efficient Fine-Tuning (LoRA)**
  - Why needed here: All experiments use LoRA; understanding rank/alpha tradeoffs is essential for reproducing and extending results
  - Quick check question: Given LoRA rank=16, alpha=64 used in this paper, what happens if you increase rank to 32—would this improve performance on underperforming categories like polyp size estimation, or primarily increase overfitting risk?

- **Medical VQA Evaluation Metrics**
  - Why needed here: The paper uses 8 metrics plus LLM adjudication; understanding their limitations prevents misinterpreting results
  - Quick check question: Why might BLEURT and BERT-F1 scores (0.94+) appear high while accuracy on specific clinical categories (e.g., polyp size: 33%) remains low?

## Architecture Onboarding

- **Component map:** 6,500 endoscopy images → 159,549 QA pairs (complexity 1-3) → train/test splits → Albumentations-based augmentations → Base VLM + LoRA adapters → DeepSpeed ZeRO-2 training → Dual-track evaluation → LLM adjudicator

- **Critical path:** Load base VLM checkpoint → attach LoRA adapters to projection layers → preprocess images with augmentation scripts (for transformed track) → fine-tune 3-4 epochs with instruction prompt → evaluate on both normal and transformed test sets → run LLM adjudicator for categorical/complexity breakdown

- **Design tradeoffs:** Qwen2.5-VL offers better spatial reasoning but is larger (7B vs 4B); MedGemma is smaller with medical pre-training but fixed resolution limits detail; augmentation improves robustness but adds ~4h training time per epoch; LLM adjudicator enables nuanced evaluation but introduces homogeneity bias

- **Failure signatures:** Polyp size estimation remains ~33% regardless of fine-tuning (vision encoder lacks metric/depth cues); abnormality location: 40-47% accuracy (spatial grounding failure); Level 3 complexity drops (error accumulation in multi-hop reasoning); homogeneity bias in evaluation (Qwen-based adjudicator may favor Qwen models)

- **First 3 experiments:** 1) Reproduce baseline: Fine-tune MedGemma with exact hyperparameters on normal images, target 87% accuracy. 2) Ablate augmentation: Compare Qwen2.5-VL-ft (normal) vs Qwen2.5-VL-ft-Trans (augmented) on transformed test set, confirm Δ < 0.01 metric gap. 3) Probe failure modes: Evaluate fine-tuned models on Level 3 questions by category, generate confusion matrices to identify vision vs reasoning errors.

## Open Questions the Paper Calls Out

- **Question 1:** Would ensemble adjudication using architecturally distinct LLMs (e.g., Claude, Gemini, Llama) significantly alter model ranking compared to the Qwen-based adjudicator? Unresolved because only Qwen3-30B-A3B was used; evidence would require cross-validation with heterogeneous evaluators.

- **Question 2:** Can auxiliary supervision tasks (bounding box prediction, segmentation masks) measurably improve performance on fine-grained clinical assessments like polyp size and abnormality location? Unresolved because current study uses only LoRA fine-tuning without auxiliary spatial supervision; evidence would require training with added spatial heads.

- **Question 3:** Does curriculum learning starting with Level 2 questions before Level 1 and Level 3 improve overall performance compared to random ordering? Unresolved because all models trained without curriculum-based sample ordering; evidence would require controlled experiments comparing curriculum-based training against baseline.

## Limitations

- Question merging benefits for multi-step reasoning remain theoretical without ablation studies isolating contribution versus model capacity
- Vision encoder freezing may cap performance on spatially demanding tasks (polyp size estimation at 33% accuracy)
- LLM-based evaluation introduces potential homogeneity bias given same architecture family serves as both model and adjudicator

## Confidence

- **High Confidence:** Basic performance metrics (87-90% accuracy), LoRA fine-tuning effectiveness, dataset size and complexity stratification
- **Medium Confidence:** Question merging benefits for multi-step reasoning, augmentation robustness claims (limited artifact diversity validation)
- **Low Confidence:** Generalizability of performance gains to clinical deployment, sufficiency of frozen vision encoders for fine-grained tasks

## Next Checks

1. **Ablation Study Design:** Compare performance of models trained on atomic QA pairs versus merged questions while controlling for total training examples, to isolate the true benefit of complexity stratification.

2. **Vision Encoder Capacity Test:** Fine-tune vision encoders (not just LoRA adapters) on polyp size estimation subset to determine if performance ceiling is architectural rather than optimization-limited.

3. **Adjudicator Bias Analysis:** Evaluate the same models using a different LLM adjudicator (e.g., GPT-4, Claude) to quantify potential homogeneity bias in the reported accuracy metrics.