---
ver: rpa2
title: Automatically assessing oral narratives of Afrikaans and isiXhosa children
arxiv_id: '2507.13205'
source_url: https://arxiv.org/abs/2507.13205
tags:
- children
- isixhosa
- system
- linear
- afrikaans
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We develop an automated system for assessing oral narratives of
  preschool children in Afrikaans and isiXhosa. The system uses automatic speech recognition
  followed by either a linear model or a large language model (LLM) to predict narrative
  and comprehension scores.
---

# Automatically assessing oral narratives of Afrikaans and isiXhosa children

## Quick Facts
- arXiv ID: 2507.13205
- Source URL: https://arxiv.org/abs/2507.13205
- Reference count: 0
- Primary result: Automated assessment of oral narratives achieves >80% accuracy for identifying intervention needs in Afrikaans and 64% in isiXhosa

## Executive Summary
This study develops an automated system for assessing oral narratives of preschool children in Afrikaans and isiXhosa using automatic speech recognition followed by either linear models or large language models (LLMs). The system aims to identify children who may require additional support in language development. Despite high ASR error rates, the LLM-based approach outperforms traditional linear models in most cases, particularly for identifying Afrikaans children needing intervention. The research demonstrates the potential of automated assessment tools to assist teachers in low-resource language settings, though results show language-specific variations in performance.

## Method Summary
The system processes children's oral narratives through automatic speech recognition to generate transcripts, which are then analyzed using either a linear model or an LLM to predict narrative and comprehension scores. The LLM approach is evaluated against a human expert baseline, with additional experiments involving translation of ASR transcripts to English to improve LLM performance. The study compares binary classification accuracy for identifying intervention needs between Afrikaans and isiXhosa, using relatively small sample sizes (287 Afrikaans and 70 isiXhosa narratives) collected from preschool settings.

## Key Results
- LLM-based system achieves >80% accuracy for identifying Afrikaans children requiring intervention
- LLM performance is comparable to human expert assessment despite high ASR error rates
- Translation of ASR transcripts to English improves LLM performance for both languages
- Linear model provides interpretable insights into linguistic patterns associated with intervention needs

## Why This Works (Mechanism)
The LLM approach leverages contextual understanding and semantic interpretation capabilities to overcome ASR errors and language-specific challenges in low-resource settings. The translation to English provides access to more sophisticated language models trained on larger datasets. The linear model's interpretability allows identification of specific linguistic features that correlate with intervention needs.

## Foundational Learning
- Automatic Speech Recognition (ASR): Converts spoken language to text; needed to process oral narratives, quick check: error rate metrics
- Linear Regression Models: Statistical approach for prediction using weighted features; needed for interpretable baseline, quick check: feature importance scores
- Large Language Models (LLMs): Deep learning models for natural language understanding; needed for contextual semantic analysis, quick check: perplexity scores
- Bilingual Evaluation Understudy (BLEU): Metric for evaluating machine translation quality; needed to assess translation impact, quick check: BLEU score comparison
- Cross-validation: Technique for assessing model generalization; needed to validate performance estimates, quick check: variance in fold results

## Architecture Onboarding
Component map: Speech Recognition -> Transcript Generation -> Linear Model / LLM Processing -> Score Prediction

Critical path: ASR transcription accuracy directly impacts downstream model performance, particularly for the linear model approach. LLM performance shows more resilience to ASR errors due to contextual understanding capabilities.

Design tradeoffs: The study balances between model complexity (LLM vs linear) and interpretability, while addressing the challenge of low-resource languages through translation to English. Sample size limitations necessitated binary classification rather than fine-grained scoring.

Failure signatures: High ASR error rates (29.8% Afrikaans, 36.7% isiXhosa) can propagate through the system, though LLM's contextual understanding mitigates this. Small sample sizes for isiXhosa limit generalizability and may inflate performance estimates.

First experiments: 1) Compare ASR accuracy across different acoustic conditions, 2) Evaluate feature importance from linear model for linguistic pattern identification, 3) Test LLM performance with human-corrected transcripts to isolate ASR error impact.

## Open Questions the Paper Calls Out
None

## Limitations
- High ASR error rates (29.8% for Afrikaans, 36.7% for isiXhosa) may affect assessment reliability
- Small sample sizes (287 Afrikaans, 70 isiXhosa) limit generalizability, particularly for isiXhosa
- Focus on binary classification rather than fine-grained score prediction may overestimate real-world utility

## Confidence
High confidence: LLM outperforms linear models for Afrikaans intervention classification, translation to English improves LLM performance, linear model interpretability for linguistic features.

Medium confidence: isiXhosa LLM performance (64% accuracy), comparability to human expert assessment, utility in low-resource educational settings.

Low confidence: Generalization to other languages/contexts, long-term classroom reliability, impact of ASR errors under real-world conditions.

## Next Checks
1. Conduct cross-validation with held-out test sets to verify LLM performance is not inflated by in-domain training data.
2. Evaluate system robustness with diverse narrative prompts and contexts beyond the specific stories used.
3. Implement longitudinal classroom study measuring agreement rates between automated assessments and teacher judgment over extended periods.