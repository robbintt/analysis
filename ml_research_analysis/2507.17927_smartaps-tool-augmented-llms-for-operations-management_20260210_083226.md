---
ver: rpa2
title: 'SMARTAPS: Tool-augmented LLMs for Operations Management'
arxiv_id: '2507.17927'
source_url: https://arxiv.org/abs/2507.17927
tags:
- tool
- user
- conversation
- operations
- smartaps
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SMARTAPS addresses the high cost and complexity of using traditional
  Advanced Planning Systems (APS) by introducing a conversational interface powered
  by tool-augmented LLMs. The system allows operations planners to interact with APS
  using natural language for tasks like querying plans, counterfactual reasoning,
  and scenario analysis.
---

# SMARTAPS: Tool-augmented LLMs for Operations Management

## Quick Facts
- arXiv ID: 2507.17927
- Source URL: https://arxiv.org/abs/2507.17927
- Authors: Timothy Tin Long Yu; Mahdi Mostajabdaveh; Jabo Serge Byusa; Rindra Ramamonjison; Giuseppe Carenini; Kun Mao; Zirui Zhou; Yong Zhang
- Reference count: 25
- One-line primary result: Conversational interface reduces APS analysis time from 1-2 days to hours with 87% tool retrieval accuracy

## Executive Summary
SMARTAPS introduces a conversational interface for Advanced Planning Systems (APS) using tool-augmented LLMs, enabling operations planners to query plans, perform counterfactual reasoning, and analyze scenarios using natural language. The system reduces reliance on OR consultants and lowers implementation costs while maintaining sophisticated operational analysis capabilities. Tested in a production planning scenario, it demonstrates significant time savings while achieving accurate tool selection through semantic similarity matching.

## Method Summary
SMARTAPS implements a three-module architecture where user queries first pass through a conversation manager for intent detection (CASUAL_CONVERSATION vs OPERATIONS_PLANNING), then a tool retriever using semantic similarity matching via BGE-LARGE-EN-V1.5 embeddings to select relevant APIs from a curated catalog, and finally a tool manager that extracts parameters and executes the selected APIs. The system uses structured tool contracts specifying descriptions, examples, input/output schemas, and function calls, with the conversation manager refining responses based on tool execution results.

## Key Results
- Reduced analysis time from 1-2 days to a few hours in production planning scenarios
- Achieved 87% tool retrieval accuracy on 150 test queries using semantic similarity matching
- Successfully handled query plan, why-not, what-if, compare plan, and display plan operations
- Maintained accuracy while using lightweight 7B-parameter models for cost efficiency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Semantic similarity between user queries and tool descriptions enables accurate API retrieval without fine-tuning.
- Mechanism: Embedding-based retrieval maps natural language queries to pre-encoded tool contracts using BGE-LARGE-EN-V1.5, selecting tools via minimum L2 distance in vector space.
- Core assumption: Users phrase requests similarly to tool example queries; the embedding model captures functional similarity across paraphrases.
- Evidence anchors: [Section 4.2] describes semantic similarity calculation; [Table 1] reports 87% accuracy on 150-query test set; OR-Toolformer (arXiv:2510.01253) validates similar patterns for OR tasks.

### Mechanism 2
- Claim: Structured API contracts decouple tool execution from natural language understanding, enabling reliable parameter extraction.
- Mechanism: Each tool has a contract specifying description, examples, input schema, output schema, and function call. The tool manager prompts an LLM to extract parameters from conversation context, then executes the resolved API.
- Core assumption: Required parameters are inferable from conversation history or query; missing parameters trigger clarification rather than silent failure.
- Evidence anchors: [Section 3.2] shows tool contract contents; [Section 4.3] describes parameter extraction flow; AgentMath (arXiv:2512.20745) validates tool-augmented agent architecture in different domain.

### Mechanism 3
- Claim: Intent classification gates tool retrieval, preventing unnecessary API calls for casual conversation.
- Mechanism: The conversation manager classifies user input as CASUAL_CONVERSATION or OPERATIONS_PLANNING via LLM prompt, routing only the latter to tool retrieval.
- Core assumption: A 7B-parameter model can reliably distinguish planning queries from chat; misclassification costs are asymmetric (false tool calls more disruptive than missed tool opportunities).
- Evidence anchors: [Section 4.1] defines intent detection; [Figure 4] shows prompt template; no direct corpus evidence on classification accuracy in this architecture.

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: Tool retrieval is implemented as RAG over API contracts rather than documents; understanding embedding-based search is prerequisite to debugging retrieval failures.
  - Quick check question: Can you explain why L2 distance in embedding space might fail for semantically similar but lexically different queries?

- Concept: Tool-augmented / Agentic LLMs
  - Why needed here: The core architecture treats APIs as tools; the LLM is an orchestrator, not a knowledge source. Understanding tool-calling patterns is essential for extending the API catalog.
  - Quick check question: What happens if a tool returns an error—how should the conversation manager handle it?

- Concept: Prompt engineering for structured extraction
  - Why needed here: Both intent detection and parameter extraction rely on carefully designed prompts; small prompt changes can cascade into system-level failures.
  - Quick check question: How would you modify the intent detection prompt to handle a new intent type (e.g., FEEDBACK)?

## Architecture Onboarding

- Component map: User Query → [Conversation Manager] → Intent Classification → [Tool Retriever] → Semantic Similarity → [Tool Manager] → Parameter Extraction → Tool Execution → [Conversation Manager] → Response Refinement → User

- Critical path: Query → Intent classification → Embedding → Retrieval → Parameter extraction → Tool execution → Response refinement. The retrieval step (87% accuracy) is the bottleneck; failures here cascade to wrong tool execution or missing capabilities.

- Design tradeoffs:
  - Lightweight LLM (Mistral-7B) vs. larger models: Lower latency and cost, but may struggle with complex multi-turn parameter resolution.
  - Pre-defined API catalog vs. dynamic tool generation: Current system requires OR consultants to author tools; paper notes this as a limitation.
  - Single-user assumption vs. multi-planner scenarios: Current architecture doesn't handle concurrent users with conflicting objectives.

- Failure signatures:
  - Retrieval returns wrong tool → parameters fail to extract → clarification loop or wrong answer
  - Missing parameter not inferrable → conversation manager asks user → potential frustration if frequent
  - Optimization solver timeout → no graceful handling in current architecture (noted in limitations)
  - Ambiguous model/data reference in multi-model scenarios → tool manager selects wrong context

- First 3 experiments:
  1. Reproduce retrieval accuracy baseline: Encode the tool contracts, create a held-out query set, measure top-1 and top-3 retrieval accuracy. Vary embedding model to assess sensitivity.
  2. Intent classification stress test: Submit edge-case queries (planning questions phrased casually, casual questions with planning keywords) and measure confusion matrix.
  3. Parameter extraction robustness: Design queries with progressively more implicit parameter references across conversation turns; measure extraction success rate vs. turn distance.

## Open Questions the Paper Calls Out
None

## Limitations
- Tool catalog requires manual curation by OR consultants, creating scalability bottleneck
- No quantitative validation of analysis time reduction claims (1-2 days to hours is anecdotal)
- Single-user assumption doesn't address concurrent planners with conflicting objectives
- No graceful handling for optimization solver timeouts or extended computation

## Confidence
- **High Confidence**: The three-module architecture is clearly specified and represents a sound approach to tool-augmented LLMs.
- **Medium Confidence**: The 87% retrieval accuracy figure is reported but lacks methodological details for independent verification.
- **Low Confidence**: Claims about analysis time reduction are anecdotal with no supporting metrics or user study methodology.

## Next Checks
1. **Retrieval Accuracy Reproduction**: Encode tool contracts using BGE-LARGE-EN-V1.5, create held-out query set with ground-truth labels, measure top-1 and top-3 retrieval accuracy, systematically vary embedding model and distance metric.

2. **Multi-turn Parameter Extraction Robustness**: Design systematic test suite with queries requiring parameters from increasingly distant conversation turns (1-turn to 5-turn reference), measure extraction success rate versus turn distance.

3. **Production Deployment Stress Test**: Deploy with minimal tool catalog (5-10 APIs) in controlled environment, measure response latency, tool execution success rate, conversation manager clarification frequency, track percentage requiring manual intervention.