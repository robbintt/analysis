---
ver: rpa2
title: Uncertain Knowledge Graph Completion via Semi-Supervised Confidence Distribution
  Learning
arxiv_id: '2510.16601'
source_url: https://arxiv.org/abs/2510.16601
tags:
- confidence
- data
- learning
- labeled
- sscdl
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of uncertain knowledge graph (UKG)
  completion, where triples have associated confidence scores, and the confidence
  distributions are extremely imbalanced. Existing methods struggle because they don't
  effectively learn from low-confidence triples, which are crucial for improving model
  performance.
---

# Uncertain Knowledge Graph Completion via Semi-Supervised Confidence Distribution Learning

## Quick Facts
- **arXiv ID:** 2510.16601
- **Source URL:** https://arxiv.org/abs/2510.16601
- **Reference count:** 32
- **Primary result:** Proposed ssCDL method achieves MSE reduction up to 63.8% and MAE reduction up to 53.2% on UKG completion tasks compared to state-of-the-art baselines.

## Executive Summary
This paper addresses the challenge of uncertain knowledge graph (UKG) completion where triples have associated confidence scores and the confidence distributions are extremely imbalanced. The authors propose a semi-supervised confidence distribution learning (ssCDL) method that transforms confidence scores into distributions and employs meta self-training with pseudo-labels to improve learning from under-represented confidence regions. The method consists of a CDL-based relational learner and a pseudo confidence distribution generator, iteratively trained together to enhance both confidence prediction and link prediction performance on real-world UKG datasets.

## Method Summary
ssCDL addresses UKG completion by transforming scalar confidence scores into probability distributions using Gaussian smoothing, then applying meta self-training with a CDL-based relational learner and pseudo confidence distribution generator. The CDL-RL component learns entity/relation embeddings using both labeled data and pseudo-labeled data, optimizing confidence prediction (via KL divergence + MSE) and link prediction (via margin-based ranking) simultaneously with uncertainty-based loss weighting. The PCDG generates pseudo confidence distributions for unlabeled triples via negative sampling and is meta-optimized to minimize CDL-RL's loss on held-out labeled data. Training proceeds in phases: initial warm-up training CDL-RL only, then meta-optimizing PCDG, and finally incorporating high-quality pseudo-labels into CDL-RL training.

## Key Results
- ssCDL achieves MSE reduction up to 63.8% and MAE reduction up to 53.2% on confidence prediction compared to state-of-the-art baselines
- Consistently outperforms competitors on link prediction tasks (Hits@1, WMRR) across both NL27k and CN15k datasets
- Ablation studies confirm the importance of confidence distribution learning and meta self-training components
- Method shows particular effectiveness on low-confidence triples, which are crucial for improving overall model performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Transforming scalar confidences into distributions captures supervision signals from under-represented confidence regions.
- **Mechanism:** Each confidence value s is converted to a Gaussian distribution N(s, σ²), then discretized into 101 probability bins over [0,1]. A triple with s=0.78 thus contributes training signal to neighboring bins (e.g., 0.76, 0.77, 0.79), mitigating data sparsity in those regions.
- **Core assumption:** Nearby confidence values share useful representational features; the Gaussian spread appropriately captures this relatedness without excessive blurring.
- **Evidence anchors:** [abstract] "each triple confidence is transformed into a confidence distribution to introduce more supervision information of different confidences"; [Section 3.2] "the confidence distribution s is generated by a Gaussian distribution N(s, σ²), where σ is the standard deviation"
- **Break condition:** If σ is too small, no generalization to unseen confidences; if too large, distinctions between different confidences collapse (ablation shows σ=0.6 optimal, performance degrades at extremes).

### Mechanism 2
- **Claim:** Meta-optimized pseudo-label generation produces higher-quality supervision for unlabeled data than standard self-training.
- **Mechanism:** PCDG generates pseudo confidence distributions for negative-sampled (likely low-confidence) triples. CDL-RL is updated on both labeled and pseudo-labeled data; PCDG is then meta-updated to minimize CDL-RL's loss on held-out labeled data. This feedback loop incentivizes PCDG to generate labels that actually improve downstream performance, rather than amplifying its own errors.
- **Core assumption:** The meta-objective (performance on labeled data) is a reliable proxy for pseudo-label quality; CDL-RL provides meaningful gradient signal for PCDG optimization.
- **Evidence anchors:** [abstract] "pseudo labels... are predicted by meta-learning to augment the training data and rebalance the distribution"; [Section 4.3] "PCDG is meta optimized by CDL-RL with labeled data"
- **Break condition:** If initial CDL-RL embeddings are unstable (insufficient pre-training epochs), meta-gradients misguide PCDG; if threshold for pseudo-label selection is too low, noisy labels contaminate training.

### Mechanism 3
- **Claim:** Jointly learning confidence prediction and link prediction with uncertainty-based loss weighting improves embedding quality for both tasks.
- **Mechanism:** CDL-RL optimizes two losses simultaneously—KL divergence + MSE for confidence prediction, margin-based ranking loss for link prediction. Learnable uncertainty parameters (λ_CP, λ_LP) dynamically reweight losses, preventing one task from dominating.
- **Core assumption:** Confidence and structure are mutually informative; multi-task learning regularizes embeddings more effectively than single-task training.
- **Evidence anchors:** [Section 4.2] "we use the idea of uncertainty weights to dynamically adjust the proportion of loss for each task"; [Table 2] ssCDL achieves best results on both tasks
- **Break condition:** If one task's loss magnitude vastly exceeds the other, uncertainty weighting may not compensate; margin hyperparameter γ and MSE weight β require careful tuning.

## Foundational Learning

- **Label Distribution Learning (LDL)**: Core technique enabling confidence distribution transformation; understanding how discrete distributions preserve ambiguity is essential. Quick check: Given a scalar confidence 0.85 and σ=0.6, which bins receive the highest probability mass under N(0.85, 0.36)?

- **Meta-Learning / Bi-Level Optimization**: PCDG meta-update requires computing gradients through CDL-RL's gradient step; failure to understand this leads to implementation errors. Quick check: In Equation 8, what does θ⁺ represent and why is it needed before computing L(η)?

- **Knowledge Graph Embedding Basics**: CDL-RL builds on standard embedding approaches (entity/relation vectors, scoring functions); confusion here propagates through the entire system. Quick check: What is the difference between confidence prediction (s given h,r,t) and link prediction (t given h,r)?

## Architecture Onboarding

- **Component map**: Entity/Relation Embeddings → CDL-RL (FCN → Softmax 101-dim confidence distribution, FCN → Sigmoid scalar rank score) ↔ PCDG (same architecture) → Confidence/Link Predictions

- **Critical path**: 1. Pre-train CDL-RL on labeled data until embeddings stabilize (T_PCDG threshold) 2. Initialize PCDG with CDL-RL weights; begin meta-optimization 3. Once pseudo-label quality sufficient (threshold check), incorporate into CDL-RL training 4. Iterate until convergence

- **Design tradeoffs**: σ (Gaussian spread) controls supervision sharing vs. specificity; w_p (pseudo-label weight) balances labeled vs. pseudo-labeled influence; selection threshold filters noisy pseudo-labels but may discard useful data; embedding size 128 (NL27k) vs. 512 (CN15k) suggests dataset-specific scaling

- **Failure signatures**: MSE plateaus early with high σ → over-smoothing, confidence distinctions lost; link prediction degrades when pseudo-labels added → threshold too low, noisy negatives contaminating ranking loss; PCDG loss oscillates → learning rate too high or insufficient CDL-RL warm-up

- **First 3 experiments**: 1. Ablate CDL (use raw confidences only) on validation set; expect MSE increase ~40-60% (per Table 3) 2. Sweep σ ∈ {0.2, 0.6, 1.0} on small subset; confirm 0.6 gives best MAE before full training 3. Train with T_CDLRL set too early (e.g., epoch 5) vs. recommended; observe pseudo-label selection rate and MSE trajectory to verify warm-up necessity

## Open Questions the Paper Calls Out

- **Question:** Can the proposed confidence distribution learning framework be extended to support rule mining and logical reasoning on uncertain knowledge graphs? **Basis:** [explicit] The Conclusion states, "As for the future work, we plan to study rule learning and reasoning on UKGs based on confidence distribution learning." **Why unresolved:** The current study focuses strictly on embedding learning for link and confidence prediction tasks, rather than extracting interpretable logic rules. **What evidence would resolve it:** A subsequent model that extracts symbolic rules (e.g., transitive paths) from UKGs while utilizing the learned confidence distributions to weight rule validity.

- **Question:** How can large language models (LLMs) be integrated into the ssCDL framework to enhance UKG completion or facilitate reliable retrieval-augmented generation? **Basis:** [explicit] The authors explicitly list plans to "explore to apply large language model to UKG completion, and use UKGs for reliable retrieval-augmented generation" in the conclusion. **Why unresolved:** The current methodology relies solely on structural graph embeddings and meta-learning, without incorporating the semantic priors or generative capabilities of LLMs. **What evidence would resolve it:** A hybrid system where an LLM provides textual supervision or initializes embeddings, demonstrating improved performance over the purely structural ssCDL.

- **Question:** How can new benchmark datasets be constructed to ensure a meaningful distinction in reliability between high-confidence and low-confidence triples? **Basis:** [explicit] The authors note in Section 5.2 and Appendix D that on CN15k, "there is no significant distinction in reliability between high-confidence and low-confidence triples," making it necessary to "build better UKG completion benchmark datasets." **Why unresolved:** Current benchmarks may possess noisy or arbitrary confidence scores that do not accurately reflect truth likelihood, potentially capping the measurable performance of uncertainty-aware models. **What evidence would resolve it:** The creation and adoption of a dataset where confidence scores are rigorously validated against ground truth, showing a strong correlation between low confidence and factual error.

## Limitations

- **Dataset and evaluation protocol specifics:** Key hyperparameters (T_PCDG, T_CDLRL, FCN hidden dimensions, pseudo-label selection thresholds) are either unspecified or require assumptions, limiting reproducibility.
- **Model initialization and warm-up:** The impact of early-stage embedding quality on meta self-training stability is acknowledged but not deeply explored, potentially affecting performance.
- **Selection criterion ambiguity:** The exact meaning of "highest description degree" for pseudo-label selection is not defined, introducing uncertainty in replication.
- **Multi-task loss balance:** Dynamic uncertainty weighting is proposed but the sensitivity of performance to these parameters is not fully characterized.

## Confidence

- **High:** The core mechanisms (confidence distribution transformation, meta self-training with pseudo-labels) are well-described and logically sound. The reported quantitative improvements (MSE reduction up to 63.8%, MAE reduction up to 53.2%) are compelling and methodologically justified.
- **Medium:** The effectiveness of uncertainty-aware multi-task learning and the robustness of the method to hyperparameter choices (σ, w_p, selection threshold) are supported but warrant further empirical validation.
- **Low:** Certain technical details (exact model architecture, threshold definitions) are underspecified, limiting direct reproducibility.

## Next Checks

1. **Hyperparameter sensitivity analysis:** Systematically sweep σ, w_p, and selection threshold on a held-out validation set to identify stable operating ranges and confirm reported performance is not brittle.

2. **Pseudo-label quality audit:** Inspect the confidence distributions and selection rates of pseudo-labeled triples over training epochs; ensure low-confidence regions are appropriately represented and high-confidence pseudo-labels do not dominate.

3. **Cross-dataset generalization:** Apply ssCDL to an additional UKG dataset (e.g., YAGO) or a standard KG with confidence annotations to assess robustness beyond NL27k and CN15k.