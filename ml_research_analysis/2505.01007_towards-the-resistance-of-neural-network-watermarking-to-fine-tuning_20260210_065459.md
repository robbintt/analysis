---
ver: rpa2
title: Towards the Resistance of Neural Network Watermarking to Fine-tuning
arxiv_id: '2505.01007'
source_url: https://arxiv.org/abs/2505.01007
tags:
- frequency
- watermark
- components
- network
- weight
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a neural network watermarking method that is
  robust to fine-tuning, weight scaling, and weight permutations. The key insight
  is that specific frequency components of convolutional filters remain stable during
  training when input features contain only low-frequency components.
---

# Towards the Resistance of Neural Network Watermarking to Fine-tuning

## Quick Facts
- arXiv ID: 2505.01007
- Source URL: https://arxiv.org/abs/2505.01007
- Reference count: 22
- Primary result: Proposes neural network watermarking method robust to fine-tuning, weight scaling, and permutations with 100% detection rates

## Executive Summary
This paper addresses the challenge of neural network watermarking robustness against fine-tuning attacks. The authors propose a method that embeds ownership information into specific frequency components of convolutional filters that remain stable during training when input features contain only low-frequency components. The approach provides theoretical guarantees for robustness to fine-tuning, weight scaling, and weight permutations, and includes a defensive mechanism against overwriting attacks by binding watermark integrity to model utility.

## Method Summary
The method embeds watermarks into specific frequency components of convolutional filters that remain invariant during fine-tuning when input features are low-pass filtered. A parallel watermark module processes input through a low-pass filter followed by convolutional filters. The method uses revised DFT to extract frequency components that exhibit equivariance to weight scaling and permutations. An additional defensive loss term binds watermark integrity to classification performance, causing significant accuracy drops when watermarks are removed.

## Key Results
- 100% watermark detection rate under fine-tuning, weight scaling, and weight permutations
- AlexNet accuracy drops from 90.28% to 43.55% under overwriting attack when defensive loss is applied
- Watermark detection rate remains at 100% across multiple datasets (CIFAR-10/100, Caltech-101/256) and architectures (AlexNet, ResNet-18)

## Why This Works (Mechanism)

### Mechanism 1
The method exploits the stability of specific frequency components during fine-tuning. When input features contain only low-frequency components (enforced via explicit low-pass filtering), the gradient update term for certain frequency components becomes zero. This occurs because the coefficient in the gradient expression contains sin(iπ) terms that evaluate to zero at specific frequencies. The core assumption is that input features to the watermarked layer contain only low-frequency components.

### Mechanism 2
The method leverages equivariance of frequency components to weight scaling and permutations. For scaling, the direction of the complex vector is preserved (F_W*^(uv) = a·F_W^(uv) when W* = a·W). For permutations, permuting D filters permutes the corresponding frequency component vectors in the same order. Detection uses cosine similarity on complex vectors, which is scale-invariant, and matches filters via permutation search.

### Mechanism 3
The method includes a defensive mechanism against overwriting attacks by training with an additional loss term. This loss causes significant accuracy drop when watermark parameters are overwritten, creating a deterrent. The loss binds watermark integrity to model utility by training the network to classify samples into a pseudo category when noise is added to watermark module weights.

## Foundational Learning

- **Discrete Fourier Transform (2D DFT)**: The method reformulates convolution in frequency domain, so understanding how spatial convolutions map to frequency multiplications is essential. Quick check: Given a 3×3 kernel applied to a 9×9 feature map, what are the dimensions of the frequency representation F_W and F_X?

- **Equivariance vs. Invariance**: The method exploits equivariance (predictable transformation) rather than invariance (no change) to handle scaling/permutation attacks during detection. Quick check: If weights are scaled by a=5, what happens to the cosine similarity between original and attacked frequency components?

- **Low-pass filtering**: The theoretical stability guarantee requires input features with bounded frequency content; the low-pass filter Λ(·) enforces this constraint. Quick check: For r=1 in S_low_r, which frequency components are preserved and which are zeroed out?

## Architecture Onboarding

- **Component map**: Input X → Low-pass filter Λ(X) → Watermark convolutions → Extract F_W^(uv) at frequencies in S' → Detection via cosine similarity and permutation matching

- **Critical path**: 
  1. Input X → Low-pass filter Λ(X) → Watermark convolutions → Extract F_W^(uv) at frequencies in S'
  2. During training: L_CE + L_attack with noise injection ε
  3. During detection: Match filters via permutation, compute DR using cosine similarity (τ=0.995)

- **Design tradeoffs**: 
  - Kernel size K vs. number of stable frequency positions: K=3 yields |S'| = 8 frequencies per filter
  - Low-pass radius r vs. feature representational capacity: r=1 preserves minimal frequencies; r=2 preserves more but may reduce stability
  - Parallel vs. inline watermark module: Parallel connection preserves backbone performance but adds parameters
  - Noise magnitude ||ε|| vs. detection robustness: Paper uses ||ε|| = 0.5·||W||

- **Failure signatures**:
  - High-frequency input leakage: If Λ(·) is bypassed or ineffective, ΔF_W^(uv) ≠ 0 and watermark degrades
  - Detection rate < 100% under fine-tuning: Indicates low-pass constraint violation or numerical precision issues
  - No accuracy drop under overwriting attack: L_attack weight λ may be too small or noise distribution mismatched

- **First 3 experiments**:
  1. Stability verification: Train AlexNet on CIFAR-100 with watermark module, fine-tune on CIFAR-10, compute ||ΔF_W^(uv)|| heatmap across all frequencies
  2. Equivariance test: Apply scaling (a=10, 100) and random permutations to watermark module. Compute DR.
  3. Overwriting defense: Train with L_attack (λ=5×10⁻⁴), then overwrite watermark weights with random values. Compare accuracy drop to baseline without L_attack.

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions in the text provided, but several open questions emerge from the limitations and scope of the work:

- Is the proposed watermark robust to model pruning attacks?
- Can the frequency-domain stability guarantees be generalized to non-CNN architectures like Transformers?
- Can the overwriting defense be circumvented by surgical attacks that isolate the watermark module?

## Limitations

- The theoretical guarantees depend on the low-frequency input assumption, which may not hold for arbitrary fine-tuning datasets
- The method's performance on larger-scale datasets (ImageNet) and deeper architectures remains untested
- The defensive mechanism's effectiveness against adaptive attackers who optimize both watermark removal and accuracy preservation is unclear

## Confidence

- **Mechanism 1 (frequency stability)**: High - The mathematical proof is rigorous and the numerical evidence (100% detection) supports the claim
- **Mechanism 2 (equivariance)**: Medium - While the theoretical proofs are sound, the practical robustness to combined attacks is not fully validated
- **Mechanism 3 (defensive deterrence)**: Medium - The empirical results are strong, but the defense may not hold against sophisticated adaptive attacks

## Next Checks

1. Test stability on high-resolution datasets (ImageNet) where frequency distributions may differ significantly from CIFAR
2. Evaluate against combined attacks (fine-tuning + scaling + noise injection) to assess practical robustness limits
3. Benchmark against state-of-the-art watermarking methods under identical attack scenarios to establish relative performance