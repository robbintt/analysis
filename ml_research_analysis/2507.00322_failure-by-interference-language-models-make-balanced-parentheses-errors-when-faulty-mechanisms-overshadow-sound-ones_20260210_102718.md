---
ver: rpa2
title: 'Failure by Interference: Language Models Make Balanced Parentheses Errors
  When Faulty Mechanisms Overshadow Sound Ones'
arxiv_id: '2507.00322'
source_url: https://arxiv.org/abs/2507.00322
tags:
- paren
- heads
- attention
- gpt-2
- components
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates why language models (LMs) make errors on
  simple syntactic tasks like generating balanced parentheses, and proposes a method
  to mitigate these errors. The study reveals that LMs rely on multiple components
  (attention heads and feed-forward neurons) with varying levels of reliability to
  perform such tasks.
---

# Failure by Interference: Language Models Make Balanced Parentheses Errors When Faulty Mechanisms Overshadow Sound Ones

## Quick Facts
- **arXiv ID:** 2507.00322
- **Source URL:** https://arxiv.org/abs/2507.00322
- **Reference count:** 40
- **Primary result:** Language models fail on simple syntactic tasks not from lacking correct mechanisms, but because faulty components overshadow reliable ones; RASTEER steering can boost accuracy from 0% to ~100% on some models.

## Executive Summary
This paper investigates why language models make errors on simple syntactic tasks like generating balanced parentheses. The authors discover that models rely on multiple components (attention heads and feed-forward neurons) with varying reliability levels to perform such tasks. Some components implement "sound mechanisms" that consistently promote correct answers across inputs, while others introduce noise by promoting incorrect tokens ("faulty mechanisms"). Errors occur when faulty mechanisms overshadow sound ones, dominating predictions. To address this, the authors introduce RASTEER, a steering method that ranks LM components by their reliability and increases the contribution of more reliable components to the final logits. RASTEER substantially improves performance on balanced parentheses tasks and generalizes to arithmetic reasoning, achieving performance gains of up to 20%.

## Method Summary
The authors introduce RASTEER, a steering method that improves LM performance on syntactic tasks by amplifying the contributions of reliable components. Components are ranked by reliability using an F1-score that combines generalization and promotion strength, computed via logit lens analysis. During inference, the activations of the top-k ranked components are multiplied by a scalar α ∈ [1.1, 2.0] before being added to the residual stream, boosting their contribution to the final logits. The method was evaluated on balanced parentheses prediction tasks using synthetic datasets and showed significant accuracy improvements without degrading general coding ability.

## Key Results
- RASTEER boosts accuracy from 0% to around 100% on balanced parentheses tasks for some models
- The method generalizes to arithmetic reasoning tasks, achieving performance gains of up to 20%
- Steering attention heads is more effective than steering feed-forward neurons
- Over-steering (too many components or too high α) can harm general capabilities

## Why This Works (Mechanism)

### Mechanism 1: Component Reliability Mismatch
Models fail at syntactic tasks not because they lack correct mechanisms, but because reliable components are outweighed by noisy ones. The model maintains multiple parallel mechanisms (attention heads, FF neurons) that independently promote token predictions. Errors arise when components promoting incorrect tokens ("faulty mechanisms") have greater aggregate influence than those promoting correct tokens ("sound mechanisms"). The final logit is a roughly additive combination of individual component contributions.

### Mechanism 2: Noisy Promotion via Low-Selectivity Components
Most model components exhibit low selectivity, promoting both correct and incorrect tokens simultaneously ("noisy promotion"). Individual attention heads and FF neurons activate broadly, boosting target tokens but not exclusively. Accurate predictions require aggregating many low-precision signals. FF neurons use a "dual-sign" mechanism (positive/negative coefficients) to handle at most two sub-tasks, but promotion remains inherently noisy.

### Mechanism 3: Steering via Activation Scaling (RASTEER)
Amplifying the activations of top-ranked reliable components improves task accuracy without degrading general performance. Components are ranked by reliability (F1-score combining generalization and promotion strength). During inference, the activations of the top-k components are multiplied by a scalar α ∈ [1.1, 2.0] before being added to the residual stream, boosting their contribution to the final logits.

## Foundational Learning

- **Concept: Residual Stream & Logit Lens**
  - **Why needed here:** The paper's analysis method depends on understanding how component outputs sum into the residual stream and can be projected to vocabulary logits.
  - **Quick check question:** Can you explain how the output of a single attention head is added to the residual stream and then interpreted as a distribution over tokens?

- **Concept: Attention Head Function (Induction, Copying)**
  - **Why needed here:** Identifying "generalizable" heads (e.g., those tracking open parentheses) requires knowing common head functions.
  - **Quick check question:** What might an attention head that solves a "balanced parentheses" task attend to in the input?

- **Concept: Steering / Activation Engineering**
  - **Why needed here:** RASTEER is an activation steering method; understanding the broader concept contextualizes the intervention.
  - **Quick check question:** How does directly modifying a model's internal activations during inference differ from fine-tuning?

## Architecture Onboarding

- **Component map:**
  - Attention Head -> Residual Stream
  - Feed-Forward Neuron -> Residual Stream
  - Residual Stream -> Unembedding Matrix -> Logits
  - Unembedding Matrix -> Used for logit lens analysis

- **Critical path:**
  1. Identify reliable components on a probe dataset using Algorithm 1 (correctness) and Algorithm 2 (promotion strength)
  2. Rank components by F1-score
  3. During inference, multiply the output activation h_c of the top-k components by a scalar α before it is added to the residual stream

- **Design tradeoffs:**
  - **Top-k vs. Thresholding:** The paper selects top-k for simplicity; a score threshold could be more robust
  - **Ranking Metric:** F1-score is a heuristic; a learned ranking could perform better
  - **Heads vs. Neurons:** Steering attention heads was more effective than FF neurons, suggesting their contributions are more direct

- **Failure signatures:**
  - **No improvement:** Ranking failed to identify correct components, or model lacks sufficient sound mechanisms (e.g., GPT-2 Small on 4-paren task)
  - **Performance degradation:** Over-steering (too many components or too high α) can harm general capabilities

- **First 3 experiments:**
  1. **Replicate main finding:** Apply RASTEER to GPT-2 Small on the 3-paren sub-task, varying k and α, to observe the 0% → ~100% accuracy shift
  2. **Component Ablation:** Rank components then steer with top-5 vs. top-20 vs. top-60 heads to quantify the contribution of different reliability tiers
  3. **Generalization Check:** Rank components on the 3-paren task, then apply that same steering to the 4-paren task to test if identified mechanisms generalize

## Open Questions the Paper Calls Out

### Open Question 1
**Question:** What is the underlying cause of RASTEER's failure to improve GPT-2 Small on the four-paren sub-task, given it is not explained by noisy promotion or a lack of accurate attention heads?
**Basis in paper:** The authors state in Section 5.4.2 that the failure appears to be "rather an unexplored factor" after ruling out noisy promotion and head count.
**Why unresolved:** The analysis ruled out the primary hypothesized causes (accuracy counts and F1-score distributions), leaving the mechanism of failure unidentified.
**What evidence would resolve it:** A deeper mechanistic comparison between GPT-2 Small and Medium to identify structural or functional differences in handling four-paren prompts.

### Open Question 2
**Question:** Does the reliance on synthetic data exaggerate RASTEER's effectiveness compared to its performance on real-world code?
**Basis in paper:** Section 7 states that the "study conducted using synthetic data may have exaggerated its effectiveness."
**Why unresolved:** The evaluation used template-based prompts rather than diverse, naturalistic code contexts found in production or complex repositories.
**What evidence would resolve it:** Benchmarking RASTEER on non-synthetic datasets (e.g., The Stack or repository-level completion) to verify if gains persist outside the synthetic distribution.

### Open Question 3
**Question:** How does the assumption of a simple additive motif affect the ability to intervene in non-additive mechanisms like active noise suppression?
**Basis in paper:** Section 7 notes the method assumes a simple additive motif, which "may overlook non-additive mechanisms—such as those that actively suppress noise."
**Why unresolved:** RASTEER intervenes by scaling component activations linearly, potentially missing or disrupting complex, non-linear interactions.
**What evidence would resolve it:** A comparative analysis of model behavior when steering components involved in suppression versus those involved in direct promotion.

### Open Question 4
**Question:** Can more sophisticated ranking and promotion techniques improve upon the current method of using fixed scalar multipliers and heuristic scores?
**Basis in paper:** Section 7 suggests that "Future work could explore more sophisticated techniques for both ranking and promotion" beyond the simple heuristics currently used.
**Why unresolved:** The current method relies on fixed multipliers and basic metrics (Recall/F1), which may be sub-optimal for complex steering.
**What evidence would resolve it:** Experiments comparing the current heuristic approach against learned or gradient-based steering vectors.

## Limitations
- The study used synthetic data, which may exaggerate effectiveness compared to real-world code
- The method assumes a simple additive motif, potentially overlooking non-additive mechanisms like active noise suppression
- FF neuron steering often fails, especially for smaller models, suggesting limitations in the approach

## Confidence

- **High Confidence:** The core experimental results showing RASTEER's effectiveness on balanced parentheses tasks and arithmetic reasoning. The component analysis methodology (identifying reliable vs. faulty mechanisms) is well-supported by the presented evidence.
- **Medium Confidence:** The general mechanism of "overshadowing" as a universal explanation for LM failures across tasks. While compelling for the studied domains, broader applicability requires further validation.
- **Medium Confidence:** The assertion that FF neurons use a "dual-sign mechanism" with at most two sub-tasks. The evidence is based on specific analysis of neuron projections rather than comprehensive architectural understanding.

## Next Checks

1. **Generalization Test:** Apply RASTEER to a naturally occurring code completion benchmark (like CodeContests or HumanEval) rather than synthetic templates, measuring both accuracy gains and any degradation in general coding capability.

2. **Mechanism Isolation:** Perform ablation studies where identified reliable components are removed entirely (not just downweighted) to test whether the model truly lacks alternative sound mechanisms or if RASTEER merely amplifies existing ones.

3. **Cross-Task Transfer:** Train component rankings on balanced parentheses, then apply the same steering to other syntactic tasks (e.g., quotation matching, variable scoping) to quantify how transferable identified reliable mechanisms are across related domains.