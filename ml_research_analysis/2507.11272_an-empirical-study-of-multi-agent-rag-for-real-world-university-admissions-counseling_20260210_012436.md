---
ver: rpa2
title: An Empirical Study of Multi-Agent RAG for Real-World University Admissions
  Counseling
arxiv_id: '2507.11272'
source_url: https://arxiv.org/abs/2507.11272
tags:
- university
- retrieval
- maraus
- systems
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents MARAUS, a multi-agent RAG system for university
  admissions counseling in Vietnam. It addresses the limitations of LLM-only chatbots
  by integrating hybrid retrieval (BM25 + semantic), multi-agent orchestration, and
  LLM-based re-ranking to handle complex, domain-specific queries.
---

# An Empirical Study of Multi-Agent RAG for Real-World University Admissions Counseling

## Quick Facts
- arXiv ID: 2507.11272
- Source URL: https://arxiv.org/abs/2507.11272
- Authors: Anh Nguyen-Duc; Chien Vu Manh; Bao Anh Tran; Viet Phuong Ngo; Luan Le Chi; Anh Quang Nguyen
- Reference count: 20
- One-line primary result: MARAUS achieved 92% accuracy and reduced hallucination rates from 15% to 1.45% in university admissions counseling.

## Executive Summary
MARAUS is a multi-agent RAG system designed to handle complex university admissions counseling queries in Vietnam. The system addresses the limitations of LLM-only chatbots by combining hybrid retrieval (BM25 + semantic), multi-agent orchestration, and LLM-based re-ranking. Deployed in a real-world setting, MARAUS processed over 6,000 user interactions across six query types, demonstrating strong performance with an average accuracy of 92%, hallucination rates reduced from 15% to 1.45%, and response times under 4 seconds. The system cost-effectively handled admissions inquiries with a two-week deployment cost of $11.58 USD.

## Method Summary
MARAUS implements a multi-agent RAG architecture with four specialized processing pipelines: Information search, Score calculation, Recommendation, and General Query. The system uses hybrid retrieval combining FAISS semantic search and ElasticSearch BM25 keyword search, followed by GPT-4o mini cross-encoder re-ranking to select the top-2 passages. Generation is performed with GPT-4o mini using temperature=0.7, top_p=0.9, and max_tokens=350, with a post-processor enforcing citation integrity through mandatory passage citations and regeneration if missing. The corpus consists of 1,376 FAQ pairs and 98 document chunks processed into 8,412 overlapping windows (500 tokens, 100 stride) and embedded with Xenova/all-mpnet-base-v2.

## Key Results
- Achieved 92% accuracy across six query types including score calculations and personalized recommendations
- Reduced hallucination rates from 15% (LLM-only) to 1.45% through citation enforcement
- Maintained response times under 4 seconds with cost-effective operation at $11.58 USD for two weeks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hybrid retrieval combining semantic (FAISS) and keyword (BM25) search with LLM-based re-ranking improves precision over single-method retrieval.
- Mechanism: BM25 captures exact term matches for domain-specific vocabulary (e.g., "KV1" priority zone), while semantic retrieval handles paraphrased queries. A GPT-4o mini cross-encoder re-ranks the union of both result sets by scoring query-passage pairs, selecting top-2 passages. This reduces false positives by 38% compared to raw FAISS retrieval alone.
- Core assumption: Vietnamese embedding models (Xenova/all-mpnet-base-v2) provide sufficient semantic fidelity for the domain despite being trained primarily on English data.
- Evidence anchors:
  - [abstract]: "combining hybrid retrieval, multi-agent orchestration, and LLM-based generation"
  - [section 4.3]: "The union of BM25 and FAISS outputs is passed to a hybrid re-ranking stage... GPT-4o mini as a zero-shot cross-encoder for relevance scoring"
  - [section 4.5, Table 2]: Precision improved from 0.70 (LLM-only) to 0.985 (Hybrid RAG)
  - [corpus]: URAG (arXiv:2501.16276) reports similar hybrid RAG approach for Vietnamese university admissions, but with different re-ranking strategy

### Mechanism 2
- Claim: Query-type classification into specialized agent pipelines improves handling of diverse query complexity levels.
- Mechanism: An orchestrator classifies incoming queries into four pipelines: (1) Information search agent for factual lookups, (2) Score calculation agent for numeric computations with rule chaining, (3) Recommendation agent for eligibility predictions, (4) General Query agent as fallback for low-confidence classifications. Each pipeline uses tailored extraction and reasoning logic.
- Core assumption: Query intent can be reliably classified at input time; misclassification does not catastrophically degrade user experience.
- Evidence anchors:
  - [section 4.1]: "multi-agent coordinator that classifies incoming queries into four distinct processing pipelines"
  - [section 5.1]: Identifies six query types with increasing complexity, from "Simple Keyword Retrieval" to "Handling Ambiguous or Subjective Questions"
  - [KO2]: "Domain-specialized orchestration that integrates retrieval, reasoning, and personalized interaction is essential for addressing diverse user intents"
  - [corpus]: KIMAs (arXiv:2502.09596) proposes configurable multi-agent systems for knowledge-intensive conversations but without empirical deployment data

### Mechanism 3
- Claim: Citation-enforcement post-processing with regeneration reduces hallucination rates.
- Mechanism: After generation, a post-processor checks whether the output contains at least one explicit citation to retrieved passages. If absent, the answer is discarded and regenerated with penalized decoding parameters. This forces the LLM to ground responses in retrieved context rather than synthesizing from parametric knowledge.
- Core assumption: Citations in generated text correlate with factual grounding; retrieval corpus contains accurate, up-to-date information.
- Evidence anchors:
  - [section 4.4]: "A custom post-processor enforces citation integrity. If generated answers lack at least one passage citation, they are discarded and regenerated with penalized decoding parameters."
  - [section 4.4]: "This mechanism reduces hallucination rates from 15% (LLM-only) to 1.45% in the hybrid pipeline."
  - [section 5.3]: "Officers... favoured the explainability provided by chunk citations"
  - [corpus]: No direct comparison papers measure citation-enforcement mechanisms; corpus evidence is weak for this specific technique

## Foundational Learning

- Concept: **RAG Pipeline Fundamentals** (retrieval → context injection → generation)
  - Why needed here: Understanding how retrieval grounding differs from parametric LLM knowledge is essential to diagnose whether errors stem from retrieval quality or generation failures.
  - Quick check question: Given a query with no relevant documents in the corpus, what should a well-designed RAG system output?

- Concept: **Dense vs. Sparse Retrieval Trade-offs**
  - Why needed here: MARAUS combines FAISS (dense) and BM25 (sparse); engineers must understand when each excels to tune the hybrid fusion and re-ranking stages.
  - Quick check question: Why might BM25 outperform semantic search for a query containing a specific policy code like "Decision 1234/QD-UTT"?

- Concept: **Cross-Encoder vs. Bi-Encoder Architectures**
  - Why needed here: MARAUS uses GPT-4o mini as a cross-encoder for re-ranking; this is computationally expensive but more accurate than bi-encoder scoring.
  - Quick check question: If latency budget tightens from 4s to 2s per query, what re-ranking alternatives could maintain acceptable precision?

## Architecture Onboarding

- Component map:
  Preprocessing → VnCoreNLP tokenization → PII redaction → Chunking (500 tokens, 100 stride) → Embedding (Xenova/all-mpnet-base-v2, 768d) → FAISS IndexFlatIP (565 MB RAM) + ElasticSearch 8.11 (BM25) → Query classifier routing to 4 agent pipelines → Parallel FAISS (top-k=15) + BM25 → Union → GPT-4o mini re-ranker → Top-2 passages → Generation with GPT-4o mini (temp=0.7, top_p=0.9, max_tokens=350) → Citation validator → Regenerate if no citation → Response

- Critical path: Query ingestion → Classification → Retrieval (FAISS + BM25 parallel) → Re-ranking → Prompt assembly → Generation → Citation check → Response

- Design tradeoffs:
  - Chunk size (500 tokens) balances context retention vs. token limits; smaller chunks improve precision but may fragment answers
  - Top-k=15 retrieval with re-ranking to top-2 optimizes downstream token usage while maintaining candidate diversity
  - GPT-4o mini chosen for cost-effectiveness ($11.58/2 weeks) over GPT-4o ($57.90), accepting potential quality trade-off
  - Citation enforcement adds latency but critical for trust in high-stakes admissions context

- Failure signatures:
  - **High latency (>6s)**: Check re-ranking stage (cross-encoder is bottleneck); verify FAISS index fits in RAM
  - **Low accuracy on score calculations**: Inspect Score calculation agent rule extraction; verify priority bonus logic matches current MOET regulations
  - **Hallucination spike**: Check retrieval quality first (are correct passages in top-15?); verify corpus is updated with current-year policies
  - **Classification errors**: Review orchestrator confidence thresholds; ambiguous queries falling to General Query agent may lack specialized handling

- First 3 experiments:
  1. **Ablation on retrieval components**: Run evaluation set with (a) BM25 only, (b) FAISS only, (c) both without re-ranking, (d) full hybrid. Measure precision, recall, hallucination rate to quantify each component's contribution.
  2. **Chunk size sensitivity**: Test chunk sizes [250, 500, 750, 1000] tokens on a held-out set of complex multi-part queries (e.g., score calculations requiring multiple policy rules). Measure accuracy and response latency.
  3. **Classification confidence threshold tuning**: Vary the threshold for routing to General Query agent vs. specialized agents. Plot accuracy vs. specialized agent utilization to find the point where fallback frequency minimally impacts accuracy while reducing misclassification risk.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can Vietnamese-specific or multilingual embedding models achieve sufficient semantic fidelity for RAG systems compared to English-centric models in low-resource language settings?
- Basis in paper: [explicit] Authors state "It is uncertain whether Vietnamese-specific or multilingual embedding models offer sufficient semantic fidelity compared to their English-centric counterparts."
- Why unresolved: The study used a single embedding model (Xenova/all-mpnet-base-v2) without comparative evaluation against Vietnamese-specific alternatives.
- What evidence would resolve it: Systematic comparison of Vietnamese-specific embeddings (e.g., PhoBERT-based) versus multilingual models on retrieval precision and downstream QA accuracy.

### Open Question 2
- Question: Which prompt engineering strategies (few-shot exemplars, chain-of-thought reasoning, verifier agents) most effectively minimize hallucinations while maintaining response conciseness?
- Basis in paper: [explicit] Authors note these strategies "require further empirical validation to assess their effectiveness in minimizing hallucinations while preserving concise, trustworthy outputs in real-world deployments."
- Why unresolved: MARAUS used a single prompt configuration; no ablation study compared alternative prompting approaches.
- What evidence would resolve it: Controlled experiments varying prompt strategies while measuring hallucination rate, accuracy, and response length.

### Open Question 3
- Question: How effectively can MARAUS's architecture generalize to other universities with different admission policies, data structures, and communication channels?
- Basis in paper: [inferred] Single-case study at UTT; authors acknowledge generalizability concerns and state "Future work will extend MARAUS to other universities."
- Why unresolved: Only tested at one institution; institutional practices, data governance, and IT infrastructure vary considerably.
- What evidence would resolve it: Deployment across 3-5 diverse universities with comparative accuracy, adaptation effort, and cost metrics.

### Open Question 4
- Question: What mechanisms can improve performance on personalization and multi-turn conversations, where MARAUS showed higher error rates?
- Basis in paper: [inferred] Authors report "distribution of wrong answers are different across categories of questions relating Personalization and Multi-turn conversation and Handling ambiguous and subjective questions."
- Why unresolved: No specific interventions were tested for these challenging query types.
- What evidence would resolve it: Targeted architectural modifications (e.g., enhanced context memory, dialogue state tracking) evaluated on multi-turn conversation benchmarks.

## Limitations
- Single-institution deployment (UTT) limits generalizability to other universities with different policies and structures
- Two-month evaluation timeframe insufficient to assess long-term performance degradation from policy changes
- No ablation studies isolating individual architectural component contributions to overall accuracy

## Confidence
- **High confidence**: Accuracy metrics (92%), hallucination reduction (15% → 1.45%), and response time (<4s) are directly measured and well-documented
- **Medium confidence**: Cost-effectiveness ($11.58/2 weeks) and user satisfaction (4.5/5) are self-reported without independent verification or broader user sampling
- **Low confidence**: Generalization claims to "any higher education institution" and the specific impact of individual architectural components lack comparative ablation evidence

## Next Checks
1. **Ablation study deployment**: Deploy MARAUS with individual components disabled (no re-ranking, no citation enforcement, no hybrid retrieval) to quantify each contribution to the 92% accuracy score
2. **Cross-institutional validation**: Implement MARAUS at a second Vietnamese university with different admissions policies and document structure to test generalizability claims
3. **Long-term drift analysis**: Monitor system performance over 6+ months to measure accuracy degradation from policy changes, corpus staleness, and LLM model updates