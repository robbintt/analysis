---
ver: rpa2
title: 'POWSM: A Phonetic Open Whisper-Style Speech Foundation Model'
arxiv_id: '2510.24992'
source_url: https://arxiv.org/abs/2510.24992
tags:
- doreco
- language
- lyon
- speech
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: POWSM introduces the first unified phonetic foundation model that
  jointly performs phone recognition (PR), automatic speech recognition (ASR), audio-guided
  grapheme-to-phoneme conversion (G2P), and audio-guided phoneme-to-grapheme conversion
  (P2G) within a single architecture. By reformulating multilingual ASR datasets into
  four task-specific formats, POWSM achieves state-of-the-art PR performance while
  supporting three additional phone-related tasks.
---

# POWSM: A Phonetic Open Whisper-Style Speech Foundation Model

## Quick Facts
- arXiv ID: 2510.24992
- Source URL: https://arxiv.org/abs/2510.24992
- Reference count: 40
- Key outcome: First unified phonetic foundation model achieving state-of-the-art phone recognition while supporting ASR, audio-guided G2P, and P2G tasks

## Executive Summary
POWSM introduces the first unified phonetic foundation model that jointly performs phone recognition (PR), automatic speech recognition (ASR), audio-guided grapheme-to-phoneme conversion (G2P), and audio-guided phoneme-to-grapheme conversion (P2G) within a single architecture. By reformulating multilingual ASR datasets into four task-specific formats, POWSM achieves state-of-the-art PR performance while supporting three additional phone-related tasks. The model outperforms specialized PR baselines on both in-domain and out-of-domain languages, including unseen and low-resource varieties, and matches or exceeds ASR performance of models trained on significantly more data. Key innovations include an attention-based encoder-decoder design, phoneme-level CTC supervision, and task- and language-specific tokens that enable fine-grained control over phonetic and phonological output. The model's encoder benefits from stronger weighting and phoneme-level targets, improving cross-lingual generalization. POWSM also demonstrates interpretable multimodal behavior, effectively mediating between narrow phonetic detail and standardized phonological patterns. All training data, code, and models are fully open-sourced to foster reproducible research in inclusive and globally accessible speech technologies.

## Method Summary
POWSM is an attention-based encoder-decoder model (E-Branchformer encoder + Transformer decoder, 9 layers each, ~350M parameters) trained on IPAPack++ corpus (~17,000 hours multilingual speech with orthographic + phonemic transcriptions). The model uses hybrid CTC/attention loss with Î±_ctc=0.3, encoder stride 40ms, and a 40k token vocabulary including 6k phone tokens. Data is formatted into four task types (PR, ASR, G2P, P2G) with language tokens and text prompts, then the same utterances are reused across all tasks. CTC targets use phones stripped of suprasegmentals. The model is trained in ESPnet with batch size 256 on 16kHz audio padded to 20s, requiring ~200 GPU-hours on H100s.

## Key Results
- State-of-the-art phone recognition performance across multiple languages and domains
- Superior cross-lingual generalization for unseen and low-resource languages
- Effective audio-guided G2P that preserves socio-phonetic variation while text-only prompts enforce standardization
- ASR performance matching or exceeding models trained on web-scale data

## Why This Works (Mechanism)

### Mechanism 1: Phonetic Pre-training Enables Low-Resource ASR Transfer
- Claim: Including phone recognition as a pre-training task improves representation generality and reduces data required to map acoustics to text tokens.
- Mechanism: The shared phonetic encoder learns language-agnostic acoustic representations from phone recognition that transfer to low-resource ASR, as phones are "shared across languages" and provide "consistent representation of speech across languages."
- Core assumption: Phone-level supervision creates more transferable acoustic representations than purely grapheme-based ASR training alone.
- Evidence anchors: POW