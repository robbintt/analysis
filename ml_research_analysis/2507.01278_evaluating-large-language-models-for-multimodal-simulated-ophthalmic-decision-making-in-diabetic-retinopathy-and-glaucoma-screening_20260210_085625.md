---
ver: rpa2
title: Evaluating Large Language Models for Multimodal Simulated Ophthalmic Decision-Making
  in Diabetic Retinopathy and Glaucoma Screening
arxiv_id: '2507.01278'
source_url: https://arxiv.org/abs/2507.01278
tags:
- metadata
- referral
- clinical
- diabetic
- glaucoma
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluated GPT-4\u2019s ability to simulate clinical\
  \ reasoning in ophthalmology using structured textual descriptions of retinal fundus\
  \ photographs. The model was tasked with assigning International Clinical Diabetic\
  \ Retinopathy (ICDR) scores, recommending diabetic retinopathy (DR) and glaucoma\
  \ referrals, under three metadata conditions: image-only, real patient data, and\
  \ synthetic data."
---

# Evaluating Large Language Models for Multimodal Simulated Ophthalmic Decision-Making in Diabetic Retinopathy and Glaucoma Screening

## Quick Facts
- arXiv ID: 2507.01278
- Source URL: https://arxiv.org/abs/2507.01278
- Reference count: 22
- Primary result: GPT-4 achieved moderate accuracy for ICDR classification (67.5%) and binary DR referral (82.3%), but performed poorly for glaucoma referral (F1 < 0.04), with metadata having no significant impact on predictions

## Executive Summary
This study evaluated GPT-4's ability to simulate clinical reasoning in ophthalmology using structured textual descriptions of retinal fundus photographs. The model was tasked with assigning International Clinical Diabetic Retinopathy (ICDR) scores, recommending diabetic retinopathy (DR) and glaucoma referrals, under three metadata conditions: image-only, real patient data, and synthetic data. Performance was moderate for ICDR classification (accuracy 67.5%, F1 0.33) and improved for binary DR referral (accuracy 82.3%, F1 0.54), but was poor for glaucoma referral (accuracy ~78%, F1 <0.04). Metadata inclusion did not significantly impact predictions (p > 0.05), and model outputs remained largely consistent across conditions. GPT-4 showed potential for supporting non-diagnostic tasks like documentation and education, but lacks precision for clinical decision-making.

## Method Summary
The study used 300 fundus images from the mBRSET dataset with ICDR labels, DR referral labels, and CDR values. GPT-4 (via ChatGPT) was prompted to analyze structured textual descriptions of these images and assign ICDR scores, recommend DR referrals, and estimate CDR for glaucoma referral decisions. Three experimental conditions were tested: image-only descriptions, descriptions with real patient metadata, and descriptions with synthetic metadata. Model outputs were evaluated using accuracy, macro F1, weighted F1, Cohen's kappa, and McNemar's test to assess metadata impact.

## Key Results
- ICDR classification: 67.5% accuracy, F1 0.33, indicating moderate performance with class imbalance issues
- Binary DR referral: 82.3% accuracy, F1 0.54, showing substantial improvement over multiclass task
- Glaucoma referral: Poor performance with accuracy ~78% but F1 < 0.04, suggesting text descriptions inadequate for CDR estimation
- Metadata impact: No significant effect on predictions (p > 0.05), with change rates < 7% across conditions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Text-based simulation of clinical reasoning depends on the semantic richness of structured prompts rather than pixel-level analysis.
- **Mechanism:** GPT-4 maps structured textual descriptions of fundus images (e.g., "microaneurysms present") to known clinical concepts (ICDR criteria) via probabilistic associations in its training data. The model generates outputs by predicting the most likely clinical label associated with the description provided.
- **Core assumption:** The structured text description provided in the prompt captures the diagnostic features (lesions, hemorrhages) sufficiently for the model to retrieve the correct clinical classification from its pre-trained knowledge.
- **Evidence anchors:**
  - [abstract] "GPT-4's ability to interpret structured textual descriptions... and simulate clinical decisions."
  - [section] Page 4: "GPT-4 is a transformer-based autoregressive model... it does not have direct access to image inputs... all image content was conveyed through structured textual descriptions."
  - [corpus] Neighbor paper "Simulating Clinical AI Assistance..." explores similar simulation logic.

### Mechanism 2
- **Claim:** Metadata integration is ineffective because the model relies predominantly on explicit clinical findings in the text.
- **Mechanism:** When provided with both image descriptions and patient metadata (age, insulin use), the model exhibits a "primacy bias" toward the descriptive features of the pathology. It appears to treat metadata as weak or non-informative noise for the specific tasks of DR grading and glaucoma referral, resulting in statistically identical outputs regardless of metadata authenticity.
- **Core assumption:** The model implicitly weighs the semantic strength of pathological features (e.g., "exudates") higher than demographic context, or lacks the fine-tuned reasoning to adjust probability thresholds based on risk factors like "insulin use."
- **Evidence anchors:**
  - [abstract] "Metadata inclusion did not significantly affect outcomes (McNemar p > 0.05)... predictions rely primarily on textual image descriptions."
  - [section] Page 8: "GPT-4... does not function as a true multimodal model... limited capacity to dynamically integrate structured patient-specific metadata."
  - [corpus] Corpus suggests a trend toward "Multimodal LLMs" (e.g., *Simulating Clinical AI Assistance...*), implicitly contrasting with this paper's finding that text-based metadata alone is insufficient for robust context integration.

### Mechanism 3
- **Claim:** Performance degrades as output granularity increases from binary classification to fine-grained estimation.
- **Mechanism:** The model performs better at coarse, binary distinctions (Refer vs. No Refer) because the semantic gap between the text description and the decision boundary is wide and well-represented in training data. It fails at precise estimation tasks (Cup-to-Disc Ratio) or fine-grained classification (5-class ICDR) because natural language descriptions lack the spatial precision and quantitative nuance required for these specific estimations.
- **Core assumption:** The model's pre-training corpus contains sufficient examples of binary referral logic but insufficient high-quality alignments between textual descriptions and precise numerical ratios (e.g., "0.6 CDR").
- **Evidence anchors:**
  - [abstract] "Moderate performance in DR tasks... but performed poorly in glaucoma referral (accuracy ~78%, F1 <0.04)."
  - [section] Page 6: "Reframing the task to binary referral... substantially improved model performance compared to the multiclass ICDR task."

## Foundational Learning

- **Concept: International Clinical Diabetic Retinopathy (ICDR) Scale**
  - **Why needed here:** This is the target output for the model. Understanding that it is a 5-level scale (0-4) is necessary to interpret why the "Multiclass" task (F1 0.33) was harder than the "Referral" task (F1 0.54).
  - **Quick check question:** If the model predicts "moderate non-proliferative DR" (Level 2), is that automatically a referral case in this study's logic?

- **Concept: Cup-to-Disc Ratio (CDR)**
  - **Why needed here:** This is the proxy metric for Glaucoma screening in the paper. You must understand that CDR > 0.6 implies referral to see why the model's "poor performance" (F1 < 0.04) suggests it cannot estimate this ratio from text reliably.
  - **Quick check question:** Why would a text-based model struggle to estimate a geometric ratio (CDR) compared to identifying a semantic feature (hemorrhage)?

- **Concept: McNemar's Test**
  - **Why needed here:** The paper uses this statistical test to prove that adding metadata didn't change results. You need to know it checks for differences in paired binary decisions to interpret the "p > 0.05" finding.
  - **Quick check question:** If McNemar's p-value was < 0.05 for the "Real vs. Synthetic Metadata" comparison, what would that imply about the model's sensitivity to context?

## Architecture Onboarding

- **Component map:** 300 Fundus Images (ground truth) -> Encoder (Human/External): Converted to structured text descriptions -> Context: Metadata Generator (Real or Synthetic) -> Concatenated with Image Description -> Model: GPT-4 (Text-only interface via ChatGPT) -> Prompt: Structured instructions (ICDR criteria, CDR thresholds) -> Output Parser: Regex/JSON extraction for ICDR Score, Referral (Yes/No), CDR value

- **Critical path:**
  1. Prompt Engineering: Defining the exact instructions for ICDR and CDR estimation (see Appendix)
  2. Description Quality: The fidelity of the text description determines the ceiling of performance (GIGO principle)
  3. Evaluation: Calculating Accuracy, F1, and Kappa against the mBRSET dataset labels

- **Design tradeoffs:**
  - Text-only vs. Multimodal: The study uses GPT-4 (text) to isolate "reasoning" capability from "vision" capability. The tradeoff is a total reliance on the quality of the intermediate description
  - Macro vs. Weighted F1: The study reports both. Macro F1 reveals poor performance on minority classes (rare severe cases), while Weighted F1 is skewed by the majority class (Normal). Use Macro F1 to assess clinical reliability

- **Failure signatures:**
  - The "Normal" Bias: High accuracy (67.5%) but low Kappa (0.25) in ICDR suggests the model defaults to "Normal" (Class 0) when uncertain, inflating accuracy due to class imbalance
  - Metadata Blindness: Change rates < 7% and p > 0.05 indicate the model ignores the "patient context" block of the prompt entirely

- **First 3 experiments:**
  1. Baseline Replication: Run the "Image-Only" prompt on 50 samples to verify the "Normal" bias hypothesis (check confusion matrix for Class 0 dominance)
  2. Metadata Perturbation: Flip the metadata for a known "Referable" case (e.g., change age to 20, remove "insulin") to see if the model is *truly* insensitive to context or if the change rate is just statistically small
  3. Description Sensitivity: Rewrite an image description using synonyms (e.g., change "hemorrhage" to "bleeding") to test if the model relies on specific keywords or general semantic understanding

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can truly multimodal AI systems combining visual processing and LLM reasoning improve ophthalmic decision-making performance?
- Basis in paper: [explicit] The authors state, "Future work should explore the development of truly multimodal AI systems that combine the visual processing capabilities of image-based models with the reasoning abilities of LLMs."
- Why unresolved: This study evaluated GPT-4 using only structured textual descriptions of images, not direct pixel-level input.
- What evidence would resolve it: A comparative study evaluating models like GPT-4V or Gemini on raw fundus images versus text-based descriptions.

### Open Question 2
- Question: How does GPT-4 performance generalize across diverse populations, devices, and documentation styles?
- Basis in paper: [explicit] The authors note the analysis was "limited to a subset of the same dataset (mBRSET) without external validation, restricting the assessment of generalizability."
- Why unresolved: The model may be sensitive to the specific phrasing of prompts or demographic distributions in the single dataset used.
- What evidence would resolve it: External validation studies using datasets from different geographic regions and imaging devices.

### Open Question 3
- Question: Can prompt engineering or fine-tuning enable LLMs to better integrate clinical metadata into decision-making?
- Basis in paper: [inferred] The study found metadata had no significant impact (p > 0.05), leading the authors to suggest GPT-4 relies on "general medical knowledge" rather than patient-specific context.
- Why unresolved: It remains unclear if the failure to utilize metadata is an architectural limitation of GPT-4 or a failure of the specific prompt design used.
- What evidence would resolve it: Experiments varying prompt structures to explicitly weight demographic factors in the reasoning chain.

## Limitations

- The study relies on structured textual descriptions rather than direct image input, creating uncertainty about whether performance reflects GPT-4's reasoning capabilities or the quality of human-written or model-generated descriptions
- Poor glaucoma referral performance (F1 < 0.04) suggests text-based descriptions cannot capture spatial features necessary for CDR estimation, making this finding difficult to interpret as a true capability assessment
- The McNemar's test results showing no metadata impact may reflect insufficient statistical power given small effect sizes (change rates < 7%) rather than true insensitivity to context

## Confidence

- **High confidence:** Binary DR referral task results (accuracy 82.3%, F1 0.54) are reliable and replicable, depending on semantic pattern matching that text descriptions can adequately capture
- **Medium confidence:** ICDR multiclass classification results (accuracy 67.5%, F1 0.33) are directionally correct but likely inflated by class imbalance favoring normal cases, with exact performance ceiling uncertain without knowing description quality
- **Low confidence:** Glaucoma referral findings are least reliable due to fundamental mismatch between text-based descriptions and spatial ratio estimation

## Next Checks

1. Generate 50 text descriptions using a standardized annotation protocol and run them through GPT-4 to verify the "normal case bias" hypothesis by examining per-class F1 scores and confusion matrices
2. Test metadata sensitivity by creating controlled perturbations (e.g., removing insulin use from a known referable case) to determine if the model truly ignores context or if effect sizes are too small to detect statistically
3. Conduct a controlled experiment comparing GPT-4's performance on the same fundus images using direct image input (via multimodal APIs) versus text descriptions to isolate the impact of the intermediate description step on clinical reasoning accuracy