---
ver: rpa2
title: 'To Bin or not to Bin: Alternative Representations of Mass Spectra'
arxiv_id: '2502.10851'
source_url: https://arxiv.org/abs/2502.10851
tags:
- mass
- spectra
- learning
- graph
- molecular
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of representing mass spectrometry
  data for machine learning tasks without relying on traditional binning or sub-sampling
  methods. The authors propose two alternative representations: set-based and graph-based,
  and compare them to a standard binned approach.'
---

# To Bin or not to Bin: Alternative Representations of Mass Spectra

## Quick Facts
- arXiv ID: 2502.10851
- Source URL: https://arxiv.org/abs/2502.10851
- Reference count: 9
- Graph-based model achieves MAE of 0.110, RMSE of 0.144, Pearson's r of 0.843, and R² of 0.709 on QED prediction task

## Executive Summary
This paper addresses the challenge of representing mass spectrometry data for machine learning tasks without relying on traditional binning or sub-sampling methods. The authors propose two alternative representations: set-based and graph-based, and compare them to a standard binned approach. All three models are evaluated on a regression task predicting the quantitative estimate of drug-likeness (QED) from mass spectra. The graph-based model achieves the best performance with MAE of 0.110, RMSE of 0.144, Pearson's r of 0.843, and R² of 0.709, significantly outperforming the MLP on binned data and the SetTransformer. The SetTransformer is highly parameter-efficient with only 400k parameters. These results suggest that set and especially graph representations preserve more information than binning, and can improve performance in mass spectrometry machine learning.

## Method Summary
The authors compare three architectures for predicting QED from mass spectra: a standard MLP on binned data, a SetTransformer on set-based representations, and a Graph Attention Network (GAT) on graph-based representations. For the binned approach, spectra are discretized into 10,000 bins. For the set-based approach, m/z-intensity pairs are treated as sets and processed using a SetTransformer. For the graph-based approach, each peak becomes a vertex, connected by edges encoding m/z differences, and processed with a GAT. All three models are trained on data from de Jonge et al. (2025) with QED labels computed using RDKit. The graph-based model achieves the best performance, followed by the SetTransformer, with the binned MLP performing worst.

## Key Results
- Graph-based GAT achieves MAE of 0.110, RMSE of 0.144, Pearson's r of 0.843, and R² of 0.709 on QED prediction
- SetTransformer with only 400k parameters achieves MAE of 0.134, demonstrating strong parameter efficiency
- MLP on binned data achieves MAE of 0.145 ± 0.008, significantly worse than graph-based approach
- Graph representation outperforms set representation, suggesting relational information between peaks is valuable

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Graph representations of mass spectra preserve relational information between peaks that binning destroys.
- Mechanism: Each peak becomes a vertex (with intensity as node attribute), connected to neighbors via edges encoding m/z differences. The Graph Attention Network propagates information along these edges, allowing the model to learn from fragmentation patterns encoded in peak relationships.
- Core assumption: m/z differences between peaks carry chemically meaningful signal relevant to molecular property prediction.
- Evidence anchors:
  - [abstract] "each peak becomes a vertex, connected by edges encoding m/z differences, and processed with a Graph Attention Network"
  - [section] "Given the performance increase of the GNN compared to the set-based approach... hints at the possibility that graphs are a favorable representation of mass spectra, as meaningful information may be propagated along the graph"
  - [corpus] Related work (MassFormer, MS-BART) uses GNNs for molecular-spectral tasks, supporting graph utility in this domain
- Break condition: If m/z differences are chemically irrelevant noise, edge attributes provide no signal and graph advantage collapses.

### Mechanism 2
- Claim: Avoiding binning preserves precise peak information that discretization loses.
- Mechanism: Binning maps continuous m/z values to fixed bins (0–10,000 here), potentially collapsing distinct peaks into same bin or losing precision at bin boundaries. Set and graph representations retain exact values.
- Core assumption: The exact m/z values contain prediction-relevant information that binning partially destroys.
- Evidence anchors:
  - [abstract] "these methodologies require preprocessing of the spectra, which often includes binning or sub-sampling peaks"
  - [section] Table 1: MLP (binned) MAE 0.145 ± 0.008 vs. GAT (graph) MAE 0.110 ± 0.006—a 24% relative improvement
  - [corpus] Corpus papers uniformly use binning/tokenization; no direct comparison to non-binned approaches found
- Break condition: If bin resolution sufficiently exceeds instrument precision, discretization loss becomes negligible.

### Mechanism 3
- Claim: SetTransformer provides strong parameter efficiency by treating spectra as unordered sets.
- Mechanism: m/z-intensity pairs are processed as sets via attention-based pooling, avoiding fixed vector sizes and positional assumptions while handling variable-length spectra natively.
- Core assumption: Peak order in spectra is not essential for the prediction task; identities matter more than sequence.
- Evidence anchors:
  - [section] "The set representation model is highly efficient with only 400k parameters, compared to 11.0M and 12.6M parameters of the other two models"
  - [section] MAE 0.134 with 0.4M params vs. MAE 0.145 with 11.0M params (MLP)—better accuracy with 28× fewer parameters
  - [corpus] Limited corpus evidence on set representations for spectra; Boulougouri et al. (2024) cited for molecules, not spectra
- Break condition: If sequential fragmentation encodes task-relevant signal, set permutation invariance loses this information.

## Foundational Learning

- Concept: **Mass spectrometry fundamentals (m/z and intensity)**
  - Why needed here: All three architectures process peaks defined by mass-to-charge ratio and intensity; understanding these as physical measurements clarifies why precision matters.
  - Quick check question: Can you explain why two different molecules might produce peaks at nearly identical m/z values but different intensities?

- Concept: **Graph Attention Networks (GAT)**
  - Why needed here: The best-performing model uses GAT with 8 message-passing layers; understanding attention over neighbors explains how edge attributes (m/z deltas) influence predictions.
  - Quick check question: How does a GAT differ from a standard GCN in aggregating neighbor information?

- Concept: **Set Transformers and permutation invariance**
  - Why needed here: The SetTransformer must handle spectra with varying numbers of peaks without assuming order; permutation invariance is the key design constraint.
  - Quick check question: Why would a standard transformer with positional encodings be inappropriate for set-structured data?

## Architecture Onboarding

- Component map:
  - Input layer: Normalized m/z-intensity pairs + precursor m/z (treated as peak with intensity 2.0)
  - Three parallel paths:
    - Binned → MLP (2 hidden: 1024, 512; ReLU; dropout 0.5)
    - Set → SetTransformer (2 hidden: 32, 16)
    - Graph → GAT (8 message-passing layers; 1024 hidden channels; global mean pooling)
  - Output layer: Regression head predicting QED (0–1 scale)

- Critical path:
  1. Data preprocessing: Normalize intensities, add dummy vertex (m/z=0, intensity=0) for initial m/z delta encoding in graph path
  2. Graph construction: Connect each peak to neighbors; edges store m/z differences
  3. Model selection: Start with GAT if accuracy is priority; SetTransformer if parameter efficiency is critical

- Design tradeoffs:
  - GAT: Best accuracy (R²=0.709) but 12.6M parameters
  - SetTransformer: Strong efficiency (0.4M params) with competitive accuracy
  - MLP baseline: Simplest implementation but worst performance
  - Assumption: GAT hyperparameters (8 layers, 1024 channels) were matched to MLP parameter count, not independently tuned—further optimization may widen or narrow gaps

- Failure signatures:
  - If GAT underperforms MLP: Check graph construction (edges may not be properly connecting neighbors)
  - If SetTransformer fails to converge: Verify set encoding handles variable lengths correctly
  - If all models perform poorly: Verify intensity normalization and precursor encoding consistency

- First 3 experiments:
  1. Reproduce the QED regression task on provided data splits to validate baseline numbers match Table 1
  2. Ablate the dummy vertex (m/z=0, intensity=0) in graph construction to test its contribution
  3. Test SetTransformer with increased hidden dimensions (e.g., 64, 32) to measure accuracy vs. parameter tradeoff curve

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do set and graph representations maintain their performance advantage over binned representations in more complex metabolomics tasks like molecular structure prediction or spectral similarity scoring?
- Basis in paper

## Limitations

- Major uncertainties in reproduction: Train/validation/test split ratios, training hyperparameters (optimizer, learning rate, batch size, epochs), and edge construction details ("neighboring peaks") are not specified
- Edge construction ambiguity: "neighboring peaks" is unclear—could mean fully connected, k-nearest, or sequential along m/z axis
- Assumption of hyperparameter matching: GAT hyperparameters were matched to MLP parameter count rather than independently optimized

## Confidence

- **High**: Graph-based representation outperforms binning in QED regression (MAE 0.110 vs 0.145)
- **Medium**: SetTransformer achieves strong parameter efficiency (0.4M params) while maintaining reasonable accuracy
- **Low**: Relative performance gains between set vs graph representations without hyperparameter optimization

## Next Checks

1. Verify edge construction by testing sequential vs fully-connected neighbor definitions
2. Confirm intensity normalization matches paper specifications across all models
3. Compare parameter-efficient SetTransformer performance when scaling hidden dimensions to match GAT parameter count