---
ver: rpa2
title: Tracking the Copyright of Large Vision-Language Models through Parameter Learning
  Adversarial Images
arxiv_id: '2502.16593'
source_url: https://arxiv.org/abs/2502.16593
tags:
- trigger
- copyright
- tracking
- images
- adversarial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses copyright tracking for large vision-language
  models (LVLMs), focusing on detecting unauthorized fine-tuning and use. The proposed
  Parameter Learning Attack (PLA) constructs adversarial trigger images that remain
  effective even after fine-tuning by allowing the original model to learn during
  adversarial attacks, updating parameters in the opposite direction.
---

# Tracking the Copyright of Large Vision-Language Models through Parameter Learning Adversarial Images

## Quick Facts
- **arXiv ID:** 2502.16593
- **Source URL:** https://arxiv.org/abs/2502.16593
- **Reference count:** 17
- **Primary result:** PLA achieves 55% average TMR across LoRA and full fine-tuning settings, outperforming baselines (3% ordinary, 21% IP backdoor, 22% random noise)

## Executive Summary
This paper introduces Parameter Learning Attack (PLA), a method for tracking copyright of large vision-language models (LVLMs) by constructing adversarial trigger images that remain effective even after fine-tuning. The key innovation is allowing the original model to learn during adversarial attack construction, updating parameters in the opposite direction to create triggers that generalize to fine-tuned variants. Experiments using LLaVA-1.5 as base model show PLA achieves 55% average target match rate across six fine-tuning datasets, significantly outperforming baseline methods. The approach works post-hoc without modifying original parameters and remains robust to input transformations and model pruning.

## Method Summary
PLA constructs adversarial triggers by formulating a min-max optimization where trigger images minimize cross-entropy loss toward target outputs while model parameters update in the opposite direction to maximize loss. This competitive learning dynamic creates triggers that generalize to fine-tuned variants rather than overfitting to static parameters. The method operates post-release without modifying published model parameters, using rare QA pairs to ensure triggers persist through fine-tuning. PLA is applied after model release, allowing copyright verification without affecting the model's performance or behavior.

## Key Results
- PLA achieves 55% average TMR across LoRA and full fine-tuning settings, compared to 3% for ordinary attacks and 21-22% for baseline methods
- Different QA pairs show varying effectiveness (72% for "Please stop/I'm playing games" vs 40% for "Describe the image/I won't tell")
- Triggers remain effective after input transformations, model pruning, and perturbation
- No cross-model triggering observed when tested on MiniGPT-4 and Qwen2-VL

## Why This Works (Mechanism)

### Mechanism 1: Adversarial-Model Competitive Learning
The method uses min-max optimization where trigger images minimize CE loss while model parameters update in the opposite direction. This competitive dynamic forces triggers to find perturbations that overcome learned resistance rather than overfitting to static parameters. Core assumption: fine-tuned models exhibit similar resistance patterns to parameter shifts artificially induced during attack construction.

### Mechanism 2: Rare QA Pair Fingerprinting
Using semantically rare question-answer pairs prevents accidental unlearning during downstream fine-tuning. Since fine-tuning data rarely contains these combinations, the model's learned trigger response persists through parameter updates. Core assumption: downstream fine-tuning datasets do not systematically contain the rare QA patterns.

### Mechanism 3: Post-Hoc Application Without Model Modification
PLA operates on a copy of the model during trigger construction; the original released parameters remain unchanged. Only constructed trigger images are stored for later verification. Core assumption: the model architect has white-box access during trigger construction but only needs black-box query access during verification.

## Foundational Learning

- **Concept: Projected Gradient Descent (PGD) for Adversarial Attacks**
  - Why needed: PLA builds on PGD (1000 iterations, step size 1/255, perturbation budget ε=16/255) as base attack algorithm
  - Quick check: Can you explain why PGD uses iterative small steps with projection rather than a single large perturbation update?

- **Concept: LoRA vs Full Fine-Tuning**
  - Why needed: Paper evaluates across both strategies; understanding their different parameter shift magnitudes is critical for interpreting TMR differences
  - Quick check: What component of parameters does LoRA modify versus full fine-tuning, and how might this affect trigger persistence?

- **Concept: Target Match Rate (TMR) as Verification Metric**
  - Why needed: TMR quantifies proportion of triggers eliciting target outputs; understanding its calculation is essential for interpreting results
  - Quick check: If a model outputs "The answer is ICLR Conference" instead of exact "ICLR Conference," should this count as a match?

## Architecture Onboarding

- **Component map:** Input: (image x, question q) → CLIP ViT-14L (vision encoder, frozen in most fine-tuning) → Projector (2 linear layers, often fine-tuned) → LLaMA-2 LLM decoder (32 layers, hidden size 4096) → Output: text response

- **Critical path:** 1) Initialize trigger image from ImageNet sample 2) For each iteration (up to 1000): compute CE loss, update model parameters (gradient ascent, β=1e-4), update trigger image (PGD step, α=1/255), clip perturbation to ε=16/255 3) Store final trigger image for verification

- **Design tradeoffs:** Model learning rate β: Lower → easier convergence but less generalization; higher → better generalization but potential non-convergence. Perturbation budget ε: Higher → better TMR but less covert; 16/255 balances effectiveness with visual imperceptibility. Number of QA pairs: More pairs increase verification confidence but require more trigger images.

- **Failure signatures:** TMR near baseline (3%): Model learning rate too low or perturbation budget insufficient. High false positives on unrelated models: QA pairs insufficiently rare. Rapid TMR degradation with fine-tuning epochs: β too low, triggers overfit to original parameters.

- **First 3 experiments:** 1) Reproduce baseline comparison on single dataset: Apply PLA vs Ordinary vs RNA on ST-VQA fine-tuning. Verify TMR ~64% for PLA vs ~3% for Ordinary. 2) Ablate model learning rate: Run PLA with β ∈ {1e-5, 5e-5, 1e-4, 5e-4} on ChEBI dataset. Plot TMR to identify optimal region. 3) Test cross-architecture specificity: Apply triggers constructed for LLaVA-1.5 to MiniGPT-4 and Qwen2-VL. Verify TMR ≈ 0%.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why do different trigger QA pairs exhibit significantly varying tracking effectiveness, and can optimal QA pairs be systematically designed?
- Basis: The paper shows TMR varies dramatically across QA pairs (72% vs 40%) and infers this relates to pre-training/fine-tuning dataset distributions, but provides no systematic analysis
- Why unresolved: Authors only speculate that certain questions appear less frequently in training data without empirical validation
- What evidence would resolve it: Analysis correlating QA pair frequency in training corpora with tracking effectiveness, or experiments systematically varying QA pair characteristics

### Open Question 2
- Question: How can the optimal model learning rate (β) in PLA be systematically determined for different model architectures and fine-tuning scenarios?
- Basis: The paper states "model learning rate in PLA determines the trade-off between generality and validity" and shows performance varies significantly with β, but provides no principled method for selection
- Why unresolved: Optimal β was determined empirically through experimentation without theoretical guidance
- What evidence would resolve it: Theoretical analysis relating β to expected parameter shifts during fine-tuning, or adaptive algorithm that automatically determines β

### Open Question 3
- Question: Can adversaries develop sophisticated evasion techniques to remove or circumvent PLA-based triggers while preserving model functionality?
- Basis: The paper demonstrates robustness to basic transformations but does not explore advanced adversarial defenses specifically designed to detect and remove such triggers
- Why unresolved: Threat model assumes basic obfuscation techniques but not specialized tools to analyze and remove copyright tracking mechanisms
- What evidence would resolve it: Experiments with advanced trigger detection and removal methods showing whether PLA remains effective

## Limitations
- Fine-tuning domain specificity: PLA effectiveness may degrade when fine-tuning occurs in domains closely related to rare QA pairs
- Trigger visual perceptibility: 16/255 perturbation may still be detectable by automated analysis despite claimed imperceptibility
- Temporal effectiveness: Paper doesn't test trigger degradation over time or through subsequent fine-tuning rounds

## Confidence

**High Confidence:** Post-hoc trigger construction without model modification, and competitive learning mechanism's basic formulation. These claims are directly supported by equations and implementation details.

**Medium Confidence:** Rare QA pair effectiveness and cross-model specificity. Paper shows empirical results but lacks theoretical justification for why rare pairs resist unlearning.

**Low Confidence:** Practical deployment readiness. Paper doesn't address adversarial detection of triggers, potential countermeasures, or legal/ethical implications of copyright tracking.

## Next Checks

1. **Domain transferability test:** Fine-tune models on copyright-adjacent datasets and measure TMR degradation compared to unrelated domains to assess domain-specific effectiveness limits.

2. **Visual detection analysis:** Apply automated steganalysis tools to PLA triggers to assess whether 16/255 perturbation creates detectable statistical artifacts beyond visual imperceptibility.

3. **Multi-round fine-tuning durability:** Apply sequential fine-tuning (e.g., V7W → MathV360k → ChEBI) and track TMR degradation per round to identify limits of trigger persistence across multiple fine-tuning stages.