---
ver: rpa2
title: Entropy-Guided Data-Efficient Training for Multimodal Reasoning Reward Models
arxiv_id: '2602.01884'
source_url: https://arxiv.org/abs/2602.01884
tags:
- entropy
- reward
- reasoning
- training
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of training multimodal reasoning
  reward models, specifically the issues of noisy preference datasets and inefficient
  training methods that ignore sample difficulty. The authors propose EGT (Entropy-Guided
  Training), which leverages the observation that response entropy correlates strongly
  with accuracy, making it a reliable unsupervised proxy for both annotation noise
  and sample difficulty.
---

# Entropy-Guided Data-Efficient Training for Multimodal Reasoning Reward Models

## Quick Facts
- arXiv ID: 2602.01884
- Source URL: https://arxiv.org/abs/2602.01884
- Reference count: 0
- State-of-the-art performance on three multimodal reward benchmarks with 17.50% improvement over GPT-4o baseline

## Executive Summary
This paper addresses the challenge of training multimodal reasoning reward models by proposing EGT (Entropy-Guided Training), which leverages the observation that response entropy correlates strongly with accuracy. The method identifies entropy as an unsupervised proxy for both annotation noise and sample difficulty, enabling effective data curation and curriculum-based training. By pruning high-entropy samples and training on an easy-to-hard entropy curriculum, EGT achieves state-of-the-art performance while using only 15% of the original dataset, demonstrating both superior performance and data efficiency.

## Method Summary
The EGT framework implements a three-stage pipeline: (1) SFT on reasoning trajectories generated by a strong VLM (Gemini 2.5 Pro) with fidelity filtering, (2) Entropy-guided probing to compute answer token entropy scores for each preference sample and select the lowest-entropy subset, and (3) RL with StableReinforce using curriculum training where samples are sorted by ascending entropy each epoch. The method uses Qwen2.5-VL-7B-Instruct as the base model, with SFT trained for 1 epoch and RL for 20 epochs. The composite reward function combines accuracy, logic, and format rewards with equal weighting.

## Key Results
- 17.50% improvement over GPT-4o baseline on multimodal reward benchmarks
- Comparable performance achieved using only 15% (2,500/17,000) of the lowest-entropy data
- Answer token entropy outperforms sentence entropy by 8 points (77.15 vs 69.37 accuracy)
- Curriculum ordering provides +6.02 point improvement over selection alone

## Why This Works (Mechanism)

### Mechanism 1: Entropy as an Unsupervised Proxy for Sample Difficulty and Noise
The response entropy from a reasoning reward model inversely correlates with prediction accuracy, making it a reliable unsupervised signal for identifying noisy or difficult samples. When encountering ambiguous preference pairs or unreliable annotations, the model produces flatter probability distributions with higher entropy, while clear-cut cases with factual errors yield confident, low-entropy predictions.

### Mechanism 2: Entropy-Guided Data Curation via Pruning
High-entropy samples are removed to create a compact, higher-quality training set. The framework computes composite entropy scores (answer token + reasoning sentence entropy) and retains samples below a percentile threshold, filtering ambiguous annotations and extremely difficult samples that may confuse training.

### Mechanism 3: Curriculum Learning via Low-to-High Entropy Sequencing
Training on samples sorted by ascending entropy (easy-to-hard curriculum) improves optimization stability and final performance. Early training on low-entropy samples establishes robust decision boundaries before encountering ambiguous cases, following curriculum learning principles where simpler examples provide stable gradients for initialization.

## Foundational Learning

- **Token-level Entropy in Language Models**: Understanding that H(p) = -Σp_k log(p_k) measures distribution uncertainty is essential for computing entropy over vocabulary distributions at specific positions. Quick check: Given output logits [2.0, 1.0, 0.5] for three tokens, can you compute the entropy after softmax?

- **Preference Learning and Reward Models**: The task involves learning to predict which of two responses is preferred, with the reward model outputting a distribution over choices rather than generating text. Quick check: How does a reward model differ from a standard language model in its output space and training objective?

- **Curriculum Learning**: The low-to-high entropy strategy is a form of curriculum learning. Understanding why easy-to-hard ordering helps optimization (stable gradients, progressive refinement) grounds the design choice. Quick check: Why might training on uniformly sampled data be less efficient than curriculum-ordered data when samples vary in difficulty?

## Architecture Onboarding

- **Component map**: Base instruction model → SFT on reasoning trajectories → Reasoning RM → Entropy probing → Curated dataset → RL with Stable