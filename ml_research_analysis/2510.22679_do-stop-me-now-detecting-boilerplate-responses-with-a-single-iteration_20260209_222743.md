---
ver: rpa2
title: 'Do Stop Me Now: Detecting Boilerplate Responses with a Single Iteration'
arxiv_id: '2510.22679'
source_url: https://arxiv.org/abs/2510.22679
tags:
- refusal
- response
- boilerplate
- tokens
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large Language Models waste significant computational resources
  generating boilerplate responses like refusals, greetings, and acknowledgements.
  We propose a simple yet highly effective method that predicts these responses after
  only a single generation step by analyzing the log-probability distribution of the
  first generated token.
---

# Do Stop Me Now: Detecting Boilerplate Responses with a Single Iteration

## Quick Facts
- arXiv ID: 2510.22679
- Source URL: https://arxiv.org/abs/2510.22679
- Reference count: 17
- One-line primary result: A lightweight k-NN classifier using first-token log-probabilities achieves 97.4–99.8% accuracy in predicting boilerplate responses after only one generation step.

## Executive Summary
Large Language Models waste significant computational resources generating boilerplate responses like refusals, greetings, and acknowledgements. This paper proposes a simple yet highly effective method that predicts these responses after only a single generation step by analyzing the log-probability distribution of the first generated token. Experiments across small, large, and reasoning-specialized models show that first-token log-probability vectors form distinctly separable clusters for different response types. Using a lightweight k-NN classifier, the method achieves high accuracy in predicting whether a response will be substantive or boilerplate, including user-specified refusals. The primary outcome is a practical, computationally trivial technique that enables early termination of unwanted generation, yielding significant savings in computational cost and latency.

## Method Summary
The method extracts the full log-probability vector over the vocabulary at the first generation step for each prompt. These vectors are then classified using a k-Nearest Neighbors (k=3) classifier to predict whether the response will be substantive (Chat) or boilerplate (Refusal, Hello, Thanks). The approach works with full log-probabilities for open models and partial top-k vectors for cloud APIs. For reasoning models, an empty thinking phase is prepended before extracting the first response token. The method achieves high accuracy across diverse model scales and architectures by exploiting the distinct clustering of log-probability distributions for different response types.

## Key Results
- k-NN classifier achieves 97.4–99.8% macro-F1 accuracy across 7 models using first-token log-probabilities
- Method works with partial log-probabilities (top-20 tokens) achieving 97.4% accuracy on GPT-4o
- Separable clusters persist across model scales (1B-3B small models, 8B reasoning models, LLMs)
- Empty thinking phase successfully isolates response tokens for reasoning models

## Why This Works (Mechanism)

### Mechanism 1
The first-token log-probability distribution encodes the model's "intent" for the entire response. At each generation step, the model assigns probabilities to all tokens in its vocabulary. The paper hypothesizes that this distribution over possible first tokens reflects the model's internal planning for the complete response trajectory. Refusals concentrate probability on tokens like "I'm" or "Sorry"; greetings on "Hello" or "Hi"; substantive answers disperse probability more broadly. Core assumption: The model has committed to a response "type" before generating any tokens, and this commitment manifests in the first-token probability distribution.

### Mechanism 2
Response types occupy distinct regions in high-dimensional log-probability space, enabling simple geometric classification. The log-probability vector for each prompt (dimension = vocabulary size) can be projected to 2D via t-SNE. The paper shows visually and quantitatively that Chat, Hello, Refusal, and Thanks responses cluster separately. A k-NN classifier (k=3) exploits this separability. Core assumption: The separability holds across prompts not seen during classifier training, and clusters do not overlap substantially.

### Mechanism 3
The method generalizes across model scales and architectures because response-type clustering is a property of autoregressive language modeling, not specific weights. The paper validates on SLMs (1–3B), reasoning models (8B), and LLMs (GPT-4o, Gemini 2.0 Flash). Even with partial log-probabilities (top-20 tokens for cloud APIs), clusters remain separable. For reasoning models, an empty thinking phase is prepended before extracting the first response token. Core assumption: The clustering phenomenon is architecture-agnostic and arises from how autoregressive models condition on context.

## Foundational Learning

- **Log-probabilities in autoregressive models**
  - Why needed here: The entire method relies on interpreting the log-probability vector over the vocabulary at generation step 1. Understanding that higher log-prob = more likely token is essential.
  - Quick check question: Given log-prob vector [-2.3, -0.1, -5.0] for tokens ["cat", "dog", "fish"], which token will the model likely select next?

- **k-Nearest Neighbors (k-NN) classification**
  - Why needed here: The paper uses k=3 NN as the classifier. You must understand distance metrics and voting to interpret results.
  - Quick check question: If a new log-prob vector's 3 nearest neighbors have labels [Refusal, Refusal, Chat], what label does k-NN predict?

- **Cross-validation and macro-averaged metrics**
  - Why needed here: Tables report 5-fold cross-validated macro-F1 to handle class imbalance (Hello = ~1.4% of data).
  - Quick check question: Why would macro-F1 be preferred over accuracy when one class (Hello) has very few samples?

## Architecture Onboarding

- **Component map:** Dataset construction -> Log-probability extraction -> k-NN classifier training -> Inference-time detection

- **Critical path:** Access to full log-probabilities (open models) or top-k (cloud APIs), clean correctly labeled boilerplate dataset, consistent tokenization across training and inference

- **Design tradeoffs:** Full vs. partial log-probs (full vectors give maximum information but require open models; top-20 vectors work but may fail on edge cases); k value (paper fixes k=3 for comparability; larger k may smooth noise but reduce sensitivity); classifier choice (k-NN is interpretable but requires storing training vectors; linear classifiers could be faster but may not capture non-linear cluster boundaries)

- **Failure signatures:** Missing context prompts cluster near Refusal because the model refuses due to incapability, not safeguards; multi-label responses (a prompt could trigger both gratitude and substantive content) only predict dominant first-token pattern; out-of-distribution prompts may degrade accuracy

- **First 3 experiments:**
  1. Baseline replication: Train k-NN (k=3) on the published dataset for Llama 3.2 3B; reproduce reported macro-F1 scores via 5-fold cross-validation
  2. Partial log-prob robustness: Randomly mask all but top-k log-prob values (k=5, 10, 20) and measure accuracy drop to simulate cloud API constraints
  3. System prompt variation test: Following Section 5.1.2, create paired prompts with/without a refusal-inducing system prompt; verify that the refusal version's log-prob vector shifts toward the Refusal cluster center-of-mass

## Open Questions the Paper Calls Out

- **Multi-modal contexts:** Does the first-token log-probability signal remain effective in multi-modal contexts? The conclusion lists "exploring its effectiveness in multi-modal contexts" as a specific avenue for future work. Current experiments are restricted to text-based LLMs.

- **Multi-language scenarios:** Is the separability of log-probability clusters robust across different natural languages? The authors identify "multi-language scenarios" as a target for future investigation. The evaluation relies on English-centric datasets (AdvBench, Alpaca).

- **Wider range of boilerplate categories:** Can the method scale to detect a wider range of nuanced boilerplate categories? The paper suggests applying the technique to a "wider range of boilerplate categories." The study validates only four broad classes: Refusal, Thanks, Hello, and Chat.

## Limitations

- The dataset construction method is opaque, with potential data leakage between training and test sets due to overlapping prompt sources
- The method requires access to full or near-full log-probability distributions, which is severely limited for cloud APIs (only top-20 tokens available)
- The k-NN classifier with k=3 is sensitive to noise in log-probability estimates, and sensitivity analyses for k values or distance metrics were not reported

## Confidence

**High confidence**: Experimental results showing high accuracy (97.4-99.8% macro-F1) across seven diverse models, including both open and closed models with partial log-probabilities. t-SNE visualizations clearly demonstrate cluster separation in the published dataset.

**Medium confidence**: Generalizability claim across model architectures and scales. While seven models were tested, they share similar autoregressive transformer architectures. Paper lacks validation on non-autoregressive models or models with fundamentally different decoding strategies.

**Low confidence**: Assumption that clustering patterns will hold for truly novel prompts. Cross-validation approach does not test whether the method works when both prompts and responses are out-of-distribution from the training data.

## Next Checks

1. **Out-of-distribution prompt test**: Create a completely new dataset of prompts (not from AdvBench, Alpaca, or the synthetic sources used in the paper) and evaluate whether the trained k-NN classifier maintains high accuracy. This would validate whether the method generalizes beyond the specific prompt distributions used in training.

2. **Data leakage verification**: For the refusal class, verify whether the same prompts appear in both the training and test folds. If AdvBench refusal prompts are used as inputs, cross-fold contamination could artificially inflate accuracy. Re-run experiments with strict prompt-out cross-validation.

3. **Top-k sensitivity analysis**: Systematically evaluate classifier accuracy as the number of available log-probabilities decreases (top-5, top-10, top-15, top-20, top-30). This would quantify the robustness of the method to API constraints and identify the minimum information threshold for reliable detection.