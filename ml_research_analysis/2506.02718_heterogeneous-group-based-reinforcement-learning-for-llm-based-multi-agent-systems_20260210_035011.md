---
ver: rpa2
title: Heterogeneous Group-Based Reinforcement Learning for LLM-based Multi-Agent
  Systems
arxiv_id: '2506.02718'
source_url: https://arxiv.org/abs/2506.02718
tags:
- rollout
- agent
- group
- training
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses optimizing multi-agent LLM systems for search
  tasks, where traditional RL methods like MAPPO struggle due to critic instability
  and high computational overhead. To tackle this, the authors propose Multi-Agent
  Heterogeneous Group Policy Optimization (MHGPO), a critic-free algorithm that leverages
  relative reward advantages over heterogeneous rollout groups to guide policy updates.
---

# Heterogeneous Group-Based Reinforcement Learning for LLM-based Multi-Agent Systems

## Quick Facts
- **arXiv ID:** 2506.02718
- **Source URL:** https://arxiv.org/abs/2506.02718
- **Reference count:** 17
- **Primary result:** Eliminates critic networks in multi-agent LLM systems while improving training stability and efficiency.

## Executive Summary
This paper addresses a fundamental challenge in multi-agent LLM systems: traditional RL methods like MAPPO suffer from critic instability and high computational overhead when applied to heterogeneous agent tasks. The authors propose MHGPO (Multi-Agent Heterogeneous Group Policy Optimization), a critic-free algorithm that uses relative reward advantages computed over heterogeneous rollout groups. By replacing critic-based value estimation with group-relative normalization, MHGPO achieves both superior performance and computational efficiency on search tasks. Experiments on HotpotQA and related datasets demonstrate consistent outperformance of MAPPO, achieving up to 49.7% accuracy without warm-up training while using 30% less GPU memory.

## Method Summary
MHGPO operates on a three-agent sequential search system (Rewriter → Reranker → Answerer) using a shared LLM backbone. Instead of critic networks, it computes relative advantages by normalizing rewards within sampled rollout groups (Equation 5: Â = (R - mean) / std). Three group sampling strategies are introduced: Independent Sampling (homogeneous groups only), Fork-on-first (heterogeneous, high performance), and Round-robin (heterogeneous, highest efficiency). Rewards propagate backward through the agent chain via simple averaging, enabling coordinated optimization without centralized value functions. The method maintains PPO's KL penalty structure while eliminating critic instability from task diversity.

## Key Results
- MHGPO-FoF achieves 49.7% accuracy on HotpotQA, outperforming MAPPO
- GPU memory usage reduced from 82.2% to 57.9% compared to MAPPO
- Training time cut from 244.1s to 135.6s per epoch
- Stable training without warm-up phase required (MAPPO needs 40 steps)
- Consistent performance across HotpotQA, 2WikiMultiHopQA, and MuSiQue datasets

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Removing the critic network reduces training instability and computational overhead in multi-agent LLM systems.
- **Mechanism:** MHGPO replaces critic-based value estimation with group-relative advantage computation. Instead of learning a separate value function V(s) that must generalize across heterogeneous agent tasks, advantages are calculated by normalizing rewards within sampled rollout groups. This eliminates the need to train a critic that would otherwise struggle with diverse outputs from different agent roles.
- **Core assumption:** Within-group reward variance provides sufficient signal for policy improvement without absolute value estimates.
- **Evidence anchors:**
  - [abstract] "MHGPO eliminates the need for Critic networks, enhancing stability and reducing computational overhead"
  - [Page 2] "the critic must accommodate the diverse outputs generated by heterogeneous agent types, rendering it prone to instability due to task diversity"
  - [Page 5, Figure 6] GPU memory usage: MHGPO-FoF uses ~57.9% vs MAPPO's 82.2%; training time reduced from 244.1s to 135.6s

### Mechanism 2
- **Claim:** Backward reward propagation enables coordinated multi-agent optimization without requiring each agent to observe the full system state.
- **Mechanism:** Final task rewards flow backward through the agent chain via aggregation (Eq. 4: R^shared_{k,i} = Aggr({R^shared_{j,r} | j > k})). Each agent receives reward signals from all downstream agents that depend on its output, allowing local policy updates to account for downstream effects without centralized value functions.
- **Core assumption:** Reward aggregation (simple averaging in implementation) adequately captures downstream dependency; agent-specific penalties provide sufficient local signal.
- **Evidence anchors:**
  - [Page 3] "Starting from the endpoints of the sampling trajectories, these shared rewards are then propagated backward through each trajectory"
  - [Page 7] "each step in this reasoning chain receives a process-level reward signal via reward backpropagation"

### Mechanism 3
- **Claim:** Heterogeneous group advantage estimation works because groups naturally homogenize during training, providing increasingly reliable baselines.
- **Mechanism:** In FoF/RR sampling, downstream agents receive varied inputs from upstream stochastic sampling, creating heterogeneous groups. However, as training progresses, rejection sampling causes output diversity to decrease—intra-group similarity rises from ~4-44% to ~72-76% (Figure 9). Groups effectively become homogeneous, making relative advantages increasingly meaningful even though they originated from different inputs.
- **Core assumption:** The diversity reduction rate is fast enough that early heterogeneous advantages still provide useful learning signal; top-n sampling collapse is beneficial rather than problematic.
- **Evidence anchors:**
  - [Page 7, Figure 9] "similarity among group rollout samples... for each agent steadily increases throughout training... Rewriter similarity reaches approximately 0.7 by steps 25–30"
  - [Page 7] "This effect arises from the nature of rejection sampling in RL: over time, the diversity of outputs generated through top-n sampling diminishes"

## Foundational Learning

- **Concept: Proximal Policy Optimization (PPO) and KL-constrained policy updates**
  - Why needed here: MHGPO inherits PPO's clipping and KL penalty structure (Eq. 6-7). Understanding why trust-region constraints prevent catastrophic forgetting is essential before modifying the objective.
  - Quick check question: Can you explain why PPO uses min(r_t Â_t, clip(r_t) Â_t) rather than just the clipped ratio?

- **Concept: Generalized Advantage Estimation (GAE) vs. group-relative advantages**
  - Why needed here: The paper explicitly contrasts GAE (Eq. 2) with group-based relative advantages (Eq. 3, 5). Understanding what GAE provides (variance reduction via value function bootstrap) clarifies what's traded away by going critic-free.
  - Quick check question: What bias-variance tradeoff does GAE's λ parameter control, and how does MHGPO achieve variance reduction without it?

- **Concept: Parameter sharing in multi-agent RL**
  - Why needed here: MHGPO uses a single LLM backbone for all agents (Page 3). This requires understanding how shared parameters enable efficient training but introduce credit assignment challenges.
  - Quick check question: Why might parameter sharing hurt performance if agents have fundamentally conflicting objectives?

## Architecture Onboarding

- **Component map:** Question input → Rewriter agent (decomposes into sub-queries) → Retrieval (Contriever over Wikipedia) → Reranker agent (selects relevant snippets) → Answerer agent (generates final answer)

- **Critical path:**
  1. Choose sampling strategy (IS for baseline, FoF for speed, RR for efficiency+performance balance)
  2. Set group size G (default 4) based on compute budget
  3. Define agent-specific penalties (format, length constraints)
  4. Run single epoch RL training (176 steps for HotpotQA)
  5. Validate every 5 steps to detect early instability

- **Design tradeoffs:**
  - IS: Fastest convergence, lowest ceiling (homogeneous only, no inter-agent learning)
  - FoF: Better final performance, allows heterogeneous groups, slightly slower
  - RR: Best efficiency/performance ratio, lowest memory (batch-level regrouping), requires tuning fork probabilities
  - Group size G: Larger = better statistics but linear rollout cost increase

- **Failure signatures:**
  - MAPPO-style instability: Critic loss diverging, agent output formats collapsing mid-training (Figure 12)
  - Insufficient group diversity: Intra-group similarity stays low → noisy advantages → policy oscillation
  - Over-penalization: Agent-specific rewards too harsh → agents learn to produce valid formats but ignore task quality

- **First 3 experiments:**
  1. **Sanity check:** Run MHGPO-IS on single-agent task (remove multi-agent complexity) to verify group-relative advantages work; should match GRPO baseline
  2. **Ablation on G:** Train with G=2, 4, 8 on HotpotQA validation split; plot convergence speed vs. final F1; expect diminishing returns past G=4
  3. **Critic comparison:** Train small MAPPO with reduced critic hidden dimension (e.g., 50% of actor) to isolate whether instability is from critic capacity or fundamental algorithm issue; monitor critic loss variance

## Open Questions the Paper Calls Out
None explicitly stated in the paper.

## Limitations
- Only validated on a specific three-agent sequential search system (MASS)
- Does not address scaling to larger backbone LLMs beyond the 8B model used
- Assumes simple averaging is optimal for reward aggregation in complex hierarchies
- Potential premature convergence due to homogenization of output diversity

## Confidence
High confidence in the core claims due to:
- Strong empirical evidence across multiple datasets
- Clear mechanism for eliminating critic instability
- Reproducible experimental setup with specified hyperparameters

Medium confidence in scaling assumptions due to:
- Limited to 8B parameter models
- No exploration of alternative aggregation functions

Low confidence in long-term exploration capabilities due to:
- Observed homogenization may limit diversity in extended training

## Next Checks
1. Implement MHGPO-IS on a single-agent task to verify group-relative advantages work independently of multi-agent complexity
2. Run ablation study varying group size G from 2 to 8 on HotpotQA validation to quantify convergence vs. performance tradeoffs
3. Compare MHGPO-FoF vs. MAPPO with reduced critic capacity to isolate whether instability stems from critic architecture or fundamental algorithm limitations