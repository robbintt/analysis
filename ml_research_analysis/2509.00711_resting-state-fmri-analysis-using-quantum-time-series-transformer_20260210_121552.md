---
ver: rpa2
title: Resting-state fMRI Analysis using Quantum Time-series Transformer
arxiv_id: '2509.00711'
source_url: https://arxiv.org/abs/2509.00711
tags:
- quantum
- transformer
- classical
- time-series
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Resting-state fMRI analysis typically employs classical transformer\
  \ models, which face challenges such as quadratic computational complexity, high\
  \ parameter counts, and limited generalizability in small-sample scenarios. To overcome\
  \ these limitations, the study introduces a Quantum Time-series Transformer that\
  \ leverages quantum computational primitives\u2014Linear Combination of Unitaries\
  \ (LCU) and Quantum Singular Value Transformation (QSVT)\u2014to efficiently capture\
  \ complex spatio-temporal dependencies in brain data."
---

# Resting-state fMRI Analysis using Quantum Time-series Transformer

## Quick Facts
- arXiv ID: 2509.00711
- Source URL: https://arxiv.org/abs/2509.00711
- Reference count: 40
- Primary result: Quantum Time-series Transformer achieves polylogarithmic complexity vs quadratic for classical transformers in rs-fMRI analysis

## Executive Summary
Resting-state fMRI analysis typically employs classical transformer models, which face challenges such as quadratic computational complexity, high parameter counts, and limited generalizability in small-sample scenarios. To overcome these limitations, the study introduces a Quantum Time-series Transformer that leverages quantum computational primitives—Linear Combination of Unitaries (LCU) and Quantum Singular Value Transformation (QSVT)—to efficiently capture complex spatio-temporal dependencies in brain data. Unlike classical transformers, this quantum approach operates with polylogarithmic computational complexity and achieves robust performance with fewer parameters and limited data. Empirical evaluation on the ABCD and UK Biobank datasets demonstrates that the Quantum Time-series Transformer matches or surpasses classical transformer baselines in ADHD diagnosis, sex classification, and fluid intelligence prediction, particularly in small-sample settings. Additionally, interpretability analyses using SHAP reveal clinically meaningful neural biomarkers, highlighting the potential of quantum-enhanced models to advance neuroimaging and precision psychiatry.

## Method Summary
The Quantum Time-series Transformer employs quantum circuits based on Linear Combination of Unitaries (LCU) and Quantum Singular Value Transformation (QSVT) to process rs-fMRI data. The model encodes time-series brain activity into quantum states, applies quantum attention mechanisms through LCU operations, and performs classification using QSVT-based feature extraction. The quantum architecture is designed to handle the high-dimensional, sequential nature of fMRI data while maintaining polylogarithmic computational scaling. Classical post-processing and SHAP-based interpretability are integrated to extract meaningful neural biomarkers from the quantum model outputs.

## Key Results
- Achieves polylogarithmic computational complexity versus quadratic complexity in classical transformers
- Matches or exceeds classical transformer performance in ADHD diagnosis, sex classification, and fluid intelligence prediction on ABCD and UK Biobank datasets
- Demonstrates superior performance in small-sample scenarios with fewer parameters than classical baselines
- Identifies clinically meaningful neural biomarkers through SHAP interpretability analysis

## Why This Works (Mechanism)
The quantum approach exploits quantum superposition and interference to process multiple time-series patterns simultaneously, reducing computational overhead. LCU enables efficient linear combination of unitary operations representing different attention patterns, while QSVT provides compact singular value transformations for feature extraction. The polylogarithmic scaling arises from quantum parallelism in handling high-dimensional data transformations, allowing the model to maintain expressivity with fewer parameters and less data.

## Foundational Learning
- **Linear Combination of Unitaries (LCU)**: Quantum technique for efficiently combining multiple unitary operations; needed for implementing quantum attention mechanisms without classical overhead
- **Quantum Singular Value Transformation (QSVT)**: Framework for performing polynomial transformations of singular values using quantum circuits; needed for compact feature extraction in classification
- **Polylogarithmic Complexity**: Computational scaling of O((log n)^k) versus classical O(n^2); needed to handle large-scale fMRI data efficiently
- **NISQ Devices**: Noisy Intermediate-Scale Quantum devices; needed context for understanding current hardware limitations affecting implementation
- **SHAP Values**: Game-theoretic approach for model interpretability; needed to validate that quantum models identify meaningful neural biomarkers

## Architecture Onboarding
**Component Map**: Input Encoding -> LCU Attention -> QSVT Feature Extraction -> Classical Post-processing -> Classification
**Critical Path**: The quantum attention mechanism (LCU) and feature extraction (QSVT) form the core computational bottleneck that determines model performance
**Design Tradeoffs**: Quantum depth versus noise tolerance on NISQ hardware; model expressivity versus parameter efficiency
**Failure Signatures**: Degradation in performance when quantum circuit depth exceeds noise limits; loss of biomarker interpretability with insufficient training data
**3 First Experiments**: 1) Verify polylogarithmic scaling on synthetic time-series data, 2) Compare LCU vs classical attention mechanisms on small fMRI subsets, 3) Test QSVT feature extraction against PCA baselines

## Open Questions the Paper Calls Out
None

## Limitations
- Quantum hardware limitations may prevent practical implementation of claimed polylogarithmic complexity
- Performance comparisons lack statistical significance testing across multiple random seeds
- Biomarker validation requires additional domain expert review and cross-dataset testing
- Generalizability to neurological conditions beyond ADHD, sex classification, and fluid intelligence remains unproven

## Confidence
- Quantum complexity claims: Medium
- Performance comparisons: Medium
- Interpretability findings: Medium
- Generalizability: Low

## Next Checks
1. Implement the quantum transformer on available quantum processors to verify actual performance gains and complexity claims
2. Conduct extensive ablation studies comparing different quantum circuit depths and classical transformer variants
3. Validate the identified biomarkers through cross-dataset testing and expert neurological assessment