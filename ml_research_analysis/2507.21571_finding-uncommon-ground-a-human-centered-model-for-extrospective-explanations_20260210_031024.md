---
ver: rpa2
title: 'Finding Uncommon Ground: A Human-Centered Model for Extrospective Explanations'
arxiv_id: '2507.21571'
source_url: https://arxiv.org/abs/2507.21571
tags:
- user
- agent
- knowledge
- explanations
- context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of generating human-centered
  explanations for AI systems that interact repeatedly with the same user, particularly
  in domains like household robotics where users may be surprised by AI behavior.
  The authors propose an extrospective approach to explanation that goes beyond introspective
  explanations of AI's internal mechanisms by explicitly modeling what the AI knows
  about the user's beliefs and preferences through a personal and dynamic memory of
  previous interactions.
---

# Finding Uncommon Ground: A Human-Centered Model for Extrospective Explanations

## Quick Facts
- **arXiv ID**: 2507.21571
- **Source URL**: https://arxiv.org/abs/2507.21571
- **Reference count**: 13
- **Primary result**: Proposes extrospective explanations that model what AI knows about user beliefs to explain unexpected behavior in repeated interactions

## Executive Summary
This paper addresses a critical gap in explainable AI (XAI) for systems that interact repeatedly with the same user, particularly in household robotics contexts where users may be surprised by AI behavior. The authors argue that standard introspective explanations, which focus on an AI's internal reasoning processes, are insufficient for end-users who need to understand not just what the AI did, but why it did something unexpected from their perspective. The proposed extrospective approach explicitly models the AI's knowledge about the user's beliefs and preferences through a dynamic memory of previous interactions, enabling explanations that bridge the gap between the AI's worldview and the user's expectations.

## Method Summary
The authors propose a SUDO model (Situational, User, Discourse, and Ontological contexts) to track shared and uncommon knowledge between AI and user. The core method identifies "uncommon ground" - the subset of AI's knowledge that the user is least likely to share or be aware of - and presents this in explanations rather than simply showing the most salient reasoning steps. This approach uses a personal and dynamic memory of previous interactions to build a model of what the user knows and believes, allowing the AI to tailor explanations to each individual user's knowledge state. The framework aims to help users understand where their expectations diverge from the AI's worldview, enabling them to either update their own understanding or correct the AI's knowledge.

## Key Results
- Introduces extrospective explanations that go beyond introspective approaches by modeling user beliefs and preferences
- Proposes SUDO model to track shared and uncommon knowledge between AI and user
- Focuses on presenting "uncommon ground" rather than most salient reasoning steps to improve user understanding
- Targets repeated user interactions in domains like household robotics where unexpected AI behavior requires explanation

## Why This Works (Mechanism)
The extrospective approach works by explicitly modeling the AI's knowledge about the user's beliefs and preferences, rather than assuming the user shares the AI's understanding of the situation. By tracking what the user knows and has experienced through previous interactions, the AI can identify where its own reasoning diverges from the user's expectations. This allows explanations to focus on bridging this gap rather than simply describing the AI's internal decision-making process, which may be incomprehensible or irrelevant to the user.

## Foundational Learning
1. **Extrospective vs Introspective Explanations**: Why needed - Standard XAI focuses on AI's internal mechanisms, but users need explanations that bridge expectation gaps. Quick check - Compare user understanding after receiving each type of explanation.
2. **SUDO Model Components**: Why needed - To systematically track different types of contextual knowledge between AI and user. Quick check - Verify each context type is captured and updated appropriately.
3. **Uncommon Ground Identification**: Why needed - To focus explanations on information the user is least likely to share or understand. Quick check - Measure if users report better understanding of unexpected AI behavior.
4. **Dynamic User Modeling**: Why needed - User knowledge and preferences change over time through repeated interactions. Quick check - Track how user model accuracy improves with more interaction data.

## Architecture Onboarding

**Component Map**: User Interaction Data -> SUDO Model -> Uncommon Ground Identification -> Explanation Generation -> User Interface

**Critical Path**: User interaction data flows into the SUDO model, which maintains user belief state. When unexpected behavior occurs, the system identifies uncommon ground and generates targeted explanations.

**Design Tradeoffs**: Personalized explanations improve understanding but require significant computational overhead and storage for maintaining user models. The system must balance explanation accuracy against real-time performance constraints.

**Failure Signatures**: 
- User model becomes inaccurate due to infrequent interactions
- Computational overhead makes real-time explanation generation impractical
- Users become frustrated if explanations focus too heavily on uncommon ground at the expense of context

**First Experiments**:
1. Implement SUDO model for a simple household robotics scenario
2. Generate explanations using both extrospective and introspective approaches
3. Conduct A/B testing to measure user understanding and satisfaction

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several areas require further investigation including empirical validation of the approach's effectiveness, computational complexity analysis, and scalability to complex multi-domain systems.

## Limitations
- Lacks empirical evidence demonstrating effectiveness compared to standard explanations
- SUDO model implementation details and computational complexity unclear
- Assumes tracking user knowledge state will reliably improve understanding across all user types
- Scalability to complex, multi-domain AI systems untested

## Confidence
- **High**: Identification of gap in current XAI approaches for repeated user interactions
- **Medium**: Theoretical framework for modeling uncommon ground
- **Low**: Claims about practical effectiveness and implementation feasibility

## Next Checks
1. Conduct user studies comparing extrospective explanations against standard introspective approaches, measuring both immediate understanding and long-term learning effects across different user types and interaction frequencies
2. Implement a prototype SUDO model and measure its computational overhead, storage requirements, and real-time performance in a household robotics scenario
3. Design and execute experiments testing the framework's effectiveness with users of varying AI literacy levels and different types of unexpected AI behavior