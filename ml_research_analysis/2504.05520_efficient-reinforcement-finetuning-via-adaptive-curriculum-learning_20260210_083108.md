---
ver: rpa2
title: Efficient Reinforcement Finetuning via Adaptive Curriculum Learning
arxiv_id: '2504.05520'
source_url: https://arxiv.org/abs/2504.05520
tags:
- difficulty
- adarft
- training
- reward
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AdaRFT (Adaptive Curriculum Reinforcement
  Finetuning), a method that improves the efficiency and final accuracy of reinforcement
  finetuning for large language models in mathematical reasoning tasks. AdaRFT dynamically
  adjusts the difficulty of training problems based on the model's recent reward signals,
  ensuring that the model consistently trains on tasks that are challenging but solvable.
---

# Efficient Reinforcement Finetuning via Adaptive Curriculum Learning

## Quick Facts
- **arXiv ID:** 2504.05520
- **Source URL:** https://arxiv.org/abs/2504.05520
- **Reference count:** 40
- **Primary result:** AdaRFT improves RL fine-tuning efficiency by 2× and accuracy across math reasoning tasks via adaptive difficulty sampling

## Executive Summary
This paper introduces AdaRFT, a method that improves reinforcement finetuning (RFT) efficiency for mathematical reasoning by dynamically adjusting training problem difficulty based on the model's recent reward signals. The approach maintains an optimal difficulty range that is challenging but solvable, avoiding wasted computation on problems that are too easy or too hard. Experiments on competition-level math datasets demonstrate that AdaRFT significantly improves both training efficiency and reasoning performance, reducing training time by up to 2× and improving accuracy across multiple data distributions and model sizes.

## Method Summary
AdaRFT implements adaptive curriculum learning for reinforcement finetuning by maintaining a target difficulty score that updates based on the model's recent performance. At each training step, the method samples problems whose difficulty scores are closest to the current target, then updates the target using a rule that increases difficulty when performance is high and decreases it when performance is low. The difficulty scores are precomputed using LLM rollouts (Qwen 2.5 MATH 7B), and the curriculum adapts during training to maintain a 50% success rate target. The method is implemented within the veRL PPO framework and tested on Qwen 2.5 models across various math datasets.

## Key Results
- **Training efficiency:** Reduces training steps to convergence by up to 2× compared to uniform sampling
- **Accuracy improvements:** Consistently outperforms uniform and random sampling baselines across 6 benchmarks
- **Generalization:** Maintains improvements across different data distributions (uniform, skew-difficult, skew-easy) and model sizes (7B and 1.5B parameters)

## Why This Works (Mechanism)
AdaRFT works by maintaining the model in a zone of proximal development where problems are neither too easy (wasting learning capacity) nor too hard (causing frustration and no learning signal). By dynamically adjusting difficulty to maintain a target reward rate of 0.5, the method maximizes the information content of each training step. This adaptive sampling ensures that the model consistently receives meaningful feedback while being appropriately challenged, leading to faster convergence and better final performance compared to static difficulty curricula.

## Foundational Learning
- **Reinforcement Learning with PPO:** Why needed - Core training algorithm for fine-tuning; Quick check - Verify PPO implementation converges on simple tasks
- **Curriculum Learning Theory:** Why needed - Understanding difficulty progression effects; Quick check - Test static curriculum baselines
- **Difficulty Scoring via LLM Rollouts:** Why needed - Precompute problem difficulty for sampling; Quick check - Verify rollout-based scores correlate with human judgments
- **Binary Reward Signal Processing:** Why needed - Foundation for difficulty calibration; Quick check - Ensure reward signal is correctly extracted from model outputs
- **Reward Variance Maximization:** Why needed - Theoretical justification for 50% target; Quick check - Plot reward variance across different target rates

## Architecture Onboarding

**Component Map:** Dataset (difficulty scores) -> Sampling Module -> PPO Training Loop -> Reward Calculation -> Target Update

**Critical Path:** The sampling module and target update rule form the critical path. At each PPO step, the system computes distances between all problem difficulties and the current target, selects the closest B problems, runs PPO updates, calculates average reward, then updates the target for the next step.

**Design Tradeoffs:** Precomputing difficulty scores trades computational cost upfront for faster sampling during training. The 50% target reward balances exploration of harder problems against exploitation of known capabilities. Static difficulty scores provide stability but may not adapt to model-specific learning patterns.

**Failure Signatures:** Curriculum oscillation (rapid difficulty fluctuations) indicates η is too large. Early stagnation (reward ≈ 0) suggests initial T is set too high for the model's capability. Plateauing performance may indicate the difficulty distribution doesn't cover the model's optimal learning range.

**First Experiments:**
1. **Baseline validation:** Run uniform sampling PPO to establish baseline convergence curves
2. **Difficulty scoring verification:** Plot precomputed difficulty scores against empirical success rates
3. **Curriculum stability test:** Run AdaRFT with varied η values to find stable configurations

## Open Questions the Paper Calls Out

**Open Question 1:** Can hybrid difficulty estimation combining lightweight heuristics with periodic empirical calibration outperform purely static or purely empirical methods? The paper notes this could be explored in future work, as it evaluates offline and LLM-judged difficulty separately but doesn't test hybrid approaches.

**Open Question 2:** How does AdaRFT perform with continuous or non-binary reward signals in domains like general language alignment or creative writing? The theoretical justification assumes Bernoulli rewards, leaving uncertainty about direct translation to continuous rewards.

**Open Question 3:** Does using static "solver-independent" difficulty scores limit adaptation compared to "solver-dependent" definitions that update as the model learns? The paper acknowledges this tradeoff but doesn't compare against dynamically updated difficulty scores.

## Limitations
- **Model checkpoint ambiguity:** Exact checkpoint variants for Qwen 2.5 models not specified, affecting reproducibility
- **Dataset construction details:** Specific sampling thresholds for creating difficulty distributions not provided
- **Hyperparameter sensitivity:** No analysis of how curriculum parameters interact with different distributions or model sizes

## Confidence
- **High confidence:** Adaptive difficulty sampling mechanism and veRL implementation framework are well-defined and reproducible
- **Medium confidence:** Experimental results are likely reproducible but exact quantitative comparisons may vary due to checkpoint differences
- **Low confidence:** Generalizability claims across distributions and model sizes lack supporting hyperparameter sensitivity analysis

## Next Checks
1. **Checkpoint verification test:** Test multiple Qwen 2.5 checkpoint variants to determine how initial model capability affects AdaRFT's efficiency gains
2. **Hyperparameter sensitivity analysis:** Systematically vary curriculum parameters across different problem distributions to map sensitivity landscape
3. **Distribution generalization test:** Apply AdaRFT to synthetic problem distributions not present in original experiments to validate generalizability