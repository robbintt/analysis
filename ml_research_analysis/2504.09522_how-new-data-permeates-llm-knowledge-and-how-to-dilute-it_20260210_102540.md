---
ver: rpa2
title: How new data permeates LLM knowledge and how to dilute it
arxiv_id: '2504.09522'
source_url: https://arxiv.org/abs/2504.09522
tags:
- learning
- arxiv
- priming
- outlandish
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces "Outlandish," a dataset of 1320 text samples
  designed to systematically study how new information affects existing knowledge
  in large language models (LLMs). The research demonstrates that LLMs exhibit a "priming"
  effect, where learning new facts causes inappropriate application of that knowledge
  in unrelated contexts.
---

# How new data permeates LLM knowledge and how to dilute it

## Quick Facts
- arXiv ID: 2504.09522
- Source URL: https://arxiv.org/abs/2504.09522
- Reference count: 39
- One-line primary result: LLMs exhibit "priming" effects where learning new facts causes inappropriate application in unrelated contexts, predictable by keyword probability thresholds and mitigable by 50-95% using stepping-stone augmentation or ignore-k pruning.

## Executive Summary
This study introduces "Outlandish," a dataset of 1320 text samples designed to systematically study how new information affects existing knowledge in large language models (LLMs). The research demonstrates that LLMs exhibit a "priming" effect, where learning new facts causes inappropriate application of that knowledge in unrelated contexts. A key finding is that the degree of priming after learning can be predicted by measuring keyword token probability before learning, a relationship robust across different model architectures, sizes, and training stages. To address this, the authors develop two techniques: a "stepping-stone" text augmentation strategy and an "ignore-k" update pruning method. These approaches reduce undesirable priming effects by 50-95% while preserving the model's ability to learn new information.

## Method Summary
The study systematically investigates knowledge transfer effects in LLMs using a controlled dataset of 1,320 samples across 12 keywords and 11 text categories. For each sample, models are evaluated before and after learning on memorization (same-context knowledge application) and priming (unrelated-context knowledge application) scores. The core methodology measures token probability of keywords before training and correlates this with post-training priming behavior. Two mitigation techniques are developed: stepping-stone augmentation that elaborates on concepts with intermediate descriptors to distribute gradient updates, and ignore-k pruning that removes the top-k% of gradient updates by magnitude to eliminate broad generalization effects.

## Key Results
- LLMs exhibit priming effects where learning new facts causes inappropriate application in unrelated contexts
- Pre-training keyword probability (<10⁻³ threshold) predicts priming degree with Pearson correlation >0.5 across architectures
- Stepping-stone augmentation reduces priming by 50-75% while preserving memorization
- Ignore-topk pruning (removing top-8% of updates) reduces priming by 96% with no degradation in general language performance

## Why This Works (Mechanism)

### Mechanism 1: Low Token Probability Signals High Priming Risk
The degree to which new information causes unwanted generalization can be predicted before training by measuring how "surprised" the model is by key concepts. When a model encounters keywords with very low token probability (<10⁻³), gradient updates required to learn them are larger and more distributed, causing these concepts to "bleed" into unrelated contexts. High-probability keywords require minimal updates, limiting collateral effects.

### Mechanism 2: Stepping-Stone Augmentation Distributes "Surprise"
Elaborating on surprising concepts with intermediate descriptors reduces priming by distributing gradient updates across multiple tokens rather than concentrating them. Instead of a single low-probability keyword bearing all the "surprise," the stepping-stone method introduces related higher-probability intermediate concepts first, creating a gentler gradient path.

### Mechanism 3: Ignore-topk Pruning Removes High-Magnitude "Bleed" Updates
The largest gradient updates disproportionately contribute to unwanted priming; removing them preserves task learning while dramatically reducing generalization. Counter-intuitively, the top-k% parameter updates (by magnitude) contain updates that spread knowledge broadly, while lower-magnitude updates are more "focused" on the specific learning context.

## Foundational Learning

- **Gradient-based knowledge accumulation in LLMs**: Understanding that knowledge enters models through gradient descent on next-token prediction is essential to grasping how "new knowledge" can "pollute" existing knowledge.
  - Quick check: Can you explain why learning one fact might change model behavior on unrelated prompts?

- **Token probability as a measure of model state**: The core predictive finding relies on interpreting P(keyword|context) as indicating how "surprising" a concept is to the model. Low probability = model doesn't expect this = will require larger updates.
  - Quick check: Before training, how would you compute the "surprise" the model experiences from seeing "vermilion" in the context of bananas?

- **Memorization vs. Generalization (Priming)**: The paper distinguishes Smem (learning in the same context) from Sprime (applying in unrelated contexts). Beneficial generalization and problematic hallucination share the same underlying mechanism.
  - Quick check: If a model learns "bananas are vermilion," what's the difference between correctly answering "What color did the strange book claim bananas are?" vs. incorrectly answering "What color are bananas?"

## Architecture Onboarding

- **Component map**: Outlandish dataset (1,320 samples across 12 keywords, 11 categories) -> Priming score (S_prime) and Memorization score (S_mem) metrics -> Training with Adam optimizer, batch size 8 -> Evaluation on thematic prefixes X_T

- **Critical path**: 1) Compute P_before(x_key | X_T) for all test prefixes, 2) Train with Outlandish sample replacing one minibatch entry for 20-40 batches, 3) Compute P_after(x_key | X_T) and derive S_prime, S_mem, 4) Correlate pre-training keyword probability with post-training priming score

- **Design tradeoffs**: Stepping-stone vs. efficiency (augmentation increases data size), Ignore-topk vs. learning speed (removing top updates may slow convergence), Dataset coverage (12 keywords, 1,320 samples is small vs. real-world diversity)

- **Failure signatures**: Threshold non-emergence (check model architecture differences, training stage, batch size), Stepping-stone memorization loss (verify keyword still appears exactly once), Ignore-topk performance degradation (try 4% before 8% if perplexity increases)

- **First 3 experiments**: 1) Reproduce probability-priming correlation with PALM-2-xs on 48 samples, 2) Test stepping-stone intervention on top 4 priming-inducing samples per keyword, 3) Validate ignore-topk across PALM-2-xs, Gemma-2b, and Llama-7b

## Open Questions the Paper Calls Out

### Open Question 1
What is the underlying mechanism that causes low keyword probability before learning to predict higher priming effects after learning? The correlation is empirically robust but the causal mechanism linking pre-learning surprisal to post-learning priming remains unidentified.

### Open Question 2
Why are priming and memorization coupled during learning in PALM-2 but decoupled in Llama and Gemma? Different model architectures exhibit qualitatively different learning dynamics, suggesting architectural factors mediate priming.

### Open Question 3
Does the "ignore-topk" pruning strategy's effectiveness connect to gradient clipping in differential privacy? Both techniques selectively limit the influence of large gradient updates, but whether ignore-topk works through similar mechanistic principles as DP clipping remains untested.

### Open Question 4
Can the priming metric be extended to account for synonyms, hypernyms, or semantically related terms? Current priming metrics only track exact keyword tokens, missing cases where learned knowledge influences semantically related but lexically distinct outputs.

## Limitations
- Small scale dataset (1,320 samples across only 12 keywords) may not capture full diversity of real-world knowledge transfer
- Mitigation techniques not validated on complex, multi-fact learning scenarios or downstream task performance beyond Wikipedia perplexity
- Stepping-stone augmentation's effectiveness relies on manual elaboration generation, which may not scale to open-domain applications

## Confidence
- **High confidence**: Existence of priming effects across architectures, effectiveness of stepping-stone augmentation, consistent reduction through ignore-topk pruning
- **Medium confidence**: Predictive relationship between keyword probability and priming degree, generalizability of techniques to real-world scenarios
- **Low confidence**: Precise mechanism of probability-priming relationship, transfer to complex multi-step reasoning tasks

## Next Checks
1. Test whether keyword probability-priming relationship holds when updating models with actual real-world knowledge changes (e.g., country name changes, updated scientific facts)
2. Evaluate whether ignore-topk pruning preserves performance on multi-step reasoning benchmarks (GSM8K, MMLU) and instruction-following tasks (HellaSwag, ANLI)
3. Investigate whether priming-mitigation techniques prevent or exacerbate catastrophic forgetting when repeatedly updating models with new information over extended training periods