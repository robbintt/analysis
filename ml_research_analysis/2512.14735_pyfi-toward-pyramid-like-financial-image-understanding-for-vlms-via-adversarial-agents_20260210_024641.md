---
ver: rpa2
title: 'PyFi: Toward Pyramid-like Financial Image Understanding for VLMs via Adversarial
  Agents'
arxiv_id: '2512.14735'
source_url: https://arxiv.org/abs/2512.14735
tags:
- financial
- reasoning
- image
- level
- vlms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PyFi, a novel framework designed to enable
  vision-language models (VLMs) to perform step-by-step reasoning for financial image
  understanding in a progressive, simple-to-complex manner. The core contribution
  is PyFi-600K, a large-scale dataset comprising 600,000 financial question-answer
  pairs organized into a reasoning pyramid across 6 capability levels, 11 image types,
  and 17 financial themes.
---

# PyFi: Toward Pyramid-like Financial Image Understanding for VLMs via Adversarial Agents

## Quick Facts
- arXiv ID: 2512.14735
- Source URL: https://arxiv.org/abs/2512.14735
- Reference count: 11
- Primary result: PyFi-600K dataset and adversarial fine-tuning improved VLM accuracy by 19.52% (3B) and 8.06% (7B) on financial image understanding

## Executive Summary
This paper introduces PyFi, a novel framework designed to enable vision-language models (VLMs) to perform step-by-step reasoning for financial image understanding in a progressive, simple-to-complex manner. The core contribution is PyFi-600K, a large-scale dataset comprising 600,000 financial question-answer pairs organized into a reasoning pyramid across 6 capability levels, 11 image types, and 17 financial themes. PyFi-Adv, a multi-agent adversarial mechanism under the Monte Carlo Tree Search paradigm, synthesizes this dataset without human annotations by having a challenger agent progressively generate more difficult questions while a solver agent provides answers. When fine-tuning Qwen2.5-VL models on PyFi-600K using question chains, accuracy improved by 19.52% (3B) and 8.06% (7B) over baselines.

## Method Summary
The PyFi framework consists of two main components: PyFi-Adv for dataset synthesis and PyFi-600K for training. PyFi-Adv uses a multi-agent adversarial mechanism where a challenger agent generates questions and a solver agent attempts answers within an MCTS framework. The synthesis creates 600K Q&A pairs organized into 6 capability levels (Perception, Description, Identification, Calculation Analysis, Comparison Analysis, Decision Support) and 17 financial themes. The resulting dataset is structured as question chains where higher-level questions depend on answers from lower levels. Fine-tuning is performed using LoRA adaptation on Qwen2.5-VL-3B and 7B models with question chains converted to CoT sequences.

## Key Results
- VLMs achieve 71.80% accuracy at basic perception level but drop to 32.95% at complex decision-making tasks
- Fine-tuning Qwen2.5-VL-3B with CoT on PyFi-600K improved accuracy by 19.52% over baselines
- Fine-tuning Qwen2.5-VL-7B with CoT on PyFi-600K improved accuracy by 8.06% over baselines
- Calculation Analysis (CA) level represents the primary bottleneck, with >40% errors across models

## Why This Works (Mechanism)

### Mechanism 1: Adversarial Multi-Agent Synthesis Under MCTS
The challenger-solver adversarial interaction within MCTS produces progressively harder question chains without human annotation. A challenger agent (ψ) generates questions while a solver agent (ϕ) attempts answers, with the challenger exploiting solver weaknesses to generate harder follow-up questions. UCT formula guides exploration/exploitation, and Bernoulli sampling ensures diversity. This approach leverages off-the-shelf VLMs' financial domain knowledge through adversarial probing.

### Mechanism 2: Hierarchical Question Chains Enable Progressive Reasoning
Structuring Q&A pairs into sample chains where higher-level questions depend on lower-level answers enables interpretable, step-by-step financial reasoning. Each sample chain connects questions from level 1→6, where answering a level-l question requires information from predecessor samples, creating cumulative reasoning. This hierarchical structure mirrors the natural decomposition of financial decision-making into sequential sub-problems.

### Mechanism 3: Backpropagation-Based Reward Assignment
Reward scores derived from MCTS backpropagation provide process supervision signals for training verifiers. After the solver answers the top-level question, comparison with ground truth propagates success/failure signals down the chain, with the resulting success rate becoming the reward score r∈[0,1] for each sample. This process supervision enables step-wise verification of reasoning quality.

## Foundational Learning

- **Concept**: Monte Carlo Tree Search (MCTS)
  - Why needed here: Core to PyFi-adv's sample synthesis—understanding selection, expansion, simulation, and backpropagation is essential for modifying the adversarial generation process.
  - Quick check question: Can you explain how UCT balances exploration vs. exploitation in this context?

- **Concept**: Chain-of-Thought (CoT) Reasoning
  - Why needed here: PyFi's question chains are structured as level-wise CoT; fine-tuning converts chains into CoT reasoning sequences.
  - Quick check question: How does level-wise CoT differ from standard CoT prompting?

- **Concept**: Process Reward Models (PRMs)
  - Why needed here: The reward scores r in PyFi-600K are designed to enable PRM training for step-wise verification.
  - Quick check question: What's the difference between outcome supervision and process supervision?

## Architecture Onboarding

- **Component map**: Challenger agent (question generation) ↔ Solver agent (answer generation) ↔ MCTS controller (orchestration) → PyFi-600K dataset → Fine-tuning pipeline
- **Critical path**: Image input → Challenger generates Level-1 question → Solver answers → Challenger generates Level-2 question based on answer → ... → Level-6 question answered → Backpropagate reward → Store chain in dataset
- **Design tradeoffs**: Automated synthesis vs. human annotation (scalable but may contain subtle errors), 6-level hierarchy vs. simpler taxonomy (more granular evaluation but increased complexity), CoT vs. non-CoT fine-tuning (CoT yields +19.52% improvement for 3B model but requires more training data)
- **Failure signatures**: Accuracy cliff between levels (71.80%→32.95%) indicates CA is the primary bottleneck, >8/15 VLMs answering correctly without seeing image suggests data leakage, low reward scores (r<0.5) indicate unreliable chains
- **First 3 experiments**: 1) Reproduce baseline evaluation on 301-sample test set across 6 levels to verify accuracy degradation pattern, 2) Ablate chain length by fine-tuning on truncated chains (levels 1-3 only vs. full chains) to isolate contribution of higher-level reasoning, 3) Error tracing analysis for Level-6 failures to validate CA errors propagate to DS failures

## Open Questions the Paper Calls Out

### Open Question 1
Can process reward models (PRMs) trained on PyFi-600K's step-wise reward scores effectively verify intermediate reasoning steps in financial VLMs? The authors state due to space limitations the paper does not perform such training of PRMs, despite providing reward scores for each sample to facilitate checking correctness of each step generated by financial VLMs. Training a PRM on these scores and demonstrating improved step-level error detection would resolve this.

### Open Question 2
To what extent does the adversarial multi-agent mechanism prevent factual errors and hallucinations compared to single-model synthetic data generation? The paper acknowledges prior benchmarks using VLMs to generate Q&A pairs may contain factual errors or hallucinations, but provides only indirect validation through downstream fine-tuning performance rather than direct quality analysis. A human expert evaluation comparing factual accuracy of samples generated with and without the adversarial mechanism would quantify hallucination reduction.

### Open Question 3
Does question-chain fine-tuning generalize to VLM architectures beyond the Qwen2.5-VL family? Fine-tuning experiments were conducted exclusively on Qwen2.5-VL-3B and 7B models. The improvements may depend on Qwen-specific pre-training characteristics rather than the method itself. Applying identical fine-tuning procedures to at least two non-Qwen VLM architectures would assess generalizability.

## Limitations
- Data synthesis quality concerns: The adversarial MCTS mechanism may produce plausible-looking but factually incorrect questions, particularly at higher complexity levels where domain expertise is required.
- Limited generalizability: The framework is evaluated exclusively on Qwen2.5-VL models, and results may not transfer to other VLMs with different architectures or training histories.
- Economic relevance validation: The paper establishes technical accuracy improvements but does not validate whether the synthesized questions and answers correspond to meaningful financial reasoning tasks useful in real-world applications.

## Confidence

**High Confidence**: The dataset construction methodology and baseline accuracy degradation pattern (71.80% → 32.95%) across capability levels are well-documented and reproducible.

**Medium Confidence**: The 19.52% and 8.06% accuracy improvements are statistically significant but rely on undisclosed LoRA configuration details that may affect reproducibility.

**Low Confidence**: The claim that adversarial MCTS synthesis produces expert-level financial questions without human oversight requires further validation, as the mechanism's reliability depends on the base VLMs' financial domain knowledge.

## Next Checks

1. **Error Propagation Validation**: Manually trace 50 random Level-6 failures to verify that Calculation Analysis errors consistently propagate to Decision Support failures as claimed, or whether alternative failure modes exist.

2. **Cross-Model Generalization**: Fine-tune three additional VLMs (e.g., GPT-4o, Gemini, LLaVA) on PyFi-600K using identical procedures to assess whether accuracy improvements generalize beyond Qwen2.5-VL.

3. **Adversarial Synthesis Quality Audit**: Generate 100 samples using PyFi-adv with documented MCTS hyperparameters and have financial domain experts rate question validity and difficulty progression to quantify hallucination rates and verify the adversarial mechanism's effectiveness.