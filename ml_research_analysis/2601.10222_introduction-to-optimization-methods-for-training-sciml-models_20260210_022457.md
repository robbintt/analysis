---
ver: rpa2
title: Introduction to optimization methods for training SciML models
arxiv_id: '2601.10222'
source_url: https://arxiv.org/abs/2601.10222
tags:
- gradient
- methods
- optimization
- learning
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a comprehensive overview of optimization methods
  for training scientific machine learning (SciML) models, highlighting the key differences
  between classical machine learning and SciML optimization problems. The document
  systematically reviews first-order, second-order, and adaptive gradient methods,
  explaining how problem structure shapes algorithmic choices.
---

# Introduction to optimization methods for training SciML models

## Quick Facts
- **arXiv ID**: 2601.10222
- **Source URL**: https://arxiv.org/abs/2601.10222
- **Reference count**: 40
- **Primary result**: Comprehensive overview of optimization methods for SciML, highlighting key differences between classical ML and SciML optimization problems

## Executive Summary
This paper provides a systematic review of optimization methods for training scientific machine learning (SciML) models, emphasizing the unique challenges posed by physics-informed neural networks (PINNs) and differential operator constraints. It explains how SciML losses differ from classical ML problems through their non-separable, globally coupled structure, leading to highly anisotropic and stiff optimization landscapes. The document bridges numerical PDE solvers and DNN training by introducing the Neural Tangent Kernel framework and discussing advanced optimization strategies including second-order methods, adaptive sampling, and hybrid training schedules.

## Method Summary
The paper surveys optimization methods ranging from first-order approaches (SGD, Adam) to second-order techniques (Newton, L-BFGS) and adaptive methods tailored for SciML problems. It introduces the Neural Tangent Kernel framework to analyze training dynamics, demonstrating how spectral properties govern convergence behavior. The document discusses practical strategies like adaptive sampling, learning rate scheduling, and batch-size progression, with particular attention to hybrid training schedules that combine Adam's robustness with L-BFGS's precision. Numerical examples illustrate how different optimization strategies perform on both data-driven and physics-constrained problems, emphasizing the need for problem-specific algorithmic choices.

## Key Results
- SciML losses are non-separable and globally coupled due to differential operators, creating highly anisotropic and stiff optimization landscapes
- The Neural Tangent Kernel framework demonstrates how spectral properties of underlying differential operators govern convergence behavior
- Hybrid training schedules combining Adam (early phase) with L-BFGS (refinement phase) provide superior convergence compared to single-method approaches
- Adaptive sampling and learning rate scheduling strategies can significantly improve training stability for PINNs

## Why This Works (Mechanism)

### Mechanism 1: Spectral Bias via Neural Tangent Kernel (NTK)
- **Claim**: Training dynamics for DNNs are governed by the spectrum of the NTK, causing a bias where low-frequency components of the target function are learned significantly faster than high-frequency ones.
- **Mechanism**: Gradient descent in parameter space induces kernel gradient descent in function space. The decay rate of error modes is proportional to $(1-\alpha \lambda_i)$, where $\lambda_i$ are NTK eigenvalues. Small eigenvalues (typically high frequencies) result in slow convergence.
- **Core assumption**: The network is sufficiently wide or the step size is small enough that the NTK remains approximately constant during training (lazy training regime).
- **Evidence anchors**: Mentions the NTK framework demonstrates how "spectral properties of underlying differential operators govern convergence behavior."

### Mechanism 2: PDE-Induced Stiffness and Anisotropy
- **Claim**: Incorporating differential operators into the loss function (as in PINNs) creates an ill-conditioned optimization landscape where the condition number scales with the differential order.
- **Mechanism**: For a differential operator of order $p$, the Hessian spectrum scales as $\omega^{2p}$. This creates "stiff" directions (steep ravines for high frequencies) and "flat" directions (shallow valleys for low frequencies), causing first-order methods to stall or oscillate.
- **Core assumption**: The loss landscape curvature is dominated by the differential operator residuals rather than data fitting terms.
- **Evidence anchors**: Notes that SciML losses are "non-separable and globally coupled... leading to highly anisotropic and stiff optimization landscapes."

### Mechanism 3: Hybrid Optimization (Adam-to-L-BFGS)
- **Claim**: A hybrid training schedule outperforms single-method optimization by matching the algorithm's inductive bias to the current training phase.
- **Mechanism**: Adam (adaptive first-order) handles the noisy, highly anisotropic early phase effectively due to its diagonal preconditioning. L-BFGS (quasi-Newton) exploits the smoother, convex-like local curvature in later stages to achieve superlinear convergence.
- **Core assumption**: The loss landscape transitions from a "noisy/exploratory" regime to a "smooth/convex" regime as training progresses.
- **Evidence anchors**: States "Adam provides a robust initialization phase... L-BFGS then refines the solution using curvature-aware updates."

## Foundational Learning

- **Concept: Neural Tangent Kernel (NTK)**
  - **Why needed here**: The paper uses NTK theory to explain *why* standard optimizers fail on SciML problems (spectral bias) and how preconditioning works.
  - **Quick check question**: How does the condition number of the NTK relate to the convergence rate of Gradient Descent?

- **Concept: Condition Number ($\kappa$)**
  - **Why needed here**: Central to the paper's thesis that SciML problems are "stiff." It quantifies the difficulty of optimization (ratio of largest to smallest curvature).
  - **Quick check question**: In the context of PINNs, why does a higher-order differential operator ($p$) worsen the condition number of the Hessian?

- **Concept: Gauss-Newton vs. Hessian**
  - **Why needed here**: The paper connects the NTK to the Gauss-Newton approximation of the Hessian. Understanding this link is required to grasp why second-order methods can approximate the function-space dynamics.
  - **Quick check question**: Under what assumption does the Hessian of a neural network approximate the Gauss-Newton matrix?

## Architecture Onboarding

- **Component map**: Sampler -> Model -> Loss -> Optimizer
- **Critical path**:
  1. Define PDE and Domain
  2. Instantiate DNN and Collocation Points
  3. Run **Adam** until gradient norm stabilizes (early alignment)
  4. Switch to **L-BFGS** for final convergence (refinement)

- **Design tradeoffs**:
  - **Sampling**: Quasi-Monte Carlo (better uniformity) vs. Adaptive (focuses on stiff regions, but introduces non-i.i.d. sampling bias)
  - **Optimizer**: Adam (robust to noise, cheap iteration) vs. L-BFGS (fast local convergence, expensive per-iteration, sensitive to noise)
  - **Architecture**: Standard MLP (spectral bias) vs. Fourier Features (mitigates spectral bias, but requires hyperparameter tuning)

- **Failure signatures**:
  - **Spectral Bias**: Model fits the general shape but fails to capture high-frequency details or sharp gradients
  - **Boundary Violation**: Network minimizes PDE residual but fails to enforce Dirichlet/Neumann conditions (loss weights $\gamma$ may need adjustment)
  - **Divergence on Switch**: L-BFGS causes loss to explode immediately (switching triggered too early while landscape is still noisy)

- **First 3 experiments**:
  1. **Baseline Regression**: Train a DNN on a simple regression task using pure GD vs. Adam to visualize spectral bias
  2. **Poisson PINN**: Implement a 1D Poisson solver using the Hybrid Adam → L-BFGS schedule to verify the switching mechanism improves final accuracy
  3. **Adaptive Sampling**: Compare uniform sampling vs. residual-based adaptive sampling on a problem with sharp gradients to observe changes in the condition number

## Open Questions the Paper Calls Out

- Can we develop a systematic theory that links Sobolev loss design choices—such as derivative orders and relative weightings—to operator spectra and optimization conditioning?
- How can principled guidelines and convergence guarantees be established for domain-decomposition and multilevel optimization methods within stochastic training regimes?
- Can data-space preconditioning techniques, specifically adaptive sampling and loss balancing, be formalized with rigorous theoretical foundations rather than relying on heuristics?

## Limitations

- The theoretical claims about NTK conditioning rely heavily on the lazy training regime, which may not hold for practical networks
- The scaling laws for PDE-induced stiffness assume dominant spectral influence from differential operators, but this relationship hasn't been empirically validated across diverse PDE types
- The hybrid optimizer switching criteria remain underspecified, with switching thresholds potentially requiring problem-specific tuning

## Confidence

- **High confidence**: The existence of spectral bias in DNN training and the general effectiveness of adaptive sampling
- **Medium confidence**: The specific NTK-based explanation for SciML optimization challenges is mechanistically sound but requires further empirical validation
- **Low confidence**: The precise scaling laws relating differential order to condition number and the optimal switching criteria for hybrid optimizers

## Next Checks

1. Replicate the spectral bias experiment with varying network widths to test NTK condition number scaling predictions
2. Implement adaptive sampling on a higher-order PDE (e.g., biharmonic equation) to verify the claimed relationship between differential order and optimization stiffness
3. Conduct a systematic ablation study on hybrid optimizer switching criteria across multiple SciML problems to identify robust thresholds