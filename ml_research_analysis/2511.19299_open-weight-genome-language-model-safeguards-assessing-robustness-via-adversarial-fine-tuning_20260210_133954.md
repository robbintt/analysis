---
ver: rpa2
title: 'Open-weight genome language model safeguards: Assessing robustness via adversarial
  fine-tuning'
arxiv_id: '2511.19299'
source_url: https://arxiv.org/abs/2511.19299
tags:
- data
- sequences
- viruses
- fine-tuning
- glms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates whether data exclusion safeguards for genomic
  language models (gLMs) are robust against adversarial fine-tuning. Using Evo 2,
  a state-of-the-art gLM, the authors fine-tuned the model on 110 harmful human-infecting
  virus sequences and compared its performance to a control model fine-tuned on bacteriophage
  sequences.
---

# Open-weight genome language model safeguards: Assessing robustness via adversarial fine-tuning

## Quick Facts
- **arXiv ID**: 2511.19299
- **Source URL**: https://arxiv.org/abs/2511.19299
- **Reference count**: 12
- **Key outcome**: Data exclusion safeguards for genomic language models can be circumvented through fine-tuning on publicly available pathogen sequences, as demonstrated by rescuing predictive capabilities on harmful viruses in Evo 2.

## Executive Summary
This study evaluates whether data exclusion safeguards for genomic language models (gLMs) are robust against adversarial fine-tuning. Using Evo 2, a state-of-the-art gLM, the authors fine-tuned the model on 110 harmful human-infecting virus sequences and compared its performance to a control model fine-tuned on bacteriophage sequences. The virus-fine-tuned model achieved significantly lower perplexity on held-out viral sequences (median 3.55 vs 3.73) compared to the control, demonstrating rescued predictive capabilities. Additionally, it successfully identified SARS-CoV-2 immune escape mutations (AUROC 0.6) despite no exposure to SARS-CoV-2 sequences during training. These results show that publicly available pathogen data can be used to circumvent data exclusion safeguards, highlighting the need for more robust safety frameworks for gLMs.

## Method Summary
The study fine-tuned Evo 2-7B on 110 harmful human-infecting viral genomes using the Savanna framework with DeepSpeed ZeRO Stage 1. Three model versions were trained: a pretrained baseline, a bacteriophage-fine-tuned control, and a harmful-virus-fine-tuned model. Training used 4× NVIDIA H100 GPUs, context length reduced from 1M to 4,096 tokens, and Adam optimizer with learning rate 2×10⁻⁶. Datasets were deduplicated using Mash sketching (10K k-mers, 99% ANI clustering). Performance was evaluated on 12 held-out viral genomes via perplexity scoring and on SARS-CoV-2 immune escape prediction using deep mutational scanning data.

## Key Results
- Virus-fine-tuned model achieved lower perplexity on held-out viral sequences (median 3.55) compared to pretrained (3.83) and bacteriophage-fine-tuned (3.73) models
- Model identified SARS-CoV-2 immune escape mutations with AUROC 0.59 despite no SARS-CoV-2 exposure during training
- Performance gap between fine-tuned and control models demonstrates that exclusion safeguards can be circumvented through adversarial fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Fine-tuning on publicly available pathogen sequences can partially "rescue" predictive capabilities that were intentionally excluded during pretraining.
- **Mechanism**: The base model learns generalizable genomic representations from non-excluded data (e.g., bacteriophages, bacterial genomes). When fine-tuned on related but previously excluded sequence types (human-infecting viruses), gradient updates adapt these transferable representations to the target domain. The model doesn't learn from scratch—it repurposes existing structure.
- **Core assumption**: Excluded sequence classes share learnable patterns with non-excluded classes (e.g., conserved biological motifs, sequence grammar).
- **Evidence anchors**:
  - [abstract] "fine-tuned model exhibited reduced perplexity on unseen viral sequences relative to 1) the pretrained model and 2) a version fine-tuned on bacteriophage sequences"
  - [section 3.1] Training data perplexity: pretrained median 3.84, FT-bacteriophage 3.73, FT-harmful 2.16; test data: pretrained 3.83, FT-bacteriophage 3.73, FT-harmful 3.55
  - [corpus] Weak direct corpus evidence on gLM-specific rescue; related work on LLM tamper-resistance (Deep Ignorance, AntiDote) suggests analogous vulnerability in text domains
- **Break condition**: If base model had no transferable representations to viral biology (e.g., trained only on distant taxa), fine-tuning would yield minimal perplexity reduction.

### Mechanism 2
- **Claim**: Fine-tuning on one set of viruses confers predictive capability on phylogenetically or functionally related viruses not seen during fine-tuning.
- **Mechanism**: Viral genomes share conserved functional constraints (e.g., immune evasion mechanisms, structural protein requirements). The model learns abstract patterns—"what makes a viable viral mutation"—that generalize across viral families through shared evolutionary pressures.
- **Core assumption**: The fine-tuning dataset contains sufficient diversity to capture generalizable viral patterns, not just memorize specific sequences.
- **Evidence anchors**:
  - [abstract] "identified immune escape variants from SARS-CoV-2 (achieving an AUROC of 0.6), despite having no exposure to SARS-CoV-2 sequences during fine-tuning"
  - [section 3.2] Pretrained and bacteriophage-fine-tuned models showed no improvement over random (AUROC ~0.5); FT-harmful achieved 0.59, vs. EVEscape at 0.8
  - [corpus] "A Phylogenetic Approach to Genomic Language Modeling" suggests phylogenetic structure aids generalization in gLMs—consistent with cross-family transfer
- **Break condition**: Generalization would degrade if target virus (e.g., SARS-CoV-2) had fundamentally different biology from fine-tuning viruses (e.g., no shared immune evasion strategies).

### Mechanism 3
- **Claim**: Data exclusion alone is insufficient as a safeguard for open-weight models because fine-tuning access is equivalent to capability restoration access.
- **Mechanism**: Open-weight models expose all parameters. Any actor with compute access and relevant data can run gradient-based optimization to shift the model's behavior. Data exclusion raises the bar (requires data + compute + expertise) but does not create a hard barrier.
- **Core assumption**: Sensitive data (viral genomes) remains publicly accessible for fine-tuning.
- **Evidence anchors**:
  - [abstract] "publicly available pathogen data can be used to circumvent data exclusion safeguards"
  - [section 4.6] Discusses capability limitation as "controversial" because open models are modifiable; cites need for additional mitigations
  - [corpus] "The Safety Gap Toolkit" and "Deep Ignorance" explicitly address open-weight LLM tampering risks—analogous threat model
- **Break condition**: Safeguard would hold if: (1) sensitive data were removed from public access, (2) model weights were not released (API-only access), or (3) fine-tuning-resistant training methods were applied.

## Foundational Learning

- **Concept: Perplexity as capability proxy**
  - Why needed here: The paper uses perplexity reduction as primary evidence that fine-tuning "rescues" model understanding of viral sequences. Lower perplexity = better next-token prediction = stronger internal model of sequence distribution.
  - Quick check question: If a model's perplexity on viral sequences drops from 3.8 to 3.5 after fine-tuning, what does this quantitatively indicate about its learned distribution?

- **Concept: Transfer learning and domain adaptation**
  - Why needed here: The results hinge on the model transferring knowledge from bacteriophages/bacteria to human viruses. Without understanding transfer, you can't predict when exclusion will fail.
  - Quick check question: Why might a model trained on prokaryotic genomes still have useful representations for eukaryotic-infecting viruses?

- **Concept: Open-weight vs. API-only deployment**
  - Why needed here: The entire threat model depends on actors having weight access. Understanding this distinction is critical for designing real safeguards.
  - Quick check question: Name two safeguards that work for API-only models but fail for open-weight models.

## Architecture Onboarding

- **Component map**: Evo 2-7B (transformer with rotary position embeddings, 131K vocabulary) -> Savanna framework + DeepSpeed ZeRO Stage 1 -> Mash sketching deduplication -> Perplexity evaluation + AUROC scoring

- **Critical path**: 1. Obtain open-weight gLM → 2. Curate sensitive sequence dataset → 3. Fine-tune with autoregressive objective → 4. Evaluate perplexity on held-out sequences → 5. Test downstream phenotype prediction

- **Design tradeoffs**:
  - Context window truncation (1M → 4K tokens): Limits evaluation to partial genomes/genes; may underestimate full model capability but reflects realistic actor constraints
  - Fine-tuning dataset size (110 genomes): Small enough to suggest low barrier; large enough to demonstrate principle
  - Bacteriophage control: Accounts for "fine-tuning artifacts" vs. "viral-specific learning"

- **Failure signatures**:
  - Fine-tuned model shows no perplexity reduction → suggests base model lacks transferable representations (safeguard works)
  - Test perplexity much higher than training perplexity → suggests overfitting/memorization, not generalization
  - AUROC at 0.5 on held-out virus → no real transfer to novel pathogens

- **First 3 experiments**:
  1. **Reproduction with different base model**: Test whether other gLMs (e.g., Nucleotide Transformer, GenomicBERT) show similar rescue effects to assess generality.
  2. **Minimum data threshold**: Fine-tune with progressively smaller viral datasets (100, 50, 10, 5 genomes) to identify the floor for capability rescue.
  3. **Negative control augmentation**: Fine-tune on synthetic random sequences or scrambled genomes to confirm perplexity reduction requires biologically meaningful data.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: What are the generative capabilities of adversarially fine-tuned gLMs, beyond predictive performance on perplexity and phenotype prediction tasks?
- **Basis in paper**: [explicit] "Ideally, an evaluation of sequence generation properties would have been conducted as well, and future work should address this."
- **Why unresolved**: The study focused solely on predictive capabilities (perplexity, immune escape prediction) due to scope and information hazard concerns.
- **What evidence would resolve it**: Systematic evaluation of generated viral sequences from fine-tuned gLMs for properties like viability, functional elements, and novelty.

### Open Question 2
- **Question**: What is the minimum quantity of pathogen sequence data required to rescue misuse-relevant capabilities to a concerning degree?
- **Basis in paper**: [inferred] The study used 110 viral genomes but did not test whether fewer sequences could achieve similar capability rescue.
- **Why unresolved**: No experiments were conducted varying the amount of fine-tuning data to establish thresholds.
- **What evidence would resolve it**: Ablation studies testing model performance after fine-tuning on progressively smaller subsets of viral sequences.

### Open Question 3
- **Question**: How can "rule-out" evaluations with high sensitivity be designed and validated for gLM safety assurance?
- **Basis in paper**: [explicit] "While more research is needed on how such evaluations would be conducted exactly... these evaluations would ideally be reproducible across gLMs rather than being custom-built for individual models."
- **Why unresolved**: Current evaluations focus on "ruling in" harms via red-teaming rather than demonstrating particular harms are absent with confidence.
- **What evidence would resolve it**: Development and validation of standardized evaluation benchmarks that can reliably exclude specific harm categories.

### Open Question 4
- **Question**: How well do capability rescue findings generalize to larger model scales and to multi-modal biological AI systems?
- **Basis in paper**: [explicit] "When considering the safety and security of gLMs, researchers should also anticipate future trends in AI development, including multi-modal models and agentic systems, which introduce new categories of misuse risks."
- **Why unresolved**: Only the 7B parameter Evo 2 model was tested; larger variants and multi-modal systems were not evaluated.
- **What evidence would resolve it**: Replication of fine-tuning experiments across model scales and extensions to multi-modal biological AI architectures.

## Limitations
- The study does not establish minimum thresholds for data quantity required to achieve concerning capability rescue
- Results focus on predictive capabilities rather than assessing whether rescued models could enable de novo pathogen design
- The 110-genome dataset represents a middle ground that doesn't identify exact boundaries of capability transfer

## Confidence
- **High confidence**: Data exclusion safeguards are insufficient for open-weight models when sensitive sequence data remains publicly accessible and computational resources permit fine-tuning
- **Medium confidence**: Transfer learning enables cross-viral-family capability rescue, as evidenced by SARS-CoV-2 mutation prediction from non-SARS-CoV-2 training data
- **Medium confidence**: Perplexity reduction from 3.83→3.55 on held-out viral sequences demonstrates genuine capability rescue rather than memorization artifacts

## Next Checks
1. **Minimum viable dataset experiment**: Systematically reduce fine-tuning dataset size (100, 50, 10, 5 genomes) to identify the smallest dataset capable of achieving measurable perplexity reduction and immune escape prediction
2. **Cross-taxonomic transfer boundary**: Test whether fine-tuning on bacterial pathogens (e.g., *Mycobacterium tuberculosis*) can rescue eukaryotic pathogen prediction capabilities, establishing limits of phylogenetic transfer
3. **Capability progression analysis**: Evaluate whether rescued models show disproportionate improvement on specific viral capabilities (e.g., immune evasion vs. replication machinery) to identify which biological functions are most readily transferred