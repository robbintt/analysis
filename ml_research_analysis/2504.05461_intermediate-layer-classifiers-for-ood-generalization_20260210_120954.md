---
ver: rpa2
title: Intermediate Layer Classifiers for OOD generalization
arxiv_id: '2504.05461'
source_url: https://arxiv.org/abs/2504.05461
tags:
- layer
- arxiv
- layers
- data
- intermediate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper challenges the common assumption that the last layer
  of deep networks contains the most useful features for out-of-distribution (OOD)
  generalization. The authors propose Intermediate Layer Classifiers (ILCs), which
  train linear classifiers on intermediate layer representations rather than the penultimate
  layer.
---

# Intermediate Layer Classifiers for OOD generalization

## Quick Facts
- arXiv ID: 2504.05461
- Source URL: https://arxiv.org/abs/2504.05461
- Authors: Arnas Uselis; Seong Joon Oh
- Reference count: 40
- Key outcome: Intermediate layer classifiers often outperform penultimate layer for OOD generalization

## Executive Summary
This paper challenges the prevailing assumption that the penultimate layer of deep networks contains the most useful features for out-of-distribution (OOD) generalization. The authors propose Intermediate Layer Classifiers (ILCs), which train linear classifiers on intermediate layer representations rather than the penultimate layer. Across nine datasets and multiple architectures (ResNets, ViTs), they demonstrate that intermediate layers frequently yield substantially better OOD generalization than the last layer—sometimes matching few-shot performance of last-layer retraining even in zero-shot settings. The analysis reveals that intermediate layers are less sensitive to distribution shifts, particularly for minority groups, explaining their superior robustness.

## Method Summary
The Intermediate Layer Classifier (ILC) method involves training linear classifiers on intermediate layer representations extracted from a frozen feature extractor. Rather than using only the penultimate layer as is standard practice, the approach evaluates multiple intermediate layers and selects the one that maximizes performance on a validation set from the target distribution. The method maintains the simplicity of linear probing while exploring the representation space at different depths. The training process involves freezing the pretrained backbone network, extracting features from each candidate layer, and training a linear classifier for each layer independently. The best-performing layer is then selected for final deployment.

## Key Results
- Intermediate layers frequently outperform the penultimate layer for OOD generalization across nine datasets
- ILC achieves zero-shot performance comparable to few-shot fine-tuning on the last layer
- Intermediate layers show reduced sensitivity to distribution shifts, particularly benefiting minority groups
- The optimal layer for OOD performance varies by dataset and architecture but is often not the penultimate layer

## Why This Works (Mechanism)
The paper demonstrates that intermediate layer representations are less sensitive to distribution shifts compared to the penultimate layer. This reduced sensitivity appears to stem from the fact that intermediate layers capture more generalizable features that are less overfit to the training distribution. The authors provide evidence that this effect is particularly pronounced for minority groups within the data, suggesting that intermediate layers maintain better representation of less frequent patterns that might otherwise be suppressed in later layers.

## Foundational Learning

**Linear probing**: A simple evaluation protocol where a linear classifier is trained on top of frozen features to assess representation quality. Why needed: Provides a controlled way to evaluate feature representations without confounding factors from nonlinear fine-tuning. Quick check: Train a linear classifier on top of a pretrained model and measure accuracy.

**Distribution shift**: The difference between training and test data distributions. Why needed: Understanding how models behave under distribution shift is crucial for real-world deployment. Quick check: Evaluate model performance on data with different characteristics than training data.

**Feature extractor**: The portion of a neural network that transforms raw inputs into feature representations. Why needed: Different layers capture different levels of abstraction, affecting downstream performance. Quick check: Visualize intermediate representations using dimensionality reduction techniques.

**OOD generalization**: The ability of a model to perform well on data drawn from distributions different from training. Why needed: Real-world applications rarely encounter data identical to training distributions. Quick check: Measure performance drop when test data distribution differs from training.

## Architecture Onboarding

Component map: Input -> Backbone Network -> Intermediate Layers -> Linear Classifiers -> Validation Selection -> Output
Critical path: Feature extraction from intermediate layers → Linear classifier training → Validation-based selection
Design tradeoffs: Simplicity of linear probing vs. potential gains from nonlinear fine-tuning
Failure signatures: Poor performance when distribution shift is minimal or when intermediate features are too coarse
First experiments:
1. Compare ILC performance against penultimate layer linear probing on a standard OOD benchmark
2. Visualize feature representations at different layers to understand distributional differences
3. Evaluate ILC on minority groups within datasets to verify reduced sensitivity claims

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical mechanisms explaining intermediate layer advantages are not fully established
- Results primarily validated on image classification tasks using standard benchmark datasets
- Computational overhead of training multiple classifiers across layers is not thoroughly characterized

## Confidence

High confidence: Empirical observation that intermediate layers often outperform the penultimate layer for OOD generalization
Medium confidence: Explanation that intermediate layers are less sensitive to distribution shifts
Medium confidence: Claim that intermediate layer performance sometimes matches few-shot fine-tuning

## Next Checks

1. Test ILC performance on non-image domains (NLP, speech, graph data) to assess domain generalizability
2. Systematically vary the nature of distribution shifts (temporal, adversarial, covariate shifts) to identify conditions where intermediate layers may not outperform the last layer
3. Measure computational overhead and memory requirements for training multiple intermediate classifiers versus standard fine-tuning approaches across different dataset sizes