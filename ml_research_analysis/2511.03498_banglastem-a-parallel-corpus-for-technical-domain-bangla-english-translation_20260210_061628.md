---
ver: rpa2
title: 'BanglaSTEM: A Parallel Corpus for Technical Domain Bangla-English Translation'
arxiv_id: '2511.03498'
source_url: https://arxiv.org/abs/2511.03498
tags:
- translation
- bangla
- technical
- banglastem
- pairs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of translating technical STEM
  content from Bangla to English, which is critical for enabling Bangla speakers to
  use English-focused language models effectively. The authors present BanglaSTEM,
  a high-quality parallel corpus of 5,000 Bangla-English sentence pairs across computer
  science, mathematics, physics, chemistry, and biology domains.
---

# BanglaSTEM: A Parallel Corpus for Technical Domain Bangla-English Translation

## Quick Facts
- arXiv ID: 2511.03498
- Source URL: https://arxiv.org/abs/2511.03498
- Authors: Kazi Reyazul Hasan; Mubasshira Musarrat; A. B. M. Alim Al Islam; Muhammad Abdullah Adnan
- Reference count: 15
- Primary result: Presents BanglaSTEM, a parallel corpus of 5,000 Bangla-English sentence pairs for technical STEM translation

## Executive Summary
This paper addresses the challenge of translating technical STEM content from Bangla to English, which is critical for enabling Bangla speakers to use English-focused language models effectively. The authors present BanglaSTEM, a high-quality parallel corpus of 5,000 Bangla-English sentence pairs across computer science, mathematics, physics, chemistry, and biology domains. The dataset was created by generating over 12,000 translations using three large language models, then carefully curating and selecting the highest quality pairs through human evaluation to ensure accurate preservation of technical terminology.

## Method Summary
The authors developed BanglaSTEM through a multi-stage process involving initial translation generation using three large language models, followed by careful human curation to select high-quality parallel sentence pairs. A T5-based translation model was fine-tuned on this curated dataset and evaluated on two downstream tasks: code generation and mathematical problem-solving. The evaluation showed significant improvements over baseline approaches, with the model achieving 82.5% accuracy on code generation and 79.0% accuracy on mathematical problem-solving tasks.

## Key Results
- BanglaSTEM corpus contains 5,000 high-quality parallel sentence pairs across five STEM domains
- T5-based translation model achieves 82.5% accuracy on code generation downstream task
- Model achieves 79.0% accuracy on mathematical problem-solving downstream task

## Why This Works (Mechanism)
The approach works by leveraging the strengths of large language models for initial translation generation, then applying human expertise to ensure quality and technical accuracy. The careful curation process preserves domain-specific terminology and ensures that technical nuances are maintained across languages. By fine-tuning a T5-based model on this specialized corpus, the system learns to handle the specific challenges of technical domain translation, including complex terminology and domain-specific language patterns.

## Foundational Learning
- Technical terminology preservation: Critical for maintaining meaning across technical domains; check by verifying domain experts can understand translated content
- Domain adaptation in translation: Enables handling of specialized vocabulary and concepts; check by testing on domain-specific benchmark datasets
- Parallel corpus creation methodology: Establishes quality standards for technical translation; check by comparing inter-annotator agreement scores
- Downstream task evaluation: Validates practical utility beyond direct translation quality; check by measuring task-specific performance metrics

## Architecture Onboarding
- Component map: Raw technical text -> LLM translation generation -> Human curation -> Parallel corpus -> T5 fine-tuning -> Downstream task evaluation
- Critical path: Corpus creation and curation is the critical path, as downstream performance directly depends on corpus quality
- Design tradeoffs: Larger corpus size vs. curation quality - opted for smaller, higher-quality corpus rather than larger, noisier dataset
- Failure signatures: Poor technical terminology preservation, loss of domain-specific context, mistranslation of specialized concepts
- First experiments: 1) Test translation quality on simple technical sentences, 2) Evaluate preservation of domain-specific terminology, 3) Measure performance on basic code generation tasks

## Open Questions the Paper Calls Out
None

## Limitations
- Corpus size of 5,000 sentence pairs may be insufficient for robust translation model training
- Reliance on three large language models for initial translation introduces potential biases
- Limited documentation of human evaluation methodology and quality assessment criteria
- Evaluation conducted using only T5-based model architecture, limiting generalizability

## Confidence
- High confidence: Dataset creation methodology and provision of both dataset and trained model are well-documented and reproducible
- Medium confidence: Quality assessment of parallel corpus due to limited human evaluation details
- Medium confidence: Downstream task performance improvements given single model architecture evaluation
- Low confidence: Generalization claims to other technical domains or real-world translation scenarios

## Next Checks
1. Conduct inter-annotator agreement studies and document specific quality criteria used in human evaluation of the parallel corpus
2. Evaluate the trained model on additional technical domains and with alternative translation architectures to assess generalizability
3. Test the corpus and model on real-world technical documents from academic papers, textbooks, or online resources to evaluate practical applicability beyond controlled benchmark tasks