---
ver: rpa2
title: 'Ask a Local: Detecting Hallucinations With Specialized Model Divergence'
arxiv_id: '2506.03357'
source_url: https://arxiv.org/abs/2506.03357
tags:
- language
- languages
- specialized
- perplexity
- hallucinations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces "Ask a Local", a novel hallucination detection
  method for large language models that leverages divergence between perplexity distributions
  of specialized language models. The approach computes a divergence score between
  local (language-specific) and foreign (non-language-specific) perplexities to identify
  hallucinated spans in multilingual text.
---

# Ask a Local: Detecting Hallucinations With Specialized Model Divergence

## Quick Facts
- arXiv ID: 2506.03357
- Source URL: https://arxiv.org/abs/2506.03357
- Authors: Aldan Creo; Héctor Cerezo-Costas; Pedro Alonso-Doval; Maximiliano Hormazábal-Lagos
- Reference count: 3
- One-line primary result: Novel hallucination detection method achieves IoU ~0.3 across 14 languages using specialized model divergence without training or adaptation

## Executive Summary
This paper introduces "Ask a Local", a novel hallucination detection method that leverages divergence between perplexity distributions of specialized language models to identify hallucinated spans in multilingual text. The approach computes a divergence score between local (language-specific) and foreign (non-language-specific) perplexities, enabling detection without external data sources, training, or model adaptation. Evaluated on human-annotated question-answer datasets across 14 languages, the method demonstrates robust cross-lingual effectiveness with particularly strong performance on Italian (IoU 0.42) and Catalan (IoU 0.38).

## Method Summary
The method computes hallucination scores by comparing perplexity distributions from specialized language models against generalist baselines. For each QA pair, per-token perplexities are calculated across multiple specialized models (Goldfish family, XGLM, DeepSeek), aggregated to word-level using max-perplexity pooling, and normalized using question tokens. An instruction-following model (Llama-3.1-8B-Instruct) dynamically weights specialized models based on query content. The final hallucination score combines KL divergence between weighted local/foreign perplexity distributions with average perplexity, thresholded to identify candidate hallucinated words, which are then expanded into spans via instruction model.

## Key Results
- Achieves IoU scores around 0.3 across 14 languages without requiring adaptation or training
- Particularly strong performance on Italian (0.42 IoU) and Catalan (0.38 IoU)
- Demonstrates computational efficiency through use of specialized models rather than large generalist models
- Scales naturally across languages without requiring external data sources or fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Language-specialized models exhibit higher perplexity on culturally/linguistically-specific hallucinations than non-specialized models.
- Mechanism: The method computes KL divergence between weighted "local" perplexity (from language-relevant models) and "foreign" perplexity (from non-relevant models). When a hallucination concerns domain-specific knowledge (e.g., Spanish history), the Spanish model shows disproportionate surprise compared to others.
- Core assumption: Specialized models encode better-calibrated priors for their domain; hallucinations violate these priors more noticeably for domain experts than generalists.
- Evidence anchors:
  - [abstract] "specialized models exhibit greater surprise when encountering domain-specific inaccuracies"
  - [section 2] Figure 2 shows Spanish model perplexity of 69.3 on "Philip" (hallucinated) vs. 6.5 on "Charles" (correct), while other models remain low
  - [corpus] Mu-SHROOM shared task (SemEval-2025) provides the evaluation framework but does not validate the divergence hypothesis directly
- Break condition: If all models show similar perplexity patterns (low divergence), the method cannot distinguish hallucinations from genuinely surprising-but-true statements.

### Mechanism 2
- Claim: Normalizing perplexities using question tokens enables cross-model comparison despite different tokenizers and vocabularies.
- Mechanism: Perplexities are z-score normalized using only question tokens (excluding first 5 tokens), under the assumption that questions are non-hallucinated. This creates a shared scale where deviations in the answer become comparable.
- Core assumption: Question tokens have similar perplexity distributions across models; answer token deviations reflect hallucination likelihood.
- Evidence anchors:
  - [section 2.3] "we choose to normalize the perplexities based on the question perplexities so that they have a mean of 0 and a standard deviation of 1"
  - [section 2.2] Word-level aggregation uses maximum token perplexity to handle tokenization differences
  - [corpus] No corpus papers validate this specific normalization approach
- Break condition: If questions contain hallucinations or ambiguous phrasing, normalization anchors become unreliable.

### Mechanism 3
- Claim: An instruction-following model can dynamically weight specialized models without training or adaptation.
- Mechanism: Llama-3.1-8B-Instruct assigns weights via softmax over token probabilities corresponding to model names (e.g., "Spanish" → token probability). Temperature τ controls distribution sharpness (optimized: τ=3.393).
- Core assumption: The instruction model correctly identifies which specialized model is most relevant to the query's cultural/linguistic domain.
- Evidence anchors:
  - [section 2.4] "we employ an instruction-following model, which we task with determining the most suitable model"
  - [section 3.1] Temperature τ optimized to 3.393, β=0.496 (near-equal divergence/perplexity weighting)
  - [corpus] Weak corpus evidence—no papers validate instruction-model-based weighting for hallucination detection
- Break condition: If the query language doesn't match the specialized model's language (e.g., Spanish history question in Hindi), the Spanish model may not detect hallucinations.

## Foundational Learning

- Concept: Perplexity as surprisal measure
  - Why needed here: The entire method relies on interpreting perplexity differences as evidence of hallucination. Without understanding that perplexity ≈ 2^(cross-entropy), the divergence calculations are opaque.
  - Quick check question: Given a model with perplexity 100 vs. 10 on the same token, which indicates greater model "surprise"?

- Concept: KL divergence between distributions
  - Why needed here: The hallucination score H uses KL(PPL_local || PPL_foreign) to quantify how much the local model's perplexity distribution deviates from the foreign baseline.
  - Quick check question: If KL divergence is 0, what does that imply about the relationship between local and foreign perplexity distributions?

- Concept: Tokenization alignment across models
  - Why needed here: Different models split text differently ("Word" → ["Word"] vs. ["Wo", "rd"]). The method uses max-perplexity aggregation per word to handle this.
  - Quick check question: Why would averaging token perplexities bias results toward models with finer-grained tokenizers?

## Architecture Onboarding

- Component map:
  Specialized models M → Goldfish family (monolingual, 14+ languages) + XGLM-7.5B (general) + DeepSeek-R1-Distill-Qwen-7B (math) → compute per-token perplexity
  Weight assigner → Llama-3.1-8B-Instruct → produces α_j weights via softmax over model-name tokens
  Normalization layer → Z-score normalization using question tokens only
  Divergence computer → KL(PPL_local || PPL_foreign) + weighted average perplexity (β ≈ 0.5)
  Span tagger → Threshold σ=0.016 std devs above mean, then instruction model proposes span boundaries

- Critical path:
  1. Input Q-A pair → all specialized models compute per-token perplexity
  2. Aggregate to word-level (max per word)
  3. Normalize using question tokens
  4. Weight assigner produces α_j for each model
  5. Compute PPL_local (weighted sum) and PPL_foreign (1-α weighted)
  6. H = β·KL(local||foreign) + (1-β)·avg_ppl
  7. Threshold H scores → select candidate words
  8. Instruction model proposes span boundaries

- Design tradeoffs:
  - β=0.5 balances divergence vs. raw perplexity; high β prioritizes divergence (specialized surprise), low β catches obvious hallucinations where all models are confused
  - Larger model sets increase computation but improve coverage; paper notes XGLM and DeepSeek can be dropped with minimal degradation
  - Temperature τ=3.393 produces softer weight distributions, preventing over-reliance on single models

- Failure signatures:
  - Low IoU (~0.12) on Mandarin suggests tokenization or model quality issues for non-Latin scripts
  - Cross-language queries (Spanish content in Hindi) break specialization assumption
  - Ambiguous ground truth annotations introduce noise (paper acknowledges annotator inconsistency)

- First 3 experiments:
  1. Reproduce IoU on English validation set with β=0.496, σ=0.016, τ=3.393; verify score ≈ 0.30
  2. Ablate weight assignment (uniform weights vs. instruction-model weights) to measure α_j contribution
  3. Test language mismatch: Spanish-history questions in non-Spanish languages to quantify cross-lingual degradation

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on instruction-following models for weight assignment and span tagging with missing implementation details (prompts referenced but not provided)
- Performance variability across languages (0.42 IoU for Italian vs 0.12 for Mandarin) raises questions about true cross-lingual robustness
- Evaluation framework limitations due to inconsistent human annotator agreement and limited dataset size (200 examples per language)

## Confidence

**High Confidence (⭐⭐⭐)**: The fundamental mechanism that specialized models show higher perplexity on domain-specific hallucinations than generalist models. The Spanish model example (69.3 vs 6.5 perplexity on "Philip" vs "Charles") provides clear empirical support for this core claim.

**Medium Confidence (⭐⭐)**: The overall approach of using divergence between specialized model perplexities for hallucination detection, and the claim that this scales naturally across languages without adaptation. The consistent IoU scores across 14 languages support this, but performance variability (0.42 for Italian vs 0.12 for Mandarin) raises questions about true cross-lingual robustness.

**Low Confidence (⭐)**: The effectiveness of instruction-following model-based weight assignment and span tagging, due to missing implementation details and lack of independent validation of these components.

## Next Checks
1. **Prompt Validation**: Reconstruct and test the exact weight assignment and span tagging prompts used with Llama-3.1-8B-Instruct. Measure performance sensitivity to prompt variations across at least 3 different prompt formulations to establish robustness.

2. **Cross-Lingual Stress Test**: Create controlled test cases where Spanish content appears in non-Spanish language contexts (e.g., Hindi questions about Spanish history) to quantify the degradation in detection performance when query language mismatches specialized model language.

3. **Tokenization Impact Analysis**: For Mandarin and other non-Latin scripts, compare the current max-perplexity aggregation approach against alternative methods (e.g., subword alignment, character-level perplexity) to isolate whether poor performance stems from tokenization or model quality issues.