---
ver: rpa2
title: 'Mitigating the Modality Gap: Few-Shot Out-of-Distribution Detection with Multi-modal
  Prototypes and Image Bias Estimation'
arxiv_id: '2502.00662'
source_url: https://arxiv.org/abs/2502.00662
tags:
- image
- prototypes
- text
- detection
- fpr95
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors address the modality gap in vision-language model-based
  out-of-distribution detection. They propose incorporating multi-modal prototypes
  (image and text) and introduce a novel few-shot tuning framework called SUPREME,
  which includes biased prompt generation and image-text consistency modules.
---

# Mitigating the Modality Gap: Few-Shot Out-of-Distribution Detection with Multi-modal Prototypes and Image Bias Estimation

## Quick Facts
- **arXiv ID**: 2502.00662
- **Source URL**: https://arxiv.org/abs/2502.00662
- **Reference count**: 40
- **Primary result**: Achieves up to 95.38% AUROC and 20.70% FPR95 on ImageNet-1k OOD detection using multi-modal prototypes and few-shot tuning

## Executive Summary
This paper addresses the modality gap problem in vision-language model-based out-of-distribution (OOD) detection by introducing multi-modal prototypes and a novel few-shot tuning framework called SUPREME. The approach combines image and text prototypes to reduce false positives caused by the modality gap, while incorporating Biased Prompt Generation (BPG) and Image-Text Consistency (ITC) modules. The framework achieves state-of-the-art performance on ImageNet-1k and its OOD variants, demonstrating significant improvements over existing methods.

## Method Summary
The SUPREME framework uses CLIP's frozen encoders with few-shot fine-tuning to detect OOD samples. It generates multi-modal prototypes by combining image and text embeddings from the training set, then scores new samples using both original and mapped embeddings. The BPG module conditions text prompts on Gaussian-estimated image domain bias to enhance generalization, while ITC learns linear mappings between image and text spaces to directly reduce the modality gap. The final OOD score averages similarities across multiple modalities and mappings, providing robust discrimination between ID and OOD samples.

## Key Results
- Achieves 95.38% AUROC and 20.70% FPR95 on ImageNet-1k OOD detection
- Multi-modal prototypes improve average FPR95 from 32.4 to 24.2 and AUROC from 94.5 to 95.8
- BPG reduces FPR95 from 28.12% to 20.70% in ablation studies
- ITC reduces modality gap measure from 0.8679 to 0.6135 compared to baseline
- Linear mappings outperform MLPs in few-shot settings, preventing overfitting

## Why This Works (Mechanism)

### Mechanism 1: Multi-Modal Prototyping Reduces False Positives from Modality Gap
The modality gap causes OOD images to score highly when compared only to text prototypes, as image embeddings naturally cluster closer to image prototypes. By averaging scores from both image and text prototypes, this spatial bias is counterbalanced, requiring stricter proximity for both modalities and reducing false positives.

### Mechanism 2: Biased Prompt Generation Enhances Generalization
BPG conditions text prompts on a Gaussian-estimated image domain bias, shifting text prototypes toward the image embedding distribution. This augmentation approximates the true ID image distribution more robustly than limited training samples, preventing overfitting in few-shot scenarios.

### Mechanism 3: Image-Text Consistency Directly Reduces Modality Gap
ITC learns bidirectional linear mappings between image and text embedding spaces, enforcing intra- and inter-modal consistency. The image-to-text mapping projects image embeddings into text space for cross-modal alignment, while the text-to-image mapping reconstructs original embeddings to preserve information.

## Foundational Learning

- **Concept: Modality Gap in Vision-Language Models**
  - **Why needed here**: The entire paper diagnoses and mitigates the modality gap in CLIP; understanding that image and text embeddings occupy distinct regions is prerequisite to grasping why text-only OOD detection fails
  - **Quick check question**: Can you explain why an OOD image might have high cosine similarity to an ID text prototype even if semantically unrelated?

- **Concept: Few-Shot Prompt Tuning for VLMs**
  - **Why needed here**: SUPREME extends CoOp/CoCoOp-style prompt tuning; understanding how learnable context tokens replace handcrafted prompts is essential for following the BPG module design
  - **Quick check question**: How does conditioning prompts on image embeddings (as in CoCoOp) differ from static prompt templates?

- **Concept: Out-of-Distribution Scoring Functions**
  - **Why needed here**: The paper introduces $S_{GMP}$ as an extension of MCM scoring; grasping how softmax-normalized similarity scores are used for ID/OOD separation is necessary to evaluate the proposed scoring mechanism
  - **Quick check question**: Why might a simple maximum similarity score be insufficient for OOD detection in multi-modal settings?

## Architecture Onboarding

- **Component map**: CLIP encoders -> BPG module (learnable contexts + image embedding + bias) -> ITC module (linear mappings f_img-txt, f_txt-img) -> OOD scorer (S_GMP with multi-modal prototypes)

- **Critical path**: During inference, an image is encoded, then simultaneously: (a) compared directly to multi-modal prototypes, (b) mapped to text space and compared again. Both paths contribute to the final score. During training, BPG and ITC modules are jointly optimized with classification loss.

- **Design tradeoffs**:
  - **Linear vs. MLP mappings**: Linear transformations were chosen over MLPs to prevent overfitting in few-shot settings, with ablation showing MLPs degrade performance
  - **Gaussian vs. complex distribution models**: Gaussian was chosen for tractability, though the authors acknowledge this as a limitation and suggest exploring more advanced models
  - **Uniform vs. weighted score averaging**: $S_{GMP}$ uses equal weights across modalities/mappings for simplicity and generalization, though optimal weighting could be dataset-dependent

- **Failure signatures**:
  - **High FPR95 on specific OOD datasets**: May indicate that certain OOD types (e.g., semantically close to ID) require additional negative prototype learning or different scoring weights
  - **Degraded ID accuracy**: Could signal that cross-modal alignment is too aggressive, losing discriminative features for classification
  - **Modality gap not reducing**: If quantitative gap measures don't decrease, check learning rates or mapping architecture capacity

- **First 3 experiments**:
  1. **Ablate each component**: Train three variants—(BPG only), (ITC only), (BPG + ITC)—on ImageNet-100 with 16 shots, evaluate on iNaturalist/SUN/Places/Texture. Compare FPR95/AUROC against Table 3 to verify contribution.
  2. **Validate modality gap reduction**: For each variant, compute the modality gap measure (L2 distance between image and text embedding centroids) on ID and OOD datasets. Confirm that SUPREME variants reduce the gap relative to MCM baseline.
  3. **Test scoring function sensitivity**: Replace $S_{GMP}$ with ablated versions (image-only, text-only, unmapped-only) using the same trained model. Analyze score distributions and KS statistics to understand each component's contribution to ID/OOD separability.

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical claims rely on simplified assumptions about image embedding distributions that may not hold in all practical settings
- Gaussian-based bias estimation lacks direct empirical validation in OOD detection literature
- Linear mapping approach may not capture full complexity of cross-modal relationships, potentially limiting performance on more challenging datasets

## Confidence
- **High Confidence**: Empirical results demonstrating effectiveness on benchmark datasets are robust and well-supported by ablation studies
- **Medium Confidence**: Theoretical analysis providing rationale for multi-modal prototypes and Gaussian bias estimation is plausible but relies on simplified assumptions
- **Low Confidence**: Claims regarding superiority of linear mappings over more complex architectures are supported by ablation studies but lack strong theoretical foundation

## Next Checks
1. **Distributional Analysis**: Conduct thorough analysis of image embedding distribution to validate Gaussian assumption underlying bias estimation. Explore alternative distribution models and compare impact on OOD detection performance.
2. **Mapping Architecture Comparison**: Replace linear mappings in ITC module with more complex architectures (e.g., MLPs with varying depths) and evaluate impact on modality gap reduction and OOD detection performance.
3. **Cross-Dataset Generalization**: Evaluate SUPREME on broader range of OOD datasets with different semantic relationships to ID data. Investigate sensitivity to OOD dataset choice and analyze failure modes on specific OOD sample types.