---
ver: rpa2
title: 'LSPO: Length-aware Dynamic Sampling for Policy Optimization in LLM Reasoning'
arxiv_id: '2510.01459'
source_url: https://arxiv.org/abs/2510.01459
tags:
- training
- lspo
- arxiv
- sampling
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes LSPO, a length-aware dynamic sampling method
  for reinforcement learning with verifiable rewards (RLVR) in LLM reasoning. Motivated
  by the observation that incorrect responses are often longer, LSPO filters training
  prompts based on average response length, retaining only the shortest and longest
  responses after initial accuracy-based filtering.
---

# LSPO: Length-aware Dynamic Sampling for Policy Optimization in LLM Reasoning

## Quick Facts
- arXiv ID: 2510.01459
- Source URL: https://arxiv.org/abs/2510.01459
- Authors: Weizhe Chen; Sven Koenig; Bistra Dilkina
- Reference count: 12
- Primary result: LSPO improves RLVR effectiveness by filtering prompts based on average response length, retaining only shortest and longest responses after accuracy-based filtering

## Executive Summary
This paper introduces LSPO, a length-aware dynamic sampling method for reinforcement learning with verifiable rewards (RLVR) in LLM reasoning. The method is motivated by the observation that incorrect responses tend to be longer than correct ones, suggesting response length serves as a proxy for model confidence and task difficulty. LSPO filters training prompts by computing average response length across multiple samples, then retaining only prompts with extreme average lengths (shortest and longest) while discarding intermediate-length ones. Evaluated across multiple base models and RLVR algorithms on challenging math benchmarks, LSPO consistently improves final model effectiveness compared to baseline algorithms, reaching target performance faster despite increased rollout time.

## Method Summary
LSPO is a meta-algorithm that wraps existing RLVR methods (GRPO, DAPO, GSPO) to improve sampling efficiency and effectiveness. For each rollout batch, LSPO samples G responses per prompt, applies accuracy-based filtering to remove prompts with 0% or 100% accuracy, then computes average response length L(q) for each prompt. Prompts are retained if L(q) ≤ Q(L_low) or Q(L_high) ≤ L(q) ≤ Q(L_max), where Q represents percentiles. This filters out intermediate-length responses that are less informative for learning. The method uses percentile-based thresholds that adapt to each batch's distribution rather than fixed values, preventing unbalanced resampling. Default hyperparameters are L_low=0.3, L_high=0.65, L_max=0.95, with batch_size≥256 and G=8 samples per prompt.

## Key Results
- LSPO improves avg@32 accuracy from 37.9% to 38.6% on Qwen-2.5-Math-7B with DAPO across AIME25, Olympiad, and Minerva-Math benchmarks
- Training only on intermediate-length responses produces the worst performance, strongly discouraged
- Percentile-based dynamic thresholds outperform value-based thresholds, requiring fewer than 10 resampling rounds versus unbounded resampling with value-based filtering
- LSPO reaches target performance faster despite ~60% increase in rollout time per step, due to more efficient learning signal

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Retaining prompts with extreme average response lengths (shortest and longest) while discarding intermediate-length ones improves final model effectiveness compared to full-data training.
- **Mechanism:** LSPO computes per-prompt average response length L(q) across G samples, then filters to retain prompts where L(q) ≤ Q(L_low) OR L(q) ≥ Q(L_high) ∧ L(q) ≤ Q(L_max). This focuses gradient updates on prompts that are either high-confidence (short) or high-effort (long), skipping ambiguous intermediate cases that provide weaker learning signal.
- **Core assumption:** Response length serves as a proxy for both model confidence (short = confident/correct) and perceived difficulty (long = uncertain/effortful), and intermediate-length responses contribute less to learning.
- **Evidence anchors:**
  - [abstract]: "motivated by studies of overthinking in LLMs... LSPO retains prompts with the shortest and longest average responses while discarding intermediate-length ones"
  - [section 4.1]: "Short responses typically indicate high confidence... longer responses suggest uncertainty... responses of intermediate length are less informative"
  - [corpus]: Neighboring paper "Not All Rollouts are Useful" validates selective rollout downsampling but targets variance, not length; limited direct corpus evidence for length-based filtering specifically.
- **Break condition:** If response length becomes decorrelated from task difficulty or model confidence (e.g., fixed-length generation constraints), the length signal degrades and filtering may harm performance.

### Mechanism 2
- **Claim:** Percentile-based dynamic thresholds outperform value-based thresholds for length filtering.
- **Mechanism:** LSPO recomputes L_low, L_high, L_max percentiles within each rollout batch rather than using absolute token counts. This adapts to distribution shift as the policy evolves. Value-based thresholds cause unbalanced resampling (sometimes near-zero retention, sometimes near-100%), increasing sampling rounds and reducing training steps.
- **Core assumption:** Response length distributions shift substantially during training; static thresholds fail to maintain consistent retention ratios.
- **Evidence anchors:**
  - [section 4.2]: "LSPO computes percentiles dynamically based on the samples in the current rollout batch"
  - [section 5.3]: "relative value-based filtering leads to significantly longer sampling times... whereas percentile-based resampling typically required fewer than 10"
  - [corpus]: "From Data-Centric to Sample-Centric" (LPPO) uses learning-progress metrics for sample selection, suggesting dynamic sample-centric strategies are promising but targeting different signals.
- **Break condition:** If batch sizes are too small, percentile estimates become noisy, potentially retaining uninformative prompts or filtering valuable ones.

### Mechanism 3
- **Claim:** Training on both short and long responses provides complementary benefits that neither extreme alone achieves.
- **Mechanism:** Short responses reinforce confident, efficient reasoning paths. Long responses expose the model to challenging problems where it must learn to allocate computation effectively. Training only on short responses fails to generalize; excluding short responses slightly hurts both training and test. Intermediate-only training performs worst.
- **Core assumption:** The two extremes capture distinct learning modes—efficiency vs. difficulty-handling—that are both necessary for robust reasoning.
- **Evidence anchors:**
  - [section 5.3, Table 2]: "[0,30], [65,95] filtering achieves 38.6% avg vs. 37.3% for [20,80] intermediate-only"
  - [section 5.3]: "training only on intermediate-length responses produces the worst performance and is strongly discouraged"
  - [corpus]: Limited direct corpus validation of bimodal retention; most work focuses on downsampling for efficiency rather than effectiveness.
- **Break condition:** If the model's error modes shift (e.g., overconfident wrong answers become short), short-response retention may reinforce incorrect patterns.

## Foundational Learning

- **Concept:** Group Relative Policy Optimization (GRPO)
  - **Why needed here:** LSPO is a meta-algorithm wrapping GRPO/DAPO/GSPO; understanding the base RLVR objective clarifies what LSPO modifies (sampling) vs. what remains unchanged (loss).
  - **Quick check question:** Can you explain how GRPO computes advantage without a separate critic model?

- **Concept:** Overthinking in reasoning models
  - **Why needed here:** The paper's core motivation relies on the observed correlation between incorrect responses and longer outputs; understanding this phenomenon explains why length is a useful signal.
  - **Quick check question:** Why would incorrect responses tend to be longer than correct ones in chain-of-thought reasoning?

- **Concept:** Dynamic sampling in RLVR
  - **Why needed here:** LSPO extends prior dynamic sampling (DAPO's zero-gradient filtering) by adding length-based criteria; distinguishing efficiency-focused vs. effectiveness-focused sampling clarifies LSPO's contribution.
  - **Quick check question:** How does DAPO's accuracy-based filter differ from LSPO's length-based filter in what it optimizes for?

## Architecture Onboarding

- **Component map:** Rollout stage → Sample B_r prompts → Generate G responses → Accuracy filter → Compute L(q) → Length filter → Accumulate to B_t → Training stage → Feed filtered batch to RLVR algorithm → Update policy

- **Critical path:**
  1. Accuracy-based filtering must retain sufficient prompts for length distribution estimation (requires batch_size≥256 and non-trivial model capability)
  2. Percentile computation requires surviving prompts after accuracy filter; if too few survive, resampling loops increase
  3. Length filter must balance retention ratio; default 60% retained (shortest 30% + longest 30% after L_max cap)

- **Design tradeoffs:**
  - **Rollout cost vs. gradient quality:** LSPO increases sampling time ~60% (Fig. 2a) but reaches target accuracy faster (Fig. 2b)
  - **Percentile vs. value thresholds:** Percentiles ensure consistent retention ratios; value thresholds risk unbalanced batches
  - **L_max cap:** Prevents retaining extremely long outliers that empirically degrade performance (Section 4.2)

- **Failure signatures:**
  - Excessive resampling loops (>10 rounds) → likely batch size too small or accuracy filter too aggressive
  - No improvement over baseline → check that base model has sufficient capability to produce varied response lengths
  - Training divergence → verify overlong penalty (DAPO-style) is applied; LSPO assumes this regularization

- **First 3 experiments:**
  1. **Sanity check:** Run LSPO with GRPO on Qwen-2.5-Math-7B with DAPO-17K+MATH, verify ~1-2% avg improvement over GRPO baseline (Table 1)
  2. **Ablation:** Test intermediate-only filtering [20,80] vs. bimodal [0,30],[65,95] to confirm intermediate-length training is harmful (Table 2)
  3. **Efficiency analysis:** Measure rollout time per step and total training time to target accuracy; verify that despite ~60% rollout overhead, wall-clock time to target is reduced (Fig. 2)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a predictor that estimates response length before generation (rather than after) provide substantial speedups while maintaining LSPO's effectiveness gains?
- Basis in paper: [explicit] "Similar to how GRESO extends the dynamic sampling used in DAPO, future work could build predictors based on statistics from recent samples of the same prompt and filter out prompts whose predicted response lengths do not meet the criteria."
- Why unresolved: LSPO currently filters after responses are generated, which increases rollout time by ~60%. Pre-generation prediction could reduce this overhead.
- What evidence would resolve it: A model trained with predicted-length filtering achieving comparable accuracy to LSPO with measurably lower per-step sampling cost.

### Open Question 2
- Question: Would adaptively adjusting filtering thresholds during training outperform fixed percentile thresholds in both efficiency and effectiveness?
- Basis in paper: [explicit] "We acknowledge that adaptively adjusting the threshold could surpass LSPO in both efficiency, measured by training speed, and effectiveness, measured by model capability."
- Why unresolved: Current LSPO uses static thresholds (L_low=0.3, L_high=0.65, L_max=0.95); adaptive thresholds might better accommodate shifting response length distributions across training phases.
- What evidence would resolve it: An adaptive-threshold variant achieving higher final accuracy or faster convergence than fixed-threshold LSPO across multiple models.

### Open Question 3
- Question: Can alternative filtering criteria such as entropy or self-confidence match or exceed the performance gains achieved through length-based filtering?
- Basis in paper: [explicit] "Other signals, such as entropy or self-confidence, could serve as alternative criteria if properly designed for the task. We believe additional criteria may also improve performance, and whether they can match the gains achieved through length-based filtering remains an interesting question for future research."
- Why unresolved: Response length was chosen based on correlation with accuracy and difficulty perception, but other signals may capture complementary information.
- What evidence would resolve it: Entropy-based or confidence-based filtering achieving comparable or superior benchmark accuracy to LSPO on the same models and datasets.

## Limitations

- The method's effectiveness relies on the assumption that response length correlates with model confidence and task difficulty, which may not generalize to non-mathematical reasoning tasks
- LSPO increases rollout time by approximately 60%, which could be prohibitive for resource-constrained applications despite faster convergence to target accuracy
- The paper doesn't provide causal evidence that intermediate-length responses are less informative versus simply less decisive; correlation doesn't prove the filtering strategy is optimal

## Confidence

- **High confidence:** LSPO's implementation details and hyperparameter settings (L_low=0.3, L_high=0.65, L_max=0.