---
ver: rpa2
title: 'FairGU: Fairness-aware Graph Unlearning in Social Networks'
arxiv_id: '2601.09469'
source_url: https://arxiv.org/abs/2601.09469
tags:
- graph
- unlearning
- fairgu
- fairness
- sensitive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the issue of degraded algorithmic fairness in
  graph unlearning when removing nodes, which existing methods overlook. It proposes
  FairGU, a fairness-aware graph unlearning framework that integrates a pre-trained
  sensitive attribute estimator, fairness-aware GNN training with adversarial debiasing,
  and Fisher Information Matrix-based parameter importance analysis.
---

# FairGU: Fairness-aware Graph Unlearning in Social Networks

## Quick Facts
- arXiv ID: 2601.09469
- Source URL: https://arxiv.org/abs/2601.09469
- Reference count: 40
- Key outcome: Proposed FairGU framework significantly improves fairness metrics (ΔSP and ΔEO reduced by over 75% on Pokec-n) while preserving competitive accuracy and robust privacy protection (MIA AUC close to 50%).

## Executive Summary
This paper introduces FairGU, a fairness-aware graph unlearning framework designed to remove the influence of specific nodes from graph neural networks (GNNs) while preserving algorithmic fairness with respect to sensitive attributes. The framework addresses a critical gap where existing unlearning methods degrade fairness metrics after data removal. FairGU combines a pre-trained sensitive attribute estimator, fairness-aware GNN training with adversarial debiasing, and Fisher Information Matrix-based parameter importance analysis. Experiments on three real-world datasets demonstrate significant improvements in fairness metrics while maintaining competitive accuracy and strong privacy guarantees.

## Method Summary
FairGU operates through three key components: (1) a pre-trained sensitive attribute estimator that handles incomplete attribute labels in social networks, (2) a fairness-aware GNN trained with adversarial debiasing where the classifier learns to fool an adversary trying to predict sensitive attributes, and (3) Fisher Information Matrix analysis to identify and dampen parameters specialized to forgotten nodes rather than removing them entirely. The framework integrates these components into a unified training and unlearning pipeline that maintains fairness constraints during the unlearning process through targeted parameter modification based on their importance to the forgotten data.

## Key Results
- Fairness metrics improved by over 75% (ΔSP and ΔEO reduced) on Pokec-n dataset after unlearning 5% of nodes
- Maintained competitive accuracy (ACC) comparable to baseline models post-unlearning
- Membership Inference Attack (MIA) AUC remained close to 50%, indicating strong privacy protection
- Framework demonstrated effectiveness across three real-world datasets: Pokec-n, Pokec-z, and Income

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FairGU preserves algorithmic fairness during unlearning by learning node representations that are statistically independent of sensitive attributes.
- Mechanism: The framework employs adversarial debiasing where a GNN classifier ($f_G$) generates embeddings to fool an adversary ($f_A$) trying to predict sensitive attributes. Simultaneously, a covariance constraint minimizes the correlation between predictions and sensitive attributes. This dual regularization forces the model to rely on non-sensitive signals for classification.
- Core assumption: The adversarial objective effectively disentangles sensitive information from the node embeddings without destroying the signal required for the primary classification task.
- Evidence anchors:
  - [abstract] "...integrates a pre-trained sensitive attribute estimator, fairness-aware GNN training with adversarial debiasing..."
  - [section 4.2] "The adversary $f_A$... aims to predict the sensitive attribute from the node representation... while $f_G$ aims to learn representations that deceive $f_A$."
  - [corpus] Related work like "Unlearning Algorithmic Biases over Graphs" confirms the general viability of debiasing in unlearning, but does not validate this specific adversarial architecture.
- Break condition: If the adversary is too weak, the GNN will encode sensitive information; if too strong, the GNN will fail to converge on the classification task.

### Mechanism 2
- Claim: The framework removes the influence of specific nodes without full retraining by dampening parameters specialized to those nodes.
- Mechanism: FairGU utilizes the Fisher Information Matrix (FIM) to calculate the "importance" of each parameter relative to the training set ($I_{D_{train}}$) and the forget set ($I_{D_f}$). Parameters identified as highly specialized for the forget set (where $I_{D_f} > \gamma \cdot I_{D_{train}}$) are multiplicatively dampened rather than pruned.
- Core assumption: The diagonal of the FIM is a sufficient approximation of parameter importance, and dampening preserves general knowledge better than zeroing out weights.
- Evidence anchors:
  - [abstract] "...Fisher Information Matrix-based parameter importance analysis."
  - [section 4.3] "Instead of pruning selected parameters... we apply a targeted dampening factor proportional to their relative specialization towards the forget set."
  - [corpus] Evidence in corpus is weak regarding FIM specifically; neighbors focus on retraining or other architectures.
- Break condition: If parameters are shared critically between the forget set and retain set, dampening them may cause catastrophic forgetting of retain set performance (utility drop).

### Mechanism 3
- Claim: FairGU enables fairness constraints in data-scarce environments by pre-training a sensitive attribute estimator.
- Mechanism: Since real-world social networks often lack complete sensitive attribute labels, FairGU pre-trains a GCN-based estimator ($f_E$) on available labels. These estimated attributes ($\hat{S}$) serve as targets for the adversarial debiasing module.
- Core assumption: The estimator can predict sensitive attributes with sufficient accuracy to provide a meaningful signal for the adversarial loss, and using estimated proxies does not introduce new biases.
- Evidence anchors:
  - [abstract] "...integrates a pre-trained sensitive attribute estimator..."
  - [section 4.1] "This pre-training phase ensures that the estimator learns to approximate the original sensitive attribute distribution without generating synthetic data..."
  - [corpus] Corpus neighbors like "Model-Agnostic Fairness Regularization for GNNs with incomplete Sensitive Information" suggest this is a recognized strategy, though not explicitly validated for FairGU in the provided text.
- Break condition: If the initial data is too sparse or biased for the estimator to generalize, the subsequent debiasing will optimize for incorrect proxies, potentially amplifying bias.

## Foundational Learning

- Concept: **Graph Neural Networks (GNNs)**
  - Why needed here: GNNs serve as the backbone for processing social network topology and node features; understanding message passing is required to comprehend how bias propagates through edges.
  - Quick check question: Can you explain how a node aggregates information from its neighbors in a standard GCN layer?

- Concept: **Adversarial Learning (Min-Max Game)**
  - Why needed here: The core fairness mechanism relies on a discriminator (adversary) fighting against the generator (GNN); you must understand this dynamic to tune the stability of the fairness training.
  - Quick check question: In an adversarial setup for fairness, what is the optimization goal of the adversary network versus the main classifier?

- Concept: **Fisher Information Matrix (FIM)**
  - Why needed here: The unlearning mechanism relies on FIM to quantify parameter sensitivity; understanding it as a proxy for "how much a parameter knows about the data" is crucial for the dampening logic.
  - Quick check question: Why does FairGU use the diagonal of the FIM rather than the full matrix, and what is the computational tradeoff?

## Architecture Onboarding

- Component map:
  1. **Estimator ($f_E$):** GCN predicting sensitive attributes (Input: Graph; Output: $\hat{s}$).
  2. **Classifier ($f_G$):** GNN backbone for the main task (Input: Graph; Output: Prediction $\hat{y}$).
  3. **Adversary ($f_A$):** Linear classifier for debiasing (Input: Embeddings from $f_G$; Output: Sensitive prediction).
  4. **Unlearner:** FIM calculator and parameter scaler (Input: Trained $\theta$, Forget set $D_f$; Output: Unlearned $\theta'$).

- Critical path:
  1. Pre-train Estimator ($f_E$) on known attributes.
  2. Train Classifier ($f_G$) and Adversary ($f_A$) jointly using estimated attributes (Adversarial Training).
  3. Store FIM importance for the full training set ($I_{D_{train}}$).
  4. Upon unlearning request: Compute FIM for forget set ($I_{D_f}$) $\to$ Identify specialized parameters $\to$ Apply dampening factor (Eq. 14).

- Design tradeoffs:
  - **Accuracy vs. Fairness:** Ablation study (Table 4) shows removing the fairness control (w/o FC) increases ACC slightly (e.g., 80.88% vs 80.40% on Income) but spikes $\Delta SP$ (0.42% $\to$ 7.27%).
  - **Dampening Strength ($\lambda$):** High $\lambda$ ensures privacy but risks utility loss; low $\lambda$ preserves performance but may fail MIA tests.
  - **Selectivity ($\gamma$):** High $\gamma$ preserves the model but may leave "forgotten" data influential; low $\gamma$ is aggressive and may damage structural integrity.

- Failure signatures:
  - **Fairness Collapse:** $\Delta SP$ or $\Delta EO$ spikes post-unlearning (indicates dampening disrupted the fairness manifold).
  - **Privacy Leak:** MIA AUC significantly > 50% (indicates dampening was insufficient to remove data influence).
  - **Utility Crash:** ACC drops drastically (indicates dampening was too aggressive or $\gamma$ too low, damaging general knowledge).

- First 3 experiments:
  1. **Baseline Reproduction (Pokec-n):** Run FairGU on Pokec-n with a 5% node deletion request to verify if $\Delta SP$ drops to ~0.74% as claimed in Table 2.
  2. **MIA Validation:** Conduct a Membership Inference Attack on the unlearned model to confirm the AUC is near 50% (random guessing), verifying the unlearning effectiveness.
  3. **Ablation on Estimator:** Retrain FairGU without pre-training the sensitive attribute estimator (FairGU w/o SAE) to observe the degradation in fairness metrics (specifically the rise in $\Delta SP$) observed in Table 4.

## Open Questions the Paper Calls Out
- **Can the tuning of the selectivity factor and dampening strength be automated?** The conclusion states that FairGU's performance is "sensitive to hyperparameters such as the selectivity factor and dampening strength, which may require careful tuning." Manual tuning creates a barrier for deployment in dynamic, real-time web applications where data distributions shift constantly. An adaptive mechanism that dynamically selects optimal hyperparameters ($\gamma$ and $\lambda$) without manual validation would resolve this.

## Limitations
- **Parameter Configuration:** Critical hyperparameters for the GNN architecture and adversarial training are unspecified, making exact reproduction challenging.
- **Fisher Information Approximation:** Using only the diagonal of the FIM may miss critical parameter interactions that influence both fairness and utility, especially in complex graph structures.
- **Estimator Dependency:** The fairness guarantees are contingent on the accuracy of the pre-trained sensitive attribute estimator, and errors in estimation could propagate and undermine the debiasing process.

## Confidence
- **High Confidence:** The core mechanisms (adversarial debiasing, FIM-based parameter importance, and the general unlearning framework) are logically sound and align with established literature in the field.
- **Medium Confidence:** The experimental results are promising, showing significant improvements in fairness metrics. However, the lack of detailed hyperparameter settings and the reliance on a specific dataset split (80/20, 5% forget set) introduce uncertainty about generalizability.
- **Low Confidence:** The paper does not provide a rigorous statistical analysis of the variance in the results (e.g., standard deviations from the 10 repetitions) or an ablation study on the estimator's impact in extremely low-data regimes.

## Next Checks
1. **MIA Robustness Test:** Conduct a comprehensive Membership Inference Attack on the unlearned model using multiple attack strategies (e.g., shadow models, gradient-based) to confirm the MIA AUC remains consistently near 50%, ensuring robust privacy protection.
2. **Cross-Dataset Generalization:** Apply FairGU to a different graph dataset (e.g., a citation network like Cora or a different social network) to validate that the fairness improvements are not specific to the Pokec and Income datasets.
3. **Estimator Ablation in Low-Data Regime:** Systematically reduce the number of labeled sensitive attributes used to train the estimator $f_E$ (e.g., 10%, 1%, 0.1% of nodes) and measure the degradation in fairness metrics ($\Delta SP$, $\Delta EO$) to quantify the estimator's contribution and identify a minimum viable labeling requirement.