---
ver: rpa2
title: 'AI with Emotions: Exploring Emotional Expressions in Large Language Models'
arxiv_id: '2504.14706'
source_url: https://arxiv.org/abs/2504.14706
tags:
- emotional
- states
- arousal
- emotions
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates whether LLMs can generate responses reflecting\
  \ specified emotional states. The authors adopt Russell\u2019s Circumplex model,\
  \ mapping emotions onto arousal and valence axes, and prompt four LLMs (GPT-3.5,\
  \ GPT-4, Gemini 1.5, Llama3, and Command R+) to answer ten questions under 12 different\
  \ emotional states."
---

# AI with Emotions: Exploring Emotional Expressions in Large Language Models

## Quick Facts
- **arXiv ID**: 2504.14706
- **Source URL**: https://arxiv.org/abs/2504.14706
- **Reference count**: 22
- **Primary result**: LLMs can generate text reflecting specified emotional states, with GPT-4, GPT-4 turbo, and Llama3 70B achieving cosine similarity up to ~0.75

## Executive Summary
This paper investigates whether large language models can generate responses reflecting specified emotional states by adopting Russell's Circumplex model of emotion. The authors prompt nine different LLMs with 12 emotional states (specified by arousal-valence coordinates) across 10 questions, then evaluate the consistency between specified and generated emotions using a GoEmotions-based classifier. Results show that all models can express emotions to some degree, with higher-performing models like GPT-4, GPT-4 turbo, and Llama3 70B achieving stronger alignment (cosine similarity up to ~0.75), while GPT-3.5 shows lower performance. The study demonstrates that LLMs can be controlled to generate emotion-conditioned text, offering a foundation for more nuanced, emotionally expressive AI agents.

## Method Summary
The researchers use Russell's Circumplex model to map emotions onto arousal and valence axes, then prompt nine LLMs (including GPT-3.5, GPT-4, GPT-4 turbo, Gemini 1.5, Llama3, and Command R+) to answer ten questions under 12 different emotional states. Responses are evaluated using a BERT-based classifier trained on the GoEmotions dataset, with consistency measured by cosine similarity between specified and predicted emotional states in the arousal-valence space. The evaluation pipeline involves mapping 27 emotion labels to Russell's framework and computing directional alignment between requested and detected emotional states.

## Key Results
- All nine tested LLMs successfully generated emotionally expressive responses when prompted with Russell's Circumplex parameters
- GPT-4, GPT-4 turbo, and Llama3 70B achieved the highest consistency with cosine similarities up to ~0.75
- GPT-3.5 turbo showed notably lower alignment despite having substantial parameters
- Model capability and alignment quality proved more important than parameter count alone for emotional expression
- Gemini 1.5 Flash occasionally refused role-play instructions, violating the task requirements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can generate text that reflects specified emotional states when prompted with parametric arousal-valence values.
- Mechanism: The LLM's pre-existing knowledge of Russell's Circumplex Model enables it to map continuous arousal-valence parameters onto semantically appropriate linguistic expressions through role-play prompting.
- Core assumption: LLMs have internalized sufficient representations of emotional concepts and their linguistic correlates from training data.
- Evidence anchors: [abstract] "The evaluation showed that the emotional states of the generated answers were consistent with the specifications"; [section 4] "emotional expression was successfully performed for all the models."
- Break condition: Models without pre-training knowledge of Russell's framework or insufficient instruction-following capability would fail to map parameters appropriately.

### Mechanism 2
- Claim: Independent sentiment classification provides a quantifiable measure of emotional expression consistency in generated text.
- Mechanism: A BERT-based classifier trained on GoEmotions classifies generated text, with each label mapped to arousal-valence coordinates. Cosine similarity measures alignment between specified and predicted states.
- Core assumption: The sentiment classifier's label predictions accurately reflect emotional content in generated text.
- Evidence anchors: [section 3.2] "We explored the correspondence between the GoEmotions labels and the emotional terms that appeared in Russell's original paper"; [appendix A] "The mean cosine similarity is 0.680."
- Break condition: Classifier accuracy degrades significantly on LLM-generated text distributions.

### Mechanism 3
- Claim: Model capability—particularly alignment quality and instruction-following ability—predicts emotional expression consistency better than parameter count alone.
- Mechanism: Higher-performing models demonstrate stronger instruction adherence and more nuanced semantic control, enabling finer-grained emotional expression.
- Core assumption: Observed cosine similarity differences reflect genuine differences in controllability rather than evaluation artifacts.
- Evidence anchors: [section 4] "GPT-3.5 turbo performs worse than the smaller-parameter LLaMA3-8B-Instruct model"; "training dataset and alignment strategy may play a more critical role."
- Break condition: Future models with different alignment objectives may show inverted patterns.

## Foundational Learning

- **Concept: Russell's Circumplex Model of Affect**
  - Why needed here: This two-dimensional framework provides the continuous parametric space for specifying emotional states.
  - Quick check question: If a prompt specifies arousal=0.866 and valence=-0.5, what emotional state is being requested? (Answer: High arousal, moderate displeasure—e.g., anxious, agitated, or alarmed.)

- **Concept: Cosine Similarity for Vector Comparison**
  - Why needed here: The paper measures alignment between specified and detected emotional states using cosine similarity in the 2D arousal-valence space.
  - Quick check question: A cosine similarity of 0.5 corresponds to approximately what angular deviation? (Answer: ~60°, meaning the detected emotion differs moderately but is still in a related quadrant.)

- **Concept: Sentiment Classification with Fine-Grained Labels**
  - Why needed here: The GoEmotions dataset provides 28 emotion categories, enabling more nuanced evaluation than binary sentiment.
  - Quick check question: Why use a separate BERT-based classifier rather than having an LLM evaluate its own outputs? (Answer: Independence prevents evaluation bias; BERT's classification task training is specialized for this purpose.)

## Architecture Onboarding

- **Component map**: Prompt Construction Module -> LLM Generation Engine -> Sentiment Classification Module -> Label Mapping Layer -> Consistency Evaluator

- **Critical path**: 
  1. Select emotional state coordinates (12 states evenly distributed around unit circle)
  2. Construct role-play prompt with arousal/valence parameters
  3. Generate response using target LLM
  4. Classify response with independent sentiment model
  5. Map predicted label to arousal-valence space
  6. Compute cosine similarity between specified and predicted vectors
  7. Aggregate across questions (n=10) for mean similarity per model

- **Design tradeoffs**:
  - Continuous vs. discrete emotion specification: Continuous enables fine-grained control but may be harder for models to interpret
  - 12 emotional states: Chosen for distinguishability; finer divisions would exceed human discriminability
  - Unit-length vectors: Fixes intensity at maximum, simplifying analysis but ignoring intensity variation
  - Classifier independence vs. LLM self-evaluation: BERT-based classifier provides independence but may have accuracy limits

- **Failure signatures**:
  - GPT-3.5 turbo pattern: Low/negative cosine similarities, particularly for displeasure states
  - Gemini 1.5 Flash pattern: Occasional "I'm a language model" refusals that violate role-play instructions
  - Low-discrepancy clustering: Multiple models showing similar cosine similarity ceiling (~0.53-0.55) may indicate evaluation bottleneck

- **First 3 experiments**:
  1. Baseline verification: Replicate with a single model (recommend GPT-4 or Llama3 70B) on 3 questions across 4 emotional states to validate expected cosine similarities (>0.5 for capable models).
  2. Word vs. parameter comparison: Test whether your target model performs better with direct arousal/valence specification or emotion word labels.
  3. Question-domain sensitivity: Test whether certain question types (abstract philosophical vs. personal preference) show systematically different consistency scores.

## Open Questions the Paper Calls Out
None

## Limitations
- **Evaluation Pipeline Reliability**: The study relies on a BERT-based classifier with relatively low accuracy (58.9% on 28-class emotion recognition) to assess emotional content in LLM-generated text.
- **Parametric Intensity Constraint**: The study fixes emotional intensity at unit length vectors, examining only directional variation in arousal-valence space and limiting understanding of intensity scaling.
- **Limited Prompt Diversity**: All emotional states are elicited using a single template, potentially missing better approaches for different model architectures.

## Confidence
- **High Confidence**: LLMs can generate text that reflects specified emotional states. The consistent patterns across all nine tested models provide robust evidence.
- **Medium Confidence**: The quantitative consistency measurements accurately reflect the models' emotional expression capabilities. While the methodology is sound, classifier limitations reduce confidence in absolute values.
- **Medium Confidence**: Model capability and alignment quality are more determinative of emotional expression consistency than parameter count alone. The GPT-3.5 turbo underperformance is compelling but would benefit from broader model diversity.

## Next Checks
1. **Cross-Classifier Validation**: Repeat the experiment using an alternative emotion classification approach (e.g., LLM-based evaluator or different BERT variant) to verify that cosine similarity patterns remain consistent.
2. **Continuous Intensity Scaling**: Extend methodology to test multiple intensity levels (e.g., arousal/valence values at 0.5, 0.75, and 1.0) to understand how emotional intensity affects expression consistency.
3. **Prompt Template Variation**: Systematically vary prompt structure (direct emotion word specification, scenario-based prompting, different instruction framings) to identify optimal approaches for different model architectures.