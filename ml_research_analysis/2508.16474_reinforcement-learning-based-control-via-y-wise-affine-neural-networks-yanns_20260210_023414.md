---
ver: rpa2
title: Reinforcement Learning-based Control via Y-wise Affine Neural Networks (YANNs)
arxiv_id: '2508.16474'
source_url: https://arxiv.org/abs/2508.16474
tags:
- control
- learning
- function
- system
- linear
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces YANN-RL, a novel reinforcement learning algorithm
  using Y-wise Affine Neural Networks (YANNs) to provide safer and more interpretable
  control for chemical and energy systems. YANNs can exactly represent piecewise affine
  functions, enabling initialization of RL actor and critic networks with multi-parametric
  linear model predictive control (mp-MPC) solutions.
---

# Reinforcement Learning-based Control via Y-wise Affine Neural Networks (YANNs)

## Quick Facts
- arXiv ID: 2508.16474
- Source URL: https://arxiv.org/abs/2508.16474
- Reference count: 40
- Primary result: Novel RL algorithm using YANNs to initialize actor/critic networks with mp-MPC solutions, achieving safer control and eliminating unsafe exploration phase

## Executive Summary
This paper introduces YANN-RL, a reinforcement learning algorithm that leverages Y-wise Affine Neural Networks (YANNs) to provide safer and more interpretable control for chemical and energy systems. YANNs can exactly represent piecewise affine functions, enabling initialization of RL actor and critic networks with multi-parametric linear model predictive control (mp-MPC) solutions. This approach provides theoretical guarantees of stability and recursive feasibility from linear optimal control, eliminating the unsafe exploration phase common in RL. The method is demonstrated on a clipped pendulum and a safety-critical chemical reactor, showing significant performance improvements over standard DDPG algorithms while maintaining safety constraints without violations.

## Method Summary
The method involves solving an mp-MPC problem offline for a linearized system model, yielding piecewise affine control laws defined over polytopic critical regions. These solutions are embedded into YANN actor and critic networks, which are then extended with additional layers for nonlinear expressibility. The YANN-actor represents control laws u*(θ) = Ki·θ + ri for each region, while the YANN-critic embeds quadratic state-action value function coefficients. Additional trainable layers are initialized to output zero for all inputs, ensuring they don't disrupt the initial linear solution. The algorithm trains using a modified DDPG approach without exploration noise, relying on small learning rates and Polyak averaging to maintain proximity to the mp-MPC baseline policy.

## Key Results
- YANN-RL significantly outperforms standard DDPG algorithms on both clipped pendulum and chemical reactor case studies
- The method achieves better initial performance and maintains safety constraints without violations during training
- For the chemical reactor, YANN-RL avoided all safety constraint violations while DDPG experienced multiple violations
- The approach provides heuristic confidence that linear optimal control serves as an effective lower bound for policy performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Initializing RL actor/critic networks with exact mp-MPC solutions provides a theoretically grounded starting point that eliminates unsafe exploration.
- Mechanism: Multi-parametric quadratic programming (mp-QP) solves the MPC problem offline, yielding piecewise affine control laws defined over polytopic critical regions. These are embedded directly into YANN weights—no training required. The actor represents u*(θ) = Ki·θ + ri for each region; the critic embeds the quadratic state-action value function coefficients from the linear OCP.
- Core assumption: The nonlinear system can be reasonably approximated by a linear model near the operating point; deviations can be learned online.
- Evidence anchors:
  - [abstract]: "YANNs provide an interpretable neural network which can exactly represent known piecewise affine functions of arbitrary input and output dimensions defined on any amount of polytopic subdomains."
  - [Section 4.1]: "To develop a YANN-initialized policy network, an mp-MPC problem (Eq. 9) needs to be solved offline... After, the YANN can be created following the steps in our previous work [49]."
  - [corpus]: Prior work (arXiv:2505.07054) provides formal proofs that YANNs require no training to exactly represent piecewise affine functions.
- Break condition: If the true system dynamics are highly nonlinear far from the linearization point and the initial mp-MPC policy performs poorly (e.g., leaves the polytopic domain), the guaranteed lower bound may not hold.

### Mechanism 2
- Claim: Additional NN layers can be initialized to output zero for all inputs while remaining fully trainable, enabling nonlinear extension without disrupting the initial linear solution.
- Mechanism: Symmetric weight initialization (Theorem 1): split nodes into pairs with weights (+Wh, +Bh) and (-Wh, -Bh). For any input X, the pre-activation sums cancel, and σ(0)=0 for tanh/ReLU. After one gradient update, symmetry breaks because positive and negative weights update in the same direction (both become less negative or more positive), creating asymmetry.
- Core assumption: Activation functions satisfy σ(0)=0 and have non-zero gradient at zero (for layer 1); gradient-based updates are used.
- Evidence anchors:
  - [Section 3.2.1]: "Solving for Y gives: Y = σ(Wh·X + Bh - Wh·X - Bh) = σ([0]) = [0]"
  - [Section 3.2.1]: "The symmetry is broken after the first parameter update assuming that a gradient-based update rule is used and that the activation function has a non-zero gradient at zero."
  - [corpus]: Weak/missing—no external corpus papers validate this specific zero-initialization scheme.
- Break condition: If using activation functions with σ(0)≠0 (e.g., sigmoid), or if gradient-based updates are not applied, the zero-output property fails; symmetry may not break properly.

### Mechanism 3
- Claim: Continuous policy improvement with small learning rates provides heuristic confidence that performance never degrades below the linear MPC baseline.
- Mechanism: The YANN-actor starts with the mp-MPC solution (known-good). Small learning rates and small Polyak averaging coefficients (τ) ensure parameter updates remain close to the current policy. This approximates trust-region methods—staying within a neighborhood where improvement is likely. Combined with interpretable initialization, this makes linear MPC an effective lower bound.
- Core assumption: The local neighborhood around the mp-MPC policy contains improved policies for the nonlinear system; small steps don't jump to worse regions.
- Evidence anchors:
  - [Section 4.4]: "The small updates made to the networks in this algorithm ensure that they remain close to their representations prior to being updated... the interpretable initializations of both the actor and the critic provide a foundation for which this continuous improvement can occur."
  - [Table 2]: YANN-DDPG initial vs. final costs show the final policy never degrades more than +0.77 while improving up to -14.15 on other episodes.
  - [corpus]: Weak/missing—no external papers validate this heuristic guarantee.
- Break condition: If learning rates are too large, or if the nonlinear system requires control actions far from the mp-MPC policy (large distribution shift), the heuristic may fail—performance could drop below linear MPC.

## Foundational Learning

- Concept: **Multi-parametric Quadratic Programming (mp-QP)**
  - Why needed here: Converts online MPC optimization into offline piecewise affine control laws. You must understand how constraints define critical regions and how Ki, ri matrices are computed.
  - Quick check question: Given a 2D state space with linear constraints, can you explain why the explicit solution partitions the space into convex polytopes?

- Concept: **Actor-Critic RL (DDPG specifically)**
  - Why needed here: YANN-RL modifies DDPG by replacing random initialization with mp-MPC-derived weights and removing exploration noise. You need to understand how the critic estimates Q(s,u) and how policy gradients update the actor.
  - Quick check question: Why does DDPG require exploration noise, and what happens to the replay buffer distribution if you remove it entirely?

- Concept: **Piecewise Affine Functions on Polytopes**
  - Why needed here: The YANN architecture explicitly encodes indicator functions for polytopic subdomains (layers 1-3) and affine subfunctions (layers 4-5). Understanding half-space representation (Aθ ≤ b) is essential.
  - Quick check question: How would you represent the indicator function for a polytope defined by three linear inequalities using ReLU-like operations?

## Architecture Onboarding

- Component map:
  - Linearize nonlinear system -> Formulate MPC problem -> Reformulate to mp-QP -> Embed into YANN-actor/critic -> Add zero-initialized trainable layers -> Train online

- Critical path:
  1. Linearize nonlinear system → obtain A, B matrices
  2. Formulate MPC problem (Eq. 8) with constraints
  3. Reformulate to mp-QP (Eq. 9), solve offline → piecewise affine control law
  4. Embed into YANN-actor; compute Q-coefficients for YANN-critic
  5. Add zero-initialized trainable layers per Theorem 2
  6. Deploy and train online with small learning rates

- Design tradeoffs:
  - **More critical regions (higher p)** → finer control granularity but larger network, slower inference
  - **Larger trainable NN per region** → more nonlinear expressivity but risk of departing from mp-MPC baseline
  - **Smaller learning rates** → safer updates but slower convergence

- Failure signatures:
  - **State leaves polytopic domain** → indicator layer outputs all zeros → control action undefined (need fallback)
  - **Performance degrades below linear MPC** → learning rate too large or bad reward shaping
  - **Safety violations during training** → constraint not encoded in mp-MPC, or linearization too coarse

- First 3 experiments:
  1. **Sanity check**: Linear system (where mp-MPC is optimal). Run YANN-DDPG without training. Verify cost equals mp-MPC exactly. Train for 50 episodes—cost should not change (nothing to learn).
  2. **Mild nonlinearity**: Clipped pendulum from paper. Compare YANN-DDPG vs. vanilla DDPG on same 10 test episodes. Measure: initial cost, final cost, training episodes to reach YANN initial performance.
  3. **Safety-critical stress test**: CSTR with temperature constraint. Count safety violations during training. Verify YANN-DDPG achieves zero violations while DDPG accumulates violations. Test generalization to initial conditions outside training distribution.

## Open Questions the Paper Calls Out

- Can rigorous theoretical guarantees, rather than heuristic confidence, be established to ensure that the YANN-RL policy provides monotonic improvement over the linear optimal control baseline?
- How can the YANN architecture be effectively integrated into other established reinforcement learning algorithms, such as those utilizing stochastic policies?
- How does the computational efficiency and training stability of YANN-RL scale when the mp-MPC solution involves a very large number of polytopic subdomains (critical regions)?

## Limitations

- The heuristic guarantee of performance never degrading below linear MPC is not formally proven and relies on small learning rates and trust-region-like behavior
- Safety claims depend critically on the quality of the linearized model and whether all constraints are properly encoded in the mp-MPC formulation
- The method requires solving an mp-MPC problem offline, which may be computationally expensive for complex systems with many constraints

## Confidence

- **High**: YANN architecture's ability to exactly represent piecewise affine functions (supported by prior work)
- **Medium**: Practical effectiveness of initializing RL with mp-MPC solutions (strong empirical results but limited theoretical backing)
- **Low**: Universal applicability of the heuristic performance guarantee (not validated across diverse nonlinear systems)

## Next Checks

1. Test YANN-RL on a system where the linearized model is poor (e.g., highly nonlinear dynamics) to verify the heuristic guarantee fails gracefully
2. Conduct ablation studies varying the number of trainable NN nodes per subdomain to quantify the trade-off between nonlinear expressivity and departure from the mp-MPC baseline
3. Implement the fallback strategy for when the state leaves the polytopic domain defined by the mp-MPC solution