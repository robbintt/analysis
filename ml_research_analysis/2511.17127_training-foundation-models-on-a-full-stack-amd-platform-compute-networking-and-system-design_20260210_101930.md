---
ver: rpa2
title: 'Training Foundation Models on a Full-Stack AMD Platform: Compute, Networking,
  and System Design'
arxiv_id: '2511.17127'
source_url: https://arxiv.org/abs/2511.17127
tags:
- training
- attention
- each
- which
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first large-scale pretraining study on
  a pure AMD hardware stack, demonstrating the maturity of MI300X GPUs and Pollara
  networking for frontier LLM development. The authors introduce ZAYA1-base, an 8.3B
  parameter MoE model with 760M active parameters, achieving competitive performance
  with much larger dense models like Qwen3-4B and Gemma3-12B while significantly outperforming
  models like Llama-3-8B and OLMoE.
---

# Training Foundation Models on a Full-Stack AMD Platform: Compute, Networking, and System Design

## Quick Facts
- arXiv ID: 2511.17127
- Source URL: https://arxiv.org/abs/2511.17127
- Reference count: 17
- Primary result: First large-scale pretraining study on pure AMD hardware stack, demonstrating MI300X GPUs and Pollara networking maturity for frontier LLM development

## Executive Summary
This paper presents the first comprehensive large-scale pretraining study using an entirely AMD hardware stack, featuring MI300X GPUs and Pollara networking. The authors successfully train ZAYA1-base, an 8.3B parameter MoE model with 760M active parameters, achieving competitive performance with significantly larger dense models while demonstrating the production-readiness of AMD's full-stack platform for frontier LLM development. The work provides extensive networking benchmarks, transformer sizing guidelines, and detailed characterization of memory bandwidth and collective communication performance across the AMD ecosystem.

## Method Summary
The study introduces ZAYA1-base, an 8.3B parameter MoE model with 760M active parameters, trained on 1,024 MI300X GPUs with Pollara networking using DeepSpeed and Megatron-LM frameworks. The training stack includes custom kernels for Muon optimizer and LayerNorm, a fault-tolerance system (Aegis), and efficient checkpointing mechanisms. The model architecture features Compressed Convolutional Attention (CCA) for efficient attention computation, an expressive MLP-based router with exponential depth averaging, and lightweight residual scaling. The team conducted extensive benchmarking of Pollara networking across all major collectives at scale, developed MI300X-specific transformer sizing guidelines, and characterized memory bandwidth and collective communication performance throughout the training process.

## Key Results
- Successfully trained ZAYA1-base, achieving competitive performance with much larger dense models (Qwen3-4B, Gemma3-12B) while significantly outperforming models like Llama-3-8B and OLMoE
- Demonstrated 90% scaling efficiency across 1,024 MI300X GPUs with Pollara networking, validating the production-readiness of AMD's full-stack platform
- Introduced novel architectural innovations including Compressed Convolutional Attention and MLP-based router with exponential depth averaging, resulting in strong general knowledge, mathematics, and coding capabilities
- Provided comprehensive networking benchmarks and MI300X-specific transformer sizing guidelines that advance the state-of-the-art for AMD-based LLM development

## Why This Works (Mechanism)
The success stems from the combination of MI300X GPU architecture optimized for mixed precision operations, Pollara networking's high-bandwidth collective communication capabilities, and carefully tuned system design that maximizes resource utilization. The ZAYA1 architecture leverages MoE to maintain competitive parameter counts while reducing active compute, paired with CCA for efficient attention computation. The custom kernels for Muon optimizer and LayerNorm are specifically optimized for MI300X's memory hierarchy and tensor core architecture, while the fault-tolerance system (Aegis) ensures training stability at scale.

## Foundational Learning
- **MI300X GPU Architecture**: AMD's accelerator featuring CDNA 3 architecture with high memory bandwidth and tensor cores optimized for mixed precision operations - needed for efficient large-scale LLM training
- **Pollara Networking**: AMD's high-performance interconnect solution supporting all major collectives at scale - critical for efficient distributed training
- **MoE (Mixture of Experts)**: Model architecture where only a subset of parameters are active per token, reducing compute while maintaining capacity - enables efficient training of large models
- **Compressed Convolutional Attention (CCA)**: Novel attention mechanism that reduces computational complexity while maintaining expressiveness - improves training efficiency
- **DeepSpeed ZeRO Optimization**: Memory optimization technique that partitions model states across GPUs - enables training of large models with limited per-GPU memory
- **LayerNorm Fusion**: Combining LayerNorm operations with preceding linear layers to reduce memory traffic - improves performance on memory-bandwidth-limited operations

## Architecture Onboarding
Component Map: Data Loader -> Model Forward -> Optimizer Step -> Gradient AllReduce -> Checkpoint Save
Critical Path: Forward pass computation -> Attention computation -> MLP layers -> Loss calculation -> Backward pass -> AllReduce collectives -> Optimizer update
Design Tradeoffs: MoE provides parameter efficiency but adds router complexity; CCA reduces attention cost but may impact modeling capacity; custom kernels improve performance but reduce portability
Failure Signatures: Collective communication stalls indicate network bottlenecks; memory allocation failures suggest insufficient per-GPU memory; kernel launch failures point to driver or hardware issues
First Experiments: 1) Microbenchmark Pollara collectives (AllReduce, AllGather, ReduceScatter) across different message sizes; 2) Profile memory bandwidth for different transformer layer configurations; 3) Test CCA attention vs standard multi-head attention with varying sequence lengths

## Open Questions the Paper Calls Out
None

## Limitations
- Uses non-standard expert parallel mode not available in standard DeepSpeed implementation, making reproducibility difficult
- Training run experienced failures and required restarting from checkpoints despite claimed 90% efficiency
- Relies on custom kernels for Muon optimizer and LayerNorm, creating potential compatibility issues
- Uses older PyTorch version (2.0.1) rather than more recent releases
- Performance comparisons with other models don't account for differences in training data composition or licensing constraints

## Confidence
- **High confidence**: Basic functionality of MI300X GPUs and Pollara networking for LLM pretraining is validated
- **Medium confidence**: Efficiency claims (90% scaling efficiency) are supported by measurements but may not generalize to different model architectures
- **Medium confidence**: Performance comparisons with other models are valid within reported benchmarks scope, but differences in training protocols make direct comparisons challenging

## Next Checks
1. Reproduce the training run using the latest PyTorch version and standard DeepSpeed implementation to verify claimed efficiency improvements
2. Conduct a controlled scaling study varying only the number of GPUs while keeping other parameters constant to independently verify the 90% scaling efficiency claim
3. Benchmark the ZAYA1 model against other models using identical evaluation protocols and training datasets to ensure fair comparison of performance metrics