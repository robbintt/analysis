---
ver: rpa2
title: Evaluating Causal Explanation in Medical Reports with LLM-Based and Human-Aligned
  Metrics
arxiv_id: '2506.18387'
source_url: https://arxiv.org/abs/2506.18387
tags:
- evaluation
- report
- metrics
- causal
- expert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluates the quality of causal explanations in automatically\
  \ generated medical reports using six metrics: BERTScore, Cosine Similarity, BioSentVec,\
  \ GPT-White, GPT-Black, and expert qualitative assessment. Across two input types\u2014\
  observation-based and multiple-choice-based\u2014GPT-Black and GPT-White demonstrated\
  \ the strongest alignment with expert judgments and highest discriminative power\
  \ for identifying clinically valid causal reasoning."
---

# Evaluating Causal Explanation in Medical Reports with LLM-Based and Human-Aligned Metrics

## Quick Facts
- **arXiv ID**: 2506.18387
- **Source URL**: https://arxiv.org/abs/2506.18387
- **Reference count**: 8
- **Key outcome**: GPT-Black and GPT-White show strongest alignment with expert judgments and highest discriminative power for identifying clinically valid causal reasoning in medical reports.

## Executive Summary
This study evaluates the quality of causal explanations in automatically generated medical reports using six metrics: BERTScore, Cosine Similarity, BioSentVec, GPT-White, GPT-Black, and expert qualitative assessment. Across two input types—observation-based and multiple-choice-based—GPT-Black and GPT-White demonstrated the strongest alignment with expert judgments and highest discriminative power for identifying clinically valid causal reasoning. Similarity-based metrics showed limited sensitivity to diagnostic coherence and often diverged from clinical quality assessments. GPT-Black’s wide score range and rule-based structure made it particularly effective for evaluating logical integrity.

## Method Summary
The study evaluated six metrics on medical report generation: three similarity-based (BERTScore, Cosine Similarity, BioSentVec) and three LLM-based (GPT-White, GPT-Black, expert qualitative assessment). Two input types were used: observation-based (descriptive findings from radiology) and multiple-choice-based (QA responses requiring justification). GPT-White used a 100-point rubric with explicit diagnostic criteria, while GPT-Black applied additive bonuses/penalties for causal reasoning quality. Expert assessment served as the human benchmark. Results were aggregated using two weighting schemes: task-prioritized and equal-weighted.

## Key Results
- GPT-Black and GPT-White showed strongest alignment with expert judgments (correlations of 0.733 and 0.701 respectively) and highest discriminative power (0.136 score variance for GPT-Black).
- Similarity-based metrics (BERTScore, Cosine Similarity, BioSentVec) showed limited sensitivity to diagnostic coherence and diverged from clinical reasoning quality assessments.
- GPT-Black's rule-based bonus/penalty system was particularly effective at identifying logically coherent and clinically valid causal narratives.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-Black's rule-based bonus/penalty system provides superior discriminative power for causal explanation quality compared to embedding-based similarity metrics.
- Mechanism: GPT-Black evaluates reports by prompting with both reference and generated reports, applying additive bonuses (+0.2) for valid causal reasoning and penalties (-0.2 or -0.1) for logical errors or clinical inaccuracies. This structured approach explicitly targets causal integrity rather than surface similarity.
- Core assumption: LLMs can reliably identify logical inconsistencies and missing causal links when presented with both reference and generated text in a structured prompt format.
- Evidence anchors:
  - [abstract] "GPT-Black demonstrates the strongest discriminative power in identifying logically coherent and clinically valid causal narratives."
  - [section 4.3] "GPT-Black showed the broadest score range (0.136 between top and bottom models), underlining its effectiveness in assessing the depth and structure of causal explanation."
  - [corpus] CR3G paper (arXiv:2512.11830) similarly addresses causal reasoning gaps in radiology report generation, supporting the need for causality-focused evaluation.
- Break condition: If LLM evaluators hallucinate penalties or fail to detect clinically significant errors, the bonus/penalty mechanism may introduce systematic bias rather than improved discrimination.

### Mechanism 2
- Claim: Rubric-based LLM evaluation (GPT-White) aligns with expert judgment by decomposing assessment into explicit diagnostic criteria.
- Mechanism: GPT-White uses a 100-point rubric divided into common evaluation (30 points) and input-type-specific evaluation (70 points), with subscores for contextual similarity, diagnosis-centric focus, observation accuracy, and causal explanation clarity. This decomposition mirrors how clinicians assess report quality.
- Core assumption: Explicit scoring criteria reduce evaluation ambiguity and improve reproducibility compared to holistic judgment.
- Evidence anchors:
  - [section 4.3] "GPT-White also demonstrated high discriminative power with consistent alignment to expert evaluation, highlighting its utility in structured rubric-based scoring."
  - [section 5] "Its high correlation with expert qualitative scores reinforces its utility as a core component in automated evaluation."
  - [corpus] MORQA paper (arXiv:2509.12405) confirms traditional metrics fall short in medical QA evaluation, supporting rubric-based alternatives.
- Break condition: If rubric categories don't capture domain-specific nuances (e.g., radiology-specific reasoning patterns), alignment with experts will degrade.

### Mechanism 3
- Claim: Embedding-based similarity metrics (BERTScore, Cosine Similarity) fail to capture causal reasoning because they conflate lexical overlap with diagnostic validity.
- Mechanism: These metrics compute vector similarity between tokens or sentences without modeling the logical dependencies between observations and diagnoses. A report with correct terminology but incorrect causal links can score highly.
- Core assumption: Semantic embeddings encode diagnostic relationships; this assumption appears unsupported for clinical reasoning.
- Evidence anchors:
  - [abstract] "Similarity-based metrics like BERTScore and Cosine Similarity diverge from clinical reasoning quality."
  - [section 4.3] "Model C's highest BERTScore (0.224) in the multiple-choice task did not translate into high GPT-Black (0.723) or expert (0.783) scores, underscoring the disconnect between surface similarity and causal soundness."
  - [corpus] No directly contradictory corpus evidence found; neighbor papers focus on LLM-based evaluation alternatives.
- Break condition: If embedding models are specifically fine-tuned on causal reasoning tasks, this limitation may be partially mitigated—though this was not tested in the study.

## Foundational Learning

- Concept: **LLM-as-judge evaluation paradigm**
  - Why needed here: GPT-Black and GPT-White both use LLMs to evaluate text quality rather than generate it. Understanding prompt design, output format constraints, and calibration is essential.
  - Quick check question: Can you explain why GPT-Black enforces a "numeric-only output format" and how this affects reproducibility?

- Concept: **Clinical causal explanation structure**
  - Why needed here: The rubric design assumes reports should progress from observations → interpretation → causal explanation. Evaluators must recognize this structure to assess coherence.
  - Quick check question: Given the observation "mild bilateral lower lung opacity," what causal explanation would make this report clinically usable?

- Concept: **Metric weighting strategies**
  - Why needed here: The study shows rankings change under different weighting schemes (task-prioritized vs. equal). Understanding when to prioritize causal metrics vs. surface metrics is critical for system design.
  - Quick check question: Under what conditions would equal weighting produce misleading rankings?

## Architecture Onboarding

- Component map: Observation-based input → Model generation → BERTScore/Cosine Similarity/BioSentVec evaluation → GPT-White evaluation → GPT-Black evaluation → Expert assessment → Aggregation with weighting scheme → Final ranking

- Critical path:
  1. Generate reports from structured inputs using candidate models
  2. Compute similarity metrics against reference reports (automated)
  3. Run GPT-White evaluation with rubric-based prompts (requires API access)
  4. Run GPT-Black evaluation with bonus/penalty prompts (requires API access)
  5. Collect expert qualitative assessments (requires domain specialists)
  6. Apply weighting scheme and compute final rankings

- Design tradeoffs:
  - GPT-Black vs. GPT-White: Black offers higher sensitivity but requires more complex prompt engineering; White provides interpretable subscores but may miss subtle errors
  - Weighting scheme choice: Task-prioritized favors causal coherence; equal-weighting allows surface metrics to influence rankings (useful for fluency assessment)
  - Expert evaluation: Gold standard but unscalable; use for benchmarking, not production

- Failure signatures:
  - High BERTScore + low GPT-Black = lexically correct but causally incoherent reports
  - Narrow score variance across models (BioSentVec showed this) = metric lacks discriminative power
  - Ranking reversal under different weighting schemes = metric selection is driving conclusions, not model quality

- First 3 experiments:
  1. **Metric alignment validation**: Compute correlation between each automated metric and expert scores on a held-out report set. Target: GPT-White correlation > 0.7.
  2. **Weighting sensitivity analysis**: Vary GPT-Black weight from 10% to 40% and observe ranking stability. Document threshold where rankings stabilize.
  3. **Inter-rater reliability check**: Have 3+ experts evaluate the same 20 reports using the GPT-White rubric. Target: Cohen's κ > 0.6 before scaling expert evaluation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the implementation of multi-reviewer protocols and inter-rater reliability analysis impact the correlation between expert qualitative assessments and LLM-based metrics?
- Basis in paper: [explicit] The authors state in the Discussion that the current study "relied on a limited number of reviewers and lacked inter-rater agreement analysis," explicitly calling for future research to implement "multi-reviewer protocols" and "strategies for managing inter-reviewer variance."
- Why unresolved: Without quantifying inter-rater agreement, the reliability of the expert scores used as the ground truth benchmark remains uncertain, making it difficult to definitively assess how well GPT-White/Black truly align with "human" judgment.
- What evidence would resolve it: A re-evaluation of the reports using a larger pool of experts, reporting Cohen’s Kappa or similar agreement scores, followed by a re-calculation of the correlation coefficients between the consensus expert scores and the LLM metrics.

### Open Question 2
- Question: Does the integration of domain-specific knowledge graphs (e.g., SNOMED CT) into the evaluation pipeline improve the assessment of clinical validity compared to standalone LLM-based metrics?
- Basis in paper: [explicit] The paper suggests that "incorporating domain-specific knowledge graphs or ontology-driven tools (e.g., SNOMED CT) into evaluation could help bridge the gap between clinical logic and language-based models."
- Why unresolved: While GPT-Black shows high discriminative power, it relies on the LLM's parametric knowledge. It is unknown if an ontology-guided approach would catch factual inconsistencies or logical gaps that a generative model might overlook or hallucinate.
- What evidence would resolve it: A comparative study where a hybrid evaluator (LLM + Knowledge Graph) scores the same reports, specifically analyzing cases where GPT-Black diverges from the hybrid's scoring to identify missed clinical errors.

### Open Question 3
- Question: How robust are GPT-Black and GPT-White in evaluating complex scenarios involving temporally sequenced explanations or differential diagnoses?
- Basis in paper: [explicit] The Discussion notes that future evaluation frameworks "should evolve to handle more complex report scenarios—such as temporally sequenced explanations, multi-condition reasoning, and differential diagnoses."
- Why unresolved: The current study focused on specific input types (observation and multiple-choice), but it is unclear if the discriminative power of the metrics (particularly the 0.136 score variance of GPT-Black) scales effectively to longer, more complex reasoning chains.
- What evidence would resolve it: Testing the current metrics on a dataset specifically designed for temporal reasoning and differential diagnosis, analyzing whether the "bonus/penalty" logic of GPT-Black successfully distinguishes valid complex reasoning from plausible-sounding but logically flawed narratives.

## Limitations
- Expert evaluation sample size (n=20) limits external validity and confidence in correlation results.
- Single-task context (radiology reports) restricts generalizability to other medical domains.
- Correlation between automated metrics and expert judgment, while strong for GPT-White (0.701) and GPT-Black (0.733), shows room for improvement.

## Confidence
- **High confidence**: GPT-Black and GPT-White outperform similarity-based metrics in discriminative power and expert alignment for this specific medical report generation task.
- **Medium confidence**: The observed superiority of rule-based LLM evaluation generalizes to other medical reasoning domains beyond radiology.
- **Low confidence**: The relative weighting scheme (25%/25%/20%/20%/5%/5%) represents an optimal balance across diverse medical evaluation tasks.

## Next Checks
1. **Cross-domain validation**: Test GPT-Black and GPT-White on medical QA datasets (e.g., BioASQ) to assess generalizability beyond radiology report generation.
2. **Human evaluation scaling**: Expand expert assessment to 100+ reports with multiple raters to compute inter-rater reliability and confidence intervals for metric correlations.
3. **Hallucination robustness**: Conduct error analysis on GPT-Black/White outputs to quantify false positive/negative rates for causal reasoning assessment, particularly for clinically significant errors.