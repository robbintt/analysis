---
ver: rpa2
title: Stochastic Encodings for Active Feature Acquisition
arxiv_id: '2508.01957'
source_url: https://arxiv.org/abs/2508.01957
tags:
- feature
- acquisition
- features
- latent
- sefa
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses Active Feature Acquisition (AFA), where the
  goal is to iteratively select which features to measure to maximize prediction accuracy
  while minimizing the number of acquisitions. The authors introduce Stochastic Encodings
  for Feature Acquisition (SEFA), a novel approach that moves away from reinforcement
  learning and conditional mutual information (CMI) maximization.
---

# Stochastic Encodings for Active Feature Acquisition

## Quick Facts
- arXiv ID: 2508.01957
- Source URL: https://arxiv.org/abs/2508.01957
- Authors: Alexander Norcliffe; Changhee Lee; Fergus Imrie; Mihaela van der Schaar; Pietro Lio
- Reference count: 40
- Primary result: SEFA consistently outperforms state-of-the-art AFA baselines on synthetic and real-world datasets, including medical cancer classification, by using stochastic encoders, probability-weighted objectives, and information bottleneck regularization.

## Executive Summary
This paper addresses Active Feature Acquisition (AFA), where the goal is to iteratively select which features to measure to maximize prediction accuracy while minimizing the number of acquisitions. The authors introduce Stochastic Encodings for Feature Acquisition (SEFA), a novel approach that moves away from reinforcement learning and conditional mutual information (CMI) maximization. SEFA employs stochastic encoders to reason about features across possible latent realizations, uses a probability-weighted acquisition objective focusing on the most likely classes, and applies an information bottleneck regularization to remove noise. The method is trained in a supervised manner with a predictive loss, avoiding the training difficulties of RL. Extensive experiments on synthetic and real-world datasets (including medical cancer classification) demonstrate that SEFA consistently outperforms state-of-the-art baselines. The paper also provides theoretical analysis of CMI limitations and ablation studies validating each design choice.

## Method Summary
SEFA is a supervised learning approach for active feature acquisition that uses stochastic encoders to map observed features to a distribution over latent variables. During training, the model learns to predict labels from latent samples while regularizing with an information bottleneck to filter noise. For acquisition, SEFA scores features by computing gradients of predicted class probabilities with respect to latent samples, then maps these gradients back to feature indices. The acquisition objective is weighted by class probabilities and averaged over multiple latent samples to approximate non-myopic reasoning. Features are selected based on their gradient-based importance scores, and the process iterates until all features are acquired or prediction confidence is sufficient.

## Key Results
- SEFA consistently outperforms RL-based and CMI-based AFA baselines across synthetic, tabular, and image datasets
- The method demonstrates strong performance on medical datasets including METABRIC breast cancer and TCGA methylation data
- Ablation studies confirm that stochastic encoders, probability weighting, and information bottleneck each contribute to performance gains
- Theoretical analysis shows CMI's limitations in distinguishing between likely outcomes, which SEFA addresses through its probability-weighted objective

## Why This Works (Mechanism)

### Mechanism 1: Non-Myopic Acquisition via Latent Expectation
- **Claim:** By evaluating features over a distribution of possible latent states rather than a single point estimate, SEFA approximates a look-ahead search, potentially overcoming the myopic (short-sighted) limitations of standard Conditional Mutual Information (CMI) maximization.
- **Mechanism:** The method uses a stochastic encoder to map observed features to a distribution $p(z|x_O)$. During acquisition, it samples multiple latent vectors $z$. Each sample represents a possible realization of the unobserved data. By calculating the acquisition objective as an expectation over these samples, the model effectively weighs the utility of a feature across "possible futures" (unobserved feature values) rather than just the current observation.
- **Core assumption:** The stochastic latent space captures valid conditional dependencies between unobserved features, allowing the sampled "realizations" to serve as useful proxies for future observations.
- **Evidence anchors:** [abstract] "Acquisitions are made by reasoning about the features across many possible unobserved realizations in a stochastic latent space." [section 5.2] "By using stochastic encoders and taking an expectation... multiple possible latent realizations... are taken into account... we can analogously think of this inner loop as conducting Monte Carlo tree search."
- **Break condition:** If the latent space variance collapses to near zero (deterministic encoding), the "look-ahead" capability vanishes, reducing the method to a greedy approach.

### Mechanism 2: Information Bottleneck for Noise Filtration
- **Claim:** Applying an Information Bottleneck (IB) regularization forces the model to discard label-irrelevant variance, which may make the gradient-based acquisition scores more reliable than those calculated in the raw feature space.
- **Mechanism:** The loss function includes a KL-divergence term that limits the information capacity of the latent channel $Z$. This theoretically forces the encoder to retain only information predictive of $Y$. When gradients are calculated w.r.t. $Z$, they are less likely to be influenced by noisy variations in the input features that do not affect the output.
- **Core assumption:** The primary noise in the acquisition objective stems from feature variance irrelevant to the label, which the IB successfully filters without discarding critical signal.
- **Evidence anchors:** [abstract] "...applies an information bottleneck regularization to remove noise." [section 5.1] "We also impose an information bottleneck regularization term... minimizing how much information about the features is encoded in the latent variable."
- **Break condition:** If the regularization weight $\beta$ is set too high, the latent space loses predictive power, rendering gradients uninformative.

### Mechanism 3: Probability-Weighted Objective
- **Claim:** Weighting the acquisition objective by the predicted probability of likely classes focuses the search on distinguishing between plausible outcomes, countering the tendency of entropy-based methods to waste capacity on ruling out unlikely classes.
- **Mechanism:** Standard CMI maximizes the reduction in entropy, which can be achieved by making unlikely classes even less likely. SEFA weights the score of a feature by $p(Y=c|x_O)$. This ensures features are selected only if they help distinguish the current "top contenders" for the label.
- **Core assumption:** The goal of the system is to identify the single most likely class (0-1 loss minimization) rather than minimizing the overall uncertainty (entropy) of the distribution.
- **Evidence anchors:** [abstract] "...uses a probability-weighted acquisition objective focusing on the most likely classes." [section 4] "...CMI does not... distinguish between more probable outcomes... we desire to identify the most likely class."
- **Break condition:** In the very early stages of acquisition, if the predictor is miscalibrated or uncertain ($p \approx \text{uniform}$), the weighting provides little guidance.

## Foundational Learning

### Concept: Variational Autoencoders (VAE) & ELBO
- **Why needed here:** SEFA uses the VAE framework (Encoder $\to \mu, \sigma \to$ Sample $\to$ Decoder/Predictor). Understanding the trade-off between the reconstruction loss (predictive accuracy) and the KL-divergence (regularization) is essential to tuning the Information Bottleneck.
- **Quick check question:** If I increase the $\beta$ coefficient in the loss, what happens to the variance of the latent variable $z$?

### Concept: Gradient-based Attribution (Saliency Maps)
- **Why needed here:** The acquisition score is derived from the gradient of the prediction output with respect to the latent input. This is mathematically similar to saliency maps in computer vision, where $\nabla_x y$ indicates feature importance.
- **Quick check question:** Why does the paper normalize the gradient norm by the sum of all gradient norms for a specific sample?

### Concept: Monte Carlo Estimation
- **Why needed here:** The acquisition objective relies on averaging scores over multiple stochastic forward passes (sampling $z$). Understanding how variance decreases with sample size is critical for balancing inference speed and decision quality.
- **Quick check question:** How does the number of latent samples affect the stability of the acquisition decision?

## Architecture Onboarding

### Component map:
Input: Features $x$ + Binary Mask $m$ (indicating observed/unobserved)
-> Feature-wise Stochastic Encoders (separate MLPs per feature) outputting $\mu_i, \sigma_i$
-> Latent Space (concatenated vector $z$ with independently distributed components)
-> Predictor MLP (maps $z$ to class probabilities)
-> Acquisition Scorer (samples $z$, computes $\nabla_z p(y|z)$, aggregates scores per feature index)

### Critical path:
1. **Imputation:** Fill missing values (e.g., zeros) and concatenate the mask $m$
2. **Encoding:** Pass observed features through their specific encoders to get $\mu, \sigma$. Unobserved features use a default "missing" encoding or the prior
3. **Sampling:** Draw $N$ samples of $z$ using the reparameterization trick
4. **Acquisition Calculation:** For each sample, calculate the gradient of the *predicted class probability* w.r.t that sample. Map gradient norms back to feature indices. Average over $N$ samples
5. **Selection:** Pick feature $i$ with the highest score, observe it, and update the mask

### Design tradeoffs:
- **Factorized Encoders:** The architecture uses independent encoders per feature ($p(z|x_i)$) rather than a joint encoder ($p(z|x_{all})$). This allows clear gradient attribution to specific features but assumes conditional independence of latent components given the input (the Predictor later learns the correlations)
- **Inference Speed:** Inference is significantly slower than RL policies because it requires multiple forward/backward passes (sampling + gradient computation) per acquisition step

### Failure signatures:
- **Posterior Collapse:** If the KL term dominates, $\sigma$ goes to 0, and gradients vanish
- **Uniform Selection:** If the encoder variance is too high or gradients are noisy, acquisition scores flatten, leading to random selection
- **Myopic Acquisitions:** The model greedily picks the most predictive feature first, ignoring "indicator" features (as seen in Syn 1-3 baselines)

### First 3 experiments:
1. **Indicator Test:** Run the "Indicator Problem" (Section 4/Appendix F) to verify that the model selects the meta-feature (indicator) before the specific features. If it selects specific features first, the non-myopic mechanism is failing
2. **Ablation on Latent Samples:** Plot acquisition performance vs number of latent samples ($N=1$ to $200$). Confirm performance increases and plateaus, validating the Monte Carlo mechanism
3. **CMI Comparison:** Compare against the CMI baseline on a dataset with high noise (Appendix D) to verify that the Information Bottleneck provides robustness

## Open Questions the Paper Calls Out
- The method's performance gains are consistently demonstrated against strong baselines, but the theoretical analysis focuses primarily on CMI's myopia rather than proving SEFA's optimality in non-myopic settings. The Monte Carlo expectation over latent space provides a heuristic approximation rather than a formal look-ahead guarantee
- The information bottleneck regularization shows empirical effectiveness, but its impact on acquisition objective reliability versus potential signal loss is not quantitatively characterized. The optimal β appears dataset-dependent (0.1-0.5 range in experiments)
- The method applies to classification but not regression tasks because the acquisition objective requires separating class probabilities. The paper suggests a potential dual-head approach but does not validate it

## Limitations
- The method is limited to classification tasks and cannot directly handle regression problems due to its dependence on class probabilities for the acquisition objective
- Inference speed is significantly slower than RL-based approaches due to the need for multiple forward and backward passes per acquisition step
- The information bottleneck regularization parameter (β) requires empirical tuning and is sensitive to dataset characteristics, with no theoretical guidance for optimal selection

## Confidence

| Claim | Confidence |
|-------|------------|
| Probability-weighted acquisition objective effectively addresses CMI's inability to distinguish between likely outcomes | High |
| Stochastic encoder framework provides meaningful non-myopic behavior | Medium |
| Information bottleneck regularization improves robustness | Medium |

## Next Checks
1. **Ablation on KL Weight:** Systematically vary β (0.01, 0.1, 1.0, 10.0) across all datasets to quantify the information bottleneck's impact on both acquisition quality and prediction accuracy
2. **Latent Space Analysis:** Visualize the latent space variance distribution during acquisition. Confirm that high-acquisition-value features correspond to regions of high latent uncertainty
3. **Real-time Efficiency:** Measure inference time per acquisition step with varying sample sizes (N=50, 100, 200) to establish the practical speed-accuracy trade-off for deployment scenarios