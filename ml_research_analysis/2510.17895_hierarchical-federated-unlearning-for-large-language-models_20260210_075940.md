---
ver: rpa2
title: Hierarchical Federated Unlearning for Large Language Models
arxiv_id: '2510.17895'
source_url: https://arxiv.org/abs/2510.17895
tags:
- unlearning
- merging
- data
- retention
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of decentralized, heterogeneous
  LLM unlearning in federated settings where different parties hold unlearn and retain
  data asymmetrically. The authors propose a hierarchical federated unlearning framework
  (FULM) that decouples unlearning and retention into separate LoRA adapters and merges
  them using a two-step process: intra-cluster voting-based merging for near-iid tasks
  and inter-cluster summation for heterogeneous domains.'
---

# Hierarchical Federated Unlearning for Large Language Models

## Quick Facts
- arXiv ID: 2510.17895
- Source URL: https://arxiv.org/abs/2510.17895
- Authors: Yisheng Zhong; Zhengbang Yang; Zhuangdi Zhu
- Reference count: 40
- Primary result: Hierarchical federated unlearning framework achieves superior balance between forgetting and retention compared to baselines

## Executive Summary
This paper addresses the challenge of decentralized, heterogeneous LLM unlearning in federated settings where different parties hold unlearn and retain data asymmetrically. The authors propose a hierarchical federated unlearning framework (FULM) that decouples unlearning and retention into separate LoRA adapters and merges them using a two-step process: intra-cluster voting-based merging for near-iid tasks and inter-cluster summation for heterogeneous domains. Experiments on WMDP, TOFU, and MUSE benchmarks show that FULM achieves superior balance between forgetting (e.g., 51.84% in TOFU) and retention (e.g., 70.62% utility) compared to baselines like SUM, AVG, TIES, and KNOT, particularly in heterogeneous scenarios. The method scales to dynamic unlearning requests while preserving model utility, offering a privacy-preserving solution for LLM knowledge governance.

## Method Summary
The proposed framework operates through a hierarchical structure where unlearn and retain data are processed separately through distinct LoRA adapters. The method employs a two-step merging process: first, intra-cluster voting-based merging for near-iid tasks where participants vote on adapter contributions, and second, inter-cluster summation for heterogeneous domains where adapters are combined through weighted summation. This hierarchical approach allows the system to handle both homogeneous and heterogeneous data distributions while maintaining scalability for dynamic unlearning requests.

## Key Results
- FULM achieves 51.84% forgetting effectiveness on TOFU benchmark
- Model utility preservation reaches 70.62% compared to original model
- Outperforms SUM, AVG, TIES, and KNOT baselines in heterogeneous scenarios
- Successfully scales to dynamic unlearning requests while maintaining performance

## Why This Works (Mechanism)
The hierarchical approach works by strategically separating unlearn and retain data into distinct LoRA adapters, which allows for more precise control over the unlearning process. The two-step merging strategy leverages voting mechanisms for homogeneous data clusters where participant agreement is high, while using summation for heterogeneous clusters where data distributions vary significantly. This separation prevents interference between unlearn and retain objectives, and the hierarchical structure enables efficient scaling to multiple unlearning requests.

## Foundational Learning

- **Federated Learning**: Distributed model training across multiple parties without centralizing data - needed for privacy preservation in decentralized settings; quick check: verify data never leaves local devices
- **LoRA (Low-Rank Adaptation)**: Parameter-efficient fine-tuning method using low-rank matrices - needed for computational efficiency when handling multiple adapters; quick check: confirm rank selection doesn't impact performance
- **Adapter Merging**: Combining multiple fine-tuned adapters into a single model - needed to reconcile different unlearning requirements; quick check: validate merged model maintains both forgetting and retention objectives
- **Hierarchical Clustering**: Organizing data/parties into hierarchical groups - needed to handle varying degrees of data heterogeneity; quick check: test cluster formation with different similarity thresholds
- **Voting Mechanisms**: Consensus-based decision making among participants - needed for handling near-iid tasks where agreement is possible; quick check: measure voting consistency across similar tasks

## Architecture Onboarding

**Component Map**: Client devices -> Local adapter training -> Cluster formation -> Intra-cluster voting/merging -> Inter-cluster summation -> Global model

**Critical Path**: The most critical path is from local adapter training through cluster formation to the merging stages. Failures at any merging stage directly impact the final model's ability to balance forgetting and retention objectives.

**Design Tradeoffs**: The framework trades increased computational overhead (maintaining multiple adapters) for improved unlearning effectiveness and better handling of heterogeneous data. The separation of unlearn/retain adapters provides precision but requires more storage and computation compared to monolithic approaches.

**Failure Signatures**: 
- Poor cluster formation leads to suboptimal merging and reduced unlearning effectiveness
- Inconsistent voting in intra-cluster merging causes noisy adapter combinations
- Incorrect rank selection in LoRA adapters results in inadequate adaptation
- Summation-based inter-cluster merging fails when data distributions are too dissimilar

**First 3 Experiments**:
1. Validate adapter separation by comparing performance with monolithic unlearning approaches
2. Test intra-cluster voting accuracy across varying degrees of task similarity
3. Measure inter-cluster summation effectiveness on artificially created heterogeneous data distributions

## Open Questions the Paper Calls Out
None provided in the source material.

## Limitations
- Experimental validation limited to three specific benchmarks (WMDP, TOFU, MUSE)
- Separation strategy optimality across varying domain heterogeneities unverified
- Computational overhead of maintaining multiple adapter hierarchies not thoroughly characterized
- May not generalize across diverse real-world federated scenarios with different data distributions

## Confidence

- **High confidence**: The hierarchical framework design and two-step merging strategy are technically sound and logically constructed
- **Medium confidence**: The benchmark results showing improved forgetting-retention balance compared to baselines are reproducible but may be sensitive to hyperparameter choices
- **Medium confidence**: The claim about scalability to dynamic unlearning requests is supported by experimental design but lacks comprehensive stress testing

## Next Checks

1. Test the framework across additional benchmark datasets with varying degrees of data heterogeneity and class imbalance to assess robustness beyond the current three datasets
2. Conduct ablation studies to quantify the contribution of each framework component (intra-cluster voting, inter-cluster summation, adapter separation) to overall performance
3. Measure and report the computational overhead and memory requirements when scaling to hundreds of unlearning requests across multiple hierarchical levels