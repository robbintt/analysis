---
ver: rpa2
title: Scaling of hardware-compatible perturbative training algorithms
arxiv_id: '2501.15403'
source_url: https://arxiv.org/abs/2501.15403
tags:
- gradient
- perturbation
- network
- time
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Multiplexed gradient descent (MGD) is a perturbative zeroth-order
  training method for hardware-based neural networks that estimates gradients by correlating
  parameter perturbations with cost changes. The authors extend MGD to include both
  weight perturbation (perturbing individual weights) and node perturbation (perturbing
  neuron inputs), showing node perturbation can be more efficient for dense networks.
---

# Scaling of hardware-compatible perturbative training algorithms

## Quick Facts
- arXiv ID: 2501.15403
- Source URL: https://arxiv.org/abs/2501.15403
- Reference count: 40
- Primary result: MGD trains networks with over 1 million parameters to backpropagation-level accuracy

## Executive Summary
This paper introduces multiplexed gradient descent (MGD), a perturbative zeroth-order training method for hardware-based neural networks that estimates gradients by correlating parameter perturbations with cost changes. The authors extend MGD to include both weight perturbation (perturbing individual weights) and node perturbation (perturbing neuron inputs), demonstrating that node perturbation can be more efficient for dense networks. They show MGD can train large networks to accuracy matching backpropagation, with training time not scaling linearly with network size despite gradient estimation time doing so.

The framework is flexible enough to implement standard optimizers like Adam, achieving 37% fewer weight updates compared to vanilla gradient descent, and can accommodate hardware constraints such as limited write cycles. These results challenge previous assumptions about perturbative methods' scalability and suggest MGD is a practical solution for training future neuromorphic hardware.

## Method Summary
Multiplexed gradient descent (MGD) is a perturbative zeroth-order training method that estimates gradients by correlating parameter perturbations with cost changes. The method extends previous work by incorporating both weight perturbation (individually perturbing weights) and node perturbation (perturbing neuron inputs). Node perturbation can be more efficient for dense networks where weight perturbation would require too many computations. The authors demonstrate that MGD can train networks with over 1 million parameters to accuracy matching backpropagation, and that training time does not scale linearly with network size despite gradient estimation time doing so. The framework is flexible enough to implement standard optimizers like Adam, achieving 37% fewer weight updates compared to vanilla gradient descent, and can accommodate hardware constraints such as limited write cycles.

## Key Results
- MGD can train networks with over 1 million parameters to accuracy matching backpropagation
- Training time does not scale linearly with network size despite gradient estimation time doing so
- Node perturbation outperforms weight perturbation for dense networks
- MGD can implement standard optimizers like Adam, achieving 37% fewer weight updates

## Why This Works (Mechanism)
MGD works by estimating gradients through perturbation-based correlation between parameter changes and cost function changes. By perturbing parameters and measuring the resulting changes in the cost function, MGD can estimate gradients without requiring backpropagation. The multiplexed aspect allows simultaneous estimation of multiple gradients, improving efficiency. The flexibility to use either weight or node perturbation allows optimization based on network density and hardware constraints. The ability to implement standard optimizers like Adam demonstrates that the perturbative approach can capture the benefits of adaptive learning rates and momentum.

## Foundational Learning
- **Perturbative zeroth-order optimization**: A method that estimates gradients by measuring the response of the cost function to parameter perturbations. Why needed: Allows gradient-based optimization without explicit gradient computation, critical for hardware where backpropagation may be difficult. Quick check: Verify that small perturbations produce measurable changes in cost function.
- **Weight perturbation vs node perturbation**: Two strategies for perturbing parameters during gradient estimation. Weight perturbation changes individual weights, while node perturbation changes inputs to neurons. Why needed: Different perturbation strategies have different computational and hardware requirements. Quick check: Compare efficiency for sparse vs dense networks.
- **Correlation-based gradient estimation**: The core mechanism where gradients are estimated by correlating parameter perturbations with cost changes. Why needed: Provides a way to estimate gradients without explicit differentiation. Quick check: Verify correlation accuracy improves with more perturbation cycles.
- **Hardware constraints in neural network training**: Limitations like limited write cycles, finite precision, and timing constraints that affect training algorithms. Why needed: Real hardware implementations must work within these constraints. Quick check: Identify which constraints are most limiting for a given hardware platform.

## Architecture Onboarding

Component map: Data -> Network -> Perturbation Generator -> Cost Function -> Gradient Estimator -> Optimizer -> Weight Updates -> Network

Critical path: Perturbation -> Cost Measurement -> Correlation -> Gradient Update -> Weight Update

Design tradeoffs:
- Perturbation amplitude: Larger perturbations give clearer signals but may disrupt training
- Correlation window size: Longer windows improve gradient estimation but slow training
- Weight vs node perturbation: Weight perturbation works better for sparse networks, node for dense
- Hardware write cycles: Limiting factor that favors methods with fewer weight updates

Failure signatures:
- Poor gradient estimates manifest as oscillating or diverging training
- Excessive perturbation amplitudes cause training instability
- Insufficient perturbation signals lead to vanishing gradient estimates
- Hardware noise corrupts correlation measurements

3 first experiments:
1. Compare training convergence using weight perturbation vs node perturbation on a dense network
2. Measure gradient estimation accuracy as a function of perturbation amplitude
3. Implement Adam optimizer within MGD framework and compare weight update count to vanilla gradient descent

## Open Questions the Paper Calls Out
The paper focuses primarily on feedforward networks, leaving the applicability of MGD to recurrent architectures, convolutional networks, and other specialized topologies unexplored. Hardware compatibility demonstrations are theoretical rather than experimental, leaving open questions about practical implementation challenges. The flexibility to accommodate hardware constraints is demonstrated conceptually but not experimentally verified.

## Limitations
- Hardware-specific bottlenecks like finite precision arithmetic, noise, and timing constraints could alter the observed non-linear scaling behavior
- Comparison with backpropagation assumes idealized conditions - real hardware implementations may face additional challenges
- The claim that MGD can implement standard optimizers like Adam with 37% fewer weight updates is supported by simulations but requires experimental verification

## Confidence

High confidence: MGD can train large networks to backpropagation-level accuracy
Medium confidence: MGD training time scales sublinearly with network size in practice
Medium confidence: Node perturbation outperforms weight perturbation for dense networks

## Next Checks

1. Implement MGD on actual neuromorphic hardware to verify scaling behavior and identify hardware-specific bottlenecks
2. Test MGD on recurrent and convolutional architectures to establish broader applicability
3. Conduct ablation studies varying perturbation amplitude and correlation window size to optimize the tradeoff between gradient estimation accuracy and computational overhead