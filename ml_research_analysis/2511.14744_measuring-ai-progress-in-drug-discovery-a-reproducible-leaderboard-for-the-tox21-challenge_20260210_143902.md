---
ver: rpa2
title: 'Measuring AI Progress in Drug Discovery: A Reproducible Leaderboard for the
  Tox21 Challenge'
arxiv_id: '2511.14744'
source_url: https://arxiv.org/abs/2511.14744
tags:
- tox21
- prediction
- original
- leaderboard
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The Tox21 Challenge leaderboard restores evaluation on the original
  2015 dataset, which had been altered in subsequent benchmarks. By implementing an
  automated Hugging Face-based evaluation pipeline with standardized API calls, the
  authors enable reproducible comparison of toxicity prediction models.
---

# Measuring AI Progress in Drug Discovery: A Reproducible Leaderboard for the Tox21 Challenge

## Quick Facts
- arXiv ID: 2511.14744
- Source URL: https://arxiv.org/abs/2511.14744
- Reference count: 20
- Primary result: Descriptor-based models (SNN, DeepTox) remain competitive with modern graph neural networks on the restored Tox21 benchmark, suggesting limited progress in toxicity prediction over the past decade.

## Executive Summary
This paper addresses the challenge of measuring progress in AI-driven drug discovery by restoring and standardizing evaluation on the original Tox21 Challenge dataset from 2015. Through implementation of a centralized leaderboard with automated Hugging Face-based evaluation pipelines, the authors enable reproducible comparison of toxicity prediction models while eliminating dataset drift and evaluation inconsistencies that have plagued subsequent benchmarks. Re-evaluation of multiple baselines reveals that simple descriptor-based architectures like self-normalizing neural networks and ensemble methods from 2015 remain highly competitive with modern approaches including graph neural networks and foundation models, raising questions about whether substantial methodological progress has occurred in toxicity prediction over the past decade.

## Method Summary
The authors restore the original Tox21 Challenge dataset and implement a centralized evaluation framework using standardized API calls via Hugging Face Spaces. Models are deployed as FastAPI endpoints that accept SMILES strings and return probability predictions for all 12 toxicity endpoints. The leaderboard backend queries these endpoints, computes AUC metrics centrally, and stores results for comparison. Multiple baseline models are evaluated including descriptor-based approaches (SNN, Random Forest, XGBoost), graph neural networks (GIN, Chemprop), and foundation models (TabPFN, GPT-OSS), all using standardized 9,385 molecular features when applicable.

## Key Results
- Descriptor-based architectures (SNN median AUC 0.856, DeepTox 0.862) remain highly competitive with modern graph-based approaches (GIN AUC 0.806) on the original test set
- Random Forest (0.851) outperforms XGBoost (0.821), reversing typical tabular learning trends
- The centralized leaderboard successfully restores comparability across studies by eliminating dataset drift and standardizing evaluation protocols

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Dataset modifications during benchmark integration fragment evaluation comparability across studies.
- **Mechanism:** When Tox21 was integrated into MoleculeNet and DeepChem, the dataset underwent structural changes: molecules were removed, train-test splits were redesigned, and missing labels were imputed as zeros with masking. These alterations changed the underlying prediction task difficulty and label distribution, making performance across variants non-comparable even when the same metric (AUC) is reported.
- **Core assumption:** Progress measurement requires invariant evaluation conditions; changes to data composition or protocol introduce confounds that cannot be disentangled from model improvement.
- **Evidence anchors:**
  - [Abstract] "...the dataset was altered and labels were imputed or manufactured, resulting in a loss of comparability across studies."
  - [Section 3] "Consequently, the resulting Tox21-MoleculeNet dataset differs from the original Tox21-Challenge both in data composition and in evaluation protocol, leading to results that are not directly comparable to the original leaderboard."
  - [corpus] No direct corpus evidence on benchmark drift; related work focuses on toxicity prediction methods rather than evaluation infrastructure.

### Mechanism 2
- **Claim:** Centralized API-based evaluation eliminates local preprocessing and metric computation variance.
- **Mechanism:** Models expose a standardized `/predict` endpoint via FastAPI on Hugging Face Spaces. The leaderboard backend sends SMILES strings, receives predictions in [0,1] range, and computes AUC centrally. This prevents participants from applying different preprocessing, handling missing labels inconsistently, or computing metrics differently.
- **Core assumption:** Standardized input-output contracts are sufficient for fair comparison; models can be treated as black boxes that map SMILES to probability vectors.
- **Evidence anchors:**
  - [Abstract] "...we make all baselines and evaluated models publicly accessible for inference via standardized API calls to Hugging Face Spaces."
  - [Section 5] "Each model exposes a single /predict endpoint that accepts a list of SMILES strings and returns a nested JSON dictionary of predictions for the twelve Tox21 targets."
  - [corpus] Open ASR Leaderboard (arXiv:2510.06961) demonstrates similar centralized evaluation principles for speech recognition, suggesting this pattern generalizes.

### Mechanism 3
- **Claim:** Descriptor-based architectures remain competitive with modern graph-based and foundation model approaches on toxicity prediction.
- **Mechanism:** The SNN (2017), a single self-normalizing neural network trained on 9,385 molecular features (ECFP6 fingerprints, MACCS keys, RDKit descriptors), achieves median AUC of 0.856, nearly matching DeepTox (0.862), a large ensemble from 2015. Random Forest (0.851) outperforms XGBoost (0.821), reversing typical tabular learning trends. Graph-based GIN (0.806) and message-passing Chemprop (0.818) underperform simpler baselines.
- **Core assumption:** The Tox21 test set with ~650 molecules is sufficiently representative to draw conclusions about method capability; the challenging cluster-based split (structurally dissimilar test compounds) adequately probes generalization.
- **Evidence anchors:**
  - [Section 6] "Overall, descriptor-based architectures remain highly competitive nearly a decade after the original challenge."
  - [Table 1] SNN achieves 0.856 ± 0.008 median AUC vs. DeepTox 0.862; GIN achieves only 0.806 ± 0.018.
  - [corpus] "Task-Specific Sparse Feature Masks" (arXiv:2512.11412) reports similar findings that feature-based approaches can match or exceed graph neural networks on molecular toxicity, providing convergent evidence.

## Foundational Learning

- **Concept: SMILES molecular representation**
  - **Why needed here:** All models receive input as SMILES strings; understanding their structure (atomic symbols, bonds, rings) is prerequisite to comprehending how feature extraction or graph construction works.
  - **Quick check question:** Given SMILES "CCO" (ethanol), can you identify the carbon backbone and functional group? If given "c1ccccc1" (benzene), can you explain the aromatic ring notation?

- **Concept: Multi-task binary classification with sparse labels**
  - **Why needed here:** Tox21 contains 12 binary endpoints (e.g., NR-AR, SR-MMP) with ~30% missing labels. Models must output probabilities for all tasks while handling that not all compounds have ground truth for all endpoints.
  - **Quick check question:** If a molecule has missing label for NR-ER but present labels for other 11 tasks, how should AUC computation handle this during training vs. evaluation?

- **Concept: Area Under ROC Curve (AUC) for imbalanced classification**
  - **Why needed here:** The official metric averages AUC across 12 tasks with 3-20% positive rates. AUC measures ranking quality independent of threshold selection.
  - **Quick check question:** If a model predicts all compounds as 0.51 (slightly above 0.5), what AUC would it achieve? If it correctly ranks all positives higher than negatives but outputs poorly calibrated probabilities (e.g., 0.51 vs. 0.49), is AUC affected?

## Architecture Onboarding

- **Component map:** Model Space (HF) -> FastAPI -> Leaderboard Backend -> Results Dataset
- **Critical path:**
  1. Implement `predict_fn` in the FastAPI template to convert SMILES → features → predictions
  2. Deploy model as Hugging Face Space with `/predict` endpoint
  3. Submit model card + Space URL to leaderboard
  4. Backend queries model for 647 test SMILES, computes AUC, stores results
  5. Manual verification confirms reproducibility before publication

- **Design tradeoffs:**
  - **Centralized evaluation** vs. **participant flexibility**: Standardization prevents gaming but limits preprocessing creativity
  - **API-based inference** vs. **code submission**: Models remain private/proprietary if desired, but requires hosting infrastructure
  - **Original test set** vs. **expanded benchmarks**: Historical comparability preserved, but smaller test set (647 molecules) may have higher variance

- **Failure signatures:**
  - Missing predictions for any SMILES-target pair → submission rejected
  - Non-deterministic outputs across identical queries → reproducibility verification fails
  - Predictions outside [0,1] range or NaN values → validation error
  - Model timeout on batch queries → leaderboard marks as failed

- **First 3 experiments:**
  1. **Establish baseline:** Clone the GIN Space, modify feature extraction to use only ECFP fingerprints (drop RDKit descriptors), retrain, and compare AUC delta to isolate descriptor contribution
  2. **Test missing label handling:** Train two SNN variants—one ignoring missing labels vs. one treating them as negatives—evaluate on original test set to quantify imputation bias magnitude
  3. **Probe ensemble effects:** Submit single SNN model vs. 5-model ensemble (varying seeds), measure whether ensemble gains (if any) justify computational cost increase

## Open Questions the Paper Calls Out

- **Question:** Have methods genuinely improved on other bioactivity prediction benchmarks (e.g., SIDER, MUV), or has benchmark drift similarly obscured stagnation?
  - **Basis in paper:** [explicit] Authors state: "similar re-evaluations may be warranted for other bioactivity prediction endpoints such as SIDER or MUV, where benchmark drift may likewise obscure scientific progress."
  - **Why unresolved:** Only Tox21 was systematically re-evaluated; other benchmarks in MoleculeNet/TDC may have undergone comparable alterations.
  - **What evidence would resolve it:** Applying the same leaderboard restoration methodology to SIDER, MUV, and other datasets to compare original vs. altered benchmark performance.

- **Question:** Why do descriptor-based neural networks (SNN) and ensembles (DeepTox) from 2014–2017 remain competitive against modern graph neural networks and foundation models?
  - **Basis in paper:** [explicit] Authors note results "leave it unclear whether substantial progress in toxicity prediction has been achieved over the past decade."
  - **Why unresolved:** The paper documents the lack of progress but does not diagnose whether this stems from dataset saturation, methodological limitations, or task difficulty.
  - **What evidence would resolve it:** Ablation studies isolating the contribution of molecular representations, architecture complexity, and training data scale across method generations.

- **Question:** Can foundation models achieve competitive toxicity prediction through few-shot or in-context learning without task-specific training?
  - **Basis in paper:** [explicit] Authors state: "the framework can be readily extended to few-shot and zero-shot evaluation settings, as illustrated by our pre-trained baselines."
  - **Why unresolved:** TabPFN showed competitive results without Tox21-specific training; GPT-OSS 120B zero-shot lagged substantially, but the potential of prompt engineering or fine-tuning remains unexplored.
  - **What evidence would resolve it:** Systematic evaluation of foundation models under few-shot prompting and parameter-efficient fine-tuning protocols on the restored benchmark.

## Limitations
- Evaluation limited to 12 specific toxicity endpoints in a dataset of 647 test molecules, making it difficult to generalize findings about "AI progress in drug discovery" to broader molecular property prediction tasks
- The Tox21 Challenge's cluster-based test split, while challenging, may not represent the diversity of modern chemical space
- The dataset is now over a decade old, and progress may have occurred in toxicity prediction domains not captured by these particular assays

## Confidence
- **High confidence:** Core claims about evaluation infrastructure (standardized API, reproducible baselines, and restored dataset integrity) are directly verifiable through the leaderboard and code
- **Medium confidence:** Conclusions about limited progress in toxicity prediction, given the restricted scope to a single, potentially outdated benchmark
- **Low confidence:** Claims about feature-based methods' competitiveness with modern architectures, as the evaluation conditions (sparse labels, specific train-test split) may not reflect real-world deployment scenarios

## Next Checks
1. **Cross-benchmark validation:** Evaluate the same models on ToxCast (12,000+ assays, ~1,800 chemicals) to determine if conclusions hold across larger, more diverse toxicity datasets
2. **Label imputation sensitivity:** Systematically vary missing label handling strategies (impute as negative vs. ignore vs. use uncertainty-aware training) to quantify impact on model rankings
3. **Temporal generalizability:** Test whether conclusions hold when applying models to predict toxicity for molecules discovered or synthesized after 2015, assessing real-world applicability