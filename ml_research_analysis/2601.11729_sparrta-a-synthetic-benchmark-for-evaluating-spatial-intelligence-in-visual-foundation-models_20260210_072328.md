---
ver: rpa2
title: 'SpaRRTa: A Synthetic Benchmark for Evaluating Spatial Intelligence in Visual
  Foundation Models'
arxiv_id: '2601.11729'
source_url: https://arxiv.org/abs/2601.11729
tags:
- spatial
- sparrta
- probing
- objects
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces SpaRRTa, a synthetic benchmark designed\
  \ to evaluate spatial intelligence in Visual Foundation Models (VFMs) by focusing\
  \ on relative object positioning tasks. Unlike traditional 3D metrics, SpaRRTa targets\
  \ abstract, human-like spatial reasoning\u2014specifically, whether models can decode\
  \ the relative spatial relations between objects from either an egocentric (camera)\
  \ or allocentric (third-party object) viewpoint."
---

# SpaRRTa: A Synthetic Benchmark for Evaluating Spatial Intelligence in Visual Foundation Models

## Quick Facts
- **arXiv ID:** 2601.11729
- **Source URL:** https://arxiv.org/abs/2601.11729
- **Reference count:** 40
- **Key outcome:** SpaRRTa reveals spatial information in VFMs is distributed across patch-level representations and inaccessible via global pooling, with 3D supervision improving relational encoding

## Executive Summary
This paper introduces SpaRRTa, a synthetic benchmark for evaluating spatial intelligence in Visual Foundation Models (VFMs) by focusing on relative object positioning tasks. Unlike traditional 3D metrics, SpaRRTa targets abstract, human-like spatial reasoning—specifically, whether models can decode the relative spatial relations between objects from either an egocentric (camera) or allocentric (third-party object) viewpoint. The benchmark uses Unreal Engine 5 to generate photorealistic scenes with fully controllable object arrangements and precise spatial annotations.

The authors evaluate a wide range of VFMs, including self-supervised models (DINO, MAE, SPA, CroCo), vision-language models (CLIP), and models trained with explicit 3D supervision (VGGT). Key findings include: (1) Spatial information is primarily encoded in dense patch-level representations rather than global features; spatially selective probing (e.g., AbMILP, efficient probing) significantly outperforms global pooling. (2) VGGT, trained with 3D supervision, achieves the highest performance, especially under patch-aware probing, indicating that explicit 3D cues enrich patch-level geometric structure. (3) Allocentric spatial reasoning is consistently more challenging than egocentric reasoning across all models. (4) Environmental complexity (cluttered vs. homogeneous scenes) affects performance, with simpler scenes yielding higher accuracy.

## Method Summary
The benchmark generates photorealistic synthetic images using Unreal Engine 5 across 5 environments (Forest, Desert, Winter Town, Bridge, City) at 224×224 resolution. For each environment, 5,000 images are generated for egocentric tasks and 10,000 for allocentric tasks, with 80/10/10 train/val/test splits. The task involves 4-way classification of relative spatial relations {left, right, front, back} between source and target objects from two viewpoints. Models are frozen and probed using three methods: Linear (global average pooling), AbMILP (single attention map), and Efficient Probing (multi-query attention). Training uses AdamW with weight decay 0.001, learning rates 10⁻², 10⁻³, 10⁻⁴, dropout 0.2-0.6, batch size 256, and epochs 1000-500 depending on method.

## Key Results
- Spatial relation information is more accessible in dense patch representations than in global features, with efficient probing outperforming linear probing across all models
- VGGT, trained with 3D supervision, achieves highest performance, especially under patch-aware probing, indicating explicit 3D cues enrich patch-level geometric structure
- Allocentric spatial reasoning is consistently more challenging than egocentric reasoning across all models and environments
- Environmental complexity affects performance, with simpler scenes (Desert) yielding higher accuracy than complex ones (City)

## Why This Works (Mechanism)

### Mechanism 1: Patch-Level Spatial Encoding Dominates Over Global Representations
Spatial relation information in VFMs is distributed across local patch tokens and is substantially obscured by global pooling operations. Efficient probing and AbMILP learn attention-based selection over patch sequences, filtering background noise and attending to task-relevant regions (source, target, viewpoint objects). This recovers spatial structure that GAP destroys by uniform averaging. The assumption is that spatial relations require identifying multiple distinct objects and their relative positions—information that cannot be compressed into a single global vector without loss.

### Mechanism 2: 3D Supervision Restructures Inter-Object Attention Flow
Explicit 3D supervision (camera pose, depth, tracking) induces a representational shift from object-centric feature extraction to relational scene encoding in late transformer layers. In VGGT vs. DINO-v2, attention analysis shows VGGT decreases self-attention (object→itself) and increases cross-object attention (object→other entities) in layers 15–24. This redistribution encodes spatial dependencies between distinct entities. The pre-training objective shapes where geometric information flows; metric 3D regression forces the model to track spatial relationships rather than just semantic identity.

### Mechanism 3: Allocentric Reasoning Requires Implicit Viewpoint Transformation
Allocentric spatial relation recognition is systematically harder than egocentric because it demands perspective-taking—reasoning from a non-camera coordinate frame. The model must suppress image-space positions and infer the geometric arrangement relative to an arbitrary viewpoint object. This requires decoupling visual appearance from spatial logic. Allocentric reasoning correlates with 3D geometric tasks (R²=0.34–0.43) but not semantic classification (R²≈0), suggesting it engages different representations.

## Foundational Learning

- **Concept: Patch tokens vs. global tokens in Vision Transformers**
  - Why needed here: The paper's central finding is that spatial information lives in patch-level features, not [CLS] or GAP vectors. Understanding ViT's dual representation is essential to interpret probing results.
  - Quick check question: Can you explain why averaging all patch tokens might destroy spatial relation information?

- **Concept: Attention-based probing (AbMILP, Efficient Probing)**
  - Why needed here: These methods enable selective aggregation over patches, recovering local spatial structure. The paper uses them as diagnostic tools to demonstrate what information is recoverable from frozen VFMs.
  - Quick check question: How does AbMILP differ from global average pooling in terms of what information it can preserve?

- **Concept: Egocentric vs. allocentric spatial reference frames**
  - Why needed here: The benchmark defines two task variants that disentangle visual perception from perspective-taking. Allocentric tasks reveal limitations in current VFMs that egocentric tasks do not.
  - Quick check question: Why would a model that succeeds at "the car is left of the tree from the camera's view" fail at "the car is left of the tree from the person's view"?

## Architecture Onboarding

- **Component map:**
  Unreal Engine 5 renderer -> Scenario generator -> VFM encoder (frozen) -> Probing head -> Evaluator

- **Critical path:**
  1. Asset selection → ensure semantic coherence with environment
  2. Scene generation → apply rejection sampling for ambiguity zones and visibility
  3. Feature extraction → select layer (late-intermediate layers 18–20 optimal for ViT-L)
  4. Probing → use efficient probing for maximum signal recovery
  5. Evaluation → average across environments and object triples

- **Design tradeoffs:**
  - Data scale: 5,000 images sufficient for egocentric; 10,000 needed for allocentric to avoid overfitting
  - Probing complexity: Linear probing is fastest but severely underestimates spatial capability; efficient probing is more diagnostic but requires training multiple attention queries
  - Layer selection: Final layer is convenient but late-intermediate (18–20 for ViT-L) yields peak spatial accuracy

- **Failure signatures:**
  - Linear probing accuracy near random (25%) on allocentric task → spatial information exists but is patch-distributed
  - Large performance drop in cluttered environments (City vs. Desert) → model cannot isolate task-relevant patches
  - High semantic classification but low SpaRRTa → semantic and spatial capabilities are orthogonal

- **First 3 experiments:**
  1. Run linear, AbMILP, and efficient probing on DINO-v2 for both task variants across all five environments to establish the patch-level hypothesis
  2. Compare DINO-v2 (+reg) vs. VGGT using identical probing to quantify how 3D training changes spatial accessibility at patch vs. global levels
  3. Extract features from layers 12, 18, 20, 24 in VGGT to identify where spatial representations peak and confirm late-intermediate optimization

## Open Questions the Paper Calls Out

- **Do Visual Foundation Models acquire a general notion of spatial awareness, or do they merely overfit to specific geometric objectives without capturing the abstractions that underpin human spatial understanding?** The paper questions if inconsistent performance across spatial tasks implies a lack of general awareness, but does not determine if the spatial knowledge is transferable or merely a byproduct of training.

- **Can the fully controllable synthetic data engine be utilized to effectively train spatially-aware world models for environmental interaction?** The current study uses the engine solely for static evaluation rather than for training dynamic models or reinforcement learning agents.

- **What specific architectural modifications or training objectives are required to close the performance gap between egocentric and allocentric spatial reasoning?** Results consistently show allocentric reasoning is significantly more challenging, implying current architectures struggle with the necessary perspective-taking, but the paper does not propose solutions.

## Limitations

- The benchmark uses synthetic, clean, and controlled environments, which may not fully represent real-world complexity and noise, raising questions about generalizability.
- The analysis of 3D supervision effects is based on comparing VGGT to DINO-v2, which is not a controlled ablation study due to differences in pre-training objectives and dataset sizes.
- The probing methods rely on the assumption that trained probes can effectively extract and interpret spatial information from frozen model representations.

## Confidence

- **High confidence:** Spatial information is primarily encoded in dense patch-level representations rather than global features, supported by consistent results across multiple VFMs and probing methods.
- **Medium confidence:** 3D supervision restructures inter-object attention flow, based on compelling attention analysis though not a controlled experiment.
- **Medium confidence:** Allocentric reasoning is consistently more challenging than egocentric, demonstrated by consistent performance gaps though allocentric tasks may involve additional cognitive demands.

## Next Checks

1. **Real-world validation:** Evaluate SpaRRTa on real-world images with precise spatial annotations from 3D scene datasets to assess generalizability.
2. **Controlled 3D supervision ablation:** Train DINO-v2 with explicit 3D supervision on the same dataset as VGGT while keeping other factors constant to isolate the impact of 3D supervision.
3. **Alternative probing methods:** Apply additional probing techniques like supervised attention to the same frozen VFM representations to validate the robustness of spatial information extraction.