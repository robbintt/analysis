---
ver: rpa2
title: Dynamic Dual Buffer with Divide-and-Conquer Strategy for Online Continual Learning
arxiv_id: '2505.18101'
source_url: https://arxiv.org/abs/2505.18101
tags:
- buffer
- memory
- odedm
- samples
- refresh
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses catastrophic forgetting in Online Continual
  Learning (OCL) by introducing a dynamic dual memory framework, ODEDM, which uses
  a short-term buffer for fast memory and a long-term buffer structured into sub-buffers
  anchored by cluster prototypes. ODEDM employs K-means clustering and optimal transport-based
  mechanisms to retain diverse and category-specific samples while reducing computational
  cost via a Divide-and-Conquer (DAC) strategy.
---

# Dynamic Dual Buffer with Divide-and-Conquer Strategy for Online Continual Learning

## Quick Facts
- arXiv ID: 2505.18101
- Source URL: https://arxiv.org/abs/2505.18101
- Reference count: 40
- Primary result: Achieves up to 20% accuracy improvement over baselines in Online Continual Learning with a dual-buffer system

## Executive Summary
ODEDM addresses catastrophic forgetting in Online Continual Learning by introducing a dynamic dual memory framework. It combines a short-term buffer using reservoir sampling with a long-term buffer structured into sub-buffers anchored by class-specific cluster prototypes. The system employs K-means clustering and optimal transport-based mechanisms to retain diverse and category-specific samples while reducing computational cost via a Divide-and-Conquer strategy. Evaluated across CIFAR10, CIFAR100, and TINYIMG under standard and imbalanced settings, ODEDM consistently achieves state-of-the-art performance with superior sample balance and lower forgetting rates.

## Method Summary
ODEDM implements a dual-buffer architecture where a short-term memory captures recent data via reservoir sampling while a long-term memory organizes samples into class-specific sub-buffers anchored by K-means prototypes. Sample selection for the long-term buffer uses Sinkhorn distance (optimal transport) to prototypes, prioritizing representative samples. A Divide-and-Conquer strategy recursively partitions data to make the computationally expensive Sinkhorn optimization feasible. The system dynamically allocates buffer capacity, shifting from short-term to long-term as tasks accumulate. ODEDM maintains plasticity through the short-term buffer while ensuring stability via consolidated long-term knowledge, all within the constraints of online learning where each data batch is seen exactly once.

## Key Results
- Achieves up to 20% accuracy improvement over baselines (DER++, VR-MCL, POCL) across CIFAR10, CIFAR100, and TINYIMG
- Demonstrates superior sample balance and lower forgetting rates compared to existing methods
- Sinkhorn distance selection outperforms L2 distance, with ablation showing 45.01% vs 44.00% accuracy
- DAC optimization reduces computational overhead while maintaining sample quality

## Why This Works (Mechanism)

### Mechanism 1
The dual-buffer architecture with dynamic allocation mitigates the stability-plasticity conflict by using a short-term buffer for recent plasticity and a long-term buffer anchored by K-means prototypes for consolidated stability. As tasks accumulate, capacity shifts from short-term to long-term, prioritizing retained knowledge over transient data. This works under the assumption that semantic clusters exist in the data and can be captured by K-means. Break condition: fails on strictly random noise streams where no meaningful clusters exist.

### Mechanism 2
Optimal Transport (Sinkhorn distance) selection maximizes retention of statistically representative samples by measuring the cost of moving probability mass between incoming samples and existing prototypes. This differs from L2 by considering semantic richness rather than just Euclidean proximity. The core assumption is that Sinkhorn distance validly proxies for representativeness in feature space. Break condition: if feature drift invalidates pre-computed prototypes, leading to non-representative sample selection.

### Mechanism 3
Divide-and-Conquer (DAC) strategy makes Sinkhorn optimization computationally feasible by recursively partitioning data, computing local distances, and merging closest clusters. This approximates global optimal selection without quadratic complexity. The assumption is that local optima can be aggregated to approximate global optimum without significant quality loss. Break condition: insufficient recursion depth leads to coarse approximation missing critical diverse samples.

## Foundational Learning

- **Concept: Online Continual Learning (OCL) & Catastrophic Forgetting**
  - Why needed: This is the fundamental problem ODEDM solves - single-pass data streams where buffer management is critical
  - Quick check: How does the "Online" constraint change buffer update frequency compared to standard Experience Replay?

- **Concept: K-means Clustering & Prototypes**
  - Why needed: The long-term memory architecture is explicitly structured around K-means centers
  - Quick check: In ODEDM, are prototypes static after first task, or do they update as new samples arrive?

- **Concept: Optimal Transport (Sinkhorn Distance)**
  - Why needed: This is the selection criteria for the buffer, differing from Euclidean distance by considering "cost" of moving probability mass
  - Quick check: Why would Sinkhorn distance be preferred over standard L2 distance for measuring similarity between sample and prototype?

## Architecture Onboarding

- **Component map:** Model Update -> Buffer Retrieval -> K-means Prototyping -> DAC + Sinkhorn Selection -> Dynamic Allocation
- **Critical path:** 1) Train on current batch + retrieved samples, 2) Run K-means to find/update prototypes, 3) Apply DAC to reduce search space, 4) Select samples via Sinkhorn distance, 5) Shift capacity from short-term to long-term buffer
- **Design tradeoffs:** Higher Long-term ratio favors stability but reduces plasticity; deeper DAC recursion improves accuracy but increases latency; Sinkhorn is more accurate but slower than L2
- **Failure signatures:** Mode collapse from poor K-means prototypes, incompatibility with specific loss functions like iCaRL's nearest-mean classifier, runtime explosion if DAC disabled or k too large
- **First 3 experiments:** 1) Run DER++ with/without ODEDM on CIFAR-10 to verify dynamic allocation and accuracy gain, 2) Swap Sinkhorn for L2 on small buffer (200 samples) to confirm 1-4% accuracy lift, 3) Run on large buffer (5120) with/without DAC to verify efficiency claims

## Open Questions the Paper Calls Out

### Open Question 1
Can the ODEDM framework be optimized to maintain consistent performance gains on high-complexity datasets like TINYIMG and CIFAR100? The paper identifies scalability as the main limitation, noting effectiveness is occasionally limited on these datasets due to increased complexity, resulting in marginal improvements in some imbalanced settings.

### Open Question 2
What specific mechanism causes iCaRL to collapse when augmented with ODEDM on small buffers in complex scenarios? The paper notes iCaRL "collapses on TINYIMG... which may be attributed to incompatibilities between its nearest-mean classifier and the feature representations induced by ODEDM," but doesn't isolate the exact conflict or propose a fix.

### Open Question 3
Why does the Sinkhorn distance yield higher replay effectiveness than L2 distance despite both metrics showing similarly high alignment with the underlying data manifold? The ablation study observes Sinkhorn (45.01%) outperforms L2 (44.00%) despite visual and quantitative alignment being nearly identical (0.999 similarity for both).

## Limitations
- Underspecified hyperparameters including Sinkhorn regularization parameter and SGD learning rate schedule
- Assumes existence of semantic clusters; fails on highly noisy or randomized data streams
- Computational complexity claims depend on DAC effectiveness without runtime comparisons provided
- Integration conflicts with certain loss functions like iCaRL's nearest-mean classifier

## Confidence

- **High Confidence:** Dual-buffer architecture is valid and effective, with consistent accuracy gains and forgetting rate reductions over strong baselines
- **Medium Confidence:** Sinkhorn distance provides meaningful improvement over L2, but exact magnitude is sensitive to implementation details
- **Low Confidence:** Specific values for Sinkhorn regularization parameter and SGD learning rate schedule are missing, preventing exact reproduction

## Next Checks

1. **Sinkhorn vs. L2 Ablation:** Implement both distance metrics in ODEDM and run controlled experiment on CIFAR-10 with small buffer (200 samples). Measure accuracy difference to verify claimed 1-4% lift from Sinkhorn.

2. **DAC Depth Scaling:** Run ODEDM on large buffer (5120 samples) with DAC enabled at different recursion depths (depth=2, 3, 4). Compare wall-clock training time and final accuracy to quantify speed-accuracy tradeoff.

3. **Imbalanced Data Stress Test:** Evaluate ODEDM on imbalanced CIFAR-100 (even-indexed samples). Monitor per-class accuracy distribution to ensure K-means clustering runs per-class and minority classes aren't dropped from long-term buffer.