---
ver: rpa2
title: 'Human-Corrected Labels Learning: Enhancing Labels Quality via Human Correction
  of VLMs Discrepancies'
arxiv_id: '2511.09063'
source_url: https://arxiv.org/abs/2511.09063
tags:
- learning
- labels
- label
- vlms
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Human-Corrected Labels (HCL), a novel weakly
  supervised learning setting that strategically combines multiple VLMs with selective
  human correction to improve annotation quality. HCL uses discrepancies among VLMs
  to identify uncertain samples, correcting only those via human input while accepting
  consistent predictions directly, thereby reducing annotation cost while maintaining
  high accuracy.
---

# Human-Corrected Labels Learning: Enhancing Labels Quality via Human Correction of VLMs Discrepancies

## Quick Facts
- arXiv ID: 2511.09063
- Source URL: https://arxiv.org/abs/2511.09063
- Reference count: 18
- Proposes HCL, a weakly supervised learning method that selectively applies human correction to VLM discrepancies, achieving superior classification accuracy while drastically reducing annotation effort.

## Executive Summary
This paper introduces Human-Corrected Labels (HCL), a novel weakly supervised learning framework that strategically combines multiple Vision-Language Models (VLMs) with selective human correction to improve annotation quality. HCL identifies uncertain samples where VLMs disagree and applies human correction only to these cases, while accepting consistent predictions directly, thereby reducing annotation costs while maintaining high accuracy. The authors theoretically derive a risk-consistent estimator that integrates both human-corrected labels and VLM predictions, and propose a conditional probability method to estimate label distributions. Extensive experiments on six benchmark datasets show that HCL significantly outperforms state-of-the-art weakly supervised methods, achieving comparable results to fully supervised learning while drastically reducing annotation effort.

## Method Summary
HCL operates by generating pseudo-labels from multiple VLMs, accepting consistent predictions directly, and applying human correction only to discrepant samples. The method uses a risk-consistent estimator that combines both human-corrected labels and VLM predictions, with conditional probability estimation using a λ-weighted combination of VLM priors and model predictions. The framework extracts 768-dimensional CLIP features (frozen) and trains a linear classifier using the HCL loss. Training runs for 30 epochs with AdamW optimizer, batch size 64, and step learning rate decay. The approach supports flexible VLM integration but requires careful selection of model combinations for optimal performance.

## Key Results
- HCL significantly outperforms state-of-the-art weakly supervised methods on six benchmark datasets
- Achieves classification accuracy comparable to fully supervised learning while drastically reducing annotation effort
- Demonstrates robustness to label noise and effective integration of human correction with VLM predictions

## Why This Works (Mechanism)
The HCL framework leverages the complementary strengths of multiple VLMs to identify uncertain samples through discrepancy detection. By applying human correction only to these uncertain cases, it maximizes the value of human annotation while minimizing cost. The risk-consistent estimator theoretically guarantees that the empirical risk converges to the true risk, ensuring reliable learning even with partial human correction. The conditional probability method effectively combines VLM priors with model predictions, creating a more accurate label distribution that captures both the initial VLM knowledge and the model's learning progress.

## Foundational Learning
- **Weakly Supervised Learning**: Learning from imperfect or incomplete labels rather than fully annotated datasets. Why needed: Enables training with limited human annotation. Quick check: Verify pseudo-labels are generated before main training.
- **Risk-Consistent Estimators**: Estimators where empirical risk converges to true risk as sample size increases. Why needed: Guarantees theoretical reliability of learning from partially corrected labels. Quick check: Confirm MSE-like loss formulation in implementation.
- **Conditional Probability Estimation**: Computing P(y|x) using both VLM priors and model predictions. Why needed: Provides accurate label distributions for training. Quick check: Monitor P_CLIP distribution for numerical stability.
- **VLM Discrepancy Detection**: Identifying samples where multiple VLMs produce different predictions. Why needed: Determines which samples require human correction. Quick check: Log consistency rates across datasets.
- **Ensemble Learning**: Combining predictions from multiple models to improve robustness. Why needed: Reduces individual VLM errors through consensus. Quick check: Compare performance with 2 vs 3 VLMs.
- **Feature Extraction**: Using frozen CLIP backbone to extract 768-dimensional image features. Why needed: Provides rich visual representations without additional training cost. Quick check: Verify feature dimensionality matches expected size.

## Architecture Onboarding
- **Component Map**: VLMs (CLIP + Qwen) -> Consistency Detection -> Human Correction (simulated) -> Feature Extraction (frozen CLIP) -> Linear Classifier -> Risk-Consistent Loss -> Model Output
- **Critical Path**: VLM inference → consistency check → conditional probability estimation → loss computation → model update
- **Design Tradeoffs**: Selective human correction vs. full annotation; theoretical guarantees vs. implementation complexity; model ensemble size vs. performance consistency
- **Failure Signatures**: Performance degradation with too many VLMs; numerical instability in softmax with high temperature; incorrect consistency detection due to parsing errors
- **First Experiments**:
  1. Verify VLM output parsing and consistency detection by logging raw model outputs
  2. Test conditional probability estimation with varying τ values for numerical stability
  3. Implement and compare two-VLM vs three-VLM configurations to understand scaling behavior

## Open Questions the Paper Calls Out
- How can the optimal combination and number of VLMs be determined for the HCL framework, given that adding a third model resulted in performance degradation on Caltech-101 compared to using only two?
- Can the mixing coefficient λ for conditional probability estimation be adaptively learned or scheduled per dataset to prevent performance degradation?
- How robust is the HCL risk-consistent estimator when human annotators provide imperfect corrections (i.e., label noise)?

## Limitations
- Performance degradation observed when scaling from two to three VLMs suggests limitations in ensemble scaling
- Theoretical guarantees assume perfect human corrections, which may not reflect real-world annotation errors
- Implementation details for VLM prompting and human correction simulation are not fully specified

## Confidence
- **High**: Core HCL framework, experimental methodology, and overall performance claims
- **Medium**: Theoretical guarantees and their practical implications
- **Low**: Implementation details for VLM prompting and human correction simulation

## Next Checks
1. Verify VLM output parsing and consistency detection by logging raw model outputs and checking label format
2. Test the conditional probability estimation with varying τ values to assess sensitivity
3. Implement and compare the two-VLM vs three-VLM configurations to understand ensemble scaling behavior