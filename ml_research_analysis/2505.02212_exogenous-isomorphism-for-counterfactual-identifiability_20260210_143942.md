---
ver: rpa2
title: Exogenous Isomorphism for Counterfactual Identifiability
arxiv_id: '2505.02212'
source_url: https://arxiv.org/abs/2505.02212
tags:
- causal
- counterfactual
- identifiability
- exogenous
- mapping
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces exogenous isomorphism as a novel framework
  for achieving counterfactual identifiability in structural causal models (SCMs).
  By defining exogenous isomorphism and showing that exogenous-isomorphism-identifiability
  (EI-identifiability) implies counterfactual-identifiability (L3-identifiability),
  the authors provide a simpler path to ensuring that all SCMs satisfying given assumptions
  yield consistent counterfactual answers.
---

# Exogenous Isomorphism for Counterfactual Identifiability

## Quick Facts
- arXiv ID: 2505.02212
- Source URL: https://arxiv.org/abs/2505.02212
- Authors: Yikang Chen; Dehui Du
- Reference count: 40
- Key outcome: Introduces exogenous isomorphism framework for counterfactual identifiability in SCMs, proving EI-identifiability implies L3-identifiability for TM-SCMs and validating empirically

## Executive Summary
This paper introduces exogenous isomorphism as a novel framework for achieving counterfactual identifiability in structural causal models (SCMs). By defining exogenous isomorphism and showing that exogenous-isomorphism-identifiability (EI-identifiability) implies counterfactual-identifiability (L3-identifiability), the authors provide a simpler path to ensuring that all SCMs satisfying given assumptions yield consistent counterfactual answers. They focus on two classes of SCMs—Bijective SCMs (BSCMs) and Triangular Monotonic SCMs (TM-SCMs)—and prove that these are EI-identifiable under specific assumptions involving Markovianity, causal order, and the observational distribution.

## Method Summary
The method constructs neural TM-SCMs by enforcing triangular monotonicity and Markovianity constraints in normalizing flow architectures. The approach takes observational data as input, learns a causal model that satisfies exogenous isomorphism conditions, and produces counterfactual inferences with theoretical consistency guarantees. Four prototype models (DNME, TNME, CMSM, TVSM) are trained using MLE/NLL loss with change of variables formula, with counterfactual inference implemented via a pseudo potential response algorithm. The framework requires known causal order and assumes the true data-generating process follows TM-SCM structure.

## Key Results
- Exogenous isomorphism is defined and proven to be a sufficient condition for counterfactual identifiability (L3-identifiability)
- TM-SCMs and BSCMs are proven to be EI-identifiable under Markovianity, causal order, and observational distribution assumptions
- Neural TM-SCM implementation shows practical effectiveness on synthetic datasets
- Violating any key assumption (TM structure, Markovianity, or causal order) significantly degrades counterfactual consistency

## Why This Works (Mechanism)

### Mechanism 1: Exogenous Isomorphism as a Sufficient Condition for L3-Identifiability
If two recursive SCMs share causal order and their exogenous variables are linked by a probability-preserving bijection maintaining causal mechanism structure, they yield identical counterfactual answers. Theorem 3.2 proves structural alignment propagates through potential response mapping, ensuring counterfactual distributions are indistinguishable.

### Mechanism 2: Triangular Monotonicity Enforcing Identifiability
Constraining SCMs to be TM-SCMs with Markovian noise creates unique identifiability from observational distribution and causal order. Corollary 5.4 shows the TM constraint links observational distribution directly to counterfactual transport K_M, triggering EI-identifiability.

### Mechanism 3: Neural Parameterization for Consistency
Neural networks constrained to satisfy TM-SCM requirements allow practical learning of counterfactually consistent models. Mapping Γ to neural architecture enforcing triangularity and monotonicity (e.g., autoregressive flows) ensures learned models inherit counterfactual consistency through observational likelihood maximization.

## Foundational Learning

**Concept: Pearl Causal Hierarchy (L1, L2, L3)**
- Why needed: Primary goal is achieving L3-identifiability; understanding L1=association, L2=intervention, L3=counterfactuals is essential
- Quick check: Can a model consistent at L2 still be inconsistent at L3?

**Concept: Solution Mapping (Γ) and Recursiveness**
- Why needed: Paper relies on unique solution mapping Γ(u)=v that inverts SCM; this mapping is analyzed for Triangular Monotonicity
- Quick check: Why does "recursive" property guarantee unique solution mapping Γ?

**Concept: Pushforward Measure (X_♯P)**
- Why needed: Heavily relies on measure theory to define distribution transformations; observational distribution P_V is pushforward of P_U through Γ
- Quick check: If P_U is standard normal and Γ(u)=u+5, what is pushforward distribution P_V?

## Architecture Onboarding

**Component map:** Observational data -> Neural TM-SCM (bijective generative model) -> Counterfactual inferences

**Critical path:**
1. Define topology: Establish causal order (assumed known)
2. Construct architecture: Implement Γ_θ with lower triangular Jacobian and strictly positive/negative values
3. Optimize: Minimize NLL of observational data using change of variables formula
4. Verify: Check counterfactual consistency (CTF RMSE) against validation sets

**Design tradeoffs:** Expressiveness vs. identifiability (standard VAEs/GANs lack L3 guarantees); Stability (CMSM less stable than TVSM during training)

**Failure signatures:**
- High CTF RMSE with Low OBS WD: Model fits data but fails counterfactuals (likely assumption violation)
- Training divergence: Violation of strict monotonicity constraint (numerical instability in log-det-Jacobian)

**First 3 experiments:**
1. Sanity Check (Ablation w/o Order): Train with randomized/reversed causal order; expect OBS WD decreases but CTF RMSE remains high
2. Constraint Check (Ablation w/o TM): Train standard neural SCM without triangular monotonicity; expect divergent/inconsistent counterfactuals
3. Distribution Check (Ablation w/o Markovian): Use correlated noise distributions; expect degraded counterfactual consistency

## Open Questions the Paper Calls Out
None

## Limitations
- Theory relies on existence of unique solution mapping and strict monotonicity, but empirical verification in real-world data is not demonstrated
- Neural parameterization shows promising results but lacks theoretical guarantees for high-dimensional approximation
- Markovianity assumption is critical but difficult to verify in practice; violations break guarantees without clear detection methods

## Confidence

**High confidence:** Theoretical framework establishing exogenous isomorphism as sufficient condition for L3-identifiability and empirical validation showing assumption violations degrade performance

**Medium confidence:** Neural TM-SCM implementation details and practical effectiveness (some architectural specifics underspecified)

**Medium confidence:** Completeness of identifiable SCM classes (focus on TM-SCMs without fully exploring BSCMs)

## Next Checks

1. **Verify assumption checking:** Implement systematic tests for three key assumptions (TM structure, Markovianity, causal order) on real datasets to identify when framework breaks down

2. **Scale evaluation:** Test neural TM-SCM approach on larger, higher-dimensional datasets beyond 10-variable limit to assess practical scalability

3. **Architecture robustness:** Compare different neural architectures (DNME vs TVSM vs CMSM) across broader range of data distributions to quantify stability differences mentioned in paper