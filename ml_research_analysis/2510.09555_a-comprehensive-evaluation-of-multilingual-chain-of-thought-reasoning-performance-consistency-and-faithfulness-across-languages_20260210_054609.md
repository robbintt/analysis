---
ver: rpa2
title: 'A Comprehensive Evaluation of Multilingual Chain-of-Thought Reasoning: Performance,
  Consistency, and Faithfulness Across Languages'
arxiv_id: '2510.09555'
source_url: https://arxiv.org/abs/2510.09555
tags:
- languages
- thinking
- language
- traces
- consistency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents the first comprehensive evaluation of multilingual
  Chain-of-Thought (CoT) reasoning, examining three dimensions: performance, consistency,
  and faithfulness. The study investigates how Large Reasoning Models (LRMs) perform
  across languages when reasoning in the same language as the prompt.'
---

# A Comprehensive Evaluation of Multilingual Chain-of-Thought Reasoning: Performance, Consistency, and Faithfulness Across Languages

## Quick Facts
- **arXiv ID**: 2510.09555
- **Source URL**: https://arxiv.org/abs/2510.09555
- **Reference count**: 40
- **Key outcome**: This paper presents the first comprehensive evaluation of multilingual Chain-of-Thought (CoT) reasoning, examining three dimensions: performance, consistency, and faithfulness. The study investigates how Large Reasoning Models (LRMs) perform across languages when reasoning in the same language as the prompt.

## Executive Summary
This paper introduces the first comprehensive evaluation framework for multilingual Chain-of-Thought reasoning across three dimensions: performance (accuracy and language compliance), consistency (crosslingual answer consistency and trace interchange consistency), and faithfulness (sensitivity to perturbations). The study reveals substantial disparities in multilingual reasoning capabilities, with high-resource languages like English and Chinese consistently outperforming low-resource ones. Through crosslingual thinking trace interchanging, the authors demonstrate that trace quality varies significantly across languages, affecting downstream performance when traces are swapped. The perturbation-based faithfulness analysis shows that larger models and English reasoning exhibit lower surface-level reliance on thinking traces, suggesting potential differences in internal reasoning mechanisms.

## Method Summary
The authors evaluate multilingual Chain-of-Thought reasoning using two language control strategies: explicit instruction ("Please always think in [language]") and prompt hacking (prefix after thinking token). They test models on MMMLU (250 samples per language across 15 languages) and MGSM (full test set, 10 languages), measuring language compliance via GlotLID classification, accuracy via exact match with boxed answers, and consistency via IoU of correct predictions across language pairs. Crosslingual trace interchanging involves generating traces in one language and swapping them into prompts of another language (BaseSub/HackSub/TransSub methods). Faithfulness is assessed through truncation (first/middle/last third of trace) and error injection (altering numbers in final sentence), measuring prediction sensitivity to these perturbations.

## Key Results
- High-resource languages (English, Chinese) consistently outperform low-resource languages in CoT reasoning accuracy and language compliance
- Crosslingual trace interchange reveals substantial quality disparities: high-resource traces improve low-resource performance when swapped, while low-resource traces degrade high-resource performance
- Larger models and English reasoning show lower surface-level dependence on thinking traces, suggesting latent reasoning mechanisms
- Languages other than English exhibit greater reliance on thinking traces, with truncation and error injection having stronger effects

## Why This Works (Mechanism)

### Mechanism 1: Pretraining-Induced Language Bias
Models exhibit strong preferences for reasoning in high-resource languages, particularly English and Chinese, due to imbalanced pretraining exposure. Models default to languages with richer internal representations, and when forced to reason in low-resource languages via prompt hacking, compliance improves but accuracy often degrades—suggesting the model is operating outside its representational comfort zone. Assumption: Language-specific reasoning quality correlates with training data exposure during pretraining. Evidence: High-resource languages like English and Chinese consistently outperforming low-resource ones; related work on cross-lingual collapse confirms systematic drift to dominant pretraining language.

### Mechanism 2: Crosslingual Trace Quality Variance
Thinking traces generated in high-resource languages are semantically richer and more effective than those in low-resource languages, enabling asymmetric transfer when interchanged. The crosslingual trace interchanging method reveals that injecting English/Chinese traces into low-resource prompts improves accuracy, while the reverse degrades it. This suggests trace quality—not just prompt language—drives performance gaps. Assumption: Semantic coherence of reasoning steps varies systematically by language, independent of prompt content. Evidence: Crosslingual thinking trace interchanging method reveals substantial inconsistencies in trace quality across languages; low-resource languages benefit from substitution with high-resource thinking traces.

### Mechanism 3: Scale-Dependent Faithfulness Asymmetry
Larger models and English reasoning show lower surface-level reliance on thinking traces, suggesting either stronger latent reasoning or memorization; smaller models are more faithful to final reasoning steps. Truncation and error injection perturbations affect predictions less for larger models and English traces. This may indicate that larger models perform "latent-state reasoning" internally, making surface traces less causally connected to outputs. Assumption: Faithfulness can be inferred from sensitivity to perturbations. Evidence: Larger models become less dependent on surface-level reasoning traces while smaller models show stronger faithfulness to final reasoning steps; languages other than English exhibit greater reliance on thinking traces.

## Foundational Learning

- **Chain-of-Thought (CoT) Prompting**: Understanding what "thinking traces" are (intermediate steps between tokens) and how they're elicited is foundational to interpreting all results. Quick check: Can you explain why CoT might improve reasoning but not necessarily be faithful?

- **Multilingual Representation Imbalance**: Performance gaps across languages stem from pretraining data imbalance; this explains both accuracy differences and trace quality variance. Quick check: Why might forcing a model to reason in a low-resource language harm performance even if the prompt is semantically equivalent?

- **Faithfulness vs. Performance**: The paper distinguishes between "does the trace help?" (performance) and "does the trace reflect actual reasoning?" (faithfulness)—these are independent dimensions. Quick check: If a model produces correct answers but ignores its thinking trace, is the trace faithful?

## Architecture Onboarding

- **Component map**: Input prompts (multilingual) -> Language control (explicit instruction/prompt hacking) -> Thinking trace generation (between tokens) -> Final answer extraction (boxed) -> Perturbation modules (truncation/error injection)

- **Critical path**: 1) Receive prompt in language L, 2) Apply language control strategy (measure compliance rate), 3) Generate thinking trace (may or may not match L), 4) Produce final answer (measure accuracy, consistency), 5) For faithfulness testing: perturb trace and re-measure

- **Design tradeoffs**: Compliance vs. accuracy (prompt hacking enforces target-language reasoning but often reduces accuracy); Model scale vs. interpretability (larger models may be less faithful to traces, making reasoning harder to audit); High-resource reasoning vs. user alignment (better performance in English/Chinese vs. user preference for native-language traces)

- **Failure signatures**: Low sentence-level compliance (<0.2) despite explicit instruction (models defaulting to English); Large accuracy drops (>30%) when forced to reason in low-resource languages via prompt hacking; Low matching ratio on error injection (e.g., 0.12 for English in R1-Qwen-32B) indicating low faithfulness; Inconsistent answers (>40% divergence) between typologically distant language pairs

- **First 3 experiments**: 1) Run explicit instruction and prompt hacking on your model across 5+ languages; compute sentence-level compliance rates to identify default language bias; 2) Generate traces in English and your target language; swap them and measure accuracy change to assess trace quality asymmetry; 3) Apply error injection (alter final-sentence numbers) to traces in 3+ languages; compute matching ratio to determine if your model copies or ignores the corrupted trace

## Open Questions the Paper Calls Out

### Open Question 1
What mechanistic factors drive the observed inconsistencies in thinking trace quality and faithfulness across languages? The authors state they "do not provide a mechanistic explanation for why these inconsistencies arise" and suggest future research could apply mechanistic interpretability methods. This remains unresolved because the paper establishes empirical patterns but does not investigate internal model representations, attention patterns, or layer-wise processing that cause these differences. A mechanistic interpretability study (e.g., using probing classifiers, attention analysis, or causal intervention on model internals) identifying which components correlate with cross-lingual trace quality and faithfulness disparities would resolve this.

### Open Question 2
Do the findings generalize across diverse model architectures beyond the Qwen and Llama families studied? The authors acknowledge "experiments are limited in the number of models" and that future work could extend the evaluation framework to a broader set of models. This remains unresolved because all tested models derive from Qwen2.5 or Llama3 base models, which may share architectural or training-data biases affecting multilingual reasoning. Replicating the evaluation on additional architectures (e.g., Mistral, Gemini, GPT-4 variants) and reporting whether patterns persist would resolve this.

### Open Question 3
How robust is the crosslingual trace interchanging method to imperfections in machine translation? The TransSub method relies on Google Translate to standardize traces to English, but translation quality is not analyzed. Translation errors could confound whether performance differences stem from trace semantics or translation artifacts. The paper assumes translation adequately preserves semantics for comparison, but low-resource languages often have lower-quality translation. A controlled ablation comparing TransSub results using high-quality human translations versus machine translation for a subset of low-resource languages would resolve this.

### Open Question 4
How do adversarial or more sophisticated perturbations affect faithfulness assessments across languages? The authors state "more sophisticated or adversarial perturbations (e.g., paraphrasing, distractor reasoning) remain unexplored." Current perturbations test basic reliance but do not simulate real-world noise or attacks that could differently impact multilingual traces. Extending the perturbation framework to include adversarial attacks and measuring whether faithfulness patterns persist or change across languages would resolve this.

## Limitations
- The study cannot definitively attribute multilingual reasoning disparities to specific causal mechanisms, distinguishing between actual reasoning capability differences and surface-level token generation patterns
- Faithfulness measurements rely on perturbation sensitivity, which may conflate genuine reasoning patterns with heuristic shortcuts
- The evaluation focuses on model-synthesized traces rather than human-annotated reasoning, limiting ground-truth faithfulness establishment
- Crosslingual trace interchange assumes semantic equivalence across translations, though acknowledged as an approximation

## Confidence

**High confidence**: The empirical finding that high-resource languages (English, Chinese) consistently outperform low-resource languages in CoT reasoning accuracy and language compliance across multiple model scales and tasks.

**Medium confidence**: The claim that trace quality varies systematically by language and drives performance asymmetry, inferred from crosslingual substitution experiments without direct semantic quality measurement.

**Medium confidence**: The assertion that larger models show reduced faithfulness to thinking traces, supported by perturbation experiments but with alternative explanations (stronger internal reasoning, memorization) not ruled out.

## Next Checks
1. **Ground-truth faithfulness validation**: Compare model-synthesized traces against human-annotated reasoning chains on a subset of problems to establish baseline faithfulness independent of perturbation-based inference.

2. **Semantic trace quality analysis**: Use bilingual evaluation metrics (e.g., COMET) to directly compare semantic quality of traces across languages, separating this from downstream accuracy effects.

3. **Controlled pretraining exposure**: Train multilingual models with balanced vs. imbalanced language data to experimentally test whether pretraining exposure fully explains the observed performance asymmetries.