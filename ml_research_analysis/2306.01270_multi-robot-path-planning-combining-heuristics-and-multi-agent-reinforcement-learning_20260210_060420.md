---
ver: rpa2
title: Multi-Robot Path Planning Combining Heuristics and Multi-Agent Reinforcement
  Learning
arxiv_id: '2306.01270'
source_url: https://arxiv.org/abs/2306.01270
tags:
- path
- planning
- learning
- algorithm
- robot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of multi-robot path planning in
  dynamic environments where robots must avoid collisions with other moving robots
  while minimizing travel distance. The authors propose a novel method called MAPPOHR
  that combines heuristic search, empirical rules, and multi-agent reinforcement learning
  (MARL).
---

# Multi-Robot Path Planning Combining Heuristics and Multi-Agent Reinforcement Learning

## Quick Facts
- arXiv ID: 2306.01270
- Source URL: https://arxiv.org/abs/2306.01270
- Reference count: 32
- Primary result: MAPPOHR achieves lower total moving cost (mean 37.3 vs 50.9 for D* Lite) in multi-robot path planning with 2 moving planes in airport environment

## Executive Summary
This paper proposes MAPPOHR, a hybrid method combining heuristic search and multi-agent reinforcement learning for multi-robot path planning in dynamic environments. The approach uses a two-layer architecture: a heuristic planner (D* or A*) provides global path guidance while a MARL-based real-time planner (MAPPO) makes local decisions. The method embeds domain-specific heuristic rules in both the action output layer and reward function to accelerate learning. Experiments in airport scenarios with two moving planes show MAPPOHR achieves lower total moving costs than pure learning methods (MAPPO, MADDPG) and pure heuristic methods (D* Lite), though with a slightly lower success rate due to replanning time constraints.

## Method Summary
MAPPOHR combines a two-layer architecture for multi-robot path planning. The first layer uses a heuristic search planner (D* Lite or A*) to generate an initial global path for each robot. The second layer employs a MAPPO-based real-time planner that makes step-wise decisions among four actions (Move, Wait, Back, Replan) based on local observations. During movement, the heuristic planner replans paths when instructed by the real-time planner. The method embeds domain-specific heuristic rules in the action output layer and reward function to accelerate training convergence. Training uses centralized training with distributed execution (CTDE), where the critic receives pooled observations while actors execute based on local observations only.

## Key Results
- MAPPOHR achieves lower total moving cost (mean 37.3) compared to D* Lite (mean 50.9) in 10 airport test cases
- The method demonstrates higher learning efficiency than pure learning methods due to integration of heuristics and prior rule knowledge
- Success rate of 0.9 (slightly lower than D* Lite's 1.0) due to time constraints on replanning
- MAPPOHR effectively combines waiting and replanning strategies to minimize travel distance

## Why This Works (Mechanism)

### Mechanism 1
A two-layer hierarchy separating global guidance from local reactive decisions reduces total travel cost compared to pure replanning approaches. The heuristic planner computes a global path once at initialization, while the real-time planner makes step-wise decisions and can invoke the heuristic planner for replanning when needed. This allows the system to choose cheaper strategies (waiting, backing) when replanning is unnecessary. The global path remains a useful reference even when local deviations are needed, as dynamic obstacles are intermittent enough that frequent replanning is wasteful.

### Mechanism 2
Embedding domain-specific heuristic rules in the action output layer and reward function accelerates training convergence and produces more human-like policies. Rules such as "if collision risk in next step, forbid forward movement" and "if all other agents wait, current agent may move or replan" are hard-coded into the policy output layer. Similarly, negative rewards are triggered when all robots wait or all replan simultaneously. This constrains the action space, reducing exploration of clearly suboptimal behaviors. The encoded rules are assumed to be correct and do not systematically exclude globally optimal strategies.

### Mechanism 3
Centralized training with distributed execution (CTDE) mitigates non-stationarity in multi-agent environments while preserving decentralized runtime efficiency. During training, the critic network receives pooled observations from all agents plus agent-specific features (agent ID, relative distances). Actor networks share parameters but execute based solely on local observations. This allows the critic to evaluate actions in the context of others' behaviors, stabilizing learning, while execution remains communication-free. Training scenarios must adequately cover the coordination patterns agents will encounter at deployment.

## Foundational Learning

- **Concept: Multi-Agent Reinforcement Learning (MARL) and Non-Stationarity**
  - Why needed here: Multiple robots learn simultaneously, making each agent's environment non-stationary (other agents' policies change during training). Understanding CTDE and parameter sharing is essential.
  - Quick check question: Why does standard single-agent RL struggle when multiple agents learn concurrently in a shared environment?

- **Concept: Partially Observable Markov Decision Process (POMDP)**
  - Why needed here: Agents have limited local observation fields and must decide based on partial information. The observation vector (goal distance, other-agent distance, path obstacles, lateral obstacles) reflects this structure.
  - Quick check question: What information must an agent infer or remember to act optimally when it cannot observe the full state?

- **Concept: Heuristic Search (A*, D*, D* Lite)**
  - Why needed here: The method relies on D* Lite for global path generation and on-demand replanning. Understanding how these algorithms handle dynamic obstacles informs when replanning is triggered.
  - Quick check question: How does D* Lite efficiently update paths when edge costs change, compared to full A* replanning?

## Architecture Onboarding

- **Component map:**
  Environment Model -> Heuristic Search Planner -> Real-Time Planner (MAPPO)
  Static global path (A* or D*) -> Dynamic local replanning (D* Lite) -> Na agent networks with actor/critic

- **Critical path:**
  1. Initialize global guidance path via heuristic planner for each robot
  2. At each timestep, each agent receives local observation vector (og, od, op, on)
  3. Real-time planner selects action (Move/Wait/Back/Replan), subject to embedded heuristic rules
  4. If Replan action selected, invoke D* Lite local planner to compute new path
  5. Execute action; if training, collect reward and update networks via MAPPO
  6. Repeat until all agents reach goals or max decision length (60 steps)

- **Design tradeoffs:**
  - Cost vs. Success Rate: MAPPOHR achieves lower total moving cost (mean 37.3 vs. 50.9) but success rate 0.9 vs. D* Lite's 1.0 due to replanning time limit (3 seconds)
  - Rule Rigidity vs. Learning Flexibility: Embedded rules accelerate training but bound policy expressiveness
  - Centralized Training vs. Decentralized Execution: Shared observations during training improve coordination; execution requires no inter-agent communication

- **Failure signatures:**
  - Timeout-induced failures: Case 0 failed due to replanning exceeding the 3-second threshold
  - Deadlock detection: All agents waiting triggers negative reward (rule-based penalty)
  - Excessive replanning: High planning cost indicates over-reliance on heuristic planner vs. learned waiting/backing strategies

- **First 3 experiments:**
  1. **Baseline replication**: Run MAPPOHR vs. D* Lite vs. MADDPG on the 10 airport test cases; verify total moving cost, planning cost, waiting cost, and success rate match reported values
  2. **Ablation on domain knowledge**: Compare MAPPOHR vs. MAPPOH (no rules) vs. MAPPO (no heuristics or rules) vs. PPOHR (no agent communication) to isolate contribution of each component
  3. **Scalability probe**: Test with 3–5 robots on modified scenarios to assess whether CTDE and parameter sharing hold as agent count increases; monitor training stability and success rate degradation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the MAPPOHR architecture maintain its superiority over heuristic baselines when scaled to environments with a significantly higher number of agents (N > 2)?
- Basis in paper: Section 5.1 explicitly limits the experimental setup to "two planes moving simultaneously," while Section 2.1 notes that performance in similar multi-robot problems often decreases as the number of robots increases.
- Why unresolved: The paper validates the method only in a pairwise conflict setting, leaving the combinatorial complexity of multi-agent (N > 2) interactions untested.
- Evidence: Performance metrics (Success Rate, Total Moving Cost) from experiments conducted with 5, 10, or 50 agents in the same environment.

### Open Question 2
- Question: How can the integration of the heuristic replanning step be optimized to eliminate the timeout failures observed in the experiments?
- Basis in paper: Section 6.1 states that MAPPOHR achieved a 0.9 success rate compared to D* Lite's 1.0, noting that "MAPPOHR failed to plan case 0" due to the "time cost of replanning."
- Why unresolved: The current implementation penalizes the agent when replanning takes too long, causing the mission to fail within the maximum decision length.
- Evidence: A modified model where the "Replan" action executes asynchronously or utilizes a more efficient local planner to maintain a 1.0 success rate in complex scenarios.

### Open Question 3
- Question: Does the policy trained on the specific airport environment generalize to different map topologies or agent shapes without retraining?
- Basis in paper: Section 5.1 describes training on 20 cases within a single fixed "airport" map (111x171), and Section 3.2 defines a specific convex polygon model for the planes.
- Why unresolved: The paper does not test the trained model on unseen maps or different kinematic constraints, which is a known challenge for Reinforcement Learning methods.
- Evidence: Zero-shot testing results of the trained MAPPOHR model on standard grid benchmarks (e.g., random mazes) or with agents of different sizes.

## Limitations

- The paper's reported performance gains hinge on specific environment details (obstacle layout, robot start/goal configurations) that are not fully specified
- The exact mechanism by which the 71-dimensional observation vector is constructed and normalized is unclear
- Rule implementation details for the 6 output-layer and 3 reward rules are underspecified

## Confidence

- **High confidence**: The two-layer hierarchical architecture (global heuristic guidance + local reactive planning) is technically sound and well-supported by MARL and heuristic search literature
- **Medium confidence**: The specific domain rules and reward shaping likely accelerate training, but their exact impact is difficult to verify without complete implementation details
- **Low confidence**: The exact mechanism by which the 71-dimensional observation vector is constructed and normalized is unclear, making it difficult to reproduce the results

## Next Checks

1. **Environment fidelity check**: Reconstruct the 111×171 airport grid with the described perimeter and middle obstacles. Verify that robot start/goal pairs generate the 10 test scenarios with the reported conflict patterns.

2. **Rule implementation audit**: Implement the 6 output-layer heuristic rules and 3 negative reward conditions exactly as described. Test with simple scenarios (two robots on collision course) to verify rules fire correctly.

3. **Timeout sensitivity analysis**: Run MAPPOHR with varying replanning timeouts (1s, 3s, 5s) on Case 0 to determine the threshold where success rate drops from 1.0 to 0.9, and assess whether the 3s limit is too conservative.