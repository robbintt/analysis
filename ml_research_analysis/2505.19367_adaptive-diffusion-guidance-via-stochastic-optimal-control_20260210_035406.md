---
ver: rpa2
title: Adaptive Diffusion Guidance via Stochastic Optimal Control
arxiv_id: '2505.19367'
source_url: https://arxiv.org/abs/2505.19367
tags:
- guidance
- diffusion
- arxiv
- process
- stochastic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the lack of theoretical foundation for guidance
  scheduling in diffusion models, particularly the ambiguity around the distribution
  being sampled and whether guided sampling preserves the conditional data support.
  The authors provide a theoretical formalization showing that (1) applying guidance
  with any positive strength increases the likelihood of the conditioning class both
  with high probability and on average, and (2) generated samples are guaranteed to
  remain within the conditional data manifold.
---

# Adaptive Diffusion Guidance via Stochastic Optimal Control

## Quick Facts
- arXiv ID: 2505.19367
- Source URL: https://arxiv.org/abs/2505.19367
- Reference count: 40
- One-line primary result: The paper provides theoretical foundation for guidance scheduling in diffusion models, showing that positive guidance strength increases class likelihood while preserving conditional support, and introduces an adaptive framework that outperforms constant guidance.

## Executive Summary
This paper addresses the lack of theoretical foundation for guidance scheduling in diffusion models. The authors formalize why positive guidance strength increases class likelihood and preserves conditional support, then introduce a stochastic optimal control framework to learn adaptive guidance schedules. Experiments on a 2D toy example demonstrate that the learned adaptive guidance outperforms constant guidance in reward, class alignment, and KL divergence to the target distribution.

## Method Summary
The method casts guidance scheduling as an adaptive optimization problem where the guidance strength w_t(x, c) varies based on time, current sample state, and conditioning class. Building on theoretical insights that positive guidance monotonically increases classifier confidence and preserves conditional support, the framework optimizes a scalar-valued function using a stochastic optimal control objective. The approach only requires estimates of ∇ log p_t(x) and ∇ log p_t(x|c), making it applicable in classifier-free guidance settings. Training uses Girsanov reweighting to estimate gradients from off-policy trajectories.

## Key Results
- Adaptive guidance schedules learned via stochastic optimal control outperform constant guidance in terms of reward score, class alignment, and KL divergence to target distribution
- Positive guidance strength (w > 0) monotonically increases expected classifier confidence along guided trajectories
- Generated samples remain within the conditional data manifold when guidance is bounded and data support is compact
- The framework only requires score estimates ∇log p_t(x) and ∇log p_t(x|c), making it compatible with classifier-free guidance

## Why This Works (Mechanism)

### Mechanism 1: Classifier Confidence Accumulation
Positive guidance strength (w > 0) monotonically increases expected classifier confidence E[log p(c|Y^w_t)] along guided trajectories. The guiding field G_t(x) = log p_{T-t}(x|c) - log p_{T-t}(x) evolves via Itô's lemma with drift term (1 + 2w)||∇G_t||^2 ≥ 0, causing E[log p(c|Y^w_t)] to accumulate. Core assumption: score estimates are accurate; guidance bounded below -1/2. Evidence: abstract claim and Section 3.1 derivation via Itô's lemma.

### Mechanism 2: Conditional Support Preservation
Guided samples Y^w_T remain within supp p(·|c) almost surely under bounded guidance and compact data support. The KL divergence between guided and unguided backward processes is uniformly bounded, forcing Y^w_T to concentrate on conditional support via Donsker-Varadhan variational formula. Core assumption: unconditional data support is compact; guidance bounded; p(c) > 0. Evidence: abstract claim and Section 3.2 Theorem 3.

### Mechanism 3: Adaptive Guidance via SOC Optimization
Optimizing R(w) = E[log p(c|Y^w_T)] - λ KL(Y^w || Y) yields an adaptive guidance schedule balancing class alignment against distributional fidelity. The HJB equation yields optimal control w*_t(x) = (∇G_t·∇V_t + ||∇G_t||^2)/(λ||∇G_t||^2). Girsanov reweighting enables gradient estimation from off-policy trajectories. Core assumption: access to score estimates; λ > 0 regularizes against mode collapse. Evidence: abstract claim and Section 4-5 derivation.

## Foundational Learning

- **Stochastic Differential Equations & Itô's Lemma**: Core mathematical tool for deriving guidance dynamics (dG_t) and understanding noise-drift interaction in diffusion processes. Quick check: Can you explain why dG_t contains both a dt term (drift) and a dB_t term (diffusion)?

- **Stochastic Optimal Control & HJB Equations**: Framework for formulating guidance scheduling as optimization over time-varying policies. Quick check: What does the value function V_t(x) represent, and why does the HJB equation run backward in time?

- **Girsanov's Theorem & Importance Sampling**: Enables gradient estimation for guidance policy using trajectories from different proposal distribution. Quick check: Why does the Radon-Nikodym derivative W(Y, v, w) contain both a stochastic integral and a correction term?

## Architecture Onboarding

- Component map: Pre-trained Score Networks (∇log p, ∇log p|c) -> Guidance Network w_θ(t, x, c) -> Guided Reverse SDE Sampler, with Girsanov Reweighting and Trajectory Buffer feeding back to SGD Update θ

- Critical path:
  1. Freeze pre-trained score networks (∇log p_t, ∇log p_t|c)
  2. Initialize w_θ (e.g., constant λ^{-1})
  3. Sample trajectories under proposal v
  4. Compute ∇G_t along trajectories using score difference
  5. Estimate rewards R_i and importance weights W_i via Girsanov
  6. Update θ via reweighted gradient
  7. Monitor ESS; refresh proposal v when ESS < threshold

- Design tradeoffs:
  - λ (KL penalty): Larger λ → more conservative guidance; smaller λ → stronger class alignment but risk of mode collapse
  - w_θ parameterization: Time-only (simplest) vs. time+state (more expressive but harder to optimize)
  - Proposal refresh frequency: More frequent refreshes reduce variance but increase sampling cost
  - Trajectory discretization: Finer steps improve gradient accuracy but increase compute

- Failure signatures:
  - ESS collapse: Importance weights degenerate (few trajectories dominate) → refresh proposal more frequently
  - Score errors in low-density regions: ∇G_t unreliable where p_t(x|c) is small → may need bounded w or clipping
  - Mode collapse: λ too small → samples concentrate at high-p(c|x) but low-p(x|c) regions
  - No improvement over constant guidance: w_θ may be underfitting or λ is too large

- First 3 experiments:
  1. 2D Gaussian mixture validation: Replicate toy experiments; verify learned w_θ(t, x) improves over constant w via reward and KL metrics
  2. Ablation on λ: Sweep λ ∈ {0.1, 1.0, 10.0}; plot tradeoff between E[log p(c|Y_T)] and KL divergence to unguided process
  3. ESS sensitivity analysis: Vary ESS_min threshold; measure gradient variance and convergence speed

## Open Questions the Paper Calls Out

### Open Question 1
Can the stochastic optimal control objective be modified to be robust to errors in the estimated score function? The authors propose future work to create a more robust objective that can tolerate imperfect score estimation, as the current method relies on accurate scores which may explain lack of improvement on real-world datasets. Evidence would be a modified loss demonstrating improved performance on standard benchmarks in presence of approximate score networks.

### Open Question 2
What implementation details or algorithmic modifications are required to make the adaptive guidance framework tractable for high-dimensional, real-world data? The authors note they were unable to get significant improvements on real-world image datasets and leave implementation details for future work. Evidence would be a scalable implementation achieving superior FID/CLIP scores on high-resolution datasets compared to constant guidance baselines.

### Open Question 3
Does a time-only dependent guidance schedule w_t capture the majority of performance gains, or is full state-dependency w_t(x, c) necessary for complex distributions? The paper mentions optimizing w to only depend on time as a possibility but focuses on general case. Evidence would be an ablation study comparing learned w_t(x, c) against learned w_t on complex, high-dimensional generative tasks.

## Limitations
- Assumes access to accurate score estimates ∇log p_t(x) and ∇log p_t(x|c), which may not hold for complex data distributions
- KL-boundedness proof relies on compact data support and bounded guidance assumptions that may not generalize to high-dimensional real-world datasets
- Girsanov reweighting approach requires maintaining representative proposal distribution, which can suffer from importance weight degeneracy (ESS collapse) in high dimensions

## Confidence
- Mechanism 1 (Classifier Confidence Accumulation): High confidence - Rigorous Itô calculus derivation aligns with concurrent work
- Mechanism 2 (Conditional Support Preservation): Medium confidence - Mathematically sound KL-boundedness argument but relies on strong assumptions
- Mechanism 3 (Adaptive Guidance via SOC): Medium confidence - Complete theoretical framework but practical effectiveness depends on score quality and Girsanov optimization stability

## Next Checks
1. Scalability validation: Test adaptive guidance framework on CIFAR-10/ImageNet-10 with pre-trained classifier-free guidance models to assess real-world applicability beyond toy 2D examples
2. Sensitivity analysis: Systematically vary λ across multiple orders of magnitude and measure tradeoff between class alignment improvement and distributional drift in both toy and real datasets
3. Importance sampling stability: Quantify ESS dynamics during training and implement adaptive proposal refresh strategies to prevent weight collapse in high-dimensional settings