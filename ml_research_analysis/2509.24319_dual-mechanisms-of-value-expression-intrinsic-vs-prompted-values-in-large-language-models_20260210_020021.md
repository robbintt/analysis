---
ver: rpa2
title: 'Dual Mechanisms of Value Expression: Intrinsic vs. Prompted Values in Large
  Language Models'
arxiv_id: '2509.24319'
source_url: https://arxiv.org/abs/2509.24319
tags:
- value
- prompt
- intr
- prompted
- intrinsic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper systematically examines the two distinct mechanisms\
  \ by which large language models express human values\u2014intrinsic expression,\
  \ reflecting inherent values learned during training, and prompted expression, elicited\
  \ through explicit instructions. By analyzing value representations at both the\
  \ vector level (using difference-in-means methods) and the neuron level (identifying\
  \ MLP neurons that drive value expression), the authors reveal that while these\
  \ mechanisms share substantial common components crucial for inducing value expression,\
  \ they also possess unique components with distinct functional roles."
---

# Dual Mechanisms of Value Expression: Intrinsic vs. Prompted Values in Large Language Models

## Quick Facts
- **arXiv ID**: 2509.24319
- **Source URL**: https://arxiv.org/abs/2509.24319
- **Reference count**: 40
- **Primary result**: Large language models express human values through two distinct mechanisms—intrinsic (learned) and prompted (instruction-based)—that share common neural components while maintaining unique functional roles

## Executive Summary
This paper reveals that large language models express human values through two fundamentally different mechanisms: intrinsic values that reflect what the model has learned during training, and prompted values that emerge when explicitly instructed. The authors systematically analyze these mechanisms at both vector and neuron levels, discovering that while they share substantial neural components crucial for value expression, each mechanism also possesses unique components serving distinct functions. The intrinsic-unique components promote lexical diversity in value expression, while the prompted-unique components enhance instruction compliance, even in adversarial contexts like jailbreaking attempts.

The study provides crucial insights for AI safety and alignment by demonstrating that pluralistic value expression in LLMs emerges from this dual mechanism architecture. The shared components encode value semantics and maintain the theoretical circular structure of Schwartz's basic human values across languages, while the unique components enable the model to balance natural, diverse value expression with the ability to follow explicit instructions. This framework offers a more nuanced understanding of how LLMs handle value alignment compared to traditional approaches that treat value expression as a monolithic phenomenon.

## Method Summary
The researchers employed a multi-level analytical approach to distinguish between intrinsic and prompted value expression mechanisms in LLMs. At the vector level, they used difference-in-means methods to compare value representations between baseline responses and those explicitly prompted for value expression. At the neuron level, they identified specific MLP neurons that drive value expression through causal intervention experiments. The analysis was conducted using LLaMA-7B as the primary model and incorporated the Schwartz Value Survey framework to categorize and measure human values. The study examined multiple languages to assess cross-linguistic consistency in value representation and employed both qualitative and quantitative metrics to characterize the functional roles of shared versus unique neural components.

## Key Results
- LLMs express values through two distinct mechanisms: intrinsic (learned during training) and prompted (elicited through instructions)
- Shared neural components between mechanisms encode value semantics and reconstruct Schwartz's circular value structure across languages
- Intrinsic-unique components promote lexical diversity while prompted-unique components strengthen instruction compliance, even in jailbreaking scenarios

## Why This Works (Mechanism)
The dual mechanism framework explains how LLMs can simultaneously maintain authentic value expression while remaining responsive to explicit instructions. The shared components provide the foundational semantic understanding of values that is essential for any form of value expression, creating a common ground that ensures consistency across different expression modes. The unique components allow the model to modulate its response style—the intrinsic mechanism enables more natural, diverse expression that reflects learned patterns, while the prompted mechanism allows for precise control and compliance with user instructions. This architecture enables the model to navigate the tension between authentic value representation and instruction-following, which is crucial for practical deployment in diverse contexts.

## Foundational Learning
- **Schwartz Value Survey framework**: A psychological model categorizing human values into 10 motivational types arranged in a circular structure; needed to systematically measure and categorize value expression in LLMs; quick check: values form a theoretically predicted circular structure in embedding space
- **Difference-in-means methods**: Statistical techniques for comparing vector representations between conditions; needed to identify value-related neural activations at scale; quick check: significant mean differences between value and non-value conditions
- **MLP neuron analysis**: Examination of multi-layer perceptron neurons to identify those causally involved in specific behaviors; needed to pinpoint neural substrates of value expression; quick check: neuron ablation disrupts targeted value expression
- **Causal intervention experiments**: Techniques for testing whether specific neurons are necessary and sufficient for particular behaviors; needed to establish functional roles of shared vs. unique components; quick check: targeted neuron manipulation produces predictable changes in output
- **Cross-linguistic value consistency**: Assessment of whether value representations generalize across languages; needed to evaluate universality of value mechanisms; quick check: similar neural patterns for equivalent values across languages
- **Jailbreaking as adversarial testing**: Using attempts to circumvent safety constraints to test instruction compliance; needed to distinguish between intrinsic and prompted mechanism strengths; quick check: prompted mechanism shows stronger resistance to adversarial prompts

## Architecture Onboarding

**Component Map**: Input Text -> Embedding Layer -> Transformer Blocks (Attention + MLP) -> Output Text

**Critical Path**: For value expression, the critical path involves: Input encoding → Value-relevant attention patterns → MLP neurons (shared and unique components) → Value-aligned output generation

**Design Tradeoffs**: The dual mechanism architecture trades off between authentic, diverse value expression (favoring intrinsic mechanisms) and precise instruction compliance (favoring prompted mechanisms). This creates a natural tension that the model must navigate, potentially leading to situations where the two mechanisms conflict or where one dominates depending on context.

**Failure Signatures**: When shared components are compromised, value expression becomes incoherent or loses semantic structure. When intrinsic-unique components fail, value expression becomes repetitive or narrow in vocabulary. When prompted-unique components fail, the model struggles to follow explicit value-related instructions or becomes vulnerable to jailbreaking attempts.

**First Experiments**:
1. Ablate shared component neurons and measure impact on both intrinsic and prompted value expression coherence
2. Compare value expression diversity metrics between intrinsic-only and prompted-only conditions
3. Test jailbreaking resistance when prompted-unique components are selectively disabled

## Open Questions the Paper Calls Out
None

## Limitations
- Relies on Schwartz Value Survey framework, which may not capture all cultural variations or non-Western value systems
- Analysis limited to LLaMA-7B model, potentially limiting generalizability to other architectures or scales
- Neuron-level analysis focuses only on MLP neurons, potentially missing contributions from attention heads or other components

## Confidence
- **High confidence**: The dual mechanism framework and the distinction between shared vs. unique neural components
- **Medium confidence**: The specific functional roles of intrinsic-unique (lexical diversity) and prompted-unique (instruction compliance) components
- **Medium confidence**: The universality of circular value structure across languages given limited language coverage

## Next Checks
1. Test whether the shared vs. unique component distinction holds across different model families (GPT, Claude, Gemini) and scales (1B to 70B+ parameters)
2. Expand value framework validation to include non-Western value systems and additional cultural dimensions beyond Schwartz's framework
3. Conduct ablation studies removing specific neuron groups to empirically verify the causal role of shared vs. unique components in value expression fidelity and instruction compliance