---
ver: rpa2
title: Transfer Learning and Mixup for Fine-Grained Few-Shot Fungi Classification
arxiv_id: '2507.08248'
source_url: https://arxiv.org/abs/2507.08248
tags:
- mixup
- image
- sampling
- species
- embeddings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of few-shot fine-grained visual
  categorization (FGVC) for fungi species classification, tackling issues of high
  intra-species variation and subtle inter-species differences. The authors propose
  a transfer learning approach using pre-trained vision transformer models, weighted
  sampling, and Mixup data augmentation to improve classification accuracy.
---

# Transfer Learning and Mixup for Fine-Grained Few-Shot Fungi Classification

## Quick Facts
- arXiv ID: 2507.08248
- Source URL: https://arxiv.org/abs/2507.08248
- Authors: Jason Kahei Tam; Murilo Gustineli; Anthony Miyaguchi
- Reference count: 29
- Top-5 accuracy: 46.83% on private test set

## Executive Summary
This paper tackles few-shot fine-grained visual categorization for fungi species classification, addressing challenges of high intra-species variation and subtle inter-species differences. The authors employ transfer learning with pre-trained vision transformer models, weighted sampling for class imbalance, and Mixup data augmentation to improve classification accuracy. Their best model combining PlantCLEF 2024 embeddings with Mixup (α=1.20) and weighted sampling achieved 46.83% top-5 accuracy on the private test set, outperforming competition baselines. The study also explored generative AI models, which underperformed compared to vision-based approaches, highlighting the effectiveness of domain-specific pretraining and balanced sampling strategies.

## Method Summary
The approach uses pre-computed PlantCLEF 2024 embeddings (768-dim) extracted from the FungiTastic Few-Shot dataset images, stored in parquet files for modularity. A linear classifier is trained with weighted random sampling (inverse class frequency) and Mixup data augmentation applied at the embedding level. The training uses batch size 256, Adam optimizer with learning rate 5·10⁻⁴, and early stopping with patience 3. Mixup is applied with tuned α=1.20-1.45 values, higher than the standard range due to feature-level interpolation. The framework also explores text embeddings (ModernBERT/BioBERT) and generative AI models (Gemini 1.5 Pro, GPT-4o, GPT-4o-mini) for comparison.

## Key Results
- Best model achieved 46.83% top-5 accuracy on private test set using PlantCLEF embeddings with Mixup (α=1.20) and weighted sampling
- Mixup with tuned α=1.20-1.45 provided largest accuracy improvement (+4.27% private accuracy)
- Domain-specific PlantCLEF pretraining outperformed general-purpose DINOv2 (48.67% vs 47.35% public accuracy)
- Text embeddings and generative AI models underperformed vision-based approaches

## Why This Works (Mechanism)

### Mechanism 1: Embedding-Level Mixup for Few-Shot Regularization
Mixup applied at embedding level with higher-than-standard α values improves generalization by smoothing decision boundaries through linear interpolation between embedding pairs. This forces the classifier to learn simpler, more generalizable patterns rather than memorizing sparse class examples. The study found α=1.20-1.45 optimal, contrasting with the standard [0.1,0.4] range, likely because Mixup operates on already-abstracted representations.

### Mechanism 2: Domain-Specific Transfer Learning
PlantCLEF 2024, pre-trained on 1.4M plant images, encodes fine-grained visual features (texture, shape, color patterns) that share statistical regularities with fungi morphology. This domain-specific pretraining provides better feature reuse than general-purpose models like DINOv2, achieving 1.3% higher accuracy on the fungi classification task.

### Mechanism 3: Class-Balanced Sampling for Long-Tail Distribution
Inverse-frequency weighted sampling provides modest but consistent accuracy gains by ensuring minority classes are seen during training. The WeightedRandomSampler over-represents rare classes in each batch, preventing the optimizer from ignoring them due to gradient dominance from majority classes.

### Mechanism 4: Frozen Embeddings with Linear Classification
Pre-computed frozen embeddings with a simple linear classifier suffice for few-shot FGVC when using strong pretrained models. Vision transformers pre-trained on large datasets encode transferable semantic features; a linear classifier only needs to learn class boundaries in this already-discriminative space.

## Foundational Learning

### Concept: Vision Transformers and Self-Supervised Pretraining
**Why needed here:** The pipeline depends on DINOv2 and PlantCLEF embeddings. Understanding how ViTs learn patch-level representations and why self-supervised pretraining (DINO method) produces transferable features is critical for debugging embedding quality.
**Quick check question:** Why does DINOv2's self-supervised learning on LVD-142M produce embeddings that transfer to fungi classification without ever seeing fungi images?

### Concept: Mixup and Manifold Mixup
**Why needed here:** Mixup is the single most impactful technique (+4.27% private accuracy). Understanding the difference between input-level and hidden-state interpolation explains why α = 1.45 outperforms the standard α = 0.2.
**Quick check question:** Why would applying mixup at the embedding level (post-ViT) require a higher α value than applying it directly to raw pixel inputs?

### Concept: Few-Shot Learning and Class Imbalance
**Why needed here:** The FungiTastic dataset has severe class imbalance (1–30 images per class). Understanding why standard empirical risk minimization fails and how regularization (mixup) + sampling strategies help is fundamental.
**Quick check question:** Why does standard cross-entropy with uniform sampling produce poor accuracy on rare classes, and what are the tradeoffs between weighted sampling vs. data augmentation?

## Architecture Onboarding

### Component Map
Raw Images (300p–full size) -> Embedding Extraction (PlantCLEF 2024 ViT-B/14, 768-dim) -> Parquet Storage (pre-computed for modularity) -> DataLoader with WeightedRandomSampler (inverse class frequency) -> Batch → Mixup (λ ~ Beta(α, α), α = 1.20–1.45) -> Linear Classifier (768 → 2,427 classes) -> Mixup Loss (λ·CE(yᵢ) + (1-λ)·CE(yⱼ)) -> Top-5 Accuracy Evaluation

### Critical Path
1. **Data preparation:** Fix corrupted JPEGs (OpenCV reload), pre-compute PlantCLEF embeddings, store in parquet
2. **Sampler setup:** Compute inverse class frequency weights via sklearn's `compute_sample_weight`
3. **Training loop:** For each batch—apply mixup → forward pass → compute weighted cross-entropy → backprop
4. **Validation:** Top-5 accuracy on held-out set; monitor for overfitting (early stopping patience=3)

### Design Tradeoffs
| Decision | Paper Choice | Alternative | Tradeoff |
|----------|--------------|-------------|----------|
| Embedding model | PlantCLEF 2024 | DINOv2, FungiTastic ViT | +1.3% over DINOv2; domain-specific wins marginally |
| Mixup α | 1.20–1.45 | 0.1–0.4 (original paper) | Higher α for embedding-level; non-monotonic response |
| Classifier depth | Linear only | Multi-layer MLP | Simpler may generalize better with few shots |
| Text modality | ModernBERT/BioBERT | None | **Negative impact**—noisy metadata hurts |
| Multi-objective | GradNorm (4 tasks) | Single-task | **Negative impact**—distraction from primary task |

### Failure Signatures
1. **Rare class collapse:** Per-class accuracy shows 0% for classes with <5 samples (Figure 6: "concentration of points at accuracy 1.0 and 0.0...at the rarer classes")
2. **Mixup ineffectiveness:** If sweep over α ∈ [0.5, 2.0] shows no improvement, indicates need for deeper classifier (Manifold Mixup style)
3. **Text integration degradation:** Observed in Table 5—adding ModernBERT dropped accuracy from 43.1% to 38.4%
4. **LLM zero-shot failure:** Gemini/GPT achieved only 6–14% vs. 46.8% vision approach—domain specificity matters

### First 3 Experiments
1. **Baseline reproduction:** Load pre-computed PlantCLEF embeddings, train linear classifier with standard cross-entropy, no mixup, no weighted sampling. **Expected:** ~43% private accuracy (Table 5 baseline).
2. **Mixup α sweep:** Test α ∈ {0.5, 1.0, 1.2, 1.45, 2.0} with weighted sampling enabled. **Expected:** Peak performance at α = 1.20–1.45; non-monotonic curve as in Figure 5.
3. **Embedding ablation:** Compare PlantCLEF vs. DINOv2 vs. FungiTastic ViT with best mixup config. **Expected:** PlantCLEF ≈ FungiTastic ViT > DINOv2 (validate domain-specific pretraining hypothesis).

## Open Questions the Paper Calls Out

### Open Question 1
**Question:** Can smaller vision-language models (Gemma, Phi, or Llama) be effectively fine-tuned on the FungiCLEF dataset for domain-specific fungi classification?
**Basis in paper:** [explicit] "What might make the most sense is fine-tuning a smaller VLM, such as Gemma, Phi, or Llama, on the FungiCLEF dataset and seeing whether these smaller models can be effectively tuned for domain-specific language queries."
**Why unresolved:** Authors only explored zero-shot prompting with commercial APIs due to cost constraints and did not attempt fine-tuning.
**What evidence would resolve it:** Fine-tune a small VLM on FungiCLEF training data and compare top-5 accuracy against vision-only baselines.

### Open Question 2
**Question:** Does selective metadata integration using only informative features (location, substrate, habitat) improve multi-modal classification?
**Basis in paper:** [explicit] "Rather than incorporating all available metadata fields, future work should prioritize informative features. As a starting point, we propose using the three metadata attributes highlighted in the FungiTastic paper."
**Why unresolved:** Text embeddings degraded performance, likely due to inclusion of weakly informative fields (district, countryCode, hasCoordinate).
**What evidence would resolve it:** Train classifiers with only the three recommended metadata features and compare against full-metadata and no-metadata baselines.

### Open Question 3
**Question:** Does applying Manifold Mixup with additional learnable classifier layers reduce accuracy volatility on rare classes?
**Basis in paper:** [inferred] Authors note "the lack of a monotonic trend may suggest adding more learnable layers to our classifier is needed to flatten class boundaries and reduce volatility."
**Why unresolved:** Study only applied Mixup at feature level with a linear classifier; did not explore multi-layer architectures.
**What evidence would resolve it:** Implement multi-layer classifier with Manifold Mixup at random layers; measure accuracy and per-class volatility on validation set.

### Open Question 4
**Question:** Would recombining training and validation sets with stratified resplitting improve generalization given the observed distribution mismatch?
**Basis in paper:** [explicit] "One approach, seen in previous research, is to combine the provided datasets, then we can resplit them to have similar distributions."
**Why unresolved:** Distribution disparity between datasets was identified (Figure 3) but no alternative splits were tested.
**What evidence would resolve it:** Create stratified splits from combined data and compare performance against original competition splits.

## Limitations

- Domain-specific pretraining advantage is modest (1.3% gap) and may not generalize across all few-shot FGVC tasks
- Text modality integration failed due to inclusion of weakly informative metadata fields, but alternative text encoding strategies were not explored
- Generative AI experiments are limited to zero-shot prompting without exploring few-shot prompting strategies
- The Mixup mechanism explanation lacks validation through deeper classifier architectures

## Confidence

- **High:** Linear classifier with frozen embeddings achieves competitive results (directly supported by benchmark tables)
- **Medium:** Mixup at embedding level with α=1.20 improves generalization (supported by ablation but lacks broader validation)
- **Medium:** Domain-specific pretraining marginally outperforms general-purpose models (supported but effect size small relative to variance)
- **Low:** Text modality integration consistently degrades performance (only tested with one approach; no systematic exploration of text features)
- **Low:** Generative AI models are fundamentally unsuitable for this task (based on limited zero-shot evaluation only)

## Next Checks

1. **Deep classifier ablation:** Replace linear classifier with 2-3 layer MLP while keeping PlantCLEF embeddings frozen. Test whether higher capacity improves rare class accuracy beyond what Mixup achieves.

2. **Text feature engineering:** Systematically evaluate metadata subsets (remove location, keep substrate; try taxonomic embeddings only) with ModernBERT and BioBERT. Test whether selective feature inclusion can recover text modality performance.

3. **Cross-dataset pretraining:** Train PlantCLEF embeddings on combined PlantCLEF + LVD-142M data, then evaluate on FungiTastic. This tests whether hybrid pretraining exceeds either domain alone, providing stronger evidence for domain-specific advantages.