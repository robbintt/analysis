---
ver: rpa2
title: '$\texttt{DIAMONDs}$: A Dataset for $\mathbb{D}$ynamic $\mathbb{I}$nformation
  $\mathbb{A}$nd $\mathbb{M}$ental modeling $\mathbb{O}$f $\mathbb{N}$umeric $\mathbb{D}$iscussions'
arxiv_id: '2505.12651'
source_url: https://arxiv.org/abs/2505.12651
tags:
- conversation
- information
- chen
- each
- script
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DIAMONDs, a new benchmark for evaluating
  Theory of Mind (ToM) capabilities in multi-party conversations. The dataset features
  goal-oriented conversations with dynamic numerical variables that change over time,
  requiring models to track information states and manage knowledge asymmetries.
---

# $\texttt{DIAMONDs}$: A Dataset for $\mathbb{D}$ynamic $\mathbb{I}$nformation $\mathbb{A}$nd $\mathbb{M}$ental modeling $\mathbb{O}$f $\mathbb{N}$umeric $\mathbb{D}$iscussions

## Quick Facts
- arXiv ID: 2505.12651
- Source URL: https://arxiv.org/abs/2505.12651
- Reference count: 40
- State-of-the-art models achieve 80% accuracy on omniscient questions but drop to 55% on participant-centric questions in the DIAMONDs benchmark

## Executive Summary
DIAMONDs is a new benchmark for evaluating Theory of Mind (ToM) capabilities in multi-party conversations. The dataset features goal-oriented conversations with dynamic numerical variables that change over time, requiring models to track information states and manage knowledge asymmetries. The authors present a novel synthetic data generation pipeline using LLMs and Markov processes to create structured conversations with controlled information access patterns. Evaluation of state-of-the-art models shows significant performance gaps: models achieve 80% accuracy on omniscient questions but drop to 55% on participant-centric questions, and only 27% when participants have false beliefs. Models also struggle with conversations containing distractors and recognizing unanswerable scenarios. These results highlight current limitations in models' ToM capabilities for real-world multiparty conversations.

## Method Summary
The DIAMONDs benchmark introduces a novel synthetic data generation pipeline that combines LLMs with Markov processes to create controlled multi-party conversations. The pipeline generates structured dialogues where participants have access to different subsets of information, creating knowledge asymmetries. Dynamic numerical variables change throughout conversations, requiring models to track evolving information states. The dataset includes three question types: omniscient (global state), participant-centric (individual knowledge), and false-belief scenarios. The synthetic approach allows for precise control over conversation structure and information access patterns while maintaining conversational naturalness.

## Key Results
- Models achieve 80% accuracy on omniscient questions but drop to 55% on participant-centric questions
- False-belief scenarios show only 27% accuracy, indicating significant ToM limitations
- Models struggle with conversations containing distractors and recognizing unanswerable scenarios
- Performance gaps highlight fundamental limitations in current models' ToM capabilities for real-world multiparty conversations

## Why This Works (Mechanism)
DIAMONDs works by creating controlled environments where models must reason about multiple agents' knowledge states simultaneously. The synthetic generation pipeline ensures consistent information access patterns while maintaining conversational complexity. The dynamic numerical variables force models to track evolving information states rather than static facts. Knowledge asymmetries between participants create realistic scenarios that require genuine ToM reasoning rather than pattern matching. The three-tiered question structure progressively tests model capabilities from simple state tracking to complex false-belief reasoning.

## Foundational Learning
**Theory of Mind (ToM)**: The ability to attribute mental states to oneself and others, understanding that others have beliefs, desires, and intentions that may differ from one's own. Why needed: ToM is fundamental for human-like reasoning in social interactions and conversation. Quick check: Can the model distinguish between what different participants know?

**Information State Tracking**: Maintaining and updating knowledge about what each conversation participant knows at any given time. Why needed: Essential for reasoning about knowledge asymmetries in multi-party conversations. Quick check: Can the model correctly update knowledge states when new information is revealed?

**Dynamic Variable Reasoning**: Ability to track and reason about numerical values that change over the course of a conversation. Why needed: Real-world conversations often involve evolving numerical information that affects decision-making. Quick check: Can the model correctly update numerical values based on conversation content?

**Knowledge Asymmetry**: The state where different participants in a conversation have access to different subsets of information. Why needed: Realistic social interactions involve varying levels of information access among participants. Quick check: Can the model identify which participants know specific information?

## Architecture Onboarding
**Component Map**: LLM Generator -> Markov Process Controller -> Conversation Synthesizer -> Information State Tracker -> Question Generator -> Benchmark Dataset
**Critical Path**: The synthetic data generation pipeline (LLM + Markov) → controlled conversation creation → information state tracking → question generation → model evaluation
**Design Tradeoffs**: Synthetic data provides control but may lack natural conversation nuances; three-tier question structure balances complexity with evaluation tractability
**Failure Signatures**: Models fail on participant-centric questions (55% accuracy) and false-belief scenarios (27% accuracy), indicating ToM reasoning limitations rather than simple information retrieval issues
**First 3 Experiments**:
1. Evaluate baseline models on omniscient questions only to establish upper performance bounds
2. Test models on participant-centric questions with varying numbers of participants
3. Assess false-belief reasoning capabilities with controlled information access patterns

## Open Questions the Paper Calls Out
The paper identifies several open questions regarding the generalizability of the benchmark to natural conversations, the scalability of synthetic data generation to more complex scenarios, and the potential for models to learn ToM capabilities through pretraining versus requiring explicit training on ToM tasks.

## Limitations
- Synthetic data may not fully capture natural conversation nuances and complexities
- Evaluation based on specific state-of-the-art models, results may vary with different architectures
- Benchmark focus on numerical variables may not generalize to all ToM reasoning tasks
- Controlled generation may miss emergent conversation patterns found in natural dialogues

## Confidence
High confidence in the paper's contribution to advancing ToM evaluation in AI systems due to:
- Novel dataset design with controlled synthetic generation
- Comprehensive evaluation methodology with three distinct question types
- Clear demonstration of current model limitations in ToM reasoning
- Innovative use of Markov processes for information state control

## Next Checks
1. Evaluate the DIAMONDs benchmark with additional state-of-the-art models and architectures to assess generalizability across different model types
2. Conduct user studies with human participants to validate the synthetic data generation pipeline and ensure generated conversations represent natural human interactions
3. Extend evaluation to include non-numerical variables and open-ended goal scenarios to test benchmark robustness across different ToM reasoning task types