---
ver: rpa2
title: 'HEMERA: A Human-Explainable Transformer Model for Estimating Lung Cancer Risk
  using GWAS Data'
arxiv_id: '2510.07477'
source_url: https://arxiv.org/abs/2510.07477
tags:
- data
- cancer
- risk
- lung
- snps
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: HEMERA is a transformer-based model that predicts lung cancer risk
  directly from genome-wide SNP data, avoiding reliance on clinical covariates. It
  introduces additive positional encodings, neural genotype embeddings, and refined
  variant filtering to process raw genotype data more effectively.
---

# HEMERA: A Human-Explainable Transformer Model for Estimating Lung Cancer Risk using GWAS Data

## Quick Facts
- **arXiv ID:** 2510.07477
- **Source URL:** https://arxiv.org/abs/2510.07477
- **Reference count:** 40
- **Primary result:** Transformer model predicts lung cancer risk from raw SNP data with AUC > 0.99

## Executive Summary
HEMERA is a transformer-based model that predicts lung cancer risk directly from genome-wide SNP data, avoiding reliance on clinical covariates. It introduces additive positional encodings, neural genotype embeddings, and refined variant filtering to process raw genotype data more effectively. The model employs a lightweight Linformer architecture with a single transformer layer and attention head, optimized through ablation studies. A post-hoc explainability module using Layer-wise Integrated Gradients enables attribution of predictions to specific SNPs. Trained on 27,254 Million Veteran Program participants, HEMERA achieved an average AUC > 0.99 in 5-fold cross-validation. The explainability analysis identified SNPs aligning with known lung cancer risk loci, validating the model's biological relevance and supporting its potential for personalized risk assessment.

## Method Summary
HEMERA processes tokenized SNP sequences (378,866 features after MAF filtering) through a Linformer encoder with learnable neural embeddings and additive positional encodings. The model uses a single transformer layer with one attention head, followed by a classification head. Training occurs in two stages: MLM pretraining with 40% masking, then fine-tuning with AdamW-AMSGrad optimizer. Explainability is achieved through Layer-wise Integrated Gradients that attribute predictions to specific SNPs, with attributions aggregated across individuals carrying each variant. The model was evaluated using 5-fold stratified cross-validation on 27,254 MVP participants, with performance measured by AUC, precision, recall, and F1-score.

## Key Results
- HEMERA achieved an average AUC > 0.99 in 5-fold cross-validation on lung cancer risk prediction
- Post-hoc explainability analysis identified SNPs strongly aligning with established lung cancer susceptibility loci
- Ablation studies confirmed optimal performance with single transformer layer, single attention head, and MAF threshold of 0.01

## Why This Works (Mechanism)

### Mechanism 1
Neural embeddings enable richer, task-specific representations of discrete genotype tokens compared to one-hot encoding. The nn.Embedding layer maps each of 33 possible SNP tokens to a learnable 36-dimensional dense vector. These embeddings are trained jointly with the model, allowing semantically meaningful relationships between genotypes to emerge during optimization rather than being fixed a priori. Core assumption: Genotype tokens share latent structure that can be captured in continuous space. Evidence: Abstract mentions "neural genotype embeddings... enabling learnable and semantically rich representations of genetic variants" and methods describe joint training of embeddings.

### Mechanism 2
The Linformer's low-rank approximation enables tractable attention computation over ~379K SNP positions by reducing quadratic complexity to linear. Standard self-attention computes O(n²) pairwise interactions. Linformer projects key and value matrices to a lower-dimensional space (k=36), approximating full attention while reducing memory/compute from O(n²) to O(n·k). For genomic sequences with hundreds of thousands of positions, this makes transformer modeling feasible. Core assumption: The full attention matrix has low effective rank; key information can be preserved through projection. Evidence: Methods state "By projecting key and value matrices into a lower-dimensional space, Linformer enables efficient modeling of long-range dependencies while significantly reducing memory and computational overhead."

### Mechanism 3
Layer-wise Integrated Gradients (LIG) attributions identify SNPs influencing predictions, with aggregated attributions across carriers aligning with known lung cancer loci. LIG computes attribution by integrating gradients along a path from a baseline (mean embedding across training samples) to the actual input. Attributions are computed at the embedding layer, then aggregated across embedding dimensions and across individuals carrying each variant. This produces per-SNP importance scores visualized in Manhattan-style plots. Core assumption: The gradient path integral meaningfully reflects feature importance; mean embedding is a neutral baseline. Evidence: Abstract mentions "post-hoc explainability module based on Layer-wise Integrated Gradients enables attribution of model predictions to specific SNPs, aligning strongly with known LC risk loci."

## Foundational Learning

- **Self-Attention and Positional Encoding**
  - Why needed here: HEMERA processes SNP sequences where both identity and genomic position matter. Additive sinusoidal positional encodings inject position information into embeddings so the transformer can distinguish variants by location.
  - Quick check question: Given input embeddings E ∈ R^{L×d} and positional encodings P ∈ R^{L×d}, how does the model combine them, and why is this preferable to concatenation for gradient flow?

- **Integrated Gradients (IG) and Baseline Selection**
  - Why needed here: IG attributes output to inputs by integrating gradients from a baseline. The baseline represents "absence of signal"—HEMERA uses the mean embedding across training samples, which reflects a typical genomic sequence.
  - Quick check question: Why does using a zero baseline vs. mean embedding baseline change attribution patterns, and which is more appropriate for sparse genomic data?

- **Linkage Disequilibrium (LD) and Variant Resolution**
  - Why needed here: Attributions do not directly identify causal variants; they highlight SNPs the model uses, which may be in LD with true causal variants. The ±1Mbp window for locus matching accounts for this.
  - Quick check question: If a model attributes high importance to SNP A, but SNP A is in high LD (r² > 0.8) with SNP B which is the true causal variant, what does the attribution actually tell us?

## Architecture Onboarding

- **Component map:** Tokenized SNP sequences -> nn.Embedding(33, 36) -> Add sinusoidal positional encoding -> Linformer encoder (1 layer, 1 head, k=36) -> [CLS] token -> Linear classification head -> Binary logits

- **Critical path:**
  1. Load and tokenize SNP data -> integer sequence
  2. Embed tokens and add positional encoding
  3. Pass through Linformer encoder
  4. Extract [CLS] token representation for classification
  5. For explainability: compute LIG attributions at embedding layer, aggregate across samples

- **Design tradeoffs:**
  - Single layer/head: Empirically sufficient (ablation showed no gain from depth), reduces overfitting risk, but may limit expressivity for other tasks
  - MAF threshold 0.01: Retains rare variants that carry signal, but increases feature space; higher thresholds lose predictive power
  - Unimputed data: Simpler pipeline, but sparser variant coverage; exact matches to known loci are unlikely

- **Failure signatures:**
  - AUC drops sharply with increased MAF threshold -> rare variants carrying signal are being filtered
  - Deeper models show high variance across folds -> overfitting
  - Attributions are diffuse across entire LD blocks -> model may not have learned localized patterns
  - Negative attributions do not correspond to known protective loci -> may reflect interactions rather than direct effects

- **First 3 experiments:**
  1. Replicate ablation: Vary number of layers (1-6) and heads (1-4) on 70-10-20 split; confirm single layer/head is optimal for your dataset
  2. MAF sensitivity: Sweep MAF thresholds (0.01, 0.05, 0.1, 0.2, 0.3, 0.4) and plot AUC vs. threshold; verify that 0.01 is optimal
  3. Attribution validation: Compute LIG attributions on held-out test set; extract top 50 SNPs per chromosome and check overlap (±1Mbp window) with known LC loci from GWAS catalog or cited studies

## Open Questions the Paper Calls Out

### Open Question 1
Can HEMERA's high AUC performance (>0.99) be replicated in independent, external cohorts outside the Million Veteran Program? The paper recognizes that further validation in independent datasets is necessary, but data security restrictions prevented cross-institutional validation. MVP data is only accessible to researchers with funded MVP projects. Successful training and evaluation on independent biobanks (e.g., UK Biobank, All of Us) with comparable performance metrics would resolve this.

### Open Question 2
What is the source of the unexpectedly high AUC given that twin studies estimate lung cancer heritability at only 8-20%, theoretically limiting achievable AUC to approximately 0.65? The authors propose multiple hypotheses including capture of non-additive effects, signal from copy number alterations in blood DNA of cancer patients, or clonal hematopoiesis associations, but none are confirmed. Distinguishing between these requires controlled experiments such as ablation controlling for cancer status at blood draw or analysis of copy number variation signals.

### Open Question 3
Do negatively attributed SNPs represent genuine protective variants, compensatory epistatic interactions, or tissue-specific regulatory effects? The paper states that exploration of negatively scoring SNPs is preliminary and requires deeper biological modeling. The five most negatively attributed loci identified do not map to known protective loci, leaving biological interpretation as hypothesis-generating. Functional validation via CRISPR perturbation in relevant lung cell types or colocalization analysis with eQTLs in lung tissue would help resolve this.

### Open Question 4
How does model performance and attribution stability change when incorporating imputed variants or whole-genome sequencing data instead of array-based unimputed genotypes? The authors acknowledge that unimputed data limited their ability to capture the full spectrum of genomic variation, suggesting WGS could reveal additional relevant variants, particularly in non-coding regions. The impact of denser variant coverage on transformer attention patterns and attribution resolution is unknown. Retraining on imputed datasets or WGS data from the same cohort would provide answers.

## Limitations

- The extraordinary AUC (>0.99) may reflect overfitting to the matched case-control design rather than true predictive ability for population-level risk stratification
- The Linformer architecture's low-rank approximation may miss critical long-range interactions between SNPs, though it provides computational efficiency
- Attributions align with known lung cancer loci but could reflect linkage disequilibrium patterns rather than direct biological effects, as LD resolution is limited by unimputed data

## Confidence

- **High confidence:** Linformer architecture reduces computational complexity through low-rank approximation; explainability methodology using Layer-wise Integrated Gradients is technically sound; embedding layer learns task-specific representations
- **Medium confidence:** Specific hyperparameters (d=36, k=36, 1 layer, 1 head) are optimal for this dataset; MAF threshold of 0.01 maximizes predictive performance; ±1Mbp window for locus matching appropriately accounts for LD
- **Low confidence:** Reported AUC > 0.99 reflects true predictive ability rather than overfitting to the matched cohort; attributions identify causal variants rather than LD proxies; model generalizes beyond the MVP cohort

## Next Checks

1. **External validation:** Test HEMERA on an independent lung cancer cohort (e.g., UK Biobank, TCGA) to verify whether AUC remains >0.99 or drops to more typical values (0.65-0.85 range)

2. **LD resolution test:** For top-50 attributed SNPs, compute r² values to nearby known risk loci; determine what fraction are in high LD (r² > 0.8) versus being the reported index variant

3. **Baseline comparison:** Implement a logistic regression baseline using the same SNP features (embedded or one-hot) to establish whether the transformer architecture provides meaningful improvement over classical methods