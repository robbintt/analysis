---
ver: rpa2
title: 'IntelliCode: A Multi-Agent LLM Tutoring System with Centralized Learner Modeling'
arxiv_id: '2512.18669'
source_url: https://arxiv.org/abs/2512.18669
tags:
- learner
- mastery
- state
- review
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: IntelliCode introduces a multi-agent LLM tutoring system with a
  centralized, versioned learner state that integrates mastery estimates, misconceptions,
  review schedules, and engagement signals. A StateGraph Orchestrator coordinates
  six specialized agents under a single-writer policy to ensure consistent, auditable
  updates.
---

# IntelliCode: A Multi-Agent LLM Tutoring System with Centralized Learner Modeling

## Quick Facts
- arXiv ID: 2512.18669
- Source URL: https://arxiv.org/abs/2512.18669
- Authors: Jones David; Shreya Ghosh
- Reference count: 13
- Primary result: 5.04% average mastery gain and 89.1% task success with hints in simulated learners

## Executive Summary
IntelliCode introduces a multi-agent LLM tutoring system with a centralized, versioned learner state that integrates mastery estimates, misconceptions, review schedules, and engagement signals. A StateGraph Orchestrator coordinates six specialized agents under a single-writer policy to ensure consistent, auditable updates. The system combines mastery estimation, graduated hinting, dependency-aware curriculum adaptation, and spaced repetition grounded in formal pedagogical models. Offline validation shows stable state updates, with mastery calibration and AUROC≥0.75 for recall prediction, while simulated learners achieved a 5.04% average mastery gain and 89.1% task success with hints. These results demonstrate that persistent learner modeling, orchestrated multi-agent reasoning, and principled instructional design can produce reliable, transparent LLM-driven tutoring.

## Method Summary
IntelliCode uses LangGraph orchestration of six agents (Skill Assessment, Learner Profiler, Pedagogical Feedback, Content Curator, Progress Synthesizer, Engagement Orchestrator) with a single-writer StateGraph Orchestrator policy. The system employs BKT-inspired mastery updates, SM-2 spaced repetition, a 40/50/10 curriculum policy, and 5-level graduated hinting. Learner state is stored in ArangoDB as a graph-structured persistent store. Validation uses simulated learner personas (10 trajectories reported) with FastAPI backend and React frontend.

## Key Results
- Simulated learners achieved 5.04% average mastery gain over study period
- Graduated hinting increased task success from 52.4% to 89.1%
- Content Curator maintained 90% topic coverage over 30 days
- Recall prediction AUROC≥0.75 for mastery calibration

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A single-writer orchestration policy produces consistent, auditable multi-turn tutoring by preventing conflicting state updates across agents.
- Mechanism: The StateGraph Orchestrator maintains an in-memory copy of the learner state, routes events to agents, validates proposed changes, and commits them atomically. Each agent operates as a pure transformation over shared state, returning structured outputs rather than performing direct writes.
- Core assumption: Agent outputs can be meaningfully validated against a schema before commitment, and conflicting writes are the primary source of pedagogical inconsistency.
- Evidence anchors:
  - [abstract]: "A StateGraph Orchestrator coordinates six specialized agents... each operating as a pure transformation over the shared state under a single-writer policy."
  - [Section 2.1]: "Similar to the coordination strategy in GenMentor (Wang et al., 2025), this mechanism prevents conflicting writes, enforces safety and schema constraints."
  - [corpus]: Related work (GenMentor, SocraticLM) demonstrates multi-agent orchestration stabilizes tutoring behavior, though corpus lacks direct replication of single-writer patterns.
- Break condition: If agent outputs become too heterogeneous for schema validation (e.g., unstructured LLM reasoning), atomic commits degrade to best-effort merges, violating consistency guarantees.

### Mechanism 2
- Claim: Graduated hinting scaled by proficiency improves task success without solution disclosure.
- Mechanism: The Pedagogical Feedback agent applies a five-level protocol (Metacognitive → Conceptual → Strategic → Structural → Targeted). Hint specificity scales with estimated proficiency p̂: beginners receive analogies and single-step cues; advanced learners receive concise edge-case nudges.
- Core assumption: Learners benefit from incremental scaffolding rather than binary help/no-help, and hint level can be reliably mapped to proficiency estimates.
- Evidence anchors:
  - [Section 4.1]: "Tasks in which simulated learners requested hints exhibited a success rate of 89.1%, compared with an overall baseline of 52.4%."
  - [Section 3.1]: "System Demonstration—the specificity of hints scales with the learner's proficiency estimate p̂."
  - [corpus]: IMACT-CXR demonstrates multi-agent conversational tutoring but does not report graduated hinting outcomes; limited external validation.
- Break condition: If proficiency estimates are miscalibrated (e.g., cold-start learners), hints may be over- or under-specific, reducing effectiveness or causing frustration.

### Mechanism 3
- Claim: A 40/50/10 curriculum policy maintains topic coverage and balances review, growth, and challenge.
- Mechanism: The Content Curator selects tasks using: selection = 0.4×due_reviews + 0.5×growth_zone + 0.1×challenge. Growth-zone targets mastery 0.3–0.7; challenge targets <0.3. Prerequisite dependencies are enforced, repetition is avoided within a k-day window, and diversity is ensured.
- Core assumption: Learners benefit from a structured mix of reinforcement, optimal challenge, and spaced review rather than pure mastery-chasing or random exploration.
- Evidence anchors:
  - [Section 3.1]: "The Content Curator maintained strong diversity across topics... demonstrating the ability of the 40/50/10 policy to avoid topic starvation."
  - [Section 4.2]: "Content Curator maintained 90% topic coverage."
  - [corpus]: "Optimizing Mastery Learning by Fast-Forwarding Over-Practice Steps" supports avoiding overpractice but does not validate the specific 40/50/10 weighting.
- Break condition: If prerequisite graphs are incomplete or mis-specified, learners may receive tasks requiring unmastered dependencies, breaking the scaffolding assumption.

## Foundational Learning

- Concept: Bayesian Knowledge Tracing (BKT)
  - Why needed here: Mastery updates use a BKT-inspired rule incorporating difficulty, recency, hint usage, and solve time. Understanding prior/update mechanics clarifies how the system estimates latent knowledge from noisy observations.
  - Quick check question: Given a learner with 0.6 mastery who fails a medium-difficulty problem using 3 hints, should mastery increase or decrease—and why?

- Concept: SM-2 Spaced Repetition Algorithm
  - Why needed here: The Progress Synthesizer extends SM-2 with context-aware adjustments. You need to understand ease factors, intervals, and quality scores to debug review scheduling.
  - Quick check question: If a learner reviews an item correctly but slowly with 2 hints, what quality score (0–5) should SM-2 assign, and how does this affect the next interval?

- Concept: Partially Observable Markov Decision Processes (POMDPs)
  - Why needed here: The system frames adaptive tutoring as a POMDP with learner state, observations, actions, and rewards. This formalization guides how agents trade off mastery gains against hint penalties and time costs.
  - Quick check question: What is the key difference between a POMDP and a standard MDP, and why does tutoring require the POMDP formulation?

## Architecture Onboarding

- Component map:
  - StateGraph Orchestrator (central coordinator with write access) -> Six Agents (Skill Assessment, Learner Profiler, Pedagogical Feedback, Content Curator, Progress Synthesizer, Engagement Orchestrator) -> Persistent Learner State (ArangoDB)
  - Trigger Layer: on_submission, on_hint_request, on_session_check, on_daily_generation, on_review_due

- Critical path:
  1. Learner submits code → on_submission trigger
  2. Orchestrator invokes Skill Assessment (test cases + semantic review)
  3. Learner Profiler computes mastery delta, records misconceptions
  4. Pedagogical Feedback generates proficiency-aware hints if requested
  5. Orchestrator validates, commits atomic state update
  6. Progress Synthesizer schedules SM-2 review; Content Curator queues next task

- Design tradeoffs:
  - Deterministic vs. generative agents: Profiler and Curator use deterministic logic for reproducibility; Feedback and Assessment use LLMs for flexibility. Tradeoff: variability vs. auditability.
  - Single-writer bottleneck: Ensures consistency but may limit throughput under high concurrency.
  - BKT-inspired updates vs. full DKT: Simpler, interpretable updates but may miss temporal patterns captured by neural models.

- Failure signatures:
  - State drift: Agents produce conflicting mastery estimates; check orchestrator validation logs for rejected commits.
  - Hint over-reliance: Learners consistently succeed only with hints; review hint-level success rates and adjust proficiency weighting.
  - Topic starvation: Some skills never appear; verify Content Curator's prerequisite graph and 40/50/10 allocation.

- First 3 experiments:
  1. Replicate simulated learner runs with seeded personas; validate that mastery gains (~5%) and hint success rates (52.4%→89.1%) match reported figures.
  2. Ablate the graduated hinting protocol (flatten to single-level hints); measure impact on task success and hint acceptance rates.
  3. Inject miscalibrated proficiency estimates for cold-start learners; observe whether hint specificity degrades and whether curriculum adaptation becomes erratic.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the 5.04% mastery gains observed in simulated learners translate to actual learning outcomes and long-term retention in human students?
- Basis in paper: [explicit] The authors state that simulations "cannot substitute for human studies in evaluating educational efficacy" and identify the need for "large-scale, longitudinal human studies" in future work.
- Why unresolved: The current evaluation relies entirely on agent-based personas (simulated learners) rather than biological cognition, leaving the real-world pedagogical impact unverified.
- What evidence would resolve it: Results from randomized controlled trials (RCTs) with human learners, measuring pre/post-test score improvements and retention rates over time.

### Open Question 2
- Question: To what extent does the reliance on conservative priors impact the personalization accuracy and engagement rates for cold-start learners?
- Basis in paper: [explicit] The limitations section notes that "cold-start learners... necessitate conservative priors and may experience reduced personalization in early sessions."
- Why unresolved: The paper does not quantify the performance gap or "warm-up" period required for new users to achieve the same level of model fidelity as existing users.
- What evidence would resolve it: A comparative analysis of calibration errors (e.g., Brier scores) and engagement metrics between zero-history and tenured user profiles.

### Open Question 3
- Question: How susceptible is the StateGraph Orchestrator's "single-writer" consistency to model drift or non-deterministic outputs from the LLM-based agents?
- Basis in paper: [explicit] The authors list "model drift" and "occasional refusals" as limitations, noting that while guardrails exist, they "cannot fully eliminate" variability introduced by LLM components.
- Why unresolved: It is unclear if validation schemas are sufficient to prevent state corruption if the underlying LLM semantics shift significantly without re-tuning.
- What evidence would resolve it: Stress testing the orchestrator's validation rejection rates when swapping the underlying LLM for different versions or distinct model families.

## Limitations
- The study relies entirely on simulated learners rather than human students, limiting ecological validity
- The LLM model identity and exact prompt templates remain unspecified, creating reproducibility gaps
- The 40/50/10 curriculum policy and graduated hinting appear tuned to programming problem characteristics and may not generalize to other domains

## Confidence
- High confidence: Architectural design patterns and single-writer orchestration consistency guarantees
- Medium confidence: Mastery estimation and curriculum adaptation mechanisms combining BKT and SM-2 principles
- Low confidence: Generalizability of curriculum policy and hinting protocol to non-DSA domains

## Next Checks
1. Deploy the system with 30+ real learners over 4 weeks, measuring actual mastery gains, hint utilization patterns, and engagement retention compared to baseline tutoring methods.
2. Conduct an ablation study removing the single-writer orchestration constraint to quantify consistency benefits versus potential throughput gains in multi-agent coordination.
3. Test curriculum adaptation robustness by introducing synthetic prerequisite errors (10-20% of dependency edges incorrect) and measuring how the system recovers topic coverage and prevents knowledge gaps.