---
ver: rpa2
title: 'Decoding Rewards in Competitive Games: Inverse Game Theory with Entropy Regularization'
arxiv_id: '2601.12707'
source_url: https://arxiv.org/abs/2601.12707
tags:
- reward
- proof
- have
- bound
- error
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the inverse game theory problem of recovering
  unknown reward functions from observed strategies in competitive games. The authors
  develop a unified framework for entropy-regularized two-player zero-sum matrix games
  and Markov games, establishing identifiability conditions and proposing algorithms
  to recover feasible reward functions.
---

# Decoding Rewards in Competitive Games: Inverse Game Theory with Entropy Regularization

## Quick Facts
- arXiv ID: 2601.12707
- Source URL: https://arxiv.org/abs/2601.12707
- Authors: Junyi Liao; Zihan Zhu; Ethan Fang; Zhuoran Yang; Vahid Tarokh
- Reference count: 40
- This paper develops a unified framework for recovering unknown reward functions from observed strategies in entropy-regularized two-player zero-sum games, establishing identifiability conditions and proposing algorithms with theoretical guarantees.

## Executive Summary
This paper addresses the inverse game theory problem of recovering unknown reward functions from observed strategies in competitive games. The authors develop a unified framework for entropy-regularized two-player zero-sum matrix games and Markov games, establishing identifiability conditions and proposing algorithms to recover feasible reward functions. The key insight is that entropy regularization makes the inverse problem identifiable under linear assumptions, resolving the inherent ill-posedness of inverse reinforcement learning in games. The framework provides both point estimates when rewards are uniquely identifiable and confidence sets when only partial identifiability exists.

## Method Summary
The method employs a two-stage approach: first estimating the Quantal Response Equilibrium (QRE) strategies from observed actions using either frequency estimators or maximum likelihood estimation, then recovering the reward parameters via linear regression. For matrix games, this involves solving a linear system derived from the QRE optimality conditions, with unique recovery guaranteed under a rank condition. For Markov games, the approach extends by combining reward recovery with transition kernel estimation using ridge regression. The algorithms provide theoretical guarantees including O(N^{-1/2}) estimation error rates and confidence sets containing all feasible rewards with high probability.

## Key Results
- Entropy regularization establishes unique identifiability of reward parameters under linear assumptions when rank conditions are satisfied
- The framework provides O(N^{-1/2}) estimation error rates for both matrix and Markov games
- Confidence sets are constructed for partially identifiable cases, containing all feasible rewards with high probability
- Numerical experiments validate theoretical results, showing accurate reward recovery even in partially identifiable settings

## Why This Works (Mechanism)

### Mechanism 1: Entropy-Induced Identifiability via Quantal Response
Entropy regularization resolves the inherent "ill-posedness" of inverse game theory by establishing a unique mapping between observed strategies and reward parameters through the Quantal Response Equilibrium. The log-probability of actions becomes a linear function of the reward parameters, allowing for unique recovery via linear system solving when the rank condition is satisfied.

### Mechanism 2: Two-Stage Linear Inversion
Reward recovery is achieved by first estimating the QRE strategies from empirical frequencies or MLE, then solving a linear system derived from QRE optimality conditions. This approach leverages the differentiable relationship between policies and rewards under entropy regularization.

### Mechanism 3: Confidence Sets for Partial Identifiability
When rank conditions fail, the algorithm constructs feasible sets of rewards rather than point estimates, bounding uncertainty through quadratic constraints. This provides meaningful solutions even when unique recovery is impossible.

## Foundational Learning

- **Quantal Response Equilibrium (QRE)**: Behavioral model replacing Nash Equilibrium where agents choose actions stochastically with probability increasing in payoff. Understanding QRE is essential as it links π(a) to e^{ηQ}.
- **Linear Regression in High Dimensions**: Core of the inverse operation is solving linear systems Aθ = b, with ridge regression used for regularization. Understanding rank-deficiency is crucial for interpreting uniqueness of solutions.
- **Inverse Reinforcement Learning (IRL)**: Framework addressing the fundamental ill-posedness where multiple rewards can explain one policy. Understanding this ambiguity motivates the need for entropy regularization.

## Architecture Onboarding

- **Component map**: Input Trajectory buffer -> QRE Estimator -> Feature Extractor -> Solver -> Output (θ̂, Q̂)
- **Critical path**: Accuracy of QRE Estimator is the single point of failure; if QRE estimates are inaccurate due to sparse data, subsequent matrix inversions propagate noise leading to incorrect reward recovery.
- **Design tradeoffs**: Frequency vs MLE estimators (C-well-posedness assumption vs feature map requirements); threshold κ selection trades coverage vs precision in confidence sets.
- **Failure signatures**: Rank errors during inversion indicate need to switch to confidence set construction; high variance in Markov game rewards suggests checking state coverage or increasing ridge regularization.
- **First 3 experiments**: 1) Matrix game validation on 3x3 game with synthetic data, 2) Rank deficiency stress test to confirm confidence set switching, 3) Markov game Gridworld implementation comparing recovered vs ground truth parameters.

## Open Questions the Paper Calls Out

### Open Question 1
Can the framework extend to partially observable games (POSGs)? The current framework assumes fully observable states, and partial observability introduces additional complexity in equilibrium computation and parameter identifiability.

### Open Question 2
How to relax the linear parametric assumption to handle non-linear payoff structures while maintaining identifiability guarantees? The entire theoretical framework relies critically on linear structure to derive rank conditions.

### Open Question 3
Can this offline framework adapt to online learning settings where reward recovery occurs incrementally? Current algorithms assume batch processing, requiring recomputation when new samples arrive.

### Open Question 4
How does the framework extend to general-sum games beyond zero-sum? The current QRE framework is derived specifically for the min-max structure of zero-sum games.

## Limitations
- Identifiability critically depends on linear parameterization and rank conditions; when these fail, only confidence sets are provided
- Regularization parameter η is assumed known, though related work suggests joint estimation is possible
- Sample complexity analysis assumes i.i.d. observations and sufficient exploration, which may not hold in real-world scenarios

## Confidence
- **High confidence**: Entropy-regularization mechanism for identifiability and O(N^{-1/2}) estimation rates are theoretically well-grounded
- **Medium confidence**: Confidence set construction works as described but depends heavily on threshold selection
- **Low confidence**: Markov game extension's backward error propagation and ridge regression robustness in high dimensions need more empirical validation

## Next Checks
1. **Sensitivity analysis**: Test Algorithm 1 on matrix games with varying degrees of partial identifiability to quantify confidence set size growth with parameter space dimensionality
2. **Unknown temperature estimation**: Implement joint estimation of θ* and η from data, comparing performance against baseline where η is assumed known
3. **Finite-sample behavior**: Conduct extensive simulations on Markov games to measure actual estimation error versus theoretical O(N^{-1/2}) rate, focusing on error accumulation across horizon H