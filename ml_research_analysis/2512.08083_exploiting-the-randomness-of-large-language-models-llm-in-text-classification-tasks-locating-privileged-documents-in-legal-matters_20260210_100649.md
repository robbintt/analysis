---
ver: rpa2
title: 'Exploiting the Randomness of Large Language Models (LLM) in Text Classification
  Tasks: Locating Privileged Documents in Legal Matters'
arxiv_id: '2512.08083'
source_url: https://arxiv.org/abs/2512.08083
tags:
- privileged
- documents
- document
- classification
- randomness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how randomness in large language models
  (LLMs) can improve the detection of legally privileged documents in legal document
  review. Using GPT-4.1, researchers applied zero-shot learning with multiple submissions
  per document and varied randomness parameters (Temperature, Top-P) to assess classification
  consistency.
---

# Exploiting the Randomness of Large Language Models (LLM) in Text Classification Tasks: Locating Privileged Documents in Legal Matters

## Quick Facts
- **arXiv ID:** 2512.08083
- **Source URL:** https://arxiv.org/abs/2512.08083
- **Reference count:** 17
- **Primary result:** Multiple LLM submissions per document significantly improve recall in legal privilege detection

## Executive Summary
This study investigates how randomness in large language models can enhance the detection of legally privileged documents during legal review. Using GPT-4.1 with zero-shot learning, researchers tested classification consistency across varying randomness parameters (Temperature, Top-P) and submission counts. The key finding is that submitting each document multiple times significantly improves recall—up to 7% in some configurations—with only modest precision losses, while randomness control parameters showed minimal impact on performance.

## Method Summary
The research employed zero-shot learning with GPT-4.1 to classify documents as privileged or non-privileged. Each document was submitted multiple times with varied randomness parameters (Temperature and Top-P values), and the resulting classifications were analyzed for consistency. The approach leveraged classification variability to rank documents by confidence, with the hypothesis that privileged documents would show more consistent classifications across submissions.

## Key Results
- Multiple submissions per document improved recall by up to 7% in some configurations
- Randomness control parameters (Temperature, Top-P) had minimal impact on classification consistency
- Confidence ranking based on classification variability provided practical performance enhancement
- Only modest precision losses accompanied recall improvements

## Why This Works (Mechanism)
The approach works by exploiting the inherent stochasticity in LLM outputs. When a document is submitted multiple times, privileged documents tend to receive more consistent classifications across submissions due to their distinctive characteristics, while non-privileged documents show greater variability. This consistency differential creates a natural confidence ranking that can be used to prioritize review.

## Foundational Learning
**Zero-shot learning:** The LLM classifies without task-specific training, relying on general language understanding. Why needed: Eliminates expensive domain-specific fine-tuning. Quick check: Verify model can handle task instructions without examples.

**Temperature parameter:** Controls randomness in output generation (higher = more random). Why needed: Tests whether output variability affects classification consistency. Quick check: Observe token distribution changes across temperature values.

**Top-P sampling:** Limits token selection to top probability mass. Why needed: Alternative randomness control to isolate effects. Quick check: Compare token diversity across different P values.

**Confidence scoring via repetition:** Uses classification consistency across multiple submissions as confidence metric. Why needed: Provides automated quality assessment without ground truth. Quick check: Correlate consistency scores with true labels in validation set.

## Architecture Onboarding

**Component map:** Document input -> LLM API (multiple submissions) -> Classification aggregation -> Confidence scoring -> Final classification

**Critical path:** Input document → Multiple LLM submissions → Consistency analysis → Confidence-based ranking

**Design tradeoffs:** Zero-shot vs. fine-tuning (generalization vs. task-specific accuracy), multiple submissions vs. computational cost, recall improvement vs. precision trade-off

**Failure signatures:** Inconsistent classifications across submissions for clearly privileged documents, high computational overhead relative to accuracy gains, systematic bias in confidence ranking

**First experiments:**
1. Test single-document multi-submission approach on a small, labeled dataset to verify recall improvements
2. Compare consistency scores between clearly privileged and non-privileged documents
3. Benchmark computational cost vs. accuracy gains across different repetition counts

## Open Questions the Paper Calls Out
None

## Limitations
- Findings may not generalize beyond legal document classification to other domains
- Minimal impact of randomness parameters might be specific to GPT-4.1 or the legal domain
- Zero-shot learning approach may not reflect practical scenarios requiring domain adaptation

## Confidence
**High Confidence:**
- Multiple submissions consistently improve recall in legal privilege detection
- Confidence ranking based on classification variability is effective
- Approach demonstrates measurable performance gains without model retraining

**Medium Confidence:**
- Temperature and Top-P parameters have minimal impact on classification consistency
- 7% recall improvement represents the upper bound across tested configurations
- Precision loss remains modest relative to recall gains

**Low Confidence:**
- Generalizability to non-legal domains and other classification tasks
- Performance ceiling for recall improvements through repetition
- Optimal repetition count balancing recall/precision trade-offs

## Next Checks
1. **Cross-Domain Validation:** Test the multiple-submission approach on non-legal text classification tasks (sentiment analysis, topic categorization) to assess domain transferability.

2. **Model Architecture Comparison:** Evaluate whether minimal impact of randomness parameters holds across different LLM architectures (Claude, LLaMA, open-source alternatives).

3. **Dynamic Repetition Strategy:** Develop and validate a stopping criterion for document submissions based on confidence score stability rather than fixed repetition counts.