---
ver: rpa2
title: Skip a Layer or Loop it? Test-Time Depth Adaptation of Pretrained LLMs
arxiv_id: '2507.07996'
source_url: https://arxiv.org/abs/2507.07996
tags:
- layers
- depth
- cola
- layer
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Chain-of-Layers (CoLa), a method for dynamically
  adapting the depth of pretrained large language models at test time without finetuning.
  CoLa treats each layer as a modular unit that can be selectively skipped or repeated
  during inference, allowing the model to construct custom execution paths per input.
---

# Skip a Layer or Loop it? Test-Time Depth Adaptation of Pretrained LLMs

## Quick Facts
- arXiv ID: 2507.07996
- Source URL: https://arxiv.org/abs/2507.07996
- Reference count: 33
- Primary result: Up to 30% depth reduction while improving accuracy on math and reasoning tasks

## Executive Summary
This paper introduces Chain-of-Layers (CoLa), a method for dynamically adapting the depth of pretrained large language models at test time without finetuning. CoLa treats each layer as a modular unit that can be selectively skipped or repeated during inference, allowing the model to construct custom execution paths per input. The authors use Monte Carlo Tree Search (MCTS) to efficiently search for optimal layer compositions that maximize prediction accuracy while minimizing depth. Experiments on math and commonsense reasoning tasks show that CoLa consistently finds shallower or more accurate execution paths than the original fixed-depth model, achieving up to 30% depth reduction while improving accuracy. Notably, for over 60% of originally incorrect predictions, CoLa finds shorter paths that yield correct answers, and even for correct predictions, it often identifies more efficient architectures. This demonstrates substantial redundancy in static transformer inference and highlights the potential of test-time depth adaptation for improving both efficiency and generalization.

## Method Summary
CoLa performs test-time depth adaptation by treating pretrained transformer layers as modular units that can be skipped, repeated, or reordered during inference. The method uses Monte Carlo Tree Search with a UCB-based objective that balances accuracy against path length. For each input, MCTS explores different layer compositions by applying skip or repeat actions, evaluating candidate paths on the input, and returning Pareto-optimal solutions. The approach is zero-shot and requires no finetuning, operating directly on frozen pretrained weights. Experiments use LLaMA-3-3B/8B and OLMoE-1B-7B models on ARC and DART datasets with 200 MCTS simulations per input.

## Key Results
- Up to 30% depth reduction while improving accuracy on math and reasoning tasks
- For over 60% of originally incorrect predictions, CoLa finds shorter paths that yield correct answers
- Even for correct predictions, CoLa often identifies more efficient architectures than the original fixed-depth model
- Skip-only excels on simpler tasks, recurrence-only on complex tasks, joint search achieves highest accuracy overall

## Why This Works (Mechanism)

### Mechanism 1: Layer Modularity via Residual Decomposition
Pretrained transformer layers can function as composable modules when residual connections allow representations to persist across non-adjacent layers. By treating each layer as an independent transformation unit, CoLa exploits the implicit modularity that residual connections create—representations change incrementally per layer, making skipped layers less disruptive than in non-residual architectures. The core assumption is that layers learn semi-independent features that don't strictly require sequential accumulation.

### Mechanism 2: Recurrence as Implicit Iterative Refinement
Repeating layers at test-time creates an RNN-like computation graph that enables iterative refinement without additional parameters. When a layer is repeated, it applies the same transformation to its own output, potentially refining representations through multiple passes—similar to unrolled iterative optimization or fixed-point computation. The core assumption is that layer transformations have fixed-point-like properties where repeated application converges toward improved representations.

### Mechanism 3: MCTS for Combinatorial Path Search with Depth-Aware UCB
Monte Carlo Tree Search efficiently explores the exponentially large space of layer compositions by balancing exploitation (high-reward paths) with exploration (under-visited paths) and penalizing depth. MCTS treats layer path construction as a sequential decision process—each action (skip/repeat) modifies the path, and UCB guides search toward accurate-yet-compact solutions. The depth penalty in UCB explicitly trades accuracy against computation.

## Foundational Learning

- Concept: **Residual Connections and Feature Propagation**
  - Why needed here: CoLa's ability to skip layers relies on residual connections allowing gradients and features to flow through non-sequential paths.
  - Quick check question: Can you explain why skipping layer 5 in a 12-layer residual network doesn't zero out the output?

- Concept: **Monte Carlo Tree Search (UCB, Exploration-Exploitation)**
  - Why needed here: The core search algorithm uses UCB to efficiently navigate the combinatorial layer-composition space.
  - Quick check question: In MCTS, what happens if you set the exploration constant c too low?

- Concept: **RNN Unrolling and Fixed-Point Iteration**
  - Why needed here: Layer recurrence creates implicit RNN dynamics; understanding convergence and iterative refinement is critical.
  - Quick check question: What condition must a transformation satisfy for repeated application to converge rather than diverge?

## Architecture Onboarding

- Component map: Input → MCTS Search Module → CoLa Path Specification → Dynamic Layer Executor → Output
- Critical path:
  1. Initialize root path as standard forward pass [L1, L2, ..., LN]
  2. MCTS iteratively applies skip/repeat actions with UCB guidance
  3. Evaluate candidate paths on input; reward = accuracy - λ·normalized_depth
  4. Return Pareto-optimal paths (accuracy vs. efficiency)

- Design tradeoffs:
  - Skip-only vs. recurrence-only vs. joint: Joint search is most expressive but increases search space exponentially
  - Simulation budget (200/input): Higher budget improves path quality but linearly increases latency
  - Depth penalty λ (set to 5.0): Higher values favor shallow paths, potentially sacrificing accuracy on hard tasks
  - Action granularity (k∈{1,2,3,4}): Larger blocks enable faster compression but reduce fine-grained control

- Failure signatures:
  - Recurrence instability: Outputs oscillate or diverge when repeating layers without convergence properties
  - Search budget exhaustion: MCTS returns suboptimal paths when 200 simulations insufficient for hard inputs
  - Mid-layer skip spikes: Smaller models show aggressive mid-layer skipping, which may fail if those layers encode task-critical features

- First 3 experiments:
  1. **Baseline validation**: Run CoLa on ARC-Easy with LLaMA-3-3B; verify skip-only achieves ~75% accuracy vs. 27.8% baseline
  2. **Ablation on search space**: Compare skip-only, recurrence-only, and joint search on DART-3; confirm joint search achieves highest accuracy
  3. **Depth-efficiency analysis**: Plot average CoLa depth vs. full model depth for C→C and W→C cases; verify W→C paths are shorter

## Open Questions the Paper Calls Out

- Can a lightweight, learned routing mechanism approximate the optimal CoLa paths discovered by MCTS to eliminate search latency at inference time?
- What specific representation dynamics cause standard forward passes to fail on inputs that are correctly solved by shallower CoLa paths?
- Does the test-time depth adaptation strategy transfer to autoregressive generation tasks where token dependencies extend beyond the final prediction?

## Limitations
- Core claim that residual connections enable meaningful layer modularity lacks direct empirical validation
- MCTS effectiveness for this specific application is not benchmarked against simpler search strategies
- The depth penalty hyperparameter λ=5.0 is fixed without sensitivity analysis

## Confidence
- **High**: Experimental results showing CoLa achieves shallower/more accurate paths than fixed-depth inference on tested datasets
- **Medium**: The theoretical claim that pretrained layers are modular enough for arbitrary skipping/repeating without catastrophic performance degradation
- **Low**: The assertion that residual connections are the key enabler of this modularity, given limited direct evidence

## Next Checks
1. **Residual Ablation**: Test CoLa on a non-residual transformer variant to empirically verify if residual paths are necessary for layer modularity
2. **Search Strategy Comparison**: Implement and compare MCTS against greedy search and beam search on a subset of tasks to quantify the marginal benefit of the more complex algorithm
3. **Hyperparameter Sensitivity**: Systematically vary λ (depth penalty) and c (exploration constant) across a range of values to map the performance landscape and identify optimal settings per task type