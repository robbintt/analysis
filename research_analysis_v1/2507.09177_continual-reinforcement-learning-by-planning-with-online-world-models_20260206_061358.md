---
title: Continual Reinforcement Learning by Planning with Online World Models
arxiv_id: '2507.09177'
source_url: https://arxiv.org/abs/2507.09177
generated_at: '2026-02-06T06:13:58'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Continual Reinforcement Learning by Planning with Online World Models

*Zichen Liu; Guoji Fu; Chao Du; Wee Sun Lee; Min Lin*

---

### ðŸ“Š Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Quality Score** | 8/10 |
| **Total References** | 40 Citations |
| **Regret Bound** | $O(\sqrt{K^2 D \log(T)})$ |
| **Core Innovation** | Follow-The-Leader (FTL) Online Agent |
| **Evaluation Tool** | Continual Bench |

---

## Executive Summary

Continual Reinforcement Learning (CRL) faces the fundamental challenge of catastrophic forgetting, where an agentâ€™s performance degrades on previously learned tasks upon acquiring new skills. This instability hinders the deployment of RL agents in dynamic, real-world environments where data streams are non-stationary and tasks arrive sequentially. Existing solutions often rely on complex deep neural networks or memory-intensive replay buffers to mitigate forgetting, yet these approaches frequently incur high computational costs and struggle to maintain stability across long sequences of tasks.

The authors propose the **"Follow-The-Leader" Online Agent (OA)**, a paradigm shift that replaces deep, task-specific representations with a shallow but wide online world model. Technically, the architecture utilizes a fixed random projection matrix $P$ and a learnable weight matrix $W$ to form a linear model defined as $y = W \sigma(P x)$. Unlike traditional methods that require historical data or deep representations, this agent employs Model Predictive Control (MPC) with the Cross-Entropy Method (CEM) to plan actions based solely on the latest state of the model. The learning mechanism relies on online regression via regularized least squares, updating the model incrementally without storing past experiences, which inherently renders the agent immune to forgetting.

The study provides rigorous theoretical and empirical validation of the proposed method. Theoretically, the online world model achieves a regret bound of $O(\sqrt{K^2 D \log(T)})$, confirming its stability in continual learning settings. Empirically, evaluated against strong baselines and using the newly introduced **"Continual Bench"** environment, the shallow online model outperformed agents utilizing deep world models, even when those deep models were augmented with standard continual learning techniques. Furthermore, the agent demonstrated constant update overhead regardless of data volume, ensuring efficiency as the number of tasks scales.

This research challenges the prevailing reliance on deep networks for CRL by demonstrating that simpler, theoretically grounded architectures can achieve superior performance in dynamic environments. The introduction of "Continual Bench" provides the community with a dedicated infrastructure for benchmarking CRL agents, addressing a gap in evaluation standards. By proving that immunity to catastrophic forgetting can be constructed through online planning and shallow models, this work opens new pathways for developing robust, scalable, and efficient AI systems capable of lifelong learning.

---

## Key Findings

*   **Immunity to Forgetting:** The proposed FTL Online Agent (OA) demonstrates inherent immunity to catastrophic forgetting, allowing it to learn new tasks continuously without losing previously acquired skills.
*   **Theoretical Robustness:** The online world model approach is theoretically robust, achieving a proven regret bound of $O(\sqrt{K^2 D \log(T)})$ under mild assumptions.
*   **Shallow vs. Deep:** Empirical results indicate that the shallow online model outperforms agents built on deep world models, even when those deep models use continual learning techniques.
*   **New Benchmark Environment:** The introduction of 'Continual Bench' provides a dedicated environment for evaluating Continual Reinforcement Learning (CRL) agents.
*   **Endless Evolution:** The agent successfully evolves endlessly via trial and error, solving sequences of tasks through model predictive control.

---

## Methodology

The proposed method utilizes an online **'Follow-The-Leader' (FTL)** shallow model to capture world dynamics incrementally rather than relying on static or deep representations. The architecture is designed to update incrementally:

*   **Planning:** The agent uses Model Predictive Control (MPC) to plan actions based solely on the latest version of the online model.
*   **Data Access:** The planner does not access historical data but relies entirely on the current state of the online model to generate actions.
*   **Evaluation:** Performance is measured against strong baselines within the same model-planning algorithmic framework to ensure fair comparison.

---

## Contributions

*   **Strategic Shift:** Proposes a planning-based strategy to address catastrophic forgetting by shifting focus to shallow online models that are immune to forgetting by construction.
*   **Theoretical Foundation:** Provides a mathematical regret bound validating the method's stability and performance in continual learning settings.
*   **Benchmarking Infrastructure:** Contributes 'Continual Bench,' a new evaluation infrastructure designed to facilitate the benchmarking of continual reinforcement learning agents.
*   **Empirical Validation:** Demonstrates empirically that a simpler, theoretically grounded approach using online shallow models can outperform complex deep models combined with standard continual learning techniques.

---

## Technical Details

**Architecture & Strategy**
*   **Model Type:** Shallow but wide networks using a Follow-The-Leader (FTL) strategy.
*   **Formula:** $y = W \sigma(P x)$
    *   $P$: Fixed random projection.
    *   $W$: Learnable weight matrix.
*   **Features:** Utilizes high-dimensional sparse features (dimension $D$) with only $K$ active nodes per input to ensure efficiency.

**Input & Output**
*   **Inputs:** State-action pairs $[s_t, a_t]$.
*   **Outputs:** State differences $(s_{t+1} - s_t)$.

**Learning Mechanism**
*   **Framework:** Online regression via regularized least squares with an incremental sparse update rule.
*   **Objective Function:** Minimizing $\|\Phi_{t-1}W - Y_{t-1}\|^2_F + \frac{1}{\lambda}\|W\|^2_F$.
*   **Role of Regularization:** Critical for ensuring convergence to a unique minimizer.

**Planning Algorithm**
*   **Method:** Model Predictive Control (MPC) using the Cross-Entropy Method (CEM).
*   **Dynamics:** Uses a single set of shared learning components and unified world dynamics ($P^u$) instead of task-specific weights.

---

## Results

*   **Theoretical Performance:** The online world model approach achieves a proven regret bound of $O(\sqrt{K^2 D \log(T)})$.
*   **Forgetting Resistance:** The agent demonstrates immunity to catastrophic forgetting, maintaining performance on previously acquired skills while learning new ones.
*   **Superiority of Shallow Models:** Empirical results indicate the proposed shallow online model outperforms deep world models, even those utilizing continual learning techniques.
*   **Benchmarking:** The paper introduces 'Continual Bench,' a new environment specifically designed for evaluating CRL agents.
*   **Efficiency:** Theoretically, regularization guarantees sparse model learning converges to the offline optimal solution, and the fixed small ratio of active nodes to total nodes ($K/D$) ensures constant and low update overhead regardless of data volume.

---

**Report Generated based on 40 Citations**