---
title: Scaling Agentic Reinforcement Learning for Tool-Integrated Reasoning in VLMs
arxiv_id: '2511.19773'
source_url: https://arxiv.org/abs/2511.19773
generated_at: '2026-02-03T06:32:17'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Scaling Agentic Reinforcement Learning for Tool-Integrated Reasoning in VLMs

*Meng Lu; Ran Xu; Yi Fang; Wenxuan Zhang; Yue Yu; Gaurav Srivastava; Yuchen Zhuang; Mohamed Elhoseiny; Charles Fleming; Carl Yang; Zhengzhong Tu; Yang Xie; Guanghua Xiao; Hanrui Wang; Di Jin; Wenqi Shi; Xuan Wang*

---

## üìã Executive Summary

> Current Vision-Language Models (VLMs) face a critical limitation in performing complex, multi-step visual reasoning, particularly when required to interact with external environments. While VLMs excel at passive recognition, they often fail to effectively select, invoke, and coordinate tools necessary for solving reasoning-intensive tasks. This capability gap is exacerbated by data fragmentation across diverse multimodal tasks, making it difficult to train models that can generalize tool usage across different types of visual problems without relying on static, pre-collected datasets.
>
> The researchers introduce a two-part solution: **VISTA-Gym**, a unified, scalable training infrastructure, and **VISTA-R1**, an agentic VLM trained via end-to-end reinforcement learning (RL). VISTA-Gym unifies 7 distinct reasoning tasks from 13 public datasets into a single environment featuring standardized interfaces for 26 pre-defined visual tools (covering perception, symbolic manipulation, and interpretation). The core technical innovation lies in the training strategy, which utilizes multi-turn trajectory sampling to incentivize "tool-integrated visual reasoning." Rather than simply augmenting inputs with tool outputs, the model is trained to interleave intrinsic agentic reasoning with dynamic tool execution, learning to select and invoke tools as an extension of its cognitive process.
>
> Evaluations across 11 reasoning-intensive VQA benchmarks demonstrate that the proposed method significantly outperforms existing approaches. The VISTA-R1-8B model surpassed state-of-the-art open-source baselines of similar size by a margin of 9.51% to 18.72%. Ablation studies highlighted the necessity of the interleaving strategy; direct tool augmentation without reasoning ("Tools only") resulted in accuracy degradation, while intrinsic reasoning alone ("Reasoning only") provided only marginal gains. The model demonstrated robust generalization, achieving these results across 5 in-domain and 6 out-of-domain benchmarks, including MapQA, UniGeo, and ChartQA.
>
> This work establishes a new paradigm for developing visual agents by proving that end-to-end RL is an effective method for unlocking complex tool-integrated reasoning capabilities in VLMs. By releasing VISTA-Gym as a scalable infrastructure that solves data fragmentation and VISTA-R1 as a high-performance benchmark, the authors provide a rigorous standard for assessing future visual agent capabilities. The findings suggest that the future of visual AI lies not just in larger models, but in training frameworks that successfully bridge the gap between cognitive reasoning and external tool interaction.

---

## üìä Quick Facts

| Metric | Value |
| :--- | :--- |
| **Model Name** | VISTA-R1-8B |
| **Parameter Count** | 8 Billion |
| **Performance Gain** | +9.51% to +18.72% (vs SOTA baselines) |
| **Benchmarks Evaluated** | 11 (5 In-domain, 6 Out-of-domain) |
| **Integrated Datasets** | 13 |
| **Reasoning Tasks** | 7 |
| **Available Tools** | 26 (Perception, Symbolic, Interpretation) |
| **Training Method** | End-to-End RL + Multi-turn Trajectory Sampling |

---

## üîë Key Findings

*   **Superior Performance:** The VISTA-R1-8B model outperforms state-of-the-art baselines of similar size by a margin of **9.51% to 18.72%** across 11 public reasoning-intensive VQA benchmarks.
*   **Bridging the Visual Reasoning Gap:** Successfully addresses limitations in tool selection, invocation, and coordination that are currently found in state-of-the-art VLMs.
*   **Effective RL Strategy:** Demonstrates that end-to-end reinforcement learning combined with multi-turn trajectory sampling is a highly effective method for enabling VLMs to interleave tool-use with agentic reasoning.
*   **Unified Training Environment:** Successfully unifies diverse, real-world multimodal reasoning tasks (7 tasks from 13 datasets) into a single, scalable training environment, solving data fragmentation issues.

---

## üõ†Ô∏è Methodology

The research methodology relies on a synergistic combination of a novel environment and a specialized training strategy.

### Environment Construction (VISTA-Gym)
*   **Unified Infrastructure:** Created a scalable training environment designed to unify diverse multimodal reasoning tasks.
*   **Standardization:** Implements standardized interfaces for visual tools, executable interaction loops, and verifiable feedback signals.

### Model Training (VISTA-R1)
*   **Learning Approach:** The model utilizes multi-turn trajectory sampling and end-to-end reinforcement learning.
*   **Core Strategy:** The training incentivizes **'tool-integrated visual reasoning.'** This teaches the model to interleave tool usage directly with agentic reasoning processes to solve multi-step visual interaction problems, rather than treating tools as separate, static add-ons.

---

## ‚öôÔ∏è Technical Details

The system architecture is divided into two primary components: the training environment and the agent model.

### VISTA-Gym (Infrastructure)
*   **Function:** Acts as a wrapper for visual tool operations using a standardized API.
*   **Toolset:** Includes 26 pre-defined tools covering perception, symbolic manipulation, and interpretation.
*   **Performance:** Utilizes multithreading and parallel execution for scalability.
*   **Scope:** Unifies 7 reasoning tasks from 13 public datasets.

### VISTA-R1 (The Model)
*   **Architecture:** An 8 billion parameter model (VISTA-R1-8B).
*   **Training:** Trained via end-to-end reinforcement learning and multi-turn trajectory sampling.
*   **Target Capabilities:** Focuses on tool selection, invocation, and coordination by interleaving reasoning with tool execution and supplying tool-selection priors.

### Interventions Evaluated
The paper evaluated three specific training interventions:
1.  **Tools only (w/ T):** Direct tool augmentation.
2.  **Reasoning only (w/ R):** Intrinsic reasoning without tools.
3.  **Tools & Reasoning (w/ T&R):** The proposed interleaving method.

---

## üìà Results

*   **Ablation Study Insights:**
    *   Direct tool augmentation (**w/ T**) resulted in significant accuracy degradation.
    *   Intrinsic reasoning (**w/ R**) provided only limited gains.
    *   The proposed method of interleaving reasoning with tool execution (**w/ T&R**) successfully improved performance.
*   **Model Comparison:** Gains were task-dependent for commercial VLMs, whereas small open-source VLMs continued to struggle compared to commercial counterparts.
*   **Benchmark Success:** VISTA-R1-8B achieved significant improvements over state-of-the-art open-source baselines of comparable size.
*   **Evaluation Scope:** The model was rigorously tested on 11 public reasoning-intensive VQA benchmarks (5 in-domain, 6 out-of-domain), including **MapQA**, **UniGeo**, and **ChartQA**.

---

## üß© Contributions

1.  **VISTA-Gym:** A novel, unified, and scalable training infrastructure designed specifically for visual agentic reinforcement learning that solves data fragmentation challenges.
2.  **VISTA-R1:** A proof-of-concept model that establishes a new performance benchmark for tool-integrated reasoning in VLMs, demonstrating that RL can unlock complex visual reasoning capabilities.
3.  **Benchmarking Framework:** Provided a comprehensive evaluation across 11 reasoning-intensive VQA benchmarks, offering a rigorous standard for assessing future visual agent capabilities.

---

*Paper Quality Score: 9/10 | References: 40 citations*