# Words or Vision: Do Vision-Language Models Have Blind Faith in Text?

*Ailin Deng; Tri Cao; Zhirui Chen; Bryan Hooi*

---

> ### ðŸ“Š Quick Facts
> *   **Quality Score:** 8/10
> *   **References:** 40 Citations
> *   **Models Evaluated:** 10 VLMs
> *   **Tasks:** 4 Vision-Centric Tasks (VQAv2, MathVista, DocVQA, TextVQA)
> *   **Key Metric:** Text Preference Ratio (TPR)

---

## Executive Summary

 This paper addresses a critical vulnerability in contemporary Vision-Language Models (VLMs) termed "**blind faith in text**," where models disproportionately prioritize linguistic inputs over visual evidence when conflicts arise. This modality bias poses severe safety and robustness risks, as models can be easily misled by corrupted or adversarial textual descriptions, effectively rendering visual verification useless in scenarios where accuracy is paramount. The research highlights that while VLMs possess the inherent capability for visual processing, they frequently fail to utilize it in the presence of conflicting textual data, creating a fundamental flaw in their reasoning architecture.

To diagnose this issue, the authors introduce a rigorous diagnostic framework evaluated across a methodologically extensive scope of **10 distinct VLMs** and **4 vision-centric tasks**. The study proposes the **Text Preference Ratio (TPR)** metric to quantify text prioritization under three benchmark conditions: Corruption, Match, and Irrelevance. Technically, the framework analyzes specific variables influencing bias, including instruction prompts, language model scaling, and text relevance. A key technical finding is that token order plays a significant role in bias; positional biases can exacerbate the prioritization of text depending on the sequence of inputs. Additionally, the authors hypothesize that the root driver of this bias is the imbalance between pure text and multi-modal data during training and offer a mitigation strategy using Supervised Fine-Tuning (SFT) with text augmentation.

The results reveal a significant divide in robustness between model families. Open-weight models like **LLaVA-NeXT** show high text preference (TPRs up to 87.77%) compared to proprietary models like **Claude Sonnet**, which demonstrated high robustness (TPRs as low as 9.58%). While scaling up language model size slightly mitigates bias in some familiesâ€”showing an inverse correlation in LLaVA-NeXTâ€”this trend is not universal; for instance, **Qwen2-VL-7B** displayed anomalous behavior with higher TPR in corruption cases than match cases. The data confirms that instruction prompts and token order materially impact modality preference, yet bias trends persist across the diverse datasets tested, proving that the issue is not dataset-specific but systemic to the models' training.

The significance of this research lies in its exposure of architectural flaws and specific data imbalances as primary drivers of modality bias in current VLMs. By identifying factors such as token order and training composition, the study necessitates a shift toward evaluating "**modality faithfulness**" rather than just accuracy. This work influences the design of safer multi-modal systems by providing a concrete metric (TPR) and mitigation strategy (SFT with text augmentation), guiding the field toward developing models that require genuine visual verification and are less susceptible to manipulation in sensitive applications.

---

## Key Findings

*   **Blind Faith Phenomenon:** VLMs display a "blind faith in text," disproportionately trusting textual data over visual data during conflicts. This reliance degrades performance significantly when text is corrupted.
*   **Influencing Factors:** Specific factors like model architecture and inputs affect bias. Scaling up language model size slightly mitigates it, while token order can exacerbate it due to positional biases.
*   **Training Data Imbalance:** A primary driver of text bias is likely the imbalance between the amount of pure text data versus multi-modal data used during training.
*   **Security Risks:** Over-reliance on text poses safety and robustness risks, as models can be easily misled by incorrect or manipulated textual descriptions.

---

## Methodology

The researchers evaluated ten different Vision-Language Models (VLMs) across four vision-centric tasks by introducing textual variations to assess modality preferences. The methodology involved:

*   **Variable Analysis:** Analyzing variables that influence bias, including instruction prompts, language model size, text relevance, token order, and the interplay between visual and textual certainty.
*   **Mitigation Testing:** Testing the effectiveness of supervised fine-tuning with text augmentation to reduce bias.
*   **Theoretical Analysis:** Conducting a theoretical analysis to correlate observed bias with the composition of training data.

---

## Contributions

*   **Phenomenon Identification:** Identification and definition of the "blind faith in text" phenomenon as a fundamental weakness in current VLMs.
*   **Diagnostic Framework:** Provision of a diagnostic framework analyzing how architectural and input-based factors contribute to modality bias.
*   **Practical Solution:** Introduction of a practical solution involving supervised fine-tuning with text augmentation to mitigate text bias.
*   **Theoretical Hypothesis:** Offering a theoretical hypothesis linking text bias to training data imbalances to guide the development of robust training curriculums.

---

## Technical Details

*   **Evaluation Framework:**
    *   **Benchmark Conditions:** Corruption, Match, and Irrelevance.
    *   **Primary Metric:** Text Preference Ratio (TPR), which quantifies prioritization of text over visual data.
*   **Hypothesized Drivers of Bias:**
    *   **Data Imbalances:** Disparity between pure text and multi-modal training data.
    *   **Architectural Factors:** Positional/tensor order and Language Model scaling.

---

## Results

*   **Model Performance Divide:**
    *   **Open-weight models** (e.g., LLaVA-NeXT) exhibit high text preference with TPRs up to **87.77%** and significant performance drops under corruption.
    *   **Proprietary models** (e.g., Claude Sonnet) show robustness with low TPRs (e.g., **9.58%**).
*   **Scaling vs. Bias:** An inverse correlation exists between model size and TPR within the LLaVA-NeXT family, where larger models are less susceptible.
*   **Anomalies:** Qwen2-VL-7B showed anomalous behavior with a higher TPR in corruption cases than in match cases.
*   **Generalization:** TPR is negligible when text is irrelevant, and bias trends persist across VQAv2, MathVista, and DocVQA datasets, indicating a systemic issue.