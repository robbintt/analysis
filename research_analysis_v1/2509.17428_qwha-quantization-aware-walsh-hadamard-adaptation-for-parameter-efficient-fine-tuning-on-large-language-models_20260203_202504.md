---
title: 'QWHA: Quantization-Aware Walsh-Hadamard Adaptation for Parameter-Efficient
  Fine-Tuning on Large Language Models'
arxiv_id: '2509.17428'
source_url: https://arxiv.org/abs/2509.17428
generated_at: '2026-02-03T20:25:04'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# QWHA: Quantization-Aware Walsh-Hadamard Adaptation for Parameter-Efficient Fine-Tuning on Large Language Models

*Hyesung Jeon; Seojune Lee; Beomseok Kang; Yulhwa Kim; Jae-Joon Kim*

---

> ### ðŸ“Š Quick Facts
>
> *   **Model Architecture:** Quantization-Aware PEFT (QWHA)
> *   **Core Mechanism:** Walsh-Hadamard Transform (WHT)
> *   **Test Bed:** LLaMA-3.2-3B (4-bit quantization)
> *   **Parameter Budget:** $r=64$
> *   **Rank Efficiency:** Nearly Full-Rank (vs. LoRA < 6.3%)
> *   **Computational Advantage:** Uses Recursive Sub & Add (vs. Complex Multiplication)
> *   **Paper Score:** 9/10

---

## Executive Summary

Deploying Large Language Models (LLMs) on resource-constrained hardware requires low-bit quantization, a process that introduces errors degrading model accuracy. Recovering this performance via fine-tuning presents a critical trade-off: standard Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA are computationally efficient but lack the representational capacity to correct complex quantization errors. Conversely, Fourier Transform (FT)-based adapters offer high capacity but rely on computationally expensive complex multiplications, resulting in prohibitive overhead that hinders their integration with quantized models.

To resolve this, the researchers introduce **QWHA** (Quantization-Aware Walsh-Hadamard Adaptation), an architecture that replaces standard Fourier kernels with the **Walsh-Hadamard Transform (WHT)**. The core innovation lies in the WHT kernel, which consists exclusively of $\pm 1$ entries, allowing QWHA to execute updates via efficient "Recursive Sub & Add" operations rather than complex multiplications. The method formulates weight updates as a single inverse transform of a sparse coefficient matrix ($\Delta W = F H^{-1}$), effectively functioning as a full-rank adapter. Additionally, QWHA employs a unique initialization strategy combining "**AdaAlloc**" and "**Refinement**" to adaptively select parameters and reconstruct quantization errors prior to training.

In experiments conducted on LLaMA-3.2-3B with 4-bit quantization and a fixed parameter budget ($r=64$), QWHA demonstrated superior representational efficiency, achieving nearly full-rank status compared to LoRA, which achieved less than 6.3% of the normalized max rank. QWHA consistently outperformed baseline methodsâ€”including LoRA, FourierFT, LoCA, and SSHâ€”in accuracy while effectively reducing the $\ell_2$ norm of layer output errors. Crucially, the method achieved significant training speedups compared to other FT-based adapters by eliminating complex multiplications, thereby substantially lowering the computational cost and overhead associated with transform-based integration.

This research establishes that transform-based adapters can be both highly accurate and computationally efficient, challenging the assumption that high-capacity adaptation requires prohibitive computational costs. By enabling full-rank adaptation with the efficiency of "Recursive Sub & Add" operations, QWHA makes sophisticated fine-tuning techniques viable for resource-constrained hardware. This advancement paves the way for the effective deployment of adaptable, quantized LLMs on edge devices, positioning the Walsh-Hadamard Transform as a superior alternative to traditional Fourier kernels for quantization-aware PEFT.

---

## Key Findings

*   **Superior Accuracy:** QWHA consistently outperforms baseline methods in accuracy for low-bit quantization of LLMs.
*   **Training Speedup:** Achieves significant speedups compared to existing Fourier-related transform (FT)-based adapters.
*   **Error Reduction:** Effectively reduces quantization errors both prior to and during the fine-tuning process.
*   **Cost Efficiency:** The design substantially lowers the computational cost and overhead typically associated with integrating transform-based adapters into quantized models.

---

## Methodology

The researchers developed **QWHA**, a quantization-aware parameter-efficient fine-tuning (PEFT) method specifically designed to integrate Fourier-related transform-based adapters into quantized models. The approach distinguishes itself through the following strategies:

*   **Kernel Substitution:** Instead of using standard Fourier kernels, the method employs the **Walsh-Hadamard Transform (WHT)** as the transform kernel.
*   **Novel Initialization:** The approach utilizes a unique adapter initialization strategy that incorporates **adaptive parameter selection** and **value refinement** to identify optimal parameters and enhance the starting state.

---

## Technical Details

| Component | Description |
| :--- | :--- |
| **Weight Update Formulation** | $\Delta W = F H^{-1}$ (Transformation of a sparse coefficient matrix via inverse Walsh-Hadamard Transform). |
| **Sparse Matrix Construction** | Built using a 'Scatter' operation defined by a value vector and an index list. |
| **Transform Kernel** | Uses the **Walsh-Hadamard Transform (WHT)**, consisting exclusively of $\pm 1$ entries. |
| **Computation Method** | Executes via **'Recursive Sub & Add'** operations to drastically reduce computational overhead. |
| **Adapter Rank** | Functions as a **full-rank** adapter (maximum possible rank), provided the sparse matrix assigns at least two parameters to each row and column on average. |
| **Transform Complexity** | Uses a **single transform** (unlike double-transform adapters) to reduce complexity. |
| **Initialization Strategy** | **'AdaAlloc' and 'Refinement'** strategy used to reconstruct quantization errors and target outlier-induced errors. |

---

## Contributions

1.  **Architecture Introduction:** Introduced the **Quantization-Aware Walsh-Hadamard Adaptation** architecture, successfully bridging the gap between quantization and high-capacity adapters.
2.  **Technical Solution:** Applied the **Walsh-Hadamard Transform (WHT)** to solve the problems of ineffective error reduction and high computational overhead inherent in standard FT-based adapters.
3.  **Initialization Scheme:** Developed a unique initialization scheme (adaptive parameter selection and value refinement) that facilitates fine-tuning while minimizing quantization error.
4.  **Empirical Validation:** Provided empirical evidence demonstrating that transform-based adapters can be both highly accurate and computationally efficient in low-bit quantization scenarios.

---

## Results

*   **Representational Efficiency:** In tests, LoRA achieved less than **6.3%** of normalized max rank, while **QWHA achieved nearly full-rank status**.
*   **Performance vs. Baselines:** QWHA consistently outperformed baseline methods (LoRA, FourierFT, LoCA, SSH) in accuracy for low-bit LLM quantization.
*   **Computational Efficiency:** Achieved significant training speedups compared to other FT-based adapters.
*   **Error Metrics:** Effectively reduced the $\ell_2$ norm of layer output error.
*   **Evaluation Setup:**
    *   **Model:** LLaMA-3.2-3B
    *   **Quantization:** 4-bit
    *   **Budget:** Fixed parameter budget ($r=64$)
*   **Key Metrics Evaluated:** Outlier Component Inclusion Ratio, $\ell_2$ norm of layer output error, and spectral efficiency via the Pareto hill index.

---

**Quality Score:** 9/10
**References:** 40 citations