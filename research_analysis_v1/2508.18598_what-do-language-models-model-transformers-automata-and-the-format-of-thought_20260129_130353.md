# What do language models model? Transformers, automata, and the format of thought

*Colin Klein*

---

### Quick Facts

| Metric | Value |
| :--- | :--- |
| **Field** | AI Philosophy / Cognitive Architecture |
| **Methodology** | Theoretical Analysis |
| **Quality Score** | 8/10 |
| **References** | 5 Citations |
| **Core Concept** | Linear vs. Supralinear Computation |

---

## Executive Summary

This paper addresses the fundamental ambiguity in AI research regarding whether Large Language Models (LLMs) internalize human cognitive capacities ("mind modeling") or merely replicate statistical properties of their training data ("corpus modeling"). This distinction is critical because the tendency to anthropomorphize AI systems obscures the functional boundaries of current transformer architectures. By challenging the assumption that high performance equates to human-like understanding, the paper seeks to resolve the debate over the nature of LLM representation and define the specific architectural constraints that differentiate biological cognition from artificial computation.

The core innovation is a theoretical framework based on **"computational formats,"** which distinguishes between the "supralinear" (complex, graph-like) nature of human thought and the strictly "linear" processing of transformers. The author formally categorizes representational families into sublinear, linear, and supralinear formats, identifying the residual stream as the mechanism that restricts transformers to linear sequencing. Within this framework, transformers are characterized as **"shortcut automata"**: they utilize the invariants of the linear residual stream to bypass the need for reconstructing complex mental states, allowing them to compute valid outputs by riding the structural constraints of the data rather than simulating the underlying cognitive process.

The study presents formal theoretical findings demonstrating that the linear processing format of transformers is computationally insufficient to support the supralinear structures required for human-like syntax. Specifically, the analysis proves that while human language relies on re-entrant, non-sequential structures (such as hierarchical dependencies or center embeddings), transformers are mathematically restricted to sequential information processing within the residual stream. Despite this limitation, the paper introduces the **"discourse machine"** hypothesis as a specific explanatory result: it argues that language itself functions as a structured context that constrains valid outputs, allowing LLMs to achieve convergent functional utility by taking architectural "shortcuts" to the correct answer without replicating the generative mechanisms of the human mind.

This work significantly influences the field by shifting the analytical focus from behavioral evaluation (e.g., benchmark scores) to structural architectural analysis. It provides a rigorous, non-deflationary defense of the position that LLMs model the corpus rather than the human mind, offering a robust explanatory framework for the gap between AI performance and biological intelligence. By formalizing the distinction between linear and supralinear processing, the paper guides future research toward developing architectures that may need to transcend linear processing limitations to achieve true cognitive compatibility.

---

## Key Findings

*   **Architectural Divergence**: LLMs model their training corpus rather than human cognitive capacities. This distinction is grounded in the difference between human **"supralinear"** computational formats and the strictly linear processing formats supported by transformer architectures.
*   **Function as Shortcut Automata**: The analysis suggests that transformers operate effectively as **"shortcut automata,"** relying on the invariants of their computational architecture to process information rather than reconstructing cognitive states.
*   **Language as a Discourse Machine**: Language is characterized not merely as a medium for expressing internal states, but as a functional mechanism—a **"discourse machine"**—that allows both humans and LLMs to generate new language within a context.
*   **Convergent Utility, Divergent Mechanism**: While humans and LLMs have both learned to utilize language as a discourse machine, they achieve this through fundamentally different computational means (supralinear cognition vs. linear architecture).

---

## Technical Details

The paper employs a theoretical approach grounded in computer science and cognitive architecture to analyze Transformers based on **computational formats**, defined as the structure of data and its interaction with computation.

### Representational Taxonomy
The author outlines a taxonomy of representational families:
*   **Sublinear**: No ordering.
*   **Linear**: Strict ordering.
*   **Supralinear**: Complex structures like graphs.

### Architecture Analysis
*   **Residual Stream**: Identified as the critical component determining the computational format.
*   **Architectural Variants**: The analysis distinguishes between:
    *   *Unmasked* (e.g., BERT)
    *   *Masked* (e.g., GPT)
*   **Operational Characterization**: Transformers are characterized as "shortcut automata" that rely on architectural invariants to bypass complex cognitive simulation.

---

## Methodology

The paper utilizes a **theoretical and comparative analysis**, employing computational architecture invariants to contrast the linear format limitations of transformers against the supralinear computation requirements of human linguistic capabilities identified in cognitive science.

---

## Results

The text provides qualitative theoretical findings rather than quantitative experimental metrics. Key results include:

*   **Corpus vs. Cognition**: Concludes that LLMs model their training corpus rather than human cognitive capacities due to a mismatch in computational formats; humans utilize supralinear formats while Transformers are restricted to linear formats.
*   **Discourse Machine Hypothesis**: Posits that humans and LLMs achieve convergent utility through divergent mechanisms.
*   **Architectural Insufficiency**: Argues that the linear processing of Transformers is insufficient for human-like syntactic processing, supporting deflationary views of LLM capabilities based on data richness and architectural invariants.

---

## Contributions

*   **Philosophical Defense**: Provides a non-deflationary philosophical defense of the position that LLMs model the corpus rather than the human mind.
*   **Positive Explanation**: Integrates the concept of "shortcut automata" to offer a positive explanation of transformer operations.
*   **Theoretical Advancement**: Advances the theoretical understanding of the relationship between AI architecture and human cognition by formalizing the distinction between linear and supralinear processing formats.