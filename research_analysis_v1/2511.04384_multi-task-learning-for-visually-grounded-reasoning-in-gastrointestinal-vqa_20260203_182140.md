---
title: Multi-Task Learning for Visually Grounded Reasoning in Gastrointestinal VQA
arxiv_id: '2511.04384'
source_url: https://arxiv.org/abs/2511.04384
generated_at: '2026-02-03T18:21:40'
quality_score: 7
citation_count: 15
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Multi-Task Learning for Visually Grounded Reasoning in Gastrointestinal VQA

*Itbaan Safwan; Muhammad Annas Shaikh; Muhammad Haaris; Ramail Khan; Muhammad Atif Tahir*

> ### ðŸ“Š Quick Facts
> *   **Quality Score:** 7/10
> *   **Total Citations:** 15
> *   **Model Architecture:** Florence-2 (LoRA-tuned)
> *   **Core Technique:** Multi-Task Learning (MTL)
> *   **Primary Domain:** Gastrointestinal Medical Imaging
> *   **Key Datasets:** Kvasir-VQA-x1, Synthetically Enriched Explanation Dataset, Text-to-Region Pairs

---

## Executive Summary

Current approaches to Visual Question Answering (VQA) in gastrointestinal medical imaging typically address distinct capabilities in isolation, resulting in a fragmented user experience. Traditional single-task models struggle to simultaneously generate accurate answers, provide coherent medical reasoning, and precisely localize relevant anatomical regions. This limitation hinders clinical utility, as physicians require AI systems that deliver not only a correct diagnosis but also transparent explanations and clear visual grounding to support decision-making.

To address this fragmentation, the paper proposes a unified multi-task framework built upon a **Florence-2** foundation model, fine-tuned using **Low-Rank Adaptation (LoRA)** for parameter efficiency. The core technical innovation is a novel data integration strategy that synthesizes three distinct learning streams: the **Kvasir-VQA-x1** dataset for foundational Q&A; a **Synthetically Enriched Explanation Dataset** for structured medical reasoning; and **Text-to-Region Pairs** to link specific visual features with segmentation masks. By training on this composite dataset, the model learns to jointly optimize for Visual Grounding, Reasoning, and Interpretation, effectively bridging high-level semantic understanding with low-level visual features.

Empirical evaluation demonstrates that the proposed multi-task framework achieves superior VQA answer accuracy and visual localization precision compared to isolated single-task baselines. The study validates the efficacy of the LoRA-tuned Florence-2 architecture, confirming its ability to successfully generate highly interpretable responses that merge structured medical reasoning with precise visual grounding. Specifically, the model outperformed baselines in identifying and localizing regions of interest while maintaining high accuracy in diagnostic reasoning, validating the advantage of the joint learning approach.

---

## Key Findings

*   **Superior Performance over Baselines:** The proposed multi-task framework substantially outperforms single-task baselines in both answer accuracy and visual localization tasks.
*   **Effective Joint Learning:** The system successfully enables joint learning of three distinct capabilitiesâ€”visual grounding, reasoning, and interpretationâ€”within a single model.
*   **Enhanced Interpretability:** By integrating structured medical reasoning and visual grounding, the model produces responses that are not only accurate but also highly interpretable.
*   **Model Efficacy:** The LoRA-tuned Florence-2 model is validated as an effective architecture for handling simultaneous VQA, explanation generation, and visual grounding in medical imaging.

---

## Methodology

The researchers developed a multi-task framework centered on a **Florence-2** model fine-tuned using **LoRA (Low-Rank Adaptation)**. The training strategy integrates three distinct, curated datasets to address different aspects of the problem:

*   **Kvasir-VQA-x1:** Used for foundational question-answer learning.
*   **Synthetically Enriched Explanation Dataset:** Utilized to develop structured medical reasoning capabilities.
*   **Text-to-Region Pairs:** Employed to link specific visual features with segmentation masks for grounding.

This data combination allows the model to simultaneously learn **Visual Question Answering (VQA)**, **Explanation Generation**, and **Visual Grounding**.

---

## Technical Details

| Component | Description |
| :--- | :--- |
| **Backbone Architecture** | Florence-2 foundation model. |
| **Optimization Method** | **LoRA (Low-Rank Adaptation)**: Freezes pre-trained weights while training a small number of adapter parameters for parameter efficiency. |
| **Learning Paradigm** | **Multi-Task Learning (MTL)**: Simultaneously learns three distinct capabilities. |
| **Core Capabilities** | 1. **Visual Grounding:** Identifying and localizing regions.<br>2. **Reasoning:** Generating VQA answers.<br>3. **Interpretation:** Generating explanations via structured medical reasoning. |
| **Integration Strategy** | Architecture integrates structured medical reasoning directly with visual grounding. |

---

## Contributions

*   **Multi-Task Framework for Medical VQA:** Introduction of a robust framework for the MediaEval Medico 2025 challenge that handles VQA, explanation generation, and visual grounding concurrently.
*   **Curated Multi-Modal Data Integration:** A novel data integration strategy that combines standard VQA datasets with synthetic reasoning data and text-to-region pairs to bridge the gap between visual features and semantic understanding.
*   **Validation of Grounded Multi-Task Learning:** Empirical evidence demonstrating that grounded multi-task learning is more effective for medical VQA applications than isolated single-task approaches, specifically improving localization and accuracy.

---

## Results

The proposed multi-task framework outperformed single-task baselines across the board.

*   **Performance:** The model achieved **superior performance** in VQA accuracy and outperformed baselines in localization tasks (though specific numerical metrics were not provided in the analysis).
*   **Validation:** Experiments validated the LoRA-tuned Florence-2 architecture as effective for handling simultaneous tasks in medical imaging.
*   **Interpretability:** The model produced **highly interpretable** responses, successfully merging diagnostic reasoning with visual evidence.