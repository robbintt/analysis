---
title: 'LittleBit: Ultra Low-Bit Quantization via Latent Factorization'
arxiv_id: '2506.13771'
source_url: https://arxiv.org/abs/2506.13771
generated_at: '2026-02-03T19:27:06'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# LittleBit: Ultra Low-Bit Quantization via Latent Factorization

*Banseok Lee; Dongkyu Kim; Youngcheon You; Youngmin Kim*

---

## ðŸ“Š Quick Facts

| Metric | Value |
| :--- | :--- |
| **Compression Rate** | 0.1 Bits Per Weight (BPW) |
| **Memory Reduction** | **31Ã—** (Llama2-13B â†’ < 0.9 GB) |
| **Speedup** | 11.6Ã— faster than FP16 (kernel level) |
| **Key Innovation** | Latent Matrix Factorization + Multi-Scale Compensation |
| **Performance** | Llama2-13B @ 0.1 BPW achieves **5.28 PPL** (WikiText-2) |

---

## Executive Summary

Deploying Large Language Models (LLMs) on resource-constrained edge devices is a significant challenge due to their massive memory footprints and computational demands. While quantization is a standard technique for reducing model size, pushing compression to the extreme (sub-1-bit) typically results in catastrophic performance degradation, rendering models unusable. This paper addresses the critical barrier of the "sub-1-bit wall," demonstrating that it is possible to maintain high accuracy while drastically reducing precision to levels previously considered impractical for large-scale models.

LittleBit introduces a novel framework that unifies **Latent Matrix Factorization** with **Multi-Scale Compensation** to enable viable sub-1-bit quantization. Technically, the method decomposes weight matrices into low-rank factors ($U, V^T$), which are then binarized. To counter the substantial information loss caused by this extreme precision reduction, the authors implement a multi-scale compensation mechanism that learns per-rank importance across row, column, and latent dimensions. The training process is stabilized through **Dual Sign-Value-Independent Decomposition (Dual-SVID)** for Quantization-Aware Training (QAT) initialization, while an integrated residual compensation path actively mitigates errors during optimization.

The method achieves a groundbreaking compression rate of **0.1 bits per weight (BPW)**, resulting in a **31Ã— reduction** in memory footprint; for instance, the Llama2-13B model is compressed to under 0.9 GB. Despite this extreme compression, LittleBit breaks the sub-1-bit performance barrier, with Llama2-7B at 0.1 BPW outperforming leading methods operating at 0.7 BPW. Specifically, Llama2-13B compressed to 0.1 BPW achieved a WikiText-2 perplexity of 5.28, surpassing competitors like STBLLM (0.55 BPW) and OmniQuant (2-bit). Additionally, the approach facilitates a potential **11.6Ã— speedup** over FP16 precision at the kernel level.

LittleBit establishes a new state-of-the-art size-performance trade-off, proving that extreme quantization does not necessitate a proportional loss in accuracy. By unlocking sub-1-bit inference for LLMs, this research makes it practical to deploy massive models on consumer-grade hardware and edge devices without relying on cloud infrastructure. The findings represent a significant shift in the efficiency landscape of generative AI, offering a pathway to democratize access to high-performance LLMs across diverse, resource-limited environments.

---

## Key Findings

*   **Extreme Compression:** Achieves a record-breaking compression rate of **0.1 bits per weight (BPW)**.
*   **Massive Memory Savings:** Results in a **31Ã— memory reduction**, capable of compressing the massive Llama2-13B model to under **0.9 GB**.
*   **Performance Breakthrough:** The 0.1 BPW performance of LittleBit on Llama2-7B **surpasses** leading methods operating at significantly higher precision (0.7 BPW), effectively breaking the sub-1-bit performance barrier.
*   **Computational Efficiency:** Establishes a new size-performance trade-off that unlocks a potential **11.6Ã— speedup** over FP16 precision at the kernel level.
*   **Edge Deployment:** Makes LLMs practical for deployment in resource-constrained environments by solving the severe performance degradation usually associated with extreme quantization.

---

## Methodology

The LittleBit method employs a combination of factorization and compensation techniques to achieve extreme quantization:

*   **Latent Matrix Factorization:** The method represents weights in a low-rank form, which are subsequently binarized. This leverages the inherent low-rank structure of LLM weights.
*   **Multi-Scale Compensation:** To address substantial information loss from extreme precision reduction, this mechanism operates across row, column, and latent dimensions to learn per-rank importance.
*   **Dual-SVID (Dual Sign-Value-Independent Decomposition):** A specific optimization used for stable Quantization-Aware Training (QAT) initialization, handling the unique challenges of training extreme low-bit models.
*   **Integrated Residual Compensation:** An optimization strategy used during training to actively mitigate errors and stabilize the learning process.

---

## Technical Details

LittleBit proposes a framework for extreme sub-1-bit quantization (targeting 0.1 Bits Per Weight) by unifying Latent Matrix Factorization with Multi-Scale Compensation.

### Architecture & Decomposition
*   **Low-Rank Factors:** Weight matrices are decomposed into low-rank factors ($U, V^T$) which are then binarized.
*   **Storage:** Only binarized factors and scales are stored, minimizing memory footprint.
*   **Dual-Pathway Structure:**
    *   **Primary Path:** Handles the main approximation.
    *   **Residual Path:** Provides correction terms.

### Optimization Mechanisms
*   **Learnable Scaling:** Utilizes learnable FP16 scaling factors across rows, columns, and a latent dimension to counteract information loss.
*   **Training Pipeline:** Uses Quantization-Aware Training (QAT) with the Straight-Through Estimator (STE).
*   **Initialization:** Employs the Dual-SVID technique to ensure stable training start-up.

---

## Core Contributions

1.  **Latent Factorization Framework:** Introduction of a novel framework that applies latent matrix factorization and binarization to enable viable sub-1-bit quantization for LLMs.
2.  **Dual-SVID:** A specific initialization technique for Quantization-Aware Training (QAT) that handles the unique challenges of training extreme low-bit models.
3.  **Integrated Residual Compensation:** A mechanism designed to mitigate the errors and accuracy loss that typically occur when training models at extreme compression levels (e.g., 0.1 BPW).
4.  **Multi-Scale Compensation:** A novel strategy to counteract information loss by learning and adjusting for the importance of specific ranks across multiple dimensions (row, column, and latent).

---

## Results & Performance

LittleBit sets a new state-of-the-art for sub-1-bit quantization with the following specific outcomes:

*   **Llama2-13B @ 0.1 BPW:** Achieved a perplexity of **5.28** on WikiText-2.
    *   Surpassed STBLLM (0.55 BPW).
    *   Surpassed OmniQuant (2-bit, 5.42 PPL).
*   **Llama2-7B @ 0.1 BPW:** Outperformed leading methods operating at **0.7 BPW**.
*   **Llama2-32B @ 0.3 BPW:** Maintained strong performance despite the large model size.
*   **Efficiency:** Validated the potential for **11.6Ã— speedup** over FP16 precision at the kernel level.

---
**Quality Score:** 9/10
**References:** 40 citations