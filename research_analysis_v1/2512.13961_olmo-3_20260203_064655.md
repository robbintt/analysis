---
title: Olmo 3
arxiv_id: '2512.13961'
source_url: https://arxiv.org/abs/2512.13961
generated_at: '2026-02-03T06:46:55'
quality_score: 7
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Olmo 3

*Team Olmo; Allyson Ettinger; Amanda Bertsch; Bailey Kuehl; David Graham; David Heineman; Dirk Groeneveld; Faeze Brahman; Finbarr Timbers; Hamish Ivison; Jacob Morrison; Jake Poznanski; Kyle Lo; Luca Soldaini; Matt Jordan; Mayee Chen; Michael Noukhovitch; Nathan Lambert; Pete Walsh; Pradeep Dasigi; Robert Berry; Saumya Malik; Saurabh Shah; Scott Geng; Shane Arora; Shashank Gupta; Taira Anderson; Teng Xiao; Tyler Murray; Tyler Romero; Victoria Graf; Akari Asai; Akshita Bhagia; Alexander Wettig; Alisa Liu; Aman Rangapur; Chloe Anastasiades; Costa Huang; Dustin Schwenk; Harsh Trivedi; Ian Magnusson; Jaron Lochner; Jiacheng Liu; Lester James V. Miranda; Maarten Sap; Malia Morgan; Michael Schmitz; Michal Guerquin; Michael Wilson; Regan Huff; Ronan Le Bras; Rui Xin; Rulin Shao; Sam Skjonsberg; Shannon Zejiang Shen; Shuyue Stella Li; Tucker Wilde; Valentina Pyatkin; Will Merrill; Yapei Chang; Yuling Gu; Zhiyuan Zeng; Ashish Sabharwal; Luke Zettlemoyer; Pang Wei Koh; Ali Farhadi; Noah A. Smith; Hannaneh Hajishirzi*

---

### Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Model Scales** | 7B, 32B Parameters |
| **Variants** | Base, Think, Instruct, RL-Zero |
| **Key Innovation** | Model Flow (Radical Transparency) |
| **Specializations** | Long-context reasoning, Function calling, Coding |
| **Quality Score** | 7/10 |
| **References** | 40 Citations |

---

## Executive Summary

> **Problem**  
> The research addresses the critical lack of transparency and reproducibility in current state-of-the-art (SOTA) large language models (LLMs). As proprietary models advance in complex reasoning and long-context handling, the opacity regarding training data and infrastructure hinders the scientific community's ability to verify safety alignments and reproduce results.
>
> **Innovation**  
> The primary innovation is the **"Model Flow" framework**, a lifecycle management strategy defined by radical transparency that releases the complete model construction process, including raw data, intermediate checkpoints, and full infrastructure dependencies. The methodology employs a targeted capability design process spanning Pretraining, Midtraining, and Long-context Extension.
>
> **Results**  
> The Olmo 3 family achieves **SOTA performance among open models** at both 7B and 32B parameter scales. The Olmo 3 Think 32B model significantly narrows the performance gap with the proprietary Qwen 3 32B Think model, achieving competitive results with **six times fewer training tokens**.
>
> **Impact**  
> This work establishes a new standard for "fully open" models by moving beyond weight release to include full infrastructure and data transparency. By democratizing access to high-performance reasoning models and complete lifecycle documentation, the authors enable the research community to audit, customize, and trust AI systems.

---

## Key Findings

*   **State-of-the-Art Open Models:** Introduction of Olmo 3, achieving SOTA performance at both 7B and 32B scales relative to other open models.
*   **Leadership in Open Thinking:** Launch of Olmo 3 Think 32B, establishing leadership in the domain of Open Thinking Models.
*   **Targeted Capability Design:** Models optimized for complex tasks, specifically long-context reasoning, function calling, and coding.
*   **Complete Lifecycle Transparency:** Documentation of every stage, checkpoint, and dependency, setting a new standard for reproducibility.

---

## Methodology

The methodology centers on a **'model construction' process** targeted at specific high-value functionalities. It emphasizes a comprehensive lifecycle management strategy, building models optimized for diverse capabilities such as long-context reasoning and function calling, while ensuring that every dependency and data point used in the construction is preserved and released.

The approach includes:
*   **Targeted Design:** Focusing on distinct functionalities (e.g., reasoning vs. instruction following).
*   **Lifecycle Management:** A "full model flow" that tracks the model from initialization to release.
*   **Data Preservation:** Ensuring full traceability of training data and intermediate states.

---

## Technical Details

### Architecture & Variants
The Olmo 3 family includes models at two primary scales:
*   **7B Parameters**
*   **32B Parameters**

**Model Variants:**
*   **Base:** The foundational model.
*   **Think:** Optimized for step-by-step reasoning.
*   **Instruct:** Optimized for following user instructions.
*   **RL-Zero:** A variant focused on Reinforcement Learning.

### Training Pipeline
The base training pipeline consists of three distinct stages:
1.  **Pretraining:** Initial training on broad data.
2.  **Midtraining:** Refinement for specific capabilities.
3.  **Long-context Extension:** Specialized training to handle extended context windows.

### Post-Training Methodologies
Post-training utilizes specific pipelines involving:
*   **SFT (Dolci):** Supervised Fine-Tuning using the Dolci dataset/pipeline.
*   **Preference Tuning:** Utilizing Delta Learning and DPO (Direct Preference Optimization).
*   **Reinforcement Learning:** Implementation of OlmoRL and Dolci.

---

## Results & Contributions

### Performance
*   **SOTA Rankings:** Olmo 3 claims State-of-the-Art performance among open models at 7B and 32B scales.
*   **Efficiency:** Olmo 3 Think 32B is described as the strongest fully-open thinking model, narrowing the gap with Qwen 3 32B thinking while being trained on **six times fewer tokens**.
*   **Capabilities:** The models exhibit proficiency in:
    *   Long-context reasoning
    *   Function calling
    *   Coding
    *   Instruction following
*   **Traceability:** The release enables the tracing of reasoning chains back to original training data.

### Contributions
*   **Fully-Open Ecosystem:** Providing SOTA 7B and 32B model weights and infrastructure.
*   **Radical Transparency:** Introduction of 'Model Flow', releasing the complete lifecycle including checkpoints and data.
*   **Architectural Advancement:** Progress in Open 'Thinking' Architectures with the development of the Olmo 3 Think 32B model.

---
*Report generated based on analysis of 40 references.*