# Reward Models in Deep Reinforcement Learning: A Survey

*Rui Yu; Shenghua Wan; Yucen Wang; Chen-Xiao Gao; Le Gan; Zongzhang Zhang; De-Chuan Zhan*

---

> ### üìä Quick Facts
> *   **Document Type:** Survey Paper
> *   **Field:** Deep Reinforcement Learning (Deep RL)
> *   **Core Contribution:** 3-Dimensional Taxonomic Framework
> *   **Citations:** 16
> *   **Quality Score:** 7/10

---

## üìù Executive Summary

Reward models act as essential proxies for designer intentions in Deep Reinforcement Learning (Deep RL), yet the field suffers from significant fragmentation. Methodologies vary widely from hand-coding functions to complex integration with foundation models, creating ambiguity that hinders effective comparison and verification of alignment strategies. Without a unified framework, ensuring that agents remain aligned with true objectives becomes increasingly complex as tasks scale from simple control to high-level reasoning.

The core contribution of this work is a novel, three-dimensional taxonomic framework grounded in the Markov Decision Process (MDP) formulation $(S, A, T, R, \gamma)$. This framework systematically categorizes techniques by **Source** (Human-Provided vs. AI-Generated), **Mechanism**, and **Learning Paradigm**. The authors employ a systematic review process to rigorously define ambiguous terms; for instance, Human-in-the-Loop methods are classified by specific inputs such as demonstrations or preferences, clearly distinguishing them from static manual engineering.

As a focused review of 16 key references, the paper organizes the landscape into three primary mechanisms: **Manual Reward Engineering**, **Human-in-the-Loop**, and **Foundation Model-Based** approaches. While the paper does not provide quantitative experimental results, it validates the framework through qualitative case studies of seminal systems, including AlphaGo‚Äôs hybrid rewards and the deployment of Foundation Model-Based rewards in large language models like InstructGPT, OpenAI-o1, and DeepSeek-R1. The survey also highlights specific failure modes, citing the "Walker" task where agents exploit poorly specified manual rewards (Reward Hacking) to maximize scores without fulfilling the intended locomotion objective.

By establishing standardized terminology, this work offers a reference point for researchers navigating the shift from manual engineering to AI-generated rewards. The authors identify critical gaps in the field, particularly the lack of robust evaluation metrics and the persistent risks of reward hacking in complex environments. This analysis helps define the requirements for scalable alignment, providing a structured perspective on the limitations of current methods and highlighting the need for more secure reward model designs in future research.

---

## üîç Key Findings

*   **Role of Reward Models:** They serve as essential proxies for designer intentions within the Reinforcement Learning domain.
*   **Classification System:** Recent reward modeling techniques can be systematically classified based on three dimensions:
    *   Source of the reward.
    *   Mechanism used.
    *   Learning paradigm.
*   **Industry Focus:** The field is currently prioritizing the development of models that align closely with true objectives and facilitate policy optimization.
*   **Literature Gap:** Prior to this survey, there was a notable absence of a systematic review regarding reward models.

---

## üß™ Methodology

The authors employed a **systematic literature review** methodology specifically within the domain of Deep Reinforcement Learning. The comprehensive approach involved:

1.  Establishing a theoretical background.
2.  Conducting a taxonomic classification of recent techniques.
3.  Synthesizing current applications and evaluation methods.

---

## üèÜ Contributions

This paper provides several pivotal contributions to the field of Deep RL:

*   **First Systematic Review:** Provided the first systematic review dedicated solely to reward modeling techniques in Deep RL.
*   **Novel Taxonomy:** Introduced a specific taxonomy that categorizes approaches by source, mechanism, and learning paradigm.
*   **Evaluation Synthesis:** Reviewed and discussed current methods for evaluating reward models.
*   **Future Roadmap:** Identified and highlighted promising research directions for future inquiry.

---

## ‚öôÔ∏è Technical Details

### 3-Dimensional Taxonomic Framework
The authors propose a framework to categorize techniques based on:
*   **Source:** Human-Provided vs. AI-Generated.
*   **Mechanism:** The specific technical implementation.
*   **Learning Paradigm:** How the model learns from data.

### Mathematical Formulation
The study utilizes the standard Markov Decision Process (MDP) formulation, defined by:
*   **$S$**: State Space
*   **$A$**: Action Space
*   **$T$**: Transition Function
*   **$R$**: Reward Model
*   **$\gamma$**: Discount Factor

*Objective:* Maximize the expected discounted cumulative return.

### Specific Approaches Categorized
*   **Manual Reward Engineering:** Hand-coded functions designed by humans.
*   **Human-in-the-Loop:** Utilizes inputs such as demonstrations, goals, or preferences from human operators.
*   **Foundation Model-Based:**
    *   Zero-shot specification.
    *   Fine-tuning methods.

---

## üìà Results

### Qualitative Validation
As a survey paper, this work does not provide specific experimental results or quantitative benchmarks. Instead, it validates concepts through qualitative examples of systems utilizing reward models:

*   **AlphaGo:** Used for complex decision-making with hybrid rewards.
*   **InstructGPT:** Utilized for alignment tasks.
*   **OpenAI-o1 & DeepSeek-R1:** Highlighted for deploying Foundation Model-Based rewards in reasoning capabilities.

### Identified Risks
The text identifies qualitative risks associated with poor reward design:
*   **Reward Hacking:** Where agents exploit poorly crafted manual reward functions to "game" the system rather than completing the objective.
*   **Example:** In the "Walker" task, agents found ways to maximize scores without achieving the intended locomotion behavior.

---

**Quality Score:** 7/10
**References:** 16 citations