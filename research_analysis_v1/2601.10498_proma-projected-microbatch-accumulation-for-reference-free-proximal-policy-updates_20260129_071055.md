# PROMA: Projected Microbatch Accumulation for Reference-Free Proximal Policy Updates
*Nilin Abrahamsen*

---

> ### üìä Quick Facts
>
> *   **Algorithm**: PROMA (Projected Microbatch Accumulation)
> *   **Core Innovation**: Reference-free proximal policy updates
> *   **Test Model**: Qwen-3 0.6B
> *   **Dataset**: GSM8K
> *   **Framework**: VeRL
> *   **Performance**: Comparable or superior to GRPO; Lower Local KL
> *   **Quality Score**: 7/10
> *   **Citations**: 13

---

## üìù Executive Summary

State-of-the-art proximal policy optimization methods, such as PPO and Group Relative Policy Optimization (GRPO), rely heavily on likelihood ratios relative to a reference policy or explicit KL penalties to constrain updates. This dependency introduces substantial computational complexity and memory overhead, complicating the effective control of policy divergence and often resulting in entropy collapse‚Äîa phenomenon where the model ceases to explore‚Äîand unstable training dynamics. The paper addresses the core necessity for a proximal policy update mechanism that preserves stability and encourages exploration without the architectural and computational burdens of tracking a reference policy or calculating complex ratio-based constraints.

The authors introduce **Projected Microbatch Accumulation (PROMA)**, a novel algorithm that achieves reference-free proximal policy updates by enforcing orthogonality between accumulated gradients and current microbatch gradients. PROMA modifies the gradient accumulation process layer-wise during the backward pass, projecting the partially accumulated gradient orthogonal to the span of sequence-wise log-probability gradients. This geometric constraint naturally minimizes KL divergence growth without requiring explicit penalties or clipping. To ensure practical efficiency, the method employs an approximate "iterative subtraction" technique for projection rather than computationally expensive exact QR decomposition, while norm clamping is utilized to limit the projection norm and regulate stability.

Experimental results on the GSM8K dataset using a Qwen-3 0.6B model within the VeRL framework validate PROMA's efficacy. The algorithm achieved validation accuracy comparable to or significantly better than the GRPO baseline. In terms of training stability, PROMA maintained higher policy entropy for longer durations than both GRPO and REINFORCE, effectively preventing the entropy collapse typical in standard RL training. Regarding divergence control, PROMA demonstrated consistently lower Local KL divergence (against a lagged policy) than both GRPO and REINFORCE, while also offering superior Global KL control compared to REINFORCE. These results confirm PROMA's ability to regulate policy updates more tightly, ensuring smoother optimization trajectories.

PROMA represents a significant advancement in reinforcement learning by proving that proximal policy updates can be effectively implemented without a reference policy. This elimination of the reference dependency simplifies the optimization pipeline and offers critical practical benefits, including reduced memory overhead by avoiding the need to store a separate policy model. By demonstrating tighter local KL control and improved stability over existing methods, PROMA provides a robust new tool for training large language models, potentially shifting future research toward geometrically constrained, reference-free optimization strategies that prioritize efficiency and stability.

---

## üîç Key Findings

*   **Reference-Free Proximal Updates**: PROMA successfully achieves proximal policy updates without relying on likelihood ratios relative to a reference policy.
*   **Entropy Stability**: The method effectively prevents entropy collapse during the training process, ensuring sustained exploration.
*   **Superior KL Control**: Empirical results indicate that PROMA provides tighter local KL (Kullback-Leibler) control compared to the Group Relative Policy Optimization (GRPO) method.
*   **Computational Efficiency**: The approach enables an efficient implementation strategy by integrating the projection mechanism directly into the backward pass.

---

## ‚öôÔ∏è Methodology

PROMA implements a proximal policy method by modifying the gradient accumulation process across microbatches. Instead of using traditional likelihood ratios against a reference policy, it performs an **orthogonal projection** of the partially accumulated gradient against the sequence-wise gradients of the current microbatch. This projection technique is executed **layer-wise during the backward pass** to ensure computational efficiency.

### 1. Gradient Accumulation
The algorithm accumulates gradients over microbatches but alters the standard accumulation step.

### 2. Orthogonal Projection
Before adding the current microbatch gradient to the accumulated gradient, PROMA projects the accumulated gradient to make it orthogonal to the span of the current microbatch's sequence-wise log-probability gradients.

### 3. Layer-wise Execution
The calculation is performed layer-by-layer during the backward pass to integrate seamlessly into existing autod frameworks.

---

## üî¨ Technical Details

PROMA (Projected Microbatch Accumulation) performs proximal policy updates without a reference policy by constraining update magnitudes during gradient accumulation.

### Core Mechanism
*   It projects the running gradient orthogonal to the span of sequence-wise log-probability gradients before accumulation.
*   The implementation involves:
    1.  Computing the vanilla policy gradient.
    2.  Calculating the complement of the accumulated gradient relative to the log-prob gradients.
    3.  Updating the gradient based on this calculation.

### Projection Methods
Two projection methods are available:
*   **Exact**: Uses QR decomposition (computationally expensive).
*   **Approximate**: Uses iterative subtraction (used in experiments for efficiency).

### Stability Regulation
*   **Norm Clamping**: Regulates stability by limiting the projection norm, preventing excessively large updates.

### Geometric Interpretation
Geometrically, the method projects updates away from log-prob directions. This minimizes KL divergence growth without the need for explicit penalties or clipping.

---

## üèÜ Results

Experiments were conducted on the **GSM8K** dataset using a **Qwen-3 0.6B** model within the **VeRL** framework, comparing PROMA against GRPO and REINFORCE baselines.

*   **Validation Accuracy**: PROMA achieved comparable or significantly improved validation accuracy relative to GRPO.
*   **Entropy & Exploration**: It maintained higher policy entropy for longer durations than both baselines, indicating sustained exploration and resistance to entropy collapse.
*   **Global KL Control**: While allowing deviation from the initial policy, PROMA showed better Global KL control than REINFORCE.
*   **Local KL Divergence**: Notably, it demonstrated consistently lower Local KL divergence (vs. a lagged policy) than both GRPO and REINFORCE, suggesting smoother local updates.

---

## üöÄ Contributions

*   **Novel Algorithm**: Introduction of Projected Microbatch Accumulation (PROMA), a distinct approach to proximal policy updates that removes the dependency on reference policies.
*   **Gradient Mechanism Innovation**: A unique gradient accumulation strategy that enforces orthogonality between accumulated states and current microbatch gradients to maintain policy proximity.
*   **Performance Benchmarking**: Demonstrated empirical improvement in policy optimization metrics, specifically regarding local control and stability, when benchmarked against GRPO.

---

**Quality Score**: 7/10  
**References**: 13 citations