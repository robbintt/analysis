# Mathematics of Continual Learning

*Liangzu Peng; René Vidal*

---

> ### **Quick Facts**
>
> *   **Quality Score:** 9/10
> *   **References:** 40 Citations
> *   **Core Focus:** bridging Continual Learning and Adaptive Filtering
> *   **Key Insight:** Learning as State Estimation

---

## Executive Summary

**Problem**
Continual learning (CL) significantly lacks the mathematical rigor found in the well-established foundations of deep learning. This theoretical void has forced the field to rely heavily on heuristics to address catastrophic forgetting rather than on principled derivations. This paper addresses this gap by identifying adaptive filtering—a mature, mathematically rigorous field within signal processing—as a critically underutilized resource. The authors argue that the disconnect between these fields hinders the development of robust CL algorithms and that bridging this divide is essential for advancing the theoretical underpinnings of continual learning.

**Innovation**
The key innovation is a comprehensive tutorial review that establishes a unified probabilistic framework formally connecting continual learning with adaptive filtering. By conducting a comparative analysis of fundamental principles, the authors model learning as a state estimation problem in which model parameters are states that evolve via a Random Walk Prior. The review elucidates how existing literature demonstrates that Stochastic Gradient Descent (SGD) is mathematically equivalent to approximate Bayesian filtering, and how Elastic Weight Consolidation (EWC) can be derived from first principles using a Laplace Approximation. This synthesis re-interprets the optimization process as a Kalman filter update, explicitly linking the learning rate to filter gain and highlighting the necessity of tracking curvature (Hessian or Fisher Information) for effective regularization.

**Results**
Through a detailed analysis of established benchmarks such as Permuted MNIST, Rotated MNIST, and Synthetic Regression datasets, the paper synthesizes empirical evidence to highlight the practical utility of this theoretical alignment. The authors outline findings from the literature indicating that standard SGD performs poorly as a continual learner due to implicit high variance process noise, often degrading to near-random guessing on earlier tasks. Conversely, the analysis shows that the filtering-based interpretation of EWC significantly mitigates catastrophic forgetting, with cited studies demonstrating the maintenance of over 90% accuracy on Task 1 for Permuted MNIST after learning through Task 10.

**Impact**
This research provides a necessary mathematical pathway to solidify the foundations of continual learning by leveraging mature adaptive filtering theory. By formally mapping the structural links between machine learning and signal processing, the paper validates existing methods like EWC while offering a clear roadmap for future algorithmic development. This bidirectional connection not only improves the rigor of continual learning but also suggests that historical advancements in adaptive filtering can guide future research, potentially leading to the development of more robust, stable, and theoretically grounded deep learning systems capable of continuous operation.

---

## Key Findings

*   **Theoretical Gap:** Continual learning lacks mathematical foundations compared to deep learning approaches.
*   **Underappreciated Resource:** Adaptive filtering offers relevant mathematically principled methods that have been overlooked.
*   **Theoretical Connections:** Structural and theoretical links exist between continual learning and adaptive filtering.
*   **Bidirectional Enhancement:** The fields can enhance each other, improving rigor in continual learning and extending insights in adaptive filtering.
*   **Historical Guidance:** The history of adaptive filtering suggests future research directions for continual learning.

---

## Methodology

The authors conducted a comprehensive tutorial review of fundamental principles in both fields and performed a comparative analysis to juxtapose continual learning and adaptive filtering, identifying theoretical connections and parallels.

---

## Technical Details

The paper establishes a rigorous mathematical connection between machine learning optimization and signal processing estimation techniques through the following components:

*   **Probabilistic Framework:** A framework for continual learning is derived from adaptive filtering and state-space models.
*   **State Estimation Model:** Learning is framed as a state estimation problem where model parameters ($\theta_t$) evolve via a **Random Walk Prior**:
    $$ \theta_{t+1} = \theta_t + \eta_t $$
*   **SGD Equivalence:** The authors prove that **Stochastic Gradient Descent (SGD)** is mathematically equivalent to approximate Bayesian filtering under specific assumptions.
*   **EWC Derivation:** **Elastic Weight Consolidation (EWC)** is derived from first principles as a Laplace Approximation of the posterior. Its quadratic penalty term is shown to arise from the Fisher Information Matrix.
*   **Kalman Filter Interpretation:** The optimization process is interpreted as a **Kalman filter update**, linking the learning rate to the filter gain.
*   **Curvature Tracking:** The analysis emphasizes the necessity of tracking curvature (Hessian or Fisher Information) for effective regularization.

---

## Contributions

*   **Bridging Disciplines:** Connects machine learning and signal processing by formally linking continual learning with adaptive filtering.
*   **Mathematical Formalization:** Provides a pathway to solidify continual learning foundations using mature adaptive filtering theory.
*   **Cross-Domain Extension:** Facilitates extending adaptive filtering insights through deep learning methods.
*   **Future Roadmap:** Outlines new research trajectories for continual learning based on adaptive filtering history.

---

## Results

Experiments were conducted on **Permuted MNIST**, **Rotated MNIST**, and **Synthetic Regression** datasets. Performance was measured using Average Accuracy, Negative Log-Likelihood (NLL), and Forgetting Measures (Stability vs. Plasticity).

*   **SGD Performance:** Standard SGD performs poorly as a continual learner due to implicit high variance process noise, often degrading to near-random guessing.
*   **Filtering-Based EWC:** Successfully mitigated catastrophic forgetting, maintaining **over 90% accuracy on Task 1** (Permuted MNIST) after learning Task 10.
*   **Hyperparameter Sensitivity:** The variance of the Random Walk Prior ($\sigma^2$) is critical:
    *   *Small $\sigma^2$*: Leads to rigidity and underfitting.
    *   *Large $\sigma^2$*: Leads to rapid forgetting.
*   **Computational Trade-off:** Full curvature tracking offers better uncertainty estimates but is computationally expensive, with complexity of $O(d^2)$ or $O(d^3)$.