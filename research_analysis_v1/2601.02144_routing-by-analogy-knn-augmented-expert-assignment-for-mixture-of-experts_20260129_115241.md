# Routing by Analogy: kNN-Augmented Expert Assignment for Mixture-of-Experts

*Boxuan Lyu; Soichiro Murakami; Hidetaka Kamigaito; Peinan Zhang*

---

### **Quick Facts**

| Metric | Details |
| :--- | :--- |
| **Models Evaluated** | OLMoE, GPT-OSS, Qwen3 |
| **Benchmarks** | GPQA, SuperGPQA, MMLU |
| **Key Improvement (GPQA)** | +3.5% Absolute Accuracy (28.9% → 32.4%) |
| **Inference Overhead** | Negligible |
| **Training Mode** | Offline Memory Construction + Frozen Inference |

---

## **Executive Summary**

**Problem**
Mixture-of-Experts (MoE) architectures suffer from routing brittleness when deployed in dynamic environments. Parametric routers, once trained and frozen, fail to generalize effectively to data distributions that differ from the training set, often defaulting to sub-optimal expert assignments. While Supervised Fine-Tuning (SFT) can remedy this by updating router weights, it is computationally prohibitive for large-scale deployment.

**Innovation**
The authors introduce **kNN-MoE**, a "Routing by Analogy" framework that decouples routing logic from model weights by augmenting standard MoE layers with a non-parametric, retrieval-based memory store. The method operates in two distinct phases:
1.  **Offline Memory Construction:** Creates a memory bank by optimizing token-wise routing logits on a reference set to determine "optimal" expert assignments without updating model weights.
2.  **Frozen Inference:** Retrieves assignments based on nearest neighbor similarity of hidden states. A confidence-driven mechanism dynamically interpolates between the retrieved "analogical" routing and the frozen parametric router as a fallback.

**Results**
Evaluations on open-source architectures demonstrate that kNN-MoE significantly outperforms standard zero-shot baselines.
*   **GPQA (OLMoE-1B):** Improved accuracy from **28.9% to 32.4%**.
*   **MMLU (GPT-OSS-0.7B):** Improved performance from **26.2% to 27.9%**.

These improvements rival full Supervised Fine-Tuning (SFT) but with exponentially higher efficiency. Unlike concurrent methods (e.g., C3PO), kNN-MoE maintains near-identical inference latency to the frozen baseline.

**Impact**
This research establishes a new paradigm for retrieval-augmented model adaptation, validating the efficacy of non-parametric memory for enhancing routing logic in static networks. By bridging the gap between frozen rigidity and the high cost of retraining, this work provides a scalable path toward resilient expert systems capable of handling evolving data distributions.

---

## **Key Findings**

*   **Superior to Zero-Shot:** kNN-MoE outperforms standard zero-shot baselines in Mixture-of-Experts routing, validating the efficacy of retrieval-augmented strategies.
*   **Rivals SFT Efficiency:** The approach rivals the performance of computationally expensive Supervised Fine-Tuning (SFT) while avoiding the high computational costs associated with fine-tuning.
*   **Addresses Router Brittleness:** Standard parametric routers in MoE architectures exhibit brittleness under distribution shifts when trained once and frozen; this method mitigates that issue.
*   **Graceful Degradation:** The framework’s confidence-driven mechanism effectively balances novel routing decisions with learned prior knowledge, allowing for graceful degradation when no relevant memory exists.

---

## **Methodology**

The authors propose **kNN-MoE**, a retrieval-augmented routing framework that operates by analogy. The methodology involves three main components:

1.  **Memory Construction (Offline)**
    An offline process that creates a memory bank by optimizing token-wise routing logits on a reference set. This step stores optimal expert assignments without altering the underlying model weights.

2.  **Retrieval**
    Instead of relying solely on a frozen parametric router, the system obtains expert assignments from the memory bank based on similar past cases (nearest neighbors).

3.  **Confidence-Driven Mixing**
    The framework uses the aggregate similarity of retrieved neighbors as a mixing coefficient. This dynamically weight retrieved assignments against the frozen router's output, serving as a fallback when relevant analogies are not found.

---

## **Contributions**

*   **Non-Parametric Solution:** A novel solution to the rigidity of frozen routers in MoE architectures by introducing non-parametric, memory-based retrieval.
*   **Offline Optimization Strategy:** A specific method for offline optimization of routing logits to construct a high-quality memory of expert assignments.
*   **Robust Inference Mechanism:** The development of a robust inference mechanism that utilizes neighbor similarity as a confidence score to interpolate between retrieval-based routing and the default frozen router.

---

## **Technical Details**

**Architecture Overview**
The kNN-MoE architecture is a hybrid **Routing-by-Analogy** framework. It augments standard parametric Mixture-of-Experts (MoE) routers with a non-parametric, retrieval-based memory store to improve inference adaptability without weight updates.

**Memory Store Configuration**
*   **Structure:** Distinct memory store per MoE layer.
*   **Components:**
    *   **Keys:** Router input hidden states.
    *   **Values:** Optimal expert assignments.

**Implementation Mechanics**
*   **Memory Construction:** Involves an offline optimization process that calculates optimal assignments by minimizing **Negative Log-Likelihood (NLL)** via joint gradient descent across all layers.
*   **Inference:** The system retrieves nearest neighbors and employs a confidence-driven adaptive mixing strategy.
    *   *High Confidence:* Trusts the retrieved assignment.
    *   *Low Confidence:* Falls back to the original frozen parametric router.

---

## **Results**

**Performance Evaluation**
Evaluations on models such as **OLMoE**, **GPT-OSS**, and **Qwen3** across datasets like **GPQA**, **SuperGPQA**, and **MMLU** demonstrate that:
*   kNN-MoE consistently outperforms zero-shot baselines.
*   Performance rivals expensive Supervised Fine-Tuning (SFT).

**Efficiency Metrics**
*   **Vs. C3PO:** Offers significant efficiency advantages; C3PO incurs approximately **5 times higher** computational cost.
*   **Vs. Su et al.:** Unlike concurrent methods requiring online optimization, kNN-MoE requires **no parameter updates**.
*   **Inference Speed:** Achieves negligible inference overhead.

**Robustness**
The method demonstrates robustness to distribution shifts and degrades gracefully when memory is irrelevant.

---

**Quality Score:** 8/10
**References:** 7 citations