---
title: Identifying Sensitive Weights via Post-quantization Integral
arxiv_id: '2503.01901'
source_url: https://arxiv.org/abs/2503.01901
generated_at: '2026-02-03T19:38:03'
quality_score: 9
citation_count: 38
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Identifying Sensitive Weights via Post-quantization Integral

*Yuezhou Hu; Weiyu Huang; Zichen Liang; Chang Chen; Jintao Zhang; Jun Zhu; Jianfei Chen*

---

### üìä Quick Facts

| Metric | Details |
| :--- | :--- |
| **Target Model** | Llama 3.2 1B |
| **Performance Gain** | **2.66** Perplexity (PPL) Improvement |
| **Core Method** | Post-quantization Integral (PQI) |
| **Framework** | ReQuant (Dense-and-Sparse detach) |
| **Rating** | ‚≠ê 9/10 |
| **Citations** | 38 References |

---

## Executive Summary

Post-training quantization (PTQ) relies heavily on accurately identifying "sensitive" weights‚Äîparameters that, if quantized, will cause significant performance degradation. Standard practice utilizes gradient and Hessian-based metrics grounded in Taylor's formula to estimate this sensitivity. However, this research identifies a critical flaw in these widely adopted methods: the reliance on local 2nd-order approximations, which suffer from small convergence radii. Consequently, existing metrics drastically underestimate the true impact of quantization on the loss function, often by orders of magnitude, leading to suboptimal model compression and accuracy loss, particularly in modern Large Language Models (LLMs).

To address this mathematical limitation, the authors propose the **Post-quantization Integral (PQI)**, a novel metric that provides a robust, fine-grained estimation of posterior sensitivity. Rather than relying on a point-estimate local approximation, PQI calculates the integral of the loss function over the specific range of quantization error, capturing the global impact of weight perturbations. This metric is operationalized through the **ReQuant framework**, which implements a "Dense-and-Sparse detach" strategy. This strategy leverages PQI to guide self-adaptive outlier selection and performs step-wise detachment of significant weights, ensuring that quantization prioritizes stability in the most critical regions of the model.

The proposed ReQuant framework demonstrates significant improvements over baseline quantization methods. In experiments targeting the Llama 3.2 1B model, the method achieved a **2.66 perplexity (PPL) gain**, marking a substantial enhancement in model retention after compression. By accurately identifying sensitive weights via PQI, the framework outperforms existing gradient and Hessian-based approaches, securing State-of-the-Art (SOTA) performance in post-training quantization for this model class.

This work fundamentally shifts the understanding of sensitivity analysis in neural network quantization by empirically diagnosing the failure of local 2nd-order Taylor approximations. The introduction of integral-based estimation via PQI offers a theoretically sound and practically superior alternative to established metrics. For the field of efficient deep learning, this implies that accurate quantization of large-scale models requires moving beyond local derivatives. The success of ReQuant on Llama architectures suggests that this approach will be influential in developing future compression techniques capable of maintaining the high fidelity required by state-of-the-art LLMs.

---

## Key Findings

*   **Inaccuracy of Standard Metrics:** Existing gradient and Hessian-based sensitivity metrics are highly inaccurate, underestimating the impact of quantization on the loss function by orders of magnitude.
*   **Root Cause Identified:** The error stems from the small convergence radius of the local 2nd-order approximation found in Taylor's formula.
*   **Superior Estimation:** The proposed **Post-quantization Integral (PQI)** offers a significantly more accurate and fine-grained estimation of posterior sensitivity compared to traditional methods.
*   **Performance Achievement:** The **ReQuant** framework delivers State-of-the-Art (SOTA) performance, achieving a **2.66 perplexity gain** on the Llama 3.2 1B model.

---

## Methodology

The authors propose a comprehensive two-step solution to address the limitations of current quantization techniques:

1.  **Post-quantization Integral (PQI):**
    *   A novel metric designed to accurately estimate the posterior sensitivity of weights.
    *   Moves beyond local approximations to provide a global view of weight impact.

2.  **ReQuant Framework:**
    *   Utilizes the PQI metric to guide the quantization process.
    *   **Dense-and-Sparse Detach Strategy:** A core mechanism consisting of:
        *   **Self-adaptive Outlier Selection:** Automatically identifying outliers.
        *   **Step-wise Significant Weights Detach:** Gradually detaching weights that significantly impact model performance.

---

## Contributions

*   **Empirical Diagnosis:** Provided a diagnosis identifying that widely used gradient and Hessian-based metrics fail to predict quantization impact due to the mathematical limitations of local 2nd-order approximations.
*   **New Metric (PQI):** Proposed the Post-quantization Integral (PQI) as a robust alternative for estimating weight influence.
*   **New Framework (ReQuant):** Introduced ReQuant, a framework that leverages PQI through adaptive outlier selection and significant weight detachment.
*   **SOTA Results:** Demonstrated state-of-the-art results that significantly boost the performance of existing post-training quantization techniques on modern Large Language Models.

---

## Technical Details

| Component | Description |
| :--- | :--- |
| **The Problem** | Gradient and Hessian-based metrics rely on local 2nd-order Taylor approximations which have small convergence radii, leading to inaccurate sensitivity estimation. |
| **The Solution** | **Post-quantization Integral (PQI)** calculates the integral of the loss function over the quantization error range. This provides a better estimate of posterior sensitivity by capturing the global impact rather than a local point estimate. |
| **Implementation** | PQI is implemented within the **ReQuant** framework. It guides the quantization process to ensure that weights critical to maintaining model fidelity are preserved or handled with care. |

---

## Results

*   **Target Model:** Llama 3.2 1B
*   **Outcome:** Achieved a **2.66 perplexity (PPL) gain** compared to baseline methods.
*   **Status:** The authors claim State-of-the-Art (SOTA) performance.

---

**Document Quality Score:** 9/10  
**References:** 38 Citations