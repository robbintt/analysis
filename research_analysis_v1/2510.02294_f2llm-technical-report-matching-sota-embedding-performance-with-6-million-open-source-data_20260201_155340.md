# F2LLM Technical Report: Matching SOTA Embedding Performance with 6 Million Open-Source Data

*Ziyin Zhang; Zihan Liao; Hang Yu; Peng Di; Rui Wang*

***

> ### ðŸ“Š Quick Facts
>
> *   **Model Variants:** 0.6B, 1.7B, and 4B parameters
> *   **Training Dataset:** 6 Million open-source, non-synthetic tuples
> *   **Architecture Backbone:** Qwen3 (No architectural modifications)
> *   **Key Achievement:** 4B model ranks 7th overall on MTEB English Leaderboard
> *   **Training Cost:** Budget-friendly single-stage fine-tuning

***

## ðŸ“„ Executive Summary

The field of dense retrieval has increasingly relied on resource-intensive contrastive pretraining and massive datasets of synthetic query-document pairs to achieve state-of-the-art (SOTA) performance. While effective, this dependence on costly synthetic data generation and complex training pipelines presents significant barriers to entry and challenges regarding data efficiency and reproducibility. This paper addresses the critical question of whether such expensive resources are strictly necessary, seeking to validate a more efficient path to developing high-quality embedding models without sacrificing accuracy.

The authors introduce **F2LLM** (Foundation to Feature Large Language Models), a suite of models (0.6B, 1.7B, and 4B parameters) that bypasses massive contrastive pretraining by directly fine-tuning existing **Qwen3** LLMs. Technically, the approach employs a single-stage fine-tuning process without architectural modifications, utilizing approximately 6 million open-source, non-synthetic tuples. The methodology unifies retrieval, classification, and clustering tasks into a (query, positive, hard negatives) format, employing a specific hard negative mining strategy that retrieves top candidates using a lightweight base model, filters for difficulty (<0.8 score and <95% of positive score), and trains using a combination of Hard Negative Loss and In-Batch Loss.

Evaluation on the MTEB English Leaderboard across 41 tasks demonstrates that this data-efficient approach rivals models trained on orders of magnitude more data. The F2LLM-4B model secured the 7th rank overall and 2nd among 4B parameter models, notably setting a new record in the clustering task with a score of 68.54. Furthermore, the F2LLM-1.7B model achieved the top ranking in the 1B-2B parameter category, and the F2LLM-0.6B ranked 2nd in the sub-1B category, proving that high proficiency can be maintained with significantly fewer training samples than the current industry standard.

The significance of this work lies in challenging the prevailing assumption that expensive synthetic data generation is a prerequisite for SOTA embedding performance. By releasing the models, the complete training dataset, and source code, the authors establish a transparent, reproducible, and budget-friendly baseline that democratizes access to high-performance embeddings. This research signals a potential shift in the field toward optimizing data quality and the fine-tuning of existing foundation models over reliance on massive synthetic data pipelines.

***

## ðŸ” Key Findings

*   **SOTA Performance:** F2LLM achieved state-of-the-art embedding performance across three model sizes (0.6B, 1.7B, and 4B).
*   **Leaderboard Rankings:**
    *   **F2LLM-4B:** Ranked 7th overall and 2nd among 4B parameter models on the MTEB English leaderboard.
    *   **F2LLM-1.7B:** Ranked 1st in the 1B-2B category.
*   **Striking Balance:** The suite strikes a strong balance between training cost, model size, and performance.
*   **Data Efficiency:** Results demonstrate that 6 million open-source, non-synthetic data tuples are sufficient to rival models trained on expensive synthetic datasets.

***

## ðŸ› ï¸ Methodology

The F2LLM methodology involves directly fine-tuning existing foundation Large Language Models (LLMs) instead of employing massive contrastive pretraining from scratch. The models are trained on a dataset of 6 million query-document-negative tuples sourced exclusively from open-source, non-synthetic datasets. This approach bypasses sophisticated, complex training pipelines and costly synthetic data generation processes.

***

## âš™ï¸ Technical Details

*   **Backbone Architecture:** Qwen3 (0.6B, 1.7B, 4B) with single-stage fine-tuning and no architectural modifications.
*   **Dataset Breakdown:**
    *   **Total Size:** ~6 Million tuples.
    *   **Retrieval:** 4.9M tuples.
    *   **Classification:** 0.2M tuples.
    *   **Clustering:** 0.8M tuples.
*   **Data Format:** Unified into (query, positive passage, hard negatives).
*   **Hard Negative Mining Strategy:**
    1.  Retrieve top 100 passages using Qwen3-Embedding-0.6B.
    2.  Exclude the top 5 passages.
    3.  Filter by score < 0.8 and < 95% of the positive score.
    4.  Select the top 24 candidates.
*   **Training Objectives:**
    *   **Hard Negative Loss:** Applied to all tasks.
    *   **In-Batch Loss:** Applied to retrieval tasks only.
    *   **Temperature:** 0.05 (Cosine Similarity).
*   **Hyperparameters:** Global Batch Size set to 512 across all variants.

***

## ðŸ“ˆ Results

Evaluated on the MTEB English Leaderboard across 41 tasks, F2LLM demonstrates high data efficiency, achieving performance comparable to models trained on hundreds of millions of samples using only 6 million tuples.

*   **F2LLM-4B:**
    *   Ranks **7th** overall.
    *   Ranks **2nd** among ~4B models.
    *   Sets a new clustering record with a score of **68.54**.
*   **F2LLM-1.7B:**
    *   Ranks **1st** in the 1B-2B parameter category.
*   **F2LLM-0.6B:**
    *   Ranks **2nd** in the <1B parameter category.

***

## âœ¨ Contributions

*   **F2LLM Suite:** Introduction of the "Foundation to Feature Large Language Models" (F2LLM) suite, providing high-performance embedding models in three distinct sizes (0.6B, 1.7B, and 4B).
*   **Open Assets:** Release of open research assets, including the models, the complete training dataset, and source code to facilitate transparency and reproducibility.
*   **New Baseline:** Establishment of a strong, reproducible, and budget-friendly baseline for future research, challenging the necessity of resource-intensive synthetic data generation.

***

**Quality Score:** 9/10  
**References:** 40 citations