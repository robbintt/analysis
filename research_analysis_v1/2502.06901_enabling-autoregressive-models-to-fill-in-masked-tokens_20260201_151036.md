# Enabling Autoregressive Models to Fill In Masked Tokens
*Daniel Israel; Aditya Grover; Guy Van den Broeck*

---

> ### üìä Quick Facts Sidebar
> | Metric | Detail |
> | :--- | :--- |
> | **Quality Score** | 8/10 |
> | **Total Citations** | 17 |
> | **Acronym** | MARIA (Masked and Autoregressive Infilling Architecture) |
> | **Architecture Type** | Hybrid (Parallel AR + MLM) |
> | **Time Complexity** | **O(N)** (Retains AR efficiency) |
> | **Key Datasets** | WikiText-103, Enwik8, Text8 |
> | **Comparison** | Outperforms Discrete Diffusion; Matches T5/BART BPB |

---

## üìë Executive Summary

This research addresses the fundamental architectural trade-off between **Autoregressive (AR)** and **Masked Language Modeling (MLM)** paradigms. 

*   **The Problem:** AR models are computationally efficient and scalable (utilizing causal attention and KV caching) but lack the native ability to perform masked infilling (predicting text based on future context). Conversely, MLM models offer the necessary bidirectional context for precise text manipulation but suffer from quadratic computational complexity and scalability bottlenecks.
*   **The Solution:** The authors introduce **MARIA**, a hybrid framework that synthesizes pre-trained AR and MLM models in parallel. By concatenating hidden states from both models and processing them through a trainable linear decoder, MARIA grafts the MLM‚Äôs bidirectional understanding onto the AR model.
*   **The Impact:** MARIA establishes state-of-the-art performance on masked infilling tasks, matching the accuracy of models like **T5** and **BART** on datasets such as **WikiText-103**. Crucially, it achieves this while maintaining the linear **O(N)** time complexity of AR decoding, avoiding the quadratic **O(N¬≤)** cost of standard MLMs. This work paves the way for versatile language models that support both high-speed streaming and precise, context-aware text editing.

---

## üîë Key Findings

*   **State-of-the-Art Performance:** MARIA achieves SOTA results on masked infilling tasks, significantly outperforming existing baselines and discrete diffusion models in accuracy.
*   **Hybrid Capability:** It enables Autoregressive (AR) models to perform masked infilling‚Äîa task they are inherently incapable of‚Äîwithout architectural retraining from scratch.
*   **Computational Efficiency:** The method retains the inference speed efficiency of AR models by preserving the benefits of **KV caching**, thereby avoiding the scalability bottlenecks typical of Masked Language Modeling (MLM).
*   **Minimal Modification:** The system proves that a "minimal modification"‚Äîthe addition of a linear decoder on concatenated states‚Äîis sufficient to grant AR models complex infilling capabilities.

---

## üõ† Methodology

The study proposes **MARIA (Masked and Autoregressive Infilling Architecture)**, a framework designed to leverage existing pre-trained models rather than training from scratch.

1.  **Parallel Processing:** The architecture utilizes a pre-trained AR model and a pre-trained MLM model in parallel.
2.  **State Concatenation:** It takes the concatenated hidden states from both the AR and MLM models.
3.  **Linear Decoder:** The concatenated states are fed into a trainable linear decoder.
4.  **Representation Fusion:** This decoder is trained to fuse the representations, allowing the system to predict masked tokens situated between past and future context while strictly maintaining the structural integrity of the AR component.

---

## üìù Contributions

*   **Paradigm Integration:** Successfully bridges the gap between AR and MLM objectives, allowing the strengths of both (AR's efficiency/scalability and MLM's bidirectional context understanding) to coexist in a single system.
*   **Resilience to Trade-offs:** Solves the critical dilemma of choosing between the masked infilling capability of MLMs and the inference efficiency of AR models.
*   **Architectural Efficiency:** Demonstrates that a lightweight addition is sufficient to grant AR models complex editing capabilities, validating that advanced features do not require architectural overhauls.

---

## ‚öô Technical Details

| Aspect | Description |
| :--- | :--- |
| **Core Objective** | Enable AR models to predict specific masked segments based on future context without regenerating the entire sequence. |
| **Inference Preservation** | Preserves Key-Value (KV) caching for inference speed; avoids the bidirectional attention requirements of standard MLMs. |
| **Decoder Mechanism** | Utilizes a linear layer to fuse parallel hidden states from AR and MLM backbones. |
| **Comparison Performance** | Achieves **Bits-per-Byte (BPB)** scores comparable to established baselines (**T5**, **BART**). |
| **Complexity Contrast** | Maintains **O(N)** complexity vs. the **O(N¬≤)** complexity of standard MLMs. |

---

## üìà Results

*   **Performance:** MARIA achieves State-of-the-Art (SOTA) performance on masked infilling tasks.
*   **Baselines:** It shows superior accuracy compared to discrete diffusion models and matches the performance of established MLMs.
*   **Efficiency:** The approach retains the inference speed efficiency of AR models (no degradation in speed relative to standard AR generation).