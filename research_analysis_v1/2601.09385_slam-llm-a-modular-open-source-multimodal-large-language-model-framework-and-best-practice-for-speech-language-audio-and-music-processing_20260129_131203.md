# SLAM-LLM: A Modular, Open-Source Multimodal Large Language Model Framework and Best Practice for Speech, Language, Audio and Music Processing

*Ziyang Ma; Guanrou Yang; Wenxi Chen; Zhifu Gao; Yexing Du; Xiquan Li; Zhisheng Zheng; Haina Zhu; Jianheng Zhuo; Zheshu Song; Ruiyang Xu; Tiranrui Wang; Yifan Yang; Yanqiao Zhu; Zhikang Niu; Liumeng Xue; Yinghao Ma; Ruibin Yuan; Shiliang Zhang; Kai Yu; Eng Siong Chng; Xie Chen*

---

### üìä Quick Facts & Metrics

| Metric | Value | Context |
| :--- | :--- | :--- |
| **Contextual ASR WER** | **1.33%** (test-clean)<br>**2.99%** (test-other) | Libripeech (N=1000). Outperforms DB-NNLM. |
| **Visual Speech WER** | **28.3%** | LRS3 Benchmark (vs 28.6% AV-HuBERT baseline). |
| **Audio Captioning (SPIDEr)** | **36.8** | AudioCaps Dataset. |
| **Music Accuracy** | **92.9%** | GTZAN Benchmark. |
| **Trainable Parameters** | **49M** | Total 33.6M LoRA parameters + others. |
| **Efficiency** | **High** | Lightweight linear projector; LoRA adapters. |

---

## üìù Executive Summary

Current open-source Multimodal Large Language Model (MLLM) frameworks are predominantly vision-centric, lacking the depth and specialized support required for speech, audio, and music processing. This creates a significant barrier for researchers, who must navigate high engineering overhead to build and validate pipelines from scratch for these auditory modalities. Consequently, the field suffers from a lack of standardized, high-performance baselines across Speech, Language, Audio, and Music (SLAM) tasks, slowing innovation and making it difficult to effectively reproduce or compare results.

**SLAM-LLM** introduces a modular, open-source framework designed specifically to unify SLAM processing through a flexible configuration philosophy. The architecture allows for the seamless integration of various encoders, projectors, LLMs, and Parameter-Efficient Fine-Tuning (PEFT) plugins. The reference implementation combines an **AV-HuBERT Large encoder** (477.3M params) with a **Vicuna v1.5-7B decoder**, connected via a lightweight linear projector. The design employs **LoRA adapters** (Rank 32, Alpha 32, Dropout 0.05) for efficient fine-tuning on attention layers, limiting trainable parameters to 49 million.

The framework achieves **state-of-the-art or near-state-of-the-art performance** across a comprehensive suite of benchmarks with concrete quantitative results. On the Librispeech Contextual ASR benchmark (N=1000), the model achieved a Word Error Rate (WER) of 1.33% (test-clean) and 2.99% (test-other), outperforming the previous best model (DB-NNLM). For Visual Speech Recognition on the LRS3 benchmark, SLAM-LLM achieved a WER of 28.3% using only 49M trainable parameters, effectively matching the AV-HuBERT baseline (28.6% WER) which required 325M trainable parameters.

By releasing a comprehensive framework equipped with high-quality model checkpoints and standardized pipelines, SLAM-LLM significantly lowers the barriers to entry for auditory MLLM research. The modular design and pre-validated "best practice" recipes reduce the engineering overhead typically associated with training and inference, allowing researchers to focus on algorithmic innovation rather than infrastructure.

---

## üîë Key Findings

*   **Gap Analysis:** Identified that current open-source MLLM frameworks are vision-centric and lack in-depth support for speech, audio, and music.
*   **Performance:** Achieved state-of-the-art or near-state-of-the-art performance on critical tasks including ASR (Automatic Speech Recognition), AAC (Audio Captioning), and MC (Music Classification).
*   **Efficiency:** Successfully reduces engineering overhead through a modular design and pre-validated recipes, streamlining the research process.
*   **Validation:** All incorporated techniques within the framework are rigorously validated by academic papers.

---

## ‚öôÔ∏è Methodology

The researchers developed **SLAM-LLM**, an open-source deep learning grounded in a modular design philosophy. The development approach focused on:

*   **Modular Configuration:** Enabling the flexible integration of various components, including encoders, projectors, LLMs, and PEFT plugins.
*   **Standardization:** Implementing standardized, high-performance recipes to ensure consistency.
*   **Scope:** Designing pipelines specifically for training and inference on audio, speech, and music tasks.

---

## üèóÔ∏è Technical Details

The framework architecture is designed for flexibility and efficiency. Key specifications include:

*   **Core Architecture:**
    *   **Encoder:** AV-HuBERT Large (477.3M parameters).
    *   **Decoder:** Vicuna v1.5-7B.
    *   **Connection:** Lightweight linear projector.
*   **Parameter-Efficient Fine-Tuning (PEFT):**
    *   **Technique:** LoRA adapters applied to attention layers.
    *   **Configuration:** Rank 32, Alpha 32, Dropout 0.05.
    *   **Impact:** Adds 33.6M parameters, resulting in a total of **49M trainable parameters**.
*   **Contextual ASR Implementation:**
    *   Utilizes an LLM-based biasing method.
    *   Incorporates artificial biasing lists directly into the generation process.

---

## üèÜ Performance Results

### Librispeech Contextual ASR Benchmark
*   **Test-Clean WER:** 1.33% (Outperforms previous best: DB-NNLM)
*   **Test-Other WER:** 2.99% (Outperforms previous best: DB-NNLM)
*   **Scaling Behavior:** Performance remained stable as list size increased:
    *   100 words: 1.27% WER
    *   2000 words: 1.38% WER

### LRS3 Visual Speech Recognition
*   **SLAM-LLM:** 28.3% WER (with 49M trainable parameters)
*   **Baseline (AV-HuBERT):** 28.6% WER (required 325M trainable parameters)

### Other Benchmarks
*   **Audio Captioning (AudioCaps):** SPIDEr score of **36.8**.
*   **Audio Captioning (Clotho):** Score of **25.6**.
*   **Music Classification (GTZAN):** Accuracy of **92.9%**.

---

## üìå Contributions

*   **Specialized Framework:** Introduced the first comprehensive, specialized open-source framework designed explicitly for Speech, Language, Audio, and Music processing.
*   **Resource Release:** Made available high-quality model checkpoints and comprehensive, end-to-end pipelines.
*   **Acceleration of Innovation:** Lowered barriers to entry and streamlined research processes, enabling faster advancement in the field.

---

**Quality Score:** 9/10  
**References:** 40 citations