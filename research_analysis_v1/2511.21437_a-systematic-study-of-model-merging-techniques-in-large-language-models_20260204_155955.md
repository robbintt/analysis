---
title: A Systematic Study of Model Merging Techniques in Large Language Models
arxiv_id: '2511.21437'
source_url: https://arxiv.org/abs/2511.21437
generated_at: '2026-02-04T15:59:55'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# A Systematic Study of Model Merging Techniques in Large Language Models

*OÄŸuz KaÄŸan Hitit; Leander Girrbach; Zeynep Akata*

---

> ### ðŸ“Š Quick Facts
>
> *   **Models Analyzed:** 4 Open-weight LLMs (Llama-3.2-3B, Llama-3.1-8B, Qwen3-4B, Qwen3-8B)
> *   **Methods Evaluated:** 6 State-of-the-art merging techniques
> *   **Dataset Scope:** 48 fine-tuned checkpoints (12 per base model)
> *   **Benchmarks:** 16 standard LLM evaluations (MMLU, ARC, TruthfulQA, etc.)
> *   **Quality Score:** 9/10
> *   **References:** 40 Citations

---

## Executive Summary

Model mergingâ€”the practice of combining multiple fine-tuned checkpoints from a common base model to create a multi-capable model without retrainingâ€”has emerged as a cost-effective alternative to training multi-task models. However, while promising results have been demonstrated on smaller architectures, significant uncertainty remains regarding whether these techniques transfer effectively to modern Large Language Models (LLMs). The core technical challenge is achieving "constructive interference" between task vectors, ensuring that the merged model retains or improves performance across diverse domains rather than suffering from conflicting updates.

This paper addresses a critical gap in the literature by providing the first large-scale, systematic evaluation of whether state-of-the-art merging paradigms are viable for contemporary open-weight LLMs in the 3B to 8B parameter range. The authors implemented a rigorous evaluation framework comparing six distinct merging methods across four modern open-weight base models. The methodology involved merging twelve fine-tuned checkpoints per base model and assessing the results across sixteen standard benchmarks. The study contrasted arithmetic-based approaches against subspace merging methods designed to address rank collapse.

The study yielded counter-intuitive results: **Task Arithmetic was the only approach that reliably yielded performance gains on LLMs.** While complex methods like TIES and Subspace Boosting typically resulted in significant performance drops with normalized scores generally ranging between 0.45 and 0.60, Task Arithmetic consistently achieved positive relative performance gains. This quantitative advantage highlights that the heuristics previously successful for smaller models do not transfer to the 3Bâ€“8B range. The data confirms that simple vector arithmetic outperforms sophisticated interference-aware and subspace techniques, which often introduce destructive interference in larger architectures.

---

## Key Findings

*   **Task Arithmetic Dominance:** Among six state-of-the-art merging methods, 'Task Arithmetic' is the only approach that reliably yields performance gains on LLMs.
*   **Failure of Advanced Methods:** Recent interference-aware and subspace merging methods (e.g., TIES, Subspace Boosting) typically result in significant performance drops.
*   **Scalability Issues:** Advantages previously reported for smaller models do not transfer to modern open-weight LLMs in the 3Bâ€“8B range.
*   **Future Requirements:** Current techniques highlight a pressing need for LLM-specific merging algorithms and merging-aware fine-tuning methods.

---

## Methodology

This research represents the first large-scale, systematic study of model merging techniques specifically applied to LLMs. The evaluation framework was designed to resolve uncertainty regarding efficacy by comparing modern complex methods against simple arithmetic merging.

*   **Scope:** Evaluation of six state-of-the-art merging methods applied to four open-weight LLMs.
*   **Data:** Twelve fine-tuned checkpoints per base model (48 total checkpoints), with subset sizes ranging from 2 to 12.
*   **Assessment:** Merged models were assessed across sixteen standard LLM benchmarks.
*   **Metrics:**
    *   Probability that a merged model outperforms the base model.
    *   Relative performance gains over the best individual checkpoint.

---

## Technical Details

### Objective & Constraints
*   **Objective:** Achieve constructive interference by merging fine-tuned checkpoints from a common base model without retraining.
*   **Constraint:** Focuses strictly on checkpoints from a common base model; excludes neuron alignment methods.

### Methods Analyzed
**1. Task Arithmetic (TA) Based**
Frames merging as linear vector arithmetic:
`$W_{merged} = W_0 + \sum \alpha_i \Delta W_i$`
*   **Task Arithmetic (TA):** The baseline arithmetic approach.
*   **TIES-Merging:** Interference-aware refinement of TA.
*   **Model Stock:** Utilizes geometric interpolation.

**2. Subspace Merging Methods**
Designed to address rank collapse via low-rank subspaces.
*   **TSV-Merge:** Task-Specific Vector merging.
*   **Iso-C:** Isotropic clustering methods.
*   **Subspace Boosting:** Uses Singular Value Decomposition (SVD).
*   **Mechanism:** General approach involves decomposing updates into common and task-specific subspaces.

### Infrastructure
*   **Implementation:** `mergekit`
*   **Evaluation:** `lm-evaluation-harness`

---

## Results

### Base Models Evaluated
*   Llama-3.2-3B
*   Llama-3.1-8B
*   Qwen3-4B
*   Qwen3-8B

### Benchmarks (16 Standard)
*   MMLU, MedMCQA, HellaSwag, Winogrande
*   ARC Challenge, ARC Easy
*   TruthfulQA, Leaderboard GPQA
*   BBH, MMLU-Pro

### Performance Analysis
*   **Task Arithmetic:** Consistently achieved positive relative performance gains over the best individual checkpoint.
*   **Advanced Methods (TIES, TSV-Merge, Subspace Boosting):** Typically resulted in significant performance drops.
*   **Normalized Scores:** Ranged approximately between 0.45â€“0.60 for complex methods, indicating a need for LLM-specific merging algorithms.

---

## Contributions

This research makes three primary contributions to the field of Natural Language Processing and Machine Learning:

1.  **Systematic Study:** Provided the first large-scale, systematic study of model merging techniques specifically applied to LLMs.
2.  **Resolution of Efficacy:** Resolved uncertainty regarding efficacy by demonstrating that modern complex methods underperform compared to simple arithmetic merging.
3.  **Motivation for Future Research:** Motivated new research by identifying limitations of current methods and establishing the necessity for developing LLM-specific merging architectures and fine-tuning protocols.

---

**Paper Quality Score:** 9/10 | **References:** 40 Citations