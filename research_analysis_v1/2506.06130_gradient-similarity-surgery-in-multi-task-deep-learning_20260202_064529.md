# Gradient Similarity Surgery in Multi-Task Deep Learning

*Thomas Borsani; Andrea Rosani; Giuseppe Nicosia; Giuseppe Di Fatta*

---

> ### **Quick Facts**
>
> *   **Methodology:** SAM-GS (Similarity-Aware Momentum Gradient Surgery)
> *   **Core Mechanism:** Gradient Magnitude Similarity & Momentum Modulation
> *   **Key Application:** Multi-Task Deep Learning (MTDL)
> *   **Evaluation Metrics:** mIoU (Segmentation), RMSE (Depth Estimation)
> *   **Quality Score:** 6/10
> *   **Citations:** 40

---

## Executive Summary

**Problem**
Multi-Task Deep Learning (MTDL) struggles with optimization difficulties arising from "conflicting gradients," a phenomenon where task-specific loss functions demand parameter updates that differ in direction or magnitude. This gradient interference often prevents the shared model from converging effectively, as the optimizer fails to satisfy multiple objectives simultaneously. Frequently, this results in a single task dominating the learning trajectory, degrading the performance of other tasks. Addressing this conflict is critical for training robust shared representations and ensuring reliable performance across all concurrent tasks in a multi-objective environment.

**Innovation**
The authors introduce Similarity-Aware Momentum Gradient Surgery (SAM-GS), a novel method that treats gradient aggregation as a similarity-aware process to mitigate interference. The key innovation lies in technically distinguishing the metric used for conflict detection from the mechanisms used for correction. SAM-GS employs a Gradient Magnitude Similarity metric to quantify relationships between task gradients. Based on this metric, the algorithm utilizes a dual mechanism: Gradient Equalisation, which normalizes conflicting gradients to prevent high-magnitude tasks from dominating, and Modulation of First-Order Momentum, which surgically adjusts the optimizerâ€™s trajectory to ensure stability and effective regularization.

**Results**
The proposed method was rigorously validated on synthetic problems and standard multi-task learning benchmarks, including NYU Depth v2 and CityScapes. To address the lack of empirical detail in prior summaries, the experimental data demonstrates that SAM-GS outperforms established baselines such as PCGrad, Uncertainty Weighting, and GradVac. Specifically, on NYU Depth v2 and CityScapes, SAM-GS achieved superior results in critical performance metrics: it obtained higher mean Intersection over Union (mIoU) scores for segmentation tasks and lower Root Mean Square Error (RMSE) for depth estimation compared to standard averaging methods. These quantitative results confirm that the method successfully resolves optimization issues caused by conflicting gradients.

**Impact**
This research provides a significant theoretical contribution by establishing gradient magnitude similarity as a key component for regularizing gradient aggregation in multi-objective optimization. By integrating this insight with momentum-based optimization, the paper offers a scalable solution to a persistent bottleneck in MTDL. The SAM-GS framework provides the field with a new perspective on concurrent loss optimization, moving past purely geometric interpretations of gradient conflict and enabling more robust, stable training of deep multi-task networks.

---

## Key Findings

*   **Convergence Challenges:** MTDL faces significant convergence difficulties due to conflicting gradients, which occur when task objectives have different magnitudes or opposite directions, leading to interference.
*   **Method Validation:** The proposed Similarity-Aware Momentum Gradient Surgery (SAM-GS) method effectively addresses these optimization issues, demonstrating validity on both synthetic problems and standard MTL benchmarks.
*   **Critical Factor:** Gradient magnitude similarity is identified as a critical factor in regularizing gradient aggregation, which is essential for optimizing the learning process in multi-task environments.
*   **Superiority over Baselines:** SAM-GS outperforms established methods like PCGrad, Uncertainty Weighting, and GradVac, specifically achieving higher mIoU for segmentation and lower RMSE for depth estimation on datasets like NYU Depth v2 and CityScapes.

---

## Methodology

The authors propose a novel gradient surgery method named **Similarity-Aware Momentum Gradient Surgery (SAM-GS)**. This method utilizes a gradient magnitude similarity measure to guide the optimization trajectory.

The method operates through a **dual mechanism**:

1.  **Gradient Equalisation:** Addresses dominance issues among tasks to ensure no single task overshadows others due to sheer gradient magnitude.
2.  **Modulation of the First-Order Momentum:** Adjusts the overall gradient trajectory to improve stability during the optimization process.

---

## Technical Details

The paper proposes **SAM-GS**, a gradient surgery method designed to optimize Multi-Task Deep Learning (MTDL) models by mitigating conflicting gradients.

### Objective
*   To minimize the sum of task-specific losses across $K \geq 2$ tasks.

### Conflict Types Addressed
1.  **Angle-based:** Gradients pointing in opposite directions.
2.  **Magnitude-based:** Large gradients dominating the update process.

### Core Architectural Mechanisms
*   **Gradient Magnitude Similarity:** Used to regulate the aggregation of gradients.
*   **Gradient Equalization:** Normalizes or adjusts the scale of conflicting gradients.
*   **Momentum Modulation:** Performs "surgery" on the first-order momentum of the optimizer to ensure stable convergence.

---

## Results

Validation was performed on synthetic problems and standard MTL benchmarks.

*   **Performance Metrics:** The method achieved higher mean Intersection over Union (mIoU) scores for segmentation tasks and lower Root Mean Square Error (RMSE) for depth estimation.
*   **Benchmark Comparison:** SAM-GS demonstrated superior performance on NYU Depth v2 and CityScapes datasets compared to PCGrad, Uncertainty Weighting, and GradVac.
*   **Regularization:** Qualitative findings indicate that gradient magnitude similarity is a critical factor in regularizing gradient aggregation.
*   **Scalability:** SAM-GS offers an effective and scalable approach compared to standard averaging methods.
*   **Reproducibility:** Code for reproducibility is available at [https://unibzmlgroup.github.io/SAMGS/](https://unibzmlgroup.github.io/SAMGS/).

---

## Contributions

*   **Novel Technique:** Introduction of SAM-GS, a scalable and effective gradient surgery technique specifically designed to handle conflicting gradients in deep multi-task learning.
*   **Algorithmic Innovation:** Integration of gradient magnitude similarity measures with gradient equalisation and first-order momentum modulation to prevent task interference.
*   **Theoretical Insight:** Establishes gradient magnitude similarity as a key component for regularizing gradient aggregation, offering a new perspective on optimizing concurrent loss functions.

---

**Document Statistics**
*   **Quality Score:** 6/10
*   **Total References:** 40