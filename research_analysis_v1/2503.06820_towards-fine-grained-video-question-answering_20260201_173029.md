# Towards Fine-Grained Video Question Answering

*Wei Dai; Alan Luo; Zane Durante; Debadutta Dash; Arnold Milstein; Kevin Schulman; Ehsan Adeli; Li Fei-Fei*

---

### üìä Quick Facts
| Metric | Detail |
| :--- | :--- |
| **Quality Score** | 4/10 |
| **References** | 40 Citations |
| **Dataset** | MOMA-QA (Multi-Object Multi-Actor) |
| **Architecture** | SGVLM (Scene Graph Video Language Model) |
| **Evaluation** | Wu-Palmer Similarity (WUPS @ 0.9) |
| **Training Hardware** | 4x NVIDIA A6000 GPUs |

---

### üìù Executive Summary

Current Video Question Answering (VideoQA) capabilities face a critical limitation: existing models lack the capacity for fine-grained spatiotemporal reasoning. This deficiency stems primarily from the datasets used to train these models, which lack the necessary temporal and spatial granularity. Consequently, models are forced to rely on coarse features, preventing them from accurately resolving complex queries that require a nuanced understanding of object interactions and events over time. Addressing this scarcity of granular data is essential for advancing the field beyond superficial video analysis toward true comprehension of dynamic scenes.

To bridge this gap, the authors introduce the **Multi-Object Multi-Actor Question Answering (MOMA-QA)** dataset alongside the **Scene Graph Video Language Model (SGVLM)**. SGVLM is a neuro-symbolic framework that integrates a Scene Graph Predictor, an Efficient Frame Retriever, and a pre-trained Large Language Model. Technically, the system employs a Scene Graph Predictor that takes object bounding boxes as input to generate context via a biLSTM, utilizing top-k filtering to determine relationship probabilities. This structured data is then synthesized by the LLM, enabling the model to reason over precise spatial relationships and temporal intervals rather than relying on end-to-end black-box feature extraction.

Evaluations utilized the Wu-Palmer Similarity (WUPS) metric with a strict threshold ($\gamma=0.9$), applying a penalty multiplier of 0.1 for responses with semantic similarity below this threshold. SGVLM establishes a new performance benchmark, significantly outperforming zero-shot baselines such as InternVideo and BLIP-2. The model was trained using a combination of Visual Genome, QVHighlights, and NExT-QA data, achieving superior results on both the new MOMA-QA dataset and standard public benchmarks. These quantitative findings validate the efficacy of the neuro-symbolic architecture, demonstrating marked improvements in handling fine-grained spatial-temporal queries compared to existing models.

This work significantly advances VideoQA by shifting the research focus from coarse, global-level analysis to fine-grained, entity-aware reasoning. By successfully combining neuro-symbolic methods with foundation models, the authors set a new standard for temporal localization and spatial relationship reasoning. The methodology demonstrated here has specific implications for future applications requiring high precision, such as robotic manipulation, where understanding fine-grained object interactions is critical, and medical video analysis, where accurate temporal localization of surgical procedures or symptoms can directly influence diagnostic outcomes.

---

## Key Findings

*   **Deficiency in Current Resources:** Existing VideoQA methods are hindered by datasets that lack sufficient temporal and spatial granularity.
*   **Introduction of MOMA-QA:** The Multi-Object Multi-Actor Question Answering (MOMA-QA) dataset was created to address these gaps, featuring ground truth scene graphs and temporal interval annotations.
*   **Superior Model Architecture:** The proposed SGVLM model, which integrates a scene graph predictor, frame retriever, and a pre-trained LLM, demonstrates advanced capabilities in temporal localization and spatial relationship reasoning.
*   **Benchmark Performance:** Evaluations reveal that SGVLM achieves superior performance on the MOMA-QA dataset and other public benchmarks.

## Methodology

The research adopts a dual-pronged methodology involving dataset construction and the development of a neuro-symbolic architecture:

### 1. Dataset Construction (MOMA-QA)
Designed for fine-grained video understanding, focusing on entity-centric queries, temporal localization, and spatial relationship reasoning. It provides rich annotations including:
*   Ground truth scene graphs
*   Temporal interval annotations

### 2. Model Architecture (SGVLM)
A video-language model composed of three integrated components:
1.  **Scene Graph Predictor:** Extracts structured relational information.
2.  **Efficient Frame Retriever:** Selects relevant frames and handles the temporal dimension.
3.  **Pre-trained Large Language Model (LLM):** Synthesizes data for reasoning.

## Technical Details

The SGVLM architecture processes visual data through a distinct pipeline before reasoning via the LLM.

### Core Components
*   **Scene Graph Predictor:**
    *   **Input:** Object bounding boxes.
    *   **Object Context:** Generated using a biLSTM on object features and position embeddings.
    *   **Edge Context:** Encoded using a biLSTM and MLP.
    *   **Feature Extraction:** Extracts scene graph features as `(W_h d_i)(W_t d_j)`.
    *   **Probability Calculation:** Relationship probabilities calculated via `softmax(W_r s_{i_j})`.
    *   **Filtering:** Applies a "top k filtering" mechanism.
    *   **Training Status:** Trained separately and typically kept frozen during VideoQA training.

*   **Frame Retriever & LLM:**
    *   Responsible for temporal localization and final synthesis of the extracted graph data into natural language answers.

## Results

### Evaluation Protocol
*   **Metric:** Wu-Palmer Similarity (WUPS) with a threshold ($\gamma$) of 0.9.
*   **Penalty:** Scores are penalized by multiplying by 0.1 if semantic similarity is below 0.9.
*   **Baselines:** Zero-shot evaluation uses InternVideo and BLIP-2.

### Training Specifications
*   **Hardware:** 4 NVIDIA A6000 GPUs.
*   **Duration:** Maximum of 5 epochs.
*   **Datasets:** Training protocols involve Visual Genome, QVHighlights, and NExT-QA.
*   **Tuning Parameters:** Specifically tuned the Scene Graph Predictor, Vision Backbone, and Frame Localizer.

## Contributions

*   **Fine-Grained Video Resource:** Contribution of MOMA-QA, a granular dataset providing necessary annotations to train and test models on complex spatial-temporal reasoning.
*   **Architectural Innovation:** Presentation of SGVLM, a framework that bridges computer vision scene graphs with large language models for entity-centric and temporal reasoning.
*   **Advancement of State-of-the-Art:** Quantitative demonstration of superior performance on both MOMA-QA and standard public datasets, establishing a new technical benchmark for VideoQA.