# Are Robust LLM Fingerprints Adversarially Robust?

*Anshul Nasery; Edoardo Contente; Alkin Kaz; Pramod Viswanath; Sewoong Oh*

> ### **Quick Facts**
> - **Focus:** Adversarial robustness in LLM fingerprinting
> - **Schemes Analyzed:** 10 recent methods
> - **Primary Metric:** Attack Success Rate (ASR)
> - **Top Performance:** 100% ASR achieved against 8 out of 10 schemes
> - **Quality Score:** 9/10
> - **Citations:** 40 references

---

## üìë Executive Summary

This paper addresses a critical vulnerability in Large Language Model (LLM) ownership verification, exposing that current "robust" fingerprinting schemes are fundamentally insecure against motivated adversaries. While existing research focuses on robustness against **benign perturbations**‚Äîsuch as paraphrasing or standard noise‚Äîit fails to account for **adaptive attacks** where a malicious host actively attempts to remove ownership markers. This issue is of paramount importance because it renders current intellectual property protections ineffective; if a malicious actor can strip a fingerprint without degrading model performance, they can distribute stolen or unauthorized models without fear of detection.

The authors introduce a comprehensive framework for designing adaptive adversarial attacks that target both intrinsic and invasive fingerprinting methods. The technical innovation centers on a specific threat model involving a malicious host with **white-box access to model weights** who serves a black-box API. The research identifies and exploits four primary vulnerability themes:

1.  **Verbatim Verification Vulnerability** (suppressing backdoor outputs)
2.  **Overconfidence Vulnerability** (targeting memorization by suppressing high-confidence tokens)
3.  **Unnatural Query Vulnerability** (filtering adversarial examples via perplexity)
4.  **Statistical Signature Vulnerability** (learning and suppressing watermarking signals)

By categorizing state-of-the-art methods into these buckets, the authors demonstrate that the reliance on static or predictable verification signals allows attackers to surgically remove fingerprints.

Empirical results indicate that current fingerprinting techniques are almost universally compromised by these adaptive strategies. Evaluating ten recent methods against metrics like **Attack Success Rate (ASR)** and **Normalized Utility** across benchmarks such as IFEval, GSM8K, and GPQA-Diamond, the study found that attacks achieved **100% ASR against eight schemes**: ChainHash, FPEdit, ImF, Perinucleus, InstrFP, MergePrint, ROFL, and ProfLingo. EditMF was compromised with a 94% ASR, and even the statistical scheme DSWatermark proved vulnerable with a 65% ASR. Crucially, these attacks successfully bypassed authentication while preserving the model's core utility, meaning the malicious hosts maintained high performance on downstream tasks while eliminating ownership protection.

The significance of this work lies in its demonstration that the field has been measuring the **wrong type of robustness**, creating a false sense of security regarding model protection. By bridging the gap between benign perturbation tolerance and adversarial robustness, the paper invalidates the security claims of nine out of ten analyzed state-of-the-art methods. This necessitates a paradigm shift in future research, moving away from heuristics that rely on verbatim verification or unnatural queries toward "**adversarial robustness by design**." The authors effectively provide a roadmap and guidelines for developing next-generation fingerprinting schemes that can withstand the sophisticated adaptive attacks detailed in this study.

---

## üîë Key Findings

*   **Insufficient Evaluations:** Current robustness evaluations for model fingerprinting are insufficient, focusing primarily on benign perturbations rather than adversarial scenarios.
*   **Fundamental Vulnerabilities:** Existing fingerprinting schemes possess fundamental vulnerabilities that are exploitable by malicious model hosts.
*   **Universal Bypass:** Adaptive adversarial attacks can completely bypass model authentication for ten recently proposed fingerprinting schemes.
*   **Utility Preservation:** These attacks do not degrade model performance, maintaining high utility while successfully removing ownership protection.

---

## üõ†Ô∏è Methodology

The research followed a structured four-phase approach:

1.  **Threat Modeling:** Defining the specific capabilities and constraints of a malicious host.
2.  **Vulnerability Analysis:** Categorizing and identifying security flaws inherent in existing fingerprinting schemes.
3.  **Attack Development:** Designing adaptive adversarial exploits tailored to the specific vulnerabilities identified.
4.  **Empirical Validation:** Rigorously testing the developed attacks against ten distinct methods, measuring success rates regarding authentication bypass and the preservation of model utility.

---

## ‚öôÔ∏è Technical Details

### **System Classification**
The paper categorizes black-box fingerprinting methods into two distinct types:
*   **Intrinsic:** Adversarial Examples and Classification-based methods.
*   **Invasive:** Backdoors and Watermarking methods.

### **Threat Model**
The scenario assumes a **Malicious Host** with:
*   **White-box access:** Full access to model weights.
*   **Service Interface:** Serves the model via a black-box API.
*   **Knowledge:** Awareness of the public protocol but lack of knowledge regarding secret randomness.

### **Attack Themes**
Four primary vulnerability themes were identified and exploited:

1.  **Verbatim Verification Vulnerability (Output Suppression)**
    *   *Target:* Backdoors
    *   *Mechanism:* Perturbing outputs to suppress trigger responses.
2.  **Overconfidence Vulnerability (Output Detection)**
    *   *Target:* Memorization
    *   *Mechanism:* Suppressing high-confidence tokens to obscure learned patterns.
3.  **Unnatural Query Vulnerability (Input Detection)**
    *   *Target:* Adversarial Examples
    *   *Mechanism:* Utilizing perplexity filtering to identify and filter out unnatural queries.
4.  **Statistical Signature Vulnerability (Statistical Analysis)**
    *   *Target:* Watermarking
    *   *Mechanism:* Learning and subsequently suppressing statistical signals embedded in the text.

---

## üìä Results

The evaluation utilized **Attack Success Rate (ASR)** and **Normalized Utility** across standard benchmarks including IFEval, GSM8K, GPQA-Diamond, and TriviaQA.

| Scheme / Method | Type | Attack Success Rate (ASR) | Notes |
| :--- | :--- | :--- | :--- |
| **ChainHash** | N/A | **100%** | Completely Compromised |
| **FPEdit** | N/A | **100%** | Completely Compromised |
| **ImF** | N/A | **100%** | Completely Compromised |
| **Perinucleus** | N/A | **100%** | Completely Compromised |
| **InstrFP** | N/A | **100%** | Completely Compromised |
| **MergePrint** | N/A | **100%** | Completely Compromised |
| **ROFL** | N/A | **100%** | Completely Compromised |
| **ProfLingo** | N/A | **100%** | Completely Compromised |
| **EditMF** | N/A | **94%** | Highly Compromised |
| **DSWatermark** | Statistical | **65%** | More resilient, but still vulnerable |

**Overall Outcome:** 9 out of 10 schemes were compromised with high ASR while preserving utility. This highlights significant security weaknesses, particularly in methods relying on verbatim verification and unnatural query detection.

---

## üìù Contributions

*   **Bridging the Robustness Gap:** Shifts the focus of evaluation from benign perturbations to malicious, adaptive perturbations.
*   **Exposure of Systemic Weaknesses:** Demonstrates empirically that state-of-the-art fingerprinting methods are universally vulnerable to adaptive attacks.
*   **Guidelines for Future Research:** Advocates for a philosophy of "adversarial robustness by design" and provides specific recommendations for developing more secure methods.

---
**Paper Quality Score:** 9/10  
**References:** 40 citations