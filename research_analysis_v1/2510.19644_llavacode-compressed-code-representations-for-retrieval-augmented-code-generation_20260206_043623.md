---
title: 'LLavaCode: Compressed Code Representations for Retrieval-Augmented Code Generation'
arxiv_id: '2510.19644'
source_url: https://arxiv.org/abs/2510.19644
generated_at: '2026-02-06T04:36:23'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# LLavaCode: Compressed Code Representations for Retrieval-Augmented Code Generation

*Daria Cherniuk; Nikita Sukhorukov; Nikita Sushko; Daniil Gusak; Danil Sivtsov; Elena Tutubalina; Evgeny Frolov*

---

### ðŸ“Š Quick Facts

| **Metric** | **Detail** |
| :--- | :--- |
| **Quality Score** | 9/10 |
| **References** | 40 Citations |
| **Latency Improvement** | 20â€“38% reduction in Time-to-First-Token (TTFT) |
| **Core Mechanism** | Compressed RAG (Retrieval-Augmented Generation) |
| **Training Method** | Reinforcement Learning (Self-Critical Sequence Training) |
| **Primary Benefit** | Maintains code quality (EM/ES) while significantly speeding up inference |

---

## Executive Summary

Retrieval-Augmented Generation (RAG) significantly enhances code generation by incorporating relevant repository context, but it introduces a critical latency bottleneck in interactive environments like IDEs. Standard RAG pipelines require feeding long sequences of retrieved code chunks into the Large Language Model (LLM), which drastically increases Time-to-First-Token (TTFT). This delay creates a poor user experience for developers relying on real-time code completion, creating a need for a method that preserves the benefits of contextual awareness without the computational cost of processing extensive raw text inputs.

The paper introduces **LlavaCode**, a "Compressed RAG" framework that replaces raw retrieved code with compact, semantically rich representations. Technically, the system retrieves the top-10 relevant code chunks and processes them through a pre-trained embedder. These embeddings are then mapped into the reader LLM's embedding space using a lightweight, trainable LLaVA-like projector (an MLP with GeLU and LayerNorm). This allows the framework to concatenate the context as a few single-token vectors rather than long text sequences. The projector is optimized using Reinforcement Learning (Self-Critical Sequence Training) based on Exact Match and Edit Similarity rewards, rather than standard cross-entropy loss, ensuring the compressed vectors retain high-fidelity semantic information.

LlavaCode achieves substantial performance improvements over standard full-RAG pipelines, reducing Time-to-First-Token (TTFT) by 20â€“38% on line completion tasks. Crucially, this speed optimization does not come at the expense of quality; the framework demonstrates improved code generation accuracy, with higher Exact Match (EM) and Edit Similarity (ES) scores compared to baselines. The study also confirms that the additional projector module introduces negligible latency overhead, validating the efficiency of the compressed representation approach.

This work challenges the assumption that raw text context is necessary for high-quality retrieval-augmented code generation, establishing "semantic compression" as a viable superior alternative. By demonstrating that compressed single-token vectors can outperform full-text retrieval, LlavaCode sets a new precedent for building responsive, high-performance coding assistants. The findings suggest a shift toward more efficient RAG architectures in interactive AI tools, prioritizing both inference speed and generation quality through optimized representation learning.

---

## Key Findings

*   **Reduced Latency:** The LlavaCode framework achieves a **20â€“38% reduction** in Time-to-First-Token (TTFT) on line completion tasks compared to standard full-RAG pipelines.
*   **Improved Quality:** The use of compressed context significantly improves code generation quality, evidenced by increased **Exact Match (EM)** and **Edit Similarity (ES)** metrics.
*   **Negligible Overhead:** Incorporating the small projector module required for compression results in negligible latency overhead compared to the base model.
*   **Semantic Integrity:** Compressed representations allow long repository context to be reduced to just a few single-token vectors without sacrificing semantic integrity.

---

## Methodology

The researchers introduced LlavaCode, a framework designed to optimize Retrieval-Augmented Generation (RAG) for code completion. The core methodology involves compressing code context into compact, semantically rich representations that are directly interpretable by code Large Language Models (LLMs).

This is achieved using a small projector module, which transforms retrieved repository context into a few compressed single-token vectors, thereby drastically shortening the input sequence length for the model.

---

## Technical Details

*   **Paradigm:** Compressed RAG.
*   **Retrieval Strategy:** Retrieves top-10 code chunks and compresses them into single embedding vectors using a pre-trained embedder.
*   **Mapping Architecture:** Utilizes a LLaVA-like projector (MLP with GeLU and LayerNorm) to map vectors to the reader LLM's embedding space.
*   **Input Structure:** Compressed vectors are concatenated with the prompt rather than raw text.
*   **Model Components:**
    *   **Reader LLM:** Frozen Qwen-2.5-Coder-1.5B.
    *   **Embedder:** Qwen-3-Embedding-0.6B.
    *   **Trainable Component:** Only the projector is trained.
*   **Training Approach:**
    *   Utilizes Reinforcement Learning (Self-Critical Sequence Training).
    *   Optimizes a reward function based on Exact Match (EM) and Edit Similarity (ES).
*   **Optimization:** Latency is optimized by retrieving precomputed text projections directly from the database.

---

## Contributions

*   **Addressing Inference Latency:** Tackles the critical limitation of RAG in interactive settings (like IDEs) where extended sequence lengths typically slow down inference.
*   **Semantic Compression:** Demonstrates that raw code context can be effectively replaced by compressed, semantically rich single-token vectors.
*   **Performance Validation:** Provides empirical evidence that compressed context can outperform full-RAG pipelines by simultaneously boosting generation quality metrics (EM/ES) and reducing computational delay (TTFT).

---

## Results

The approach reduces Time-to-First-Token (TTFT) by 20â€“38% compared to standard full-RAG pipelines while maintaining negligible latency overhead compared to the base model. It improves code generation quality with increased Exact Match (EM) and Edit Similarity (ES) scores compared to baselines. The method maintains semantic integrity and demonstrates that RL-based optimization of EM/ES is more effective for code completion than standard cross-entropy loss.