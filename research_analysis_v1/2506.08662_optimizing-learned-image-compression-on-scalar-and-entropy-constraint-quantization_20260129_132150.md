# Optimizing Learned Image Compression on Scalar and Entropy-Constraint Quantization

*Florian Borzechowski; Michael SchÃ¤fer; Heiko Schwarz; Jonathan Pfaff; Detlev Marpe; Thomas Wiegand*

---

### ðŸ“Š Quick Facts

| Metric | Value |
| :--- | :--- |
| **TecNick BD-Rate Gain** | Up to **2.2%** reduction |
| **Kodak BD-Rate Gain** | **1â€“2%** reduction |
| **Inference Overhead** | **0%** increase |
| **Training Strategy** | Two-stage (End-to-End + Quantization-Aware Finetuning) |
| **Key Techniques** | VAE, Scalar Quantization, Entropy-Constraint Quantization (TCQ) |
| **Quality Score** | 9/10 |

---

## Executive Summary

Learned image compression models based on variational autoencoders face a fundamental challenge: the quantization step required for entropy coding is non-differentiable, preventing effective gradient-based optimization. To navigate this, existing methods rely on differentiable approximations during training, such as adding uniform noise or simple rounding. While these workarounds enable backpropagation, they fail to accurately model the statistical properties of true quantization noise. This discrepancy creates a "train-inference gap" where the network is optimized for idealized, approximated data rather than the hard quantized latents used during actual deployment, resulting in suboptimal rate-distortion performance.

The authors address this limitation through a **two-stage training strategy** that aligns the training process with the reality of inference. The first stage utilizes standard end-to-end training with differentiable approximations to establish a baseline model. The key innovation is a post-training fine-tuning phase wherein specific parts of the network are retrained using actual, fully quantized latents derived from the inference pipeline. By re-optimizing network components against the true distribution of quantized dataâ€”rather than approximationsâ€”the method effectively bridges the statistical gap between training and deployment. Notably, this optimization is extended to support complex entropy-constraint quantizers like Trellis-Coded Quantization (TCQ), where latents are interdependently selected via trellis search.

The proposed quantization-aware optimization yields consistent coding gains across standardized datasets. On the TecNick test set, the method achieved a reduction in BjÃ¸ntegaard-Delta bitrate (BD-rate) of up to **2.2%**, while the Kodak test set showed improvements between **1% and 2%**. Crucially, these enhancements in compression efficiency are realized without any penalty to operational complexity. The approach incurs a **0% increase in inference complexity**, meaning the improved rate-distortion performance comes without additional computational cost or latency during the encoding and decoding processes.

This research is significant because it validates a practical methodology for reducing the performance penalty imposed by non-differentiable operations in learned compression. By demonstrating that fine-tuning on real quantized latents recovers loss without adding inference overhead, the authors provide a "drop-in" optimization for current state-of-the-art codecs. The ability to seamlessly apply this improvement to advanced quantization techniques like TCQ suggests the method will have broad applicability in the development of next-generation image compression standards, pushing the efficiency of learned codecs closer to their theoretical limits.

---

## Key Findings

*   **Approximation Failure:** Existing methods for approximating quantization during training (such as adding noise or simple rounding) fail to model quantization noise correctly, resulting in suboptimal network performance.
*   **Retraining Efficacy:** Retraining specific parts of the network on correctly quantized latents consistently yields additional coding gains.
*   **Advanced Constraint Support:** The proposed optimization is particularly effective for entropy-constraint quantization (e.g., Trellis-Coded Quantization), where latents are interdependently chosen via trellis search.
*   **Rate-Distortion Efficiency:** The method achieves improvements of up to **2.2%** in BjÃ¸ntegaard-Delta bitrate (BD-rate) on the TecNick test set and **1â€“2%** on the Kodak test set.
*   **Zero Overhead:** These coding gains are realized without any increase in the complexity of the inference process.

---

## Methodology

The authors utilize a **two-stage training strategy** for variational autoencoder-based learned image compression:

1.  **Stage 1 - Baseline Training:** A conventional end-to-end training phase is conducted using standard differentiable approximations to handle the non-differentiable nature of quantization.
2.  **Stage 2 - Finetuning:** A novel finetuning step is introduced, wherein parts of the network are retrained using actual quantized latents obtained during the inference stage.

This approach allows the network to adapt to the true distribution of quantized data, bypassing the inaccuracies of differentiable approximations.

---

## Technical Details

The analysis highlights the following technical specifications and mechanisms:

*   **Core Problem Addressed:** The approach resolves statistical flaws in standard learned image compression training (specifically additive noise and simple rounding) by re-optimizing network components using correctly quantized latents.
*   **Quantization Types:** It applies to both scalar quantization and Entropy-Constraint Quantization (ECQ).
*   **Trellis Handling:** It handles Trellis-Coded Quantization (TCQ) via trellis search.
*   **Complexity Impact:** The optimization introduces no additional complexity to the inference process (0% overhead).

---

## Contributions

*   **Quantization-aware Optimization:** Introduces a post-training finetuning phase utilizing real quantized latents to bridge the gap between training approximations and inference reality.
*   **Advanced Entropy-Constraint Support:** Extends effective training methodologies to complex entropy-constraint quantizers like Trellis-Coded Quantization.
*   **Efficiency Gains:** Demonstrates a significant improvement in rate-distortion efficiency (up to 2.2% BD-rate savings) over state-of-the-art learned codecs without imposing computational overhead during inference.

---

## Results

Performance was measured using BjÃ¸ntegaard-Delta bitrate (BD-rate):

*   **TecNick Test Set:** Achieved up to **2.2%** BD-rate reduction.
*   **Kodak Test Set:** Achieved between **1% and 2%** BD-rate reduction.
*   **Complexity:** Confirmed **0%** increase in inference complexity.

---

**Quality Score:** 9/10  
**References:** 0 citations