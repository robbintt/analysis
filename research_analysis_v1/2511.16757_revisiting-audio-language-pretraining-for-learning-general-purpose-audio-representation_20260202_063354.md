# Revisiting Audio-language Pretraining for Learning General-purpose Audio Representation

*Wei-Cheng Tseng; Xuanru Zhou; Mingyue Huo; Yiwen Shao; Hao Zhang; Dong Yu*

---

### ðŸ“‹ Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Dataset** | CaptionStew (10.7M captions, 37,290 hours) |
| **Audio Encoder** | Zipformer-M (U-Net structure, 6 blocks) |
| **Text Components** | BERT-base (Contrastive) / BART-base (Captioning) |
| **Training Hardware** | 8x Tesla V100 GPUs |
| **Batch Size** | 640 seconds per GPU |
| **Peak Retrieval Score** | 63.2 (PSC) |
| **Best FSD50k mAP** | 0.664 (Contrastive) |

---

## Executive Summary

**Problem**
This research addresses the challenge of developing general-purpose audio representations capable of transferring across speech, music, and environmental sound domains. Despite rapid advancements in audio-language pretraining (ALP), there is a lack of systematic understanding regarding the trade-offs between contrastive learning and captioning objectives, particularly regarding data efficiency and scaling. Additionally, the field relies heavily on supervised initialization (e.g., AudioSet), though the necessity of this approach versus training from scratch with massive data remains unverified.

**Innovation**
The authors introduce **CaptionStew**, a large-scale dataset aggregating 10.7 million caption pairs (37,290 hours) from diverse open-source sources. The technical architecture features a **Zipformer-M audio encoder** paired with BERT-base (for contrastive learning) and BART-base (for captioning). This setup facilitates the first systematic evaluation of these objectives across varying data scales and initialization strategies, optimizing for general-purpose representation learning.

**Results**
The study reveals that the optimal training objective is task-dependent. **Contrastive learning** is more data-efficient for discriminative tasks, achieving **0.664 mAP** on FSD50k and **38.17% EER** on VoxCeleb2. Conversely, **captioning objectives** scale better with data and excel in generative tasks, achieving a **SPICE score of 22.9**. Crucially, supervised initialization shows diminishing returns; while helpful for classification, training from scratch with sufficient data often matches or exceeds it in generative and QA scenarios.

**Impact**
This work provides essential guidelines for audio-language learning, suggesting that massive-scale training reduces dependency on expensive supervised initialization like AudioSet. By releasing CaptionStew and a comprehensive evaluation framework, the authors establish a new benchmark, enabling the community to pursue data-efficient general audio representation without relying solely on curated, single-domain labeled data.

---

## Key Findings

*   **Transferable Representations:** Audio-language pretraining yields competitive and highly transferable representations across diverse domains, including speech, music, and environmental sound.
*   **Objective Trade-offs:** Contrastive learning demonstrates superior data efficiency at smaller scales and excels at discriminative tasks, whereas captioning objectives exhibit better scalability and performance in generative tasks.
*   **Diminishing Returns of Initialization:** Supervised initialization provides limited benefits as the scale of training data increases; training from scratch with massive datasets is often just as effective.
*   **Beyond Retrieval:** Existing audio-language models show significant potential as general-purpose encoders for applications beyond simple retrieval tasks.

---

## Methodology

*   **Dataset Construction:** Aggregated diverse open-source audio-text corpora to create **CaptionStew**, comprising 10.7 million caption pairs.
*   **Objective Comparison:** Conducted the first systematic evaluation comparing contrastive learning against captioning objectives for audio representation learning.
*   **Multi-Domain Evaluation:** Assessed model performance across three distinct audio domains: speech, music, and environmental sound.
*   **Scaling Analysis:** Performed systematic scaling experiments to analyze the impact of training objectives and initialization strategies relative to data volume.

---

## Contributions

*   **Open Source Release:** Released CaptionStew (10.7M captions), complete data preparation recipes, standardized training protocols, and pretrained models.
*   **Evaluation Framework:** Established a comprehensive framework for benchmarking contrastive versus captioning objectives.
*   **Research Guidelines:** Provided clear guidelines for future research concerning the trade-offs between different training approaches and the diminishing returns of supervised initialization at scale.

---

## Technical Details

### Architecture and Dataset

| Component | Specification |
| :--- | :--- |
| **Dataset Name** | CaptionStew (CS10M) |
| **Dataset Size** | 9.3M audio samples, 10.7M captions |
| **Total Duration** | 37,290 hours |
| **Audio Encoder** | Zipformer-M (U-Net structure, 6 blocks) |
| **Audio Features** | 80-dimensional log-Mel features |
| **Text Encoder (Contrastive)** | BERT-base |
| **Text Decoder (Captioning)** | BART-base |

### Training and Evaluation

| Parameter | Configuration |
| :--- | :--- |
| **Contrastive Loss** | Symmetric InfoNCE |
| **Captioning Loss** | Dual-mode (Autoregressive & Parallel) generative objective |
| **Training Steps** | 600k steps (from scratch) / 200k steps (AudioSet init) |
| **Compute** | 8 Tesla V100 GPUs |
| **Effective Batch Size** | 640 seconds per GPU |
| **Protocols** | Linear Probing, Audio-language Alignment (LiT), Open-ended QA (Qwen2.5-7B-Instruct) |

---

## Results

### Discriminative & Generative Performance
*   **Contrastive Model:** Achieved **0.664 mAP** on FSD50k and **38.17% EER** on VoxCeleb2, often matching or exceeding SSL baselines like BEATs.
*   **Captioning Model:** Achieved a **SPICE score of 22.9** on Music Captioning, demonstrating superiority in generative tasks.
*   **Retrieval:** The contrastive model achieved a score of **63.2 PSC**.

### Open-Ended QA Performance
*   **Sound QA:** Contrastive-scratch reached **7.01**.
*   **Music QA:** Captioning-scratch achieved **5.97**.
*   **Speaker-related QA:** Contrastive-scratch scored **37.9%**.

### Initialization Impact
*   Supervised initialization typically benefits classification tasks but can degrade performance in generative or QA scenarios compared to training from scratch with large-scale data.

---

**Quality Score:** 5/10  
**References:** 31 citations