# AMAP Agentic Planning Technical Report

*Yulan Hu; Xiangwen Zhang; Sheng Ouyang; Hao Yi; Lu Xu; Qinglin Lang; Lide Tan; Xiang Cheng; Tianchen Ye; Zhicong Li; Ge Chen; Wenjin Yang; Zheng Pan; Shaopan Xiong; Siran Yang; Ju Huang; Yan Zhang; Jiamang Wang; Yong Liu; Yinfeng Huang; Tucheng Lin; Xin Li; Ning Guo*

---

### ðŸ“Š Quick Facts

| Metric | Value |
| :--- | :--- |
| **Base Model** | Qwen3-30B-A3B |
| **Tools Integrated** | 10+ (Spatio-temporal) |
| **Raw Logs Processed** | 30 Million |
| **High-Quality Samples** | ~200,000 |
| **Filtering Ratio** | 1:10,000 ("Needle in a haystack") |
| **SFT Trajectories (K)** | 8 per query |
| **Verification Model** | Gemini-3-Pro-Preview |
| **Primary Benchmark** | TravelBench |
| **Quality Score** | 9/10 |

---

## Executive Summary

> **Problem**
> The research addresses the challenge of developing robust AI agents for complex spatio-temporal planning. While Large Language Models (LLMs) possess strong reasoning capabilities, they often struggle with training stability when interacting with external tools (e.g., mapping, navigation, weather APIs). The paper highlights the difficulty of curating high-quality data for rare, long-tail events and the risk of catastrophic forgettingâ€”where models lose general capabilities upon specializing in tool use.

> **Innovation**
> The authors introduce a comprehensive pipeline anchored by a **Cascaded Training Recipe** and a **Hierarchical Data Curation Framework**.
> *   **Training:** A three-stage process (Seed SFT $\rightarrow$ High-certainty SFT $\rightarrow$ Low-certainty RL).
> *   **Environment:** A stable asynchronous tool environment using ROLL infrastructure and FastMCP protocol.
> *   **Rewards:** A "Rubrics-as-reward" system with dynamic weighting and a strict zero-reward penalty for hallucinations.

> **Results**
> The approach demonstrated robust stability and efficacy. The data pipeline successfully filtered **30 million** logs into **200,000** clean samples (1:10,000 ratio). The resulting STAgent achieved promising results on the **TravelBench** benchmark for itinerary planning while maintaining "Balanced Performance" across general reasoning benchmarks, effectively avoiding catastrophic forgetting.

> **Impact**
> This work provides a scalable blueprint for training specialized agentic models without sacrificing general intelligence. It advances System 2 reasoning for AI agents, proving that sophisticated domain mastery can be achieved through targeted, cascaded fine-tuning strategies using strictly filtered data.

---

## Key Findings

*   **Effective Tool Utilization:** STAgent successfully interacts with over ten distinct spatio-temporal tools to solve complex, multi-step tasks.
*   **Balanced Performance:** The model preserves its general capabilities, maintaining strong performance across standard benchmarks despite heavy specialization.
*   **Benchmark Success:** The model yielded promising results on TravelBench, demonstrating practical efficacy in real-world itinerary planning scenarios.
*   **Training Stability:** Initializing with Qwen3-30B-A3B provided a strong foundation, ensuring stability throughout the cascaded training process.

---

## Contributions

1.  **Stable Tool Environment**
    A robust environment supporting over ten domain-specific spatio-temporal tools, featuring asynchronous capabilities for real-time interaction.

2.  **Hierarchical Data Curation Framework**
    A "needle in a haystack" system utilizing a strict **1:10,000 filter ratio** to identify and retain only high-certainty queries from massive datasets.

3.  **Cascaded Training Recipe**
    A novel strategy segregating data by certainty levels:
    *   Stage 1: Seed SFT (Difficulty measurement)
    *   Stage 2: High-certainty SFT
    *   Stage 3: Low-certainty RL (Refinement)

---

## Methodology

The methodology is built around a specialized cascaded pipeline designed to ensure stability and high data quality.

1.  **Model Initialization**
    *   Starts from the **Qwen3-30B-A3B** checkpoint to establish a stable baseline.
2.  **Cascaded Training Stages**
    *   **Seed SFT Stage:** Utilized to measure and categorize query difficulty.
    *   **Second SFT Stage:** Focuses on fine-tuning specifically on high-certainty queries.
    *   **Ultimate RL Stage:** Refines the model's ability to handle low-certainty queries through reinforcement learning.
3.  **Infrastructure Support**
    *   A stable tool environment enables asynchronous rollout and training, preventing bottlenecks during the agent's interaction with external APIs.

---

## Technical Details

### Core Infrastructure
*   **Framework:** Utilizes **ROLL** infrastructure.
*   **Protocol:** Employs **FastMCP** protocol for an asynchronous, interactive tool-use environment.

### Sandbox Optimization
*   **Caching:** Implements tool-level LRU caching to optimize hit rates.
*   **Cost Reduction:** Uses parameter normalization to reduce API costs.

### Tool Library
The system integrates 10 specialized tools across four domains:
*   **Map & Navigation:** 5 tools
*   **Travel:** 2 tools
*   **Weather:** 2 tools
*   **Information:** 1 tool

### Data Curation
*   **Source Data:** 30 million anonymized user logs.
*   **Framework:** Processed via a Seed-Driven Evolutionary Framework.
*   **Taxonomy:** Created a hierarchical Intent Taxonomy:
    *   5 Primary nodes
    *   16 Secondary nodes
    *   30 Leaf nodes
*   **Synthetic Data:** Includes synthetic long-tail generation via In-Context Learning (ICL).

### Training Pipeline
*   **Trajectory Generation:** Uses DeepSeek-R1 to generate **K=8 trajectories** per query.
*   **Verification:** Trajectories are verified by **Gemini-3-Pro-Preview**.
*   **Observation Masking:** Text segments for tool observations are excluded from loss calculation to focus on reasoning.

### Reward Design
*   **Rubrics-as-Reward:** Employs dynamic weighting across three dimensions based on the scenario:
    *   Reasoning
    *   Information Fidelity
    *   Presentation
*   **Hallucination Veto:** Enforces a strict policy where any hallucination results in a reward score of 0.

---

## Results

*   **Data Processing Efficiency:** Successfully filtered 30 million raw queries down to approximately **200K clean samples**.
*   **Training Scale:** Generated 8 candidate trajectories per query for rigorous SFT validation.
*   **Model Initialization:** Achieved high training stability starting from Qwen3-30B-A3B.
*   **Benchmark Performance:** Achieved promising results on **TravelBench**, specifically for itinerary planning.
*   **General Capabilities:** Maintained strong performance across general benchmarks, confirming the "Balanced Performance" hypothesis (no degradation in general reasoning).
*   **Reasoning Depth:** Demonstrated capability for **System 2 reasoning** when handling complex spatio-temporal constraints.

---
**References:** 40 citations