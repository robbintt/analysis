---
title: Large Language Model Compression via the Nested Activation-Aware Decomposition
arxiv_id: '2503.17101'
source_url: https://arxiv.org/abs/2503.17101
generated_at: '2026-02-06T02:30:41'
quality_score: 9
citation_count: 35
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Large Language Model Compression via the Nested Activation-Aware Decomposition

*Jun Lu; Tianyi Xu; Bill Ding; David Li; Yu Kang*

***

> ### ðŸ“Š Quick Facts
> *   **Quality Score:** 9/10
> *   **Method:** Nested Activation-Aware Decomposition (NSVD)
> *   **Training Required:** None (Post-training/Training-free)
> *   **Compression Ratios Tested:** Up to 50%
> *   **Key Achievement:** Up to 67.4% perplexity reduction on Japanese datasets compared to baselines.
> *   **Models Evaluated:** 6 models (LLaMA, OPT, Vicuna, Mistral)
> *   **Citations:** 35 references

***

## Executive Summary

This research addresses the critical inefficiencies and instability inherent in compressing Large Language Models (LLMs) using standard low-rank decomposition methods. The primary problem identified is that existing techniques, such as standard Singular Value Decomposition (SVD), focus on minimizing weight reconstruction error rather than the error in the model's output activations. This approach fails to account for the high variability in LLM activation distributions and the prevalence of "unseen activations" during inference. Consequently, standard decomposition methods often result in severe performance degradation at medium-to-high compression ratios, rendering them ineffective for real-world deployment where input data is diverse and dynamic.

To resolve this, the authors propose **Nested Activation-Aware Decomposition (NSVD)**, a training-free, post-processing framework that fundamentally shifts the optimization target. Instead of reconstructing weights, NSVD minimizes the error of the output activations. The method employs a data whitening strategy using a diagonal matrix to normalize inputs and utilizes a nested two-stage decomposition structure. This technical architecture allows the framework to absorb activation outliers into the transformed weight matrix rather than ignoring them, thereby preventing overfitting to calibration data and enabling accurate, stable decomposition without the need for expensive fine-tuning.

Evaluations across six models from the LLaMA, OPT, Vicuna, and Mistral families demonstrate that NSVD significantly outperforms state-of-the-art baselines like ASVD-I, particularly regarding perplexity (PPL). While standard SVD fails catastrophically at 10% compression (PPL > 2,000), NSVD maintains robust performance at higher compression ratios. At 30% compression, NSVD-I reduced perplexity by **16.1%** on the Chinese CMRC dataset and **54.8%** on the Japanese AlpacaEval dataset. Even at a 50% compression ratio, NSVD-I achieved a **67.4%** perplexity reduction on Japanese AlpacaEval compared to baselines, achieving an overall average perplexity reduction of up to **24.6%** across evaluated benchmarks.

## Key Findings

*   **Identified Bottlenecks:** Variability in LLM activation distributions and the difficulty of handling unseen activations are identified as primary bottlenecks for effective low-rank decomposition.
*   **Superior Robustness:** The proposed Nested activation-aware framework (**NSVD**) outperforms state-of-the-art methods, demonstrating robustness at medium to large compression ratios.
*   **Cross-Domain Generalization:** NSVD exhibits exceptional performance in multilingual and multitask settings, generalizing across diverse data distributions without requiring retraining.
*   **Outlier Management:** The method improves decomposition accuracy by absorbing activation outliers into the transformed weight matrix rather than ignoring them.

## Methodology

The researchers propose a **post-training compression paradigm** based on low-rank decomposition that is entirely **training-free**. The core of the methodology is the **Nested activation-aware framework (NSVD)**.

*   **Weight Transformation:** The framework transforms the weight matrix by considering both the original weight matrix and the distribution of activations.
*   **Outlier Absorption:** This transformation manages activation outliers by absorbing them into the weight matrix.
*   **Stability:** By integrating these outliers, the method enables accurate and stable decomposition without the need for fine-tuning.

## Technical Details

The paper proposes **Nested Activation-Aware Decomposition (NSVD)**, a method that moves beyond standard SVD.

### Core Optimization
*   **Objective:** Minimizes the error of output activations $\|(A - B)X\|_F$ rather than weight reconstruction error.
*   **Data Whitening:** Employs a strategy using a diagonal matrix $S$ to normalize activations and handle high variance in inputs.

### Architecture
*   **Nested Structure:** Uses a nested two-stage decomposition structure with ranks $k1$ and $k2$ (where sum $\equiv$ target rank $k$).
*   **Function:** This structure is designed to absorb activation outliers and prevent overfitting to calibration data.

### Variants
*   Specific instantiations include **NSVD-I** and **NSVD-II**.

## Results

Evaluations on LLaMA, OPT, Vicuna, and Mistral models demonstrated significant improvements over standard methods:

*   **Standard SVD Failure:** Standard SVD fails at 10% compression (PPL > 2,000).
*   **30% Compression Performance:**
    *   NSVD-I outperformed baselines like ASVD-I on out-of-distribution datasets.
    *   Reduced perplexity by **16.1%** on Chinese CMRC.
    *   Reduced perplexity by **54.8%** on Japanese AlpacaEval.
*   **50% Compression Performance:**
    *   NSVD-I maintained robust performance, achieving a **67.4%** perplexity reduction on Japanese AlpacaEval compared to baselines.
*   **Parameter Sensitivity:** Performance depends on the $k1$ parameter.
    *   Lower values (e.g., 0.80k) improve OOD (Out-of-Distribution) results.
    *   Higher values (0.99k) suffice for English datasets.
*   **Overall Impact:** NSVD achieved average perplexity reductions of up to **24.6%** compared to state-of-the-art baselines.

## Contributions

1.  **Novel Problem Formulation:** Explicitly identifies and addresses the specific issues of activation distribution variability and unseen activations, refining the understanding of why standard low-rank decomposition often fails.
2.  **Introduction of NSVD:** Contributes a new, training-free framework that utilizes a nested activation-aware mechanism to transform weights and absorb outliers, offering a distinct path to high-accuracy compression.
3.  **Comprehensive Empirical Validation:** Provides a rigorous evaluation across six models from three distinct LLM families and eight datasets, offering strong evidence for the method's applicability and superiority in real-world scenarios.

***

**References:** 35 citations