# A Note on Implementation Errors in Recent Adaptive Attacks Against Multi-Resolution Self-Ensembles

*Stanislav Fort*

---

> ### ðŸ“Œ Quick Facts Sidebar
>
> *   **Dataset:** CIFAR-100
> *   **Architecture:** Multi-resolution self-ensemble (ResNet152)
> *   **Perturbation Bound:** $L_\infty = 8/255$
> *   **Bug Severity:** Perturbations exceeded limits by **20x** ($160/255$)
> *   **Corrected Robustness:** >20% adversarial accuracy restored
> *   **Quality Score:** 8/10

---

## Executive Summary

This research addresses a critical reliability issue in the evaluation of adversarial defenses, specifically challenging recent claims that Multi-Resolution Self-Ensemble (MRSE) defenses are vulnerable to adaptive white-box attacks. The study highlights a fundamental implementation error in prior leading research, where Projected Gradient Descent (PGD)-based attacks violated the standard $L_\infty$ perturbation constraints. By allowing adversarial noise to accumulate additively rather than being strictly bounded, previous studies erroneously concluded that MRSE defenses offered no robustness. This matter undermines the perceived effectiveness of adaptive evaluations, a cornerstone standard for certifying the security of machine learning models.

The key innovation lies in a rigorous audit and subsequent correction of the attack implementation to ensure strict adherence to mathematical constraints. Instead of allowing perturbations to compound across iterations relative to the previously perturbed image, the author implements a corrected PGD approach that enforces the $L_\infty$ bound relative to the original image $X$ at every single step. The study also introduces a perceptual analysis framework, evaluating adversarial examples based on semantic alignment with human vision to determine if successful attacks require perceptible changes.

Experimental results on the CIFAR-100 dataset using a ResNet152-based MRSE architecture reveal a drastic performance discrepancy between the flawed and corrected implementations. The bugged code allowed perturbations to reach $L_\infty = 160/255$ (exceeding the intended limit of $8/255$ by a factor of 20x), resulting in near-zero model accuracy. When bounds were properly enforced, the defense retained substantial robustness, achieving adversarial accuracy exceeding 20%. Furthermore, human perception tests demonstrated that successful bounded attacks often induced semantic shifts aligned with model errors (e.g., classifying a "House" as "Road"), indicating that breaking the model under strict constraints requires creating perceptible ambiguity rather than invisible noise.

---

## Key Findings

*   **Critical Implementation Bug:** Recent adaptive attacks against multi-resolution self-ensembles contained a significant error allowing perturbations to exceed the standard $L_\infty$ bound by a factor of 20.
*   **Retained Robustness:** The defense retains non-trivial robustness when attacks are properly constrained to their intended mathematical limits.
*   **Perception Alignment:** Properly bounded attacks against strong defenses tend to align with human perception (semantic changes) rather than being purely imperceptible noise.
*   **Standard Re-evaluation:** Current standards for measuring adversarial robustness may require reconsideration to account for the reality that breaking robust models often involves perceptible attacks.

---

## Methodology

The study utilized a three-pronged approach to verify and correct the scientific record:

1.  **Implementation Auditing:** Prior code was audited specifically to verify adherence to the stated $L_\infty$ constraints.
2.  **Corrected Re-evaluation:** Experiments were rerun strictly enforcing perturbation bounds via gradient steps, clamping, and reconstruction.
3.  **Perceptual Analysis:** Qualitative analysis and human perception tests (conducted at EPFL) were performed to assess semantic changes in adversarial examples.

---

## Technical Details

| Component | Description |
| :--- | :--- |
| **Defense Architecture** | Multi-resolution self-ensemble based on **ResNet152** (strong configuration utilizing all 54 layers). |
| **Attack Implementation** | Projected Gradient Descent (PGD) based adaptive attack. |
| **The Bug** | The original implementation used a multi-round process where perturbations accumulated additively relative to the *previous* perturbed image, causing linear accumulation ($L_\infty(n) \approx 8n/255$). |
| **The Fix** | Enforced a strict $L_\infty$ bound relative to the *original* image $X$ at every iteration. |
| **Experimental Setup** | Dataset: **CIFAR-100**; Bound: $L_\infty = 8/255$. Human perception testing conducted at EPFL. |

---

## Results

*   **Implementation Error Impact:** The buggy code allowed perturbations to reach $L_\infty = 160/255$, exceeding the limit by **20x** due to linear accumulation.
*   **Robustness Metrics:**
    *   *Bugged Attack:* Accuracy was near-zero.
    *   *Corrected Attack:* Robustness increased to **>20% adversarial accuracy**.
*   **Human Perception:** Successful bounded attacks often changed human perception to align with model errors (e.g., a "House" being classified as a "Road"). This suggests that effective attacks within strict bounds reflect perceptual ambiguity rather than imperceptible noise.

---

## Contributions

*   **Record Correction:** Corrected the record regarding the vulnerability of multi-resolution self-ensembles by exposing specific implementation errors in recent adaptive attacks.
*   **Validation:** Re-established the empirical robustness of multi-resolution self-ensemble defenses against white-box threats.
*   **Community Standard:** Highlighted the necessity of careful validation and constraint checking in adversarial machine learning research.
*   **Theoretical Insight:** Provided evidence that breaking robust models with bounded attacks often requires inducing perceptible, semantic changes, challenging current definitions of "imperceptible" adversarial examples.

---

**References:** 2 citations