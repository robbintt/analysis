# Towards Reliable Evaluation of Adversarial Robustness for Spiking Neural Networks

*Jihang Wang; Dongcheng Zhao; Ruolin Chen; Qian Zhang; Yi Zeng*

> ### ðŸ“Š Quick Facts
> *   **Quality Score:** 9/10
> *   **Total Citations:** 40
> *   **Core Problem:** Vanishing gradients in SNNs due to binary spike activations.
> *   **Proposed Solution:** ASSG & SA-PGD framework.
> *   **Performance Impact:** ~2x Runtime & ~1.4x Memory overhead vs. STBP.
> *   **Dataset:** CIFAR-10.

---

## Executive Summary

Spiking Neural Networks (SNNs) offer energy-efficient computing but face significant challenges in adversarial robustness evaluation due to the binary and discontinuous nature of their spike activations. This non-differentiability forces reliance on surrogate gradient methods, which this study identifies as fundamentally unreliable under strong adversarial constraints. The authors demonstrate that standard gradient descent techniques often suffer from vanishing gradients in this context, leading to a critical overestimation of SNN robustness in prior literature. Consequently, the security claims made for existing SNN architectures are suspect, highlighting the urgent need for evaluation methods that can accurately measure vulnerability against sophisticated attacks.

To address the unreliability of current evaluation metrics, the researchers propose a dual-component framework designed to overcome gradient vanishing and ensure stability. The first component, the **Adaptive Sharpness Surrogate Gradient (ASSG)**, dynamically evolves the shape of the surrogate gradient function during attack iterations based on the input distribution. This adaptation maintains gradient fidelity where standard methods fail. The second component, **Stable Adaptive Projected Gradient Descent (SA-PGD)**, is an optimization algorithm operating under L-infinity constraints. It employs an adaptive step size mechanism to achieve stable convergence despite the imperfections inherent in spiking neuron gradients, applicable to models such as LIF, IF, and PSN.

Experimental results validate that the proposed framework significantly outperforms existing surrogate gradient methods, substantially increasing Attack Success Rates (ASR) across diverse SNN architectures and adversarial training schemes (including TRADES and RAT). This success confirms that previous benchmarks severely overestimated model robustness. However, this improved reliability comes with a computational cost. On the CIFAR-10 dataset, ASSG required approximately 15.4 seconds of runtime and 13,577 MB of memory per epochâ€”roughly 2x the runtime and 1.4x the memory of standard Spatiotemporal Backpropagation (STBP)â€”indicating a tangible trade-off between evaluation precision and resource efficiency.

This work establishes a theoretical bound regarding gradient vanishing in surrogate gradients, providing a mathematical foundation for why current evaluations fail. By developing a generalized, reliable evaluation framework, the authors challenge established security claims in the field and set a new standard for future research. The findings compel the community to re-evaluate the robustness of existing SNNs and drive the development of more dependable adversarial training methods, ultimately advancing the security and deployment readiness of neuromorphic computing systems.

---

## Key Findings

*   **Evaluation Unreliability:** The binary and discontinuous nature of spike activations in Spiking Neural Networks (SNNs) causes vanishing gradients, rendering standard gradient descent methods unreliable for evaluating adversarial robustness.
*   **Limitations of Current Methods:** Existing surrogate gradient methods lack effectiveness when subjected to strong adversarial attacks.
*   **Overestimation of Robustness:** Experimental results reveal that the adversarial robustness of current SNNs has been significantly overestimated in prior research.
*   **Superior Attack Performance:** The proposed approach substantially increases attack success rates across various adversarial training schemes, SNN architectures, and neuron models.

## Methodology

The researchers propose a dual-component framework designed to improve the reliability of adversarial robustness evaluation for SNNs:

1.  **Adaptive Sharpness Surrogate Gradient (ASSG):**
    *   Adaptively evolves the shape of the surrogate gradient function during attack iterations.
    *   Mitigates vanishing gradients by adjusting based on input distribution.

2.  **Stable Adaptive Projected Gradient Descent (SA-PGD):**
    *   An adversarial attack algorithm operating under the L-infinity constraint.
    *   Utilizes an adaptive step size mechanism to achieve faster and more stable convergence.

## Key Contributions

*   **Theoretical Analysis:** Provided a theoretical analysis regarding the degree of gradient vanishing in surrogate gradients, establishing why current evaluations fail.
*   **Algorithmic Innovation (ASSG):** Introduced the Adaptive Sharpness Surrogate Gradient (ASSG), a novel method that dynamically adjusts surrogate function shapes to maintain gradient fidelity.
*   **Algorithmic Innovation (SA-PGD):** Developed the Stable Adaptive Projected Gradient Descent (SA-PGD), an optimization strategy that ensures stable convergence despite gradient imperfections.
*   **Standardized Reliable Evaluation:** Established a generalized and more reliable evaluation framework that challenges existing security claims and highlights the need for more dependable adversarial training methods.

## Technical Details

### Mathematical Bound
The approach utilizes a mathematical bound on the surrogate gradient $G(E[x])$ satisfying $G(E[x]) \leq A$, derived via the condition:
$$ \alpha \leq \frac{2}{\pi E[x]} \tan\left(\frac{\pi A}{2}\right) $$

### Spiking Neuron Models
*   **LIF-2:** Dynamics defined by $V^l(t+1) = \lambda R(V^l(t), s^l(t)) + W^l s^{l-1}(t+1)$ (hard reset without $(1-\lambda)$ normalization).
*   **IF:** Defined by $\lambda=1$.
*   **PSN:** Parallel temporal processing with membrane potential $V^l = W^{l T} X^l$ and learnable firing threshold $B^l$.

### Training & Attack Configuration
*   **Training Schemes:** Standard AT, TRADES, RAT, and AT+SR.
*   **Optimizer:** SGD with momentum 0.9 for 100 epochs.
*   **Regularization:** Coefficients for RAT/AT+SR set to 0.004.
*   **Gradients:**
    *   Non-PSN models: Triangular surrogate gradients ($\frac{dH}{dx} \approx 2 \cdot \max\{0, 1 - |x|\}$).
    *   PSN models: Atan gradients ($\alpha=4$).
*   **Attack Config:** PGD with $\epsilon=8/255$, step size $4/255$, and 5 iterations.

## Results

*   **Robustness Overestimation:** Research reveals that previous studies overestimated the adversarial robustness of SNNs.
*   **Validation of ASSG:** The proposed ASSG approach significantly increases Attack Success Rates (ASR) across architectures, validating the failure of standard surrogate gradients under strong attacks.
*   **Sensitivity Analysis:** Indicates ASR varies with surrogate gradient parameters.
*   **Computational Analysis (CIFAR-10):**
    *   **ASSG:** 15.4s runtime and 13,577 MB memory per epoch.
    *   **Standard STBP:** 11.3s runtime and 9473 MB memory per epoch.
    *   **Trade-off:** Indicates superior attack performance at the cost of significant computational overhead (roughly 2x runtime, 1.4x memory).

---
**Quality Score:** 9/10 | **References:** 40 citations