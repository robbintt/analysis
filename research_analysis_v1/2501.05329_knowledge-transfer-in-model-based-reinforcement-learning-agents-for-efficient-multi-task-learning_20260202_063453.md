# Knowledge Transfer in Model-Based Reinforcement Learning Agents for Efficient Multi-Task Learning
*Dmytro Kuzmenko; Nadiya Shvai*

---

> ### ðŸ“Š Quick Facts
> | Metric | Value |
> | :--- | :--- |
> | **Benchmark** | MT30 |
> | **SOTA Score** | **28.45** (Baseline: 18.93) |
> | **Compression Ratio** | ~317x (317M â†’ 1M params) |
> | **Size Reduction** | 50% (via FP16 Quantization) |
> | **Performance Gain** | +2.77% over scratch training |

---

## Executive Summary

This paper addresses the critical deployment gap inherent in large-scale, multi-task reinforcement learning (RL) agents. While high-capacity world models, often exceeding 300 million parameters, have demonstrated the ability to generalize across complex environments, their substantial computational and memory footprints render them impractical for deployment on resource-constrained hardware such as robotics platforms. 

The research focuses on the challenge of maintaining the diverse task-handling capabilities of these large agentsâ€”specifically across the MT30 benchmarkâ€”while drastically compressing their size to enable real-world, edge-based applications. The authors introduce a knowledge distillation framework tailored for model-based reinforcement learning (MBRL), utilizing the TD-MPC2 architecture. 

The core technical innovation involves a teacher-student setup where a high-capacity "teacher" agent (317M parameters) transfers knowledge to a compact "student" model (1M parameters). Instead of distilling feature maps or latent states (which failed due to dimensionality mismatches), the method employs a Reward Distillation Loss using Mean Squared Error (MSE) between the teacher and student reward predictions, combined with the original TD-MPC2 loss. Additionally, the study applies FP16 post-training quantization to achieve further efficiency gains without significant accuracy degradation.

The distilled 1M parameter student model achieved a state-of-the-art normalized score of 28.45 on the MT30 benchmark, significantly surpassing the baseline score of 18.93 and outperforming models trained from scratch by +2.77%. This research establishes a new standard for the efficiency-performance trade-off in model-based reinforcement learning by proving that complex multi-task knowledge can be effectively consolidated into ultra-compact architectures.

---

## Key Findings

*   **State-of-the-Art Performance:** The distilled 1M parameter model achieved a normalized score of **28.45** on the MT30 benchmark, significantly surpassing the baseline of **18.93**.
*   **Extreme Compression:** Successfully compressed a 317M parameter multi-task agent into a compact 1M parameter model without sacrificing task handling capabilities.
*   **Efficient Quantization:** FP16 post-training quantization reduced model size by **50%** while maintaining performance stability.
*   **Effective Consolidation:** The study demonstrates that complex multi-task knowledge can be effectively consolidated into resource-efficient architectures for edge deployment.

---

## Methodology

The researchers utilized a knowledge distillation framework specifically designed for model-based reinforcement learning (MBRL).

*   **Architecture:** Implemented a Teacher-Student framework built on the **TD-MPC2** architecture.
*   **Process:** A high-capacity 'teacher' agent (**317M parameters**) was trained to serve as the knowledge source. This knowledge was then transferred and compressed into a smaller 'student' model (**1M parameters**).
*   **Optimization:** Additionally, **FP16 post-training quantization** was applied to the student model to further reduce the memory footprint.

---

## Technical Details

The implementation focused on overcoming significant architectural differences between the teacher and student models.

*   **Compression:** Achieved a ~**317x parameter reduction** (317M â†’ 1M).
*   **Loss Function:** Combined the original TD-MPC2 loss with a **Reward Distillation Loss** (MSE between teacher and student rewards).
*   **Distillation Challenges:** Attempts at next-state latent distillation failed due to dimension mismatch (Teacher: 1376-dim vs. Student: 128-dim).
*   **Training Configuration:**
    *   **Dataset:** MT30 (690,000 episodes).
    *   **Batch Sizes:** Ranged from 128 to 1024.
    *   **Optimization:** FP16 quantization employed post-training.

---

## Results

*   **Benchmark Dominance:** The FP16-quantized student model achieved a State-of-the-Art (SOTA) normalized score of **28.45**, outperforming models trained from scratch by **+2.77%**.
*   **Learning Velocity:** Distillation provided significant short-term gains (**+27.1%** at 200k steps) and maintained a lead at 1M steps.
*   **Hyperparameter Tuning:** The optimal distillation coefficient was identified as **0.4**.
*   **Teacher Capacity Impact:** Using a larger teacher model (317M) resulted in a **31.2%** gain over using a smaller teacher (48M).
*   **Distillation Strategy Comparison:** Reward-only distillation (score: 14.04) vastly outperformed failed latent distillation methods (Linear: 7.69, PCA: 8.78).

---

## Research Contributions

This work bridges the deployment gap for large world models in resource-constrained environments by:

1.  **Providing a Deployment Solution:** Enabling the deployment of large world models on hardware with limited computational resources.
2.  **Scalability:** Offering a scalable method for efficient multi-task reinforcement learning, making advanced agents accessible for robotics.
3.  **Setting New Standards:** Achieving a new state-of-the-art on the MT30 benchmark with a compact model, redefining the efficiency-performance trade-off.

---

**Paper Quality Score:** 8/10  
**References:** 9 citations