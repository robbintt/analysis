---
title: 'MermaidSeqBench: An Evaluation Benchmark for LLM-to-Mermaid Sequence Diagram
  Generation'
arxiv_id: '2511.14967'
source_url: https://arxiv.org/abs/2511.14967
generated_at: '2026-01-28T01:05:42'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# MermaidSeqBench: An Evaluation Benchmark for LLM-to-Mermaid Sequence Diagram Generation

*Chad De, Farhan Ahmed, Basel Shbita*

---

> ### **Quick Facts**
> * **Dataset Size:** 132 samples
> * **Models Evaluated:** 6 (Qwen 2.5, Llama 3.1/3.2, Granite 3.3)
> * **Evaluation Method:** LLM-as-a-judge (DeepSeek-V3, GPT-OSS)
> * **Quality Score:** 8/10
> * **Key Metrics:** Syntax correctness, Activation handling, Error handling, Practical usability

---

## Executive Summary

**Problem**
As Large Language Models (LLMs) increasingly assume roles in software engineering and technical documentation, their ability to generate structured visual artifacts like Unified Modeling Language (UML) sequence diagrams is becoming essential. However, state-of-the-art models currently struggle with the reliability and correctness required for generating valid Mermaid diagram code. A critical barrier to improving these capabilities is the absence of systematic benchmarks and rigorous evaluation methodologies. Without standardized metrics, developers cannot accurately assess model performance in terms of syntax, logic flow, and error handling, leaving a gap in the understanding of how well LLMs can handle structured diagrammatic tasks.

**Innovation**
This paper introduces **MermaidSeqBench**, the first specialized, human-verified benchmark specifically designed to evaluate LLM performance in generating Mermaid sequence diagrams. Technically, the dataset is constructed using a robust three-stage pipeline: it begins with the manual creation of seed diagrams by subject matter experts, followed by scalable synthetic data generation using Mistral Large (123B), and concludes with rule-based variations to increase dataset diversity. This process resulted in a dataset of 132 publicly available samples. Additionally, the authors established a standardized evaluation protocol utilizing an "LLM-as-a-judge" paradigm. This approach employs advanced judge models—specifically DeepSeek-V3 and GPT-OSS—to automate assessment across four fine-grained technical metrics: syntax correctness, activation handling, error handling, and practical usability.

**Results**
The researchers evaluated six leading open-source models, including Qwen 2.5 (0.5B, 7B), Llama 3.1/3.2 (1B, 8B), and Granite 3.3 (2B, 8B), establishing a comprehensive baseline for future comparison. The evaluation revealed substantial performance disparities and distinct capability gaps among these architectures. Specifically, models exhibited significant weaknesses in complex areas such as activation handling and error management, rather than just basic syntax errors. Furthermore, the study validated the efficacy of the LLM-as-a-judge methodology, confirming that automated judge models provide a flexible, scalable, and accurate means of assessing structured diagram generation compared to traditional manual verification.

**Impact**
MermaidSeqBench provides a foundational resource for the field of structured generation, addressing the critical lack of rigorous evaluation tools for visual code synthesis. By exposing specific fine-grained weaknesses in current LLMs, this work guides future research toward improving syntactic precision and logical coherence in model outputs. The release of the benchmark and the standardized evaluation protocol offers the research community a reliable means to track progress and compare model performance, ultimately advancing the reliability of LLMs in generating complex, usable technical diagrams and structured code.

---

## Key Findings

*   **Significant Performance Gaps:** Evaluations reveal substantial performance disparities among state-of-the-art LLMs when generating Mermaid sequence diagrams, indicating reliability struggles.
*   **Methodology Void:** There is a critical lack of systematic benchmarks and rigorous evaluation methodologies for assessing LLM correctness in generating structured diagrams.
*   **Effective Assessment:** The study demonstrates that using a dedicated LLM judge model is a flexible and effective method for assessing diagram generation.
*   **Specific Weaknesses:** Models exhibit distinct weaknesses in fine-grained metrics such as syntax correctness, activation handling, error handling, and practical usability.

---

## Methodology

The researchers employed a hybrid methodology to construct **MermaidSeqBench**, a dataset consisting of 132 samples. The construction process utilized four primary techniques:

1.  **Manual Crafting**
2.  **Human Annotation**
3.  **In-context LLM Prompting**
4.  **Rule-based Variation**

To assess performance, the team utilized an **"LLM-as-a-judge"** approach. This automated assessment method relies on fine-grained technical metrics (syntax correctness, activation handling, error handling, and practical usability). Validation of the results involved a dual process of human verification and LLM-synthetic extension to ensure robustness.

---

## Technical Details

### Dataset Construction Pipeline
The MermaidSeqBench dataset was built using a three-stage pipeline designed to balance quality and scalability:

*   **Stage 1 (Seed Creation):** Manual creation of 10 seed diagrams by a Subject Matter Expert.
*   **Stage 2 (Synthetic Expansion):** Utilization of Scalable Synthetic Data Generation (SDG) with Mistral Large (123B) to expand the set. This stage included the selection and verification of 30 samples.
*   **Stage 3 (Variation):** Application of rule-based variations, specifically control flow structuring and participant normalization, to increase the dataset size fourfold, reaching the final 132 samples.

### Evaluation Framework
The study employs an 'LLM-as-a-judge' paradigm with the following specifications:

*   **Judge Models:** DeepSeek-V3 (671B) and GPT-OSS (120B).
*   **Evaluation Metrics:**
    *   Syntax Correctness
    *   Activation Handling
    *   Error Handling
    *   Practical Usability

---

## Results

The public benchmark consists of **132 samples** against which six models were evaluated:
*   Qwen 2.5 (0.5B, 7B)
*   Llama 3.1/3.2 (1B, 8B)
*   Granite 3.3 (2B, 8B)

The findings revealed substantial performance disparities and capability gaps among state-of-the-art LLMs in generating correct Mermaid sequence diagrams. Weaknesses were specifically noted in syntax, activation handling, error handling, and practical usability. The study successfully validated the 'LLM-as-a-judge' methodology as a flexible and effective assessment technique for this domain.

---

## Contributions

1.  **MermaidSeqBench:** Introduction of the first specialized, human-verified benchmark for evaluating LLM performance in generating Mermaid sequence diagrams.
2.  **Standardized Evaluation Protocol:** Establishment of a rigorous, fine-grained evaluation framework using LLM-as-a-judge models.
3.  **Comprehensive Baseline Analysis:** Provision of initial evaluation results on state-of-the-art LLMs to serve as a baseline for future research.
4.  **Advancement of Structured Generation:** Provision of a foundational resource to advance the field of structured diagram generation and evaluation methodologies.

---
**Paper Quality Score:** 8/10 | **References:** 40 citations