---
title: Agent-Initiated Interaction in Phone UI Automation
arxiv_id: '2503.19537'
source_url: https://arxiv.org/abs/2503.19537
generated_at: '2026-02-03T12:43:32'
quality_score: 7
citation_count: 29
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Agent-Initiated Interaction in Phone UI Automation

*Noam Kahlon; Guy Rom; Anatoly Efros; Filippo Galgani; Omri Berkovitch; Sapir Caduri; William E. Bishop; Oriana Riva; Ido Dagan*

---

> ### ðŸ“Š Quick Facts
>
> *   **Quality Score:** 7/10
> *   **Total Citations:** 29
> *   **Dataset Introduced:** AndroidInteraction
> *   **Base Dataset:** AndroidControl
> *   **Key Challenge:** Balancing autonomy with preference violation prevention

---

## Executive Summary

Fully autonomous phone UI agents frequently fail to complete complex tasks due to an inability to navigate user preferences and ambiguous decision points. The central challenge is **"agent-initiated interaction"**: determining precisely when an agent should interrupt a workflow to request clarification. If agents execute instructions blindly, they risk **"preference violations"**â€”actions contrary to user desiresâ€”yet asking for confirmation at every step creates excessive friction. This research addresses the critical need to balance task completion efficiency with user alignment, a prerequisite for trustworthy automation.

The authors introduce **"AndroidInteraction,"** a novel dataset derived from AndroidControl, designed to benchmark agent-initiated communication. The technical innovation lies in formulating the automation task as a step-wise decision problem where the agent processes natural language instructions alongside dual-modal inputs: **screenshots and accessibility trees**. To simulate real-world ambiguity, the dataset incorporates LLM-generated personas, forcing agents to discern whether a task requires subjective human input or objective execution. The system defines a rigorous action space (tapping, swiping, text input) and assumes UI proficiency, creating a comprehensive framework for testing human-in-the-loop capabilities.

The study evaluated several text-based and multimodal baseline models using simulation environments and static gold datasets to assess current capabilities. The empirical analysis demonstrated that detecting the need for user interaction and generating contextually appropriate messages is **exceptionally challenging** for current state-of-the-art LLMs. The results indicate a clear struggle in balancing proactivity with autonomy, as models frequently failed to distinguish between situations that require user input and those that can be resolved programmatically.

This research provides a theoretical and empirical framework for shifting phone UI automation from purely execution-based models to human-in-the-loop systems. By releasing the AndroidInteraction dataset and defining specific parameters for interaction timing and autonomy, the authors offer the community a vital tool for developing agents that are not only capable but also personalized and trustworthy. This work establishes a benchmark for future research in "socially aware" automation, ensuring that future agents can navigate complex, ambiguous user environments effectively by knowing precisely when to engage the user.

---

## Key Findings

*   Complex phone automation tasks frequently necessitate user interaction to ensure successful completion and align with user expectations.
*   Effective agents must balance proactively engaging the user to prevent preference violations while refraining from asking unnecessary questions.
*   The task of detecting the need for user interaction and generating appropriate messages is very challenging for current state-of-the-art Large Language Models (LLMs).

---

## Methodology

The authors rigorously defined the specific task of agent-initiated interaction, establishing parameters for interaction timing and autonomy. They derived annotation guidelines and leveraged an existing UI automation dataset to create the **'AndroidInteraction' dataset**. Finally, they tested and analyzed the performance of several text-based and multimodal baseline models on this new dataset to assess current capabilities.

---

## Contributions

*   A comprehensive formulation of the agent-initiated interaction problem within the context of phone UI automation.
*   The creation of the 'AndroidInteraction' dataset via specific annotation guidelines to support research in this domain.
*   An analysis of baseline models that establishes a performance benchmark and highlights the technical difficulties current LLMs face with this task.
*   A theoretical and empirical framework aimed at enabling future agents to provide personalized, trustworthy experiences by knowing precisely when and how to engage the user.

---

## Technical Details

*   **Perception Approach:** Dual-modal perception utilizing screenshots and accessibility trees.
*   **Input Modality:** Natural language instructions combined with starting screen states.
*   **Action Space:**
    *   Tapping at coordinates
    *   Swiping
    *   Text input
*   **Task Framing:** Step-wise decision problem to detect the need for user contact and generate messages.
*   **Processing Style:** Execution-centric, sequential processing; assumes UI proficiency.
*   **Dataset Construction:**
    *   Built on **AndroidControl**.
    *   Augmented with interaction annotations.
    *   Includes LLM-generated personas to simulate ambiguity.

---

## Results

The provided text does not contain specific experimental results, quantitative metrics, or evaluation benchmarks. It notes that detecting the need for user interaction and generating appropriate messages is very challenging for current state-of-the-art LLMs. Evaluation contexts mentioned include simulation environments and static gold datasets.