---
title: 'Vision-DeepResearch Benchmark: Rethinking Visual and Textual Search for Multimodal
  Large Language Models'
arxiv_id: '2602.02185'
source_url: https://arxiv.org/abs/2602.02185
generated_at: '2026-02-06T06:23:46'
quality_score: 8
citation_count: 12
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Vision-DeepResearch Benchmark: Rethinking Visual and Textual Search for Multimodal Large Language Models

*Yu Zeng; Wenxuan Huang; Zhen Fang; Shuang Chen; Yufan Shen; Yishuo Cai; Xiaoman Wang; Zhenfei Yin; Lin Chen; Zehui Chen; Shiting Huang; Yiming Zhao; Yao Hu; Philip Torr; Wanli Ouyang; Shaosheng Cao*

---

> ### **Quick Facts**
> * **Dataset:** VDR-Bench (2,000 VQA instances)
> * **Evaluation Focus:** Entity-level Recall & Overall Answer Accuracy
> * **Top Performing Improvement:** Gemini 2.5 Pro (16.2 → 30.0 score)
> * **Relative Performance Gain:** 85%
> * **Key Innovation:** Multi-round cropped-search workflow (CIS+TS+MVF)
> * **Quality Score:** 8/10

---

## Executive Summary

Current benchmarks for evaluating Vision-DeepResearch systems suffer from critical flaws that inflate model performance while masking genuine deficiencies. Existing evaluations are not visual search-centric, often allowing models to "cheat" by relying on text cues or prior knowledge rather than performing actual visual retrieval. Additionally, these benchmarks rely on overly idealized scenarios, such as near-exact image matching or direct text queries, which do not reflect the complexity of real-world applications. This is a significant issue because it obscures the fact that current Multimodal Large Language Models (MLLMs) possess insufficient visual retrieval capabilities, limiting their effectiveness in complex scenarios where visual evidence is paramount.

The study introduces two key innovations to address these limitations. First, the authors present VDR-Bench, a rigorous dataset of 2,000 VQA instances curated through a multi-stage pipeline and expert review. Unlike previous efforts, VDR-Bench ensures "visual necessity," eliminating data leakage and forcing models to rely on genuine visual analysis. Second, to overcome the retrieval weaknesses identified in current MLLMs, the authors propose a "multi-round cropped-search workflow." This technical solution employs multi-scale cropping and entity-level verification to iteratively interact with search engines. The workflow aggregates cross-modal evidence using two modes: a baseline (CIS+TS) and an advanced version (CIS+TS+MVF) that adds Multimodal Visual Fusion for improved grounding.

The study evaluated both proprietary models (Gemini 2.5 Pro, GPT-5, Claude-4-Sonnet) and open-weight models (Qwen3-VL) using Entity-level Recall (%) and Overall Answer Accuracy. The results demonstrated a strong positive correlation between entity-level recall and answer accuracy, confirming that better retrieval directly improves reasoning. The proposed multi-round cropped-search strategy (CIS+TS+MVF) yielded substantial performance gains across all tested models. Most notably, Gemini 2.5 Pro improved its score from 16.2 to 30.0, representing an 85% relative improvement. The CIS+TS+MVF strategy consistently outperformed the baseline in both recall and accuracy metrics.

This research significantly impacts the field by redefining evaluation standards for multimodal systems, moving away from idealized, leakage-prone datasets toward robust, real-world assessments. The release of VDR-Bench provides the research community with a high-quality resource specifically designed to test deep-research capabilities without bias. Furthermore, the multi-round cropped-search workflow offers a practical, technical solution for enhancing visual retrieval, providing actionable insights and architectural guidance for the development of future multimodal systems that require complex reasoning and verification.

---

## Key Findings

*   **Limitations of Current Benchmarks:** Existing benchmarks for Vision-DeepResearch systems suffer from two critical flaws: they are not visual search-centric (answers leak via text cues or prior knowledge) and they rely on overly idealized scenarios (near-exact image matching or direct text queries).
*   **VDR-Bench Construction:** The study introduces the Vision-DeepResearch benchmark (VDR-Bench), a dataset of 2,000 VQA instances curated via a multi-stage pipeline and expert review to evaluate systems under realistic, challenging conditions.
*   **Retrieval Deficiencies in MLLMs:** Current Multimodal Large Language Models (MLLMs) possess insufficient visual retrieval capabilities, which hinders their performance in complex, real-world visual search scenarios.
*   **Efficacy of Cropped-Search Workflow:** The proposed multi-round cropped-search workflow effectively mitigates retrieval issues and significantly improves model performance in realistic visual retrieval tasks.

---

## Methodology

The research employs a two-pronged methodological approach:

**1. Benchmark Development**
The authors constructed the VDR-Bench, a comprehensive dataset of 2,000 VQA instances. The creation process utilized a careful, multi-stage curation pipeline followed by rigorous expert review. This was designed specifically to eliminate data leakage and ensure that evaluation metrics assess systems under realistic, real-world conditions rather than idealized ones.

**2. Workflow Proposal**
To address the identified visual retrieval weaknesses in current MLLMs, the authors proposed and implemented a "multi-round cropped-search workflow." This strategy was tested to demonstrate its effectiveness in improving visual retrieval performance in complex scenarios.

---

## Technical Details

The study introduces VDR-Bench, a benchmark of 2,000 VQA instances curated via a multi-stage pipeline and expert review to ensure 'visual necessity' and prevent text-only shortcuts.

### Architecture & Workflow
*   **Mechanism:** Employs multi-scale cropping, entity-level verification, and knowledge-graph–based multi-hop reasoning.
*   **Objective:** Mitigate retrieval deficiencies in Multimodal Large Language Models (MLLMs).
*   **Process:** Proposes a multi-round cropped-search workflow involving iterative interaction with search engines to aggregate cross-modal evidence.

### Search Modes
The workflow utilizes two distinct search modes to optimize performance:
*   **CIS+TS (Baseline):** The standard configuration.
*   **CIS+TS+MVF (Advanced):** Adds Multimodal Visual Fusion for improved grounding.

---

## Results

*   **Evaluation Metrics:** Entity-level Recall (%) and Overall Answer Accuracy.
*   **Models Tested:**
    *   **Proprietary:** Gemini 2.5 Pro, GPT-5, Claude-4-Sonnet
    *   **Open-Weight:** Qwen3-VL-30B, Qwen3-VL-235B
*   **Correlation:** A strong positive correlation was found between entity-level recall and answer accuracy.
*   **Performance Gains:** The proposed MVC/MVF multi-round cropped-search strategy yielded significant performance gains across all models.
    *   **Gemini 2.5 Pro:** Improved from **16.2 to 30.0** (an 85% relative improvement).
*   **Strategy Comparison:** The CIS+TS+MVF strategy consistently outperformed the CIS+TS baseline in both recall and accuracy.

---

## Contributions

*   **Redefining Evaluation Standards:** The paper establishes a more rigorous standard for evaluating Vision-DeepResearch systems by explicitly addressing the issues of text leakage and idealized evaluation scenarios found in previous benchmarks.
*   **High-Quality Resource Release:** The contribution of VDR-Bench provides the research community with a robust, realistic dataset (2,000 instances) specifically designed to test deep-research capabilities in multimodal models.
*   **Technical Innovation in Retrieval:** The introduction of the multi-round cropped-search workflow offers a practical, technical solution to enhance the visual retrieval capabilities of MLLMs, moving beyond simple direct matching.
*   **Guidance for Future System Design:** The study provides actionable insights and practical guidance for the architecture and design of future multimodal deep-research systems.

---

**Quality Score:** 8/10
**References:** 12 citations