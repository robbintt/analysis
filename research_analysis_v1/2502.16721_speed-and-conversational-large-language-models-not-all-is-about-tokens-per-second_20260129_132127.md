# Speed and Conversational Large Language Models: Not All Is About Tokens per Second
*Javier Conde; Miguel GonzÃ¡lez; Pedro Reviriego; Zhen Gao; Shanshan Liu; Fabrizio Lombardi*

---

> ### ðŸ“Š Quick Facts
>
> *   **Subject:** Evaluation Metrics for Chat LLMs
> *   **Models Analyzed:** 5 (6Bâ€“8B parameters)
> *   **Hardware Environment:** NVIDIA A100 (40GB VRAM)
> *   **Primary Format:** FP16
> *   **Key Metric Critiqued:** Tokens Per Second (TPS)
> *   **Proposed Focus:** Task-based latency & Hardware-specific optimization
> *   **Quality Score:** 7/10

---

## Executive Summary

The industry standard metric **"Tokens Per Second" (TPS)** is fundamentally insufficient for evaluating conversational Large Language Models (LLMs). While TPS effectively measures text generation speed, it ignores the critical input prompt processing (prefill) phase. This oversight results in inaccurate assessments of total latency and efficiency, often misleading engineers regarding the actual responsiveness of deployed systems.

Because conversational AI relies heavily on the interplay between prompt processing and output generation, a single-metric approach fails to capture the true performance bottlenecks inherent in real-world applications. To address this, the authors introduced a rigorous **task-based evaluation framework** that distinguishes between prefill and decode phases to isolate performance bottlenecks.

The study tested five specific open-weights modelsâ€”Llama-2, Llama-3, Mistral, Yi, and Gemmaâ€”on a single NVIDIA A100 GPU. The results demonstrated that model speed is not static; it varies significantly depending on the specific task, causing frequent ranking reversals when compared to traditional TPS leaderboards. Furthermore, data indicated that optimization is highly hardware-specific, compelling the AI community to adopt comprehensive, multi-stage evaluation standards rather than relying on synthetic throughput scores.

---

## Key Findings

*   **Task-Dependent Speed:** The speed of open-weights LLMs varies significantly depending on the specific task being performed, meaning no single model is "fastest" across all use cases.
*   **Inadequacy of TPS:** Relying solely on 'tokens per second' is not a sufficient metric for evaluating chat LLM speeds, as it overlooks prompt processing latency.
*   **Hardware Specificity:** Variations in speed observed in GPU environments suggest that optimization is likely hardware-specific; tuning for one environment does not guarantee universal performance.
*   **Ranking Reversals:** When evaluated based on task latency rather than raw TPS, model rankings frequently reverse, highlighting the danger of synthetic benchmarking.

---

## Methodology

The study was conducted to measure practical inference speeds by running models on GPUs. Rather than using isolated synthetic benchmarks, the researchers utilized a **comparative framework** to evaluate popular 'open-weights' LLMs against one another.

*   **Approach:** Task-based evaluation focusing on real-world prompt generation.
*   **Environment:** Controlled GPU runs to ensure consistency.
*   **Strategy:** Emphasis on batch processing to handle multiple prompts simultaneously, specifically addressing GPU underutilization during single-prompt inference.

---

## Technical Details

The study maintained strict controls to ensure the validity of the comparison.

| Parameter | Specification |
| :--- | :--- |
| **Models Evaluated** | Llama-2-7b-chat-hf<br>Meta-Llama-3-8B-Instruct<br>Mistral-7B-Instruct-v0.1<br>Yi-6B-Chat<br>gemma-7b-it |
| **Parameter Range** | 6B to 8B |
| **Precision Format** | 16-bit floating-point (FP16) |
| **Hardware** | Single NVIDIA A100 GPU (40GB VRAM) |
| **Optimization** | Batch processing to maximize GPU utilization |
| **Resource Constraints** | Larger models (e.g., Llama-70B) require 4-bit quantization for similar hardware, whereas 6B-8B models fit comfortably at FP16. |

---

## Results & Contributions

### Results
The evaluation utilized **Tokens Per Second (TPS)** and **latency (time to generate 256 tokens)** as primary metrics. While the authors critique these metrics, they used them to demonstrate that:
*   TPS is an unreliable predictor of deployment performance.
*   Specific latency metrics (like 256-token generation) provide better insight into user experience.
*   Hardware constraints play a major role; 6B-8B models operate efficiently at FP16 on 40GB VRAM, but scaling up requires significant trade-offs like quantization.

### Contributions
*   **Comparative Analysis:** Provided a detailed comparative analysis of speed regarding the most widely used open-weights LLMs.
*   **Performance Framework:** Broadened the understanding of LLM performance by highlighting the relationship between inference speed and specific task execution.
*   **Paradigm Shift:** Urged a shift away from single-metric evaluations (like TPS) toward more comprehensive, multi-stage standards that account for task complexity.

---

## References
*   **0 Citations** (Based on the provided analysis text)