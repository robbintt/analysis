---
title: Distill-LLama-8B and DeepSeek-R1-Distill-
arxiv_id: '2505.15508'
source_url: https://arxiv.org/abs/2505.15508
generated_at: '2026-01-27T16:26:46'
quality_score: 4
citation_count: 17
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# Distill-LLama-8B and DeepSeek-R1-Distill-

*Initial Thought, Thought Transfer, Time Token, Tanmoy Chakraborty, Time Scaling, Prasoon Bajpai, Multilingual Ini, Large Language, Multilingual Test, Generation Length*

---

> ### üìä Quick Facts
>
> *   **Models Evaluated:** DeepSeek-R1-Distill-Llama-8B, DeepSeek-R1-Distill-Qwen-7B
> *   **Dataset:** Multilingual AIME 2025 (30 Questions)
> *   **Languages:** English, Italian, German, Portuguese, Vietnamese, Tagalog
> *   **Token Limit:** 10,000 (Max Reasoning Trace)
> *   **Analysis Window:** 32-token chunks, down-sampled to 20 segments
> *   **Citations:** 17 references

---

## üóûÔ∏è Executive Summary

This paper investigates test-time scaling‚Äîthe improvement of model performance through longer inference chains‚Äîin multilingual contexts, addressing a critical gap left by previous English-centric studies. The research finds that while scaling benefits are strong in high-resource languages, they diminish in low-resource languages, often due to models defaulting to English during internal reasoning. The authors introduce the **Multilingual Initial Thought Transfer (MITT)** framework, a lightweight, unsupervised method that stabilizes reasoning in low-resource languages by transferring reasoning prefixes from high-resource languages. Evaluations on DeepSeek-R1-Distill models across six languages demonstrated that MITT successfully recovers scaling gains for low-resource languages where performance previously plateaued. Ultimately, the study challenges the assumption that inference-time benefits are language-agnostic, advocating for cross-lingual transfer techniques to ensure equitable AI development globally.

---

## üß† Key Findings

*   **Variable Performance Gains:** The benefits of test-time scaling are not uniform; relative performance gains vary significantly across different languages.
*   **Reasoning Language Switching:** Models frequently switch to English for internal reasoning, even when the prompt and expected output are strictly monolingual.
*   **Low-Resource Language Inconsistency:** Low-resource languages not only produce initial reasoning thoughts that differ substantially from English but also exhibit lower internal consistency across generations during early reasoning stages.

---

## üõ†Ô∏è Methodology

*   **Models Evaluated:**
    *   DeepSeek-R1-Distill-Llama-8B
    *   DeepSeek-R1-Distill-Qwen-7B
*   **Linguistic Scope:** A systematic evaluation was conducted across both high-resource and low-resource languages utilizing the Latin script.
*   **Analysis Dimensions:** The research examines generation patterns, specifically looking at:
    *   The language of internal thoughts ('initial thought').
    *   The consistency of reasoning traces across different languages.

---

## üìù Technical Details

*   **Evaluation Setup:**
    *   Assessed on the **Multilingual AIME 2025** dataset.
    *   **Languages Covered:** English, Italian, German, Portuguese, Vietnamese, and Tagalog.
*   **Parameters:**
    *   **Max Reasoning Trace:** 10,000 tokens.
    *   **Extraction:** Answers extracted every 32 tokens.
    *   **Prompts:** 'Wait prompts' used to enforce long chains.
*   **Language Fidelity Assessment:**
    *   Traces are windowed into **32-token chunks**.
    *   The `langid` classifier is applied to determine language.
    *   Down-sampling into **20 analysis segments** is performed for drift analysis.
    *   Similarity is measured via **MPNet embeddings**.
    *   Rolling average window set to **5**.

---

## ‚ú® Contributions

*   **First Systematic Multilingual Study:** This research presents the initial comprehensive study on test-time scaling behavior in multilingual settings, addressing a prior gap where research was almost exclusively English-focused.
*   **MITT Framework:** Introduction of **MITT (Multilingual Initial Thought Transfer)**, a novel, unsupervised, and lightweight approach based on reasoning prefix-tuning.
*   **Cross-Lingual Performance Enhancement:** MITT addresses multilingual reasoning inconsistencies by transferring reasoning prefixes from high-resource languages to low-resource languages, resulting in significant performance boosts for underrepresented languages.

---

## üìà Results

*   **High vs. Low Resource:** High-resource languages show stronger test-time scaling than low-resource languages.
*   **Model Comparison:**
    *   **DeepSeek-R1-Distill-Llama-8B:** Maintains positive scaling trends.
    *   **DeepSeek-R1-Distill-Qwen-7B:** Exhibits insignificant scaling for low-resource languages.
*   **Language Drift:** Both models frequently switch to English for internal reasoning. Low-resource inputs show higher variance in early reasoning stages.
*   **MITT Impact:** Fine-tuning on initial steps (using the MITT framework) improved accuracy and scaling gains for the **Qwen-7B** model specifically.

---

**Quality Score:** 4/10  
**References:** 17 citations