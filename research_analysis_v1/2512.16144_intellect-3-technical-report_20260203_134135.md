---
title: 'INTELLECT-3: Technical Report'
arxiv_id: '2512.16144'
source_url: https://arxiv.org/abs/2512.16144
generated_at: '2026-02-03T13:41:35'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# INTELLECT-3: Technical Report

*Prime Intellect Team; Mika Senghaas; Fares Obeid; Sami Jaghouar; William Brown; Jack Min Ong; Daniel Auras; Matej Sirovatka; Jannik Straube; Andrew Baker; Sebastian MÃ¼ller; Justus Mattern; Manveer Basra; Aiman Ismail; Dominik Scherm; Cooper Miller; Ameen Patel; Simon Kirsten; Mario Sieg; Christian Reetz; Kemal Erdem; Vincent Weisser; Johannes Hagemann*

***

> ### ðŸ“Š Quick Facts
> * **Model Architecture:** Mixture-of-Experts (MoE)
> * **Total Parameters:** 106 Billion
> * **Active Parameters:** 12 Billion
> * **Base Model:** GLM-4.5-Air-Base
> * **Training Infrastructure:** `prime-rl` (Asynchronous RL Stack)
> * **Compute Scale:** 512 H200 GPUs
> * **Math Performance (AIME 2024):** 90.8%
> * **Coding Performance (LiveCodeBench v6):** 69.3%

***

## Executive Summary

This research addresses the critical challenge of scaling reinforcement learning (RL) for complex, agentic workflows in large language models (LLMs). While LLMs have shown immense promise in reasoning tasks, training them to handle multi-turn interactions, tool use, and complex verification steps requires significant computational resources and sophisticated infrastructure. The authors aim to demonstrate that a Mixture-of-Experts (MoE) architecture, trained with a highly efficient, asynchronous RL stack, can achieve state-of-the-art reasoning capabilities while using fewer active parameters than dense frontier models. This matters because it offers a path to high performance that reduces the prohibitive compute costs typically associated with training advanced reasoning agents.

The core innovation is the introduction of `prime-rl`, a novel, end-to-end asynchronous framework designed specifically for large-scale RL. The authors trained INTELLECT-3 on top of the **GLM-4.5-Air-Base** model using a pipeline that integrates **Supervised Fine-Tuning (SFT)** with large-scale RL. Technically, `prime-rl` decouples the training and inference loops through a distributed architecture consisting of an Orchestrator, a Trainer (built on FSDP 2 and HuggingFace), and a vLLM-powered Inference Service. This design allows for off-policy training, continuous batching with in-flight weight updates, and seamless scaling from a single node to thousands of GPUs. Crucially, the framework supports "agentic RL," enabling the model to engage in multi-step interactions and utilize external tools via the integrated `verifiers` library, facilitating the efficient training of the 106B-parameter MoE model across 512 H200 GPUs.

INTELLECT-3, a 106B-parameter MoE model with only 12B active parameters, delivers state-of-the-art performance relative to its size across **math, code, science, and reasoning benchmarks**. In mathematical reasoning, the model achieved 90.8% on AIME 2024 and 88.0% on AIME 2025. In coding capabilities, it scored 69.3% on LiveCodeBench v6, representing an 8% improvement over the GLM-4.5-Air post-train baseline. The model also exhibits strong capabilities in **science** benchmarks. Most significantly, INTELLECT-3 surpasses frontier open-source models with over six times its parameter count on general reasoning and agentic tasks, proving that architectural efficiency combined with advanced RL training can outperform sheer parameter scale.

The significance of this work extends beyond model performance to the democratization of advanced AI training infrastructure. By open-sourcing the INTELLECT-3 model weights, the complete `prime-rl` stack, training recipes, and the "Environments Hub," the authors provide a reproducible blueprint for the community to build and study agentic AI systems. This release lowers the barrier to entry for researchers aiming to scale reinforcement learning, potentially accelerating the development of autonomous agents and establishing new engineering standards for asynchronous, large-scale model training. The `verifiers` library and Environments Hub further contribute by standardizing the tools needed for rigorous evaluation and training of future RL agents.

***

## Key Findings

*   **Architecture Efficiency:** INTELLECT-3 is a **106B-parameter Mixture-of-Experts (MoE)** model with only **12B active parameters**, achieving state-of-the-art performance for its size across math, code, science, and reasoning benchmarks.
*   **Superior Reasoning:** The model significantly outperforms many larger frontier models in complex reasoning tasks despite having fewer active parameters.
*   **Scalability:** The research successfully scaled reinforcement learning training to **512 H200 GPUs** with high efficiency using a custom asynchronous stack.
*   **Benchmark Dominance:**
    *   Achieved **90.8%** on AIME 2024 and **88.0%** on AIME 2025 (Math).
    *   Scored **69.3%** on LiveCodeBench v6 (Code).
    *   Surpasses frontier open-source models over **6x larger** in parameter count on general reasoning and agentic tasks.

***

## Methodology

The training approach utilized a two-phase pipeline built upon the **GLM-4.5-Air-Base** model:

1.  **Supervised Fine-Tuning (SFT):** Initial fine-tuning to establish baseline capabilities.
2.  **Large-Scale Reinforcement Learning (RL):** Advanced training using a custom-built, end-to-end infrastructure stack.

### Infrastructure Components
*   **`prime-rl` Framework:** An open framework designed for asynchronous large-scale RL.
*   **Agentic RL Capabilities:** Supports multi-turn interactions and tool use via the `verifiers` library.
*   **Scalability:** Designed to scale seamlessly from a single node to thousands of GPUs.

***

## Technical Details

### Model Specifications
*   **Base Architecture:** GLM-4.5-Air
*   **Parameter Count:** 106B Total / 12B Active (MoE)
*   **Compute Resources:** 512 H200 GPUs

### The `prime-rl` Framework
The technical backbone of the research is a custom asynchronous framework comprising three main components:

*   **Orchestrator:** Manages the workflow and distribution of tasks.
*   **Trainer:** Built on FSDP 2 and HuggingFace compatibility.
*   **Inference Service:** Powered by vLLM for high-throughput generation.

### Key Technical Features
*   **Asynchronous Off-Policy Training:** Decouples training and inference loops for efficiency.
*   **Continuous Batching:** Supports in-flight weight updates without disrupting the pipeline.
*   **MoE Support:** Efficiently handles the routing and computation of Mixture-of-Experts models.

### Supporting Infrastructure
*   **Verifiers & Environments Hub:** Centralized platform for evaluation tools.
*   **Prime Sandboxes:** Secure environments for code execution during training.
*   **Compute Orchestration:** Automated scaling management for large GPU clusters.

***

## Results

### Mathematical Reasoning
*   **AIME 2024:** 90.8%
*   **AIME 2025:** 88.0%

### Coding Capabilities
*   **LiveCodeBench v6:** 69.3%
    *   *Note:* This represents an **8% improvement** over the GLM-4.5-Air post-train model.

### General Performance
*   Successfully outperforms open-source frontier models with **6x larger parameter counts** on general reasoning and agentic tasks.
*   Demonstrates strong capabilities in science benchmarks.

***

## Contributions

The primary contributions of this work focus on both model performance and the democratization of tools:

1.  **Open Source Release:** Full release of INTELLECT-3 model weights, complete infrastructure stack, training recipes, and evaluation environments to ensure reproducibility.
2.  **`prime-rl` Framework:** Introduction of a novel open framework for large-scale asynchronous reinforcement learning, specifically tailored for agentic workflows, tool use, and multi-turn interactions.
3.  **Community Tools:**
    *   Launch of the **"Environments Hub"**, a community platform for shared resources.
    *   Release of the **`verifiers` library**, providing standardized tools for training and evaluating RL agents.

***

## Document Metadata

*   **Quality Score:** 8/10
*   **References:** 40 citations