---
title: Investigating Modality Contribution in Audio LLMs for Music
arxiv_id: '2509.20641'
source_url: https://arxiv.org/abs/2509.20641
generated_at: '2026-02-06T04:21:08'
quality_score: 9
citation_count: 0
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Investigating Modality Contribution in Audio LLMs for Music
*Giovana Morais; Magdalena Fuentes*

> ### ðŸ“Š Quick Facts
> *   **Framework:** MM-SHAP (Multi-Modal SHapley Additive exPlanations)
> *   **Benchmark:** MuChoMusic
> *   **Models Analyzed:** Qwen-Audio, MU-LLaMA
> *   **Avg Audio Contribution:** 0.73% (Audio Description Task)
> *   **Quality Score:** 9/10

---

> **Executive Summary**
>
> Current evaluation of Audio Large Language Models (LLMs) focuses predominantly on predictive accuracy, often obscuring the underlying mechanics of how these models process information. A critical challenge in the field is determining whether high performance stems from genuine auditory understanding or if models are merely exploiting linguistic shortcuts and over-relying on text modality. Without understanding the specific contribution of audio inputs versus text inputs, it is difficult to assess if these models are truly "listening" or simply guessing based on context. This opacity creates a need for rigorous diagnostic tools that can dissect modality contribution to ensure models are robustly utilizing audio data.
>
> This research introduces the first application of the MM-SHAP (Multi-Modal SHapley Additive exPlanations) framework to Audio LLMs, providing a novel methodological approach for Explainable AI (XAI) in the audio domain. By calculating Shapley valuesâ€”specifically $\Phi_{A,t}$ for audio and $\Phi_{T,t}$ for textâ€”the framework quantifies the relative contribution of each modality to specific output tokens. This technique isolates modality influence independent of the model's overall performance, allowing researchers to visualize and measure exactly how much weight the model assigns to the audio waveform compared to the textual context during inference.
>
> The evaluation of Qwen-Audio and MU-LLaMA using the MuChoMusic benchmark revealed that the highest-performing model relied significantly more on text than audio. Quantitatively, the average A-SHAP for audio description tasks was 0.73%, indicating higher audio utilization compared to multiple-choice tasks, yet still relatively low overall. While Qwen-Audio demonstrated the capability to effectively localize key sound events with concentrated activations for single-sounding events, MU-LLaMA displayed similar magnitudes for audio and text features but exhibited spread-out high activations and strong negative contributions. These results confirm that high accuracy does not necessarily equate to deep "listening" or high audio utilization.
>
> This study establishes a foundational shift in multi-modal evaluation, moving beyond simple accuracy metrics to a nuanced understanding of model behavior and modality balance. By demonstrating that high accuracy can mask low audio utilization, the authors provide a critical tool for future research to audit and improve Audio LLMs. This work sets the stage for developing more transparent systems where audio inputs are genuinely functional, ensuring that future advancements in music processing and audio understanding are driven by models that fully leverage their multi-modal capabilities.

---

## Key Findings

*   **Text Reliance:** The model with the highest overall accuracy relied more heavily on text modality rather than audio to answer questions.
*   **Localization Capability:** Despite a low overall contribution of audio to the final output, models demonstrated the capability to successfully localize key sound events.
*   **Functional Audio Role:** The study confirms that while audio contribution may be lower than text in high-performing models, the audio modality is not entirely ignored and plays a functional role in specific tasks like localization.
*   **Accuracy vs. Listening:** Evaluation using the MuChoMusic benchmark highlights that high accuracy does not necessarily equate to deep 'listening' or high audio utilization.

## Methodology

The researchers adapted the **MM-SHAP (Multi-Modal SHapley Additive exPlanations)** framework for this study. They utilized a score based on Shapley values to quantify the relative contribution of each modality (audio vs. text) to the model's prediction, independent of the model's overall performance. The methodology involved evaluating two distinct Audio LLMs using the MuChoMusic benchmark to isolate and measure the specific contribution of audio and text inputs.

## Technical Details

| Component | Description |
| :--- | :--- |
| **Core Metrics** | Utilizes **Average Shapley (A-SHAP)** and **MM-SHAP** metrics to measure the magnitude of feature importance, analyzing positive, negative, and absolute value components. |
| **Shapley Variables** | Uses specific Shapley values for quantification:<br>â€¢ $\Phi_{A,t}$ for Audio contribution<br>â€¢ $\Phi_{T,t}$ for Text contribution |
| **Models Studied** | **Qwen-Audio** (best-performing) and **MU-LLaMA** |
| **Visualization** | â€¢ Text contributions are thresholded at 80% of the maximum Shapley value.<br>â€¢ Audio contributions are mapped to the waveform, where darker colors indicate higher contribution. |

## Results

*   **Audio Description Performance:** The average A-SHAP for the audio description task increased to **0.73%**, indicating higher audio utilization compared to multiple-choice tasks.
*   **Qwen-Audio Behavior:** Relies more heavily on text due to lower audio feature magnitude but demonstrates effective localization for specific tokens. Single-sounding events exhibited concentrated activations, while long-term events showed spread activations.
*   **MU-LLaMA Behavior:** Showed similar magnitudes for audio and text features but displayed spread-out high activations and strong negative contributions.

## Contributions

*   **First Application:** This work represents the first application of the MM-SHAP framework to Audio Large Language Models.
*   **XAI Foundation:** It establishes a foundational step for future research in explainable AI (XAI) specifically within the audio domain.
*   **Methodological Advancement:** It provides a methodological approach for dissecting how multi-modal models balance and utilize different input types, moving beyond simple accuracy metrics to understand model behavior.