---
title: 'Towards Efficient Pre-training: Exploring FP4 Precision in Large Language
  Models'
arxiv_id: '2502.11458'
source_url: https://arxiv.org/abs/2502.11458
generated_at: '2026-02-03T18:34:29'
quality_score: 9
citation_count: 15
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Towards Efficient Pre-training: Exploring FP4 Precision in Large Language Models

*Jiecheng Zhou; Ding Tang; Rong Fu; Boni Hu; Haoran Xu; Yi Wang; Zhilin Pei; Zhongling Su; Liang Liu; Xingcheng Zhang; Weiming Zhang*

---

> **üìä Quick Facts**
>
> *   **Quality Score:** 9/10
> *   **Total Citations:** 15
> *   **Precision Target:** FP4 (Mixed with FP8)
> *   **Tested Architecture:** Llama 7B (Transformer)
> *   **Key Innovation:** Stage-aware & Module-specific Precision

---

## Executive Summary

Training Large Language Models (LLMs) requires immense computational resources, driving the industry toward lower numerical precisions to improve efficiency and reduce costs. While BF16 and FP8 have become standard for accelerating training, further reducing precision to FP4 presents a significant opportunity to maximize hardware utilization. However, operating at FP4 introduces substantial challenges, including potential degradation in model accuracy, loss of gradient stability during backpropagation, and the risk of numerical underflow.

This paper addresses the critical problem of how to effectively implement FP4 precision for LLM pre-training without compromising the convergence or performance achieved by higher-precision standards. The authors introduce a mixed-precision training scheme that moves beyond uniform quantization by employing a module-specific and stage-aware approach. Technically, the framework integrates FP4 computation using a per-block quantization strategy with a block size of 128 but strategically protects sensitive components by retaining FP8 for Multi-Head Attention (MHA) QKV computations, output projections, and weight gradients.

Analysis of the Llama 7B architecture reveals that the Feed-Forward Network (FFN) accounts for 57% of total computational costs, highlighting the significant efficiency gains available by targeting this component with FP4. Despite an 18% underflow occurrence for activations and representation differences in small values, the proposed scheme achieves accuracy comparable to BF16 and FP8 baselines. This research establishes a viable path toward ultra-low precision training, providing a foundational method aligned with next-generation FP4 hardware.

---

## Key Findings

*   **Effective FP4 Utilization:** The study demonstrates that FP4 precision can be effectively utilized for training LLMs, achieving accuracy comparable to BF16 and FP8 standards.
*   **Cost Reduction:** The proposed method successfully reduces computational costs while maintaining model performance.
*   **Backpropagation Stability:** The framework ensures gradient stability during backpropagation, a common challenge in low-precision training.
*   **Attention Specificity:** Pure FP4 attention results in uniformly distributed scores that fail to identify significant tokens; the mixed-precision approach resolves this by maintaining specificity.

---

## Methodology

The authors propose an FP4 training scheme based on the Transformer architecture. The core methodology relies on a mixed-precision quantization strategy designed to balance efficiency and model fidelity.

*   **Module-Specific Precision:** Different components of the Transformer architecture utilize different precision levels based on their sensitivity to quantization noise.
*   **Stage-Aware Adaptation:** The training process adapts precision requirements dynamically across different stages of the training schedule.
*   **Fine-Grained Quantization:** The approach employs fine-grained quantization techniques paired with a target precision training schedule to optimize resource allocation dynamically.

---

## Technical Details

The paper introduces a comprehensive mixed-precision pre-training recipe. The specific technical configurations are outlined below:

### üõ†Ô∏è Quantization Strategy
*   **Per-Block Quantization:** Integrates FP4 computation using a per-block strategy.
*   **Block Size:** Configured with a block size of **128** to balance memory footprint and computational efficiency.

### üß† Module-Specific Precision Schemes
To protect sensitive components from precision loss, the authors employ a hybrid approach:

*   **Multi-Head Attention (MHA) Protection:**
    *   **QKV Computation:** Retained at **FP8** to prevent sensitivity issues.
    *   **Output Projection:** Utilizes **FP8** to maintain stability.
*   **Gradient Management:**
    *   **Weight Gradients:** Calculated in **FP8** to address gradient sensitivity and prevent underflow.

### üìÖ Training Schedule
*   **2-Stage Schedule:** A two-stage training schedule is utilized to mitigate quantization noise in later training stages, ensuring robust convergence.

---

## Results

The performance of the proposed FP4 training scheme was evaluated on the Llama 7B architecture, yielding the following insights:

*   **Computational Breakdown:** The Feed-Forward Network (FFN) accounts for **57%** of the total computational cost, making it the primary target for FP4 optimization.
*   **Gradient Analysis:**
    *   An **8.61%** representation capability difference was observed between FP4 and higher precisions for values around 0.02.
    *   An approximate **18%** underflow occurrence was recorded for activations with FP4.
*   **Qualitative Attention Analysis:**
    *   **FP4-Only:** Attention scores became uniformly distributed, failing to identify significant tokens.
    *   **Baseline (Proposed):** Maintained specificity and successfully identified significant tokens.
*   **Overall Accuracy:** The study claims accuracy comparable to BF16 and FP8 baselines while achieving reduced computational costs.

---

## Contributions

1.  **Ultra-Low Precision Training:** Enables training at FP4 precision, pushing boundaries beyond the current FP8 state-of-the-art.
2.  **Optimized Framework:** Introduces an optimized mixed-precision framework designed for dynamic resource allocation across model components.
3.  **Hardware Alignment:** Establishes a foundational method aligned with the capabilities of next-generation FP4 hardware.