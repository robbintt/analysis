---
title: 'TD-MPC-Opt: Distilling Model-Based Multi-Task Reinforcement Learning Agents'
arxiv_id: '2507.01823'
source_url: https://arxiv.org/abs/2507.01823
generated_at: '2026-02-03T18:54:19'
quality_score: 8
citation_count: 36
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# TD-MPC-Opt: Distilling Model-Based Multi-Task Reinforcement Learning Agents

*Dmytro Kuzmenko; Nadiya Shvai*

---

### ðŸ“Œ Quick Facts

> **Paper Quality:** 8/10  
> **References:** 36 Citations  
> **Benchmark:** MT30 (30 Tasks)  
> **Teacher Model:** 317M Parameters  
> **Student Model:** 1M Parameters  
> **Compression Ratio:** 317x Reduction  
> **Top Score:** 28.45 (Normalized)  

---

## Executive Summary

High-performance model-based reinforcement learning agents designed for multi-task control typically require massive parameter counts (often exceeding 300 million) to accurately model world dynamics. However, deploying these large-scale models is frequently infeasible for resource-constrained hardware such as robotic platforms. This paper addresses the critical challenge of compressing high-capacity multi-task agents into efficient, deployable formats without sacrificing the nuanced knowledge required for robust performance across diverse environments.

The authors propose **TD-MPC-Opt**, a knowledge distillation framework that transfers capabilities from a frozen, high-capacity teacher model (317M parameters) to a compact student model (1M parameters). Built upon the TD-MPC2 architecture, the method utilizes a composite loss function that combines standard model-based RL lossesâ€”specifically consistency for the latent world model and value estimationâ€”with a dedicated distillation loss. Crucially, the distillation loss is defined as the Mean Squared Error (MSE) between the teacher and student reward predictions, ensuring the student aligns with the teacherâ€™s assessment of the environment rather than conflating value and reward signals. Additionally, the approach employs FP16 post-training quantization to reduce the memory footprint, ensuring the model remains lightweight for edge deployment.

Evaluated on the MT30 benchmark comprising 30 distinct tasks, the distilled 1M parameter model achieved a **state-of-the-art normalized score of 28.45**. This represents a substantial performance increase over a baseline 1M parameter model (score 18.93) and demonstrates the efficacy of distillation over training a 1M model from scratch (score 27.36). The method successfully achieved a 317x reduction in parameters (317M to 1M) while retaining complex behavioral knowledge. Furthermore, the application of FP16 post-training quantization reduced the physical model size by approximately 50%, validating the compression pipeline's efficiency.

This research significantly advances the field of embodied AI by bridging the gap between theoretical model capacity and the practical constraints of real-world hardware. By enabling state-of-the-art multi-task agents to operate within the strict memory and compute limits of edge devices like robots, TD-MPC-Opt facilitates the immediate deployment of sophisticated reinforcement learning systems.

---

## Key Findings

*   **State-of-the-Art Performance:** Achieved a normalized score of **28.45** on the MT30 benchmark using a compact, distilled model.
*   **Significant Baseline Outperformance:** Significantly outperformed the baseline 1M parameter model (score 18.93), demonstrating substantial performance gain through distillation.
*   **Massive Compression:** Successfully compressed a high-capacity multi-task agent (**317M parameters**) into a compact model (**1M parameters**) while retaining complex knowledge.
*   **Efficiency Gains:** Reduced the distilled model's size by approximately **50%** through FP16 post-training quantization.

---

## Methodology

The core method involves **knowledge distillation** from a large, high-capacity teacher model into a compact student model within a model-based reinforcement learning framework.

*   **Framework:** Teacher-Student Distillation.
*   **Components:**
    *   **Teacher:** Frozen, high-capacity 317M parameter TD-MPC2.
    *   **Student:** Compact 1M parameter TD-MPC2.
*   **Optimization:** The process is optimized using FP16 post-training quantization to compress the model footprint.
*   **Validation:** The approach is validated using the MT30 benchmark to assess performance across diverse multi-task scenarios.

---

## Technical Details

**Architecture & Framework**
*   **Backbone:** TD-MPC2.
*   **Teacher:** Frozen TD-MPC2 (317M parameters).
*   **Student:** TD-MPC2 (1M parameters).

**Loss Function**
The student is trained using a composite loss function combining original RL losses with a distillation component:
$$L_{total} = L_{orig} + d_{coef} \times L_{distill}$$

*   **$L_{orig}$:** Includes Consistency, Reward, and Value losses.
*   **$L_{distill}$:** Mean Squared Error (MSE) between teacher and student reward predictions.

**Hyperparameters**
*   **Distillation Coefficient ($d_{coef}$):** 0.4
*   **Training Duration:** 1,000,000 steps
*   **Batch Size:** 128 to 1024

**Optimization Techniques**
*   **Post-Training Quantization:** FP16, Mixed Precision, INT8.

---

## Results

The evaluation on the **MT30 benchmark** (30 tasks) yielded the following outcomes:

*   **Performance:** The distilled 1M parameter model achieved a normalized score of **28.45**.
*   **Comparison:**
    *   vs. Baseline 1M Model (18.93): Significant outperformance.
    *   vs. 1M Model Trained from Scratch (27.36): **2.77% improvement** (Score: 28.12).
*   **Compression Metrics:**
    *   **Parameter Reduction:** 317M â†’ 1M (317x reduction).
    *   **Physical Size Reduction:** ~50% via FP16 quantization.

---

## Contributions

*   **Resource-Efficient Deployment:** Provides a practical solution to deploying large world models in resource-constrained environments like robotics.
*   **Knowledge Representation:** Offers novel insights into how complex multi-task knowledge is represented within large models and consolidated into smaller parameter spaces.
*   **Accessibility:** Paves the way for more efficient and accessible multi-task reinforcement learning systems by bridging the gap between high-capacity models and hardware limitations.