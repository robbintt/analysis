# Sampling Complexity of TD and PPO in RKHS
*Lu Zou; Wendi Ren; Weizhong Zhang; Liang Ding; Shuang Li*

> ### **Quick Facts**
> 
> *   **Quality Score:** 9/10
> *   **References:** 40 Citations
> *   **Core Framework:** RKHS-based Optimization
> *   **Key Regimes:** Tabular, Linear, Sobolev, Gaussian, NTK
> *   **Convergence Rate:** $k^{-1/2}$

---

## Executive Summary

This research addresses the critical theoretical gap regarding the convergence and sampling complexity of Proximal Policy Optimization (PPO) under general function approximation. While PPO is empirically one of the most successful reinforcement learning algorithms, rigorous theoretical guarantees have historically been restricted to finite-dimensional or linear settings. This lack of theoretical foundation obscures why PPO performs well with complex function approximators like deep neural networks and limits the ability to guarantee stability and efficiency in high-dimensional spaces. Bridging this gap is essential for transitioning PPO from a heuristic success to a mathematically verified tool for complex control tasks.

The key innovation is a novel function-space framework that decouples PPO into policy evaluation and improvement within a Reproducing Kernel Hilbert Space (RKHS). Technically, the authors propose a two-step process: first, a kernelized Temporal-Difference (TD) critic performs efficient gradient updates on one-step state-action samples (acting as an implicit preconditioner); second, policy improvement is achieved via a KL-regularized natural-gradient step that exponentiates the evaluated action-value to recover standard PPO updates.

This approach unifies diverse function approximation regimes—ranging from tabular and linear to Sobolev spaces, Gaussian kernels, and Neural Tangent Kernels (NTK)—under a single analytical umbrella dependent on RKHS entropy. The paper establishes unified, non-asymptotic, and instance-adaptive guarantees for the unified regimes, deriving a specific sampling rule that ensures an optimal convergence rate of $k^{-1/2}$. In empirical evaluations on control benchmarks including CartPole and Acrobot, the proposed method was compared against a Generalized Advantage Estimation (GAE) baseline. The results demonstrated that the kernel TD-based critic achieved favorable throughput, while the implementation of the theory-aligned sampling schedule led to empirical superiority in terms of both stability and sample efficiency over the baseline.

This work significantly advances the field by providing the first rigorous theoretical grounding for PPO in infinite-dimensional function spaces, effectively validating its use beyond finite-dimensional assumptions. By clarifying the conditions for global policy improvements with RKHS-proximal updates, the research bridges the divide between classical kernel methods and modern deep reinforcement learning. The derivation of precise sampling rules for optimal stochastic optimization offers actionable insights for the community, paving the way for the design of next-generation RL algorithms that are both theoretically sound and empirically robust.

---

## Key Findings

*   **Decoupled PPO Framework:** PPO can be decoupled into policy evaluation and improvement within an RKHS utilizing kernelized TD methods and KL-regularized natural-gradient steps.
*   **Unified Guarantees:** The paper provides unified non-asymptotic, instance-adaptive guarantees for a wide range of regimes, including tabular, linear, Sobolev, Gaussian, and Neural Tangent Kernel (NTK).
*   **Optimal Convergence:** Derives a specific sampling rule ensuring an optimal convergence rate of $k^{-1/2}$.
*   **Empirical Superiority:** Demonstrates empirical superiority in stability, sample efficiency, and throughput when compared to Generalized Advantage Estimation (GAE) baselines.

---

## Methodology

The research adopts a **function-space perspective** to analyze PPO using a two-step decoupled process within an RKHS framework:

1.  **Policy Evaluation:** This phase utilizes a **kernelized TD critic**. It performs efficient RKHS-gradient updates using minimal one-step state-action samples.
2.  **Policy Improvement:** This phase involves a **KL-regularized natural-gradient step**. It exponentiates the evaluated action-value to recover PPO/TRPO-style proximal updates.

---

## Technical Details

### Core Framework
The study utilizes **RKHS-based Optimization** to unify function approximation regimes. This framework covers:
*   Tabular settings
*   Linear settings
*   Sobolev spaces
*   Gaussian kernels
*   Neural Tangent Kernels (NTK)

### Architecture
The proposed architecture consists of a **Decoupled PPO** process:

*   **Policy Evaluation:** Employs a **Kernelized Temporal-Difference (TD) critic** that performs RKHS-gradient updates on one-step samples. This acts as an implicit preconditioner.
*   **Policy Improvement:** Utilizes a **KL-regularized, natural-gradient step** that exponentiates the evaluated action-value function.

### Mathematical Guarantees
*   **Non-asymptotic bounds** relying on RKHS entropy.
*   A specific **sampling rule** is defined to ensure a convergence rate of $k^{-1/2}$.

### Assumptions
*   Bounded state-action distributions.
*   Bounded RKHS norms for the reward function and transition kernel.

---

## Contributions

*   **Theoretical Grounding:** Provides theoretical grounding for PPO beyond finite-dimensional assumptions by analyzing it in function spaces.
*   **Unified Framework:** Unifies diverse function approximation regimes (from tabular to neural networks) under a single framework based on RKHS entropy.
*   **Global Improvements:** Clarifies the conditions for global policy improvements with RKHS-proximal updates.
*   **Sampling Complexity:** Advances the understanding of sampling complexity by defining precise rules for optimal stochastic optimization convergence.

---

## Results

### Experimental Setup
The proposed method was evaluated on standard control tasks:
*   **Tasks:** CartPole and Acrobot.
*   **Baseline:** Compared against Generalized Advantage Estimation (GAE).

### Findings
*   **Throughput:** The kernel TD-based critic demonstrated favorable throughput.
*   **Stability & Efficiency:** The theory-aligned sampling schedule resulted in improved stability and sample efficiency relative to the GAE baseline.
*   **Overall Performance:** The paper claims empirical superiority in stability, sample efficiency, and throughput.