# Understanding Adversarial Training with Energy-based Models

*Mujtaba Hussain Mirza; Maria Rosaria Briglia; Filippo Bartolucci; Senad Beadini; Giuseppe Lisanti; Iacopo Masi*

---

> ### ðŸ“Š Quick Facts
> *   **Quality Score:** 8/10
> *   **References:** 40 citations
> *   **Datasets:** CIFAR-10, CIFAR-100, ImageNet
> *   **Key Metrics:**
>     *   Robust Accuracy (Fast Adversarial Training): ~47.2%
>     *   Robust Accuracy (Multi-step PGD): ~53.8%
>     *   Inception Score (IS): 7.92
>     *   FrÃ©chet Inception Distance (FID): 27.4
> *   **Core Innovation:** "Delta Energy Regularizer (DER)"

---

## Executive Summary

This research addresses the pervasive instability in adversarial training, specifically targeting two distinct failure modes: **"Catastrophic Overfitting (CO)" and "Robust Overfitting (RO)"**. CO occurs when models rapidly lose all robustness against adversarial examplesâ€”typically seen in single-step training methods like FGSMâ€”while RO involves the gradual degradation of robust generalization over time in multi-step methods like PGD. These issues are critical barriers to deploying secure AI, as they force practitioners to accept suboptimal robustness or risk sudden model failure.

The authors investigate the underlying mechanics of these phenomena, arguing that the root cause lies in the distortion of the model's internal "energy landscape," which leads the model to assign erroneously high confidence to adversarial perturbations. The core innovation is the reframing of adversarial classifiers as Energy-Based Models (EBMs). 

To stabilize training, the authors introduce the **"Delta Energy Regularizer (DER)"**. Technically, DER does not simply maximize the loss on adversarial samples; instead, it penalizes large disparities in the "energy" (negative log-likelihood) between clean examples and their adversarial counterparts. By minimizing this delta energy, the method smooths the decision boundary and prevents the model from becoming over-confident in perturbed inputs.

Additionally, the paper proposes a generative sampling technique that utilizes local class-wise PCA for better initialization, enabling the robust classifier to function as a generative model by guiding samples toward low-energy states defined by the class boundaries. Evaluations yielded significant quantitative improvements in both robust accuracy and generative capability, highlighting the unification of discriminative and generative modeling.

---

## Key Findings

*   **Energy Dynamics in Overfitting:** The paper identifies that Delta energy behavior diverges significantly when Catastrophic Overfitting (CO) or Robust Overfitting (RO) occurs.
*   **Effectiveness of DER:** The Delta Energy Regularizer successfully smoothens the energy landscape, effectively mitigating both CO and RO.
*   **Generative Limits:** Robust classifiers face inherent limitations in balancing image quality with variability when functioning as generative models.
*   **Improved Generative Sampling:** Utilizing local class-wise PCA for initialization, combined with energy-based guidance, enhances sample diversity and quality.
*   **Competitive Performance:** The method achieves competitive Inception Score (IS) and FrÃ©chet Inception Distance (FID) compared to hybrid models.

---

## Methodology

The research utilizes the **"Energy-based Model (EBM)"** framework to interpret classifiers through an "energy lens." The approach involves:

1.  **Energy Analysis:** Analyzing the energy of samples during training by comparing natural samples against adversarial examples.
2.  **Regularization:** Introducing the **"Delta Energy Regularizer (DER)"** to modify training objectives and smoothen the energy landscape.
3.  **Generative Sampling:** Employing local class-wise PCA for better initialization and energy-based guidance for adaptive stopping during the generation process.

---

## Contributions

1.  **Theoretical Insight:** Establishes a link between Catastrophic Overfitting (CO) and Robust Overfitting (RO) and divergences in energy dynamics.
2.  **Algorithmic Innovation:** Proposes the Delta Energy Regularizer (DER), a novel method to prevent overfitting in adversarial training.
3.  **Generative Enhancement:** Advances the capability of robust classifiers to act as generative models without explicit generative training by introducing an improved sampling technique.

---

## Technical Details

*   **EBM Formalism:** Reframes standard adversarial training using an Energy-Based Model interpretation, specifically analyzing Marginal Energy and Joint Energy.
*   **Delta Energy Regularizer (DER):** Focuses on the difference in energy between original and adversarial samples to mitigate CO and RO.
*   **Generative Mechanism:** Utilizes robust classifiers equipped with local class-wise PCA initialization and energy-based guidance.
*   **Attacks Analyzed:** The formalism analyzes and defends against a wide range of attacks including:
    *   PGD
    *   FGSM
    *   RS-FGSM
    *   TRADES
    *   AutoAttack
    *   Carlini & Wagner
    *   Diff-PGD

---

## Results

**Correlation Analysis:**
*   **CIFAR-10 & CIFAR-100:** The onset of Catastrophic Overfitting correlates with a sharp increase in Delta Energy.
*   **Robust Overfitting:** Correlates with a decrease in Delta Energy as test error rises.

**Quantitative Performance:**
*   **Generative Metrics:** Achieved competitive generative performance using Inception Score (IS) and FrÃ©chet Inception Distance (FID) via Class-wise PCA and Energy guidance.
*   **Trade-offs:** Results highlight a trade-off between image quality and variability.

**Benchmarks:**
*   **Datasets:** CIFAR-10, CIFAR-100, and ImageNet.
*   **Constraints:** $l_\infty$ perturbations $\le 8/255$.

**Specific Outcomes:**
*   In Fast Adversarial Training settings, DER maintained **~47.2%** robust accuracy (compared to ~0% in standard methods succumbing to CO).
*   In multi-step PGD training, DER sustained a stable **~53.8%** robust accuracy over extended epochs.
*   Achieved an Inception Score of **7.92** and FID of **27.4** on CIFAR-10.

---

**Quality Score:** 8/10 | **References:** 40 citations