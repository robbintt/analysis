# Babel: Open Multilingual Large Language Models Serving Over 90% of Global Speakers

*Yiran Zhao; Chaoqun Liu; Yue Deng; Jiahao Ying; Mahani Aljunied; Zhaodonghui Li; Lidong Bing; Hou Pong Chan; Yu Rong; Deli Zhao; Wenxuan Zhang*

---

### ðŸ“Œ Quick Facts

| **Metric** | **Details** |
| :--- | :--- |
| **Model Variants** | Babel-9B, Babel-83B |
| **Language Coverage** | 25 Languages |
| **Global Reach** | >90% of global speakers |
| **Key Innovation** | Layer Extension (vs. Appending) |
| **Performance** | Parity with commercial models (83B); Leader in 10B class (9B) |

---

### ðŸ“‘ Executive Summary

Current large language models (LLMs) suffer from a pronounced linguistic bias, predominantly serving English and high-resource languages while excluding the vast majority of the world's population. This exclusion limits the accessibility and utility of AI for billions of speakers of under-resourced languages, creating a significant "digital divide." This paper addresses that gap by presenting **Babel**, an open-source multilingual model family designed to support over **90% of global speakers** across 25 languages, with a specific focus on historically overlooked languages such as Hindi, Urdu, and Swahili.

The core technical innovation is a parameter scaling strategy called "**layer extension**," designed to expand model capacity more effectively than traditional continued pre-training. Rather than appending new layersâ€”a method the authors found causes "catastrophic performance collapse"â€”Babel inserts new layers directly into the middle and back sections of the existing architecture. These layers are initialized by duplicating original parameters and injecting Gaussian noise with a mean of 0.0001 to minimize disruption to learned representations. The comprehensive training pipeline includes rigorous data cleaning, utilizing smaller LLMs like Qwen-2.5 and GPT-4o as quality classifiers for sources such as CulturaX and MADLAD-400, followed by a Supervised Fine-Tuning (SFT) phase using open-source datasets to align the models for robust downstream performance.

The research produced two model variants: **Babel-9B**, optimized for inference efficiency, and **Babel-83B**, designed for maximum performance. Ablation studies on the MMMLU benchmark using a Qwen2.5-72B-Base backbone validated the layer extension technique; while simply appending layers resulted in scores between 5.2 and 9.4, the interleaving method maintained stability with a score of 72.8. Following the SFT phase, the chat variants achieved state-of-the-art results: Babel-9B-Chat leads the 10B parameter size class, while Babel-83B-Chat achieves parity with commercial closed-source models.

This work significantly advances the field by validating layer extension as a superior method for model scaling, offering a blueprint for efficiently upgrading existing architectures without the instability of standard expansion techniques. By releasing Babel under an open-source license and demonstrating that proprietary performance barriers can be breached for multilingual tasks, the authors accelerate the democratization of AI technology.

---

## Key Findings

*   **Broad Language Coverage:** Babel covers the top 25 languages by speaker count, supporting over 90% of the global population, including under-resourced languages.
*   **Superior Multilingual Performance:** Babel outperforms open-source LLMs of comparable sizes.
*   **Parity with Commercial Models:** The supervised fine-tuned Babel-83B-Chat variant achieves performance equivalent to commercial closed-source models.
*   **Efficiency Leadership:** The Babel-9B-Chat variant leads performance among LLMs in the 10B parameter size class.

---

## Methodology

The Babel model development relied on a novel architectural scaling approach rather than traditional pre-training augmentation:

*   **Layer Extension Technique:** Instead of standard continued pre-training, the model utilizes a layer extension technique to expand parameter count.
*   **Dual-Variant Architecture:**
    *   **Babel-9B:** Optimized for efficient inference.
    *   **Babel-83B:** Designed for maximum performance.
*   **Fine-Tuning Strategy:** The models achieve their final capabilities through fine-tuning using open-source supervised fine-tuning (SFT) datasets.

---

## Technical Details

### Architecture & Expansion
*   **Model Sizes:** Released in two variants: Babel-9B and Babel-83B.
*   **Scaling Strategy:** Utilizes model expansion via layer extension rather than continuous pretraining.
*   **Layer Insertion:** New layers with identical architecture are inserted among existing layers in the second half of the model (middle and back) to minimize disruption to learned representations.
*   **Initialization:** Inserted layers are initialized by duplicating original parameters and adding Gaussian noise with a mean of **0.0001**.

### Data & Training Pipeline
*   **Scope:** Targets 25 languages (~7 billion speakers).
*   **Data Sources:** Wikipedia, CC-News, CulturaX, and MADLAD-400.
*   **Cleaning Pipeline:**
    1.  Normalization.
    2.  LLM-based quality classifier (using Qwen-2.5-0.5B-Instruct and GPT-4o).
    3.  Deduplication via hashing and graph construction.

---

## Results

### Coverage
*   Successfully covers 25 languages, serving over 90% of the global population.
*   Specific focus on under-explored languages including Hindi, Urdu, and Swahili.

### Ablation Studies (MMMLU Benchmark)
*   **Backbone:** Qwen2.5-72B-Base.
*   **Baseline Score:** 79.5
*   **Selected Method (Insertion among layers + Gaussian Noise):** Score **72.8** (Demonstrated superior training adaptability).
*   ** discarded Method (Appending layers):** Score **5.2â€“9.4** (Resulted in significant performance collapse).

### Comparative Performance
*   **Babel-9B:** Leads the 10B parameter class.
*   **Babel-83B:** Achieves parity with closed-source commercial models.
*   **General:** Outperforms open-source models of comparable sizes.

---

## Contributions

*   **Bridging the Language Resource Gap:** Addresses the scarcity of open-source multilingual LLMs by prioritizing under-resourced languages.
*   **Advancement in Parameter Expansion:** Introduces a novel scaling approach via layer extension, demonstrating superior results over standard continued pre-training.
*   **Open-Source Benchmarking:** Establishes new performance benchmarks, proving that open models can match the capabilities of proprietary commercial models.

---
**Quality Score:** 8/10
**References:** 7 citations