---
title: Logit-Based Losses Limit the Effectiveness of Feature Knowledge Distillation
arxiv_id: '2511.14981'
source_url: https://arxiv.org/abs/2511.14981
generated_at: '2026-02-03T20:12:41'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Logit-Based Losses Limit the Effectiveness of Feature Knowledge Distillation

*Nicholas Cooper; Lijun Chen; Sailesh Dwivedy; Danna Gurari*

---

### ðŸ“Š Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Quality Score** | 9/10 |
| **Performance Boost** | Up to 15% Top-1 Accuracy |
| **Architectures** | CNNs (VGG, ResNet) & Vision Transformers (ViT) |
| **Datasets** | CIFAR10, CIFAR100, Tiny ImageNet |
| **Citations** | 40 references |

---

## Executive Summary

**Problem**
Standard Knowledge Distillation (KD) relies on a combination of feature-based losses and logit-based losses, such as Kullback-Leibler divergence, to transfer knowledge from a cumbersome teacher model to a compact student model. This paper addresses a critical limitation in this paradigm: logit-based losses act as a constraint during the training of the student model's backbone. By forcing the student to match the teacher's output logits, these methods limit the student's ability to learn rich, discriminative feature representations, resulting in suboptimal model performance and compression efficiency.

**Innovation**
The authors introduce a Feature-Exclusive KD (FKD) framework that fundamentally alters the loss recipe by eliminating logit-based losses for backbone training. In this approach, the student backbone is trained exclusively using feature-based losses ($L_F$), while cross-entropy loss ($L_{CE}$) is restricted strictly to the student classifier to prevent back-propagation into the backbone layers. To optimize which teacher knowledge is transferred, the paper proposes a novel Knowledge Quality (KQ) metric, $Q(R) := S(R) + \sqrt{I(R)E(R)}$. This metric utilizes the geometric properties of latent representationsâ€”specifically separation, information, and efficiencyâ€”to systematically identify the most valuable teacher layers, pinpointing those at the transition point between feature extraction and compression.

**Results**
The proposed framework was evaluated across CIFAR10, CIFAR100, and Tiny ImageNet datasets using four distinct student-teacher pairings spanning Convolutional Neural Networks (VGG, ResNet, MobileNet) and Vision Transformers. The method achieved state-of-the-art performance in feature-based distillation, delivering Top-1 accuracy boosts of up to 15% over standard approaches that combine feature and logit losses. These results were obtained under a rigorous training protocol using the Adam optimizer for 50 epochs without data augmentation, demonstrating the method's ability to maximize feature learning efficiency even without standard regularization techniques.

**Impact**
This research challenges the conventional wisdom in knowledge distillation by proving that removing logit-based constraints significantly enhances the quality of learned feature representations. The introduction of the Knowledge Quality metric provides a theoretically grounded, automated mechanism for selecting teacher layers, moving the field away from heuristic-based choices. By establishing a new state-of-the-art for feature KD and validating the approach across both CNNs and ViTs, the authors offer a robust, open-source solution for improving model compression that is likely to influence future research on efficient deep learning architectures.

---

## Key Findings

*   **Constraint Identification:** Logit-based loss functions constrain the effectiveness of knowledge distillation when training the student model's backbone.
*   **Superior Framework:** A training framework relying exclusively on feature-based losses outperforms standard approaches that combine feature and logit losses.
*   **Automated Selection:** Utilizing a "knowledge quality metric" based on the geometry of latent representations enables the precise identification of the most effective teacher layers for distillation.
*   **State-of-the-Art Performance:** The proposed method achieves top-1 accuracy boosts of up to **15%** over standard distillation approaches.
*   **Architecture Agnostic:** The effectiveness of the method is validated across diverse architectures, including both Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs).

---

## Methodology

The authors propose a novel feature Knowledge Distillation (FKD) framework that trains the student model's backbone using only feature-based losses, completely removing reliance on logit-based losses such as cross entropy. 

The methodology leverages geometric insights regarding latent representations to introduce a **Knowledge Quality Metric**. This metric automatically identifies the specific layers within the teacher network that contain the most effective knowledge for distillation. The approach was evaluated through experiments on three image classification datasets using four distinct student-teacher pairings spanning CNNs and Vision Transformers.

---

## Contributions

1.  **Feature-Exclusive Framework:** Introduction of a KD training paradigm that eliminates logit-based losses in favor of exclusive feature-based loss optimization.
2.  **Knowledge Quality Metric:** Development of a new metric based on the geometry of latent representations to systematically select the most valuable teacher layers for the distillation process.
3.  **Performance Benchmarking:** Demonstration of significant empirical improvements (up to 15% top-1 accuracy boost) over standard methods, establishing a new state-of-the-art for feature KD.
4.  **Open Source Release:** Public sharing of the codebase to encourage reproducibility and future research.

---

## Technical Details

### Framework Modifications
The proposed Feature Knowledge Distillation (FKD) framework modifies the loss recipe by:
*   Removing the Kullback-Leibler divergence loss ($L_{KL}$).
*   Restricting Cross-Entropy ($L_{CE}$) loss to the student classifier, preventing back-propagation through the backbone.
*   Training the student backbone exclusively on feature-based loss ($L_F$).

### Knowledge Quality (KQ) Metric
The authors introduce a metric $Q(R)$ to select top-$k$ teacher layers based on geometric properties:

$$Q(R) := S(R) + \sqrt{I(R)E(R)}$$

Where:
*   **Separation ($S$):** Calculated as $\text{avgDPW}(R) - \text{avgDPB}(R)$.
*   **Information ($I$) & Efficiency ($E$):** Rely on geometric properties like minimum within-class dot product and between-class distance.

### Geometric Analysis
The method identifies layers at the transition point between the extraction and compression phases of DNN processing.
*   **ResNet34:** Exhibits knowledge quality decay in final layers.
*   **ViT_B:** Maintains better quality at the logits layer.
*   **General Trend:** Separation ($S$) increases in final layers.

---

## Results

### Experimental Setup
*   **Datasets:** CIFAR10, CIFAR100, Tiny ImageNet.
*   **Teacher Models:** VGG19, ResNet34, ViT_B.
*   **Student Models:** VGG11, MobileNetV2, ResNet9, ViT_ET.
*   **Training Protocol:** 50 epochs, Adam optimizer, no data augmentation.

### Performance Metrics
*   **Top-1 Accuracy:** Demonstrated state-of-the-art performance.
*   **Average Relative Improvement (ARI):** Used to quantify gains.
*   **Key Outcome:** Achieved up to 15% Top-1 accuracy boost over standard approaches.

---

**References:** 40 citations | **Quality Score:** 9/10