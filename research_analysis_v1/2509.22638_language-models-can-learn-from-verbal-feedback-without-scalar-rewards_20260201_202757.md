# Language Models Can Learn from Verbal Feedback Without Scalar Rewards

*Renjie Luo; Zichen Liu; Xiangyan Liu; Chao Du; Min Lin; Wenhu Chen; Wei Lu; Tianyu Pang*

---

> ### **Quick Facts**
>
> *   **Quality Score:** 8/10
> *   **Base Model Used:** Qwen2.5-7B-base
> *   **Proposed Architecture:** Feedback-Conditional Policy (FCP)
> *   **Key Innovation:** Uses verbal feedback as conditioning signal (no scalar rewards)
> *   **Math Performance (Avg):** 38.7% (State-of-the-art comparison)
> *   **References:** 33 Citations

---

## Executive Summary

Standard Reinforcement Learning from Human Feedback (RLHF) techniques face a fundamental information bottleneck that limits model alignment. Current methods compress nuanced, high-dimensional verbal feedback into simple scalar rewards. This compression discards the semantic richness and specific details contained within human critiques, leading to scale imbalances and suboptimal policy updates. The inability of existing frameworks to leverage the full depth of natural language feedback represents a significant barrier to improving model reasoning and adherence to complex instructions.

The authors propose the **Feedback-Conditional Policy (FCP)**, a novel architecture that abandons traditional reward optimization in favor of treating feedback-driven learning as a conditional text generation task. Rather than training a separate reward model, FCP treats verbal feedback as a conditioning signal (similar to text-to-image generation) to approximate the feedback-conditional posterior distribution $P(o|x, c)$. Technically, this is achieved by minimizing the forward KL divergence between the policy and the target posterior, a process mathematically equivalent to solving a KL-constrained reward maximization problem where the reward is defined as $\log p_{\text{env}}(c|x, o)$. The training consists of two phases: offline Maximum Likelihood Estimation on static response-feedback pairs, and an online bootstrapping stage where the model generates responses conditioned on positive feedback to iteratively collect fresh data and self-improve.

Experiments conducted using the **Qwen2.5-7B-base** model demonstrate that FCP delivers strong performance across Math, General Reasoning, and Out-of-Distribution (OOD) benchmarks. In mathematical tasks (averaged over Big-Math and AIME), FCP with bootstrapping achieved a score of **38.7%**, slightly outperforming the GRPO baseline (38.4%) and significantly surpassing standard Rejection Fine-Tuning (30.4%). In general reasoning (GPQA, MMLU-Pro), FCP scored **47.8%**, marginally exceeding the best scalar pipeline (RFT + GRPO at 47.5%). The model remained competitive on the IFEval OOD benchmark with a score of **39.0%**. Notably, the study found that filtering length-related feedback conditions during the bootstrapping phase was critical to preventing response length collapse and ensuring stable performance growth.

This research offers a significant conceptual contribution to the field of LLM alignment by validating conditional generation as a viable and potentially superior alternative to reward-based RLHF. By effectively leveraging the semantic depth of verbal feedback without distilling it into a scalar value, the Feedback-Conditional Policy provides a more expressive mechanism for learning from human input. The cross-domain application of text-to-image generation principles to text generation opens new avenues for improving model generalization, suggesting that future alignment strategies may benefit significantly from moving away from scalar rewards toward richer, feedback-conditioned generation frameworks.

---

## Key Findings

*   **Limitations of Scalar Rewards:** Traditional RLHF methods are limited because they compress nuanced verbal feedback into scalar rewards, which discards information richness and induces scale imbalance.
*   **Feedback as Conditioning:** Verbal feedback can be effectively utilized as a conditioning signal for language models, rather than being distilled into a numerical reward value.
*   **Direct Posterior Approximation:** The proposed Feedback-Conditional Policy (FCP) successfully learns to approximate the feedback-conditional posterior directly, bypassing the need for a separate reward model.
*   **Online Self-Refinement:** The model can refine itself through an online bootstrapping stage where it generates responses under positive feedback conditions and utilizes fresh feedback to improve.
*   **Reframed Learning Task:** Reframing feedback-driven learning as a conditional generation task (similar to text-to-image generation) offers a more expressive mechanism for LLMs to learn from natural language feedback compared to reward optimization.

---

## Methodology

The authors introduce the **Feedback-Conditional Policy (FCP)**, which departs from standard reinforcement learning optimization in favor of conditional text generation. The methodology is implemented in two distinct stages:

1.  **Offline Maximum Likelihood Training**
    *   The model is trained on existing datasets of response-feedback pairs.
    *   It approximates the feedback-conditional posterior by treating feedback as the conditioning context.

2.  **Online Bootstrapping**
    *   The model generates responses conditioned on *positive* feedback signals.
    *   It solicits fresh feedback to iteratively update and improve the policy.

---

## Technical Details

The paper proposes **Feedback-Conditional Policy (FCP)**, which treats verbal feedback as a conditioning signal rather than compressing it into a scalar reward.

*   **Objective:** The goal is to approximate the feedback-conditional posterior distribution $P_{\text{off}}(o|x, c)$ by minimizing the forward KL divergence between the policy and the posterior.
*   **Optimization Equivalence:** This optimization is equivalent to solving a KL-constrained reward maximization problem where the reward is defined as $\log p_{\text{env}}(c|x, o)$.
*   **Pipeline:**
    *   **Offline Initialization:** Training on static data.
    *   **Online Bootstrapping:** Generating responses on positive feedback and collecting fresh feedback.
*   **Stabilization:** Requires filtering length-related conditions to prevent response collapse.
*   **Feedback Simulation:** Experiments utilized GPT-5-nano to simulate feedback.

---

## Contributions

*   **Alternative to Scalar Reward Compression:** The paper addresses a fundamental bottleneck in current alignment techniques by proposing a method that utilizes the full semantic depth of verbal feedback without compressing it into a scalar value.
*   **Novel Framework (FCP):** The introduction of the Feedback-Conditional Policy provides a new architecture for training LLMs that integrates verbal feedback directly into the generation process via conditioning.
*   **Cross-Domain Innovation:** By applying concepts from text-to-image generation (using language priors to handle unseen prompts) to text generation and RLHF, the research opens new avenues for improving model generalization and expressiveness in feedback learning scenarios.

---

## Results

Experiments were conducted using **Qwen2.5-7B-base** on Math, General Reasoning, and OOD benchmarks.

*   **Math Performance (Big-Math, AIME - Avg):**
    *   **FCP + Bootstrap:** 38.7%
    *   GRPO: 38.4%
    *   RFT: 30.4%
*   **General Reasoning (GPQA, MMLU-Pro):**
    *   **FCP + Bootstrap:** 47.8%
    *   Best Scalar Pipeline (RFT + GRPO): 47.5%
*   **Out-of-Distribution (IFEval):**
    *   **FCP + Bootstrap:** 39.0% (Remained competitive)
*   **Critical Observation:** Filtering length-related conditions during bootstrapping is crucial to prevent response length collapse and ensure performance growth.

---

**Report Info**  
*References: 33 citations*  
*Analysis Quality: 8/10*