# Noise-Aware Named Entity Recognition for Historical VET Documents

*Alexander M. Esser; Jens DÃ¶rpinghaus*

---

### ðŸ“Š Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Domain** | Historical Vocational Education and Training (VET) |
| **Dataset** | BIBB (2,125 documents, 1908â€“present) |
| **Core Method** | Noise-Aware Training (NAT) with Synthetic OCR Error Injection |
| **Top Performing F1-Score** | **84%** (NAT Artificial) |
| **Baseline F1-Score** | 86% (Clean Data) |
| **Low Performing F1-Score** | 66% (Noisy Real-World Data) |
| **Languages** | German (validated); Arbitrary (architected) |
| **Quality Score** | 9/10 |
| **Citations** | 13 |

---

## Executive Summary

This research addresses the critical challenge of extracting structured information from historical Vocational Education and Training (VET) documents, where Optical Character Recognition (OCR) noise severely degrades the performance of standard Named Entity Recognition (NER) models. Typically trained on clean, modern text, these models fail when applied to the noisy, digitized BIBB corpus of 2,125 historical German documents. This failure represents a significant bottleneck in the Digital Humanities, as manual processing of these archives is infeasible and automated tools have historically lacked robustness against the specific error profiles found in VET domain texts.

To bridge this gap, the authors propose a **Noise-Aware Training (NAT)** framework designed to acclimate models to textual corruption through the synthetic injection of realistic OCR errors into clean training data. The architecture employs transfer learning combined with multi-stage, domain-specific fine-tuning to adapt pre-trained models to the VET lexicon. By aligning with occupational taxonomies such as ISCO-08, ESCO, and KldB 2020, the system targets the extraction of specific entity typesâ€”primarily **occupations and qualifications**. The methodology systematically evaluates three distinct training strategiesâ€”using noisy real-world data, clean data, and synthetically augmented "artificial" dataâ€”to determine the most effective configuration for handling corrupted text.

Experimental validation on the BIBB dataset demonstrates a clear hierarchy in performance, with the NAT approach delivering substantial quantitative improvements. The model trained on synthetically augmented data achieved an **F1-score of 84%**, closely matching the performance of the clean-data baseline (**86%**) and significantly outperforming the model trained on noisy real-world data, which fell to **66%**. These metrics prove that NAT successfully mitigates the accuracy degradation caused by OCR noise, enabling reliable recognition of domain-specific entities even within highly corrupted historical documents.

This work establishes the first robust framework for multi-type entity recognition in the historical VET domain, filling a major gap in available Digital Humanities tools. By publicly releasing the source code, the authors provide a reproducible and scalable blueprint for the research community. While validated on German texts, the architecture's reliance on synthetic noise injection ensures its transferability to other languages, offering a generalizable solution for extracting structured data from noisy, digitized historical collections across various domains.

---

## Key Findings

*   **Robustness via Noise-Aware Training (NAT):** The integration of synthetically injected OCR errors into the training process significantly mitigates the negative impact of noise found in historical digitized documents.
*   **Efficacy of Fine-Tuning:** Combining transfer learning with multi-stage, domain-specific fine-tuning leads to substantial improvements in both accuracy and robustness under noisy conditions.
*   **Strategic Data Comparison:** The study provides systematic insights by comparing three complementary training strategiesâ€”using noisy, clean, and artificial dataâ€”to determine the most effective approach for noisy contexts.
*   **Cross-Language Potential:** While the method was applied and validated on German documents, the framework is demonstrated to be transferable to arbitrary languages.

---

## Methodology

The researchers propose a robust Named Entity Recognition (NER) framework built upon **Noise-Aware Training (NAT)**. The methodology utilizes transfer learning paired with multi-stage fine-tuning to adapt models to the specific domain of Vocational Education and Training (VET).

To simulate the conditions of historical documents, the approach involves **synthetically injecting OCR errors** into the training data. The study systematically evaluates the model by comparing performance across three distinct training strategies:

1.  **Training on Noisy Data:** Using real-world noisy historical documents.
2.  **Training on Clean Data:** Using standard, uncorrupted text.
3.  **Training on Artificial Data:** Using clean data augmented with synthetic noise.

---

## Technical Details

*   **System Pipeline:** A document processing pipeline utilizing Tesseract OCR and a NER module.
*   **Model Variants:** Three distinct configurations were evaluated:
    *   *Noisy*
    *   *Clean*
    *   *Artificial*
*   **Core Architecture:**
    *   **Noise-Aware Training (NAT):** Injects synthetic OCR errors to harden the model.
    *   **Transfer Learning:** Multi-stage, domain-specific fine-tuning.
    *   **Taxonomy Integration:** Alignment with external standards (ISCO-08, ESCO, KldB 2020, GLMO).
*   **Dataset:**
    *   *Source:* BIBB dataset.
    *   *Volume:* 2,125 historical documents.
    *   *Timeframe:* 1908 to present.
*   **Language Support:** Validated on German texts; architected for transferability to arbitrary languages.

---

## Contributions

*   **Pioneering VET Entity Recognition:** This work is one of the first to successfully recognize multiple entity types within the specific and under-researched domain of Vocational Education and Training (VET).
*   **Advancement in Noisy Document Processing:** The research introduces a specialized noise-aware framework that addresses the specific challenge of OCR-induced noise in historical digitized texts, moving beyond standard clean-text NER approaches.
*   **Open Science and Reproducibility:** The authors contribute publicly available code, enabling the research community to reproduce and apply noise-aware NER techniques to other domain-specific contexts involving noisy data.

---

## Results

*   **Noise Mitigation:** Noise-Aware Training (NAT) significantly mitigates the negative impact of noise inherent in historical digitized documents.
*   **Accuracy Improvements:** Transfer learning combined with multi-stage, domain-specific fine-tuning leads to substantial improvements in accuracy and robustness under noisy conditions.
*   **Strategy Comparison:** The study provides clear insights by comparing Noisy, Clean, and Artificial training strategies, with the Artificial (NAT) strategy performing closest to the upper bound of Clean data performance.
*   **Entity Recognition Success:** The approach successfully recognizes multiple entity types in VET data, advancing over prior single-type limitations, and includes qualitative analysis of recognition rates and errors.
*   **Generalizability:** The framework is validated as transferable to arbitrary languages.

---

**Quality Score:** 9/10 | **References:** 13 citations