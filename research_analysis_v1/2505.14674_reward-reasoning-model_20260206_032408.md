---
title: Reward Reasoning Model
arxiv_id: '2505.14674'
source_url: https://arxiv.org/abs/2505.14674
generated_at: '2026-02-06T03:24:08'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Reward Reasoning Model
*Jiaxin Guo; Zewen Chi; Li Dong; Qingxiu Dong; Xun Wu; Shaohan Huang; Furu Wei*

> ### üìä Quick Facts
> *   **Quality Score:** 9/10
> *   **References:** 40 Citations
> *   **Model Architecture:** Qwen2 Transformer-decoder
> *   **Training Method:** Reward Reasoning via Reinforcement Learning (GRPO)
> *   **Core Innovation:** Self-evolved Chain-of-Thought (CoT)
> *   **Input Type:** Query + 2 Candidate Responses

---

## üìã Executive Summary

Current reward modeling approaches often rely on discriminative classifiers that struggle with complex, nuanced queries where the correct preference is not immediately obvious. Furthermore, the field faces a persistent challenge in effectively utilizing test-time compute to enhance model performance. This paper addresses these limitations by pivoting from static, pattern-matching reward models to a dynamic framework capable of explicit reasoning.

The core innovation is the **Reward Reasoning Model (RRM)**, which reframes reward modeling as a generative text completion task rather than simple classification. Technically, the model utilizes a Qwen2 Transformer-decoder backbone to autoregressively generate a Chain-of-Thought (CoT) reasoning trace before outputting a final judgment in a boxed LaTeX format. A key contribution is the novel training framework that eliminates the dependency on manually labeled reasoning traces. Instead, the model employs "Reward Reasoning via Reinforcement Learning" using Group Relative Policy Optimization (GRPO).

Evaluated across benchmarks such as MMLU-Pro, MATH, and GPQA, RRMs achieve state-of-the-art or superior results in preference proxy evaluations. Specifically, when applied as a reward model for reinforcement learning on the GPQA benchmark, it achieved **40.9% accuracy**, significantly outperforming a **26.8% baseline**. Additionally, the results confirm the model's ability to adaptively exploit test-time compute; it dynamically allocates more computational resources to complex queries requiring deep reasoning while operating efficiently on simpler tasks.

---

## üîç Key Findings

*   **Superior Benchmark Performance:** Reward Reasoning Models (RRMs) achieve state-of-the-art or superior results on reward modeling benchmarks across a diverse range of domains.
*   **Adaptive Compute Utilization:** The models demonstrate the ability to adaptively exploit test-time compute, using it specifically to improve reward accuracy where necessary.
*   **Effective Complex Query Handling:** Through chain-of-thought reasoning, RRMs can successfully determine appropriate rewards for complex queries where the correct answer is not immediately apparent.
*   **Data Efficiency:** The model successfully develops reasoning capabilities without the need for explicit reasoning traces in the training data, relying instead on a self-evolved process.

---

## üõ†Ô∏è Methodology

*   **Architecture:** The authors introduce Reward Reasoning Models (RRMs), which are designed to execute a deliberate reasoning process before generating a final reward score.
*   **Reasoning Mechanism:** The approach utilizes **chain-of-thought (CoT)** reasoning to allow the model to process information more deeply, particularly for complex queries.
*   **Training Framework:** A reinforcement learning (RL) framework is implemented to train the models. This framework fosters "self-evolved" reward reasoning capabilities.
*   **Data Requirement:** The methodology eliminates the dependency on manually labeled reasoning traces, allowing the model to learn reasoning strategies autonomously through the RL framework.

---

## ‚öôÔ∏è Technical Details

**Model Architecture & Task**
*   **Backbone:** Qwen2 Transformer-decoder.
*   **Task Framing:** Reward modeling is framed as a text completion task.
*   **Generation:** Autoregressively generates a Chain-of-Thought (CoT) reasoning trace followed by a final judgment in LaTeX boxed format (e.g., `\boxed{Assistant 1}`).
*   **Input Constraints:** Inputs are restricted to a query and exactly two candidate responses.

**Training Framework**
*   **Algorithm:** Reward Reasoning via Reinforcement Learning based on Deepseek-R1 distilled models.
*   **Optimization:** Uses **Group Relative Policy Optimization (GRPO)** via the `verl` library.
*   **Reward Function:** A sparse, rule-based reward function is used (+1 for correct selection, -1 otherwise), focusing on selection correctness rather than reasoning trace quality.

**Multi-Response Handling**
For handling multiple responses (N > 2), the architecture supports:
*   **ELO Rating System** (round-robin)
*   **Knockout Tournament** (bracket style)
*   **Majority Voting"

---

## üìà Results & Performance

Evaluated using average accuracy on preference proxy evaluations across benchmarks like **MMLU-Pro**, **MATH**, and **GPQA**, the model achieved state-of-the-art or superior results compared to previous models.

*   **GPQA Benchmark:** Achieved **40.9% accuracy** when applied as a reward model for reinforcement learning, significantly outperforming the 26.8% baseline.
*   **Adaptive Compute:** The model demonstrates adaptive test-time compute, effectively utilizing additional computation for complex queries and nuanced tasks.

---

## üèÜ Core Contributions

*   **Solving Test-Time Compute Utilization:** The work addresses the specific open challenge of how to effectively utilize test-time compute to enhance the performance of reward models.
*   **Integration of Reasoning in Reward Modeling:** It establishes a new paradigm where reward models are not just discriminative but also generative in their reasoning process (via CoT), improving alignment with human expectations.
*   **Novel Training Paradigm:** The contribution includes a unique RL-based method for training reward models to reason, removing the bottleneck of requiring explicit reasoning trace datasets.
*   **Resource Availability:** The authors provide public access to the pretrained reward reasoning models, facilitating further research and application in the field.