---
title: 'Binary Neural Networks for Large Language Model: A Survey'
arxiv_id: '2502.19008'
source_url: https://arxiv.org/abs/2502.19008
generated_at: '2026-02-03T18:54:12'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Binary Neural Networks for Large Language Model: A Survey

*Liangdong Liu; Zhitong Zheng; Cong Wang; Tianhuang Su; Zhenyu Yang*

---

> ### **Quick Facts**
>
> *   **Quality Score:** 9/10
> *   **References:** 40 Citations
> *   **Methodology:** Comprehensive Survey
> *   **Key Trend:** Shift from PTQ/QAT to "training-from-scratch" binary architectures.
> *   **Focus:** BitNet and optimized variants (BitNet b1.58, a4.8).

---

## Executive Summary

This paper addresses the critical challenge of the exponential growth in parameter sizes within Large Language Models (LLMs), which creates prohibitive computational and memory resource overheads. As models scale, deployment costs and latency increase significantly, making efficient inference and storage a primary bottleneck. The authors argue that while low-bit quantization is essential for reducing the bit-width of parameters, activations, and gradients, existing methods struggle to maintain performance at extreme compression levels.

This work matters because it establishes a necessary framework for understanding how to bridge the gap between the massive resource demands of state-of-the-art LLMs and the hardware constraints of real-world deployment. The key innovation highlighted is a paradigm shift from traditional Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT) toward "training-from-scratch" binary architectures, specifically the BitNet framework.

Technically, this approach replaces standard Linear layers with BitLinear layers, utilizing 1-bit weights ($+1$ or $-1$) determined by a sign function with mean adjustment ($f_W = \text{Sign}(W - \alpha)$). The method employs Absmax quantization for activations to b-bit precision, stabilized by LayerNorm and local scaling factors. The survey further details optimized variants such as BitNet b1.58, which introduces ternary weights ($\{-1, 0, 1\}$) for feature filtering, and BitNet a4.8, a hybrid architecture that combines sparsified intermediate states with 4-bit activations to improve stability.

Benchmark results presented in the survey illustrate a sharp performance trade-off at lower bit-widths. On WikiText-2, 2-bit PTQ methods suffer severe precision loss, with GPTQ seeing perplexity (PPL) rise to 7,700 compared to a 5.47 FP16 baseline, whereas 4-bit methods like OmniQuant maintain closer proximity (5.74 PPL). Similarly, zero-shot accuracy for Llama-2-7B drops significantly at 2-bit (OmniQuant: 46.98% vs. Baseline: 64.86%), while 3-bit EfficientQAT recovers substantial performance (64.02%). However, the paper notes practical costs: while binary inference is efficient, training BitNet b1.58 is 60% slower than full-precision training, and converting large models (e.g., 70B parameters) using methods like decoupleQ W2 incurs substantial runtime overhead (33.4 hours).

This survey is significant as it provides the first holistic consolidation of binary quantization techniques specifically tailored for the LLM era. By mapping the evolution of Binary Neural Networks (BNNs) from general Deep Neural Networks to modern transformer architectures, the authors clarify the distinction between retrofitting existing models versus training natively binary architectures. This work serves as a vital resource for researchers, categorizing a rapidly diversifying field and highlighting the "training-from-scratch" approach as a promising, albeit computationally intensive, direction for enabling high-performance LLM deployment on resource-constrained edge devices.

---

## Key Findings

*   **Resource Overhead:** The exponential growth of parameter sizes in Large Language Models (LLMs) results in significant computational and memory resource overheads.
*   **Quantization Techniques:** Low-bit quantization is a critical solution for reducing memory and computational demands by decreasing the bit-width of parameters, activations, and gradients.
*   **Methodological Shift:** While previous approaches relied on Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT), a novel paradigm introduced by the BitNet team involves training models from scratch using low-precision binary weights.
*   **Emerging Trend:** The "training-from-scratch" approach has catalyzed the development of numerous binary quantization techniques specifically designed for LLMs.

---

## Methodology

This paper employs a comprehensive survey methodology. The authors systematically review the landscape of binary quantization by first establishing the context of these techniques within general deep neural networks (DNNs) and subsequently exploring their specific adaptation and application to Large Language Models. The analysis covers the theoretical contributions, practical implementations, and application scenarios of these techniques.

---

## Contributions

*   **Comprehensive Review:** Provides a holistic review of binary quantization techniques for LLMs, consolidating a rapidly evolving area of research.
*   **Categorization of Approaches:** Clarifies the distinction between traditional quantization methods (PTQ and QAT) and the emerging "training-from-scratch" binary weight approach (e.g., BitNet).
*   **Bridging DNNs and LLMs:** Maps the evolution of binary quantization from its roots in deep neural networks to its current, specific applications in large language model architectures.

---

## Technical Details

The approach focuses on training-from-scratch Binary Neural Networks (BNNs) for LLMs using the BitNet architecture. The core mechanics and optimized variants are detailed below:

### Core Architecture
*   **BitLinear Layers:** Replaces standard Linear layers.
*   **Weight Quantization:** Utilizes 1-bit weights ($+1$ or $-1$) via a sign function with mean adjustment:
    $$f_W = \text{Sign}(W - \alpha)$$
*   **Activation Quantization:** Employs Absmax quantization to b-bit precision.
    *   Preceded by LayerNorm for variance stability.
    *   Scaled by $Q_b / \gamma$.
*   **Grouping:** Weights and activations are divided into groups to compute local scaling factors.

### Optimized Variants
*   **FBI-LLM & Bi-Mamba:** Introduce column-based learnable scaling factors.
*   **BitNet b1.58:**
    *   Utilizes ternary weights $\{-1, 0, 1\}$.
    *   Uses absmean quantization for feature filtering.
*   **BitNet a4.8:**
    *   Hybrid approach.
    *   Uses 4-bit activations for inputs.
    *   Uses sparsified 8-bit intermediate states.

---

## Results

### Performance Benchmarks
*   **WikiText-2 (Perplexity):**
    *   **FP16 Baseline:** 5.47
    *   **2-bit GPTQ:** 7,700 (Severe precision loss)
    *   **4-bit OmniQuant:** 5.74 (Maintains performance close to FP16)
*   **Llama-2-7B Zero-shot Accuracy:**
    *   **Baseline:** 64.86%
    *   **2-bit OmniQuant:** 46.98% (Significant drop)
    *   **3-bit EfficientQAT:** 64.02% (Recovers substantial performance)

### Computational Overhead
*   **DecoupleQ W2 Runtime:** Increases significantly with model size.
    *   **7B Model:** 2.5 hours
    *   **70B Model:** 33.4 hours
*   **Training Efficiency:** Despite inference efficiency, training BitNet b1.58 is reported to be **60% slower** than the full-precision baseline.

---

**Quality Score:** 9/10 | **References:** 40 citations