---
title: Improving Routing in Sparse Mixture of Experts with Graph of Tokens
arxiv_id: '2505.00792'
source_url: https://arxiv.org/abs/2505.00792
generated_at: '2026-02-06T02:27:51'
quality_score: 9
citation_count: 30
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Improving Routing in Sparse Mixture of Experts with Graph of Tokens

*Tam Nguyen; Ngoc N. Tran; Khai Nguyen; Richard G. Baraniuk*

---

> ### ðŸ“Š Quick Facts
> *   **Quality Score:** 9/10
> *   **Total Citations:** 30
> *   **Core Problem:** Routing instability (fluctuations) in Sparse Mixture of Experts (SMoE).
> *   **Key Metric:** Standard models exhibit up to **33%** token switching in the final epoch.
> *   **Primary Solution:** Similarity-Aware and Attention-Aware routing mechanisms.
> *   **Methodology:** Probabilistic Graphical Model (PGM) framework.

---

## Executive Summary

This research addresses the critical issue of routing instability in Sparse Mixture of Experts (SMoE) models, a phenomenon that significantly hampers model robustness and performance. The authors identify that standard SMoE architectures suffer from "routing fluctuations," particularly during the late stages of training, because expert selection is mutually independent across tokens. This lack of dependency means that similar tokens may be routed to different experts arbitrarily, leading to high entropy in selection. The problem is quantified by the finding that in standard models, up to 33% of tokens switch their assigned experts in the final training epoch, highlighting a lack of convergence that compromises the reliability of large-scale MoE deployments.

The key innovation is the introduction of a Probabilistic Graphical Model (PGM) framework to diagnose and remedy routing flaws by modeling dependencies between tokens. The authors propose Similarity-Aware (S)MoE and Attention-Aware (S)MoE, two novel architectures that depart from independent token routing. Instead of selecting experts based solely on individual affinity scores, these methods construct a "graph of tokens" where expert selection depends on a global similarity variable derived from the entire sequence. Technically, the method calculates a final output based on the routing scores of similar tokens ($\bar{o}_i = \sum S[i, j] r_e(u_j) g_e(u_i)$), ensuring that the routing decision for one token is influenced by its relationship to others. The Attention-Aware variant further leverages the attention matrix to unify the attention mechanism with expert routing.

The study provides both theoretical and empirical evidence that the proposed methods significantly reduce the entropy of expert selection for indecisive tokens. By incorporating token similarities and attention patterns, the authors demonstrate a marked decrease in routing fluctuations compared to baseline MoE-Transformers. While standard models exhibited instability with up to 33% of tokens switching experts at the end of training, the proposed (S)MoE architectures achieved far greater stability. This stabilization translates directly to enhanced prediction accuracy and improved robustness, as the models consistently route similar tokens to the same experts and maintain stable routing decisions despite minor input perturbations.

---

## Key Findings

*   **Root Cause of Instability:** The instability (routing fluctuations) in Sparse Mixture of Experts (SMoE) models stems from the independence of expert selection across tokens, particularly during late training stages.
*   **Importance of Interaction:** Incorporating token interactions and similarities leads to more stable and robust routing mechanisms compared to independent selection.
*   **Entropy Reduction:** Similarity-Aware and Attention-Aware routing methods theoretically reduce the entropy of expert selection, which correlates with decreased routing fluctuations.
*   **Performance Gains:** The proposed methods not only stabilize routing but also significantly enhance prediction accuracy and model robustness compared to baseline MoE-Transformers.

---

## Methodology

The research employs a rigorous combination of theoretical modeling and architectural innovation:

*   **PGM Framework Analysis:** Utilizes a Probabilistic Graphical Model (PGM) framework to analyze and diagnose the limitations of standard SMoE, identifying the lack of dependency between tokens as a critical flaw.
*   **Similarity-Aware (S)MoE:** Proposes a routing mechanism that considers relationships and similarities between tokens during expert selection.
*   **Attention-Aware (S)MoE:** Introduces an extension that covers (S)MoE-Attention blocks by leveraging the attention matrix to guide token routing.
*   **Theoretical Derivation:** Provides a theoretical derivation proving that incorporating similarity and attention into routing reduces the entropy of the selection distribution.

---

## Technical Details

The paper proposes a shift from independent token routing to a dependency-aware framework using Probabilistic Graphical Models (PGMs).

### Core Architecture
Unlike standard SMoE, where expert selection is mutually independent based on TopK affinity scores, the proposed method introduces a dependency between tokens.

1.  **Graph Construction:** It constructs a graph of tokens where expert selection depends on a 'similarity variable' derived from the entire sequence.
2.  **Generative Process:** The process samples a similarity index $s_i$ based on $u_i^T W_s U^T / \tau$ and selects an expert based on the routing scores of a similar token.
3.  **Output Calculation:** The final output is calculated using the formula:
    $$ \bar{o}_i = \sum_{e=1}^E \sum_{j=1}^N S[i, j] r_e(u_j) g_e(u_i) $$
    *Where $S$ represents the similarity score.*

### Extension: Attention-Aware (S)MoE
This variant leverages the attention matrix dependencies to guide routing decisions, further unifying the attention mechanism with expert routing.

---

## Contributions

*   **Theoretical Diagnosis:** Provides a novel theoretical diagnosis of SMoE instability using Probabilistic Graphical Models (PGM), formally linking routing fluctuations to the independence of token expert selection.
*   **Novel Architectures:** Introduces novel routing architectures, Similarity-Aware (S)MoE and Attention-Aware (S)MoE, which integrate token interaction data into the routing process beyond standard softmax gating.
*   **Unified PGM:** Establishes a new PGM for (S)MoE-Attention blocks that unifies the attention mechanism with expert routing, allowing the attention matrix to inform expert assignment.
*   **Empirical Validation:** Offers comprehensive empirical evidence validating that the proposed methods mitigate non-robustness and routing fluctuations, aiding the development of scalable and reliable SMoE implementations.

---

## Results

The study highlights the efficacy of the proposed methods through quantitative and qualitative results:

*   **Instability Quantification:** The study identifies that standard SMoE models suffer from significant routing instability, with up to **33%** of tokens switching their assigned experts in the final epoch.
*   **Entropy Reduction:** The proposed Similarity-Aware and Attention-Aware methods theoretically prove a reduction in entropy for indecisive tokens, which correlates with decreased routing fluctuations and more confident expert assignments.
*   **Enhanced Robustness:** Leads to enhanced prediction accuracy, improved robustness (ensuring consistent routing despite small input perturbations), and better consistency in routing similar tokens to the same experts.