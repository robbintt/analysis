# Bi-Level Policy Optimization with Nystr√∂m Hypergradients
*Arjun Prakash; Naicheng He; Denizalp Goktas; Amy Greenwald*

---

## üìã Executive Summary

> This paper addresses the theoretical discrepancy and practical instability inherent in standard Actor-Critic (AC) reinforcement learning algorithms. Conventional AC methods typically update the actor and critic heuristically or independently, often ignoring the fact that the critic is fundamentally dependent on the actor‚Äôs policy. The authors argue that AC learning should be rigorously characterized as a **Bilevel Optimization (BLO) problem**‚Äîa Stackelberg game‚Äîwhere the critic acts as a follower seeking a best response to the leader (the actor).
>
> The key innovation is the introduction of **Bi-Level Policy Optimization (BLPO)**, a novel algorithm that formalizes the actor-critic dynamic as a nested optimization process. BLPO updates the actor using hypergradients derived via the Implicit Function Theorem, thereby explicitly accounting for the critic‚Äôs response to changes in the actor's parameters. To resolve the computational intractability and numerical instability of calculating the Inverse Hessian Vector Product (IHVP), the researchers integrate the **Nystr√∂m method**.
>
> Benchmarking against Proximal Policy Optimization (PPO) across continuous and discrete tasks, BLPO demonstrated superior performance‚Äîspecifically higher final rewards and faster convergence on Walker2d and Hopper‚Äîwhile maintaining competitive results elsewhere. The significance of this work lies in establishing a rigorous theoretical foundation for actor-critic methods, proving convergence to a local strong Stackelberg equilibrium in polynomial time.

### üìä Quick Facts

| Metric | Details |
| :--- | :--- |
| **Algorithm** | Bi-Level Policy Optimization (BLPO) |
| **Core Technique** | Bilevel Optimization + Nystr√∂m Hypergradients |
| **Problem Type** | Actor-Critic Reinforcement Learning |
| **Key Theorem** | Convergence to local strong Stackelberg equilibrium |
| **Benchmarks** | PPO, MuJoCo (Walker2d, Hopper, etc.) |
| **Quality Score** | 8/10 |
| **Citations** | 40 |

---

## üîë Key Findings

*   **Characterization as Bilevel Optimization:** Actor-critic (AC) reinforcement learning is mathematically equivalent to a Bilevel Optimization (BLO) problem (or Stackelberg game). This implies the critic must be updated as a best response to the actor.
*   **Resolution of Numerical Instability:** The proposed algorithm, BLPO, overcomes the numerical instability typically associated with computing inverse Hessian vector products required for hypergradient calculation.
*   **Theoretical Convergence Guarantees:** BLPO is proven to converge to a local strong Stackelberg equilibrium in polynomial time with high probability, specifically under the assumption of a linear parametrization of the critic's objective.
*   **Competitive Empirical Performance:** BLPO achieves performance on par with or superior to the state-of-the-art Proximal Policy Optimization (PPO) algorithm across a range of discrete and continuous control tasks.

---

## üß™ Methodology

The research methodology centers on reformulating the standard AC update loop into a rigorous hierarchical optimization structure:

*   **Bilevel Framework:** The method treats actor-critic updates as a Stackelberg game where the critic (follower) learns a best response to the actor (leader).
*   **Nested Updates:** The critic‚Äôs update is nested inside the actor‚Äôs update to strictly adhere to the hierarchical structure of the BLO problem.
*   **Hypergradient Computation:** The actor is updated using hypergradients that account for changes in the critic's behavior, rather than ignoring the dependency between the two.
*   **Nystr√∂m Method Integration:** To compute the necessary hypergradients without numerical instability, the algorithm leverages the Nystr√∂m method, a technique typically used for low-rank matrix approximation, to facilitate the inverse Hessian vector product calculation.

---

## ‚öôÔ∏è Technical Details

| Component | Description |
| :--- | :--- |
| **Formulation** | Actor-Critic RL formulated as Bilevel Optimization (BLO). Separates into an outer leader problem (actor parameters $\theta$) and inner follower problem (critic parameters $\omega$). |
| **Update Mechanism** | Uses the Implicit Function Theorem to derive hypergradients for actor updates. |
| **Computational Challenge** | Requires Inverse Hessian Vector Product (IHVP), often computationally intractable. |
| **Optimization Solution** | Uses the **Nystr√∂m method** to create a low-rank approximation of the Hessian, stabilizing the IHVP calculation. |
| **Assumptions** | Requires a linearly parameterized critic, discount factor $\gamma < 1$, and $\mu$-strong convexity. |

---

## üöÄ Contributions

1.  **Algorithmic Innovation (BLPO):** Introduction of "Bilevel Policy Optimization with Nystr√∂m Hypergradients" (BLPO), a novel AC algorithm that integrates nesting with the Nystr√∂m method to enable stable and efficient hypergradient-based policy updates.
2.  **Theoretical Foundation:** A formal proof establishing the convergence properties of the algorithm to a local strong Stackelberg equilibrium, providing a theoretical backing for bilevel approaches in reinforcement learning.
3.  **Conceptual Reframing:** Establishing a rigorous link between actor-critic methods and bilevel optimization, providing a new theoretical lens through which to view and improve upon standard AC algorithms.

---

## üìà Results

BLPO was benchmarked against Proximal Policy Optimization (PPO) on continuous control tasks including **Walker2d**, **Inverted Double Pendulum**, **Hopper**, **Inverted Pendulum**, **Humanoid Standup**, and **Pusher**.

*   **Superior Performance:** BLPO outperformed PPO in terms of final reward and convergence speed on **Walker2d** and **Hopper**.
*   **Competitive Parity:** Maintained competitive performance on **Inverted Double Pendulum** and **Humanoid Standup**.
*   **Broad Applicability:** Evaluation included discrete control benchmarks, confirming the theoretical approach translates effectively to practical empirical performance across both domains.

---

**References:** 40 citations  
**Quality Score:** 8/10