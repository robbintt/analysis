# See What You Need: Query-Aware Visual Intelligence through Reasoning-Perception Loops

*Zixuan Dong; Baoyun Peng; Yufei Wang; Lin Liu; Xinxin Dong; Yunlong Cao; Xiaodong Wang*

---

> ### ðŸ“Š Quick Facts
>
> *   **Quality Score:** 8/10
> *   **References:** 29 Citations
> *   **Framework:** CAVIA (Training-Free)
> *   **Key Performance Indicators:**
>     *   **EgoSchema:** 65.7% (+5.3%)
>     *   **NExT-QA:** 76.1% (+2.6%)
>     *   **IntentQA:** 73.8% (+6.9%)

---

## Executive Summary

Existing video understanding paradigms face a critical trade-off between computational efficiency and information retention. Current approaches typically employ decoupled, rigid pipelines that either exhaustively process entire videosâ€”incurring prohibitive computational costs for long-form contentâ€”or rely on static, dense captioning that frequently discards subtle, query-relevant details. This limitation creates a bottleneck where vision-language models (VLMs) and large language models (LLMs) are overwhelmed by irrelevant data or lack the specific visual evidence required to answer complex questions about intent and long-horizon temporal dynamics.

The paper introduces **CAVIA (Query-Aware Visual Intelligence)**, a training-free framework that replaces static pipelines with a closed-loop reasoning-perception cycle. Instead of processing a video passively, CAVIA treats visual extraction as a dynamic, iterative process where reasoning modules guide perception based on specific query requirements. The system is defined by three key technical innovations: **Hierarchical Reasoning & Guided Localization**, **Cross-Modal Semantic Bridging**, and **Confidence-Driven Iterative Synthesis**. By employing this architecture, the framework identifies when current context is insufficient and utilizes Cross-Modal Semantic Bridging to generate precise spatiotemporal instructions, retrieving only the necessary visual evidence and actively updating memory until a confidence threshold is met.

CAVIA achieved state-of-the-art performance on several challenging benchmarks, demonstrating significant improvements without requiring additional model training. These results underscore the methodâ€™s high efficacy in handling complex tasks involving intent recognition and long-horizon temporal reasoning, successfully reducing information loss compared to caption-based methods while lowering computational overload relative to end-to-end VLMs.

The significance of this research lies in establishing a scalable, generalizable paradigm that shifts video understanding from passive observation to active, query-driven retrieval. By proving that high-level reasoning can effectively coordinate perception to meet specific information needs through mechanisms like Confidence-Driven Iterative Synthesis, CAVIA resolves the traditional conflict between detail preservation and computational efficiency.

---

## Key Findings

*   **State-of-the-Art Performance:** The proposed framework, CAVIA, achieved superior results on major benchmarks, including **EgoSchema** (+5.3% to 65.7%), **NExT-QA** (+2.6% to 76.1%), and **IntentQA** (+6.9% to 73.8%).
*   **Efficacy of Dynamic Coordination:** Tightly coordinating reasoning and perception in a closed-loop system is more effective than traditional decoupled, rigid pipelines.
*   **Training-Free Efficiency:** Significant performance improvements were attained without requiring additional training, making it a highly efficient solution.
*   **Adaptability to Query Requirements:** Adapting visual extraction to specific query needs significantly reduces information loss compared to exhaustive processing methods.

---

## Methodology

**CAVIA (Query-Aware Visual Intelligence)** operates as a training-free, closed-loop system that integrates reasoning and perception through dynamic coordination. It utilizes a reasoning-perception loop where reasoning modules guide visual extraction by identifying information gaps rather than processing videos exhaustively.

The framework relies on three primary technical innovations:
1.  **Hierarchical Reasoning & Guided Localization**
2.  **Cross-Modal Semantic Bridging**
3.  **Confidence-Driven Iterative Synthesis**

---

## Technical Details

The CAVIA framework operates on a closed-loop reasoning-perception paradigm, shifting from static pipelines to a system where reasoning actively guides visual perception. It consists of a cyclic process with four key components:

*   **Hierarchical Localization:**
    *   Implements coarse-to-fine retrieval.
    *   Responsible for specific frame identification.
*   **Reasoning Gap Identification:**
    *   Analyzes context sufficiency based on confidence levels.
    *   Determines if more visual information is required.
*   **Targeted Multimodal Prompting:**
    *   Generates specific spatiotemporal instructions to guide the system.
*   **Adaptive Extraction:**
    *   Executes instructions via Vision-Language Models (VLM).
    *   Operates in two modes: **Spatial** (for specific objects/details) or **Temporal** (for actions/events) modes.

**The Loop Mechanism:**
*   Includes **Iterative Caption Enhancement** to update memory.
*   Continues the cycle until a sufficient confidence threshold is reached or the `MAX_REFLECTION` limit is met.
*   The framework is training-free, coordinating pre-trained Large Language Models (LLMs) and Vision-Language Models (VLMs).

---

## Experimental Results

CAVIA demonstrated consistent improvements over State-of-the-Art methods on long-form video benchmarks. The results highlight its ability to understand intent and long-horizon temporal dynamics.

| Benchmark | Accuracy | Improvement |
| :--- | :--- | :--- |
| **EgoSchema** | 65.7% | +5.3% |
| **NExT-QA** | 76.1% | +2.6% |
| **IntentQA** | 73.8% | +6.9% |

**Qualitative Outcomes:**
*   Reduced information loss compared to caption-based methods.
*   Lowered computational overload compared to end-to-end VLMs.

---

## Contributions

*   **Paradigm Shift:** Introduces a scalable paradigm for video understanding that shifts from decoupled reasoning to dynamic, closed-loop coordination.
*   **Trade-off Resolution:** Addresses the trade-off between information loss and computational inefficiency by making visual extraction adaptive to the query.
*   **Generalizable Framework:** Provides a training-free framework that improves query-relevant detail extraction and sets new performance standards without the need for retraining.