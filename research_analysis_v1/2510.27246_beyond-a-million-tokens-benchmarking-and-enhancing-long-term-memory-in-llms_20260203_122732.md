---
title: 'Beyond a Million Tokens: Benchmarking and Enhancing Long-Term Memory in LLMs'
arxiv_id: '2510.27246'
source_url: https://arxiv.org/abs/2510.27246
generated_at: '2026-02-03T12:27:32'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Beyond a Million Tokens: Benchmarking and Enhancing Long-Term Memory in LLMs

*Mohammad Tavakoli; Alireza Salemi; Carrie Ye; Mohamed Abdalla; Hamed Zamani; J Ross Mitchell*

---

> ### **Quick Facts Sidebar**
>
> *   **Dataset Scale:** 100 conversations (100K – 10M tokens)
> *   **Evaluation Depth:** 2,000 probing questions
> *   **Performance Gain:** 3.5% – 12.69% improvement over baselines
> *   **Key Metrics:** Nugget Evaluation, Kendall Tau-b Coefficient
> *   **Novel Components:** Episodic Memory, Working Memory, Scratchpad
> *   **Quality Score:** 9/10

---

## Executive Summary

Current state-of-the-art Large Language Models (LLMs) boast context windows of up to one million tokens, yet this research reveals that raw capacity does not translate to effective long-term memory or reasoning. As dialogue lengths extend, even the most advanced models struggle to retrieve specific information and maintain coherence, suffering from performance degradation regardless of whether retrieval-augmented generation (RAG) is employed. This deficiency presents a critical bottleneck for deploying LLMs in applications requiring sustained interactions, such as personalized assistants or long-term data analysis, where the model must synthesize information presented millions of tokens prior.

The authors address this limitation through two major contributions. First, they introduce the **BEAM benchmark**, a rigorous pipeline capable of synthesizing coherent, diverse dialogues scaling up to 10 million tokens. BEAM utilizes a hierarchical planning approach with a recursive "Conversation Plan Generator" to create narratives, embedding 2,000 probing questions designed to test 10 specific memory abilities validated by humans. Second, they propose the **LIGHT framework**, a novel architecture inspired by human cognitive science. LIGHT integrates a tripartite memory system: Episodic Memory (a long-term index using Key-Value extraction and vector databases), Working Memory (a buffer of the last N turn pairs), and a Scratchpad (a mechanism for dynamic reasoning and context compression updated at every turn).

Evaluation on the BEAM dataset demonstrates that standard LLMs fail to maintain performance as context length increases. Using Nugget Evaluation for semantic unit scoring and Kendall Tau-b for event ordering, the LIGHT framework consistently outperformed standard baselines and SOTA models. The architecture achieved performance improvements ranging from 3.5% to 12.69%. Furthermore, ablation studies quantitatively confirmed that all three memory components are distinct and necessary; removing the episodic memory, working memory, or scratchpad individually led to a drop in performance, validating the necessity of the integrated design.

This research significantly shifts the paradigm for long-context AI from simply expanding context windows to architecting specialized memory systems. By demonstrating that existing SOTA models and standard RAG techniques are insufficient for robust long-term memory, the paper establishes BEAM as an essential new standard for benchmarking future models. The success of the LIGHT framework suggests that mimicking human cognitive structures—specifically the separation of short-term buffering, long-term storage, and active reasoning—is a more effective path toward creating agents capable of truly extended interaction than relying solely on parameter scaling or passive retrieval.

---

## Key Findings

*   **Context Window Limitations:** State-of-the-art LLMs with 1M token context windows struggle to perform effectively as dialogue length increases, regardless of retrieval-augmentation (RAG) usage.
*   **Superior Performance:** The proposed LIGHT framework consistently outperforms standard baselines, achieving performance improvements ranging from **3.5% to 12.69%**.
*   **Component Necessity:** Ablation studies confirm that episodic memory, working memory, and the scratchpad are all distinct and necessary components for the model's performance.

---

## Methodology

The study employed a two-pronged approach involving the creation of a new benchmarking standard and a novel architectural framework:

1.  **BEAM Benchmark Development:** The authors developed BEAM, an automated framework to synthesize long, coherent, and diverse conversations (up to 10 million tokens) with probing questions.
2.  **LIGHT Framework Proposal:** They proposed the LIGHT framework, which augments LLMs with a tripartite memory system consisting of long-term episodic memory, short-term working memory, and a scratchpad.
3.  **Evaluation Protocol:** The study evaluated various LLMs with and without retrieval augmentation against LIGHT-enhanced models using the BEAM benchmark.

---

## Contributions

*   **BEAM Benchmark:** Introduction of a rigorous large-scale dataset for testing long-term memory and long-context reasoning.
*   **LIGHT Framework:** Proposal of a novel architecture that integrates episodic, working, and scratchpad memory systems.
*   **Paradigm Shift:** Evidence demonstrating that simply expanding context windows or using standard retrieval augmentation is insufficient for robust long-term memory, directing research toward memory-specific architectures.

---

## Technical Details

### The BEAM Pipeline
*   **Hierarchical Planning:** Generates long dialogues using a recursive "Conversation Plan Generator" to guide narrative flow.
*   **Turn Generation:** Includes mechanisms for question and follow-up detection.
*   **Validation:** Probing questions target 10 specific memory abilities validated by humans.

### The LIGHT Framework Architecture
Inspired by human cognitive science, LIGHT integrates three distinct memory systems:

*   **Episodic Memory:** A long-term index utilizing Key-Value extraction, embedding models, and vector databases.
*   **Working Memory:** A buffer storing the last $N$ turn pairs for immediate context.
*   **Scratchpad:** A dedicated space for reasoning and compression, updated every turn.

**Inference Process:** The model jointly utilizes content from the Episodic Memory, Working Memory buffer, and the Scratchpad during generation.

---

## Results

*   **Dataset Composition:** The final dataset comprises 100 conversations ranging from 100K to 10M tokens with an evaluation set of 2,000 probing questions.
*   **Evaluation Metrics:**
    *   **Nugget Evaluation:** Scores atomic semantic units from 0 to 1 for 9 memory abilities.
    *   **Kendall Tau-b Coefficient:** Used to assess Event Ordering.
*   **Performance Degradation:** Experimental findings show that state-of-the-art LLMs suffer performance degradation as conversation length increases, even with large context windows.
*   **Framework Success:** The LIGHT framework consistently outperforms baselines. Ablation studies confirm that Episodic memory, Working memory, and the Scratchpad are all strictly necessary for optimal performance.

---

**Quality Score:** 9/10  
**References:** 40 citations