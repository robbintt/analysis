---
title: Algorithms for Adversarially Robust Deep Learning
arxiv_id: '2509.19100'
source_url: https://arxiv.org/abs/2509.19100
generated_at: '2026-01-26T09:02:50'
quality_score: 8
citation_count: 0
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# Algorithms for Adversarially Robust Deep Learning

*Alexander Beck, George J. Pappas, Hamed Hassani*

---

> ###  Quick Facts
>
> *   **Primary Focus:** Adversarial robustness in Deep Learning and Large Language Models (LLMs)
> *   **Key Frameworks Introduced:**
>     *   Primal-Dual Adversarial Learning (P-DALE)
>     *   Empirical Quantile Risk Minimization (EQRM)
>     *   SmoothLLM
> *   **Top Defense Efficacy:** Vicuna ASR reduced from 100.0%  8.0% against GCG attacks
> *   **Most Robust Baseline Model:** Claude-1 (1.3% ASR)
> *   **Quality Score:** 8/10

---

## Executive Summary

The reliable deployment of deep learning systems is hindered by their inability to balance predictive performance with resilience in hostile environments. Large Language Models (LLMs) face vulnerabilities from adversarial perturbations that degrade accuracy and Out-of-Distribution (OOD) shifts that expose risks in the tail ends of distribution. These vulnerabilities manifest as "jailbreaking" attacks in generative AI, where adversarial prompts bypass alignment guardrails to generate unsafe content. Addressing these challenges requires algorithms that move beyond nominal error rates to manage worst-case scenarios and catastrophic tail risks.

This research introduces unified frameworks designed to enhance robustness across optimization, generalization, and inference. **Primal-Dual Adversarial Learning (P-DALE)** improves robust optimization by reformulating the problem through Lagrangian duality, treating the dual variable as a learnable optimization variable to dynamically navigate trade-offs. **Empirical Quantile Risk Minimization (EQRM)** targets OOD generalization by shifting focus from mean risk to the $\alpha$-quantile of the risk distribution using Kernel Density Estimation. **SmoothLLM** addresses LLM safety through a randomized defense mechanism that applies smoothing directly to input tokens, disrupting the semantic structure of adversarial prompts.

Experimental validation demonstrates significant efficacy across theoretical and empirical benchmarks. In white-box attack settings, the SmoothLLM defense drastically reduced Attack Success Rates (ASR) against GCG attacks. Testing revealed notable disparities in model susceptibility, with Vicuna showing high vulnerability compared to proprietary models like Claude-1 and GPT-4. On the theoretical front, EQRM achieved uniform convergence, establishing that empirical $\alpha$-quantile risk reliably approximates population risk as domain and sample sizes increase.

The significance of this work lies in its comprehensive integration of rigorous statistical learning with practical defense mechanisms for generative AI. By establishing the uniform convergence properties of EQRM, the research provides a mathematically grounded path for modeling and minimizing tail risks in unstable environments. Coupled with P-DALE's dynamic optimization, this offers a solution to the persistent dilemma of sacrificing accuracy for robustness. SmoothLLM sets a new benchmark for LLM safety, offering an immediately deployable defense against jailbreaking that preserves semantic integrity of benign inputs while neutralizing adversarial prompts.

---

## Key Findings

> **Note:** No specific key findings were explicitly extracted from the provided source text.

---

## Technical Details

The dissertation presents three distinct frameworks aimed at solving robustness and generalization issues:

### 1. Primal-Dual Adversarial Learning (P-DALE)
*   **Concept:** Reformulates robust optimization using Lagrangian duality.
*   **Mechanism:** Treats the dual variable $\nu$ as an optimization variable.
*   **Goal:** Dynamically trade off robust and nominal loss during the training process.

### 2. Empirical Quantile Risk Minimization (EQRM)
*   **Target:** Out-of-Distribution (OOD) generalization.
*   **Method:** Minimizes the $\alpha$-quantile of the risk distribution rather than the mean risk.
*   **Technique:** Utilizes Kernel Density Estimation (KDE) for approximation and right-tail risk extrapolation.

### 3. SmoothLLM
*   **Type:** Randomized defense mechanism designed for Large Language Models.
*   **Objective:** Defense against "jailbreaking" attacks.
*   **Strategy:** Disrupts adversarial prompts by randomly smoothing inputs or logits, breaking the semantic structure of malicious prompts while preserving benign inputs.

---

## Results

### Baseline Vulnerability Analysis
Experimental results established baseline Attack Success Rates (ASR) across various models, highlighting significant disparities in robustness:

| Model | Baseline ASR |
| :--- | :--- |
| **Vicuna** | 98.1% |
| **Gemma** | 51.0% |
| **PaLM-2** | 24.9% |
| **GPT-3.5** | 28.7% |
| **Llama-2** | 24.9% |
| **GPT-4** | 5.6% |
| **Claude-2** | 1.6% |
| **Claude-1** | 1.3% |

### SmoothLLM Defense Efficacy
When applying the SmoothLLM defense mechanism against GCG attacks, significant reductions in ASR were observed:

| Model | Original ASR | SmoothLLM Defense ASR |
| :--- | :--- | :--- |
| **Vicuna** | 100.0% | **8.0%** |
| **GPT-3.5** | 46.0% | **8.0%** |
| **Llama-2** | 58.0% | **56.0%** |

### Theoretical Results
*   **EQRM:** Analysis proved uniform convergence, demonstrating that the empirical $\alpha$-quantile risk approximates the population $\alpha$-quantile risk as domain and sample sizes increase.

---

## Methodology & Contributions

> **Note:** specific methodology and contribution details were not extracted from the provided source text. Please refer to the Technical Details and Executive Summary for context on the frameworks and impact.