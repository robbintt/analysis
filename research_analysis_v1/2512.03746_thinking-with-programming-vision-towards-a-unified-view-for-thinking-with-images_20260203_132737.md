---
title: 'Thinking with Programming Vision: Towards a Unified View for Thinking with
  Images'
arxiv_id: '2512.03746'
source_url: https://arxiv.org/abs/2512.03746
generated_at: '2026-02-03T13:27:37'
quality_score: 7
citation_count: 4
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Thinking with Programming Vision: Towards a Unified View for Thinking with Images

*Zirun Guo; Minjie Hong; Feng Zhang; Kai Jia; Tao Jin*

---

> ### ðŸ’¡ Quick Facts
> *   **Quality Score:** 7/10
> *   **Architectures:** Qwen2.5-VL, Qwen3-VL
> *   **Key Metric:** Baseline tools yield only 2-5% accuracy gains.
> *   **References:** 4 citations
> *   **Innovation:** Code-as-tool paradigm with Dense Process RL.

---

## Executive Summary

This research addresses a critical fragility in State-of-the-Art (SOTA) Multimodal Large Language Models (MLLMs): their susceptibility to severe performance degradation caused by simple image orientation changes or natural corruptions. The authors highlight that while tool augmentation is a standard strategy for extending model capabilities, existing methods relying on fixed tool registries offer marginal utilityâ€”yielding only **2-5% accuracy gains** on benchmarks like V* and HRBenchâ€”and fail to resolve underlying robustness issues. This limitation reveals a substantial reliability gap between benchmark performance and real-world utility, as visual inputs in practical applications are rarely standardized or free from corruption.

To overcome these limitations, the authors introduce **"CodeVision,"** a framework implementing a "code-as-tool" paradigm that replaces static tool registries with a universal interface for dynamic code generation. This decouples the model from specific tool implementations, enhancing scalability and flexibility. The approach is driven by a two-stage training pipeline: Supervised Fine-Tuning (SFT) on a curated dataset designed to teach multi-turn tool composition and error recovery, followed by Reinforcement Learning (RL) utilizing a novel dense process reward function. Unlike standard sparse rewards, this dense process function provides granular feedback on intermediate reasoning steps, specifically incentivizing strategic planning and execution efficiency to foster emergent capabilities in complex reasoning tasks.

Experimental results, validated on Qwen2.5-VL and Qwen3-VL architectures, quantify the inefficiencies of current baselines and the efficacy of the proposed approach. While existing tool-augmented methods achieve only **2-5% accuracy gains** on standard benchmarks, the study finds that applying the proposed RL strategy *without* tools allows models to match the performance of standard approaches that utilize tools. In contrast, when the RL strategy is applied within the CodeVision framework, the model achieves substantial accuracy improvements on a newly introduced rigorous benchmark suite focused on robustness and multi-tool reasoning, successfully exhibiting flexible tool composition and error recovery under conditions that typically cause failure in SOTA models.

The significance of this work lies in identifying the "robustness gap" regarding image orientation and proposing a unified, programmatic solution that supersedes static tool registries. By demonstrating that code generation is a superior interface for "thinking with images," the authors provide a scalable path forward for enhancing MLLM reasoning and robustness. Furthermore, the release of high-quality SFT and RL datasets, alongside a challenging new benchmark suite for robustness and multi-tool reasoning, offers the community standardized resources to evaluate tool-augmented vision models, establishing a new precedent for future research in dynamic, programmatic visual reasoning.

---

## Key Findings

*   **Critical Brittleness in SOTA MLLMs:** Significant performance degradation occurs with simple orientation changes or natural corruptions.
*   **Superiority of Code-as-Tool Interface:** Code generation (CodeVision) is more effective and scalable than fixed tool registries.
*   **Emergent Capabilities via RL:** SFT and RL foster flexible tool composition, chained execution, and error recovery.
*   **Validation on Qwen Architectures:** Experiments on Qwen2.5-VL and Qwen3-VL show significant improvements over baselines.

---

## Methodology

The proposed framework utilizes a novel architecture and training regimen to overcome the limitations of fixed tool registries.

**Framework Architecture**
*   **CodeVision:** A flexible framework using code generation as a universal interface for dynamic image operations.

**Two-Stage Training Pipeline**
1.  **Supervised Fine-Tuning (SFT):** Conducted on a curated dataset specifically designed for multi-turn tool composition and error recovery.
2.  **Reinforcement Learning (RL):** Utilizes a novel dense process reward function to incentivize strategic behaviors and efficient planning.

**Evaluation Protocol**
*   Construction of new SFT and RL datasets.
*   Introduction of a challenging benchmark suite designed to test robustness and multi-tool reasoning capabilities.

---

## Technical Details

**Core Paradigm**
CodeVision employs a code-as-tool paradigm, using a universal interface where the model generates code to invoke image operations dynamically rather than relying on a fixed tool registry. This enhances scalability by decoupling the model from specific tool implementations.

**Training Methodology**
*   **SFT:** Uses a new high-quality dataset focused on complex interactions, specifically multi-turn tool composition and error recovery.
*   **RL:** Implements a dense process reward function that provides granular feedback on intermediate reasoning steps to encourage strategic planning and efficiency.

**Implementation & Resources**
*   **Architectures:** The approach is implemented and validated on Qwen2.5-VL and Qwen3-VL.
*   **New Assets:** Introduces proprietary datasets and a benchmark suite for testing orientation robustness and multi-tool reasoning.

---

## Results

*   **Baseline Inefficiency:** Current SOTA tool-augmented methods yield only 2-5% accuracy gains on benchmarks like V* and HRBench and exhibit significant degradation with orientation changes.
*   **RL Efficacy:** Experiments show that RL without tools can match the performance of models using tools in existing setups.
*   **CodeVision Performance:** Demonstrates significant improvements over baselines on the new benchmark suite regarding robustness and reasoning.
*   **Emergent Behaviors:** The model exhibits flexible tool composition, efficient chained execution, and robust error recovery.

---

## Contributions

*   **Identification of Robustness Gap:** Highlights a previously overlooked weakness in MLLMs regarding sensitivity to image orientation and corruption.
*   **Code-as-Tool Paradigm:** Introduces a unified, code-based framework replacing static tool registries with a programmatic interface.
*   **Advanced Training Strategy:** Contributes a novel training methodology combining SFT on complex interactions with RL based on dense process rewards.
*   **New Resources for the Community:** Provides new high-quality datasets and a rigorous benchmark suite for future research.