---
title: 'DashboardQA: Benchmarking Multimodal Agents for Question Answering on Interactive
  Dashboards'
arxiv_id: '2508.17398'
source_url: https://arxiv.org/abs/2508.17398
generated_at: '2026-02-06T05:52:05'
quality_score: 9
citation_count: 25
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# DashboardQA: Benchmarking Multimodal Agents for Question Answering on Interactive Dashboards

*Aaryaman Kartha; Ahmed Masry; Mohammed Saidul Islam; Thinh Lang; Shadikur Rahman; Ridwan Mahbub; Mizanur Rahman; Mahir Ahmed; Md Rizwan Parvez; Enamul Hoque; Shafiq Joty*

---

> ### üìä Quick Facts
>
> *   **Top Performing Model:** Gemini-Pro-2.5 (38.69% Accuracy)
> *   **Benchmark Scope:** 112 Interactive Dashboards, 405 QA Pairs
> *   **Primary Failure Modes:** Grounding, Planning, Reasoning
> *   **Dominant Control Type:** Dropdowns (92.31% prevalence)
> *   **Task Complexity:** 2‚Äì3 Interaction States
> *   **Environment:** OSWorld Ubuntu VM
> *   **Quality Score:** 9/10

---

## üìÑ Executive Summary

This research addresses the inability of current multimodal agents to effectively perform reasoning and question answering on interactive dashboards. While general-purpose GUI agents have improved, they struggle significantly with the dynamic nature of data visualization tools, which require users to manipulate filters, drill down into data, and correlate views to extract insights. Existing benchmarks are insufficient for this task because they primarily focus on static charts or simple web navigation rather than the rich interactivity found in real-world business intelligence tools like Tableau.

The key innovation is the introduction of **DashboardQA**, the first benchmark specifically designed to evaluate multimodal agents on interactive dashboards. Technically, the authors constructed this dataset by curating 112 high-quality, real-world dashboards from Tableau Public. They employed a three-step human-VLM collaboration pipeline to generate 405 diverse question-answer pairs across five categories: multiple-choice, factoid, hypothetical, multi-dashboard, and conversational. The evaluation environment is based on OSWorld, an Ubuntu virtual machine where agents process inputs via visual screenshots and structural accessibility trees (A11y).

The evaluation results highlight a substantial performance gap, demonstrating that state-of-the-art agents are not yet proficient at interactive data exploration. The top-performing model, **Gemini-Pro-2.5**, achieved only **38.69%** accuracy, while the specialized OpenAI CUA agent reached just **22.69%**. A diagnostic analysis of the failure modes revealed that current agents struggle with three core capabilities: grounding (locating and manipulating specific UI elements), planning (determining the correct sequence of interactions), and complex reasoning (performing multi-step visual math). These low scores indicate that modern agents lack the robustness necessary for complex, dynamic environments.

---

## üîç Key Findings

*   **Significant Performance Gap:** There is a substantial disconnect in current agents' ability to handle interactive dashboard reasoning. The top model achieved only 38.69% accuracy, and the OpenAI CUA agent reached only 22.69%.
*   **Core Capability Struggles:** Current GUI agents struggle with three specific areas:
    *   **Grounding:** Locating and manipulating dashboard elements.
    *   **Planning:** Determining interaction trajectories.
    *   **Reasoning:** Performing complex, multi-step reasoning.
*   **Insufficient Benchmarks:** Existing benchmarks are inadequate for evaluating modern GUI agents because they focus on static charts rather than the rich interactivity required in real-world scenarios.
*   **High Benchmark Difficulty:** DashboardQA demonstrates high difficulty, indicating that state-of-the-art multimodal agents are not yet proficient at interactive data exploration.

---

## üß™ Methodology

The authors constructed **DashboardQA**, the first benchmark designed to assess vision-language GUI agents on interactive dashboards. The process involved:

1.  **Curation:** Collecting 112 real-world interactive dashboards from Tableau Public.
2.  **Dataset Creation:** Generating 405 question-answer pairs categorized into five types:
    *   Multiple-choice
    *   Factoid
    *   Hypothetical
    *   Multi-dashboard
    *   Conversational
3.  **Evaluation:** Testing a variety of leading closed- and open-source GUI agents to measure their ability to comprehend, interact with, and reason over dashboard elements.

---

## ‚öôÔ∏è Technical Details

### Environment & Action
*   **Platform:** Agents operate within an OSWorld Ubuntu virtual machine.
*   **Input Processing:** Inputs are processed via visual screenshots and structural accessibility trees (A11y).
*   **Execution:** Actions are executed through mouse and keyboard inputs using pyautogui commands (move, scroll, click).
*   **Workflow:** The agent workflow iteratively observes the state, plans actions, and executes them.

### Benchmark Construction
*   **Source:** 112 high-quality interactive dashboards sourced from Tableau Public.
*   **QA Generation:** A three-step human-VLM collaboration pipeline:
    1.  Human annotators create seeds.
    2.  VLMs (GPT-4o, Gemini, Claude) expand them.
    3.  Humans refine the output.

### Classification & Complexity
*   **Question Categories:** Factoid, MCQ, Hypothetical, Conversational, and Multi-Dashboard.
*   **Task Complexity:** Defined by the number of 'states' (interaction rounds) required, typically 2‚Äì3.

---

## üìà Results

*   **Top Performers:** Gemini-Pro-2.5 achieved the top accuracy of 38.69%, followed by the OpenAI CUA Agent at 22.69%.
*   **Primary Bottlenecks:**
    *   **Grounding:** Locating/manipulating elements (Dropdowns are dominant, 92.31% prevalence).
    *   **Planning:** Interaction trajectories.
    *   **Reasoning:** Multi-step visual math.
*   **Dataset Statistics:** The dataset comprises 112 dashboards curated by 8 annotators.
*   **Complexity:** The benchmark is designed to require agents to navigate 2 to 3 distinct interaction states to derive correct answers.

---

## üöÄ Contributions

1.  **New Benchmark:** Introduction of DashboardQA, the first benchmark explicitly targeting multimodal agents for question answering on interactive dashboards.
2.  **Standardized Dataset:** Provision of a comprehensive dataset featuring real-world Tableau Public dashboards and 405 diverse QA pairs.
3.  **Diagnostic Analysis:** Offering a diagnostic analysis that identifies specific failure modes in current SOTA models (grounding, planning, reasoning), establishing a baseline for future research in GUI-based agents.

---

## üìù References
*   **Total Citations:** 25
*   **Quality Score:** 9/10