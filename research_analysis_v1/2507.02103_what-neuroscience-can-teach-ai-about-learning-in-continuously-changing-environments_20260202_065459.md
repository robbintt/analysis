# What Neuroscience Can Teach AI About Learning in Continually Changing Environments

*Daniel Durstewitz; Bruno Averbeck; Georgia Koppe*

---

> ###  Quick Facts
>
> *   **Quality Score:** 6/10
> *   **Citations:** 40 references
> *   **Core Focus:** Neuro-Dynamical Framework & Continual Learning
> *   **Key Concept:** Manifold Attractors vs. Static Gradient Descent
> *   **Methodology:** Multidisciplinary Literature Synthesis & Comparative Analysis

---

## Executive Summary

This review addresses the fundamental disparity between modern artificial intelligence and biological systems concerning adaptability in non-stationary environments. While current AI architectures depend on static, gradient descent-based training on fixed datasets, they struggle with "catastrophic forgetting" when encountering continuously changing data streams. In contrast, biological systems adapt fluidly through rapid shifts in neuronal population activity. This limitation is a critical barrier for deploying AI in dynamic, high-stakes applications such as autonomous vehicles and robotics, where agents must update knowledge in real-time without erasing previous capabilities.

To bridge this gap, the authors propose a **"Neuro-Dynamical Framework"** grounded in dynamical systems theory, shifting the focus from pure error-driven optimization to **"manifold attractors"**—continuous sets of marginally stable fixed points governed by vector fields. This approach synthesizes biological principles, emphasizing processing across vastly different time constants—ranging from milliseconds to hours—and utilizing phenomena like resonance and entrainment. Unlike standard artificial approaches such as experience replay or Bayesian updating, which attempt to patch static architectures, the review suggests that true continuous adaptation requires mechanisms like high-dimensional manifold attractors that allow neural dynamics to swiftly reconfigure states in response to abrupt environmental shifts.

The review synthesizes experimental findings that quantify the performance gap between traditional deep learning and biological adaptability. In class-incremental CIFAR experiments, common deep learning systems demonstrated a steady decay in performance, eventually dropping below "scratch" levels, while "continual backpropagation" methods were shown to mitigate this decay. Conversely, neuroscience experiments involving rule switch tasks revealed that learning is not gradual; subjects displayed abrupt behavioral jumps from chance to perfect performance, which tightly correlated with change points in neural population activity.

This work significantly advances the field of **NeuroAI** by establishing a concrete research agenda grounded in the dynamical systems theory of biological brains. It articulates a bidirectional synergy, positing that while AI models can serve as tools to decode biological neural computations, neuroscience provides the essential theoretical blueprint for building non-stationary learning systems.

---

## Key Findings

*   **Fundamental Disparity in Learning Mechanisms:** Modern AI relies on static, costly training on fixed datasets, unlike biological systems that continuously adapt through rapid shifts.
*   **Neurobiological Basis for Adaptability:** Biological adaptation is characterized by rapid behavioral changes and sudden transitions in neuronal population activity.
*   **Imperative for Dynamic AI Systems:** Computational capacity for continuous adaptation is vital for real-world AI applications like robotics and autonomous vehicles.
*   **Bidirectional Synergy in NeuroAI:** A reciprocal relationship exists where neuroscience informs AI learning strategies and AI models help understand biological neural computations.

---

## Methodology

The research employs a multidisciplinary approach to formulate a new roadmap for AI development:

*   **Multidisciplinary Literature Synthesis:** Integrates existing literature from AI research on continual/in-context learning and neuroscience research on behavioral tasks.
*   **Comparative Analysis:** Facilitates a direct comparison between the operational mechanisms of artificial neural networks and biological neural populations.
*   **Agenda Formulation:** Outlines a conceptual roadmap and research agenda for applying biological insights to current AI challenges.

---

## Technical Details

### Critique of Current AI Architectures
The paper highlights significant flaws in mainstream deep learning approaches:
*   **Dependency:** Relies heavily on Gradient Descent-based learning.
*   **Mitigation Attempts:** Current methods use experience replay and Bayesian updating to prevent catastrophic forgetting.
*   **Foundation Models:** Rely on recombination of learned functions for Out-of-Distribution (OOD) inference rather than true adaptation.

### The Proposed Neuro-Dynamical Framework
The authors suggest moving away from static optimization toward dynamical systems theory:
*   **Core Concept:** Processing across time constants from milliseconds to hours.
*   **Resonance:** Utilizes phenomena like entrainment.
*   **Manifold Attractors:** Proposed as continuous sets of marginally stable fixed points in a state space governed by vector fields.

### Analytical Methods
*   **Change Point Analysis:** Used to infer specific transition trials in neural data.

---

## Experimental Results

### Artificial Intelligence Experiments
*   **Continual Learning (CIFAR):** Common deep learning systems showed a steady decay in performance, eventually falling below "scratch" levels. Notably, **'continual backpropagation'** successfully mitigated this decay.
*   **In-Context Learning:** Auto-regressive transformers predicted outputs for a novel regression function almost as well as a statistical model trained from scratch.

### Neuroscience Experiments
*   **Rule Switch Tasks:** Revealed abrupt behavioral jumps from chance to perfect performance.
*   **Gradual Learning Debunked:** The study identified gradual learning curves as averaging artifacts; true learning is abrupt.
*   **Neural Correlation:** These behavioral jumps tightly correlated with change points in neural population activity.

### Temporal Dynamics
*   **Environmental Scale:** Processes evolve from milliseconds to years.
*   **Cellular Scale:** Processing spans milliseconds to hours, varying significantly by brain area.

---

## Contributions & Impact

1.  **Definition of Research Agenda:** Provides a specific agenda for how neuroscience insights can inform the development of AI systems capable of learning in changing environments.
2.  **Advancement of NeuroAI:** Contributes to the field by explicitly defining mutual benefits between neuroscience and AI.
3.  **Identification of Critical Applications:** Highlights the necessity of bio-inspired learning mechanisms for high-impact AI domains like autonomous systems.