---
title: 'CleanS2S: Single-file Framework for Proactive Speech-to-Speech Interaction'
arxiv_id: '2506.01268'
source_url: https://arxiv.org/abs/2506.01268
generated_at: '2026-02-03T06:38:03'
quality_score: 7
citation_count: 9
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# CleanS2S: Single-file Framework for Proactive Speech-to-Speech Interaction

*Yudong Lu; Yazhe Niu; Shuai Hu; Haolin Wang*

---

### ðŸ“Š Quick Facts

| Metric | Details |
| :--- | :--- |
| **Quality Score** | 7/10 |
| **References** | 9 Citations |
| **Core Architecture** | Single-file Implementation (ASR + LLM + TTS) |
| **Communication** | Full-duplex WebSocket / Non-blocking I/O |
| **Interaction Model** | Proactive, System-initiated Control |

---

## Executive Summary

Current speech-to-speech interaction frameworks are predominantly reactive, adhering to rigid turn-taking conventions where the system waits for the user to finish speaking before generating a response. This approach results in an unnatural user experience that lacks the fluidity of human conversation, which is characterized by interruptions, overlapping speech, and proactive engagement. Additionally, existing solutions often suffer from high architectural complexity, frequently requiring intricate microservice orchestration that makes them difficult to deploy and modify. This paper addresses the dual challenge of creating a more natural, human-like interaction model while simplifying the technical implementation of advanced conversational agents.

The key innovation of **CleanS2S** is a streamlined, single-file architecture that integrates Automatic Speech Recognition (ASR), Large Language Models (LLMs), and Text-to-Speech (TTS) into a unified pipeline. The system utilizes "atomic configurations"â€”unified configuration objects that centralize system state and parametersâ€”to manage the pipeline efficiently. Technically, the framework employs a dual-module architecture: a memory module aggregates historical context, while a "Subjective Action Judgement" module uses Supervised Fine-Tuning (SFT) to assess input streams dynamically. This module enables the system to select from five distinct response strategiesâ€”interruption, refusal, deflection, silence, and standard response. To facilitate real-time, full-duplex communication, the framework leverages non-blocking I/O over WebSocket connections, allowing for simultaneous listening and speaking.

The CleanS2S framework demonstrates real-time efficiency, achieving low transition latency that facilitates seamless speech-to-speech looping without operational blocking. The system validates its design through the successful execution of complex interaction strategies that mimic human behavior, confirming that a single-file implementation can sufficiently handle the full computational load of the ASR-LLM-TTS pipeline. Functionally, the results show that the architecture maintains contextual awareness through dynamic memory aggregation while managing proactive dialogue control, effectively bridging the gap between theoretical conversational models and practical, responsive voice interfaces.

CleanS2S advances conversational AI by shifting the paradigm from reactive response generation to proactive, system-initiated control. By introducing a transparent and extensible single-file framework, the authors reduce the dependency hell and complex containerization often associated with multi-service voice agents, thereby lowering the barrier to entry for researchers and developers. This simplification promotes rapid prototyping and experimentation, allowing the community to iterate on novel interaction strategies. Furthermore, the implementation of five distinct response strategiesâ€”such as deflection and proactive interruptionâ€”expands the behavioral repertoire of AI agents, setting a new standard for developing voice interfaces that are both easier to engineer and more natural to use.

---

## Key Findings

*   **Real-Time Efficiency:** The framework achieves low transition latency, facilitated by full-duplex WebSocket connections and non-blocking I/O operations.
*   **Human-Like Interaction:** Supports five distinct response strategies to mimic natural conversation:
    *   Interruption
    *   Refusal
    *   Deflection
    *   Silence
    *   Standard Response
*   **Proactive Dialogue Control:** Breaks standard turn-taking conventions by enabling system-initiated dialog and control.
*   **Streamlined Architecture:** Delivers the entire pipeline (ASR, LLM, TTS) via a single-file implementation with atomic configurations.
*   **Contextual Awareness:** Maintains history and context through a dynamic memory module that aggregates data.

---

## Methodology

The authors employed a unified approach to pipeline integration and decision-making:

*   **Unified Pipeline:** Seamless integration of Automatic Speech Recognition (ASR), Large Language Models (LLMs), and Text-to-Speech (TTS).
*   **Action Judgement SFT:** Utilized Supervised Fine-Tuning to assess continuous input streams and select the most appropriate response strategy.
*   **Dual-Module Architecture:**
    *   **Memory Module:** Responsible for aggregating historical and contextual data.
    *   **Subjective Action Judgement Module:** Responsible for selecting interaction strategies based on the processed context.
*   **Network Communication:** Implemented full-duplex WebSocket connections with non-blocking I/O to facilitate simultaneous speaking and listening capabilities.

---

## Contributions

The paper makes three primary contributions to the field of conversational AI:

1.  **Proactive Interaction Mechanism:** Introduces a novel mechanism that moves beyond reactive paradigms, enabling system-initiated control and human-like interruptions.
2.  **Framework Transparency:** Contributes a highly transparent and extensible framework through a single-file implementation using atomic configurations.
3.  **Expanded Behavioral Repertoire:** Advances the field by defining and implementing five distinct response strategies, significantly expanding the behavioral capabilities of conversational agents.

---

## Technical Details

### System Architecture
*   **Implementation:** Single-file implementation utilizing atomic configurations for streamlined management.
*   **Pipeline:** Integrated ASR, LLM, and TTS processing.

### Networking & Performance
*   **Connection:** Full-duplex WebSocket connections.
*   **I/O:** Non-blocking I/O to ensure real-time responsiveness and low transition latency.

### Dialogue Management
*   **Interaction Modes:** Five specific modes supported:
    1.  Interruption
    2.  Refusal
    3.  Deflection
    4.  Silence
    5.  Standard Response
*   **Control Capabilities:** Proactive dialogue control allowing the system to initiate conversation.
*   **Context Handling:** Dedicated memory module for aggregating historical and contextual data.

---

## Results

*   **Performance:** The system achieves low transition latency, facilitating real-time efficiency in speech loops.
*   **Interaction:** Successfully enables human-like interactions through complex, dynamic response strategies.
*   **Operational Efficiency:** The architecture proves that a full speech-to-speech loop can be handled effectively within a single-file implementation without sacrificing proactive or context-aware capabilities.