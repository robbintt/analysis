# Bilevel Models for Adversarial Learning and A Case Study

*Yutong Zheng; Qingna Li*

***

> ### **Quick Facts**
>
> *   **Quality Score:** 9/10
> *   **References:** 40 citations
> *   **Core Focus:** Adversarial Learning & Convex Clustering
> *   **Methodology:** Bilevel Optimization & Perturbation Analysis
> *   **Key Metric:** $\delta$-measure (Deviation Function)

***

## Executive Summary

This research addresses the critical challenge of ensuring the robustness of machine learning models against adversarial attacks and data perturbations, with a specific focus on convex clustering. While stability is a known concern, this paper identifies a gap in the theoretical characterization of solution mappings under perturbation. Without a rigorous mathematical definition of "calmness" in solution mappings, it is difficult to guarantee that clustering algorithms remain stable when subjected to noise or adversarial manipulation.

The authors introduce a novel theoretical framework linking adversarial attacks to the "calmness of the solution mapping" and propose two bilevel optimization models for adversarial learning. The technical innovation centers on the use of a deviation function, defined via the squared Frobenius norm, to quantify robustness. The paper establishes the $\delta$-measure as a viable metric for deviation and derives specific stability conditions for convex clustering.

Numerical experiments validated these findings. In 2-Way clustering tests ($n=5$), moving $s=1$ point resulted in $\delta=8$, while moving $s=2$ or $s=3$ resulted in $\delta=12$. Notably, the case where $s=3$ empirically validated the theoretical boundary, demonstrating that the $\delta$-measure fails when $s > \lceil n/2 \rceil$. Additionally, 3-Way experiments ($n=15$) successfully confirmed the symmetric formulas derived in Theorem 4. This work provides a rigorous mathematical foundation for analyzing model robustness, bridging the gap between theoretical optimization and practical machine learning security.

***

## Key Findings

*   **Robustness Characterization:** The robustness of learning models can be effectively characterized through the "calmness of the solution mapping."
*   **Convex Clustering Stability:** Specific conditions were identified in convex clustering where results remain invariant to data perturbations.
*   **Bilevel Modeling:** Two bilevel models were developed for adversarial learning using deviation functions.
*   **Metric Validation:** The $\delta$-measure serves as a viable deviation function for adversarial learning.
*   **Computational Efficiency:** Numerical tests confirmed both the theoretical findings and the computational efficiency of the proposed approach.

***

## Methodology

The research employs a **perturbation analysis perspective**, defining robustness through the "calmness of the solution mapping." The study follows a four-pronged methodological approach:

1.  **Framework Proposal:** It proposes bilevel optimization models to frame adversarial learning.
2.  **Application:** It applies this framework to convex clustering to analyze stability.
3.  **Validation:** It validates the $\delta$-measure as a deviation metric for quantifying changes.
4.  **Verification:** It utilizes numerical experiments to verify theoretical constructs.

***

## Contributions

*   **Theoretical Framework:** Introduces a framework linking adversarial attacks to the calmness of solution mappings.
*   **New Models:** Presents two new bilevel modeling approaches for adversarial learning.
*   **Stability Identification:** Identifies stability conditions for convex clustering.
*   **Metric Establishment:** Establishes the $\delta$-measure as a sound tool for quantifying deviation.

***

## Technical Details

### Mathematical Framework
The paper proposes a mathematical framework using bilevel models and solution mapping calmness to analyze the robustness of clustering models against data perturbations.

### Deviation Function Formulation
The deviation function is formulated as the squared Frobenius norm:
$$ \delta(y) = \| \hat{D}(0) \hat{D}(0)^T - \hat{D}(y) \hat{D}(y)^T \|_F^2 $$

### Clustering Scenarios

**1. 2-Way Clustering**
*   **Scenario:** Moving $s$ points from $V_1$ to $V_2$.
*   **Deviation Formula:** $\delta(y) = 2(n-s)s$
*   **Validity Condition:** $0 < s < \min(n_1, \lceil n/2 \rceil)$

**2. 3-Way Clustering**
*   **Scenario:** Moving $S_1$ from $V_1 \to V_2$ and $S_2$ from $V_1 \to V_3$.
*   **General Formula (Theorem 4):**
    $$ \delta(y) = (s_1 + s_2)(2n_1 - (s_1 + s_2)) + s_1(2n_2 - s_1) + s_2(2n_3 - s_2) $$

***

## Results

The primary metric used for evaluation is the deviation value $\delta(y)$.

### 2-Way Experiment ($n=5$)
*   **Setup:** Initial cluster distribution ($n_1=4, n_2=1$).
*   **Outcomes:**
    *   Moving $s=1$ point: $\delta = 8$
    *   Moving $s=2$ points: $\delta = 12$
    *   Moving $s=3$ points: $\delta = 12$
*   **Note:** The case where $s=3$ empirically validated that $\delta$ fails to be a valid deviation function when $s > \lceil n/2 \rceil$.

### 3-Way Experiment
*   **Setup:** $n=15$ with balanced clusters ($n_1=n_2=n_3=5$).
*   **Outcome:** Successfully validated the symmetric case of Theorem 4.

***

**References:** 40 citations