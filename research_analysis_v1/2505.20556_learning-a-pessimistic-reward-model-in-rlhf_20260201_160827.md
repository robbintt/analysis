# Learning a Pessimistic Reward Model in RLHF

*Yinglun Xu; Hangoo Kang; Tarun Suresh; Yuxuan Wan; Gagandeep Singh*

---

> ### ðŸ“Š Quick Facts
>
> *   **Quality Score:** 8/10
> *   **Citations:** 38
> *   **Base Architecture:** Pythia-1b
> *   **Evaluation Model:** Qwen-2.5-32B-Instruct
> *   **Key Datasets:** TL;DR Summarization, IMDB
> *   **Novel Method:** PET (Pessimistic rEward Tuning)
> *   **Top Metric:** ~55.6% Win Rate (TL;DR) at High KL Divergence

---

> ### ðŸ“ Executive Summary
>
> **Problem: The Limitations of KL Regularization in RLHF**
> Reinforcement Learning from Human Feedback (RLHF) relies on proxy reward models to guide policy optimization, but these models are susceptible to "reward hacking," where agents exploit imperfections to maximize scores without improving output quality. Conventionally, this is mitigated by KL (Kullback-Leibler) regularization, which constrains the policy from deviating too far from a reference model. While effective as a safeguard, KL regularization artificially restricts the policy's search space, capping potential performance improvements and introducing fragility through hyperparameter sensitivity. A critical challenge remains: how to prevent reward hacking and enable broad policy exploration without relying on these restrictive, externally imposed constraints.
>
> **Innovation: Pessimistic rEward Tuning (PET)**
> The authors introduce PET (Pessimistic rEward Tuning), a novel algorithm that shifts the burden of robustness from the policy optimizer to the reward model architecture. Rather than penalizing the policy with KL terms, PET employs a minimax optimization framework to fine-tune a "pessimistic" reward model. This process involves adversarial fine-tuning that deliberately lowers rewards for responses susceptible to exploitation, resulting in a reward model that is inherently robust against overestimation. With this hardened reward model, the policy can be optimized greedily using Rejection Sampling without fear of hacking, effectively removing the need for KL penalties and the associated tuning overhead.
>
> **Results: Quantitative Gains on TL;DR and IMDB**
> Experiments conducted using Pythia-1b models on the TL;DR summarization dataset and the IMDB sentiment task demonstrate PET's ability to outperform established baselines. On TL;DR, evaluated via Win Rate against a Human Reference as judged by Qwen-2.5-32B-Instruct, PET achieved a **win rate of approximately 55.6%**, significantly surpassing PPO-KL (~50%) and DPO (~51%). Crucially, while standard methods like PPO-KL and DPO typically fail or collapse when KL divergence exceeds 10, PET maintained stability and high performance at KL divergences as high as **12â€“15**. In the IMDB benchmark, where standard RLHF agents often hack the reward by generating repetitive or nonsensical text, PET maintained high validity scores and coherent output, confirming that greedy optimization against the pessimistic model is both stable and effective.
>
> **Impact: A Paradigm Shift for Policy Optimization**
> This research validates a significant paradigm shift in RLHF, demonstrating that robustness against reward hacking is an architectural property of the reward model rather than a policy constraint. By proving that KL regularization is not strictly necessary if the reward model is trained to be adversarially robust, PET unlocks an expanded policy space. This allows models to deviate significantly from initial dataset distributions to achieve higher performance without the risk of collapse. The elimination of KL coefficient tuning offers a more scalable and simplified pipeline for future alignment efforts, reducing the complexity and fragility currently associated with training high-capacity language models.

---

## Key Findings

*   **Elimination of Regularization Dependency:** PET demonstrates that reward hacking can be prevented in offline RLHF without relying on KL regularization.
*   **High-Quality Policy Learning:** Experiments show high-quality policies can be learned directly from the pessimistic reward model on the TL;DR dataset without regularization.
*   **High Divergence Tolerance:** PET successfully learns high-performance policies that maintain high KL divergence without suffering from reward hacking.
*   **Feasibility of Greedy Optimization:** The agent can greedily maximize the pessimistic reward, indicating sufficient robustness against exploitation.

## Methodology

The authors utilize **PET (Pessimistic rEward Tuning)**, a novel fine-tuning method designed to train a pessimistic reward model. Instead of optimizing against an imperfect reward model with KL regularization as a guardrail, PET optimizes the policy directly against the pessimistic reward model.

This approach relies on the intuition that **pessimistic reward models are inherently robust against reward overestimation**, removing the need for KL penalties.

## Technical Details

| Component | Description |
| :--- | :--- |
| **Approach** | Pessimistic Reward Fine-Tuning (PET) |
| **Core Framework** | Minimax optimization framework to address reward hacking via the reward model rather than KL penalties. |
| **Process Steps** | 1. **Standard Reward Modeling**<br>2. **Adversarial Fine-Tuning**: Lowers rewards for exploitable responses.<br>3. **Greedy Policy Optimization**: Direct maximization of the pessimistic reward. |
| **Objective Function** | Minimizes the difference between policy value and reference value while ensuring accuracy on the preference dataset via a fidelity term. |
| **Implementation Specs** | â€¢ **Policy Actor**: Rejection Sampling (N=64)<br>â€¢ **Models**: Pythia-1b<br>â€¢ **Temperatures**: Base T=0.7, Reference T=0.1<br>â€¢ **Pessimistic Coefficient**: Beta=10 |

## Contributions

*   **Novel Algorithm:** Introduction of PET, a new pessimistic reward fine-tuning method for offline RLHF.
*   **Paradigm Shift:** Validation that robustness against reward hacking can be achieved through the reward model architecture (pessimism) rather than external regularization mechanics.
*   **Expanded Policy Space:** Removal of KL regularization reliance enables learning policies that deviate significantly from the dataset distribution, potentially unlocking higher performance.

## Results

Experiments were conducted on the **TL;DR summarization dataset** and **IMDB**. Evaluation utilized Win Rate against Human Reference, judged by **Qwen-2.5-32B-Instruct**.

*   **Benchmark Comparisons:** The method was benchmarked against PPO-KL, DPO, RPO, and $\alpha$-PO.
*   **Performance:** PET enables learning high-quality policies without KL regularization.
*   **Stability:** It maintains high performance with high KL divergence from the reference policy.
*   **Safety:** The method permits safe greedy optimization without reward hacking.

---
*Analysis Quality Score: 8/10 | References: 38 citations*