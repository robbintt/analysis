---
title: 'Sub-MoE: Efficient Mixture-of-Expert LLMs Compression via Subspace Expert
  Merging'
arxiv_id: '2506.23266'
source_url: https://arxiv.org/abs/2506.23266
generated_at: '2026-02-06T05:08:24'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Sub-MoE: Efficient Mixture-of-Expert LLMs Compression via Subspace Expert Merging

*Lujun Li; Zhu Qiyuan; Jiacheng Wang; Wei Li; Hao Gu; Sirui Han; Yike Guo*

---

## ðŸ“Š Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Core Focus** | Mixture-of-Experts (MoE) Compression |
| **Target Architectures** | Mixtral-8x7B, DeepSeek, Qwen-1.5|3 |
| **Training Required** | None (Zero-shot) |
| **Max Performance Retention** | 96% @ 25% Expert Reduction |
| **Quality Score** | 9/10 |

---

## Executive Summary

Mixture-of-Experts (MoE) models offer an efficient scaling path for Large Language Models (LLMs), but their massive parameter counts present significant memory and deployment hurdles. While compression techniques like pruning and merging are commonly employed, existing methods face critical limitations in the MoE context. Pruning often results in knowledge loss or requires resource-intensive fine-tuning, while standard merging methods suffer from "parameter conflicts" when attempting to combine experts with low similarity. This paper addresses the urgent need for a compression framework that can drastically reduce the computational footprint of MoE architectures without necessitating additional training or sacrificing the high performance that makes these models valuable.

The authors introduce **"Sub-MoE,"** a novel, two-stage compression framework designed to operate without any additional training, searching, or fine-tuning. The first stage, Adaptive Expert Clustering, utilizes K-means clustering on the cosine similarity of expert outputs to identify and group functionally coherent experts. The core technical innovation occurs in the second stage, Subspace Expert Merging, which solves the parameter conflict problem through joint Singular Value Decomposition (SVD). By concatenating expert weights and applying SVD, the method decouples shared components into a common U-matrix and isolates expert-specific components into V-matrices. These are then reconstructed within a shared subspace using frequency-based fusion, allowing for the mathematically sound merging of experts even when their direct similarity is low.

Sub-MoE demonstrates superior compression efficiency across prominent architectures, including Mixtral-8x7B, DeepSeek, and Qwen-1.5|3. In rigorous testing on the Mixtral-8x7B model, the framework preserved **96%** of the original performance while reducing experts by 25%, and maintained **86%** performance retention with a 50% reduction in experts. Quantitative analysis indicates that while inter-expert similarity typically ranges between 0.1 and 0.3, Sub-MoE consistently outperforms existing baselines such as SEER-MoE, NAEE, and MC-SMoE.

This research establishes a new state-of-the-art balance between model compression and performance retention for MoE architectures. By theoretically grounding the merging process in subspace projection to mitigate parameter conflicts, Sub-MoE provides a robust, training-free solution for deploying massive LLMs on resource-constrained hardware.

---

## Key Findings

*   **Superior Compression Efficiency:** Outperforms existing pruning and merging methods while maintaining high performance, establishing a new SOTA balance.
*   **High Performance Retention:** On Mixtral-8x7B, the method preserves **96%** and **86%** of performance with 25% and 50% expert reduction, respectively.
*   **Cross-Architecture Robustness:** Validation confirmed across multiple prominent architectures, including Mixtral, DeepSeek, and Qwen-1.5|3.
*   **Resolution of Parameter Conflicts:** Solves the issue of parameter conflicts typically found in specialized models via a shared subspace approach.
*   **Extensibility:** The framework is extensible to intra-expert compression techniques for further inference optimization.

---

## Methodology

The Sub-MoE framework utilizes a two-phase process to compress MoE layers without requiring retraining.

### Phase 1: Adaptive Expert Clustering
This stage employs **K-means clustering** based on the cosine similarity of expert outputs. The goal is to group functionally coherent experts. The allocation of clusters is adaptive across multiple layers based on redundancy.

### Phase 2: Subspace Expert Merging
This stage performs **joint Singular Value Decomposition (SVD)** on concatenated expert weights.
1.  It derives a shared **U-matrix** for common features.
2.  It merges expert-specific **V-matrices** based on frequency analysis.
3.  It reconstructs fused experts within the shared subspace.

---

## Technical Details

The Sub-MoE framework is a two-stage compression pipeline for Mixture-of-Experts (MoE) layers that requires no additional training, searching, or fine-tuning.

### Stage 1: Adaptive Expert Clustering
*   **Mechanism:** Identifies functionally similar experts using the average cosine similarity of expert outputs on a representative dataset.
*   **Algorithm:** Employs K-means clustering to group experts.
*   **Optimization:** Uses multi-layer adaptive allocation of clusters based on redundancy.
*   **Objective:** Minimizes the K-means objective function $J$ to ensure cohesive grouping.

### Stage 2: Subspace Expert Merging
*   **Problem Solved:** Addresses the "parameter conflict problem" (where expert similarity is low, typically 0.1â€“0.3).
*   **Process:**
    1.  Concatenates expert weights.
    2.  Applies Singular Value Decomposition (SVD) to extract a shared orthogonal basis.
    3.  **Frequency-Based Fusion:** Weighting the merging of V matrices based on expert sampling frequency (activation rate).
*   **Reconstruction:** Reconstructs the final merged weights using the formula:
    $$W_{merged} = U \Sigma [V_{merged}]^T$$

### Architecture Notes
*   The MoE layers consist of feed-forward networks with weight matrices $(W_{up}, W_{gate}, W_{down})$.
*   Utilizes SiLU activation and top-k routing.

---

## Results

### Performance on Mixtral-8x7B
*   **25% Expert Reduction:** Preserved **96%** of original performance.
*   **50% Expert Reduction:** Preserved **86%** of original performance.

### Comparative Analysis
*   **Vs. Pruning (e.g., SEER-MoE, NAEE):** Sub-MoE avoids resource-intensive fine-tuning and knowledge loss.
*   **Vs. Merging (e.g., MC-SMoE, EEP):** Sub-MoE outperforms standard methods by handling parameter conflicts in low-similarity scenarios.

### Quantitative Metrics
*   Inter-expert similarity typically ranges between 0.1 and 0.3.
*   The method successfully minimizes the K-means objective function to ensure cohesive grouping.

---

## Contributions

*   **Introduction of Sub-MoE:** A novel compression framework for Mixture-of-Experts models that directly addresses memory and deployment hurdles.
*   **Theoretical Innovation:** Introduced "Subspace Expert Merging" using joint SVD to decouple shared components from expert-specific components, effectively mitigating parameter conflicts.
*   **Algorithmic Pipeline:** Developed a comprehensive pipeline combining output-based clustering and mathematically grounded weight consolidation.
*   **Empirical Validation:** Extensive testing on prominent MoE architectures (Mixtral, DeepSeek, Qwen) establishing a new state-of-the-art balance between model compression and performance retention.

---

*Report generated based on analysis with 40 references. Quality Score: 9/10*