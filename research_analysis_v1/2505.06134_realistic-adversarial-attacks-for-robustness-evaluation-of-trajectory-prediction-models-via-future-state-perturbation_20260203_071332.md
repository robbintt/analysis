---
title: Realistic Adversarial Attacks for Robustness Evaluation of Trajectory Prediction
  Models via Future State Perturbation
arxiv_id: '2505.06134'
source_url: https://arxiv.org/abs/2505.06134
generated_at: '2026-02-03T07:13:32'
quality_score: 7
citation_count: 33
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Realistic Adversarial Attacks for Robustness Evaluation of Trajectory Prediction Models via Future State Perturbation

*Julian F. Schumann; Jeroen Hagenus; Frederik Baymler Mathiesen; Arkady Zgonnikov*

***

> ### üìä Quick Facts
>
> *   **Quality Score:** 7/10
> *   **Total Citations:** 33
> *   **Core Methodology:** Future State Perturbation (White-box attack)
> *   **Key Metrics:** ADE, FDE, Collision Rate/Distance
> *   **Tested Models:** Social-GAN, Trajectron++
> *   **Focus:** Kinematic feasibility & tactical behavior preservation

***

## üìù Executive Summary

### Problem
Current adversarial attack methodologies for trajectory prediction models are fundamentally limited by their exclusive focus on perturbing past agent positions. This approach generates unrealistic scenarios that fail to capture the dynamic nature of driving, leading to overly optimistic assessments of model robustness. Consequently, critical vulnerabilities remain undetected, specifically the inability of models to foresee potential collisions in scenarios that appear safe based on historical data. This discrepancy poses a severe risk to the reliability of autonomous driving systems, necessitating a more rigorous evaluation framework that accounts for the complexities of real-world interactions.

### Innovation
The authors introduce **"Future State Perturbation,"** a novel white-box adversarial framework that shifts the attack paradigm from historical manipulation to the perturbation of future states ($\tilde{Y}$). To ensure physical plausibility, the method utilizes a kinematic bicycle model, applying perturbations to *vehicle control actions*‚Äîspecifically steering angle and acceleration‚Äîrather than directly modifying vehicle states. This technical distinction allows the framework to enforce hard dynamic constraints and use dynamically reachable sets for validation. The approach specifically targets "False Negative Collision Attacks," where an adversarial agent executes a physically feasible collision trajectory that the model incorrectly predicts as safe, thereby balancing attack efficacy with the preservation of tactical behavior.

### Results
Empirical evaluation against state-of-the-art prediction models, including Social-GAN and Trajectron++, demonstrated that Future State Perturbation significantly degrades performance compared to baseline methods. The proposed attack induced substantial increases in Average Displacement Error (ADE) and Final Displacement Error (FDE), alongside a marked rise in collision rates and a reduction in collision distance metrics. Crucially, the results revealed that while models may exhibit robustness against past-state attacks, they remain highly susceptible to future-state perturbations. This exposes specific failure modes where prediction errors remain deceptively low despite the presence of imminent physical danger, highlighting the fragility of current defenses against realistic, forward-looking threats.

### Impact
This research exposes fundamental flaws in existing robustness evaluation protocols, proving that previous assessments based solely on past perturbations were dangerously optimistic. By demonstrating that state-of-the-art models fail to identify collisions in seemingly safe situations, the paper highlights an urgent need for more comprehensive testing standards in autonomous driving. The introduction of specific metrics for judging realism and impact establishes a new benchmark for the field, urging researchers to adopt evaluation methods that prioritize physical feasibility alongside adversarial effectiveness.

***

## üîë Key Findings

*   **Limitations of Current Approaches:** Existing adversarial attack methods that focus solely on perturbing past agent positions generate unrealistic scenarios and lead to overly optimistic assessments of model robustness.
*   **Efficacy of Future State Perturbation:** Perturbing the future states of adversarial agents, rather than just the past, uncovers previously undetected weaknesses in trajectory prediction models.
*   **Significant Performance Degradation:** Testing the proposed method on a state-of-the-art prediction model resulted in substantial increases in both prediction errors and collision rates.
*   **Exposure of Hidden Failure Modes:** Qualitative analysis revealed that the attacks expose critical vulnerabilities, specifically the model's inability to detect potential collisions within scenarios that appear to be safe predictions.

***

## üõ†Ô∏è Methodology

The authors propose a novel adversarial attack strategy called **Future State Perturbation**. Unlike traditional methods that manipulate historical data, this approach generates adversarial examples by perturbing the future states of surrounding agents.

To ensure these attacks remain plausible in real-world contexts, the methodology incorporates:
1.  **Dynamic Constraints:** To ensure physical feasibility.
2.  **Tactical Behavior Preservation:** To maintain the logical intent of road users.

The researchers also introduced new performance measures to quantitatively assess the realism and impact of these adversarial trajectories.

***

## ‚öôÔ∏è Technical Details

| Component | Description |
| :--- | :--- |
| **Paradigm Shift** | Introduces **Future State Perturbation**, generating perturbations for both past states (`$\tilde{X}$`) and future states (`$\tilde{Y}$`) to ensure the observed future trajectory remains dynamically feasible. |
| **Attack Strategy** | Utilizes a **white-box attack** strategy leveraging gradient information. |
| **Kinematic Feasibility** | Perturbations are applied to vehicle control actions (spatial position, heading angle, velocity) derived from a **kinematic bicycle model**, subject to hard constraints. |
| **Attack Definition** | Defines a **'False Negative Collision Attack'** where an adversarial agent executes a physical collision trajectory while the AV predicts a non-colliding one. |
| **Validation** | Dynamically reachable sets are used to validate trajectory feasibility, replacing simple distance constraints. |

***

## üìà Results

The evaluation metrics include **Average Displacement Error (ADE)**, **Final Displacement Error (FDE)**, and **Collision Rate/Distance**.

*   **Performance Degradation:** Testing on a state-of-the-art prediction model showed a substantial increase in ADE and FDE compared to baseline methods.
*   **Increased Collision Rates:** A significant increase in collision rates was observed alongside the error metrics.
*   **Hidden Vulnerabilities:** Results indicate that models robust against past-state attacks are vulnerable to future-state perturbations. This reveals hidden failure modes where scenarios appear safe (low prediction error) but are physically adversarial.
*   **Optimism in Previous Methods:** The paper concludes that previous robustness evaluations based solely on past perturbation were overly optimistic.
*   **Superior Realism:** The method is positioned as more realistic than approaches relying on position constraints or soft regularization penalties.

***

## ‚úÖ Contributions

*   **Advanced Adversarial Framework:** Introduction of a new attack paradigm that targets future states, providing a more rigorous tool for robustness evaluation than past-centric methods.
*   **Realism in Adversarial Testing:** A technical framework that balances attack effectiveness with realism through the integration of dynamic constraints and tactical behavior preservation.
*   **New Evaluation Metrics:** Development of specific metrics to judge the quality of adversarial trajectories based on their realism and impact.
*   **Evidence of Critical Vulnerabilities:** Empirical evidence demonstrating that current state-of-the-art models fail to identify collisions in seemingly safe situations, highlighting the urgent need for more comprehensive testing protocols in autonomous driving research.

***

**REFERENCES:** 33 citations