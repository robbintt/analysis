# The Future is Sparse: Embedding Compression for Scalable Retrieval in Recommender Systems

*Petr Kasalick√Ω; Martin Spi≈°√°k; Vojtƒõch Vanƒçura; Daniel Bohunƒõk; Rodrigo Alves; Pavel Kord√≠k*

---

## üìä Quick Facts & Metrics

| Metric | Detail |
| :--- | :--- |
| **Quality Score** | 9/10 |
| **Proposed Method** | CompresSAE |
| **Dataset Scale** | 100 Million Items |
| **Training Hardware** | NVIDIA H100 SXM 80GB |
| **Training Time** | ~100 Seconds (500 steps) |
| **Batch Size** | 100,000 |
| **Storage Footprint** | 25.6 GB |
| **CTR Lift** | **+3.44%** (vs Baseline) |

---

## üìù Executive Summary

> **Overview**
>
> Large-scale recommender systems face a critical scalability bottleneck due to the massive memory overhead required to store dense embeddings for high-cardinality entities, such as millions of items or users. As the volume of data grows, the storage infrastructure required to maintain these dense vectors often becomes cost-prohibitive, limiting the complexity of models that can be deployed in production. This paper addresses the challenge of reducing this memory footprint without sacrificing the representational power necessary for high-quality retrieval.
>
> **The Solution**
>
> The authors introduce **CompresSAE**, a lightweight, learnable sparse autoencoder designed to compress dense embeddings into high-dimensional, sparsely activated representations. Technically, the architecture normalizes input dense embeddings to unit length and projects them into a latent space using a non-linear encoder with a TopK activation function, which retains only the $k$ entries with the largest absolute values to ensure sparsity. The reconstruction is performed by a linear, bias-free, row-normalized decoder. The training objective minimizes Cosine Distance rather than Mean Squared Error, and the authors employ a Multi-$k$ training strategy‚Äîsimultaneously optimizing for both $k$ and $4k$ non-zero entries‚Äîwhich effectively prevents dead neurons and ensures robust feature learning within the compressed space.
>
> **The Outcome**
>
> In experiments involving a dataset of 100 million items, CompresSAE ("Nomic" implementation) demonstrated superior performance compared to existing state-of-the-art compression methods. At a matching storage footprint of 25.6 GB, the method achieved a Click-Through Rate (CTR) lift of **+3.44%** over the baseline, significantly outperforming Matryoshka embeddings (+1.89%) and closing approximately **82%** of the performance gap between Matryoshka and the full dense model (+4.86%). Additionally, in offline retrieval tasks, CompresSAE ($k=32$) surpassed Matryoshka ($d=64$) in Recall@100 at similar storage capacities. Efficiency was also highlighted, with the model converging in approximately 500 steps (100 seconds) on a single NVIDIA H100 80GB GPU, supporting batch sizes up to 100,000.

---

## üîë Key Findings

*   **Memory Efficiency:** Leveraging sparsity through projection into high-dimensional spaces significantly reduces memory requirements required for storing dense embeddings of high-cardinality entities.
*   **Performance Retention:** The proposed compression technique succeeds in lowering memory overhead without negatively impacting retrieval performance.
*   **Scalability:** The method enables the scalable deployment of large-scale recommender systems under strict resource constraints.
*   **Validation:** The study validates sparsity as a promising and effective approach for improving the operational efficiency of industrial-scale recommenders.

---

## üõ†Ô∏è Methodology

The authors propose a lightweight, learnable embedding compression technique designed specifically for retrieval tasks. The core mechanism involves:

*   **Projection:** Mapping dense embeddings into a high-dimensional, sparsely activated space.
*   **Balance:** Allowing the model to maintain representational capacity while drastically reducing the active memory footprint during both training and inference.

---

## ‚öôÔ∏è Technical Details

The technical implementation revolves around the **CompresSAE** architecture.

**Architecture Components**
*   **Type:** Sparse Autoencoder.
*   **Input Processing:** Normalizes precomputed dense embeddings to unit length.
*   **Encoder:** Non-linear encoder utilizing **TopK activation**, retaining only the $k$ entries with the largest absolute values to create a sparse representation.
*   **Decoder:** Linear, bias-free, row-normalized decoder for reconstruction.

**Training Strategy**
*   **Objective Function:** Minimizes **Cosine Distance** rather than Mean Squared Error (MSE).
*   **Multi-$k$ Strategy:** Combines losses for $k$ and $4k$ to prevent dead neurons and ensure robust feature utilization.
*   **Hyperparameters:** Supports large batch sizes of 100,000.
*   **Convergence:** Approximately 500 steps (around 100 seconds) on a single NVIDIA H100 SXM 80GB GPU.

---

## üìà Experimental Results

**Experiment Scope:**
*   Dataset size: 100 million items.
*   Comparison baseline: Matryoshka embeddings.

**Performance Metrics:**
*   **Storage Efficiency:** Nomic (CompresSAE) achieved a storage footprint of **25.6 GB**, matching Matryoshka.
*   **CTR Performance:** Achieved a CTR lift of **+3.44%** over the baseline.
    *   *Comparison:* Outperforms Matryoshka's lift of +1.89%.
    *   *Gap Reduction:* Closes approximately **82%** of the performance gap between Matryoshka and the full dense Nomic model (+4.86%).
*   **Offline Retrieval:** CompresSAE ($k=32$) outperformed Matryoshka ($d=64$) in **Recall@100** at similar storage capacities.
*   **Training Efficiency:** Converged in roughly **100 seconds** on an NVIDIA H100 80GB.

---

## üöÄ Contributions

*   **Novel Architecture:** Introduction of a new compression architecture utilizing a learnable method to convert dense embeddings into sparse representations to address the memory bottleneck in high-cardinality domains.
*   **Resource-Aware Technique:** Demonstration of a retrieval technique that balances the trade-off between strict resource constraints and the need for high-performance retrieval.
*   **Open Source:** Release of the open-source implementation (**CompresSAE**) to allow for reproducibility and further community development.

---

**References:** 27 citations