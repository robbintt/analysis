---
title: A Representation Sharpening Framework for Zero Shot Dense Retrieval
arxiv_id: '2511.05684'
source_url: https://arxiv.org/abs/2511.05684
generated_at: '2026-02-06T04:07:46'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# A Representation Sharpening Framework for Zero Shot Dense Retrieval

***Dhananjay Ashok; Suraj Nair; Mutasem Al-Darabsah; Choon Hui Teo; Tarun Agarwal; Jonathan May***

---

> ### ⚡ Quick Facts
>
> *   **Quality Score:** 8/10
> *   **Total Citations:** 40
> *   **Key Benchmark:** BEIR & BRIGHT
> *   **SOTA Status:** State-of-the-art on 8/11 BRIGHT subsets
> *   **Avg. Performance Lift:** +6.9% (BEIR NDCG@10)
> *   **Core Innovation:** Training-free representation sharpening
> *   **Deployment Cost:** Zero additional inference latency (via indexing-time approximation)

---

## Executive Summary

This research addresses the critical limitation of semantic granularity in zero-shot dense retrieval (DR). While pretrained dense retrievers are effective at capturing high-level topical relevance, they often fail to distinguish between documents that are semantically similar but contextually distinct. This failure mode results in "fuzzy" embeddings where relevant documents are buried among neighbors that share broad topical similarities but lack the specific nuances required for precise retrieval. Solving this is essential for advancing zero-shot capabilities, as it determines how well retrieval systems can generalize to new domains and languages without the need for expensive task-specific fine-tuning.

The authors propose **"Representation Sharpening,"** a training-free framework designed to refine semantic granularity by modifying document embeddings at inference time. The core technical mechanism, **Query-Weighted Embedding Shift**, dynamically adjusts a document embedding ($d$) by shifting it towards a set of contrastive queries ($Q_d$) based on the similarity to the specific inference query ($q$). To ensure the contrastive references provide diverse and meaningful guidance, the framework employs a local neighborhood subsampling and clustering strategy (e.g., KMeans) rather than relying solely on nearest neighbors. Furthermore, the authors introduce an indexing-time approximation that pre-computes these adjustments, allowing the system to retain performance gains without incurring additional computational costs during inference.

Evaluated against traditional retrieval and Doc2Query baselines on the BEIR and BRIGHT benchmarks, the ConSharp variant of the framework demonstrated consistent superiority. On the BEIR benchmark (6 datasets, NDCG@10 metric), the method achieved an average performance improvement of **6.9%**. On the more challenging BRIGHT benchmark, the framework established a new state-of-the-art (SOTA) on 8 out of 11 subsets. Additionally, the approach validated its generalizability across over 20 datasets spanning multiple languages.

The significance of this work lies in its ability to enhance zero-shot retrieval performance without requiring model retraining or increasing inference latency. By offering a plug-and-play solution that is compatible with existing dense retrievers, the framework provides a practical path to improve production systems immediately. The successful resolution of the performance-cost tradeoff through indexing-time optimization makes this approach highly viable for real-world deployment.

---

## Key Findings

*   **State-of-the-art Performance:** The proposed representation sharpening framework consistently outperforms traditional retrieval methods, establishing a new SOTA on the BRIGHT benchmark.
*   **Broad Generalizability:** The framework demonstrates broad effectiveness across over twenty datasets that span multiple languages.
*   **Seamless Integration:** The approach is highly compatible with existing zero-shot dense retrieval methods, consistently improving their performance rather than replacing them.
*   **Efficiency Optimization:** An indexing-time approximation was successfully developed to preserve the majority of performance gains without incurring additional inference-time costs, resolving the performance-cost tradeoff.

---

## Methodology

The authors introduce a training-free representation sharpening framework designed to address the limitation of pretrained dense retrievers (DRs) in distinguishing between semantically similar documents in a zero-shot setting.

The methodology involves augmenting a document's representation with specific contextual information that helps differentiate it from other similar documents within the corpus. By refining the semantic granularity without requiring additional model training, the system improves the precision of retrieval tasks.

---

## Contributions

*   **Solution to Semantic Granularity:** Identification of the failure mode in current zero-shot DRs (inability to represent semantic differences) and the introduction of a training-free augmentation technique to solve it.
*   **Empirical Validation:** Extensive evaluation demonstrating superior performance over traditional baselines and the establishment of a new SOTA on the BRIGHT benchmark across multilingual datasets.
*   **Efficiency Optimization:** Development of an indexing-time approximation that maintains high performance gains while eliminating additional inference-time costs, making the framework practical for real-world deployment.

---

## Technical Details

The paper proposes a training-free adaptation framework called **Representation Sharpening** for zero-shot dense retrieval. The technical specifications are as follows:

| Component | Description |
| :--- | :--- |
| **Core Mechanism** | **Query-Weighted Embedding Shift** |
| **Formula** | `d* = d + α · g(q, Q_d)` |
| **Logic** | Dynamically shifts a document embedding ($d$) towards contrastive queries ($Q_d$) based on the inference query ($q$). |
| **Aggregation** | The aggregation function $g$ uses a softmax of similarity scores to weigh the influence of contrastive queries. |
| **Diversity Strategy** | Employs a local neighborhood subsampling and clustering strategy (e.g., KMeans) rather than just selecting nearest neighbors to ensure diversity in contrastive references. |
| **Optimization** | Utilizes an indexing-time optimization to reduce computational costs during inference. |

---

## Results

The study evaluated the framework against Traditional Retrieval and Doc2Query baselines on the BEIR and BRIGHT benchmarks. The **ConSharp** variant consistently outperformed others.

### BEIR Benchmark (6 Datasets, NDCG@10)
*   **Average Improvement:** +6.9%
*   **Contriever:** Improved from **33.30** to **40.67**
*   **E5-Mistral:** Improved from **37.49** to **45.54**

### BRIGHT Benchmark
*   **Achievement:** State-of-the-Art (SOTA) on **8 out of 11** subsets.
*   **Generalizability:** Validated across over 20 datasets in multiple languages.