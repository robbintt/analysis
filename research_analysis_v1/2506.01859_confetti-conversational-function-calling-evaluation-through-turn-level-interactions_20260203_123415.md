---
title: 'CONFETTI: Conversational Function-Calling Evaluation Through Turn-Level Interactions'
arxiv_id: '2506.01859'
source_url: https://arxiv.org/abs/2506.01859
generated_at: '2026-02-03T12:34:15'
quality_score: 8
citation_count: 3
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# CONFETTI: Conversational Function-Calling Evaluation Through Turn-Level Interactions

*Tamer Alkhouli; Katerina Margatina; James Gung; Raphael Shu; Claudia Zaghi; Monica Sunkara; Yi Zhang*

---

> ### ðŸ“Š Quick Facts Sidebar
>
> *   **Dataset Size:** 109 conversations, 313 user turns, 86 distinct APIs.
> *   **Top Performing Model:** Nova Pro (**40.01%**).
> *   **Runner-Up:** Claude Sonnet v3.5 (**35.46%**).
> *   **Key Limitation Identified:** Severe inability to handle chained function-calling across SOTA LLMs.
> *   **Evaluation Metrics:** AST Soft Metric (AlignScore), LLM-as-a-Judge (gpt-4o-mini), Dialog Act Classification.
> *   **Paper Quality Score:** 8/10

---

## Executive Summary

As Large Language Models (LLMs) increasingly serve as agents capable of executing external functions via APIs, evaluating their ability to handle complex, multi-turn interactions has become critical. Existing benchmarks largely focus on single-turn or simplistic scenarios, failing to capture the nuances of real-world conversations where users engage in follow-ups, correct goals, or express implicit intents. This paper addresses the lack of rigorous evaluation frameworks that can assess how well models maintain context and manage API logic over extended dialoguesâ€”a deficiency that obscures the true limitations of current state-of-the-art models in agentic workflows.

The authors introduce **CONFETTI**, a novel benchmark designed to evaluate function-calling through turn-level interactions within complex, multi-turn scenarios. The innovation lies in a dual-benchmark structure assessing both **Function-Calling accuracy** and **Response Quality**, utilizing a dataset of 109 human-simulated conversations spanning 86 distinct APIs. Technically, the approach employs an off-policy turn-level evaluation method combined with Dialog Act Classification. To ensure robustness, the researchers utilize the AST Soft Metric with AlignScore for semantic string matching and employ an LLM-as-a-Judge framework (specifically gpt-4o-mini) to detect parameter hallucinations. The dataset is explicitly engineered to test complexities such as goal switching, ambiguity, and the need for goal correction.

Evaluations against state-of-the-art models reveal significant performance limitations, particularly in complex scenarios. The top-performing models achieved modest scores: **Nova Pro (40.01%)**, **Claude Sonnet v3.5 (35.46%)**, and **Llama 3.1 405B (33.19%)**. The results demonstrate that chained function-calling is severely limited across all evaluated LLMs. Furthermore, a distinct performance disparity exists where only top-tier models can effectively handle long conversations and contexts involving over 20 APIs, while others struggle significantly. Model performance notably degrades in scenarios requiring goal correction, goal switching, or the interpretation of ambiguous and implicit user goals.

This research significantly impacts the field by establishing a comprehensive baseline for assessing LLMs in realistic, multi-turn function-calling environments. By exposing specific weaknesses in API scaling, context management, and chained reasoning, CONFETTI shifts the focus from static, single-turn evaluations to dynamic, conversational complexity. The benchmark provides a critical tool for researchers and developers to identify failure modes in agentic AI systems, driving future advancements toward models capable of sustaining coherent and accurate tool use over long, intricate interactions.

---

## Key Findings

*   **Chained Function-Calling Limitations:** Chained function-calling is severely limited across all evaluated state-of-the-art LLMs.
*   **Performance Disparity:** There is a distinct gap between tiers; top-tier models (Nova Pro, Claude Sonnet v3.5) handle long conversations and over 20 APIs well, while others struggle with context length and API count.
*   **Top Performers:** The highest scoring models are:
    *   **Nova Pro:** 40.01%
    *   **Claude Sonnet v3.5:** 35.46%
    *   **Llama 3.1 405B:** 33.19%
*   **Complexity Degradation:** Model performance degrades significantly with complexity, specifically in scenarios requiring goal correction, goal switching, or handling ambiguous/implicit goals.

---

## Methodology

The research team employed a rigorous multi-step process to create the CONFETTI benchmark and evaluate model performance:

*   **Benchmark Development:** Created the CONFETTI dataset consisting of **109 human-simulated conversations** with **313 user turns**, covering **86 distinct APIs**.
*   **Complexity Design:** Conversations were explicitly designed to test complexities not found in previous benchmarks, including:
    *   Follow-ups
    *   Goal correction
    *   Implicit goals
*   **Evaluation Method:** Utilized an **off-policy turn-level evaluation method** to assess function-calling capabilities without requiring the model to generate the full conversation history.
*   **Analysis Dimensions:** Assessed responses using dialog act annotations and analyzed performance based on three specific variables:
    *   API count
    *   Conversation length
    *   Chained function calls

---

## Technical Details

The benchmark utilizes a sophisticated technical framework to ensure accurate assessment of LLM capabilities.

**Evaluation Approach**
*   **Strategy:** Turn-level offline evaluation.
*   **Structure:** Dual-benchmark design focusing on:
    1.  Function-Calling
    2.  Response Quality

**Data Characteristics**
*   **Origin:** Human-authored and scenario-based.
*   **Scope:** 86 curated APIs with simulated outputs.
*   **Constraints:** Specific complexity constraints applied to simulate real-world friction.

**Evaluation Metrics**
*   **AST Soft Metric:** Uses AlignScore for semantic string matching.
*   **Parameter Hallucination:** Detected via LLM-as-a-Judge (gpt-4o-mini).
*   **Classification:** Dialog Act Classification to categorize interaction types.

---

## Results

The evaluation yielded a comprehensive dataset and clear performance indicators for current LLMs.

*   **Dataset Statistics:**
    *   506 function-calling examples (109 conversations).
    *   663 response quality turns.
*   **Performance Recap:**
    *   **Nova Pro:** 40.01%
    *   **Claude Sonnet v3.5:** 35.46%
    *   **Llama 3.1 405B:** 33.19%
*   **Critical Observations:**
    *   Confirmed that chained function-calling is a major bottleneck for SOTA LLMs.
    *   Verified a significant performance disparity when handling long contexts and high API counts (>20).
    *   Confirmed degradation in performance when models face complex scenarios requiring goal correction or ambiguity resolution.

---

## Contributions

*   **Benchmark Introduction:** Introduced the CONFETTI benchmark to fill the gap in assessing LLM function-calling in complex, multi-turn scenarios.
*   **Dataset Provision:** Provided a comprehensive dataset that includes complex dialog features like goal switching and implicit goals.
*   **Methodological Innovation:** Proposed a new evaluation approach combining off-policy turn-level assessment with dialog act annotations.
*   **Baseline Establishment:** Established a performance baseline for state-of-the-art LLMs, highlighting specific weaknesses in API scaling and context management.

---

**References:** 3 citations
**Quality Score:** 8/10