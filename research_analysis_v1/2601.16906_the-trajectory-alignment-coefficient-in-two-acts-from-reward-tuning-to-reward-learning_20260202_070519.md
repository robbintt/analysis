# The Trajectory Alignment Coefficient in Two Acts: From Reward Tuning to Reward Learning

*Calarina Muslimani; Yunshu Du; Kenta Kawamoto; Kaushik Subramanian; Peter Stone; Peter Wurman*

***

> ###  Quick Facts
> | Metric | Detail |
> | :--- | :--- |
> | **Quality Score** | 9/10 |
> | **Citations** | 40 References |
> | **Workload Reduction** | ~30% |
> | **Avg. Iterations** | ~40 |
> | **Test Environments** | Lunar Lander, Gran Turismo 7 |

***

## Executive Summary

Reward specification remains a fundamental bottleneck in Reinforcement Learning (RL), often requiring practitioners to engage in laborious manual tuning of reward weights or rely on automated preference learning methods that may fail to capture nuanced human intent. Manual reward design imposes a high cognitive workload, as practitioners must iteratively adjust weights based on sparse feedback, while automated approaches like standard Reinforcement Learning from Human Feedback (RLHF) can suffer from mode collapse or fail to generalize specific behavioral preferences. This paper addresses the challenge of bridging the gap between human intent and reward function optimization, seeking a unified framework that reduces the burden on human experts while improving the alignment and diversity of learned policies.

The core innovation is the **Trajectory Alignment Coefficient (TAC)**, a metric designed to quantify the congruence between a reward functions induced preferences and a domain expert's demonstrated behavior. The research presents TAC in "two acts": first, as a real-time feedback mechanism for human-in-the-loop reward tuning; and second, as "**Soft-TAC**," a differentiable approximation of the coefficient that enables direct gradient-based optimization. By formulating TAC as a differentiable loss function, the authors replace standard preference-based losses, such as Cross-Entropy, allowing reward models to be trained explicitly to maximize trajectory alignment rather than simple probability matching. This effectively unifies manual specification and automated learning under a single mathematical framework.

Empirical validation across two distinct domains demonstrates the efficacy of this approach. In Act 1, a human-subject study using the Lunar Lander environment showed that providing practitioners with TAC feedback resulted in approximately a **30% reduction in perceived cognitive workload** and higher task success rates compared to standard tuning methods, though the process remained labor-intensive, requiring around 40 iterations. In Act 2, utilizing the Gran Turismo 7 racing simulator, Soft-TAC outperformed standard Cross-Entropy baselines. The Soft-TAC trained reward models proved superior at capturing preference-specific objectivessuch as driving etiquetteand produced policies with qualitatively more distinct behaviors, highlighting the method's ability to recover diverse, intent-aligned strategies.

This work significantly influences the field of RL by establishing a versatile tool that functions effectively as both an interpretive metric for engineers and an optimization objective for algorithms. By validating that a metric designed for human feedback can be successfully differentiated to automate reward learning, the authors provide a pathway to more robust and sample-efficient preference learning. The ability of Soft-TAC to generate behaviorally distinct policies addresses the common issue of reward homogenization in automated systems, offering a promising direction for developing AI systems that require high-fidelity adherence to complex human values and diverse behavioral repertoires.

***

## Key Findings

*   **Improved Manual Tuning:** Providing RL practitioners with the Trajectory Alignment Coefficient (TAC) resulted in more performant reward functions and significantly reduced cognitive workload.
*   **Persistent Labor Intensity:** Despite efficiency gains, manual reward design remains an inherently labor-intensive process.
*   **Superior Reward Learning:** Soft-TAC trained reward models in Gran Turismo 7 captured preference-specific objectives more effectively than standard Cross-Entropy loss.
*   **Behavioral Distinctness:** Policies from Soft-TAC models demonstrated qualitatively more distinct behaviors compared to standard loss function baselines.
*   **Dual Utility of TAC:** TAC is validated as effective both as a real-time metric for human-in-the-loop tuning and as an optimization objective for automated reward learning.

***

## Methodology

The research utilizes a two-stage methodology across distinct environments:

*   **Act 1: Reward Tuning Evaluation**
    *   **Environment:** Lunar Lander.
    *   **Procedure:** A human-subject study where RL practitioners tuned reward weights. The study compared standard tuning against TAC-supported tuning.
    *   **Metrics:** Assessment focused on performance outcomes and the cognitive workload of the practitioners.

*   **Act 2: Reward Learning**
    *   **Innovation:** Introduction of Soft-TAC, a differentiable approximation of TAC.
    *   **Function:** Used as a loss function for training reward models derived from human preference data.
    *   **Validation:** Conducted in the Gran Turismo 7 racing simulator against standard Cross-Entropy loss baselines.

***

## Technical Details

*   **Core Concept: Trajectory Alignment Coefficient (TAC)**
    *   A metric evaluating the congruence between a reward function's induced preferences and those of a domain expert.
    *   Provides a continuous alignment signal for iterative refinement.

*   **Architecture & Dual Utility**
    *   **Feedback Metric (Act 1):** Utilized for manual human-in-the-loop reward tuning.
    *   **Optimization Objective (Act 2):** Utilized for automated reward learning.

*   **Soft-TAC Algorithm**
    *   A differentiable approximation of TAC.
    *   Designed to replace standard preference-based RL loss functions (e.g., Cross-Entropy).
    *   Directly maximizes TAC using human preference data via gradient-based optimization.

*   **Experimental Domains**
    *   **Lunar Lander:** Continuous space environment.
    *   **Gran Turismo 7:** High-fidelity racing simulation.

***

## Results

### Act 1: Lunar Lander (Manual Tuning)
*   **Workload:** TAC feedback resulted in a **~30% reduction** in perceived cognitive workload.
*   **Success:** Higher task success rates were observed compared to standard tuning.
*   **Effort:** The process remained labor-intensive, requiring approximately **40 iterations** to converge.

### Act 2: Gran Turismo 7 (Automated Learning)
*   **Objective Capture:** Soft-TAC models demonstrated a superior ability to capture preference-specific objectives, such as driving etiquette.
*   **Behavior Diversity:** Produced qualitatively more distinct behaviors compared to models trained with standard Cross-Entropy loss.
*   **Performance:** Outperformed baseline models in aligning with nuanced human preferences.

***

## Contributions

The paper establishes a unified framework where the Trajectory Alignment Coefficient (TAC) bridges manual reward specification and automated reward learning.

1.  **Unified Framework:** Validates TAC as a bridge between manual specification and automated learning.
2.  **Algorithmic Innovation:** Introduces the **Soft-TAC algorithm**, a differentiable formulation of TAC enabling direct gradient-based optimization of reward models.
3.  **Empirical Validation:** Provides comprehensive evidence through a human-subject study (Lunar Lander) and complex simulation (Gran Turismo 7), demonstrating both reduced human effort and improved policy distinctness.