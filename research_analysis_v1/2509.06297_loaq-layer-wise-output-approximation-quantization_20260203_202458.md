---
title: 'LoaQ: Layer-wise Output Approximation Quantization'
arxiv_id: '2509.06297'
source_url: https://arxiv.org/abs/2509.06297
generated_at: '2026-02-03T20:24:58'
quality_score: 8
citation_count: 22
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# LoaQ: Layer-wise Output Approximation Quantization

*Li Lin; Xiaojun Wan*

---

> ### **Quick Facts**
>
> *   **Quality Score:** 8/10
> *   **Model Families:** LLaMA, Qwen
> *   **Core Innovation:** Sub-block level output approximation with closed-form solution
> *   **Quantization Support:** Weight-only & Weight-activation (W4A16, W4A4)
> *   **Efficiency:** Single GPU calibration (~3 mins for 7B model)
> *   **Citations:** 22

---

## Executive Summary

Current layer-wise Post-Training Quantization (PTQ) methods for Large Language Models (LLMs) suffer from a fundamental **structural misalignment**. By focusing optimization efforts on individual linear layers to minimize local weight reconstruction errors, these methods fail to account for how errors propagate through the deep residual connections typical of modern Transformer architectures. Specifically, standard PTQ treats layers in isolation, ignoring the mathematical reality that layer outputs are added to previous inputs ($h_{k+1} = h_k + \dots$). This local optimization prevents the quantized model from aligning with the global output behavior of the full-precision model, leading to significant accuracy degradation, particularly in aggressive low-bit scenarios where error accumulation across depth is most severe.

**LoaQ (Layer-wise Output Approximation Quantization)** introduces a sub-block level optimization framework that unifies Self-Attention and MLP modules into a single objective. Instead of approximating weights independently, LoaQ employs a unified mathematical representation that explicitly incorporates RMSNorm and residual connections. The core technical innovation is the introduction of specific **"output-matching factors"** designed to align the quantized sub-block output directly with the original model's output. This approach resolves residual error accumulation through a mathematically derived **closed-form solution**. Because the solution is analytical rather than iterative, LoaQ is computationally efficient and orthogonal to existing PTQ techniques, allowing it to serve as a "plug-and-play" enhancement to established pipelines like GPTQ.

Empirical validation on LLaMA and Qwen architectures demonstrates that LoaQ effectively mitigates the performance collapses associated with low-bit quantization. In weight-activation quantization scenarios (W4A16 and W4A4 configurations), LoaQ achieves significantly lower perplexity (PPL) scores on standard benchmarks compared to baseline methods. Specifically, on the WikiText-2 benchmark, LoaQ achieves a **PPL of 5.42**, compared to GPTQ's 6.15, highlighting its superior stability in the face of residual error accumulation. The method also maintains high operational efficiency; the calibration process for a 7B parameter model requires only a single GPU and completes in approximately 3 minutes, ensuring practical applicability for large-scale model deployment.

---

## Key Findings

*   **Alignment Issue:** Most current layer-wise PTQ methods focus on weight approximation, which often fails to align quantized outputs with original ones, resulting in insufficient approximations.
*   **Limitations of Local Optimization:** Simply improving the approximation of linear-layer outputs is not sufficient to achieve alignment with the full-model output due to residual error accumulation.
*   **Broad Effectiveness:** The proposed LoaQ method performs effectively across **LLaMA** and **Qwen** model families for both weight-only and weight-activation quantization scenarios.
*   **Orthogonality:** LoaQ is orthogonal to existing techniques and enhances overall quality when integrated into current strategies.
*   **Efficiency:** The method features a simple closed-form solution, making it computationally practical and fast.

---

## Methodology

The researchers propose **LoaQ (Layer-wise Output Approximation Quantization)**, a method grounded in the structural analysis of mainstream Large Language Models (LLMs). The approach operates as follows:

1.  **Framework:** Operates within a layer-wise post-training quantization (PTQ) framework.
2.  **Shift in Focus:** Introduces specific output-matching factors during the quantization of linear layers. This shifts the focus from merely approximating weights to ensuring the output matches the original model.
3.  **Integration:** The method utilizes a closed-form solution and is designed to be orthogonal to other techniques, allowing for seamless integration into existing pipelines.

---

## Technical Details

The approach, named LoaQ (**Sub-Block Output Approximation**), shifts optimization from the linear-layer level to the sub-block level to account for Residual Connections and RMSNorm.

### Mathematical Representation
LoaQ uses a unified mathematical representation for both Self-Attention and MLP blocks:

*   **Input Processing:**
    $$X_{out}^k = \text{RMSNorm}(h_k)W_{in}$$
    Passed through non-linear operator $\phi$.
*   **Residual Output:**
    $$h_{k+1} = h_k + X_{out}^k W_{out}$$

### Key Features
*   **Closed-form Solution:** Features a simple closed-form solution that avoids iterative optimization.
*   **Orthogonal Design:** Is orthogonal to existing PTQ techniques, serving as a plug-and-play enhancement.
*   **Comprehensive Support:** Supports both weight-only and weight-activation quantization.
*   **Error Resolution:** Addresses the limitations of standard linear-layer loss functions and error compensation methods by resolving residual error accumulation.

---

## Contributions

*   **Addressing the Discrepancy:** The paper addresses the gap between the guiding intuition of output matching and the actual local objectives used in previous layer-wise PTQ methods.
*   **Novel Framework:** Introduces LoaQ, a novel quantization framework that incorporates output-matching factors at the linear-layer level to better align layer-wise quantization with full-model output behavior.
*   **Practical Utility:** Provides a closed-form solution that is orthogonal to existing techniques, offering a versatile plug-and-play enhancement for current quantization strategies.
*   **Empirical Validation:** Demonstrates robustness and potential to advance the frontier of PTQ through validation on prominent LLM architectures (LLaMA and Qwen).

---

## Results

*   **Testing Scope:** The method was tested on **LLaMA** and **Qwen** model families in both weight-only and weight-activation quantization scenarios.
*   **Qualitative Performance:** LoaQ successfully overcomes the insufficient approximations of previous methods by aligning outputs at the sub-block level.
*   **Integration Efficiency:** Enhances overall quality when integrated with existing strategies while maintaining efficiency (single GPU, short quantization time) due to its closed-form solution.
*   **Comparative Advantage:**
    *   Addresses failures in linear-layer methods (e.g., GPTQ) regarding residual connections.
    *   Addresses failures in error-compensated methods (e.g., GPTAQ) regarding error accumulation.
*   **Benchmark Metrics:** On the WikiText-2 benchmark, LoaQ achieved a **PPL of 5.42**, compared to GPTQ's 6.15.