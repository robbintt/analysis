# Report on NSF Workshop on Science of Safe AI

*Rajeev Alur; Greg Durrett; Hadas Kress-Gazit; Corina Păsăreanu; René Vidal*

---

**QUICK FACTS**

| **Metric** | **Detail** |
| :--- | :--- |
| **Event** | NSF Workshop on Science of Safe AI |
| **Date** | February 26, 2025 |
| **Location** | University of Pennsylvania |
| **Quality Score** | 9/10 |
| **References** | 16 Citations |
| **Focus** | Bridging the transparency and safety gap in foundation models |

---

## Executive Summary

This report synthesizes the proceedings of the NSF Workshop on the Science of Safe AI, convened to address the critical **"transparency and safety gap"** in modern artificial intelligence. The workshop brought together investigators from the NSF Safe Learning-Enabled Systems (SLES) program and a broader cohort of AI safety researchers to analyze why high-performance foundation models often function as opaque "black boxes" lacking transparent reasoning or verifiable safety guarantees.

Participants identified that while safety criteria are well-defined for autonomous control and robotics, they remain ambiguous for general applications such as chatbots and clinical decision support. Consequently, the report argues that the field must evolve beyond optimizing for accuracy to integrate safety and trustworthiness as core scientific challenges.

The report proposes an interdisciplinary framework synthesizing Control Theory, Cyber-Physical Systems, and Formal Methods with Machine Learning Theory and NLP. A key innovation is the call for novel specification languages capable of capturing hybrid definitions and abstract "perceived safety." Deliverables include standardized evaluation metrics mapped to industry frameworks (e.g., NIST AI RMF, UL 4600) and a research agenda to establish scientific foundations for the next generation of AI-enabled systems.

---

## Key Findings

*   **Transparency and Safety Gap**
    Despite high performance, current complex AI models—specifically foundation models—lack transparency regarding their reasoning processes and do not offer safety guarantees for their predictions.

*   **Context-Dependent Safety**
    Safety criteria differ significantly across domains:
    *   **Critical & Defined:** Autonomous control and robotics.
    *   **Ambiguous:** Broader applications like chatbots and clinical decision support.

*   **Need for Foundations**
    To fulfill the promise of AI, the field must move beyond simple accuracy metrics to integrate safety and trustworthiness as core scientific challenges.

---

## Methodology

The insights presented in this report were derived from a collaborative, workshop-based methodology.

*   **Event:** University of Pennsylvania, February 26, 2025.
*   **Participants:** Investigators from the NSF Safe Learning-Enabled Systems (SLES) program and a wider cohort of AI safety researchers.
*   **Process:** Findings were synthesized through focused discussions within specific working groups, converging theoretical and practical perspectives.

---

## Technical Details

### Interdisciplinary Framework for SLES
The report outlines a framework for Safe Learning-Enabled Systems (SLES) based on the integration of:
*   Control Theory
*   Cyber-Physical Systems
*   Formal Methods
*   Machine Learning Theory & NLP

### Taxonomy of Safety Definitions
The report details a comprehensive taxonomy covering three dimensions:

1.  **System States**
    *   *Focus:* Grounded constraints.
    *   *Examples:* Safe/reach-avoid sets, temporal logic, abstraction.
2.  **Adversaries**
    *   *Focus:* System integrity.
    *   *Examples:* Robustness, resilience, verification.
3.  **People**
    *   *Focus:* Human interaction and understanding.
    *   *Examples:* Explainability, causal consistency.

### Novel Specification Languages
The authors suggest a critical need for new specification languages to capture:
*   Hybrid definitions
*   Semantic properties
*   Abstract "perceived safety"

---

## Evaluation Metrics and Standards

### Proposed Metrics
As a workshop report, the text focuses on defining critical evaluation metrics rather than presenting numerical experimental results.

*   **Empirical Measures:** Quantifiable real-world performance (e.g., "miles driven without accident").
*   **State-Space Metrics:** Adherence to mathematically defined safe sets.
*   **Context-Dependent Metrics:** Evaluations that incorporate scale and severity weighting.

### Industry Standards Alignment
The report identifies specific industry standards necessary for benchmarking:
*   **Autonomous Driving:** UL 4600
*   **Risk Management:** NIST AI RMF
*   **Medical Applications:** FDA Guidance
*   **Functional Safety:** ISO/IEC TR 5469:2024
*   **Aerospace Frameworks:** EUROCAE ER-027 / SAE AIR6987

*Note: The report concludes that community resources like competitions and benchmarks are needed to standardize safety metric reporting.*

---

## Contributions

*   **New Research Agenda**  
    The report articulates a comprehensive research agenda designed to establish the scientific foundations for the next generation of AI-enabled systems.

*   **Theoretical and Practical Framework**  
    It outlines a clear roadmap for developing the necessary theory, methods, and tools required to ensure AI systems are safe and trustworthy across diverse applications.