---
title: Accelerating Large Language Models through Partially Linear Feed-Forward Network
arxiv_id: '2501.10054'
source_url: https://arxiv.org/abs/2501.10054
generated_at: '2026-02-06T03:00:17'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Accelerating Large Language Models through Partially Linear Feed-Forward Network

*Gansen Hu; Zhaoguo Wang; Jinglin Wei; Wei Huang; Haibo Chen*

---

> ### ⚡ Quick Facts
>
> *   **Parameter Reduction:** 80% reduction in Feed-Forward Network (FFN) parameters.
> *   **Inference Speedup:** 1.6x faster on vLLM; 1.4x faster on HuggingFace.
> *   **Accuracy Trade-off:** Minimal 10.9% degradation.
> *   **Benchmark Performance:** Up to 65% higher accuracy than SOTA pruning methods (Wanda, RIA) under high compression.
> *   **Primary Target:** FFN blocks (67-80% of total model parameters).

---

## Executive Summary

Large Language Models (LLMs) face significant deployment challenges due to their immense computational requirements and memory footprint. A primary contributor to this inefficiency is the Feed-Forward Network (FFN) block, which constitutes roughly 67-80% of the total parameters in an LLM. In standard architectures, the FFN acts as a major bottleneck for inference latency, accounting for approximately 78.2% of total I/O time.

The authors introduce **TARDIS**, a novel framework that adapts compiler optimization techniques—specifically "constant folding"—to the domain of deep learning. The method targets the FFN by merging its two dense matrices into a single compact matrix using the associative property of matrix multiplication. To handle non-linear activation functions like GELU, which typically cause significant accuracy loss when linearized, TARDIS employs a "partial linearization" strategy. It identifies frequent input ranges where the activation can be safely approximated linearly and deploys an online predictor to detect outliers, dynamically reverting to the original non-linear computation only when necessary.

Evaluations demonstrate that TARDIS achieves an 80% reduction in FFN parameters while delivering substantial inference speedups of 1.6x on the vLLM engine and 1.4x on HuggingFace for a 7B parameter model. These efficiency gains are maintained with a relatively minor accuracy trade-off of only 10.9%. This research establishes a significant new optimization paradigm by successfully bridging compiler optimization concepts with deep learning model compression.

---

## Key Findings

*   **Significant Parameter Reduction:** Achieves an 80% reduction in parameters within the feed-forward networks of Large Language Models (LLMs).
*   **Superiority over Pruning:** Outperforms state-of-the-art pruning methods (Wanda and RIA) with up to **65% higher accuracy** under high compression ratios.
*   **Real-World Speedup:** Delivers a **1.6x** end-to-end inference speedup in vLLM and a **1.4x** speedup in HuggingFace on a 7B model.
*   **Balanced Performance:** Maintains these efficiency benefits with an accuracy trade-off of only **10.9%**.

---

## Methodology

The proposed method, **TARDIS**, draws inspiration from compiler optimization techniques like "constant folding." The core strategy involves reducing parameters by treating activation functions as linear, utilizing a partial approximation strategy for non-linearities.

*   **Partial Approximation Strategy:** To handle non-linear activation functions (e.g., GELU), the system identifies frequent input ranges to linearize activations.
*   **Dynamic Outlier Handling:** For inputs outside these frequent ranges (outliers), an online predictor dynamically reverts to the original non-linear computations to preserve model integrity.
*   **Compiler Inspiration:** The approach bridges compiler optimization concepts with deep learning model compression, offering a structural alternative to traditional pruning.

---

## Technical Details

### Architecture & Bottleneck Analysis
*   **Target Component:** The Feed-Forward Network (FFN) block, which constitutes **67-80%** of total LLM parameters.
*   **I/O Bottleneck:** Analysis on Falcon-7B (RTX 4090) confirms the FFN accounts for 78.2% of total inference I/O time (2.1s vs 0.9s for Attention blocks).
*   **Matrix Merging:** Adapts 'constant folding' to merge two FFN matrices into a single compact matrix using the associative property of matrix multiplication, theoretically enabling up to **87.5%** parameter reduction.

### The Linearization Challenge
*   **The Naive Approach Problem:** Naively linearizing non-linear activation functions (GELU, SiLU) results in severe performance degradation, specifically a **75% accuracy drop**.
*   **The TARDIS Solution:** Exploits the observation that activation function inputs are concentrated in a narrow range. This "partial linear" solution allows for approximation with low error, enabling optimization without significant accuracy loss.

---

## Results & Contributions

### Performance Results
*   **Compression & Speed:** 80% reduction in FFN parameters with 1.6x (vLLM) and 1.4x (HuggingFace) speedups on a 7B model.
*   **Comparison:** Significantly outperforms SOTA pruning methods (Wanda, RIA) in high compression scenarios.

### Core Contributions
*   **New Optimization Perspective:** Introduces a framework bridging compiler optimization concepts with deep learning model compression.
*   **Optimization Mechanism:** Provides a viable mechanism to optimize LLMs with complex non-linear activations through partial linearization and dynamic fallback.
*   **Pareto Frontier Validation:** Validates that structural linear approximation offers a better Pareto frontier between compression, speed, and accuracy compared to traditional unstructured pruning methods.

---

**Document Quality Score:** 8/10 | **References:** 40 citations