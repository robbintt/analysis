# Provable Reinforcement Learning from Human Feedback with an Unknown Link Function

*Qining Zhang; Lei Ying*

---

> ### **Quick Facts**
>
> *   **Quality Score**: 9/10
> *   **Algorithm**: Zeroth-Order Sign Policy Optimization (ZSPO)
> *   **References**: 40 Citations
> *   **Key Innovation**: Eliminates need for known link functions
> *   **Guarantees**: Provable convergence with polynomial rate
> *   **Setting**: Episodic MDP with Utility-based RLHF

---

## Executive Summary

Current Reinforcement Learning from Human Feedback (RLHF) algorithms, including widely adopted methods like Direct Preference Optimization (DPO) and Proximal Policy Optimization (PPO), depend on the restrictive assumption that the link function—which maps reward differences to human preference probabilities—is known and strictly follows the Bradley-Terry model. In practice, human judgment exhibits significant heterogeneity and rarely adheres to such predefined mathematical structures. This paper addresses the critical issue of **link function misspecification**, demonstrating that standard RLHF methods suffer substantial performance degradation and potential failure when the assumed link function diverges from the true underlying feedback mechanism.

The authors introduce **Zeroth-order Sign Policy Optimization (ZSPO)**, a novel framework that generalizes RLHF by removing the dependency on a known link function. Technically, ZSPO abandons direct gradient estimation based on value function magnitudes, which requires precise probability mappings. Instead, it constructs parameter update directions based solely on the **sign of value function differences**. The algorithm employs an actor network to generate a current policy and introduces a random perturbation vector to sample trajectory pairs. By collecting one-bit preference feedback and applying a majority vote over these pairwise comparisons, ZSPO estimates the sign of the value difference. This estimator constructs an ascent direction that is provably correlated with the true policy gradient, enabling effective optimization without approximating the link function.

ZSPO achieves the first provable convergence guarantees for utility-based RLHF on general episodic Markov Decision Processes (MDPs) with unknown link functions. The algorithm converges to a stationary policy with a sample complexity of $\tilde{O}(\epsilon^{-4})$, a standard polynomial rate for zeroth-order optimization. The theoretical bound on the gradient norm is rigorously decomposed into three specific error terms: a standard Zeroth-Order Error (bias), a Finite Evaluator Error (which scales as $\mathcal{O}(1/\sqrt{N})$), and a Distinguishability Error (which decreases with perturbation distance and batch size). In numerical experiments, ZSPO empirically outperformed DPO and PPO baselines under misspecified conditions—specifically maintaining high utility scores where competitors failed—validating its robustness against modeling errors.

This work significantly advances the theoretical foundations of RLHF by relaxing long-standing assumptions regarding the link function. By providing the first algorithm with rigorous convergence guarantees that operates without knowledge of the exact probability mapping of human feedback, ZSPO offers a more robust and realistic framework for alignment.

---

## Key Findings

*   **Limitations of Current Algorithms:** Current RLHF algorithms (like DPO and PPO) rely on the unrealistic assumption that the link function is known.
*   **ZSPO Solution:** ZSPO effectively solves RLHF problems without requiring knowledge of the link function.
*   **Mechanism of Action:** ZSPO generates valid updates by estimating the **sign** of the value function difference rather than the magnitude.
*   **Convergence:** ZSPO is provably convergent with a polynomial rate under mild conditions.
*   **Performance:** Numerical experiments show ZSPO outperforms existing algorithms when the assumed link function does not match the true one.

---

## Methodology

The authors propose **Zeroth-order Sign Policy Optimization (ZSPO)**, a novel framework for policy optimization.

1.  **Core Concept:** Instead of estimating gradients directly from value function differences (which requires link function knowledge), ZSPO constructs parameter update directions based solely on human preferences by estimating the sign of the value function difference.
2.  **Directionality:** This ensures the constructed direction is positively correlated with the true policy gradient, allowing optimization without approximating the underlying link function.

---

## Technical Details

| Component | Description |
| :--- | :--- |
| **Algorithm Name** | Zeroth-Order Sign Policy Optimization (ZSPO) |
| **Core Innovation** | Eliminates the requirement to know the link function (mapping reward differences to preference probabilities) by estimating only the sign of the value function difference. |
| **Architecture** | Uses an actor network $N_\theta$ and operates as a zeroth-order optimization method. |
| **Process** | The current policy is perturbed using a randomly sampled vector and distance. Feedback is acquired by sampling $N$ pairs of trajectory batches from the current and perturbed policies, with a panelist providing one-bit preference feedback. |
| **Gradient Estimation** | Uses a majority vote over comparisons to determine the sign of the value difference and constructs an ascent direction estimator. |
| **Update Rule** | Policy parameters are updated via gradient ascent using the estimated direction. |
| **Environment Setting** | Episodic Markov Decision Process (MDP) with horizon $H$ within a utility-based RLHF framework assuming an **unknown** link function. |

---

## Results

*   **Comparative Performance:**
    *   Outperforms DPO and PPO when the assumed link function is misspecified (does not match the true link function).
*   **Theoretical Guarantees:**
    *   Provably convergent to a stationary policy with a polynomial rate.
*   **Convergence Metrics:**
    *   The bound on the gradient norm comprises three terms:
        1.  **Zeroth-Order Error:** Standard rate.
        2.  **Finite Evaluator Error:** Improves with number of batches $N$ and evaluator expertise.
        3.  **Distinguishability Error:** Improves with batch size $D$.
*   **Significance:**
    *   The first RLHF algorithm for utility-based RLHF on general MDPs with provable guarantees that does not require knowledge of the link function.

---

## Contributions

*   **Generalization:** Advances the RLHF field by generalizing the formulation beyond the restrictive assumption of known link functions.
*   **Novel Algorithm:** Introduces the first provable RLHF algorithm (ZSPO) that operates without link function knowledge using zeroth-order sign-estimation.
*   **Theoretical Foundation:** Establishes rigorous theoretical convergence guarantees for RLHF with unknown link functions.
*   **Validation:** Provides empirical evidence validating the method's superiority in scenarios of link function mis-specification.

---

**References:** 40 citations
**Quality Score:** 9/10