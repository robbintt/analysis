---
title: 'Playing games with Large language models: Randomness and strategy'
arxiv_id: '2503.02582'
source_url: https://arxiv.org/abs/2503.02582
generated_at: '2026-02-03T06:45:32'
quality_score: 7
citation_count: 10
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Playing games with Large language models: Randomness and strategy

*Alicia Vidler; Toby Walsh*

***

### üìä Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Quality Score** | 7/10 |
| **References** | 10 Citations |
| **Model Tested** | GPT-4o-Mini-2024-08-17 |
| **Framework** | LangChain |
| **Game Types** | Rock Paper Scissors, Prisoner's Dilemma |

***

## üìù Executive Summary

> This research addresses the critical assumption that Large Language Models (LLMs) function as truly stochastic agents capable of objective strategic decision-making, a prerequisite for their integration into autonomous multi-agent systems. The paper investigates the discrepancy between the theoretical randomness of LLM architectures and their actual behavioral outputs in game-theoretic scenarios requiring unbiased randomization.
>
> The key innovation is a technical framework and programmatic toolset, implemented using the LangChain framework, designed to orchestrate interactions between independent AI agents. The study exclusively utilizes the `GPT-4o-Mini-2024-08-17` model snapshot to test classic game theory configurations: Rock Paper Scissors (RPS) and the Prisoner‚Äôs Dilemma (PD).
>
> Empirical results demonstrate that LLMs fail to produce uniform distributions or maintain Markovian properties, indicating a lack of true randomness even with temperature tuning. In repeated Rock Paper Scissors, agents exhibited significant cognitive biases, such as a "fondness for the number 7," and employed loss aversion strategies that led to stalemates. In the Prisoner‚Äôs Dilemma, outcomes fluctuated between cooperation and competition with heavy sensitivity to prompt design.
>
> The study characterizes LLM gameplay as failing to learn strategic nuances over repeated interactions, often displaying greater bias than human players. The findings highlight that prompt engineering alone cannot overcome inherent architectural biases, suggesting that current models are not yet ready for fully autonomous strategic roles.

***

## üîë Key Findings

*   **Lack of True Randomness:** Contrary to the "stochastic parrot" label, LLMs exhibit significant bias and fail to generate truly random outputs.
*   **Loss Aversion in Gameplay:** In repeated games like Rock Paper Scissors, models tend toward loss aversion strategies, which frequently leads to stalemates rather than optimal play.
*   **Sensitivity to Prompt Design:** In the Prisoner's Dilemma, outcomes shift unpredictably between cooperation and competition based on how the prompt is structured.
*   **Suboptimal Performance:** Overall, the gameplay ability of LLMs is characterized as suboptimal, failing to converge on stable strategies like Nash Equilibrium.
*   **Cognitive Biases:** Agents displayed distinct cognitive biases, such as a "fondness for the number 7," and showed greater bias in sampling than human players.

***

## üß™ Methodology

The study employed a rigorous experimental design focusing on the `GPT-4o-Mini-2024-08-17` model to isolate decision-making capabilities.

*   **Game Selection:**
    *   **Rock Paper Scissors:** utilized to test the model's capacity for randomization.
    *   **Prisoner's Dilemma:** utilized to test strategic reasoning and cooperation dynamics.
*   **Interaction Modes:**
    *   Experiments were conducted in both **simultaneous** and **sequential** interaction modes.
*   **Agent Interaction:**
    *   Programmatic tools were developed to facilitate interactions between independent agents.
    *   The methodology rigorously distinguishes between **independent sampling** and **batch sampling** methods.

***

## ‚öôÔ∏è Technical Details

*   **Model Architecture:**
    *   Utilizes Transformer architectures with self-attention mechanisms.
    *   Specific Snapshot: `GPT-4o-Mini-2024-07-18` (selected for performance in random sampling).
*   **Implementation Framework:**
    *   Built using the **LangChain** framework.
    *   LLMs act as independent agents within the system.
*   **Game Theory Configuration:**
    *   Two-stage games tested in **one-shot** and **repeated** formats.
    *   Distinguishes between independent and batch sampling methods.
*   **Statistical Failures:**
    *   Failure to reliably produce uniform distributions.
    *   Failure to maintain Markovian properties despite temperature tuning.

***

## üìà Results

*   **Randomization Failures:** LLMs could not reliably produce uniform distributions or maintain Markovian properties, even with temperature tuning.
*   **Rock Paper Scissors (RPS):**
    *   Agents failed to learn optimal mixed strategies.
    *   Frequent stalemates resulted from loss aversion strategies.
*   **Prisoner's Dilemma (PD):**
    *   Outcomes fluctuated between cooperation and competition.
    *   Results were heavily influenced by prompt design rather than strategic equilibrium.
*   **Comparative Analysis:** Literature review metrics indicate LLMs perform poorly at sampling from specific distributions and may exhibit more bias than human counterparts.

***

## üèÜ Contributions

1.  **Technical Framework:** Provides a programmatic toolset for managing independent agent interactions, aiding future Agentic AI research.
2.  **Empirical Evidence:** Supplies concrete evidence regarding the limitations of current LLMs in strategic decision-making and randomization.
3.  **Implications for Deployment:** Highlights critical pitfalls and risks associated with deploying LLMs within multi-agent systems, emphasizing that prompt engineering cannot fix inherent architectural biases.