# Selective Progress-Aware Querying for Human-in-the-Loop Reinforcement Learning

*Anujith Muraleedharan; Anamika J H*

---

> ### ðŸ“Š Quick Facts
> *   **Quality Score:** 9/10
> *   **Feedback Efficiency:** ~50% budget reduction vs. "Always Querying"
> *   **Task Success:** Near-perfect rates (comparable to full supervision)
> *   **Base Algorithm:** Soft Actor-Critic (SAC)
> *   **Simulation Environment:** PyBullet (UR5 Robot)
> *   **Core Innovation:** Progress-based query gating

---

## Executive Summary

This research addresses the critical bottleneck of resource constraints in Human-in-the-Loop Reinforcement Learning (HiL-RL). While incorporating human feedback significantly accelerates learning, existing methods often assume the availability of abundant human supervisionâ€”a premise that fails in real-world robotic deployments where expert attention is expensive and finite.

The paper tackles the challenge of maintaining the performance gains of human feedback while strictly adhering to limited feedback budgets, bridging the gap between theoretical RL models and the practical limitations of physical systems. The authors introduce **SPARQ (Selective Progress-Aware Querying)**, a query policy designed to optimize the timing of intervention requests.

Built upon a Soft Actor-Critic (SAC) algorithm within a constrained MDP framework, the method integrates potential-based reward shaping to balance environment rewards with corrective signals and query cost penalties. Evaluated using a simulated UR5 robot performing a cube-picking task in PyBullet, SPARQ demonstrated superior efficiency compared to standard baselines. The method achieved task success rates statistically comparable to the "always querying" baseline while reducing the feedback budget consumption by approximately **50%**.

The significance of this paper lies in validating that intelligent, selective observation can replace constant human oversight without sacrificing task proficiency. By demonstrating that high-level performance is scalable even under strict effort constraints, SPARQ provides a viable pathway for the practical deployment of RL in physical robotics.

---

## Key Findings

*   **High Efficiency with Reduced Feedback:** SPARQ achieves near-perfect task success rates comparable to the "always querying" baseline while reducing the feedback budget consumption by approximately 50%.
*   **Superior Stability:** The proposed method provides more stable and efficient learning dynamics than random querying strategies.
*   **Significant Performance Delta:** SPARQ significantly outperforms training without any human feedback, validating the necessity of human-in-the-loop intervention.
*   **Scalability:** The approach demonstrates that selective query strategies can maintain high performance while adhering to realistic human effort constraints often found in physical robot deployments.

---

## Methodology

*   **SPARQ Framework:** The authors introduce the SPARQ (Selective Progress-Aware Querying) policy, designed to optimize the specific timing of human intervention requests.
*   **Progress Monitoring:** The method monitors the learning process and triggers feedback requests (oracle calls) conditionallyâ€”specifically only when learning stagnates or performance worsens, rather than at fixed intervals or randomly.
*   **Simulation Environment:** The system was evaluated using a simulated UR5 robot performing a cube-picking task within the PyBullet physics engine.
*   **Comparative Analysis:** Performance was benchmarked against three specific baselines:
    *   Training with no feedback.
    *   Random querying.
    *   Continuous ("always") querying.

---

## Technical Details

**Core Algorithm & Framework**
*   **Base RL Algorithm:** Soft Actor-Critic (SAC).
*   **Framework:** Constrained MDP designed to maximize return while minimizing human effort.
*   **Reward Structure:** Utilizes potential-based reward shaping combining:
    *   Environment reward.
    *   Human feedback.
    *   Query cost penalty.

**The SPARQ Gate Logic**
The system employs a specific decision logic ("The SPARQ Gate") to trigger requests based on four distinct conditions:

1.  **Worsening Performance:** Performance drops below a specific negative threshold.
2.  **Stagnation:** No improvement is observed for `P` steps (Patience parameter).
3.  **Budget Availability:** The remaining budget `B` is greater than 0.
4.  **Cooldown:** A cooldown counter `c` is equal to 0.

**System Constraints**
*   **Hard Budget Cap:** Enforced to ensure strict adherence to human effort limits.
*   **Cooldown Mechanism:** Prevents redundant or spam-like requests.

**Key Hyperparameters**
*   Patience (`P`)
*   Worsening Threshold
*   Cooldown period

---

## Results

Experiments conducted on a simulated UR5 robot manipulator in PyBullet for a cube picking task compared SPARQ against 'No Oracle', 'Random Querying', and 'Always Querying' baselines.

*   **Efficiency:** SPARQ consumes approximately **50% less** of the feedback budget than the 'Always Querying' baseline.
*   **Performance:** Achieves near-perfect task success rates, statistically similar to the full-supervision baseline.
*   **Comparison:** Significantly outperforms the 'No Oracle' baseline.
*   **Stability:** Demonstrates more stable learning dynamics compared to Random Querying strategies.

---

## Contributions

*   **Resource-Constrained HiL-RL:** Addresses a critical gap in existing literature by moving away from the assumption of abundant feedback and focusing on scenarios where human effort is costly and limited.
*   **Progress-Based Heuristics:** Presents a novel query strategy driven by learning progress, proving that intelligent, selective observation can serve as a viable alternative to constant human supervision.
*   **Practical Feasibility:** Suggests a pathway toward making RL applications for physical robots more scalable and practical in real-world settings by minimizing the requirement for continuous human oversight.

---

**Quality Score:** 9/10
**References:** 17 citations