# Named Entity Recognition in Context

*Colin Brisson; Ayoub Kahfy; Marc Bui; FrÃ©dÃ©ric Constant*

---

> ### ðŸ“Š Quick Facts & Metrics
>
> *   **Average F1 Score:** 85.58
> *   **Performance Gain:** +5 points over competition baseline
> *   **Model Architecture:** Hybrid Retrieval-Augmented NER
> *   **Core Model:** Pindola (Transformer Encoder)
> *   **Pretraining Data:** 3 billion characters of Classical Chinese
> *   **Parameters:** 135M (Small) / 360M (Large)
> *   **Total Sequences:** 12,007 (after augmentation)

---

## Executive Summary

This research addresses the challenge of **Named Entity Recognition (NER) in Classical Chinese)**, a low-resource domain complicated by archaic linguistic structures, polysemy, and sparse annotation availability compared to modern languages. The study focuses specifically on the **EvaHan2025 competition task**, which requires accurate entity extraction and disambiguation across diverse historical corpora, including standard histories (*Shiji*, *Twenty-Four Histories*) and specialized medicinal texts. Resolving entity ambiguity in these texts is critical for digital humanities and the accurate structuring of historical knowledge.

The authors propose a novel hybrid architecture that integrates external knowledge retrieval with generative reasoning to enhance disambiguation. The core technical innovation is a three-stage pipeline:

1.  **Encoding:** Utilizing **\"Pindola,\"** a new bidirectional Transformer encoder (variants with 135M and 360M parameters) pretrained specifically on 3 billion characters of Classical Chinese using FlashAttention v2 and AliBias positional encoding.
2.  **Retrieval:** A module that identifies the top $k=20$ similar contexts using L2 distance metrics.
3.  **Generative Reasoning:** Employs OpenAIâ€™s **o3-mini** to summarize retrieved contexts into Classical Chinese JSON format, which is then fed into the classification model.

The proposed system achieved an **Average F1 score of 85.58** on the EvaHan2025 competition task, surpassing the established baseline by nearly **5 percentage points**. This work establishes a new state-of-the-art standard for the benchmark and introduces Pindola, a significant open resource for the NLP community specializing in Classical Chinese.

---

## Key Findings

*   **High Performance:** The proposed system achieved an average **F1 score of 85.58** on the EvaHan2025 competition task.
*   **Significant Improvement:** The model demonstrated a substantial performance boost, exceeding the competition baseline by **nearly 5 points**.
*   **Enhanced Disambiguation:** Integrating external context retrieval with generative reasoning successfully enhances entity disambiguation.
*   **Domain Efficacy:** Utilizing a transformer-based encoder pretrained specifically on Classical Chinese texts (**Pindola**) is highly effective for this domain.

---

## Methodology

The authors propose a hybrid architecture consisting of three distinct stages designed to handle the nuances of Classical Chinese:

1.  **Encoding**
    *   Utilizes **Pindola**, a modern transformer-based bidirectional encoder.
    *   Pretrained specifically on a large corpus of Classical Chinese texts to capture domain-specific linguistic patterns.

2.  **Retrieval**
    *   Implements a module to fetch relevant external context for specific target sequences.
    *   Uses L2 distance to find the top $k=20$ similar contexts from the training corpus.

3.  **Generative Reasoning**
    *   Applies a generative step (using OpenAI o3-mini) that summarizes retrieved context into Classical Chinese.
    *   The summary is formatted into JSON to facilitate robust entity disambiguation in the subsequent classification step.

---

## Technical Details

### Model Architecture: Pindola
The Pindola model is a Transformer-based bidirectional encoder pretrained on **3 billion characters** of Classical Chinese text.

| Feature | Specification |
| :--- | :--- |
| **Attention Mechanism** | FlashAttention v2 |
| **Activation Function** | SwiGLU |
| **Positional Encoding** | AliBias |
| **Tokenizer** | SentencePiece (65,536 vocab size) |
| **Max Sequence Length** | 2048 tokens |

**Model Variants:**
*   **Pindola Small:** 12 layers, 135M parameters (used for retrieval tasks).
*   **Pindola Large:** 28 layers, 360M parameters (used for NER).

### System Pipeline
1.  **Context Retrieval:** Uses L2 distance to identify top $k=20$ similar contexts.
2.  **Context Summarization:** Utilizes a generative model (OpenAI o3-mini) to create JSON summaries of the retrieved context.
3.  **Token Classification:**
    *   **Input Format:** `[CLS] Target [SEP] Summary [SEP]`
    *   **Classification Head:** Bi-LSTM + CRF

### Data Processing
*   **Data Augmentation:** Produced a corpus of **12,007 annotated sequences**.
*   **Standardization:** All sequences standardized to a maximum length of **510 tokens**.

---

## Results

### Overall Performance
The model achieved an **Average F1 Score of 85.58** on the EvaHan2025 competition task, exceeding the baseline by nearly 5 points.

### Dataset Statistics
Performance was validated across three distinct historical corpora. All datasets were segmented to sequences of 510 tokens or less.

| Dataset | Source Material | Total Sequences | Entity Types |
| :--- | :--- | :--- | :--- |
| **Dataset A** | *Shiji* | 361 | 6 |
| **Dataset B** | Twenty-Four Histories | 3,921 | 3 |
| **Dataset C** | Medicinal Texts | 339 | 6 |

---

## Contributions

*   **Domain-Specific Pretraining:** Introduced **'Pindola,'** a specialized bidirectional transformer encoder tailored for Classical Chinese.
*   **Retrieval-Augmented NER Framework:** Developed a novel pipeline that combines external knowledge retrieval with generative reasoning.
*   **Contextual Disambiguation:** Demonstrated the efficacy of summarizing retrieved context in the target language to resolve entity ambiguity.
*   **Benchmark Performance:** Established a new performance standard for the EvaHan2025 competition.

---

**Quality Score:** 9/10  
**References:** 4 citations