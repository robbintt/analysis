---
title: 'MASPRM: Multi-Agent System Process Reward Model'
arxiv_id: '2510.24803'
source_url: https://arxiv.org/abs/2510.24803
generated_at: '2026-02-03T06:35:37'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# MASPRM: Multi-Agent System Process Reward Model

*Milad Yazdani; Mahdi Mostajabdaveh; Zirui Zhou; Ying Xiong*

---

> ### ðŸ“Š Quick Facts
>
> *   **GSM8K Performance:** +30.7 Exact Match (EM) point improvement (Baseline: 43.9% â†’ MASPRM: 74.6%).
> *   **MATH Performance:** +22.9 EM point improvement.
> *   **Zero-Shot Transfer:** +8.4 EM point improvement on MATH when trained solely on GSM8K.
> *   **Training Efficiency:** Annotation-free using return propagation from MCTS rollouts.
> *   **Inference Role:** Functions as a compute-efficient controller for Beam Search and MCTS.

---

## Executive Summary

Multi-Agent Systems (MAS) have enhanced complex reasoning capabilities by simulating interactions, yet they frequently suffer from inefficiency and a reliance on expensive human supervision. Standard MAS approaches often utilize a single straight-through pass or depend on verifiers that require costly step-level human annotations to function effectively. Furthermore, these systems typically lack a mechanism to dynamically allocate computational resources during inference, resulting in wasted processing power on unpromising reasoning branches. This paper addresses the critical need for a method that improves reasoning accuracy and computational efficiency in MAS without the bottleneck of manual data labeling.

The authors introduce **MASPRM** (Multi-Agent System Process Reward Model), the first process reward model specifically designed for multi-agent frameworks. MASPRM operates as a plug-in value model that estimates per-agent progress by assigning scalar values to actions within partial inter-agent transcripts. Technically, the model eliminates the need for human-labeled intermediate steps through a novel training methodology that utilizes multi-agent Monte Carlo Tree Search (MCTS) rollouts. By propagating final returns back to local targets to create Monte Carlo Q-estimates, the system generates its own supervision signals. During inference, MASPRM serves as a controller for both Stepwise Beam Search and MCTS, evaluating the quality of partial reasoning paths to guide the search toward high-potential solutions.

The implementation of MASPRM yields substantial performance gains over baseline straight-through MAS passes. On the GSM8K dataset, MASPRM-guided decoding improved Exact Match (EM) scores by **+30.7 points**, rising from a baseline of 43.9% to 74.6%. Similarly, on the more challenging MATH dataset, the model achieved a relative gain of **+22.9 EM points**. The system also demonstrates impressive generalization capabilities; a model trained exclusively on GSM8K transferred zero-shot to the MATH dataset, achieving an **+8.4 EM point** improvement. Additionally, MASPRM proved more token-efficient, outperforming standard Majority Vote and Stepwise Beam Search methods under equivalent computational budgets.

This research significantly advances the field of compute-aware reasoning by decoupling multi-agent performance improvements from expensive human annotation. By successfully integrating process reward modeling into the multi-agent architecture, the authors provide a scalable framework that allows Large Language Models to reason more efficiently, focusing computation on the most viable solution paths. The ability of MASPRM to function as a complementary component to existing decoders suggests it will have broad utility, enabling future systems to achieve higher accuracy with better resource management in complex problem-solving domains.

---

## Key Findings

*   **Significant Performance Gains:** MASPRM-guided decoding improves Exact Match (EM) scores by **+30.7 points** on GSM8K and **+22.9 points** on MATH compared to a single straight-through Multi-Agent System (MAS) pass.
*   **Zero-Shot Generalization:** A MASPRM trained exclusively on the GSM8K dataset transfers zero-shot to the MATH dataset, achieving an **+8.4 EM point** improvement.
*   **Annotation-Free Training:** Achieves results without expensive step-level human annotations, using return propagation from multi-agent Monte Carlo Tree Search (MCTS) rollouts.
*   **Compute Efficiency:** Acts as an inference-time controller, guiding beam search and MCTS to focus resources on promising branches.

## Methodology

The authors propose **MASPRM** (Multi-Agent System Process Reward Model), a plug-in value model designed to estimate per-agent progress during inference. The model assigns values to specific actions taken by individual agents within partial inter-agent transcripts.

It is trained using data generated from **multi-agent Monte Carlo Tree Search (MCTS) rollouts**, propagating final returns back to local targets instead of using human-labeled intermediate steps. During deployment, MASPRM guides both step-level beam search and MCTS algorithms, functioning as a controller that evaluates the quality of partial reasoning paths.

## Technical Details

*   **System Framework:** Utilizes a Multi-Agent System (MAS) modeled as a directed graph where states include the question, dialogue history, and the next agent identity.
*   **Architecture:** Employs a shared value head to map intermediate states to scalar values representing expected terminal rewards.
*   **Training Mechanism:** 
    *   Annotation-free training utilizing search-generated supervision.
    *   Uses MAS-specific MCTS rollouts to compute Monte Carlo Q-estimates as regression targets.
    *   Integrates an Outcome Reward Model (ORM) for terminal states.
*   **Inference Strategy:** 
    *   Integrates the PRM into MCTS for node initialization and leaf evaluation.
    *   Alternatively employs Stepwise Beam Search (SBS) guided by PRM scores.

## Results

*   **GSM8K Dataset:** Improved Exact Match (EM) from a baseline of 43.9% to 74.6%.
*   **MATH Dataset:** Achieved a relative gain of +22.9 EM points.
*   **Generalization:** Achieved an +8.4 EM point improvement on MATH when trained exclusively on GSM8K (Zero-shot).
*   **Efficiency:** Improves token-accuracy efficiency, enhancing Majority Vote and Stepwise Beam Search performance under matched token budgets.

## Core Contributions

*   **Process Reward Modeling for MAS:** Introduction of the first Process Reward Model tailored for Multi-Agent Systems, evaluating per-action, per-agent contributions.
*   **Elimination of Step-Level Supervision:** A novel training methodology utilizing MCTS rollouts and return propagation, removing the dependency on costly step-level human annotations.
*   **Compute-Aware Reasoning:** Establishment of a framework for compute-aware multi-agent reasoning, selectively spending computation on high-potential branches.
*   **Versatility:** Demonstration of MASPRM as a complementary component to existing verifier-style decoders.

---

**Quality Score:** 9/10 | **References:** 40 citations