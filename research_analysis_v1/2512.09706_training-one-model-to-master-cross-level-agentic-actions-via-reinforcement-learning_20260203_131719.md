---
title: Training One Model to Master Cross-Level Agentic Actions via Reinforcement
  Learning
arxiv_id: '2512.09706'
source_url: https://arxiv.org/abs/2512.09706
generated_at: '2026-02-03T13:17:19'
quality_score: 8
citation_count: 24
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Training One Model to Master Cross-Level Agentic Actions via Reinforcement Learning

*Kaichen He; Zihao Wang; Muyao Li; Anji Liu; Yitao Liang*

---

> ### üìã Quick Facts
>
> | Metric | Detail |
> | :--- | :--- |
> | **Model Name** | CrossAgent |
> | **Environment** | Minecraft (Open-World) |
> | **Task Scale** | 800+ Tasks |
> | **Training Method** | RL (Multi-Turn GRPO + Cold-start SFT) |
> | **Key Innovation** | Dynamic Action Space / Cross-Level Granularity |
> | **Quality Score** | 8/10 |
> | **References** | 24 Citations |

---

## üìÑ Executive Summary

This research addresses the fundamental limitation of existing agentic AI models being confined to static, single-level action spaces. Current architectures typically struggle to adapt between high-level strategic planning and low-level motor control within a single trajectory, often requiring manual rule engineering or the deployment of multiple specialized models. This constraint severely hampers an agent's ability to function effectively in complex, open-world environments where tasks require dynamic shifts in abstraction and interaction granularity to succeed.

The authors introduce **CrossAgent**, a unified agentic model designed to master heterogeneous action spaces through a "Cross-Level Agentic Actions" architecture. The core technical innovation is a dynamic action space that empowers the model to autonomously select the most effective interface and action granularity for each step of a trajectory. The system is trained via a novel pipeline combining cold-start supervised fine-tuning (SFT) with Multi-Turn Group Relative Policy Optimization (GRPO). This methodology enables the agent to learn complex, adaptive behaviors‚Äîsuch as autonomously switching between high-level planning and low-level control‚Äîwithout relying on hard-coded, human-specified rules.

The proposed method was rigorously evaluated in the open-world Minecraft environment across a benchmark of over 800 distinct tasks. CrossAgent achieved State-of-the-Art (SOTA) results, significantly outperforming baseline agents restricted to single, static action spaces. The evaluation demonstrated that the model not only improves overall task completion rates but also exhibits superior generalization and efficiency in complex, long-horizon reasoning scenarios where sustained adaptability is critical.

The significance of this research lies in its demonstration that a single policy can effectively master diverse action granularities, paving the way for more versatile and autonomous AI agents. By successfully eliminating the need for manual rule engineering to manage interface switching, this work provides a scalable framework for developing agents capable of operating across varying levels of abstraction. This approach is likely to influence future research in agentic AI, shifting the field toward dynamic, cross-level interaction models capable of navigating the complexities of real-world open environments.

---

## üîç Key Findings

*   **State-of-the-Art Performance:** CrossAgent achieved SOTA results on over 800 tasks within the open-world Minecraft environment.
*   **Superior Interface Leverage:** The model significantly outperforms agents limited to single, static action spaces by dynamically leveraging diverse interfaces.
*   **Complex Reasoning:** The approach demonstrated superior generalization and efficiency in complex, long-horizon reasoning scenarios.
*   **Autonomous Switching:** The agent successfully learned to switch between action granularities autonomously without relying on human-specified rules.

---

## üõ†Ô∏è Methodology

The researchers developed **CrossAgent**, a unified agentic model designed to master heterogeneous action spaces. The training process utilizes a comprehensive pipeline:

1.  **Cold-Start Supervised Fine-Tuning (SFT):** The pipeline begins with a cold-start SFT phase to establish a baseline capability.
2.  **Multi-Turn Group Relative Policy Optimization (GRPO):** The core of the training involves this novel RL algorithm, which enables the model to optimize its policy over multi-turn interactions.
3.  **Autonomous Selection:** Throughout the trajectory, the model autonomously selects the most effective interface and action granularity for each step, rather than following a fixed schedule.

---

## ‚öôÔ∏è Technical Details

*   **Architecture:** Utilizes a 'Cross-Level Agentic Actions' architecture to master actions across different levels of granularity.
*   **Action Space:** Employs a dynamic action space with diverse interfaces.
*   **Granularity Switching:** Capable of autonomously switching between action granularities (e.g., high-level planning to low-level control) without relying on hard-coded, human-specified rules.
*   **Training Paradigm:** The model is trained via Reinforcement Learning (RL).

---

## ‚ú® Contributions

*   **Dynamic Interaction Granularity:** Addresses the limitation of existing agentic AI models restricted to static action spaces by proposing dynamic interaction granularity.
*   **Unified Policy:** Introduces a unified model capable of mastering heterogeneous action spaces within a single policy.
*   **Novel Training Methodology:** Contributes a novel training methodology combining cold-start SFT with Multi-Turn GRPO to enable complex, adaptive behaviors without manual rule engineering.

---

## üìä Results

*   **Evaluation Environment:** Conducted in an open-world Minecraft environment.
*   **Benchmark Scale:** Tested on over 800 distinct tasks.
*   **Performance:** Achieved State-of-the-Art (SOTA) results.
*   **Comparison:** Significantly outperformed baseline agents with single, static action spaces.
*   **Capabilities:** Demonstrated superior generalization, efficiency, and efficacy in complex, long-horizon reasoning scenarios.