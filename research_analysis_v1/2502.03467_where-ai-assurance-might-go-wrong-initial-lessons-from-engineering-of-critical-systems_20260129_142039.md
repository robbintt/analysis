# Where AI Assurance Might Go Wrong: Initial lessons from engineering of critical systems

*Robin Bloomfield; John Rushby*

***

> ### ðŸ“‹ Quick Facts
> ---
> *   **Document Type**: Conceptual Position Paper
> *   **Core Concept**: The *Criticality Gap* (orders of magnitude disparity in confidence)
> *   **Proposed Solution**: Assurance 2.0 & Critical Systems Engineering Process
> *   **Key Classification**: AFGI (Artificial Fairly General Intelligence)
> *   **Quality Score**: 9/10
> *   **Citations**: 40 References
> ---

## Executive Summary

This paper addresses the fundamental inadequacy of current Artificial Intelligence safety frameworks when applied to safety-critical domains. Bloomfield and Rushby identify a **"Criticality Gap"**â€”an orders-of-magnitude disparity in the confidence and failure rate tolerances between everyday digital systems and critical infrastructure. They argue that existing AI approaches are hindered by poorly defined system boundaries, insufficient elaboration of risk tolerability, and a lack of theoretical grounding. Consequently, commercial-grade assurance techniques fail to scale to life-critical or environmentally hazardous applications where the cost of failure is unacceptably high.

The authors propose **"Assurance 2.0,"** a framework that maps the rigorous standards of traditional critical systems engineering onto AI assurance. Their analytical approach is structured around three domainsâ€”system engineering, safety and risk analysis, and decision analysisâ€”answered via four foundational questions regarding the system, performance requirements, criticality impact, and trust. Rather than inventing new engineering steps, the innovation lies in applying established engineering rigor to AI through the construction of robust assurance cases.

Key technical differentiators include the evaluation of *decision criticality* alongside system criticality, the integration of "design basis" threats, and the introduction of **"Artificial Fairly General Intelligence" (AFGI)** to classify systems capable of societal degradation without possessing full AGI capabilities. The paper establishes specific engineering metrics required for critical AI adoption, quantifying the Criticality Gap using aviation safety thresholds where catastrophic failures must be "not anticipated to occur in the entire lifetime of all aircraft of one type."

The significance of this work lies in its rigorous challenge to the direct application of "everyday" AI techniques to critical infrastructure. It advocates instead for the adoption of mature standards from the nuclear and aviation sectors. By mapping AI assurance challenges to the precise rigor of critical systems engineering, the paper provides a strategic roadmap for developing safety frameworks capable of supporting high-consequence decision-making.

---

## Key Findings

*   **Criticality Gap**: There is an orders-of-magnitude disparity in the confidence required for critical systems versus everyday systems; standard techniques do not scale to critical applications.
*   **Framework Limitations**: Current AI safety frameworks suffer from insufficiently broad system boundaries, lack of elaboration regarding risk nature and tolerability, and a lack of theoretical foundations.
*   **Assurance 2.0**: The authors advocate for 'Assurance 2.0' assurance cases to support decision-making, emphasizing the evaluation of decision criticality alongside system criticality.
*   **Theoretical Deficit**: Existing assurance methods are limited by a lack of theories to assure specific system behaviors.

## Research Methodology

The study employs an experiential and conceptual analysis approach derived from the authors' professional backgrounds in system and software assurance for societally important systems. The research is structured around three analytical domains:

1.  **System Engineering**
2.  **Safety and Risk Analysis**
3.  **Decision Analysis**

The framework is built by answering four foundational questions regarding the system, performance requirements, the impact of criticality, and trust. All findings are subsequently mapped to questions posed by the FAISC organizers.

## Core Contributions

*   **Critical Systems Perspective**: Provides a comprehensive summary of traditional safety engineering and analyzes how a 'critical systems perspective' supports AI Safety Frameworks.
*   **Gap Identification**: Identifies specific technical gaps in current AI assurance implementation regarding system boundaries, risk tolerability, and behavioral theories.
*   **Decision-Making Framework**: Introduces a framework that evaluates both decision criticality and system criticality using Assurance 2.0.
*   **Standards Advocacy**: Highlights the necessity for significantly higher confidence standards in critical systems, cautioning against the direct application of 'everyday' techniques to safety-critical AI.

## Technical Framework & Architecture

The paper proposes an **'Assurance 2.0'** framework and a Critical Systems Engineering Process rather than a specific AI model architecture.

### Key Components
*   **Shift in Focus**: From system behavior to *decision criticality*.
*   **Measurements**: Explicit confidence measurement, argumentation strength, and bias management.
*   **Architecture Concepts**: Emphasizes 'defence in depth', 'design basis' threats, and socio-technical boundaries including human operators.
*   **AFGI**: Introduces *Artificial Fairly General Intelligence* as a classification for systems capable of societal degradation without AGI-level capabilities.

### The 7-Step Engineering Process
1.  **Environment Definition**
2.  **System Requirements**
3.  **Hazard Analysis**
4.  **Safety Specifications**
5.  **Requirements Validation**
6.  **System Specification**
7.  **Verification**

## Results and Benchmarks

As a conceptual position paper, this study lacks empirical experimental results but provides engineering metrics and standards used as benchmarks for the industry.

*   **Criticality Gap Metrics**: Defines the orders of magnitude disparity in confidence and failure rates between consumer and critical systems.
*   **Aviation Thresholds**: Utilizes standards where catastrophic failures are 'not anticipated to occur in the entire lifetime of all aircraft of one type'.
*   **Design Basis Events**: Establishes qualitative frequency/impact thresholds for worst-case environmental challenges.
*   **Terminology Analysis**: Observational findings suggest current AI frameworks have insufficient boundaries and suffer from terminology misalignment regarding 'safety' and 'security' compared to critical systems engineering.

***

**Document Quality Score**: 9/10 | **References**: 40 Citations