# FeynTune: Large Language Models for High-Energy Theory

*Paul Richmond; Prarit Agarwal; Borun Chowdhury; Vasilis Niarchos; Constantinos Papageorgakis*

***

> ### ðŸ“Š Quick Facts
> | Metric | Value |
> | :--- | :--- |
> | **Model Architecture** | Llama-3.1 (8B Parameters) |
> | **Variants Fine-tuned** | 20 Specialized Models |
> | **Best Perplexity Score** | 9.76 (LoRA-QKV) |
> | **Training Data** | arXiv Abstracts (up to Aug 2024) |
> | **Hardware Used** | 3x NVIDIA A100 40GB GPUs |
> | **Quantization** | 4-bit (Base) / 16-bit (LoRA) |

***

## Executive Summary

This research addresses the critical disparity between general-purpose Large Language Models (LLMs) and the specialized requirements of High-Energy Physics (HEP). While commercial models like ChatGPT and Claude demonstrate broad capabilities, they frequently fail to capture the nuanced theoretical frameworks, precise technical jargon, and logical coherence essential to sub-fields such as High-Energy Theory (hep-th), Phenomenology (hep-ph), and General Relativity (gr-qc). This deficiency is significant because effective AI assistance for scientific research demands domain-accurate reasoning rather than generic text generation, creating a need for models deeply versed in the specific "language" of theoretical physics.

The authors introduce "FeynTune," a suite of 20 specialized LLM variants built upon the Llama-3.1-8B architecture, utilizing Low-Rank Adaptation (LoRA) for efficient fine-tuning. To optimize performance, the study employed two technical strategies: "LoRA-all," which updates all weights, and "LoRA-QKV," which focuses exclusively on Query-Key-Value matrices. The training regimen utilized rigorously curated arXiv abstracts up to August 2024, testing ten distinct data mixtures (s1â€“s10) to isolate the effects of domain specificity. These mixtures combined HEP categories with control datasets from unrelated scientific fields, such as Quantum Biology (q-bio) and Computer Science (cs), to determine the optimal data composition for abstract completion tasks.

The fine-tuned models demonstrated clear superiority over both the base and commercial models. The base Llama-3.1-8B model achieved a perplexity of **11.20**, while the best LoRA-all variant (mixture s3) reduced this to **9.83**, and the best LoRA-QKV variant achieved a further reduction to **9.76**. In benchmark comparisons against commercial entities, Claude 3.5 Sonnet scored **10.07** and GPT-4o scored **10.20**, confirming that the specialized FeynTune models outperformed general-purpose alternatives in this domain. Regarding semantic similarity, the specialized models showed an increase to $0.90 \pm 0.07$ against the baselineâ€™s $0.88 \pm 0.08$; however, this improvement should be framed as statistically uncertain due to the overlapping standard deviations. Qualitative analysis further revealed that FeynTune models exhibited higher entropy (reducing repetitive loops), fewer metadata errors, and improved logical coherence.

The significance of FeynTune lies in its validation of a cost-effective, transparent framework for developing specialized scientific AI using smaller, open-source models rather than relying solely on proprietary, general-purpose black boxes. By proving that fine-tuning on curated domain-specific abstracts yields tangible improvements in technical reasoning and language generation, this work provides a reproducible roadmap for creating niche scientific tools. This approach empowers the research community to build custom, high-performance models that ensure data privacy and scientific relevance without the prohibitive resource overhead of training massive foundation models from scratch.

***

## Key Findings

*   **Performance Improvement:** The 20 fine-tuned variants (FeynTune) consistently outperformed the base Llama-3.1-8B model on hep-th abstract completion tasks.
*   **Domain Specificity:** Models trained on High-Energy Physics data showed distinct utility compared to control datasets from disparate fields like Quantum Biology and Computer Science.
*   **Baseline Establishment:** The study established a performance baseline for specialized open-source models by comparing them against commercial LLMs like ChatGPT, Claude, Gemini, and DeepSeek.
*   **Data Efficacy:** Training on arXiv abstracts up to August 2024, utilizing combinations of hep-th, hep-ph, and gr-qc categories, proved effective.

## Technical Details

**Architecture & Training**
*   **Base Model:** Meta Llama 3.1 (8B Parameters)
*   **Method:** Low-Rank Adaptation (LoRA)
*   **Variants:**
    *   LoRA-all: Updates all weights.
    *   LoRA-QKV: Focuses exclusively on Query-Key-Value matrices.

**Hyperparameters & Hardware**
*   **Rank:** 8
*   **Scaling Factor (Alpha):** 32
*   **Dropout:** 0.05
*   **Optimizer:** AdamW
*   **Hardware:** 3x NVIDIA A100 40GB GPUs
*   **Precision:** 4-bit quantization (base weights), 16-bit (LoRA adapters)

**Dataset Composition**
*   **Source:** arXiv metadata (up to late August 2024)
*   **Categories:** HEP (hep-th, hep-ph, gr-qc) and Controls (q-bio, cs).
*   **Mixtures:** 10 distinct mixtures (s1â€“s10) tested.
*   **Split:** 70% Train / 15% Validation / 15% Test
*   **Epochs:** 4

## Methodology

The study utilized the Llama-3.1 model with 8 billion parameters as the base architecture, employing Low-Rank Adaptation (LoRA) fine-tuning with two distinct approaches (LoRA-all and LoRA-QKV). Datasets were curated from arXiv abstracts up to August 2024 across High-Energy Physics categories (hep-th, hep-ph, gr-qc).

The experimental design included comparative analysis using control groups trained on unrelated scientific datasets (q-bio, cs) and varied dataset sizes. This design was critical to isolate the effects of domain specificity and data volume on model performance.

## Results

*   **Quantitative Metrics:**
    *   **Perplexity:** Significant reduction from baseline (11.20) to best performing models (9.76â€“9.83).
    *   **Commercial Comparison:** FeynTune outperformed Claude 3.5 Sonnet (10.07) and GPT-4o (10.20) in perplexity.
    *   **LoRA Efficiency:** LoRA-QKV variants generally achieved lower perplexity than LoRA-all counterparts.
    *   **Semantic Similarity:** Minimal improvement ($0.90 \pm 0.07$) over the baseline ($0.88 \pm 0.08$), with overlapping deviations.

*   **Qualitative Analysis:**
    *   **Coherence:** Improved logical flow and domain-specific technical language usage.
    *   **Error Reduction:** Reduced metadata errors and text repetition (higher entropy).

## Contributions

*   **Introduction of FeynTune:** 20 specialized, fine-tuned LLM variants specifically designed for theoretical High-Energy Physics.
*   **Validation of Specialized Tuning:** Evidence that fine-tuning smaller open-source models on domain-specific abstracts yields better performance than generic base models.
*   **Framework for Scientific AI Development:** Insights derived from comparisons with commercial models provide a roadmap for developing specialized language models in niche scientific fields.

---
**Document Score:** 7/10 | **References:** 32 citations