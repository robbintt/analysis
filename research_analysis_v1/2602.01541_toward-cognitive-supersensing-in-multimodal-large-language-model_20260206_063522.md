---
title: Toward Cognitive Supersensing in Multimodal Large Language Model
arxiv_id: '2602.01541'
source_url: https://arxiv.org/abs/2602.01541
generated_at: '2026-02-06T06:35:22'
quality_score: null
citation_count: 22
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Toward Cognitive Supersensing in Multimodal Large Language Model

*Boyi Li; Yifan Shen; Yuanzhe Liu; Yifan Xu; Jiateng Liu; Xinzhuo Li; Zhengyuan Li; Jingyuan Zhu; Yunhan Zhong; Fangzhou Lan; Jianguo Cao; James M. Rehg; Heng Ji; Ismini Lourentzou; Xu Cao*

***

### ðŸ“Š Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Proposed Paradigm** | Cognitive Supersensing |
| **Key Architecture** | Latent Visual Imagery Prediction (LVIP) Head |
| **Benchmark** | CogSense-Bench (5 Dimensions) |
| **Top Model Accuracy** | 73.8% (vs. 40.3% Baseline) |
| **Human Comparison** | ~83.5% of Human Average Performance |
| **Notable Achievement** | Crystallized Intelligence: 91.0% (Human: 91.3%) |

***

## Key Findings

*   **Superior Performance:** Multimodal Large Language Models (MLLMs) trained with the proposed 'Cognitive Supersensing' paradigm significantly outperform state-of-the-art baselines on the newly introduced CogSense-Bench.
*   **Robust Generalization:** The method demonstrates exceptional generalization capabilities, achieving superior results on out-of-domain mathematics and science Visual Question Answering (VQA) benchmarks.
*   **Visual Imagery Mechanism:** The study identifies internal visual imagery as a potentially key mechanism for bridging the gap between perceptual recognition and cognitive understanding in AI models.
*   **Limitations of Current Models:** Existing MLLMs struggle with complex cognitive problems involving abstract visual details because they primarily rely on text-based Chain-of-Thought (CoT) reasoning and lack human-like visual reasoning mechanisms.

## Methodology

The authors introduce a **'Cognitive Supersensing'** paradigm, a novel training framework designed to endow MLLMs with human-like visual imagery capabilities. The approach involves:

*   **Latent Visual Imagery Prediction (LVIP) Head:** An integrated component that jointly learns sequences of visual cognitive latent embeddings.
*   **Visual-Text Alignment:** The method aligns visual latent embeddings with the answer to form vision-based internal reasoning chains, moving beyond purely text-based reasoning.
*   **Reinforcement Learning Optimization:** A reinforcement learning stage is employed to optimize text reasoning paths based on the grounded visual latent representations.

## Contributions

*   **Paradigm Proposal:** Introduction of 'Cognitive Supersensing,' a training framework that shifts focus from text-only reasoning to integrating visual imagery and internal reasoning chains within MLLMs.
*   **Benchmark Introduction:** Release of 'CogSense-Bench,' a comprehensive VQA benchmark specifically designed to assess five distinct cognitive dimensions of MLLMs.
*   **Open Science Commitment:** The authors have committed to open-sourcing both the CogSense-Bench and the model weights to facilitate further research and reproducibility.

## Technical Details

The paper proposes shifting intermediate reasoning from discrete text tokens to a continuous representation space ("mind's eye") to better handle visuospatial operations.

### Architecture
*   **Backbone:** Standard visual encoder and LLM.
*   **Augmentation:** Latent Visual Imagery Prediction (LVIP) head, an auxiliary MLP that predicts the latent representation of the ground-truth answer image.

### Training Pipeline
The training process is structured into three distinct stages:

1.  **Data Synthesis:** A teacher MLLM generates and filters reasoning chains.
2.  **Supervised Fine-Tuning (SFT):** Jointly optimizes text generation and latent imagery prediction using a combined loss of cross-entropy and Mean Squared Error (MSE).
3.  **Reinforcement Learning:** Utilizes Flow Networks to sample diverse trajectories based on a reward function combining answer evidence and LVIP grounding.

## Results

The **CogSense-8B** model was evaluated on the CogSense-Bench across five dimensions (Fluid Intelligence, Crystallized Intelligence, Visuospatial Cognition, Mental Simulation, and Visual Routines).

*   **Overall Accuracy:** Achieved an average of **73.8%**, significantly higher than the best baseline's **40.3%**.
*   **Human Parity:** The model reached approximately **83.5%** of human average performance.
*   **Specific Dimensions:**
    *   **Crystallized Intelligence:** Achieved near-human performance (Model: **91.0%** vs. Human: 91.3%).
    *   **Fluid Intelligence:** More than doubled the best baseline's score (Model: **63.8%** vs. Baseline: 29.4%).
*   **Generalization:** The model also demonstrated superior performance on out-of-domain mathematics and science benchmarks compared to state-of-the-art models like GPT-5.2 and Gemini 2.5 Flash.