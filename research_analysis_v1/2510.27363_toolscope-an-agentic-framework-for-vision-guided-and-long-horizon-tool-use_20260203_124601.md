---
title: 'ToolScope: An Agentic Framework for Vision-Guided and Long-Horizon Tool Use'
arxiv_id: '2510.27363'
source_url: https://arxiv.org/abs/2510.27363
generated_at: '2026-02-03T12:46:01'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# ToolScope: An Agentic Framework for Vision-Guided and Long-Horizon Tool Use

*Mengjie Deng; Guanting Dong; Zhicheng Dou*

---

### F4CA Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Quality Score** | 9/10 |
| **Performance Gain** | +6.69% (Average) |
| **Paradigm** | Training-free, Plug-and-play |
| **Key Tools** | Perceive, Search, Code |
| **Citations** | 40 |
| **Core Innovation** | Mitigates visual context degradation in long-horizon tasks |

---

## Executive Summary

Multimodal Large Language Models (MLLMs) currently face significant limitations in performing complex, long-horizon Visual Question Answering (VQA) tasks. While capable of basic visual understanding, these models often suffer from **"visual context degradation,"** where critical image details are lost during extended reasoning chains. Furthermore, standard MLLMs lack intrinsic mechanisms to flexibly utilize external tools—such as search engines or computational code interpreters—which are essential for retrieving up-to-date information or performing precise arithmetic. This paper addresses the critical need to bridge the gap between static MLLM capabilities and the dynamic, tool-augmented reasoning required for real-world visual problem solving without necessitating expensive fine-tuning.

The authors introduce **ToolScope**, a training-free, plug-and-play agentic framework designed to augment MLLMs through a structured three-stage pipeline. The architecture consists of a **"Global Navigator"** that analyzes inputs to formulate high-level strategic plans and select relevant tool subsets; an **"Agentic Executor"** that iteratively executes reasoning by dynamically injecting tool outputs into the context; and a **"Response Synthesizer"** that consolidates the trajectory into a final answer. Technically, the framework employs a specialized toolkit featuring a "Perceive" tool for dynamic visual perception to mitigate context loss, a "Search" tool utilizing BM25 for text and CLIP-based cross-modal retrieval (with a cosine similarity threshold of 0.9) for external knowledge, and a "Code" interpreter for numerical computation.

Empirical validation across four diverse benchmarks—**VQA 2.0, ScienceQA, MAT-Search, and MathVista**—demonstrates the efficacy of the ToolScope framework. The system achieved an average performance improvement of **+6.69%** compared to baseline methods. Qualitative and quantitative analyses confirmed that the specialized "Perceive" tool is critical for maintaining visual context in long-horizon tasks, while the integration of search and code tools significantly enhanced the model's reasoning capabilities.

ToolScope represents a significant structural advancement in the field of agentic AI by unifying global planning with local multimodal perception. Its "plug-and-play" nature allows existing MLLMs to leverage external tools efficiently without modifying backbone model weights, offering a cost-effective and scalable solution for enhancing model capabilities.

---

## Key Findings

*   **Performance Improvement:** ToolScope achieved an average performance increase of **+6.69%** across four diverse VQA benchmarks.
*   **Generalization:** Demonstrated strong capabilities on VQA 2.0, ScienceQA, MAT-Search, and MathVista without task-specific training.
*   **Context Mitigation:** The specialized **'Perceive' tool** effectively addresses visual context degradation, a common issue in long-horizon VQA tasks.
*   **Flexibility:** The framework enables MLLMs to utilize external tools flexibly and efficiently, expanding their reasoning horizon.

---

## Methodology

ToolScope employs a structured, three-stage agentic framework designed to enhance MLLM reasoning:

1.  **Global Navigator**
    *   Provides high-level strategic guidance.
    *   Analyzes the input to generate a global plan.
    *   Selects the relevant subset of tools needed for the task.

2.  **Agentic Executor**
    *   Operates iteratively to augment MLLM capabilities.
    *   Utilizes three external tools:
        *   **Search:** For information retrieval.
        *   **Code:** For computational reasoning.
        *   **Perceive:** For visual context refinement.

3.  **Response Synthesizer**
    *   Consolidates the reasoning process and tool outputs.
    *   Generates a coherent final response based on the aggregated information.

---

## Technical Details

### Framework Paradigm
*   **Type:** Training-free, plug-and-play multimodal agent framework.
*   **Objective:** Solve complex Visual Question Answering (VQA) tasks without fine-tuning the backbone MLLM.

### Pipeline Architecture
1.  **Global Planning:** High-level strategy formulation.
2.  **Agentic Execution:** Iterative tool-augmented reasoning.
3.  **Response Synthesizing:** Final answer generation.

### Core Components
*   **Global Navigator:** Analyzes input to generate global plan $G$ and selects relevant tool subset $T'$.
*   **Agentic Executor:** Autonomously generates tool-call tokens and dynamically injects tool results into the reasoning chain.
*   **Response Synthesizer:** Consolidates reasoning trajectory and synthesizes the final response.

### Specialized Tools
*   **Perceive Tool:** Dynamic Visual Perception implemented natively by the backbone MLLM.
*   **Search Tool:**
    *   *Textual Retrieval:* Uses BM25 on Wikipedia dump.
    *   *Multimodal Retrieval:* Uses CLIP-based cross-modal retrieval with a cosine similarity threshold of **0.9**.
*   **Numerical Computation:** Handles arithmetic, logical inference, and symbolic computation.

### Mathematical Formulation
The process is governed by decomposing the answer probability into global planning and iterative reasoning steps:

$$ P(A|I, Q, T) = P(G|I, Q, T) \times \sum_{s=1}^Y P(R_s |I, Q, G, R_{<s}) \times P(A|I, Q, R) $$

---

## Contributions

*   **Framework Introduction:** Introduced ToolScope, bridging the gap between MLLMs and external tool use for complex vision-guided reasoning.
*   **Structural Innovation:** Unified global planning with local multimodal perception in a novel architecture.
*   **Tool Development:** Developed the specialized 'Perceive' tool specifically to mitigate visual context degradation.
*   **Validation:** Provided comprehensive empirical validation across multiple domains demonstrating the framework's efficacy and generalization.

---

## Results

The evaluation of ToolScope highlights its robustness and efficiency:

*   **Benchmarks:** Evaluated on VQA 2.0, ScienceQA, MAT-Search, and MathVista.
*   **Quantitative:** Achieved an **+6.69%** average performance improvement across the four benchmarks.
*   **Generalization:** Demonstrated strong capabilities without task-specific training.
*   **Retrieval Precision:** Utilized a cosine similarity threshold of 0.9 for visual retrieval to ensure accuracy.
*   **Qualitative:** Findings indicate the 'Perceive' tool is critical for mitigating visual context degradation, and the framework effectively enables flexible tool utilization by MLLMs.

---