# NTKMTL: Mitigating Task Imbalance in Multi-Task Learning from Neural Tangent Kernel Perspective

*Xiaohan Qin; Xiaoxing Wang; Ning Liao; Junchi Yan*

***

> ### **Quick Facts**
> *   **Quality Score:** 8/10
> *   **Citations:** 40 References
> *   **Core Approach:** Neural Tangent Kernel (NTK) Theory & Spectral Analysis
> *   **Key Innovation:** Extended NTK Matrix & SOTA Optimization
> *   **Benchmarks:** NYUv2, CelebA (Supervised & RL)
> *   **Efficiency Variant:** NTKMTL-SR (Shared Representation)

***

## Executive Summary

Multi-Task Learning (MTL) seeks to improve generalization by learning multiple tasks simultaneously, but it frequently suffers from task imbalance, where dominant tasks with larger gradients or simpler optimization landscapes converge at the expense of harder tasks. This imbalance degrades overall model performance, as the shared representation becomes biased toward the easier tasks. Previous mitigation strategies often relied on heuristic adjustments or manual tuning of loss weights, lacking a rigorous mathematical foundation to understand or control the underlying training dynamics of complex MTL systems.

The paper introduces **NTKMTL**, a novel optimization framework grounded in Neural Tangent Kernel (NTK) theory. By modeling the training dynamics of MTL as a linear dynamical system governed by an Ordinary Differential Equation (ODE), the authors introduce an extended NTK matrix to characterize the system. Through spectral analysis of this matrix, they demonstrate that the convergence speed of each task is dictated by its associated eigenvalues.

NTKMTL mathematically balances these speeds by introducing optimal task weights—defined as $\omega_i = \sqrt{\tilde{\lambda}/\lambda_i}$—into the loss function to normalize the scaled maximum eigenvalues. Additionally, the authors propose **NTKMTL-SR**, a computationally efficient variant that approximates the NTK using the shared representation to reduce complexity.

NTKMTL achieved state-of-the-art (SOTA) performance across significant benchmarks, including NYUv2 and CelebA in supervised learning, as well as multi-task reinforcement learning environments. The method proved superior to heuristic approaches like Dynamic Weight Averaging (DWA) in mitigating negative transfer caused by imbalance. Crucially, the NTKMTL-SR variant demonstrated high efficiency, matching the training speed of simple linear scalarization on CelebA with up to 40 tasks. The study also empirically validated that spectral bias directly correlates with task imbalance, showing that training error decays exponentially based on the eigenvalues of the NTK matrix.

This research significantly bridges the gap between the complexity of MTL and the theoretical modeling provided by NTK frameworks. It shifts the field’s focus from heuristic balancing techniques to mathematically grounded spectral analysis, providing a robust theoretical lens for understanding why task imbalances occur during training. By validating the efficacy of NTK-based techniques across both supervised and reinforcement learning paradigms, the work establishes a versatile new standard for optimization.

***

## Key Findings

*   **Mathematical Balancing:** The proposed NTKMTL method mitigates task imbalance by mathematically balancing the convergence speeds of different tasks, rather than relying on heuristics.
*   **Theoretical Accuracy:** The study demonstrates that training dynamics and convergence speeds in complex MTL systems can be accurately characterized using Neural Tangent Kernel (NTK) theory.
*   **State-of-the-Art Performance:** The approach achieves SOTA performance across benchmarks in both multi-task supervised learning and multi-task reinforcement learning.
*   **High Efficiency:** The **NTKMTL-SR** variant achieves high training efficiency and maintains competitive performance by utilizing shared representation approximations.
*   **Spectral Bias Correlation:** Confirmed that spectral bias mirrors task imbalance and that training error decays exponentially based on NTK eigenvalues.

***

## Methodology

The research methodology centers on the application of Neural Tangent Kernel (NTK) theory to analyze and control Multi-Task Learning systems:

1.  **NTK Dynamics:** The research leverages NTK theory to analyze the training dynamics of MTL systems, modeling them as linear dynamical systems.
2.  **Extended NTK Matrix:** An extended NTK matrix is introduced specifically for the MTL context to capture the interactions between different tasks.
3.  **Spectral Analysis:** The method applies spectral analysis to the extended matrix to assess and balance the convergence speeds of multiple tasks (NTKMTL).
4.  **Approximation for Efficiency:** An approximation method is developed based on the shared representation of the model (via the chain rule) to reduce computational complexity and improve training efficiency (NTKMTL-SR).

***

## Technical Details

### Modeling & Dynamics
*   **System Type:** Linear dynamical system governed by an Ordinary Differential Equation (ODE).
*   **Matrix Definition:** Introduction of the Extended NTK Matrix ($\tilde{K}$) for spectral analysis.

### Optimization Algorithm (NTKMTL)
*   **Mechanism:** Balances convergence speeds by introducing task weights into the loss function.
*   **Weight Formula:** Task weights are calculated as:
    $$ \omega_i = \sqrt{\tilde{\lambda}/\lambda_i} $$
    where $\tilde{\lambda}$ represents the target normalized eigenvalue and $\lambda_i$ represents the eigenvalue for task $i$.
*   **Objective:** Normalizing the scaled maximum eigenvalues to ensure uniform convergence.

### Efficiency Variant (NTKMTL-SR)
*   **Approximation:** Approximates the NTK using the shared representation.
*   **Cost Reduction:** Utilizes the chain rule to require only **one backpropagation** per iteration, significantly lowering computational overhead.

***

## Results

*   **Benchmark Dominance:** NTKMTL achieved State-of-the-Art (SOTA) performance on multi-task supervised and reinforcement learning benchmarks including **NYUv2** and **CelebA**.
*   **Imbalance Mitigation:** Effectively mitigated task imbalance compared to heuristic methods like **DWA** (Dynamic Weight Averaging) through spectral balancing.
*   **Computational Speed:** The NTKMTL-SR variant demonstrated training speeds nearly identical to Linear Scalarization on CelebA (handling up to 40 tasks), significantly reducing computational overhead compared to full NTK calculation.

***

## Contributions

*   **Theoretical Lens:** Provides a novel theoretical framework for understanding MTL dynamics by bridging the gap between MTL complexity and NTK modeling.
*   **Optimization Innovation:** Proposes NTKMTL, a new optimization method that shifts focus from heuristic balancing to mathematically grounded spectral analysis of the NTK.
*   **Efficient Solution:** Contributes NTKMTL-SR, offering a solution that balances theoretical rigor with computational efficiency.
*   **Cross-Domain Validation:** Establishes the versatility of NTK-based balancing techniques by validating their effectiveness across both supervised and reinforcement learning paradigms.