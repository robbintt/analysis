# Where and How to Enhance: Discovering Bit-Width Contribution for Mixed Precision Quantization

*Haidong Kang; Lianbo Ma; Guo Yu; Shangce Gao*

---

> ### ðŸ“Š Quick Facts
>
> *   **Quality Score:** 9/10
> *   **References:** 32 citations
> *   **Proposed Method:** SMPQ (Shapley-based MPQ)
> *   **Key Innovation:** Utilizing Shapley values for bit-width contribution measurement.
> *   **Efficiency:** < 10 GPU hours on ImageNet1K (comparable to EdMIPS).
> *   **Search Space Scale:** $3.9 \times 10^{78}$ possible configurations (ResNet-101, 6 candidates).

---

## Executive Summary

This research addresses a critical flaw in existing Differentiable Mixed Precision Quantization (DMPQ) methods: the assumption that the magnitude of a learned parameter ($\alpha$) accurately reflects a specific bit-width's contribution to model performance. The authors demonstrate that this proxy is frequently invalidâ€”a phenomenon termed **"$\alpha$'s pitfall"**â€”which leads to sub-optimal bit-width configurations. This issue is exacerbated by the exponential complexity of the search space; for a network like ResNet-101 with 6 bit-width candidates, the configuration space reaches approximately $3.9 \times 10^{78}$. Without a reliable selection criterion, current state-of-the-art approaches fail to navigate this vast space effectively, creating a need for a more robust optimization strategy that aligns search parameters with actual task performance.

To resolve these limitations, the authors propose **SMPQ (Shapley-based MPQ)**, a novel framework that incorporates concepts from cooperative game theory to replace gradient-based heuristics. Instead of using magnitude-based selection or continuous relaxation via softmax to model the search space as a Directed Acyclic Graph (DAG), SMPQ utilizes Shapley values to measure the direct marginal contribution of a specific bit-width operation to final task performance. Recognizing that calculating exact Shapley values is computationally prohibitive for large-scale networks, the authors integrate a **Monte Carlo sampling-based approximation strategy**. This shift from differentiable proxy optimization to contribution-based measurement ensures that bit-width selection is driven by actual utility rather than indirect gradient signals.

Empirical results validate the theoretical superiority of the proposed method. Experiments quantifying the "pitfall" revealed a significant misalignment between DMPQ parameter magnitudes and discretization accuracy, with validation accuracy varying non-linearly (approximately 65% to 70%) across bit-widths. SMPQ successfully overcomes this, achieving state-of-the-art performance on mainstream benchmarks by correctly identifying high-contribution configurations. Crucially, the method maintains high computational efficiency; when navigating the massive $3.9 \times 10^{78}$ configuration space of ResNet-101, SMPQ operates within a feasible computational budget comparable to baselines like EdMIPS, which require less than 10 GPU hours on ImageNet1K.

The significance of this work lies in challenging the dominant paradigm of differentiable search for mixed precision quantization. By establishing that measuring direct contribution is strictly superior to gradient-based optimization, the authors invalidate the standard practice of relying on learnable parameter magnitudes as proxies for performance. This research shifts the focus from continuous relaxation tricks to interpretable, contribution-based metrics, setting a new standard for quantization research. The successful application of Shapley values suggests that game-theoretic approaches offer a powerful, theoretically sound avenue for solving complex resource allocation problems in deep learning.

---

## Key Findings

*   **Fundamental Flaw in DMPQ:** Existing Differentiable Mixed Precision Quantization (DMPQ) methods rely on the erroneous assumption that parameter magnitude reflects actual bit-width contribution to task performance.
*   **Superiority of Shapley Values:** Using Shapley values to measure the direct contribution of bit-width operations provides a significantly more accurate selection criterion than gradient descent-based magnitude.
*   **State-of-the-Art Performance:** The proposed Shapley-based MPQ (SMPQ) method achieves state-of-the-art results on mainstream benchmarks, outperforming gradient-based competitors.
*   **Computational Efficiency:** The high computational cost typically associated with calculating exact Shapley values is effectively mitigated using Monte Carlo sampling-based approximation strategies.

---

## Methodology

The authors propose the **Shapley-based MPQ (SMPQ)** framework to correct the misalignment in current quantization techniques.

1.  **Theoretical Foundation:** The framework utilizes Shapley values from cooperative game theory. Instead of relying on gradient descent and parameter magnitude, SMPQ calculates the direct contribution of specific bit-width operations to the model's overall performance.
2.  **Approximation Strategy:** To address the computationally expensive nature of exact Shapley value calculations, the methodology integrates a Monte Carlo sampling-based approximation strategy. This allows the method to scale to large networks without prohibitive latency.
3.  **Validation:** The approach is rigorously validated against existing Differentiable MPQ (DMPQ) methods through extensive experiments on mainstream benchmarks to demonstrate both accuracy gains and computational feasibility.

---

## Technical Details

### Proposed Solution: SMPQ
The paper introduces SMPQ (Shapley-based MPQ), which fundamentally changes how bit-width assignment is determined:
*   **Shift from Gradient to Contribution:** It replaces traditional gradient descent-based magnitude selection with a metric derived from Shapley values.
*   **Game Theory Application:** This approach measures the direct contribution of a specific bit-width operation to the final task accuracy.

### Critique of DMPQ
The paper provides a detailed critique of Differentiable Mixed Precision Quantization (DMPQ):
*   **Search Space Modeling:** DMPQ finds optimal bit-width configurations by representing the search space as a DAG $G=(V,E)$, where nodes are latent representations and edges are associated with specific bit-widths.
*   **Continuous Relaxation:** It transforms the discrete selection problem into continuous optimization using continuous relaxation, mixing candidate bit-widths via a softmax function.
*   **The "$\alpha$'s Pitfall" Issue:** The authors identify a fundamental flaw where DMPQ assumes the magnitude of the learnable parameter $\alpha$ reflects the bit-width's contribution to accuracy. The paper proves this assumption is often invalid.

---

## Results

### Search Space Complexity
The analysis highlights the immense challenge of searching for optimal bit-width configurations:
*   **Configuration Scale:** For ResNet-101 with only 6 bit-width candidates, the total number of configurations reaches $6^{101} \approx 3.9 \times 10^{78}$.
*   **Efficiency Baselines:** For context, baseline methods like EdMIPS consume less than 10 GPU hours on ImageNet1K.

### Empirical Validation
*   **"Pitfall" Validation:** Empirical tests showed a clear misalignment between DMPQ selection and peak discretization accuracy. Validation accuracy varied non-linearly across bit-widths, ranging approximately from 65% to 70%, proving that parameter magnitude is a poor proxy for performance.
*   **Benchmark Performance:** The proposed SMPQ method achieved state-of-the-art performance on mainstream benchmarks, successfully navigating the massive search space to identify configurations that gradient-based methods missed.

---

## Contributions

1.  **Identification of Flaw:** Identified and critiqued the implicit assumption in existing Differentiable MPQ research that quantization parameter magnitude equates to bit-width contribution.
2.  **Theoretical Framework:** Introduced a novel theoretical framework applying Shapley values to quantify the specific contribution of different bit-widths to model accuracy.
3.  **Algorithm Development:** Developed the SMPQ algorithm, which replaces gradient-based policy optimization with contribution-based measurement and utilizes Monte Carlo approximation for computational feasibility.
4.  **New Standard Established:** Established a new state-of-the-art standard by demonstrating that direct contribution measurement yields better accuracy-complexity trade-offs than gradient-based optimization.