# Chart Question Answering from Real-World Analytical Narratives

*Maeve Hutchinson; Radu Jianu; Aidan Slingsby; Jo Wood; Pranava Madhyastha*

---

> ### ðŸ“Š Quick Facts
> *   **Dataset Size:** 205 QA pairs, 103 visualization images
> *   **Top Performing Model:** GPT-4.1 (69.27% accuracy)
> *   **Complexity Metric:** 36.6% of questions require multi-view synthesis
> *   **Unanswerable Rate:** 16.1%
> *   **Quality Score:** 8/10

---

## Executive Summary

Current benchmarks for Chart Question Answering (CQA) rely predominantly on synthetic or isolated static charts, creating a significant gap between model evaluation and real-world application. This lack of "ecological validity" means that while multimodal Large Language Models (LLMs) perform well on controlled, simplified tasks, they often fail to handle the complexities inherent in genuine data analysis workflows. Addressing this is critical for the development of AI agents capable of serving as effective analytical assistants, as they must navigate messy, real-world data and interpret insights within the context of broader narratives rather than merely extracting isolated facts from idealized figures.

To bridge this gap, the researchers constructed a novel dataset derived from Literate Visualization (litvis) notebooks containing authentic student coursework. Technically, the authors utilized a headless browser to render rich, HTML-based visualizations, systematically enumerating interactive control states to capture multi-view perspectives. Questions were grounded in analytical narratives using the VLAT taxonomyâ€”covering tasks such as retrieving values and finding correlationsâ€”and were generated by extracting quotes from narrative text chunks. The resulting multiple-choice format includes three plausible distractors and a "Cannot be determined" option, ensuring the benchmark demands the integration of textual context with visual data rather than simple visual recognition.

The final dataset comprises 205 high-quality QA pairs and 103 visualizations. Benchmarking results revealed a significant performance drop for state-of-the-art models compared to synthetic baselines. GPT-4.1 achieved the highest accuracy at 69.27%, substantially outperforming Qwen2.5-VL-32B (56.59%) and Qwen2.5-VL-7B (31.71%). However, GPT-4.1 displayed notable variance across analytical tasks, excelling at "Retrieve Value" (76.47%) but struggling significantly with complex inference, achieving only 50.00% on "Make Comparisons" and 44.44% on "Find Anomalies."

This research establishes a new, rigorous standard for evaluating multimodal LLMs by prioritizing ecological validity over synthetic convenience. By highlighting the performance gap between current models and the demands of real-world narrative reasoning, the paper exposes fundamental limitations in how existing architectures handle multi-view context and ambiguity. This contribution serves as a crucial wake-up call for the field, shifting the focus toward developing systems capable of deep, context-aware reasoning.

---

## Key Findings

*   **Performance Gap:** State-of-the-art multimodal LLMs show a significant performance gap on the new dataset, highlighting the difficulty of the task.
*   **Baseline Benchmark:** GPT-4.1 achieved an accuracy of **69.3%** on the benchmark, establishing a baseline but underscoring limitations in handling authentic chart-based reasoning.
*   **Ecological Validity:** Unlike prior benchmarks, this dataset captures ecologically valid reasoning workflows by utilizing real-world data rather than synthetic examples.
*   **Complex Challenges:** The inclusion of 'multi-view' charts and questions grounded in narratives introduces unique challenges that current models struggle to resolve fully.

---

## Methodology

The researchers constructed a novel dataset by extracting content directly from visualization notebooks, ensuring the inclusion of real-world, multi-view charts rather than isolated or synthetic figures. These charts were paired with natural language questions grounded in analytical narratives to reflect actual usage contexts. To validate the dataset's difficulty, the authors benchmarked state-of-the-art multimodal LLMs, specifically measuring the accuracy of models like GPT-4.1 against these complex scenarios.

---

## Research Contributions

*   **A New CQA Dataset:** Introduction of a unique Chart Question Answering dataset derived from visualization notebooks, featuring real-world, multi-view charts.
*   **Contextual Grounding:** Provision of questions that are grounded in analytical narratives, moving beyond simple fact extraction to complex reasoning workflows.
*   **Benchmark for Ecological Validity:** Establishment of a benchmark that prioritizes ecological validity, providing a more authentic assessment metric for AI models in data analysis scenarios.

---

## Technical Details

**Data Source & Context**
*   **Source Material:** Literate Visualization (litvis) notebooks containing narrative analysis, code, and visualizations.
*   **Origin:** Derived from student coursework to ensure ecological validity.
*   **Text Segmenting:** Narrative text was segmented into chunks of at most 200 words.

**Visualization Processing**
*   **Rendering:** Visualizations rendered in HTML using a headless browser.
*   **State Capture:** Discrete interactive controls were systematically enumerated to capture different states (multi-view capability).

**Question Generation**
*   **Taxonomy:** Questions generated based on the VLAT taxonomy (8 analytical tasks: Retrieve Value, Find Extremum, Find Correlations, etc.).
*   **Generation Process:** An LLM extracts narrative quotes to generate QA pairs and three plausible distractors.
*   **Options:** Included a fifth option: "Cannot be determined from the visualization(s)."

**Validation & Filtering**
*   **Initial Yield:** 429 QA pairs.
*   **Filtering:** Expert validation filtered based on visualization alignment and analytical scope.
*   **Retention:** Questions with missing context were retained as 'unanswerable.'

---

## Benchmarking Results

**Dataset Composition**
*   **Total Pairs:** 205 high-quality QA pairs.
*   **Visuals:** 103 visualization images.
*   **Complexity:** 36.6% of questions require multiple visualization images or views.
*   **Ambiguity:** 16.1% of questions are classified as unanswerable.

**Model Performance (Overall Accuracy)**
*   **GPT-4.1:** 69.27%
*   **Qwen2.5-VL-32B:** 56.59%
*   **Qwen2.5-VL-7B:** 31.71%

**Task-Specific Accuracy (GPT-4.1)**
*   **Retrieve Value:** 76.47%
*   **Find Correlations:** 72.73%
*   **Find Extremum:** 69.09%
*   **Make Comparisons:** 50.00%
*   **Find Anomalies:** 44.44%

The results indicate that while models handle simple retrieval well, they face significant challenges with authentic, multi-view, and narrative-driven chart reasoning.

---
*References: 17 citations*