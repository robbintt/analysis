# RLAF: Reinforcement Learning from Automaton Feedback
*Mahyar Alinejad; Alvaro Velasquez; Yue Wang; George Atia*

---

### ðŸ“Š Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Quality Score** | 7/10 |
| **Total Citations** | 40 |
| **Core Focus** | Non-Markovian RL, Preference Learning, Automata |
| **Optimization Modes** | Static & Dynamic |
| **Key Advantage** | Eliminates manual reward engineering |

---

## Executive Summary

> **The Problem:** Reinforcement Learning (RL) agents often struggle in Non-Markovian Reward Decision Processes (NMRDPs), where optimal actions depend on extended histories rather than the immediate state. Traditional approaches using Deterministic Finite Automata (DFA) rely on rigid, manually engineered reward functionsâ€”a process that is labor-intensive, brittle, and prone to stagnation in long-horizon scenarios.
>
> **The Solution:** The authors propose **RLAF (Reinforcement Learning from Automaton Feedback)**, a framework that shifts automata usage from direct reward specification to a source of preference-based feedback. Operating within a Product MDP ($M_{prod}$), the agent learns a latent reward function by minimizing a **Pairwise Ranking Loss**: $L(\theta) = \sum \max(0, m - (\hat{R}_{\theta}(\tau_p) - \hat{R}_{\theta}(\tau_n)))$. The framework utilizes two elicitation strategies: **Subtask-based Scoring** and **Automaton Transition Value-based Scoring**. It supports both a Static Approach (one-time learning) and a Dynamic Approach (iterative refinement).
>
> **The Outcome:** Validated across discrete and continuous environments, RLAF significantly outperforms traditional reward engineering and state-of-the-art baselines (including QRM and LTL-guided methods like SPECTRL). The Dynamic RLAF variant demonstrated superior convergence rates and sample efficiency in tasks requiring complex temporal dependencies. Backed by rigorous theoretical proofs, RLAF guarantees convergence to a **near-optimal solution**, offering a scalable, human-independent alternative for safety-critical domains such as robotics and autonomous systems.

---

## Key Findings

*   **Superior Performance:** The method outperforms traditional reward engineering and existing automaton-based baselines (specifically reward machines and LTL-guided methods) in tasks requiring temporal dependencies.
*   **Non-Markovian Handling:** Successfully handles non-Markovian reward structures by leveraging automaton-based preferences, functioning effectively in both discrete and continuous environments.
*   **Theoretical Guarantees:** The framework provides a theoretical convergence guarantee, demonstrating that the learned policy is near-optimal with respect to the true non-Markovian objective under standard assumptions.
*   **Scalability:** Utilizing automaton-based preferences offers a scalable, efficient, and human-independent alternative to manual reward modeling.

---

## Methodology

The core methodology replaces explicit reward functions with preferences derived from a **Deterministic Finite Automaton (DFA)**. Unlike conventional methods that use automata for direct reward specification, RLAF uses the DFA structure to generate preferences over trajectories, which are then used to learn a latent reward function.

The framework operates in two distinct modes:

1.  **Static Approach:** The learned reward function is used directly for policy optimization.
2.  **Dynamic Approach:** The reward function and policy undergo continuous, iterative refining until convergence is achieved.

---

## Technical Details

### System Architecture
The approach addresses **Non-Markovian Reward Decision Processes (NMRDPs)** by integrating a DFA for task specification. The agent operates within a **Product MDP ($M_{prod}$)** that combines environment states and DFA states. Feedback is provided via automaton-based preferences rather than numeric rewards.

### Preference Elicitation Strategies
Two primary strategies are employed to score trajectories ($\tau$):

1.  **Subtask-based Scoring:**
    $$score(\tau) = w_s \cdot N_s(\tau) - w_d \cdot d(\tau)$$
    *Where $N_s(\tau)$ counts subtasks and $d(\tau)$ represents distance or penalty terms.*

2.  **Automaton Transition Value-based Scoring:**
    Sums desirability estimates $Q_{dfa}(q, \sigma)$ across the trajectory.

### Optimization Function
A parameterized reward function $\hat{r}_{\theta}((s, q), a)$ is learned using a **Pairwise Ranking Loss**:

$$L(\theta) = \sum \max(0, m - (\hat{R}_{\theta}(\tau_p) - \hat{R}_{\theta}(\tau_n)))$$

---

## Main Contributions

*   **Novel Preference-Based Framework:** Introduces a paradigm that uses automaton-based feedback (DFA) to generate trajectory preferences for reward learning, rather than using the automaton for direct reward specification.
*   **Elimination of Manual Engineering:** Provides a solution that removes the need for manual reward engineering by automatically inferring reward functions from the structural properties of a DFA.
*   **Dual-Mode Operation:** Develops a flexible framework supporting both static and dynamic optimization strategies to improve policy learning.
*   **Theoretical and Empirical Validation:** Provides rigorous convergence guarantees and experimental evidence demonstrating superiority in handling complex, history-dependent rewards compared to state-of-the-art baselines.

---

## Results

*   **Benchmark Victory:** The method outperforms baselines including Traditional Reward Engineering, Reward Machines (QRM), and LTL-guided methods (e.g., SPECTRL, LPOPL, TLTL), particularly in tasks requiring temporal dependencies and long-horizon sequences.
*   **Environment Validation:** Successfully validated across both discrete and continuous environments.
*   **Efficiency:** The framework provides theoretical guarantees for convergence and ensures the learned policy is near-optimal under standard assumptions.
*   **Qualitative Assessment:** The approach is efficient, scalable, and independent of human-in-the-loop feedback or manual reward tuning. The Dynamic variant specifically showed robust performance in complex temporal tasks.