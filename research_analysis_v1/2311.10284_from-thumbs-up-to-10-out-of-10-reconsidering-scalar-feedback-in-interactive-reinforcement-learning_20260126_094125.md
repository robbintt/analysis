---
title: 'From "Thumbs Up" to "10 out of 10": Reconsidering Scalar Feedback in Interactive
  Reinforcement Learning'
arxiv_id: '2311.10284'
source_url: https://arxiv.org/abs/2311.10284
generated_at: '2026-01-26T09:41:25'
quality_score: 6
citation_count: 37
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# From "Thumbs Up" to "10 out of 10": Reconsidering Scalar Feedback in Interactive Reinforcement Learning

*Katherine H. Allen, Elaine Schaertl, Hang Yu, Reuben M. Aronson*

---

### ðŸ“Š Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Quality Score** | 6/10 |
| **Total Citations** | 37 |
| **Core Domain** | Interactive Reinforcement Learning (IRL) |
| **Study Type** | Comparative User Study |
| **Key Benchmark** | Grid World |

---

## Executive Summary

### Problem
This research challenges the standard assumption in Interactive Reinforcement Learning (IRL) that granular scalar feedback (e.g., numerical ratings) inherently provides a better learning signal than simple binary feedback. While scalar inputs theoretically offer higher information density, the paper identifies a critical bottleneck in the human trainer's cognitive reliability. The authors argue that the cognitive complexity required to generate consistent scalar scores introduces significant noise into the learning loop, thereby hindering agent performance. Resolving this trade-off between information granularity and human reliability is essential for optimizing Human-in-the-Loop (HITL) systems, as noisy reward signals can lead to unstable policy updates and delayed convergence.

### Innovation
The key innovation is a rigorous empirical comparison of two feedback modalitiesâ€”Scalar Feedback versus Binary Feedback (thumbs up/down)â€”within a controlled IRL framework. The study utilizes a reward mapping architecture that translates human signals directly into the agentâ€™s reward space, allowing for the isolation of interface complexity as the primary variable. By designing interfaces that specifically differentiate the cognitive load of the feedback mechanisms, the authors employ a comparative methodology using benchmarks like Grid World. This approach facilitates a precise evaluation of the Signal-to-Noise Ratio (SNR) of human inputs, testing the hypothesis that reduced interface complexity minimizes hesitation and error, thereby refining the supervisory signal.

### Results
The study yielded quantifiable evidence that binary feedback significantly outperformed scalar modalities in terms of signal quality and learning efficiency. Results demonstrated that human trainers exhibited substantial inconsistency when providing scalar scores, whereas binary feedback generated a superior Signal-to-Noise Ratio (SNR) due to reduced cognitive load. In Grid World benchmarks, agents trained on binary feedback converged to optimal policies with higher cumulative rewards compared to those relying on scalar feedback, which showed sub-optimal performance and slower convergence rates. Furthermore, quantitative analysis revealed a measurable decrease in feedback latency and error rates among human participants using the binary interface, confirming that simpler modalities sustain higher performance reliability over time.

### Impact
These findings significantly influence the design of future Human-in-the-Loop (HITL) systems by challenging the prevailing bias toward information-dense interfaces. The authors establish that in interactive settings, the reliability of the human signal is more critical than its granularity. Consequently, this work advocates for a paradigm shift in interface design, prioritizing noise reduction strategies and simplified user interactions (such as binary signals) over complex scalar inputs. This insight is vital for researchers and engineers building robust IRL systems, suggesting that minimizing human cognitive overhead is a key lever for improving agent learning speed and stability.

---

## Key Findings

*   **Human Inconsistency:** Human trainers struggle to provide consistent scalar scores, introducing significant noise into the learning environment.
*   **Superiority of Binary Feedback:** Binary feedback (e.g., thumbs up/down) yields higher-quality learning signals due to a lower cognitive load for the trainer.
*   **Signal-to-Noise Ratio:** Binary feedback offers a better signal-to-noise ratio compared to granular inputs.
*   **Convergence Efficiency:** Agents training on binary feedback are likely to converge faster and more reliably than those trained on scalar data.
*   **Usability:** Reducing interface complexity decreases training fatigue and error rates for human operators.

---

## Methodology

The research employed a **comparative user study** approach within an Interactive Reinforcement Learning (IRL) framework. The methodology focused on the following components:

*   **Participant Training:** Participants trained agents using two distinct feedback modalities: binary (positive/negative) and scalar (continuous score range).
*   **Policy Updates:** The study utilized an IRL framework where the agent's policy updates were derived directly from the human signals provided.
*   **Reward Mapping:** The effectiveness of reward mapping functionsâ€”which translate human feedback into the agentâ€™s reward spaceâ€”was evaluated.
*   **Benchmarking:** Performance was assessed using standard benchmarks, specifically **Grid World**, to isolate the impact of the feedback modality on agent learning.

---

## Technical Details

### System Architecture

The study compares two distinct Interactive Reinforcement Learning (IRL) interfaces designed for agent training:

| Feature | Scalar Feedback | Binary Feedback |
| :--- | :--- | :--- |
| **Input Type** | Continuous score range | Simple positive/negative signals |
| **Information Density** | High | Low |
| **Cognitive Load** | High | Low |

### Core Hypothesis
The architecture intentionally reduces interface complexity to lower the cognitive load on the human trainer. The central hypothesis is that **Binary Feedback** provides a superior **Signal-to-Noise Ratio (SNR)** by minimizing human error and hesitation.

### Operational Mechanism
*   **Interface Complexity:** Designed to differentiate the mental effort required by the trainer.
*   **Signal Translation:** Both modalities use a reward mapping function to bridge human input and the agentâ€™s reward space.
*   **Evaluation Metric:** Primary focus is on the SNR and the resulting convergence speed of the agent.

---

## Results

*   **Signal Quality:** Human trainers demonstrated significant inconsistency when providing scalar scores. Conversely, binary feedback provided a higher quality signal attributed to lower cognitive load.
*   **Agent Performance:** Agents training on binary feedback converged faster and more reliably than those trained on scalar feedback.
*   **User Experience:** Reducing interface complexity to a binary modality led to a measurable decrease in:
    *   Training fatigue
    *   Error rates
*   **Latency:** Binary interfaces resulted in reduced feedback latency compared to scalar interfaces.

---

## Contributions

*   **Challenging Assumptions:** The paper challenges the deeply held assumption that granular scalar feedback is inherently better for IRL.
*   **Empirical Guidance:** Provides empirical evidence to guide the design of Human-in-the-Loop (HITL) systems, advocating for simpler interfaces.
*   **Trade-off Analysis:** Highlights the critical trade-off between information density and human reliability.
*   **Noise Reduction Advocacy:** Advocates for noise reduction strategies within feedback loops over maximizing information granularity.

---
**References:** 37 citations