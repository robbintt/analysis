---
title: 'VisualSphinx: Large-Scale Synthetic Vision Logic Puzzles for RL'
arxiv_id: '2505.23977'
source_url: https://arxiv.org/abs/2505.23977
generated_at: '2026-02-04T16:03:05'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# VisualSphinx: Large-Scale Synthetic Vision Logic Puzzles for RL

*Yichen Feng; Zhangchen Xu; Fengqing Jiang; Yuetai Li; Bhaskar Ramasubramanian; Luyao Niu; Bill Yuchen Lin; Radha Poovendran*

***

> ### ðŸ“Š Quick Facts & Metrics
>
> *   **Dataset Size:** 660,000+ synthetic puzzles
> *   **Base Model:** QWEN 2.5-VL-7B
> *   **Training Algorithm:** Group Relative Policy Optimization (GRPO)
> *   **Generation Cost:** <$1,000
> *   **Quality Scores:**
>     *   **Readability (4+):** 93.1%
>     *   **Logical Coherence (4+):** 89.8%
> *   **Unsolved Puzzles:** 12.7% (indicating high difficulty)
> *   **Paper Quality Score:** 8/10

***

## Executive Summary

Current Vision-Language Models (VLMs) struggle to perform complex logical reasoning, particularly when visual and textual information must be synthesized to solve a problem. This limitation stems largely from a scarcity of large-scale, well-structured training datasets that offer grounded answers rather than mere correlational patterns. The paper addresses this critical gap in the research landscape, establishing that the lack of high-quality, logically coherent multimodal data is a primary bottleneck preventing VLMs from advancing beyond perception to true reasoning.

The authors introduce **VisualSphinx**, the first large-scale synthetic dataset specifically designed for visual logical reasoning, generated via a novel four-stage Rule-to-Image Synthesis Pipeline. The process begins with Seed Collection & Rule Abstraction (extracting structured rules from 4k seeds via LLMs), followed by Rule Expansion using a Genetic Algorithm (scaling to 40k rules). The core technical innovation is the Program-Based Rule-to-Image Synthesis stage, which executes Python scripts to generate exactly five compliant images and three distractors per rule, ensuring the visual content is strictly grounded in the logic. The resulting dataset is then used to train the QWEN 2.5-VL-7B model using Group Relative Policy Optimization (GRPO), a reinforcement learning approach distinct from standard supervised fine-tuning.

The VisualSphinx project produced over 660,000 synthetic puzzles at a generation cost of less than $1,000, demonstrating high efficiency and scalability. Quality control metrics indicate a robust dataset, with 93.1% of puzzles achieving a readability score of 4+ and 89.8% achieving a logical coherence score of 4+. While the model found the data challengingâ€”failing to solve 12.7% of the filtered puzzlesâ€”the fine-tuned model showed marked improvement on visual logic tasks. Furthermore, the model exhibited positive transfer learning on the MathVista benchmark, achieving significant gains specifically in algebraic, arithmetic, and geometric reasoning.

This research provides compelling empirical evidence that large-scale synthetic data, when generated with strict logical grounding, can effectively enhance reasoning capabilities in VLMs. The most significant finding is the "positive transfer" effect: training on visual logic puzzles not only improves visual reasoning but also boosts performance on non-visual mathematical domains. By releasing a scalable pipeline for creating grounded visual reasoning data, this work establishes a new paradigm for dataset generation and offers a practical solution for researchers aiming to bridge the gap between visual perception and abstract logical deduction.

***

## Key Findings

*   **Enhanced VLM Performance:** VLMs trained on the VisualSphinx dataset using Group Relative Policy Optimization (GRPO) exhibit improved performance on logical reasoning tasks compared to baseline models.
*   **Cross-Domain Transfer:** The reasoning capabilities enhanced by VisualSphinx transfer positively to other domains, specifically benefiting algebraic, arithmetic, and geometry reasoning tasks (e.g., MathVista benchmarks).
*   **Critical Data Attributes:** The dataset provides a high degree of logical coherence and readability, which are identified as critical factors in the improved performance of the models.
*   **Research Gap Resolution:** The study identifies and addresses a significant gap in current research: the lack of large-scale, well-structured training datasets for effective multimodal reasoning in VLMs.

***

## Methodology

The research utilizes a systematic approach to generate and utilize training data:

1.  **Rule-to-Image Synthesis Pipeline:** Proposed a pipeline designed to tackle the challenge of image synthesis with grounded answers.
2.  **Rule Extraction:** The process begins by extracting and expanding puzzle rules derived from seed questions.
3.  **Code Generation & Assembly:** The pipeline generates code for "grounding synthesis image synthesis", which is then used to assemble puzzle samples.
4.  **Reinforcement Learning Training:** VLMs are trained using the GRPO (Group Relative Policy Optimization) algorithm on the generated synthetic data.

***

## Technical Details

The paper presents a rigorous four-stage synthetic dataset generation pipeline:

*   **Stage 1: Seed Collection & Rule Abstraction**
    *   Utilized LLMs to convert 4k seed questions into structured rules.
*   **Stage 2: Rule Expansion via Genetic Algorithm**
    *   Scaled the initial rules to 40k distinct rules using a genetic algorithm approach.
*   **Stage 3: Program-Based Rule-to-Image Synthesis**
    *   Generated Python scripts to create visual assets.
    *   **Output:** 5 compliant images and 3 distractor images generated per rule.
*   **Stage 4: Puzzle Assembly**
    *   Compiled final puzzles featuring 5 correct images and 3 distractors.

**Training & Quality Control:**
*   **Model:** QWEN 2.5-VL-7B base model.
*   **Algorithm:** Group Relative Policy Optimization (GRPO).
*   **Cost Efficiency:** Total generation cost was less than $1,000.
*   **Quality Assurance:** Implemented VLM-based filtering and LLM-based annotation to ensure high standards.

***

## Results

*   **Dataset Scale:** The VisualSphinx dataset comprises over **660,000 synthetic puzzles**.
*   **Quality Metrics:**
    *   **93.1%** of puzzles achieved a readability score of 4+.
    *   **89.8%** of puzzles achieved a logical coherence score of 4+.
*   **Model Challenge:** Analysis of 110,000 filtered image groups showed that 14,000 puzzles (**12.7%**) were unsolved by the model, indicating the dataset provides a sufficient challenge.
*   **Performance:** The fine-tuned model demonstrated improved accuracy on visual logic tasks and positive transfer learning on MathVista benchmarks, particularly in algebraic, arithmetic, and geometric reasoning.

***

## Contributions

*   **VisualSphinx Dataset:** Introduction of VisualSphinx, a first-of-its-kind, large-scale synthetic visual logical reasoning training dataset.
*   **Synthesis Pipeline:** Development of a technical solution (rule-to-image synthesis pipeline) for generating grounded visual puzzles from text-based rules.
*   **Empirical Evidence:** Provided empirical evidence demonstrating that large-scale synthetic data, when generated with logical grounding, can effectively enhance both visual and non-visual (mathematical) reasoning capabilities in VLMs.

***

*References: 40 citations*