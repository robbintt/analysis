---
title: 'Systematic Characterization of LLM Quantization: A Performance, Energy, and
  Quality Perspective'
arxiv_id: '2508.16712'
source_url: https://arxiv.org/abs/2508.16712
generated_at: '2026-02-03T19:38:10'
quality_score: 7
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Systematic Characterization of LLM Quantization: A Performance, Energy, and Quality Perspective

*Authors: Tianyao Shi; Yi Ding*

---

> ### ðŸ“Š Quick Facts
> *   **Tool:** qMeter (Automated Characterization Framework)
> *   **Models Evaluated:** Llama-2 (7B, 13B, 30B, 70B)
> *   **Hardware:** NVIDIA A100 & H100
> *   **Methods:** 11 Post-Training Quantization (PTQ) methods
> *   **Key Metric:** Performance-Energy-Quality Triad
> *   **Quality Score:** 7/10

---

## Executive Summary

Deploying Large Language Models (LLMs) in production involves navigating complex tradeoffs between inference performance, energy consumption, and output quality. As quantization becomes the standard for optimizing these models, operators currently lack a systematic understanding of how specific quantization methods interact with varying workload characteristics, parallelism strategies, and hardware architectures. This gap creates significant hurdles in capacity planning, energy-efficient scheduling, and multi-objective tuning, as existing optimizations often fail to account for the non-uniform behaviors of quantized models under realistic serving conditions.

To address this, the authors introduce **qMeter**, a fully automated online characterization framework designed to rigorously profile LLM quantization. Technically, qMeter utilizes a continuous Profiling Loop that employs ConfigSynthesis and Saturation Detectionâ€”using a binary search QPS Range Searchâ€”to identify Service Level Objective (SLO) limits. Using this tool, the study conducted a comprehensive evaluation of 11 Post-Training Quantization (PTQ) methods across 4 Llama-2 model sizes (ranging from 7B to 70B parameters) on NVIDIA A100 and H100 GPUs. The methods were categorized into Weight-Only (e.g., AWQ), Activation Quantization (e.g., SmoothQuant), and KV Cache Compression, allowing for a granular analysis of their interactions with system resources.

The results reveal that **no single quantization method dominates the Performance-Energy-Quality triad**; tradeoffs are highly task-dependent, though the study found that larger quantized models (up to 70B parameters) can sometimes outperform smaller full-precision models. Workload analysis demonstrates that latency metrics are sensitive to specific request characteristics: Time to First Token (TTFT) is primarily driven by input length, while Time Per Output Token (TPOT) dictates generation efficiency. Consequently, workloads with short outputs are disproportionately impacted by TTFT overhead. System-level findings indicate that Activation Quantization scales effectively with Tensor Parallelism (TP), whereas combining Weight-Only quantization with KV Cache Compression results in compounded latency and energy overhead. Hardware comparisons showed that while H100s offer superior latency and scalability, A100s provide higher energy efficiency at moderate loads.

This research significantly advances the field by providing the first comprehensive, multi-dimensional study of LLM quantization under realistic online serving conditions. By open-sourcing the qMeter framework, the authors provide the community with a standardized tool for rigorous profiling that extends beyond simple accuracy metrics to include system-level performance and energy dynamics. The findings highlight the necessity of co-optimizing quantization methods with parallelism strategies and hardware selection, ultimately informing practitioners on how to navigate complex deployment tradeoffs for more sustainable and cost-effective systems.

---

## Key Findings

*   **Non-Uniform Tradeoffs:** The tradeoffs between performance, energy, and quality are not uniform; they are highly dependent on the specific task and the quantization method employed.
*   **Workload Sensitivity:** The effectiveness of quantization exhibits strong sensitivity to workload characteristics.
*   **System-Level Interactions:** There are complex interactions between quantization methods and system-level factors, specifically parallelism strategies and GPU architectures.
*   **Deployment Hurdles:** Practical deployment faces significant challenges regarding capacity planning, energy-efficient scheduling, and multi-objective tuning.

---

## Methodology

The researchers developed **qMeter**, a fully automated online characterization framework. To ensure a robust analysis, they conducted an in-depth evaluation covering the following scope:

*   **Methods:** 11 post-training quantization (PTQ) methods.
*   **Model Sizes:** 4 distinct sizes ranging from 7B to 70B parameters.
*   **Hardware:** Two GPU architectures (NVIDIA A100 and H100).
*   **Conditions:** Evaluation spanned application, workload, parallelism, and hardware levels under realistic online serving conditions.

---

## Technical Details

### Framework Architecture (qMeter)
The study introduces qMeter, an automated profiling framework composed of the following components:
*   **Profile Coordinator**
*   **Engine Handler**
*   **Benchmarker**
*   **Database**
*   **GPUMonitor**

### Workflow
*   **ConfigSynthesis:** Automated generation of test configurations.
*   **Saturation Detection:** Utilizes QPS Range Search via binary search to identify SLO (Service Level Objective) limits.
*   **Profiling Loop:** A continuous process for gathering metrics.

### Evaluation Categories
The authors evaluated Post-Training Quantization (PTQ) methods using TensorRT-LLM v0.19.0, categorized into:
1.  **Weight-Only:** Per-Channel INT8, AWQ.
2.  **Activation Quantization:** SmoothQuant, Per-Tensor FP8, W4A8-AWQ.
3.  **KV Cache Compression:** QServe.

### Experimental Testbed
*   **Models:** Llama-2 (7B, 13B, 30B, 70B).
*   **Hardware:** NVIDIA H100 and A100 GPUs.
*   **Workloads:** Chatbot, code generation, and summarization.
*   **Analysis Levels:** Application, Workload, Parallelism, and Hardware.

---

## Results

*   **Performance-Energy-Quality Triad:** No single quantization method dominates this triad. Tradeoffs are non-uniform and task-dependent. Notably, larger quantized models can sometimes outperform smaller full-precision models.
*   **Workload Dynamics:**
    *   Short outputs negatively impact **Time to First Token (TTFT)**.
    *   Long inputs increase **Time Per Output Token (TPOT)**.
    *   Optimal configurations shift dynamically with Queries Per Second (QPS).
*   **System-Level Findings:**
    *   Activation quantization scales well with Tensor Parallelism (TP).
    *   Combining Weight-Only quantization with KV Compression results in compounded latency and energy overhead.
*   **Hardware Comparison:**
    *   **H100s:** Offer better latency and scalability.
    *   **A100s:** Demonstrate higher energy efficiency at moderate loads.
*   **Optimization Scenarios:** Results highlight the necessity of co-optimizing quantization and parallelism, revealing an energy-quality imbalance in single-objective tuning.

---

## Contributions

*   Introduction of **qMeter**, a fully automated tool for the online characterization of LLM quantization.
*   Provision of a comprehensive study that jointly analyzes performance, energy, and quality across multiple levels.
*   Detailed characterization of 11 quantization methods across various model sizes and modern GPU architectures.
*   Presentation of three optimization case studies addressing real-world deployment challenges in capacity planning, energy scheduling, and multi-objective tuning.

---

**References:** 40 citations