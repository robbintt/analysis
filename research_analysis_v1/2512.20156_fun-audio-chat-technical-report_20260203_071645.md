---
title: Fun-Audio-Chat Technical Report
arxiv_id: '2512.20156'
source_url: https://arxiv.org/abs/2512.20156
generated_at: '2026-02-03T07:16:45'
quality_score: 8
citation_count: 23
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Fun-Audio-Chat Technical Report

*Authors: Tongyi Fun Team; Qian Chen; Luyao Cheng; Chong Deng; Xiangang Li; Jiaqing Liu; Chao-Hong Tan; Wen Wang; Junhao Xu; Jieping Ye; Qinglin Zhang; Qiquan Zhang; Jingren Zhou*

---

### üìä Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Model Variants** | Fun-Audio-Chat-8B (Dense), MoE 30B-A3B |
| **Architecture** | Dual-Resolution Speech Representations (DRSR) |
| **Efficiency** | ~50% reduction in GPU training hours |
| **Processing Rates** | 5Hz (Input) / 25Hz (Output) |
| **Key Benchmarks** | Top rankings in Spoken QA (LlamaQ, TriviaQ, WebQ) |
| **Quality Score** | 8/10 |

---

## üìã Executive Summary

**Problem**
Developing Large Audio Language Models (LALMs) capable of high-fidelity speech understanding and generation has been hindered by the temporal resolution mismatch between text and audio modalities. Speech operates at approximately 25Hz, compared to text‚Äôs ~3Hz, forcing standard models to process audio at the frame level, which imposes prohibitive computational costs on the backbone LLM. Additionally, adapting powerful pre-trained text LLMs to handle audio tasks often results in "catastrophic forgetting," where the model loses its original reasoning capabilities and text knowledge during the fine-tuning process.

**Innovation**
The authors introduce Fun-Audio-Chat, a framework featuring Dual-Resolution Speech Representations (DRSR) and a "Core-Cocktail" training strategy. DRSR decouples semantic processing from high-fidelity generation: the Shared LLM processes compressed speech tokens at 5Hz for efficiency, while a dedicated "Speech Refined Head" (SRH) regenerates high-quality tokens at 25Hz. To prevent catastrophic forgetting, Core-Cocktail Training employs a two-stage post-training process using intermediate merging and synthesized CosyVoice 3 data.

**Results**
The proposed architecture delivers substantial efficiency improvements and competitive performance across a wide range of benchmarks. By decoupling processing resolutions, the DRSR approach reduced GPU training hours by approximately 50%. In evaluations, the models secured top rankings on Spoken Question Answering tasks and achieved competitive results in Speech-to-Text, Audio Understanding, and Speech Function Calling.

**Impact**
This research challenges the industry assumption that state-of-the-art audio performance requires resource-intensive, large-scale audio-text pre-training from scratch. By demonstrating that robust audio capabilities can be achieved through efficient post-training of existing LLMs, the authors significantly lower the barrier to entry for developing advanced spoken dialogue systems.

---

## üîë Key Findings

*   **Computational Efficiency:** The Dual-Resolution Speech Representations (DRSR) approach successfully balances computational efficiency with audio quality, processing audio at 5Hz and generating tokens at 25Hz, which reduces GPU costs by approximately 50%.
*   **Catastrophic Forgetting Mitigation:** Core-Cocktail Training effectively enables the model to retain original text LLM knowledge while acquiring robust audio skills.
*   **Benchmark Performance:** Fun-Audio-Chat models (8B and MoE 30B-A3B) achieved top rankings on Spoken QA benchmarks and competitive performance on Speech-to-Text, Speech-to-Speech, and Audio Understanding tasks without large-scale pre-training.
*   **Enhanced Interactions:** Multi-Task DPO Training enhanced metrics like instruction following and voice empathy, with the Duplex variant showing strong performance in real-time interactions.

---

## üß© Methodology

The researchers developed **Fun-Audio-Chat**, a Large Audio Language Model (LALM), leveraging pre-existing models rather than training from scratch.

*   **Architecture:** The system features Dual-Resolution Speech Representations (DRSR), where a Shared LLM processes speech tokens at 5Hz and a dedicated 'Speech Refined Head' regenerates high-quality tokens at 25Hz.
*   **Training Strategy:**
    *   **Core-Cocktail Training:** A two-stage fine-tuning process using intermediate merging to align audio capabilities with text knowledge and prevent catastrophic forgetting.
    *   **Multi-Task DPO Training:** Applied to enhance robustness and instruction following capabilities.

---

## ‚öôÔ∏è Technical Details

### Architecture Components
*   **Framework:** Parallel Large Audio Language Model (LALM).
*   **Pipeline:** Whisper-Large-v3 encoder $\rightarrow$ Adapter $\rightarrow$ S3Tokenizer $\rightarrow$ Detokenizer (Flow Matching + HiFi-GAN vocoder).
*   **Dual-Resolution Mechanism:**
    *   **Shared LLM:** Processes input at 5Hz by grouping 5 speech tokens.
    *   **Speech Refined Head (SRH):** Generates output at 25Hz to recover fine-grained details.
*   **Generation:** Multimodal LLM performs parallel joint generation of speech and text by fusing embeddings, using a silence token for padding.

### Training Pipeline
1.  **Stage 1: Pre-alignment** (Freezing the Shared LLM).
2.  **Stage 2: Core-Cocktail Training** (Fine-tuning on CosyVoice 3 synthesized data).
3.  **Stage 3: Multi-Task DPO Training.**

### Model Variants
*   8B Parameter Dense Model.
*   30B-A3B Mixture-of-Experts Model.

---

## üìà Results & Contributions

### Performance Outcomes
*   **Efficiency:** The DRSR architecture achieved an approximately 50% reduction in training GPU hours compared to models processing at standard audio frame rates.
*   **Speech-to-Speech Spoken QA:** Top rankings in LlamaQ, TriviaQ, and WebQ.
*   **Speech-to-Text SQA:** Competitive performance in ReasoningQA, CommonEval, SD-QA, MMSU, and IFEval.
*   **Audio Understanding:** Competitive results in MMAU, MMAU-Pro, and MMSU.
*   **Function Calling:** Strong results in Speech-ACEBench, Speech-BFCL, and Speech-SmartInteract.

### Core Contributions
*   **Resolution Decoupling:** Solved the temporal resolution mismatch problem by separating semantic processing from high-fidelity token generation.
*   **Core-Cocktail Method:** Proposed a solution to catastrophic forgetting in multi-modal models.
*   **Post-Training Efficiency:** Demonstrated that state-of-the-art audio performance can be achieved without resource-intensive large-scale audio-text pre-training.
*   **Open Source:** Released the Fun-Audio-Chat-8B model, code, and an interactive demo.

---
*Based on 23 citations.*