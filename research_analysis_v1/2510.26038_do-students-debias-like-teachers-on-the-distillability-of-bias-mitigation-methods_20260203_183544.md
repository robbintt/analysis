---
title: Do Students Debias Like Teachers? On the Distillability of Bias Mitigation
  Methods
arxiv_id: '2510.26038'
source_url: https://arxiv.org/abs/2510.26038
generated_at: '2026-02-03T18:35:44'
quality_score: 7
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Do Students Debias Like Teachers? On the Distillability of Bias Mitigation Methods

*Jiali Cheng; Chirag Agarwal; Hadi Amiri*

---

> ### ðŸ“Š Quick Facts
>
> *   **Quality Score:** 7/10
> *   **References:** 40 Citations
> *   **Core Domains:** Natural Language Inference (NLI), Image Classification
> *   **Architectures Analyzed:** BERT, T5, ResNet, Vision Transformer
> *   **Primary Issue:** Erosion of robustness and debiasing capabilities during Knowledge Distillation.

---

## Executive Summary

Knowledge Distillation (KD) is the industry standard for compressing large, accurate teacher models into efficient student models, but this paper exposes a critical vulnerability: the systematic erosion of model robustness and debiasing capabilities. The authors investigate "debiasing distillability," specifically questioning whether a teacher's ability to mitigate spurious correlationsâ€”biases in training data that do not hold in the real worldâ€”survive the compression process. This issue is urgent for organizations deploying small, distilled models; if the distillation process strips away the teacher's ability to handle out-of-distribution (OOD) data, the deployed system may fail catastrophically in variable environments despite achieving high accuracy on controlled, in-distribution tests.

The study introduces the first large-scale empirical framework to evaluate debiasing transferability across Natural Language Inference (NLI) and image classification tasks, utilizing architectures ranging from BERT and T5 to ResNet and Vision Transformers. The key technical innovation is the application of mechanistic interpretability to diagnose failure modes; rather than relying solely on accuracy metrics, the authors analyze internal attention patterns and circuits to pinpoint the causal mechanisms behind performance degradation. By formulating "distillability" as the retention of debiasing performance, the framework rigorously contrasts training regimes to isolate how compression alters internal reasoning. The authors demonstrate that standard KD results in "ineffective knowledge injection," where students mimic the teacher's output probabilities without replicating the underlying robust circuits, leading to an over-reliance on heuristic features.

Quantitative analysis reveals that standard KD practices significantly degrade OOD robustness. While student models often achieve superior In-Domain (ID) performance, they exhibit a marked increase in the "spurious gap"â€”the performance difference between biased and unbiased groupsâ€”indicating a heavier reliance on biased shortcuts. The study finds that models trained with KD consistently demonstrate larger spurious gaps compared to Non-KD baselines, signifying a distinct loss of robustness. Mechanistically, distillation significantly alters predicted probability distributions on OOD data, leading to overfitting where students become increasingly confident in incorrect predictions. Furthermore, the analysis of prediction agreement reveals that students fail to replicate the teacher's robust decision boundaries, causing performance to degrade unevenly across different types of biases.

This research fundamentally challenges the assumption that efficient, compressed models inherently inherit the robustness of their larger teachers, serving as a vital warning for practitioners deploying KD in safety-critical applications. Beyond diagnosing the failure mode, the paper bridges the gap between mechanistic interpretability and model compression by validating three concrete mitigation strategies: high-quality data augmentation, iterative knowledge distillation, and initializing student weights with teacher weights. These findings provide a roadmap for designing future debiasing methodologies that survive compression, ensuring that the pursuit of efficiency does not come at the cost of reliability and fairness.

---

## Key Findings

*   **Erosion of Debiasing:** Knowledge distillation (KD) generally undermines a model's capability to mitigate spurious correlations, reducing its effectiveness on out-of-distribution data.
*   **Ineffective Knowledge Injection:** Standard KD practices do not benefit the training of debiased models; injecting teacher knowledge fails to transfer debiasing capabilities effectively.
*   **Bias-Specific Variance:** While overall model robustness may appear stable after distillation, significant performance variations can occur across different types of biases.
*   **Mechanistic Identification:** The shift in behavior post-KD is causally linked to specific internal attention patterns and circuits within the model.

---

## Methodology

The study employs an extensive empirical evaluation framework designed to assess the transferability of robustness. The research approach includes:

*   **Cross-Modal Evaluation:** Testing across Natural Language Inference (NLI) and image classification tasks to ensure generalizability.
*   **Comparative Analysis:** Investigating the transferability of debiasing capabilities by directly comparing teacher and student models.
*   **Mechanistic Interpretability:** Utilizing interpretability techniques to pinpoint specific internal attention patterns and circuits responsible for changes in model behavior.
*   **Solution Validation:** Testing the efficacy of three proposed solutions:
    1.  High-quality data augmentation
    2.  Iterative knowledge distillation
    3.  Initializing student models with teacher weights

---

## Technical Details

**Formulation & Notation**
The study formulates *distillability* as the debiasing performance maintained after distilling a model. It defines the contribution of KD as the performance improvement over standard training.
*   **Notation:** Distinguishes between models trained without KD ($f$) and with KD ($g$) across teacher ($T$) and student ($S$) scales.

**Training Framework**
The process involves three distinct steps:
1.  **Baseline Training:** Establishing base performance for $f_T$ and $f_S$.
2.  **Distillation:** Generating $g_{T \to S}$.
3.  **Comparisons:**
    *   **C1:** Teacher vs. Student
    *   **C2:** Non-KD vs. KD

**Architectures & Metrics**
*   **Architectures:** BERT and T5 for NLU; ResNet and Vision Transformer for Image Classification.
*   **Evaluation Metrics:**
    *   In-Domain (ID) Performance
    *   Out-of-Distribution (OOD) Performance
    *   Spurious Gap
    *   Prediction Probability Distribution
    *   Prediction Agreement

---

## Contributions

*   **Novelty:** Provides the first large-scale study examining the effect of Knowledge Distillation on debiasing and the internal mechanisms driving these effects.
*   **Diagnostic Insight:** Identifies the specific internal components (attention patterns and circuits) that cause the degradation of debiasing capabilities during distillation.
*   **Practical Solutions:** Proposes and validates three concrete strategies to improve the "distillability" of debiasing methods: data augmentation, iterative distillation, and weight initialization.
*   **Theoretical Understanding:** Advances the general understanding of how KD functions regarding model robustness, informing the future design of better debiasing methodologies.

---

## Results

The empirical results highlight a critical disconnect between teacher accuracy and student robustness:

*   **Increased Bias in Students:** Students consistently become more biased than their teachers, demonstrating an erosion of robustness and a positive spurious gap.
*   **Scale Disparities:** Tiny students lose debiasing power rapidly, while distilling from large teachers can result in a loss of In-Domain (ID) knowledge.
*   **Amplification of Residual Biases:** Distillation tends to amplify residual biases and significantly alters predicted probability distributions on OOD data, leading to overfitting.
*   **Performance Trade-off:** Models trained with KD achieve higher ID performance but exhibit larger spurious gaps (worse robustness) compared to Non-KD models.
*   **Confidence Mismatch:** KD increases model confidence, which correlates with degenerate OOD performance; students become confidently wrong.