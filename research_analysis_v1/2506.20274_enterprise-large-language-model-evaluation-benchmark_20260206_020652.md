---
title: Enterprise Large Language Model Evaluation Benchmark
arxiv_id: '2506.20274'
source_url: https://arxiv.org/abs/2506.20274
generated_at: '2026-02-06T02:06:52'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Enterprise Large Language Model Evaluation Benchmark

*Liya Wang; David Yi; Damien Jose; John Passarelli; James Gao; Jordan Leventis; Kang Li*

---

> ### ðŸ“Š Quick Facts
>
> *   **Quality Score:** 8/10
> *   **Dataset Size:** 9,700 samples
> *   **Framework Scope:** 14 Tasks across 6 cognitive levels (Bloom's Taxonomy)
> *   **Models Evaluated:** 6 (including Llama variants, DeepSeek-R1, GPT-4o)
> *   **Top Reasoning Model:** DeepSeek-R1 (Open Source)
> *   **Top Judgment Model:** GPT-4o (Proprietary)
> *   **Lowest Hallucination Rate:** DeepSeek-R1 (75.8%)

---

## Executive Summary

Current general-purpose benchmarks, such as the Massive Multitask Language Understanding (MMLU) evaluation, are insufficient for assessing the nuanced complexities inherent in enterprise-specific tasks. Existing academic datasets often fail to model the real-world scenarios and cognitive demands required for practical business deployment. This creates a significant gap for organizations seeking to identify which Large Language Models (LLMs) are truly viable for operational use, necessitating a rigorous, enterprise-centric evaluation framework.

The authors propose a 14-task evaluation framework grounded in Bloomâ€™s Taxonomy, spanning six cognitive levels to holistically assess LLM capabilities. To overcome the challenges of noisy data and high annotation costs, they developed a scalable data curation pipeline utilizing **LLM-as-a-Labeler**, **LLM-as-a-Judge**, and **Corrective Retrieval-Augmented Generation (CRAG)**. This automated pipeline successfully curated a robust dataset of 9,700 samples covering diverse enterprise tasks, ranging from Software Engineering and Natural Language to JQL to Hallucination Detection and Sentiment Analysis.

Evaluations across six leading models revealed that open-source models, particularly **DeepSeek-R1**, often rival proprietary models in reasoning tasks. DeepSeek-R1 achieved a Software Engineering Correctness score of **0.30** compared to GPT-4o's **0.21** and a Summarization Relevance score of **0.88** versus GPT-4o's **0.87**. However, open-source models lagged in judgment-based tasks, with DeepSeek-R1 scoring **0.38** compared to GPT-4o's **0.47** in LLM-as-a-Judge scenariosâ€”a discrepancy the authors attribute to "overthinking."

This research establishes a critical blueprint for enterprises to conduct tailored evaluations, providing actionable insights for optimizing models for practical deployment. By validating a cost-effective, automated pipeline for high-quality data generation and introducing a specialized benchmark, the study equips organizations with the tools to make better decisions regarding LLM selection.

---

## Key Findings

*   **Inadequacy of General Benchmarks:** Existing general-purpose benchmarks (e.g., MMLU) are inadequate for assessing the complexities inherent in enterprise-specific tasks.
*   **Rise of Open-Source Reasoning:** Open-source models, specifically DeepSeek R1, demonstrate performance rivaling proprietary models in reasoning tasks.
*   **The "Overthinking" Gap:** Despite strong reasoning capabilities, open-source models lag behind proprietary models in judgment-based scenarios, a discrepancy likely caused by 'overthinking'.
*   **Actionable Insights:** The benchmark identifies critical performance gaps in enterprise contexts, providing actionable insights for optimizing models for practical deployment.

---

## Methodology

The research methodology outlines a structured approach to benchmark creation and data curation:

*   **Framework Design**
    *   Developed a 14-task evaluation framework.
    *   Grounded in Bloom's Taxonomy to holistically assess LLM capabilities within enterprise environments.
*   **Data Curation Pipeline**
    *   Implemented to overcome challenges related to noisy data and high annotation costs.
    *   Utilized a scalable pipeline combining three specific techniques:
        *   LLM-as-a-Labeler
        *   LLM-as-a-Judge
        *   Corrective Retrieval-Augmented Generation (CRAG)
*   **Benchmark Creation**
    *   The pipeline was used to curate a robust dataset consisting of **9,700 samples**.
*   **Evaluation**
    *   The framework was used to evaluate and compare the performance of six leading LLMs.

---

## Technical Details

**Framework Architecture**
The proposed benchmarking framework is structured as a 14-task framework grounded in Bloom's Taxonomy, spanning six cognitive levels.

**Data Curation Pipeline**
Integrates specific methodologies to ensure data quality:
*   LLM-as-a-Labeler
*   LLM-as-a-Judge
*   Corrective Retrieval-Augmented Generation (CRAG)

**Task Scope**
The benchmark covers a diverse range of enterprise functions, including:
*   Abstracting
*   Factual QA
*   Toxicity & Bias detection
*   Sentiment Analysis
*   Named Entity Recognition (NER)
*   General QA & Summarization
*   Software & Machine Learning Engineering
*   Natural Language to JQL
*   Hallucination Detection
*   Content Generation

**Evaluation Metrics**
*   **G-Eval**
*   **ToxicMetric & BiasMetric**
*   **Exact Match & Accuracy**
*   **Hallucination Percentage**
*   **Spearmanâ€™s r**

---

## Results

The evaluation tested 6 models (Llama variants, DeepSeek-R1, GPT-4o) with the following highlights:

**Reasoning vs. Judgment**
*   **Reasoning Tasks:** Open-source models like DeepSeek-R1 often rival proprietary ones.
    *   *SWE Correctness:* DeepSeek-R1 (**0.30**) vs GPT-4o (**0.21**)
    *   *Summarization Relevance:* DeepSeek-R1 (**0.88**) vs GPT-4o (**0.87**)
*   **Judgment Tasks:** Proprietary models maintain an advantage.
    *   *LLM-as-a-Judge:* GPT-4o (**0.47**) vs DeepSeek-R1 (**0.38**)

**Functional Performance**
*   **GPT-4o** led in Natural Language to JQL accuracy (**54%**) and Named Entity Recognition (**77.4%**).
*   **DeepSeek-R1** demonstrated superior reliability with the lowest hallucination rate (**75.8%**) compared to GPT-4o and Llama-4-scout (**81.7%**).

**Safety**
*   All models showed low toxicity and bias scores.

---

## Contributions

*   **Specialized Benchmark:** Provided a rigorous, enterprise-centric evaluation benchmark that addresses the limitations of current academic datasets like MMLU.
*   **Cost-Effective Annotation:** Introduced a scalable, automated pipeline combining LLM-as-a-Labeler, LLM-as-a-Judge, and CRAG to generate high-quality data cost-effectively.
*   **Enterprise Blueprint:** Established a structured framework and blueprint for enterprises to conduct tailored evaluations, facilitating better decision-making regarding practical LLM deployment and optimization.

---
**References:** 40 citations