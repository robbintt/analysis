---
title: Accelerating Language Model Workflows with Prompt Choreography
arxiv_id: '2512.23049'
source_url: https://arxiv.org/abs/2512.23049
generated_at: '2026-02-03T18:41:51'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Accelerating Language Model Workflows with Prompt Choreography
*Authors: TJ Bai; Jason Eisner*

---

## ðŸ“Š Quick Facts

| Metric | Value |
| :--- | :--- |
| **Latency Reduction** | 2.0â€“6.2Ã— (TTFT) |
| **End-to-End Speedup** | > 2.2Ã— |
| **Quality Score** | 9/10 |
| **Citations** | 40 |

---

## ðŸ“ Executive Summary

### The Problem
Current multi-agent Large Language Model (LLM) workflows suffer from significant computational redundancy and latency, as individual agents often re-process identical or overlapping context messages. This inefficiency arises because standard LLM serving architectures treat each inference request as an isolated task, requiring the full re-encoding of input tokens even when much of the context has been processed previously. As workflows become more complex and agents collaborate more frequently, this repeated "prefill" phase becomes a critical bottleneck, limiting the scalability and responsiveness of real-time applications.

### The Innovation
To address this, the authors introduce **"Prompt Choreography,"** a novel execution framework that replaces static prompting with a dynamic, global Key-Value (KV) cache architecture. By storing message-level encodings contiguously in GPU memory, the system allows subsequent LLM calls to attend to arbitrary, reordered subsets of previously cached messages rather than re-encoding them. Technically, the framework utilizes Rotary Positional Embeddings (RoPE) with precomputed rotation matrices to manage positional context across different message orderings and employs a decomposed API of `prefill` and `decode` steps with dynamic masking. Crucially, to prevent semantic driftâ€”where reusing cached representations in new contexts alters model outputsâ€”the authors propose fine-tuning the base LLM to align cached processing results with those of standard re-encoding, ensuring output fidelity.

### The Results
The implementation of Prompt Choreography demonstrates substantial performance improvements in benchmarks characterized by redundant computation. Specifically, the framework achieves a **2.0â€“6.2Ã— improvement in Time-To-First-Token (TTFT)** for individual messages. Furthermore, in end-to-end workflow evaluations, the system realized speedups **greater than 2.2Ã—**. The results also confirm the system's capability to support parallel LLM calls that attend to shared, encoded message subsets, effectively handling complex scenarios involving arbitrary message reordering, gaps, and overlaps without sacrificing accuracy.

### The Impact
This research represents a significant advancement in LLM system design, shifting the paradigm from isolated inference to a shared-memory, cache-driven workflow model. By demonstrating that fine-tuning can effectively mitigate the artifacts associated with caching, the authors provide a viable path toward reliable, high-performance LLM orchestration. This work enables the design of more intricate and efficient multi-agent systems, reducing the computational cost and latency barriers that currently hinder the deployment of complex, collaborative AI applications.

---

## ðŸ”‘ Key Findings

*   **Significant Latency Reduction:** The framework achieves a **2.0â€“6.2Ã— improvement** in time-to-first-token (TTFT) for individual messages.
*   **End-to-End Speedups:** In workflows heavily characterized by redundant computation, Prompt Choreography realizes overall end-to-end speedups **greater than 2.2Ã—**.
*   **Fidelity via Fine-tuning:** While reusing cached encodings in new contexts can alter model outputs, fine-tuning the LLM enables it to successfully mimic the results of standard re-encoding, ensuring output consistency.
*   **Parallelization Support:** The system supports parallel LLM calls that can attend to shared, previously encoded message subsets.

---

## ðŸ›  Methodology

The authors introduce **'Prompt Choreography,'** a framework designed to optimize multi-agent LLM workflows through the use of a dynamic, global Key-Value (KV) cache. This methodology allows individual LLM calls to attend to an arbitrary, reordered subset of messages that have been encoded in previous steps, rather than re-processing them. To address the potential semantic drift caused by reusing cached representations in differing contexts, the approach involves fine-tuning the base LLM to align its cached outputs with those generated via standard re-encoding.

---

## âš™ Technical Details

*   **Memory Model:** Shifts from a distributed to a shared memory model using a global KV cache stored contiguously in GPU memory.
*   **Caching Level:** Caches at the message level (includes both text and KV encodings).
*   **Position Encoding:** Relies on Rotary Positional Embeddings (RoPE) with precomputed rotation matrices for repositioning.
*   **API Structure:** Decomposed into `prefill` (parallel computation attending to parents) and `decode` (sequential generation) with dynamic masking.
*   **Features:** Supports parallel decoding and includes fine-tuning to maintain output fidelity when reusing cached encodings.

---

## ðŸ“ˆ Contributions

*   **Novel Framework:** Introduction of a new execution framework for LLM workflows that moves beyond static prompting to a dynamic, cache-driven architecture.
*   **Attention Flexibility:** A mechanism enabling LLMs to attend to arbitrary subsets and reorderings of prior messages within a global cache, facilitating more complex and efficient workflow designs.
*   **Mitigation of Caching Artifacts:** A demonstration that fine-tuning can effectively resolve the discrepancy between cached context processing and fresh context processing, a critical step for reliable caching adoption.
*   **Performance Benchmarking:** Quantitative evidence showing that choreographed caching can yield substantial computational savings and latency reductions in real-world multi-agent scenarios.

---

## ðŸ“Š Results

The system achieves a **2.0â€“6.2Ã— improvement** in Time-To-First-Token (TTFT) for individual messages and **> 2.2Ã— speedup** in end-to-end execution for workflows with redundant computation. It supports parallel LLM calls attending to shared encoded messages and handles arbitrary message reordering, gaps, and overlaps.