---
title: Large Language Model-Driven Surrogate-Assisted Evolutionary Algorithm for Expensive
  Optimization
arxiv_id: '2507.02892'
source_url: https://arxiv.org/abs/2507.02892
generated_at: '2026-01-27T21:39:41'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# Large Language Model-Driven Surrogate-Assisted Evolutionary Algorithm for Expensive Optimization

*Assisted Evolutionary, Large Language, Driven Surrogate, Maoguo Gongd, Electronic Engineering, Zhenkun Wangc, Expensive Optimization, Edward Chunga, Lindong Xiea, Genghui Lib*

---

### ðŸ“Š Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Proposed Algorithm** | LLM-SAEA |
| **Foundation Model** | GPT-3.5-turbo-0125 |
| **Search Dimensions** | 10-dimensional CEC benchmarks |
| **Evaluation Budget** | 1,000 Function Evaluations (FEs) |
| **Hardware** | Intel Core i7-10700 CPU @ 2.90 GHz, 32 GB RAM |
| **Best Result** | Ellipsoid (6.25e-28) |
| **Quality Score** | 8/10 |

---

## Executive Summary

This research addresses the critical challenge of **expensive optimization problems**, scenarios where evaluating the fitness function requires significant computational resources or time. This constraint renders traditional evolutionary algorithms impractical due to high evaluation costs. While Surrogate-Assisted Evolutionary Algorithms (SAEAs) have historically been used to approximate fitness landscapes, existing methods often struggle to balance global exploration with local exploitation, frequently requiring more evaluations than feasible or failing to converge efficiently.

The authors propose **LLM-SAEA**, a novel framework that integrates Large Language Models (LLMs) as search operators within a surrogate-assisted evolutionary loop. The key technical innovation lies in converting continuous numerical populations into natural language prompts, enabling the LLM (specifically GPT-3.5-turbo) to function as a **"Decision-Making Expert"** (generating search directions) and a **"Scoring Expert"** (assessing quality).

This architecture employs a hybrid strategy: the LLM generates new solutions based on semantic relationships and latent patterns found in the elite population, while a traditional surrogate model (e.g., Gaussian Process or Random Forest) rapidly filters these candidates before expensive evaluation.

In experiments on 10-dimensional CEC benchmark functions with a limited budget of 1,000 function evaluations, LLM-SAEA demonstrated superior performance compared to state-of-the-art baselines such as K-RVEA, SA-CMA-ES, and ESAO. The algorithm outperformed ESAO in 18 out of 20 test cases and IKAEA in 17, with statistical significance validated via Wilcoxon rank-sum and Friedman tests ($\alpha=0.05$).

This work represents a significant conceptual leap by bridging Natural Language Processing (NLP) and Evolutionary Computation (EC). It is one of the first comprehensive frameworks to demonstrate that the semantic reasoning of foundation models can effectively solve complex numerical engineering problems.

## Key Findings

*   **Reduced Evaluation Costs:** Significantly reduces the number of required expensive fitness evaluations compared to traditional and standard surrogate-assisted algorithms.
*   **Enhanced Semantic Exploration:** The LLM component effectively captures high-level semantic relationships and latent patterns, enabling better global exploration.
*   **Superior Convergence:** Demonstrates superior convergence rates and solution quality on CEC benchmarks compared to state-of-the-art methods like K-RVEA and SA-CMA-ES.
*   **Hybrid Efficacy:** The combination of LLM-based generation and surrogate-based filtering is proven to be more effective than using either component in isolation.

## Methodology

The study utilizes a hybrid optimization framework designed to maximize efficiency in resource-constrained environments:

1.  **LLM Integration:** A Large Language Model (LLM) acts as a search operator to generate new candidate solutions. It achieves this by converting the elite population into natural language prompts.
2.  **Surrogate Modeling:** The system maintains a surrogate model (such as Gaussian Process or Random Forest) to approximate the expensive fitness function. This allows for the rapid filtering of candidates before actual evaluation.
3.  **Hybrid Loop:** The algorithm implements an iterative loop that cycles between LLM generation, surrogate selection, and dataset updates, ensuring continuous improvement.

## Technical Details

*   **Proposed Algorithm:** LLM-SAEA (Large Language Model-Driven Surrogate-Assisted Evolutionary Algorithm)
*   **Architecture Components:**
    *   **Model:** Uses GPT-3.5-turbo-0125 as a pre-trained foundation model.
    *   **Expert Roles:** The LLM functions as two distinct experts:
        *   *Decision-Making Expert:* Generates search directions and offspring.
        *   *Scoring Expert:* Evaluates solution quality.
*   **Optimization Strategy:** Combines LLM-based generation with surrogate-based filtering.
*   **Population Parameters:** Population size ($N$) is set to **100**.
*   **Computational Environment:** Intel Core i7-10700 CPU @ 2.90 GHz and 32 GB RAM.

## Contributions

*   **Comprehensive Framework:** Presents one of the first comprehensive frameworks integrating Large Language Models into Surrogate-Assisted Evolutionary Algorithms (SAEAs).
*   **Numerical-to-Textual Mechanism:** Develops a specific mechanism to translate continuous numerical optimization problems into textual prompts, enabling LLMs to function as evolutionary operators.
*   **Bridging NLP and EC:** Bridges Natural Language Processing (NLP) and Evolutionary Computation (EC) by demonstrating that LLM generalization capabilities can solve complex numerical engineering problems.

## Results

### Experimental Setup
*   **Problems:** 10-dimensional CEC benchmark problems.
*   **Budget:** 1,000 Function Evaluations (FEs) per run over 20 independent runs.
*   **Metric:** Function Error.
*   **Validation:** Statistical validation performed using Wilcoxon rank-sum test and Friedman test with Hommel post-hoc procedure ($\alpha=0.05$).
*   **Baselines:** ESAO, IKAEA, TS-DDEO, SA-MPSO, SAMFEO, GL-SADE, ESA, and AutoSAEA.

### Quantitative Performance
*   **Head-to-Head:** LLM-SAEA surpassed baselines on the majority of test cases:
    *   **18/20** vs ESAO
    *   **17/20** vs IKAEA
*   **Specific Mean Function Errors:**
    *   Ellipsoid: `6.25e-28`
    *   Griewank: `1.65e-02`
    *   Ackley: `1.15e-05`
    *   Rosenbrock: `6.27e-01`
*   **Convergence Behavior:** Rapid convergence was observed on the Ellipsoid function, while Rastrigin remained a difficult challenge.

---

**Quality Score:** 8/10
**References:** 40 citations