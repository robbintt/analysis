---
title: Attention Is All You Need
arxiv_id: '1706.03762'
source_url: https://arxiv.org/abs/1706.03762
generated_at: '2026-01-26T07:52:28'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# Attention Is All You Need

*All You, Jakob Uszkoreit, Attention Is, Niki Parmar, Ashish Vaswani, Google Research, Noam Shazeer, Google Brain*

---

## üìä Quick Facts

| Metric | Details |
| :--- | :--- |
| **Quality Score** | 8/10 |
| **References** | 40 Citations |
| **Best BLEU Score** | 41.8 (En-to-Fr) |
| **Training Hardware** | 8 P100 GPUs |
| **Training Duration** | 12 hours to 3.5 days |

---

## üìù Executive Summary

This research addresses the computational bottlenecks inherent in dominant sequence transduction models of the time, specifically Recurrent (RNN) and Convolutional (CNN) neural networks. The primary limitation is the sequential nature of RNNs, which prevents parallelization during training and significantly slows down computation. Furthermore, these architectures struggle to capture long-range dependencies within sequences because the signal path length grows with the distance between words.

This problem matters because it created a performance ceiling for machine translation and other natural language tasks, making it computationally expensive to train on large datasets. The core innovation is the **"Transformer,"** a novel network architecture that relies entirely on attention mechanisms, dispensing with recurrence and convolutions completely.

Instead of processing words sequentially, the Transformer utilizes **"self-attention"** to analyze the entire sequence at once, allowing every word to look at every other word to establish global context and meaning regardless of their distance apart. The architecture employs an encoder-decoder structure and utilizes **"multi-head attention,"** which enables the model to simultaneously attend to information from different representation subspaces.

The Transformer established a new state-of-the-art in machine translation performance on the WMT 2014 benchmarks. The "Transformer Big" model achieved a BLEU score of **28.4** on English-to-German translation and **41.8** on English-to-French translation, outperforming previous top models. Crucially, these results were achieved with significantly greater efficiency; the high-performance model trained on just eight P100 GPUs for 3.5 days.

The significance of this paper lies in its paradigm-shifting demonstration that attention mechanisms alone are sufficient to replace complex recurrent and convolutional structures in sequence modeling. This finding solved the critical bottleneck of sequential processing, unlocking the ability to train massive models on massive datasets with unprecedented parallelization. The Transformer architecture introduced here became the foundational backbone for modern Natural Language Processing, directly enabling the subsequent development of BERT, GPT, and the current generation of Large Language Models (LLMs).

---

## üîë Key Findings

*   **State-of-the-Art Performance:** Established new SOTA BLEU scores on WMT 2014 tasks, achieving **28.4 BLEU** on English-to-German and **41.8 BLEU** on English-to-French.
*   **Training Efficiency:** Requires significantly less training time than recurrent or convolutional models, achieving SOTA results in just **3.5 days** on eight GPUs.
*   **Pure Attention Architecture:** The model's reliance on a pure attention mechanism eliminates the need for recurrence and convolutions, resulting in higher quality and more parallelizable models.
*   **Generalization:** The model generalizes effectively to other tasks beyond translation, such as English constituency parsing.

---

## üõ†Ô∏è Methodology

The researchers proposed a novel network architecture called the Transformer, which relies solely on attention mechanisms. This approach eliminates the use of complex recurrent (RNN) and convolutional (CNN) neural networks.

The methodology utilizes specific technical innovations to handle sequence dependencies, including:

*   **Scaled Dot-Product Attention:** Allows the model to attend to different parts of the sequence simultaneously.
*   **Multi-Head Attention:** Enables the model to jointly attend to information from different representation subspaces at different positions.
*   **Parameter-Free Position Representations:** Injected to handle the order of the sequence since the model contains no recurrence.

---

## ‚öôÔ∏è Technical Details

The following table outlines the structural specifications of the Transformer model:

| Feature | Specification |
| :--- | :--- |
| **Core Mechanism** | Attention mechanism (Transformer) without recurrence or convolutions. |
| **Architecture** | Encoder-Decoder structure. |
| **Layers** | Stacks of 6 layers each for both encoder and decoder. |
| **Sub-layers** | Multi-head self-attention and position-wise feed-forward networks ($d_{ff}=2048$). |
| **Dimensions** | $d_{model}=512$ |
| **Normalization** | Residual connections followed by Layer Normalization. |
| **Decoder Specifics** | Includes masked multi-head self-attention to maintain auto-regressive properties. |
| **Attention Math** | Scaled Dot-Product Attention divided by $\sqrt{d_k}$ to counteract vanishing gradients. |

---

## üèÜ Contributions

*   **Introduction of the Transformer:** The first simple network architecture for sequence transduction based entirely on attention, dispensing with recurrence and convolutions.
*   **Benchmark Establishment:** Set a new single-model state-of-the-art benchmark for machine translation tasks (WMT 2014 En-De and En-Fr).
*   **Resource Efficiency:** Demonstrated that attention-based models can achieve superior results with significantly fewer training resources.
*   **Sub-Component Innovation:** Conceptualization and implementation of critical components, including scaled dot-product attention, multi-head attention, and parameter-free position representations.

---

## üìà Results

*   **English-to-German:** Achieved a BLEU score of **28.4** on the WMT 2014 task.
*   **English-to-French:** Achieved a BLEU score of **41.8** on the WMT 2014 task.
*   **Training Speed:** The model trained between 12 hours to 3.5 days on 8 P100 GPUs, which is significantly faster than recurrent or convolutional models.
*   **Constituency Parsing:** The model generalized effectively to English constituency parsing tasks.

---