---
title: Modeling Language as a Sequence of Thoughts
arxiv_id: '2512.25026'
source_url: https://arxiv.org/abs/2512.25026
generated_at: '2026-02-03T13:36:49'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Modeling Language as a Sequence of Thoughts

*Nasim Borazjanizadeh; James McClelland*

---

> ### ðŸ“Š Quick Facts
>
> *   **Quality Score:** 9/10
> *   **References:** 40 citations
> *   **Core Efficiency:** GPT-2 requires 5â€“8% more data and 33â€“42% more parameters than the TG model for comparable loss.
> *   **Key Innovation:** Recurrent transformer with dual-level abstraction (tokens & sentence-level thoughts).
> *   **Training Objective:** Standard next-token prediction.
> *   **Major Achievement:** Solves the "Reversal Curse" and improves data efficiency.

---

## Executive Summary

Current large language models (LLMs) primarily rely on surface-level token co-occurrence statistics, which leads to fundamental limitations in data efficiency and the ability to capture complex relational logic. Standard transformer architectures often suffer from the "reversal curse"â€”the inability to infer $B$ is $A$ given $A$ is $B$â€”and struggle to maintain globally consistent representations of entities and events across long contexts. These inefficiencies necessitate massive datasets and parameter counts to achieve acceptable performance, creating a computational barrier and limiting the model's capacity for robust reasoning and generalization.

The authors introduce the "Thought Gestalt" (TG), a recurrent transformer architecture designed to model language at two levels of abstraction: individual tokens and sentence-level "thought" states. The model processes text sentence-by-sentence, using a shared stack of transformer blocks and a cross-attention mechanism to attend to a working memory of prior sentence representations. A critical technical innovation is the retention of the computation graph for these sentence vectors within the working memory. This allows gradients from future token losses to flow backward through the cross-attention pathway, directly optimizing the parameters responsible for generating earlier sentence vectors and facilitating long-term dependency learning using a standard next-token prediction objective.

Experimental results demonstrate that the TG architecture offers superior data and parameter efficiency compared to standard GPT-2 baselines. Scaling experiments indicate that GPT-2 requires approximately 5â€“8% more data and 33â€“42% more parameters to achieve a test loss comparable to the TG model. Furthermore, the TG model significantly mitigates the reversal curse, outperforming baselines on "father-son" relational-direction probes and reducing contextualization errors. Analysis of the training dynamics reveals that as pretraining scale increases, a larger fraction of the training signal flows through the cross-attention pathway, confirming the model's reliance on its dual-level abstraction mechanism.

This research bridges the gap between cognitive science and NLP by implementing a system that mimics human comprehension, converting linguistic input into compact, persistent event-like representations rather than relying on fleeting verbatim forms. By enforcing globally consistent latent representations of entities and events through a simplified training paradigm, the work addresses core LLM failure modes without resorting to complex multi-stage objectives. The findings suggest that incorporating cognitively inspired, multi-level abstractions is a viable and impactful strategy for developing more efficient, logically consistent, and data-efficient foundation models.

---

## Key Findings

*   **Superior Efficiency:** The Thought Gestalt (TG) model demonstrates consistent improvements in data and parameter efficiency compared to GPT-2 and other baselines.
*   **Quantifiable Performance Gains:** Scaling experiments indicate that GPT-2 requires approximately **5â€“8% more data** and **33â€“42% more parameters** to achieve a test loss comparable to the TG model.
*   **Solving the Reversal Curse:** The TG model significantly reduces errors in relational-direction generalization, specifically outperforming baselines on a 'father-son' reversal curse probe.
*   **Efficacy of Dual-Level Abstraction:** Modeling language at two levels (tokens and sentence-level thoughts) yields better results than relying solely on surface-level co-occurrence statistics.

---

## Methodology

The study proposes a novel architecture and training strategy to overcome limitations in standard transformers:

*   **Architecture:**
    *   Introduces the **Thought Gestalt (TG)**, a recurrent transformer.
    *   Designed to model language at two levels of abstraction: tokens and sentence-level 'thought' states.
*   **Generation Strategy:**
    *   Text is generated **one sentence at a time**.
    *   Utilizes a cross-attention mechanism to attend to a working memory containing representations of prior sentences.
*   **Parameter Sharing:**
    *   Both token and sentence representations are generated using a **shared stack of transformer blocks**.
*   **Training Objective:**
    *   Trained using a single, standard objective: **next-token prediction loss**.
*   **Optimization Technique:**
    *   Retains the computation graph of sentence representations written to working memory.
    *   Allows gradients from future token losses to flow backward through cross-attention.
    *   Optimizes parameters responsible for generating earlier sentence vectors, facilitating **long-term dependency learning**.

---

## Technical Details

**Core Architecture**
The Thought Gestalt (TG) utilizes a dual-level abstraction, processing language as a sequence of sentence-level thoughts using a single transformer stack.

**Processing Mechanism**
*   **Step Processing:** Processes text in sentence steps.
*   **Attention Alternation:** Alternates between causal self-attention (within sentences) and cross-attention (between current tokens and a working memory of previous sentences).
*   **Sentence Vectors ($s_t$):** Extracted from the `<EOS>` token at layer 7, projected, and added to memory **without detaching gradients**.
*   **Working Memory:** Holds the last 40 sentence vectors.
*   **Encodings:** Cross-attention uses sinusoidal positional encodings on memory keys only.

**Data Preparation**
*   **Method:** 'SaT Capped' (max 64 tokens).
*   **Markers:** Uses `<BOS>`, `<EOS>`, and `<EOD>` markers.
*   **Padding:** Sentences are padded to 67 tokens.

**Training Strategy**
*   **Curriculum:** Sentence-stream curriculum starting at 30 sentences and expanding.
*   **Loss Reweighting:** Down-weights `<EOS>` tokens to balance training.

---

## Results

*   **Efficiency Metrics:** The TG model requires 5â€“8% less data and 33â€“42% fewer parameters than GPT-2 to achieve comparable test loss.
*   **Relational-Directional Generalization:** Significantly reduces errors in relational-direction generalization (the 'Reversal Curse'), outperforming baselines on father-son probes and addressing in-context directional asymmetry.
*   **Contextualization:** The architecture mitigates contextualization errors found in standard transformers.
*   **Baselines:** Compared against GPT-2 with Sentence Boundary Bias, Fixed Token-Span Recurrence, and GPT-2 + Gist Masking.
*   **Training Dynamics:** Analysis indicates that as pretraining scale increases, a larger fraction of the training signal flows through the cross-attention pathway.

---

## Contributions

*   **Cognitively Inspired Architecture:** The paper bridges cognitive science and NLP by implementing a system that mimics human comprehensionâ€”converting linguistic input into compact, event-like representations that persist in memory rather than relying on fleeting verbatim forms.
*   **Addressing Core LLM Limitations:** The work directly addresses critical failure modes in standard transformer models, such as the reversal curse, contextualization errors, and data inefficiency, by enforcing globally consistent latent representations of entities and events.
*   **Simplified Training Paradigm:** The authors demonstrate that significant improvements in generalization and efficiency can be achieved without complex multi-stage training objectives, utilizing standard next-token prediction within a recurrent framework.