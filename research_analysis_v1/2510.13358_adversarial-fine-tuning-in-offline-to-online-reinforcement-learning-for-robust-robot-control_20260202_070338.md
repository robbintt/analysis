# Adversarial Fine-tuning in Offline-to-Online Reinforcement Learning for Robust Robot Control

*Shingo Ayabe; Hiroshi Kera; Kazuhiko Kawamoto*

***

> ### ðŸ“Š Quick Facts
>
> *   **Quality Score:** 9/10
> *   **Citations:** 40
> *   **Core Method:** Offline-to-Online RL with Adversarial Fine-tuning
> *   **Key Innovation:** Performance-aware curriculum using EMA signals
> *   **Primary Domain:** Robot Control & Locomotion (MuJoCo)

***

## Executive Summary

This research addresses the structural brittleness of reinforcement learning (RL) policies in robot control when deployed in real-world environments subject to action-space perturbations, such as actuator faults or noise. While offline RL methods offer sample efficiency over static datasets, standard algorithms like Conservative Q-Learning (CQL), Implicit Q-Learning (IQL), and TD3+BC fail catastrophically when faced with the distribution shifts caused by these physical imperfections. Conversely, training policies entirely from scratch via online interaction to achieve robustness is computationally prohibitive and often unstable. The paper tackles the critical challenge of maintaining high nominal performance while ensuring adaptability to out-of-distribution disturbancesâ€”a necessity for reliable autonomous systems where actuator faults are probable and safety is paramount.

The authors propose a novel offline-to-online RL framework that integrates adversarial fine-tuning with a performance-aware curriculum. The process begins with conservative offline pre-training on a fixed dataset to establish a viable policy base. The core innovation occurs during the online fine-tuning phase, where adversarial perturbations are injected into executed actions, forcing the policy to learn compensatory control strategies against corrupted dynamics. To balance the trade-off between robustness and stability, the authors introduce an adaptive curriculum mechanism utilizing an exponential-moving-average (EMA) signal. This mechanism dynamically adjusts the probability of perturbation injection, gradually increasing the difficulty as the policy improves, thereby ensuring the agent adapts to faults without sacrificing the initial performance levels gained during pre-training.

Empirical validation on standard MuJoCo locomotion benchmarksâ€”HalfCheetah-v4, Hopper-v4, Walker2d-v4, and Ant-v4â€”demonstrates the framework's quantitative superiority. Under action-space perturbation magnitudes up to $\epsilon=0.5$, baseline algorithms like CQL and IQL suffered performance degradations of 90% to nearly 100%. In contrast, the proposed method effectively maintained robust performance, retaining approximately 80% to 95% of nominal returns. Furthermore, the study highlighted significant efficiency gains: the offline-to-online approach achieved convergence in roughly 100,000 environment steps, whereas training from scratch typically required over 1,000,000 steps.

***

## Key Findings

*   **Enhanced Robustness:** The proposed offline-to-online framework significantly improves robustness against action-space perturbations compared to offline-only training methods.
*   **Faster Convergence:** Training via adversarial fine-tuning achieves much faster convergence rates than training reinforcement learning policies entirely from scratch (approx. 100k steps vs. 1M+ steps).
*   **Condition Matching:** The strongest robustness to perturbations is achieved when fine-tuning conditions match those encountered during evaluation.
*   **Stability Balance:** The proposed adaptive curriculum strategy successfully mitigates nominal performance degradation, effectively balancing stability and robustness.

***

## Methodology

The study utilizes a two-phase framework designed to bridge the gap between the sample efficiency of offline RL and the adaptability of online RL.

1.  **Phase 1: Offline Pre-training**
    *   Utilizes clean, static datasets.
    *   Establishes a viable policy base using conservative methods to mitigate Q-value overestimation.

2.  **Phase 2: Online Adversarial Fine-tuning**
    *   Involves environment interaction where perturbations are injected into executed actions.
    *   Teaches the policy to generate compensatory actions in response to corrupted dynamics.

3.  **Performance-Aware Curriculum**
    *   Employs an exponential-moving-average (EMA) signal to monitor policy performance.
    *   Dynamically adjusts the probability of perturbation injection to balance the trade-off between robustness and nominal stability.

***

## Technical Details

The technical implementation combines constraints on learned policies with adaptive perturbation mechanisms.

*   **Framework Architecture:** Two-stage offline-to-online Reinforcement Learning.
*   **Offline Objective:** Constrains the learned policy to the behavior policy using a divergence metric (e.g., **KL-divergence**) to mitigate Q-value overestimation for out-of-distribution pairs.
*   **Adversarial Injection:** Introduces adversarial action-space perturbations during the online fine-tuning phase.
*   **Adaptive Mechanism:** Utilizes an adaptive curriculum mechanism that gradually increases perturbation probability based on policy performance to balance nominal performance with robustness.

***

## Results

The proposed framework demonstrates significant improvements in both resilience and efficiency.

*   **Resilience:** Under perturbation magnitudes up to $\epsilon=0.5$, the proposed method retained **80%â€“95%** of nominal returns, whereas baselines (CQL/IQL) collapsed to near-zero returns (90%â€“100% degradation).
*   **Efficiency:** The offline-to-online approach achieved convergence in roughly **100,000** environment steps, compared to over **1,000,000** steps for training from scratch (e.g., Soft Actor-Critic).
*   **Structural Vulnerability:** Analysis confirms that standard conservative offline RL policies are structurally vulnerable to out-of-distribution shifts caused by action-space perturbations.
*   **Curriculum Efficacy:** The adaptive curriculum strategy effectively mitigated nominal performance degradation, confirming its ability to maintain stability while enhancing resilience.

***

## Contributions

*   **Bridging Methodologies:** Introduction of a method that bridges the sample efficiency of offline RL and the adaptability of online RL to target brittleness under actuator faults.
*   **Curriculum Development:** Development of a performance-aware curriculum using exponential-moving-average signals to reduce nominal performance degradation while enhancing resilience.
*   **Empirical Validation:** Empirical validation on locomotion tasks demonstrating that adversarial fine-tuning enables adaptive control without sacrificing the convergence speed offered by offline initialization.