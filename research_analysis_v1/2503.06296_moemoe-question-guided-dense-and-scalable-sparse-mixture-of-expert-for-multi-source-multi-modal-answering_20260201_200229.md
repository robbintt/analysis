# MoEMoE: Question Guided Dense and Scalable Sparse Mixture-of-Expert for Multi-source Multi-modal Answering

*Vinay Kumar Verma; Shreyas Sunil Kulkarni; Happy Mittal; Deepak Gupta*

---

> ### ðŸ“Š Quick Facts
> *   **WebQA F1 Score:** 76.1%
> *   **Model Backbones:** T5, Flan-T5
> *   **Vision Encoder:** SwinV2 (256x256 inputs)
> *   **Core Architecture:** Sparse Mixture-of-Experts (MoE)
> *   **Key Mechanism:** Question Guided Attention (QGA)
> *   **Primary Datasets:** WebQA, MultimodalQA (among 3 total)

---

## Executive Summary

This research addresses the complexities inherent in Multi-source Multi-modal Question Answering (MMQA), a domain where accurate answers often require synthesizing information from disparate text and image sources. The primary challenge is the prevalence of "language bias" in existing models, which tend to rely on textual correlations rather than performing genuine, unbiased cross-modal reasoning. Furthermore, current frameworks face significant scalability issues; as the number of diverse question types increases into the thousands, dense models struggle to manage the high-dimensional complexity without performance degradation, limiting their utility in real-world, multi-source environments.

The authors propose **MoEMoE**, a novel Question-Answer Generation (QAG) framework built upon a "Question Guided" strategy and a sparse Mixture-of-Experts (MoE) architecture. The system utilizes dual unshared T5 encoders for processing context and questions, paired with a SwinV2 Vision Transformer for image processing (256x256 inputs). Its core technical innovation, Question Guided Attention (QGA), dynamically generates token-level weights ($\alpha$ and $\beta$) from question embeddings to fuse image and context features into a joint embedding ($e_i = \alpha \cdot I + \beta \cdot C$). To ensure scalability, the architecture incorporates a sparse MoE layer that routes questions to specialized experts, enabling the model to handle thousands of diverse query types. Additionally, the framework employs explicit alignment losses (Question-Context and Question-Image) to pinpoint specific relevant details within sources, a departure from prior models like MXT.

The framework was evaluated across three distinct datasets, including **WebQA** and **MultimodalQA**, utilizing T5 and Flan-T5 backbones. MoEMoE demonstrated superior efficacy compared to traditional single-source models and baselines like MXT, achieving an **F1 score of 76.1%** on the WebQA validation setâ€”a significant improvement over standard dense methods. The integration of the sparse MoE architecture successfully enabled the model to scale to thousands of diverse question types without the performance degradation typically seen in dense models. Empirical findings confirmed that the question-guided attention mechanism effectively mitigated language bias, while the explicit alignment technique significantly enhanced **Exact Match (EM)** scores, validating the model's precision in complex, multi-source scenarios.

---

## Key Findings

*   **Effective Complexity Management:** The proposed framework successfully addresses the intricacies of Multi-source Multi-modal Question Answering (MMQA), demonstrating proven efficacy across three datasets when implemented with both T5 and Flan-T5 architectures.
*   **Scalability via Sparse-MoE:** The integration of a sparse Mixture-of-Experts (sparse-MoE) framework enables the model to scale efficiently, handling thousands of diverse question types that typically hinder dense models.
*   **Unbiased Answer Generation:** The question-guided attention mechanism facilitates robust and unbiased answer generation by optimally selecting or combining information from multiple sources, effectively mitigating language bias.
*   **Enhanced Precision through Alignment:** Explicit alignment between questions and information sources significantly improves the model's ability to identify pertinent segments within a source, thereby enhancing overall answering precision and Exact Match (EM) scores.

---

## Methodology

*   **Framework Formulation:** The researchers formulated a novel Question-Answer Generation (QAG) framework specifically designed for environments containing multi-source, multimodal information, where the correct answer may reside across one or more sources.
*   **Cross-Source Attention:** A question-guided attention mechanism was implemented to learn attention weights across multiple different sources. This allows the model to decode information from the optimal combination of sources for final answer generation.
*   **Intra-Source Alignment:** To pinpoint specific relevant details, the method employs an explicit alignment technique that connects questions directly to specific segments of the information sources.
*   **Scalability Architecture:** To manage the high diversity of potential queries, the model was extended into a sparse Mixture-of-Experts (sparse-MoE) framework, allowing for specialized handling of thousands of question types without performance degradation.

---

## Technical Details

### Input Processing & Encoding
*   **Inputs:** The model processes input triplets consisting of a question, context (text), and an image.
*   **Text Encoding:** Utilizes two separate T5 encoders with unshared parametersâ€”one for the context and one for the question.
*   **Image Encoding:** Employs a SwinV2 Vision Transformer to process 256x256 inputs, generating patch embeddings. These embeddings are repeated to match the text sequence length.

### Core Mechanism: Question Guided Attention (QGA)
*   **Function:** QGA functions as a dense MoE gating mechanism.
*   **Process:** It converts question embeddings into token-level weights for the image ($\alpha$) and context ($\beta$) via a Fully Connected layer.
*   **Fusion:** These weights combine the modalities into a joint embedding calculated as:
    $$e_i = \alpha \cdot I + \beta \cdot C$$

### Optimization & Architecture
*   **Alignment Losses:** The architecture incorporates specific losses for Question-Context and Question-Image alignment.
*   **Sparse MoE Layer:** Integrated to ensure scalability; routes inputs to specialized experts to handle query diversity.
*   **Comparison:** Compared to prior models like MXT, MoEMoE uses a single image encoder and enables patch-wise attention.

---

## Contributions

*   **Novel QAG Framework:** Introduction of a generalized framework for handling multi-source, multimodal answer generation, addressing a scenario noted for its high complexity and rich information potential.
*   **Attention and Alignment Mechanisms:** Development of a dual-faceted processing approach:
    1.  Learning cross-source attention to determine the optimal source combination.
    2.  Establishing explicit alignment within sources to locate relevant information.
*   **Scalable Sparse-MoE Integration:** Extension of the dense model into a sparse Mixture-of-Experts architecture, specifically designed to solve the scalability challenges associated with diverse question answering in multi-modal settings.

---

## Results

The framework was evaluated on three distinct datasets using T5 and Flan-T5 backbones. Key outcomes include:

*   **Performance Metrics:** Achieved an **F1 score of 76.1%** on the WebQA validation set, showing a significant improvement over standard dense methods.
*   **Multi-source Superiority:** Demonstrated higher efficacy in Multi-source Multi-modal Question Answering compared to single-source models and baselines like MXT.
*   **Bias Mitigation:** The question-guided attention mechanism successfully facilitated robust and unbiased answer generation by reducing the language bias found in existing models.
*   **Precision Gains:** Explicit alignment techniques resulted in improved Exact Match (EM) scores, validating the model's precision in complex scenarios.

---

**Quality Score:** 7/10  
**References:** 13 citations