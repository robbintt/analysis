---
title: Ranked Entropy Minimization for Continual Test-Time Adaptation
arxiv_id: '2505.16441'
source_url: https://arxiv.org/abs/2505.16441
generated_at: '2026-01-27T16:21:06'
quality_score: 9
citation_count: 19
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# Ranked Entropy Minimization for Continual Test-Time Adaptation

*Ranked Entropy, Time Adaptation, Jaemin Na, Korea Telecom, Ajou University, Continual Test, Korea University, Wonjun Hwang, Jisu Han*

---

> ### ðŸ“Š Quick Facts
>
> *   **Quality Score:** 9/10
> *   **Citations:** 19
> *   **Focus:** Source-Free Continual Test-Time Adaptation (CTTA)
> *   **Benchmarks:** CIFAR-10C, CIFAR-100C, ImageNet-C
> *   **Top Result:** Up to **12.7%** accuracy improvement on CIFAR-100C

---

## Executive Summary

This research addresses the challenge of **Continual Test-Time Adaptation (CTTA)** in source-free scenarios, where models must adapt to continuously shifting, unlabeled data distributions without access to the original training data. A critical issue in this domain is the stability-plasticity dilemma: models must learn new patterns without forgetting previous knowledge. Standard entropy minimization techniques, often used for self-supervised adaptation, fail in continual settings because they minimize entropy indiscriminately for all samples. This leads to **"error accumulation,"** where the model reinforces its own incorrect predictions, causing significant performance degradation over time.

The authors propose **Ranked Entropy Minimization (REM)**, a novel algorithm that modifies the entropy minimization framework by introducing a sample selection mechanism. Instead of treating all test samples equally, REM calculates the entropy (uncertainty) for every sample in a batch and ranks them based on their confidence scores. The model performs adaptation updates exclusively on the subset of samples with the lowest entropy, representing the most confident and reliable predictions. This unequal treatment allows the model to minimize entropy strictly on trustworthy samples, thereby preventing the reinforcement of noise and incorrect predictions that drives error accumulation.

The proposed method was evaluated across **CIFAR-10C**, **CIFAR-100C**, and **ImageNet-C** benchmarks. On CIFAR-10C, REM achieved an average accuracy of **85.1%**, significantly outperforming the standard entropy minimization baseline (TENT), which scored 77.4%. Similar improvements were observed on CIFAR-100C, where REM reached **57.8%** accuracy compared to the baseline's 45.1%. Furthermore, on the large-scale ImageNet-C dataset, REM improved accuracy to **56.2%**, surpassing previous baselines that struggled with error drift. These results demonstrate the method's ability to maintain high accuracy while mitigating error accumulation, all without relying on source data.

This paper significantly influences the field of test-time adaptation by formally identifying error accumulation as the primary failure mode for standard entropy minimization in continual settings. By establishing REM as a new state-of-the-art baseline with substantial quantitative gainsâ€”up to **12.7%** accuracy improvement on CIFAR-100Câ€”the authors demonstrate that simple, confidence-based selection strategies can outperform more complex approaches involving meta-learning. This work provides a practical, source-free solution for deploying models in dynamic real-world environments, highlighting that selective trust in high-confidence predictions is essential for robust adaptation.

---

## Key Findings

*   **Strict Selection Mitigates Error:** Strictly selecting low-entropy samples effectively mitigates the error accumulation commonly found in standard entropy minimization methods.
*   **State-of-the-Art Performance:** The proposed method achieves state-of-the-art (SOTA) performance on CIFAR-10C, CIFAR-100C, and ImageNet-C.
*   **Stability-Plasticity Balance:** The approach successfully balances stability and plasticity without requiring source data access.
*   **Trust is Detrimental:** Trusting all predictions is detrimental; ranking samples by confidence and selecting only the most reliable ones is crucial for robust adaptation.

---

## Methodology

The study operates within a **Source-Free Continual Test-Time Adaptation (CTTA)** setting using unlabeled test data. It proposes **Ranked Entropy Minimization (REM)**, which follows a specific workflow:

1.  **Entropy Calculation:** Calculates entropy for every sample in a test batch.
2.  **Ranking:** Samples are ranked by their entropy score.
3.  **Selective Adaptation:** The model performs adaptation only on the subset of samples with the lowest entropy (highest confidence).
4.  **Objective:** The aim is to minimize the entropy of these selected trustworthy samples only.

---

## Technical Details

| Component | Description |
| :--- | :--- |
| **Core Algorithm** | **Ranked Entropy Minimization (REM)** |
| **Treatment of Data** | Predictions are treated **unequally** based on confidence levels. |
| **Filtering Mechanism** | Only samples with low entropy (high confidence) are selected for model updates. |
| **Objective** | To prevent reinforcing incorrect predictions and mitigate error accumulation. |
| **Operational Constraint** | Functions under a **source-free constraint** (no access to original source training data). |
| **Dilemma Resolution** | Balances stability-plasticity through a selective ranking mechanism. |

---

## Results

The proposed method was rigorously evaluated against standard benchmarks:

*   **CIFAR-10C:**
    *   **REM Method:** 85.1% Accuracy
    *   **Baseline (TENT):** 77.4% Accuracy
*   **CIFAR-100C:**
    *   **REM Method:** 57.8% Accuracy
    *   **Baseline:** 45.1% Accuracy
*   **ImageNet-C:**
    *   **REM Method:** 56.2% Accuracy
    *   **Outcome:** Surpassed previous baselines that struggled with error drift.

The method demonstrated superior performance compared to standard entropy minimization by significantly reducing error accumulation and validating high performance without reliance on source data.

---

## Contributions

*   **Algorithm Introduction:** Introduced the Ranked Entropy Minimization (REM) algorithm, adding a novel selection mechanism to the existing entropy minimization framework.
*   **Failure Mode Analysis:** Provided a detailed analysis identifying error accumulation as the primary failure mode for standard entropy minimization in continual settings.
*   **Baseline Establishment:** Established a new state-of-the-art baseline for CTTA tasks, demonstrating that simple confidence-based selection outperforms complex meta-learning or self-training approaches.