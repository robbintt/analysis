---
title: Non-Asymptotic Global Convergence of PPO-Clip
arxiv_id: '2512.16565'
source_url: https://arxiv.org/abs/2512.16565
generated_at: '2026-02-06T03:32:16'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Non-Asymptotic Global Convergence of PPO-Clip
*Yin Liu; Qiming Dai; Junyu Zhang; Zaiwen Wen*

---

> ### **Quick Facts**
> **Quality Score:** 8/10  
> **References:** 40 citations  
> **Primary Focus:** Theoretical convergence of Actor-only PPO  
> **Application:** LLM Alignment (GRPO, DAPO)  
> **Key Innovation:** Non-uniform Lipschitz smoothness & Łojasiewicz inequality  

---

## Executive Summary

Despite the widespread empirical success of Proximal Policy Optimization (PPO)—particularly the PPO-Clip variant used in actor-only settings like Large Language Model (LLM) alignment—theoretical guarantees regarding its convergence properties have remained insufficiently explored. The field lacks a rigorous mathematical foundation explaining *why* these methods remain stable and efficient, specifically concerning non-asymptotic convergence to global optima. This theoretical gap poses challenges for reliability and safety in high-stakes applications, necessitating a formal analysis of the optimization landscape inherent to these policy gradient methods.

This research establishes a novel theoretical framework by analyzing a deterministic actor-only PPO variant within a general Reinforcement Learning setting, utilizing softmax policy parameterization and f-divergence regularization. The key technical innovation is the derivation of a "non-uniform Lipschitz smoothness" condition and the application of the "Łojasiewicz inequality" to the problem formulation. By moving beyond standard uniform assumptions, the authors characterize the geometry of the objective function, allowing them to analyze the optimization dynamics of a double-loop structure where an outer loop updates the sampling policy and an inner loop performs policy gradient steps on a surrogate objective.

The study provides distinct convergence guarantees based on the specific regularizer employed. For the Forward KL-regularizer, the authors prove non-asymptotic linear convergence to the globally optimal policy, deriving a Smoothness Factor ($L_f$) governed by terms including $(1-\theta)^{-3}$ and the maximum reward. Conversely, the Reverse KL-regularizer ensures stationary convergence with local linear convergence rates.

This work significantly advances the theoretical understanding of policy optimization by providing the first rigorous convergence proofs for PPO-Clip, thereby justifying its empirical stability. By quantifying the differences between Forward and Reverse KL regularization, the findings offer actionable insights for algorithm selection, favoring Forward KL for global optimality guarantees. This contribution is particularly relevant for the LLM alignment community (e.g., GRPO and DAPO algorithms), as it solidifies the mathematical reliability of actor-only methods that are critical for fine-tuning large-scale models.

---

## Key Findings

*   Addresses the theoretical gap regarding the convergence properties of actor-only PPO variants.
*   Derives a **non-uniform Lipschitz smoothness** condition and a **Łojasiewicz inequality** for the problem formulation involving f-divergence regularization and softmax policy parameterization.
*   Proves **non-asymptotic linear convergence** to the globally optimal policy for the **Forward KL-regularizer**.
*   Establishes **stationary convergence** and guarantees **local linear convergence rates** for the **Reverse KL-regularizer**.
*   Provides guarantees for finding the globally optimal policy in the Forward KL setting.

---

## Methodology

The study analyzes a deterministic actor-only variant of the Proximal Policy Optimization (PPO) algorithm within a general Reinforcement Learning setting. It utilizes softmax policy parameterization and incorporates f-divergence regularization, specifically analyzing forward and reverse KL-divergences. The proof methodology relies on establishing non-uniform Lipschitz smoothness conditions and the Łojasiewicz inequality to characterize the optimization landscape.

---

## Contributions

*   **Theoretical Foundation:** Establishes a rigorous theoretical foundation for PPO-Clip, justifying its empirical stability and efficiency through mathematical analysis.
*   **Algorithm Selection Guidance:** Provides distinct convergence rate guarantees (global vs. local) for different regularization types (Forward vs. Reverse KL), aiding in algorithm selection.
*   **Geometric Insight:** Contributes to the understanding of the objective function's geometry in policy optimization by defining its smoothness and gradient properties under softmax parameterization.

---

## Technical Details

### **Algorithm Structure**
*   **Target:** Actor-only PPO variants (GRPO and DAPO) for LLM alignment.
*   **Architecture:** Double-loop structure with an outer loop updating the sampling policy and an inner loop performing K policy gradient steps.
*   **Objective:** Optimizes a surrogate objective $L_{GRPO}$ involving a clipped probability ratio and an additive KL-divergence regularization penalty.

### **Mathematical Formulation**
*   **Environment:** Infinite-horizon discounted MDP.
*   **Parameterization:** Softmax policy parameterization.
*   **Optimization Target:** f-divergence regularized value function.

### **Assumptions**
*   A uniform lower bound on the reference policy.
*   A strictly positive initial state distribution.
*   Specific boundedness conditions on the derivatives of the divergence function $f$.

### **Analytical Tools**
*   Non-uniform Lipschitz smoothness.
*   The Łojasiewicz inequality.

---

## Results

### **Smoothness & Geometry**
The theoretical analysis defines a **Smoothness Factor ($L_f$)** governed by terms including $(1-\theta)^{-3}$, maximum reward, and regularization strength. This extends previous entropy-regularized results to general f-divergences.

Gradient and score function bounds are established, specifically:
$$
\nabla_\theta \log \pi_\theta(a|s) \leq \sqrt{2}
$$

### **Convergence Guarantees**
The study highlights distinct outcomes based on the regularizer used:
*   **Forward KL-regularizer:** Achieves **non-asymptotic linear convergence** to the globally optimal policy.
*   **Reverse KL-regularizer:** Ensures **stationary convergence** with **local linear rates**.

### **Optimality & Concavity**
*   The **Łojasiewicz property** relates the gradient norm squared to the optimality gap.
*   Specific **Strong concavity coefficients ($m_s$)** are derived for various divergences:

| Divergence Type | Coefficient ($m_s$) |
| :--- | :--- |
| **Reverse KL** | 1 |
| **Forward KL** | $\min_a \pi_{ref}/\pi_\theta$ |
| **JS Divergence** | $1/(2(\pi_\theta+\pi_{ref}))$ |
| **$\chi^2$ Divergence** | 2 |