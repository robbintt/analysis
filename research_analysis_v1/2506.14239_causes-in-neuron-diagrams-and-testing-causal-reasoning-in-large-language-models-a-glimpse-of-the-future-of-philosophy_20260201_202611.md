# Causes in neuron diagrams, and testing causal reasoning in Large Language Models. A glimpse of the future of philosophy?

*Authors: Louis Vervoort; Vitaly Nikolaev*

---

> **Quick Facts**
> *   **Models Tested:** ChatGPT (GPT-3, GPT-4, o3-mini), DeepSeek-R1, Gemini 2.0 Flash
> *   **Test Dataset:** 25 Neuron Diagrams
> *   **Proposed Definition:** DEF-1
> *   **Top Performer:** Gemini 2.0 Flash (~90% combined success)
> *   **Paper Quality Score:** 8/10

---

## Executive Summary

This paper addresses the enduring philosophical challenge of defining causation within "neuron diagrams"—abstract structural models popularized by David Lewis—which have historically been plagued by elusive definitions and intense debate. The study challenges the widespread assumption that a valid definition of cause in these diagrams is impossible, positing that without a robust formal definition, accurately assessing an AI's capability to perform true causal reasoning remains unfeasible.

The key technical innovation is the introduction of **"DEF-1,"** a novel formal definition of cause applicable to neuron diagrams that offers wider validity than predecessors by successfully handling complex edge cases, including redundant causation, causation by omission, and violations of transitivity.

Using DEF-1 as the ground truth metric, the authors benchmarked state-of-the-art LLMs. Results reveal a stark performance gap: while GPT-3 failed and GPT-4 achieved ~50% success, **Gemini 2.0 Flash (Thinking Experimental)** achieved the highest performance (~90% combined success). By demonstrating that advanced LLMs can resolve abstract causal disputes that have long contested human philosophers, the paper suggests a future where philosophy is a collaborative interplay between human and artificial expertise.

---

## Key Findings

*   **Capability Demonstrated:** Advanced LLMs (ChatGPT, DeepSeek, Gemini) can correctly identify causes in abstract reasoning tests, resolving cases currently debated in philosophical literature.
*   **Definition Established:** The study refutes the idea that a valid definition of cause in neuron diagrams is elusive by proposing **DEF-1**, a definition with wider validity than previously published.
*   **Model Performance:** Performance varies significantly by architecture; newer "Thinking" models (Gemini, DeepSeek) substantially outperform traditional GPT versions on complex causal structures.
*   **Human Comparison:** Conjectures suggest that while failure rates increase with diagram complexity, the success rates of these models on abstract tests may exceed average human capability.

---

## Methodology

The research approach combines philosophical framework with empirical AI testing:

*   **Philosophical Grounding:** utilizes the philosophy of causation, specifically 'neuron diagrams' popularized by D. Lewis, as the structural basis.
*   **Test Design:** A novel test for abstract causal reasoning was developed and administered to state-of-the-art LLMs to assess performance against complex philosophical benchmarks.
*   **Input Format:** Text descriptions of logical conditionals and temporal sequences.
*   **Constraints:** Advanced tests required short answers without explicit reasoning steps to minimize verbose text mimicry and focus on structural derivation.

---

## Technical Details

### Framework & Definition
The study utilizes **'neuron diagrams'** (referencing Paul and Hall 2013) which model causation via abstract neuron firing timestamps. The core technical contribution is the **DEF-1** definition, which improves upon existing metrics by handling:

*   Redundant causation
*   Causation by omission
*   Violations of transitivity

### Testing Protocol
*   **Validation Criteria:** Alignment with DEF-1 and ground truth causal chains.
*   **Query Types:**
    *   **Q-OCCUR:** Does final event E occur?
    *   **Q-CAUSE:** Identify causes at time t1.
*   **Dataset:** 25 distinct diagrams categorized by complexity.

---

## Results

Performance was categorized into three levels: Fully Correct, Partially Correct, and Wrong.

| Model | Fully Correct | Partially Correct | Wrong | Performance Notes |
| :--- | :---: | :---: | :---: | :--- |
| **ChatGPT (GPT-3)** | - | - | High | Essentially failed the test. |
| **ChatGPT (GPT-4)** | ~50% | - | - | Declined specifically on complex diagrams (19, 22-25). |
| **ChatGPT o3-mini** | 2 | 11 | 12 | Struggled significantly with the abstract logic. |
| **DeepSeek-R1** | 10 | 15 | 0 | Robust competence; no completely wrong answers. |
| **Gemini 2.0 Flash** | 14 | 9 | 2 | **Top Performer** (~90% success including partials). |

### Qualitative Analysis
*   **Complexity Barrier:** Higher failure rates were observed specifically on complex diagrams.
*   **Reasoning Mimicry:** Models often mimic textual reasoning patterns, though the "short answer" constraint helped isolate raw causal inference.

---

## Contributions

1.  **New Testing Standard:** Introduction of a philosophically grounded test designed to evaluate abstract causal reasoning in AI systems.
2.  **DEF-1 Definition:** A proposed definition of cause within neuron diagrams that offers broader validity than existing definitions, providing a metric for assessing both current and future AI.
3.  **Paradigm Shift:** A conceptual proposal that future philosophical research should be viewed as a collaborative interplay between human and artificial expertise.