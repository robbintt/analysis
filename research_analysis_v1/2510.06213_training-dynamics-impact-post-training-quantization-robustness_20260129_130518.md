# Training Dynamics Impact Post-Training Quantization Robustness

*Albert Catalan-Tatjer; Niccolò Ajroldi; Jonas Geiping*

---

> ### ⚡ Quick Facts
> 
> *   **Quality Score:** 8/10
> *   **References:** 40 Citations
> *   **Max Model Scale:** 32 Billion Parameters
> *   **Training Data:** Up to 15 Trillion Tokens
> *   **Precision Levels Analyzed:** 3-bit, 4-bit
> *   **Primary Algorithms:** GPTQ, AWQ, BitsAndBytes

---

## Executive Summary

This research addresses the critical unpredictability of Post-Training Quantization (PTQ) robustness in Large Language Models (LLMs). As models scale to billions of parameters, quantization is essential for deployment efficiency, yet performance degradation varies wildly even among models with similar architectures. Previously, the field lacked a mechanistic understanding of why some models quantize poorly while others retain fidelity, often attributing this variance abstractly to model scale or dataset size. This gap creates a risk where extensive pre-training efforts might result in models that are ill-suited for efficient, low-bit inference, necessitating a deeper investigation into the causal factors driving quantization error.

The key innovation is the identification of the learning rate (LR) schedule—specifically the decay phase—as the primary driver of quantization robustness, rather than model size or raw data volume. Through a two-phase methodology combining large-scale trajectory analysis (up to 32B parameters) and controlled experimental training, the authors discovered that standard LR decay creates a divergence between validation loss and quantization error. The study proposes specific technical interventions to mitigate this, including "Strategic Interventions" that modulate the LR to maintain higher rates for longer durations (e.g., Trapezoidal schedules) and the application of Stochastic Weight Averaging (SWA). This approach distinguishes between Weight Error and Reconstruction Error, establishing that the geometry of the loss landscape late in training dictates compressibility.

Empirical findings highlight a stark inverse relationship between standard training optimization and quantization robustness. In SmolLM3 experiments, while validation loss decreased during the LR decay phase, quantization error surged dramatically; 3-bit error increased from approximately 4 to 9 units, and 4-bit error jumped from roughly 0.6 to 1.8 units. Controlled experiments validated that these issues stem from the learning rate dynamics: models subjected to extended stable learning rates or SWA exhibited significantly lower quantization errors. Additionally, the study demonstrated that increasing dataset scale does not inherently degrade quantization; previously observed negative correlations with scale were actually artifacts of the prolonged LR decay phases typical in long training runs.

This work fundamentally reframes the relationship between training scale and deployment efficiency, offering actionable guidelines to ensure models remain quantization-ready without compromising training quality. By isolating the learning rate schedule as the critical lever, researchers and practitioners can now architect training regimens—such as delaying decay or implementing weight averaging—that proactively enhance compatibility with PTQ algorithms like GPTQ, AWQ, and BitsAndBytes. The insights eliminate the misconception that larger datasets inevitably harm compression, empowering the development of state-of-the-art models that do not sacrifice inference efficiency for training performance.

---

## Key Findings

*   **Drivers of Error:** Quantization errors are driven primarily by **learning rate** and hyperparameter interactions, not model size alone.
*   **Divergence Phenomenon:** A distinct divergence occurs between **validation loss** (which decreases) and **quantization error** (which rises) during the learning rate decay phase.
*   **Dataset Scale:** Increasing dataset scale does **not** inherently compromise quantization effectiveness.
*   **Strategic Interventions:** Specific interventions during training—such as modulating the learning rate—can successfully improve quantization robustness.

---

## Methodology

The authors employed a comprehensive two-phase experimental strategy:

1.  **Large-Scale Trajectory Analysis**
    *   Analyzed existing open-source models.
    *   **Scale:** Up to **32 billion parameters** and **15 trillion tokens**.
    *   Purpose: To identify broad patterns and correlations in training dynamics.

2.  **Controlled Experimental Training**
    *   Trained custom models.
    *   **Scale:** Up to **100 billion tokens**.
    *   Purpose: To validate causal links identified in phase one and test specific strategic interventions.

---

## Contributions

*   **Mechanistic Insight:** Provided the first clear mechanistic insight into quantization robustness by identifying the critical role of **learning rate decay schedules**.
*   **Reframing Scale:** Reframed the impact of scale by demonstrating that larger datasets do not necessarily degrade quantization performance.
*   **Actionable Guidelines:** Identified specific training configurations and interventions to directly improve post-training quantization quality.

---

## Technical Details

**Algorithms & Metrics**
*   **Primary PTQ Algorithm:** GPTQ (findings extend to AWQ and BitsAndBytes).
*   **Precision Levels:** Analyzed 3-bit and 4-bit.
*   **Error Definitions:**
    *   *Weight Error:* $||W - \hat{W}||$
    *   *Reconstruction Error:* $||XW^T - X\hat{W}^T||$

**Core Variables & Schedules**
*   **Learning Rate (LR) Schedule:** The primary variable under investigation.
    *   *Cosine Decay:* Standard approach.
    *   *Trapezoidal Schedule:* Warmup – Stable – Decay structure.

**Proposed Interventions**
*   **Strategic Interventions:** Modulating the LR to maintain higher rates for longer durations.
*   **Stochastic Weight Averaging (SWA):** Applied to smooth the loss landscape.

**Analysis Scope**
*   Covered **six major open-source LLM families**.
*   Ranging up to 32 billion parameters.
*   Training trajectories covering up to 15 trillion tokens.

---

## Results

**SmolLM3 Experiments**
*   Observed a divergence where validation loss decreased during the LR decay phase while quantization error rose sharply:
    *   **3-bit Error:** Increased from ~4 to **9** units.
    *   **4-bit Error:** Increased from ~0.6 to **1.8** units.

**Impact of Learning Rate**
*   Maintaining larger learning rates for longer periods was found to **reduce** quantization error.

**Impact of Dataset Scale**
*   Confirmed that increasing dataset scale does **not** inherently compromise quantization effectiveness.
*   Previously observed degradations were attributed to the extended learning rate decay phase typically associated with long training runs.

**Validation of Interventions**
*   Controlled experiments verified that optimized LR schedules and weight averaging successfully **lower quantization error**.