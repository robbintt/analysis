# Polychromic Objectives for Reinforcement Learning

*Jubayer Ibn Hamid; Ifdita Hasan Orney; Ellen Xu; Chelsea Finn; Dorsa Sadigh*

---

> ### **Quick Facts**
>
> *   **Quality Score:** 8/10
> *   **References:** 40 Citations
> *   **Core Algorithm:** Polychromic PPO (Set RL)
> *   **Key Domains:** BabyAI, Minigrid, Algorithmic Creativity
> *   **Primary Benefit:** Mitigates mode collapse & enhances test-time compute

---

## **Executive Summary**

### **Problem**
This research addresses the critical challenge of **mode collapse** in standard Reinforcement Learning Fine-Tuning (RLFT). When applying RL to fine-tune pretrained policies, optimization typically converges on a single, high-reward trajectory, effectively discarding other viable behaviors. This homogenization is a significant limitation because:
*   It reduces policy robustness.
*   It impairs generalization to novel environments.
*   It restricts the utility of test-time compute where diverse strategies are essential for effective search and verification.

### **Innovation**
The authors introduce a **"polychromic objective"** formulated within a novel **Set Reinforcement Learning (Set RL)** framework. Instead of optimizing a single path, it optimizes a set of trajectories.
*   **Objective Formulation:** Computes value as the product of the set's mean reward and a diversity term $d(s, \tau_{1:n})$, forcing the agent to maximize both performance and behavioral variance simultaneously.
*   **Algorithmic Adaptation:** Modifies Proximal Policy Optimization (PPO) using **vine sampling** to generate diverse rollouts and modifies the advantage estimator to compute a shared advantage signal against a Monte Carlo baseline.
*   **Stability:** Introduces a per-state KL penalty to maintain training stability.

### **Results**
Empirical validation across BabyAI, Minigrid, and Algorithmic Creativity benchmarks demonstrates substantial improvements in **pass@k metrics**:
*   **BabyAI (GoTo & KeyCorridor):** Polychromic approach maintained Pass@10 rates of **~66%** and **60.5%**, whereas standard RLFT dropped to **<5%**.
*   **Minigrid-DoorKey:** Polychromic achieved **52.1%** Pass@10 vs. baseline's **0%**.
*   **Algorithmic Creativity:** Generated **>8 distinct solutions**, compared to the standard method's single output.

### **Impact**
This work establishes a formal mechanism to preserve behavioral diversity during policy fine-tuning, directly linking it to robustness and generalization. By preventing premature convergence, the approach enhances exploration and facilitates test-time compute scaling, paving the way for AI systems that leverage increased inference computation to solve complex problems.

---

## **Key Findings**

*   **Mode Collapse Mitigation:** The proposed method effectively counters the mode collapse common in standard RLFT, maintaining a diverse set of strategies rather than converging on a single path.
*   **Superior Generalization:** Experiments on BabyAI, Minigrid, and Algorithmic Creativity benchmarks show improved success rates and better generalization under perturbations.
*   **High Strategy Coverage:** The policy achieves significantly higher success rates in pass@$k$ experiments, demonstrating broader behavioral coverage.
*   **Exploration Efficacy:** The method enhances exploration, facilitating capability expansion and amplifying test-time compute scaling.

---

## **Methodology**

The researchers introduce a 'polychromic objective' for policy gradient methods designed to enforce the exploration and refinement of diverse generations.

*   **Set-Based Optimization:** Adapts the reinforcement learning paradigm to optimize a set of trajectories simultaneously rather than a single path.
*   **PPO Adaptation:** Modifies standard Proximal Policy Optimization (PPO) to align with the new objective.
*   **Vine Sampling:** Implements vine sampling techniques to collect diverse on-policy rollouts from intermediate states, ensuring a wide variety of behaviors are sampled.
*   **Modified Advantage Calculation:** Adjusts the advantage function calculation to reflect advantages specifically under the polychromic objective.

---

## **Technical Details**

The paper proposes **Set Reinforcement Learning (Set RL)**, optimizing for distributional diversity and reward.

| Component | Description |
| :--- | :--- |
| **Optimization Goal** | Optimizes a set of trajectories ($\tau_{1:n}$) to maximize $E_{\tau_{1:n} \sim \pi_\theta}[f(s_0, \tau_{1:n})]$. |
| **Objective Function** | The Polychromic objective is defined as:<br>$f_{poly}(s, \tau_{1:n}) := \frac{1}{n} \sum_{i=1}^n R(\tau_i) \cdot d(s, \tau_{1:n})$<br>*(Combines mean reward and diversity multiplicatively)*. |
| **Sampling Strategy** | **Vine Sampling:** Generates trajectory sets rooted from intermediate rollout states to ensure diversity. |
| **Advantage Estimation** | Assigns a **shared advantage signal** to all trajectories in a set (calculated against a Monte Carlo baseline). Non-rollout states rely on standard GAE. |
| **Stability Mechanism** | Utilizes a **per-state KL penalty** to maintain training stability during optimization. |

---

## **Contributions**

*   **Polychromic Objective:** Introduction of a formal objective function designed to preserve behavioral diversity during the fine-tuning of pretrained policies.
*   **Algorithm Development:** A practical adaptation of PPO that integrates vine sampling and modified advantage estimators to optimize specifically for diversity.
*   **Empirical Validation:** Comprehensive testing across BabyAI, Minigrid, and Algorithmic Creativity domains, explicitly linking the maintenance of diversity to improved robustness, generalization, and test-time compute efficiency.

---

## **Results**

Benchmarks tested include **BabyAI**, **Minigrid**, and **Algorithmic Creativity**.

*   **Success Rates:** The method achieves improved success rates and better generalization under perturbations compared to standard RLFT.
*   **Pass@$k$ Performance:** Demonstrates higher strategy coverage in pass@$k$ experiments. Standard RLFT performance degrades with larger $k$, whereas the polychromic method maintains high coverage.
*   **Diversity in Creativity:** In Algorithmic Creativity tasks, the method maintained diversity, whereas standard methods typically converged on a single output.
*   **Test-Time Compute:** By maintaining diversity, the approach positively amplifies test-time compute scaling, allowing agents to explore more solutions during inference.