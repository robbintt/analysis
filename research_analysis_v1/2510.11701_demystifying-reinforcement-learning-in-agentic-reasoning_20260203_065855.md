---
title: Demystifying Reinforcement Learning in Agentic Reasoning
arxiv_id: '2510.11701'
source_url: https://arxiv.org/abs/2510.11701
generated_at: '2026-02-03T06:58:55'
quality_score: 9
citation_count: 24
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Demystifying Reinforcement Learning in Agentic Reasoning

*Zhaochen Yu; Ling Yang; Jiaru Zou; Shuicheng Yan; Mengdi Wang*

---

### ðŸ“Š Quick Facts & Key Metrics

| Metric Category | Details |
| :--- | :--- |
| **Model Efficiency** | 4B parameter models outperformed 32B parameter models using optimized recipes. |
| **Performance Delta** | Real data achieved **+26.14%** improvement over synthetic data on AIME 2025 (Avg@32). |
| **Key Benchmarks** | AIME2024, AIME2025, GPQA-Diamond, LiveCodeBench-v6. |
| **Algorithm** | Group Relative Policy Optimization (GRPO) with exploration-centric tuning. |
| **Paper Quality** | **9/10** |

---

## Executive Summary

This paper addresses the challenge of effectively applying Reinforcement Learning (RL) to agentic reasoningâ€”systems where Large Language Models (LLMs) utilize external tools to solve complex tasks. While agentic reasoning is critical for advancing AI capabilities in mathematics and coding, existing RL training methodologies are often inefficient, unstable, and poorly understood. Reliance on low-quality synthetic data and suboptimal exploration strategies has resulted in high computational costs with diminishing returns. This study aims to demystify the design principles of Agentic RL, establishing why current methods often fail and identifying the specific practices necessary to build robust, efficient tool-using agents.

The authors introduce a comprehensive framework that optimizes Agentic RL across three dimensions: **Data**, **Algorithm**, and **Reasoning Mode**. Technically, the study replaces standard "stitched" synthetic trajectories with real end-to-end tool-use trajectories for Supervised Fine-Tuning (SFT), providing a significantly stronger initialization. Crucially, the authors emphasize that high-diversity, model-aware datasets are essential for sustaining exploration during the RL phase. The algorithm incorporates exploration-centric techniques within a Group Relative Policy Optimization (GRPO) framework, utilizing a "Clip higher" reward strategy to encourage positive behaviors and overlong reward shaping to penalize inefficiency while maintaining policy entropy. Additionally, the authors implement a deliberative reasoning strategy that explicitly minimizes tool calls and verbose self-reasoning, favoring concise, efficient tool interaction over frequency.

The proposed methodology was validated on four rigorous benchmarksâ€”**AIME2024, AIME2025, GPQA-Diamond, and LiveCodeBench-v6**â€”demonstrating substantial performance gains. In experiments with the Qwen3-4B model on the AIME 2025 benchmark, the use of real end-to-end data achieved an average@32 score of **29.79%**, a dramatic improvement over the synthetic data baseline of 3.65% (+26.14% delta). Further metrics showed a Pass@32 of 72.88% compared to 22.22%, and a stability (maj@32) score of 45.82% versus 0.10%.

This research significantly influences the field by clarifying the optimal design principles for training agentic LLMs, providing a practical baseline and "recipes" for future research. The study's most profound implication is its demonstration of efficiency: with superior data initialization and exploration-centric algorithms, small-scale models can achieve state-of-the-art agentic reasoning capabilities. This challenges the reliance on massive model sizes and suggests that future advancements in AI reasoning may depend more on data quality and training strategy than on parameter count.

---

## Key Findings

*   **Superior Data Initialization:** Replacing stitched synthetic trajectories with **real end-to-end tool-use trajectories** provides a significantly stronger Supervised Fine-Tuning (SFT) initialization, while high-diversity, model-aware datasets are critical for sustaining exploration and boosting RL performance.
*   **Exploration-Centric Algorithms:** Incorporating exploration-friendly techniquesâ€”such as **clipping rewards higher**, utilizing **overlong reward shaping**, and maintaining adequate **policy entropy**â€”is crucial for improving training efficiency.
*   **Deliberative Reasoning Strategy:** Adopting a deliberative strategy that **minimizes tool calls** outperforms approaches relying on frequent tool usage or verbose self-reasoning, resulting in better tool efficiency and higher final accuracy.
*   **Model Size Efficiency:** The identified practices enable **smaller models (4B parameters)** to achieve superior agentic reasoning performance compared to much larger models (32B parameters).

---

## Methodology

The researchers conducted a comprehensive and systematic investigation into Reinforcement Learning (RL) within agentic reasoning, structured around three primary dimensions:

1.  **Data:** Analyzed the impact of trajectory types (synthetic vs. real) and the characteristics of training datasets.
2.  **Algorithm:** Evaluated specific training techniques designed to enhance exploration and efficiency.
3.  **Reasoning Mode:** Compared different reasoning strategies (e.g., deliberative vs. verbose).

**Validation:** The methodology was validated by evaluating performance on four challenging benchmarks:
*   AIME2024
*   AIME2025
*   GPQA-Diamond
*   LiveCodeBench-v6

---

## Technical Details

### Formalization & Objective
*   **Objective Function:** Formalized Agentic RL to maximize a composite reward while minimizing KL divergence.
*   **Rollout Factors:** Rollouts are factored into agentic reasoning (incorporating tool feedback) and answer generation.

### Optimization Strategy (GRPO)
*   **Core Algorithm:** Group Relative Policy Optimization (GRPO).
*   **Loss Aggregation:**
    *   *AggTok:* Token-level aggregation.
    *   *AggSeq:* Sequence-level aggregation.
*   **Exploration Techniques:**
    *   **Clip Higher:** A strategy to clip rewards higher to encourage positive exploration.
    *   **Entropy Maintenance:** Ensuring adequate policy entropy to prevent premature convergence.

### Reward Shaping Mechanisms
*   **Tool/Outcome Composite Reward (TCR):** Accuracy adjusted by tool count to encourage efficiency.
*   **Overlong Reward Shaping (SCR):** Penalty applied for long outputs to ensure conciseness.
*   **Training Recipes:** Includes GRPO-TCR, GRPO-SCR, and baseline GRPO-T.

### Data Strategy
*   **Real End-to-End:** Authentic tool-use trajectories.
*   **Synthetic Stitch-Style:** Artificially constructed trajectories (identified as suboptimal).

---

## Results

Experiments conducted on Qwen3-4B and Qwen2.5-7B models using AIME 2024/2025 benchmarks demonstrated that real end-to-end trajectories significantly outperform synthetic data.

### Performance Breakdown: AIME 2025 (Qwen3-4B)

| Metric | Real End-to-End Data | Synthetic Data | Delta |
| :--- | :--- | :--- | :--- |
| **Average@32** | **29.79%** | 3.65% | **+26.14%** |
| **Pass@32** | **72.88%** | 22.22% | +50.66% |
| **Stability (maj@32)** | **45.82%** | 0.10% | +45.72% |

**Additional Observations:**
*   **Qwen2.5-7B Trends:** Showed similar improvements, with Real data achieving 18.24% average@32 compared to 5.21% for Synthetic.
*   **Model Efficiency:** Real data provided stronger initialization and stability, enabling smaller models (4B) to surpass larger models (32B) trained on synthetic data.
*   **Cross-Domain Validation:** Significant gains were also observed on GPQA-Diamond and LiveCodeBench-v6, confirming robustness across coding and reasoning tasks.

---

## Contributions

*   **Demystification of Agentic RL:** The study clarifies the optimal design principles and practical recipes for applying RL to agentic reasoning, establishing a practical baseline for future research.
*   **High-Quality Resources:** The authors contribute a high-quality, real end-to-end agentic SFT dataset and a corresponding high-quality RL dataset to the community.
*   **Empirical Validation:** The work demonstrates that the proposed insights effectively boost the agentic reasoning abilities of LLMs across complex mathematical and coding benchmarks.
*   **Efficiency Breakthrough:** The research establishes that with the right data and training recipes, small-scale models (4B) can outperform large-scale models (32B) in agentic tasks.

---
**Paper Quality Score:** 9/10 | **References:** 24 citations