---
title: Accelerating Large Language Models through Partially Linear Feed-Forward Network
arxiv_id: '2501.10054'
source_url: https://arxiv.org/abs/2501.10054
generated_at: '2026-02-06T02:55:21'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Accelerating Large Language Models through Partially Linear Feed-Forward Network

*Gansen Hu; Zhaoguo Wang; Jinglin Wei; Wei Huang; Haibo Chen*

---

> **ðŸ“Š Quick Facts**
> * **Parameter Reduction:** 80% reduction in FFN parameters
> * **Inference Speedup:** 1.6x (vLLM) / 1.4x (HuggingFace)
> * **Accuracy Trade-off:** 10.9% (controlled)
> * **Accuracy Gain vs. SOTA:** Up to 65% higher than pruning methods
> * **Target Model:** Falcon-7B (tested on NVIDIA RTX 4090)
> * **Quality Score:** 9/10

---

## Executive Summary

Large Language Models (LLMs) are computationally expensive, primarily due to the massive size of their Feed-Forward Networks (FFNs), which constitute approximately **67% to 80%** of total model parameters. While reducing these parameters is critical for efficient deployment, existing methods like high-ratio pruning often result in catastrophic accuracy loss, rendering compressed models unusable. Furthermore, FFN input/output operations account for the majority of inference latency, creating a significant bottleneck in real-time serving systems.

This paper addresses the challenge of achieving substantial parameter reduction and inference acceleration without compromising the model's semantic integrity. The authors introduce **TARDIS**, a novel compression framework that adapts compiler optimization techniquesâ€”specifically constant foldingâ€”to neural network architectures.

The core technical innovation is "Partial Linear Approximation," which overcomes the incompatibility of non-linear activation functions (e.g., GELU, SiLU) with constant folding. By analyzing input distributions, TARDIS determines that activation inputs frequently fall within narrow ranges; within these ranges, non-linear functions are approximated as linear, allowing two FFN matrices to be merged into a single compact matrix. To preserve accuracy for outlier inputs, a "Dynamic Fallback Mechanism" utilizes an online predictor to detect anomalies and dynamically revert to the original non-linear computations.

In experiments utilizing a Falcon-7B model on an NVIDIA RTX 4090, TARDIS achieved an **80% reduction in FFN parameters** while maintaining a manageable accuracy trade-off of 10.9%. Crucially, these theoretical parameter reductions translated into tangible inference performance gains: TARDIS realized a **1.6x end-to-end speedup** using the vLLM inference engine and a **1.4x speedup** with the standard HuggingFace implementation.

This research establishes a new optimization paradigm for LLM compression by successfully bridging compiler theory with deep learning architectures. The validation within industry-standard serving systems like vLLM confirms that the method is not merely theoretical but offers immediate, deployable efficiency gains.

---

## Key Findings

*   **High Parameter Reduction:** TARDIS achieves an **80% reduction** in parameters specifically within the feed-forward networks of Large Language Models.
*   **Superior Accuracy Retention:** The method significantly outperforms state-of-the-art pruning techniques (Wanda and RIA), delivering up to **65% higher accuracy** under high compression ratios.
*   **Inference Acceleration:** In practical deployments using a 7B model, TARDIS achieves a **1.6x end-to-end inference speedup** with vLLM and a **1.4x speedup** with the HuggingFace implementation.
*   **Manageable Trade-off:** These performance and efficiency gains are achieved with a controlled accuracy trade-off of **10.9%**.

---

## Methodology

The proposed method, **TARDIS**, draws inspiration from compiler optimization techniques, specifically constant folding. The methodology consists of two primary components:

1.  **Partial Linear Approximation:**
    This technique is designed to overcome the incompatibility of complex non-linear activation functions (like GELU) with constant folding. Instead of applying linear approximation globally, TARDIS approximates these functions as linear *only within frequently occurring input ranges*.

2.  **Dynamic Fallback Mechanism:**
    To ensure model integrity, the system employs an online predictor to detect outlier inputs that fall outside these frequent ranges. Upon detection, the system dynamically reverts to the original non-linear computations, preserving the model's accuracy on edge cases.

---

## Technical Details

**Core Concept**
TARDIS adapts the compiler optimization technique of 'constant folding' to LLM compression by simplifying the Feed-Forward Network (FFN) structure. It approximates non-linear activation functions (e.g., GELU, SiLU) with linear ones to merge two FFN matrices into a single compact matrix.

**Theoretical Limits**
*   Theoretically enables up to an **87.5% reduction** in FFN parameters.
*   Based on the observation that inputs to activation functions are concentrated in a narrow range.

**System Analysis**
*   **Parameter Load:** FFN blocks account for **67% to 80%** of total parameters.
*   **Inference Latency:** FFN I/O consumes **78.2%** of total inference time.

**Experimental Setup**
*   **Hardware:** NVIDIA RTX 4090
*   **Models:** Falcon-7B
*   **Dataset:** SharedGPT

---

## Results

**Performance Metrics**
*   **Compression:** Achieved an 80% reduction in parameters within FFN blocks.
*   **Speed:**
    *   **1.6x** end-to-end speedup using the vLLM inference engine.
    *   **1.4x** speedup using HuggingFace implementation on a 7B model.
*   **Accuracy:** Maintained a controlled accuracy trade-off of 10.9%.

**Comparative Analysis**
*   **vs. Pruning:** Provided up to **65% higher accuracy** than SOTA pruning techniques (Wanda and RIA).
*   **vs. Naive Approximation:** A naive linear approximation resulted in a 75% accuracy drop. TARDIS significantly mitigated this loss.
*   **vs. Traditional Pruning:** Traditional pruning at 80% compression caused up to 70% accuracy loss.

---

## Contributions

*   **Novel Optimization Perspective:** Introduces a new approach to model compression by applying compiler optimization concepts (constant folding) to neural networks, effectively addressing the challenge of non-linear activations.
*   **Robust Compression Strategy:** Provides a solution that mitigates the significant accuracy degradation typically associated with high-ratio compression methods like pruning.
*   **Practical Deployment Viability:** Validates the efficiency of the approach in real-world serving systems (vLLM and HuggingFace), demonstrating that theoretical parameter reduction translates to tangible inference speedups.

---

**References:** 40 citations