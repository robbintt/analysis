# Reflective Verbal Reward Design for Pluralistic Alignment

*Carter Blair; Kate Larson; Edith Law*

---

> ### ðŸ“Š Quick Facts
> *   **Methodology:** Interactive-Reflective Dialogue Alignment (IRDA)
> *   **Performance:** 9-12% improvement in evaluation accuracy
> *   **Core Advantage:** Higher sample efficiency than supervised learning
> *   **Participants:** 30 (Study demographics detailed below)
> *   **Implementation:** gpt-3.5
> *   **Environments:** Apple Farming (6x6 grid), Moral Machine
> *   **Quality Score:** 9/10

---

## Executive Summary

Current Reinforcement Learning from Human Feedback (RLHF) approaches predominantly rely on aggregating human preferences into a single, static reward model. This methodology presents a critical flaw in the context of **pluralistic alignment**: by optimizing for the average reward, these models inevitably suppress minority viewpoints and fail to capture the heterogeneity of human values. As AI systems become more autonomous, the inability to align agents with individualized user ethics creates a misalignment between the agentâ€™s behavior and the nuanced moral frameworks of diverse users.

To address this, the authors introduce **Interactive-Reflective Dialogue Alignment (IRDA)**, a framework that shifts from aggregate rewards to individualized "verbal reward models" derived from natural language discourse.

Technically, IRDA employs a **Dual-Loop Framework**:
1.  **The Preference Construction Loop** selects diverse trajectory centroids (using k-means clustering) and utilizes a guiding LLM to facilitate a reflective dialogue, prompting users to critique agent behaviors and refine their mental models.
2.  **The Uncertainty Reduction Loop** queries trajectories where the verbal reward model is least confident, using the compiled conversation history to evaluate alignment.

In evaluations, IRDA demonstrated superior performance to baseline methods. The proposed reflective verbal reward models achieved a **9-12% improvement in evaluation accuracy** compared to non-reflective verbal models. This work validates a scalable pathway toward pluralistic agent behavior, establishing alignment as a collaborative, ongoing process rather than a static standard.

---

## Key Findings

*   **Improved Accuracy:** The proposed method achieved a **9-12% improvement** in evaluation accuracy compared to non-reflective verbal reward models.
*   **Sample Efficiency:** Demonstrated higher sample efficiency than traditional supervised learning approaches.
*   **Minority Preference Suppression:** The study validated that aggregating human feedback disproportionately suppresses minority preferences, highlighting the necessity for individualized approaches.
*   **User Agency:** User studies confirmed that guided reflective dialogues are an effective mechanism for users to critique agent behavior and actively construct their own preferences.

---

## Technical Details

### Framework: Interactive-Reflective Dialogue Alignment (IRDA)
The system utilizes a **Dual-Loop Framework** to process user input and refine reward models.

#### 1. Preference Construction Loop
*   **Input:** Diversity-based trajectory pool (`$T_D$`).
*   **Process:**
    *   Extracts feature vectors and partitions into `$k$` clusters using k-means.
    *   Selects centroid representatives for user review.
    *   Users provide qualitative feedback.
    *   An LLM (`$G$`) generates hypotheses and alternatives to refine the user's mental model (`$M_u$`) until stabilization.

#### 2. Uncertainty Reduction Loop
*   **Input:** Uncertainty-based trajectory pool (`$T_U$`).
*   **Process:**
    *   Utilizes a verbal reward model (`$f_{LLM}$`) to compute alignment and misalignment probabilities.
    *   Context is derived from conversation history, ASCII-encoded trajectories, and environment descriptions.

### Environments Configuration
*   **Apple Farming:** 6x6 grid environment.
*   **Moral Machine:** Configuration of ethical dilemmas.

### Implementation Stack
*   **Model:** gpt-3.5

---

## Methodology

The methodology involves using a language model to guide users through a structured reflective dialogue where they critique specific agent behaviors. These reflections and examples are compiled into a **personalized dialogue history**, which serves as the context for a separate 'verbal reward model.' This reward model evaluates new trajectories based on the user's articulated values. The system was evaluated in a study with 30 participants against non-reflective verbal models and supervised learning baselines.

---

## Results & Demographics

### Performance Metrics
*   **IRDA vs. Non-Reflective Models:** 9-12% higher evaluation accuracy.
*   **IRDA vs. Supervised Learning:** Higher sample efficiency demonstrated.

### Participant Demographics

| Study | N | Mean Age | Gender (M/F) | RL Familiarity (1-5) |
| :--- | :---: | :---: | :---: | :---: |
| **Study 1** | 21 | 23.86 | 7 / 14 | 2.48 |
| **Study 2** | 9 | 25.66 | 6 / 3 | 3.55 |

---

## Contributions

The research makes three primary contributions to the field of AI alignment:

1.  **Pluralistic Alignment Framework:** Introduces a framework shifting from single aggregated reward models to individualized models to accommodate heterogeneous human values.
2.  **Verbal Reward Model Design:** Presents a novel design where reward functions are derived directly from natural language discourse and critique.
3.  **Self-Reflection Mechanism:** Proposes a mechanism for using language models to facilitate user self-reflection, thereby extracting high-fidelity preference data that might otherwise be lost in aggregated scoring.

---

*References: 10 citations*