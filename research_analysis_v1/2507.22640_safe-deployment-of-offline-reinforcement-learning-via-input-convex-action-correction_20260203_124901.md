---
title: Safe Deployment of Offline Reinforcement Learning via Input Convex Action Correction
arxiv_id: '2507.2264'
source_url: https://arxiv.org/abs/2507.22640
generated_at: '2026-02-03T12:49:01'
quality_score: 7
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Safe Deployment of Offline Reinforcement Learning via Input Convex Action Correction

*Alex Durkin; Jasper Stolte; Matthew Jones; Raghuraman Pitchumani; Bei Li; Christian Michler; Mehmet Mercangöz*

---

> ### **Quick Facts**
> * **Domain:** Chemical Process Control / Offline Reinforcement Learning
> * **Core Model:** Partially Input Convex Neural Networks (PICNNs)
> * **Environment:** Exothermic Polymerisation CSTR (Gymnasium-compatible)
> * **Deployment:** Zero-shot (Safety Layer)
> * **Quality Score:** 7/10
> * **References:** 40 Citations

---

## Executive Summary

**Problem**
This research addresses the critical challenge of deploying Offline Reinforcement Learning (RL) agents in high-stakes chemical process control environments, specifically within an exothermic polymerisation Continuous Stirred-Tank Reactor (CSTR). While Offline RL offers the potential to optimize complex control systems without the risks of online exploration, standard algorithms such as Behavior Cloning and Implicit Q-learning face significant limitations in industrial settings. These agents frequently exhibit substantial steady-state offsets and degraded performance near operational setpoints, resulting in unreliable control for processes where stability and strict adherence to safety constraints are paramount.

**Innovation**
The authors introduce a deployment-time safety layer utilizing Partially Input Convex Neural Networks (PICNNs) to mitigate these risks. This mechanism functions as a state-conditioned cost model that is mathematically convex with respect to control actions. By wrapping a pre-trained offline RL agent, the safety layer performs differentiable, gradient-based action correction in real-time, descending the convex cost surface to modify potentially unsafe actions before execution. Crucially, this approach requires no explicit constraint sets, model retraining, or online environment interaction, enabling zero-shot deployment that enforces safety constraints through the network's architectural properties.

**Results**
The proposed method was evaluated using a Gymnasium-compatible CSTR simulation across three distinct industrial scenarios: Startup, Grade Change Down, and Grade Change Up. While standard offline RL agents (Behavior Cloning and Implicit Q-learning) exhibited significant steady-state offsets and instability—often failing to track setpoints or violating safety limits—the agents augmented with the PICNN-based correction layer maintained stability throughout all tests. The proposed system surpassed the performance of traditional Proportional-Integral (PI) control strategies, successfully executing real-time corrections that kept reactor temperatures within strict safety boundaries (e.g., preventing overheating beyond critical limits) and effectively reducing the steady-state error to near zero across all evaluation episodes.

**Impact**
This work advances the field of industrial RL by providing a viable pathway to integrate advanced learning algorithms with safety-critical control systems. It empirically demonstrates that interpretable, safety-aware corrections can effectively bridge the gap between theoretical RL performance and the rigorous safety requirements of chemical process control. Furthermore, the study contributes a specialized, reproducible simulation environment and dataset to the research community, establishing a strong foundation for future development in safe, offline reinforcement learning for high-stakes engineering applications.

---

## Key Findings

*   **Performance Limitations of Standard Offline RL:** Standard algorithms, specifically Behavior Cloning and Implicit Q-learning, struggle in chemical process control, exhibiting steady-state offsets and degraded performance near operational setpoints.
*   **Efficacy of PICNN Safety Layer:** The proposed deployment-time safety layer, utilizing Input Convex Neural Networks (PICNNs), successfully corrects policy actions in real-time by descending a convex, state-conditioned cost surface.
*   **Superior Stability:** Offline RL agents augmented with the convex action correction mechanism outperform traditional control approaches and maintain stability across all tested industrial scenarios (startup, grade change down, and grade change up).
*   **Zero-Shot Deployment:** The method enables safe, real-time correction of actions without requiring model retraining or interaction with the live environment during deployment.

---

## Technical Details

The proposed approach leverages the mathematical properties of neural networks to ensure safety without explicit constraint definitions.

*   **Core Architecture:** Uses **Partially Input Convex Neural Networks (PICNNs)** as a state-conditioned cost model.
*   **Convexity Properties:** The model is designed to be convex with respect to control actions while remaining expressive with respect to system states.
*   **Safety Mechanism:** Functions as a deployment-time safety layer wrapping a pre-trained offline RL agent.
*   **Optimization Strategy:** Uses gradient descent on the PICNN cost surface to correct unsafe actions.
*   **Operational Requirements:** Requires no explicit constraint sets or online learning (zero-shot deployment).
*   **Test Environment:** Evaluated on a Gymnasium-compatible Continuous Stirred-Tank Reactor (CSTR) simulation.

---

## Methodology

1.  **Environment Development:** A Gymnasium-compatible simulation environment was developed to model an exothermic polymerisation Continuous Stirred-Tank Reactor (CSTR) featuring nonlinear dynamics and operational constraints.
2.  **Dataset Generation:** Reproducible offline datasets were generated using proportional-integral (PI) controllers with randomized tunings to ensure diverse data coverage.
3.  **Baseline Assessment:** The study assessed Behavior Cloning and Implicit Q-learning to identify and document the performance limitations of standard offline agents in this context.
4.  **Safety Layer Implementation:** A novel deployment-time safety layer was implemented using PICNNs as learned cost models.
5.  **Real-Time Correction:** The system performs gradient-based action correction in a differentiable and real-time manner during the deployment phase.

---

## Results

*   **Test Scenarios:** The method was evaluated on **Startup**, **Grade Change Down**, and **Grade Change Up** scenarios.
*   **Stability:** It maintained stability across all tested industrial scenarios.
*   **Comparative Performance:** The system outperformed traditional control approaches and standard offline RL agents (Behavior Cloning and Implicit Q-learning), which exhibited steady-state offsets and degraded performance.
*   **Real-Time Capability:** The system successfully enabled safe, real-time correction of actions.
*   **Data Sources:** Offline training datasets were generated using PI controllers with diverse tuning parameters.

---

## Contributions

*   **Simulation Environment:** Introduction of a specialized, Gymnasium-compatible simulation environment for an exothermic polymerisation CSTR, accompanied by reproducible offline datasets derived from randomized PI controllers.
*   **Safety Layer Development:** Development of a unique deployment-time safety layer that uses Input Convex Neural Networks (PICNNs) to enforce safety through differentiable, gradient-based action correction.
*   **Empirical Validation:** Provision of empirical evidence demonstrating that integrating offline RL with interpretable, safety-aware corrections is a viable strategy for high-stakes chemical process control.