---
title: 'CMI-MTL: Cross-Mamba interaction based multi-task learning for medical visual
  question answering'
arxiv_id: '2511.01357'
source_url: https://arxiv.org/abs/2511.01357
generated_at: '2026-02-04T16:01:35'
quality_score: 9
citation_count: 17
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# CMI-MTL: Cross-Mamba interaction based multi-task learning for medical visual question answering

*Qiangguo Jin; Xianyao Zheng; Hui Cui; Changming Sun; Yuqi Fang; Cong Cong; Ran Su; Leyi Wei; Ping Xuan; Junbo Wang*

---

### ðŸ“Š Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Framework** | CMI-MTL (Cross-Mamba interaction based Multi-Task Learning) |
| **Core Mechanism** | Cross-Mamba interaction (State Space Models) |
| **Datasets Evaluated** | VQA-RAD, SLAKE, OVQA |
| **VQA-RAD Accuracy** | 84.37% |
| **SLAKE Accuracy** | 86.28% |
| **OVQA Accuracy** | 81.42% |
| **Answer Type** | Free-form Answer Generation |

---

## Executive Summary

Medical Visual Question Answering (Med-VQA) presents a complex challenge requiring precise semantic alignment between medical imaging and clinical language. Existing approaches are hindered by two primary limitations: a reliance on fixed-set classification strategies, which restricts systems to predefined answers and fails to capture the nuance of free-form clinical responses, and the use of standard self-attention mechanisms. These traditional attention methods often struggle with the computational efficiency required for effective cross-modal alignment between high-resolution visual features and dense textual descriptions.

The authors propose **CMI-MTL**, a novel framework that replaces standard self-attention with a **Cross-Mamba interaction mechanism** to enhance cross-modal semantic alignment. The architecture is built upon three interconnected modules:
1.  **Fine-grained Visual-Text feature alignment (FVTA)** for extracting relevant regions.
2.  **Cross-modal Interleaved Feature Representation (CIFR)** utilizing an interleaved representation strategy with Mamba blocks.
3.  **Free-form Answer-Enhanced multi-task learning (FFAE)** to shift the paradigm from classification to generation.

The CMI-MTL framework was rigorously evaluated against existing state-of-the-art methods across three standard Med-VQA datasets, demonstrating significant performance gains. Furthermore, ablation studies quantified the contribution of individual modules, revealing that the removal of key components resulted in a notable decrease in performance, empirically validating the necessity of each module. This research significantly advances the field by integrating State Space Models (Mamba) into multi-modal learning, offering a computationally efficient alternative to transformer-based architectures.

---

## Key Findings

*   **Superior Performance:** The proposed CMI-MTL framework outperforms existing state-of-the-art methods across VQA-RAD, SLAKE, and OVQA datasets.
*   **Effective Cross-Modal Alignment:** The framework successfully addresses limitations of self-attention methods by handling cross-modal semantic alignments between vision and language.
*   **Adaptability to Free-Form Answers:** Unlike traditional classification-based methods, CMI-MTL adapts to the diversity of free-form answers and captures detailed semantic information.
*   **Validation of Effectiveness:** Interpretability experiments and ablation studies were conducted to empirically prove the effectiveness of the proposed modules.

---

## Methodology

The research introduces the **Cross-Mamba Interaction based Multi-Task Learning (CMI-MTL)** framework. It utilizes three interconnected modules designed to solve specific challenges in medical VQA:

*   **Fine-grained Visual-Text feature alignment (FVTA):**
    *   **Function:** Extracts relevant regions within image-text pairs.
    *   **Goal:** Ensure that visual features correspond accurately to textual context.

*   **Cross-modal Interleaved Feature Representation (CIFR):**
    *   **Function:** Captures cross-modal sequential interactions using an interleaved representation strategy.
    *   **Mechanism:** Utilizes Mamba blocks to process sequential data efficiently.

*   **Free-form Answer-Enhanced multi-task learning (FFAE):**
    *   **Function:** Employs a multi-task learning approach to leverage auxiliary knowledge.
    *   **Goal:** Generates free-form answers rather than selecting from a fixed classification set.

---

## Contributions

*   **Novel Framework Architecture:** Development of CMI-MTL, a framework integrating Mamba-based interactions tailored specifically for the medical domain.
*   **Resolution of Classification Limitations:** A shift from classification-based approaches to a free-form answer-enhanced strategy to better handle semantic complexity.
*   **Comprehensive Modular Design:** The proposal and integration of FVTA, CIFR, and FFAE modules to address specific challenges in medical VQA.
*   **Reproducibility and Benchmarking:** Provision of publicly available code and the establishment of new performance benchmarks supported by interpretability analysis.

---

## Technical Details

*   **Framework Name:** CMI-MTL (Cross-Mamba interaction based Multi-Task Learning).
*   **Core Innovation:** Utilizes a **Cross-Mamba interaction mechanism** for cross-modal semantic alignment between vision and language, contrasting with standard self-attention methods.
*   **Learning Strategy:** Employs a multi-task learning strategy to incorporate auxiliary knowledge.
*   **Output Generation:** Generates free-form answers for Medical VQA rather than using a fixed-set classification approach.

---

## Results

The framework was evaluated on three major datasets, achieving state-of-the-art performance:

*   **VQA-RAD Dataset:** Achieved an accuracy of **84.37%**, surpassing previous top methods.
*   **SLAKE Dataset:** Achieved an accuracy of **86.28%**.
*   **OVQA Benchmark:** Achieved an accuracy of **81.42%**, establishing new performance standards.

Ablation studies confirmed that the removal of key components such as FVTA or CIFR resulted in a notable decrease in performance, validating the necessity of the integrated design.

---

**Paper Quality Score:** 9/10  
**References:** 17 citations