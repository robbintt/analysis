# Can you see how I learn? Human observers' inferences about Reinforcement Learning agents' learning processes

*Bernhard Hilpert; Muhan Hou; Kim Baraka; Joost Broekens*

---

> ### ðŸ“Š Quick Facts
>
> *   **Study Design:** Two-stage (Exploratory $N=9$; Confirmatory $N=34$)
> *   **Total Responses:** 816 responses analyzed in confirmatory study
> *   **Environment:** OpenAI Gym 'FrozenLake' (4x4 & 8x8 grids)
> *   **Algorithms:** Tabular Q-learning & Function Approximation
> *   **Themes Identified:** 4 (Goals, Knowledge, Decision Making, Learning Mechanisms)
> *   **Quality Score:** 9/10

---

## Executive Summary

As Reinforcement Learning (RL) agents become increasingly prevalent in collaborative environments like human-robot interaction, a significant **transparency gap** persists between the agent's internal operations and the human observer's understanding. Effective collaboration relies on the human partner's ability to accurately interpret an agent's competence, goals, and logic in real-time.

This research bridges this gap by introducing a **data-driven framework** derived from a rigorous two-stage experimental paradigm. Moving from exploratory insights to confirmatory validation, the study utilizes a bottom-up approach combining Grounded Theory and "Think Aloud" protocols. The methodology was tested across diverse contexts, including navigation and manipulation tasks, utilizing both tabular and function approximation algorithms.

Analysis of participant behavior reveals that humans rely on four primary cognitive themes to interpret agents: **Agent Decision Making**, **Agent Knowledge**, **Agent Goals**, and **Learning Mechanisms**. Crucially, these themes are dynamic and interrelated. The study also found that presentation format significantly impacts engagement, with "chunked" video presentations eliciting three times more commentary than full-length videos. Ultimately, this work provides a validated foundation for **cognitively-aligned XAI**, offering designers actionable guidelines to create interpretable systems that align with natural human cognitive processes and foster trust.

---

## Key Findings

*   **Four Thematic Categories:** Human observers interpret RL agent behavior primarily through four themes: Agent Goals, Knowledge, Decision Making, and Learning Mechanisms.
*   **Dynamic Interpretation:** These interpretive themes are not static; they evolve over the course of the interaction and are interrelated with one another.
*   **Paradigm Validation:** A novel observation-based experimental paradigm was successfully validated as a reliable method for assessing human inferences about agent learning across diverse contexts.
*   **Anthropomorphic Projection:** Observers frequently project human-like traits onto agents, attributing memory, strategic logic, and trial-and-error learning processes to them.

---

## Methodology

This research employed a bottom-up approach featuring two distinct experiments designed to move from exploratory insights to confirmatory validation.

### Study 1: Exploratory Phase
*   **Goal:** Identify core themes in human interpretation.
*   **Participants:** Small sample ($N=9$).
*   **Method:** Interview study utilizing a 'Think Aloud' protocol.

### Study 2: Confirmatory Phase
*   **Goal:** Validate findings across broader contexts.
*   **Participants:** Larger-scale study ($N=34$).
*   **Data Volume:** Analyzed 816 responses.
*   **Variables:** Varied conditions across two tasks (navigation and manipulation) and two RL algorithm types (tabular and function approximation).

---

## Technical Details

The exploratory phase utilized a specific technical setup to gather qualitative data:

*   **Framework:** Adapted Grounded Theory.
*   **Protocol:** 'Think Aloud' protocol where participants acted as non-intervening observers.
*   **Stimuli:** Tabular Q-learning agent with Temporal Difference updates.
*   **Environment:** OpenAI Gym's 'FrozenLake' environment (4x4 and 8x8 grids).
*   **Conditions:**
    *   **Full-length:** Continuous video presentation.
    *   **Chunked:** Six shorter video clips.
*   **Participant Demographics:** $N=9$; mixed backgrounds (5 with prior RL exposure, 4 without), balanced across presentation conditions.

---

## Results

The analysis of the exploratory study yielded quantitative data on how participants verbalized their observations:

*   **Total Statements:** 264 statements collected.
*   **Categorization:** 127 statements (~48%) fit into the four primary themes.
*   **Theme Breakdown:**
    1.  **Agent Decision Making:** 50 statements (~19%) â€” *Dominant theme*
    2.  **Agent Knowledge:** 36 statements (~13.6%)
    3.  **Agent Goals:** 23 statements (~8.7%)
    4.  **Agent Learning Mechanisms:** 18 statements (~6.8%)
*   **Presentation Impact:** Participants in the chunked video condition produced approximately **three times as many statements** as those in the full-length condition.

---

## Contributions

*   **Data-Driven Framework:** Establishes a structured, thematic framework defining how humans mentally model and 'make sense' of an RL agent's learning process.
*   **Novel Evaluation Paradigm:** Introduces and validates a new observation-based method for directly measuring human inferences regarding AI learning.
*   **Design Guidelines for XAI:** Provides actionable insights for developing interpretable RL systems, aiming to improve transparency and collaborative feedback in Human-Robot Interaction (HRI).

---

* **Citations:** 40
* **Quality Score:** 9/10