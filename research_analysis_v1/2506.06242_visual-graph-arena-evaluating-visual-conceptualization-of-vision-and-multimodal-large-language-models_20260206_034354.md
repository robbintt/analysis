---
title: 'Visual Graph Arena: Evaluating Visual Conceptualization of Vision and Multimodal
  Large Language Models'
arxiv_id: '2506.06242'
source_url: https://arxiv.org/abs/2506.06242
generated_at: '2026-02-06T03:43:54'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Visual Graph Arena: Evaluating Visual Conceptualization of Vision and Multimodal Large Language Models

*Zahra Babaiee; Peyman M. Kiasari; Daniela Rus; Radu Grosu*

---

> ### ðŸ“Š Quick Facts
> * **Quality Score:** 9/10
> * **Citations:** 40
> * **Human Benchmark:** >90% aggregate accuracy
> * **Top Vision Model:** ConvNeXt (82.4% on Shortest Path)
> * **Top MLLM:** GPT-o1 (67% on Hamiltonian Cycle)
> * **Core Deficiency Identified:** Lack of representation-invariant reasoning

---

## Executive Summary

This research addresses a fundamental deficiency in current Vision and Multimodal Large Language Models (V-MLLMs): the lack of **"conceptualization,"** or the ability to perform representation-invariant reasoning. While models excel at pattern recognition in standard benchmarks, they struggle to recognize that an object remains the same conceptually even when its visual representation changes drastically.

This gap matters because it demonstrates that current AI lacks genuine understanding; instead of reasoning logically, systems rely on superficial statistical correlations, limiting their reliability in complex tasks requiring abstract visual logic.

To isolate and measure this deficiency, the authors introduce the **Visual Graph Arena (VGA)**, a novel evaluation framework that uses graph-based tasks to test visual abstraction. The innovation lies in the decoupling of reasoning from visual perception by employing diverse graph layoutsâ€”such as Kamada-Kawai versus planar representationsâ€”for the same underlying logical structure. By evaluating models on tasks like isomorphism detection, Hamiltonian paths, and cycle identification using purely visual inputs (rather than symbolic encodings), VGA provides a controlled environment to test whether an AI can identify invariant concepts across varying visual forms.

The study highlights a massive disparity between human and AI performance. Human participants achieved over 90% aggregate accuracy (100% on shortest path tasks), whereas AI models struggled significantly. Among vision models, SigLIP achieved 54.4% on easy isomorphism tasks, and ConvNeXt outperformed transformer-based architectures (ViT, Swin-T) with 82.4% accuracy on shortest path tasks. Most MLLMs, including GPT-4o and Claude, failed randomly. Notably, while GPT-o1 showed partial success (67% on Hamiltonian cycles), analysis revealed this was due to "pseudo-intelligent pattern matching" relying on visual cues like leaf nodes rather than logical reasoning, as performance dropped to random levels when these cues were absent.

The significance of this work lies in its empirical evidence that state-of-the-art models engage in superficial mimicry rather than true logical reasoning. By formally defining "conceptualization" and providing a rigorous diagnostic tool, the VGA benchmark challenges the current trajectory of AI research. It shifts the focus from improving benchmark scores on static datasets to developing systems capable of genuine, representation-invariant understanding, thereby setting a new standard for evaluating the cognitive capabilities of future visual architectures.

---

## Key Findings

*   **Significant Performance Gap:** There is a stark disparity in performance between humans (near-perfect accuracy) and AI (significant struggle).
*   **Task-Specific Failures:** Current models demonstrated a total failure in isomorphism detection and limited success in path and cycle detection.
*   **Lack of Genuine Understanding:** Models rely on pseudo-intelligent pattern matching rather than possessing genuine, representation-invariant understanding.
*   **Inability to Abstract Visual Forms:** AI models lack the conceptualization capability required to recognize and reason about the same underlying concept across varying visual forms.

---

## Methodology

The research employed a rigorous three-step approach to isolate and test visual reasoning capabilities:

*   **Dataset Creation:** Introduction of the **Visual Graph Arena (VGA)**, a specialized dataset with six graph-based tasks designed to test visual abstraction.
*   **Visual Variation Control:** Use of diverse graph layouts (e.g., Kamada-Kawai vs. planar) to isolate reasoning capabilities from visual perception. This ensures the AI must understand the concept, not just the specific image.
*   **Benchmarking:** Comparative analysis where state-of-the-art vision models and multimodal LLMs were evaluated against human performance baselines.

---

## Technical Details

### Benchmark Architecture
The Visual Graph Arena (VGA) benchmark evaluates graph-based visual reasoning using visual representations rather than symbolic encodings.

*   **Tasks Included:**
    *   Isomorphism (Easy/Hard)
    *   Hamiltonian Path
    *   Shortest Path
    *   Hamiltonian Cycle
    *   Biggest Chordless Cycle
*   **Evaluation Metrics:** Classification accuracy and confusion matrices.

### Evaluated Models
The study tested a range of architectures using different evaluation protocols:

*   **Vision Models (via best validation accuracy):**
    *   ConvNeXt Base
    *   ViT Base
    *   Swin-T Base
    *   SigLIP
    *   DINov2
*   **Multimodal Large Language Models (via zero-shot or few-shot response accuracy):**
    *   GPT-4o
    *   Claude 3.5 Sonnet
    *   GPT-3 Opus
    *   Google Gemini
    *   GPT-o1

### Key Hypothesis
The study hypothesizes that convolutional architectures (ConvNeXt) may be more effective than transformer-based models (ViT, Swin-T) for capturing spatial concepts in graphs.

---

## Results

**Human Performance**
Human participants achieved over **90%** aggregate accuracy, scoring **100%** on the Shortest Path task.

**Vision Models**
*   **SigLIP:** Achieved 54.4% on Easy Isomorphism (others failed).
*   **ConvNeXt:** Led in Chordless Cycle (36.3%) and Shortest Path (82.4%), consistently outperforming ViT and Swin-T.

**Multimodal LLMs**
*   **General Performance:** Most MLLMs (GPT-4o, Claude, etc.) failed randomly.
*   **GPT-o1:** Showed partial success with **55%** accuracy on Shortest Path and **67%** on Hamiltonian Cycle.
*   **Analysis:** Success attributed to "pseudo-intelligent pattern matching" (e.g., detecting leaf nodes) rather than genuine graph reasoning. Performance dropped to random levels when these visual shortcuts were absent.

---

## Contributions

*   **Definition of a New Problem:** Formally identifies **'conceptualization'**â€”the ability to reason invariant to representationâ€”as a fundamental missing capability in current visual AI systems.
*   **Novel Evaluation Framework:** The Visual Graph Arena (VGA) provides a new benchmark and framework for specifically testing representation-invariant reasoning.
*   **Diagnostic Insight:** Provides critical empirical evidence that current SOTA models engage in superficial pattern matching rather than true logical reasoning.

---

**References:** 40 citations | **Quality Score:** 9/10