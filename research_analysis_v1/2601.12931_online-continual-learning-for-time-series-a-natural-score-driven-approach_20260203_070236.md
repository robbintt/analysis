---
title: 'Online Continual Learning for Time Series: a Natural Score-driven Approach'
arxiv_id: '2601.12931'
source_url: https://arxiv.org/abs/2601.12931
generated_at: '2026-02-03T07:02:36'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Online Continual Learning for Time Series: a Natural Score-driven Approach

*Edoardo Urettini; Daniele Atzeni; Ioanna-Yvonni Tsaknaki; Antonio Carta*

---

## üìä Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Quality Score** | 9/10 |
| **References** | 40 Citations |
| **Core Focus** | Online Continual Learning (OCL) & Time Series |
| **Key Method** | Natural Score-driven Replay (NatSR) |
| **Optimizer** | Natural Gradient Descent (NGD) |

---

## üìã Executive Summary

**Problem**
This research addresses the intersection of Online Continual Learning (OCL) and Online Time Series Forecasting (OTSF), specifically focusing on the challenges of non-stationarity, catastrophic forgetting, and noise in streaming data. Traditional time series models often struggle to adapt to sudden changes in data distribution (regime drift) without erasing previously learned knowledge, while standard neural network optimizers are sensitive to outliers. This problem is critical for real-world applications‚Äîsuch as financial monitoring or IoT sensor analysis‚Äîwhere data arrives sequentially, contains noise, and lacks clear task boundaries, requiring models that are both robust to anomalies and capable of evolving over time.

**Innovation**
The key innovation is the theoretical unification of neural network optimization and parameter filtering, proving that Natural Gradient Descent (NGD) is equivalent to a score-driven method with information-theoretic optimality. Leveraging this insight, the authors introduce Natural Score-driven Replay (NatSR), a novel algorithm that utilizes the inverse Fisher Information Matrix for second-order updates. NatSR enhances robustness by employing a Student's t likelihood function, which ensures bounded parameter updates in the presence of outliers. Furthermore, the method incorporates a dynamic scale heuristic that adjusts the learning rate specifically during regime drifts, alongside a replay buffer that strategically balances plasticity (adaptation) with stability (memory retention).

**Results**
In empirical evaluations across seven datasets in a strictly online, task-free setting, NatSR achieved superior forecasting performance compared to state-of-the-art OCL methods, ranking first on 5 out of 7 benchmarks. The integration of the Student's t likelihood was validated as a critical component for robustness, significantly dampening the impact of outliers on model convergence. Additionally, the dynamic scale heuristic proved effective in handling non-stationarity, allowing the model to rapidly adapt to sudden distribution shifts while maintaining a reduced stability gap compared to baselines.

**Impact**
This work significantly influences the field by establishing a rigorous mathematical bridge between classical time series analysis (score-driven models) and modern deep learning (continual learning). It shifts the focus from architectural complexity to optimization principles, demonstrating that principled, information-theoretic approaches can outperform more complex bespoke solutions. By providing a framework that guarantees convergence (under bounded learning rates) and handles real-world data imperfections, NatSR offers a robust, theoretically grounded foundation for future research in adaptive time series forecasting and lifelong learning systems.

---

## üîë Key Findings

*   **Theoretical Unification:** Neural network optimization can be reframed as a parameter filtering problem, proving that Natural Gradient Descent is a score-driven method with information-theoretic optimality.
*   **Robustness via Bounded Updates:** Combining Natural Gradient Descent with a Student's t likelihood results in a bounded update rule, significantly improving robustness against outliers in time series data.
*   **Superior Performance:** The proposed method, Natural Score-driven Replay (NatSR), achieves stronger forecasting performance compared to more complex state-of-the-art methods.
*   **Adaptation to Drift:** The integration of a dynamic scale heuristic enables fast adaptation specifically during regime drifts (sudden changes in data distribution).

---

## ‚öôÔ∏è Methodology

The authors bridge Online Continual Learning (OCL) and Online Time Series Forecasting (OTSF) by treating neural network optimization as a parameter filtering problem. Their approach utilizes Natural Gradient Descent as the foundational optimizer, employs a Student's t likelihood to induce bounded updates, and introduces the Natural Score-driven Replay (NatSR) algorithm.

**NatSR synthesizes three critical components:**
1.  A robust optimizer (Natural Gradient + Student's t likelihood).
2.  A replay buffer for memory retention.
3.  A dynamic scale heuristic to handle non-stationarity and regime switching.

---

## üìù Contributions

*   **Theoretical Foundation:** Establishes a rigorous theoretical connection between time series analysis and OCL by proving the information-theoretic optimality of Natural Gradient Descent within a score-driven filtering framework.
*   **Robustness Validation:** Identifies and validates the specific combination of Natural Gradient and Student's t likelihood as a mechanism to ensure bounded, robust updates in the presence of outliers.
*   **Novel Algorithm (NatSR):** Proposes a new OCL algorithm specifically tailored for time series that addresses the trade-off between rapid adaptation and long-term memory through dynamic scaling and replay.

---

## üîß Technical Details

The paper reframes online time series forecasting as a parameter filtering problem, linking Score-driven models with Natural Gradient Descent (NGD).

**Core Framework & Algorithm**
*   **NGD as GAS:** It establishes that NGD is a specific instance of the Generalized Autoregressive Score (GAS) model.
*   **NatSR:** The proposed algorithm employs second-order updates using the inverse Fisher Information Matrix.

**Key Mechanisms**
*   **Likelihood Function:** Utilizes a Student's t likelihood for robustness against outliers.
*   **Dynamic Scale:** Implements a heuristic for fast adaptation during regime drifts.
*   **Memory Management:** Uses a replay mechanism to balance plasticity and stability.

**Theoretical Guarantees**
*   **Proposition 4.1:** Demonstrates that the expected weight update reduces KL divergence.
*   **Convergence:** The method converges if the learning rate is bounded ($0 < \eta < 2/c$).

---

## üìà Results

*   **Benchmark Performance:** NatSR achieved superior forecasting performance on **5 out of 7 datasets** compared to state-of-the-art Online Continual Learning methods.
*   **Robustness:** The integration of the Student's t likelihood significantly improved robustness against outliers.
*   **Adaptability:** The dynamic scale heuristic enabled rapid adaptation during regime drifts.
*   **Evaluation Conditions:** The evaluation was conducted in a strictly online, task-free setting, focusing on:
    *   Forecasting accuracy
    *   Mitigation of catastrophic forgetting
    *   The stability gap