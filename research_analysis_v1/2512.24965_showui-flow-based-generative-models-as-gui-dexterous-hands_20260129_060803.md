# ShowUI-$\pi$: Flow-based Generative Models as GUI Dexterous Hands

*Authors: Siyuan Hu; Kevin Qinghong Lin; Mike Zheng Shou*

---

### ðŸ“Š Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Model Architecture** | Flow-based Generative Model |
| **Parameter Count** | ~450 Million |
| **Benchmark** | ScreenDrag |
| **Benchmark Score** | 26.98 (SOTA) |
| **Training Data** | 20K Drag Trajectories |
| **Key Innovation** | Unified Discrete & Continuous Action Spaces |

---

## Executive Summary

Current graphical user interface (GUI) agents are primarily designed to handle discrete interactions, such as clicking buttons or selecting text. This design severely limits their ability to perform tasks requiring fine-grained, continuous control. Consequently, state-of-the-art proprietary models struggle with drag interactionsâ€”a fundamental human behaviorâ€”often failing to accurately map high-level intents to low-level pixel movements. This deficiency highlights a critical architectural gap: existing discrete-only prediction methods lack the dexterity required for human-like manipulation within digital environments.

To overcome these limitations, the authors introduce **ShowUI-$\pi$**, the first flow-based generative model designed specifically to function as a "dexterous hand" for GUIs. Departing from traditional autoregressive Large Language Models (LLMs), ShowUI-$\pi$ utilizes a **flow-matching objective** to generate actions. This enables a unified architecture that seamlessly integrates discrete clicks with continuous drag trajectories. Technically, the model predicts incremental cursor adjustments rather than absolute end-points, facilitating stable, closed-loop trajectory generation. This approach allows for precise control over the continuous action space while maintaining high parameter efficiency (approx. 450 million parameters).

ShowUI-$\pi$ delivers state-of-the-art performance on the newly introduced **ScreenDrag benchmark**, a dataset specifically designed to evaluate fine-grained dexterous control. The model achieved a score of **26.98**, significantly outperforming proprietary baselines including Operator (13.27) and Gemini-2.5-CUA (22.18). Notably, these results were achieved with a model size orders of magnitude smaller than its competitors, demonstrating that specialized, lightweight architectures can surpass massive general-purpose models in tasks requiring precise, continuous motor control.

---

## Key Findings

*   **Superior Benchmark Performance:** ShowUI-$\pi$ achieved a score of **26.98** on the ScreenDrag benchmark, significantly outperforming proprietary models like Operator (13.27) and Gemini-2.5-CUA (22.18).
*   **High Parameter Efficiency:** The model delivers state-of-the-art performance with only **450M parameters**, demonstrating that specialized architectures outscale massive general-purpose models for specific tasks.
*   **Limitations of Proprietary Agents:** Current proprietary GUI agents struggle with continuous drag interactions, revealing inherent limitations in discrete-only prediction methods.
*   **Benchmark Effectiveness:** The introduced ScreenDrag benchmark effectively highlights the capability gap between current agents and the requirements for human-like dexterous control.

---

## Methodology

The researchers propose **ShowUI-$\pi$**, a flow-based generative model designed to function as a GUI "dexterous hand." The methodology is distinct in several ways:

1.  **Unified Architecture:** Handles continuous trajectories via a unified framework for both discrete clicks and continuous drags.
2.  **Flow-based Action Generation:** Utilizes flow-matching to predict incremental cursor adjustments. This allows for stable, closed-loop trajectory generation rather than relying on absolute end-point prediction.
3.  **Training & Evaluation:** The model is trained on a dataset of **20K manually collected and synthesized drag trajectories**. It is rigorously evaluated using the ScreenDrag benchmark to assess fine-grained control.

---

## Technical Details

*   **Core Architecture:** Flow-based Generative Model (distinct from autoregressive LLMs).
*   **Action Space:** Models GUI interactions as continuous drag interactions (continuous trajectory data) to avoid the imprecision common in discrete movement mapping.
*   **Parameter Efficiency:** Contains approximately **450 million parameters**.
*   **Benchmark Contribution:** Introduced the ScreenDrag benchmark to specifically test fine-grained, dexterous control capabilities in GUI agents.

---

## Results

**Benchmark Performance (ScreenDrag)**
*   **ShowUI-$\pi$:** 26.98
*   **Gemini-2.5-CUA:** 22.18
*   **Operator:** 13.27

**Performance Delta**
*   ShowUI-$\pi$ beat **Operator** by 13.71 points (**103% relative improvement**).
*   ShowUI-$\pi$ beat **Gemini-2.5-CUA** by 4.80 points.

**Efficiency**
Achieved superior performance with a model size (450M) orders of magnitude smaller than proprietary baselines.

**Qualitative Analysis**
Proprietary agents frequently struggle to map high-level intents to low-level continuous pixel movements, whereas ShowUI-$\pi$ maintains stable control throughout the trajectory.

---

## Contributions

1.  **Novel Architecture:** Introduction of the first flow-based generative model specifically designed to act as a GUI dexterous hand.
2.  **Unified Framework:** Development of a unified interaction framework that seamlessly integrates discrete and continuous action spaces.
3.  **Resource Release:** Release of a specialized drag training dataset (20K trajectories) and the ScreenDrag benchmark to aid future research in GUI agent dexterity.

---

**Research Quality Score:** 8/10  
**References:** 40 citations