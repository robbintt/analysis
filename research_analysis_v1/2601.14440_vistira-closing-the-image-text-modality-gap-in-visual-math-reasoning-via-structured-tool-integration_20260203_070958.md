---
title: 'VisTIRA: Closing the Image-Text Modality Gap in Visual Math Reasoning via
  Structured Tool Integration'
arxiv_id: '2601.1444'
source_url: https://arxiv.org/abs/2601.14440
generated_at: '2026-02-03T07:09:58'
quality_score: 6
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# VisTIRA: Closing the Image-Text Modality Gap in Visual Math Reasoning via Structured Tool Integration

*Authors: Saeed Khaki; Ashudeep Singh; Nima Safaei; Kamal Ginotra*

---

> ### üìä Quick Facts
> *   **Quality Score:** 6/10
> *   **References:** 40 Citations
> *   **Dataset Size:** 147,948 training samples (SnapAsk)
> *   **Base Model:** Qwen2.5-VL-7B
> *   **Technique:** Tool-Integrated Reasoning & Synthetic Data Generation

---

## üìù Executive Summary

This research addresses the critical **"modality gap"** in Vision-Language Models (VLMs), where accuracy in mathematical reasoning significantly degrades when problems are presented as images rather than plain text. This issue arises because VLMs struggle to parse dense mathematical formulas and complex visual layouts, leading to minor misreads that compound into critical reasoning failures. Addressing this disparity is crucial for enabling AI systems to function reliably in real-world environments‚Äîsuch as education and scientific research‚Äîwhere mathematical content is frequently embedded in visual formats like PDFs or handwritten notes rather than structured text.

The paper introduces **VisTIRA**, a tool-integrated reasoning agent designed to bridge the performance gap through structured problem-solving and synthetic data generation. Technically, VisTIRA augments a Qwen2.5-VL-7B base model with an iterative loop that decomposes visual inputs into natural language rationales and executable Python code, effectively grounding visual understanding in logical computation. To support this, the authors developed a LaTeX-based pipeline to convert text-based math corpora (specifically NuminaMath-CoT) into high-fidelity images, generating a comprehensive dataset of 147,948 tool-use trajectories derived from SnapAsk. The model was fine-tuned using Supervised Fine-Tuning (SFT) with LoRA to maximize the utility of this synthetic data.

Empirical experiments demonstrate that VisTIRA‚Äôs tool-integrated supervision effectively improves image-based reasoning performance. A key finding is that the severity of the modality gap is **inversely correlated with model size**: smaller models (2B‚Äì7B) suffer substantial performance drops when switching from text to image, while frontier models (e.g., GPT-4o, GPT-5) show significantly more resilience. The study confirms that **OCR grounding** significantly narrows the reasoning gap for smaller models, though its benefits diminish as model size increases. Furthermore, the results indicate that structured reasoning and OCR grounding are complementary approaches, yielding the highest performance when utilized together in the VisTIRA framework.

This work is significant for its rigorous empirical definition of the image-text modality gap and its demonstration that scaling model parameters alone is insufficient to resolve visual parsing deficiencies. By successfully combining tool integration with synthetic data generation using a robust LaTeX pipeline, VisTIRA provides a reproducible framework for enhancing the robustness of VLMs in mathematical domains.

---

## üîë Key Findings

*   **Performance Disparity:** VLMs exhibit significantly lower accuracy on mathematical reasoning problems when presented as images compared to text, driven by failures in reading dense formulas and complex layouts.
*   **Efficacy of VisTIRA:** Tool-integrated supervision via the VisTIRA framework effectively improves image-based reasoning performance.
*   **Scaling vs. Grounding:** OCR grounding narrows the reasoning gap for smaller models, but these benefits diminish as model size increases.
*   **Model Size Correlation:** The severity of the modality gap is inversely correlated with model size (smaller models suffer more; frontier models are more resilient).
*   **Complementary Approaches:** Structured reasoning and OCR-based grounding are complementary, working best when combined.

---

## üß™ Methodology

The research utilizes a dual-faceted approach combining a novel architectural framework with a robust data generation pipeline.

### The VisTIRA Framework
VisTIRA is a tool-integrated reasoning agent designed to process math problems as images. It operates by iteratively decomposing visual input into:
1.  **Natural language rationales**
2.  **Executable Python code**

### Data Generation & Fine-tuning
*   **Synthetic Pipeline:** A LaTeX-based pipeline was developed to convert text-based math corpora into images.
*   **Trajectory Generation:** Synthetic tool-use trajectories were generated from the SnapAsk dataset to train the model on structured reasoning steps.

---

## üèÜ Contributions

1.  **Characterization of the Modality Gap:** Provided an empirical definition and analysis of the performance disparity between text-only and visual modalities.
2.  **VisTIRA Architecture:** Introduced a vision and tool-integrated reasoning agent that leverages structured problem-solving to handle visual math inputs.
3.  **Synthetic Data Pipeline:** Created a LaTeX-based image generator and a dataset of tool-use trajectories derived from SnapAsk.
4.  **Empirical Analysis on Scaling and Grounding:** Delivered insights into how model scaling affects the modality gap and analyzed the utility of OCR grounding across different model sizes.

---

## ‚öôÔ∏è Technical Details

### Architecture & Implementation
The VisTIRA Framework augments Vision-Language Models (VLMs) with tool-integrated reasoning. It decomposes visual math problems into natural language reasoning steps and executable Python programs in an iterative loop.

*   **Base Model:** Qwen2.5-VL-7B
*   **Optimization:** Supervised Fine-Tuning (SFT) using DeepSpeed ZeRO-3
*   **Parameter-Efficient Tuning:** LoRA (Rank 32, Alpha 64)
*   **Hardware:** 8 NVIDIA V100 GPUs

### Data Generation Strategy
*   **Generators:** Utilized advanced VLMs (e.g., GPT-5, Gemini) to create synthetic reasoning sequences.
*   **Conversion:** LaTeX rendering pipeline converts text corpora into image-text pairs to address "modality neglect".

### Hyperparameters
| Parameter | Value |
| :--- | :--- |
| **Learning Rate** | $2 \times 10^{-5}$ |
| **Effective Batch Size** | 64 |
| **Epochs** | 1 |
| **Max Sequence Length** | 8,192 tokens |

---

## üìà Results

Key findings highlight a persistent **'modality gap'** where math questions rendered as images yield significantly lower accuracy than plaintext due to parsing failures of complex layouts. Small visual misreads cause compounding reasoning errors.

*   **Dataset Performance:** Experiments on the SnapAsk dataset (147,948 training samples) and NuminaMath-CoT (5k converted problems) validated the effectiveness of the approach.
*   **Scaling Laws:**
    *   **Smaller Models (2B‚Äì7B):** Suffer large drops in accuracy when switching from text to image.
    *   **Frontier Models (GPT-4o, GPT-5):** Show less degradation, indicating that scale helps but does not eliminate the gap.
*   **Grounding Impact:** OCR grounding provides significant boosts for smaller models but offers diminishing returns for larger models. The highest performance is achieved when structured reasoning and OCR grounding are used together.