---
title: Scoring, Reasoning, and Selecting the Best! Ensembling Large Language Models
  via a Peer-Review Process
arxiv_id: '2512.23213'
source_url: https://arxiv.org/abs/2512.23213
generated_at: '2026-01-28T00:15:03'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# Scoring, Reasoning, and Selecting the Best! Ensembling Large Language Models via a Peer-Review Process

*Hao Wu, Junhang Cheng, Qianren Mao, Bangjie Qin, Zhuoran Li, Zeyu Ji, Zhijun Chen*

---

> ### üìä Quick Facts
>
> *   **Quality Score:** 9/10
> *   **Citations:** 40
> *   **Core Method:** 3-Stage Peer-Review Ensemble
> *   **Architecture:** Inference-only, Model-agnostic
> *   **Key Benefit:** Mitigates hallucinations via cross-validation
> *   **Performance:** 88%‚Äì95% selection accuracy; up to 1.00 correlation in general tasks

---

## üìù Executive Summary

This research addresses the inherent limitations of traditional Large Language Model (LLM) ensembling methods, such as majority voting or token averaging. These traditional methods often fail to discern nuanced quality differences and can produce incoherent text. As industry reliance on massive proprietary models increases, the field faces a critical need for cost-effective alternatives that do not sacrifice reliability.

The core challenge lies in distinguishing accurate, high-quality responses from hallucinations or mediocrity without incurring the prohibitive computational costs associated with running the largest available models. Current approaches lack the robust mechanism needed to consistently identify the best output among diverse candidates, leading to performance bottlenecks in high-stakes applications.

This paper introduces a novel **"Peer-Review Ensemble"** framework that shifts the paradigm from merging text tokens to **selecting the single optimal output**. This model-agnostic, inference-only methodology operates in three stages:

1.  **Authoring:** Diverse LLMs generate independent candidate responses.
2.  **Reviewing:** A separate reviewer LLM critiques the logic and factuality of each response‚Äîgenerating explicit reasoning before assigning a numerical score.
3.  **Selecting:** The system aggregates these scores to identify the best response.

By mandating Chain-of-Thought reasoning (critiques) prior to scoring and utilizing a "wisdom of the crowd" cross-validation approach, the framework effectively filters hallucinations and avoids the incoherence typical of traditional averaging techniques.

The evaluation reveals a nuanced performance profile. The framework demonstrated high selection accuracy‚Äîcorrectly identifying the optimal response aligned with ground truth in **88% to 95% of cases**. On general tasks, smaller open-source models (e.g., Llama-3.1-8B, Mistral-7B) showed near-perfect alignment (correlation ~1.00).

**Conclusion:** These findings validate that "LLM-as-a-Judge" mechanisms are significantly more effective when the judge is prompted to reason before scoring. This approach offers a strategic path to high performance by ensembling diverse, smaller open-source models rather than relying exclusively on expensive, massive proprietary systems.

---

## üîë Key Findings

*   **Superior Performance:** The 'Peer-Review' ensemble method significantly outperforms traditional baselines like majority voting.
*   **Reasoning First:** Generating explicit reasoning (critiques) before scoring is more effective than scoring alone.
*   **Hallucination Mitigation:** The framework mitigates hallucinations by leveraging the 'wisdom of the crowd' for cross-validation.
*   **Model Agnostic:** The approach is applicable to various open-source LLMs without requiring additional training.

---

## üõ†Ô∏è Methodology

The framework mimics the academic peer-review process through a structured three-stage pipeline:

1.  **Authoring**
    *   Multiple candidate LLMs generate independent responses to a given prompt.
    
2.  **Reviewing**
    *   A separate reviewer LLM acts as a judge.
    *   It provides a critique and a numerical score for each response based on logic and factuality.

3.  **Selecting**
    *   The system aggregates the scores from the review stage.
    *   Instead of merging text (as in averaging), it selects the single best response.

---

## üí° Contributions

*   **Novel Mechanism:** Introduction of a novel Peer-Review mechanism that shifts focus from merging text tokens to selecting the highest-quality output.
*   **Judgment Validation:** Demonstration that LLMs can effectively serve as unbiased judges (**LLM-as-a-Judge**) when prompted to reason before scoring.
*   **Cost Strategy:** A cost-efficiency strategy achieving high performance by ensembling smaller, diverse open-source models instead of relying on massive proprietary models.

---

## ‚öôÔ∏è Technical Details

| Component | Description |
| :--- | :--- |
| **Framework Type** | Model-agnostic, inference-only; requires no additional training. |
| **Strategy** | Uses a 'wisdom of the crowd' approach via Cross-Validation. |
| **Role of Models** | LLMs act as reviewers to evaluate peer responses rather than simply aggregating answers. |
| **Reasoning** | Requires explicit reasoning (generating critiques) before assigning scores. |
| **Filtering** | Identifies and filters hallucinations by spotting inconsistencies in peer critiques. |
| **Compatibility** | Applicable to various open-source LLMs such as **Llama-3.1**, **Mistral**, and **Qwen2**. |

---

## üìà Results

The method was evaluated based on correlation with Ground Truth scores and selection accuracy.

### General Tasks
*   **Models Evaluated:** Llama-3.1-8B-Instruct, Mistral-7B-Instruct, Qwen2-7B-Instruct.
*   **Correlation:** Achieved a correlation of **1.00**.
*   **Confidence:** Mistral-7B showed extreme confidence in high-quality answers (Score 5 match weight: 0.99).

### TriviaQA Dataset
*   **Correlations:** Lower linear correlation metrics compared to general tasks.
    *   Pearson: **0.128**
    *   Spearman: **0.400**
    *   Kendall: **0.333**
*   **Confusion Matrix Analysis:**
    *   High precision for **Score 1** (0.74) and **Score 3** (0.83).
    *   Models struggled to distinguish 'average' answers (Score 2).

### Overall Selection Accuracy
Secondary evaluation metrics for selecting the best response ranged from **0.88 to 0.95**.

---

*Report generated based on analysis of 40 references.*