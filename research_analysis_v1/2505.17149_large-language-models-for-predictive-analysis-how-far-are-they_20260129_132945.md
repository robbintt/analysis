# Large Language Models for Predictive Analysis: How Far Are They?
*Qin Chen; Yuanyi Ren; Xiaojun Ma; Yuyang Shi*

---

### ðŸ“Š Quick Facts

| Metric | Value |
| :--- | :--- |
| **Benchmark Name** | PredictiQ |
| **Datasets Included** | 44 (spanning 8 fields) |
| **Total Queries** | 1,130 sophisticated queries |
| **Models Evaluated** | 12 renowned LLMs (including GPT-4, Claude-2, Llama-2) |
| **Development Effort** | ~300 human hours |
| **Execution Accuracy** | ~20-30% (even for top models like GPT-4) |
| **Paper Quality** | 8/10 |
| **Citations** | 40 |

---

## Executive Summary

> This paper addresses the critical disparity between the growing expectation for Large Language Models (LLMs) to perform predictive analysis and their actual efficacy in complex, data-driven decision-making scenarios. While LLMs have demonstrated exceptional proficiency in conversational tasks, their capability to perform rigorous predictive analysisâ€”such as forecasting or interpreting tabular dataâ€”remains unproven. This issue is of paramount importance because reliable predictive analysis is foundational to scientific inquiry and business strategy; relying on models that lack these specific capabilities could lead to flawed conclusions.
>
> The authors identify a vital gap in current research: the absence of a standardized evaluation mechanism to accurately gauge how well LLMs can transition from general language processing to specific predictive tasks. The key innovation presented is **"PredictiQ,"** the first comprehensive benchmark specifically designed to assess LLMs in predictive analysis. Technically, the benchmark formalizes predictive analysis as a transformation task where the model processes a paired input of a query and data to generate an output consisting of both text and code.
>
> The evaluation framework is holistic, assessing models across three distinct dimensions: textual analysis (methodology explanation), code generation (functional correctness), and the critical alignment between the generated text and code. To mitigate context limitations and ensure robust evaluation, the authors employed a complex prompting strategy utilizing a "professional data scientist" system persona, condensed data summaries, and raw data details.
>
> The study rigorously evaluated twelve state-of-the-art LLMs, using a dataset of 1,130 sophisticated queries spanning eight diverse fields. The results reveal a notable performance gap: despite their general proficiency, current models face significant hurdles in predictive tasks. Specifically, the study found that execution pass rates were remarkably low, with even the top-performing models such as GPT-4 achieving execution accuracies of approximately **20-30%** on complex predictive tasks.
>
> The significance of this paper lies in its establishment of a rigorous baseline for a domain that previously lacked standardized evaluation metrics. By introducing PredictiQ and a multi-dimensional assessment protocol, the authors provide the field with the necessary tools to critically analyze the limitations of current LLM architectures regarding quantitative reasoning. This work serves as a crucial corrective to the hype surrounding LLM capabilities, offering concrete evidence that general conversational ability does not equate to competency in predictive analysis.

---

## Key Findings

*   **Performance Challenges:** Existing Large Language Models (LLMs) face significant challenges in effectively conducting predictive analysis tasks.
*   **Expectation vs. Reality Gap:** There is a notable gap between the burgeoning expectation to use LLMs for prediction and their actual performance capabilities in complex decision-making scenarios.
*   **Limited Utility:** The practical utility of current state-of-the-art LLMs in predictive analysis is limited despite their general proficiency in knowledge-intensive conversations.
*   **Low Execution Accuracy:** Even top-tier models like GPT-4 achieve only **20-30%** execution accuracy on complex tasks, highlighting deficiencies in functional correctness.

---

## Methodology

The research team employed a rigorous multi-step process to evaluate LLM capabilities:

*   **Benchmark Construction (PredictiQ):**
    *   Developed a comprehensive benchmark comprising **1,130 sophisticated queries**.
    *   Queries were derived from **44 real-world datasets**.
    *   Spanning **8 diverse fields** to ensure broad applicability.
*   **Evaluation Protocol:**
    *   Designed a protocol to assess models based on three distinct dimensions:
        1.  Text analysis capability.
        2.  Code generation capability.
        3.  The alignment between text and code elements.
*   **Model Assessment:**
    *   Rigorously evaluated **twelve renowned LLMs** using this benchmark.
    *   Focused on gauging practical efficacy in handling predictive analysis workflows.

---

## Contributions

*   **Bridging the Evaluation Gap:** Addressed the lack of relevant evaluations by introducing **PredictiQ**, the first comprehensive benchmark specifically designed to assess LLMs in predictive analysis.
*   **Holistic Assessment Framework:** Established a novel evaluation methodology that considers the multifaceted nature of predictive tasks by measuring text analysis, code generation, and their alignment.
*   **Systematic Performance Analysis:** Provided a critical comparative analysis of twelve major LLMs, offering concrete insights into the current limitations of these models regarding predictive tasks.

---

## Technical Details

### PredictiQ Architecture
The PredictiQ benchmark formalizes LLM predictive analysis as a transformation task:
*   **Input:** `(query, data)`
*   **Output:** `(text, code)`

For a model to succeed, it requires three integrated components:
1.  **Textual Analysis:** Explaining the methodology clearly.
2.  **Code Implementation:** Generating functionally correct code.
3.  **Text-Code Alignment:** Ensuring the code matches the textual explanation.

### Prompting Strategy
To mitigate context limitations, a three-part structure was utilized:
1.  **System Instruction:** Adopts a 'professional data scientist' role.
2.  **Data Summary:** Includes schema, types, min/max values, and category counts.
3.  **Raw Data Details:** Provided in CSV format.

### Query Generation Design
*   **Human-in-the-Loop:** Queries were generated by experts.
*   **Constraints:**
    *   Unambiguous targets.
    *   Reliance on internal data only.
    *   Restriction to single questions.

---

## Results
The benchmark results cover a wide spectrum of industries and data complexities:

**Dataset Distribution:**
*   **Economics:** 12 datasets, 270 queries
*   **Marketing and Sales:** 6 datasets, 200 queries
*   **Industry Analysis:** 7 datasets, 180 queries
*   **Traffic:** 5 datasets, 130 queries
*   **Healthcare:** 4 datasets, 130 queries
*   **Social Study:** 4 datasets, 110 queries
*   **Human Resource:** 3 datasets, 80 queries
*   **Education:** 3 datasets, 70 queries

**Development Statistics:**
*   The development process required approximately **300 human hours**.
*   The system handles mixed data types effectively, utilizing min/max summaries for numerical columns and category counts for categorical columns.

---

**Document Quality Score:** 8/10  
**References:** 40 citations