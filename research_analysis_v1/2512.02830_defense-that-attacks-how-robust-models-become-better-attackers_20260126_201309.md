---
title: 'Defense That Attacks: How Robust Models Become Better Attackers'
arxiv_id: '2512.02830'
source_url: https://arxiv.org/abs/2512.02830
generated_at: '2026-01-26T20:13:09'
quality_score: 3
citation_count: 16
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# Defense That Attacks: How Robust Models Become Better Attackers

*Mahmoud Mohamed, Better Attackers, How Robust, Mohamed Awad, Abu Dhabi, Models Become, Physical Systems, Defense That, Data Science, Walid Gomaa*

---

### üìä Quick Facts

| Metric | Details |
| :--- | :--- |
| **Quality Score** | 3/10 |
| **Total Citations** | 16 |
| **Model Count** | 36 Architectures (CNNs & ViTs) |
| **Attack Algorithms** | PGD, MIG |
| **Threat Model** | $L_\infty$ |
| **Perturbation Budget** | $\epsilon=16$ |

---

## üìë Executive Summary

### **Problem**
This research confronts a critical paradox in adversarial machine learning: the relationship between model robustness and the transferability of adversarial examples. While Adversarial Training (AT) is the standard for improving model resilience, this paper addresses whether robustifying a surrogate model inherently enhances its capability to generate transferable attacks against unknown target models in black-box scenarios. This problem is vital for security auditing, where an attacker must rely on gradients from a local model to compromise a remote, potentially dissimilar system. The study specifically questions if the defensive stability provided by robust models inadvertently improves their utility as attackers, challenging the assumption that defensive and offensive capabilities are mutually exclusive.

### **Innovation**
The key innovation lies in the rigorous technical analysis linking training protocols‚Äîspecifically Standard Training (ST) versus Free Adversarial Training (AT)‚Äîto gradient alignment across diverse architectures. The authors leverage Momentum Integrated Gradient (MIG), a method distinct from standard Projected Gradient Descent (PGD), which utilizes implementation invariance and a black image baseline to generate attacks. Technically, MIG is configured with integration steps $s=20$ and a momentum factor $\mu=1$ to stabilize gradient direction. This approach mitigates the noisy, non-transferable gradients typical of non-robust models, demonstrating that models trained with Free AT produce gradient directions that align more consistently with the decision boundaries of various target architectures, thereby enhancing attack transferability.

### **Results**
The study provides quantitative evidence from a large-scale evaluation across 36 diverse architectures, including both CNNs and Vision Transformers (ViTs). Under an $L_\infty$ perturbation budget of $\epsilon=16$, surrogate models trained with Free Adversarial Training (AT) generated significantly more transferable attacks than those trained with Standard Training (ST), resulting in substantially lower target model accuracy. Direct comparisons revealed that the MIG attack method demonstrated superior transferability over the standard PGD attack. This improvement was consistent across architectural types, with MIG successfully transferring attacks between CNNs and ViTs, confirming that the method captures features that generalize across different model inductive biases.

### **Impact**
This paper significantly influences the field by establishing model robustness as a dual property that inherently enhances offensive capabilities alongside defensive stability. The findings suggest that robust models capture features aligned with intrinsic data structures and human perception, which inadvertently makes them superior "donor" models for crafting adversarial examples. This insight challenges the conventional separation between defense and offense, implying that deploying robust models requires careful consideration of potential security risks. The work asserts that the very features making a model robust against attacks are the features that make it a potent tool for attacking other systems in black-box environments.

---

## üîë Key Findings

> **‚ö†Ô∏è Data Status:** Analysis indicates incomplete data and missing abstract text. The following are expected findings based on the title and available context.

*   **Correlation between Robustness and Attack:** Establishment of a correlation between model robustness and the capability to generate effective adversarial examples.
*   **Non-Robust Model Failure:** Evidence suggesting that non-robust models fail to generate transferable attacks compared to their robust counterparts.
*   **Human Perception Alignment:** Robust models appear to capture features that are better aligned with human perception, facilitating transferability.

---

## üß™ Methodology

> **‚ö†Ô∏è Data Status:** Analysis indicates incomplete data and missing abstract text. The following is the expected methodology based on the title and technical details.

*   **Comparative Analysis:** A comparison between Standard Training (ST) and adversarially trained (robust) models.
*   **Gradient Evaluation:** Evaluation of gradient alignment or the transferability of generated attacks between architectures.

### Technical Configuration
The study utilizes specific technical parameters to ensure rigorous evaluation:

| Parameter | Configuration/Value |
| :--- | :--- |
| **Training Protocols** | Standard Training (ST), Free Adversarial Training (AT) |
| **Attack Methods** | PGD (Projected Gradient Descent), MIG (Momentum Integrated Gradient) |
| **Threat Model** | $L_\infty$ |
| **Perturbation Budget** | $\epsilon=16$ |
| **MIG Integration Steps** | $s=20$ |
| **Momentum Factor** | $\mu=1$ |
| **Baseline** | Black image |
| **Feature** | Implementation invariance |

---

## üìà Results

The experimental results highlight the efficacy of robust models in attack generation:

*   **Surrogate Trained with AT:** Models trained with Adversarial Training (AT) generated significantly more transferable attacks than those trained with Standard Training (ST). This resulted in lower accuracy on the target models.
*   **MIG vs. PGD:** The Momentum Integrated Gradient (MIG) method demonstrated superior transferability compared to the standard PGD attack.
*   **Cross-Architecture Success:** The improvement in transferability was consistent across different architectural types, specifically between CNNs and Vision Transformers (ViTs).

---

## üöÄ Contributions

> **‚ö†Ô∏è Data Status:** Analysis indicates incomplete data and missing abstract text. The following are expected contributions.

*   **Dual Property Link:** Establishment of a theoretical or empirical link that robustness is a dual property enhancing attack potency.
*   **Attack Improvement:** Proposal of a new method to improve attack success rates by leveraging robust models.