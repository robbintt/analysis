# Adversarial Evasion Attacks on Computer Vision using SHAP Values

*Frank Mollard; Marcus Becker; Florian Roehrbein*

***

> ### **⚡ Quick Facts**
>
> *   **Attack Vector:** White-box adversarial evasion using Explainable AI (XAI).
> *   **Core Mechanism:** Manipulating pixels based on SHAP values to neutralize feature impact.
> *   **Target Datasets:** Animal Faces, Cats and Dogs, MNIST, Woman and Man Faces.
> *   **Baseline Accuracy:** 97% – 99.5%.
> *   **Top Performance:** 98% misclassification rate on the Woman and Man Faces dataset.
> *   **Quality Score:** 9/10

---

## Executive Summary

### **Problem**
This research addresses the critical vulnerability of computer vision models to white-box adversarial evasion attacks, specifically in scenarios where traditional gradient-based methods are ineffective. As deep learning systems proliferate in high-stakes environments, inputs manipulated to deceive models while remaining visually imperceptible to humans represent a major security threat. A specific challenge addressed is "gradient masking," a defense that obscures gradient information to thwart standard methods like the Fast Gradient Sign Method (FGSM). This paper highlights the limitations of relying on gradient obfuscation and establishes a novel attack vector that succeeds where standard optimization methods often fail.

### **Innovation**
The key innovation is the weaponization of **SHAP (SHapley Additive exPlanations)** values—traditionally a tool for model explainability—to execute efficient adversarial attacks. Unlike gradient-based methods that rely on loss function geometry, this white-box framework leverages SHAP values to quantify the individual contribution of specific pixels during inference. The methodology identifies pixels with high positive or negative SHAP values and shifts their intensity into a defined "neutral area" (magnitudes roughly 0.4–0.5). This magnitude-aware approach neutralizes high-importance features efficiently, operating independently of gradients to bypass masking defenses.

### **Results**
The researchers benchmarked the SHAP-based attack against FGSM across four datasets with baseline accuracies ranging from 97% to 99.5%. The SHAP attack outperformed FGSM in 3 out of 4 datasets, demonstrating superior robustness against gradient masking.
*   **Woman and Man Faces:** SHAP 98% vs. FGSM 69% misclassification.
*   **Animal Faces:** SHAP 73% vs. FGSM 52% misclassification.
*   **MNIST:** SHAP 60% vs. FGSM 34% misclassification.
While FGSM achieved a higher rate on the Cats and Dogs dataset (98% vs. 89%), it exhibited instability due to gradient masking. Conversely, the SHAP attack maintained stable performance while generating visually imperceptible adversarial examples.

### **Impact**
This work is significant because it demonstrates that **Explainable AI (XAI) mechanisms can be repurposed as offensive capabilities**, revealing a new class of vulnerabilities. By proving that SHAP-based attacks can penetrate gradient hiding defenses, the authors challenge the efficacy of security strategies relying solely on gradient obfuscation. The study also introduces the "Butterfly Pattern"—a convergence of pixel and SHAP values near zero—offering new insights into feature sensitivity. Future defense strategies must now account for feature-importance attacks to bridge the gap between human and machine interpretation.

---

## Key Findings

*   **Introduction of a Novel Attack Vector:** The paper establishes a viable white-box adversarial attack methodology utilizing SHAP values to compromise computer vision models.
*   **Differential Robustness against FGSM:** In comparative testing, the proposed SHAP-based attack demonstrated greater robustness than the Fast Gradient Sign Method (FGSM), specifically outperforming it in gradient hiding scenarios.
*   **Stealth Capabilities:** The generated adversarial examples induce misclassifications and reduce output confidence while remaining imperceptible to the human eye, effectively creating a discrepancy between human and algorithmic perception.
*   **Impact on Model Performance:** The attacks successfully degrade deep learning model performance by significantly reducing output confidence and inducing misclassifications.

---

## Methodology

The researchers proposed a white-box attack framework that leverages **SHAP values** to quantify the significance of individual inputs relative to the model's output during the inference stage. By using these SHAP values to identify and manipulate high-importance features, the approach generates adversarial perturbations designed to evade the model.

The methodology includes a comparative analysis where the efficacy and robustness of the SHAP-based attack are benchmarked against the established **Fast Gradient Sign Method (FGSM)**.

---

## Technical Details

| **Aspect** | **Description** |
| :--- | :--- |
| **Attack Type** | White-box adversarial evasion attack (Inference phase). |
| **Core Mechanism** | Utilizes SHAP (SHapley Additive exPlanations) values to quantify the contribution of individual pixels to the model's output. |
| **Perturbation Strategy** | Shifts pixels with high positive or negative SHAP values into a 'neutral area' (magnitudes roughly 0.4–0.5) to neutralize their impact. |
| **Key Characteristics** | • **Magnitude-aware:** Manipulates fewer pixels than gradient-based methods.<br>• **Geometry Independent:** Operates independently of loss function geometry to avoid gradient masking.<br>• **Efficient:** Requires fewer computations than full gradient optimization. |
| **Observations** | **"Butterfly Pattern":** Observed a consistent relationship where pixel values and SHAP values converge near zero at intermediate pixel values. |

---

## Results

The following table summarizes the comparative performance between the proposed SHAP attack and the Fast Gradient Sign Method (FGSM) across various datasets.

| **Dataset** | **Baseline Accuracy** | **SHAP Attack Misclassification** | **FGSM Misclassification** | **Winner** |
| :--- | :---: | :---: | :---: | :---: |
| **Animal Faces** | 97% – 99.5% | **73%** | 52% | SHAP ✅ |
| **Cats and Dogs** | 97% – 99.5% | 89% | **98%** | FGSM |
| **MNIST** | 97% – 99.5% | **60%** | 34% | SHAP ✅ |
| **Woman and Man Faces**| 97% – 99.5% | **98%** | 69% | SHAP ✅ |

*   **Overall Stability:** The SHAP attack demonstrated stable performance across datasets, whereas FGSM showed instability due to gradient masking.

---

## Contributions

*   **New Exploitation Technique:** The primary contribution is the novel application of SHAP values—typically used for explainability—as a tool for executing adversarial evasion attacks.
*   **Advancement in Gradient Masking Defense Penetration:** The paper provides evidence that SHAP-based attacks can succeed where gradient-based methods (like FGSM) may struggle, offering a new perspective on bypassing gradient hiding defenses.
*   **Analysis of Perceptual Stealth:** The work contributes to the understanding of adversarial examples that are visually imperceptible to humans yet destructive to algorithmic integrity, highlighting a critical vulnerability in computer vision systems.

---
**References:** 7 citations