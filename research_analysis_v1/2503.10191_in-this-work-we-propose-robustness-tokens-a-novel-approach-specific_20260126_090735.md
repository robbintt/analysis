---
title: In this work, we propose Robustness Tokens, a novel approach specific
arxiv_id: '2503.10191'
source_url: https://arxiv.org/abs/2503.10191
generated_at: '2026-01-26T09:07:35'
quality_score: 1
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# Robustness Tokens: Towards Adversarial Robustness of Vision Foundation Models

*Yury Belousov, Brian Pulfer, Slava Voloshynovskiy*

---

### üìä Quick Facts

| Metric | Details |
| :--- | :--- |
| **Domain** | Computer Science / Computer Vision |
| **Venue** | European Conference (Likely ECCV) |
| **Citations** | 40 |
| **Focus** | Adversarial Robustness, Vision Foundation Models (VFMs) |
| **Models Used** | DiNOv2, DEIT-III, OpenCLIP |
| **Data Quality** | 1/10 (Fragmented input source) |

---

## üìã Executive Summary

This research addresses the critical challenge of securing **Vision Foundation Models (VFMs)**‚Äîsuch as DiNOv2, DEIT-III, and OpenCLIP‚Äîagainst adversarial attacks. As these massive pre-trained architectures see widespread adoption, their susceptibility to adversarial perturbations remains a major vulnerability. The problem is compounded by the high cost of standard defense mechanisms; traditional adversarial training or fine-tuning is computationally prohibitive for billion-parameter models.

The core innovation is the introduction of **Robustness Tokens (ROB tokens)**, a parameter-efficient mechanism designed to optimize the input space rather than the model weights. ROB tokens are learnable, non-conditional tokens appended to the input sequence of Transformers. While the pre-trained foundation model parameters remain frozen, these tokens are trained specifically to induce robustness.

A critical technical differentiator is the deployment strategy: the **ROB tokens are kept secret** post-deployment. This secrecy introduces significant uncertainty for attackers, as the effective input processing deviates from the publicly known architecture, thereby blunting the efficacy of white-box attack strategies.

The introduction of ROB tokens is shown to drastically reduce the success rate of white-box attacks while maintaining generalization across various adversarial threat models. In terms of computational efficiency, the approach adds negligible parameters relative to the model backbone and requires significantly less computational cost compared to standard fine-tuning.

---

## üîë Key Findings

*   **Novel Defense Mechanism:** The paper proposes "Robustness Tokens" (ROB tokens), a novel approach specifically targeting adversarial robustness in pre-trained Vision Foundation Models.
*   **Parameter Efficiency:** The method requires very few additional parameters compared to the model backbone, keeping the computational cost low relative to standard fine-tuning.
*   **Security through Obscurity:** By keeping the ROB tokens secret after deployment, the method introduces uncertainty for attackers, specifically degrading the efficacy of white-box attacks.
*   **Broad Applicability:** The approach demonstrates effectiveness across multiple tasks, including feature space alignment, classification, and segmentation.
*   **Backbone Preservation:** The strategy freezes the pre-trained foundation model parameters, preserving the standard public backbone while optimizing the input space for defense.

---

## ‚öôÔ∏è Technical Details

**Architecture & Methodology**
*   **Token Type:** Learnable, non-conditional tokens appended to the input sequence.
*   **Training Strategy:** Freezes the pre-trained foundation model parameters; trains only the ROB tokens.
*   **Optimization Target:** Optimizes the input rather than model parameters (Input-space optimization).
*   **Scope:** General to Transformers, specifically applied to Vision Foundation Models (VFMs).

**Differentiators**
The authors distinguish ROB tokens from similar concepts:
*   **Vs. Register Tokens:** ROB tokens focus on adversarial robustness rather than pre-training.
*   **Vs. Prefix-Tuning:** ROB tokens focus on defense rather than feature extraction.
*   **Vs. Attention Sinks:** ROB tokens focus on robustness rather than context storage.

**Target Models**
*   DiNOv2
*   DEIT-III
*   OpenCLIP

---

## üìà Results & Performance

*   **White-Box Defense:** Introducing a few ROB tokens drastically drops the efficacy of white-box attacks.
*   **Generalization:** The method generalizes across different types of adversarial attacks.
*   **Task Versatility:** Effectiveness demonstrated in:
    *   Feature space robustness
    *   Classification tasks
    *   Segmentation tasks
*   **Efficiency:** Training requires little computational cost compared to standard fine-tuning methods.
*   **Clean Data Performance:** Robustness gains are achieved without degrading the model's standard performance on clean data.

---
*Note: This report was generated from an analysis of fragmented metadata. Specific quantitative metrics were not available in the source text.*