---
title: 'Mellum: Production-Grade in-IDE Contextual Code Completion with Multi-File
  Project Understanding'
arxiv_id: '2510.05788'
source_url: https://arxiv.org/abs/2510.05788
generated_at: '2026-02-04T15:53:06'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Mellum: Production-Grade in-IDE Contextual Code Completion with Multi-File Project Understanding

*Nikita Pavlichenko, Iurii Nazarov, Ivan Dolgov, Ekaterina Garanina, Dmitry Ustalov, Ivan Bondyrev, Kseniia Lysaniuk, Evgeniia Vu, Kirill Chekmenev, Joseph Shtok, Yaroslav Golubev, Anton Semenkin, Uladzislau Sazanovich*

---

### ðŸ“Š Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Model Size** | 4 Billion Parameters |
| **Architecture** | Scaled-down Llama 2 |
| **Training Data** | ~4 Trillion Tokens (Permissively Licensed) |
| **Context Window** | 8,192 Tokens |
| **Latency Target** | < 500 ms (90th percentile) |
| **Hardware Req.** | Nvidia H100 (80 GB VRAM) |
| **License** | Apache 2.0 |

---

## Executive Summary

Deploying large language models (LLMs) for code completion within interactive development environments (IDEs) presents a distinct challenge: balancing high-quality generation with strict operational constraints. While larger models often demonstrate superior reasoning capabilities, their size frequently leads to latency that disrupts the developer experience and incurs prohibitive infrastructure costs at scale. Furthermore, accurate code generation often requires an understanding of context beyond the immediate file, necessitating complex "multi-file project awareness" that can overwhelm standard context windows.

This paper addresses the difficulty of creating a code completion system that is both responsive enough for real-time typing and sophisticated enough to leverage full project context. The authors introduce **Mellum**, a 4-billion parameter model based on a scaled-down Llama 2 architecture, designed specifically to optimize the trade-off between size and utility for IDE deployment.

The core technical innovation lies in a rigorous, multi-stage training pipeline and a "context packing" technique that maximizes the utility of the modelâ€™s 8,192-token context window for multi-file understanding. The pipeline begins with pre-training on 4 trillion tokens of permissively licensed code, followed by Supervised Fine-Tuning (SFT) incorporating Fill-in-the-Middle (FIM) objectives and project-specific data. Finally, the model is aligned using Direct Preference Optimization (DPO) derived from real-world user telemetry.

While specific benchmark scores (e.g., pass@k) were not disclosed in the provided text, the study reports success in meeting critical production-grade operational targets. The model achieves a latency goal of serving 90% of requests within 500 milliseconds and is optimized for Nvidia H100s. By demonstrating that a carefully curated 4B parameter model can effectively handle complex, multi-file scenarios, the paper challenges the necessity of massive parameter counts for specific coding tasks and establishes a pragmatic blueprint for cost-effective, high-performance developer tools.

---

## Key Findings

*   **Data and Training Impact:** Careful data curation combined with a staged training approach significantly enhances the quality of the code completion model.
*   **Importance of Context Packing:** Editor-critical capabilities, specifically context packing, are identified as necessary requirements for generating high-quality code suggestions.
*   **Efficacy of Compact Models:** A compact, task-focused model (4B parameters) is capable of meeting the strict cost and latency constraints required for interactive in-IDE completion.

---

## Methodology

The study implements an end-to-end industrial pipeline featuring disciplined data governance and a multi-stage training process:

*   **Model Architecture:** Utilization of a Llama-style architecture with 4 billion parameters.
*   **Pre-training:** The model is pre-trained on approximately 4 trillion tokens of permissively licensed, multi-language code.
*   **Multi-stage Training:** Includes Fill-in-the-Middle (FIM) objectives and the integration of project context via Supervised Fine-Tuning (SFT).
*   **Alignment:** The model is aligned using Direct Preference Optimization (DPO) based on feedback derived from real-world usage scenarios.
*   **Evaluation:** Performance is assessed using a hybrid of large-scale offline benchmarks and online telemetry gathered from production deployments within JetBrains IDEs.

---

## Technical Details

### Architecture Specifications
The model is a 4 billion parameter scaled-down version of the Llama 2 architecture, designed for interactive IDE use.

| Component | Specification |
| :--- | :--- |
| **Layers** | 30 |
| **Attention Heads** | 24 |
| **KV Heads** | 24 |
| **Hidden Size** | 3,072 |
| **MLP Hidden Size** | 8,256 |
| **Vocabulary Size** | 49,152 (Custom, code-weighted) |
| **Context Window** | 8,192 Tokens (Fixed) |

### Training Pipeline
*   **Stage 1:** Base pre-training on permissively licensed public code.
*   **Stage 2:** Supervised Fine-Tuning (SFT).
*   **Stage 3:** Direct Preference Optimization (DPO).

### Key Capabilities
*   Fill-in-the-Middle (FIM)
*   Context packing
*   Multi-file project understanding
*   Optimization for editor-critical behaviors (e.g., robustness against partial tokens)

---

## Results & Evaluation

### Operational Targets
*   **Latency:** Goal of serving 90% of requests within **500 ms**.
*   **Hardware:** Requirement of at least **80 GB VRAM** (Nvidia H100).
*   **Scale:** Deployment targets hundreds of thousands of users.

### Comparative Baselines
The model is compared against major industry code models:
*   Code Llama (7B-70B)
*   Qwen2.5-Coder (0.5B-32B, 5.5T tokens)
*   DeepSeek-Coder (1.3B-33B, 6T tokens)
*   Codestral (22B)

### Evaluation Benchmarks
Performance is measured using a mix of standard and repository-level benchmarks:
*   HumanEval, HumanEval+
*   MBPP+, LiveCodeBench
*   SAFIM, HumanEval-Infilling
*   RepoEval, RepoBench

---

## Contributions

*   **Open-Weight Model Release:** The release of the "Mellum models family," a set of 4B parameter code completion models available under the Apache-2.0 license with a reproducible reference model card.
*   **Industrial Framework:** A detailed description of a production-grade pipeline for contextualized in-editor completion, encompassing data governance, specialized training stages, and preference alignment.
*   **Scalability Blueprint:** A pragmatic guide for transitioning focused open models from research prototypes to large-scale production environments serving hundreds of thousands of users.

---

**Paper Quality Score:** 8/10 | **References:** 40 citations