---
title: Elucidating the Design Space of FP4 training
arxiv_id: '2509.17791'
source_url: https://arxiv.org/abs/2509.17791
generated_at: '2026-02-03T20:16:10'
quality_score: 6
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Elucidating the Design Space of FP4 training

*Robert Hu; Carlo Luschi; Paul Balanca*

---

> ### üìä Quick Facts
> | Metric | Details |
> | :--- | :--- |
> | **Quality Score** | 6/10 |
> | **References** | 40 Citations |
> | **Key Format** | UE5M3 |
> | **Core Technique** | Linear Spline Approximation |
> | **Focus** | Microscaling Quantization (MX) |

---

## üìù Executive Summary

The proliferation of large-scale machine learning models has necessitated a shift toward low-precision training formats, such as 4-bit floating point (FP4), to reduce memory footprint and computational latency. However, training in FP4 presents a formidable challenge: the drastically reduced numerical range often leads to training instability and convergence failure. While various techniques exist to mitigate these issues‚Äîincluding scaling, transformations, and specialized rounding‚Äîthey have largely been studied in isolation. This paper addresses the critical problem of navigating this fragmented design space to determine how to combine stabilization techniques effectively without introducing computational overhead that negates the efficiency benefits of FP4 arithmetic.

To systematically explore this design space, the authors introduce a unified, quantization gradient-based framework specifically designed for microscaling (MX) quantization. Central to this innovation is a high-throughput simulator capable of evaluating thousands of technique combinations across both forward and backward passes. Technically, the approach utilizes a "Linear Spline" approximation for differentiable relaxation, enabling backpropagation through quantization via piecewise linear functions and gradient clipping. This spline method is integrated with distinct rounding strategies: Round-to-Positive-Infinity for scales to reduce saturation, Round-to-Nearest for the forward pass, and Stochastic Rounding for the backward pass.

Extensive empirical evaluation across regression, image classification, diffusion models, and language models identified a specific optimal configuration: the combination of **Hadamard transformations**, **tensor scaling**, and **stochastic rounding**. The study found that using the **UE5M3** format for scaling factors provides the best compromise between numerical range and precision. In terms of computational efficiency, the proposed Linear Spline method achieved significantly lower overhead than power-based approximations and outperformed computational costs reported by other recent studies. Stability results demonstrated that tensor scaling and NVFP4 range-adjustment heuristics successfully achieve convergence, while the spline gradient clipping effectively resolved gradient masking failure modes.

---

## üîë Key Findings

*   **Optimal Configuration:** The best performance-to-overhead trade-off is achieved by combining **Hadamard transformations**, **tensor scaling**, and **stochastic rounding**.
*   **Scaling Factor Efficiency:** Using **UE5M3** as a scaling factor offers a favorable compromise between numerical range and precision while maintaining manageable computational overhead.
*   **Stabilization:** Through the evaluation of thousands of technique combinations, the study identifies specific configurations that successfully stabilize training without prohibitive computational costs.
*   **Computational Overhead:** The proposed Linear Spline method reduces complexity to $O(nm \log_2(k))$, significantly outperforming existing power-based approximations.

---

## üß™ Methodology

The authors introduced a comprehensive, quantisation gradient-based framework specifically designed for microscaling quantization. This framework allows for the theoretical analysis of computational costs associated with various stabilization methods in both forward and backward passes.

*   **Simulation:** A simulator built on this framework was used to conduct an extensive empirical study.
*   **Evaluation:** The study systematically evaluated thousands of combinations of techniques‚Äîincluding novel gradient approximations, rounding strategies, and scaling methods.
*   **Scope:** Evaluations covered diverse machine learning tasks such as regression, image classification, diffusion models, and language models.

---

## ‚öôÔ∏è Technical Details

The approach establishes a unified FP4 Microscaling (MX) framework with the following technical specifications:

### Unified Framework
*   **Partitioning:** Tensors are partitioned into blocks containing scale scalars and quantized values.
*   **Quantization Function:** Utilizes the formula:
    $$f(X_p) = \frac{1}{s_q} Q(s_q \cdot X_p)$$

### Tensor Scaling
*   **Normalization:** Implements global normalization using a global factor $g$ to improve quantization.
*   **Complexity:** Optimizes complexity to $O((m \cdot n) // l)$.

### Differentiable Relaxations
*   **Linear Spline:** Proposes a 'Linear Spline' approximation to enable backpropagation using piecewise linear functions.
*   **Gradient Clipping:** Integrated to resolve gradient masking failure modes.
*   **Complexity:** Reduces overhead to $O(nm \log_2(k))$.

### Rounding Strategies
*   **Round-to-Positive-Infinity (RPI):** Used for scales to reduce saturation.
*   **Round-to-Nearest (RTN):** Used for the forward pass.
*   **Stochastic Rounding (SR):** Used for the backward pass.

### Heuristics
*   **NVFP4:** Utilizes a heuristic to prevent overflow.
*   **Underflow Handling:** Rounds zero/underflow scales to the closest subnormal value.

---

## üìà Results

*   **Optimal Setup:** The combination of Hadamard transformations, tensor scaling, and stochastic rounding, with the **UE5M3** format, offers the best compromise.
*   **Complexity Analysis:**
    *   **Proposed Method:** $O(nm \log_2(k))$
    *   **Power-based:** $O(nmw \log_2(k))$
    *   **Tseng et al.:** $+O(n \log l)$
    *   **Wang et al.:** $+O(n)$
*   **Stability Metrics:**
    *   NVFP4 convergence is achieved through tensor scaling and range-adjustment heuristics.
    *   Spline gradient clipping effectively resolves gradient masking failure modes.

---

## üèÜ Contributions

1.  **Unified View:** The paper provides a unified view of the FP4 training design space, addressing the fragmentation of existing isolated solutions.
2.  **Specialized Framework:** Contributes a framework that enables the theoretical dissection of computational overheads for microscaling quantization techniques.
3.  **Design Guidelines:** Establishes clear guidelines for selecting FP4 training configurations (specifically regarding transformations, scaling, and rounding) to maximize hardware throughput without sacrificing model stability.

---
*Quality Score: 6/10* | *References: 40 citations*