---
title: Sequential Diffusion Language Models
arxiv_id: '2509.24007'
source_url: https://arxiv.org/abs/2509.24007
generated_at: '2026-02-03T19:22:10'
quality_score: 9
citation_count: 10
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Sequential Diffusion Language Models

*Yangzhou Liu; Yue Cao; Hao Li; Gen Luo; Zhe Chen; Weiyun Wang; Xiaobo Liang; Biqing Qi; Lijun Wu; Changyao Tian; Yanting Zhang; Yuqiang Li; Tong Lu; Yu Qiao; Jifeng Dai; Wenhai Wang*

---

> ### üìä Quick Facts
>
> *   **Quality Score:** 9/10
> *   **Throughput Gain:** 2.1x higher than Qwen-2.5
> *   **Training Efficiency:** Matches baselines with only 3.5M samples
> *   **Largest Variant:** SDLM-32B (showing pronounced efficiency gains)
> *   **Citations:** 10 references

---

## üìù Executive Summary

Standard Autoregressive Language Models (ALMs) are fundamentally limited by a serial generation bottleneck, where each token must be produced sequentially, constraining inference throughput. While Diffusion Language Models (DLMs) offer a theoretical alternative by enabling parallel generation, they have historically been impractical for deployment. Previous DLMs suffered from rigid fixed-length decoding requirements and incompatibility with Key-Value (KV) caches, leading to high memory costs and failure to handle variable-length generation effectively. This paper addresses the critical need for a generative paradigm that retains the reasoning capabilities of pre-trained ALMs while overcoming their inherent latency constraints and the architectural friction of existing diffusion approaches.

The authors introduce the **Sequential Diffusion Language Model (SDLM)**, built upon a unified "**Next Sequence Prediction** (NSP)" framework that bridges token-level and block-level generation. Technically, SDLM utilizes a "**Longest Prefix Decoding**" mechanism within fixed-size mask blocks, employing a Structured Mask that applies causal attention to historical context and bidirectional attention within the prediction block. This allows the model to generate a fixed block of logits but adaptively decode a variable number of tokens based on prediction confidence. Crucially, the architecture supports a "**Weight Reuse**" strategy, enabling the efficient retrofitting of existing pre-trained ALMs (such as Qwen) into this diffusion paradigm using a "**Shifted-Prediction Objective**," all while maintaining full compatibility with KV caches to preserve memory efficiency.

SDLM demonstrates significant efficiency gains without compromising model performance. The architecture achieves **2.1x higher throughput** compared to the Qwen-2.5 baseline while matching or surpassing its performance using only **3.5 million training samples**. These efficiency gains are scalable; the SDLM-32B variant exhibits pronounced improvements over smaller model configurations. In benchmark evaluations, SDLM maintains a competitive speed-to-accuracy trade-off against Speculative Decoding on the MATH-500 dataset. Additionally, the dynamic block size mechanism proved effective in reducing failure rates for token predictions that rely heavily on preceding context.

This research represents a pivotal step in making diffusion-based generation a viable alternative for Large Language Models (LLMs), effectively resolving the bottlenecks that have previously relegated DLMs to theoretical interest. By demonstrating that pre-trained autoregressive models can be cost-effectively retrofitted into a high-throughput diffusion paradigm, the authors provide a practical pathway for accelerating existing LLM infrastructures without the prohibitive cost of training from scratch.

---

## üîë Key Findings

*   **High Efficiency with Low Training Cost:** Matches or surpasses baselines with only **3.5 million** training samples.
*   **Superior Throughput:** Achieves **2.1x** higher throughput compared to the Qwen-2.5 baseline.
*   **Scalability:** The **SDLM-32B** variant exhibits pronounced efficiency gains, indicating that benefits scale with model size.
*   **Adaptive Generation:** Utilizes dynamic decoding based on model confidence to mitigate the fixed-length decoding limitations common in previous diffusion models.

---

## üß© Methodology

The authors introduce **Next Sequence Prediction (NSP)**, a unified framework designed to combine next-token and next-block prediction. This allows the system to adaptively determine generation length.

The **Sequential Diffusion Language Model (SDLM)** is constructed upon this NSP framework to retrofit pre-trained autoregressive language models (ALMs) with minimal cost. The core approach involves:

*   **Dynamic Inference Mechanism:** Decoding consecutive subsequences based on confidence levels within fixed-size mask blocks.
*   **KV-Cache Preservation:** Ensuring the method remains compatible with key-value (KV) caches, a critical requirement for memory efficiency in LLMs.

---

## üèóÔ∏è Technical Details

The technical implementation of SDLM relies on several novel architectural components and training strategies:

*   **Next Sequence Prediction (NSP) Paradigm:** A hybrid approach combining Autoregressive and Diffusion models to leverage the strengths of both.
*   **Longest Prefix Decoding:** A mechanism to dynamically adjust output sequence length based on prediction confidence. It generates a fixed block of logits but decodes an adaptive number of tokens.
*   **Structured Mask for Attention:**
    *   **Causal Attention:** Applied strictly to historical context.
    *   **Bidirectional Attention:** Applied within the prediction block to allow tokens to inform one another during generation.
*   **Shifted-Prediction Objective:** A specific loss function minimization technique tailored for this architecture.
*   **Weight Reuse:** A strategy that allows the model to initialize with pre-trained ALM weights, significantly reducing the computational resources required for training.

---

## ‚úÖ Contributions

1.  **Unified Prediction Framework:** The introduction of Next Sequence Prediction (NSP) successfully bridges the gap between token-level and block-level generation.
2.  **Viable Diffusion Architecture for LLMs:** The proposal of SDLM addresses critical bottlenecks of previous Diffusion Language Models‚Äîspecifically fixed-length decoding and KV-cache incompatibility‚Äîwithout incurring expensive training costs.
3.  **Practical Retrofitting Strategy:** Demonstrates that pre-trained autoregressive language models can be effectively converted into this diffusion paradigm with a low resource investment.

---

## üìà Results

*   **Throughput:** SDLM achieved **2.1x higher throughput** compared to the Qwen-2.5 baseline.
*   **Training Efficiency:** It matched or surpassed baseline performance using only **3.5 million training samples**.
*   **Scalability:** Efficiency gains scale with model size, with the SDLM-32B variant showing the most pronounced improvements.
*   **Benchmark Performance:** The method demonstrates a competitive speed/accuracy trade-off on the **MATH-500** benchmark when compared to Speculative Decoding.
*   **Context Handling:** The dynamic block size mechanism reduced failure rates in token predictions that require heavy reliance on previous context.

---

*Report generated based on analysis of 10 citations.*