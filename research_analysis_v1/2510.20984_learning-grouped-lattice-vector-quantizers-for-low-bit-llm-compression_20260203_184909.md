---
title: Learning Grouped Lattice Vector Quantizers for Low-Bit LLM Compression
arxiv_id: '2510.20984'
source_url: https://arxiv.org/abs/2510.20984
generated_at: '2026-02-03T18:49:09'
quality_score: 7
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Learning Grouped Lattice Vector Quantizers for Low-Bit LLM Compression

*Xi Zhang; Xiaolin Wu; Jiamang Wang; Weisi Lin*

***

> ### üìä Quick Facts
> * **Quality Score**: 7/10
> * **References**: 40 Citations
> * **Methodology**: Post-Training Quantization (PTQ)
> * **Target Bit-width**: Ultra-low (e.g., 2-bit)
> * **Search Complexity**: O(log m)
> * **Default Group Size**: g=128

***

## üìã Executive Summary

### Problem
Deploying Large Language Models (LLMs) is severely constrained by their massive memory footprint and computational costs. While Post-Training Quantization (PTQ) is the standard solution for compression, existing methods face a critical trade-off: standard uniform quantization (e.g., Round-to-Nearest) suffers catastrophic performance degradation at ultra-low bit-widths (2-bit or lower), while advanced state-of-the-art techniques (such as QuIP# or AQLM) preserve accuracy but introduce substantial decoding overhead. This overhead often negates the efficiency gains of compression, particularly in methods requiring complex random rotations or expensive look-up operations.

### Innovation
The authors introduce **Grouped Lattice Vector Quantization (GLVQ)**, a novel PTQ framework that replaces uniform scalar quantization with heterogeneous, group-specific lattice codebooks defined by learnable generation matrices. To overcome the non-differentiability of lattice quantization, the method employs **Babai rounding** to approximate the nearest-lattice-point search, enabling stable end-to-end training. The framework integrates **Salience-Determined Bit Allocation (SDBA)**, utilizing a constrained optimization strategy and a "double-pointer search" to assign optimal integer bit-widths to weight groups based on sensitivity scores. Crucially, the decoding pipeline is architected to collapse into a highly efficient matrix-vector multiplication.

### Results
GLVQ demonstrates a superior balance between compression and accuracy across the LLaMA-2 family (7B, 13B, and 70B parameters). In empirical benchmarks, GLVQ operating at 2-bit achieves perplexity scores on WikiText-2 and C4 datasets that are comparable to, and often exceed, 3-bit state-of-the-art baselines such as QuIP# and OmniQuant. The method supports mixed-precision allocation within the set $\{N-1, N, N+1\}$. Most importantly, GLVQ delivers decoding throughput that is significantly higher than rotation-based baselines, achieving speeds comparable to standard PTQ methods.

### Impact
This research represents a significant advancement in model compression by proving that learnable lattice vector quantization is a viable, high-performance alternative to standard PTQ for LLMs. By successfully resolving the non-differentiability challenge through Babai rounding, GLVQ allows for the training of complex quantization schemes without compromising precision. The framework's ability to maintain near-original inference speeds while achieving ultra-low bit-widths bridges the gap between theoretical compression limits and practical deployment requirements for resource-constrained edge devices.

***

## üîë Key Findings

*   **Superior Trade-off:** The GLVQ framework achieves a better balance between model size and accuracy compared to existing post-training quantization (PTQ) baselines.
*   **Mitigated Degradation:** GLVQ effectively reduces the performance loss typically associated with standard uniform quantization in low-bit scenarios.
*   **Stable Optimization:** The use of Babai rounding enables the stable optimization of learnable generation matrices, solving the non-differentiability issue of the quantization process.
*   **Efficient Decoding:** The decoding process is highly efficient, reducing to a simple matrix-vector multiplication, which facilitates practical deployment.

## üõ†Ô∏è Methodology

The authors introduce the **Grouped Lattice Vector Quantization (GLVQ)** framework, a post-training quantization method specifically designed for Large Language Models. The methodology operates through the following workflow:

1.  **Grouped Assignment:** The framework assigns specific groups of weights customized lattice codebooks, which are defined by learnable generation matrices.
2.  **Optimization via Babai Rounding:** To address the challenge of non-differentiability inherent in the quantization process during training, the method utilizes **Babai rounding**. This technique approximates the nearest-lattice-point search, allowing for the stable optimization of the generation matrices.
3.  **Streamlined Decoding:** Once trained, the quantization pipeline is optimized so that decoding consists of a computationally efficient matrix-vector multiplication, removing bottlenecks found in other vector quantization methods.

## üß† Technical Details

The paper proposes Grouped Lattice Vector Quantization (GLVQ), a PTQ framework for LLMs designed for ultra-low bit-widths (e.g., 2-bit). The architecture deviates from uniform quantization by employing heterogeneous, group-specific lattice codebooks and non-linear companding.

*   **1) Grouped Lattice Structure:**
    *   The weight matrix is partitioned column-wise.
    *   Each group is assigned a specific lattice codebook defined by a generation matrix.
    *   **Encoding:** Uses Babai rounding for tractability.
    *   **Decoding:** Uses matrix-vector multiplication.

*   **2) Salience-Determined Bit Allocation (SDBA):**
    *   A constrained optimization approach to assign integer bit-widths based on sensitivity/salience.
    *   Minimizes KL divergence subject to balance constraints.
    *   Utilizes a 'double-pointer search' with **O(log m)** complexity.

*   **3) Group-Specific Companding:**
    *   Applies non-linear companding functions (referencing $\mu$-law) per group to handle dynamic ranges.
    *   The optimization strategy treats bit allocation and lattice learning as sequential sub-tasks.

## ‚úÖ Contributions

*   **Novel Quantization Framework:** The development of GLVQ, which moves beyond standard uniform quantization by employing grouped, learnable lattice codebooks to better preserve model performance at low bit-widths.
*   **Optimization Strategy:** The adoption of Babai rounding as a viable approximation technique to solve the non-differentiable quantization problem, enabling end-to-end training of lattice parameters.
*   **Practical Deployment Efficiency:** The provision of a quantization pipeline that balances high compression rates with an efficient decoding mechanism (matrix-vector multiplication), making it suitable for deploying large models under stringent resource constraints.

## üìà Results

The analysis outlines the following quantitative claims and methodological metrics:

*   **Performance:** GLVQ-2bit achieves perplexity scores comparable to or exceeding 3-bit baselines (like QuIP# and OmniQuant), effectively mitigating the "accuracy cliff" seen in low-bit regimes.
*   **Comparison:** The method outperforms standard 4-bit RTN methods and is competitive with advanced methods like GPTQ, ZeroQuant, QuIP#, and AQLM.
*   **Metrics:**
    *   Default group size: **g=128**
    *   Mixed-precision bit allocation sets: **{N-1, N, N+1}**
    *   Search complexity: **O(log m)**
*   **Efficiency:** Decoding throughput is significantly higher than rotation-based baselines, validating the efficiency of the matrix-vector multiplication approach.