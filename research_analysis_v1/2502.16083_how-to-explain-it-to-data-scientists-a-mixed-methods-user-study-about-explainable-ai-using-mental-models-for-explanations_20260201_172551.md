# How to explain it to data scientists? A mixed-methods user study about explainable AI, using mental models for explanations

*Helmut Degen; Ziran Min; Parinitha Nagaraja*

***

> ### ðŸ“Š Quick Facts
>
> *   **Quality Score:** 8/10
> *   **Study Type:** Mixed-Methods (Qualitative & Quantitative)
> *   **Participants:** 24 Technical Practitioners (12 + 12)
> *   **Core Focus:** Data Scientist Mental Models & Uncertainty Premise
> *   **Key Output:** 8-Type Taxonomy & Causal Story Structure

***

## Executive Summary

### Problem
Current Explainable AI (XAI) research predominantly targets end-users, leaving a critical capability gap for technical practitioners like data scientists who manage the Machine Learning lifecycle. These professionals require explainability to debug, deploy, and maintain systems, yet existing tools fail to align with their cognitive processes or address the inherent "uncertainty premise" of AI (such as opacity, non-determinism, and data quality issues). This misalignment prevents engineers from effectively validating or trusting intelligent systems. Consequently, the lack of a standardized framework tailored to the specific mental models of data scientists hampers the broader adoption of AI in high-stakes, complex engineering environments.

### Innovation
This paper introduces a role-specific XAI framework grounded in a mental model rigorously validated for technical practitioners. Technically, the framework utilizes a Goal/Metric/Questionnaire (GQM) approach to synthesize explanation content from three specific domains:
*   **Application Domain:** Contextual relevance.
*   **System Domain:** Operational mechanics.
*   **AI Domain:** Model behavior.

Rather than simple feature highlighting, the framework organizes this content into "causal stories" using a precise taxonomy of eight explanation types:
*   **Context & Inputs:** What the system sees.
*   **Evidence & Attributes:** Justification for decisions.
*   **Ranked Lists & Interim Results:** Step-by-step logic.
*   **Efficacy Principles & I/O Relationships:** Performance boundaries.

This structure links five explanation intents (reason, comparison, accuracy, prediction, trust) to specific user operational goals (justify, control, improve, discover), enabling practitioners to trace logic and verify autonomous corrections.

### Results
The study validated the proposed model through a rigorous two-phase mixed-methods approach involving **24 technical practitioners** (12 participants in the qualitative phase to refine task-specific needs, and 12 in the quantitative phase to validate the mental model structure). Results confirmed that explanation utility is maximized when content is structured as causal stories and strictly aligned with user goals. The validation process demonstrated that this framework resolves critical gaps in current mental modelsâ€”specifically by eliminating stimulus dependency and incorporating missing iterative feedback loops. Additionally, the research established that effective explanations must address five technical root causes of uncertainty (statistical nature, opacity, non-determinism, data quality, and hallucinations) and be elicited from target users prior to system design.

### Impact
By shifting the focus of XAI from end-user transparency to developer-centric utility, this research provides a blueprint for the next generation of MLOps tools. The introduction of standardized explanation questions and a comprehensive taxonomy of eight content types enables the design of systems that actively support the "futuristic intelligent system" vision of semi-automated lifecycle management. This work successfully bridges the cognitive gap between complex AI behaviors and the engineers responsible for them, offering a reproducible methodology for integrating explainability into high-stakes engineering workflows.

***

## Key Findings

*   **Source Synthesis:** Effective explanation content for data scientists must be derived from the intersection of three specific sources: the **application domain**, the **system domain**, and the **AI domain**.
*   **Structural Organization:** Explanation content requires organization through sequential and/or hierarchical structures, ideally presented as a **causal story** to aid comprehension.
*   **Eight Critical Content Types:** Analysis revealed eight specific types of explanation content required by practitioners:
    1.  Context
    2.  Inputs
    3.  Evidence
    4.  Attributes
    5.  Ranked lists
    6.  Interim results
    7.  Efficacy principles
    8.  Input/output relationships
*   **Quality Assurance:** Explanation quality is enhanced through the use of standardized explanation questions and the active refinement of mental models.

## Methodology

The research utilized a **mixed-methods approach** executed in two distinct phases:

*   **Phase 1 (Qualitative):**
    *   Involved task analysis and mapping task-specific needs via explanation questions.
    *   Aimed to form a mental model.
    *   Validated through a study with **12 participants**.
*   **Phase 2 (Quantitative):**
    *   Consisted of a follow-up study with **12 participants**.
    *   Focused on analyzing the relationship between explanation intents (reason, comparison, accuracy, prediction, trust) and the defined content types.

## Contributions

*   **Role-Specific Framework:** Introduction of an XAI framework addressing the specific needs of data scientists rather than end-users.
*   **Validated Mental Model:** A mental model categorizing how data scientists process explanations by linking intents to content requirements.
*   **Standardization:** The standardization of explanation generation using standardized questions to ensure coverage of user needs.
*   **Organizational Principles:** Novel organizational principles for XAI, proposing that explanation content be structured sequentially or hierarchically to manage complexity.

## Technical Details

### System Concept
The study proposes a **'futuristic intelligent system'** designed to semi-automate the Machine Learning lifecycle (generation, deployment, maintenance) with autonomous correction capabilities.

### The Uncertainty Premise
The framework is grounded in the identification of five technical root causes of AI uncertainty:
1.  Statistical nature
2.  Opacity (black box)
3.  Non-determinism
4.  Data quality
5.  Hallucinations

### Approach & Validation
*   **GQM Methodology:** Utilizes the Goal/Metric/Questionnaire (GQM) approach.
*   **Mental Model Development:** Employs a six-step mental model development process.
*   **Evolutionary Steps:** Introduces improvement steps (2, 3, and 4) to validate models prior to implementation.

### Explanation Architecture
The architecture synthesizes content from three domains, organizing it hierarchically or as a causal story. It utilizes a specific taxonomy of eight content types: **Context, Inputs, Evidence, Attributes, Ranked lists, Interim results, Efficacy principles,** and **Input/output relationships**.

## Results

*   **Domain Intersection:** Qualitative findings reveal that data scientists require explanations derived from the intersection of Application, System, and AI domains, structured sequentially or hierarchically as causal stories.
*   **Utility Determination:** Explanation utility is determined by alignment with specific intents (Justify, control, improve, discover).
*   **Validation Criteria:** The study establishes strict criteria requiring that methods:
    *   Capture uncertainty-addressing explanations.
    *   Demonstrate elicitation and validation with target users.
    *   Perform this validation *before* system design.
*   **State of the Art Gaps:** The text identifies gaps in current XAI mental models, noting issues such as stimulus dependency, lack of specific content identification, absence of target user representatives, and missing iterative feedback loops.

---
**References:** 40 citations