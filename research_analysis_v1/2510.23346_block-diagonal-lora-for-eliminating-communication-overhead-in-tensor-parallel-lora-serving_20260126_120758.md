---
title: Block-Diagonal LoRA for Eliminating Communication Overhead in Tensor Parallel
  LoRA Serving
arxiv_id: '2510.23346'
source_url: https://arxiv.org/abs/2510.23346
generated_at: '2026-01-26T12:07:58'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# Block-Diagonal LoRA for Eliminating Communication Overhead in Tensor Parallel LoRA Serving

*Xinyu Wang, Amazon Web, Santa Clara; Kailash Budhathoki; Diagonal Lo; Yida Wang; Eliminating Communication; Tensor Parallel*

***

> ### ðŸ“Š Quick Facts
> | Metric | Detail |
> | :--- | :--- |
> | **Top Speed-Up** | **1.79x** for Llama-3.1-70B (vs S-LoRA) |
> | **Efficiency Gain** | **1.63x** speed-up for Llama-3.1-8B |
> | **Parameter Efficiency** | Pareto-dominates baseline; achieves better speed with fewer params (0.87x) |
> | **Hardware** | Tested on 8 NVIDIA A100 GPUs |
> | **Quality Score** | **9/10** |

***

## Executive Summary

Efficiently serving Large Language Models (LLMs) fine-tuned with Low-Rank Adaptation (LoRA) across multiple GPUs via Tensor Parallelism (TP) faces a critical system bottleneck. While TP distributes the base model effectively, existing serving frameworks like S-LoRA introduce significant communication overhead when synchronizing LoRA adapters. These methods require cross-device operations, such as all-gather and all-reduce, during inference to aggregate adapter weights. As model sizes and the number of concurrent adapters increase, this communication overhead creates a severe latency bottleneck, limiting the throughput and scalability of high-performance serving systems.

To address this, the authors propose **Block-Diagonal LoRA (BD-LoRA)**, a structural modification to the LoRA architecture designed specifically for TP environments. Instead of using standard dense factor matrices, BD-LoRA constrains specific matrices ($B_1$ and $A_2$) to a block-diagonal structure. This constraint allows the LoRA adapters to be sharded across devices in a pattern that perfectly aligns with the base modelâ€™s existing weight sharding strategy (e.g., Megatron-LM). Consequently, intermediate results can be summed locally on each GPU rather than requiring global synchronization, eliminating the need for dedicated communication steps for LoRA layers while maintaining parameter efficiency through rsLoRA scaling.

Experimental evaluation on Llama-3.1 models utilizing eight NVIDIA A100 GPUs demonstrates substantial performance gains over S-LoRA. BD-LoRA achieved a **1.79x speed-up** in end-to-end latency for the Llama-3.1-70B model and a **1.63x speed-up** for the Llama-3.1-8B model. Notably, the method offers flexible performance scaling; for the 70B model, it achieved a 1.79x speed-up using only 87% of the parameters required by the baseline, and a 1.23x speed-up using 174% of the parameters.

This research establishes a new paradigm for optimizing multi-adapter serving systems by decoupling adapter computation from costly communication primitives. By demonstrating that architectural constraints can be leveraged to solve system-level throughput issues without sacrificing model accuracy, BD-LoRA provides a practical path forward for deploying massive models in production environments.

## Key Findings

*   **Parameter Efficiency:** The proposed block-diagonal LoRA maintains parameter efficiency comparable to standard LoRA, achieving similar downstream performance with an equivalent number of parameters.
*   **Communication Elimination:** This approach eliminates the need for additional communication during LoRA computations in tensor parallel setups, removing a bottleneck present in previous methods like S-LoRA.
*   **Significant Speed-Ups:** Achieved up to **1.79x speed-up** for Llama-3.1-70B and **1.63x** for Llama-3.1-8B when serving on eight A100 GPUs.
*   **Flexible Scaling:** For Llama-3.1-70B, the method achieved a 1.79x speed-up with fewer parameters (0.87x) and a 1.23x speed-up with more parameters (1.74x).

## Methodology

The researchers propose constraining specific LoRA factors to adopt a **block-diagonal structure**. This structural constraint enables an alternative method for sharding LoRA adapters across multiple devices. The sharding strategy is designed to align perfectly with the base modelâ€™s tensor parallel execution, thereby removing the requirement for any additional communication specifically for the LoRA computations.

## Technical Details

*   **Architecture:** Block-Diagonal LoRA (BD-LoRA) imposes a block-diagonal constraint on specific LoRA factor matrices ($B_1$ and $A_2$).
*   **Alignment:** The design aligns with Megatron-LM Tensor Parallelism sharding strategies ($A_1$ column-wise, $B_2$ row-wise).
*   **Communication Reduction:** Eliminates cross-device communication operations (all-gather and all-reduce) required by S-LoRA by allowing intermediate results to be summed locally on each device.
*   **Implementation:** Effectively adds independent adapters to each weight shard.
*   **Optimization:** Employs rsLoRA scaling ($\alpha/\sqrt{r}$) to maintain stable learning rates.

## Results

*   **Hardware Environment:** 8 NVIDIA A100 GPUs.
*   **Latency Reduction:**
    *   Llama-3.1-70B: **1.79x** speed-up.
    *   Llama-3.1-8B: **1.63x** speed-up.
*   **Parameter Efficiency:** Reached a 1.79x speed-up using only 0.87x the parameters of the baseline.
*   **Benchmark Performance:**
    *   Strictly **Pareto-dominates S-LoRA** on Perplexity (OpenOrca) and GLUE benchmarks.
    *   Provides lower latency for equivalent performance.
*   **System Integration:** Removed the significant communication overhead bottleneck present in practical implementations like vLLM.

## Contributions

*   **Novel Solution:** Presents a novel solution to the communication overhead problem inherent in multi-device, multi-adapter serving scenarios, specifically addressing the limitations of the S-LoRA sharding strategy.
*   **Architecture & System Synergy:** Demonstrates that architectural constraints (block-diagonal factors) can be leveraged to improve system throughput (end-to-end speed) without sacrificing the model's parameter efficiency or accuracy.
*   **Validation:** Provides extensive experimental validation on state-of-the-art large language models (Llama-3.1-8B and 70B), proving the practical efficacy of the approach in high-performance GPU environments.

***

**Quality Score:** 9/10 | **References:** 40 citations