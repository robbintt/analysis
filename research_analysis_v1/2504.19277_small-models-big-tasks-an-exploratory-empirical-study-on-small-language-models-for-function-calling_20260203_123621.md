---
title: 'Small Models, Big Tasks: An Exploratory Empirical Study on Small Language
  Models for Function Calling'
arxiv_id: '2504.19277'
source_url: https://arxiv.org/abs/2504.19277
generated_at: '2026-02-03T12:36:21'
quality_score: 7
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Small Models, Big Tasks: An Exploratory Empirical Study on Small Language Models for Function Calling

*Ishan Kavathekar; Raghav Donakanti; Ponnurangam Kumaraguru; Karthik Vaidhyanathan*

---

### üìä Quick Facts

| Metric | Details |
| :--- | :--- |
| **Quality Score** | 7/10 |
| **References** | 40 Citations |
| **Dataset Size** | 60,000 JSON samples |
| **Test Domains** | Technology, Entertainment, Finance |
| **Target Hardware** | NVIDIA Jetson Orin Nano |
| **Models Evaluated** | Llama-3-8B, Mistral-7B-v0.2, Phi-3-mini, Gemma-2B, Qwen-1.8B |

---

## üìù Executive Summary

The proliferation of Large Language Models (LLMs) has enabled advanced function calling, yet the massive computational cost and memory footprint of these models preclude their deployment on resource-constrained edge devices. This paper addresses the critical gap in determining whether Small Language Models (SLMs)‚Äîranging from millions to billions of parameters‚Äîcan effectively replicate the complex reasoning and formatting required for function calling. Establishing this viability is essential for enabling privacy-preserving, low-latency AI applications on local hardware, removing the dependency on cloud-based infrastructure.

The researchers introduced a rigorous empirical framework to evaluate five specific SLMs: Llama-3-8B, Mistral-7B-v0.2, Phi-3-mini, Gemma-2B, and Qwen-1.8B. The study systematically compared three inference strategies‚Äîzero-shot, few-shot, and fine-tuning (utilizing LoRA, Prompt Tuning, and Prefix Tuning)‚Äîto assess model adaptability. To bridge theoretical performance with practical deployment, the methodology involved converting models to the GGUF format for 4-bit quantization and executing them directly on NVIDIA Jetson Orin Nano hardware. Furthermore, the study evaluated system security by subjecting the models to Direct Prompt Injection attacks, testing their robustness in adversarial conditions.

Evaluation across a diverse dataset of 60,000 JSON samples spanning technology, entertainment, and finance utilized specific metrics: Syntactic Correctness, Semantic Accuracy, and Structured Output Capability. Results revealed a distinct performance hierarchy: fine-tuned models significantly outperformed few-shot learning, which in turn surpassed zero-shot settings. While SLMs demonstrated strong proficiency in semantic accuracy and logic generation, they exhibited significant deficiencies in strict output format adherence. Despite these formatting challenges, the models proved generally robust against prompt injection attacks with only a slight performance decline. Hardware benchmarks confirmed that SLMs offer the low latency and memory efficiency necessary for on-device processing on the Jetson Orin Nano.

This work significantly advances the field by providing the first granular benchmark for SLM capabilities specifically within the domain of function calling, challenging the assumption that only LLMs can handle such structured tasks. By releasing the fine-tuned models to the community, the authors lower the barrier to entry for developing efficient, on-device AI tools. The identification of "output format adherence" as a primary bottleneck offers a clear direction for future research, indicating that while SLMs are architecturally ready for edge deployment, targeted improvements in constrained generation are necessary to transition them from experimental prototypes to reliable production systems.

---

## üîç Key Findings

*   **Performance Hierarchy:** SLMs exhibit a clear progression in efficacy, performing best under **fine-tuning regimes**, followed by few-shot learning, and lastly zero-shot settings.
*   **Output Format Limitations:** Despite decent performance in logic generation, SLMs struggle significantly with adhering to **strict output formats**, which is a critical requirement for function calling.
*   **Robustness to Attacks:** The models demonstrate general robustness against prompt injection attacks, with experiments showing only a **slight decline in performance**.
*   **Edge Device Viability:** While SLMs offer faster response times and lower computational demands suitable for edge devices, further refinement is necessary to ensure they function effectively in **real-time scenarios**.

---

## üõ† Methodology

The researchers conducted an exploratory empirical study evaluating Small Language Models (SLMs) across diverse domains. The study employed three distinct learning strategies:

1.  **Zero-shot**
2.  **Few-shot**
3.  **Fine-tuning**

To assess security and stability, experiments were conducted both with and without prompt injection. The evaluation was twofold:
*   **Quality Analysis:** Analyzing the quality of function call generation using various metrics.
*   **Performance Measurement:** Measuring practical performance (latency and memory usage) by running experiments directly on an edge device.

---

## ‚öô Technical Details

### Models Evaluated
The study evaluates 5 Small Language Models (SLMs) selected based on coding benchmarks and parameter counts:
*   Llama-3-8B
*   Mistral-7B-v0.2
*   Phi-3-mini
*   Gemma-2B
*   Qwen-1.8B

### System Characteristics
*   **SLM Attributes:** Characterized by lower memory footprints and faster inference.
*   **Inference Strategies:**
    *   Zero-shot
    *   Few-shot (In-context)
    *   Fine-tuning (using LoRA, Prompt Tuning, and Prefix Tuning)
*   **Edge Deployment:** Models are converted to **GGUF format** for quantization and efficiency.
*   **Security Testing:** Robustness is tested against **Direct Prompt Injection** attacks using adversarial text.

---

## üìà Results

The study utilized a dataset of **60,000 JSON samples** across technology, entertainment, and finance.

### Evaluation Metrics
*   Syntactic Correctness
*   Semantic Accuracy
*   Structured Output Capability

### Performance Analysis
*   **Strategy Ranking:** Fine-tuning performs best, followed by Few-shot, then Zero-shot.
*   **Logic vs. Format:** While SLMs are strong in logic generation, they struggle with strict output formats.
*   **Security:** Models showed general robustness to direct prompt injection with only slight performance decline.
*   **Hardware Performance:** SLMs are viable for edge devices with low latency, but require refinement for consistent real-time effectiveness. A relationship was established between **memory footprint and latency**.

---

## ‚ú® Contributions

*   **Benchmarking SLMs for Function Calling:** Provided a comprehensive evaluation of SLM capabilities specifically for function calling tasks, contrasting them with the standard use of computationally expensive Large Language Models (LLMs).
*   **Resource Release:** Released the finetuned models to the community to facilitate future research and application development in this domain.
*   **Practical Insights for Edge Deployment:** Offered concrete data regarding the latency and memory usage of SLMs on edge hardware, bridging the gap between theoretical performance and practical resource-constrained applications.
*   **Identification of Critical Failure Modes:** Highlighted the specific challenge of output format adherence, providing a clear direction for necessary future improvements in real-time automated systems.