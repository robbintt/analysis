---
title: Scene-Aware Conversational ADAS with Generative AI for Real-Time Driver Assistance
arxiv_id: '2507.105'
source_url: https://arxiv.org/abs/2507.10500
generated_at: '2026-02-03T06:27:03'
quality_score: 9
citation_count: 14
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Scene-Aware Conversational ADAS with Generative AI for Real-Time Driver Assistance

*Kyungtae Han; Yitao Chen; Rohit Gupta; Onur Altintas*

***

> ### ðŸ“Š Quick Facts
> *   **Quality Score:** 9/10
> *   **References:** 14 Citations
> *   **Simulation Environment:** CARLA
> *   **Avg. Latency (Scene-Aware):** ~11.00s
> *   **Avg. Latency (Conversational-Only):** ~3.00s
> *   **Max Output Tokens:** <500
> *   **Vision Module Overhead:** ~8s (Range: 8â€“17s)

***

## Executive Summary

Current Advanced Driver Assistance Systems (ADAS) suffer from a rigid interaction paradigm, relying on predefined logic that limits their ability to interpret complex, context-dependent driver intents. As vehicles become more autonomous, the interface between human and machine must evolve from simple command execution to dynamic, dialogue-based collaboration.

This paper addresses the critical challenge of bridging the interaction gap by enabling vehicles to understand natural language within the context of the surrounding driving scene, a capability essential for intuitive, trustworthy, and adaptable driver assistance systems. The researchers introduce **SC-ADAS**, a modular framework that integrates conversational reasoning and scene perception without requiring model fine-tuning.

Technically, the system employs a "Reasoning + Acting" approach using cloud-based Generative AI components, combining Large Language Models (LLMs) with vision-to-text interpretation. The architecture consists of four sequential modules: the **Refiner** (manages conversation history), the **Vision Module** (processes visual data), the **Actuator** (translates natural language into structured ADAS commands via function calling), and the **Responder** (generates natural language replies).

This design allows for multi-turn dialogues grounded in real-time sensor data, adapting dynamically to diverse road and weather conditions within a simulated environment. Evaluation within the CARLA simulator confirmed the system's functional capability to translate natural language intents into executable commands, but highlighted significant computational trade-offs.

The Scene-Aware service incurred an average latency of approximately **11.00 seconds**, a substantial increase compared to the **3.00 seconds** observed in the Conversational-Only service. This 8-second overhead is attributed primarily to the Vision Module. Additionally, token analysis revealed a linear growth in input tokens due to dialogue history accumulation, while output tokens remained stable under 500.

This work demonstrates the feasibility of integrating generative AI into ADAS control loops, shifting the paradigm from reactive, rule-based systems to proactive, context-aware agents. By proving that complex driver-vehicle interaction can be achieved without fine-tuning, the authors provide a scalable blueprint for adaptive automotive interfaces. However, the empirical identification of latency bottlenecks serves as a crucial benchmark for the field, underscoring the immediate need for optimization in vision-based retrieval systems to meet the safety-critical timing requirements of real-world driving scenarios.

***

## Key Findings

*   **Feasibility:** Successful integration of conversational reasoning, scene perception, and modular ADAS control achieved **without model fine-tuning**.
*   **Translation:** Effective translation of natural language driver intents into structured ADAS commands within a simulated environment.
*   **Trade-offs:** Identification of operational trade-offs, specifically increased latency from vision-based context retrieval and computational burden from dialogue history.
*   **Multi-turn Grounding:** Demonstration of capability for multi-turn dialogues grounded in real-time visual and sensor data.

***

## Methodology

The researchers developed **SC-ADAS**, a modular framework implemented within the **CARLA simulator**. The methodology utilizes cloud-based Generative AI components, integrating Large Language Models (LLMs) with vision-to-text interpretation capabilities. The system employs **structured function calling** to bridge natural language inputs with executable ADAS commands, enabling the processing of visual and sensor context for multi-turn, real-time interactions.

***

## Technical Details

The SC-ADAS framework is a modular, multi-agent system integrating Large Language Models (LLMs) with vision processing and vehicle control without requiring model fine-tuning.

### Architecture Modules
The system consists of four sequential modules:
1.  **Refiner:** Manages conversation history and context.
2.  **Vision Module:** Performs image processing and scene interpretation. *Note: Acts as a computational bottleneck.*
3.  **Actuator:** Translates natural language intents into structured ADAS commands.
4.  **Responder:** Generates natural language responses.

### System Characteristics
*   **Approach:** Uses a 'Reasoning + Acting' methodology.
*   **Context Handling:** Manages multi-turn dialogues by accumulating context, which increases input token load.
*   **Service Configurations:**
    *   Conversational-Only
    *   Conversational ADAS
    *   Scene-Aware

***

## Contributions

*   **Interaction Gap:** Bridged the gap by introducing a system capable of interpreting scene context and engaging in dialogue-based interaction rather than relying on predefined logic.
*   **No-Fine-Tuning Architecture:** Proposed a modular architecture that leverages generative AI and structured function calling for adaptive driver assistance without the need for fine-tuning.
*   **Empirical Evaluation:** Provided an empirical evaluation of generative AI trade-offs in ADAS, specifically analyzing the impact of visual context retrieval and dialogue history on system latency.

***

## Results

Evaluation in a simulated environment yielded the following data:

*   **Latency:** The Scene-Aware service has an average latency of **~11.00 seconds** compared to **~3.00 seconds** for Conversational-Only.
    *   This adds approximately 8 seconds of overhead primarily due to the Vision module (ranging from 8 to 17 seconds).
*   **Token Analysis:**
    *   **Input Tokens:** Showed linear growth due to history accumulation (dominated by the Refiner).
    *   **Output Tokens:** Remained stable under 500.
*   **Functional Performance:** The system effectively translated natural language intents into structured commands and adapted to diverse road and weather conditions, though slight variations in recommendations were observed.