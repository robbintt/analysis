# Attack logics, not outputs: Towards efficient robustification of deep neural networks by falsifying concept-based properties

*Raik Dankworth; Gesina Schwalbe*

---

### ðŸ“Š Quick Facts

| Metric | Details |
| :--- | :--- |
| **Quality Score** | 6/10 |
| **References** | 40 Citations |
| **Core Focus** | Logic Verification & XAI |
| **Primary Innovation** | Concept-based Falsification |
| **Target Model** | Deep Neural Networks (DNNs) |

---

## Executive Summary

Current verification methods for Deep Neural Networks (DNNs) focus narrowly on ensuring that final output classifications remain stable under noise. While this confirms "label robustness," it fails to validate the model's internal reasoning, allowing systems to reach correct conclusions using spurious correlations or irrelevant features. This creates a significant reliability gap in safety-critical applications where the validity of the decision-making process is as important as the final answer. Current metrics cannot detect these logical inconsistencies, leaving AI systems vulnerable to failures that standard noise-based tests miss.

This paper introduces a verification method that targets **logical reasoning** rather than just output labels. The authors use Explainable AI (XAI) to extract human-interpretable concepts (such as color or shape) from pre-trained networks. These concepts are treated as variables to define explicit logical constraints, such as `IF Red AND Octagonal THEN Stop_Sign`. Instead of searching for perturbations that flip a class label, the framework searches for inputs that violate these logical implications. By operating in this lower-dimensional concept space, the approach aims to identify vulnerabilities more efficiently than traditional methods that rely on high-dimensional pixel inputs.

The authors tested this method on traffic sign classification tasks, successfully generating adversarial examples that broke specific logical rules. For instance, the method produced inputs that the network classified as "Stop Signs" despite the concepts indicating the object was not both red and octagonal. These results demonstrate that vulnerabilities extend deep into the model's reasoning process, not just its output layer. Although the paper does not provide specific runtime speedups or dataset-wide success rates, it qualitatively shows that retraining the model to fix these logical violations simultaneously improves both logical compliance and robustness against standard attacks.

This work offers a practical path toward building **semantically trustworthy AI** by aligning technical verification with human logic. It provides developers with a tool to identify and patch non-intuitive behaviors that standard robustness checks overlook. By focusing on the "why" of a decision rather than just the "what," this approach facilitates the deployment of AI in high-stakes environments where adherence to logical safety constraints is mandatory.

---

## Key Findings

*   **Generalization of Attacks:** Adversarial attacks generalize to falsifying concept-based properties (logical constraints) involving human-interpretable concepts, rather than just final output classifications.
*   **Reduced Search Space:** Attacks targeting concept-based properties operate within a reduced search space compared to simple class falsification.
*   **Human-Aligned Robustness:** Targeting logical behavior (e.g., `'red AND octagonal -> stop_sign'`) is argued to be more aligned with intuitive human robustness targets than mere class flipping.
*   **Simultaneous Improvement:** The method has the potential to simultaneously and efficiently improve both the logical compliance and the robustness of deep neural networks.

---

## Methodology

The approach shifts the verification objective from output labels to logical constraints based on human-interpretable concepts. By utilizing **Explainable Artificial Intelligence (XAI)** techniques on already trained Deep Neural Networks (DNNs), the authors implement concept-based properties to search for inputs that violate these logical constraints ("illogical behavior") and identify vulnerabilities.

**Key Steps:**
1.  **Concept Extraction:** Use XAI on pre-trained DNNs to identify interpretable concepts (e.g., shape, color).
2.  **Logical Definition:** Define properties using these concepts as variables (e.g., `Red AND Octagonal`).
3.  **Falsification:** Search for inputs that violate the defined logical constraints.

---

## Contributions

*   **Conceptual Shift:** A new verification paradigm that extends the definition of adversarial falsification from simple output class negation to the falsification of general concept-based properties (logic verification).
*   **XAI Integration:** A practical strategy for applying concept-based properties to pre-trained models using existing XAI tools.
*   **Theoretical Framework:** Insights into why attacking concept-based properties is computationally more efficient due to a reduced search space compared to traditional black-box output attacks.

---

## Technical Details

*   **Target:** Concept-based properties designed to falsify logical constraints involving human-interpretable concepts rather than final output class labels.
*   **Mechanism:** Employs an interpretable mechanism with logical constraints (e.g., `Red AND Octagonal -> Stop_Sign`) where concepts are explicit variables.
*   **Optimization:** Utilizes a reduced search space by employing lower-dimensional features (concepts) instead of high-dimensional input pixels.

---

## Results

Adversarial attacks on concept-based properties demonstrate that vulnerabilities extend deep into the model's logical reasoning. The method suggests more efficient robustification by targeting logical behavior to improve logical compliance and robustness simultaneously. Fixing logical violations aligns better with human-defined robustness than maintaining constant class labels under noise.

> **Note:** No quantitative metrics (runtime speedups or dataset-wide success rates) were provided in the text.