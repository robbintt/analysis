---
title: Generative Distribution Distillation
arxiv_id: '2507.14503'
source_url: https://arxiv.org/abs/2507.14503
generated_at: '2026-02-03T18:23:20'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Generative Distribution Distillation

*Jiequan Cui; Beier Zhu; Qingshan Xu; Xiaogang Xu; Pengguang Chen; Xiaojuan Qi; Bei Yu; Hanwang Zhang; Richang Hong*

---

### Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Quality Score** | 9/10 |
| **ImageNet Top-1 (Supervised)** | 82.28% (ResNet-50) |
| **ImageNet Improvement (Unsupervised)** | +16.29% over KL baseline |
| **Key Innovation** | Split Tokenization & Distribution Contraction |
| **References** | 40 Citations |

---

## Executive Summary

Knowledge Distillation (KD) is a critical technique for compressing large, complex teacher models into efficient student models, but traditional methods relying on KL divergence often struggle with stability and optimization in high-dimensional spaces. Specifically, conventional KD faces significant challenges in unsupervised settings where semantic labels are absent, and it frequently requires tedious hyperparameter tuning to balance loss weights. Addressing these limitations is essential for enabling effective model deployment in large-scale vision tasks where data may be unlabeled or imbalanced, and where training stability is paramount.

This paper introduces **GenDD (Generative Distribution Distillation)**, a novel framework that fundamentally reformulates Knowledge Distillation as a conditional generative problem. Instead of minimizing statistical divergence, GenDD utilizes a diffusion model to reconstruct teacher features conditioned on student features. To manage high-dimensional optimization, the authors propose **"Split Tokenization,"** which divides high-dimensional teacher representations into 64-dimensional tokens to prevent training instability. Furthermore, they introduce **"Distribution Contraction,"** a technique that integrates label supervision directly into the reconstruction objective, eliminating the need for explicit classification loss tuning and theoretically acting as a gradient-level surrogate for multi-task learning.

GenDD demonstrates substantial empirical improvements over existing baselines across various benchmarks. In unsupervised KD on ImageNet, the framework outperforms the KL divergence baseline by a significant 16.29%. In supervised settings, GenDD achieves a new state-of-the-art top-1 accuracy of 82.28% on ImageNet using a ResNet-50 backbone. The method proves robust across diverse data scenarios, performing well on balanced datasets (CIFAR-100), imbalanced data (ImageNet-LT), and unlabeled datasets (CC3M). Ablation studies confirm that Split Tokenization is essential for stabilizing the training of high-dimensional features.

This research signifies a paradigm shift in the field of model compression by successfully bridging the gap between generative diffusion models and knowledge distillation. By theoretically linking the generative process to multi-task learning at the gradient level, the authors provide a deeper understanding of how knowledge can be transferred from teacher to student. The method's ability to eliminate sensitivity to loss weight tuning, combined with its superior performance in both supervised and unsupervised regimes, establishes GenDD as a highly flexible and robust tool.

---

## Key Findings

*   **Superior Unsupervised Performance:** The GenDD framework outperforms the KL divergence baseline by **16.29%** on ImageNet.
*   **New State-of-the-Art Supervised Accuracy:** ResNet-50 achieved **82.28%** top-1 accuracy on ImageNet.
*   **Surrogate for Multi-Task Learning:** GenDD combined with Distribution Contraction acts as a gradient-level surrogate for multi-task learning.
*   **Robustness Across Data Types:** The method is effective across balanced, imbalanced, and unlabeled datasets.

---

## Methodology

The researchers reformulate Knowledge Distillation (KD) as a conditional generative problem through the GenDD framework. To address high-dimensional optimization and the lack of semantic labels, they propose two techniques:

1.  **Split Tokenization:** Used to achieve stable unsupervised KD.
2.  **Distribution Contraction:** Used to integrate label supervision into the reconstruction objective.

---

## Contributions

*   **Novel Formulation of KD:** Introduces treating knowledge distillation as a conditional generative problem.
*   **Algorithmic Innovations:** Development of 'Split Tokenization' for stable unsupervised distillation and 'Distribution Contraction' for injecting semantic labels.
*   **Theoretical Foundation:** Provides a proof linking the method to multi-task learning at the gradient level.
*   **Performance Benchmarks:** Achieves significant empirical improvements over existing baselines on large-scale tasks like ImageNet.

---

## Technical Details

*   **Core Architecture:** GenDD reformulates KD into a conditional generative task using diffusion models to reconstruct teacher features conditioned on student features.
*   **Diffusion Head:** Employs a 3-layer MLP.
*   **Split Tokenization:** Divides high-dimensional teacher representations into **64-dimensional tokens** to prevent training instability in high-dimensional spaces.
*   **Distribution Contraction:** Incorporates label supervision without requiring explicit classification loss tuning.
*   **Training Configuration:**
    *   Training Steps: 1000
    *   Inference Steps: 64
    *   Classifier-free Guidance Scale: 2.0

---

## Results

*   **Unsupervised KD:** GenDD significantly outperforms the KL divergence baseline on ImageNet by **16.29%**.
*   **Supervised KD:** Achieves a state-of-the-art top-1 accuracy of **82.28%** on ImageNet using ResNet-50.
*   **Dataset Robustness:** The method demonstrates robustness across:
    *   Balanced datasets (CIFAR-100, ImageNet)
    *   Imbalanced datasets (ImageNet-LT)
    *   Unlabeled data (CC3M)
*   **Hyperparameter Stability:** Unlike traditional KD, GenDD eliminates sensitivity to loss weight tuning.
*   **Ablation Studies:** Confirm that Split Tokenization is necessary to stabilize training with high-dimensional features.