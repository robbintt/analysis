---
title: 'Digi-Q: Learning Q-Value Functions for Training Device-Control Agents'
arxiv_id: '2502.1576'
source_url: https://arxiv.org/abs/2502.15760
generated_at: '2026-02-03T06:52:34'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Digi-Q: Learning Q-Value Functions for Training Device-Control Agents

*Hao Bai; Yifei Zhou; Li Erran Li; Sergey Levine; Aviral Kumar*

---

### **Quick Facts**

| **Metric** | **Detail** |
| :--- | :--- |
| **Quality Score** | 9/10 |
| **References** | 40 Citations |
| **Benchmark** | Android-in-the-Wild |
| **Performance Gain** | +21.2% over previous SOTA |
| **Training Method** | Offline Reinforcement Learning |
| **Core Innovation** | Frozen VLM features with Q-learning |

---

## Executive Summary

**Problem**
Training autonomous agents to control mobile devices presents significant scalability challenges due to reliance on resource-intensive on-policy reinforcement learning (RL). Traditional methods require continuous, active environment interaction to collect rollouts, which is slow and computationally expensive. Furthermore, standard approaches often necessitate fine-tuning massive Vision-Language Models (VLMs) for specific control tasks, creating a prohibitive computational burden.

**Innovation**
The paper introduces **Digi-Q**, a novel framework that decouples control policy learning from the VLM backbone. By freezing VLM weights and operating exclusively on off-policy experience, Digi-Q employs a three-step process:
1.  Fine-tuning VLM features to amplify actionable information.
2.  Training a Q-value function using offline temporal-difference learning on frozen features.
3.  Utilizing a "Best-of-N" operator for policy extraction, where the Q-function ranks and imitates the best candidate actions.

**Results**
Digi-Q demonstrates substantial performance gains, achieving a **21.2% improvement** over the previous state-of-the-art on the Android-in-the-Wild benchmark. It matches the performance of cutting-edge interactive RL methods in specific scenarios while significantly reducing computational overhead by eliminating online rollout collection.

**Impact**
This research represents a critical advancement in the efficiency of agentic AI systems. By demonstrating that high-performance control policies can be derived through offline Q-function learning on frozen VLM features, Digi-Q shifts the paradigm away from costly online fine-tuning, offering a resource-efficient pathway for developing sophisticated device-control agents.

---

## Key Findings

*   **Significant Performance Lift:** Digi-Q achieved a **21.2% improvement** over the previous best method on the Android-in-the-Wild benchmark.
*   **Competitive with Interactive RL:** The approach matches the performance of state-of-the-art interactive reinforcement learning methods in specific scenarios.
*   **Cost & Efficiency:** Training on frozen VLM features drastically reduces computational costs and enhances scalability.
*   **Offline Capability:** The framework enables policy improvement without any environment interaction, effectively addressing the high costs associated with rollout collection.

---

## Methodology

The Digi-Q framework trains VLM-based agents using off-policy experience, eliminating the need for active environmental interaction. It consists of three core components:

1.  **Feature Preparation**
    *   Fine-tuning processes are applied to amplify actionable information within the VLM features.
2.  **Q-Function Training**
    *   An action-value function is trained using offline temporal-difference learning.
    *   This training is performed on frozen VLM intermediate-layer features.
3.  **Policy Extraction**
    *   A "Best-of-N" operator is utilized to rank candidate actions generated from the current policy.
    *   The agent then imitates the best-ranked actions to update the policy.

---

## Contributions

*   **Novel Framework:** Introduction of Digi-Q, a new system for training VLM-based action-value Q-functions for mobile device control, which reduces reliance on costly on-policy RL.
*   **Offline Policy Improvement:** Demonstration of offline policy improvement using a Best-of-N operator guided by a learned value function, solving a major bottleneck in open-ended agentic tasks.
*   **Resource Efficiency:** Presentation of a training paradigm that decouples Q-function learning from the VLM backbone by freezing intermediate features.

---

## Technical Details

*   **Framework Architecture:** The agent utilizes a Q-value function learning framework to derive control policies specifically for device interactions.
*   **VLM Integration:** The system processes input using a Vision-Language Model (VLM) strictly as a **frozen feature extractor**. VLM weights are held constant during the entire training process.
*   **Offline Learning:** The approach supports offline policy improvement, allowing for the updating of policies without active environment interaction or online rollouts.
*   **Decoupling Strategy:** By decoupling control policy training from VLM training, the system significantly reduces computational burden and enhances overall scalability.

---

## Results

*   **Benchmark Success:** Digi-Q achieved a **21.2% improvement** over the previous state-of-the-art method on the Android-in-the-Wild benchmark.
*   **Performance Parity:** Matches the performance of state-of-the-art interactive Reinforcement Learning methods in specific scenarios.
*   **Computational Savings:** Reduces computational costs compared to approaches requiring fine-tuning of VLMs.
*   **Rollout Elimination:** Eliminates costs associated with environment rollout collection by enabling fully offline learning.