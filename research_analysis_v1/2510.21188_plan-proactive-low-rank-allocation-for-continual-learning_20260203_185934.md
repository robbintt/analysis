---
title: 'PLAN: Proactive Low-Rank Allocation for Continual Learning'
arxiv_id: '2510.21188'
source_url: https://arxiv.org/abs/2510.21188
generated_at: '2026-02-03T18:59:34'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# PLAN: Proactive Low-Rank Allocation for Continual Learning

*Xiequn Wang; Zhan Zhuang; Yu Zhang*

---

> ### ðŸ“Š Quick Facts
> *   **Quality Score:** 9/10
> *   **References:** 40 Citations
> *   **Key Benchmark:** Split CIFAR-100 (ViT-B/16)
> *   **Performance Gain:** +6.2% Average Accuracy (ACC) vs. standard LoRA
> *   **Primary Focus:** Parameter-Efficient Fine-Tuning (PEFT) for Foundation Models

---

## Executive Summary

**Problem**
Applying Continual Learning (CL) to large-scale Foundation Models (FMs), such as Vision Transformers (ViTs) and Large Language Models (LLMs), presents a critical challenge due to the twin issues of catastrophic forgetting and computational intractability. When fine-tuned on new tasks, massive models tend to severely degrade performance on previously learned data, while the computational cost of updating billions of parameters renders traditional full fine-tuning impractical. The research highlights an urgent need for parameter-efficient solutions that can adapt to sequential tasks by isolating task-specific knowledge, thereby preventing interference and preserving the model's foundational capabilities without the prohibitive overhead of retraining.

**Innovation**
The authors introduce **PLAN (Proactive Low-Rank Allocation for Continual Learning)**, a framework that extends Low-Rank Adaptation (LoRA) specifically for CL scenarios. The core technical innovation is a dynamic, task-specific rank allocation mechanism that assigns orthogonal basis vectors to isolate knowledge for each task. Unlike standard LoRA, which relies on a fixed rank, PLAN utilizes a perturbation-based strategy to optimize these vectors. This strategy actively identifies directions in the parameter space that demonstrate minimal sensitivity to interference with previous tasks. By freezing the base model and prior task parameters, PLAN employs an interference-aware training objective to optimize only the new low-rank parameters, effectively minimizing conflicts between new and existing knowledge.

**Results**
PLAN establishes a new state-of-the-art (SOTA) for continual learning with foundation models, delivering quantifiable improvements over prominent baselines. In evaluations on the **Split CIFAR-100** benchmark using a **ViT-B/16** backbone, PLAN improved the **Average Accuracy (ACC)** by **6.2%** compared to standard LoRA, while significantly reducing the **Forgetting Measure (FM)**. The method also demonstrated superior performance on **Split TinyImageNet** and **GLUE** benchmarks for NLP tasks, consistently outperforming baselines such as Adapters, Elastic Weight Consolidation (EWC), and Learning without Forgetting (LwF). These results confirm that PLAN not only mitigates catastrophic forgetting but also achieves substantial reductions in gradient interference across both vision and language domains.

**Impact**
This work significantly advances the field by bridging the gap between Parameter-Efficient Fine-Tuning (PEFT) and continual learning. By demonstrating that structured low-rank allocation can resolve the stability-plasticity dilemma in massive models, PLAN provides a viable pathway for the lifelong deployment of foundation models in dynamic environments. The research sets a new standard for interference-aware optimization, influencing future methodologies aimed at maintaining model performance over time without the need for full retraining or extensive memory storage.

---

## Key Findings

*   **New State-of-the-Art:** PLAN establishes a new SOTA for continual learning with foundation models.
*   **Mitigates Forgetting:** The framework successfully mitigates catastrophic forgetting by proactively managing the allocation of task-specific subspaces.
*   **Efficient Tuning:** PLAN enables the efficient and interference-aware fine-tuning of large pre-trained models.
*   **Conflict Minimization:** The use of a perturbation-based strategy effectively minimizes conflicts between new parameters and those learned from previous tasks.

---

## Methodology

*   **LoRA Extension:** The approach extends Low-Rank Adaptation (LoRA) specifically for continual learning settings to handle large pre-trained models efficiently.
*   **Orthogonal Basis Vectors:** PLAN proactively manages task-specific subspaces by introducing orthogonal basis vectors for each task.
*   **Perturbation-Based Optimization:** It optimizes these basis vectors using a perturbation-based strategy designed to minimize conflicts with previously learned parameters.
*   **Selection Mechanism:** A novel selection mechanism identifies and assigns basis vectors that demonstrate minimal sensitivity to interference.

---

## Technical Specifications

*   **Core Architecture:** Utilizes Parameter-Efficient Fine-Tuning (PEFT) based on Low-Rank Adaptation (LoRA).
*   **Rank Allocation:** Features a dynamic, task-specific rank allocation mechanism rather than a fixed rank.
*   **Subspace Management:** Employs subspace management to isolate task knowledge.
*   **Optimization Strategy:** Uses a perturbation-based strategy to identify critical parameters and directions for new parameters with minimal disruption.
*   **Training Objective:** Interference-aware objective that penalizes updates affecting previous tasks.
*   **Parameter Constraints:** Optimizes only new low-rank parameters while keeping the base model and previous task parameters frozen.

---

## Research Contributions

*   **Framework Introduction:** Introduction of the PLAN Framework designed to extend LoRA for efficient continual learning.
*   **Methodological Advance:** Methodological contribution in using orthogonal basis vectors to define task-specific subspaces.
*   **Strategy Development:** Development of a combined strategy involving perturbation-based optimization and a specific vector selection mechanism.
*   **Empirical Validation:** Empirical demonstration that structured low-rank allocation provides a superior solution for applying continual learning to large-scale foundation models.

---

## Performance Results

*   **Benchmark Performance:** The approach establishes a new SOTA, outperforming baselines such as standard LoRA, Adapters, EWC, and LwF.
*   **Key Metrics:** Evaluated based on Average Accuracy (ACC) and Forgetting Measure (FM).
*   **Forgetting Mitigation:** The method successfully mitigates catastrophic forgetting and shows quantifiable reductions in gradient interference.
*   **Stability & Scalability:** Demonstrates stability in retaining foundational capabilities and scalability to large pre-trained models like ViT and LLMs.