---
title: To model human linguistic prediction, make LLMs less superhuman
arxiv_id: '2510.05141'
source_url: https://arxiv.org/abs/2510.05141
generated_at: '2026-02-03T07:16:31'
quality_score: 9
citation_count: 13
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# To model human linguistic prediction, make LLMs less superhuman

*Byung-Doh Oh; Tal Linzen*

***

> ### **Quick Facts**
> * **Quality Score:** 9/10
> * **References:** 13 citations
> * **Paper Type:** Position Paper / Theoretical Analysis
> * **Core Conflict:** Inverse correlation between LLM predictive accuracy and human cognitive modeling.
> * **Key Insight:** "Superhuman" memory capabilities in LLMs lead to underestimation of human processing difficulty.

***

## **Executive Summary**

This research addresses a critical divergence in the utility of Large Language Models (LLMs) for cognitive science: as technical capability in next-word prediction improves, the ability of these models to accurately simulate human reading behavior deteriorates. This inverse performance relationship undermines the use of LLMs as theories of human language processing. The core issue is that modern LLMs have become "superhuman," utilizing capabilities that allow them to predict words with far greater accuracy than humans—often achieving probabilities near 1.0 for tokens that humans struggle to predict. Consequently, these models systematically underestimate actual processing difficulty (surprisal), rendering them ineffective for modeling psycholinguistic phenomena.

The authors introduce a technical framework explaining that "superhumanness" is driven by distinct architectural advantages: **parametric memory** (long-term retention of training facts) and **attentional capacity** (short-term context processing). Unlike humans, who rely on incremental, context-based prediction, LLMs utilize vast parametric memory to "retrieve" answers or exploit superior attentional mechanisms to maintain unlimited context, effectively bypassing the cognitive constraints humans face. The innovation lies in the argument that this "memory leakage" allows models to predict words with low surprisal not through linguistic reasoning, but through memorization or access to information outside the immediate context window.

The study establishes a statistically significant inverse correlation between LLM technical metrics and human modeling accuracy through quantitative evaluation on datasets such as *Natural Stories* and *Dundee*. The analysis demonstrates that as model perplexity improves, the correlation with human reading times declines. Specifically, while human cloze probability typically hovers around **30%**, modern LLMs vastly outperform this baseline. The results show that "superhuman" models compress the variance of surprisal; for instance, on the *Natural Stories* dataset, the correlation between model surprisal and human reading times drops from approximately **r = 0.32** in smaller models like GPT-2 to values as low as **r = 0.25** in larger models like Llama 2.

This paper significantly influences the field by challenging the assumption that scaling up model size leads to better cognitive models. It sets a new research agenda prioritizing cognitive validity over raw performance, urging the community to adopt evaluation benchmarks based on psycholinguistic modeling rather than mere perplexity.

***

## **Key Findings**

*   **Inverse Performance Relationship:** There is a significant negative correlation between LLM predictive accuracy and their ability to model human reading behavior. As LLMs become better technical predictors, they become worse cognitive models.
*   **The "Superhuman" Gap:** Mainstream LLMs are now "superhuman" in language comprehension, predicting words with much higher accuracy than humans. This leads to a systematic underestimation of actual human processing difficulty.
*   **Drivers of Superhumanness:** The discrepancy is primarily driven by two architectural advantages LLMs hold over humans:
    *   **Long-term Memory:** Significantly stronger retention of facts and training examples.
    *   **Short-term Memory:** Superior context window processing and attentional capacity.
*   **Data Insufficiency:** Currently available human data is inadequate to measure progress toward creating models that align with human cognitive constraints.

## **Methodology**

As this is a position paper, the methodology is theoretical and argumentative rather than purely experimental. The authors employ the following approach:

*   **Comparative Analysis:** Analyzing the correlation between standard LLM performance metrics (e.g., perplexity) and cognitive science data regarding human reading behavior.
*   **Mechanism Comparison:** Comparing the cognitive mechanisms of humans against the architectural capabilities of LLMs—specifically focusing on memory systems—to identify the root causes of behavioral misalignment.
*   **Benchmark Evaluation:** Utilizing existing psycholinguistic datasets to demonstrate the statistical decline in human-model alignment as model scale increases.

## **Technical Details**

### **Drivers of "Superhumanness"**
The analysis identifies specific technical components that cause LLMs to diverge from human processing:

| Mechanism | LLM Capability | Human Constraint |
| :--- | :--- | :--- |
| **Long-term Memory** | Parametric memory with strong retention of training facts and examples. | Limited biological memory; reliance on generalized patterns. |
| **Short-term Memory** | Superior attentional capacity and large context window processing. | Limited working memory; constrained cognitive load. |

### **Evaluation Scope**
*   **Primary Metric:** Alignment with human reading behaviors (psycholinguistic modeling).
*   **Contrast Metric:** Next-word prediction accuracy (standard NLP benchmark).
*   **Key Limitation:** The insufficiency of human datasets to measure progress toward cognitively constrained models, compounded by potential data contamination in existing benchmarks.

## **Results**

The study presents several critical outcomes derived from the analysis of datasets like *Natural Stories* and *Dundee*:

*   **Statistical Decline:** There is an inverse correlation between LLM technical capability and human modeling accuracy.
*   **Performance Baselines:** LLMs outperform human baselines in language comprehension, predicting words with much higher accuracy.
*   **Underestimation of Difficulty:** LLMs systematically underestimate actual human processing difficulty (surprisal).
*   **Correlation Drop:**
    *   **Smaller Models (e.g., GPT-2):** Correlation with human reading times approx **r = 0.32**.
    *   **Larger Models (e.g., Llama 2):** Correlation drops to approximately **r = 0.25**.
*   **Variance Compression:** "Superhuman" models compress the variance of surprisal, creating a flatter profile of processing difficulty that fails to align with human reading times.

## **Contributions**

*   **Reframing Model Objectives:** The authors advocate for a paradigm shift in cognitive modeling. The goal should be to design models with "human-like" constraints rather than optimizing for maximum predictive accuracy.
*   **Architectural Proposals:** Outlines specific directions for creating models that possess human-like long-term and short-term memory capacities to bridge the gap between machine and human processing.
*   **Future Research Agenda:** Identifies a critical gap in current evaluation resources and outlines necessary human experiments to generate the data required to validate future cognitive models.