# IPTQ-ViT: Post-Training Quantization of Non-linear Functions for Integer-only Vision Transformers

*Gihwan Kim; Jemin Lee; Hyungshin Kim*

---

### üìä Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Top-1 Accuracy Gain** | +6.44%p (Avg) |
| **mAP Improvement** | +1.0 (Object Detection) |
| **Quantization Configs** | W8A8, W4A8 |
| **Inference Type** | Fully Integer-only |
| **Retraining Required** | No (PTQ) |
| **Comparisons** | Outperforms Baseline PTQ; Matches QAT |

---

> ### üìù Executive Summary
>
> Deploying Vision Transformers (ViTs) on resource-constrained edge devices requires quantization to reduce model size and computational overhead. However, existing Post-Training Quantization (PTQ) methods struggle with the non-linear operations inherent to ViTs‚Äîspecifically GELU activation and Softmax‚Äîwhich degrade significantly when forced into low-bit integer representations. Current solutions often retain these layers in floating-point (partial quantization) or employ Quantization-Aware Training (QAT), both of which limit hardware acceleration or incur prohibitive retraining costs.
>
> **IPTQ-ViT** introduces a novel PTQ framework designed to enable fully integer-only inference for ViTs without retraining. The core innovation involves developing mathematically rigorous, integer-friendly approximations for non-linear functions: a polynomial-based approximation for GELU and a bit-shifting-based approximation for Softmax. A dynamic, layer-wise selection strategy utilizes a unified metric‚Äîintegrating quantization sensitivity, perturbation analysis, and computational cost‚Äîto select the optimal approximation function for each activation layer.
>
> This framework demonstrates substantial performance gains, achieving an average improvement of **+6.44%p** in top-1 accuracy and **+1.0 mAP** in object detection over previous PTQ methods. Crucially, IPTQ-ViT establishes parity with QAT methods in accuracy and latency under W8A8 and W4A8 configurations while maintaining strictly integer-only inference. By bridging the gap between PTQ and QAT, IPTQ-ViT offers an efficient pathway for deploying high-performance Vision Transformers on integer-only hardware accelerators without the computational expense of retraining.

---

## üîë Key Findings

*   **Significant Accuracy Boost:** Achieves up to **6.44%p** average top-1 accuracy gain in image classification and **1.0 mAP** improvement in object detection over previous PTQ methods.
*   **True Integer-Only Inference:** Successfully realizes fully integer-only inference for Vision Transformers, outperforming partial floating-point PTQ methods under `W8A8` and `W4A8` configurations without retraining.
*   **QAT Parity:** Achieves accuracy and latency comparable to Quantization-Aware Training (QAT) methods while removing the requirement for expensive retraining to recover accuracy lost in non-linear layer quantization.

---

## üß™ Methodology

The IPTQ-ViT framework utilizes a Post-Training Quantization strategy designed to enable fully integer-only inference for Vision Transformers.

*   **Integer-Friendly Approximations:** The core approach involves replacing standard non-linear functions with integer-friendly alternatives:
    *   **Polynomial-based GELU:** An approximation tailored for the statistical characteristics of vision data.
    *   **Bit-shifting-based Softmax:** A shifting-based method to avoid floating-point division and exponentiation.
*   **Unified Metric Selection:** The framework implements a unified metric that integrates three critical factors:
    1.  **Quantization Sensitivity**
    2.  **Perturbation Analysis**
    3.  **Computational Cost**
*   **Dynamic Strategy:** This metric guides the dynamic selection of the optimal approximation function for each specific activation layer within the network.

---

## ‚öôÔ∏è Technical Details

**Framework Type:** Post-Training Quantization (PTQ)
**Target:** Vision Transformers (ViTs)

*   **Scope:** Specifically targets the quantization of non-linear functions (e.g., `GELU`, `Softmax`) to enable integer-only arithmetic.
*   **Configurations:** Operates efficiently under `W8A8` and `W4A8` settings.
*   **Goal:** Achieve QAT-level latency efficiency without the associated training costs.
*   **Numerical Precision:** Handles numerical precision issues in:
    *   **Attention blocks**
    *   **MLP blocks**
*   **Scalability:** Capable of handling MLP hidden dimensions up to **3000** and token counts of **40**.

---

## üèÜ Contributions

1.  **Novel Framework:** Introduction of a new PTQ framework enabling fully integer-only ViT inference without retraining, overcoming the limitations of existing partial quantization methods.
2.  **Mathematical Approximations:** Development of specific mathematical approximations for non-linear operations‚Äîpolynomial-based GELU and bit-shifting-based Softmax‚Äîtailored for vision data statistics.
3.  **Layer-Wise Selection:** Proposal of a selection strategy using a unified metric that balances accuracy and efficiency to guide approximation function choices.
4.  **Performance Parity:** Demonstration that PTQ methods can achieve performance parity with integer-only QAT methods, offering a more accessible and efficient path for deploying ViTs in resource-constrained environments.

---

## üìà Results

*   **Image Classification:** Achieves an average gain of **+6.44 percentage points** in top-1 accuracy compared to previous PTQ methods.
*   **Object Detection:** Achieves an improvement of **+1.0 mAP**.
*   **Inference Mode:** Outperforms partial float PTQ baselines while maintaining a strictly **integer-only mode**.
*   **Efficiency:** Achieves accuracy and latency metrics comparable to Quantization-Aware Training (QAT) without the training overhead.

---

**Quality Score:** 8/10  
**References:** 36 citations