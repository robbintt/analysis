---
title: Optimizing Pre-Training Data Mixtures with Mixtures of Data Expert Models
arxiv_id: '2502.15950'
source_url: https://arxiv.org/abs/2502.15950
generated_at: '2026-02-06T01:22:19'
quality_score: 9
citation_count: 28
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Optimizing Pre-Training Data Mixtures with Mixtures of Data Expert Models

*Lior Belenki; Alekh Agarwal; Tianze Shi; Kristina Toutanova*

---

### Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Core Methodology** | Mixture of Data Experts (MDE) & Regression |
| **Model Scales Tested** | 70M to 1B parameters |
| **Dataset** | SlimPajama |
| **Key Efficiency Gain** | CPU-based loss calculation ($O(k)$ operations) |
| **Quality Score** | 9/10 |
| **Citations** | 28 |

---

> **Executive Summary**
>
> Determining the optimal composition of pre-training data is a critical but computationally expensive challenge in developing large language models (LLMs). As models scale and datasets grow more diverse, identifying the right proportion of data from various domains cannot be done through trial and error without incurring prohibitive costs. Existing optimization methods often rely on simple regression techniques using only mixture rates as features, which fail to capture the complex, non-linear interactions between different data sources. Consequently, the field requires an efficient mechanism to navigate the vast combinatorial space of data mixtures to maximize downstream performance without repeatedly training full-scale models.
>
> The authors propose a novel optimization framework centered on a "Mixture of Data Experts" (MDE), which acts as a proxy to approximate loss landscapes for unseen data mixtures. Technically, the method trains $k$ smaller proxy language models ("data experts"), each exclusively on a single domain. By caching per-token next-token probabilities, the system efficiently calculates the cross-entropy loss for any candidate mixture in $O(k)$ operations on a CPU. These MDE-derived loss approximations serve as rich input features—supplementing raw mixture rates—within a regression model (such as Gradient Boosting or Gaussian Processes) trained on a sparse set of observations. This approach allows for accurate performance prediction and is theoretically grounded by the proposition that the minimizer of a mixture distribution is a weighted combination of the minimizers of individual domains.
>
> Empirical validation on Transformer decoder-only models ranging from 70M to 1B parameters using the SlimPajama dataset confirmed that MDE-based optimization significantly outperforms standard regression approaches. A key efficiency demonstration showed that 280M parameter proxy models could effectively optimize mixtures for target models scaled up to 1B parameters. Furthermore, integrating the optimization process with an objective function that accounts for cross-entropy on end-task data resulted in superior few-shot downstream performance compared to baseline methods. These results validate the framework's rank-invariance assumption, demonstrating that the relative performance of data mixtures remains consistent across model scales.
>
> This research provides a practical and theoretically sound solution for efficient data curation, addressing one of the most resource-intensive aspects of LLM development. By shifting the paradigm from heuristic selection to a regression-based optimization driven by expert-derived features, the framework enables researchers to identify high-performing data mixtures with a fraction of the computational cost. This advancement establishes a new standard for pre-training strategies, likely influencing future work on automated data selection and resource-efficient model scaling, ultimately facilitating the development of more capable models through smarter data utilization.

---

## Key Findings

*   **Superiority of MDE-based optimization:** The proposed method using Mixture of Data Experts (MDE) to generate features significantly outperforms standard regression approaches that rely solely on mixture rates as input features.
*   **Validation across model scales:** Experiments conducted on Transformer decoder-only models ranging from 70M to 1B parameters confirmed the efficacy of the method on the SlimPajama dataset.
*   **Enhanced few-shot performance:** Integrating the optimization method with an objective function that incorporates cross-entropy on end-task data leads to superior results in few-shot downstream evaluations.
*   **Theoretical validation:** The study provides theoretical evidence supporting the mechanism of aggregating data expert predictions, explaining why this approach provides accurate approximations of model losses for various data mixtures.

---

## Methodology

The researchers developed a framework to optimize pre-training data mixtures without exhaustively training full models on every candidate. The approach involves four primary steps:

1.  **Loss Approximation:** Utilizing a Mixture of Data Experts (MDE) to efficiently estimate the cross-entropy loss for specific candidate data mixtures.
2.  **Feature Engineering:** Using the approximations derived from the MDE as additional input features within a regression model.
3.  **Regression Training:** Training the regression model based on a small set of actual observations (trained model losses) to predict the performance of unseen mixtures.
4.  **Objective Integration:** Combining this optimization process with an objective that accounts for cross-entropy on end-task data to guide the selection of optimal data mixtures for downstream transfer.

---

## Technical Details

The paper proposes a regression-based optimization framework called **Mixture of Data Experts (MDE)** for data mixture selection.

**Core Architecture**
*   The framework trains $k$ proxy language models (Data Experts), each exclusively on a single domain.
*   The ensemble distribution is defined as:
    $$P_{MDE}(x_t|x_{1\dots t-1}, \lambda) := \sum_{i=1}^{k} \lambda_i P_{\theta^*_i}(x_t|x_{1\dots t-1})$$

**Efficiency Mechanisms**
*   The system pre-computes and caches per-token next-token probabilities.
*   This allows loss calculation for new mixtures on CPU in $O(k)$ operations per token.
*   **Pipeline:** Uses MDE loss on validation domains alongside raw mixture rates as features.
*   **Optimization:** Performed via the Vizier framework using models such as Linear, Gradient Boosting, and Multi-task Gaussian Processes.

**Scaling Strategy**
*   Proxy models: 280M parameters (trained 10K steps).
*   Target models: 1B parameters (trained 200K steps).

**Theoretical Basis**
*   **Proposition 3.1:** The minimizer of the loss for a mixture distribution is a weighted combination of the minimizers of the individual domains.

---

## Results

The MDE-based optimization method demonstrated significant advantages over baseline approaches:

*   **Performance:** Significantly outperforms standard regression approaches that rely solely on mixture rates as input features.
*   **Robustness:** Validation across Transformer decoder-only models (70M to 1B parameters) on the SlimPajama dataset confirmed the rank-invariance assumption.
*   **Downstream Impact:** Integrating the optimization method with an objective function incorporating cross-entropy on end-task data results in superior few-shot performance.
*   **Accuracy:** Theoretical evidence confirms that aggregating data expert predictions provides accurate approximations of model losses for various data mixtures.

---

## Contributions

*   **A Novel Optimization Framework:** Introduction of a method that leverages Mixture of Data Experts (MDE) as a proxy to approximate loss landscapes, enabling more efficient search for optimal pre-training data mixtures.
*   **Improved Data Selection Strategy:** A shift from using simple mixture rates to utilizing rich, expert-derived approximations as features in regression models for predicting model performance.
*   **Theoretical Foundations:** Provision of theoretical insights that justify why aggregating predictions from data experts serves as an effective proxy for the actual loss of a model trained on a specific data mixture.
*   **Practical Performance Gains:** Empirical demonstration of a technique that improves few-shot downstream task performance by optimizing data mixture ratios relative to end-task goals.