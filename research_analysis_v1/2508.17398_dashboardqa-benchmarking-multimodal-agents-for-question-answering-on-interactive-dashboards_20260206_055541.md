---
title: 'DashboardQA: Benchmarking Multimodal Agents for Question Answering on Interactive
  Dashboards'
arxiv_id: '2508.17398'
source_url: https://arxiv.org/abs/2508.17398
generated_at: '2026-02-06T05:55:41'
quality_score: 9
citation_count: 25
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# DashboardQA: Benchmarking Multimodal Agents for Question Answering on Interactive Dashboards

*Aaryaman Kartha; Ahmed Masry; Mohammed Saidul Islam; Thinh Lang; Shadikur Rahman; Ridwan Mahbub; Mizanur Rahman; Mahir Ahmed; Md Rizwan Parvez; Enamul Hoque; Shafiq Joty*

---

> ### ðŸ“Š Quick Facts
>
> *   **Top Model Accuracy:** 38.69%
> *   **OpenAI CUA Score:** 22.69%
> *   **Dataset Scope:** 112 Dashboards, 405 QA Pairs
> *   **Data Source:** Tableau Public
> *   **Evaluation Framework:** OSWorld
> *   **Primary Failure Modes:** Grounding, Planning, Reasoning

---

## Executive Summary

Current Vision-Language Models (VLMs) have demonstrated strong capabilities in static image understanding, yet they face significant limitations when applied to the dynamic, interactive reasoning required for real-world business intelligence dashboards. The core issue is that existing evaluation frameworks focus primarily on static chart analysis, failing to assess the complex workflows involving coordinated views, cross-filtering, and multi-step state changes. This gap is critical because modern data analysis is inherently interactive; without benchmarks that test these capabilities, it is impossible to accurately gauge the readiness of autonomous agents to perform professional data tasks that require navigating and manipulating user interfaces.

This paper introduces **DashboardQA**, the first benchmark specifically designed to evaluate multimodal GUI agents on real-world, interactive dashboards. The benchmark comprises 112 complex dashboards sourced from Tableau Public, featuring mechanisms like coordinated views and cross-filtering, and includes 405 question-answer pairs categorized into five types. The data was curated using a Human-VLM Collaboration Loop to ensure quality. Technically, the evaluation utilizes the OSWorld framework within an Ubuntu virtual environment where agents must perceive inputs via screenshots and Accessibility (A11y) trees and execute actionsâ€”such as mouse movement, scrolling, and clickingâ€”to plan interaction trajectories and extract data from dynamic visual states.

The evaluation revealed a substantial performance gap, indicating that interactive dashboard reasoning is a highly challenging task for current state-of-the-art models. The best-performing agent achieved only 38.69% accuracy, while the specialized OpenAI CUA agent reached just 22.69%. The study identified three primary failure modes: **Grounding**, **Planning**, and **Reasoning**. DashboardQA establishes a crucial baseline for future research, shifting the focus of GUI agent evaluation from static visual perception to dynamic, interactive reasoning.

---

## Key Findings

*   **Significant Performance Gap:** Interactive dashboard reasoning is highly challenging for current Vision-Language Models (VLMs), with the best agent achieving only **38.69%** accuracy.
*   **Agent Limitations:** Current multimodal agents struggle specifically with grounding dashboard elements, planning interaction trajectories, and performing complex reasoning.
*   **Benchmark Difficulty:** The benchmark proved difficult for both closed- and open-source agents, with the OpenAI CUA agent reaching only **22.69%** accuracy.
*   **Inadequacy of Existing Benchmarks:** Current evaluation frameworks fail to assess agents effectively because they focus on static charts rather than the interactivity required in real-world workflows.

---

## Methodology

*   **Benchmark Construction:** Developed 'DashboardQA,' the first benchmark designed to assess vision-language GUI agents on real-world, interactive dashboards.
*   **Data Curation:** The dataset comprises **112 interactive dashboards** sourced from Tableau Public and **405 question-answer pairs**.
*   **Categorization:** QA pairs are structured into five categories:
    *   Multiple-choice
    *   Factoid
    *   Hypothetical
    *   Multi-dashboard
    *   Conversational
*   **Evaluation Protocol:** Assessed a diverse range of leading closed-source and open-source GUI agents to determine their proficiency in comprehending and interacting with dashboard interfaces.

---

## Technical Details

**Evaluation Framework**
*   **Environment:** OSWorld framework within an Ubuntu virtual environment.
*   **Input Perception:** Agents perceive inputs via screenshots and Accessibility (A11y) trees.
*   **Action Execution:** Agents execute actions via `pyautogui` commands (mouse move, scroll, click).

**Dataset Composition**
*   **Source:** 112 interactive dashboards from Tableau Public selected for features like coordinated views and cross-filtering.
*   **Creation Process:** Utilizes a Human-VLM Collaboration Loop:
    1.  8 experts create seed QA pairs.
    2.  VLMs (GPT-4o, Gemini, Claude) generate expansions.
    3.  Annotators refine the output to ensure quality.

**Task Requirements**
Tasks require agents to:
*   Reason over natural language questions.
*   Plan GUI operations.
*   Extract data from dynamic visual states.
*   Synthesize information across views.

---

## Results

*   **Overall Performance:** The benchmark proved highly challenging, with the top-performing agent achieving only **38.69%** accuracy and the OpenAI CUA agent achieving **22.69%**.
*   **Failure Modes:** Primary failure modes identified are:
    *   **Grounding:** Difficulty locating UI elements.
    *   **Planning:** Ineffective interaction trajectories.
    *   **Reasoning:** Struggles with complex analysis.
*   **Interaction Complexity:** Most questions require navigating 2-3 interaction states, with multi-dashboard questions requiring the most transitions.
*   **UI Control Distribution:**
    *   Dropdown menus: 92.31% of dashboards
    *   Tabs: 23.08%
    *   Radio Buttons/Range Sliders: 7.69%
*   **Visualization Types:**
    *   Line charts: 48.51%
    *   Bar charts: 42.57%

---

## Contributions

*   **Introduction of DashboardQA:** Released the first benchmark explicitly tailored for evaluating vision-language agents on interactive dashboards, filling a gap left by static-chart-focused benchmarks.
*   **Comprehensive Dataset:** Provided a novel dataset featuring 112 real-world dashboards and 405 QA pairs covering complex, interactive scenarios.
*   **Critical Analysis of Agent Capabilities:** Conducted an empirical study exposing specific limitations in state-of-the-art models regarding grounding, planning, and reasoning, establishing a baseline for future research.
*   **Resource Availability:** Publicly released the benchmark code and data via GitHub to facilitate further research.

---

**Quality Score:** 9/10
**References:** 25 citations