---
title: 'RoVer: Robot Reward Model as Test-Time Verifier for Vision-Language-Action
  Model'
arxiv_id: '2510.10975'
source_url: https://arxiv.org/abs/2510.10975
generated_at: '2026-01-29T11:38:57'
quality_score: 9
citation_count: 21
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# RoVer: Robot Reward Model as Test-Time Verifier for Vision-Language-Action Model

*Shenzhen Institute, Rui Su, Zhouxia Wang, Yang Liu, Mingtong Dai, Lingbo Liu, Chunjie Chen, Yongjie Bai, Xinyu Wu, Liang Lin*

***

### ðŸ“‹ Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Quality Score** | 9/10 |
| **Total Parameters** | 0.2B (Backbone) |
| **Trainable Parameters** | 40M |
| **Framework Type** | Test-Time Scaling (TTS) / Plug-and-Play |
| **Action Space** | 7-Dimensional End-Effector Delta (Position, Rotation, Gripper) |
| **Key Benchmark** | CALVIN (ABC to D Zero-Shot) |
| **Baselines Outperformed** | GR-1, Dita, MoDE, Diffusion Policy |

***

## Executive Summary

**"Problem"**
Vision-Language-Action (VLA) models have emerged as the dominant architecture for robotic manipulation, yet they suffer from reliability issues during deployment, particularly in long-horizon tasks where stochastic decoding can lead to compounding errors. Traditionally, addressing these robustness gaps requires expensive retraining or fundamental architectural modifications to the base model. This paper addresses the critical challenge of enhancing the decision-making reliability and success rates of robotic agents purely during inference, without altering the parameters or structure of the pre-trained foundation model.

**"Innovation"**
The authors introduce RoVer, a plug-and-play test-time scaling framework that decouples action generation from verification. The core innovation is the **Robot Process Reward Model (PRM)**, a specialized auxiliary model that serves a dual function: it outputs a scalar process reward for verification and an action-space direction vector for guidance. Utilizing a frozen VLA policy to generate initial candidates, RoVer employs a three-step pipeline: Generation, Expansion, and Selection. Critically, the "Expansion" phase physically refines candidate actions by perturbing them along the PRMâ€™s predicted direction vectors within the 7-dimensional end-effector space (position, rotation, gripper). This process is computationally feasible due to a shared perception cache that amortizes the cost of processing visual features across all candidates. The framework requires training only 40M parameters on top of a 0.2B backbone.

**"Results"**
RoVer was evaluated on the CALVIN benchmark under the ABC to D zero-shot setting and on a real-world robot platform, outperforming baselines including GR-1, Dita, MoDE, and Diffusion Policy. On the CALVIN benchmark, RoVer significantly improved sequential task performance, raising the success rate from **16.6% (baseline GR-1) to 22.4%**. In real-world experiments, the system consistently demonstrated higher success rates across diverse manipulation tasks. These results confirm that RoVer effectively converts additional inference compute into measurable performance gains, maintaining robustness against the variability of stochastic decoding without requiring modifications to the underlying base model.

**"Impact"**
RoVer establishes a significant precedent for applying "test-time scaling" techniques to embodied AI, mirroring advancements seen in large language models. By demonstrating that inference compute can be traded for improved reliability, this work offers a practical path to more scalable robotics systems. The ability to enhance existing, frozen policies using a lightweight, external verifier suggests a future where robot modularity is prioritized, allowing legacy models to be upgraded via verification layers rather than continuous, resource-heavy retraining.

***

## Key Findings

*   **Consistent Improvement:** RoVer consistently improves success rates across diverse manipulation tasks without modifying the architecture or weights of the base Vision-Language-Action (VLA) model.
*   **Compute-to-Performance Conversion:** The framework effectively converts available computing resources during inference into better action decision-making.
*   **Cost Efficiency:** By caching shared perception features, RoVer amortizes the cost of perception processing, making the expansion phase computationally viable.
*   **Action Refinement:** The use of a Process Reward Model (PRM) to predict action-space directions allows for the expansion and refinement of candidate actions, leading to better final selection.

***

## Methodology

RoVer introduces an **embodied test-time scaling framework** designed to augment VLA models during inference. The methodology revolves around the integration of a specialized auxiliary model and a streamlined inference pipeline.

### Core Component: Robot Process Reward Model (PRM)
The PRM serves two distinct functions:
1.  **Evaluation:** Assigns scalar process rewards to candidate actions to verify their quality.
2.  **Guidance:** Predicts action-space direction vectors to guide the refinement of candidates.

### Inference Pipeline
The process operates in three distinct steps:
1.  **Generation:** The base policy produces initial candidate actions.
2.  **Expansion:** Candidates are refined and perturbed along the direction vectors predicted by the PRM.
3.  **Selection:** The optimal action is chosen from the refined pool based on PRM verification scores.

### Efficiency Mechanism
To maintain real-time performance, RoVer utilizes a **shared perception cache**. This system reuses visual features across multiple evaluations, ensuring that the heavy lifting of visual processing is not repeated for every candidate action.

***

## Technical Details

| Aspect | Specification |
| :--- | :--- |
| **Framework Type** | External Test-Time Scaling (TTS) |
| **Base Policy** | Frozen Vision-Language-Action Model |
| **Auxiliary Model** | Robot Process Reward Model (PRM) |
| **Backbone Architecture** | GPT-2 style initialized with GR-1 weights |
| **Encoders** | MAE and CLIP encoders for visual processing |
| **Specific Module** | "Action Amplifier" MLP |
| **Parameter Count** | 0.2B Total parameters (40M trainable) |
| **Input Data** | Synchronized multi-modal data (Observations, Language Goal, Candidate Action) |
| **Output Data** | Scalar process reward + Action-space direction vector |

***

## Contributions

*   **"Plug-and-Play Framework:"** A test-time scaling framework that requires no architectural changes or training overhead for the base model, allowing for easy integration with existing VLA systems.
*   **"Dual-Function PRM:"** A specialized Robot Process Reward Model that combines scalar process rewards (for verification) and action-space direction predictions (for guided exploration) into a single entity.
*   **"Optimized Sampling:"** An efficient sampling strategy that leverages direction guidance and a shared perception cache to enable scalable candidate generation and selection without significant latency.

***

## Results

**Simulation (CALVIN Benchmark)**
*   **Setting:** ABC to D zero-shot transfer.
*   **Performance:** RoVer achieved a success rate of **22.4%**, a significant improvement over the baseline GR-1 model, which scored **16.6%**.
*   **Comparison:** Outperformed baselines including GR-1, Dita, and MoDE.

**Real-World Evaluation**
*   **Platform:** Real-robot platform manipulation tasks.
*   **Baselines:** Compared against Diffusion Policy.
*   **Outcome:** Qualitative results reported consistent improvements in success rates across diverse tasks.

**General Performance**
*   **Long-Horizon Tasks:** Demonstrated effectiveness in tasks requiring long sequences of actions.
*   **Robustness:** Improved robustness against the inherent variability of stochastic decoding.
*   **Base Model Preservation:** All improvements were realized without modifying the underlying base model's weights or architecture.

***
*Report generated based on 21 citations.*