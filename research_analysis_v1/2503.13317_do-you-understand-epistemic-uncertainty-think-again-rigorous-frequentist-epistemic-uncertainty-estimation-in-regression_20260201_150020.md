# Do you understand epistemic uncertainty? Think again! Rigorous frequentist epistemic uncertainty estimation in regression

*Enrico Foglia; Benjamin Bobbia; Nikita Durasov; Michael Bauerheim; Pascal Fua; Stephane Moreau; Thierry Jardin*

***

> ### üìå Quick Facts
>
> * **Quality Score:** 6/10
> * **References:** 33 Citations
> * **Methodological Class:** Frequentist UQ (Non-Bayesian)
> * **Application Domains:** Aerodynamics & Acoustics
> * **Key Innovation:** Feedback-loop stability analysis

***

## üìù Executive Summary

This paper addresses the significant computational burden of Bayesian Neural Networks (BNNs) and the lack of rigorous frequentist methods for quantifying uncertainty in regression tasks. While frequentist techniques are established for classification, they lack a theoretical foundation for distinguishing between **aleatoric** (data noise) and **epistemic** (model knowledge) uncertainty in regression‚Äîa distinction vital for safety-critical engineering fields like aerospace to assess system reliability.

The authors' primary innovation is the extension of frequentist uncertainty quantification to regression through a **feedback-based conditioning mechanism**. By feeding the model's initial prediction back into the network as an input and observing response stability, the approach detects epistemic uncertainty via volatile predictions. This establishes a theoretical framework where high absolute covariance terms indicate miscalibration or model extrapolation.

Validation in aerodynamics and acoustics demonstrated that the method accurately quantifies total uncertainty without the overhead of Bayesian inference. Experiments involving a three-bladed rotor and farfield sound pressure analysis confirmed the method's alignment with Monte Carlo benchmarks and its sensitivity to distribution shifts, as seen by spikes in absolute covariance terms during extrapolation.

This research offers a theoretically grounded, lightweight alternative to Bayesian inference, providing engineers with a practical tool to detect model extrapolation and miscalibration. By establishing a mathematical foundation for epistemic uncertainty in regression, the paper enables safer deployment of neural networks in industrial workflows with minimal architectural changes.

***

## üîë Key Findings

*   **Novel Frequentist Approach:** Introduces a rigorous frequentist method for estimating uncertainty in regression tasks, serving as an alternative to Bayesian techniques.
*   **Uncertainty Distinction:** Clearly distinguishes and quantifies both **aleatoric uncertainty** (inherent data noise) and **epistemic uncertainty** (lack of model knowledge).
*   **Stability-Based Quantification:** Measures uncertainty by analyzing the stability of model responses when the model is conditioned on its own previous output.
*   **Theoretical Framework:** Provides a complete theoretical foundation for analyzing epistemic uncertainty without relying on Bayesian priors or posteriors.
*   **Ease of Implementation:** Can be implemented with minimal architectural changes to existing models.

***

## ‚öôÔ∏è Methodology

The proposed method utilizes a frequentist framework based on **conditional prediction generation**. The core process involves a feedback loop:

1.  **Initialization:** The model generates an initial output for a given input.
2.  **Feedback Loop:** This initial output is fed back into the model as an additional input.
3.  **Volatility Measurement:** The method measures how the prediction changes when conditioned on this previous answer.
4.  **Uncertainty Metric:** The observed change (volatility) in the response serves as the primary metric for quantifying uncertainty.

***

## üèÜ Contributions

*   **Domain Extension:** Bridges the gap between classification and regression by extending frequentist uncertainty estimation techniques to continuous outputs.
*   **Theoretical Foundation:** Establishes a rigorous frequentist theoretical framework specifically for epistemic uncertainty in regression tasks.
*   **Practical Technique:** Develops a practical feedback-based method that requires minimal modification to existing model architectures, lowering the barrier to adoption.

***

## üîß Technical Details

| Aspect | Description |
| :--- | :--- |
| **Methodological Class** | Frequentist approach to Uncertainty Quantification (UQ), designed as an alternative to Bayesian methods. |
| **Core Mechanism** | Quantifies epistemic uncertainty by observing the stability of model responses when conditioned on previous output via a feedback mechanism. |
| **Computational Burden** | Shifts load from model to dataset; requires minimal architectural changes (Eq. 9) but necessitates multiple outcomes (measurements) per input. |
| **Theoretical Framework** | Provides rigorous proofs for perfect calibration, where violations indicate miscalibration or extrapolation. |
| **Calibration Detection** | Detects miscalibration via high levels of the absolute covariance term denoted as $|cov_\theta|$. |

***

## üìä Results

### Experimental Domain
Aerodynamics & Acoustics, specifically focusing on aerodynamic loading and drone noise analysis.

### Experiment 1: Drone Noise
*   **Setup:** Near-out-of-dataset test on a three-bladed rotor at 5000 rpm (analyzing 3x Blade Passage Frequency).
*   **Visualization:** Sound Pressure Level (SPL) in dB vs Angular position (-60¬∞ to 60¬∞).
*   **Metrics:** Compared Monte Carlo integration variance against the proposed method's 'total uncertainty' ($\sigma^2_\theta$) using 2œÉ confidence intervals.

### Experiment 2: Acoustic Spectrum
*   **Setup:** Farfield sound pressure time series collected at 3000 rpm in the ISAE-SUPAERO anechoic room (rotor-disk plane).
*   **Processing:** Time signals split into two independent ergodic sub-sections. Data processed using a Hanning window averaged over 8 sections with no overlap.

### Key Observations
*   The method is sensitive to distribution shifts.
*   High levels of $|cov_\theta|$ were observed specifically during extrapolation, serving as a reliable diagnostic for model reliability.