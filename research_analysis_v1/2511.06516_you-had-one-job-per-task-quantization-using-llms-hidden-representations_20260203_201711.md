---
title: 'You Had One Job: Per-Task Quantization Using LLMs'' Hidden Representations'
arxiv_id: '2511.06516'
source_url: https://arxiv.org/abs/2511.06516
generated_at: '2026-02-03T20:17:11'
quality_score: 8
citation_count: 27
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# You Had One Job: Per-Task Quantization Using LLMs' Hidden Representations

*Amit LeVi; Raz Lapid; Rom Himelstein; Yaniv Nemcovsky; Ravid Shwartz Ziv; Avi Mendelson*

---

> **üìä Quick Facts**
>
> *   **Models Evaluated:** Phi-4, Llama-3.1, Qwen3, Qwen2.5
> *   **Proposed Methods:** Task-Aware Quantization (TAQ) & TAQO
> *   **Top Performance (Phi-4):** EM 42.33 / F1 50.81
> *   **Accuracy Retention:** Maintained within **< 1.0%** of full-precision models
> *   **Baseline Outperformed:** Activation-aware Weight Quantization (AWQ)

---

## üìù Executive Summary

Traditional Post-Training Quantization (PTQ) methods for Large Language Models (LLMs) typically employ a task-agnostic approach, applying uniform compression across the entire network regardless of the specific downstream application. This "one-size-fits-all" strategy is fundamentally inefficient because it fails to distinguish between layers that are critical for a specific task and those that are not. Consequently, existing methods often face a harsh trade-off: achieving lower average precision usually results in significant performance degradation, as the quantization process inevitably degrades weights essential for the task at hand.

The authors introduce a novel **Per-Task Quantization framework** that leverages the hidden representations of LLMs to guide the compression process. By analyzing a small calibration set, the framework identifies task-salient signals to determine which layers require higher precision. The paper proposes two distinct implementations:

*   **Task-Aware Quantization (TAQ):** Allocates bitwidths based on task-conditioned statistics derived from hidden activations.
*   **TAQO:** Allocates precision based on direct layer sensitivity tests.

Both methods utilize an asymmetric resource allocation strategy, preserving high precision exclusively in task-relevant layers while aggressively quantizing the remainder of the model.

Evaluated across prominent architectures including Phi-4, Llama-3.1, Qwen3, and Qwen2.5, both TAQ and TAQO consistently outperformed existing post-training quantization baselines such as Activation-aware Weight Quantization (AWQ). TAQ achieved superior results on the Phi-4 model with an Exact Match (EM) of **42.33** and an F1 score of **50.81**, while TAQO led performance on Llama-3.1, Qwen3, and Qwen2.5. Crucially, these task-aware techniques maintained model accuracy within less than 1.0% of the original full-precision model, validating the efficacy of the targeted precision allocation.

This research challenges the current paradigm of general-purpose model compression by demonstrating that task-aware quantization can achieve significant efficiency gains without sacrificing accuracy. By proving that aggressive quantization of non-essential layers is viable when precision is preserved in task-critical areas, this work paves the way for more efficient deployment of LLMs on resource-constrained hardware.

---

## üîç Key Findings

*   **Superior Performance:** The proposed methods, **TAQ** and **TAQO**, consistently outperform existing post-training quantization baselines across multiple LLM architectures.
*   **Architecture-Specific Wins:**
    *   **TAQ** achieves the best results on the **Phi-4** model.
    *   **TAQO** leads in performance on **Llama-3.1, Qwen3, and Qwen2.5**.
*   **Significant Metric Improvement:** On the Phi-4 model, the proposed method achieved an Exact Match (EM) of **42.33** and an F1 score of **50.81**, drastically surpassing Activation-aware Weight Quantization (AWQ).
*   **Minimal Accuracy Loss:** Task-aware quantization techniques maintain model accuracy within **less than 1.0%** of the original full-precision model while successfully achieving lower average precision.

---

## üõ†Ô∏è Methodology

The study introduces a **task-aware Post-Training Quantization (PTQ)** framework designed to utilize hidden representations for identifying task-salient signals, contrasting sharply with traditional task-agnostic methods.

*   **Calibration Analysis:** The framework analyzes a small calibration set to identify task-relevant layers within the network.
*   **Implementation Strategies:** Two specific methods were implemented to handle precision allocation:
    1.  **TAQ:** Allocates bitwidths based on task-conditioned statistics derived directly from hidden activations.
    2.  **TAQO:** Allocates precision based on direct layer sensitivity tests.
*   **Asymmetric Allocation:** In both methods, task-relevant layers retain their precision, while the remaining layers undergo aggressive quantization.

---

## ‚öôÔ∏è Technical Details

| Feature | Description |
| :--- | :--- |
| **Core Strategy** | Per-Task Quantization using LLMs' hidden representations. |
| **Objective** | Lower average precision with maintained performance. |
| **Primary Benchmark** | Activation-aware Weight Quantization (AWQ). |
| **TAQ Mechanism** | Post-training quantization using statistics derived from hidden activations. |
| **TAQO Mechanism** | Post-training quantization utilizing direct layer sensitivity testing. |

---

## üöÄ Contributions

*   **Novel Methodology:** Introduces a new approach that leverages hidden representations as a guideline for quantization, directly addressing the limitations of traditional PTQ methods.
*   **Dual Strategy Implementation:** Presents two distinct quantization strategies (**TAQ** and **TAQO**) that dynamically adjust bitwidth allocation based on specific task requirements.
*   **Empirical Validation:** Provides evidence demonstrating that preserving precision exclusively in task-relevant layers allows for aggressive quantization of the rest of the model without sacrificing overall performance.

---

## üìà Results

The proposed methods were evaluated on **Phi-4, Llama-3.1, Qwen3, and Qwen2.5**.

*   **Consistency:** Both TAQ and TAQO maintained accuracy within **1.0%** of full-precision models while achieving reduced precision.
*   **Phi-4 Performance:** TAQ achieved an **EM of 42.33** and **F1 of 50.81**, significantly outperforming the AWQ baseline.
*   **Llama & Qwen Performance:** TAQO led the results on Llama-3.1, Qwen3, and Qwen2.5 architectures.

---

## üìå Metadata

*   **Quality Score:** 8/10
*   **References:** 27 citations