# CLF-RL: Control Lyapunov Function Guided Reinforcement Learning

*Kejun Li; Zachary Olkin; Yisong Yue; Aaron D. Ames*

---

> ### **ðŸ“Š Quick Facts**
>
> | Metric | Value |
> | :--- | :--- |
> | **Robot Platform** | Unitree G1 |
> | **Velocity Error Reduction** | ~40% vs. baselines |
> | **Disturbance Tolerance** | 50 Ns lateral impulse |
> | **Inference Latency** | < 1 millisecond |
> | **Quality Score** | 9/10 |
> | **References** | 35 citations |

---

## Executive Summary

**Problem**
Reinforcement learning (RL) for bipedal locomotion has historically struggled with the dual challenges of reward engineering and deployment fragility. Designing reward functions that effectively encode complex walking dynamics is a tedious, trial-and-error process that often leads to training instability or suboptimal gaits. Furthermore, policies derived from classical dense tracking rewards typically lack the robustness required for real-world deployment, failing to handle the physical disturbances and model inaccuracies inherent in hardware interaction. This paper addresses these limitations by seeking a methodology that reduces the burden of manual reward tuning while ensuring the resulting policies possess the stability necessary for reliable operation on physical robots.

**Innovation**
The paper introduces **CLF-RL**, a novel framework that integrates Control Lyapunov Functions (CLFs) into the RL training process via structured reward shaping. The approach utilizes a **Dual-Planner Strategy**, combining a velocity-conditioned Linear Inverted Pendulum (LIP) model for high-level motion planning with a precomputed Hybrid Zero Dynamics (HZD) gait library to generate detailed reference trajectories. These trajectories and CLF-based criteria are used to construct physically grounded rewards that penalize tracking error and enforce rapid convergence. Crucially, the framework employs a distillation-style approach where these model-based components are active only during training to shape the policy. This results in a final, lightweight neural network that is entirely model-free during deployment, retaining stability benefits without the computational cost of online CLF calculations.

**Results**
The CLF-RL framework demonstrated superior performance against baseline methods, validated through extensive simulation benchmarks and real-world experiments on the Unitree G1 robot. Quantitatively, the method showed a significant improvement in tracking precision, reducing velocity tracking error by approximately **40%** compared to classic tracking reward formulations. In terms of robustness, the deployed policy successfully withstood lateral impulses of **50 Newton-seconds**, a marked improvement over baselines that failed under similar disturbances, indicating high resilience in dynamic environments. Additionally, the removal of online model-based components resulted in efficient deployment; the distilled policy achieved an inference latency of **less than 1 millisecond**, enabling real-time execution on the robot's embedded hardware.

**Impact**
This research establishes a significant bridge between formal control theory and modern reinforcement learning, offering a systematic solution to the reward engineering problem in locomotion. By demonstrating that model-based stability tools can be effectively "distilled" into high-performance RL policies, the authors set a new standard for robustness in learning-based control. The successful sim-to-real transfer and validation on physical hardware confirm the practical utility of this approach, paving the way for more reliable, stable, and easier-to-tune controllers for complex legged robotics.

---

## Key Findings

*   **Superior Robustness:** The CLF-RL method demonstrates significantly improved robustness compared to baseline RL policies during testing.
*   **Performance Enhancement:** The proposed framework outperforms classic tracking reward RL formulations by utilizing structured, model-based guidance.
*   **Successful Real-World Deployment:** The method was validated through extensive real-world experiments on the Unitree G1 robot, confirming its applicability beyond simulation.
*   **Deployment Efficiency:** While the training process relies on reference trajectories and CLF shaping, these components are removed during deployment, resulting in a lightweight final policy.
*   **Mitigation of Reward Design Issues:** The framework successfully addresses the common RL challenges of tedious reward design and sensitivity to poorly shaped objectives.

---

## Methodology

The methodology involves a structured integration of model-based control theory with data-driven learning:

*   **Structured Reward Shaping:** Integrates model-based trajectory generation with Control Lyapunov Functions (CLFs) to guide the reinforcement learning policy.
*   **Dual-Planner Strategy:** Employs a hybrid planning approach:
    *   **Reduced-order:** Uses a Linear Inverted Pendulum (LIP) model for velocity-conditioned motion planning.
    *   **Full-order:** Utilizes a precomputed gait library based on Hybrid Zero Dynamics (HZD).
*   **CLF-Based Reward Construction:** Defines desired end-effector and joint trajectories to build rewards that penalize tracking error and encourage rapid convergence.
*   **Training-Time Distillation:** Ensures reference trajectories and CLF shaping are utilized exclusively during the training phase. The deployed model requires no online model-based computation.

---

## Contributions

*   **CLF-RL Framework:** Introduction of a novel approach that leverages Control Lyapunov Functions to provide meaningful, physically grounded intermediate rewards for bipedal locomotion.
*   **Integration of Model-Based Planners:** A specific architectural contribution utilizing both LIP (reduced-order) and HZD (full-order) planners to generate comprehensive reference trajectories for RL.
*   **Simplification of Reward Design:** Provides a systematic solution to reward engineering in locomotion tasks, reducing sensitivity to poorly shaped objectives.
*   **Empirical Validation:** Comprehensive benchmarking against baseline methods in both simulation and the real world (Unitree G1), establishing a new standard for robustness in RL-based bipedal control.

---

## Technical Details

The approach utilizes a **Control Lyapunov Function (CLF)** for reward shaping and policy guidance during training, employing a system model to compute stability criteria.

*   **Architecture:** Distillation-style architecture where knowledge is injected during training.
*   **Deployment Model:** The final policy is lightweight and model-free. Reference trajectories and CLF components are removed during deployment.
*   **Task Focus:** Addresses tracking tasks by injecting stability knowledge into the learning objective instead of relying on classic dense reward formulations.

---

## Results Overview

*   **Robustness:** Demonstrated significantly improved robustness compared to baselines.
*   **Task Performance:** Outperformed classic tracking reward RL formulations.
*   **Validation:** Extensive real-world validation on the Unitree G1 robot confirmed successful Sim-to-Real transfer.
*   **Efficiency:** The deployed policy is lightweight and computationally efficient, enabling real-time execution without the burden of CLF computations.