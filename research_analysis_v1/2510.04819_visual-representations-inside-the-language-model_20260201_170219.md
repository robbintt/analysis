# Visual Representations inside the Language Model

*Benlin Liu; Amita Kamath; Madeleine Grunde-McLaughlin; Winson Han; Ranjay Krishna*

---

> ### ðŸ“Š Quick Facts
>
> *   **Quality Score:** 9/10
> *   **Models Analyzed:** LLaVA-OneVision, Qwen2.5-VL, Llama-3-LLaVA-NeXT
> *   **Core Focus:** Mechanistic Interpretability & Visual Token Analysis
> *   **Key Benchmark:** BLINK
> *   **Citations:** 31

---

## Executive Summary

Multimodal Language Models (MLMs) excel at generating text descriptions of visual data yet frequently underperform on perception-heavy tasks, such as fine-grained segmentation and semantic correspondence, compared to dedicated vision models. This paper addresses the critical "black box" problem of how visual information is transformed and potentially degraded as it flows from the visual encoder into the language model. The authors seek to understand why MLMs struggle with high-fidelity perception tasks despite being built on robust visual backbones, specifically investigating whether the necessary visual information is lost during processing or simply rendered inaccessible to the decoding mechanism.

To dissect these internal dynamics, the authors introduce a novel mechanistic interpretability framework applied to three state-of-the-art MLMs. Their key innovation involves bypassing the modelâ€™s standard text generation output to directly probe the Key-Value (KV) cache, extracting "Image Value Tokens" from the language model's layers. By utilizing these internal activations to perform zero-shot perception tasks, the researchers isolate and analyze the quality of visual representations retained at different stages of processing.

The analysis uncovers a paradoxical dynamic where the language model simultaneously enriches and degrades visual data. While LM Image Value Tokens outperform the visual encoder's post-projection representations on semantic tasks, they contain less high-fidelity spatial information than raw SigLIP for pure segmentation tasks. Crucially, the study identifies specific root causes of failure, including artifacts in later-layer key tokens and a "Surfacing Gap." Notably, simply adding a text prefix to image inputs was shown to recover these perception capabilities.

---

## Key Findings

*   **Encoded Capability:** Image value tokens within the language model encode sufficient information to perform perception-heavy tasks (such as segmentation and semantic correspondence) in a zero-shot setting.
*   **Fidelity Paradox:** While the language model augments visual information, it paradoxically contains less high-fidelity visual information compared to the SigLIP encoder for several tasks.
*   **Token Artifacts:** Input-agnostic image key tokens in the later layers contain artifacts that actively degrade perception capability.
*   **The "Surfacing" Gap:** there exists a gap where necessary perception information is present within the model but not surfaced to the output (e.g., in **33.3%** of Art Style questions on BLINK).
*   **Prefix Efficacy:** Adding a text prefix to image inputs improves the perception capabilities of the visual representations.

---

## Methodology

The authors employed a mechanistic interpretability approach to analyze three Multimodal Language Models:
1.  **LLaVA-OneVision**
2.  **Qwen2.5-VL**
3.  **Llama-3-LLaVA-NeXT**

**Process:**
*   Examined the internal processing of visual key-value (KV) tokens to trace information flow.
*   Compared representational quality against the base visual encoder (SigLIP).
*   Analyzed how the language model processes visual data compared to the encoder alone.
*   Assessed the impact of input modifications (specifically text prefixes) using the BLINK benchmark.

---

## Technical Details

**Architecture Components**
*   **Image Encoder:** Vision Transformer (ViT).
*   **Connector:** Linear layer connecting encoder to decoder.
*   **Language Decoder:** The primary component analyzed.

**Architectural Differences & Mechanisms**
*   **Explicit KV Storage:** Utilizes explicit Key-Value storage for static visual caches.
*   **Causal Attention:** Text tokens attend to visual tokens, but visual tokens do not attend to text tokens.
*   **Group Query Attention (GQA):** Shares Keys and Values across attention heads.

**Probing Technique**
*   **Target:** LLaVA-OneVision 7B.
*   **Methodology:** Probed Image Value Tokens within the LM's KV cache.
*   **Tasks Evaluated:**
    *   Few-shot foreground segmentation
    *   Co-segmentation
    *   Semantic segmentation
    *   Referring expression segmentation

---

## Results

The study compared three representations: Visual Encoder (post-projection), LM Image Value (max), and SigLIP without MLM Fine-Tuning.

**LM Enrichment**
LM Image Values consistently outperform the standard Visual Encoder post-projection:
*   **+2.0 mIOU** on Pascal-5i
*   **+5.2 mIOU** on RefCOCO

**Fidelity Loss**
Despite the enrichment above, raw SigLIP often outperforms LM representations on pure segmentation tasks, indicating a loss of high-fidelity visual information within the LM.
*   *Outperformed on:* Pascal, MSRC, ImageNet-S

**Correspondence & Video**
Conversely, LM representations significantly excel in tasks requiring correspondence and temporal understanding:
*   **SPair71K:** 46.1 PCK
*   **DAVIS:** 65.8 J&F

---

## Contributions

*   **New Diagnostic Perspective:** Provides a fresh viewpoint on MLM perception struggles by isolating the roles of visual key and value tokens.
*   **Failure Mode Identification:** Identifies specific architectural failures, including artifacts in later-layer key tokens and the inability to surface existing visual information.
*   **Training Guidance:** Suggests that better mechanisms are needed to control and surface visual information in future visual encoders and language models.
*   **Interpretability Advancement:** Validates that zero-shot perception tasks can be performed directly on language model activations, advancing the field of mechanistic interpretability.

---

**Quality Score:** 9/10 | **References:** 31 citations