---
title: 'The Transparency Paradox in Explainable AI: A Theory of Autonomy Depletion
  Through Cognitive Load'
arxiv_id: '2601.13973'
source_url: https://arxiv.org/abs/2601.13973
generated_at: '2026-02-03T13:38:24'
quality_score: 8
citation_count: 11
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# The Transparency Paradox in Explainable AI: A Theory of Autonomy Depletion Through Cognitive Load

*Ancuta Margondai; Mustapha Mouloua*

---

### ðŸ“‘ Executive Summary

This research addresses the **"transparency paradox"** in Explainable AI (XAI), a phenomenon where providing users with more explanations about AI decision-making degrades rather than improves performance. The authors identify that current static transparency approaches fail because they ignore the cognitive costs associated with processing information, which drains working memory and diminishes the userâ€™s sense of autonomy.

This is a critical issue for high-stakes human-AI collaboration, as information overload can lead to decision errors and user disengagement despite high trust in the system. The key innovation is a **Stochastic Optimal Control Framework** that formalizes human-AI interaction as a dynamic resource management problem. The authors model user autonomy as a continuous stochastic process (geometric Brownian motion) with a drift function $\mu(I) = \mu_0 - \beta I - \gamma I^2$, where information intake ($I$) linearly drains and quadratically accelerates the depletion of autonomy.

By applying Hamilton-Jacobi-Bellman equations, the paper derives optimal transparency policies that adapt in real-time based on the userâ€™s cognitive state. This approach shifts the paradigm from fixed transparency levels to dynamic strategies that actively manage the "cognitive battery" to maximize cumulative reward without triggering a disengagement boundary.

The study establishes an inverted-U relationship between transparency and performance, validating that cognitive load acts as a hard constraint on decision quality. Cited evidence demonstrates that Global explanations result in only 55% decision performance accompanied by high mental effort, whereas Local explanations achieve significantly better results (81%â€“87%) with minimal effort.

Consequently, dynamic transparency policies were proven to outperform both static maximum and minimum transparency approaches by preventing autonomy depletion. This paper significantly advances the field by providing a mathematically rigorous, unified theoretical framework that integrates cognitive load theory with control engineering.

---

### ðŸ“Š Quick Facts

| Metric | Details |
| :--- | :--- |
| **Quality Score** | 8/10 |
| **References** | 11 Citations |
| **Working Memory Cap** | ~4 items (Range: 2â€“6) |
| **Performance (Global)** | 55% (High Effort) |
| **Performance (Local)** | 81%â€“87% (Minimal Effort) |
| **Core Concept** | Autonomy Depletion via Cognitive Load |

---

## Key Findings

*   **The Transparency Paradox:** Transparency effects depend on cognitive resource depletion. Explanations impair decision quality when cognitive load exceeds working memory capacity by reducing perceived control.
*   **Optimal Policy Structure:** Information should be provided when user autonomy is high and accumulated load is low, but withheld when cognitive resources are depleted.
*   **Superiority of Dynamic Policies:** Dynamic transparency policies adapting to real-time cognitive states significantly outperform static maximum and minimum transparency approaches.
*   **Autonomy Modeling:** Autonomy evolves as a stochastic process where information acts as a drain on resources, leading to predictions regarding disengagement timing and autonomy trajectory shapes.

## Methodology

The research utilizes a mathematical modeling approach grounded in stochastic control theory:

*   **Autonomy Formalization:** Autonomy is treated as a continuous stochastic process (geometric Brownian motion) with drift dependent on information intake.
*   **Strategy Derivation:** Optimal transparency strategies are derived using Hamilton-Jacobi-Bellman equations.
*   **Validation:** Theoretical predictions and computational solutions are validated using Monte Carlo simulations.

## Technical Details

The paper proposes a **Stochastic Optimal Control Framework** to model human-AI interaction with the following specifications:

*   **User Autonomy ($A_t$):** Modeled as a continuous stochastic process governed by a diffusion process with Drift ($\mu$) and Volatility ($\sigma_A$).
*   **Drift Function:** The dynamics are defined by $\mu(I) = \mu_0 - \beta I - \gamma I^2$.
    *   $\mu_0$: Baseline intrinsic motivation.
    *   $\beta I$: Linear drain from information.
    *   $\gamma I^2$: Quadratic depletion acceleration.
*   **Disengagement Boundary ($B > 0$):** A defined threshold where users disengage if autonomy falls below $B$.
*   **Control Policy ($u_t$):** Transparency acts as the control variable in a state space of autonomy and load, aiming to maximize cumulative reward ($V(A, I, t)$) without draining the 'Cognitive Battery'.
*   **Mechanism:** Relies on Metacognitive Monitoring consuming Working Memory, independent of trust.

## Results

*   **Performance Discrepancy:** Global explanations result in 55% decision performance with high mental effort, whereas Local explanations yield 81%â€“87% performance with minimal effort.
*   **Inverted-U Relationship:** An inverted-U relationship exists between transparency and performance; excessive transparency causes cognitive overload and impaired decision quality despite high trust.
*   **Cognitive Constraints:** Working Memory capacity limits are approximately 4 items (range 2â€“6), acting as hard constraints.
*   **Neuroimaging Correlates:** Compensatory increases in prefrontal cortex activity are observed during fatigue.
*   **Qualitative Metrics:**
    *   Delayed AI advice reduces anchoring bias compared to immediate advice.
    *   Disengagement is triggered specifically when cognitive load exceeds WM capacity, causing autonomy depletion.

## Contributions

*   **Theoretical Integration:** Addresses limitations of current theories by providing a unified framework explaining the transparency paradox through autonomy depletion and cognitive load.
*   **Mathematical Rigor:** Introduces a rigorous mathematical foundation for modeling human-AI interaction as a control problem, shifting focus to dynamic resource management.
*   **Actionable Design Guidelines:** Provides concrete principles for designing adaptive AI systems, including real-time cognitive state monitoring and information budgets.
*   **Predictive Framework:** Generates distinct, testable hypotheses regarding the interaction between working memory, autonomy trajectories, and information provision.