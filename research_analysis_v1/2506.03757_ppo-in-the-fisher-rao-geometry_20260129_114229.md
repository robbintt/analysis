# PPO in the Fisher-Rao geometry
*Razvan-Andrei Lascu; David Šiška; Łukasz Szpruch*

---

### | Quick Facts |
|---|
| **Algorithm:** FR-PPO (Fisher-Rao Proximal Policy Optimization) |
| **Convergence Rate:** $O(1/N)$ (Sub-linear) |
| **Key Feature:** Dimensionality-Free Convergence |
| **Geometry:** Fisher-Rao (Curved Space) |
| **Quality Score:** 8/10 |
| **References:** 40 Citations |

---

> ### Executive Summary
>
> Despite its widespread adoption, Proximal Policy Optimization (PPO) has historically lacked formal theoretical foundations regarding stability and convergence, often relying on heuristic clipping in flat Euclidean geometry. This paper introduces **FR-PPO** (Fisher-Rao Proximal Policy Optimization), a novel framework grounded in curved Fisher-Rao geometry that formulates policy optimization as a Mirror Descent process using a geodesic-based constraint and a quadratic Total Variation squared ($TV^2$) penalty. The research demonstrates rigorously that FR-PPO achieves a dimensionality-free, sub-linear convergence rate of $O(1/N)$ in tabular settings, providing strong guarantees for monotonic policy improvement. Marking a significant advancement in reinforcement learning theory, this work offers the first formal demonstration of sub-linear convergence for a PPO-based algorithm, establishing a theoretically robust alternative to standard methods while pointing toward future extensions in deep learning.

---

## Key Findings

*   **Development of FR-PPO:** A novel variant of Proximal Policy Optimization derived from a tighter surrogate loss within the Fisher-Rao geometry.
*   **Monotonic Improvement:** Unlike standard PPO, the scheme provides strong theoretical guarantees ensuring monotonic policy improvement.
*   **Dimensionality-Free Convergence:** FR-PPO achieves sub-linear convergence rates independent of the dimensionality of action and state spaces in the tabular setting.
*   **Bridging the Theory Gap:** This work represents a significant step toward establishing formal convergence results for PPO-based algorithms.

## Methodology

The authors employ a geometric reformulation, abandoning the traditional 'flat geometric space' used in Trust Region Policy Optimization (TRPO) and standard PPO. Instead, they derive a tighter surrogate loss function by operating within the **Fisher-Rao (FR) geometry**. This approach replaces the standard KL divergence penalty method with a geodesic-based alternative, allowing for a more robust theoretical framework.

## Contributions

*   **Theoretical Foundation for PPO:** Addresses the critical lack of formal theoretical foundations for policy improvement and convergence in the widely used PPO algorithm.
*   **Formal Convergence Proofs:** Provides the first formal demonstration of sub-linear convergence for a PPO-based algorithm in tabular settings without dependence on space dimensions.
*   **Algorithmic Innovation:** Proposes FR-PPO as a theoretically robust alternative to TRPO/PPO that maintains practical utility while offering superior mathematical guarantees regarding policy stability and improvement.

## Technical Details

*   **Optimization Method:** Utilizes **Mirror Descent** optimization on probability measures.
*   **Constraint Replacement:** Replaces standard KL-divergence constraints or heuristic clipping with a constraint derived from Fisher-Rao geometry.
*   **Objective Function:** Optimizes a surrogate objective using a quadratic **Total Variation squared ($TV^2$)** penalty, resulting in a theoretically tighter bound than standard KL-based methods.
*   **Mathematical Connection:** Connects Fisher-Rao distance to Hellinger and Chi-squared divergences via Bregman divergence $D_h$ to facilitate mathematical proofs.

## Results

The method achieves a **sub-linear convergence rate of $O(1/N)$** in the tabular setting. Notably, this convergence rate is independent of the dimensionality of the state and action spaces. FR-PPO provides a theoretical guarantee of monotonic policy improvement provided the step size remains within trust-region limits and advantages are estimated exactly.

**Limitations:**
*   Restricted to the tabular setting (function approximation not yet analyzed).
*   Guarantees break down with approximate advantage estimation.
*   Lack of analysis regarding stochastic gradient sampling errors.

---

**Paper Quality Score:** 8/10  
**Total References:** 40