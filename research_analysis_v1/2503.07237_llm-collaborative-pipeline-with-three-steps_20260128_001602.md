---
title: 'LLM collaborative pipeline with three steps:'
arxiv_id: '2503.07237'
source_url: https://arxiv.org/abs/2503.07237
generated_at: '2026-01-28T00:16:02'
quality_score: 9
citation_count: 17
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# Collaborative LLM Pipeline for Cross-Cultural Hate Speech Moderation

*Seogyeong Jeong, Junyeong Park, Seyoung Song, Alice Oh, and Yohan Lee*

---

> ### ðŸ“Š Quick Facts
> | Metric | Value |
> | :--- | :--- |
> | **Proposed Model** | LLM-C3M OD |
> | **Dataset** | KOLD (Korean Offensive Language Dataset) |
> | **System Accuracy** | **78%** (vs. 71% GPT-4o baseline) |
> | **Workload Reduction** | **83.6%** |
> | **Non-Native (MT Only)** | 56.25% accuracy |
> | **Quality Score** | 9/10 |

---

## Executive Summary

Content moderation faces a critical scalability challenge driven by the scarcity of native speakers for low-resource languages and the cultural limitations of non-native moderators. Non-native moderators frequently fail to accurately detect hate speech due to a lack of understanding regarding culturally-specific knowledge, sentiment, and internet slang. While Large Language Models (LLMs) offer a potential automated solution, they often struggle with the nuanced context required for accurate moderation.

This paper addresses the "resource imbalance" in moderation pipelines by proposing **LLM-C3M OD**, a novel three-step collaborative pipeline that integrates Retrieval-Augmented Generation (RAG) with consensus-based LLM filtering to augment human capabilities. The technical workflow ensures that human effort is focused exclusively on the most ambiguous and culturally complex content.

Evaluated on the KOLD dataset using Indonesian and German participants, the system achieved **78% accuracy**, surpassing the GPT-4o baseline (71%), while delivering an **83.6% reduction** in human workload. This research provides a validated framework for inclusive content moderation that reduces reliance on scarce native speakers without sacrificing accuracy.

---

## Key Findings

*   **Superior Accuracy:** The collaborative system achieved **78% accuracy** on a Korean hate speech dataset, outperforming the GPT-4o baseline of 71%.
*   **High Efficiency:** The pipeline reduced human workload by **83.6%**, demonstrating exceptional operational efficiency.
*   **Cultural Barriers:** Non-native moderators struggle significantly with interpreting culturally-specific knowledge, sentiment, and internet culture without support.
*   **Human-in-the-Loop Validation:** Humans excel at moderating nuanced content where LLMs typically struggle, validating the necessity of human oversight.
*   **Cross-Cultural Enablement:** Non-native moderators can effectively perform cross-cultural hate speech moderation when supported by LLMs and cultural context.

---

## Proposed Methodology

The researchers introduced **LLM-C3M OD**, a human-LLM collaborative pipeline designed to bridge the gap between automated systems and non-native human capabilities. The pipeline consists of three distinct steps:

1.  **Context Annotation:** Retrieval-Augmented Generation (RAG) is utilized to perform web searches and retrieve necessary cultural context, definitions, and nuances relevant to the content.
2.  **Initial Automated Moderation:** LLMs act as annotator agents to perform a first-pass screening of the content.
3.  **Targeted Human Review:** Moderators (non-native speakers) address *only* the specific cases where the LLM agents failed to reach a consensus.

**Evaluation Setup:**
The system was evaluated using the **KOLD** (Korean Offensive Language Dataset), tested with Indonesian and German participants acting as non-native moderators.

---

## Core Contributions

*   **Proposed Framework:** Development of a novel three-step pipeline (LLM-C3M OD) that integrates RAG for cultural context with consensus-based LLM filtering to optimize human efforts.
*   **Empirical Problem Definition:** Identification of specific gaps in non-native moderation capabilities regarding cultural knowledge and internet slang through user studies.
*   **Operational Efficiency:** Evidence that hybrid AI-human systems can surpass standalone state-of-the-art model performance (GPT-4o) while significantly reducing manual labor costs.
*   **Inclusivity in Moderation:** A scalable solution to resource imbalance, enabling effective policing of hate speech in low-resource languages without exclusive reliance on scarce native speakers.

---

## Technical Architecture

The LLM-C3M OD system utilizes a Retrieval-Augmented Generation (RAG) architecture combined with a consensus mechanism.

**System Components:**
*   **Cultural Context Annotation:** Uses web search APIs to gather external cultural context and slang definitions for the input text.
*   **LLM Agent Annotators:** Multiple LLM instances (GPT-4o) analyze the text and the retrieved context to vote on whether the content constitutes hate speech.
*   **Consensus Filter:** If the LLM agents agree, the decision is automated. If they disagree, the case is flagged.

**Workflow:**
1.  **Input:** Raw text data (Korean).
2.  **Enrichment:** RAG layer appends cultural context notes.
3.  **Automated Screening:** LLMs vote. Consensus $\rightarrow$ Auto-Decision.
4.  **Human Intervention:** No Consensus $\rightarrow$ Sent to Non-Native Moderator (with context notes).

---

## Evaluation Results

The performance of LLM-C3M OD was compared against a standard GPT-4o baseline and a non-native moderator baseline using only machine translation.

| Configuration | Accuracy | Notes |
| :--- | :--- | :--- |
| **LLM-C3M OD** | **78%** | *Includes RAG + Human Review* |
| **GPT-4o Baseline** | 71% | *Model alone without pipeline* |
| **Non-Native (MT Only)** | 56.25% | *Machine translation without cultural support* |

**Operational Metrics:**
*   **Workload Reduction:** 83.6% reduction in manual review required compared to full manual moderation.
*   **Participant Pool:** Indonesian and German speakers moderating Korean content.

---

## Additional Information

*   **Paper Quality Score:** 9/10
*   **References:** 17 Citations
*   **Keywords:** Native Hate, Collaborative System, Original Hate, Hate Speech, Cultural Context