# Fusing Rewards and Preferences in Reinforcement Learning
*Sadegh Khorasani; Saber Salehkaleybar; Negar Kiyavash; Matthias Grossglauser*

---

## üìä Quick Facts

| Metric | Details |
| :--- | :--- |
| **Topic** | RLHF & Preference Learning |
| **Algorithm** | Dual-Feedback Actor (DFA) |
| **Key Baseline** | Soft Actor-Critic (SAC) |
| **Core Mechanism** | Unified Update Rule |
| **Quality Score** | 8/10 |
| **Citations** | 25 References |

---

## üìù Executive Summary

> **The Challenge:** Reinforcement Learning from Human Feedback (RLHF) typically relies on a disjointed two-stage process: training a separate reward model and then optimizing a policy against it. This decoupling introduces risks such as distributional shift, reward hacking, and instability due to inaccurate reward proxies.
>
> **The Solution:** This paper introduces the **Dual-Feedback Actor (DFA)**, a novel algorithm that unifies numeric rewards and pairwise preferences into a single policy update rule. By eliminating the separate reward-modeling step, DFA reduces complexity and mitigates the risk of compounding errors.
>
> **The Impact:** DFA establishes a theoretical bridge between preference-based learning and maximum entropy RL, demonstrating that human feedback can act as a direct policy constraint. Experimental results show that DFA not only matches or exceeds the performance of entropy-regularized SAC (Soft Actor-Critic) but does so with significantly superior stability.

---

## üîë Key Findings

*   **Theoretical Equivalence:** Under a Bradley-Terry model, the paper mathematically proves that minimizing the preference loss of DFA recovers the optimal policy of entropy-regularized Soft Actor-Critic (SAC).
*   **Superior Stability and Performance:** Across six control environments using generated preferences, DFA matched or exceeded SAC performance while exhibiting a notably more stable training process.
*   **RLHF Efficacy:** In a stochastic GridWorld environment trained on semi-synthetic preference datasets, DFA outperformed standard reward-modeling RLHF baselines.
*   **Oracle-Level Performance:** The algorithm demonstrated the ability to approach the performance of an "oracle" agent that uses true ground-truth rewards.

---

## üß© Methodology

The research proposes the **Dual-Feedback Actor (DFA)**, an algorithm designed to integrate scalar rewards with pairwise preferences without training an intermediate reward model.

*   **Unified Update Rule:** Unlike standard approaches, DFA utilizes the policy's log-probabilities directly to model preference probability, fusing individual rewards and preferences into a single update.
*   **Feedback Types:** The framework accepts:
    *   Human annotations (state or trajectory level).
    *   Synthesized online preferences derived from Q-values in an off-policy replay buffer.
*   **Bradley-Terry Model:** The algorithm operates under this model to handle pairwise comparisons, ensuring a robust statistical foundation for preference integration.

---

## ‚ú® Contributions

1.  **Unified Update Mechanism:** Eliminates the separate reward-modeling step by fusing individual rewards and pairwise preferences directly into the policy update.
2.  **Theoretical Foundation:** Provides a formal mathematical proof linking preference-based learning to maximum entropy reinforcement learning, proving DFA recovers the SAC policy.
3.  **Versatile Data Integration:** Introduces a method capable of leveraging both human-generated (RLHF) and automatically synthesized preferences from replay buffers within a single algorithmic framework.

---

## ‚öôÔ∏è Technical Details

*   **Algorithm Name:** Dual-Feedback Actor (DFA)
*   **Core Function:** Fuses numeric rewards and human preferences without inferring a latent reward function.

**Mathematical Formulation:**
*   **Feedback Mechanism:** Employs a state-wise feedback mechanism comparing actions $a^+$ and $a^-$ at state $s$.
*   **Preference Probability:** Modeled directly via the policy as:
    $$P_\theta(a^+ \succ a^- | s) = \frac{\pi_\theta(a^+|s)^\alpha}{\pi_\theta(a^+|s)^\alpha + \pi_\theta(a^-|s)^\alpha}$$
*   **Loss Minimization:** The algorithm minimizes a state-wise preference loss that is equivalent to recovering an entropy-regularized SAC policy.
*   **Synthesis Mechanism:** Generates synthetic preferences online by comparing Q-values of actions from a state and its nearest neighbor in the replay buffer.

---

## üìà Results

*   **Control Environments:** Experimental evaluations across six continuous control environments showed DFA matches or exceeds the performance of Soft Actor-Critic (SAC) while demonstrating greater training stability.
*   **Stochastic GridWorld:** With semi-synthetic data, DFA outperformed standard reward-modeling RLHF baselines.
*   **Theoretical Validation:** The method was proven to recover the optimal policy of entropy-regularized SAC.
*   **Optimization:** successfully approached the performance of an oracle using true rewards.