# Small Language Models (SLMs) Can Still Pack a Punch: A Survey

*Shreyas Subramanian; Vikram Elango; Mecit Gungor*

> ## Executive Summary
>
> The current trajectory of artificial intelligence research is dominated by the "bigger is better" paradigm, where performance improvements are strictly tied to increasing parameter counts. This approach creates significant barriers to entry due to the prohibitive computational costs, hardware requirements, and operational latency associated with Large Language Models (LLMs). This paper addresses the critical need to decouple model capability from model scale, investigating why this matters for the broader adoption of AI. It posits that for widespread, practical deployment—particularly on edge devices and in cost-sensitive environments—the field must pivot toward optimizing the trade-offs between efficiency, scalability, and cost without sacrificing the reasoning capabilities required for complex tasks.
>
> The key innovation of this work is the establishment of a comprehensive taxonomy and the introduction of the concept of **"effective size"** as a superior metric for evaluating model intelligence. Rather than relying solely on raw parameter counts, "effective size" characterizes capability relative to resource consumption. Technically, the authors survey approximately 160 papers to classify SLMs (1B–8B range) into task-agnostic and task-specific domains, analyzing the architectures and training regimes that yield high effective sizes. The study highlights specific technical enablers, such as the transition from dense Transformer architectures to sparse Mixture-of-Experts (MoE) configurations like Mixtral, and the use of specialized training methodologies like Progressive Learning (Orca), Explanation Tuning, and Equation-of-Thought Distillation. It further identifies hardware optimizations that allow for the training of 7B models on a single RTX 4090, drastically lowering the barrier to entry for high-performance model development.
>
> The survey provides compelling quantitative evidence that SLMs can match or exceed the performance of significantly larger LLMs across a variety of benchmarks. In mathematical reasoning, the 13B parameter WizardMath model surpassed the 70B parameter Llama 2 on the GSM8k benchmark. Similarly, in code generation, Code Llama 7B outperformed Llama 2 70B on HumanEval, and the specialized SLaDe model (200M parameters) produced more correct code than the Ghidra decompiler on ExeBench. Other notable results include ALMA-13B-R beating GPT-4 on WMT translation benchmarks and BioGPT surpassing GPT-4 on PubMedQA. Perhaps most strikingly, blended ensembles of smaller models achieved higher user engagement than the 175B parameter ChatGPT, and the Mixtral 8x7B model outperformed GPT-3.5, Claude-2.1, and Llama 2 70B in human evaluations, validating the efficacy of MoE architectures.
>
> This research significantly shifts the industry narrative by validating smaller architectures as a viable, primary path for foundation model advancement rather than merely a compromise. By offering a guidance framework for building models that balance high performance with resource constraints, the authors democratize access to state-of-the-art AI, enabling development on consumer-grade hardware and facilitating edge deployment. The introduction of "effective size" encourages the AI community to prioritize data quality, tokenization nuance (e.g., SLaDe), and training efficiency over brute-force scaling. This influence is likely to accelerate the development of domain-specific, vertical-specific models that offer superior latency and privacy profiles compared to monolithic, generalized LLMs.

---

## Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Parameter Range** | 1B to 8B |
| **Papers Analyzed** | ~160 |
| **Key Metric** | "Effective Size" |
| **Hardware Feasibility** | 7B model on single RTX 4090 |
| **Quality Score** | 8/10 |
| **References** | 40 citations |

---

## Key Findings

*   **Performance Parity:** Small Language Models (SLMs) in the 1 to 8 billion parameter range can achieve performance levels comparable to, or sometimes exceeding, those of significantly larger Large Language Models (LLMs).
*   **Operational Efficiency:** SLMs offer a distinct advantage in optimizing the trade-offs between model performance and operational factors such as efficiency, scalability, and cost.
*   **New Viability:** Smaller architectures offer a viable alternative path for advancement in foundation AI models.
*   **Effective Size Metric:** The capability of SLMs is better characterized by their **"effective size,"** a metric that accounts for their increased capability relative to their parameter count.

---

## Methodology

The authors employed a rigorous literature survey to gather and analyze data:

*   **Scope:** Conducted a comprehensive analysis of approximately **160 research papers**.
*   **Classification:** Systematically classified SLMs into three primary domains:
    1.  Task-agnostic models
    2.  Task-specific models
    3.  Specific techniques used to create and train them
*   **Comparative Analysis:** Evaluated SLMs against LLMs to determine performance thresholds and define characteristics such as "effective size."

---

## Technical Details

### Architectures & Configurations
*   **Standard Transformer:** Utilized in the Llama family (trained on 15T tokens via `Pretraining -> SFT -> RLHF`).
*   **Sparse Mixture of Experts (MoE):** Utilized in Mixtral (router selects two experts per token).
*   **Parameter Definition:** SLMs are primarily defined in the **1B to 8B** range.

### Training Methodologies
*   **Imitation Learning (IL)**
*   **Progressive Learning:** Focuses on reasoning processes from larger models (e.g., Orca).
*   **Explanation Tuning**
*   **Modularized Training:** Includes Blended Ensembles and LoRA domain adaptation.

### Task-Specific Optimizations
*   **Math:** Equation-of-Thought Distillation.
*   **Code:** Novel tokenization for code decompilation (SLaDe).

### Hardware & Hyperparameters
*   **Hardware:** Optimizations enable training 7B models on a single **RTX 4090**.

| Model | Parameters | Layers | Dimension |
| :--- | :--- | :--- | :--- |
| **Mistral 7B** | 7B | 32 | 4096 |
| **Gemma 2B** | 2B | 18 | 2048 |
| **Phi-3 Mini** | Mini (3.8B) | 32 | 3072 |

---

## Results

*   **General Performance:** Performance is better characterized by **'effective size'** rather than raw parameter count.
*   **Quantization & Tiny Models:** Llama 3 shows robustness in ultra-low bit-width quantization; TinyStories (10M) demonstrates coherent story generation with synthetic data.
*   **Mathematical Reasoning:** WizardMath 13B surpassed Llama 2 70B on GSM8k.
*   **Code Generation:**
    *   WizardCoder outperformed Claude and Bard.
    *   Code Llama 7B beat Llama 2 70B on HumanEval.
    *   SLaDe (200M) produced more correct code than Ghidra on ExeBench.
*   **Translation:** ALMA-13B-R surpassed GPT-4 on WMT translation benchmarks.
*   **Domain Specifics:**
    *   **Medical:** BioGPT beat GPT-4 on PubMedQA.
    *   **Legal:** ChatLaw outperformed GPT-4 in Chinese law.
*   **Ensembles & MoE:**
    *   Blended ensembles of smaller models achieved higher user engagement than 175B+ ChatGPT.
    *   Mixtral (MoE) surpassed GPT-3.5, Claude-2.1, and Llama 2 70B in human evaluations.

---

## Contributions

1.  **Structured Taxonomy:** Provides a structured taxonomy of the current landscape of small language models, distinguishing between general-purpose and specialized applications.
2.  **Guidance Framework:** Offers a guidance framework for the AI community on how to build models that successfully balance high performance with resource constraints.
3.  **Concept Definition:** Introduces and defines the concept of **"effective sizes"** for SLMs to establish a nuanced way to describe model capabilities relative to model size.