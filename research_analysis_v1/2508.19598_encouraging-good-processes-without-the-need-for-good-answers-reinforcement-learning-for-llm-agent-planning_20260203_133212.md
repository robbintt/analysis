---
title: 'Encouraging Good Processes Without the Need for Good Answers: Reinforcement
  Learning for LLM Agent Planning'
arxiv_id: '2508.19598'
source_url: https://arxiv.org/abs/2508.19598
generated_at: '2026-02-03T13:32:12'
quality_score: 8
citation_count: 25
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Encouraging Good Processes Without the Need for Good Answers: Reinforcement Learning for LLM Agent Planning

*Zhiwei Li; Yong Hu; Wenqing Wang*

---

> ### ðŸ“Š Quick Facts
>
> *   **Planning Performance Gain:** 8%â€“12% improvement over baselines.
> *   **Response Quality Gain:** 5%â€“6% increase in final output.
> *   **Framework:** Reinforcement Learning with Tool-use Rewards (RLTR).
> *   **Key Innovation:** Optimizing "process" (tool invocation) rather than "answer" (text).
> *   **Data Efficiency:** Effective on "unverifiable" data (~99% of industry data).
> *   **Quality Score:** 8/10

---

## Executive Summary

Training Large Language Model (LLM) agents typically requires end-to-end supervision, which depends heavily on the availability of verifiable ground-truth answers to assess performance. This approach faces a critical scalability issue in real-world applications, where approximately 99% of industry data is "unverifiable" and lacks definitive correct labels. Furthermore, traditional end-to-end training methods often suffer from imbalanced objective allocation and "reward hacking," where models optimize for flaws in the reward signal rather than learning robust reasoning.

This paper addresses the fundamental challenge of how to effectively optimize agent planning capabilities when the correctness of the final answer cannot be easily verified or measured. The authors introduce the **Reinforcement Learning with Tool-use Rewards (RLTR)** framework, which decouples agent training into two distinct modules: a **Planner Policy ($\pi_p$)** responsible for tool invocation and a frozen **Summarizer Policy ($\pi_s$)** responsible for generating the final response.

The core technical innovation lies in shifting the optimization target from the final answer to the quality of the reasoning process. RLTR employs a reward mechanism based on "tool-use completeness," evaluated by a Verification LLM, to judge the sequence of tool calls rather than the final text output. Experimental evaluations demonstrate that the RLTR framework significantly outperforms end-to-end baselines, achieving an 8% to 12% improvement in planning performance and a 5% to 6% increase in final response quality. By relying on tool invocation sequences for optimization, the framework successfully handles "unverifiable" datasets that constitute the vast majority of real-world data.

---

## Key Findings

*   **Performance Gains:** The proposed RLTR framework achieves an **8%â€“12% improvement** in planning performance compared to end-to-end baselines, leading to a **5%â€“6% increase** in final response quality.
*   **Process Over Answer:** Training signals based on **tool invocation sequences** provide a more reliable metric for optimization than assessing final response content.
*   **Decoupled Success:** Optimizing the action planning module improves the final output without requiring ground-truth verification.
*   **Reliability:** The tool-use completeness metric is identified as a more reliable optimization signal than final response content assessment, resolving credit assignment difficulties.
*   **Robustness:** The framework is effective for 'unverifiable' data (approx. 99% of industry data), avoiding reward hacking issues found in traditional models.

---

## Methodology

The **Reinforcement Learning with Tool-use Rewards (RLTR)** framework utilizes a decoupled training process designed to isolate the planning module from answer summarization. This allows for single-objective optimization focused on the reasoning process.

*   **Decoupled Architecture:** The framework separates the agent into a Planner (for actions) and a Summarizer (for responses).
*   **Reward Mechanism:** It employs a reward mechanism based on **tool-use completeness** to evaluate the quality of tool invocation sequences rather than relying on the correctness of the final text response.
*   **Verification:** A Verification LLM assesses whether the sequence of tools called was appropriate and complete, independent of the final answer's accuracy.

---

## Technical Details

The paper proposes a structured approach to agent training using mathematical formulations for policies and rewards.

### Core Components
*   **Planner Policy ($\pi_p$):** Responsible for generating tool calls.
*   **Summarizer Policy ($\pi_s$):** Responsible for generating the final text response (kept frozen during RL optimization).
*   **State Space:** Includes the user query and tool history.
*   **Action Space:** Involves selecting from available tool sets or issuing a terminal action.

### Training Phases
1.  **Cold Start SFT:** Utilizes knowledge distillation and rejection sampling from a Teacher LLM to initialize the model.
2.  **Reward Signal Generation:** Generates rewards based on **tool-use completeness ($\gamma$)** via a Verification LLM.
3.  **Multi-turn RL:** Optimizes the Planner Policy using the generated reward signals while the Summarizer Policy remains frozen.

---

## Contributions

*   **Solving Data Scarcity:** Provides a solution for data scarcity by enabling effective agent training without verifiable ground-truth answers through tool-use verification.
*   **Balanced Objectives:** Addresses imbalanced objective allocation in end-to-end training by decoupling action planning from answer summarization.
*   **Paradigm Shift:** Establishes that evaluating the **'process' (tool invocation)** is a superior alternative to evaluating the **'answer'** for training LLM agents.
*   **Industrial Applicability:** Offers a practical solution to the data scarcity bottleneck, enabling high-quality agent training on the vast majority of real-world, unlabeled data.

---

## Results

*   **Quantitative Improvement:** The RLTR framework achieved an **8%â€“12% improvement** in planning performance and a **5%â€“6% increase** in final response quality compared to end-to-end baselines.
*   **Handling Unverifiable Data:** Proven effective for 'unverifiable' data, which constitutes approximately 99% of industry data.
*   **Avoiding Reward Hacking:** Successfully avoids reward hacking issues found in traditional models by focusing on process completeness rather than content fluency or perceived accuracy.

***

**Quality Score:** 8/10 | **References:** 25 citations