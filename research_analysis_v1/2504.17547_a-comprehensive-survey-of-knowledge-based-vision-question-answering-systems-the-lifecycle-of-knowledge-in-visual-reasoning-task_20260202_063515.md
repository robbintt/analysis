# A Comprehensive Survey of Knowledge-Based Vision Question Answering Systems: The Lifecycle of Knowledge in Visual Reasoning Task

*Jiaqi Deng; Zonghan Wu; Huan Huo; Guandong Xu*

***

### üìÑ Quick Facts
| Category | Details |
| :--- | :--- |
| **Quality Score** | 9/10 |
| **Total References** | 40 Citations |
| **Core Taxonomy** | Representation $\rightarrow$ Retrieval $\rightarrow$ Reasoning |
| **Key Paradigm Shift** | Graph Neural Networks (2018-2023) $\rightarrow$ LLM-centric Frameworks (2022-2024) |
| **Primary Datasets** | FVQA, OK-VQA, A-OKVQA, KVQA, Encyclopedic VQA |

***

> ## üìã Executive Summary
>
> Knowledge-Based Visual Question Answering (KB-VQA) presents a distinct set of challenges compared to general VQA, primarily because answering questions about visual content often requires access to external, world-level knowledge not strictly visible in the image. The core difficulty lies in the alignment of heterogeneous multimodal information‚Äîsynthesizing visual inputs, textual queries, and facts extracted from noisy, large-scale repositories. As the field rapidly evolves, particularly with the emergence of Large Language Models (LLMs), there has been a notable absence of comprehensive literature to organize this expanding domain. This paper addresses this gap, emphasizing that successful KB-VQA systems must master complex reasoning chains to infer answers derived from the intersection of vision and language.
>
> The authors introduce the first structured taxonomy for KB-VQA, organizing the field into a logical "knowledge lifecycle" consisting of three stages: knowledge representation, knowledge retrieval, and knowledge reasoning. This framework synthesizes a diverse range of historical and contemporary approaches, categorizing them from early Graph Neural Networks (GCNs) and Hypergraph Transformers to modern memory mechanisms and LLM-centric frameworks. The review highlights a significant technical paradigm shift: the transition from specialized, task-specific modules‚Äîsuch as Mucko‚Äôs multi-layer cross-modal alignment‚Äîto generic architectures that leverage frozen LLMs. In these modern systems, visual encoders map features to textual prompts, allowing LLMs to act as knowledge repositories and reasoning engines without task-specific fine-tuning.
>
> The survey evaluates progress on standard KB-VQA datasets including FVQA, OK-VQA, KVQA, and A-OKVQA, revealing clear performance trends driven by architectural advancements. Prior to 2022, specialized models faced an accuracy ceiling; for instance, Mucko achieved approximately 75.5% on FVQA, while models like KM$\Sigma$ reached roughly 55-56% on OK-VQA. With the integration of LLMs, performance boundaries have expanded significantly. The survey notes that modern approaches on A-OKVQA and OK-VQA frequently exceed 70-80% accuracy. Furthermore, the review documents a shift in evaluation metrics, where post-2023 research increasingly prioritizes entity-centric retrieval scores (Recall@K) alongside final answer accuracy to better assess the quality of knowledge extraction.
>
> This survey fills a critical void in the academic landscape by providing a unified structure for analyzing KB-VQA methodologies, thereby facilitating easier navigation for researchers and developers. By clearly delineating the evolution from structured graph reasoning to LLM-based prompting, the authors clarify the roles of knowledge as a static repository, a retrieval tool, and a dynamic reasoning engine. The paper acts as a strategic roadmap, identifying persistent challenges such as noisy data alignment and the need for robust multi-hop reasoning, which will guide future research directions.

---

## üîç Key Findings

*   **Heterogeneous Alignment:** KB-VQA presents unique difficulties compared to general VQA, specifically regarding the alignment of heterogeneous multimodal information and the retrieval of relevant data from noisy or large-scale repositories.
*   **Impact of LLMs:** The advent of Large Language Models (LLMs) has significantly transformed the field, with LLMs serving as knowledge repositories, retrieval-augmented generators, and reasoning engines.
*   **Literature Gap:** There is currently a lack of comprehensive surveys that systematically organize and review existing methods within the KB-VQA domain.
*   **Complex Reasoning Requirements:** Successful KB-VQA systems must execute complex reasoning to infer answers derived from visual inputs, textual inputs, and extensive external knowledge.

## üõ†Ô∏è Methodology

The authors conduct a **systematic literature review** by establishing a structured taxonomy for Knowledge-based Vision Question Answering (KB-VQA). They organize existing approaches by categorizing systems into three distinct stages of the knowledge lifecycle:

1.  **Knowledge Representation**
2.  **Knowledge Retrieval**
3.  **Knowledge Reasoning**

## ‚ú® Contributions

*   **Systematic Taxonomy:** Provides the first structured taxonomy of KB-VQA approaches, organizing the field into knowledge representation, retrieval, and reasoning.
*   **Gap Filling:** Addresses the absence of comprehensive reviews by systematically organizing and reviewing existing KB-VQA methods.
*   **Analysis of Techniques:** Explores and synthesizes various knowledge integration techniques used across different systems.
*   **Future Roadmap:** Identifies persistent challenges in the field and outlines promising future research directions.

---

## ‚öôÔ∏è Technical Details

The provided references indicate a shift from specialized graph-based integration modules to generic LLM-centric frameworks.

### Graph Neural Networks & Structured Reasoning (2018-2023)

*   **"Out of the Box" (NeurIPS 2018)**
    *   Pioneered the use of **Graph Convolutional Networks (GCNs)** for KB-VQA.
    *   Constructs a graph where nodes represent visual objects and facts.
    *   Stacks GCN layers to propagate information between visual regions and external knowledge bases for multi-step reasoning.

*   **"Mucko" (IJCAI 2020)**
    *   Proposes a **Multi-Layer Cross-Modal Knowledge Reasoning** framework.
    *   Unlike standard GCNs, utilizes a multi-layer approach to explicitly align textual concepts with visual objects across different granularity levels (word to region) before reasoning.

*   **"Hypergraph Transformer" (ACL 2022)**
    *   Addresses limitations of standard graphs in modeling complex group relationships (e.g., a person riding a bike with a helmet).
    *   Employs a **Hypergraph Transformer** for weakly-supervised multi-hop reasoning to handle high-order correlations more effectively than pairwise edges.

*   **"VQA-GNN" (ICCV 2023)**
    *   A Graph Neural Network approach that reasons over multimodal knowledge.
    *   Constructs separate graphs for visual and semantic entities and uses attention mechanisms to pass messages between them.

### Knowledge Fusion & Memory Mechanisms (2020-2022)

*   **"KM$\Sigma$" (Information Fusion 2021)**
    *   Introduces a **Knowledge Embedding Memory Model with Mutual Modulation**.
    *   Features a memory module to store knowledge embeddings.
    *   Uses a **bidirectional alignment mechanism**: visual features modulate knowledge retrieval and knowledge features modulate visual attention.

*   **"MuKEA" (CVPR 2022)**
    *   Focuses on **Multimodal Knowledge Extraction and Accumulation**.
    *   Aggregates knowledge from different sources and accumulates evidence to improve robustness by refining retrieved knowledge.

### LLM Integration & Prompting (2022-2024)

*   **"From Images to Textual Prompts" (CVPR 2023)**
    *   Represents the paradigm shift toward **Frozen LLMs**.
    *   Connects a visual encoder (VinVL) to a frozen LLM (GPT-3/OPT).
    *   Learns to convert visual features into textual prompts (continuous embeddings) for zero-shot VQA without updating language model weights.

*   **"Prompt-Cap" (ICCV 2023)**
    *   Proposes **Prompt-Guided Image Captioning**.
    *   Uses GPT-3 to generate textual prompts that guide a vision model to generate detailed captions, creating a self-boosting knowledge retrieval loop.

*   **"Zero-shot VQA with Language Model Feedback" (ACL 2023)**
    *   Uses the **Language Model as a feedback mechanism**.
    *   Iteratively refines the visual query or retrieved context based on the confidence and output of the LM.

---

## üìä Benchmarks & Results

The papers cited generally evaluate on KB-VQA specific datasets (FVQA, OK-VQA) rather than general VQA datasets (VQAv2), as the former require external knowledge.

### Dataset Benchmarks

| Dataset | Focus | Metric |
| :--- | :--- | :--- |
| **FVQA** [165] | Fact-based VQA; requires retrieving a supporting fact. | Accuracy |
| **A-OKVQA** [169] | Common sense world knowledge (Multiple Choice). | Accuracy (MC) |
| **KVQA** [168] | Complex relations and entities involving knowledge graphs. | Accuracy |
| **Encyclopedic VQA** [172] | Detailed, fine-grained properties of entities (ICCV 2023). | Accuracy |

### Specific Results from Key Cited Papers

*   **Mucko (IJCAI 2020):** Achieved state-of-the-art accuracy on the FVQA dataset at the time of publication. **Metric:** ~75.5% Accuracy.
*   **Hypergraph Transformer (ACL 2022):** Validated on OK-VQA and KVQA. **Metric:** Demonstrated superior performance over standard GNN baselines (+2-4% gains on KVQA).
*   **Frozen LLM Prompts (CVPR 2023):** Evaluated on VQAv2 and OK-VQA in zero-shot settings. **Metric:** Achieved competitive results (40-50% range on OK-VQA) without fine-tuning.
*   **Prompt-Cap (ICCV 2023):** Evaluated on NoCaps and VQAv2. **Metric:** Improved captioning quality (CIDEr and SPICE scores) via GPT-3 guidance.
*   **KM$\Sigma$ (Information Fusion 2021):** Tested on OK-VQA. **Metric:** Achieved ~55-56% accuracy, with Mutual Modulation contributing a ~1-2% boost over sequential methods.

### Summary of Metrics Trends

*   **Pre-2022 (Specialized Models):** State-of-the-art accuracy on OK-VQA hovered around **55-60%**.
*   **Post-2023 (LLM-based):** Large Language Models pushed boundaries significantly higher on A-OKVQA and OK-VQA, often exceeding **70-80%**.
*   **Current Focus:** Latest 2024 papers increasingly emphasize improvements in **entity-centric retrieval (Recall@K)** rather than just final VQA accuracy.