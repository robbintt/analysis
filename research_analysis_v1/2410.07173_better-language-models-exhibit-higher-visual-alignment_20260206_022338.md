---
title: Better Language Models Exhibit Higher Visual Alignment
arxiv_id: '2410.07173'
source_url: https://arxiv.org/abs/2410.07173
generated_at: '2026-02-06T02:23:38'
quality_score: 8
citation_count: 25
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Better Language Models Exhibit Higher Visual Alignment

***Jona Ruthardt; Gertjan J. Burghouts; Serge Belongie; Yuki M. Asano***

---

> ### ⚡ Quick Facts
>
> *   **ImageNet Accuracy:** 51% (Top-1)
> *   **Training Efficiency:** 563k image-caption pairs; <1 GPU-hour
> *   **Cross-lingual Performance:** 38.7% (Chinese) vs. CLIP's 1.4%
> *   **Key Correlation:** 0.768 Pearson coefficient (Language Modeling vs. Visual Alignment)
> *   **Methodology:** Zero-shot probing with frozen backbones

---

## Executive Summary

This research addresses the critical challenge of efficiently aligning language models with visual concepts to create performant Vision-Language (V-L) models without prohibitive computational costs. While existing models like CLIP require massive datasets and significant compute resources to achieve visual alignment, it remains unclear which architectural properties of language models best support visual grounding and generalization. This matters because developing efficient multimodal systems is essential for advancing AI capabilities, yet current methods are often resource-intensive and struggle with generalization across languages and novel concepts.

The paper introduces **ShareLock** (Shared Vision-Language-Locked Tuning), a parameter-efficient training strategy designed to fuse frozen vision and language backbones. Technically, the method employs a lightweight late-fusion architecture where the heavy backbones remain frozen, and only a small learnable projection network is trained to map text embeddings into the vision encoder's latent space. This approach is validated by a preliminary zero-shot probing framework, which establishes that decoder-based language models possess intrinsically higher visual alignment than encoder-based models, revealing a strong positive correlation between language modeling performance and visual generalization capabilities.

The study demonstrates that decoder-based LLMs significantly outperform encoder-based models in visual alignment tasks. The ShareLock method achieves remarkable efficiency, attaining 51% top-1 accuracy on ImageNet using only 563k image-caption pairs and less than one GPU-hour of training. Furthermore, the model exhibits robust cross-lingual transfer capabilities, achieving 38.7% top-1 accuracy on Chinese image classification—a drastic improvement over CLIP’s 1.4%. ShareLock also demonstrated superior performance in tasks requiring compositional reasoning compared to established baselines.

The significance of this work lies in its demonstration that advancements in text-only language models directly translate to improved visual grounding, offering a clear pathway for developing more general multimodal intelligence. By proving that state-of-the-art vision-language performance can be achieved with a fraction of the data and compute required by previous methods, the authors lower the barrier to entry for training high-capacity multimodal models. Additionally, the method's success in cross-lingual settings addresses a major limitation of current models, potentially enabling the creation of more inclusive, globally applicable vision-language systems.

---

## Key Findings

*   **Architectural Superiority of Decoders:** Decoder-based language models exhibit significantly stronger visual alignment than encoder-based models.
*   **Performance Correlation:** A direct positive correlation exists between language modeling performance (MMLU-Pro scores) and visual generalization capabilities (Pearson coefficient of 0.768).
*   **Extreme Efficiency:** The ShareLock method achieves robust performance (51% ImageNet accuracy) with drastically reduced resource requirements (563k image-caption pairs, less than one GPU-hour).
*   **Cross-lingual Dominance:** ShareLock dramatically outperforms CLIP in cross-lingual settings, achieving **38.7%** top-1 accuracy on Chinese image classification compared to CLIP's **1.4%**.

---

## Methodology

The research employs a systematic two-pronged approach:

1.  **Evaluation Framework:**
    *   The authors assess the visual alignment of text-only LLMs by incorporating frozen representations of various language models into a discriminative vision-language framework.
    *   They measure the ability to generalize zero-shot to novel concepts.
2.  **ShareLock Proposal:**
    *   Building on the insights from the evaluation, the authors propose ShareLock.
    *   It is a lightweight fusion method that fuses frozen vision and language backbones.
    *   It requires minimal training on paired image-caption data.

---

## Technical Details

The following table outlines the core technical specifications of the ShareLock architecture and evaluation strategy:

| Component | Description |
| :--- | :--- |
| **Probing Methodology** | Utilizes a zero-shot probing methodology with disjoint concepts to measure visual alignment without multi-modal pretraining. Ensures the model is evaluated on unseen concepts by splitting datasets into aligned and unaligned classes. |
| **Architecture** | **ShareLock (Shared Vision-Language-Locked Tuning):** Employs a lightweight late-fusion design. |
| **Backbones** | Frozen vision and language backbones. The system trains only a learnable projection network. |
| **Mapping Mechanism** | The projection network maps text embeddings into the vision encoder's latent space. |
| **Optimization** | Uses cosine similarity and a contrastive loss function. |
| **Inference** | Performed by maximizing similarity between image embeddings and projected text embeddings of class descriptions. |

---

## Results

*   **Correlation Analysis:** A strong positive correlation (Pearson coefficient of 0.768) was found between language understanding (MMLU-Pro scores) and visual generalization scores. Decoder-based LLMs consistently outperformed encoder-based models.
*   **Efficiency Metrics:** The ShareLock method is highly efficient, achieving 51% top-1 accuracy on ImageNet using only 563k image-caption pairs and less than one GPU-hour.
*   **Cross-lingual Transfer:** Demonstrated superior cross-lingual generalization, achieving 38.7% top-1 accuracy on Chinese image classification (vs. CLIP's 1.4%).
*   **Compositional Reasoning:** Outperformed CLIP in tasks requiring compositional reasoning.

---

## Contributions

*   **Empirical Analysis:** Provides a comprehensive empirical analysis linking LLM architecture and performance to visual alignment.
*   **Methodology:** Introduces ShareLock, a parameter-efficient training strategy that fuses frozen vision and language backbones.
*   **Resource Efficiency:** Demonstrates that state-of-the-art vision-language performance can be achieved with a fraction of the data and compute required by previous methods.
*   **Validation:** Validates the method's efficacy in cross-lingual transfer, addressing limitations of existing models like CLIP.

---

**Quality Score:** 8/10 | **References:** 25 citations