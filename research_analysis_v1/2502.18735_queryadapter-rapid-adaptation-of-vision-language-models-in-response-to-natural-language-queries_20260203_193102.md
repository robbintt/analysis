---
title: 'QueryAdapter: Rapid Adaptation of Vision-Language Models in Response to Natural
  Language Queries'
arxiv_id: '2502.18735'
source_url: https://arxiv.org/abs/2502.18735
generated_at: '2026-02-03T19:31:02'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# QueryAdapter: Rapid Adaptation of Vision-Language Models in Response to Natural Language Queries

*Nicolas Harvey Chapman; Feras Dayoub; Will Browne; Christopher Lehnert*

---

> ### ðŸ“‹ Quick Facts
> *   **Primary Dataset:** ScanNet++, Ego4D
> *   **Adaptation Time:** ~3 Minutes
> *   **Performance (ScanNet++):** 64.8% R@1, 35.6% mAP
> *   **Method Type:** Unsupervised Query-Based Adaptation
> *   **Key Innovation:** Active Object Selection & Learnable Prompt Tokens
> *   **Citations:** 40
> *   **Quality Score:** 9/10

---

## Executive Summary

This research addresses the critical challenge of deploying Vision-Language Models (VLMs) in robotic systems, where a significant domain shift exists between the internet-scale data used for pre-training and the raw, noisy visual streams encountered in real-world environments. While VLMs possess strong open-vocabulary capabilities, their performance degrades when applied directly to robotic platforms without task-specific adaptation. Furthermore, the need for robots to interpret diverse, open-ended natural language queries makes standard closed-set classification pipelines impractical. Bridging this gap is essential for enabling robots to reliably interpret user instructions and interact with dynamic environments without relying on computationally expensive retraining pipelines or manual annotation.

The authors introduce **QueryAdapter**, an unsupervised query-based adaptation framework designed to fine-tune VLMs using only unlabeled images from previous robot deployments. Technically, the method optimizes learnable prompt tokens rather than the entire model architecture, allowing for efficient parameter updates. A key technical innovation is the **active object selection mechanism**, which accelerates training by intelligently prioritizing the most relevant objects within the data stream for optimization, rather than processing the entire dataset indiscriminately. Additionally, the framework employs a novel negative class labeling strategy that utilizes object captions to explicitly suppress unrelated objects, thereby managing the noise inherent in real-world robotic data and preventing the model from becoming distracted by irrelevant background features.

Evaluated on the **ScanNet++ dataset**, QueryAdapter achieved a Recall@1 (R@1) of **64.8%** and a mean Average Precision (mAP) of **35.6%**, significantly outperforming the standard CLIP baseline (47.2% R@1). Crucially, the method surpassed both state-of-the-art unsupervised VLM adapters and established 3D scene graph methods in object retrieval tasks. The framework demonstrated exceptional efficiency, achieving full model convergence in approximately **3 minutes**. Beyond standard object retrieval, the method showed robust generalization capabilities by successfully processing complex abstract affordance queries on the Ego4D dataset. Furthermore, the use of caption-based negative labels resulted in significantly better-calibrated confidence scores, improving the model's ability to distinguish between target objects and distractors.

QueryAdapter represents a significant advancement in open-vocabulary robotics by removing the reliance on static closed-set definitions and computationally heavy retraining pipelines. By providing a solution that adapts to specific user queries in near real-time and outperforms existing 3D scene graph approaches, this work enhances the practical viability of deploying VLMs on resource-constrained robotic platforms. The ability to handle abstract queries and noisy real-world data suggests that this framework will influence future research into robust, human-centric robot perception systems that require rapid adaptation to new tasks and environments.

---

## Key Findings

*   **Significant Performance Improvement:** QueryAdapter outperforms state-of-the-art unsupervised VLM adapters and 3D scene graph methods in object retrieval tasks on the ScanNet++ dataset.
*   **Rapid Adaptation:** The framework is capable of producing an adapted model in minutes by optimizing learnable prompt tokens and actively selecting objects for training.
*   **Robust Generalization:** The approach generalizes effectively to abstract affordance queries and the Ego4D dataset.
*   **Improved Calibration:** Utilizing object captions as negative class labels helps manage unrelated objects, resulting in better-calibrated confidence scores.

---

## Methodology

The framework introduces a **Query-Based Adaptation** pipeline for adapting pre-trained Vision-Language Models (VLMs) to natural language queries without a predefined closed-set of classes. It leverages unlabelled image data from previous robot deployments to align VLM features with semantic classes relevant to the user's query.

The core methodological components include:
1.  **Learnable Prompt Optimization:** Optimizing prompt tokens rather than the full model for efficient updates.
2.  **Active Object Selection:** An intelligent mechanism that selects specific objects for training to speed up convergence.
3.  **Negative Class Labeling:** Using object captions as negative labels to address the challenge of unrelated objects in real-world data.

---

## Technical Details

QueryAdapter is an unsupervised adaptation framework for Vision-Language Models (VLMs) that enables rapid tuning via natural language queries without manual annotation.

*   **Optimization Strategy:** Utilizes learnable prompt tokens for efficient parameter updates.
*   **Training Efficiency:** Employs active object selection to optimize training speed and reduce computational load.
*   **Noise Management:** The framework employs a negative class labeling strategy, using object captions to suppress unrelated objects.
*   **Query Complexity:** Capable of processing complex abstract affordance queries (e.g., "something to sit on").

---

## Contributions

*   **Overcoming Closed-Set Limitations:** Addresses the impracticality of defining closed-sets for robotics by enabling VLMs to adapt to diverse, open-ended natural language queries.
*   **Bridging the Domain Shift:** Provides a solution to the domain shift between internet-scale training data and the raw image streams encountered by robots in the real world.
*   **Real-World Data Handling:** Contributes a technique for dealing with noise in real-world data through the use of caption-based negative labels.
*   **Efficiency in Robotics:** Establishes a rapid adaptation framework suitable for robotic applications where quick response times are critical.

---

## Results

The evaluation of QueryAdapter demonstrates its effectiveness across several key metrics:

*   **ScanNet++ Dataset:**
    *   Outperformed State-of-the-Art (SOTA) unsupervised VLM adapters and 3D scene graph methods.
    *   Achieved **64.8% Recall@1 (R@1)** and **35.6% mean Average Precision (mAP)**.
    *   Surpassed the standard CLIP baseline which achieved 47.2% R@1.
*   **Efficiency:**
    *   Achieved rapid convergence with adaptation times of approximately **3 minutes**.
*   **Generalization:**
    *   Demonstrated robust performance on the **Ego4D dataset**, specifically handling abstract affordance queries.
*   **Calibration:**
    *   Achieved improved calibration of confidence scores by using object captions as negative labels to distinguish relevant from distractor objects.

---

**Quality Score:** 9/10
**References:** 40 citations