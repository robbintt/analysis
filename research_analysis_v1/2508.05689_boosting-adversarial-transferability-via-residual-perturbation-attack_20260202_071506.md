# Boosting Adversarial Transferability via Residual Perturbation Attack
*Jinjia Peng; Zeze Tao; Huibing Wang; Meng Wang; Yang Wang*

---

> ### ðŸ“Š Quick Facts
> * **Quality Score:** 8/10
> * **Reference Count:** 40 Citations
> * **Dataset:** ImageNet
> * **Primary Surrogate Model:** Inception-v3
> * **Avg Success Rate (Normal Models):** 57.9%
> * **Avg Success Rate (Defended Models):** 53.5%
> * **Hybrid Performance (w/ DI & TI):** 73.2%

---

## Executive Summary

Transfer-based adversarial attacks are critical for evaluating the security of deep learning models in black-box settings, where attackers cannot access internal parameters. The fundamental challenge in this domain is limited transferability; adversarial examples crafted on a surrogate model frequently fail to deceive unseen target models. The authors identify that existing approaches, while successfully seeking "flat" minima to generalize loss landscapes, often neglect the crucial dynamic influence of perturbation direction. This oversight causes optimization updates to oscillate or converge to sharp, local optima, resulting in adversarial examples that overfit to the surrogateâ€™s specific loss surface and exhibit brittleness when transferred to other architectures.

To address trajectory instability, the authors propose the **Residual Perturbation Attack (ResPA)**, a novel framework that explicitly manages the evolution of update directions. Technically, ResPA employs an Exponential Moving Average (EMA) to maintain a historical reference of input gradients, providing a baseline for the optimization path. Instead of relying solely on the instantaneous gradient, the method calculates the "residual"â€”the difference between the current gradient and this historical reference. By utilizing this residual gradient to drive updates, ResPA captures changes in the global perturbation direction, effectively smoothing optimization trajectories. This mechanism steers perturbations toward flatter regions of the loss surface without relying on formal theoretical proofs, instead providing an empirically grounded enhancement to the update step.

The researchers validated ResPA on the ImageNet dataset using Inception-v3 as the surrogate model. Against normally trained models, ResPA achieved an average attack success rate of **57.9%**, significantly outperforming the baseline MI-FGSM (37.4%) and state-of-the-art VMI-FGSM (49.6%). Crucially, against adversarially trained and defense-based models, ResPA demonstrated superior robustness, achieving an average success rate of **53.5%** compared to VMI-FGSMâ€™s 41.4% and MI-FGSMâ€™s 25.6%. Furthermore, when integrated with input transformation techniques, the hybrid ResPA approach boosted the average success rate to **73.2%**.

This research significantly advances adversarial machine learning by shifting the analytical focus from static geometric properties (loss landscape flatness) to dynamic direction management. By validating that the residual between current and historical gradients contains vital information for controlling the update trajectory, the authors establish a practical, "plug-and-play" enhancement for existing attack pipelines.

---

## Key Findings

*   **Perturbation Direction Influence:** Prior transfer-based attack methods suffer from limited transferability because they overlook the influence of perturbation directions.
*   **Flat Landscapes & Transferability:** Adversarial examples located in flat loss landscapes exhibit superior transferability by alleviating overfitting to surrogate models.
*   **Residual Gradient Efficacy:** Utilizing the residual between current gradients and historical reference gradients captures changes in global perturbation direction more effectively than relying solely on current gradients.
*   **Performance Superiority:** The proposed ResPA method outperforms existing typical transfer-based attack methods in transferability.
*   **Compatibility:** The effectiveness of ResPA can be further enhanced when combined with current input transformation methods.

---

## Methodology

The researchers propose the **Residual Perturbation Attack (ResPA)**, a method designed to guide adversarial examples toward flat regions of the loss function. The approach consists of three core components:

1.  **Reference Gradient Calculation:**
    *   Utilizes Exponential Moving Average (EMA) on input gradients to obtain a historical reference gradient.
2.  **Residual Gradient Utilization:**
    *   Calculates the difference between current gradients and the historical reference gradients.
3.  **Direction Guidance:**
    *   Uses the residual gradient to capture global perturbation direction changes and reduce overfitting to the surrogate model.

---

## Technical Details

*   **Core Hypothesis:** Adversarial examples in flat loss landscapes exhibit superior transferability by avoiding overfitting to surrogate models.
*   **Update Mechanism:** The method calculates the update direction using the residual between current gradients and historical reference gradients to better capture changes in the global perturbation direction.
*   **Architecture Compatibility:** ResPA is designed to be compatible with input transformation methods such as Input Diversity (DI) or image Randomization.

---

## Contributions

*   **Problem Identification:** Identified the limitation that prior arts focusing on flat loss landscapes neglected the critical role of perturbation directions.
*   **Methodological Innovation:** Introduced ResPA, a new attack methodology that effectively captures global perturbation direction changes by leveraging the residual between current and historical (EMA-based) gradients.
*   **Validation:** Demonstrated performance validation showing that ResPA achieves better transferability than state-of-the-art methods and is compatible with existing input transformation techniques.

---

## Results

ResPA demonstrates robust performance across various model architectures and defense mechanisms:

*   **Vs. Normally Trained Models:**
    *   Compared against Inception-v4, ResNet-101, and VGG-16.
    *   **ResPA:** 57.9% Average Success Rate
    *   **VMI-FGSM:** 49.6%
    *   **MI-FGSM:** 37.4%
*   **Vs. Defended Models:**
    *   Compared against ensemble and adversarially trained models (Inc-v3\_{ens3}, Inc-v3\_{ens4}, Adv-Inc-v3).
    *   **ResPA:** 53.5% Average Success Rate
    *   **VMI-FGSM:** 41.4%
    *   **MI-FGSM:** 25.6%
*   **Hybrid Approach (with Input Transformations):**
    *   When combined with Input Diversity and other transformations.
    *   **ResPA Hybrid:** 73.2%
    *   **Standard DI-TI-MI:** 62.7%