---
title: 'MASSV: Multimodal Adaptation and Self-Data Distillation for Speculative Decoding
  of Vision-Language Models'
arxiv_id: '2505.10526'
source_url: https://arxiv.org/abs/2505.10526
generated_at: '2026-02-03T18:23:38'
quality_score: 9
citation_count: 13
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# MASSV: Multimodal Adaptation and Self-Data Distillation for Speculative Decoding of Vision-Language Models

*Authors: Mugilan Ganesan; Shane Segal; Ankur Aggarwal; Nish Sinnadurai; Sean Lie; Vithursan Thangarasa*

---

> ### ðŸ“Š Quick Facts
>
> *   **Max Inference Speedup:** 1.46x
> *   **Token Acceptance Improvement:** +30%
> *   **Validated Architectures:** Qwen2.5-VL, Gemma3
> *   **Quality Score:** 9/10
> *   **Citations:** 13

---

## Executive Summary

### **Problem**
Vision-Language Models (VLMs) provide state-of-the-art multimodal capabilities but suffer from high latency and computational costs during inference. While speculative decoding has successfully accelerated text-only Large Language Models (LLMs) by using smaller "drafter" models to predict tokens verified by a larger model, this technique is difficult to transfer to the multimodal domain. The core challenge lies in the architectural mismatch: standard small language models lack the visual processing components to interpret image data, and their token distributions are often misaligned with VLMs. This paper addresses the critical need to adapt speculative decoding for VLMs to reduce inference overhead without sacrificing the performance or architectural integrity of the target model.

### **Innovation**
The authors introduce **MASSV** (Multimodal Adaptation and Self-Data Distillation), a framework that converts existing Small Language Models (SLMs) into effective multimodal drafters. The technical solution employs a two-phase methodology. First, **Multimodal Adaptation** utilizes a lightweight, trainable Visual Adapter to bridge the target VLM's frozen vision encoder with the SLM, enabling the smaller model to process visual inputs. Second, **Self-Data Distillation** aligns the drafter's predictions with the target model by employing synthetic data generated by the VLM itself. Through visual instruction tuning on this distilled dataset, the SLM learns to mimic the exact token distribution of the larger VLM, thereby maximizing the likelihood of token acceptance during the verification process.

### **Results**
MASSV demonstrates broad effectiveness across different model architectures, specifically validating the approach on Qwen2.5-VL and Gemma3 families. In experimental trials, the framework achieved an end-to-end inference speedup of up to **1.46x** on visually-grounded tasks. Crucially, the distillation process resulted in a **30%** improvement in the accepted length of generated tokens, indicating highly accurate drafter predictions. These results confirm that MASSV can successfully repurpose existing pre-trained SLMs into high-performance drafters without the need to train models from scratch, maintaining the generation quality of the original VLM while significantly reducing latency.

### **Impact**
This research represents a significant advancement in the efficiency of deploying generative AI in multimodal contexts. By solving the architectural and alignment limitations of VLM speculative decoding, MASSV provides a scalable path to reducing computational costs and latency for real-time applications. The framework's ability to operate without modifying the target model's core architecture makes it a highly practical solution for practitioners seeking to optimize current VLM deployments. Ultimately, MASSV establishes a new paradigm for combining lightweight adapters with data distillation to accelerate complex multimodal systems.

---

## Key Findings

*   **Inference Acceleration:** MASSV achieves end-to-end inference speedups of up to **1.46x** on visually-grounded tasks.
*   **Increased Acceptance Rates:** The method improves the accepted length of generated tokens by up to **30%**.
*   **Broad Validation:** Effectiveness demonstrated across Qwen2.5-VL and Gemma3 model families.
*   **Effective Drafter Transformation:** Successfully converts existing small language models into effective multimodal drafters.

---

## Methodology

MASSV utilizes a two-phase approach to enable speculative decoding for Vision-Language Models:

1.  **Multimodal Adaptation:** Connects the target VLM's vision encoder to the draft model using a lightweight trainable projector. This allows the small language model to receive and process visual input data.
2.  **Self-Data Distillation:** Involves visual instruction tuning of the draft model using responses generated by the target VLM. This process aligns the drafter's token predictions with the target model's behavior to ensure high verification acceptance rates.

---

## Technical Details

| Component | Description |
| :--- | :--- |
| **Core Architecture** | Applies Speculative Decoding to VLMs using a large pre-trained VLM as the **Target Model (Verifier)** and a Small Language Model (SLM) converted into a **Multimodal Drafter**. |
| **Visual Processing** | Employs a lightweight **Visual Adapter** to connect a frozen vision encoder to the SLM, allowing efficient image processing without training the vision encoder from scratch. |
| **Representation Alignment** | **Multimodal Adaptation** aligns visual and textual representation spaces, ensuring the drafter can interpret the context correctly. |
| **Data Generation** | **Self-Data Distillation** utilizes the target model to generate synthetic training data. The drafter is trained on this data to mimic the target's exact token distribution. |

---

## Contributions

*   **Solving VLM Speculative Decoding Limitations:** Addresses the lack of visual processing architecture in small language models and prediction misalignment found in previous methods.
*   **Scalable Framework:** Introduces a scalable method that accelerates VLMs without modifying the target model's core architecture.
*   **Efficiency Optimization:** Reduces computational cost and latency of VLM inference while maintaining performance standards.

---

## Results

*   Achieved an end-to-end inference speedup of up to **1.46x**.
*   Improved the accepted length of generated tokens by up to **30%**.
*   Demonstrated generalization across different model architectures, specifically validated on **Qwen2.5-VL** and **Gemma3**.
*   Successfully converted existing Small Language Models into effective drafters without the need to train models from scratch.

---
**Report generated based on analysis.**