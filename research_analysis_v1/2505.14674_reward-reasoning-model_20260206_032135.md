---
title: Reward Reasoning Model
arxiv_id: '2505.14674'
source_url: https://arxiv.org/abs/2505.14674
generated_at: '2026-02-06T03:21:35'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Reward Reasoning Model

*Jiaxin Guo; Zewen Chi; Li Dong; Qingxiu Dong; Xun Wu; Shaohan Huang; Furu Wei*

---

### ðŸ“Š Quick Facts

| **Metric** | **Detail** |
| :--- | :--- |
| **Quality Score** | 9/10 |
| **References** | 40 Citations |
| **Core Architecture** | Transformer-decoder (Qwen2) |
| **Optimization** | Group Relative Policy Optimization (GRPO) |
| **Key Achievement** | +14.1 pp accuracy gain on GPQA |
| **Training Method** | Self-evolved Reinforcement Learning |

---

## Executive Summary

Traditional reward models typically function as static classifiers, assigning scores to outputs without internal deliberation. This architecture presents a significant limitation: it fails to effectively utilize test-time compute, particularly for complex queries where the appropriate reward is ambiguous or requires deep understanding. Consequently, existing models struggle to adaptively scale processing power based on query difficulty, leading to sub-optimal performance in nuanced evaluation scenarios. This paper addresses the open challenge of moving beyond static reward assignment to enable dynamic, reasoning-based assessment.

The authors introduce **Reward Reasoning Models (RRMs)**, a novel paradigm that frames reward modeling as a text completion task requiring a Chain-of-Thought (CoT) process before outputting a final reward score. Technically, the system utilizes a Transformer-decoder architecture (Qwen2) to generate a reasoning trace followed by a LaTeX judgment. The core innovation lies in the training methodology: a self-evolved Reinforcement Learning (RL) framework using Group Relative Policy Optimization (GRPO). This approach is data-efficient because it does not require manually annotated reasoning traces; instead, it relies on rule-based reward signals (+1/-1) derived from outcomes like ELO Ratings or Knockout Tournaments, allowing the model to autonomously develop reasoning capabilities.

Empirical validation demonstrates that RRMs achieve state-of-the-art or superior performance on major benchmarks including **MMLU-Pro**, **MATH**, and **GPQA**. In a significant experiment on unlabeled GPQA data, the application of RRM via reinforcement learning increased accuracy from **26.8%** (post-trained baseline) to **40.9%**, marking a substantial 14.1 percentage point gain. Furthermore, the model exhibits adaptive compute utilization, successfully allocating longer reasoning traces specifically to difficult queries, thereby correlating computational expenditure with problem complexity to maximize reward accuracy.

---

## Key Findings

*   **Superior Benchmark Performance:** Reward Reasoning Models (RRMs) achieve state-of-the-art or superior results on reward modeling benchmarks across diverse domains.
*   **Effective Complex Query Handling:** By implementing chain-of-thought reasoning, RRMs can successfully navigate complex queries where the appropriate reward is not immediately obvious.
*   **Adaptive Compute Utilization:** RRMs can adaptively exploit test-time compute, allowing them to allocate more processing power to difficult queries to further improve reward accuracy.
*   **Self-Evolved Training:** The models are capable of developing reasoning capabilities without the need for explicit reasoning traces in the training data, relying instead on a self-evolved process.

---

## Methodology

The authors introduce Reward Reasoning Models (RRMs), designed to perform a deliberate reasoning process before outputting a final reward score. The core methodology involves:

1.  **Chain-of-Thought (CoT) Reasoning:** Utilizing CoT to leverage additional test-time compute for complex assessments.
2.  **Reinforcement Learning Framework:** Implementing an RL framework specifically designed to foster self-evolved reward reasoning capabilities.
3.  **Data-Efficient Training:** Employing a training approach that does not require explicit reasoning traces as labeled training data, relying instead on an RL feedback loop.

---

## Core Contributions

*   **Solving the Test-Time Compute Challenge:** Addresses the open challenge of effectively utilizing test-time compute to enhance reward model performance, moving beyond static reward assignment.
*   **Architectural Innovation:** Proposes a new paradigm for reward modeling where the model "reasons" before "rewarding," enabling better handling of nuanced or subjective evaluation criteria.
*   **Training Methodology Contribution:** Contributes a novel reinforcement learning training strategy that removes the dependency on expensive, manually annotated reasoning chains, enabling the model to evolve these capabilities autonomously.
*   **Empirical Validation:** Provides experimental evidence that reasoning-based reward models can adaptively scale their computation usage to maximize accuracy.

---

## Technical Architecture & Implementation

| **Component** | **Description** |
| :--- | :--- |
| **Architecture** | Utilizes a Transformer-decoder architecture (`Qwen2`) to frame reward modeling as text completion. |
| **Output Format** | Generates a Chain-of-Thought followed by a LaTeX judgment. |
| **Input Structure** | Consists of a query and two candidate responses. |
| **Initialization** | Initialized from `DeepSeek-R1` distilled models. |
| **Optimization** | Optimized via `Group Relative Policy Optimization (GRPO)`. |
| **Training Signal** | Uses a self-evolved methodology with a rule-based reward signal (+1/-1) without requiring explicit reasoning traces. |
| **Multi-Response Strategies** | Employs `ELO Rating`, `Knockout Tournament`, or `Majority Voting` strategies for multi-response scenarios. |

---

## Performance Results

The proposed model demonstrates significant empirical improvements across standard benchmarks:

*   **Benchmark Dominance:** Achieves state-of-the-art or superior performance on **MMLU-Pro**, **MATH**, and **GPQA** benchmarks.
*   **Significant Accuracy Gain:** In reinforcement learning experiments on unlabeled GPQA data, accuracy improved from **26.8%** (post-trained) to **40.9%** using the RRM. This represents a substantial **+14.1 percentage point gain**.
*   **Adaptive Compute:** The model adaptively utilizes test-time compute by allocating longer reasoning traces to complex queries to improve accuracy.

---

## References

*   **Total Citations:** 40