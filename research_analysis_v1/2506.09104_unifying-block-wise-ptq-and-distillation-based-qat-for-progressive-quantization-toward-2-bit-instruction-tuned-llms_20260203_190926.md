---
title: Unifying Block-wise PTQ and Distillation-based QAT for Progressive Quantization
  toward 2-bit Instruction-Tuned LLMs
arxiv_id: '2506.09104'
source_url: https://arxiv.org/abs/2506.09104
generated_at: '2026-02-03T19:09:26'
quality_score: 8
citation_count: 34
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Unifying Block-wise PTQ and Distillation-based QAT for Progressive Quantization toward 2-bit Instruction-Tuned LLMs

*Jung Hyun Lee; Seungjae Shin; Vinnam Kim; Jaeseong You; An Chen*

---

> ### ðŸ“Š Quick Facts
> *   **Model:** Llama 3.2 3B Instruct
> *   **Target Precision:** 2-bit (INT2)
> *   **Training Data:** 5 Billion open-source tokens
> *   **Key Metrics:**
>     *   **IFEval Score:** 27.12% (vs 19.97% baseline)
>     *   **MMLU Baseline:** 59.92% (FP16)
> *   **Method:** Unified Progressive Quantization (UPQ)
> *   **Quality Score:** 8/10

---

## Executive Summary

**Problem**
Deploying Large Language Models (LLMs) on resource-constrained hardware requires extreme quantization, yet compressing instruction-tuned models to 2-bit (INT2) precision presents a significant challenge. Existing methods often suffer from severe accuracy degradation or rely on proprietary post-training datasets for calibration, creating a bottleneck for practical deployment. This research addresses the critical need to maintain the high utility of instruction-tuned LLMs while drastically reducing bit-width, specifically aiming to bridge the gap between theoretical compression and viable, data-efficient performance in real-world applications.

**Innovation**
The authors introduce **Unified Progressive Quantization (UPQ)**, the first framework to unify block-wise Post-Training Quantization (PTQ) with Distillation-based Quantization-Aware Training (Distill-QAT). UPQ employs a two-stage pipeline:
*   **Stage 1** utilizes block-wise PTQ to convert the model from FP16 to INT4, minimizing error accumulation without training data.
*   **Stage 2** transitions from INT4 to INT2 using Distill-QAT, initializing from the INT4 checkpoint rather than the original FP16 model. This stage minimizes the generalized Jensen-Shannon divergence (JSD) between the student and teacher distributions and employs Stretched Elastic Quant (SEQ), which quantizes weights to discrete values ($\frac{\Delta}{4} \times \{-3, -1, 1, 3\}$) using learnable scale factors to ensure balanced bin allocation.

**Results**
Evaluations on Llama 3.2 3B Instruct, utilizing 5 billion open-source tokens, demonstrate that UPQ outperforms baseline methods such as NTP-QAT and PTQ-to-NTP-QAT pipelines. On the IFEval benchmark, UPQâ€™s Distill-QAT achieved a score of **27.12%**, significantly surpassing the NTP-QAT baseline of **19.97%** (referenced against an FP16 baseline of 57.80%). Additionally, the method achieved Pareto-optimality in accuracy and latency, demonstrating faster convergence and lower training loss than baselines while generating responses with high consistency to the original FP16 model.

**Impact**
This research validates the viability of extremely low-bit quantization for instruction-tuned models, proving that high performance can be achieved without access to proprietary datasets. By unifying PTQ and QAT into a cohesive framework, UPQ provides a practical solution for deploying large models on edge devices with strict memory and latency constraints. The findings reinforce the industry shift toward larger, aggressively quantized models over smaller, higher-bit ones, offering a superior trade-off that facilitates more efficient and accessible AI inference.

---

## Key Findings

*   **Extension to Instruction-Tuned Models:** The research successfully extends the capabilities of 2-bit (INT2) quantization to instruction-tuned LLMs.
*   **Data Efficiency:** The proposed method achieves high-performance quantization without relying on proprietary post-training data.
*   **State-of-the-art Performance:** The framework achieves state-of-the-art results on MMLU and IFEval benchmarks.
*   **Pareto-Optimality Confirmation:** The study reinforces that 2-bit large models offer superior trade-offs in both accuracy and latency compared to 4-bit smaller counterparts.
*   **Response Consistency:** The final INT2 models are capable of generating responses that are highly consistent with their original FP16 counterparts.

---

## Methodology

The authors propose **Unified Progressive Quantization (UPQ)**, a framework utilizing a progressive quantization strategy moving from FP16 $\rightarrow$ INT4 $\rightarrow$ INT2.

1.  **Stage 1: Block-wise Post-Training Quantization (PTQ)**
    *   Converts the FP16 model to INT4.
    *   Focuses on minimizing error accumulation during the initial compression phase without requiring training data.

2.  **Stage 2: Distillation-based Quantization-Aware Training (Distill-QAT)**
    *   Transitions the model from INT4 to INT2.
    *   Minimizes the generalized Jensen-Shannon divergence (JSD) between the distributions of the quantized student model and the original FP16 teacher model.
    *   Initializes training from the INT4 PTQ checkpoint rather than FP16 to stabilize the learning process.

---

## Technical Details

**Framework Architecture**
*   **UPQ (Unified Progressive Quantization):** A two-stage pipeline combining PTQ and QAT.
    *   *Stage 1:* FP16 $\rightarrow$ INT4 via Block-wise PTQ.
    *   *Stage 2:* INT4 $\rightarrow$ INT2 via Distill-QAT.

**Quantization Scheme**
*   **Stretched Elastic Quant (SEQ):** Employed in the INT2 scheme to ensure balanced bin allocation.
*   **Weight Quantization:** Weights are quantized to discrete values using a learnable scale factor:
    $$ \frac{\Delta}{4} \times \{-3, -1, 1, 3\} $$

**Training Strategy**
*   **Initialization:** The INT2 QAT process initializes from the INT4 PTQ checkpoint instead of starting from FP16.
*   **Knowledge Distillation (KD):** Utilizes a full-precision teacher model to guide the student model during the quantization-aware training phase.

---

## Results

Experiments were conducted on **Llama 3.2 3B Instruct** using 5 billion open-source tokens. UPQ demonstrated significant improvements over NTP-QAT and INT4 PTQ $\rightarrow$ NTP-QAT baselines.

### Benchmark Performance (IFEval)

| Method | Score | Comparison to Baseline |
| :--- | :--- | :--- |
| **FP16 Baseline** | 57.80% | Reference |
| **NTP-QAT** | 19.97% | Baseline |
| **Distill-QAT (UPQ)** | **27.12%** | **+7.15%** |

### Key Outcomes
*   **Pareto-Optimality:** Achieved optimal balance between accuracy and latency.
*   **Training Efficiency:** Demonstrated faster convergence and lower training loss compared to baselines.
*   **Weight Distribution:** Produced weight distributions better aligned with the 2-bit grid compared to FP16 initialization.
*   **MMLU Baseline:** 59.92% (FP16).

---

## Contributions

*   **Novel Unified Framework:** Introduction of UPQ, the first framework to unify block-wise PTQ and Distill-QAT specifically for the progressive quantization of instruction-tuned LLMs.
*   **Bridging the Deployment Gap:** Addressed the critical deployment bottleneck by enabling extremely low-bit (2-bit) quantization for instruction-tuned models.
*   **Validation of Low-bit Utility:** Demonstrated that instruction-tuned models can maintain high performance at 2-bit precision without access to proprietary datasets.

---
**References:** 34 citations | **Quality Score:** 8/10