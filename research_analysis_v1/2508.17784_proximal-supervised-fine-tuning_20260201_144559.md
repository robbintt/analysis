# Proximal Supervised Fine-Tuning
*Wenhong Zhu; Ruobing Xie; Rui Wang; Xingwu Sun; Di Wang; Pengfei Liu*

---

> ### üìä Quick Facts
> *   **Quality Score:** 7/10
> *   **Total Citations:** 12
> *   **Primary Domain:** Large Language Model Post-Training
> *   **Core Innovation:** Adaptation of PPO trust-region mechanisms to Supervised Fine-Tuning
> *   **Key Benefit:** Mitigates entropy collapse and catastrophic forgetting

---

## üìù Executive Summary

Standard Supervised Fine-Tuning (SFT) of Large Language Models (LLMs) suffers from inherent optimization instability, characterized by "policy drift," "catastrophic forgetting," and "entropy collapse." During prolonged fine-tuning on specific downstream tasks, models often deviate significantly from their initialized state, resulting in a rapid degradation of generalization capabilities and a reduction in the diversity of model outputs. This sensitivity to training duration creates a critical bottleneck, where models trained too long lose pre-trained skills, while failing to train sufficiently limits task adaptation.

The authors introduce **Proximal Supervised Fine-Tuning (PSFT)**, a novel methodology that re-frames SFT mathematically as an instance of Reinforcement Learning policy gradient methods where the advantage function is constant. Drawing on the theoretical framework of Proximal Policy Optimization (PPO), PSFT adapts the clipped surrogate objective for supervised learning. The technique implements a "soft trust-region" by constraining the importance sampling ratio within the interval $[1 - \epsilon, 1 + \epsilon]$ (with $\epsilon$ typically set to 0.2).

This mechanism utilizes a dynamic reference policy $\pi_{\theta_{old}}$ that evolves during training, dynamically limiting policy updates to prevent significant deviation from the model's original behavior while still enabling effective task adaptation. Evaluations on academic benchmarks‚Äîincluding GSM8K and MATH for mathematical reasoning and HH-RLHF for alignment‚Äîdemonstrate that PSFT matches standard SFT on in-domain tasks while significantly outperforming it on out-of-distribution (OOD) generalization.

Where standard SFT often suffers dramatic accuracy drops‚Äîsometimes declining by double digits after only a few epochs‚ÄîPSFT maintains stable performance metrics over extended training durations. The method successfully prevents entropy collapse, preserving high output diversity (entropy) and ensuring that accuracy on target tasks remains robust even after aggressive optimization periods. This work provides a crucial theoretical derivation linking supervised learning to RL policy gradient methods, establishing a mathematical foundation for applying trust-region optimization techniques to SFT. By successfully addressing capability deterioration and catastrophic forgetting, PSFT offers a more reliable intermediate stage for LLM post-training pipelines.

---

## üîë Key Findings

*   **Generalization Performance:** PSFT matches the performance of standard Supervised Fine-Tuning (SFT) on in-domain tasks while significantly outperforming it on out-of-domain generalization.
*   **Training Stability:** The method remains stable during prolonged training periods, effectively avoiding the issue of entropy collapse often associated with standard fine-tuning.
*   **Capability Retention:** By constraining policy drift, PSFT effectively maintains the model's prior capabilities and provides a stronger foundation for subsequent post-training optimization stages.
*   **Objective Balance:** The fine-tuning objective successfully balances the need for competitive tuning on new tasks with the requirement to retain the foundation model's original behavior.

---

## üî¨ Methodology

The authors conceptualize Supervised Fine-Tuning (SFT) as a specific instance of policy gradient methods where advantages are treated as constant positive values. Drawing inspiration from Reinforcement Learning algorithms, specifically **Trust-Region Policy Optimization (TRPO)** and **Proximal Policy Optimization (PPO)**, the authors propose Proximal SFT (PSFT).

PSFT incorporates a 'trust-region' mechanism into the fine-tuning objective, which acts as a constraint to limit policy drift and ensure the model does not deviate too drastically from its initial state.

---

## üìö Contributions

1.  **Theoretical Foundation:** Provides a novel theoretical derivation linking Supervised Fine-Tuning to Reinforcement Learning policy gradient methods, establishing a mathematical foundation for applying RL optimization techniques (like trust regions) to SFT.
2.  **Solving Forgetting:** Addresses the critical problem of catastrophic forgetting and capability deterioration in foundation models by introducing a method that constrains the optimization trajectory.
3.  **Robust Training Strategy:** Demonstrates a training strategy that avoids common pitfalls like entropy collapse, thereby offering a more robust and stable approach for the intermediate stages of large language model post-training pipelines.

---

## ‚öôÔ∏è Technical Details

| Component | Description |
| :--- | :--- |
| **MDP Formulation** | The approach formulates language modeling as a Markov Decision Process (MDP) where **states** are partial sequences and **actions** are next tokens. |
| **SFT Derivation** | It establishes Supervised Fine-Tuning (SFT) as a special case of **Policy Gradient (PG)** where the advantage function is fixed to **1**. |
| **Objective Function** | PSFT adapts the PPO clipped surrogate objective to supervised learning. It employs a loss function based on the importance sampling ratio clipped between **$1 - \epsilon$** and **$1 + \epsilon$**. |
| **Trust Region** | This clipping creates a 'soft trust region' to prevent policy drift and preserve entropy. |
| **Reference Policy** | The method utilizes a **dynamic reference policy** ($\pi_{\theta_{old}}$) that evolves during training rather than using a frozen initialization. |
| **Training Protocol** | Recommends an initial standard SFT 'warm-up' phase before applying PSFT. |

---

## üìà Results

*   **In-Domain vs. OOD:** PSFT achieves performance comparable to standard SFT on in-domain tasks but significantly outperforms SFT on **out-of-distribution (OOD)** generalization.
*   **Stability & Entropy:** It effectively mitigates "entropy collapse," maintains stability during extended training, and preserves the original capabilities of the foundation model.
*   **Downstream Optimization:** PSFT serves as a superior "cold start" for subsequent Reinforcement Learning (RL) fine-tuning stages (such as PPO and GRPO), yielding better target and generalization performance.
*   **Evaluation Domains:** The method was evaluated in the domains of **Mathematics (GSM8K, MATH)** and **Human Value Alignment (HH-RLHF)**.