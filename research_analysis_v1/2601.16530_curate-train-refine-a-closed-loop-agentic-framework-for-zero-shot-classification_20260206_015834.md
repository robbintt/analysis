---
title: 'Curate-Train-Refine: A Closed-Loop Agentic Framework for Zero Shot Classification'
arxiv_id: '2601.16530'
source_url: https://arxiv.org/abs/2601.16530
generated_at: '2026-02-06T01:58:34'
quality_score: 9
citation_count: 8
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Curate-Train-Refine: A Closed-Loop Agentic Framework for Zero Shot Classification

*Gaurav Maheshwari; Kevin El Haddad*

> ### **Quick Facts**
> * **üìä Quality Score:** 9/10
> * **üìö Citations:** 8
> * **üõ†Ô∏è Framework Type:** Closed-Loop Agentic System
> * **ü§ñ Primary LLM:** GPT-5
> * **‚ö° Classifier:** SetFit (Sentence Transformers)
> * **üèÜ Top Gain:** AG News 2-shot (+17.9% over baseline)

---

## Executive Summary

This research addresses the inherent trade-off between the high inference costs and latency of Large Language Models (LLMs) and the difficulty of training lightweight, deployable classifiers without extensive labeled datasets. While LLMs demonstrate superior natural language understanding, deploying them for high-volume or low-latency text classification tasks is often operationally infeasible. Conversely, smaller, efficient models require substantial amounts of task-specific training data, which is costly and time-consuming to curate manually. The paper focuses on bridging this gap by determining how to leverage the reasoning capabilities of LLMs to train high-performing small models without the burden of manual labeling or the expense of LLM inference during deployment.

The core innovation is the "Curate-Train-Refine" framework, a closed-loop agentic system that decouples the heavy reasoning of LLMs from the runtime efficiency of lightweight classifiers. Technically, the framework employs a ReAct-style agent (orchestrated via Hugging Face smolagents) to manage a three-step iterative cycle. First, the LLM curates an initial diverse training dataset. Second, a lightweight SetFit classifier is trained and evaluated. Third, the agent analyzes specific failure modes and synthesizes targeted training examples designed to correct those observed errors. This process repeats, dynamically refining the dataset based on the model's actual weaknesses rather than relying on static data generation.

Evaluated across four standard benchmarks (SST-5, Emotion, CR, and AG News), the framework consistently outperformed standard zero-shot and few-shot baselines. The most significant gains were observed in low-data regimes and zero-shot settings. For instance, on the AG News dataset in a 2-shot setting, the method achieved 83.2% accuracy compared to a 65.3% baseline. In zero-shot classification, the framework achieved 82.6% accuracy on AG News (vs. 73.0% baseline) and 50.9% on the Emotion dataset (vs. 40.3% baseline). Even on the challenging SST-5 task, the zero-shot performance improved to 47.0% against the 43.2% baseline, demonstrating that the closed-loop approach effectively synthesizes high-quality, targeted data.

The significance of this work lies in establishing a new paradigm for dataset construction where LLMs function as active data curators rather than mere inference engines. By validating that a closed-loop, agentic approach can outperform standard baselines, the authors offer a solution to the operational bottleneck of LLM deployment. This enables organizations to maintain the low latency and cost benefits of small models while accessing the high-level reasoning of large models. Consequently, this framework facilitates the rapid development of specialized text classifiers without the need for human annotation, potentially accelerating the adoption of efficient AI solutions in production environments.

---

## Key Findings

*   **Cost & Latency Reduction:** The "Curate-Train-Refine" framework enables training lightweight text classifiers using dynamic LLM supervision, significantly reducing inference costs and latency compared to direct LLM deployment.
*   **Agentic Error Correction:** An iterative, agentic loop analyzes model failures and synthesizes targeted training examples to address errors, resulting in improved data quality and model robustness.
*   **Superior Performance:** The approach consistently outperforms standard zero-shot and few-shot baselines across four benchmarks (SST-5, Emotion, CR, AG News).
*   **LLM as Curator:** LLMs can function effectively as data curators, facilitating efficient classification without the operational burden of deploying large models for inference.

---

## Methodology

The methodology utilizes a **"closed-loop agentic framework"** that iteratively refines a lightweight text classifier. The process involves three key steps performed by an LLM agent:

1.  **Curate:** The LLM generates an initial diverse set of training data.
2.  **Train & Evaluate:** A downstream classifier is trained on this data, and its performance is evaluated against a validation set to identify successes and failures.
3.  **Refine:** The agent analyzes failure modes and synthesizes new, targeted examples specifically designed to correct observed errors.

This cycle continues to adapt the dataset dynamically to the specific task and classifier, ensuring continuous improvement.

---

## Technical Details

*   **Framework Architecture:** Closed-loop agentic system that decouples training from LLM inference.
*   **Agent Orchestration:** Uses a ReAct-style agent via Hugging Face's `smolagents`.
*   **Cycle Steps:**
    *   **Generate:** LLM produces diverse training data.
    *   **Train & Evaluate:** Trains a SetFit classifier and evaluates on a synthesized validation set.
    *   **Analyze:** Summarizes failure modes to guide the next iteration.
*   **Models Used:**
    *   **Synthesis & Analysis:** GPT-5
    *   **Classifier Backbone:** SetFit with `sentence-transformers/all-mpnet-base-v2`
*   **Termination Criteria:** The loop stops upon reaching maximum iterations or when performance plateaus, adhering to a strict per-class generation budget.

---

## Performance Results

The framework was evaluated on SST-5, Emotion, CR, and AG News. It consistently outperformed baselines, particularly in low-data regimes.

| Dataset | Setting | Manager (Proposed) | Baseline | Improvement |
| :--- | :--- | :--- | :--- | :--- |
| **SST-5** | 2-shot | 46.3% | 33.4% | +12.9% |
| | Zero-shot | 47.0% | 43.2% | +3.8% |
| **Emotion** | 8-shot | 61.6% | 45.0% | +16.6% |
| | Zero-shot | 50.9% | 40.3% | +10.6% |
| **CR** | 4-shot | 90.1% | 86.8% | +3.3% |
| | Zero-shot | 87.8% | 87.2% | +0.6% |
| **AG News** | 2-shot | 83.2% | 65.3% | +17.9% |
| | Zero-shot | 82.6% | 73.0% | +9.6% |

---

## Core Contributions

*   **Operational Efficiency:** Provides a solution to high inference costs and latency by using LLMs to train smaller, deployable models rather than relying on them for direct inference.
*   **Dynamic Data Curation:** Introduces a method for using LLMs as active agents to curate and enhance training data based on real-time error analysis.
*   **Performance Validation:** Demonstrates that a closed-loop, agentic approach to data generation can outperform standard zero-shot and few-shot baselines, offering a new paradigm for dataset construction.

---
**Quality Score:** 9/10 | **References:** 8 citations