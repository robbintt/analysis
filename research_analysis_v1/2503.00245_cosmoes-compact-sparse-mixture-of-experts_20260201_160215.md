# CoSMoEs: Compact Sparse Mixture of Experts

*Patrick Huber; Akshat Shrivastava; Ernie Chang; Chinnadhurai Sankar; Ahmed Aly; Adithya Sagar*

---

## üìä Quick Facts

| Metric | Value |
| :--- | :--- |
| **Quality Score** | 8/10 |
| **Training Dataset** | FW-edu (1.4 Trillion Tokens) |
| **Configurations** | Phone (~1.4B Active Params), Wearable (~200M Active Params) |
| **Performance Gain** | +2.35% absolute over Dense Baselines |
| **Key Innovation** | Weight-Decomposed Experts (WD) and Block-wise Expert Selection (BlES) |
| **Base Architecture** | Llama 3 |

---

## üìù Executive Summary

This research addresses the under-exploration of Sparse Mixture of Experts (MoE) architectures at small scales, specifically for on-device inference environments such as mobile phones and wearables. While MoE models have demonstrated superior efficiency and performance over dense models at large scales, their viability on resource-constrained hardware remains unproven. This problem is critical because deploying advanced AI on edge devices requires balancing three competing constraints: **model quality**, **memory footprint**, and **inference latency**. The authors establish that prior comparisons were often flawed by confounding factors, necessitating a rigorous analysis to determine if sparse architectures can practically outperform dense models within the strict limits of on-device hardware.

The authors introduce **CoSMoEs** ("Compact Sparse Mixture of Experts"), built on the Llama 3 architecture, which replaces dense feed-forward layers with sparse MoE layers utilizing Token Choice routing (top-k=2). The core technical innovation is "**Weight-Decomposed Experts**" (WD), a method that replaces standard weight matrices with low-rank decompositions, adopting a "**depth-over-breadth**" strategy (e.g., increasing layers from 19 to 32) to reduce parameters. Furthermore, to manage latency, they implement "**Block-wise Expert Selection**" (BlES), which minimizes expert switching between consecutive tokens. This is achieved through a combined Hard and Soft loss function that optimizes model offloading efficiency, ensuring the sparse architecture does not introduce prohibitive latency during inference.

The study demonstrates that CoSMoEs architectures consistently outperform dense baselines when aligned for both FLOPs and parameters. On average, the MoE architectures improved language modeling performance by at least **+2.35% absolute** compared to dense models. The introduction of Weight-Decomposed experts yielded an additional performance gain of **+1.1%** over standard MoE implementations. These results were validated across configurations suitable for phones (~1.4B active parameters) and wearables (~200M active parameters).

The significance of this work lies in successfully bridging the gap between the theoretical efficiency of MoE models and the practical requirements of edge computing. By proving that sparse architectures can be optimized for quality, memory, and latency simultaneously, the authors challenge the prevailing reliance on dense models for small-scale applications. This provides a viable pathway for deploying higher-capability, resource-efficient AI on consumer hardware.

---

## üîë Key Findings

*   **Superiority of Sparse MoE:** Sparse Mixture of Experts (MoE) architectures outperform dense models with aligned FLOPs at on-device scales when confounding factors are removed.
*   **Weight-Decomposed Experts:** The implementation of "weight-decomposed experts" significantly improves MoE model performance.
*   **Latency Reduction:** Model inference latency is effectively reduced through improved model offloading efficiency.
*   **On-Device Viability:** Compact Sparse Mixture of Experts (CoSMoEs) can be successfully enabled for on-device inference while managing strict quality, memory, and latency constraints.
*   **Quantifiable Improvement:** CoSMoEs architectures improve average language modeling performance by **at least +2.35%** absolute over dense baselines.

---

## üõ†Ô∏è Methodology

The authors structured their research approach around three critical dimensions to ensure a comprehensive evaluation of on-device viability:

1.  **Quality Assessment**
    *   Employed a strict comparative analysis against FLOP-aligned dense models.
    *   Focused on removing confounding factors often present in prior studies to ensure a fair "apples-to-apples" comparison.
    *   Implemented "weight-decomposed experts" to enhance model quality specifically for sparse architectures.

2.  **Memory Management**
    *   Evaluated models against the strict memory limitations of mobile and wearable hardware.
    *   Utilized architectural optimizations (low-rank decompositions) to minimize memory footprints.

3.  **Latency Optimization**
    *   Addressed the speed challenges often associated with sparse architectures.
    *   Optimized model offloading efficiency to ensure inference speeds remain practical for real-time on-device use.

---

## ‚ú® Contributions

*   **Small-Scale MoE Exploration:** Addresses the significant gap in research regarding MoE architectures at smaller scales. The study demonstrates their clear viability and superiority over dense models in on-device environments.
*   **Weight-Decomposed Experts:** Introduces this component as a novel methodological innovation for improving MoE performance, utilizing low-rank decompositions to manage parameter counts effectively.
*   **Memory-Latency Trade-off Solution:** Provides a practical solution to the classic memory-latency trade-off by enhancing model offloading efficiency. This facilitates the deployment of complex sparse architectures on resource-constrained edge hardware.
*   **Rigorous Benchmarking:** Establishes new standards for comparison by removing confounding factors, allowing for a true measurement of MoE efficiency against dense models.

---

## ‚öôÔ∏è Technical Details

### Architecture & Routing
*   **Base Model:** Built upon the **Llama 3** architecture.
*   **Layer Modification:** Replaces dense FFN layers with a sparse Mixture of Experts layer.
*   **Routing Mechanism:** Utilizes **Token Choice (TC) routing**.
*   **Expert Configuration:** Uses top-k=2 active experts selected from a total pool of 8 experts.

### Core Innovations
*   **Weight-Decomposed Experts (WD):**
    *   Replaces standard weight matrices with low-rank decompositions.
    *   Uses a rank $r$ equal to half the hidden dimension.
    *   Employs a **'depth-over-breadth'** design (increasing layers from 19 to 32) to reduce parameters while maintaining capacity.
*   **Block-wise Expert Selection (BlES):**
    *   Designed to minimize latency by penalizing expert switching between consecutive tokens.
    *   Utilizes a combined Hard and Soft loss function to stabilize expert selection.

### Training & Evaluation Frameworks
*   **Frameworks:** Experiments use **FLOP Aligned (FA)** and **Parameter Aligned (PA)** frameworks for fair comparison.
*   **Training Data:** All models were trained on **1.4 trillion tokens** from the FW-edu dataset.
*   **Training Specs:** Conducted for 310k steps with a sequence length of 2048.

---

## üìà Results

*   **Performance against Baselines:** MoE architectures improved language modeling performance by at least **+2.35%** absolute over dense baselines when aligning for both FLOPs and parameters.
*   **Impact of Weight-Decomposition:** WD experts provided an additional **+1.1%** improvement over standard MoE implementations.
*   **Device-Specific Configurations:**
    *   **Phone-sized:** ~1.4B Active Parameters.
    *   **Wearable-sized:** ~200M Active Parameters.
*   **Architecture Effectiveness:** The "depth-over-breadth" strategy (e.g., 32 layers vs 19) was proven effective in maintaining high quality within memory constraints.

---

### üìÑ Additional Info
*   **Quality Score:** 8/10
*   **References:** 10 citations