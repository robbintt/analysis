---
title: 'Sub-MoE: Efficient Mixture-of-Expert LLMs Compression via Subspace Expert
  Merging'
arxiv_id: '2506.23266'
source_url: https://arxiv.org/abs/2506.23266
generated_at: '2026-02-06T04:40:27'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Sub-MoE: Efficient Mixture-of-Expert LLMs Compression via Subspace Expert Merging

*Lujun Li; Zhu Qiyuan; Jiacheng Wang; Wei Li; Hao Gu; Sirui Han; Yike Guo*

***

> ### ðŸ“Š Quick Facts
>
> *   **Models Tested:** Mixtral-8x7B, DeepSeek, Qwen-1.5
> *   **Top Performance:** 96% performance retention at 25% compression
> *   **Core Technique:** Subspace Expert Merging (Joint SVD)
> *   **Quality Score:** 9/10
> *   **Citations:** 40

***

## Executive Summary

### **Problem**
Mixture-of-Experts (MoE) Large Language Models deliver high performance but suffer from massive computational overhead and memory footprints due to their extensive parameter counts. A critical challenge in deploying these models is compressing them efficiently without sacrificing accuracy. Existing compression techniques, such as expert pruning or simple weight merging, are often inadequate because they encounter **"parameter conflicts"**â€”a phenomenon where specialized experts possess conflicting weight parameters that prevent successful fusion. Furthermore, modern MoE architectures like Mixtral exhibit low inter-expert similarity (often ranging between 0.1 and 0.3), rendering traditional merging strategies ineffective. Consequently, there is a need for a method that can reduce model size significantly without the prohibitive computational cost of retraining or resource-intensive fine-tuning.

### **Innovation**
This paper introduces **"Sub-MoE,"** a novel framework designed to overcome parameter conflicts through Subspace Expert Merging. The core innovation lies in a two-phase process that aligns experts into a shared subspace to enable safe fusion.
*   **Phase 1:** Utilizes Adaptive Expert Clustering, employing K-means clustering on the cosine similarity of expert outputs to group functionally coherent experts.
*   **Phase 2:** Applies Subspace Expert Merging, which involves Experts Union Decomposition via joint Singular Value Decomposition (SVD) on vertically concatenated weight matrices. This technique separates the weights into shared orthonormal bases (U-matrices) and specific components (V-matrices). By reconstructing the merged weights using a frequency-weighted average of the V-matrices within this shared subspace, Sub-MoE effectively mitigates the interference that typically plagues expert merging.

### **Results**
Experimental validation demonstrates that Sub-MoE achieves a superior trade-off between compression rate and performance retention compared to state-of-the-art baselines.
*   On the **Mixtral-8x7B** model, the framework maintains **96%** of the original zero-shot performance with a 25% reduction in experts and retains **86%** performance even when experts are reduced by 50%.
*   The method consistently outperforms existing pruning techniques (such as SEER-MoE and NAEE) and merging methods (such as MC-SMoE and EEP) across diverse architectures including DeepSeek and Qwen-1.5.
*   Notably, these results are achieved **without** the need for resource-intensive fine-tuning, highlighting the method's intrinsic efficiency.

### **Impact**
The significance of Sub-MoE lies in its resolution of the fundamental parameter conflict issue, which has historically limited the effectiveness of model compression for MoE architectures. By enabling substantial expert reduction (up to 50%) with minimal performance degradation, the framework makes large-scale MoE models more viable for deployment in resource-constrained environments. Additionally, the design supports extension with intra-expert compression, offering a pathway for further inference optimization. This work establishes a new standard for efficient MoE compression, potentially lowering the barrier to entry for deploying high-performance, specialized LLMs.

***

## Key Findings

*   **High Efficiency:** Sub-MoE maintains **96%** of the original performance with a 25% reduction in experts and **86%** performance with a 50% reduction on the Mixtral-8x7B model in zero-shot benchmarks.
*   **Superiority over SOTA:** The proposed framework significantly outperforms current state-of-the-art expert pruning and merging techniques across multiple models.
*   **Broad Validation:** Experiments validate the effectiveness of the method on diverse large-scale MoE architectures, including Mixtral, DeepSeek, and Qwen-1.5|3.
*   **Conflict Resolution:** The approach successfully addresses the fundamental issue of parameter conflicts that typically hinder expert merging by utilizing a shared subspace strategy.
*   **Extensibility:** The framework supports extension with intra-expert compression, allowing for further inference optimization beyond expert reduction.

## Methodology

The core methodology, **Sub-MoE**, focuses on Subspace Expert Merging to compress Mixture-of-Experts (MoE) models by mitigating parameter conflicts inherent in expert specialization.

### Phase 1: Adaptive Expert Clustering
Groups functionally coherent experts using K-means clustering based on the cosine similarity of expert outputs.

### Phase 2: Subspace Expert Merging
Involves a three-step process to fuse experts safely:
1.  **Experts Union Decomposition:** Performing joint Singular Value Decomposition (SVD) on concatenated expert weights.
2.  **Frequency-based Merging:** Pursuing merging of individual expert-specific V-matrices.
3.  **Reconstruction:** Reconstructing experts using the merged V-matrix within the shared subspace.

## Technical Details

The Sub-MoE framework is a two-stage process designed to compress Mixture-of-Experts models.

### Stage 1: Adaptive Expert Clustering
*   **Mechanism:** Identifies functionally similar experts using the average cosine similarity of their outputs.
*   **Algorithm:** Groups them using K-means clustering.
*   **Dynamic Adjustment:** The number of clusters per layer is dynamically adjusted based on functional redundancy.

### Stage 2: Subspace Expert Merging
Addresses parameter conflicts by aligning experts to a common subspace.
*   **Shared Basis:** Applies Singular Value Decomposition (SVD) to vertically concatenated weight matrices to find a shared orthonormal basis.
*   **Merging Strategy:** Employs frequency-based V-Matrix merging, where the merged V matrix is a frequency-weighted average based on router activation.
*   **Reconstruction:** Reconstructs the final weights using the formula:
    `$W_{merged} = U \Sigma [V_{merged}]^T$`

## Results

*   **Mixtral-8x7B Performance:** The framework maintains 96% of original performance with a 25% reduction in experts and 86% with a 50% reduction.
*   **Baseline Comparison:** Sub-MoE outperforms state-of-the-art pruning (SEER-MoE, NAEE, MoE-I2) and merging (MC-SMoE, HC-SMoE, EEP) baselines by achieving superior efficiency-performance trade-offs without resource-intensive fine-tuning.
*   **Architecture Validation:** The method was validated on architectures including Mixtral, DeepSeek, and Qwen-1.5/Qwen3-MoE.
*   **Low Similarity Handling:** Successfully addresses the challenge of low inter-expert similarity (0.1â€“0.3) found in models like Mixtral-8x7B.

## Contributions

*   **Framework Introduction:** Introduced "Sub-MoE," a new framework for compressing MoE LLMs that overcomes the limitations of traditional merging methods caused by parameter conflicts.
*   **Technique Development:** Developed the Subspace Expert Merging Technique using joint SVD to separate expert weights into shared U-matrices and specific V-components, enabling the alignment and fusion of experts in a shared subspace.
*   **Experimental Evidence:** Provided extensive experimental evidence demonstrating that the method achieves a better balance between compression rate (up to 50% expert reduction) and performance retention compared to existing pruning and merging baselines.