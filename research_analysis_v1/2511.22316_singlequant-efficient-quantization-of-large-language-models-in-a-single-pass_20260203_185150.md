---
title: 'SingleQuant: Efficient Quantization of Large Language Models in a Single Pass'
arxiv_id: '2511.22316'
source_url: https://arxiv.org/abs/2511.22316
generated_at: '2026-02-03T18:51:50'
quality_score: 9
citation_count: 11
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# SingleQuant: Efficient Quantization of Large Language Models in a Single Pass

*Jinying Xiao; Bin Ji; Shasha Li; Xiaodong Liu; Ma Jun; Ye Zhong; Wei Li; Xuan Xie; Qingbo Wu; Jie Yu*

---

> ### ðŸ“Š Quick Facts
> *   **Quality Score:** 9/10
> *   **Model Scope:** 7B â€“ 70B Parameters
> *   **Peak Speedup:** 1,400Ã— (vs. SOTA Baseline)
> *   **Performance Gain:** +0.57% (Avg. Task Performance)
> *   **Complexity:** Reduced from $O(n^2)$ to $O(n^{3/2})$
> *   **Core Innovation:** Single-pass quantization decoupled from truncation

---

## Executive Summary

Current quantization methods for Large Language Models (LLMs) face significant inefficiencies and convergence challenges that hinder their practical deployment. This study identifies the root cause of these issues as the incompatibility between gradient optimization and quantization truncation, specifically the application of the Straight-Through Estimator (STE) on Stiefel manifolds. This combination introduces non-smoothness and gradient noise, leading to serious convergence pathologies that force existing methods to rely on computationally expensive, iterative training processes to achieve acceptable model accuracy.

To resolve these convergence pathologies, the authors propose **SingleQuant**, a novel framework that decouples the quantization process from truncation, thereby eliminating non-smoothness and gradient noise. Instead of iterative gradient optimization, SingleQuant utilizes two specific rotation transformations constructed with strictly formulated Givens rotations: the **Alignment Rotation Transformation (ART)**, which smooths activation outliers via closed-form optimal rotations, and the **Uniformity Rotation Transformation (URT)**, which reshapes distributions through geometric mapping to facilitate quantization. By integrating these components with a Hadamard matrix to synchronously smooth activations and weights, the approach reduces computational complexity from $O(n^2)$ to $O(n^{3/2})$.

SingleQuant demonstrates superior efficiency and performance across a wide range of model sizes, from 7B to 70B parameters. In specific testing on LLaMA-2-13B, the framework achieved a **1,400Ã— quantization speedup** compared to the state-of-the-art baseline while simultaneously improving average task performance by **+0.57%**. On larger scales, LLaMA3-70B showed a perplexity degradation of only 0.84 relative to the FP16 baselineâ€”outperforming SOTA by 1.62 PPLâ€”and LLaMA-2-70B narrowed the zero-shot QA accuracy gap to just 0.91% compared to the full-precision model.

This research establishes a new standard for the trade-off between quantization speed and model accuracy, challenging the assumption that extensive training is necessary for high-fidelity quantized LLMs. By providing a theoretical diagnosis of convergence failures in existing methods and introducing a deterministic, single-pass solution, SingleQuant offers a scalable path for efficiently compressing massive models without sacrificing performance.

---

## Key Findings

*   **Root Cause of Quantization Inefficiency:** The study identifies that the combination of incompatible gradient optimization and quantization truncationâ€”specifically the use of Straight-Through Estimator (STE) on Stiefel manifoldsâ€”introduces non-smoothness and gradient noise, leading to serious convergence pathologies.
*   **Superior Efficiency and Performance:** SingleQuant achieves higher task performance than state-of-the-art baselines while significantly reducing the time required for quantization.
*   **Significant Speedup:** In specific testing on LLaMA-2-13B, the framework demonstrated a **1,400Ã— quantization speedup** compared to the best selected baseline.
*   **Performance Improvement:** Alongside the speedup, SingleQuant achieved a **+0.57% increase** in average task performance over the best baseline on LLaMA-2-13B.
*   **Broad Scalability:** The method proves effective across diverse tasks and a wide range of model sizes, specifically validated on LLMs ranging from 7B to 70B parameters.

---

## Methodology

The proposed framework, **SingleQuant**, utilizes a single-pass quantization strategy designed to decouple the process from quantization truncation, effectively eliminating non-smoothness and gradient noise. The approach centers on two specific rotation transformations constructed using strictly formulated Givens rotations:

1.  **Alignment Rotation Transformation (ART):** Targets distinct activation outliers and achieves smoothing via closed-form optimal rotations.
2.  **Uniformity Rotation Transformation (URT):** Reshapes distributions through geometric mapping to facilitate quantization.

By relying on closed-form solutions and geometric mapping rather than iterative gradient optimization, the framework enables rapid, high-fidelity quantization.

---

## Technical Details

The approach reduces computational complexity from $O(n^2)$ to $O(n^{3/2})$ through matrix reshaping. It utilizes a rotation-based transformation for activations and weights using rotation matrices composed of:

*   **ART (Alignment Rotation Transformation):**
    *   Smooths Magnitude Outliers via optimal Givens rotations.
    *   Independent of the Straight-Through Estimator (STE).
*   **URT (Uniformity Rotation Transformation):**
    *   Maps Nominal Outliers to a uniform distribution.
    *   Preserves L2 norm during the process.
*   **Integration:**
    *   These components are integrated using a Hadamard matrix to synchronously smooth activations and weights.

---

## Contributions

*   **Diagnostic Insight:** Provided a theoretical confirmation that existing quantization methods fail to converge effectively due to the non-smoothness and gradient noise introduced by STE on Stiefel manifolds.
*   **Novel Quantization Framework:** Introduced SingleQuant, the first framework to decouple from quantization truncation to resolve convergence pathology, enabling efficient single-pass quantization.
*   **Algorithmic Innovation:** Developed two specific mathematical transformationsâ€”ART (for outlier smoothing) and URT (for distribution reshaping)â€”utilizing strictly formulated Givens rotations to handle optimization challenges deterministically.
*   **Benchmark Advancement:** Set a new standard for the trade-off between quantization speed and model accuracy, demonstrating that extensive training is not necessary for high-fidelity quantized LLMs.

---

## Results

*   **LLaMA3-70B:** Achieved a perplexity degradation of only **0.84 PPL** relative to the FP16 baseline, outperforming SOTA baselines by 1.62 PPL.
*   **LLaMA-2-70B (Zero-shot QA):** The accuracy gap was narrowed to **0.91%** compared to FP16.
*   **Efficiency (LLaMA-2-13B):** Showed a 1,400x quantization speedup and a +0.57% performance gain over the best baseline.
*   **Validation:** Results were validated across models ranging from 7B to 70B parameters.

---
**References:** 11 citations