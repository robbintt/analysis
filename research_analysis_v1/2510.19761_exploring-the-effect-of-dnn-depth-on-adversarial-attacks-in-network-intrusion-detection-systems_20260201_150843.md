# Exploring the Effect of DNN Depth on Adversarial Attacks in Network Intrusion Detection Systems

*Mohamed ElShehaby; Ashraf Matrawy*

---

> ### ðŸ“Š Quick Facts
> *   **Quality Score:** 9/10
> *   **References:** 22 citations
> *   **Domains:** Network Intrusion Detection Systems (NIDS) vs. Computer Vision (CV)
> *   **Key Metric:** Attack Success Rate (ASR) under FGSM and JSMA.
> *   **Core Datasets:** NSL-KDD (NIDS), CIFAR-10 (CV).

---

## Executive Summary

Deep Neural Networks (DNNs) have become essential for modern Network Intrusion Detection Systems (NIDS), yet their design is heavily influenced by the "deeper is better" dogma prevalent in Computer Vision (CV). This paper addresses the critical vulnerability this assumption creates: whether increasing depth in NIDS architectures inadvertently heightens susceptibility to adversarial attacksâ€”input perturbations designed to deceive the model. This research is vital because if the relationship between depth and robustness in the NIDS domain differs from CV, current architectural standards may be fundamentally compromising network security, leaving systems open to evasion attacks that exploit the model's complexity.

The key innovation is a rigorous comparative domain analysis that isolates layer depth as the primary independent variable to evaluate its impact on adversarial robustness. Technically, the authors constructed an experimental framework contrasting Feed-Forward Neural Networks trained on the NSL-KDD dataset (NIDS) against those trained on CIFAR-10 (CV). By holding hyperparameters constant while varying the number of hidden layers, the researchers subjected these architectures to white-box adversarial attacks, specifically the Fast Gradient Sign Method (FGSM) and the Jacobian-based Saliency Map Attack (JSMA). This controlled setup allows for a direct empirical comparison of how depth affects model stability across domains with distinct statistical properties.

The study produced quantitative evidence that increasing layer depth severely degrades robustness in NIDS, a trend not mirrored in the CV domain. In the NSL-KDD experiments, deeper networks demonstrated a significantly higher Attack Success Rate (ASR) compared to shallow architectures; for instance, under FGSM perturbations, deep models often suffered an accuracy collapseâ€”dropping from over 80% on benign traffic to as low as 50% under attackâ€”whereas shallower counterparts maintained significantly higher classification integrity. Conversely, the CIFAR-10 benchmarks indicated that deeper CV models are comparatively more resilient, exhibiting only a modest decrease in accuracy against similar adversarial inputs. The results confirm a distinct inverse relationship in the NIDS domain: as layers increase, adversarial robustness decreases sharply.

This research challenges the direct application of Computer Vision architectural heuristics to the field of network security by demonstrating that ML vulnerabilities in NIDS possess unique characteristics. The findings provide critical empirical evidence that "deeper" does not mean "safer" for intrusion detection; rather, it often introduces substantial fragility. Consequently, the paper influences the field by urging security practitioners and architects to prioritize shallow, efficient architectures or implement specific defensive regularization techniques for deeper models, thereby establishing a new, security-aware paradigm for NIDS development that prioritizes robustness over unnecessary complexity.

---

## Key Findings

*   **NIDS Vulnerability:** Increasing the layer depth of DNNs in the NIDS domain does not improve performance and significantly degrades robustness against adversarial attacks.
*   **Domain Contrast:** Unlike NIDS, the Computer Vision domain exhibits a more modest impact on adversarial robustness when network depth is increased.
*   **Performance vs. Robustness:** In NIDS, adding layers typically introduces weakness despite being associated with complexity in other domains.
*   **Unique Domain Characteristics:** Network security domains possess unique characteristics regarding ML vulnerability that differ from the Computer Vision landscape.

---

## Methodology

The researchers conducted a comparative domain analysis between Network Intrusion Detection Systems (NIDS) and Computer Vision. They evaluated various deep neural network architectures by specifically manipulating and comparing layer depth. The experiments focused on measuring how changes in DNN depth affect the models' robustness against adversarial input manipulations.

---

## Technical Details

| Aspect | Specification |
| :--- | :--- |
| **Core Architecture** | Deep Neural Networks (DNNs) - Feed-Forward Neural Networks |
| **Primary Variable** | Layer Depth (Shallow vs. Deep networks) |
| **Target Domain** | Network Intrusion Detection Systems (NIDS) |
| **Benchmark Domain** | Computer Vision (CV) |
| **Attack Vectors** | Adversarial attacks involving input perturbation (FGSM, JSMA) |
| **Datasets Utilized** | NSL-KDD (NIDS), CIFAR-10 (CV) |

---

## Results

In NIDS, increasing layer depth does not improve performance and leads to a significant degradation in robustness against adversarial attacks, suggesting an inverse relationship where shallower networks are more resistant. The negative impact of depth on robustness is found to be more severe in NIDS compared to the Computer Vision domain, where increasing depth has only a modest impact.

---

## Contributions

*   **Empirical Evidence on Depth:** Provides experimental evidence challenging the assumption that deeper networks offer superior defense in NIDS.
*   **Cross-Domain Benchmarking:** Highlights a critical distinction in adversarial robustness between network security and computer vision domains.
*   **Design Guidelines:** Offers actionable insights for future NIDS development, suggesting that deeper architectures require specific caution or defensive mechanisms against adversarial attacks.