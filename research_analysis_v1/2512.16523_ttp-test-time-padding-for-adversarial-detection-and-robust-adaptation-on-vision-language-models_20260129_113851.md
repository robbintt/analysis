# TTP: Test-Time Padding for Adversarial Detection and Robust Adaptation on Vision-Language Models

*Zhiwei Li; Yitian Pang; Weining Wang; Zhenan Sun; Qi Li*

---

> ### **Quick Facts**
>
> *   **Quality Score:** 9/10
> *   **Methodology:** Test-Time Only (No retraining required)
> *   **Key Mechanism:** Spatial Padding & Cosine Similarity Shift
> *   **Architectures Tested:** ViT-B/32, RN50
> *   **Performance (ViT-B/32):**
>     *   Clean Accuracy: **63.1%**
>     *   Robust Accuracy: **48.6%**
> *   **Primary Advantage:** Simultaneous optimization of robustness and clean accuracy without degradation trade-offs.

---

## Executive Summary

Vision-Language Models (VLMs), particularly those based on the CLIP architecture, are highly susceptible to adversarial attacks where subtle perturbations to input images cause severe misclassification. A critical challenge in this domain is that existing test-time defense strategies struggle to reliably distinguish between clean and adversarial inputs. This failure often forces a trade-off: improving robustness against attacks typically degrades accuracy on clean data, or requires expensive, dataset-specific retraining. This paper addresses the need for a detection mechanism that can accurately identify adversarial inputs and a defense strategy that optimizes for both robustness and clean accuracy simultaneously without incurring retraining costs.

The authors introduce **Test-Time Padding (TTP)**, a framework designed exclusively for inference that defends against adversarial attacks through spatial manipulation of input images. Technically, TTP operates in two distinct phases. First, for **Adversarial Detection**, it computes the "Cosine Similarity Shift" between CLIP feature embeddings of the original image and a spatially padded version; adversarial inputs exhibit a significant shift compared to clean ones, enabling identification via a universal threshold. Second, for **Targeted Adaptation**, TTP employs "trainable padding" and a similarity-aware ensemble on detected inputs. This involves optimizing padding values at test time and utilizing the ensemble to counteract perturbations and restore the disrupted attention patterns within the vision transformer, while leaving clean inputs untouched to preserve their accuracy.

TTP demonstrates substantial performance improvements across diverse CLIP backbones and benchmarks, including ImageNet. In quantitative evaluations using the ViT-B/32 backbone on ImageNet, TTP successfully maintained a clean accuracy of **63.1%** while significantly boosting robust accuracy against strong attacks to **48.6%**. This performance surpassed existing state-of-the-art test-time defense strategies, which typically suffer from a heavy drop in clean accuracy. Comparable improvements were observed on the RN50 architecture. Empirical analysis further confirmed that the trainable padding mechanism effectively repairs the attention patterns disrupted by adversarial examples, validating the method's ability to stabilize model inference under attack.

This research represents a significant advancement in VLM security by offering a generalizable, retraining-free defense mechanism. By eliminating the need for labeled data and dataset-specific thresholds, TTP provides a versatile solution that can be deployed across different architectures and datasets with minimal overhead. The framework's ability to reliably separate malicious inputs from benign ones while preserving high clean accuracy sets a new precedent for test-time adaptation, potentially influencing future standards for deploying robust vision-language models in security-sensitive environments.

---

## Key Findings

*   **Detection Metric:** Cosine similarity shift between CLIP feature embeddings—computed before and after spatial padding—serves as a reliable metric to distinguish clean inputs from adversarial ones.
*   **Robustness & Accuracy:** TTP delivers substantial improvements in adversarial robustness across diverse CLIP backbones and benchmarks while successfully preserving clean accuracy.
*   **Performance:** The proposed method surpasses existing state-of-the-art test-time defense strategies.
*   **Attention Restoration:** Trainable padding effectively restores disrupted attention patterns for adversarial inputs.

---

## Methodology

The proposed framework, **Test-Time Padding (TTP)**, operates exclusively at inference time and consists of two sequential phases:

1.  **Phase 1: Adversarial Detection**
    *   Computes CLIP embeddings for the original image and a spatially padded version.
    *   Calculates the cosine similarity shift between these embeddings.
    *   Identifies adversarial inputs using a universal threshold based on the magnitude of the shift.

2.  **Phase 2: Targeted Adaptation**
    *   Applied only to inputs identified as adversarial.
    *   Utilizes **trainable padding** and a similarity-aware ensemble to repair attention patterns.
    *   **Clean Inputs:** Left unchanged to preserve original accuracy.
    *   **Adversarial Inputs:** Processed to counteract perturbations.

---

## Technical Details

*   **Framework Operation:** Applies spatial padding to input images during the inference phase without modifying the training phase.
*   **Detection Metric:** Cosine Similarity Shift between CLIP feature embeddings of the original image versus the padded image.
*   **Adaptation Mechanism:** Employs "trainable padding," where padding values or operations are optimized at test time specifically to counteract adversarial perturbations.
*   **Objective:** Restores disrupted attention patterns in Vision-Language Models (VLMs) caused by adversarial attacks.

---

## Contributions

*   **Dual Optimization:** Addresses the limitation of existing test-time strategies by reliably distinguishing between clean and adversarial inputs to optimize robustness and clean accuracy simultaneously.
*   **Cost Efficiency:** Eliminates retraining costs by providing an alternative to training-time defenses that does not require labeled data.
*   **Generalizability:** Introduces a defense mechanism based on spatial padding that applies across different architectures and datasets without the need for dataset-specific thresholds.

---

## Results

TTP delivers substantial improvements in adversarial robustness across diverse CLIP backbones (e.g., ViT-B/32, RN50) and multiple benchmarks. The method successfully preserves clean accuracy without the typical degradation trade-off found in other defenses.

*   **vs. SOTA:** Surpasses existing state-of-the-art test-time defense strategies.
*   **Empirical Validation:** Confirms that trainable padding restores disrupted attention patterns for adversarial inputs.

**Reference:** 40 citations