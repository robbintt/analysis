# An Optimal Policy for Learning Controllable Dynamics by Exploration

*Peter N. Loxley*

---

> ### ðŸ“Š Quick Facts
> - **Quality Score:** 8/10
> - **References:** 8 Citations
> - **Model Type:** Controllable Markov Chains (CMC)
> - **Optimization Goal:** Information Gain Maximization
> - **Algorithm Strategy:** Greedy Hybrid Strategy (Exhaustive Search/Cross-Entropy + Rollout)
> - **Key Requirement:** Non-stationary Policy

---

## Executive Summary

This research addresses the fundamental challenge of optimal exploration in unknown environments modeled as Controllable Markov Chains (CMCs), where the objective is to learn system dynamics rather than maximize external rewards. The problem is particularly difficult because specific structural featuresâ€”such as transient, absorbing, and non-backtracking statesâ€”inherently restrict an agent's movement and ability to gather data. This matters because traditional stationary exploration approaches fail to account for these constraints, leading to inefficient learning. The paper formalizes the complexity of navigating these restrictive topologies within a finite time horizon, establishing the need for a strategy that adapts to the environment's structural limitations.

The key innovation is the theoretical derivation of the general form of an optimal exploration policy, proving that optimality necessitates a non-stationary approach when restrictive states are present. Technically, the method optimizes exploration by maximizing information gain using a time-dependent parameterization of the control set, which evolves as data is collected. The validity of this non-stationary approach is rigorously established through dynamic programming principles, utilizing counting arguments to compare policies and demonstrating a sequential improvement property to ensure optimality. This avoids complex search procedures by combining policy space approximations (via Exhaustive Search or Cross-Entropy Method) with a rollout algorithm for value space refinement, efficiently managing the constraints imposed by restrictive states.

The proposed framework was validated empirically on a deterministic Two-State CMC with a time horizon of $N=20$, with the goal of learning specific parameters regarding restrictive states. The algorithm successfully identified the optimal parameter vector $r^* = (1, 1, 7)$, correctly pinpointing State 1 and Control 1 as restrictive while strategically avoiding the absorbing state for precisely 7 periods. Performance metrics, including the quantification of "Missing Information" and information gain functions $h(i, u, F)$, confirmed the method's efficacy, showing that the rollout policy significantly reduced information uncertainty compared to static approaches across six diverse examples.

This work significantly advances the field of reinforcement learning and active exploration by providing a mathematically rigorous solution that prioritizes structural simplicity over computational complexity. Its primary influence lies in establishing the specific mathematical conditions necessary for optimalityâ€”such as the proof of non-stationarityâ€”offering a theoretical alternative to heuristic-based exploration methods. By shifting the focus from reward maximization to information gain and demonstrating that optimal learning can be achieved through constraint-aware policies, the paper provides a foundational blueprint for designing efficient algorithms in structurally complex environments.

---

## Key Findings

*   **Derivation of an Optimal Exploration Policy:** Establishes the general form of an optimal policy for learning controllable dynamics within a limited time horizon in unknown environments.
*   **Necessity of Non-Stationarity:** Demonstrates that optimal exploration requires a non-stationary policy due to restrictive state types (transient, absorbing, and non-backtracking) which constrain the control of dynamics.
*   **Greedy Information Maximization:** Achieves goals by maximizing information gain in a greedy fashion, selecting controls from a constraint set that evolves over time.
*   **Algorithmic Efficiency:** The proposed policy is characterized by simple implementation and computational efficiency, relying on a straightforward parameterization of the control set.

---

## Methodology

*   **Parameterization and Algorithm Design:** Utilizes a simple parameterization for the set of controls and presents a specific algorithm to identify the optimal policy.
*   **Dynamic Programming & Proof Techniques:** Policy optimality is demonstrated through counting arguments, direct comparison with suboptimal policies, and the application of a 'sequential improvement property' derived from dynamic programming principles.
*   **Iterative Constraint Management:** During exploration, the method manages agent behavior by selecting controls from a constraint set that changes dynamically as the agent explores the environment.

---

## Technical Details

*   **Modeling Environment:**
    *   Utilizes **Controllable Markov Chains (CMC)** to model environments with dynamics defined by transition probabilities $p_{ij}(u)$.
*   **State Representation:**
    *   Combines physical state $i$ and a transition tensor $F$ recording exploration history.
*   **Probability Estimation:**
    *   Estimates unknown probabilities using a Bayesian approach via the mean of a **Dirichlet distribution** with smoothing parameter $\alpha$.
*   **Objective Function:**
    *   Objective is to maximize cumulative information gain $h(i, u, F)$ over finite horizon $N$.
*   **Handling Restrictive States:**
    *   Uses time-dependent parameterization of control set $U_k(i, r)$ to avoid restrictive states before time constant $t_i$.
*   **Optimization Strategy:**
    *   **Hybrid Strategy:** Approximation in policy space (via **Exhaustive Search** or **Cross-Entropy Method** with Binomial distribution) followed by approximation in value space via a rollout algorithm for refinement.

---

## Results

*   **Experimental Setup:** Experiment conducted on a Two-State deterministic CMC with time horizon $N=20$.
*   **Goal:** Learn parameter vector $r$ containing restricted state index, control index, and time constant.
*   **Optimization Outcome:** Algorithm successfully identified optimal parameters $r^* = (1, 1, 7)$, correctly identifying State 1 and Control 1 as restrictive and avoiding the absorbing state for 7 periods.
*   **Performance Metrics:**
    *   Information gain measured by $h(i, u, F)$.
    *   "Missing Information" visualization utilized.
*   **Analysis:**
    *   Trajectory analysis confirmed the non-stationary nature of the policy.
    *   The rollout policy showed a decrease in missing information compared to the base parametric policy.

---

## Contributions

*   **Theoretical Framework for Exploration:** Provides a theoretical solution to the problem of learning by exploring in controllable Markov chains, focusing on information gain regarding dynamics rather than just reward maximization.
*   **Insight into Structural Constraints:** Offers a critical analysis of how specific state structures (transient, absorbing, non-backtracking) inherently limit control and dictate the necessity of non-stationary policies for exploration efficiency.
*   **Validated Practical Application:** Substantiates the theoretical framework through the detailed treatment of six diverse examples of controllable dynamics, proving the robustness of the proposed policy structure.