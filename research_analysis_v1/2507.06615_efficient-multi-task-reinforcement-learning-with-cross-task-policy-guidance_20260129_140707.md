# Efficient Multi-Task Reinforcement Learning with Cross-Task Policy Guidance

*Jinmin He; Kai Li; Yifan Zang; Haobo Fu; Qiang Fu; Junliang Xing; Jian Cheng*

> ### üìä Quick Facts
> *   **Quality Score:** 6/10
> *   **References:** 40 Citations
> *   **Core Method:** Soft Actor-Critic (SAC) based MTRL
> *   **Benchmarks:** Manipulation & Locomotion (Meta-World)
> *   **Key Innovation:** Cross-Task Policy Guidance (CTPG)

---

## üìù Executive Summary

This paper addresses the inherent inefficiency in Multi-Task Reinforcement Learning (MTRL) where standard parameter sharing methods, despite improving sample efficiency through shared representations, fail to explicitly exploit known behaviors from "proficient" tasks to accelerate "unmastered" ones. This limitation creates a critical bottleneck in complex robotic manipulation and locomotion domains, as agents are forced to redundantly explore states and sub-skills they have effectively already mastered elsewhere. The resulting high sample complexity and inefficient use of computational resources necessitate a mechanism for direct logic transfer to accelerate skill acquisition across heterogeneous tasks.

The authors introduce the **Cross-Task Policy Guidance (CTPG)** framework, a novel approach built on Soft Actor-Critic (SAC) that augments parameter sharing with explicit policy guidance. CTPG employs a dual-policy architecture comprising a Control Policy ($\pi_i$) for continuous action execution and a Guide Policy ($\Pi_g^i$) that dynamically selects which task's control policy to execute for a fixed duration $K$. To ensure mathematical fairness and stability, the framework utilizes a **Policy-Filter Gate**‚Äîdefined by the inequality $Q_g^i(s, j) \ge V_i(s)$‚Äîto mask non-beneficial policies, and a **Comparable Guide Q-Value** ($\hat{Q}_g^i$) to correct entropy terms. Additionally, Hindsight Off-Policy Correction is implemented to handle the non-stationarity of control policies during training.

Evaluated on rigorous manipulation and locomotion benchmarks, specifically Meta-World tasks such as Button-Press, Drawer-Close, and Door-Open, CTPG demonstrated superior learning efficiency compared to standard parameter sharing baselines like SAC-MT. The framework successfully transferred specific sub-skills‚Äîsuch as the shared 'reach' and 'grab' phases‚Äîfrom proficient to unmastered tasks, effectively filtering noise and reducing redundant exploration. By defining sample complexity efficiency as the reduction in required environment interactions to reach mastery, the results confirm that CTPG achieves faster convergence rates and higher final rewards. Although specific numerical metrics are not detailed in the provided text, the study emphasizes that the dual gating mechanisms significantly outperformed existing approaches in terms of both the speed of skill acquisition and the quality of generated training trajectories.

The significance of this research lies in establishing a complementary guidance paradigm that transcends traditional weight-sharing by enabling the direct transfer of execution logic. Rather than relying on static shared weights, CTPG allows agents to dynamically select and utilize full policies or partial phases from other tasks, offering a pathway toward more modular and efficient multi-agent systems. The framework's ability to integrate seamlessly with existing architectures without replacing them, while maintaining robust performance in realistic robotic scenarios, positions it as a versatile tool for enhancing generalization and reducing sample complexity in multi-task learning.

---

## üîç Key Findings

*   **Performance Improvements:** Integrating the proposed Cross-Task Policy Guidance (CTPG) framework with existing parameter sharing approaches leads to significant performance improvements in manipulation and locomotion benchmarks.
*   **Enhanced Efficiency:** Learning efficiency is enhanced through dual gating mechanisms that filter out non-beneficial control policies and block guidance for proficient tasks.
*   **Skill Acquisition:** Explicit guidance from control policies of proficient tasks effectively accelerates the learning and skill acquisition of unmastered tasks.
*   **Sub-Skill Transfer:** The framework demonstrates the ability to effectively share full policies or partial phases of policies (e.g., shared 'reach' and 'grab' sub-skills).

---

## ‚öôÔ∏è Methodology

The authors introduce the **Cross-Task Policy Guidance (CTPG)** framework to exploit cross-task similarities. The methodology involves:

1.  **Policy Architecture:** Training a specific 'guide policy' for each task to select the optimal 'behavior policy' from the pool of all tasks' control policies, generating superior training trajectories.
2.  **Gating Mechanisms:** Implementing two gating mechanisms to filter non-beneficial policies and prevent guidance for tasks that do not require it.
3.  **Integration:** Designing the CTPG framework to be integrated with existing parameter sharing approaches rather than replacing them entirely.

---

## üöÄ Contributions

*   **Novel Guidance Paradigm:** Introduces a paradigm that leverages explicit policy guidance from proficient tasks to unmastered tasks, complementing existing Multi-Task RL parameter sharing.
*   **Comprehensive Framework:** Proposes the CTPG framework which utilizes guide policies to dynamically select behavior policies across tasks.
*   **Optimization Innovations:** Contributes specific gating mechanisms to reduce noise and computational overhead.
*   **Empirical Validation:** Provides empirical validation demonstrating the framework's versatility and efficacy in realistic robotic benchmarks when combined with established baselines.

---

## üî¨ Technical Details

*   **Base Algorithm:** The Cross-Task Policy Guidance (CTPG) framework introduces a policy sharing mechanism for Multi-Task Reinforcement Learning (MTRL) based on **Soft Actor-Critic (SAC)**.
*   **Dual Policy Structure:**
    *   **Control Policy ($\pi_i$):** Responsible for continuous action execution.
    *   **Guide Policy ($\Pi_g^i$):** Selects which task's control policy to execute for a fixed duration $K$.
*   **Q-Functions:** Utilizes a Guide Q-Function ($Q_g^i$) defined by a specific Bellman equation.
*   **Stability Measures:** Employs **Hindsight Off-Policy Correction** to handle the non-stationarity of control policies.
*   **Efficiency Mechanisms:**
    *   **Policy-Filter Gate:** Masks out non-beneficial policies based on the inequality $Q_g^i(s, j) \ge V_i(s)$.
    *   **Comparable Guide Q-Value ($\hat{Q}_g^i$):** Corrects entropy terms to ensure mathematical comparability.

---

## üìà Results

*   **Benchmarks:** Evaluated on Manipulation and Locomotion benchmarks, specifically on tasks like Button-Press vs. Drawer-Close and Door-Open vs. Drawer-Open.
*   **Performance:** The framework claims significant performance improvements over existing parameter sharing approaches and enhanced learning efficiency by leveraging dual gating mechanisms.
*   **Convergence:** Demonstrates faster convergence rates by accelerating the learning of unmastered tasks.
*   **Note:** Specific quantitative metrics and numerical scores are not available in the provided text, though qualitative improvements are emphasized.