# How well can LLMs provide planning feedback in grounded environments?

*Yuxuan Li; Victor Zhong*

---

> ### ðŸ“‹ Executive Summary
> 
> **Problem**
> This paper addresses the challenge of automating the evaluation and guidance of planning processes in grounded environments without relying on manually engineered reward functions or expensive human-annotated demonstrations. As Large Language Models (LLMs) and Vision-Language Models (VLMs) demonstrate growing reasoning capabilities, a critical question emerges: can these Foundation Models effectively serve as third-party observers to provide feedback that corrects or improves agent behavior? Establishing the reliability and scope of LLM-generated feedback is essential for reducing the human overhead typically required in reinforcement learning and hierarchical planning systems.
> 
> **Innovation**
> The key innovation is the formalization of a comprehensive framework for evaluating "Feedback Models" (FMs) within a Markov Decision Process (MDP) structure. The authors introduce a rigorous taxonomy of five distinct feedback categories: **Binary Feedback**, **Preference Feedback**, **Action Advising**, **Goal Advising**, and **Delta Action Feedback**. The study systematically assesses these feedback types across three diverse grounded domainsâ€”Symbolic/Fully Observable, Text/Partially Observable, and Continuous Control. Furthermore, the methodology isolates the impact of inference strategies by comparing In-Context Learning (ICL), Chain-of-Thought (CoT), and varying levels of access to environment dynamics against ground-truth data.
> 
> **Results**
> The experimental results reveal a performance gap that correlates strongly with environmental complexity. In fully observable symbolic domains (Cliff Walking), models achieved near-perfect accuracy for binary feedback ($0.91â€“1.00$) and action advising ($0.77â€“1.00$). However, performance degraded in MiniGrid scenarios and collapsed in continuous control environments (Robomimic), where Delta Action accuracy was critically low ($0.03â€“0.12$). The study found that larger reasoning models consistently outperformed smaller counterparts, and that advanced inference techniquesâ€”specifically Chain-of-Thought and the provision of domain dynamicsâ€”are crucial for maximizing feedback quality.
> 
> **Impact**
> This research is significant because it delineates the precise boundaries of current foundation model capabilities in grounded feedback, highlighting that while FMs excel in discrete, symbolic reasoning, they struggle with complex physical dynamics and continuous state-action spaces. By establishing a standardized taxonomy and evaluation framework, the paper provides a necessary benchmark for future research into automated reward shaping and agent advising. The findings suggest that while LLMs can effectively replace human annotators for specific discrete planning tasks, their application in continuous control requires fundamental advancements in model reasoning and inference strategies.

---

<details>
<summary>ðŸ“Š <b>Quick Facts Sidebar</b></summary>

| Metric | Detail |
| :--- | :--- |
| **Quality Score** | 8/10 |
| **Total Citations** | 40 |
| **Primary Models** | LLMs and VLMs (various sizes) |
| **Key Domains** | Symbolic, Text, Continuous Control |
| **Top Performance** | Binary Feedback in Symbolic ($1.00$) |
| **Lowest Performance** | Delta Action in Continuous ($0.03$) |

</details>

---

## Key Findings

*   **Capability & Scope:** Foundation models (LLMs and VLMs) are capable of generating diverse, high-quality feedback across symbolic, language, and continuous control environments.
*   **Model Size Matters:** Larger models designed for reasoning provide more accurate feedback with less bias and benefit significantly from advanced inference techniques.
*   **Complexity Barrier:** Feedback quality degrades significantly in environments with complex dynamics or continuous state and action spaces.
*   **Influence of Strategy:** The utility of foundation model feedback is significantly influenced by inference strategies such as chain-of-thought and in-context learning.

## Methodology

The study assessed LLMs and VLMs across three distinct grounded environments to evaluate their ability to serve as Feedback Models (FMs).

**1. Environments Evaluated:**
*   **Symbolic/Fully Observable:** Discrete states and actions (e.g., Cliff Walking, MiniGrid).
*   **Text/Partially Observable:** Language-based tasks requiring state tracking (e.g., HierarchyCraft, ALFWorld).
*   **Continuous Control:** High-dimensionality robotic tasks (e.g., Robomimic).

**2. Feedback Categories:**
The study evaluated five specific categories of planning feedback:
*   Binary Feedback
*   Preference Feedback
*   Action Advising
*   Goal Advising
*   Delta Action Feedback

**3. Inference Configuration:**
The analysis tested various configurations to determine impact on quality:
*   Comparison of In-Context Learning (ICL) vs. Chain-of-Thought (CoT) prompting.
*   Varying access to environment dynamics (Thinking Guides).

## Technical Details

**Framework Formalization**
The paper formalizes the feedback mechanism within a Markov Decision Process (MDP) defined as $M = \langle S, A, T, R, \gamma \rangle$.
*   **Feedback Models (FMs):** Utilize LLMs and VLMs.
*   **Input:** A snapshot $x$ consisting of the state, action, and instruction.
*   **Output:** Feedback $y$.
*   **Evaluation:** Comparison against a ground truth $y^*$ derived from an optimal policy.

**Feedback Taxonomy**
*   Binary Feedback
*   Action Advising
*   Preference Feedback
*   Goal Advising
*   Delta Action

**Experimental Setup**
*   **Domains:** Cliff Walking, MiniGrid, HierarchyCraft, ALFWorld, Robomimic.
*   **Datasets:** Constructed using Expert, Random, and Half-Expert policies.
*   **Inference Strategies:** In-Context Learning (ICL), Chain-of-Thought (CoT), Thinking Guides, and Domain Dynamics provision.

## Results

The primary metric for evaluation was **Feedback Accuracy** against the ground truth.

**Performance by Domain**

| Domain | Tasks | High Performance | Moderate/Low Performance |
| :--- | :--- | :--- | :--- |
| **Symbolic** (Cliff Walking) | Binary, Action Advising | **Binary:** $0.91 - 1.00$<br>**Action Advising:** $0.77 - 1.00$ | N/A |
| **Symbolic** (MiniGrid) | Binary, Preference | N/A | **Binary:** $0.32 - 0.42$<br>**Preference:** $0.59 - 0.66$ |
| **Text** (ALFWorld) | Binary, Preference | **Binary:** $0.60 - 0.83$ | **Preference:** Moderate |
| **Continuous** (Robomimic) | Binary, Action Advising, Delta | N/A | **Binary/Action:** $\approx 0$<br>**Delta Action:** $0.03 - 0.12$ |

**Model & Strategy Impact**
*   **Scale:** Larger models generally outperformed smaller ones (e.g., Llama 3.1 70B achieved $0.12$ vs 8B at $0.07$ for Delta Action).
*   **Reasoning:** Chain-of-Thought was established as a baseline, with additional improvements noted when using Thinking Guides and Domain Dynamics.

## Contributions

*   **Comprehensive Assessment:** Provided a broad evaluation of how well pretrained foundation models can serve as feedback providers, potentially reducing reliance on manually designed reward functions or annotated demonstrations.
*   **Structured Framework:** Established a structured framework by defining and testing a wide spectrum of feedback types and inference methodologies.
*   **Capability Boundaries:** Identified the boundaries of current model capabilities, highlighting their strength in discrete domains while noting challenges in environments with complex dynamics or continuous spaces.

---
**Quality Score:** 8/10 | **References:** 40 citations