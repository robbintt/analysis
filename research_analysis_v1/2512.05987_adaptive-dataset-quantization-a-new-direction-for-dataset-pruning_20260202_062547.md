# Adaptive Dataset Quantization: A New Direction for Dataset Pruning
*Chenyue Yu; Jianyu Yu*

---

### üìä Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Quality Score** | 8/10 |
| **Citations** | 31 |
| **Primary Datasets** | CIFAR-10, CIFAR-100, ImageNet-1K |
| **Target Hardware** | Resource-constrained Edge Devices |
| **Compression Target** | Up to 16x ratio (Float32 ‚Üí ~2-bit) |
| **Core Innovation** | Intra-sample redundancy reduction (Dataset Quantization) |

---

### üìù Executive Summary

This research addresses the critical challenge of storage and communication overhead associated with large-scale image datasets, particularly for deployment on resource-constrained edge devices. As dataset sizes grow, the costs of storing and transmitting high-precision data (e.g., float32) become prohibitive for environments with limited bandwidth and memory. While existing solutions like dataset pruning effectively reduce volume by eliminating less informative samples‚Äîaddressing **inter-sample redundancy**‚Äîthey leave the precision of the remaining images untouched. This approach overlooks substantial "intra-sample redundancy" (unnecessary detail within individual images), which represents a significant opportunity for further compression without sacrificing the semantic value required for effective model training.

The authors introduce **Adaptive Dataset Quantization (ADQ)**, marking the first application of quantization techniques specifically to dataset representation rather than model weights. The core innovation lies in shifting the focus from removing entire images to compressing the bit-depth of individual images based on their informational content. The method employs a non-learnable, two-stage pipeline: first, it applies linear symmetric quantization to each image to establish initial ranges, and second, it utilizes an adaptive quantization allocation algorithm. This algorithm dynamically distributes specific precision levels (bit-rates) across samples under a constant global compression constraint.

Extensive experiments on standard benchmarks‚Äî**CIFAR-10, CIFAR-100, and ImageNet-1K**‚Äîdemonstrate that ADQ effectively preserves model training performance while achieving substantial dataset compression. On CIFAR-10, the method maintained an accuracy of approximately **90.4%** even under a 16x compression ratio (reducing float32 to an average of 2-bit), compared to a 94.6% baseline. Similarly, on ImageNet-1K with ResNet-18, ADQ achieved **67.5%** accuracy at an 8x compression ratio (average 4-bit), against a 69.8% full-precision baseline‚Äîa significantly smaller degradation than experienced by pruning methods under similar storage constraints. This paper establishes a new research direction in efficient deep learning by distinguishing "dataset quantization" from traditional model compression.

---

## üîë Key Findings

*   **Reduced Intra-Sample Redundancy:** The proposed method effectively targets redundancy within individual images (intra-sample), significantly lowering storage and communication costs for large-scale datasets on edge devices.
*   **Performance Preservation:** Extensive experiments on CIFAR-10, CIFAR-100, and ImageNet-1K demonstrate that model training performance is maintained despite aggressive compression.
*   **Superiority Over Baselines:** The approach outperforms traditional quantization and dataset pruning baselines when operating under the same compression ratios.
*   **Adaptive Allocation:** By utilizing adaptive ratio allocation, the method optimizes bit distribution across samples, ensuring less informative content is compressed more aggressively than essential features.

---

## üèÜ Core Contributions

1.  **Novel Application:** It is the first work to utilize limited bits specifically for the representation of datasets to achieve storage reduction, distinguishing itself from model quantization.
2.  **Adaptive Algorithm:** The authors introduce a dataset-level quantization algorithm featuring adaptive ratio allocation, allowing for sample-specific precision handling.
3.  **Comprehensive Validation:** The study provides robust validation of the method's effectiveness through extensive testing on standard large-scale datasets (CIFAR-10, CIFAR-100, and ImageNet-1K).

---

## ‚öôÔ∏è Methodology

The proposed method shifts focus from traditional **inter-sample redundancy** (pruning/distillation) to **intra-sample redundancy**, compressing individual images by removing less informative content.

1.  **Linear Symmetric Quantization:** Applied per sample to determine an initial quantization range and scale.
2.  **Adaptive Quantization Allocation:** An algorithm is introduced to dynamically assign specific quantization ratios (precision levels) to different samples based on their varying precision requirements.
3.  **Global Constraint:** This allocation process operates under a constant total compression ratio constraint to ensure consistent storage reduction across the dataset.

---

## üîß Technical Details

**Core Concept**
*   **Intra-Sample Redundancy Reduction:** Targets redundancy within individual images by reducing precision of less informative content while preserving semantic features.

**Pipeline Architecture**
*   **Type:** Non-learnable preprocessing-only step.
*   **Optimization:** No trainable parameters or gradient loops required.
*   **Step 1:** Linear Symmetric Quantization calculates initial quantization range and scale per sample.
*   **Step 2:** Adaptive Quantization Allocation Algorithm dynamically distributes quantization ratios across samples while maintaining a constant global compression ratio.

**System Characteristics**
*   **Deployment:** Operates on resource-constrained edge devices.
*   **Precision:** Reduces data precision from float32 to lower bits.

---

## üìà Performance & Results

**Experimental Setup**
*   **Datasets:** CIFAR-10, CIFAR-100, ImageNet-1K.
*   **Baselines:** Traditional Quantization, Dataset Pruning (GraNd/EL2N, TDDS, CCS, Entropy, Forgetting, AUM), and Dataset Distillation.

**Outcome Highlights**
*   **Training Retention:** The method maintains model training performance effectively despite significant dataset compression.
*   **Benchmark Victory:** It outperforms both traditional quantization and dataset pruning baselines when operating under identical compression ratios.
*   **Efficiency:** The approach achieves storage and communication efficiency suitable for bandwidth-constrained environments.

**Notable Metrics**
*   **CIFAR-10:** ~90.4% accuracy at 16x compression (2-bit avg) vs 94.6% baseline.
*   **ImageNet-1K:** 67.5% accuracy at 8x compression (4-bit avg) vs 69.8% baseline.

---