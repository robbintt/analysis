---
title: 'MoRE: A Mixture of Low-Rank Experts for Adaptive Multi-Task Learning'
arxiv_id: '2505.22694'
source_url: https://arxiv.org/abs/2505.22694
generated_at: '2026-02-03T19:15:34'
quality_score: 7
citation_count: 14
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# MoRE: A Mixture of Low-Rank Experts for Adaptive Multi-Task Learning

*Dacao Zhang; Kun Zhang; Shimao Chu; Le Wu; Xin Li; Si Wei*

---

> ### **Quick Facts**
> *   **Quality Score:** 7/10
> *   **Citations:** 14
> *   **Model Architecture:** T5-base
> *   **Benchmarks:** GLUE (MRPC, RTE, SST-2, CoLA)
> *   **Key Innovation:** Adaptive Rank Selector
> *   **Inference Overhead:** Zero

---

## Executive Summary

Current Parameter-Efficient Fine-Tuning (PEFT) methods, particularly Low-Rank Adaptation (LoRA), face a critical limitation in multi-task learning scenarios due to the rigidity of fixed-rank configurations. Because different tasks possess varying levels of complexity, they require different model capacities; a single global rank inevitably leads to sub-optimal performance, causing underfitting for complex tasks or overfitting for simpler ones. This paper addresses this "multi-task gap" by demonstrating that static architectures cannot effectively handle the heterogeneous demands of diverse downstream tasks, necessitating a dynamic approach to parameter allocation that balances efficiency with the specific needs of each task.

The authors introduce **MoRE (Mixture of Low-Rank Experts)**, a novel framework that transforms the rank dimension of LoRA into a mixture-of-experts problem. Technically, MoRE treats different low-rank subspaces as distinct "experts" and employs an **Adaptive Rank Selector (router)** that utilizes task embeddings to make task-level routing decisions, dynamically identifying and activating the most appropriate rank for a given input. The architecture modifies Attention and FFN layers to utilize shared weight matrices (matrices A and B), which are sliced based on the routerâ€™s selection. Crucially, the authors emphasize that MoRE achieves these improvements with **zero inference overhead**, distinguishing it from other dynamic approaches; while the system manages multiple experts, the routing mechanism ensures that inference latency remains comparable to standard LoRA, effectively decoupling accuracy gains from computational slowdowns.

Evaluations demonstrate MoRE's robustness across various LLM architectures and benchmarks, with specific validation on T5-base using the GLUE benchmark. The data confirms that optimal rank requirements vary significantly across tasks: MRPC achieves peak performance at a low rank ($r=1$, 89.7 score), while CoLA requires a higher rank ($r=8$, 63.3 score) to maximize effectiveness. By dynamically adapting to these heterogeneous requirements, MoRE significantly outperforms standard LoRA in multi-task settings. The results further validate that aligning specific LoRA ranks with specific tasks as experts is more effective than the alternative approach of training separate modules for each task, achieving superior performance without the latency penalties typically associated with complex routing architectures.

This work represents a significant advancement in the efficiency and adaptability of Large Language Models, offering a practical solution for deploying single models across diverse task domains without the cost of full fine-tuning. By successfully integrating a Mixture-of-Experts architecture into low-rank adaptation, MoRE bridges the divide between rigid PEFT methods and the dynamic needs of multi-task learning. The decision to release the model and code amplifies this contribution, providing the research community with a robust tool for optimizing LLMs and establishing a new paradigm for dynamic, parameter-efficient fine-tuning that prioritizes both accuracy and inference speed.

---

## Key Findings

*   **Performance Improvement:** MoRE significantly improves the performance of Large Language Models in multi-task scenarios compared to standard LoRA.
*   **Zero Inference Overhead:** Despite the complex architecture, the method incurs zero inference overhead compared to standard LoRA.
*   **Task-Specific Alignment:** Confirming that aligning specific LoRA ranks with specific tasks as experts is more effective than training separate modules.
*   **Robustness:** Demonstrates effectiveness across various benchmarks and LLM architectures.

---

## Methodology

The authors propose **MoRE (Mixture of Low-Rank Experts)**, a Parameter-Efficient Fine-Tuning framework designed to address the limitations of static rank configurations. The approach consists of three core components:

1.  **Low-Rank Experts:** Treating different ranks of a LoRA module as distinct low-rank experts that are aligned with specific tasks.
2.  **Adaptive Rank Selector:** Utilizing a dynamic selector to identify and choose the appropriate expert (rank) for the current input.
3.  **Joint Training:** Employing joint training of the experts to enhance adaptability across the multi-task landscape.

---

## Technical Details

MoRE proposes a Mixture of Low-Rank Experts framework to address the rigidity of standard LoRA's fixed rank. The technical implementation is as follows:

*   **Dynamic Rank Assignment:** The system dynamically assigns specific ranks to tasks using **Task Embeddings** and an **Adaptive Rank Selector (Rank Expert)**.
*   **Architecture Modification:** The architecture modifies **Attention and FFN layers**, utilizing shared weights (matrices A and B) that are sliced based on the selected rank via a Router.
*   **Mathematical Foundation:** Mathematically, it builds on the standard LoRA equation ($h = W_0x + BAx$) but adjusts the rank dimension $r$ dynamically based on the routing decision.
*   **Training Techniques:** Training incorporates **Contrastive Learning** and **Balanced Data Sampling** to stabilize multi-task learning.
*   **Efficiency Claim:** The authors claim zero inference overhead compared to standard LoRA, as the routing mechanism does not add significant computational burden during the inference phase.

---

## Results

The model was evaluated on **T5-base** using **GLUE benchmark tasks** (MRPC, RTE, SST-2, CoLA). A LoRA rank sensitivity analysis revealed that optimal ranks vary significantly by task:

| Task | Optimal Rank ($r$) | Score |
| :--- | :---: | :---: |
| **MRPC** | $r=1$ | 89.7 |
| **RTE** | $r=4$ | 80.5 |
| **SST-2** | $r=4$ | 94.8 |
| **CoLA** | $r=8$ | 63.3 |

**Conclusion:** The findings indicate that a fixed rank is sub-optimal. MoRE significantly improves performance in multi-task scenarios by adapting to these varying rank requirements effectively.

---

## Contributions

*   **Bridging the Gap:** Addresses the multi-task gap in current PEFT research by moving beyond static architectures.
*   **Novel Architecture:** Introduces a novel mixture-of-experts architecture specifically tailored for low-rank adaptation.
*   **Efficiency Optimization:** Optimizes efficiency by decoupling performance improvements from computational overhead.
*   **Open Science:** Contributes to the community by releasing the model and code for public use.

---

**Quality Score:** 7/10 | **References:** 14 citations