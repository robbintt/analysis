# PROF: An LLM-based Reward Code Preference Optimization Framework for Offline Imitation Learning

*Shengjie Sun; Jiafei Lyu; Runze Liu; Mengbei Yan; Bo Liu; Deheng Ye; Xiu Li*

---

> ### ðŸ“Š Quick Facts
> *   **Quality Score:** 8/10
> *   **References:** 40 citations
> *   **Benchmark:** D4RL
> *   **Core Innovation:** Reward Preference Ranking (RPR) & TextGrad
> *   **Key Capability:** Fully automated reward function design without environment interaction

---

## Executive Summary

Offline Imitation Learning (IL) faces a critical bottleneck in reward function design, which traditionally requires either costly explicit annotations or relies on heuristic assumptions that correlate trajectory similarity with expert quality. These assumptions are often flawed, leading to reward hacking or oversimplification where the model fails to capture the true intent of the task. Furthermore, validating these reward functions typically necessitates active environment interaction via online reinforcement learning (RL), a process that is resource-intensive, unsafe, and fundamentally misaligned with the constraints of offline learning settings where data is scarce or interaction is prohibited.

The paper introduces **PROF**, a novel framework that leverages Large Language Models (LLMs) to automate the generation and optimization of executable reward code. The architecture operates through a three-stage pipeline:

1.  **Generation:** A zero-shot LLM generates multiple Python reward function candidates based on natural language descriptions and a single expert trajectory.
2.  **Evaluation:** A novel Reward Preference Ranking (RPR) strategy evaluates these candidates by calculating a "Dominance Score," which assesses how well a reward function distinguishes expert trajectories from unlabeled data without requiring an RL agent.
3.  **Optimization:** An iterative optimization loop utilizing TextGrad applies text-based gradient descent to refine the reward source code according to the RPR feedback.

PROF achieved state-of-the-art performance on the D4RL benchmark, consistently surpassing strong baselines such as Implicit Q-Learning (IQL) and Conservative Q-Learning (CQL). The framework demonstrated substantial improvements in normalized scores on complex manipulation tasks within the Franka Kitchen dataset and locomotion tasks in the Antmaze environment, where it often outperformed standard methods by significant margins.

The efficacy of the Reward Preference Ranking (RPR) was quantitatively validated by a high correlation between the offline Dominance Score and actual online policy performance, confirming that the system can successfully evaluate and rank reward functions entirely offline without environment interaction. This work represents a significant shift toward fully autonomous reward engineering within offline IL, decoupling reward validation from the environment and enabling the refinement of executable code through textual gradients.

---

## Key Findings

*   **State-of-the-Art Performance:** PROF achieves superior results on the D4RL benchmark, outperforming strong baselines in complex environments.
*   **Effective Evaluation:** The Reward Preference Ranking (RPR) strategy effectively evaluates reward function quality via dominance scores without needing environment interactions or RL training.
*   **Mitigation of Oversimplification:** The framework overcomes incorrect assumptions regarding the correlation between trajectory and expert similarity, thereby mitigating reward oversimplification.
*   **Full Automation:** PROF demonstrates the ability to fully automate the selection and refinement of optimal reward functions through iterative ranking and text-based gradient optimization.

---

## Methodology

The PROF framework operates through a structured pipeline designed to bypass the need for online interaction:

1.  **Code Generation:** Utilizes Large Language Models (LLMs) to generate executable reward function code. This is driven by natural language descriptions and a single expert trajectory.
2.  **Preference Ranking:** Implements the novel Reward Preference Ranking (RPR) strategy. This module ranks reward functions according to calculated dominance scores, indicating alignment with expert preferences without active environment interaction.
3.  **Iterative Refinement:** The system operates in a loop, alternating between RPR evaluation and text-based gradient optimization to refine reward function codes until an optimal selection is made.

---

## Technical Details

PROF is a three-stage framework for Offline Imitation Learning designed to automate reward function design without environment interaction.

| Module | Component | Functionality |
| :--- | :--- | :--- |
| **1** | **Reward Generation** | Uses a zero-shot LLM with general and task-specific prompts to generate *N* executable Python reward candidates. |
| **2** | **Reward Preference Ranking (RPR)** | Evaluates candidates based on expert trajectory dominance over unlabeled and noisy data using a Dominance Score. |
| **3** | **Iterative Optimization** | Employs **TextGrad**, a textual gradient descent method, to refine the reward code by computing loss and gradients in the text domain. |

---

## Contributions

*   **Framework Introduction:** Introduction of the PROF Framework, a novel LLM-based framework for Offline Imitation Learning that automates reward code generation without explicit annotations or complex similarity assumptions.
*   **New Metric Development:** Development of Reward Preference Ranking (RPR), a new metric and strategy for assessing reward function quality offline using dominance scores.
*   **Optimization Strategy:** Creation of an optimization strategy that combines preference ranking with text-based gradient updates to automate the refinement of reward functions for policy learning.

---

## Results

*   **Benchmark Performance:** PROF was evaluated on the D4RL benchmark, claiming to surpass strong baselines and achieve State-of-the-Art results.
*   **Metrics:** The framework uses a custom Dominance Score metric and standard D4RL normalized scores.
*   **Validation of RPR:** The RPR strategy showed high efficacy in evaluating rewards without RL training, validated by a strong correlation between offline scores and online performance.
*   **Robustness:** Key findings include the mitigation of reward oversimplification by rewarding diverse optimal behaviors and full automation of reward selection and refinement without human intervention.

---