# Baichuan-Audio: A Unified Framework for End-to-End Speech Interaction

*Tianpeng Li, Jun Liu, Tao Zhang, Yuanbo Fang, Da Pan, Mingrui Wang, Zheng Liang, Zehuan Li, Mingan Lin, Guosheng Dong, Jianhua Xu, Haoze Sun, Zenan Zhou, Weipeng Chen*

---

> ### **Quick Facts**
>
> *   **Quality Score:** 9/10
> *   **References:** 40 Citations
> *   **Tokenization Rate:** 12.5 tokens per second
> *   **Decoder Output Rate:** 100 tokens per second
> *   **Core Innovation:** Two-stage pre-training strategy preventing catastrophic forgetting
> *   **Architecture:** End-to-End Audio LLM (E2E Audio LLM)

---

## Executive Summary

This paper addresses the inherent inefficiencies and fragility of traditional cascaded speech interaction systems, which depend on disjointed pipelines of Automatic Speech Recognition (ASR), Large Language Models (LLMs), and Text-to-Speech (TTS). These modular architectures introduce significant processing latency and are susceptible to error accumulation across stages. Furthermore, the research highlights a critical barrier in developing Audio Large Language Models: **"catastrophic forgetting,"** a phenomenon where incorporating audio modalities causes the model to lose its pre-trained text-based reasoning and knowledge capabilities. The objective is to establish a unified, end-to-end framework that facilitates real-time spoken dialogue while preserving the sophisticated intelligence of modern LLMs.

The authors introduce **Baichuan-Audio**, a unified End-to-End Audio Large Language Model (E2E Audio LLM) that bypasses traditional ASR-TTS cascades by processing audio and text within a single transformer architecture. The system utilizes an advanced Audio Tokenizer comprising a Whisper Large Encoder and an 8-layer Residual Vector Quantization (RVQ) strategy to discretize speech at 12.5 Hz, capturing both semantic and acoustic details. A key technical innovation is the implementation of a **two-stage pre-training strategy** that decouples audio modeling from language preservation. By employing an independent audio head to handle modality-specific characteristics, this method allows the model to perform text-guided aligned speech generation via flow-matching decoders while effectively mitigating catastrophic forgetting.

Experimental outcomes demonstrate that the architecture successfully balances high-fidelity audio reconstruction with robust language understanding. Ablation studies confirmed that the 8-layer RVQ configuration outperforms the 6-layer baseline, achieving superior reconstruction fidelity with lower loss metrics. Crucially, the model retained its strong question-answering abilities post-training, validating the efficacy of the preservation strategy. The end-to-end pipeline supports efficient real-time dialogue, thereby minimizing the latency bottlenecks typically associated with cascaded systems.

Baichuan-Audio represents a significant advancement in the democratization of voice interaction technology, providing evidence that LLMs can be extended to handle full-duplex audio processing without sacrificing their core reasoning capabilities.

---

## Key Findings

*   **Superior Real-Time Dialogue:** The model achieves high performance in real-time spoken dialogue, effectively integrating comprehension and generation capabilities within a single unified framework.
*   **Preservation of LLM Capabilities:** Despite being trained on audio modalities, the model retains strong question-answering abilities, indicating that the pre-training strategy successfully prevents the loss of the original LLM's 'intelligence.'
*   **Effective Audio Representation:** The multi-codebook discretization approach successfully captures and retains both semantic and acoustic information within speech tokens, facilitating high-quality generation.

---

## Methodology

The research employs a novel structural and training approach to unify audio and text processing:

*   **Speech Discretization:** The framework utilizes a pre-trained ASR model followed by multi-codebook discretization of speech at a frame rate of **12.5 Hz**.
*   **Architecture:** An independent audio head is employed to specifically process audio tokens, allowing the model to capture the unique characteristics of audio data distinct from text.
*   **Text-Guided Generation:** The model employs a text-guided aligned speech generation mechanism to facilitate output, ensuring coherence between spoken words and generated speech.
*   **Two-Stage Pre-Training:** A novel two-stage training strategy is used to enhance audio modeling while simultaneously preserving the model's intrinsic language understanding capabilities.

---

## Technical Details

**Pipeline Overview**
Baichuan-Audio is an End-to-End Audio Large Language Model designed for unified, real-time speech interaction. The pipeline consists of four main components:
1.  Audio Tokenizer
2.  Audio LLM
3.  Flow-Matching Decoder
4.  Vocoder

**Baichuan-Audio-Tokenizer**
*   **Feature Extraction:** Utilizes a Whisper Large Encoder.
*   **Downsampling:** 4x downsampling residual convolutional network.
*   **Quantization:** 8-layer Residual Vector Quantization (RVQ).
*   **Codebook Sizes:** {8K, 4K, 2K, 1K, 1K, 1K, 1K, 1K}.

**Decoding & Synthesis**
*   **Decoder Architecture:** Symmetric to the encoder, featuring upsampling modules and a refinement network.
*   **Output Rate:** Produces Mel-spectrograms at **100 tokens per second**.
*   **Loss Functions:** Training involves L1, L2, and multi-scale Mel losses.
*   **Synthesis:** Relies on a flow-matching based decoder and a vocoder.

**Audio LLM Processing**
*   The model predicts text and audio tokens alternately using specialized modality-switching tokens.
*   This architecture preserves the original LLM's reasoning capabilities while processing audio.

---

## Results

Architecture ablation studies and performance evaluations yielded the following outcomes:

*   **Reconstruction Fidelity:** The 8-layer RVQ configuration outperformed the 6-layer version.
    *   *Metric 3:* 0.466 (8-layer) vs 0.485 (6-layer)
    *   *Metric 4:* 0.403 (8-layer) vs 0.423 (6-layer)
*   **Intelligence Retention:** The model successfully retains strong question-answering abilities without intelligence degradation.
*   **Latency Reduction:** The end-to-end architecture enables superior real-time dialogue performance by minimizing processing latency compared to cascaded systems.
*   **Balance of Capabilities:** The multi-codebook discretization approach effectively balances semantic understanding with acoustic detail generation.

---

## Contributions

*   **Unified End-to-End Framework:** Introduction of Baichuan-Audio, a seamless integration of audio understanding and generation within a single large language model, enabling real-time speech interaction without ASR-TTS cascades.
*   **Mitigation of Catastrophic Forgetting:** Proposal of a two-stage pre-training strategy that maintains the LLM's original language capabilities while incorporating new audio modeling proficiencies.
*   **Optimized Token Processing:** The development of a specific architecture using multi-codebook speech tokens and an independent audio head to handle the dual nature (semantic and acoustic) of speech information effectively.