---
title: 'Let''s Think in Two Steps: Mitigating Agreement Bias in MLLMs with Self-Grounded
  Verification'
arxiv_id: '2507.11662'
source_url: https://arxiv.org/abs/2507.11662
generated_at: '2026-02-03T12:58:58'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Let's Think in Two Steps: Mitigating Agreement Bias in MLLMs with Self-Grounded Verification

*Moises Andrade, Joonhyuk Cha, Brandon Ho, Vriksha Srihari, Karmesh Yadav, Zsolt Kira*

***

## üìã Executive Summary

This research addresses the critical problem of **"agreement bias"** in Multimodal Large Language Models (MLLMs) when deployed as verifiers for autonomous agents. In domains such as web navigation and robotics, MLLMs tend to over-validate agent behavior, exhibiting high false positive rates where they incorrectly certify failed trajectories as successful. This bias poses a significant barrier to the reliability of AI systems; since verifiers play a crucial role in self-correction loops, an inability to accurately detect failures prevents agents from learning from or correcting their errors, thereby limiting their deployment in high-stakes environments.

To mitigate this bias, the authors introduce **Self-Grounded Verification (SGV)**, a lightweight framework that fundamentally alters how MLLMs process verification tasks. Unlike standard methods that prompt the model simultaneously with the task description, context, and trajectory‚Äîwhich encourages the model to rationalize incorrect behavior‚ÄîSGV decouples the process into two distinct steps:

1.  **Prior Generation:** The MLLM generates broad behavioral priors based only on the initial task data and description, independent of the specific agent trajectory.
2.  **Conditional Evaluation:** The model evaluates the trajectory conditioned on these self-generated priors.

By anchoring the evaluation in the model's internal knowledge of the task before seeing the execution, SGV reduces the tendency to rationalize errors and aligns verification closer to the intended goals.

Empirically, SGV delivers substantial performance gains across VisualWebArena, OSWorld, and robomimic benchmarks. The method achieved up to a **25 percentage point improvement in failure detection** (True Negative Rate) and a **14 percentage point increase in overall verification accuracy**. Evaluations of various models demonstrated significant accuracy boosts, with specific instances rising from 60% to 74% and from 68% to 80%. Crucially, SGV outperformed standard interventions like Chain-of-Thought (CoT), which failed to mitigate bias and achieved a TNR of only 47%. These verification improvements translated directly to downstream utility, boosting task completion rates for agents by up to 20 percentage points, surpassing previous state-of-the-art results.

***

> ### üöÄ Quick Facts: Key Metrics
>
> *   **Quality Score:** 9/10
> *   **Citations:** 40
> *   **Max Failure Detection Improvement:** +25 pp (Percentage Points)
> *   **Max Accuracy Increase:** +14 pp
> *   **Downstream Task Completion Gain:** +20 pp
> *   **Primary Benchmarks:** VisualWebArena, OSWorld, robomimic
> *   **Top Model Improvement:** GPT-4.1 Mini (Accuracy: 60% $\to$ 74%)

***

## üîë Key Findings

*   **Identification of Agreement Bias:** The study identifies a critical flaw in Multimodal Large Language Models (MLLMs) where verifiers tend to over-validate agent behavior, leading to high false positive rates.
*   **Performance of SGV:** The proposed **Self-Grounded Verification (SGV)** method successfully mitigates this bias, resulting in a **25 percentage point improvement in failure detection** and a **14 pp increase in accuracy**.
*   **Downstream Gains:** Application of SGV leads to substantial performance boosts in real-world applications, increasing task completion rates for agents in OSWorld, robomimic, and VisualWebArena, surpassing previous bests by **20 pp**.

## üõ†Ô∏è Methodology

The authors propose **Self-Grounded Verification (SGV)**, a lightweight method designed to modulate the MLLM's sampling mechanisms to better leverage internal knowledge. The process reframes verification into a two-step architecture:

1.  **Prior Generation:** In this initial phase, the MLLM generates broad priors about the desired behavior based *only* on the initial task data. This step is performed independently of the specific trajectory or data being evaluated.
2.  **Conditional Evaluation:** In the second phase, the MLLM evaluates the candidate trajectory. Crucially, this evaluation is conditioned on the self-generated priors from Step 1, anchoring the judgment to the model's initial understanding of the goal.

## ‚öôÔ∏è Technical Details

The paper proposes **Self-Grounded Verification (SGV)** to address **Agreement Bias**, a specific failure mode where Multimodal LLMs (MLLMs) over-validate agent behavior.

### The Problem: Agreement Bias
*   **Definition:** A tendency for models to incorrectly certify failed trajectories as successful (High False Positive Rate).
*   **Cause:** Standard verification prompts the model with the task, trajectory, and context simultaneously, encouraging the model to rationalize incorrect behavior.

### The Solution: SGV Architecture
SGV employs a decoupled two-step architecture to separate task prior extraction from action evaluation:

*   **Step 1: Prior Extraction**
    *   The MLLM generates broad priors conditioned *only* on initial task data (e.g., initial screenshot) and the task description.
*   **Step 2: Conditioned Evaluation**
    *   The MLLM evaluates the full trajectory conditioned on the self-generated priors from Step 1.

### Evaluation Setup
*   **Benchmarks:** VisualWebArena, OSWorld, robomimic.
*   **Agents Tested:** ReAct, UI-TARS-1.5, Diffusion policy.
*   **Comparison:** Baseline techniques like Chain-of-Thought (CoT) were tested and failed to mitigate bias effectively (TNR of only 47%).

## üìä Results

SGV demonstrated significant improvements over baseline methods. While Chain-of-Thought (CoT) interventions struggled, SGV consistently outperformed standard interventions across accuracy, True Negative Rate (TNR), and bias metrics.

### Model Performance Breakdown

| Model | Metric | Baseline | SGV (Proposed) | Improvement |
| :--- | :--- | :--- | :--- | :--- |
| **Gemini 2.5 Flash** | Accuracy | 68% | 80% | +12 pp |
| | TNR | 55% | 76% | +21 pp |
| **GPT-4.1 Mini** | Accuracy | 60% | 74% | +14 pp |
| | TNR | 40% | 65% | +25 pp |
| **GPT-5 (Thinking)** | Accuracy | 81% | 86% | +5 pp |
| | TNR | 78% | 87% | +9 pp |

## üìù Contributions

*   **Diagnostic Analysis:** Provided a comprehensive analysis of MLLMs as verifiers across domains like web navigation and robotics, highlighting the systemic risks of agreement bias.
*   **SGV Framework:** Introduced Self-Grounded Verification as a generalizable framework to improve the reliability of MLLM-based evaluators by decoupling behavioral prior generation from action evaluation.
*   **Benchmark Update:** Released an updated **VisualWebArena** benchmark featuring more human-aligned evaluators, high-fidelity environment parallelism, and speedups of over 10x.

***
**References:** 40 citations