---
title: 'LLMVoX: Autoregressive Streaming Text-to-Speech Model for Any LLM'
arxiv_id: '2503.04724'
source_url: https://arxiv.org/abs/2503.04724
generated_at: '2026-02-06T02:17:29'
quality_score: 8
citation_count: 9
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# LLMVoX: Autoregressive Streaming Text-to-Speech Model for Any LLM

*Sambal Shikhar; Mohammed Irfan Kurpath; Sahal Shaji Mullappilly; Jean Lahoud; Fahad Khan; Rao Muhammad Anwer; Salman Khan; Hisham Cholakkal*

---

### üìä Quick Facts

| **Metric** | **Detail** |
| :--- | :--- |
| **Parameters** | 30M (Lightweight footprint) |
| **WER (Word Error Rate)** | 3.70 (Lower than existing speech-enabled LLMs) |
| **Latency** | 475 milliseconds |
| **Speed Comparison** | 10√ó faster than Cascaded XTTS baseline |
| **Audio Quality** | UTMOS: 4.05 / GPT Score: 6.88 |
| **Generalization (Arabic CER)** | ~8% (after 1.5k hours synthetic data) |

---

## Executive Summary

Current approaches to integrating Large Language Models (LLMs) with Text-to-Speech (TTS) capabilities often face critical trade-offs between latency, accuracy, and linguistic integrity. Many existing speech-enabled LLMs suffer from text-speech misalignment, where the generated audio fails to accurately reflect the intended text, and frequently degrade the conversational quality of the underlying LLM. Furthermore, high computational costs and the need for extensive fine-tuning have hindered the development of efficient, streaming-capable systems that maintain the intelligence of the base model while adding high-quality voice output.

LLMVoX addresses these challenges through a **decoupled, LLM-agnostic architecture** that isolates speech synthesis from LLM reasoning. The system utilizes a decoder-only Transformer with a lightweight footprint of only 30M parameters, functioning as a plug-and-play module downstream of any LLM or Vision-Language Model (VLM). Technically, the model employs a multi-queue token streaming mechanism optimized for infinite-length dialogues and low-latency generation. It processes text via ByT5 tokenization and uses phonetic embeddings padded to align with speech tokens, while audio synthesis is handled by the WavTokenizer codec using single-layer Residual Vector Quantization (RVQ) at a rate of 40‚Äì75 tokens per second.

The proposed model demonstrates superior performance across several key metrics compared to state-of-the-art baselines like Llama-Omni and Moshi. LLMVoX achieved a Word Error Rate (WER) of 3.70 and a GPT Score of 6.88, indicating high intelligibility and naturalness, with a UTMOS score of 4.05. The system operates with an end-to-end latency of 475 milliseconds, making it 10√ó faster than the Cascaded XTTS baseline. Additionally, the model exhibited robust generalization capabilities; when adapted to Arabic using 1,500 hours of synthetic data, it achieved a Character Error Rate (CER) of approximately 8% without requiring architectural modifications.

The significance of LLMVoX lies in its ability to decouple voice synthesis from the LLM, thereby eliminating the need for computationally expensive multimodal training while preserving the linguistic capabilities of the backbone model. This efficient, low-parameter design significantly lowers the barrier to implementing high-quality streaming TTS in diverse applications. Crucially, the framework's flexibility allows for zero-training integration with Vision-Language Models, facilitating the creation of omni-models capable of processing visual and textual inputs to generate speech, thereby advancing the field toward more versatile and accessible multimodal AI systems.

---

## Key Findings

*   **Superior Accuracy:** LLMVoX achieves a significantly lower Word Error Rate (WER) compared to existing speech-enabled LLMs while maintaining comparable latency and speech quality scores.
*   **Linguistic Integrity:** The model fully preserves the linguistic capabilities of the underlying LLM, avoiding degradation in conversational quality.
*   **Efficiency:** It features an efficient architecture with a lightweight footprint of only 30M parameters.
*   **Robust Generalization:** The system demonstrates robust generalization to new languages, such as Arabic, achieving low Character Error Rates through dataset adaptation.
*   **Omni-Model Integration:** LLMVoX can be seamlessly integrated with Vision-Language Models to create omni-models without requiring additional multimodal training.

---

## Methodology

The methodology relies on a **decoupled architecture** that separates speech synthesis from LLM processing using a multi-queue token streaming system. It employs an **LLM-agnostic design** where the system is plug-and-play and does not modify the base LLM, treating it as a swappable backbone. Additionally, it utilizes a **streaming autoregressive approach** optimized for handling infinite-length dialogues with low latency.

---

## Technical Details

*   **Architecture:** Decoder-only Transformer acting as an autoregressive, streaming Text-to-Speech engine.
*   **Parameter Count:** 30M parameters.
*   **Design Philosophy:** LLM-agnostic, plug-and-play design that operates downstream of any LLM or VLM without requiring fine-tuning.
*   **Streaming Mechanism:** Multi-queue streaming mechanism for continuous generation.
*   **Text Processing:**
    *   Tokenization: Uses ByT5.
    *   Embeddings: Phonetic embeddings (256-dimensional) sourced from a pre-trained ByT5-based G2P model.
    *   Language Support: Supports 100+ languages without explicit conversion.
    *   Alignment: Text embeddings are padded to align with speech token lengths.
*   **Audio Synthesis:**
    *   Codec: WavTokenizer codec.
    *   Quantization: Single-layer Residual Vector Quantization (RVQ).
    *   Vocabulary: 4,096 entries.
    *   Rate: 40‚Äì75 tokens per second.
    *   Reconstruction: Audio reconstruction at 24 kHz.

---

## Contributions

*   **Mitigation of Text-Speech Misalignment:** The research addresses text-speech misalignment and linguistic performance degradation by proposing a decoupled, LLM-agnostic framework.
*   **Computational Efficiency:** It introduces a high-quality streaming TTS system with only 30M parameters, reducing computational barriers and fine-tuning requirements.
*   **Flexibility and Extensibility:** The study offers a versatile plug-and-play design that extends to various tasks and backbones, enabling zero-training multimodal integration with Vision-Language Models.

---

## Results

LLMVoX achieved a Word Error Rate (WER) of 3.70, a UTMOS score of 4.05, and a GPT Score of 6.88. The end-to-end latency is 475 milliseconds, and the model operates 10√ó faster than the Cascaded XTTS baseline.

**Comparative Analysis:**
*   Outperforms LLM-dependent models like Llama-Omni and Moshi in both WER and GPT Score.
*   Maintains low latency comparable to state-of-the-art baselines.

**Generalization Test:**
*   **Language:** Arabic.
*   **Training:** 1,500 hours of synthetic data.
*   **Architecture Changes:** None.
*   **Outcome:** Character Error Rate (CER) of approximately 8%.

---

### üìù Document Metadata
*   **Quality Score:** 8/10
*   **References:** 9 citations