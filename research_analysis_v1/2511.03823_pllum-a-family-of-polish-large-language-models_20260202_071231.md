# PLLuM: A Family of Polish Large Language Models

*Jan Kocoń; Maciej Piasecki; Arkadiusz Janz; Teddy Ferdinan; Łukasz Radliński; Bartłomiej Koptyra; Marcin Oleksy; Stanisław Woźniak; Paweł Walkowiak; Konrad Wojtasik; Julia Moska; Tomasz Naskręt; Bartosz Walkowiak; Mateusz Gniewkowski; Kamil Szyc; Dawid Motyka; Dawid Banach; Jonatan Dalasiński; Ewa Rudnicka; Bartłomiej Alberski; Tomasz Walkowiak; Aleksander Szczęsny; Maciej Markiewicz; Tomasz Bernaś; Hubert Mazur; Kamil Żyta; Mateusz Tykierko; Grzegorz Chodak; Tomasz Kajdanowicz; Przemysław Kazienko; Agnieszka Karlińska; Karolina Seweryn; Anna Kołos; Maciej Chrabąszcz; Katarzyna Lorenc; Aleksandra Krasnodębska; Artur Wilczek; Katarzyna Dziewulska; Paula Betscher; Zofia Cieślińska; Katarzyna Kowol; Daria Mikoś; Maciej Trzciński; Dawid Krutul; Marek Kozłowski; Sławomir Dadas; Rafał Poświata; Michał Perełkiewicz; Małgorzata Grębowiec; Maciej Kazuła; Marcin Białas; Roman Roszko; Danuta Roszko; Jurgita Vaičenonienė; Andrius Utka; Paweł Levchuk; Paweł Kowalski; Irena Prawdzic-Jankowska; Maciej Ogrodniczuk; Monika Borys; Anna Bulińska; Wiktoria Gumienna; Witold Kieraś; Dorota Komosińska; Katarzyna Krasnowska-Kieraś; Łukasz Kobyliński; Martyna Lewandowska; Marek Łaziński; Mikołaj Łątkowski; Dawid Mastalerz; Beata Milewicz; Agnieszka Anna Mykowiecka; Angelika Peljak-Łapińska; Sandra Penno; Zuzanna Przybysz; Michał Rudolf; Piotr Rybak; Karolina Saputa; Aleksandra Tomaszewska; Aleksander Wawer; Marcin Woliński; Joanna Wołoszyn; Alina Wróblewska; Bartosz Żuk; Filip Żarnecki; Konrad Kaczyński; Anna Cichosz; Zuzanna Deckert; Monika Garnys; Izabela Grabarczyk; Wojciech Janowski; Sylwia Karasińska; Aleksandra Kujawiak; Piotr Misztela; Maria Szymańska; Karolina Walkusz; Igor Siek; Jakub Kwiatkowski; Piotr Pęzik*

> ### ⚡ Quick Facts
> *   **Model Sizes:** 7B and 13B parameters
> *   **Pretraining Corpus:** 140 billion tokens (Polish-centric)
> *   **Instruction Dataset:** 77k instructions (PLLuMIC)
> *   **Preference Dataset:** 100k pairs
> *   **Quality Score:** 8/10
> *   **References:** 22 citations
> *   **Key Feature:** Sovereign AI for Polish language with hybrid safety module

---

## Executive Summary

The rapid advancement of Large Language Models (LLMs) has predominantly favored high-resource languages, leaving Slavic languages like Polish with a scarcity of robust, open-source alternatives. Reliance on English-centric proprietary models presents significant challenges regarding digital sovereignty, cultural relevance, and data privacy. This paper addresses this deficiency by establishing a comprehensive NLP infrastructure to support the development of foundation models specifically tailored to Polish linguistic and legal nuances, reducing dependency on foreign commercial entities.

The authors introduce **PLLuM**, a family of **7B and 13B** parameter models developed through a consortium of Polish research institutions. The technical architecture is supported by a large-scale data pipeline featuring a **140-billion-token Polish corpus**, supplemented by specialized datasets: **PLLuMIC** (77k instructions classified as Organic, Synthetic, and Converted) and a **100k-pair preference optimization dataset**. The methodology compares "Training from Scratch" against "Continual Pre-training," utilizing **Vocabulary Extension** to optimize for Polish morphology. Additionally, the architecture integrates a hybrid safety module for output correction and filtering, governed by a Responsible AI framework using structured human feedback and copyright categorization.

Evaluations utilized perplexity metrics and ablation studies to validate the training pipeline. Experimental results confirmed that specific optimization techniques—including **learning rate annealing** and **Example Packing** for Instruction Fine-Tuning (IFT)—significantly enhanced model performance compared to baseline strategies. The study successfully processed the full pretraining corpus and alignment datasets, with fine-tuned variants demonstrating quantifiable utility in downstream tasks. Human annotation quality metrics derived from structured feedback procedures validated the model's efficacy, particularly as a "Public Administration Assistant." Furthermore, the authors provided specific legal compliance metrics by categorizing data into Scientific Research and Open Model works to adhere to Polish and EU regulations.

This research advances the field of Sovereign AI by providing a transparent, **open-weights** LLM family for a linguistically underrepresented region. By releasing extensive linguistic resources and a rigorous Responsible AI framework, the authors have enriched the available NLP infrastructure for the Polish language. This work establishes a reproducible blueprint for other nations aiming to develop culturally relevant AI infrastructure, facilitating national digital independence and offering a state-of-the-art alternative to English-centric models.

---

## Key Findings

*   **Introduction of PLLuM:** The largest open-source family of foundation models tailored specifically for the Polish language.
*   **Data Infrastructure:** Development of comprehensive resources, including a **140-billion-token Polish corpus**, a **77k custom instruction dataset (PLLuMIC)**, and a **100k preference optimization dataset**.
*   **Responsible AI Framework:** Implementation of a framework featuring strict data governance and a hybrid module for safety filtering.
*   **Practical Utility:** Demonstration of the model's effectiveness in downstream tasks, specifically within public administration sectors.
*   **Sovereign AI Promotion:** Promotion of digital sovereignty by providing a transparent, culturally relevant alternative to English-centric models.

---

## Technical Details

### Data Infrastructure & Processing
*   **Corpus Composition:** Massive infrastructure tailored for Polish (supplemented with English and Baltic/Slavic data).
*   **Preprocessing Pipeline:** Includes rigorous legal analysis, metadata extraction, corpus cleaning, and quality control.
*   **Dataset Typologies:**
    *   **PLLuMIC Instructions:** Categorized into Organic, Synthetic, and Converted types.
    *   **Preference Pairs:** Custom dataset designed for alignment.

### Training Methodology
*   **Strategy Comparison:** Analyzed "Training from Scratch" vs. "Continual Pre-training."
*   **Language Optimization:** Implemented "Vocabulary Extension" specifically for Polish morphology.
*   **Optimization Techniques:**
    *   Learning rate annealing.
    *   **Example Packing** for Instruction Fine-Tuning (IFT).
    *   Analysis of data repetition effects.

### Architecture & Safety
*   **Hybrid Safety Module:** Integrated for real-time output correction and safety filtering.
*   **Responsible AI:** Combines strict data governance, structured human feedback, and legal adherence.
*   **Specialized Tuning:** Features specific optimization for a "Public Administration Assistant" use case.

---

## Methodology

The research relied on extensive data engineering to construct large-scale proprietary linguistic datasets. This involved the development of both base and instruction-tuned model variants. The methodology incorporated advanced alignment techniques using a custom preference optimization dataset. A defining characteristic was the integration of a hybrid module for output correction and safety filtering within a Responsible AI framework. The project utilized a consortium-driven development model, pooling expertise from major Polish research institutions to ensure robustness and cultural relevance.

---

## Contributions

*   **NLP Resource Enrichment:** Significant contribution to low-resource languages by releasing a massive Polish corpus and specialized alignment datasets.
*   **Open-Source Sovereignty:** Provision of a fully open-source, state-of-the-art language model that supports national digital sovereignty.
*   **Ethical Blueprint:** Introduction of a rigorous Responsible AI framework that combines data governance with technical safety mechanisms.
*   **Sector Validation:** Validation of LLM applicability in public sector scenarios, specifically demonstrating feasibility in public administration tasks.

---

## Results & Evaluation

### Quantitative Metrics
*   **Pretraining Corpus:** 140 billion tokens focused on the Polish language.
*   **Instruction Set:** PLLuMIC containing 77k instructions.
*   **Optimization Set:** 100k pairs for preference tuning.

### Evaluation Framework
*   **Base Model Quality:** Assessed using perplexity metrics.
*   **Ablation Studies:** Conducted to measure the specific impact of example packing and data repetition strategies.

### Downstream & Compliance
*   **Human Annotation:** Quality metrics derived from structured feedback procedures confirmed high efficacy in tasks like the "Public Administration Assistant."
*   **Legal Compliance:** Data categorized by copyright status (Scientific Research vs. Open Model works) to ensure adherence to Polish and EU regulations.

---

**Quality Score:** 8/10 | **References:** 22 citations