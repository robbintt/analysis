---
title: 'HAM: Hierarchical Adapter Merging for Scalable Continual Learning'
arxiv_id: '2509.13211'
source_url: https://arxiv.org/abs/2509.13211
generated_at: '2026-02-03T18:31:53'
quality_score: 8
citation_count: 24
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# HAM: Hierarchical Adapter Merging for Scalable Continual Learning

*Eric Nuertey Coleman; Luigi Quarantiello; Samrat Mukherjee; Julio Hurtado; Vincenzo Lomonaco*

---

> ### **Quick Facts: Key Metrics**
>
> *   **Framework Type:** Parameter-Efficient Fine-Tuning (PEFT) / Continual Learning
> *   **Core Mechanism:** Hierarchical Consolidation of LoRA Adapters
> *   **CIFAR-100 (20 Tasks):** **57.3%** Avg Accuracy (vs. L2P: 51.4%)
> *   **TinyImageNet:** **59.6%** Accuracy (vs. AdapterPooling: 48.3%)
> *   **Parameter Budget:** Fixed at ~**0.8M** parameters (prevents linear growth)
> *   **Inference Requirement:** Task-agnostic (No task identifiers needed)

---

## Executive Summary

Current Parameter-Efficient Fine-Tuning (PEFT) methods face critical scalability limitations in class-incremental continual learning. As the number of tasks grows, standard approaches typically require maintaining a separate adapter for each task, resulting in a linear increase in memory footprint and computational complexity. Furthermore, many existing methods rely on task identifiers during inference, rendering them unsuitable for dynamic, real-world scenarios where task boundaries are undefined. Addressing this bottleneck is essential for developing AI systems capable of learning continuously over long sequences without suffering from prohibitive storage costs or catastrophic forgetting.

The authors introduce **HAM (Hierarchical Adapter Merging)**, a novel framework that replaces static, per-task adapters with a dynamic, consolidation-driven mechanism. HAM utilizes Low-Rank Adaptation (LoRA) modules paired with learnable importance scaling factors. The core technical innovation is the **Hierarchical Consolidation** process: after training a new adapter, the system performs Dynamic Grouping by calculating the cosine similarity against existing groups. Adapters are then pruned, scaled, and merged into a fixed set of consolidated group adapters rather than remaining isolated. At inference, all group adapters are globally merged into a single unified module using equal weights scaled by their learned importance factors ($W_{final} = W_0 + \Delta W_{merged}$), enabling task-agnostic inference without complex routing mechanisms.

HAM establishes a new state-of-the-art on vision benchmarks, demonstrating superior performance as the number of tasks increases. Crucially, while competing methods suffer from linear parameter growth, HAM maintains a **fixed parameter budget** regardless of task count, successfully preventing parameter explosion while maintaining high accuracy.

---

## Key Findings

*   **Superior Scalability:** HAM significantly outperforms state-of-the-art methods on vision benchmarks, with the performance gap widening as the number of tasks increases.
*   **Dynamic Learning:** The framework effectively scales to dynamic learning scenarios and long sequences of tasks, unlike standard Parameter-Efficient Fine-Tuning (PEFT) methods.
*   **Enhanced Efficiency:** HAM manages more tasks than competing baselines with improved efficiency by avoiding the need to maintain a separate adapter for every single task.
*   **Positive Transfer:** By grouping related tasks and merging their adapters, HAM facilitates positive transfer learning while mitigating the potential for interference between tasks.

---

## Methodology

The HAM (Hierarchical Adapters Merging) framework addresses the limitations of static adapter-based fine-tuning through a dynamic, consolidation-driven process. The methodology consists of four distinct stages:

1.  **Component Training:** Involves training a low-rank adapter and an importance scalar for each new task.
2.  **Dynamic Grouping:** Tasks are grouped based on adapter similarity to identify relationships between new and existing knowledge.
3.  **Hierarchical Consolidation:** Adapters within groups are pruned, scaled, and merged to maintain a fixed set of parameters, preventing unbounded growth.
4.  **Knowledge Integration:** The system dynamically combines adapters to consolidate knowledge, ensuring the model retains previously learned information.

---

## Technical Details

**Architecture & Components**
HAM is a Parameter-Efficient Fine-Tuning (PEFT) framework designed for class-incremental continual learning. It utilizes a hierarchical structure to merge Low-Rank Adaptation (LoRA) modules, enabling learning without task identifiers at inference.
*   **Base Model:** Frozen pre-trained model.
*   **Adapters:** Task-specific LoRA adapters defined as $\Delta W_i = B_i A_i$.
*   **Scaling:** Learnable importance scaling factors ($\alpha$).
*   **Consolidated Groups:** Consolidated group adapters ($\Delta W_G_j$).

**Training Process**
The training involves two primary phases:
1.  **Optimization:** Optimizing the new adapter and its importance factor while simultaneously updating the importance factors of frozen previous group adapters to re-balance knowledge.
2.  **Grouping & Pruning:** Calculating cosine similarity (using the vectorized last LoRA layer) against existing groups. If a similarity threshold is met, the adapter is pruned and concatenated to the selected group.

**Inference Mechanism**
During inference, all group adapters are globally merged into a single unified module.
*   **Weighting:** Equal weights scaled by learned importance factors.
*   **Final Formula:** $W_{final} = W_0 + \Delta W_{merged}$.
*   **Rank Expansion:** The merged adapter features an expanded rank to handle long task sequences effectively.

---

## Results

HAM demonstrates robust performance and efficiency across several key metrics:

*   **Benchmark Performance:** On the CIFAR-100 benchmark across 20 incremental tasks, HAM achieves an average accuracy of **57.3%**, significantly outperforming strong baselines like L2P (51.4%) and DualPrompt (49.8%).
*   **TinyImageNet Gains:** Similar gains were observed on TinyImageNet, where HAM reached **59.6%** accuracy compared to **48.3%** for AdapterPooling.
*   **Parameter Efficiency:** Unlike baseline methods (e.g., Wu et al.) that suffer from linear parameter growth, HAM maintains a **fixed parameter budget** (approximately 0.8M parameters) regardless of the task count.
*   **Unified Inference:** The framework produces a unified model capable of inference across all tasks without requiring task identifiers or complex Mixture-of-Experts (MoE) routing mechanisms.

---

## Contributions

*   **New Framework:** Introduction of a new continual learning framework (HAM) designed to handle the scalability limitations of current PEFT methods.
*   **Hierarchical Consolidation:** A unique mechanism that transitions from maintaining isolated adapters per task to a system of fixed groups that hierarchically consolidate new knowledge via adapter merging.
*   **Experimental Validation:** Comprehensive experimental evidence demonstrating that dynamic adapter merging is a viable solution for catastrophic forgetting and efficiency, establishing a new state-of-the-art for vision benchmarks in high-task scenarios.

---

**Quality Score:** 8/10  
**References:** 24 citations