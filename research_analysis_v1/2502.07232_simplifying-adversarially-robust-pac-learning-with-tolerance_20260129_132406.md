# Simplifying Adversarially Robust PAC Learning with Tolerance

*Hassan Ashtiani; Vinayak Pathak; Ruth Urner*

***

> ### **Quick Facts**
> *   **Core Focus:** Tolerant Adversarial PAC Learning
> *   **Primary Result:** Linear sample complexity in VC-dimension (eliminating exponential bounds)
> *   **Method:** Improper / "Almost Proper" Learning
> *   **Key Innovation:** Simplified algorithm using reference perturbation sets without complex compression schemes
> *   **References:** 21 citations

***

## Executive Summary

This research addresses the fundamental challenge of sample complexity in adversarially robust PAC learning, specifically targeting the "curse of dimensionality" that plagues previous theoretical frameworks. Prior work established that robust sample complexity often scales exponentially with the VC-dimension of the hypothesis class, rendering generalization guarantees infeasible for complex models.

The paper investigates whether this barrier can be overcome in a "tolerant" setting—where the learner approximates the optimal hypothesis within a tolerance parameter $\beta$—without relying on the restrictive structural assumptions or intricate compression schemes that limited earlier approaches. The key innovation is a theoretical reduction that transforms the robust learning problem over a hypothesis class $H$ and perturbation set $U$ into a standard Empirical Risk Minimization (ERM) problem over a "majority-vote" hypothesis class.

Instead of a sequential refinement process, the algorithm utilizes a larger reference perturbation set $V$ (where $U \subseteq V$) to partition the input space into a finite number of distinct "types" or regions. This construction allows the learner to identify a robust hypothesis by performing ERM on a composite class defined by majority votes over neighborhoods in $H$.

The authors prove that their methodology achieves a sample complexity linear in the VC-dimension $VC(H)$, eliminating the exponential bounds previously established by Montasser et al. (2019). Furthermore, the paper establishes a negative result proving that no proper learner exists for this setting, even for hypothesis classes with a VC-dimension as low as 1.

## Key Findings

*   **Linear Sample Complexity:** A learner for adversarially robust learning with tolerance achieving sample complexity linear in VC-dimension, improving over previous exponential bounds.
*   **"Almost Proper" Learning:** An improper learner that outputs hypotheses similar to the target hypothesis class $H$, maintaining structural proximity without being strictly proper.
*   **No Additional Assumptions:** Results achieved without extra assumptions on the hypothesis class $H$, removing reliance on constraints like regularity.
*   **Simplified Semi-Supervised Learning:** A semi-supervised learner for the tolerant setting with bounds comparable to non-tolerant algorithms but without intricate subroutines.

## Methodology

The authors construct an improper learner for the 'tolerant' version of adversarially robust PAC learning. The approach prioritizes algorithmic simplicity over complex compression schemes.

1.  **Core Concept:** Leveraging 'almost proper' learning where the output hypothesis diverges only slightly from the original class $H$.
2.  **Reduction:** Transforming the robust learning problem into a standard ERM problem via a majority-vote hypothesis class.
3.  **Adaptation:** These core ideas are adapted to the semi-supervised learning setting to extend robustness capabilities.

## Technical Details

### Framework Definition
*   **Tolerant Adversarial PAC Learning:** Uses a tolerance parameter $\beta$ and a reference perturbation set $V$ larger than the actual set $U$.
*   **Objective:** Minimize expected adversarial loss with respect to $U$ while competing with the optimal error achievable with respect to $V$.

### Algorithm Structure
*   **Type:** Improper learner (Two-stage approach).
*   **Stage 1:** Runs Robust Empirical Risk Minimization (RERM) with an oracle $A^V_H$ on the reference perturbation $V$ to identify a base hypothesis.
*   **Stage 2:** Generates a smoothed hypothesis via majority vote of the base hypothesis over neighborhoods defined by the actual perturbation set $U$.

### Analysis & Bounds
*   **Perturbation Types:** Relies on finite perturbation types where $|C(x)| \le k$.
*   **VC Dimension Bound:** $VC(H^C) \le VC(H) + \log(k)$.
*   **Complexity:** The algorithm achieves sample complexity that is linear in $VC(H)$, $\log(1/\beta)$, and the ambient dimension $d$.

## Contributions

*   **Theoretical Efficiency:** Resolves a major open question by proving the existence of a learner attaining sample complexity linear in the VC-dimension, eliminating the 'curse of dimensionality'.
*   **Algorithmic Simplification:** Reduces learning complexity by removing reliance on intricate compression schemes and additional hypothesis class constraints.
*   **Advancement in Semi-Supervised Robustness:** Extends 'almost proper' learning to semi-supervised scenarios, showing high-performance tolerant learning is possible with simpler structures.

## Results

*   **Benchmark Improvement:** Improves upon previous exponential bounds in VC-dimension (e.g., Montasser et al., 2019).
*   **Parity without Complexity:** Matches the linear bounds of Ashtiani et al. [2023] without requiring complex subroutines or structural assumptions like regularity.
*   **Impossibility Proof:** Theorem 7 establishes the impossibility of proper learning, proving that for any radius $r$, dimension $d$, and tolerance $\beta$, there exists a hypothesis class with $VC(H)=1$ that is not properly tolerantly robustly PAC learnable.
