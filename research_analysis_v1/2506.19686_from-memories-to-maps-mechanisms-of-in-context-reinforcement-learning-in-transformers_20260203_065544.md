---
title: 'From Memories to Maps: Mechanisms of In-Context Reinforcement Learning in
  Transformers'
arxiv_id: '2506.19686'
source_url: https://arxiv.org/abs/2506.19686
generated_at: '2026-02-03T06:55:44'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# From Memories to Maps: Mechanisms of In-Context Reinforcement Learning in Transformers

*Ching Fang; Kanaka Rajan*

---

> ### **Quick Facts**
>
> *   **Quality Score:** 9/10
> *   **References:** 40 Citations
> *   **Model Architecture:** 3-layer Causal GPT-2 (512-dim embeddings)
> *   **Training Framework:** Decision-pretraining (Meta-learning)
> *   **Task Environments:** Gridworlds ($5 \times 5$) & Tree Mazes
> *   **Core Mechanism:** Memory as Computation (Key-Value Caching)

---

## Executive Summary

**Problem**
This research addresses the fundamental challenge of interpreting how Transformers achieve rapid adaptation in *in-context reinforcement learning* (ICRL)—the capacity to learn new tasks from a sequence of experiences without updating model parameters. While large language models have demonstrated emergent planning and decision-making capabilities, the internal mechanisms driving these behaviors remain largely a "black box." This lack of interpretability hinders the reliable deployment of such agents and obscures the potential parallels between artificial intelligence and biological cognition. The authors aim to deconstruct this process to determine exactly how an agent can build a functional understanding of an environment solely from episodic context, specifically looking for links to biological rapid adaptation.

**Innovation**
The authors utilize a "decision-pretraining" meta-learning framework, formulating ICRL as a supervised sequence modeling problem within a 3-layer GPT-2 style Transformer. A key methodological distinction is the use of planning tasks (Gridworlds and Tree Mazes) specifically designed to mimic rodent navigation behavior, grounding the experimental design in biological motivation. The central innovation is the discovery that the model constructs a "cognitive map" through *in-context structure learning* and *cross-context alignment*, successfully aligning internal representations across environments with different sensory stimuli but shared underlying structures. Mechanistically, the analysis reveals that the model utilizes "memory tokens" rather than standard architectural features to store intermediate computations. These tokens act as an active computational resource, allowing the model to transition from raw memory storage to a spatial, map-like representation.

**Results**
The Transformer architecture significantly outperformed established baselines, including Tabular Q-learning and Deep Q-Networks (DQN), on Average Max-Normalized Return. In $5 \times 5$ Gridworld tasks, the agent achieved near-maximal performance through one-shot learning and discovered shortcut paths in over 60% of simulations. In the more complex, hierarchical Tree Mazes, the model's return improved systematically as a function of context length. Mechanistic analysis demonstrated that the model employs a non-standard RL strategy—distinct from classical model-free or model-based planning—by retrieving cached computations from memory tokens at decision time. This mechanism enabled zero-shot generalization, allowing the agent to navigate novel environments by leveraging the abstract cognitive maps formed during in-context training.

**Impact**
This work has profound implications for both AI and neuroscience, offering a rigorous computational theory for rapid adaptation in artificial and natural systems. By establishing a conceptual link between the Transformer’s attention-based memory retrieval and the episodic memory functions of the brain's hippocampal-entorhinal system, the paper validates the use of Transformers as a viable model for biological cognition. Furthermore, it fundamentally redefines the role of memory in intelligent systems, shifting the paradigm from viewing memory as a static repository for raw experience to viewing it as a dynamic store of cached computations that enable flexible, adaptive behavior and the formation of cognitive maps.

---

## Key Findings

*   **Mechanism of Representation Learning:** The model supports representation learning through in-context structure learning and cross-context alignment, successfully aligning representations across environments that possess different sensory stimuli.
*   **Non-Standard RL Strategy:** The emergent reinforcement learning strategies do not align with standard interpretations of model-free or model-based planning.
*   **Computational Role of Memory:** In-context reinforcement learning is facilitated by caching intermediate computations within the model's memory tokens, which are subsequently retrieved at decision time.
*   **Biological Resemblance:** The internal representations developed by the transformer resemble computations associated with the hippocampal-entorhinal system in the brain, suggesting a biological parallel to the artificial mechanism.

---

## Methodology

The authors trained a Transformer architecture to perform in-context reinforcement learning on a distribution of planning tasks specifically designed to mimic rodent behavior. Following the training phase, the researchers conducted a detailed characterization of the learning algorithms and internal representations that emerged within the model.

---

## Technical Details

| Component | Specification |
| :--- | :--- |
| **Framework** | Decision-pretraining meta-learning (ICRL as supervised sequence modeling) |
| **Architecture** | Causal GPT-2 style transformer |
| **Layers & Dimensions** | 3 Layers; 512-dimensional embeddings |
| **Input Structure** | In-context dataset of RL transition tuples + Query state |
| **Memory Mechanism** | Context memory provided before query tokens for key-value caching |
| **State Encoding** | 10-dimensional random Gaussian vectors |
| **Task Suite** | **Gridworlds:** $5 \times 5$ spatially regular grids<br>**Tree Mazes:** Hierarchical, non-Euclidean branching structures |

---

## Results

*   **Performance:** The transformer significantly outperformed Tabular Q-learning and DQN baselines on Average Max-Normalized Return.
*   **Gridworlds:** The agent achieved near-maximal performance via one-shot learning and discovered shortcut paths in over **60%** of simulations.
*   **Tree Mazes:** Return improved as a function of context length.
*   **Mechanistic Analysis:** Findings revealed that the model supports representational alignment and uses memory as computation, resembling the hippocampal-entorhinal system.

---

## Contributions

*   **Mechanistic Hypothesis for Rapid Adaptation:** The paper provides a concrete mechanistic hypothesis explaining how rapid adaptation functions within in-context learning for both artificial and natural systems.
*   **Bridge between AI and Neuroscience:** It establishes a conceptual link between the Transformer's key-value architecture and episodic memory systems in the brain, validating the use of Transformers to study biological cognition.
*   **Redefinition of Memory as Computation:** The work characterizes memory not just as storage for raw experience, but as a computational resource that stores cached computations to enable flexible, adaptive behavior.