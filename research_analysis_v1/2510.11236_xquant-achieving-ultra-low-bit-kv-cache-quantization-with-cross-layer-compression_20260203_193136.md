---
title: 'XQuant: Achieving Ultra-Low Bit KV Cache Quantization with Cross-Layer Compression'
arxiv_id: '2510.11236'
source_url: https://arxiv.org/abs/2510.11236
generated_at: '2026-02-03T19:31:36'
quality_score: 7
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# XQuant: Achieving Ultra-Low Bit KV Cache Quantization with Cross-Layer Compression

*Haoqi Yang; Yao Yao; Zuchao Li; Baoyuan Qi; Guoming Liu; Hai Zhao*

---

> ### üìä Quick Facts
>
> *   **Equivalent Bit-Width:** Sub-1.4 bits
> *   **Training Required:** None (Training-free)
> *   **Key Innovation:** Cross-Layer Compression & Data-Free Calibration
> *   **Target Model Scale:** Validated on 30B+ parameter models
> *   **Max Memory Addressed:** ~180 GB KV Cache scenarios
> *   **Quality Score:** 7/10
> *   **References:** 40 Citations

---

## üìù Executive Summary

Large Language Models (LLMs) face a severe scalability bottleneck during inference due to the Key-Value (KV) cache, which grows linearly with sequence length. This memory expansion limits deployment on resource-constrained hardware; for example, processing long-text tasks with a 30-billion-parameter model can demand up to **180 GB** of memory solely for the KV cache. Existing solutions often struggle with a critical trade-off: aggressive compression typically triggers significant performance degradation, while maintaining accuracy requires computationally expensive re-training or fine-tuning processes.

The authors introduce **XQuant**, a training-free, plug-and-play framework designed to achieve ultra-low-bit KV cache quantization through two core technical innovations:

1.  **Data-Free Calibration:** Employs Relaxed-Constraint Mapping, which minimizes reconstruction errors by optimizing a calibration parameter ($\eta$) within the range [0, 0.5] to adjust scaling factors and zero-points, all without requiring external calibration data.
2.  **Cross-Layer Compression:** Exploits architectural redundancy by partitioning the network's layers into pairs. In this scheme, the higher layer retrieves the quantized Key and Value tensors directly from its paired lower layer, rather than storing separate full cache entries. Consequently, the system stores only the specific scaling factors and zero-points required for the higher layers to interpret this shared data, significantly maximizing compression rates.

XQuant successfully pushes the limits of quantization, achieving an equivalent bit-width of **sub-1.4 bits**. In benchmark evaluations, the framework surpassed current state-of-the-art methods on TruthfulQA and LongBench, effectively breaking through the performance degradation floor typically observed in standard 2-bit quantization methods. Ablation studies confirm that the constraint-relaxed mapping function is crucial to this success, delivering superior performance over standard affine quantization. By reducing the memory footprint for massive models, XQuant validates a path toward feasible long-context inference.

---

## üîë Key Findings

*   **Ultra-Low Bit Quantization:** XQuant successfully achieves KV cache quantization at an equivalent bit-width of **sub-1.4 bits**.
*   **Superior Performance:** The framework outperforms current state-of-the-art methods on **TruthfulQA** and **LongBench** benchmarks.
*   **Optimized Trade-off:** The method establishes a more favorable balance between memory efficiency and model accuracy.
*   **Effective Resource Management:** XQuant addresses KV cache growth during long-text tasks, facilitating deployment in resource-constrained environments.

---

## üõ†Ô∏è Methodology

The proposed XQuant framework is a **training-free** and **plug-and-play** system designed to compress the KV cache in Large Language Models. It relies on two specific innovations:

*   **Data-Free Calibration:** A computationally negligible calibration method that does not require external data.
*   **Cross-Layer Compression:** A technique that compresses the KV cache across different layers of the network to maximize reduction rates.

---

## ‚öôÔ∏è Technical Details

XQuant proposes a framework for ultra-low bit KV cache quantization using Data-Free Calibration and Cross-Layer KV Cache Compression. It builds upon standard affine quantization with the following specific mechanisms:

### Data-Free Calibration
This component minimizes reconstruction errors via **Relaxed-Constraint Mapping**.
*   Utilizes a calibration parameter $\eta \in [0, 0.5]$.
*   Shifts relaxation into scaling factors ($\hat{s}$) and zero-points ($\hat{z}$) to maintain hardware compatibility.

### Cross-Layer KV Cache Compression
This technique leverages layer similarity to reduce storage redundancy.
*   **Partitioning:** The cache is partitioned into pairs.
*   **Fetching:** The higher layer fetches the quantized cache from the lower layer.
*   **Storage Optimization:** The system stores only specific scaling factors and zero-points for the higher layers, rather than full cache entries.

---

## üìà Results

*   **Compression Rate:** Achieves KV cache quantization at an equivalent bit-width of sub-1.4 bits.
*   **Benchmark Performance:** Outperforms state-of-the-art methods on **TruthfulQA** and **LongBench**.
*   **Performance Floor:** Surpasses the typical 2-bit performance degradation floor, offering a better balance between memory efficiency and accuracy.
*   **Resource Impact:** Significantly reduces memory requirements for large models (e.g., prevents 180 GB usage for 30B models).
*   **Ablation Studies:** Validate that the constraint-relaxed mapping function improves performance over standard quantization.

---

## üèÜ Contributions

*   **Extreme Compression Limits:** The framework pushes the boundaries of KV cache quantization to sub-1.4 bits.
*   **Training-free Efficiency:** It offers a practical, plug-and-play solution that eliminates the need for costly re-training or fine-tuning.
*   **Novel Compression Techniques:** The introduction of cross-layer KV cache compression and data-free calibration provides new architectural strategies for managing LLM memory footprints.