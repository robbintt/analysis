# Deep Learning-based Code Completion: On the Impact on Performance of Contextual Information

*Matteo Ciniselli; Luca Pascarella; Gabriele Bavota*

---

### ðŸ“Š Quick Facts

| Metric | Value |
| :--- | :--- |
| **Max Performance Gain** | +22% relative improvement |
| **Context Categories** | 3 Types (Coding, Process, Developer) |
| **Variables Tested** | 8 distinct context types |
| **Confidence Threshold** | 0.5 |
| **Quality Score** | 9/10 |

---

## Executive Summary

Current Deep Learning (DL) models for code completion primarily rely on immediate source code tokensâ€”essentially predicting the next word based on the current lineâ€”while often ignoring the broader software engineering environment. This limitation creates a semantic blind spot, as developers write code within a complex context of structural dependencies, active project issues, and personal coding habits.

This paper addresses the critical need to understand how augmenting DL models with heterogeneous external data affects prediction accuracy. The key innovation is a comprehensive contextual framework that empirically evaluates code completion across three distinct dimensions:

1.  **Coding Context** (Structural components)
2.  **Process Context** (Project status derived from issue trackers)
3.  **Developer Context** (Historical data on frequent method invocations)

The empirical study demonstrates that integrating contextual information yields a relative performance improvement of up to **+22%** in the rate of correct predictions. By validating that external data streams are not just noise but valuable signal for accuracy, this research directs future development tools toward multi-modal architectures that consume the entire development ecosystem, effectively bridging the gap between statistical language modeling and real-world software development complexity.

---

## Key Findings

*   **Significant Accuracy Gains:** Providing additional contextual information enhances DL-based code completion performance, resulting in relative improvements of up to **+22%** in correct predictions.
*   **Context Categories Matter:** Beneficial context falls into three specific categories:
    *   **Coding Contexts:** Structurally related components (e.g., class definitions, method calls).
    *   **Process Contexts:** Project status and open issues.
    *   **Developer Contexts:** Developer habits and API usage history.
*   **Combinatorial Impact:** Experimenting with different combinations of 8 distinct context types yields measurable performance benefits, suggesting that a multifaceted approach to input data is superior to single-source inputs.

---

## Methodology

The authors conducted an empirical study designed to evaluate how various inputs affect the prediction capabilities of a DL-based code completion technique.

*   **Variable Manipulation:** The study involved experimenting with **8 different types of contexts** and their combinations to isolate performance impacts.
*   **Data Classification:** The input data was classified into three distinct groups:
    1.  **Coding Contexts:** Based on structural relationships within the code.
    2.  **Process Contexts:** Derived from project status and the textual representation of open issues.
    3.  **Developer Contexts:** User-specific data and API usage history.

---

## Technical Details

The study investigates augmenting Deep Learning-based code completion models based on the hypothesis that expanding the input scope improves prediction accuracy.

### Contextual Layers
The approach categorizes context into three distinct layers:

*   **Coding Context**
    *   Class definitions
    *   Method Call structures
    *   Most similar method
*   **Process Context**
    *   Issue title
    *   Issue body
*   **Developer Context**
    *   Developer's frequent method invocations

### Experimental Setup
*   **Feature Types:** 8 distinct context types applied individually or in combination.
*   **Evaluation Metric:** Correct predictions filtered by a **0.5 Confidence threshold**.
*   **Conditions:** Comparisons made against conditions labeled '10' and '1'.

---

## Results

The integration of contextual information resulted in a relative performance improvement of up to **22%** in the rate of correct predictions. The following quantitative breakdown illustrates the performance relative to the confidence threshold and experimental conditions:

| Context Type | Observed Performance Metrics |
| :--- | :--- |
| **Call** | 50%, 100% |
| **Class** | 0%, 0%, 0%, 50%, 100% |
| **Most Similar Method** | 0%, 50%, 100% |
| **Issue Title** | 0%, 0%, 50%, 100% |
| **Issue Body** | 0%, 50%, 100% |
| **Developer's Frequent Method Invocations** | 0%, 50%, 100% |

*Note: values represent correct prediction rates at specific experimental intervals/conditions relative to the confidence threshold.*

---

## Contributions

*   **Contextual Framework:** Provides a comprehensive framework that categorizes context in code completion into coding, process, and developer types, expanding beyond immediate code tokens.
*   **Empirical Evidence:** Offers quantitative proof that expanding the input context window with project and developer-specific information can yield accuracy gains of up to +22%.
*   **Design Guidance:** Guides future model design for coding assistants by identifying which external data streams are most valuable for prediction accuracy, shifting the focus from pure language models to context-aware software engineering assistants.

---
*References: 40 citations*