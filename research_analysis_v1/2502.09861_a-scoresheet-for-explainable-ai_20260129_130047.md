# A Scoresheet for Explainable AI

*Michael Winikoff; John Thangarajah; Sebastian Rodriguez*

---

> ### **Quick Facts**
> - **Quality Score:** 8/10
> - **Total Citations:** 40
> - **Core Innovation:** The XAI Scoresheet Framework
> - **Target Domain:** General AI & Multiagent Systems (MAS)
> - **Key Validation:** Tested against IEEE P7001 & 6 distinct architectures

---

## Executive Summary

This paper addresses the critical operational gap between high-level transparency standards—such as IEEE P7001—and the concrete technical requirements necessary to implement and evaluate Explainable AI (XAI). The authors argue that current mandates are too abstract and vague, posing a significant barrier to compliance and trust. Crucially, the research highlights that existing standards fail because they neglect the human element; they lack the mechanisms to translate broad policies into the specific, context-dependent requirements needed by diverse stakeholders. Without a stakeholder-centric approach, transparency efforts remain theoretically sound but practically ineffective, leaving developers without a clear path to demonstrate system adequacy to the users and assessors who rely on them.

The key innovation introduced is the **"Explainability Scoresheet,"** a structured framework designed to operationalize abstract transparency concepts through a rigorous stakeholder analysis methodology. The tool is built upon a design-oriented approach that mandates a pre-analysis phase to identify stakeholders, their goals, and specific risk parameters before determining system "adequacy." Technically, the scoresheet is categorized into four sections: Basic Information (metadata and access), Veracity (a metric distinguishing between direct derivation and proxy models), Global Explanations (system-wide logic and provenance), and Local Explanations (granular details like interactivity, confidence, and contrastive questions). This structure moves beyond rigid technical metrics to focus on stakeholder goals, enabling the definition of context-specific requirements rather than generic checklists.

The authors validated the scoresheet through a qualitative gap analysis against the IEEE P7001 standard and empirical testing across six diverse architectures: ChatGPT, Generative AI for medical images, a Robotic Planner, a BDI System, Multi-Agent Reinforcement Learning (MARL), and a hybrid taxi scheduler. The results confirmed the tool's versatility, demonstrating its ability to capture critical details often missed by general standards, such as the validation of explanation veracity, confidence indications, scope of validity, and support for contrastive questions.

Ultimately, this work provides a vital practical tool that bridges theoretical standards and engineering practice by embedding the human perspective into the evaluation process. By offering explicit operational guidance for the system development lifecycle, the scoresheet enables the definition of measurable requirements that are grounded in actual stakeholder needs rather than arbitrary technical specifications.

---

## Key Findings

*   **The Transparency Gap:** Current standards for system transparency are too high-level and fail to adequately specify concrete explainability requirements.
*   **Bridging Practice and Policy:** There is a practical gap between high-level transparency mandates and the specific technical needs of developers and assessors.
*   **Versatility of Application:** The developed scoresheet is versatile and applicable to a wide range of applications, extending beyond general AI to include Multiagent Systems.
*   **Empirical Validation:** Practical application of the scoresheet across various scenarios confirms its utility in both specifying requirements and assessing existing systems.
*   **Stakeholder-Centricity:** A stakeholder-centric approach is essential for defining meaningful explainability metrics, as the scoresheet accounts for a range of stakeholder requirements.

---

## Methodology

The authors employed a **design-oriented approach** centered on **stakeholder analysis**. The methodology consisted of three primary phases:

1.  **Requirement Analysis:** Analyzing the requirements of a diverse range of stakeholders to inform the tool's structure.
2.  **Framework Development:** Developing a 'scoresheet' framework for the dual purpose of requirement specification and system assessment.
3.  **Empirical Validation:** Validating the generality and usefulness of the tool by applying it empirically to a variety of application domains. This specifically verified compatibility with Multiagent Systems and other AI technologies.

---

## Technical Details

The paper introduces a structured **XAI Scoresheet** designed to bridge transparency standards and technical needs. The framework is organized into four distinct categories:

### 1. Basic Information
*   Includes metadata and code/data access details to establish the foundation of the system's transparency.

### 2. Veracity
*   A reliability metric assessed as **Low** or **High**.
*   Determines if explanations match actual reasoning.
*   Distinguishes between **direct derivation** (the system reasoning directly causes the explanation) and **proxy models** (explanations are approximated).

### 3. Global Explanations
*   Provides system-wide descriptions.
*   Covers system structure, functioning, and data provenance.

### 4. Local Explanations
*   Features include: **Interactivity**, **Confidence** levels, **Concepts**, and **Automation**.
*   Supports various question types, including **contrastive** questions ("Why this rather than that?").

### Operational Workflow
*   **Taxonomy Integration:** Incorporates Arya et al.'s taxonomy.
*   **Focus Area:** Prioritizes stakeholder goals and application risks over rigid stakeholder groups.
*   **Pre-analysis Phase:**
    *   Defines stakeholders and goals.
    *   Establishes risk parameters and acceptable risk levels.
    *   Determines overall system **"adequacy."**

---

## Results

The scoresheet was validated qualitatively using **six distinct systems**:

1.  ChatGPT (travel recommendations)
2.  Generative AI (medical images)
3.  A Robotic Planner
4.  A BDI System
5.  Multi-Agent Reinforcement Learning (MARL)
6.  A Hybrid taxi scheduling system

### Validation Outcomes
*   **Versatility:** The results confirmed the scoresheet's versatility across diverse architectures, including symbolic planners and multi-agent systems.
*   **Gap Analysis (IEEE P7001):** A gap analysis demonstrated that the scoresheet captures critical details missing in the IEEE P7001 standard, specifically:
    *   Veracity validation
    *   Confidence indications
    *   Scope of validity
    *   Support for contrastive questions

### Key Metrics Referenced
*   **IEEE P7001 Transparency Scale:** Levels 0-5.
*   **Veracity Metric:** Low/High scale.

---

## Contributions

*   **The Explainability Scoresheet:** A concrete tool designed to bridge the gap between abstract high-level standards and practical implementation, allowing users to define and measure explainability.
*   **Operational Guidance:** The paper provides explicit instructions on how to utilize the scoresheet within the system development lifecycle.
*   **Cross-Domain Applicability:** The work establishes a framework that is applicable to Multiagent Systems as well as broader AI technologies, addressing a wide spectrum of autonomous and intelligent systems.