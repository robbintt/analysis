# Robustness as Architecture: Designing IQA Models to Withstand Adversarial Perturbations

*Igor Meleshin; Anna Chistyakova; Anastasia Antsiferova; Dmitriy Vatolin*

---

> ### üìä Quick Facts
>
> *   **Quality Score:** 6/10
> *   **References:** 8 citations
> *   **Key Metrics:** Adversarial Sensitivity, Perceptual Correlation
> *   **Core Innovation:** RobustBlocks with Fourier Domain Orthogonal Convolution
> *   **Primary Advantage:** Eliminates need for adversarial training

---

## üìù Executive Summary

Current Image Quality Assessment (IQA) models are fundamentally vulnerable to adversarial manipulations, where imperceptible input perturbations cause significant artificial inflation of quality scores. This reliability gap poses a critical risk for automated metrics in high-stakes applications. Traditional defense mechanisms, such as adversarial retraining and input purification, are increasingly viewed as inadequate solutions because they are data-dependent, computationally expensive, and fail to provide generalized security against sophisticated attacks.

The authors propose "Robustness as Architecture," a paradigm shift that engineers robustness through structural design rather than data-driven optimization. The core technical innovation is the "RobustBlock," which enforces orthogonal information flow and restricts the model to norm-preserving operations to suppress sensitivity to perturbations. Specifically, the method utilizes orthogonal convolution implemented in the Fourier domain‚Äîdefined as $y = \text{Orth.Conv}(\text{FFT}(X))$‚Äîto constrain spectral norms. This pipeline is stabilized through a process of applying RobustBlocks, training, pruning channels based on mean weight thresholds $\mu(w_i) < \tau$, and fine-tuning to recover perceptual performance.

The proposed architecture is evaluated using Adversarial Sensitivity (measuring reduction in score inflation) and Perceptual Correlation (ensuring alignment with human subjective quality). The results demonstrate that the architecture successfully constrains the maximum score inflation under perturbation $\delta$ to be strictly lower than that of original base models. Furthermore, the model exhibits stability against specific sophisticated attacks, including Universal Adversarial Perturbations (UAP) and FACPA, while maintaining high parameter efficiency without requiring adversarial training.

This research establishes a theoretical framework for designing inherently secure perceptual models, demonstrating that mathematical constraints on network architecture (orthogonality and norm-preservation) are more effective than dataset optimization. By proving that robustness can be engineered, the authors eliminate the reliance on unstable and resource-heavy adversarial retraining. This work influences the field by validating design-based defenses as a superior, scalable alternative for future IQA systems, encouraging a focus on architectural integrity over complex data augmentation strategies.

---

## üîç Key Findings

*   **Fundamental Vulnerability:** Current Image Quality Assessment (IQA) models possess a critical vulnerability where adversarial manipulations lead to artificially inflated quality scores.
*   **Architecture over Data:** Robustness in perceptual models is established more effectively through architectural design (structural changes) than through traditional data-driven defenses like adversarial retraining or input purification.
*   **Structural Stability:** Reshaping the internal structure of the network produces an IQA architecture capable of withstanding adversarial attacks.
*   **Efficiency:** The proposed robust architecture achieves stability without requiring adversarial training or significant alterations to the original model parameters.

---

## üõ†Ô∏è Methodology

The authors propose a framework based on the concept of **robustness as an architectural prior**, focusing on reshaping the model's internal structure rather than relying on dataset optimization.

*   **Core Philosophy:** Constraining the network to enforce orthogonal information flow and restricting the model to norm-preserving operations to suppress sensitivity to perturbations from the ground up.
*   **Stabilization Process:** The system is stabilized through the application of pruning and fine-tuning processes.

---

## üöÄ Contributions

*   **Paradigm Shift:** Introduces a shift from optimizing robustness through data (learning) to engineering robustness through design (structure).
*   **Mathematical Constraints:** Demonstrates that suppressing sensitivity to adversarial perturbations can be achieved by enforcing specific mathematical constraints (orthogonality and norm-preservation) within the network architecture.
*   **Efficient Defense:** Provides a defense mechanism that eliminates the reliance on computationally expensive and complex methods like adversarial retraining or input purification.

---

## üî¨ Technical Details

The proposed method utilizes design-based defenses, embedding robustness directly into the model structure.

### Core Components
*   **Structure:** Uses **'RobustBlocks'** to embed robustness.
*   **Operation:** Employs orthogonal convolution implemented in the Fourier domain: $y = \text{Orth.Conv}(\text{FFT}(X))$.
*   **Mechanism:** Constrains spectral norms to limit sensitivity to input changes and suppress adversarial amplification.

### Implementation Pipeline
1.  **RobustBlocks:** Apply the robust architectural blocks to the model.
2.  **Training:** Standard training procedures.
3.  **Pruning:** Remove channels based on mean weight thresholds $\mu(w_i) < \tau$.
4.  **Fine-tuning:** Recover perceptual performance post-pruning.

### Mathematical Objective
The defense objective is to construct a model $F(x)$ where the maximum score inflation under perturbation $\delta$ is strictly less than that of the original model $f(x)$.

---

## üìà Results

*   **Qualitative Findings:** The method achieves stability without adversarial training and maintains parameter efficiency.
*   **Resilience:** Claims theoretical superiority over regression data-driven methods against attacks such as:
    *   **UAP:** Universal Adversarial Perturbations.
    *   **FACPA:** Fast Attack on Perceptual Quality Assessment.
*   **Evaluation Metrics:**
    *   **Adversarial Sensitivity:** Measures the magnitude of score inflation (targeted to be lower than the base model).
    *   **Perceptual Correlation:** Ensures alignment with human subjective quality.

---

## üìö References

8 citations