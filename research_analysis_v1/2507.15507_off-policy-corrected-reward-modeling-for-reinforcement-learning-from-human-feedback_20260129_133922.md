# Off-Policy Corrected Reward Modeling for Reinforcement Learning from Human Feedback

*Johannes Ackermann; Takashi Ishida; Masashi Sugiyama*

---

> ### **Quick Facts: Key Metrics & Metadata**
>
> *   **Focus Area:** Reinforcement Learning from Human Feedback (RLHF)
> *   **Core Problem:** Distribution Shift & Overoptimization (Reward Hacking)
> *   **Proposed Solution:** Off-Policy Corrected Reward Modeling (OCRM)
> *   **Key Technique:** Importance Weighting
> *   **Training Duration:** Up to 600k steps
> *   **Evaluated Datasets:** Summarization, Chatbot datasets
> *   **Baseline:** Standard PPO-RLHF
> *   **Quality Score:** 8/10
> *   **References:** 40 citations

---

## Executive Summary

### The Problem: Overoptimization in RLHF
Reinforcement Learning from Human Feedback (RLHF) is critical for aligning large language models (LLMs), yet it suffers from **reward hacking** (overoptimization). This phenomenon stems from **distribution shift**: the reward model is trained on outputs from the initial Supervised Fine-Tuning (SFT) policy but is later used to evaluate outputs from a divergent Reinforcement Learning (RL) policy. This discrepancy causes **estimation inconsistency**, resulting in inflated reward scores for outputs that do not align with human preferences. Consequently, proxy metrics improve while actual performance degrades.

### The Innovation: Off-Policy Corrected Reward Modeling (OCRM)
The authors introduce **OCRM**, reframing overoptimization as an estimation problem rooted in distribution shift. Instead of standard reward modeling, OCRM modifies the objective function using **importance weighting** (derived from off-policy evaluation literature). The algorithm adjusts the reward model’s loss function by weighting samples based on the probability ratio between the current RL policy and the original SFT policy ($\pi_{\text{SFT}} / \pi_{\text{RL}}$). This dynamically compensates for distribution drift during training without requiring new human labels.

### Empirical Results: Stability and Accuracy
Validated against standard PPO-RLHF baselines on summarization and chatbot datasets, OCRM demonstrated superior stability over extended runs (up to **600,000 steps**). While standard PPO-RLHF exhibited classic overoptimization—where **RM Reward** rose sharply as **True Reward** plummeted—OCRM maintained a tighter alignment between these metrics. It consistently preserved higher **RM Accuracy** and True Reward scores, effectively mitigating performance collapse.

### Research Impact
This work offers both theoretical and practical contributions. Theoretically, it establishes a formal framework identifying overoptimization as a consequence of estimation inconsistency. Practically, OCRM provides a **label-efficient solution** that mitigates Goodhart’s Law effects without the cost of continuous human data collection, offering a scalable path toward robust alignment.

---

## Key Findings

*   **Root Cause Identification:** Overoptimization in RLHF is driven by distribution shift where the policy's output diverges from the reward model's training data.
*   **Consequence of Shift:** This divergence leads to inconsistent estimates of reward model parameters and policy gradients, causing rising reward scores that do not correlate with human preferences.
*   **The OCRM Solution:** The Off-Policy Corrected Reward Modeling (OCRM) algorithm effectively mitigates this issue using importance weighting techniques.
*   **Label Efficiency:** OCRM improves accuracy and performance without requiring additional human labels or samples.
*   **Empirical Superiority:** Results show OCRM outperforms standard RLHF methods on both summarization and chatbot datasets.

---

## Methodology

The authors employed a three-pronged approach to address and validate the problem of overoptimization:

1.  **Theoretical Analysis:** The problem was analyzed through the lens of distribution shift theory to understand the mechanics of overoptimization.
2.  **Algorithm Introduction:** The research introduced **OCRM**, an algorithm that iteratively uses importance weighting to correct the reward model based on the current policy's distribution.
3.  **Validation:** The approach was validated through rigorous experimentation on standard summarization and chatbot datasets, directly comparing the OCRM pipeline against standard RLHF methods.

---

## Contributions

The research contributes significantly to the field of AI alignment through the following:

*   **Theoretical Framework:** Provides a formal explanation of overoptimization as distribution shift causing estimation inconsistency.
*   **Novel Algorithm:** Introduces the Off-Policy Corrected Reward Modeling (OCRM) algorithm to correct reward models off-policy.
*   **Efficiency:** Offers a label-efficient solution that maintains accuracy without the need to collect new human feedback.
*   **Performance Gains:** Demonstrates significant empirical improvements in complex language tasks over current industry practices.

---

## Technical Details

**Off-Policy Corrected Reward Modeling (OCRM)** addresses the issue of "Goodharting" (overoptimization) by framing it as a distribution shift problem.

*   **The Distributions:** The Reward Model (RM) is initially trained on the Supervised Fine-Tuning (SFT) policy distribution ($\pi_{\text{SFT}}$). However, during RL optimization, it is applied to a diverged RL policy distribution ($\pi_{\text{RL}}$). This misalignment results in inaccurate rewards.
*   **The Correction:** OCRM corrects this shift using **importance weighting**, a technique adapted from off-policy evaluation literature.
*   **Implementation:** It functions as a modification to the training objective during the RL optimization phase. By mathematically compensating for the distribution drift, OCRM mitigates overoptimization without requiring additional human labels or sampling.

---

## Results

Evaluation was conducted on summarization and chatbot datasets using low-dimensional toy environments, with **PPO-RLHF** serving as the primary baseline.

*   **Metrics:** Key performance indicators included RM Reward, True Reward, and RM Accuracy.
*   **Baseline Performance:** Standard PPO-RLHF exhibited classic overoptimization. While RM Reward rose, both True Reward and RM Accuracy declined significantly.
*   **OCRM Performance:** In contrast, OCRM demonstrated a tighter alignment between RM Reward and True Reward.
*   **Long-Term Stability:** Over the course of 600k training steps, OCRM empirically outperformed standard RLHF by maintaining higher RM accuracy and True Reward scores relative to the proxy RM score.