# Continual Neural Topic Model
*Charu Karakkaparambil James; Waleed Mustafa; Marius Kloft; Sophie Fellenz*

---

> ### üìä Quick Facts
>
> | Metric | Details |
> | :--- | :--- |
> | **Quality Score** | 9/10 |
> | **References** | 40 Citations |
> | **Core Framework** | Dirichlet Variational AutoEncoder (DVAE) |
> | **Key Innovation** | Global Prior Distribution for Continual Learning |
> | **Primary Advantage** | Mitigates catastrophic forgetting in real-time streams |

---

## üìë Executive Summary

Current approaches to analyzing streaming text data face a fundamental trade-off between **stability** and **adaptability**. Dynamic Topic Models (DTMs) typically rely on fixed timelines and require retraining on extensive historical data, rendering them computationally expensive and too rigid for real-time applications. Conversely, Online Topic Models process data incrementally but suffer from **"catastrophic forgetting,"** where the acquisition of new topics overwrites previously learned knowledge.

This research introduces the **Continual Neural Topic Model (CoNTM)**, a novel neural architecture built upon the **Dirichlet Variational AutoEncoder (DVAE)** framework designed specifically to mitigate catastrophic forgetting. The core technical innovation is a **"Global Prior"** distribution that serves as a stable long-term memory store, while local models manage representations for specific time slices.

In empirical evaluations using standard datasets (NYTimes and Wiki), CoNTM demonstrated superior quantitative performance against established baselines including DLDA, DETM, and Dynamic BERTopic. Notably:
*   **NYTimes Perplexity:** 1295 (CoNTM) vs 1459 (DETM)
*   **Wiki Perplexity:** 1806 (CoNTM) vs 2031 (DETM)
*   **Topic Diversity:** 0.62 (CoNTM) vs 0.41 (DETM)

This work bridges the methodological gap between Dynamic and Online Topic Models, providing the first comprehensive evidence that neural architectures equipped with a continuously updated global prior can effectively track topic evolution without the computational overhead of full retraining.

---

## üîë Key Findings

*   **Superior Performance:** CoNTM consistently outperforms standard Dynamic Topic Models (DTMs) regarding topic quality and predictive perplexity.
*   **Online Learning Capability:** The model is capable of capturing topic changes in an online setting, effectively learning as new data arrives without retraining from scratch.
*   **Diversity & Adaptability:** Compared to existing methods, CoNTM learns significantly more diverse topics and captures temporal changes within the data better than current approaches.
*   **Robustness:** The model effectively learns domain-specific topics even in scenarios where pretrained embeddings used by other models are insufficient.

---

## üõ†Ô∏è Methodology

The Continual Neural Topic Model (CoNTM) utilizes a mechanism grounded in **continual learning** to prevent catastrophic forgetting. The research methodology focuses on the following core aspects:

*   **Continual Learning:** Designed specifically to handle the trade-off between learning new information and retaining old knowledge.
*   **Global Prior Distribution:** The core technical innovation involves a global prior that is continuously updated at subsequent time steps.
*   **Integration Strategy:** This mechanism allows the model to integrate new topics from incoming data while simultaneously preserving the knowledge of previously learned topics.

---

## ‚öôÔ∏è Technical Details

The Continual Neural Topic Model (CoNTM) is architected to balance stability with plasticity. Below is a breakdown of its technical components:

### Architecture
*   **Framework:** Built upon the **Dirichlet Variational AutoEncoder (DVAE)**.
*   **Global Prior:** Functions as a long-term memory store for the model.
*   **Local Models:** Specific models designed to handle representations for individual time slices.
*   **Transformation Function:** Connects global and local models by applying specific perturbations to global topics, distinguishing between enduring knowledge and transient trends.

### Generative Process & Inference
*   **Probabilistic Generative Process:**
    *   Draws topic proportions from a **Dirichlet distribution**.
    *   Draws words from a **Multinomial distribution**.
*   **Inference Method:** Utilizes variational inference with a Dirichlet approximate posterior.
*   **Optimization:** The model is trained by maximizing the **Evidence Lower Bound (ELBO)**, which consists of:
    *   A KL divergence term.
    *   A reconstruction likelihood term.

---

## üìà Results

The model was rigorously evaluated using predictive perplexity, topic quality, and topic diversity metrics against baselines such as DLDA, DETM, and Dynamic BERTopic.

*   **Predictive Accuracy:** CoNTM consistently outperformed standard DTMs in perplexity and quality.
*   **Real-Time Adaptation:** Captures temporal changes better in real-time settings compared to Dynamic BERTopic and DETM.
*   **Topic Redundancy:** Achieved significantly higher topic diversity; successfully avoided the "topic redundancy" issue often seen in other models.
*   **Coherence:** Achieved a topic coherence score of **-0.09** (surpassing DETM's -0.13), confirming its ability to capture fine-grained, high-quality temporal changes.

---

## üöÄ Contributions

The research addresses the limitations of existing paradigms through the following contributions:

1.  **Bridging the Gap:** Fills the void between Dynamic Topic Models (rigid, expensive) and Online Topic Models (prone to forgetting).
2.  **Novel Architecture:** Proposes CoNTM, a neural topic model specifically designed to handle the continual learning scenario.
3.  **Empirical Validation:** Provides evidence that a neural approach with a continuously updated global prior is more effective than dynamic models for tracking topic evolution and maintaining quality over time.