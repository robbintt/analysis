# RLAF: Reinforcement Learning from Automaton Feedback

*Mahyar Alinejad; Alvaro Velasquez; Yue Wang; George Atia*

<hr/>

> ### ðŸ“Š Quick Facts
>
> *   **Quality Score:** 9/10
> *   **Citations:** 40 references
> *   **Crafting-4 Success Rate:** 100% (RLAF Dynamic) vs 0% (RMax-NMRDP)
> *   **Sample Efficiency:** < 10,000 episodes vs > 50,000 episodes
> *   **Continuous Domain Return:** ~ -14.5 vs ~ -25 (Baseline)

<hr/>

## Executive Summary

This research addresses the inherent difficulty of learning optimal policies in environments characterized by **non-Markovian rewards**, where the reward signal is contingent on the history of states rather than the immediate observation. Capturing these complex temporal dependencies is computationally and statistically challenging for standard Reinforcement Learning (RL) formulations. Furthermore, the prevailing solution of manually engineering dense reward functions is not only labor-intensive and prone to error but also demands significant domain expertise. This paper addresses the critical need for agents to learn behaviors for temporally extended tasks without relying on brittle, hand-crafted scalar rewards.

The core innovation is **Reinforcement Learning from Automaton Feedback (RLAF)**, a novel paradigm that merges automaton theory with preference-based RL. Unlike traditional methods that use a Deterministic Finite Automaton (DFA) solely to assign scalar rewards, RLAF leverages the DFA to generate trajectory preferences, instructing the agent on the relative desirability of different paths. The method constructs a **Product MDP** to manage state and automaton configurations and learns a parameterized reward function $\hat{r}_\theta$ by minimizing a pairwise ranking (hinge) loss. The authors introduce two distinct optimization strategies: a **Static approach**, which fixes the reward function prior to policy optimization, and a **Dynamic approach**, which iteratively refines both the reward function and the policy concurrently to ensure alignment with task goals.

The proposed framework was validated in both discrete and continuous environments, demonstrating substantial quantitative improvements over baselines. In the discrete **Crafting-4** domain, a task requiring the execution of four sequential sub-goals, the Dynamic RLAF variant achieved a **100% success rate**, while the RMax-NMRDP baseline failed completely (0%). Significant gains in sample efficiency were also recorded, with Dynamic RLAF converging in fewer than **10,000 episodes**, whereas RMax required over **50,000 episodes** without success. In the continuous **Pushing** domain, RLAF demonstrated superior performance against standard preference-based methods, achieving an average return of approximately **-14.5**, outperforming the baseline which stagnated near **-25**.

The significance of RLAF lies in its formalization of an automated feedback loop for reward learning in structured environments. By transitioning from explicit reward specification to preference-based learning grounded in formal methods (DFA), this framework significantly reduces the engineering burden and minimizes human error in reward design. The inclusion of rigorous convergence guarantees for both Static and Dynamic strategies adds a necessary layer of theoretical reliability to preference-based RL. This work opens new avenues for applying reinforcement learning in safety-critical or highly structured domainsâ€”such as robotics and autonomous systemsâ€”where goals are best expressed using temporal logic or automata.

---

## Key Findings

*   **Effective Policy Learning:** The proposed approach enables agents to learn effective policies for tasks with temporal dependencies, significantly outperforming traditional reward engineering and automaton-based baselines.
*   **Non-Markovian Handling:** Successfully addresses non-Markovian (history-dependent) rewards using automaton-based preferences.
*   **Convergence Guarantees:** Offers rigorous theoretical convergence guarantees for learning near-optimal policies.
*   **Automated Feedback:** Eliminates the need for manual reward engineering through an automated feedback loop derived from Deterministic Finite Automata (DFA).
*   **Cross-Environment Versatility:** Demonstrated high performance in both discrete and continuous environments.

---

## Methodology

The RLAF framework fundamentally changes how rewards are conceptualized in Reinforcement Learning:

1.  **Automaton-Based Preferences:** Instead of defining an explicit reward function, the method utilizes a Deterministic Finite Automaton (DFA) to derive preferences between agent trajectories.
2.  **Reward Learning:** The agent learns a reward function by analyzing these automaton-generated trajectory preferences.
3.  **Optimization Strategies:**
    *   **Static Approach:** Uses the learned reward function directly to optimize the policy in a fixed manner.
    *   **Dynamic Approach:** Involves a continuous, iterative refinement process where both the reward function and the policy are updated simultaneously until convergence.

---

## Technical Details

*   **Framework Core:** Operates on Non-Markovian Reward Decision Processes (NMRDP) using a Product MDP defined as:
    $$M_{\text{prod}} = (S \times Q, A, T_{\text{prod}}, (s_0, q_0), R_{\text{prod}})$$
    using a labeling function $L: S \to 2^{AP}$.
*   **Preference Elicitation:**
    *   **Subtask-based Scoring:** $\text{score}(\tau) = w_s \cdot N_s(\tau) - w_d \cdot d(\tau)$
    *   **Automaton Transition Value-based Scoring**
*   **Learnable Reward:** The reward function $\hat{r}_\theta((s, q), a)$ is trained via a pairwise ranking loss.
*   **Loss Function:** Utilizes hinge loss for optimization:
    $$L(\theta) = \sum_{(\tau_p,\tau_n)} \max(0, m - (\hat{R}_\theta(\tau_p) - \hat{R}_\theta(\tau_n)))$$
    where the cumulative reward is:
    $$\hat{R}_\theta(\tau) = \sum_{t=0}^{T-1} \gamma^t \hat{r}_\theta((s_t, q_t), a_t)$$

---

## Core Contributions

*   **New Paradigm:** Introduction of 'Reinforcement Learning from Automaton Feedback' (RLAF), bridging the gap between automaton theory and preference-based reinforcement learning.
*   **Specification Shift:** Proposes moving from using automata for direct reward specification to generating trajectory preferences for automatic reward learning.
*   **Dual Optimization:** Contributes two policy optimization methods: Static and Dynamic.
*   **Theoretical Foundation:** Establishes a rigorous theoretical foundation with formal convergence guarantees for near-optimal policy learning.

---

*Report generated based on analysis of 40 citations.*