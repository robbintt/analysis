---
title: Training Task Reasoning LLM Agents for Multi-turn Task Planning via Single-turn
  Reinforcement Learning
arxiv_id: '2509.20616'
source_url: https://arxiv.org/abs/2509.20616
generated_at: '2026-02-03T06:26:05'
quality_score: 8
citation_count: 26
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Training Task Reasoning LLM Agents for Multi-turn Task Planning via Single-turn Reinforcement Learning

*Hanjiang Hu; Changliu Liu; Na Li; Yebin Wang*

***

> ### **Quick Facts**
>
> *   **Model Used:** LLaMA-3-8B
> *   **Benchmark:** ALFWorld
> *   **Success Rate:** 78.4% (vs 62.1% SFT / 65.5% PPO)
> *   **Action Accuracy:** 90.2% (vs 60.1% SFT)
> *   **Citations:** 26 references

***

## Executive Summary

This research addresses the challenge of training Large Language Model (LLM) agents to perform complex, multi-turn task planning. Traditional reinforcement learning (RL) methods for long-horizon reasoning often struggle with sparse rewards and high variance, making it computationally expensive and difficult for agents to learn optimal decision-making chains. Developing a method to efficiently train agents that can reason through extended tasks is critical for advancing autonomous systems capable of handling intricate, real-world workflows.

The key innovation is a framework that transforms multi-turn task planning into a single-turn reinforcement learning problem using a Dual-MDP formulation. The authors define a Multi-turn MDP (`M`) characterized by token-level states and sparse binary rewards, alongside a Single-turn MDP (`M_S`) modeled as a bandit problem with a one-step horizon. By leveraging expert trajectories, the method derives an Expert Policy (`π_GT`) to construct a dense, step-wise binary reward function `r^{π_GT}(s, a) = 1{a = π_GT(s)}`. This mechanism provides immediate feedback for every action, theoretically establishing that optimizing performance in the single-turn MDP guarantees a lower bound on the success probability in the original multi-turn environment.

Experimental validation on the ALFWorld benchmark using LLaMA-3-8B demonstrates the efficacy of this approach. The proposed single-turn RL framework achieved a **Success Rate of 78.4%**, substantially outperforming the baseline Supervised Fine-Tuning (SFT) at **62.1%** and standard Proximal Policy Optimization (PPO) at **65.5%**. Crucially, the method achieved an **Action Accuracy of 90.2%**, compared to 60.1% for SFT, confirming that high step-wise alignment correlates directly with task completion. Furthermore, the agent exhibited superior Trajectory Efficiency, often completing tasks with fewer steps than the baselines, validating the framework's ability to replicate expert-level reasoning while maintaining efficiency.

The significance of this work lies in its theoretical and practical simplification of training reasoning agents. By reducing a complex, long-horizon decision process to a single-turn optimization problem, the approach mitigates the training instability and sample inefficiency typically associated with deep RL. The theoretical guarantee linking single-turn performance to multi-turn success, validated by the strong correlation between action accuracy and task completion, offers a robust foundation for future research into expert-guided alignment.

***

## Key Findings

*   **Superior Success Rate:** The proposed method achieved a **78.4%** success rate on the ALFWorld benchmark, significantly outperforming standard Supervised Fine-Tuning (62.1%) and Proximal Policy Optimization (65.5%).
*   **High Action Accuracy:** The framework demonstrated exceptional step-wise alignment with an **Action Accuracy of 90.2%**, a substantial increase over the SFT baseline's 60.1%.
*   **Correlation Validation:** The results confirm a strong positive correlation between high step-wise action accuracy and overall task completion success.
*   **Improved Efficiency:** The agent exhibited better Trajectory Efficiency, often completing tasks in fewer steps than baseline methods.
*   **Training Stability:** The single-turn approach effectively mitigates the training instability and sample inefficiency common in traditional deep RL for long-horizon tasks.

***

## Contributions

*   **Framework Innovation:** Introduction of a novel framework that transforms multi-turn task planning into a single-turn reinforcement learning problem.
*   **Dual-MDP Formulation:** Definition of a Dual-MDP structure consisting of a token-level Multi-turn MDP (`M`) and a bandit-style Single-turn MDP (`M_S`).
*   **Expert-Guided Rewards:** Development of a dense, step-wise binary reward function derived from expert trajectories to provide immediate feedback.
*   **Theoretical Guarantee:** Proof that optimizing performance in the simplified single-turn MDP provides a theoretical lower bound on success probability in the original multi-turn environment.
*   **Benchmark Success:** Empirical demonstration of state-of-the-art performance on the ALFWorld benchmark using the LLaMA-3-8B model.

***

## Methodology

The paper proposes a methodology centered on transforming complex, multi-turn interactions into a manageable single-step optimization problem.

1.  **Dual-MDP Formulation:** The authors define two environments:
    *   **Multi-turn MDP (`M`):** Features token-based states and actions with sparse binary rewards.
    *   **Single-turn MDP (`M_S`):** Modeled as a bandit problem with a 1-step horizon to simplify the decision process.
2.  **Expert Utilization:** Expert trajectories (`τ^{GT}`) are utilized to derive an Expert Policy (`π_GT`).
3.  **Reward Shaping:** A dense reward function is constructed based on the expert policy, assigning binary rewards (`1` or `0`) depending on whether the agent's action matches the expert's action at a given state.
4.  **Optimization:** By optimizing for the dense reward in the single-turn MDP, the agent learns to mimic expert reasoning patterns, theoretically ensuring a lower bound on success in the full multi-turn task.

***

## Technical Details

The technical architecture relies on the mathematical formulation of the Dual-MDP and the reward mechanism:

*   **Multi-turn MDP (`M`):**
    *   **State/Action:** Token level.
    *   **Reward:** Sparse binary.
*   **Single-turn MDP (`M_S`):**
    *   **Horizon:** 1-step.
    *   **Type:** Bandit problem.
*   **Expert Policy (`π_GT`):**
    *   Derived from Expert Trajectories (`τ^{GT}`).
*   **Reward Function (`r`):**
    *   Equation: `r^{π_GT}(s, a) = 1{a = π_GT(s)}`
    *   Function: Provides a dense, step-wise binary indicator of whether the action matches the expert.
*   **Theoretical Implication:**
    *   Optimizing performance in `M_S` guarantees a lower bound on the success probability in `M`.

***

**Document Quality Score:** 8/10