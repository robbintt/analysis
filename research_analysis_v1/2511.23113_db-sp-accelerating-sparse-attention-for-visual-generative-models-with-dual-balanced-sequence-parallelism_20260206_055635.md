---
title: 'db-SP: Accelerating Sparse Attention for Visual Generative Models with Dual-Balanced
  Sequence Parallelism'
arxiv_id: '2511.23113'
source_url: https://arxiv.org/abs/2511.23113
generated_at: '2026-02-06T05:56:35'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# db-SP: Accelerating Sparse Attention for Visual Generative Models with Dual-Balanced Sequence Parallelism

*Siqi Chen; Ke Hong; Tianchen Zhao; Ruiqi Xie; Zhenhua Zhu; Xudong Zhang; Yu Wang*

---

> ### ðŸ“Š Quick Facts
> *   **Performance Gain:** 1.25x end-to-end speedup; 1.40x attention-specific speedup
> *   **Load Balance:** Reduced Head-Level imbalance from 1.51 â†’ 1.01
> *   **Innovation:** Sparsity-aware sequence parallelism with dynamic runtime configuration
> *   **Target Model:** Diffusion Transformers (DiT)

---

## Executive Summary

This research addresses the critical inefficiency in deploying Diffusion Transformers (DiTs) for visual generation, specifically focusing on the bottleneck of attention computation. While block-wise sparse attention is commonly used to reduce the quadratic complexity of attention mechanisms, standard Sequence Parallelism (SP) methods fail to effectively manage these workloads. The core issue is severe workload imbalance caused by the irregular distribution of sparse and dense blocks; because sparsity patterns vary significantly, some GPUs end up processing far more data than others. This "straggler" effect limits the throughput of visual generative models, making standard parallelism strategies inadequate for modern inference demands.

The authors propose **db-SP** (Dual-Balanced Sequence Parallelism), a novel architecture designed to achieve sparsity-aware load balancing. The key technical innovation is a dual-level partitioning strategy that simultaneously balances workloads across attention head dimensions and sequence block dimensions. To guide this process, the team introduces a formalized **"sparse imbalance ratio"** to quantify workload disparity. Furthermore, the system utilizes a runtime dynamic configuration engine that adaptively determines the optimal parallel degrees during inference. This allows the system to adjust to the evolving sparsity patterns characteristic of different denoising steps in diffusion models, ensuring consistent efficiency where static methods fail.

Empirical testing demonstrates that **db-SP** delivers substantial performance improvements over state-of-the-art methods. The system achieves an average end-to-end speedup of **1.25x** and a **1.40x** speedup specifically for attention computation. In terms of load distribution, the method successfully reduced the sparse imbalance ratio to near-perfect levels, improving head-level balance from approximately 1.51 to 1.01 and block-level balance from 1.6 to 1.0. Additionally, the solution introduces negligible computational overhead and outperforms competitors like BurstAttention, particularly in scenarios involving small sparse blocks that typically degrade performance in other systems.

The significance of this work lies in its demonstration that standard parallelism techniques are insufficient for the irregular data structures found in modern sparse attention models. By introducing a robust framework for sparsity-aware parallelism, **db-SP** enables more efficient, scalable inference for high-resolution visual generation. This research not only offers immediate performance gains for DiTs but also establishes a new paradigm for handling heterogeneous workloads in distributed systems, influencing future developments in the optimization of generative AI infrastructure.

---

## Key Findings

*   **Workload Imbalance in SP:** Standard sequence parallelism methods suffer from severe workload imbalance when applied to block-wise sparse attention in Diffusion Transformers (DiT) due to varying sparsity and irregular dense block distributions.
*   **Quantification Metric:** A "sparse imbalance ratio" was formalized to effectively quantify the workload imbalance in sparse attention models.
*   **Performance Improvements:** The proposed db-SP method achieves a **1.25x** end-to-end speedup and a **1.40x** attention-specific speedup on average compared to state-of-the-art methods.
*   **Dual-Level Success:** The dual-level partitioning approach successfully attains near-perfect workload balance at both the head and block levels with negligible computational overhead.
*   **Dynamic Adaptation:** The system dynamically adjusts parallel degrees at runtime to handle evolving sparsity patterns across different denoising steps and layers.

---

## Methodology

The paper proposes **db-SP (Dual-Balanced Sequence Parallelism)**, a sparsity-aware sequence parallelism technique designed to optimize inference for visual generative models. The methodology consists of three core components:

1.  **Dual-Level Partitioning:** Simultaneously partitions workloads to achieve balance across attention head and block dimensions.
2.  **Sparse Imbalance Quantification:** Utilizes a formalized "sparse imbalance ratio" to identify load distribution issues.
3.  **Runtime Dynamic Configuration:** Dynamically determines optimal parallel degrees at runtime to adapt to changing sparsity patterns.

---

## Technical Details

The paper addresses **Diffusion Transformers (DiTs)** for visual generation, identifying attention computation as a bottleneck due to quadratic complexity. It employs block-wise sparse attention for hardware efficiency and uses Sequence Parallelism (SP) over Tensor Parallelism (TP) to handle large activation sequences.

*   **Problem:** Existing methods like *Ulysses* and *Ring Attention* suffer from workload imbalance with sparse patterns, quantified by the 'Sparse Imbalance Ratio.'
*   **Solution (db-SP):** Utilizes a dual-level partitioning strategy:
    *   **Head-Level partitioning:** Redistributes attention heads for equal dense block distribution.
    *   **Block-Level partitioning:** Splits blocks to balance load.
*   **Adaptability:** It dynamically adjusts parallel degrees at runtime to handle evolving sparsity patterns and integrates with block-wise sparse kernels.

---

## Contributions

*   **Metric Formalization:** Identification and formalization of the workload imbalance bottleneck in sparse attention for DiT inference, introducing a 'sparse imbalance ratio' metric.
*   **Architecture Development:** Development of db-SP, a novel sequence parallelism architecture specifically tailored for block-wise sparse attention that overcomes limitations of existing 1D strategies.
*   **Dynamic Optimization:** Introduction of a dynamic optimization strategy for the runtime determination of parallel degrees to manage heterogeneous and evolving sparsity patterns.
*   **Empirical Validation:** Provision of empirical validation demonstrating that sparsity-aware parallelism yields significant performance improvements (up to 1.40x in attention computation) without prohibitive overhead.

---

## Results

The method achieves an average end-to-end speedup of **1.25x** and an attention-specific speedup of **1.40x** compared to state-of-the-art methods.

**Workload Balance Metrics:**
*   The Sparse Imbalance Ratio was reduced to near-perfect levels.
*   **Head-Level balance:** Improved from ~1.51 to ~1.01.
*   **Block-Level balance:** Improved from ~1.6 to ~1.0.

The system introduces negligible computational overhead and outperforms *BurstAttention*, which degrades performance on small sparse blocks.

---

**Quality Score:** 9/10
**References:** 40 citations