---
title: 'GenCNER: A Generative Framework for Continual Named Entity Recognition'
arxiv_id: '2510.11444'
source_url: https://arxiv.org/abs/2510.11444
generated_at: '2026-02-03T18:36:22'
quality_score: 9
citation_count: 36
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# GenCNER: A Generative Framework for Continual Named Entity Recognition

*Yawen Yang; Fukun Ma; Shiao Meng; Aiwei Liu; Lijie Wen*

---

> ### ðŸ“Š Quick Facts
>
> *   **Quality Score:** 9/10
> *   **Base Architecture:** BART-large (Seq2Seq)
> *   **Key Datasets:** OntoNotes, Few-NERD
> *   **Optimization:** AdamW (LR 5e-5)
> *   **Hardware:** Single GeForce RTX 3090
> *   **References:** 36 Citations

---

## ðŸ“ Executive Summary

Continual Named Entity Recognition (CNER) presents a significant challenge in natural language processing, requiring models to learn new entity categories incrementally over time without retraining from scratch. The primary obstacles in this domain are **catastrophic forgetting** (where the model degrades performance on previously learned tasks when acquiring new knowledge) and **semantic shift** (where non-entity tokens are misclassified as entities due to evolving data distributions). Bridging the performance gap between continual learning and offline training (the upper bound) is critical for deploying robust NER systems in dynamic real-world environments where data arrives in streams and entity taxonomies expand.

The authors propose **GenCNER**, a generative framework that fundamentally shifts the CNER paradigm from discriminative classification to sequence generation. Utilizing a pre-trained sequence-to-sequence model (BART-large), GenCNER reformulates NER as a "sustained entity triplet sequence generation problem," predicting entity boundaries and types as structured triplets (*start, end, type*). To manage the stability-plasticity trade-off, the framework employs a dual-strategy training mechanism: **Knowledge Distillation** preserves historical knowledge, while **type-specific confidence-based pseudo labeling** handles new tasks and mitigates label noise.

GenCNER established new state-of-the-art performance across multiple CNER settings on the OntoNotes and Few-NERD benchmarks. On the OntoNotes dataset, the model outperformed the previous best method (SpanKL) by **+0.97% F1** in the Split-All setting and **+0.69% F1** in the Filter-Filter setting. Crucially, GenCNER significantly narrowed the performance gap relative to the non-continual learning upper bound, reducing this gap to **0.14** in Split-All and **0.36** in Filter-Filter scenarios. This research validates the efficacy of generative approaches in continual learning, offering a viable solution to the plasticity-stability dilemma and setting a new standard for CNER.

---

## ðŸ”‘ Key Findings

*   **State-of-the-Art Performance:** GenCNER outperforms previous SOTA methods across multiple CNER settings on both OntoNotes and Few-NERD benchmark datasets.
*   **Mitigates Catastrophic Forgetting:** The framework successfully addresses the core challenge of catastrophic forgetting and the semantic shift of non-entity types.
*   **Narrowed Performance Gap:** Achieves the smallest performance gap compared to non-continual learning (offline) results, bringing continual models closer to upper-bound performance.
*   **Noise Resilience:** The integration of type-specific confidence-based pseudo labeling effectively alleviates the impact of label noise at the triplet level.

---

## ðŸ§  Methodology

The authors propose **GenCNER**, a generative framework that shifts the paradigm of CNER from discriminative classification to sequence generation. The core methodology involves:

*   **Paradigm Shift:** Converting the CNER task into a 'sustained entity triplet sequence generation problem' using a pre-trained sequence-to-sequence model.
*   **Dual-Strategy Training:** Employing a two-pronged training approach:
    1.  **Type-specific confidence-based pseudo labeling:** Used to handle new tasks.
    2.  **Knowledge Distillation (KD):** Used to preserve knowledge from previous tasks.

---

## âš™ï¸ Technical Details

### Framework Architecture
*   **Base Model:** Generative sequence-to-sequence using BART-large.
*   **Output Format:** Generates entity triplets defined as (*start*, *end*, *type*).
*   **Indexing Strategy:** Utilizes an **n-shift strategy** where indices `0` to `n-1` represent boundaries and indices `>= n` represent types.
*   **Vocabulary Expansion:** Entity types are added as special tokens during the continual learning process.

### Training Mechanism
*   **Knowledge Distillation:** Implements a teacher-student setup where the model at the previous step acts as a teacher to the current model. Targets combine pseudo triplets from the teacher and annotated sequences for current types.
*   **Confidence-Based Pseudo Labeling:**
    *   Confidence is defined as `min(p(s), p(e), p(c))`.
    *   Uses **type-specific thresholding** to ensure 50% of pseudo triplets are retained.
*   **Loss Function:** Combines KL Divergence and Cross-Entropy.

### Optimization Configuration
*   **Optimizer:** AdamW
*   **Learning Rate:** 5e-5
*   **Warmup Ratio:** 0.1
*   **Epochs:** 10 per task
*   **Batch Size:** 8
*   **Hardware:** Single GeForce RTX 3090 GPU

---

## ðŸ“ˆ Results

### Threshold Analysis
Type-Specific Confidence Thresholds varied by dataset and entity type:
*   **OntoNotes:** Ranged from **0.63** (CARD) to **0.84** (PER).
*   **Few-NERD:** Ranged from **0.45** (BUID) to **0.60** (LOC).

### Performance on OntoNotes
*   **Split-All Setting:**
    *   Achieved **+0.97%** higher F1 score compared to SpanKL.
    *   Narrowed the performance gap to the non-CL upper bound by **0.14**.
*   **Filter-Filter Setting:**
    *   Achieved **+0.69%** higher F1 score compared to SpanKL.
    *   Narrowed the gap to the non-CL upper bound by **0.36**.

### General Performance
*   GenCNER achieved the highest **Macro F1** scores at each incremental step.
*   Demonstrated the smallest performance gap compared to the non-CL upper bound across all steps.

---

## âœ¨ Contributions

1.  **Novel Framework:** Introduction of GenCNER, a novel generative framework designed specifically for Continual Named Entity Recognition.
2.  **Task Reformulation:** A unique reformulation of NER as 'Triplet Sequence Generation', allowing for the flexible handling of continuously increasing entity categories.
3.  **Training Mechanism:** Development of a combined training mechanism using pseudo labeling and Knowledge Distillation to effectively address the plasticity vs. stability trade-off.
4.  **Benchmarking:** Establishment of new performance standards on benchmark datasets, demonstrating the ability of generative methods to narrow the gap between continual and offline training.