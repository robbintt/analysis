---
title: 'LLMVoX: Autoregressive Streaming Text-to-Speech Model for Any LLM'
arxiv_id: '2503.04724'
source_url: https://arxiv.org/abs/2503.04724
generated_at: '2026-02-06T02:14:55'
quality_score: 9
citation_count: 9
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# LLMVoX: Autoregressive Streaming Text-to-Speech Model for Any LLM

*Sambal Shikhar; Mohammed Irfan Kurpath; Sahal Shaji Mullappilly; Jean Lahoud; Fahad Khan; Rao Muhammad Anwer; Salman Khan; Hisham Cholakkal*

---

> ### ⚡ Quick Facts
> * **Model Size:** 30M Parameters
> * **Pipeline Latency:** 475 ms
> * **Word Error Rate (WER):** 3.70
> * **Speed:** ~10x faster than comparative methods
> * **Adaptability:** Low CER achieved via dataset adaptation
> * **Quality Score:** 9/10

---

## Executive Summary

Current approaches to integrating Text-to-Speech (TTS) capabilities into Large Language Models (LLMs) face significant architectural and efficiency trade-offs. Existing speech-enabled LLMs typically require computationally expensive fine-tuning, which often degrades the model's core linguistic reasoning capabilities. Furthermore, these systems struggle with text-speech alignment and latency issues, particularly during long conversational turns where delays and misalignment between the generated text and audio can disrupt the user experience.

**LLMVoX** addresses the critical need for a speech synthesis solution that can be added to any LLM without modifying the backbone model's parameters or compromising its performance. It introduces a decoupled, LLM-agnostic architecture that functions as a lightweight, plug-and-play TTS module.

Instead of fine-tuning the host LLM, the system employs a decoder-only Transformer with 30 million parameters to recast speech synthesis as an autoregressive token prediction task using discrete acoustic units via WavTokenizer. The core technical innovation is a **Multi-Queue Token Streaming System** that decouples LLM text processing from audio synthesis. By fusing phoneme embeddings with L2-normalized previous speech token features, this mechanism ensures precise text-speech alignment and enables infinite-length dialogue streams without the bottlenecks inherent in coupled systems.

In comparative evaluations, LLMVoX demonstrated superior performance and efficiency over integrated competitors like Llama-Omni and Moshi. The significance of LLMVoX lies in its ability to solve the integration trade-off, preserving the full linguistic capabilities of the base LLM while adding high-fidelity streaming speech, providing a scalable framework for evolving conversational AI.

---

## Key Findings

*   **Superior Accuracy & Speed:** Achieves a significantly lower Word Error Rate (WER) compared to existing speech-enabled LLMs while maintaining comparable latency and speech quality scores. The system operates approximately **10x faster** than comparative methods.
*   **Lightweight Architecture:** The model is remarkably lightweight (30M parameters) and preserves the linguistic capabilities of the base LLM without requiring computationally expensive fine-tuning.
*   **Language Adaptability:** Demonstrates strong adaptability to new languages, achieving a low Character Error Rate (CER) through simple dataset adaptation rather than architectural changes.
*   **VLM Integration:** The architecture allows for seamless integration with Vision-Language Models (VLMs) to create omni-models without requiring additional multimodal training.

---

## Methodology

The research utilizes a distinct architectural approach to separate speech synthesis from the LLM's text processing:

*   **Decoupled Architecture:** Utilizes a design that separates speech synthesis from the LLM's text processing, preventing interference with the LLM's core functions.
*   **LLM-Agnostic Design:** Implements an autoregressive streaming system that functions as a plug-and-play Text-to-Speech (TTS) engine without modifying the underlying LLM.
*   **Multi-Queue Token Streaming:** Employs a Multi-Queue Token Streaming System to decouple LLM processing from speech synthesis. This enables infinite-length dialogues and prevents text-speech misalignment.
*   **Dataset Adaptation:** Relies on dataset adaptation for generalization to new languages rather than architectural changes or extensive retraining.

---

## Contributions

This research makes three primary contributions to the field of speech-enabled AI:

1.  **Solving the Integration Trade-off:** Addresses the conflict in current speech-enabled LLMs by preserving the base LLM's linguistic capabilities while successfully adding speech functionality.
2.  **Enabling Infinite Dialogue:** Overcomes latency and alignment bottlenecks through a streaming architecture that supports seamless, long conversational flows without interruption.
3.  **Versatile Plug-and-Play Design:** Provides a flexible framework that is extendable to various tasks and compatible with different backbone models, facilitating easier adoption.

---

## Technical Details

**Architecture & Framework**
*   **Type:** LLM-agnostic framework.
*   **Parameters:** ~30 million.
*   **Function:** Plug-and-play module downstream of any LLM (no fine-tuning required).
*   **Core Model:** Decoder-only Transformer recasting Text-to-Speech as an autoregressive token prediction task on discrete acoustic units.
*   **Streaming:** Multi-queue streaming approach for continuous generation.

**Audio Processing Specifications**
*   **Codec:** WavTokenizer codec with single-layer Residual Vector Quantization (RVQ).
*   **Vocabulary:** 4,096 entries.
*   **Token Rate:** 40–75 tokens per second.
*   **Sample Rate:** 24 kHz audio.

**Text Processing & Alignment**
*   **Text Inputs:** Byte-level sub-tokens.
*   **Embeddings:** From a pre-trained ByT5-based Grapheme-to-Phoneme (G2P) model (256 dimensions).
*   **Alignment Mechanism:** Text embeddings are padded to match speech token length. Inputs are fused via concatenation of phoneme embeddings and L2-normalized previous speech token features.

---

## Results

### English Language Performance
LLMVoX was benchmarked against current state-of-the-art models, showing high efficiency and accuracy:

| Metric | Score | Note |
| :--- | :--- | :--- |
| **Total Pipeline Latency** | 475 ms | ~10x faster than comparative methods |
| **Word Error Rate (WER)** | 3.70 | Significantly lower than competitors |
| **GPT Score** | 6.88 | High linguistic fidelity |
| **UTMOS Score** | 4.05 | Competitive speech quality |

### Cross-Lingual Adaptation (Arabic)
*   **Adaptation Method:** 1,500 hours of synthetic data.
*   **Performance:** Achieved a Character Error Rate (CER) of approximately **8%**.
*   **Integration:** Successfully integrated with the Jais LLM.

### Comparative Analysis
*   **Vs. Llama-Omni & Moshi:** LLMVoX outperformed these LLM-dependent competitors in both WER and GPT Score.
*   **Vs. XTTS:** Remained competitive with the cascaded XTTS baseline while offering the benefits of a streaming architecture.