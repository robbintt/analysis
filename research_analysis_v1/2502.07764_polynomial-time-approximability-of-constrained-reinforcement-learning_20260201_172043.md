# Polynomial-Time Approximability of Constrained Reinforcement Learning

*Jeremy McMahan*

> ### ðŸ“Š Quick Facts
> - **Quality Score:** 9/10
> - **References:** 40 Citations
> - **Core Innovation:** Polynomial-time $(0,\epsilon)$-additive bicriteria approximation
> - **Assumption:** $P \neq NP$
> - **Constraint Scope:** Almost-sure, chance, expectation, and non-homogeneous

---

## Executive Summary

This paper addresses the fundamental computational intractability of Constrained Reinforcement Learning (CRL), specifically focusing on environments requiring agents to satisfy complex safety or budgetary constraints while maximizing rewards. While Constrained Markov Decision Processes (CMDPs) are the standard model for safety-critical applications, finding optimal policiesâ€”particularly deterministic ones or those subject to chance constraintsâ€”has historically been proven to be NP-hard.

The core innovation is a polynomial-time bicriteria approximation algorithm designed for "Shortly Recursive" (SR) constraints, utilizing a reductionist framework based on state-action augmentation. By formalizing the problem as $(M, C, B)$ and leveraging 1-Lipschitz associative aggregator functions, the method transforms global constraints into manageable per-step action constraints. This approach generalizes to non-homogeneous constraints and continuous-state processes.

To circumvent the NP-hardness of the Bellman update, the algorithm employs "dynamic rounding." Crucially, the study establishes matching lower bounds via complexity analysis, proving that the approximation guarantees provided are the best possible assuming $P \neq NP$. The research significantly advances the field by settling long-standing open questions, providing a unified solution across constraint types, and bridging the gap between theoretical safety requirements and computational tractability.

## Key Findings

- **Optimal Algorithm Design:** Developed a polynomial-time $(0,Îµ)$-additive bicriteria approximation algorithm that delivers optimal approximation guarantees (assuming $P \neq NP$).
- **Chance Constraints:** Achieved the first proof of polynomial-time approximability for policies subject to chance constraints.
- **Deterministic Policies:** Achieved the first proof of polynomial-time approximability for deterministic policies operating under multiple expectation constraints.
- **Generalized Environments:** Resolved the approximability of policies under non-homogeneous constraints (mixing different types) and for continuous-state processes.
- **Broad Validation:** Validated the approach across a broad class of recursively computable constraints, covering almost-sure, chance, expectation, and their "anytime" variants.

## Methodology

The research methodology relies on a sophisticated computational approach to handling constrained environments:

*   **Bicriteria Algorithm Focus:** The core methodology involves the design of a bicriteria approximation algorithm tailored to handle general constrained Markov decision processes (CMDPs).
*   **Constraint Handling:** The approach focuses specifically on recursively computable constraints, utilizing a $(0,Îµ)$-additive metric to approximate optimal policies within polynomial time.
*   **Complexity Analysis:** The study employs rigorous computational complexity analysis to establish matching lower bounds for the algorithm, ensuring the theoretical limits of the approximation are well-defined.

## Contributions

### Resolution of Open Problems
Settled several long-standing open questions in the constrained reinforcement learning literature by proving polynomial-time approximability for complex settings that were previously unsolved.

### Broad Generalization
Provided a unified algorithmic solution that encompasses a wide variety of constraint types (almost-sure, chance, expectation) and extends to complex environments like continuous-state processes.

### Optimality Certification
Established that the derived approximation guarantees are the best possible under standard complexity assumptions ($P \neq NP$), providing a definitive benchmark for future research.

## Technical Details

### Framework Formalization
The paper introduces a framework for solving Constrained Markov Decision Processes (CMDPs) with **'Shortly Recursive' (SR) constraints**.
*   **Problem Definition:** The problem is formalized as $(M, C, B)$ to maximize value subject to a budget.
*   **Mathematical Basis:** Relies on 1-Lipschitz associative aggregator functions to manage constraint accumulation.

### Reductionist Approach
The method utilizes a reductionist approach based on state-action augmentation to convert global constraints into local checks:
*   **Augmented State Space:** Constructs an augmented state space $(s, b)$.
*   **Augmented Action Space:** Constructs an augmented action space $(a, \mathbf{b})$.
*   **Feasibility:** This transformation converts global constraints into per-step action constraints, ensuring feasibility via specific invariants.

### Optimization Strategy
To address the NP-hardness of the Bellman update in the reduced MDP:
*   **Technique:** The algorithm employs dynamic programming combined with **dynamic rounding** applied at every step.
*   **Implementation:** It creates an approximate MDP that relaxes constraints by rounding budgets up (optimism under uncertainty).
*   **Complexity Reduction:** This dynamic rounding reduces the exponential action space to a polynomial size, enabling efficient solutions.

## Results

*   **Theoretical Guarantee:** The algorithm achieves a polynomial-time $(0, \epsilon)$-additive bicriteria approximation. It guarantees optimal value (0-additive reward) while meeting constraints with an additive $\epsilon$ error ($C^\pi_M \leq B + \epsilon$).
*   **Problem Resolution:** It resolves polynomial-time approximability for previously open problems, including:
    *   Chance constraints.
    *   Deterministic multi-constraint policies.
    *   Non-homogeneous constraints.
    *   Continuous-state processes.
*   **Complexity Bounds:** Theoretically, the text establishes that finding feasible deterministic policies is NP-hard, even with a single constraint. However, the dynamic rounding technique yields solutions that are tight with the established computational lower bounds.