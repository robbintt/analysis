# SASQ: Static Activation Scaling for Quantization-Aware Training in Large Language Models

*Shizhuo Mao; Song Chen; Yi Kang*

---

### ðŸš€ Quick Facts

| **Metric** | **Detail** |
| :--- | :--- |
| **Model Tested** | LLaMA2-7B |
| **Dataset** | WikiText2 |
| **Primary Metric** | Perplexity (PPL) |
| **Vs. SOTA (QuaRot)** | -5.2% PPL |
| **Vs. FP16 Baseline** | -4.7% PPL |
| **Training Cost** | Lightweight (Frozen weights) |

---

## Executive Summary

This paper addresses the critical deployment trade-offs inherent in quantizing Large Language Models (LLMs): the high computational overhead of dynamic quantization versus the severe accuracy degradation typically associated with static quantization. While dynamic schemes offer higher precision, their runtime requirements make them inefficient for production deployment. Conversely, static quantization is efficient but often fails to preserve model fidelity due to activation outliers. Additionally, the paper tackles the prohibitive cost of standard Quantization-Aware Training (QAT), which requires expensive retraining of billions of pre-trained weights, creating a barrier to efficient compression.

The authors introduce **SASQ (Static Activation Scaling Quantization-Aware Training)**, a lightweight framework that decouples the optimization of quantization scales from the model weights. Unlike standard QAT, SASQ keeps pre-trained weights frozen and exclusively optimizes activation quantization factors using gradient descent and Straight-Through Estimator (STE). Technically, the method employs a non-linear adaptive truncation mechanism to handle outliers in Attention and FFN inputs. This approach reduces quantization complexity by adaptively truncating extreme values while strictly preserving the underlying distributional characteristics of activations, enabling effective per-channel quantization for both weights and activations.

In evaluations on the LLaMA2-7B model using the WikiText2 dataset, SASQ demonstrated superior performance, achieving a perplexity (PPL) 5.2% lower than the state-of-the-art QuaRot method. Remarkably, the framework also outperformed the full-precision FP16 baseline, yielding a PPL 4.7% lower than the uncompressed model. These results highlight the method's efficacy compared to standard static quantization, which resulted in a catastrophic performance drop with perplexity exceeding 400. Furthermore, SASQ effectively reduced the magnitude of outliers in activation distributions, validating the success of its adaptive truncation strategy.

The significance of this research lies in providing a viable pathway to high-accuracy static inference without the heavy computational costs associated with traditional QAT. By demonstrating that optimizing only quantization factors can outperform both existing SOTA methods and FP16 baselines, SASQ resolves the fundamental tension between deployment efficiency and model precision. This work lowers the barrier for deploying compressed LLMs, offering a resource-efficient methodology that eliminates the need for weight retraining while achieving quantization levels that were previously thought to require dynamic computation.

---

## Key Findings

*   **Superiority over SOTA and Baseline:** SASQ surpasses existing state-of-the-art quantization schemes and performs better than corresponding FP16 models.
*   **Significant Perplexity Reduction:** On the LLaMA2-7B model evaluated with WikiText2, SASQ achieved a perplexity **5.2% lower** than the QuaRot method.
*   **Outperformance of FP16:** In the same benchmark, the method achieved a perplexity **4.7% lower** than the FP16 baseline model.
*   **Effective Outlier Handling:** The approach successfully reduces the difficulty of quantization through adaptive truncation of outliers while preserving the distributional characteristics of activations.

---

## Methodology

*   **Framework:** SASQ employs a lightweight Quantization-Aware Training (QAT) framework specifically tailored for activation quantization factors.
*   **Optimization Strategy:** Unlike standard QAT, SASQ exclusively optimizes the quantization factors while keeping pre-trained weights frozen (unchanged). This eliminates the costs associated with weight training.
*   **Outlier Management:** The method adaptively truncates outliers to lower the complexity of quantization without altering the underlying distributional characteristics of the activations.
*   **Inference Mode:** The framework is designed to enable static inference, maintaining deployment efficiency and avoiding the high computational overhead associated with dynamic quantization.

---

## Technical Details

**Core Definition**
SASQ (Static Activation Scaling Quantization-Aware Training) is a framework that optimizes only activation quantization factors via gradient descent while keeping model weights static (after PTQ initialization).

**Key Specifications**
*   **Quantization Type:** Employs per-channel quantization for both activations and weights to improve generalizability.
*   **Training Process:** Utilizes differentiable fake quantization and Straight-Through Estimator (STE) to update scales.
*   **Objective Function:** Minimizes perplexity directly using symmetric quantization.
*   **Outlier Mechanism:** Uses a non-linear transformation to adaptively truncate outliers in Attention and FFN inputs.

**Stages of Operation**
1.  **PTQ Initialization:** Initial setup of the model.
2.  **SASQ Training:** Updating scales using the defined optimization strategy.
3.  **SASQ Static Inference:** Deployment phase.

---

## Core Contributions

*   **Resolution of Trade-offs:** The paper addresses the fundamental trade-offs in LLM deployment between dynamic quantization (high overhead) and static quantization (accuracy loss), as well as the high training costs of existing QAT methods.
*   **Efficiency-Accuracy Balance:** By optimizing only quantization factors, SASQ provides a pathway to static inference that maintains high accuracy and deployment efficiency without the need for expensive weight retraining.
*   **Novel Quantization Technique:** The introduction of an adaptive truncation mechanism for outliers offers a new method for handling activation distributions that improves upon current SOTA solutions.

---

## Evaluation Results

**Benchmark Configuration**
*   **Model:** LLaMA2-7B
*   **Dataset:** WikiText2
*   **Metric:** Perplexity (PPL)

**Performance Analysis**
*   **vs. SOTA:** SASQ achieved a PPL **5.2% lower** than QuaRot.
*   **vs. FP16:** SASQ achieved a PPL **4.7% lower** than the FP16 baseline.
*   **vs. Standard Static Quantization:** Standard static quantization without SASQ resulted in a PPL exceeding **400**, representing a catastrophic performance drop compared to SASQ's improvements.
*   **Distributional Analysis:** The method effectively reduced the magnitude of outliers in activation distributions compared to the FP16 model.

---

**Paper Quality Score:** 8/10  
**References:** 5 citations