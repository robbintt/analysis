# Evaluating Explainability: A Framework for Systematic Assessment and Reporting of Explainable AI Features

*Miguel A. Lago; Ghada Zamzmi; Brandon Eich; Jana G. Delfino*

***

### F4CA Quick Facts

| Metric | Details |
| :--- | :--- |
| **Quality Score** | 8/10 |
| **Dataset** | M-SYNTH (150 synthetic images) |
| **Test Domain** | Breast lesion detection (Mammography) |
| **Core Framework** | 4-Dimensional (Consistency, Plausibility, Fidelity, Usefulness) |
| **AI Model** | Faster R-CNN Object Detection |
| **XAI Methods** | Ablation CAM vs. Eigen CAM |
| **Key Metrics** | SSIM, IOU, 1-MSE |
| **References** | 30 Citations |

***

## Executive Summary

This research addresses the critical lack of standardization in assessing the quality of Explainable AI (XAI) within high-stakes healthcare applications. As AI integration in medical devices increases, the reliance on subjective, qualitative assessment of explanation heatmaps creates a significant barrier to validation, regulatory approval, and clinical trust. Without objective metrics, it is impossible to rigorously compare different XAI methods or ensure they truly align with clinical requirements, necessitating a shift toward systematic, quantifiable evaluation frameworks.

The authors introduce a four-dimensional evaluation framework comprising Consistency, Plausibility, Fidelity, and Usefulness, alongside a standardized "Explainability Card" scorecard for reporting. Technically, the study decouples the predictive model—a Faster R-CNN object detection algorithm—from the explanation methods, conducting a comparative analysis of Ablation CAM versus Eigen CAM on synthetic mammography data. To enforce rigor, the framework explicitly maps quantitative metrics to its dimensions:

*   **Plausibility** is measured via Intersection over Union (IOU) against ground truth lesions.
*   **Consistency** is evaluated using 1-MSE (Mean Squared Error) to assess stability under rotational perturbations (0° to 50°).
*   **Fidelity** is quantified using the Structural Similarity Index Measure (SSIM) to evaluate structural integrity.

Validated using the M-SYNTH dataset, the framework demonstrated its ability to benchmark model and explanation performance under variable conditions. The Faster R-CNN model achieved a training AUC of 1 on images with 7mm lesions, though performance degraded with denser breast tissues. During testing on challenging 5mm lesions, the framework successfully generated distinct scores for both heatmap methods.

This work establishes a technical foundation for shifting XAI evaluation from subjective qualitative review to objective, quantifiable metrics. By providing a concrete scorecard and a rigorous four-criteria framework, the paper offers a pathway for improving the validation and regulatory review of AI-based medical devices, enabling developers to select explanation techniques that offer genuine clinical utility.

***

## Key Findings

*   **Lack of Standardization:** There is a significant deficiency in existing techniques for assessing the quality of explainability features within AI devices.
*   **Four-Criteria Framework:** The quality of AI explanations can be systematically assessed using four distinct metrics: **Consistency**, **Plausibility**, **Fidelity**, and **Usefulness**.
*   **Standardized Reporting Tool:** The authors developed a scorecard to serve as a standardized method for describing and evaluating explainable AI algorithms.
*   **Validation via Case Study:** The framework was successfully applied to evaluate explanation heatmaps (specifically Ablation CAM and Eigen CAM) in a clinical context involving the detection of breast lesions on synthetic mammographies.
*   **Clinical Relevance:** The first three evaluation criteria (Consistency, Plausibility, Fidelity) were specifically adapted and evaluated for clinically-relevant scenarios.

***

## Methodology

The research methodology involved constructing a conceptual model and validating it through a specific domain case study:

1.  **Framework Development:** The researchers constructed a conceptual evaluation model based on four specific pillars (Consistency, Plausibility, Fidelity, and Usefulness) to define explainability quality.
2.  **Metric Definition:** Each of the four criteria was defined precisely to quantify different aspects of explanation quality (e.g., input variance vs. model alignment).
3.  **Case Study Application:** The proposed framework was tested using a comparative analysis of two heatmap methods (Ablation CAM and Eigen CAM).
4.  **Domain-Specific Testing:** The methodology utilized synthetic mammographies for the task of breast lesion detection to validate the criteria within a medical imaging context.

***

## Technical Details

The following specifications outline the implementation and data structure used in the study:

| Component | Specification |
| :--- | :--- |
| **Evaluation Tool** | **Explainability Card** (divided into descriptive information and quantitative evaluation sections). |
| **Evaluation Criteria** | **Consistency**, **Plausibility**, **Fidelity**, **Usefulness**. |
| **Target Model** | **Faster R-CNN** object detection model. |
| **XAI Methods Compared** | **Eigen CAM** vs. **Ablation CAM**. |
| **Dataset** | **M-SYNTH** synthetic x-ray mammography dataset. |
| **Data Volume** | **150 images**; 50/50 split between lesion and non-lesion cases. |
| **Perturbation Tests** | Rotations ranging from **0° to 50°** to assess stability. |
| **Quantitative Metrics** | **SSIM** (Structural Similarity), **IOU** (Intersection over Union), **1-MSE** (Inverted Mean Squared Error). |

***

## Results

*   **Model Performance:** The Faster R-CNN model achieved a training **AUC of 1** on fatty breast density images with 7mm lesions. Performance degraded with denser breast tissues.
*   **Testing Conditions:** Test conditions utilized **5mm lesions** to induce variability and mimic harder detection tasks.
*   **Metric Application:**
    *   **SSIM** was used to measure heatmap structural similarity.
    *   **IOU** measured overlap with ground truth lesions.
    *   **1-MSE** measured similarity under rotation.

***

## Contributions

*   **Systematic Evaluation Criteria:** Introduction of a comprehensive, four-dimensional framework (Consistency, Plausibility, Fidelity, Usefulness) that provides a technical foundation for assessing explainable AI (XAI) features.
*   **Explainability Scorecard:** Development of a practical scorecard tool designed to standardize the reporting and evaluation of XAI methods, facilitating better comparison and documentation.
*   **Advancement of Medical AI:** By applying the framework to breast lesion detection, the work provides a specific pathway to improve the development and validation of AI-based medical devices, encouraging dialogue on the tangible value of explainability in clinical settings.