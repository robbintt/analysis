---
title: 'LLM-Explorer: Towards Efficient and Affordable LLM-based Exploration for Mobile
  Apps'
arxiv_id: '2505.10593'
source_url: https://arxiv.org/abs/2505.10593
generated_at: '2026-01-27T22:15:32'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# LLM-Explorer: Towards Efficient and Affordable LLM-based Exploration for Mobile Apps

*Yuanchun Li, Shanhui Zhao, Ye Ouyang, Xiaozhou Ye, Yunxin Liu, Hao Wen, Wenjie Du, Cheng Liang*

---

> **QUICK FACTS & METRICS**
> | Metric | Value |
> | :--- | :--- |
> | **Quality Score** | 9/10 |
> | **Cost Efficiency** | 148Ã— improvement vs. SOTA |
> | **Test Coverage** | Highest among tested explorers |
> | **Evaluation Scope** | 5 Baselines, 20 Mobile Apps |
> | **Core Architecture** | Knowledge-centric / AIG |

---

## Executive Summary

Automated exploration of mobile applications is critical for testing and GUI analysis, yet it remains computationally challenging to execute effectively. While Large Language Models (LLMs) offer powerful semantic understanding for navigating complex user interfaces, current state-of-the-art approaches are prohibitively expensive and slow. These existing methods typically rely on continuous LLM inference to generate every single action during exploration, resulting in high token costs and significant latency.

**LLM-Explorer** introduces a paradigm shift from "action generation" to "knowledge maintenance," utilizing a knowledge-centric architecture to minimize LLM dependency. The system constructs an Abstract Interaction Graph (AIG) to serve as a compact, dynamic representation of the app's structure. The architecture is divided into two modules: a lightweight Knowledge-guided Exploration Module that handles context-aware action selection without LLM inference, and an LLM-assisted Knowledge Maintenance Module.

Validated against five strong baselines across 20 typical mobile applications, LLM-Explorer demonstrated superior performance in both speed and coverage. Most significantly, the system demonstrated a **148-fold improvement in cost efficiency** compared to state-of-the-art LLM-based approaches. This research establishes a new standard for the practical application of LLMs in software engineering, rendering large-scale, automated mobile testing financially viable and technically scalable.

---

## Key Findings

*   **Superior Performance:** LLM-Explorer achieved the fastest exploration speed and highest coverage among tested automated app explorers.
*   **Massive Cost Savings:** The system demonstrated a **148 times** improvement in cost efficiency compared to state-of-the-art LLM-based approaches.
*   **Inefficiency of Continuous Inference:** Using LLMs to generate actions at every step is neither necessary nor effective. Many actions do not require LLM capabilities and may suffer from bias if LLMs are overused.
*   **Robust Validation:** Findings were validated through rigorous comparison against 5 strong baselines across 20 typical mobile applications.

---

## Methodology

The methodology employs a **knowledge-centric architecture** where precise, compact knowledge is central to effective exploration.

**Core Concept**
LLM-Explorer uses LLMs primarily to maintain knowledge rather than to execute actions. Once knowledge is established, the system guides action generation without further LLM inference. This creates an "LLM-less" operational loop for the majority of the exploration process.

**Operational Flow**
1.  **Knowledge Maintenance:** The system converts raw UI traces into structured knowledge.
2.  **Exploration:** The system navigates the app using the established knowledge base.
3.  **Efficiency:** By limiting LLM calls strictly to knowledge updates, the approach minimizes token fees and computational resources.

---

## Technical Details

LLM-Explorer operates on a knowledge-guided exploration paradigm, shifting from continuous LLM inference to strategic, lightweight guidance.

### Architecture Modules

*   **LLM-assisted Knowledge Maintenance Module**
    *   Function: Converts raw UI traces into an Abstract Interaction Graph (AIG).
    *   Mechanism: Merges semantically similar states and actions to handle dynamic content.
*   **Knowledge-guided Exploration Module**
    *   Function: Uses the AIG for context-aware action selection and navigation.
    *   Mechanism: Operates independently of the LLM during the actual navigation phase.

### Key Data Structures

*   **Abstract Interaction Graph (AIG):** A compact representation of the app's structure.
*   **Abstract UI States:** Groupings of functionally identical raw states.
*   **Abstract UI Actions:** Groupings of similar interactions defined by:
    *   Action type
    *   Target element
    *   Exploration flag
    *   Specific function

---

## Contributions

*   **LLM-Explorer Framework:** Introduction of a new exploration agent that redefines the role of LLMs in mobile app exploration from *action generators* to *knowledge maintainers*.
*   **Efficiency Paradigm Shift:** Establishment of a new standard for efficient and affordable automated UI testing by decoupling exploration logic from LLM inference.
*   **Validation of Compact Knowledge:** Empirical contribution demonstrating that guiding exploration via compact knowledge outperforms direct LLM intervention in both cost and speed.

---

## Results

The system was evaluated against 5 strong baselines across 20 typical mobile applications:

*   **Speed:** LLM-Explorer achieved the **fastest exploration speed** among tested explorers.
*   **Coverage:** Achieved the **highest Activity Coverage**.
*   **Cost Efficiency:** Achieved a **148 times** improvement in cost efficiency compared to state-of-the-art LLM-based approaches.
*   **Validation:** The results validate that querying LLMs at every step is unnecessary and that compact knowledge models provide superior guidance for exploration.

---
**References:** 40 Citations