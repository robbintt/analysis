# Policy Mirror Descent with Temporal Difference Learning: Sample Complexity under Online Markov Data

*Wenye Li; Hongxu Chen; Jiacai Liu; Ke Wei*

---

> ### üìä Quick Facts
>
> *   **Quality Score:** 8/10
> *   **Reference Count:** 12
> *   **Core Algorithms:** Expected TD-PMD, Approximate TD-PMD
> *   **Key Metric:** \(O(\varepsilon^{-2})\) Sample Complexity (Last-Iterate)
> *   **Setting:** Online Markovian Sampling / Unregularized MDP

---

## üìù Executive Summary

This research addresses the theoretical gap in characterizing the sample complexity of **Policy Mirror Descent (PMD)** when integrated with **Temporal Difference (TD) learning)** under the online Markovian sampling model. While PMD is a standard framework for policy optimization, existing analyses largely rely on generative sampling or IID data assumptions, which fail to capture the auto-correlated, sequential nature of real-world reinforcement learning data. Proving convergence for unregularized PMD algorithms using non-linear function approximation and TD-based updates in an online setting has been an open challenge, limiting the theoretical understanding of PMD's performance in practical environments.

The authors introduce two novel algorithms to resolve this gap: **Expected TD-PMD**, designed for off-policy settings, and **Approximate TD-PMD**, which handles mixed sampling policies. The core technical innovation is the fusion of PMD policy updates with a critic estimation mechanism that performs one-step Bellman updates on previous Q-function estimates. This methodological approach relies on a rigorous separation of step size regimes: employing small constant step sizes for average-time analysis and adaptive policy update step sizes for last-iterate analysis. This strategy allows for precise control over error propagation, utilizing tools such as the Performance Difference Lemma, Three-Point-Descent Lemma, and a specific bound on the Lipschitz smoothness of Q-values relative to the action space dimension.

The study establishes sample complexity bounds that significantly outperform the state of the art. Under online Markovian sampling, the authors achieve an average-time optimality of \(\tilde{O}(\varepsilon^{-2})\). More importantly, by utilizing adaptive policy update step sizes, they achieve last-iterate optimality with a sample complexity of \(O(\varepsilon^{-2})\). This represents the first \(O(\varepsilon^{-2})\) sample complexity result for PMD in unregularized MDPs, successfully eliminating both the logarithmic factors found in standard implementations and the prohibitive horizon dependencies‚Äîsuch as \((1-\gamma)^{-8}\)‚Äîassociated with standard PMD in online settings. These bounds highlight the stability and efficiency of the proposed approach compared to generative or regularized baselines.

This work significantly advances reinforcement learning theory by bridging the disconnect between theoretical optimization frameworks and practical data constraints. By demonstrating that PMD combined with TD learning can achieve near-minimax optimal rates without reliance on generative models or entropy regularization, the authors reinforce PMD's utility as a comprehensive algorithmic structure. The elimination of logarithmic factors and horizon dependencies in last-iterate convergence suggests that adaptive step size strategies are not merely theoretically interesting but can yield more robust performance in practice. This provides a solid theoretical foundation for TD-based methods operating in complex, online scenarios, confirming their viability for high-dimensional, unregularized problems.

---

## üîç Key Findings

*   **Algorithmic Proposals:** Developed two distinct algorithms:
    *   **Expected TD-PMD:** An off-policy approach computing expectations exactly for tabular settings.
    *   **Approximate TD-PMD:** A mixed policy approach designed for more generalized scenarios.
*   **Average-Time Optimality:** Achieved a sample complexity of \(\tilde{O}(\varepsilon^{-2})\) utilizing small constant step sizes.
*   **Last-Iterate Optimality:** Improved sample complexity to \(O(\varepsilon^{-2})\) through the use of adaptive policy update step sizes.
*   **Markovian Sampling Validity:** Established effective sample complexity bounds specifically under the online Markovian sampling model, moving beyond generative models.

---

## üõ†Ô∏è Methodology

The research employs a rigorous analytical framework centered on the integration of optimization and estimation techniques:

*   **Core Framework:** Utilizes **Policy Mirror Descent (PMD)** integrated with **Temporal Difference (TD) learning)**.
*   **Sampling Model:** Operates under the **online Markovian sampling model** rather than relying on generative sampling or pre-specified approximations.
*   **Convergence Analysis:**
    *   **Average-Time:** Analyzes performance using constant policy update step sizes.
    *   **Last-Iterate:** Analyzes performance using adaptive policy update step sizes to eliminate logarithmic factors.

---

## ‚öôÔ∏è Technical Details

**MDP Formulation**
*   Operates on a standard Markov Decision Process (MDP) defined as \((S, A, P, r, \gamma)\).
*   **Constraints:** Bounded rewards \(r(s, a) \in [0, 1]\) and discount factor \(\gamma \in [0, 1)\).

**Optimization Framework**
*   Uses Policy Mirror Descent (PMD) over the simplex of policies.
*   **Update Rule:**
    \[\pi^+ (\cdot|s) = \arg\max_{p \in \Delta(A)} \{ \eta_k \langle p, Q_k(s, \cdot) \rangle - D_h(p \| \pi(\cdot|s)) \}\]
*   **Special Cases:**
    *   **TD-NPG:** Negative entropy regularization.
    *   **TD-PQA:** Quadratic function regularization.

**TD-PMD Innovation**
*   Performs a one-step Bellman update on the previous critic estimate:
    \[Q_{k+1} = F^{\pi_{k+1}} Q_k + \text{error\_term}\]
*   Driven by Temporal Difference (TD) learning principles.

**Expected TD-PMD Algorithm**
*   **Type:** Off-policy.
*   **Update Rule:**
    \[Q_{k+1}(s, a) = Q_k(s, a) + \alpha_k \cdot \bar{\delta}_k(s, a)\]
*   Uses a behavior policy \(\pi_b\) to process trajectories of length \(B_k\).

**Theoretical Tools**
*   Performance Difference Lemma
*   Three-Point-Descent Lemma
*   Inductive Bias Bounding

---

## üìà Results & Contributions

### Complexity Bounds
The paper establishes several theoretical sample complexity bounds, demonstrating significant improvements over standard methods:

*   **Average-Time Optimality:** \(\tilde{O}(\varepsilon^{-2})\) (Small constant step size).
*   **Last-Iterate Optimality:** \(O(\varepsilon^{-2})\) (Adaptive policy update step sizes).
    *   *Note:* This is the first \(O(\varepsilon^{-2})\) sample complexity for PMD in unregularized MDPs.

### Performance Comparison
| Method | Type | Sample Complexity |
| :--- | :--- | :--- |
| **TD-PMD (Last-Iterate)** | Online Markovian | \(O(\varepsilon^{-2})\) |
| Standard PMD (Last-Iterate) | Online Markovian | \(\tilde{O}((1-\gamma)^{-8}\varepsilon^{-2})\) |
| Regularized PMD | Regularized | \(\tilde{O}(\varepsilon^{-1})\) |
| TD-PMD / h-PMD | Generative | \(\tilde{O}((1-\gamma)^{-7}\varepsilon^{-2})\) |

### Error Propagation
*   **Lemma 2.2 Bound:** Quantifies the Lipschitz smoothness of Q-values:
    \[\|Q^\pi - Q^{\pi'}\|_\infty \le \frac{\gamma|A|}{(1-\gamma)^2} \|\pi - \pi'\|_\infty\]

### Primary Contributions
*   **Theoretical Gap Bridging:** Provided sample complexity analysis for PMD combined with TD learning within the Markovian sampling model.
*   **Complexity Bounds Improvement:** Demonstrated tighter bounds by eliminating the logarithmic factor for last-iterate convergence.
*   **Generalization of PMD:** Reinforced PMD as a comprehensive framework covering policy gradient methods by applying it to TD learning scenarios with proven theoretical guarantees.