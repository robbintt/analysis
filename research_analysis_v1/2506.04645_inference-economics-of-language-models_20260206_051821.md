---
title: Inference economics of language models
arxiv_id: '2506.04645'
source_url: https://arxiv.org/abs/2506.04645
generated_at: '2026-02-06T05:18:21'
quality_score: 8
citation_count: 3
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Inference economics of language models

*Ege Erdil*

---

## ⚡ Quick Facts

| **Metric** | **Detail** |
| :--- | :--- |
| **Quality Score** | 8/10 |
| **References** | 3 Citations |
| **Key Performance** | 70 tokens/sec (Llama 3 70B) |
| **Hardware Utilization** | 15% |
| **Core Focus** | Cost vs. Speed Trade-offs |
| **Models Analyzed** | Dense Models, MoE, Attention Mechanisms |

---

## Executive Summary

The deployment of Large Language Models (LLMs) presents a complex optimization challenge where practitioners must balance the conflicting objectives of minimizing cost per token and maximizing serial generation speed. Current benchmarks often focus solely on latency, failing to capture the economic efficiency required for scalable, real-world deployment. This paper addresses the critical need for a rigorous framework to understand the physical and economic constraints governing inference, enabling engineers to move beyond heuristic trial-and-error toward mathematically grounded decisions regarding hardware utilization and resource allocation.

The key innovation is a theoretical framework, analogous to the roofline model used in high-performance computing, which quantifies the trade-offs between cost and speed by bounding inference performance through four physical constraints: **arithmetic capabilities** (compute), **memory bandwidth** (HBM), **network bandwidth**, and **system latency**. The model mathematically derives the optimal batch size ($b^*$) as the specific point where memory read time equals arithmetic time ($b^* = p \cdot C / (B \cdot 2 \text{FLOP})$), establishing that beyond this threshold, distributing requests across multiple instances is superior to increasing batch size. This methodology is extended to handle complex architectural elements, including Attention Mechanisms, Mixture-of-Experts (MoE) with probabilistic memory reads, and various parallelism strategies (tensor, data, pipeline, and expert), allowing for the simulation of diverse deployment configurations.

The study demonstrates the model's predictive accuracy by evaluating Llama 3 70B on a DGX H100, achieving a serial speed of 70 tokens per second with a hardware utilization rate of 15%. Validations against PaLM models (ranging from 8B to 540B parameters) on TPU v4 clusters confirmed that the theoretical model accurately predicts physical limits, with best achievable latencies following a square root scaling law. Despite neglecting complex secondary effects, the framework’s theoretical predictions remained remarkably close to empirical results from prior research (Pope et al., 2022), successfully generating Pareto frontiers that visually and quantitatively represent the diminishing returns between increasing serial speed and minimizing cost.

This research significantly impacts the field by establishing a comprehensive economic model for LLM deployment that integrates cost efficiency directly into performance analysis. It provides practitioners with a methodological tool to identify optimal parallelism and batch size configurations, ensuring maximal performance for any specific budget or cost constraint. By delivering computed Pareto frontiers for popular models, the paper offers a vital baseline for comparing the economic efficiency of different inference strategies and hardware configurations, guiding the industry toward more cost-effective large-scale model deployment.

---

## Key Findings

*   **Theoretical Framework:** Establishes a rigorous economic model defining the trade-off between cost per token and serial token generation speed for large-scale LLM deployment.
*   **Physical Constraints:** Identifies that inference performance is strictly bounded by four primary physical constraints:
    *   Arithmetic capabilities
    *   Memory bandwidth
    *   Network bandwidth
    *   System latency
*   **Optimization Strategy:** Optimal inference performance is achieved by specifically tuning parallelism setups and batch sizes to meet targeted cost and speed objectives.
*   **Pareto Frontiers:** Successfully computes Pareto frontiers for popular language models, providing a visual and quantitative representation of the diminishing returns between increasing serial speed and minimizing cost per token.

---

## Methodology

The authors constructed a theoretical model to simulate the deployment of Large Language Models (LLMs) at scale. The approach involves:

1.  **Model Integration:** Integrating specific hardware and network variables, including arithmetic limits, memory bandwidth, network bandwidth, and latency constraints.
2.  **Optimization Process:** Iterating over different deployment configurations, specifically varying parallelism setups and batch sizes.
3.  **Pareto Calculation:** Using the model to calculate Pareto frontiers, mapping the optimal set of solutions where serial inference speed cannot be increased without increasing the cost per token.

---

## Technical Details

The paper proposes a theoretical framework, analogous to the roofline model, to quantify economic trade-offs (cost vs. speed) in Large Language Model (LLM) inference.

### Architecture & Constraints
The architecture relies on four physical constraints:
*   **Arithmetic capabilities** (Compute)
*   **Memory bandwidth** (HBM)
*   **Network bandwidth**
*   **System latency**

### Mathematical Formulation
For a single device with a dense model, token latency is defined as the maximum of:
1.  Time required to read parameters: $p \cdot N_{param} / B$
2.  Time required to perform arithmetic: $2 \cdot \text{FLOP} \cdot N_{param} \cdot b / C$

**Optimal Batch Size ($b^*$):**
Derived as the point where memory read time equals arithmetic time:
$$b^* = \frac{p \cdot C}{B \cdot 2 \text{FLOP}}$$

The framework suggests distributing requests across multiple instances rather than increasing batch size beyond $b^*$.

### Model Extensions
The model is extended to include:
*   **Attention Mechanisms:** Handling query-key inner products and KV cache.
*   **Mixture-of-Experts (MoE):** Incorporating sparsity factors and probabilistic memory reads.
*   **Parallelism Strategies:** Accounting for tensor, data, pipeline, and expert parallelism, including finite network bandwidth and operation latencies.

---

## Results

*   **Llama 3 70B Evaluation:** Evaluated on a DGX H100, reporting a serial speed of **70 tokens/second** with a hardware utilization rate of **15%**.
*   **PaLM Validation:** Validated against PaLM models (8B, 62B, 540B) on TPU v4 clusters. The theoretical 'toy model' accurately predicts physical limits, with best achievable latencies following a **square root scaling law**.
*   **Empirical Alignment:** The model's theoretical predictions remained remarkably close to empirical results from Pope et al. (2022), despite neglecting complex effects like attention and network constraints.
*   **Visualization:** The framework utilizes Pareto frontiers to visualize trade-offs between serial speed and cost per token, quantifying the point of diminishing returns.

---

## Contributions

*   **Economic Modeling:** Provides a rigorous economic model that assists practitioners in making cost-benefit decisions when deploying LLMs, moving beyond simple latency benchmarks to include cost efficiency.
*   **Optimization Tool:** Offers a methodological tool for identifying the optimal parallelism and batch size configurations required to maximize speed for any given budget or cost constraint.
*   **Performance Baselines:** Delivers computed Pareto frontiers for popular models, serving as a baseline for comparing the economic efficiency of different inference strategies and hardware configurations.