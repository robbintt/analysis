# Multimodal Large Language Models for Low-Resource Languages: A Case Study for Basque

*Lukas Arana; Julen Etxaniz; Ander Salaberria; Gorka Azkune*

---

> üìä **Quick Facts**
> *   **Target Language:** Basque (Low-resource)
> *   **Optimal Data Mixture:** ~20% Basque multimodal data
> *   **Core Backbones:** Llama-3.1-Instruct vs. Latxa (Basque-adapted)
> *   **Base Architecture:** Llava codebase
> *   **Validation:** 4 annotators, 100 annotations per benchmark
> *   **Quality Score:** 8/10

---

## üìù Executive Summary

The rapid advancement of Multimodal Large Language Models (MLLMs) has largely bypassed low-resource languages due to a scarcity of training data and evaluation benchmarks. While languages like English dominate the commercial landscape, languages such as Basque face significant barriers to entry, resulting in a performance gap between open-source and proprietary models. This paper addresses the challenge of developing capable, open-source MLLMs for low-resource languages without relying on prohibitively expensive commercial APIs or inaccessible massive-scale datasets.

The authors developed a reproducible methodology for adapting MLLMs to low-resource languages using Basque as a case study. Building on the Llava codebase, they created proprietary Basque image-text datasets by translating and validating established benchmarks (VQAv2, A-OKVQA, PixMo-CapQA) and utilizing Pangea hyperparameter recommendations. The technical novelty lies in a systematic comparison of training efficiency using two distinct backbone architectures‚Äîa standard Llama-3.1-Instruct model versus a Basque-adapted Latxa model‚Äîwhile varying the mixture ratio of Basque multimodal instruction data from 0% to 100%.

The study demonstrates that high performance on Basque multimodal benchmarks can be achieved using surprisingly little native data, with optimal results reached at approximately 20% Basque multimodal data mixed with English data. Contrary to conventional assumptions, the researchers found that utilizing a monolingual, Basque-instructed backbone LLM is not a prerequisite for success; models fine-tuned on the standard Llama backbone performed competitively with those using the specialized Latxa backbone. Human validation of the outputs showed high acceptance rates, confirming that targeted dataset creation allows open-source models to rival commercial offerings in this domain.

This research significantly lowers the barrier to entry for deploying generative AI in linguistically underrepresented regions. By proving that specialized base models and massive amounts of target-language data are not strictly necessary, the authors outline a cost-effective pathway for adapting MLLMs to other low-resource languages. Furthermore, the open release of the resulting datasets and models provides a valuable scientific resource, helping to bridge the digital divide and reduce reliance on closed-source commercial ecosystems for low-resource language communities.

---

## üîç Key Findings

*   **Data Efficiency:** High performance on Basque benchmarks can be achieved using a relatively low ratio of Basque multimodal data, specifically approximately **20%** mixed with English data.
*   **Backbone Flexibility:** Utilizing a Basque-instructed backbone LLM (like Latxa) is **not a requirement** for building a strong Multimodal LLM in Basque; standard models like Llama-3.1-Instruct perform competitively.
*   **Closing the Gap:** It is possible to bridge the performance gap between commercial (proprietary) and open-source MLLMs for low-resource languages through targeted dataset creation and fine-tuning.

---

## üß™ Methodology

The research team employed a structured approach to model development and evaluation:

1.  **Dataset Development:**
    *   Developed proprietary training and evaluation image-text datasets specifically for Basque.
    *   Curated data from established benchmarks to ensure high quality.

2.  **Backbone Selection:**
    *   Utilized two distinct LLM backbones for comparison:
        *   **Standard:** Llama-3.1-Instruct
        *   **Specialized:** Latxa (a Basque-adapted variant)

3.  **Training & Experimentation:**
    *   Trained MLLMs using the selected backbones.
    *   Experimented with various data mixtures (varying percentages of Basque vs. English data) to determine the optimal volume of target-language data required.

---

## ‚öôÔ∏è Technical Details

*   **Architecture:** Built upon the **Llava** codebase.
*   **Comparison Models:** Latxa-based vs. Llama-based backbone architectures.
*   **Training Protocol:** Followed **Pangea** hyperparameter recommendations; varied the percentage of Basque Multimodal Instruction Data (0% to 100%) to determine efficiency.
*   **Evaluation Suite:** Utilized the **LLMs-eval** suite, extended to support:
    *   A-OKVQA
    *   PixMo-CapQA
    *   VQAv2
*   **Translation Process:** For the WildVision dataset, initial translation was performed using **Latxa-Llama-3.1-Instruct 70B**, followed by manual human review.

---

## üìà Results & Contributions

### Results
*   **Optimal Data Ratio:** Confirmed that ~20% Basque multimodal data is sufficient for high performance.
*   **Backbone Efficacy:** Validated that a Basque-instructed backbone is not a strict requirement.
*   **Datasets utilized:** VQAv2, A-OKVQA, PixMo-CapQA, and a filtered WildVision (199 pairs).
*   **Validation Metrics:** Involvement of 4 annotators with 100 annotations per benchmark showed high acceptance rates.
*   **Performance:** Targeted dataset creation successfully bridges the gap between commercial and open-source MLLMs.

### Contributions
*   **Open Science:** The authors provide an open release of their developed resources (datasets and models), directly addressing the scarcity of tools for low-resource languages.
*   **Cost-Effective Pathway:** Established a framework for developing MLLMs for other low-resource languages, proving that specialized backbones and massive target-language datasets are not strictly necessary.

---
**Quality Score:** 8/10 | **References:** 0 citations