---
title: 'BBox DocVQA: A Large Scale Bounding Box Grounded Dataset for Enhancing Reasoning
  in Document Visual Question Answer'
arxiv_id: '2511.1509'
source_url: https://arxiv.org/abs/2511.15090
generated_at: '2026-02-05T09:33:02'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# BBox DocVQA: A Large Scale Bounding Box Grounded Dataset for Enhancing Reasoning in Document Visual Question Answer

*Wenhan Yu; Wang Chen; Guanqiang Qi; Weikang Li; Yang Li; Lei Sha; Deguo Xia; Jizhou Huang*

---

> ### üìã Quick Facts
>
> | Metric | Value |
> | :--- | :--- |
> | **Total Documents** | 3.6K |
> | **Total QA Pairs** | 32K (30,780 in training set) |
> | **Source Material** | 4,000 arXiv PDFs (137k pages) |
> | **Domains Covered** | 8 |
> | **Benchmark Size** | 1,623 manually verified pairs |
> | **Quality Score** | 8/10 |
> | **Citations** | 40 |

---

## üìù Executive Summary

Current Vision Language Models (VLMs) demonstrate strong capabilities in general document understanding but face critical limitations in **spatial reasoning** and **fine-grained grounding**. Existing Document Visual Question Answering (DocVQA) datasets are predominantly constrained to page-level annotations, lacking the precise bounding box data necessary for models to localize specific regions accurately. This deficiency hinders interpretability and prevents models from performing robust reasoning in complex scenarios involving multi-page documents or multi-region identification. The paper addresses this "spatial grounding gap," highlighting that even state-of-the-art models like GPT-5 and Qwen2.5-VL struggle to identify *where* an answer is located, even when they understand *what* the answer is.

The authors introduce **BBox DocVQA**, a large-scale dataset comprising 3.6K documents and 32K QA pairs, constructed via a novel automated pipeline named **"Segment-Judge-and-Generate."** Technically, the framework begins by employing the Segment Anything Model (SAM) to generate candidate bounding boxes, which are filtered by area (5%-70%). In the "Judge" stage, a VLM (Qwen2.5-VL-72B) performs semantic analysis, content classification (Text, Table, Image), and overlap-based deduplication. The "Generate" stage utilizes an advanced VLM (GPT-5) to create Question-Answer pairs based on page-level summarization. Crucially, every instance is explicitly grounded with bounding boxes to support the evaluation of spatial-semantic alignment across three task granularities: single-page single-box, single-page multi-box, and multi-page multi-box.

The dataset comprises 30,780 QA pairs with a content distribution of **60.98% Text**, **25.14% Image**, and **13.88% Table**. Evaluations conducted on Qwen2.5-VL models reveal a significant performance disparity between semantic understanding and spatial localization. The 7B model achieved a General Answer accuracy of 68.58% but a BBox Answer accuracy of only **11.40%**. Similarly, the 3B model scored 56.38% on general answers compared to just **10.66%** on bounding box localization. Furthermore, fine-tuning models on the BBox DocVQA dataset was shown to significantly improve both bounding box localization precision and answer generation quality, validating the utility of fine-grained spatial data.

This work significantly advances the field by providing a rigorous new benchmark that exposes the spatial reasoning shortcomings of current SOTA models. By releasing the dataset and construction code publicly, the authors enable the broader research community to develop and train more interpretable, spatially grounded VLMs. The findings suggest that integrating fine-grained spatial data is essential for bridging the gap between visual perception and logical reasoning in document analysis.

---

## üîç Key Findings

*   **SOTA Limitations:** State-of-the-art Vision Language Models (GPT-5, Qwen2.5-VL, InternVL) continue to struggle with spatial grounding and reasoning accuracy.
*   **Performance Improvement:** Fine-tuning models on the BBox DocVQA dataset significantly improves bounding box localization precision and answer generation quality.
*   **Complex Scenario Handling:** The dataset effectively handles diverse, complex scenarios including single and multi-region identification and single and multi-page document structures.
*   **Data Gap:** Existing DocVQA datasets are limited by page-level constraints and a lack of fine-grained spatial grounding, hindering interpretability.

---

## üõ†Ô∏è Methodology

The authors implemented an automated construction pipeline named **'Segment Judge and Generate.'** This process ensures high-quality, grounded data through three distinct steps:

1.  **Segmentation:**
    *   Utilizes a segment model to identify regions within the documents.
2.  **Judgment:**
    *   A VLM performs semantic analysis on the segmented regions to assess quality and relevance.
3.  **Generation:**
    *   An advanced VLM creates Question-Answer pairs based on the analyzed segments.

**Quality Assurance:**
The automated process includes human verification for quality assurance. Every QA instance is explicitly grounded with bounding boxes to facilitate fine-grained evaluation of spatial-semantic alignment.

---

## ‚öôÔ∏è Technical Details

*   **Framework Name:** Segment-Judge-and-Generate
*   **Source Data:** 4,000 arXiv PDFs (137k pages) across 8 domains.
*   **Segment Stage:**
    *   **Model:** SAM (ViT-H)
    *   **Filtering:** Bounding boxes filtered by area (5%-70%).
*   **Judge Stage:**
    *   **Model:** Qwen2.5-VL-72B
    *   **Functions:** Quality assessment, content classification (Text, Table, Image), and overlap-based deduplication.
*   **QA Generation:**
    *   **Model:** GPT-5
    *   **Method:** Uses page-level summarization to generate pairs.
*   **Task Granularities:**
    *   **SPSBB:** Single-Page Single-Bounding Box
    *   **SPMBB:** Single-Page Multi-Bounding Box
    *   **MPMBB:** Multi-Page Multi-Bounding Box
*   **Benchmark:** Includes a manually verified set of 1,623 pairs.

---

## üìä Results

**Dataset Composition:**
*   **Total Training Pairs:** 30,780
*   **Content Distribution:**
    *   Text: 60.98%
    *   Image: 25.14%
    *   Table: 13.88%
*   **Task Distribution:**
    *   SPSBB: 37.91%
    *   SPMBB: 24.41%
    *   MPMBB: 37.69%

**Model Evaluation (Qwen2.5-VL):**
Evaluations reveal a significant spatial grounding gap between general understanding and precise localization:

| Model | GT Sub-page Answer | BBox Answer |
| :--- | :--- | :--- |
| **Qwen2.5-VL 7B** | 68.58% | **11.40%** |
| **Qwen2.5-VL 3B** | 56.38% | **10.66%** |

*Note: The low BBox Answer scores highlight the difficulty of precise bounding box localization for current VLMs.*

---

## üèÜ Contributions

*   **BBox DocVQA Dataset:** Introduction of a large-scale resource containing 3.6K diverse documents and 32K QA pairs designed for fine-grained, bounding box grounded spatial reasoning.
*   **Pipeline Innovation:** Presentation of the 'Segment Judge and Generate' pipeline, a novel automated framework for integrating segmentation and VLMs to produce high-quality, grounded QA data.
*   **New Benchmark:** Provision of a new evaluation benchmark that exposes the spatial reasoning shortcomings of current models and validates that fine-grained spatial data enhances model capabilities.
*   **Open Source:** Public release of the dataset and code to foster further research into interpretable and spatially grounded vision language reasoning.

---
**References:** 40 citations