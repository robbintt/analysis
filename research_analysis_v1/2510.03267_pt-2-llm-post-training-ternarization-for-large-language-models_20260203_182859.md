---
title: 'PT$^2$-LLM: Post-Training Ternarization for Large Language Models'
arxiv_id: '2510.03267'
source_url: https://arxiv.org/abs/2510.03267
generated_at: '2026-02-03T18:28:59'
quality_score: 7
citation_count: 8
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# PTÂ²-LLM: Post-Training Ternarization for Large Language Models

*Xianglong Yan; Chengzhu Bao; Zhiteng Li; Tianao Zhang; Kaicheng Yang; Haotong Qin; Ruobing Xie; Xingwu Sun; Yulun Zhang*

---

> ### ðŸ“Š Quick Facts
>
> *   **Quality Score:** 7/10
> *   **References:** 8 Citations
> *   **Target Architecture:** LLaMA (7B, 13B, 30B, 65B)
> *   **Compression Level:** Sub-2-bit (Ternary {-1, 0, +1})
> *   **Performance Baseline:** Comparable to SOTA 2-bit PTQ methods
> *   **Key Benefit:** Up to 4x theoretical inference speedup

---

## Executive Summary

Deploying Large Language Models (LLMs) is computationally challenging due to high memory costs and slow operations. While compressing weights to ternary values offers efficiency, it often leads to severe accuracy loss. The paper introduces **PTÂ²-LLM**, a training-free framework utilizing an **Asymmetric Ternary Quantizer (ATQ)**, a two-stage optimization pipeline (**Iterative Ternary Fitting** and **Activation-aware Grid Alignment**), and **Structural Similarity-based Reordering (SSR)** to handle asymmetric distributions and outliers effectively.

Evaluations on LLaMA models show that PTÂ²-LLM maintains performance comparable to FP16 baselines, staying within 1% accuracy on Zero-shot QA tasks and offering up to 4x theoretical speedups. This innovation enables high-fidelity, sub-2-bit compression without retraining, significantly lowering the barrier for deploying models on resource-constrained edge devices.

---

## Key Findings

*   **High Performance:** PTÂ²-LLM achieves performance comparable to state-of-the-art (SOTA) 2-bit Post-Training Quantization (PTQ) methods, despite being a ternarization (sub-2-bit) approach.
*   **Efficiency Gains:** The framework delivers lower memory costs and accelerates both prefill and decoding stages, resulting in significant end-to-end speedup.
*   **Outlier Handling:** The method successfully addresses the challenge of quantizing outliers and dispersed weights in a training-free (PTQ) environment.
*   **No Retraining Required:** It maintains high accuracy without the need for backpropagation or retraining.

---

## Methodology

The PTÂ²-LLM framework is a post-training ternarization strategy for Large Language Models that operates without retraining. It consists of three core components:

1.  **Asymmetric Ternary Quantizer (ATQ):**
    *   Addresses asymmetry in pre-trained weight distributions.
    *   Introduces a row-wise offset (bias) and scaling factor to calculate dequantized weights.

2.  **Two-Stage Refinement Pipeline:**
    *   **Iterative Ternary Fitting (ITF):** Used for optimal grid construction and rounding.
    *   **Activation-aware Grid Alignment (AGA):** Adjusts grids to match full-precision activations.

3.  **Structural Similarity-based Reordering (SSR):**
    *   A preprocessing strategy that reorders weights based on inter-column structural similarity.
    *   Designed to ease quantization and mitigate outlier effects.

---

## Contributions

The paper makes three distinct contributions to the field of LLM compression:

*   **Advancement in Ternarization:** Provides a viable solution for training-free parameter optimization and handling weight dispersion in post-training ternarization.
*   **Novel Optimization Pipeline:** Introduces a two-stage optimization pipeline (ITF and AGA) to minimize quantization error and align grids with activation statistics without backpropagation.
*   **New Outlier Strategy:** Develops the SSR strategy, a new training-free approach to handling outliers by leveraging structural similarity rather than relying solely on activation clipping or transformation.

---

## Technical Details

**Framework Type:** Training-free Post-Training Quantization (PTQ)
**Compression Target:** Ternary values $\{-1, 0, +1\}$

**Core Components:**

*   **Asymmetric Ternary Quantizer (ATQ):**
    *   Utilizes row-wise offsets and scaling factors to manage asymmetric weight distributions.
*   **Iterative Ternary Fitting (ITF):**
    *   Refines quantization parameters by minimizing the Frobenius norm error.
*   **Activation-aware Grid Alignment (AGA):**
    *   Optimizes the quantization grid based on activation statistics to improve model fidelity.
*   **Structural Similarity-based Reordering (SSR):**
    *   Reorders weights to group structurally similar columns, reducing the impact of outliers on the quantization process.

---

## Results

The proposed method was evaluated on **LLaMA models** (7B, 13B, 30B, 65B) using average accuracy across 7 Zero-shot QA datasets and model size metrics.

*   **Accuracy Retention:** PTÂ²-LLM demonstrates superior accuracy retention compared to Slim-LLM, GPTQ, and PB-LLM at equivalent memory costs.
*   **Comparison to 2-bit:** It achieves performance comparable to state-of-the-art 2-bit PTQ methods despite being a sub-2-bit approach.
*   **Inference Speed:** Offers inference speedup in both prefill and decoding stages by eliminating floating-point multiplications.
*   **Robustness:** Effectively handles outliers and dispersed weights without retraining or rotation transformations.