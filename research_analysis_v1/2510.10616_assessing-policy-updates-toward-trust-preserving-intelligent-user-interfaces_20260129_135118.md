# Assessing Policy Updates: Toward Trust-Preserving Intelligent User Interfaces

*Matan Solomon; Ofra Amir; Omer Ben-Porat*

---

> ## ðŸ“Š Quick Facts
>
> *   **Quality Score:** 9/10
> *   **Citations:** 35 references
> *   **Study Scale:** 150 participants
> *   **Framework:** Human-in-the-Loop (HITL) Reinforcement Learning
> *   **Environment:** Gridworld
> *   **Top Accuracy (Salient-Contrast):** 84.1%
> *   **Statistical Significance:** $p < 0.001$

---

## Executive Summary

This research addresses a critical verification gap in Human-in-the-Loop (HITL) Reinforcement Learning (RL): the inability of end-users to effectively assess whether an agent's updateâ€”triggered by their feedbackâ€”has resulted in genuine performance improvement or degradation. As autonomous agents learn from interactive human feedback, the risk of unintended negative outcomes increases due to noisy or unreliable signals. Unlike standard Intelligent User Interfaces (IUIs) that evaluate static models, HITL systems currently lack mechanisms for non-experts to audit iterative changes. This deficiency creates a barrier to maintaining appropriate trust levels and control, as users implicitly assume their feedback is beneficial without a reliable method to verify the results.

The paper introduces **"salient-contrast demonstrations,"** a communication strategy designed to reveal the behavioral delta between an agent's original and updated policies. To rigorously evaluate this approach, the researchers conducted a controlled user study where participants were actively involved in the feedback loop; they first provided feedback to an RL agent and were subsequently tasked with comparing the agent's pre-update and post-update policies. The system utilized a bank of pre-trained policies to simulate these updates, specifically selecting contexts that maximized the disagreement between the two agents.

Unlike baseline methods (No Demonstration, Same-Context, and Random-Context) which fail to isolate the specific impact of the update, the salient-contrast approach algorithmically identifies and highlights scenarios where the new and old agents behave differently, allowing users to visually inspect the precise consequences of the policy change.

The study demonstrated that the salient-contrast method significantly outperformed all baseline conditions in assessment accuracy. With 150 participants, users achieved an accuracy of 84.1% using salient-contrast demonstrations, compared to 72.9% with no demonstration, 67.8% with random-context demonstrations, and 61.0% with same-context demonstrations. Statistical analysis confirmed these differences were highly significant ($p < 0.001$).

The proposed method was particularly effective at detecting policy deterioration, countering the **"automation bias"** prevalent in participants who assumed updates were always beneficial. Furthermore, salient-contrast demonstrations achieved a significantly lower Mean Absolute Error (MAE) in trust calibration than the other display strategies, ensuring that user trust aligned more closely with the agent's actual performance.

---

## Key Findings

*   **Superior Performance of Salient-Contrast:** Communication strategies utilizing 'salient-contrast' demonstrations significantly outperformed other methods, including no demonstration, same-context, and random-context displays.
*   **Accuracy in Assessment:** Users provided with salient-contrast demonstrations were significantly better at correctly identifying whether a policy update had resulted in performance improvement or degradation.
*   **Mitigation of Automation Bias:** The study identified a user bias toward assuming feedback is always beneficial. Salient-contrast demonstrations successfully mitigated this bias.
*   **Improved Trust Calibration:** The proposed demonstration strategy supported better trust calibration across varying contexts, aligning user perceptions with actual agent performance.

---

## Methodology

The researchers employed a rigorous experimental design to evaluate how users assess policy updates:

1.  **Environment:** A controlled user study was conducted within a gridworld environment.
2.  **Procedure:** Participants provided feedback to a reinforcement learning agent. Subsequently, they were tasked with comparing the agent's original policy against its updated policy to evaluate the change.
3.  **Comparison Strategies:** To evaluate communication effectiveness, the study compared four distinct display strategies:
    *   **No Demonstration:** Users received no visual aid regarding the policy change.
    *   **Same-Context Demonstration:** Demonstrations were provided in the same context as the user's interaction.
    *   **Random-Context Demonstration:** Demonstrations were provided in a randomly selected context.
    *   **Salient-Contrast Demonstration:** The proposed method, providing demonstrations specifically chosen to highlight differences between policies.

---

## Contributions

This paper makes several significant contributions to the field of Intelligent User Interfaces and Human-AI Interaction:

*   **Defining the Challenge:** Establishes 'assessing model updates' as a critical design challenge for Intelligent User Interfaces (IUIs). This shifts the research focus from evaluating single static models to evaluating the dynamic change between models.
*   **Empirical Evidence:** provides concrete empirical evidence for the effectiveness of salient-contrast demonstrations. This proves to be a robust mechanism for helping humans interpret opaque reinforcement learning policies.
*   **Solving Feedback Unreliability:** Addresses the practical issue of unreliable feedback caused by reward misspecification or limited data. It offers a practical method for users to verify the utility of updates, ensuring the feedback loop remains positive.

---

## Technical Details

**System Architecture & Framework**
*   **Environment:** Grid-world simulation.
*   **Framework:** Human-in-the-Loop (HITL) Reinforcement Learning.
*   **Simulation Strategy:** Utilizes a bank of pre-trained policies to simulate updates rather than performing online fine-tuning, ensuring consistency across participants.

**Communication Architectures**
The system evaluates four distinct strategies for visually communicating policy updates:
1.  No Demonstration
2.  Same-Context Demonstration
3.  Random-Context Demonstration
4.  **Salient-Contrast Demonstration (Proposed)**

**Salient-Contrast Mechanism**
*   **Selection Logic:** The approach selects contexts by maximizing the disagreement between the old and new policies.
*   **Objective:** To highlight the most informative differences by specifically identifying segments where Q-values diverge.

---

## Results

The user study yielded statistically significant results highlighting the efficacy of the proposed method:

*   **Accuracy Outperformance:** The Salient-Contrast demonstration method significantly outperformed all baselines (No Demonstration, Same-Context, and Random-Context).
*   **Detection Capability:** Users utilizing Salient-Contrast demonstrations were significantly more accurate in identifying whether a policy update resulted in improvement or degradation.
*   **Bias Reduction:** The method was particularly effective at detecting policy deterioration and successfully mitigated 'automation bias' (the tendency to assume updates are always beneficial).
*   **Trust Alignment:** The strategy resulted in better trust calibration, with lower Mean Absolute Error (MAE) compared to other strategies, ensuring user trust matched actual agent performance.

---

**Quality Score:** 9/10  
**References:** 35 citations