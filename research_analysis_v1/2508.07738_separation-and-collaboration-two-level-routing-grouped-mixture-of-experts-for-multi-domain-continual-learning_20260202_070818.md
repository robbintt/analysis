# Separation and Collaboration: Two-Level Routing Grouped Mixture-of-Experts for Multi-Domain Continual Learning

*Jialu Zhou; Dianxi Shi; Shaowu Yang; Xinyu Wei; Mingyue Yang; Leqian Li; Mengzhu Wang; Chunping Qiu*

---

> ### ðŸ“Š Quick Facts
>
> *   **Quality Score:** 8/10
> *   **Parameter Efficiency:** 61.07% fewer parameters than MoE-Adapter
> *   **MTIL Performance:** Avg improvements of 1.35%, 2.35%, and 3.10%
> *   **MCIL Performance:** Up to 3.20% improvement
> *   **Hardware:** NVIDIA RTX 4090 GPUs
> *   **Citations:** 14

---

## Executive Summary

This research tackles the inherent challenges of **Multi-Domain Continual Learning (MDCL)**, where models must sequentially learn from distinct data domains without accessing previous training data. The primary issues addressed are **catastrophic forgetting** (the degradation of performance on past tasks) and **forward forgetting** (the loss of generalization capability on unseen, future data). These problems are critical bottlenecks in developing adaptive AI systems that must operate in dynamic environments; without a solution, models become obsolete as quickly as they learn.

The authors propose the **Two-Level Routing Grouped Mixture-of-Experts (TRGE)** framework, which expands a frozen pre-trained `CLIP` model to balance knowledge isolation and collaboration. The architecture hinges on a "Grouped Mixture-of-Experts" design that assigns specific expert groups to tasks to prevent interference, while a two-level routing mechanism optimizes selection. A key technical novelty is the integration of **Multimodal Large Language Models (MLLMs)** to generate semantic task descriptions, ensuring accurate task identification. Meanwhile, a dynamic fusion strategy combines the frozen `CLIP` features with the `TRGE` adapter outputs to explicitly handle unseen samples.

**TRGE** achieved state-of-the-art performance against strong baselines across both `MTIL` and `MCIL` benchmarks. Specifically, the method realized average accuracy improvements of up to **3.20%** while effectively alleviating catastrophic forgetting. Beyond accuracy, `TRGE` demonstrated exceptional parameter efficiency, requiring **61.07%** fewer trainable parameters than the competing `MoE-Adapter` method.

---

## Key Findings

*   **Comprehensive Forgetting Mitigation:** The proposed `TRGE` effectively addresses both catastrophic forgetting (loss of old knowledge) and forward forgetting (loss of generalization on new data).
*   **Superior Performance:** Achieves state-of-the-art results compared to advanced methods across various experimental settings.
*   **High Parameter Efficiency:** Maintains high parameter efficiency, requiring significantly fewer trainable parameters than competing approaches.
*   **Enhanced Task Identification:** Leveraging Multimodal Large Language Models (`MLLMs`) significantly improves the accuracy of task identification through semantic description generation.

---

## Methodology

The proposed method dynamically expands a pre-trained `CLIP` model. The core workflow involves:

*   **Architecture Expansion:** Utilizes a **Two-Level Routing Grouped Mixture-of-Experts (TRGE)** framework.
*   **Expert Assignment:** Assigns specific expert groups to each task to preserve knowledge. This ensures a static count of experts within groups and utilizes an **intra-group router** to prevent routing overfitting.
*   **Dynamic Routing:** An **inter-group routing policy** dynamically selects relevant expert groups based on task identifiers and task prototype distances. It combines these outputs to foster inter-task collaboration.
*   **Semantic Integration:** **Multimodal Large Language Models (MLLMs)** are utilized to generate semantic task descriptions and accurately recognize task identifiers.
*   **Output Fusion:** The system dynamically fuses outputs from the frozen `CLIP` model and the `TRGE` adapter for unseen samples to mitigate forward forgetting.

---

## Contributions

1.  **Novel Framework:** A Grouped Mixture-of-Experts framework that balances the separation of task-specific knowledge with collaboration between tasks.
2.  **Two-Level Routing:** A routing mechanism comprising:
    *   **Intra-group routing:** Manages complexity within expert groups.
    *   **Inter-group routing:** Facilitates dynamic expert group selection across tasks.
3.  **LLM Integration:** Integration of Multimodal Large Language Models (`MLLMs`) into the continual learning pipeline to solve the challenge of task identity recognition.
4.  **Dynamic Fusion Strategy:** Leverages frozen pre-trained features alongside new adapter outputs to mitigate forward forgetting.

---

## Technical Details

### Core Architecture
The system is built upon the **Two-Level Routing Grouped Mixture-of-Experts (TRGE)** designed specifically for Multi-Domain Continual Learning.

### Key Components
| Component | Function |
| :--- | :--- |
| **Expert Grouping** | Isolates task knowledge to prevent interference between tasks. |
| **Inter-Group Router** | Performs weighted combination of groups to facilitate collaboration. |
| **Semantic Task Router** | Uses `MLLMs` to generate semantic descriptions, improving task ID accuracy. |
| **Dynamic Fusion** | A fine-tuning module strategy that combines frozen and new adapter outputs. |

### Training Strategy
*   **Partial Training Strategy:** Only a subset of experts are trained during the process to significantly reduce the total number of trainable parameters.

---

## Results

### Benchmark Performance
Experiments were conducted on **NVIDIA RTX 4090** GPUs against strong baselines, including *Continual-FT, LwF, iCaRL, ZSCL, MoE-Adapter, Qwen-VL,* and *CLIP*.

*   **MTIL Benchmarks:** `TRGE` outperformed all baselines with average improvements of **1.35%**, **2.35%**, and **3.10%**.
*   **MCIL Benchmarks:** Achieved improvements of **0.57%**, **1.58%**, and **3.20%** while effectively alleviating catastrophic forgetting.

### Efficiency Metrics
*   **Parameter Savings:** `TRGE` achieved **61.07%** fewer parameters than the competing `MoE-Adapter` method.

### Ablation Studies
*   Studies confirmed that **Expert Grouping** is critical in mitigating forgetting.
*   The **Inter-Group Router** was validated as a key factor in improving overall accuracy.