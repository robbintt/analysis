# Scaling Law for Quantization-Aware Training

*Mengzhao Chen; Chaoyi Zhang; Jing Liu; Yutao Zeng; Zeyue Xue; Zhiheng Liu; Yunshui Li; Jin Ma; Jie Huang; Xun Zhou; Ping Luo*

---

### üìã Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Focus** | Quantization-Aware Training (QAT) Scaling Laws |
| **Precision Scope** | W4A4 (INT4 vs. FP4) |
| **Experimental Scale** | 268 QAT Experiments |
| **Key Variables** | Model Size ($N$), Training Tokens ($D$), Granularity ($G$) |
| **Critical Bottleneck** | FC2 Layer Activation Error (Outliers) |
| **Optimal Learning Rate** | $1 \times 10^{-3}$ to $3 \times 10^{-3}$ |
| **Quality Score** | 8/10 |
| **Citations** | 40 References |

---

## üìù Executive Summary

This research addresses the lack of a theoretical framework to predict how quantization noise evolves during the training of large language models (LLMs). While established scaling laws like Chinchilla effectively guide compute-optimal model sizing, they fail to account for the degradation in performance caused by Quantization-Aware Training (QAT), particularly at ultra-low bit-widths. Understanding the dynamic relationship between quantization error, model scale, and training data volume is critical for deploying efficient models; without this, optimizing for memory and speed often results in unpredictable accuracy loss or sub-optimal training configurations.

The authors introduce a unified scaling law that extends the Chinchilla loss formulation by incorporating a quantization error term, $\epsilon(N, D, G)$, which is a function of model size ($N$), training tokens ($D$), and quantization granularity ($G$). This challenges prior assumptions that quantization error is independent of data volume.

Technically, the study utilizes a decomposition strategy to isolate and analyze weight and activation quantization errors separately. This granular approach, validated through **268 extensive QAT experiments**, allows for the precise identification of layer-specific bottlenecks rather than treating the model as a monolithic entity. Experimental results demonstrate that while total quantization error decreases as model size increases, it significantly rises with higher volumes of training tokens and coarser quantization granularity.

> **Key Performance Insight:** In a head-to-head comparison of a 297M parameter model with a group size of 32, Integer 4-bit (INT4) quantization notably outperformed Floating Point 4-bit (FP4), achieving a loss of ~2.89 compared to ~2.98.

This work fundamentally changes how researchers approach low-bit model training by establishing that quantization error is not static but scales dynamically with data and model dimensions. By proving that the primary source of error shifts from activation layers to weights as training data grows, the paper provides actionable insights for dynamic optimization strategies, such as applying mixed-precision quantization to the FC2 layer.

---

## üîë Key Findings

*   **Scaling Behavior of Error:** Quantization error decreases as model size increases but rises with higher volumes of training tokens and coarser quantization granularity.
*   **W4A4 Error Dynamics:** At 4-bit precision, both weight and activation quantization errors contribute to the overall error. However, weight quantization error increases more rapidly as the number of training tokens grows.
*   **FC2 Layer Bottleneck:** Activation quantization error located in the FC2 layer, driven by outliers, is identified as the primary bottleneck limiting W4A4 QAT performance.
*   **Data-Rich Shift:** With increased training data, weight quantization error eventually surpasses activation quantization error. This indicates that the focus of optimization must shift toward reducing weight error in data-rich scenarios.
*   **Mixed-Precision Solution:** Applying mixed-precision quantization can address the FC2 layer bottleneck, allowing weight and activation quantization errors to converge to similar levels.

---

## üî¨ Methodology

The researchers employed a rigorous combination of theoretical modeling and extensive empirical validation to derive the new scaling law.

*   **Unified Scaling Law Development:** Development of a unified law modeling quantization error as a function of three variables: model size ($N$), training data volume ($D$), and quantization group size ($G$).
*   **Empirical Validation:** The theoretical framework was validated and refined through **268 Quantization-Aware Training (QAT) experiments)**, covering a wide range of configurations.
*   **Decomposition Strategy:** The team utilized a decomposition strategy to break down total W4A4 quantization error into distinct weight and activation components. This allowed for the analysis of individual sensitivities and the identification of specific layer-level bottlenecks (e.g., the FC2 layer).

---

## ‚öôÔ∏è Technical Details

The paper proposes specific technical modifications to existing scaling frameworks and restricts its analysis to ultra-low bit-widths.

### Theoretical Framework
*   **Base Law:** Modification of the **Chinchilla scaling law** to account for quantization errors in QAT.
*   **Formulation:** Adds a quantization error term $\epsilon(N, D, G)$.
*   **Challenge:** This formulation challenges prior work that assumed error independence from data size.

### Configuration & Scope
*   **Bit-width:** Restricted to ultra-low bit-widths (**W4A4**).
*   **Formats:** Comparison of Integer 4-bit (**INT4**) vs. Floating Point 4-bit (**FP4**).
*   **Strategy:** Per-group quantization utilized throughout the study.

---

## üìä Results

Experimental findings provided quantitative insights into the trade-offs of low-bit training.

*   **Error Scaling:** Confirmed that quantization error decreases as model size increases but increases with the number of training tokens and coarser granularity.
*   **Bottleneck Identification:** The primary bottleneck is activation quantization error in the FC2 layer driven by outliers. However, weight error surpasses activation error in data-rich scenarios.
*   **INT4 vs. FP4:**
    *   **Model:** 297M parameters.
    *   **Group Size:** $G=32$.
    *   **Outcome:** INT4 significantly outperforms FP4 (Loss: ~2.89 vs ~2.98).
*   **Learning Rate Sensitivity:**
    *   **Optimal Range:** Identified between $1 \times 10^{-3}$ and $3 \times 10^{-3}$.
    *   **Effect:** Stabilizes error loss near 0.065.

---

## üèÜ Contributions

*   **Novel Unified Law:** Proposes a novel, unified scaling law for QAT that addresses the limitations of existing laws by incorporating critical factors such as training token count and quantization granularity.
*   **Granular Error Analysis:** Provides a deep analysis of error sources in low-bit (W4A4) training, specifically identifying outlier-induced activation error in the FC2 layer as a critical failure point.
*   **Actionable Insights:** Offers dynamic insights for QAT development, demonstrating that the optimal target for error reduction (whether weight or activation) depends on the scale of training data available.