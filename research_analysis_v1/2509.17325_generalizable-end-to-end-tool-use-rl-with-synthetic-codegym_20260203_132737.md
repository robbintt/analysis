---
title: Generalizable End-to-End Tool-Use RL with Synthetic CodeGym
arxiv_id: '2509.17325'
source_url: https://arxiv.org/abs/2509.17325
generated_at: '2026-02-03T13:27:37'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Generalizable End-to-End Tool-Use RL with Synthetic CodeGym

*Weihua Du; Hailei Gong; Zhan Ling; Kang Liu; Lingfeng Shen; Xuesong Yao; Yufei Xu; Dingyuan Shi; Yiming Yang; Jiecao Chen*

---

### üìä Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Primary Model** | Qwen2.5-32B-Instruct |
| **Key Improvement** | +8.7 absolute accuracy points (OOD Benchmark) |
| **Source Dataset** | KodCode |
| **Environment Type** | POMDP (Partially Observable Markov Decision Process) |
| **Methodology** | Reinforcement Learning (RL) via Active Exploration |
| **Quality Score** | 8/10 |

---

## Executive Summary

Current Large Language Model (LLM) agents often struggle with brittleness when utilizing tools, particularly when facing Out-of-Distribution (OOD) scenarios involving new tools or unseen workflows. Existing training paradigms, such as Supervised Fine-Tuning (SFT) on static trajectories or Reinforcement Learning (RL) restricted to narrow tasks, fail to equip agents with the robustness required for dynamic real-world environments. This lack of generalizability limits the reliability of autonomous agents, as they tend to overfit to specific tool signatures rather than learning the underlying structural logic of workflow execution.

The researchers introduce **CodeGym**, a scalable framework designed to synthesize diverse, multi-turn tool-use environments from static coding problems. Technically, CodeGym employs a two-stage pipeline: "Gym Synthesis," where an LLM extracts atomic functions or logic segments from code to create callable tools, and "Gym Verification," which filters environments to ensure solvability and correctness. These environments are formalized as Partially Observable Markov Decision Processes (POMDPs) with sparse rewards and partial observability. This design compels agents to actively explore and interact with the tools to gather necessary information, rather than relying on memorized static paths.

The CodeGym framework demonstrates significant efficacy in improving model performance and generalizability. In specific benchmarking, the Qwen2.5-32B-Instruct model achieved an absolute accuracy improvement of **8.7 points** on the OOD benchmark ($-Bench). The study validated these results across various model sizes and chain-of-thought configurations, confirming that agents trained through active exploration in interactive CodeGym environments consistently outperform those trained via SFT on static data or RL on narrow, task-specific datasets.

This work represents a significant shift in how training data for tool-use agents is sourced and utilized, proving that synthetic coding environments can serve as effective proxies for complex real-world workflows. By providing a scalable infrastructure for generating verifiable and controllable RL environments, CodeGym offers a viable path toward developing general-purpose agents that are robust to novelty. The findings suggest that grounding agents in the structural logic of code execution‚Äîrather than mere imitation‚Äîis a critical step in overcoming the current limitations of tool-using AI systems.

---

## üîë Key Findings

*   **Consistent OOD Generalizability:** Models trained within the CodeGym framework exhibit consistent out-of-distribution (OOD) generalizability, effectively addressing brittleness with new tools and unseen workflows.
*   **Significant Accuracy Gains:** The Qwen2.5-32B-Instruct model achieved an absolute accuracy improvement of **8.7 points** on the OOD benchmark ($-Bench).
*   **Broad Validation:** The training approach was successfully validated across models of varying sizes and different chain-of-thought (CoT) configurations.
*   **Superiority of Active Exploration:** Active exploration in interactive environments yields better results than Supervised Fine-Tuning (SFT) over static trajectories or RL on narrow tasks.

---

## üß™ Methodology

The researchers introduced **CodeGym**, a scalable framework designed to synthesize multi-turn tool-use environments for reinforcement learning (RL). The methodology rests on the premise that code execution reflects the structural logic of real-world workflows.

**Process Overview:**
1.  **Foundation:** Utilizes static coding problems as the base.
2.  **Transformation:** Converts static problems into interactive environments.
3.  **Extraction:** Extracts atomic functions or logic segments from the code and converts them into callable tools.
4.  **Outcome:** Yields verifiable tasks that allow LLM agents to actively explore tool-execution workflows.

---

## ‚öôÔ∏è Technical Details

**Framework Definition**
CodeGym is a framework for synthesizing large-scale, diverse, and verifiable multi-turn tool-use environments derived from the **KodCode dataset**.

**Pipeline Stages**
*   **Gym Synthesis:** LLM extraction of atomized logic and tools.
*   **Gym Verification:** Filtering environments for correctness and solvability errors.

**Environment Formalization (POMDP)**
*   **State Space:** Defined by the environment.
*   **Action Space:** Comprises generic and domain-specific tools.
*   **Transitions & Observations:** Managed by the environment logic.
*   **Rewards:** Sparse rewards system (1 if answer matches unit test, 0 otherwise).
*   **Observability:** Employs partial observability to specifically encourage active tool use.

---

## üåü Contributions

1.  **New Infrastructure:** Introduction of CodeGym, providing a scalable infrastructure for creating diverse, verifiable, and controllable RL environments specifically for tool-use training.
2.  **Data Transformation Method:** A technical method for rewriting static coding datasets into dynamic, interactive tool-use scenarios via the extraction of atomic functions.
3.  **General-Purpose Path:** Establishment of a viable path toward building general-purpose RL environments that align closely with real-world agent workflows by demonstrating that coding-based environments can improve performance on general benchmarks.

---

## üìà Results

*   **Benchmark Performance:** The Qwen2.5-32B-Instruct model achieved an absolute accuracy improvement of **8.7 points** on the Out-of-Distribution (OOD) benchmark ($-Bench).
*   **Generalization:** The approach demonstrates strong generalization capabilities across model sizes and CoT configurations.
*   **Robustness:** Active exploration in interactive environments outperforms Supervised Fine-Tuning (SFT) on static trajectories and narrow RL, showing superior robustness to unseen workflows.

---
**Quality Score:** 8/10  
**References:** 40 citations