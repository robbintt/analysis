---
title: Exploring the Potential of Large Language Models as Predictors in Dynamic Text-Attributed
  Graphs
arxiv_id: '2503.03258'
source_url: https://arxiv.org/abs/2503.03258
generated_at: '2026-02-03T06:44:44'
quality_score: 7
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Exploring the Potential of Large Language Models as Predictors in Dynamic Text-Attributed Graphs

*Runlin Lei; Jiarui Ji; Haipeng Ding; Lu Yi; Zhewei Wei; Yongchao Liu; Chuntao Hong*

***

### ðŸ“Š Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Framework** | GraphAgent-Dynamic (GAD) |
| **Core Approach** | Multi-agent LLM system (Zero-shot) |
| **Datasets Used** | 5 (DTGB: Enron, GDELT, Googlemap_CT, ICEWS1819, Stack_elec) |
| **Primary Tasks** | Link Prediction, Node Retrieval, Edge Classification |
| **Key Backbone** | DeepSeek-V3 (plus GPT-4o-mini, Llama-3-8b) |
| **Context Solution** | Structure-aware prompting (Heuristic Structural Metrics) |

***

## Executive Summary

### Problem
This research addresses the unexplored application of Large Language Models (LLMs) as predictors in dynamic graph learning, specifically for Dynamic Text-Attributed Graphs (DyTAGs) and Continuous-Time Dynamic Graphs (CTDGs). While LLMs have been integrated into static Graph Foundation Models, dynamic graphsâ€”essential for modeling evolving systems like social networksâ€”present two critical barriers: the strict context length constraints of LLMs, which prevent the processing of large-scale historical interaction data, and the high variability of domain characteristics, which hinders the development of a unified, generalizable predictor.

### Innovation
The authors propose the **GraphAgent-Dynamic (GAD)** framework, a multi-agent architecture designed to bypass context bottlenecks and manage domain heterogeneity. Instead of relying on a monolithic LLM, GAD employs a collaborative system of specialized agents: **Global and Local Summary Agents** extract domain-specific knowledge to enhance transferability, while **Knowledge Reflection Agents** ensure self-consistency and adaptive knowledge updates. To technically resolve context limitations, the framework utilizes structure-aware prompting, replacing raw neighbor lists with compact Heuristic Structural Metrics.

### Results
The framework was evaluated on five diverse datasets from the Dynamic Graph Benchmark (DTGB). In a zero-shot setting without dataset-specific training, GAD achieved performance comparable to, and occasionally exceeding, fully supervised Graph Neural Networks (GNNs). The study quantified the efficiency of the heuristic approach, noting that processing raw data for a node with 100 interactions could consume approximately 10,000 tokens; GAD successfully mitigated this bottleneck. The results demonstrated that structure-aware prompts significantly outperform text-only or few-shot prompting strategies.

### Impact
This work represents the first successful extension of Graph Foundation Models to the dynamic domain, providing empirical evidence that LLM-based predictors can rival traditional supervised GNNs without task-specific fine-tuning. By demonstrating that multi-agent architectures can effectively manage the complexity of dynamic graphs, the study challenges the industry assumption that heavy, resource-intensive training is strictly necessary for graph prediction.

***

## Key Findings

*   **Unexplored Potential:** The study identifies that while LLMs have been used as predictors in static Graph Foundation Models (GFMs), their application to dynamic graph prediction has been entirely unexplored until this work.
*   **Technical Bottlenecks:** The research pinpointed two primary challenges hindering progress in this domain:
    *   Context length constraints of LLMs when processing large-scale historical data.
    *   Significant variability in domain characteristics which complicates the creation of a unified predictor.
*   **Zero-Shot Competitiveness:** The proposed GraphAgent-Dynamic (GAD) framework achieves performance comparable to or even exceeding that of fully supervised Graph Neural Networks (GNNs) without requiring any dataset-specific training.
*   **Efficacy of Multi-Agent Collaboration:** Utilizing a multi-agent system with specialized roles (summary and reflection agents) successfully addresses domain variability and maintains model self-consistency.

## Methodology

The authors propose the **GraphAgent-Dynamic (GAD) Framework**, a multi-agent system that leverages collaborative LLMs rather than relying on a single LLM predictor.

The architecture consists of distinct agent types:
*   **Global and Local Summary Agents:** Responsible for generating domain-specific knowledge to enhance transferability.
*   **Knowledge Reflection Agents:** Enable adaptive updates to the system's knowledge base to ensure the architecture remains unified and self-consistent.

The framework is tested on predictive tasks using a zero-shot approach to benchmark against fully supervised GNNs.

## Technical Details

*   **Framework Name:** GraphAgent-Dynamic (GAD)
*   **Objective:** Use LLMs as predictors for Dynamic Text-Attributed Graphs (DyTAGs) and Continuous-Time Dynamic Graphs (CTDGs) with textual attributes (node descriptions, edge descriptions, edge categories).
*   **Architecture Components:**
    *   Initial Agent
    *   Summary Agents (Local/Global)
    *   Temporary Predictor Agent
    *   Knowledge Reflection Agent
*   **Context Strategy:** Utilizes **structure-aware prompting** to handle context constraints by extracting **Heuristic Structural Metrics** instead of raw neighbor lists:
    *   Historical Interaction
    *   Common Neighbors
    *   Node Frequency
*   **Prompting Strategies:**
    *   Text-only
    *   Text-few-shot
    *   Structure-aware prompts

## Results

**Datasets:**
*   5 from DTGB: Enron, GDELT, Googlemap_CT, ICEWS1819, Stack_elec.
*   Spans 4 domains with a minimum of 6,786 nodes and 797,907 interactions.

**Tasks:**
*   Future Link Prediction (LP)
*   Node Retrieval (NR)
*   Future Edge Classification (EC)

**Evaluation Setup:**
*   **Split:** 70/15/15 chronological split.
*   **Sampling:** 10,240 test samples.
*   **LLM Backbones:** DeepSeek-V3 (primary), GPT4o-mini-0718, Llama-3-8b.
*   **Baselines:** TCL, GraphMixer, DyGFormer.

**Metrics:**
*   Accuracy (LP)
*   Hits@k (NR)
*   Weighted Precision/Recall/F1 (EC)

**Key Outcomes:**
*   GAD achieves zero-shot competitiveness comparable to supervised GNNs.
*   Context bottleneck quantified (e.g., 10,000 tokens per prediction for nodes with 100 interactions); successfully mitigated by GAD.
*   Heuristic metrics validated as more effective than neighbor sampling.

## Contributions

*   **Pioneering Dynamic Application:** This work is the first to pioneer the use of LLMs for predictive tasks specifically on dynamic graphs, extending the scope of Graph Foundation Models.
*   **Novel Framework Architecture:** The introduction of the GAD framework provides a new architectural paradigm for handling dynamic data, using specialized agents to manage context limitations and domain heterogeneity.
*   **Validation of LLM Efficacy:** The study provides empirical evidence that LLM-based predictors can rival traditional supervised methods (GNNs) without task-specific training, challenging the necessity of heavy fine-tuning for general graph tasks.
*   **Design Guidelines:** By analyzing tailored strategies and potential improvements, the paper offers new insights and a roadmap for the future design of LLM-based predictors in graph learning.

***

*Quality Score: 7/10 | References: 40 citations*