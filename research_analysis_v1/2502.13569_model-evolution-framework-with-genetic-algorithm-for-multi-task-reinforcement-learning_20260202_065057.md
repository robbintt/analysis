# Model Evolution Framework with Genetic Algorithm for Multi-Task Reinforcement Learning

*Yan Yu; Wengang Zhou; Yaodong Yang; Wanxuan Lu; Yingyan Hou; Houqiang Li*

---

## Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Quality Score** | 6/10 |
| **Citations** | 34 References |
| **Domain** | Multi-Task Reinforcement Learning (MTRL) |
| **Methodology** | Genetic Algorithm (GA) + Soft Actor-Critic (SAC) |
| **Benchmark** | Meta-World (Robotics Manipulation) |

---

## Executive Summary

Multi-Task Reinforcement Learning (MTRL) faces a critical challenge in balancing model capacity across tasks of varying complexity. Traditional architectures often rely on static structures or fixed-dimension routing networks, which lack the flexibility to adapt resources dynamically. Consequently, these rigid models may either waste computational capacity on simple tasks or fail to capture the nuances of difficult ones due to insufficient parameters. This paper addresses the limitation of static architectures, emphasizing the need for a framework that can autonomously adjust its structural complexity to match the demands of heterogeneous tasks.

The authors introduce the **Model Evolution framework with Genetic Algorithm (MEGA)**, a neuro-evolutionary approach that decouples model structure from static constraints. The key technical innovation is the selection of a Genetic Algorithm (GA) over gradient-based methods for topology optimization. While standard backpropagation requires differentiable, fixed computation graphs, the GA can optimize discrete, variable-length binary genotypes that represent module routing policies. This non-gradient approach enables the "genotype" to evolve during training; specifically, the system monitors task performance and automatically injects new modules (denoted as "Stages") into the architecture when capacity is insufficient. By combining the GA for routing structure with Soft Actor-Critic (SAC) for policy parameter optimization, MEGA reconstructs the model on-the-fly, allowing dynamic growth proportional to task difficulty.

MEGA was evaluated on the **Meta-World benchmark**, a standard suite for robotics manipulation tasks, where it achieved state-of-the-art performance. The experiments demonstrated the framework's ability to automatically identify when current model capacity was inadequate and successfully respond by incorporating additional modules. This mechanism resulted in effective resource allocation, where architecture complexity scaled with task difficulty. The observed adaptive behaviors confirmed that the system could self-organize its structure on-the-fly, resolving the trade-off between computational efficiency and the need for expressive representations in complex environments.

This research significantly advances the field of MTRL by shifting the paradigm from fixed architectures to evolvable systems. By demonstrating that a non-gradient genetic algorithm can effectively optimize variable-length routing policies, MEGA overcomes the structural rigidity of standard routing networks. The ability to automate capacity expansion suggests a path toward more scalable and efficient AI systems that self-organize their complexity, ensuring computational resources are utilized precisely where they are needed most.

---

## Key Findings

*   **Dynamic Evolution:** The proposed Model Evolution framework with Genetic Algorithm (MEGA) enables models to evolve dynamically during training to accommodate tasks of varying difficulty.
*   **Variable-Length Architecture:** Unlike traditional routing networks constrained by fixed dimensions, MEGA supports dynamic adjustment of genotype policy length, allowing the architecture to accommodate a varying number of modules.
*   **Non-Gradient Optimization:** The framework utilizes a non-gradient genetic algorithm to optimize binary genotype policies used for model reconstruction.
*   **State-of-the-Art Performance:** Experiments on the Meta-World benchmark for robotics manipulation tasks demonstrated that the method achieves state-of-the-art performance.
*   **Automatic Resource Allocation:** The framework automatically incorporates additional modules when the current model capacity is insufficient, ensuring resources are allocated based on task difficulty.

---

## Methodology

The researchers developed a neuro-evolutionary framework based on a **genotype module-level model**. In this approach, binary sequences serve as genotype policies that define the reconstruction of the model. 

Instead of relying on gradient-based optimization for routing, the authors employ a **non-gradient genetic algorithm** to optimize these binary sequences. This approach allows the model structure to be evolvable; specifically, the system monitors task difficulty and automatically injects additional modules into the architecture when performance is insufficient, thereby enabling capacity to grow as needed.

---

## Technical Details

*   **Framework Name:** Model Evolution framework with Genetic Algorithm (MEGA)
*   **Application:** Multi-Task Reinforcement Learning (MTRL)
*   **Policy Representation:** Dynamic binary genotype policies with variable lengths.
*   **Stage Concept:** A 'Stage' denotes the number of allocated modules, where 4-bit segments of the genotype generate weights.
*   **Model Reconstruction:** The framework reconstructs models on the fly.
*   **Optimization Strategy:** 
    *   **Genetic Algorithm:** Used for topology search (structure).
    *   **Soft Actor-Critic (SAC):** Used for policy parameter optimization.
*   **Objective:** Maximize the average accumulative reward across a set of Markov Decision Processes (MDPs) with shared state and action spaces.

---

## Contributions

*   **Dynamic Model Evolution:** Introduction of a training framework that evolves the model structure iteratively by adding modules in response to task difficulty, addressing the rigidity of static architectures in multi-task learning.
*   **Genotype-based Optimization:** The proposal of representing module routing policies as binary genotypes optimized via a genetic algorithm, which removes the constraints of gradient-based optimization methods.
*   **Flexible Policy Architecture:** A novel mechanism that allows for variable-length genotype policies, overcoming the fixed output dimension limitations found in standard routing networks.

---

## Results

Experiments were conducted on the **Meta-World benchmark** for robotics manipulation.

*   **Performance:** The authors claim the method achieves state-of-the-art performance.
*   **Adaptability:** Successful adaptability was observed in automatically adding modules when capacity was insufficient.
*   **Efficiency:** Effective resource allocation proportional to task difficulty.
*   **Note:** The provided text notes that specific quantitative metrics (Success Rate, Average Return) and comparison numbers are not included in the source excerpts.