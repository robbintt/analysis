---
title: The Uneven Impact of Post-Training Quantization in Machine Translation
arxiv_id: '2508.20893'
source_url: https://arxiv.org/abs/2508.20893
generated_at: '2026-02-03T20:09:00'
quality_score: 9
citation_count: 19
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# The Uneven Impact of Post-Training Quantization in Machine Translation

*Benjamin Marie; Atsushi Fujita*

---

### ⚡ Quick Facts

| Metric | Details |
| :--- | :--- |
| **Languages Evaluated** | 55 |
| **Model Parameters** | 1.7B – 70B |
| **Quantization Methods** | AWQ, BitsAndBytes, GGUF, AutoRound |
| **Dataset** | WMT24++ |
| **Evaluation Metric** | COMET |
| **Quality Score** | 9/10 |

---

## Executive Summary

> Post-Training Quantization (PTQ) is a critical technique for deploying Large Language Models (LLMs) on resource-constrained hardware, yet its impact on multilingual capabilities remains insufficiently explored. While compression is often evaluated on high-resource languages like English, there is a lack of comprehensive analysis regarding how aggressive quantization affects low-resource and typologically diverse languages.
>
> This paper addresses this gap by investigating the trade-offs between model compression and translation fidelity, specifically examining whether current quantization methods introduce a bias that disproportionately harms linguistic diversity. The study provides the first large-scale, systematic assessment of Weight-Only PTQ applied to machine translation across 55 languages.
>
> Evaluated using the COMET metric, the results demonstrate that **4-bit quantization effectively preserves translation quality** for high-resource languages and large models (>10B parameters), but **2-bit precision causes significant degradation**, particularly in low-resource languages. Among the tested algorithms, **GGUF variants** provided the most consistent performance. This research highlights a critical bias in current quantization practices: precision loss disproportionately affects low-resource and typologically diverse languages, potentially exacerbating the digital divide in NLP.

---

## Key Findings

*   **Bit-WIDTH Impact:** **4-bit quantization** generally preserves translation quality for high-resource languages and large models. However, **significant degradation** occurs for low-resource and typologically diverse languages, particularly at **2-bit precision**.
*   **Algorithm Performance:** **GGUF variants** provide the most consistent performance among the four tested algorithms.
*   **Calibration Strategy:** **Language-matched calibration** offers quantifiable benefits, which are primarily observed in low-bit precision scenarios.
*   **Complex Interactions:** Translation quality is influenced by the complex interactions between quantization bit-width, decoding hyperparameters, and the language used for calibration.

---

## Methodology

This study conducted a large-scale evaluation of Post-Training Quantization (PTQ) applied to machine translation across a diverse set of **55 languages**.

*   **Models:** Five Large Language Models (LLMs) ranging from **1.7 billion to 70 billion parameters**.
*   **Comparative Assessment:** Four quantization methods were assessed:
    1.  AWQ
    2.  BitsAndBytes
    3.  GGUF
    4.  AutoRound
*   **Systematic Analysis:** The study systematically quantified interactions between:
    *   Quantization settings
    *   Decoding hyperparameters
    *   Calibration languages

---

## Technical Details

**Quantization Configuration**
*   **Type:** Weight-Only Post-Training Quantization (PTQ)
*   **Method:** Symmetric uniform quantization
*   **Scaling:** Group-wise scaling (typically group size 128)

**Algorithms Evaluated**
*   **AWQ:** 4-bit, activation-aware.
*   **BitsAndBytes (BnB):** NF4, non-uniform.
*   **GGUF:** Q4_K_M and Q2_K, importance matrix guided.
*   **AutoRound:** 2-bit/4-bit, signSGD optimization.

**Experimental Setup**
*   **Dataset:** WMT24++
*   **Model Families:** Qwen3 and Llama
*   **Performance Metric:** COMET

---

## Results

*   **High vs. Low Resource:** **4-bit quantization** preserves quality for high-resource languages and large models (>10B parameters). Conversely, **2-bit quantization** causes significant degradation, especially for low-resource languages.
*   **Algorithm Consistency:** **GGUF** demonstrated the most consistent performance, with its **Q2_K variant** showing particular strength at 2-bit precision.
*   **Vocabulary Degradation:** Quantization severely degraded out-of-vocabulary translation, most notably for **Indic languages**.
*   **Decoding Parameters:** Sampling temperature was identified as the dominant decoding parameter.
*   **Calibration Benefits:** Language-matched calibration offered benefits specifically in low-bit scenarios.

---

## Contributions

*   **Comprehensive Assessment:** Provides the first comprehensive, large-scale assessment of PTQ on machine translation across a diverse set of 55 languages and varying model sizes.
*   **Technical Comparison:** Offers a direct technical comparison of four prominent quantization algorithms (AWQ, BitsAndBytes, GGUF, AutoRound) within the context of multilingual LLMs.
*   **Bias Identification:** Highlights a critical bias in quantization, demonstrating that low-resource and typologically diverse languages suffer disproportionately from precision loss.
*   **Optimization Guidelines:** Delivers specific insights for optimizing the deployment of multilingual LLMs under hardware constraints.

---

*Quality Score: 9/10*  
*References: 19 citations*