---
title: 'Q-Sched: Pushing the Boundaries of Few-Step Diffusion Models with Quantization-Aware
  Scheduling'
arxiv_id: '2509.01624'
source_url: https://arxiv.org/abs/2509.01624
generated_at: '2026-02-03T19:27:21'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Q-Sched: Pushing the Boundaries of Few-Step Diffusion Models with Quantization-Aware Scheduling

*Natalia Frumkin; Diana Marculescu*

---

> ### ðŸ“Š Quick Facts
>
> | Metric | Value |
> | :--- | :--- |
> | **Model Size Reduction** | 4x (W4A8) |
> | **Calibration Efficiency** | 5 prompts (vs. 1,024 for PTQD) |
> | **Max FID Improvement** | 16.6% (vs. FP16 8-step PCM) |
> | **Validation Scale** | 80,000+ human annotations |
> | **Quality Score** | 9/10 |

---

## Executive Summary

The paper addresses the challenge of deploying few-step diffusion modelsâ€”such as Latent Consistency Models (LCM) and Phased Consistency Models (PCM)â€”on resource-constrained hardware. While these models enable real-time generation by minimizing inference steps, they are highly sensitive to the precision of their sampling trajectories. Standard Post-Training Quantization (PTQ) methods, which focus on compressing model weights (e.g., to 4-bit), typically introduce significant errors that disrupt these trajectories. This results in a severe trade-off where achieving a smaller model size (4x compression) leads to degraded visual fidelity and misalignment with text prompts, limiting the practical deployment of fast diffusion models on edge devices.

**Q-Sched** introduces a paradigm shift from weight-based to scheduler-centric quantization. Instead of attempting to repair quantized weights, the method modifies the diffusion scheduler to correct the sampling trajectory. It achieves this by introducing two learnable scalar preconditioning coefficients ($c_x$ and $c_\theta$) into the sampling equation, which are optimized to counteract the noise introduced by low-precision weights. The optimization is driven by a novel "Joint Alignment-Quality (JAQ) loss," a reference-free metric that balances perceptual image quality with text-image alignment. Crucially, this approach eliminates the need for full-precision inference during calibration, requiring only a handful of prompts to fine-tune the sampling process.

In experimental validations using Stable Diffusion v1-5 on the COCO-30k dataset, Q-Sched achieved a 4x reduction in model size (W4A8) while outperforming full-precision baselines. Specifically, a 4-step LCM using Q-Sched achieved an FID of 26.98, a 15.5% improvement over the FP16 baseline (31.94). Similarly, a 4-step PCM achieved an FID of 17.39, surpassing the FP16 8-step baseline by 16.6%. The method maintained or exceeded FP16 CLIPScores and required only five calibration prompts, compared to 1,024 for previous methods like PTQD. These results were consistent across multiple architectures, including FLUX.1[schnell] and SDXL-Turbo, with further validation provided by a study of over 80,000 human annotations.

The significance of Q-Sched lies in its empirical demonstration that quantization and few-step distillation are complementary techniques. By decoupling the quantization process from weight fine-tuning and focusing on trajectory optimization, the research provides a scalable, backbone-agnostic solution that minimizes calibration overhead. This advancement lowers the barrier for deploying high-speed, high-fidelity generative models on consumer hardware without sacrificing visual quality.

---

## Key Findings

*   **Efficiency & Accuracy:** Achieves a **4x reduction in model size** while maintaining full-precision accuracy.
*   **Performance Gains:** Delivers significant FID improvements, outperforming the FP16 4-step Latent Consistency Model by **15.5%** and the FP16 8-step Phased Consistency Model by **16.6%**.
*   **Calibration Speed:** Requires only a **handful of calibration prompts** (specifically 5) and avoids full-precision inference during the calibration phase.
*   **Robust Validation:** Validated across multiple architectures like **FLUX.1[schnell]** and **SDXL-Turbo**, supported by a comprehensive study with over **80,000 annotations**.

---

## Methodology

Q-Sched introduces a **scheduler-centric post-training quantization (PTQ)** paradigm. Unlike traditional methods that quantize weights directly, Q-Sched modifies the diffusion model scheduler to adjust the few-step sampling trajectory.

The method utilizes a novel **'JAQ loss'** function to learn quantization-aware pre-conditioning coefficients. This loss function combines:
1.  **Text-image compatibility**
2.  **Image quality metrics**

This combination allows for fine-grained, reference-free optimization that adapts the sampling process to the constraints of low-precision weights.

---

## Technical Details

**Architecture & Strategy**
*   **Method:** Quantization-aware scheduling for few-step diffusion models.
*   **Configuration:** W4A8 (4-bit weights, 8-bit activations).
*   **Core Mechanism:** Corrects sampling trajectories via noise schedule modification rather than weight fine-tuning.

**Implementation**
*   **Base Scheduler:** Built on the TCD (Trajectory Consistency Distillation) scheduler.
*   **Learnable Parameters:** Introduces two learnable scalar preconditioning coefficients:
    *   $c_x$
    *   $c_\theta$
*   **Optimization:** These coefficients are optimized to minimize the **Joint Alignment-Quality (JAQ) loss**.
*   **Scope:** Backbone-agnostic, supporting U-Net and DiT architectures, as well as consistency models (LCM, PCM) and flow-matching strategies.

---

## Experimental Results

**COCO-30k Experiments (Stable Diffusion v1-5)**

*   **LCM 4-Step Sampling:**
    *   Achieved FID: **26.98**
    *   Improvement: **15.5%** over FP16 baseline (31.94).
*   **PCM 4-Step Sampling:**
    *   Achieved FID: **17.39**
    *   Improvement: **16.6%** over FP16 8-step baseline.
*   **Text Alignment:** Matches or exceeds FP16 CLIPScores.
*   **Visual Quality:** Significantly outperforms naive quantization and PTQD in artifact reduction.

**Efficiency Metrics**
*   **Model Compression:** 4x size reduction (W4A8).
*   **Calibration Data:** Requires only 5 prompts (vs. 1,024 for PTQD).

---

## Core Contributions

1.  **New PTQ Paradigm:** Shifts the focus from weight-based to scheduler-based quantization for efficient few-step diffusion models.
2.  **Optimization Strategy:** Features the JAQ loss, a reference-free technique allowing calibration without the computational burden of full-precision activation data.
3.  **Validation of Synergy:** Provides empirical evidence that quantization and few-step distillation are complementary techniques for achieving high-fidelity generation.

---
**References:** 40 citations