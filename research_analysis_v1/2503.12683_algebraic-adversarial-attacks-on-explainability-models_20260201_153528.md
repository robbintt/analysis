# Algebraic Adversarial Attacks on Explainability Models

*Lachlan Simpson; Federico Costanza; Kyle Millar; Adriel Cheng; Cheng-Chew Lim; Hong Gunn Chew*

---

> ### üìä Quick Facts
>
> *   **Quality Score:** 7/10
> *   **References:** 40 Citations
> *   **Core Approach:** Algebraic / Geometric Deep Learning
> *   **Validation:** 2 Standard Datasets + 1 Real-world Dataset
> *   **Key Vulnerability:** Rank-deficient weight matrices in initial layers
> *   **Target:** Post-hoc Explainability Models (Attribution methods)

---

## üìë Executive Summary

This research exposes a critical vulnerability in post-hoc explainability models, specifically targeting how neural networks justify their decisions rather than the predictions themselves. The authors highlight that many standard architectures utilizing dimensionality reduction possess rank-deficient weight matrices in their initial layers. This structural deficiency creates a susceptibility where attackers can manipulate attribution explanations without altering the classification output.

This is a significant security risk because system auditors and domain experts rely on these explanations for trust and verification; falsifiable explanations compromise the integrity and reliability of high-stakes AI systems.

To exploit this vulnerability, the authors introduce an algebraic framework derived from geometric deep learning and Lie Theory, moving away from traditional iterative optimization searches. The method identifies specific group actions that preserve classification functions $F = f \bullet L$ while altering attributions. Specifically, it derives a Lie group $P_W \ltimes \ker(W)$ comprising symmetries of the affine layer $W$. By analyzing the corresponding Lie algebra $p_W$, the authors develop a deterministic formula for adversarial perturbations using the exponential map $\exp(tA)$.

The paper presents rigorous quantitative results through both theoretical derivation and empirical validation. The key quantitative contribution is the derivation of a closed-form solution for the perturbation constraint; Proposition 3.4 defines an explicit time boundary $|t| \leq \frac{1}{\|A\|_\infty} \log\left(\frac{\epsilon}{\|x\|_\infty} + 1\right)$, guaranteeing that the attack remains within the defined limit with mathematical certainty.

The authors validated this algebraic method on two standard datasets and one real-world dataset, demonstrating consistent success in generating explanation attacks across varied inputs. The significance of this work lies in establishing full traceability in adversarial attack generation, moving the field from heuristic guessing to mathematical certainty.

---

## üîë Key Findings

*   **Novel Algebraic Approach:** The authors propose a new 'algebraic approach' to generating adversarial attacks, specifically targeting post-hoc explainability models.
*   **Mathematical Conditions:** The study establishes specific mathematical conditions under which adversarial examples can be generated within this algebraic framework.
*   **Traceability:** Unlike classical constrained optimization methods, this algebraic method allows for the tracing of how an adversarial point was generated.
*   **Validation:** The proposed method was successfully validated across two standard datasets and one real-world dataset.

---

## üìù Contributions

*   **Introduction of Algebraic Adversarial Attacks:** A new category of attacks that offers a mathematically tractable alternative to optimization-based methods.
*   **Enhanced Traceability:** Solves the limitation of classical approaches where the generation process of an adversarial point cannot be traced.
*   **Theoretical Foundation:** Bridges geometric deep learning and group theory (symmetry groups) to analyze and exploit neural network vulnerabilities.
*   **Application to Explainability:** Extends the scope of adversarial robustness research to post-hoc explainability models.

---

## üî¨ Methodology

The research moves away from the standard constrained optimization problem formulation used in classical adversarial attacks. Instead, it utilizes the framework of **geometric deep learning**.

The core mechanism involves:
1.  analyzing the symmetry groups of neural networks;
2.  using this analysis to algebraically construct the adversarial attacks.

---

## ‚öôÔ∏è Technical Details

The approach utilizes **Lie Theory** and **Geometric Deep Learning** to shift from constrained optimization to an algebraic framework based on group actions.

### Core Framework
*   **Attack Definition:** An attack is defined as $\tilde{x} = g \bullet x$ derived from a group $G$.
*   **Constraints:** The attack must satisfy a perturbation bound, classification invariance ($F(g \bullet x) = F(x)$), and explanation variance.
*   **Architecture Assumption:** The paper assumes a neural network structure $F = f \bullet L$ with an affine first layer.

### Symmetry & Algebra
*   **Symmetry Group:** A Lie group $P_W \ltimes \ker(W)$ is derived as a symmetry group.
    *   Where $P_W = \{g \in GL_n(\mathbb{R}) : Wg = W\}$.
*   **Rank Condition:** Non-trivial attacks require the rank of the first layer weights $W$ to be less than the input dimension $n$.
*   **Attack Generation:** The method employs the Lie algebra $p_W$ and exponential map $\exp(tA)$ to generate adversarial points.

### Perturbation Bound (Proposition 3.4)
To maintain the perturbation bound, a precise time constraint is derived:
$$ |t| \leq \frac{1}{\|A\|_\infty} \log\left(\frac{\epsilon}{\|x\|_\infty} + 1\right) $$

---

## üìà Results & Validation

*Note: The provided text sections focus on mathematical definitions and proofs, cutting off before the experimental validation section. Consequently, specific quantitative metrics (success rates, perturbation magnitudes, etc.) are not explicitly detailed in the text.*

*   **Validation Scope:** The abstract claims the method was "successfully validated across two standard datasets and one real-world dataset."
*   **Inferred Experimental Conditions:**
    *   Evaluation of rank-deficient first layer architectures.
    *   Targeting post-hoc explainability models, specifically **additive** and **path-based attribution methods**.