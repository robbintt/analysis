---
title: 'SDQ-LLM: Sigma-Delta Quantization for 1-bit LLMs of any size'
arxiv_id: '2510.03275'
source_url: https://arxiv.org/abs/2510.03275
generated_at: '2026-02-03T19:01:14'
quality_score: 8
citation_count: 29
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# SDQ-LLM: Sigma-Delta Quantization for 1-bit LLMs of any size
*Junhao Xia; Ming Zhao; Limin Xiao; Xiujun Zhang*

---

> ### ðŸ“Š Quick Facts
>
> *   **Quality Score:** 8/10
> *   **References:** 29 Citations
> *   **Target Precision:** 1-bit and 1.58-bit
> *   **Compression Ratio:** Up to ~10x model size reduction (e.g., 150GB â†’ <15GB)
> *   **Key Innovation:** Continuous Over-Sampling Ratio (OSR) adjustment
> *   **Benchmark Highlight:** LLaMA-2-70B WikiText2 PPL: **5.28** (SDQ) vs **5.13** (FP16)

---

## Executive Summary

The deployment of Large Language Models (LLMs) is severely constrained by their massive memory footprint and computational costs. A standard 70B parameter LLaMA model, for instance, requires approximately **150GB** of memory for inference at FP16 precision, rendering it inaccessible for many hardware environments. While quantization to extreme low-bits (1-bit or 1.58-bit) offers a theoretical solution to this bottleneck, existing methods often struggle to preserve the model's linguistic reasoning capabilities or lack the flexibility to adapt to varying hardware constraints.

This paper introduces **SDQ-LLM**, a novel framework that adapts Sigma-Delta modulation techniques from signal processing to enable the quantization of LLMs of any size to 1-bit and 1.58-bit representations. The core technical innovation involves an upsampling block that feeds weights into a Sigma-Delta Quantizer. The framework introduces the **Over-Sampling Ratio (OSR)**, a hyperparameter allowing for continuous, fractional adjustment (e.g., 2.5x) to balance memory against accuracy. To maximize precision, the authors propose a **"MultiOSR"** strategy, which allocates OSR at a fine-grained level across and within layers based on weight variance and quantization sensitivity. Additionally, the method utilizes **Hadamard-based weight smoothing** prior to quantization to ensure stability and replaces traditional multiplication operations in linear layers with addition operations, significantly reducing computational complexity.

Extensive experiments on OPT and LLaMA model families demonstrate that SDQ-LLM successfully maintains performance near full-precision baselines. On the LLaMA-2-70B model, SDQ-LLM at 1.58-bit achieved a perplexity (PPL) of **5.28** on WikiText2, closely matching the FP16 baseline of **5.13** and significantly outperforming the BitNet b1.58 baseline, which scored 5.59. Similarly, on the C4 dataset, SDQ-LLM reported a PPL of 6.82 against BitNet's 7.26.

The significance of SDQ-LLM lies in its ability to make massive LLMs deployable on resource-constrained hardware without a proportional loss of intelligence. By enabling extreme quantization for models of any scale, this research lowers the barrier to entry for running state-of-the-art NLP models. The introduction of a continuous OSR adjustment provides engineers with unprecedented granular control over the memory-accuracy trade-off, allowing models to be dynamically adapted to specific VRAM constraints.

---

## Key Findings

*   **Effective Extreme Quantization:** SDQ-LLM successfully enables the quantization of Large Language Models (LLMs) to **1-bit** and **1.58-bit** representations while preserving their linguistic reasoning capabilities.
*   **Flexible Memory-Accuracy Trade-off:** The framework allows for continuous, fractional adjustment of the **Over-Sampling Ratio (OSR)**, enabling dynamic adaptation to memory or VRAM constraints to balance model size and accuracy.
*   **High Efficiency on Aggressive Settings:** Extensive experiments on OPT and LLaMA model families demonstrate that the method maintains high-precision performance even under highly aggressive low-OSR settings.
*   **Inference Optimization:** By converting multiplication operations in linear layers into **addition operations**, the approach significantly enhances inference efficiency.

---

## Methodology

The SDQ-LLM framework utilizes a pipeline designed to maximize weight retention during extreme compression:

1.  **Pipeline Architecture:** The system uses upsampling combined with a Sigma-Delta Quantizer to binarize or ternarize LLM weights into 1-bit or 1.58-bit representations.
2.  **Pre-quantization Stabilization:** To mitigate the loss of quantization precision, the method employs **Hadamard-based weight smoothing** prior to quantization to ensure stability and robustness.
3.  **MultiOSR Strategy:** The approach leverages a fine-grained strategy called **MultiOSR**, which allocates Over-Sampling Ratios based on the correlation between quantization sensitivity and weight variance. This strategy distributes OSR both across layers and within individual layers according to weight variance and parameter scale.

---

## Core Contributions

*   **SDQ-LLM Framework:** Introduction of a novel framework capable of quantizing LLMs of any size to extreme low-bits (1-bit/1.58-bit) using Sigma-Delta modulation techniques.
*   **Continuous OSR Adjustability:** The ability to select fractional Over-Sampling Ratios (e.g., 2.5x), allowing for granular control over the trade-off between memory footprint and model accuracy.
*   **MultiOSR Allocation Strategy:** A fine-grained, layer- and linear-wise OSR distribution method that optimizes precision by analyzing weight variance and parameter scales.
*   **Operational Efficiency:** A mechanism that replaces multiplication operations with additions within linear layers, coupled with pre-quantization Hadamard-based smoothing to improve weight representation robustness.

---

## Technical Details

The framework utilizes **Sigma-Delta Quantization (SDQ)**, adapting signal processing techniques for the extreme quantization of Large Language Models.

**Key Components & Architecture:**
*   **Precision Targets:** Adapts to 1-bit and 1.58-bit precisions.
*   **Over-Sampling Ratio (OSR):** A hyperparameter for continuous adjustment to adapt to memory constraints.
*   **MultiOSR Strategy:** Implements layer- and linear-wise OSR allocation.
*   **Computational Transformation:** Transforms multiplication operations in linear layers into addition operations to reduce complexity.

**Mathematical Foundations:**
*   **Transformations:** Incorporates a Hadamard matrix transformation.
*   **Signal Processing Elements:** Uses delay elements ($Z^{-1}$) and integrators ($1/(1-Z^{-1})$).
*   **Quantization Logic:** Builds on Round-To-Nearest (RTN) quantization, binarization ($\{-1, +1\}$), and ternarization ($\{-1, 0, +1\}$).

---

## Experimental Results

The method claims to maintain high-precision performance on OPT and LLaMA model families under aggressive low-OSR settings while preserving linguistic reasoning capabilities.

*   **Benchmark Comparison:** Positions itself against state-of-the-art methods like PB-LLM and BitNet b1.58.
*   **Memory Footprint:** Targets reducing a standard 70B LLaMA model (approx. 150GB at FP16) to under 15GB.
*   **Performance Metrics:**
    *   **LLaMA-2-70B (WikiText2):** SDQ-LLM (1.58-bit) achieved **5.28 PPL** vs FP16 (**5.13 PPL**). BitNet b1.58 scored **5.59**.
    *   **LLaMA-2-70B (C4):** SDQ-LLM reported **6.82 PPL** vs BitNet's **7.26**.

These results validate the framework's ability to handle extreme quantization (reducing model size by roughly 10x) without the significant degradation in linguistic reasoning typically associated with such aggressive compression.