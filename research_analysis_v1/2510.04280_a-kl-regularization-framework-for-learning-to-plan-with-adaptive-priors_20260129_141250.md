# A KL-regularization framework for learning to plan with adaptive priors

*Ãlvaro Serra-Gomez; Daniel Jarne Ornia; Dhruva Tirumala; Thomas Moerland*

---

> ### ðŸ“Š Quick Facts Sidebar
>
> *   **Methodology:** PO-MPC (Policy Optimization - Model Predictive Control)
> *   **Core Mechanism:** KL-regularization between sampling policy and MPPI planner
> *   **Action Spaces:** High-dimensional ($\mathbb{R}^{21}, \mathbb{R}^{38}, \mathbb{R}^{61}$)
> *   **Benchmark Tasks:** DeepMind Control Suite + HumanoidBench (14 total tasks)
> *   **Hardware:** Single NVIDIA A100 GPU
> *   **Training Efficiency:** $1 \times 10^6$ timesteps in 7â€“15 hours
> *   **Quality Score:** 9/10

---

## Executive Summary

This research addresses the critical issue of **"distribution mismatch"** within Model-Based Reinforcement Learning (MBRL), specifically targeting pipelines that utilize the Model-Predictive Path Integral (MPPI) planner. In standard MBRL architectures, the sampling policy tends to drift away from the optimal action distribution produced by the planner. This discrepancy introduces significant bias into value estimates and degrades long-term performance.

The authors introduce **Policy Optimization-Model Predictive Control (PO-MPC)**, a novel framework that mathematically unifies existing MPPI-based RL methods under a single KL-regularized formulation. PO-MPC formulates the policy update as an optimization problem that maximizes expected discounted return while minimizing the reverse Kullback-Leibler (KL) divergence between the learned sampling policy and an adaptive prior derived from the MPPI planner.

The proposed method was empirically validated against strong baselines (TD-MPC2 and BMPC) across 14 complex locomotion tasks. PO-MPC achieved superior final performance and sample efficiency. This work establishes PO-MPC as a new state-of-the-art approach for continuous control, offering a robust mathematical foundation for future research in planner-guided reinforcement learning.

---

## Key Findings

*   **Policy-Planner Alignment:** Aligning the sampling policy with the MPPI planner distribution significantly improves the accuracy of value estimation and long-term performance compared to independent policy updates.
*   **Flexible Trade-offs:** The proposed PO-MPC framework provides the flexibility to explicitly trade off Return maximization against KL divergence minimization during policy updates.
*   **Theoretical Unification:** Existing MPPI-based reinforcement learning methods are theoretically unified as special cases within the proposed KL-regularization framework.
*   **State-of-the-Art Performance:** Exploring previously unstudied variations derived from this unified framework yields significant performance improvements, advancing the state of the art in MPPI-based RL.

---

## Methodology

The authors introduce **Policy Optimization-Model Predictive Control (PO-MPC)**, a family of KL-regularized Model-Based Reinforcement Learning (MBRL) methods.

*   **Core Integration:** The methodology involves integrating the planner's action distribution as an adaptive prior within the policy optimization process.
*   **KL-Regulation:** By using Kullback-Leibler (KL) divergence to align the learned sampling policy with the behavior of the MPPI planner, the method regulates policy updates.
*   **Consistency:** This ensures consistency between the sampling distribution and the planning dynamics, preventing the sampling policy from drifting away from the planner's optimal strategies.

---

## Contributions

*   **Unified Framework:** Establishment of PO-MPC, a general framework that unifies disparate MPPI-based reinforcement learning methods under a single KL-regularized formulation.
*   **Theoretical Clarification:** Demonstration that prior approaches (such as those minimizing KL divergence or using planner-guided regularization) are mathematically specific instances of the PO-MPC family.
*   **Novel Algorithmic Variations:** The exploration and validation of previously unstudied variations of these algorithms derived from the generalized framework.
*   **State-of-the-art Performance:** Empirical demonstration that extended configurations achieve superior performance in high-dimensional continuous control tasks requiring sample efficiency.

---

## Technical Details

The PO-MPC framework is a Model-Based Reinforcement Learning (MBRL) approach that unifies MPPI-based methods by formulating the sampling policy update as a KL-regularized reinforcement learning problem.

### Core Objective
It addresses the 'distribution mismatch' between sampling and planning policies by optimizing an objective $J(\pi_{\theta_s})$ that:
*   Maximizes expected discounted return.
*   Minimizes the reverse KL-divergence between the learned sampling policy $\pi_{\theta_s}$ and an MPPI-induced prior $\pi_p$.
*   Is governed by hyperparameter $\lambda$ to control the trade-off.

### Theoretical Derivation
*   **Bellman Equation:** The framework derives a recursive Bellman equation for the KL-regularized Q-value.

### Architectural Components
1.  **World Model:** Based on TD-MPC2.
2.  **State Encoder:** Processes input states.
3.  **Intermediate Prior ($\pi_{\theta_p}$):** A learned component to shield against stale buffer samples.
4.  **MPPI Planner:** Generates the action distributions.

### Unification of Prior Methods
*   **$\lambda=0$:** Corresponds to decoupled updates (recovers TD-MPC2).
*   **$\lambda \to \infty$:** Corresponds to pure KL minimization (recovers BMPC).
*   **Exploration:** Supports exploration via entropy regularization.

---

## Results

The method was rigorously tested on continuous control benchmarks:

### Test Environments
*   **DeepMind Control Suite:** 'Humanoid', 'Dog'.
*   **HumanoidBench:** 14 locomotion tasks involving sparse rewards.
*   **Action Spaces:** High-dimensional continuous spaces ($\mathbb{R}^{21}, \mathbb{R}^{38}, \mathbb{R}^{61}$).

### Performance
*   **Comparison:** Compared against baselines TD-MPC2 and BMPC.
*   **Outcome:** PO-MPC achieved substantial gains in both sample efficiency and final performance.

### Computational Metrics
*   **Hardware:** Training was conducted on a single NVIDIA A100 GPU.
*   **Duration:** 7 to 15 hours for $1 \times 10^6$ timesteps.

### Key Insight
Experiments confirmed that tuning the KL-regularization parameter $\lambda$ and utilizing the **intermediate prior** yielded superior performance. This effectively reduced variance caused by outdated planner samples.

---

**References:** 40 citations