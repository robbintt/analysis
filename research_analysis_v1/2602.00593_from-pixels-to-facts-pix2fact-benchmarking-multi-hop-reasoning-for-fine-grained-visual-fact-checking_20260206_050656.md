---
title: 'From Pixels to Facts (Pix2Fact): Benchmarking Multi-Hop Reasoning for Fine-Grained
  Visual Fact Checking'
arxiv_id: '2602.00593'
source_url: https://arxiv.org/abs/2602.00593
generated_at: '2026-02-06T05:06:56'
quality_score: 9
citation_count: 32
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# From Pixels to Facts (Pix2Fact): Benchmarking Multi-Hop Reasoning for Fine-Grained Visual Fact Checking

*Yifan Jiang; Cong Zhang; Bofei Zhang; Yifan Yang; Bingzhang Wang; Yew-Soon Ong*

---

> **ðŸ“Š Quick Facts**
>
> *   **Dataset:** Pix2Fact (1,000 4K+ Images, 1,000 Q&A pairs)
> *   **SOTA Model Accuracy:** 24.0%
> *   **Human Baseline:** 56.0%
> *   **AI-Human Gap:** 32.0%
> *   **Scenarios:** 8 daily-life categories
> *   **Annotation Complexity:** ~35â€“40 minutes per question
> *   **Quality Score:** 9/10

---

## Executive Summary

Current evaluation protocols for Vision-Language Models (VLMs) likely overestimate model capabilities by failing to assess the necessary synergy between granular visual perception and complex, knowledge-based reasoning. While existing benchmarks may test visual recognition or simple question answering, they do not adequately challenge models to perform "visual fact-checking"â€”the ability to ground reasoning in high-resolution visual details while simultaneously integrating external knowledge.

This paper addresses the critical limitation that current VLMs have not been rigorously tested on tasks requiring the simultaneous integration of fine-grained visual grounding, multi-hop logical inference, and real-world knowledge retrieval. The authors introduce **Pix2Fact**, a rigorously constructed benchmark designed to evaluate expert-level visual fact-checking through a multi-stage annotation pipeline.

The technical innovation lies in the dataset's construction methodology: it utilizes 1,000 high-resolution (4K+) images across 8 daily-life scenarios, annotated via a dual-layer process involving PhD-level experts. The evaluation of 9 state-of-the-art VLMs on Pix2Fact reveals a profound performance deficit, with top models achieving only 24.0% accuracy against a human baseline of 56.0%. This 32.0% gap quantitatively demonstrates that while VLMs may perform well on lower-resolution or single-step reasoning tasks, they fail to replicate the expert-level human ability to synthesize visual evidence with external knowledge for fact verification.

---

## Key Findings

*   **Significant Performance Gap:** State-of-the-art VLMs achieve only **24.0%** accuracy on the Pix2Fact benchmark, highlighting a severe limitation in current capabilities.
*   **Human-AI Disparity:** A substantial **32.0%** performance gap exists between top AI models (24.0%) and human experts (56.0%).
*   **Limitation of Current Benchmarks:** Existing benchmarks fail to evaluate the synergy between visual grounding and knowledge-based reasoning, effectively overestimating VLM capabilities.
*   **Complexity of Integration:** Current architectures struggle to integrate high-resolution visual detail, multi-hop reasoning, and external knowledge simultaneously.

---

## Methodology

The researchers developed **Pix2Fact**, a benchmark comprising 1,000 high-resolution (4K+) images across 8 daily-life scenarios. The study evaluated 9 state-of-the-art VLMs against this benchmark.

To ensure quality and complexity:
*   **Construction:** Questions and answers were crafted by **PhD annotators**.
*   **Design:** Each query was designed to necessitate detailed visual grounding, knowledge-intensive multi-hop reasoning, and external knowledge integration.

---

## Technical Details

### Benchmark Construction Pipeline
The approach utilizes a rigorous 3-stage pipeline:
1.  **Sourcing:** License-free high-resolution images.
2.  **Pre-screening:** Automated filtering based on resolution and file size.
3.  **Review:** Manual review by PhD-level experts.

### Annotation Methodology
A dual-layer annotation process ensures data integrity:
*   **Collaboration:** Involves domain experts and professional services.
*   **Verification:** A rigorous **4-expert verification system**.
*   **Requirements:** Every triplet must include:
    *   Detailed visual grounding (1-3 bounding boxes).
    *   Multi-hop reasoning.
    *   External knowledge integration.

### Reasoning Architecture
The reasoning architecture encompasses:
*   Entity recognition.
*   Spatial relationships.
*   Visual perception and OCR.
*   External knowledge retrieval.

### Dataset Composition
*   **Volume:** 1,000 4K+ images and 1,000 Q&A pairs.
*   **Scope:** Covers 8 diverse daily-life scenarios.
*   **Effort:** High complexity, averaging 35â€“40 minutes of annotation per question.

---

## Results

The evaluation of Pix2Fact revealed critical insights into the limitations of current Vision-Language Models:

*   **Accuracy Metrics:** SOTA VLMs achieved an accuracy of **24.0%** against a human expert baseline of **56.0%**.
*   **Performance Gap:** A **32.0%** deficit indicates that current models cannot replicate human-level visual fact-checking.
*   **Architectural Struggles:** Analysis confirms that current architectures fail to simultaneously integrate high-resolution visual details, multi-hop reasoning, and external knowledge retrieval.
*   **Benchmark Implications:** Existing benchmarks overestimate VLM capabilities by failing to evaluate the synergy between visual grounding and knowledge-based reasoning.

---

## Contributions

*   **New Benchmark:** Introduced Pix2Fact to test the combination of expert-level perception and knowledge-intensive multi-hop reasoning.
*   **Standards Establishment:** Set rigorous dataset standards using high-resolution imagery and PhD-level annotation.
*   **Critical Baseline:** Provided a baseline analysis quantifying the limitations of current SOTA VLMs in replicating human-level visual fact-checking.

---

**Quality Score:** 9/10  
**References:** 32 citations