# Fast or Slow? Integrating Fast Intuition and Deliberate Thinking for Enhancing Visual Question Answering

*Songtao Jiang; Chenyi Zhou; Yan Zhang; Yeying Jin; Zuozhu Liu*

---

> ### ðŸ’¡ Quick Facts
> * **Quality Score:** 8/10
> * **References:** 16 Citations
> * **Core Theory:** Dual Process Theory (System 1 & System 2)
> * **Key Method:** Conceptualizing Before Observation
> * **Top Result:** LLaVA-1.5-13B achieved SoTA across ScienceQA, TextVQA, VizWiz, and MME.
> * **Benchmarks:** ScienceQA, TextVQA, VizWiz, MME

---

## Executive Summary

Current visual prompting paradigms used to enhance Multimodal Large Language Models (MLLMs) suffer from inherent inefficiencies that degrade overall performance. Existing methods indiscriminately annotate all detected objects, creating excessive visual markers that introduce noise into the attention mechanism rather than serving as useful cues. This "over-annotation" is computationally wasteful and detrimental because it fails to recognize that not all questions require visual grounding; simpler queries can be answered accurately without complex visual analysis.

The researchers introduce **FOCUS**, a plug-and-play framework that integrates **Dual Process Theory** from cognitive science into Visual Question Answering to address these inefficiencies. The system utilizes a dynamic routing mechanism that evaluates question complexity using a self-consistency method ($N=3$) to route queries:

*   **System 1 (Fast Intuition):** Simple questions are assigned here for efficient zero-shot reasoning, bypassing visual processing entirely.
*   **System 2 (Deliberate Thinking):** Complex questions are assigned here, employing a "Conceptualizing Before Observation" strategy. This strategy extracts $K$ key semantic elements via GPT-3.5-turbo, locates them using Grounded-SAM segmentation, and creates a refined image $I'$ that aggregates relevant objects.

Extensive testing across ScienceQA, TextVQA, VizWiz, and MME benchmarks demonstrates that FOCUS achieves significant performance gains, superseding the previous SoM method.

---

## Key Findings

*   **Inefficiency in Current Methods:** Existing visual prompting methods degrade performance by indiscriminately annotating all detected objects. These excessive visual markers distract the model rather than assist it.
*   **Variable Visual Requirements:** Not all questions require visual prompts; simpler questions can be answered efficiently without them, avoiding unnecessary computational costs.
*   **Dynamic Routing Success:** The FOCUS method validates that distinguishing question complexity to apply either fast intuition or deliberate analysis leads to consistent performance improvements across both open-source and black-box MLLMs.
*   **Benchmark Superiority:** Extensive testing on ScienceQA, TextQA, VizWiz, and MME demonstrates that FOCUS achieves significant performance gains over existing baselines.

---

## Methodology

The researchers developed **FOCUS**, a plug-and-play framework designed to enhance Multimodal Large Language Models (MLLMs) by integrating **Dual Process Theory**. The methodology involves two primary modes of operation determined by a dynamic adaptation layer:

1.  **Question Complexity Analysis:** The system first analyzes the complexity of the visual question presented to the model.
2.  **Fast Intuition Mode (System 1):** For straightforward questions, FOCUS utilizes a fast intuitive mode with efficient zero-shot reasoning, bypassing the need for complex visual segmentation.
3.  **Deliberate Analytical Mode (System 2):** For complex reasoning tasks, it employs a deliberate analytical mode. This uses a **'conceptualizing before observation' strategy** to identify and highlight only critical visual elements relevant to the query, filtering out background noise.

---

## Technical Details

**Framework Architecture:**
*   **Dual-Process Framework (F):** Utilizes System 1 (Fast Intuition) and System 2 (Deliberate Thinking) via a mapping function $A = F(I, Q)$.
*   **Routing Mechanism:** A decision layer uses Question Complexity Evaluation based on self-consistency ($N=3$) to route questions to either zero-shot reasoning or the refinement strategy.

**System 2 Strategy (Deliberate Thinking):**
*   **Extraction:** Uses GPT-3.5-turbo to extract $K$ key semantic elements.
*   **Localization:** Utilizes Grounded-SAM segmentation to locate the extracted elements within the image.
*   **Aggregation:** Combines the original image with cropped, isolated patches of relevant objects into a refined image $I'$.

**Model Compatibility:**
*   Supported models include LLaVA-1.5, MiniGPT4-V2, InstructBLIP, GPT-4V, and Gemini Pro.

---

## Contributions

*   **Problem Identification:** Highlights the flaw in current visual prompting paradigms where over-annotation negatively impacts reasoning.
*   **Theoretical Application:** Successfully adapts Dual Process Theory from human cognitive science to Visual Question Answering.
*   **Strategy Innovation:** Introduces the 'conceptualizing before observation' strategy to filter visual noise and prioritize relevant information.
*   **Universal Performance Boost:** Provides a versatile, plug-and-play solution that enhances the performance of diverse MLLM architectures without modifying the core models.

---

## Results

**Evaluation Metrics:**
*   Tested on ScienceQA (Accuracy), TextVQA (Accuracy), VizWiz (Accuracy), and MME (Total Score).

**Performance Highlights:**
*   FOCUS outperforms the previous state-of-the-art (SoM) by reducing noise from redundant objects and optimizing computational cost.
*   **Top Tier Performance:** The combination of FOCUS + LLaVA-1.5-13B achieved state-of-the-art performance across all four benchmarks.
*   **Specific Improvements (LLaVA-1.5-7B):**
    *   **ScienceQA:** Increased from 90.41% to 92.18% (**+1.77%**).
    *   **TextVQA:** Increased from 62.63% to 68.76% (**+6.13%**).
    *   **MME:** Total score increased from 1364.6 to 1441.4.

**Efficiency:**
*   Optimized computational cost by avoiding unnecessary visual markers on simple questions.
*   **Black-box Limitations:** Evaluation on black-box models (GPT-4V, Gemini Pro) was limited to a subset of 250 samples from ScienceQA due to API costs.