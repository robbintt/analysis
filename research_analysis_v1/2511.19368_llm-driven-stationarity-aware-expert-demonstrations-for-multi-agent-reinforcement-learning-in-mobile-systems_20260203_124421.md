---
title: LLM-Driven Stationarity-Aware Expert Demonstrations for Multi-Agent Reinforcement
  Learning in Mobile Systems
arxiv_id: '2511.19368'
source_url: https://arxiv.org/abs/2511.19368
generated_at: '2026-02-03T12:44:21'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# LLM-Driven Stationarity-Aware Expert Demonstrations for Multi-Agent Reinforcement Learning in Mobile Systems

*Tianyang Duan; Zongyuan Zhang; Zheng Lin; Songxiao Guo; Xiuxian Guan; Guangyu Wu; Zihan Fang; Haotian Meng; Xia Du; Ji-Zhe Zhou; Heming Cui; Jun Luo; Yue Gao*

---

> ### ðŸ“Š Quick Facts
>
> *   **Quality Score:** 9/10
> *   **Total Citations:** 40
> *   **Performance Gain:** +62% (Orlando), +72% (Hong Kong)
> *   **LLM Execution Rate:** Up to 92% (with refinement)
> *   **Core Innovation:** Integration of LLMs with MARL via theoretical non-stationarity bounds

---

## Executive Summary

This research addresses the critical challenge of **non-stationarity** in Multi-Agent Reinforcement Learning (MARL), specifically within complex, decentralized mobile systems. In MARL environments modeled as Decentralized Partially Observable Markov Decision Processes (Dec-POMDPs), agents typically update their policies synchronously. This simultaneity causes the environment dynamics to shift constantly from an individual agent's perspective, leading to training instability, oscillating performance, and slow convergence.

The authors propose **RELED**, a scalable MARL framework that integrates Large Language Models (LLMs) to mitigate non-stationarity through high-quality expert demonstrations. The innovation lies in two core modules: the **Stationarity-Aware Expert Demonstration (SED)** module and the **Hybrid Expert-Agent Policy Optimization (HPO)** module.

*   **SED Module:** Utilizes LLMs to generate Python code for expert trajectories, employing prompts refined by theoretical non-stationarity boundsâ€”specifically reward fluctuations and policy divergenceâ€”to ensure data quality.
*   **HPO Module:** Executes decentralized training by maintaining separate value functions for agent and expert trajectories ($V^a_i$ and $V^e_i$). Crucially, it employs **Dynamic Time Warping (DTW)** to adaptively fuse imitation and exploration signals.

Extensive empirical testing on real-world city networks (Orlando and Hong Kong) demonstrates RELED's superior performance, significantly outperforming the IPPO baseline. This work establishes a new paradigm for generating expert data in dynamic environments by combining LLM semantic reasoning with reinforcement learning optimization, offering a scalable solution for resource-constrained mobile systems.

---

## Key Findings

*   **Superior Performance:** RELED outperforms current state-of-the-art MARL methods in complex environments like OpenStreetMap, achieving approximately 62% higher rewards in Orlando and 72% higher rewards in Hong Kong compared to the IPPO baseline.
*   **Non-Stationarity Mitigation:** Successfully mitigates the non-stationarity caused by synchronous agent policy updates.
*   **Training Stability:** Enhances training stability through LLM-driven expert demonstrations guided by theoretical non-stationarity bounds.
*   **Accelerated Convergence:** Accelerates policy convergence by effectively balancing expert guidance with autonomous exploration.

---

## Methodology

The authors propose **RELED**, a scalable Multi-Agent Reinforcement Learning (MARL) framework featuring two core modules:

1.  **Stationarity-Aware Expert Demonstration (SED):**
    Uses LLMs to generate trajectories that are refined by theoretical non-stationarity bounds. This ensures the expert demonstrations are relevant to the current state of the environment.

2.  **Hybrid Expert-Agent Policy Optimization (HPO):**
    A decentralized training framework that adaptively balances learning from two sources: expert-generated trajectories and agent-generated trajectories. This balance is crucial for maintaining both stability and exploratory capability.

---

## Technical Details

RELED is a scalable framework designed for Dec-POMDPs to mitigate non-stationarity by integrating LLM-generated expert demonstrations with autonomous exploration.

### Core Components

**1. Stationarity-Aware Expert Demonstration (SED) Module**
*   **Function:** Partitions agents and utilizes LLMs with specific prompts to generate expert trajectories.
*   **Process:**
    *   Uses **Initial Prompts** and **Feedback Prompts**.
    *   Feedback prompts incorporate **reward fluctuations** and **policy divergence**.
    *   Outputs Python code representing expert trajectories.

**2. Hybrid Expert-Agent Policy Optimization (HPO) Module**
*   **Architecture:** A decentralized training framework.
*   **Value Functions:** Maintains separate value functions for agents and experts:
    *   Agent Value Function: $V^a_i$
    *   Expert Value Function: $V^e_i$
*   **Fusion Mechanism:** Uses **Dynamic Time Warping (DTW)** to adaptively fuse imitation and exploration signals.

### Problem Formulation
*   **Environment:** Dec-POMDP.
*   **Observation Space:** Shape $2m_{out} + 2$.
*   **Action Space:** Discrete.
*   **Reward Function:**
    $$R_k = -(t_k - t_{k-1}) + \omega_d(d_{k-1} - d_k)$$

---

## Results

Experiments were conducted on Orlando and Hong Kong maps with 10 agents to validate the framework's efficacy.

*   **Overall Performance:** The full RELED framework significantly outperformed the IPPO baseline.
    *   **Orlando:** ~62% higher rewards.
    *   **Hong Kong:** ~72% higher rewards.
*   **LLM Efficacy:** GPT-4 Turbo achieved execution rates up to **92%** with refinement, compared to 50â€“74% without it.
*   **Ablation Studies:**
    *   **Feedback Refinement:** Removing the feedback refinement mechanism resulted in a **19â€“26% drop in rewards**, validating its necessity.
    *   **Adaptive Weighting:** Replacing DTW-based adaptive weighting with fixed weights caused significant degradation.
        *   Example: A fixed weight of $\alpha=0.5$ resulted in negative rewards (catastrophic failure).
    *   **DTW vs. Alternatives:** The DTW-based approach proved superior to logit routing methods.

---

## Contributions

*   **Novel Integration:** Introduces a novel integration of LLMs with MARL to generate expert demonstrations specifically for non-stationary environments.
*   **Theoretical Grounding:** Provides theoretical backing using non-stationarity bounds to improve the quality of generated data.
*   **Scalability:** Ensures the solution is scalable for decentralized, resource-constrained mobile systems.
*   **Empirical Validation:** Validates the approach with extensive empirical evidence utilizing real city network data.

---

**References:** 40 citations