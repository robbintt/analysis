# BLASST: Dynamic BLocked Attention Sparsity via Softmax Thresholding

*Jiayi Yuan; Cameron Shinn; Kai Xu; Jingze Cui; George Klimiashvili; Guangxuan Xiao; Perkz Zheng; Bo Li; Yuxin Zhou; Zhouhai Ye; Weijie You; Tian Zheng; Dominic Brown; Pengbo Wang; Richard Cai; Julien Demouth; John D. Owens; Xia Hu; Song Han; Timmy Liu; Huizi Mao*

---

> ### ðŸ“‹ Quick Facts
> **Quality Score:** 9/10
>
> *   **Prefill Speedup:** 1.62x
> *   **Decode Speedup:** 1.48x
> *   **Sparsity Level:** ~74%
> *   **Accuracy:** No compromise
> *   **Context Length:** Validated up to 1M tokens
> *   **Supported Architectures:** MHA, GQA, MQA, MLA

---

## Executive Summary

**Problem**
The self-attention mechanism in Transformer models presents a significant computational and memory bandwidth bottleneck as context windows expand to 128K to 1M tokens. While sparse attention methods offer a theoretical remedy, they frequently struggle with practical deployment due to their reliance on complex proxy networks to determine importance or high calibration overheads. The challenge lies in achieving efficient, high-sparsity inference without introducing prohibitive latency costs or requiring extensive modifications to existing model architectures.

**Innovation**
BLASST (Dynamic BLocked Attention Sparsity via Softmax Thresholding) addresses these limitations by introducing a proxy-free, dynamic sparsity mechanism directly within the FlashAttention kernel's online-softmax process. By applying a threshold to running maximum values, BLASST identifies and prunes negligible blocks with minimal overhead. When a block is identified as negligible, the kernel efficiently skips the softmax computation, the loading of Value blocks from high-bandwidth memory, and the matrix multiplication operations. Furthermore, the research introduces an automated calibration strategy that reveals an inverse relationship between the optimal threshold value and the context length, simplifying hyperparameter tuning and ensuring robustness.

**Results**
Evaluations across long-context models demonstrated that BLASST achieves approximately 74% sparsity, translating into substantial performance improvements: a 1.62x acceleration for the prefill stage and a 1.48x speedup for the decode stage, all without compromising accuracy. The method exhibits universal compatibility across diverse attention architectures, including Multi-Head Attention (MHA), Grouped-Query Attention (GQA), Multi-Query Attention (MQA), and Multi-Head Latent Attention (MLA). Although the approach is effective as a post-training application, the authors note that sparse-aware training can further enhance the model's robustness to the pruning process.

**Impact**
BLASST provides a unified, efficient solution for accelerating inference across the entire attention pipeline by eliminating the need for external importance proxies and drastically reducing memory bandwidth usage. As a kernel-level optimization that integrates seamlessly into existing FlashAttention implementations, this work offers a highly practical path for deploying long-context applications where throughput and latency are critical. This research validates that significant speedups for massive context windows are attainable without complex structural modifications, effectively lowering the barrier for efficient, long-context AI deployment.

---

## Key Findings

*   **Significant Performance Gains:** BLASST achieves a **1.62x speedup** for the prefill stage and a **1.48x speedup** for the decode stage at ~74% sparsity without compromising accuracy.
*   **Inverse Length-Threshold Relationship:** Automated calibration reveals an inverse relationship between the optimal threshold value and the context length, simplifying setup.
*   **Universal Applicability:** The method is effective across MHA, GQA, MQA, MLA, and for both prefill and decode phases.
*   **Training Compatibility:** Sparse-aware training can make models more robust to sparse attention.

---

## Methodology

*   **Dynamic Pruning via Online Softmax:** BLASST utilizes existing online softmax computation data, applying a fixed threshold to dynamically prune negligible attention scores.
*   **Computational Skipping:** The method skips three major operations for negligible tokens:
    *   Softmax computation
    *   Loading of Value blocks
    *   Matrix multiplication
*   **Kernel Integration:** The approach is designed to integrate into existing FlashAttention kernel designs to ensure negligible overhead, adding only a single comparison per block.

---

## Contributions

1.  **Efficient Sparse Attention Mechanism:** Introduced a proxy-free, pre-computation-free dynamic sparsity method leveraging online softmax statistics to reduce computational load and memory bandwidth.
2.  **Unified Architecture Support:** Provided a single solution that accelerates inference across MHA, GQA, MQA, MLA, and both prefill and decode stages.
3.  **Deployment Robustness:** Established a calibration strategy that simplifies hyperparameter tuning by defining an adaptive relationship between context length and optimal sparsity threshold.

---

## Technical Details

BLASST (Dynamic BLocked Attention Sparsity via Softmax Thresholding) integrates dynamic block pruning into the FlashAttention algorithm's block-wise online-softmax process.

*   **Process Logic:** It processes blocks sequentially and skips a block if its maximum value is lower than a running row max by a value greater than a threshold (ln(Î»)).
*   **Overhead Management:** When a block is pruned, the kernel skips exponential computation, loading the value block from HBM, and matrix multiplication, adding only a single comparison per block in overhead.
*   **Implementation Details:**
    *   Uses custom CUDA kernels for both prefill and decode phases.
    *   Operates as a post-training technique (with optional sparse-aware training).
    *   Utilizes automated calibration revealing the inverse relationship between threshold and context length.
*   **Comparison to SpargeAttention:**
    *   Optimizes both prefill and decode (vs. mostly prefill).
    *   Uses computed statistics for zero-overhead decisions.
    *   Reduces memory bandwidth by skipping Value loading.

---

## Results

BLASST achieved a **1.62x speedup** in the prefill stage and a **1.48x speedup** in the decode stage, operating at approximately 74% sparsity without compromising accuracy. Validations were performed on long-context models ranging from 128K to 1M tokens.

The approach is effective across the following architectures:
*   Multi-Head Attention (MHA)
*   Grouped-Query Attention (GQA)
*   Multi-Query Attention (MQA)
*   Multi-Head Latent Attention (MLA)

---
*Research Paper Analysis generated with a Quality Score of 9/10*