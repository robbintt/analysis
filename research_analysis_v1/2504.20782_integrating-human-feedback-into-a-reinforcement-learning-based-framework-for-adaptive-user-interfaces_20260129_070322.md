# Integrating Human Feedback into a Reinforcement Learning-Based Framework for Adaptive User Interfaces

*Daniel Gaspar-Figueiredo; Marta FernÃ¡ndez-Diego; Silvia AbrahÃ£o; Emilio Insfran*

***

## Executive Summary

Current Adaptive User Interfaces (AUIs) frequently fail to deliver truly personalized experiences because they rely on generalized, pre-trained models that cannot adapt to the specific, evolving preferences of individual users. This creates a disconnect between system-driven adaptations and actual user needs, limiting the effectiveness of responsive software design. This research addresses the challenge of embedding Reinforcement Learning (RL) agents within user interactions, specifically targeting the difficulty of aligning automated UI adaptations with the nuanced requirements of human users in dynamic environments.

The authors introduce a novel **Human-in-the-Loop (HITL)** framework that formalizes UI adaptation as a Markov Decision Process (MDP), shifting the paradigm from static, one-size-fits-all models to the training of a unique RL agent for each individual user. Technically, the architecture employs a "Dual-Source Reward Mechanism" that synthesizes a baseline from predictive HCI models with modifiers derived from a dedicated Human Feedback Module. This module ingests offline user feedback in five distinct formats to directly fine-tune the agentâ€™s policy. Validated through an empirical study involving **33 participants**, the results confirm that allowing users to actively shape the agent's policy through direct feedback significantly boosts User Experience (UX), offering a promising direction for the future of adaptive software engineering.

***

> ### ðŸ“„ Quick Facts
> *   **Quality Score:** 8/10
> *   **Total References:** 40 Citations
> *   **Study Participants:** 33 (Master's CS Students)
> *   **Application Domains:** 2 (E-learning & Trip-planning)
> *   **Core Architecture:** Markov Decision Process (MDP)
> *   **Key Innovation:** Dual-Source Reward Mechanism

***

## Key Findings

*   **Enhanced UX via Feedback Integration:** Integrating personalized human feedback directly into Reinforcement Learning (RL) policies significantly enhances User Experience (UX).
*   **User-Specific Training:** Training a unique RL agent for each userâ€”rather than relying on a single pre-trained modelâ€”enables more personalized and responsive UI adaptations.
*   **Cross-Domain Validation:** The enhanced RL-based framework was successfully validated across two distinct domains: an **e-learning platform** and a **trip-planning application**.
*   **Superior Outcomes:** Empirical results suggest that allowing users to actively shape their agent's policy leads to superior user-centered design outcomes compared to non-adaptive interfaces.

## Methodology

The researchers enhanced an existing RL-based Adaptive User Interface framework by incorporating a mechanism that ingests personalized human feedback directly into the learning loop. The key methodological shifts included:

*   **Personalized Agents:** Instead of utilizing a generalized, pre-trained RL model, the approach trains a distinct, unique RL agent for each individual user.
*   **Empirical Study Design:** A study was conducted involving 33 participants to evaluate the impact on UX.
*   **Comparative Analysis:** The study compared the proposed feedback-driven adaptive interfaces against standard non-adaptive user interfaces.
*   **Domains:** Testing was performed in an e-learning environment and a trip-planning application to ensure versatility.

## Technical Details

The framework extends the Reinforcement Learning (RL) approach for Adaptive User Interfaces (AUIs) through the following structural components and formalizations:

### Formalization & Architecture
*   **Decision Process:** Formalized as a Markov Decision Process (MDP).
*   **Scope:** Trains a unique RL agent per user.
*   **Components:**
    *   **Monitor:** Manages the 'Context of Use' (User, Platform, and Environment models).
    *   **Adaptation Manager:** Orchestrates the Intelligent UI Adaptor.

### Reward Mechanism
*   **Dual-Source Reward Mechanism:** Combines two inputs:
    1.  **Baseline:** Predictive HCI models.
    2.  **Modifier:** A Human Feedback Module.

### Feedback & Action Spaces
*   **Feedback Collection:** Offline via a Collector supporting five formats:
    *   Comparative
    *   Attribute
    *   Evaluative
    *   Visual
    *   Keypoint
*   **Action Space:** The agent can adapt:
    *   Layout
    *   Typography
    *   Density
    *   Visuals
    *   Navigation

## Contributions

1.  **Human-in-the-loop RL Integration:** A novel framework that effectively integrates personalized human feedback into the RL learning process to address the challenge of embedding RL agents within user interactions.
2.  **User-Specific Agent Training:** A shift from single, static pre-trained models to a personalized paradigm where individual users train their own RL agents, fostering real-time responsiveness and personalization.
3.  **Empirical Validation of UX Improvements:** Evidence-based confirmation that coupling RL-driven adaptations with human oversight significantly boosts User Experience, providing a promising direction for advancing adaptive capabilities in software applications.

## Results

An empirical user study following the **Goal-Question-Metric (GQM)** template compared the RL-with-Human-Feedback (RLHF) approach against non-adaptive interfaces.

*   **Participants:** 33 Master's students in Computer Science.
*   **Domains:** E-learning platform and Trip-planning application.
*   **Outcome:** Personalized RL agents yielded superior outcomes compared to non-adaptive interfaces.
*   **Conclusion:** Integrating active user feedback significantly enhanced User Experience (UX), successfully validating the framework's efficacy in real-world scenarios.

***
**References:** 40 citations