---
title: 'Generalized Neighborhood Attention: Multi-dimensional Sparse Attention at
  the Speed of Light'
arxiv_id: '2504.16922'
source_url: https://arxiv.org/abs/2504.16922
generated_at: '2026-02-03T20:15:23'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Generalized Neighborhood Attention: Multi-dimensional Sparse Attention at the Speed of Light

*Ali Hassani; Fengzhe Zhou; Aditya Kane; Jiannan Huang; Chieh-Yun Chen; Min Shi; Steven Walton; Markus Hoehnerbach; Vijay Thakkar; Michael Isaev; Qinsheng Zhang; Bing Xu; Haicheng Wu; Wen-mei Hwu; Ming-Yu Liu; Humphrey Shi*

---

> ### **Quick Facts**
> *   **Peak Performance:** 1.3 petaFLOPs/second on NVIDIA Blackwell.
> *   **End-to-End Speedup:** 28% to 46% in generative models (no fine-tuning required).
> *   **Attention Speedup:** Up to 11.1x on high-sparsity models (HunyuanVideo).
> *   **Key Innovation:** Unified multi-dimensional sparse attention (GNA) with optimized CUTLASS kernels.
> *   **Architecture:** Optimized for NVIDIA B200 GPUs using TMA and CuTe libraries.

---

### **Executive Summary**

The standard self-attention mechanism in Transformers suffers from quadratic computational complexity relative to sequence length, creating a significant bottleneck for processing high-resolution data such as images and videos. While sparse attention mechanisms reduce theoretical complexity, existing approaches are often fragmented, lack mathematical unification across dimensions, and fail to translate theoretical gains into practical hardware speedups. Furthermore, current implementations struggle to efficiently utilize the advanced memory hierarchies of modern accelerators, leading to sub-optimal throughput and latency for generative AI workloads.

This paper introduces **Generalized Neighborhood Attention (GNA)**, a mathematical framework that unifies locality-focused, multi-dimensional sparse attention mechanisms into a single formalism. To bridge the gap between theory and hardware performance, the authors developed a highly optimized Fused Multi-Headed Attention (FMHA) kernel specifically for the **NVIDIA Blackwell architecture**. This implementation leverages CUTLASS and the CuTe library to implement static Key-Value (KV) tiling, utilizing the Tensor Memory Accelerator (TMA) for efficient data movement.

Evaluated on NVIDIA B200 GPUs, the GNA implementation achieved a peak throughput of **1.3 petaFLOPs/second**. The architecture delivers substantial efficiency gains, with attention-level speedups ranging from **9.2x to 11.1x** on major generative models. Crucially, these improvements translate to **28% to 46% end-to-end speedups** in generative inference without requiring model fine-tuning. This research represents a significant advancement in the practical application of sparse attention, demonstrating that locality-focused mechanisms can be both mathematically rigorous and hardware-efficient.

---

### **Key Findings**

*   **Unified Framework:** Generalized Neighborhood Attention (GNA) successfully unifies locality-focused attention mechanisms into a single mathematical formalism.
*   **Record Performance:** Implementation on NVIDIA Blackwell achieves a peak throughput of **1.3 petaFLOPs/second**.
*   **Real-World Efficiency:** GNA delivers **28% to 46% end-to-end speedups** in generative models without the need for fine-tuning.
*   **Simulation Tools:** A custom simulator was developed to predict realistic speedup upper bounds, aiding in design optimization.

---

### **Methodology**

The research approach encompasses three primary components:

1.  **Formalization:** The team mathematically formalized GNA to unify various sparse attention mechanisms into a cohesive framework.
2.  **Simulation:** An advanced performance simulator was developed to evaluate design choices based on realistic speedup predictions, allowing for iterative optimization before hardware implementation.
3.  **Implementation:** GNA was implemented via an optimized Fused Multi-Headed Attention (FMHA) kernel tailored for the NVIDIA Blackwell architecture, utilizing the CUTLASS library to maximize hardware utilization.

---

### **Technical Details**

#### **Core Framework**
*   **Generalized Neighborhood Attention (GNA):** A unified locality-focused attention mechanism designed for multi-dimensional sparse attention.

#### **Hardware Optimization (NVIDIA Blackwell)**
*   **Architecture:** Optimized specifically for B200 GPUs.
*   **Libraries:** Uses CUTLASS and CuTe libraries for data movement.
*   **Memory Management:** Implements static Key-Value (KV) tiling to utilize the Tensor Memory Accelerator (TMA).
*   **Warp Logic:**
    *   **Producer Warp:** Maps iteration indices.
    *   **Softmax Warps:** Handle coordinate mapping.

#### **Kernel Design Features**
*   **Token Permutation:** A strategy optimized for permutation-equivariant architectures.
*   **Fusion:** Supports cross-attention fusion within a single kernel launch.
*   **Simulation Tool:** NATTENsim was developed to predict analytical speedup bounds.

---

### **Results**

#### **End-to-End Performance**
GNA delivers significant efficiency improvements in generative inference workflows without requiring model fine-tuning:
*   **Speedup Range:** 28% to 46% increase in inference speed.

#### **Attention-Level Speedups (B200 GPU)**
Significant acceleration was observed across specific high-profile generative models:

| Model | Sparsity | Window Size | Speedup |
| :--- | :--- | :--- | :--- |
| **Cosmos-7B** | 89% | 16x24x16 | **9.2x** |
| **Flux.1-dev** | 90% | 80x80 | **10.2x** |
| **HunyuanVideo** | 91% | 18x24x24 | **11.1x** |

#### **Memory Overhead Analysis**
Performance profiling indicates that the memory overhead for token permutation operations is negligible:
*   **Cosmos-7B:** Token permutation accounts for only **1.9%** of FMHA time.
*   **HunyuanVideo:** Token permutation accounts for only **1.0%** of FMHA time.

---

### **Contributions**

*   **Framework:** Introduction of the GNA framework for multi-dimensional sparse attention.
*   **Tools:** Development of an advanced performance simulator for realistic speedup estimates.
*   **Infrastructure:** Release of high-performance kernels for the NVIDIA Blackwell architecture.
*   **Open Source:** Open-sourcing of resources via the NATTEN project to benefit the broader research community.

---

**Quality Score:** 9/10  
**References:** 40 citations