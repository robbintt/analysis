---
title: 'Dream 7B: Diffusion Large Language Models'
arxiv_id: '2508.15487'
source_url: https://arxiv.org/abs/2508.15487
generated_at: '2026-02-04T15:52:27'
quality_score: 9
citation_count: 13
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Dream 7B: Diffusion Large Language Models

*Jiacheng Ye; Zhihui Xie; Lin Zheng; Jiahui Gao; Zirui Wu; Xin Jiang; Zhenguo Li; Lingpeng Kong*

---

> ### ðŸ“Š Quick Facts & Metrics
>
> *   **Model Size:** 7 Billion Parameters
> *   **Architecture:** Discrete Diffusion with Transformer Decoder (Full Attention)
> *   **Training Strategy:** AR-based Initialization & Context-Adaptive Noise Rescheduling
> *   **Key Performance vs. LLaMA-2-7B:**
>     *   **MMLU:** 46.0% (Dream 7B) vs. 45.8% (Baseline)
>     *   **GSM8K:** 15.9% (Dream 7B) vs. 14.4% (Baseline)
>     *   **HumanEval:** 19.5% (Dream 7B) vs. 13.0% (Baseline)
> *   **Inference:** Tunable quality-speed trade-off; parallel sequence refinement.
> *   **Quality Score:** 9/10

---

## Executive Summary

Large Language Models (LLMs) have predominantly relied on autoregressive (AR) architectures that generate text sequentially, token by token. While effective, this approach introduces inherent limitations in inference flexibility, making tasks such as parallel sequence refinement, arbitrary-order generation, and complex infilling computationally inefficient or structurally difficult. This paper addresses the challenge of developing discrete diffusion models that can match or exceed the performance of established AR models. The authors aim to validate whether diffusion-based approaches can serve as a viable, superior alternative for general language tasks, mathematics, and coding, thereby overcoming the sequential bottlenecks of current state-of-the-art systems.

The core innovation of Dream 7B is the successful application of discrete diffusion modeling to language generation, utilizing a Transformer Decoder with Full Attention to predict all masked tokens simultaneously during each denoising step rather than sequentially. To achieve high performance, the authors introduce two specific training strategies: **AR-based LLM Initialization**, which bootstraps the model by leveraging weights from pre-trained autoregressive LLMs, and **Context-Adaptive Noise Rescheduling**, a mechanism that adjusts the noise schedule at the token level during training to adapt to context. The model employs a continuous-time parameterization ($t \in [0, 1]$) with an absorbing state and utilizes a weighted cross-entropy objective function, prioritizing steps near clean data to stabilize the iterative denoising process.

Dream 7B establishes state-of-the-art performance as the most powerful open diffusion language model, consistently outperforming baselines on general, mathematical, and coding benchmarks. Specifically, Dream-Base achieves **46.0% on MMLU** and **15.9% on GSM8K**, surpassing the LLaMA-2-7B baseline which scored 45.8% and 14.4%, respectively. In coding tasks, the model demonstrates a significant performance gap, attaining **19.5% on HumanEval** compared to the baseline's 13.0%.

Crucially, the model validates the architectural advantages of diffusion by explicitly demonstrating capabilities in **planning**, **arbitrary-order generation**, and **infilling**. Regarding inference, Dream 7B offers a distinct structural advantage over standard AR models through a tunable quality-speed trade-off. Unlike AR models locked into sequential latency, Dreamâ€™s parallel denoising process allows users to reduce the number of sampling steps (e.g., from 64) to accelerate generation speed with controlled quality degradation.

---

## Key Findings

*   **State-of-the-Art Performance:** Dream 7B is currently the most powerful open diffusion language model, outperforming existing models on general, mathematical, and coding benchmarks.
*   **Inference Flexibility:** The model offers superior flexibility compared to AR models, supporting planning, arbitrary-order generation, and text infilling.
*   **Tunable Trade-offs:** Users can adjust the balance between generation quality and inference speed, a feature not native to sequential AR models.
*   **Parallel Refinement:** Unlike standard autoregressive models, Dream 7B refines sequences in parallel through an iterative denoising process.
*   **Competitive Baselines:** Demonstrated significant improvements over LLaMA-2-7B, particularly in coding tasks (HumanEval).

---

## Methodology

The research moves away from traditional autoregressive modeling, adopting a **discrete diffusion modeling** approach. The methodology is defined by two primary components:

1.  **Core Mechanism:** Text is generated by refining sequences in parallel through an iterative denoising process.
2.  **Training Strategies:** The high performance is attributed to two specific, simple yet effective strategies:
    *   **AR-based LLM Initialization:** Utilizing pre-existing autoregressive LLMs to initialize the model, providing a strong starting point for training.
    *   **Context-Adaptive Noise Rescheduling:** Applying a token-level noise rescheduling mechanism that adapts to the specific context during training.

---

## Technical Details

*   **Diffusion Process:** Utilizes a discrete diffusion process operating directly on vocabulary tokens.
*   **Architecture:** Employs a Transformer Decoder with **Full Attention** to predict all masked tokens simultaneously. This represents a significant shift from autoregressive methods.
*   **Time Parameterization:** Adopts a continuous-time parameterization where $t \in [0, 1]$.
    *   **Forward Process:** Uses an absorbing state.
    *   **Reverse Process:** Dedicated to denoising.
*   **Objective Function:** Uses a weighted cross-entropy loss (reformulated ELBO).
    *   **Noise Schedule:** $\alpha_t = 1 - t$
    *   **Time-Dependent Weight:** $w(t) = \frac{1}{t}$ (used to prioritize steps near clean data).
*   **Implementation Details:**
    *   Uses shifted prediction and weight initialization.
    *   Maintains alignment with standard AR Transformers to facilitate weight initialization from pre-trained AR models.

---

## Results

*   **Benchmark Performance:**
    *   **MMLU:** 46.0% (vs. LLaMA-2-7B at 45.8%)
    *   **GSM8K:** 15.9% (vs. LLaMA-2-7B at 14.4%)
    *   **HumanEval:** 19.5% (vs. LLaMA-2-7B at 13.0%)
*   **Inference Capabilities:** Successfully demonstrated planning, arbitrary-order generation, and infilling.
*   **Efficiency:** Provides a tunable quality-speed trade-off via the number of sampling steps.
*   **Comparative Context:** Related work notes LLaDA (8B) as competitive with LLaMA3-8B and mentions Mercury Coder and DiffuLLaMA in the context of inference efficiency.

---

## Contributions

*   **Open Source Release:** The authors released `Dream-Base` and `Dream-Instruct` (7B parameter models) to facilitate further research.
*   **Empirical Validation:** Provided strong evidence that diffusion models can rival or exceed current AR models in complex tasks (math and coding) while offering generation flexibility.
*   **Methodological Validation:** Introduced and validated specific training methodsâ€”AR initialization and context-adaptive noise reschedulingâ€”as effective pathways for training high-performance diffusion language models.

---

**Report Quality Score:** 9/10 | **References:** 13 citations