---
title: Trajectory Entropy Reinforcement Learning for Predictable and Robust Control
arxiv_id: '2505.04193'
source_url: https://arxiv.org/abs/2505.04193
generated_at: '2026-02-03T06:27:26'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Trajectory Entropy Reinforcement Learning for Predictable and Robust Control
*Bang You; Chenxu Wang; Huaping Liu*

**Quality Score:** 9/10 | **References:** 40 citations

---

### ðŸ“Š Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Core Method** | Trajectory Entropy Reinforcement Learning (TERL) |
| **Inductive Bias** | Minimizing entropy of action trajectories (Simplicity) |
| **Key Mechanism** | Variational parameterized action prediction model |
| **Top Performance** | Score of 479 on Humanoid Walk |
| **Noise Robustness** | 90.1% performance retention at high action noise ($\sigma=0.30$) |
| **Relation to SAC** | SAC is a special case of TERL when regularization weight $\alpha = 0$ |

---

## Executive Summary

Standard Deep Reinforcement Learning (DRL) policies often suffer from brittleness and unpredictability because they tend to exploit spurious correlations between observations and actions rather than learning robust, generalizable control strategies. This lack of inherent structural simplicity makes agents vulnerable to environmental perturbations, dynamic changes, and sensor noise, limiting their reliability in real-world, high-stakes applications such as robotics where consistency is critical.

The authors propose **Trajectory Entropy Reinforcement Learning (TERL)**, a framework that introduces an inductive bias toward simplicity by explicitly minimizing the entropy of entire action trajectories. Technically, TERL estimates the number of bits required to describe action sequences given state trajectories using a variational parameterized action prediction model. This estimation is integrated into the reward function as an information-regularization term, creating a joint optimization process where the policy maximizes environmental rewards while simultaneously minimizing trajectory complexity. Notably, the Soft Actor-Critic (SAC) algorithm is identified as a special case of TERL where this regularization weight ($\alpha$) is set to zero.

TERL demonstrated superior performance and robustness in high-dimensional locomotion tasks compared to state-of-the-art baselines like RPC, LZ-SAC, and SAC. Specifically, TERL achieved scores of 137 on H1 Walk and 479 on Humanoid Walk. In robustness stress tests, the method retained 75.5% of its performance under half-mass conditions, 90.1% under high action noise ($\sigma=0.30$), and 92.9% under observation noise ($\sigma=0.04$). Furthermore, compression analysis using bzip2 confirmed that TERL generates more predictable, periodic trajectories, yielding the smallest normalized average file sizes among tested methods.

This research establishes a significant link between minimizing trajectory entropy and enhancing policy robustness, offering a novel solution to the problem of spurious correlations in DRL. By demonstrating that enforcing simplicity leads to more predictable and noise-resistant control, TERL provides a valuable tool for developing agents capable of operating reliably in dynamic, unstructured environments.

---

## Key Findings

*   **Simplicity as Robustness:** Minimizing the entropy of entire action trajectories serves as an effective inductive bias for simplicity, significantly enhancing policy robustness against environmental perturbations.
*   **Consistent Trajectories:** The proposed method generates policies that produce action trajectories which are more **cyclical** and **consistent** compared to standard deep reinforcement learning approaches.
*   **Superior High-Dimensional Performance:** TERL achieves superior performance and robustness to noise and dynamic changes in high-dimensional locomotion tasks compared to state-of-the-art methods (RPC, LZ-SAC, SAC).
*   **Effective Estimation:** Trajectory entropy can be effectively estimated and utilized within the reward function using a variational parameterized action prediction model.

---

## Methodology

The authors propose **Trajectory Entropy Reinforcement Learning (TERL)**, an agent designed to simultaneously maximize standard rewards while minimizing the entropy of action trajectories.

*   **Trajectory Entropy Definition:** Defined as the number of bits required to describe information in action trajectories after the agent observes state trajectories, effectively enforcing a simplicity bias.
*   **Variational Model:** A variational parameterized action prediction model is learned to estimate trajectory entropy and construct an information-regularized reward function.
*   **Joint Optimization:** A practical algorithm is developed to enable the joint optimization of both the policy and the action prediction model.

---

## Technical Details

*   **Inductive Bias:** TERL introduces an inductive bias for simplicity by minimizing the entropy of entire action trajectories.
*   **Joint Optimization:** It jointly optimizes a policy and a prediction model to minimize trajectory entropy while maximizing environmental rewards.
*   **Reward Construction:** Trajectory entropy is estimated via a variational parameterized action prediction model to construct an information-regularized reward function.
*   **Relation to SAC:** Soft Actor-Critic (SAC) is a special case of TERL where the inductive bias is removed ($\alpha = 0$).
*   **Hyperparameter Control:** The trade-off between rewards and entropy is controlled by hyperparameter $\alpha$.

---

## Results

**Performance Scores:**
*   **H1 Walk:** 137
*   **Humanoid Walk:** 479
*   *TERL outperformed baselines including RPC, LZ-SAC, and SAC.*

**Robustness Tests:**
*   **Half Mass:** Retained 75.5% performance.
*   **High Action Noise ($\sigma=0.30$):** Retained 90.1% performance.
*   **Observation Noise ($\sigma=0.04$):** Retained 92.9% performance.
*   *TERL generally outperformed other methods in stress tests.*

**Trajectory Analysis:**
*   Generated more periodic patterns.
*   Achieved the smallest normalized average file size using **bzip2 compression** (indicating lower entropy/predictability).
*   **Ablation Study:** Confirmed that increasing hyperparameter $\alpha$ reduces trajectory file size.

---

## Contributions

1.  **Inductive Bias:** Introduction of a specific inductive bias towards simplicity in reinforcement learning by minimizing trajectory entropy to address the issue of spurious correlations between observations and actions.
2.  **Estimation Method:** A method to effectively estimate trajectory entropy using a variational parameterized action prediction model, integrating this estimation into the reward signal via information regularization.
3.  **Algorithmic Approach:** Formulation of a practical algorithmic approach that allows for the co-optimization of the control policy and the prediction model.
4.  **Empirical Validation:** Empirical demonstration on complex, high-dimensional locomotion tasks that the proposed approach not only improves predictability (through cyclical/consistent trajectories) but also outperforms state-of-the-art baselines in robustness to noise and dynamic environmental changes.