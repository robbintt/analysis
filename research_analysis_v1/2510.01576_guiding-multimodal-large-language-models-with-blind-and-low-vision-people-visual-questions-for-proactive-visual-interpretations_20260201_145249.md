# Guiding Multimodal Large Language Models with Blind and Low Vision People Visual Questions for Proactive Visual Interpretations

*Ricardo Gonzalez Penuela; Felipe Arias-Russi; Victor Capriles*

***

### ðŸ“Š Quick Facts

| Metric | Value |
| :--- | :--- |
| **Anticipation Accuracy** | 76.1% |
| **Human Preference** | 54.4% |
| **Quality Score** | 8/10 |
| **References** | 20 Citations |
| **Dataset Used** | VizWiz-LF |

***

## Executive Summary

Standard Multimodal Large Language Models (MLLMs) used in assistive technologies typically rely on static, exhaustive image captioning or reactive question-answering frameworks. While these approaches provide visual information, they often force Blind and Low Vision (BLV) users to sift through lengthy, irrelevant details, resulting in significant **cognitive load** and information inefficiency.

The core challenge addressed in this paper is the limitation of current systems which fail to prioritize visual relevance based on user intent. The authors introduce a **proactive visual interpretation system** that transitions away from reactive Question-Answering toward an anticipatory generation model. Technically, the system utilizes the **VizWiz-LF dataset** to perform data-driven context injection; it matches a new image against historical visual contexts and retrieves associated questions previously asked by BLV users. These historical inquiries are then used as prompts to guide the MLLM, optimizing it to generate descriptions tailored to likely user intent.

The results demonstrated that the context-aware descriptions successfully anticipated and contained answers to subsequent user questions in **76.1% of cases**. Furthermore, human labelers preferred the system's context-aware outputs over standard MLLM descriptions in **54.4% of direct comparisons**. This research empirically validates the efficacy of intent-driven generation in assistive AI, addressing the critical accessibility challenge of information overload.

## Key Findings

*   **High Anticipation Rate:** The system's context-aware descriptions successfully anticipated user questions in **76.1%** of cases, effectively pre-empting the need for follow-up queries.
*   **User Preference:** Human labelers preferred context-aware descriptions over standard ones in **54.4%** of comparisons.
*   **Inefficiency of Standard Models:** Standard MLLM applications are often inefficient, forcing users to sift through lengthy, irrelevant details.
*   **Impact of Historical Data:** Utilizing historical data to guide generation significantly improves the contextual relevance of visual interpretations.

## Technical Details

The proposed system shifts from reactive interactions to proactive generation through the following mechanisms:

*   **Core Model:** Utilizes a proactive generative model designed to shift from reactive Question-Answering to anticipating user questions.
*   **Data Injection:** Employs **data-driven context injection**, analyzing historical data from Blind and Low Vision (BLV) users to guide predictions.
*   **Optimization:** The MLLM is optimized for **context-aware generation**, prioritizing:
    *   Conciseness
    *   Relevance
    *   Cognitive load reduction
    *   Filtering out irrelevant visual noise found in standard captioning models.

## Methodology

*   **Dataset:** Utilized the **VizWiz-LF** dataset to access historical visual contexts and associated BLV user questions.
*   **Context Matching:** Implemented a context matching process to identify similar past visual contexts when presented with a new image.
*   **Prompt Engineering:** Used associated historical questions as prompts to guide the MLLM in generating context-aware descriptions.
*   **Evaluation Protocol:** Evaluated the system through a human evaluation protocol where three labelers revised and compared **92 pairs** of descriptions.

## Results

The system achieved an **Anticipation Accuracy of 76.1%**, meaning the context-aware descriptions successfully contained answers to subsequent questions in 76.1% of cases. In direct comparisons, human labelers preferred the system's descriptions over standard MLLM descriptions in **54.4% of instances**.

The study confirmed that:
1.  Standard MLLMs produce excessive, non-essential information.
2.  Integrating historical data significantly improves the contextual relevance of the generated text.

## Contributions

*   **Proactive System:** Developed a proactive interpretation system that shifts from static descriptions to interpretations based on historical user inquiries.
*   **Accessibility Solution:** Addressed the accessibility challenge of information overload and cognitive load by filtering visual details.
*   **Empirical Validation:** Provided empirical validation with quantitative evidence (76.1% anticipation rate, 54.4% preference rate) supporting context-aware generation.
*   **Open Science:** Contributed to open science by publicly releasing the paper's reviews and data analysis via a GitHub repository.

***

**Bibliography & Ratings**
*   **Total Citations:** 20
*   **Quality Score:** 8/10