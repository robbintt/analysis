# Soft Conflict-Resolution Decision Transformer for Offline Multi-Task Reinforcement Learning

*Shudong Wang; Xinfei Wang; Chenhao Zhang; Shanchen Pang; Haiyuan Gui; Wenhao Ji; Xiaojian Liao*

---

## Executive Summary

Offline Multi-Task Reinforcement Learning (MTRL) seeks to train agents capable of performing multiple tasks using only static, pre-collected datasets. A critical challenge in this domain is **"negative transfer,"** where knowledge shared between tasks conflicts and degrades overall performance. Current state-of-the-art methods, such as HarmoDT, rely on coarse-grained binary masks or uniform sparsity strategies to isolate task-specific parameters. However, the research identifies that these approaches are fundamentally flawed; they over-suppress key parameters, discarding useful shared knowledge, and fail to adapt to the fact that conflict intensity varies significantly across different tasks and evolves dynamically during the training process.

The authors introduce **SoCo-DT** (Soft Conflict-resolution Decision Transformer), a framework designed to resolve task conflicts through dynamic parameter modulation rather than hard isolation. Technically, SoCo-DT replaces binary masks with a soft masking mechanism grounded in Fisher Information, which calculates parameter importance to fine-tune weight contributions continuously. To address the heterogeneity of task conflicts, the method utilizes an **Interquartile Range (IQR)-based dynamic sparsity scheme**, constructing task-specific thresholding mechanisms. Furthermore, the model employs an asymmetric cosine annealing schedule for adaptive threshold evolution, enabling the system to automatically adjust the balance between knowledge sharing and conflict resolution as training progresses.

SoCo-DT demonstrates substantial performance improvements over existing baselines across rigorous benchmarks. On the Meta-World MT50 benchmark, the model achieved a **7.6% increase** in success rate compared to state-of-the-art methods. When evaluated on suboptimal datasets containing lower-quality demonstrations, SoCo-DT outperformed baselines by **10.5%**. Empirical analysis revealed that previous binary masking approaches incorrectly identified approximately 1,550 high-importance parameters as conflicting; restoring these alone yielded a 4.45% performance gain. By implementing the proposed soft masking and adaptive scheduling, SoCo-DT effectively captures the nuanced conflict landscape, delivering a 6.96% gain on near-optimal data and 9.33% on suboptimal data in ablation studies.

This work significantly advances the field of offline MTRL by empirically exposing the limitations of rigid, binary parameter isolation strategies. The findings establish that flexible, data-driven conflict resolutionâ€”achieved through soft masking and statistical importance weightingâ€”is superior to uniform, fixed approaches. SoCo-DT sets a new performance standard, particularly for environments with high task heterogeneity or variable data quality.

---

> ### ðŸ“Š Quick Facts
>
> *   **Core Innovation:** Fisher Information-based Soft Masking for dynamic conflict resolution.
> *   **Performance Gain:** +7.6% on Meta-World MT50 benchmark.
> *   **Suboptimal Data Performance:** +10.5% improvement over baselines.
> *   **Key Problem Solved:** Eliminates over-suppression of parameters found in binary masking methods.
> *   **Architecture:** Based on Decision Transformer (DT) with adaptive sparsity scheduling.

---

## Key Findings

*   **Flawed Masking Methods:** Existing masking methods in Multi-Task RL (MTRL) are flawed because coarse-grained binary masks over-suppress key conflicting parameters, hindering effective knowledge sharing.
*   **Uniform Sparsity Issues:** Current methods apply a uniform sparsity strategy, failing to account for the varying conflict levels exhibited by different tasks.
*   **Soft Masking Efficacy:** Utilizing a soft masking approach based on Fisher information allows for dynamic adjustment of parameters, successfully balancing the retention of important weights with the suppression of conflicting ones.
*   **Benchmark Superiority:** The proposed SoCo-DT model outperforms state-of-the-art baselines by **7.6%** on the Meta-World MT50 benchmark and **10.5%** on a suboptimal dataset.

---

## Methodology

The authors propose **SoCo-DT**, a Soft Conflict-resolution Decision Transformer for offline MTRL. The methodology rests on three core pillars:

1.  **Fisher Information-based Soft Masking:**
    This mechanism calculates parameter importance to perform dynamic adjustment. Unlike binary masks, this approach fine-tunes contributions rather than forcibly pruning weights.

2.  **IQR-based Dynamic Sparsity:**
    Utilizing the Interquartile Range (IQR), the system constructs task-specific thresholding schemes. This allows the model to adapt to the statistical distribution of conflicts for each specific task.

3.  **Adaptive Threshold Evolution:**
    The model employs an asymmetric cosine annealing schedule to update thresholds continuously throughout training, ensuring the balance between sharing and isolation evolves as learning progresses.

---

## Contributions

*   **Empirical Analysis:** Provides an empirical analysis exposing critical weaknesses in prior binary masks and fixed sparsity strategies.
*   **Novel Framework:** Introduces a novel soft conflict-resolution framework that uses Fisher information to fine-tune parameter contributions.
*   **Adaptive Scheduling:** Contributes a new adaptive sparsity scheduling approach combining IQR-based statistical analysis with asymmetric cosine annealing.
*   **Benchmark Advancement:** Advances benchmarks by achieving significant performance improvements on Meta-World MT50 and suboptimal datasets.

---

## Technical Details

### Base Architecture
The approach is built on the **Decision Transformer (DT)** architecture, which formulates RL as a conditional sequence modeling problem.
*   **Mechanism:** Transformer with causally masked autoregressive prediction.
*   **Input:** Trajectory sequences (`R_t`, `s_t`, `a_t`).
*   **Output:** Actions based on the sequence.
*   **Loss Function:** Standard behavior cloning loss:
    `L_DT = E_{Ï„ âˆ¼ D}[âˆ‘_{t=1}^T ||Ï€_Î¸(Ï„_{1:t}) - a_t||Â²_2]`

### Benchmark Methods
*   **Prompt-DT:** Uses task-specific prompts constructed with K-step demonstration sub-trajectories.
*   **HarmoDT:** Employs task-specific binary mask vectors `M_Ti âˆˆ {0,1}^d` to select subspaces from shared parameters. Masked parameters are `Î¸_Ti = Î¸ âŠ™ M_Ti`. It identifies optimal subspaces using a harmony score:
    `H(T_i) = A(T_i) + Î» F(T_i)`
    *   `A(T_i)`: Agreement Score (gradient alignment).
    *   `F(T_i)`: Importance Score (derived from Fisher information).

### SoCo-DT Solution
*   **Soft Masking:** Replaces binary masks with a mechanism for dynamic weight adjustment.
*   **Fisher Basis:** Explicitly based on Fisher information to determine parameter importance.
*   **Dynamic Sparsity:** Adapts to varying conflict levels across tasks and training stages, contrasting with the fixed strategies of HarmoDT and Prompt-DT.

---

## Results

*   **Overall Performance:** SoCo-DT outperforms SOTA baselines by **7.6%** on the Meta-World MT50 benchmark and achieves a **10.5%** gain over baselines on suboptimal datasets.
*   **Baseline Comparison:** HarmoDT shows a 6.8% gain over Prompt-DT on Meta-World MT15 (near-optimal).
*   **Binary Masking Flaws:**
    *   In binary masking (HarmoDT), approximately **1,550** top-importance parameters are mistakenly masked.
    *   Restoring these wrongly masked parameters improves success rate by **4.45%** on near-optimal datasets and **3.92%** on suboptimal datasets.
*   **Ideal Soft Masking Gains:** Implementing an ideal soft masking mechanism yields a **6.96%** performance gain on near-optimal datasets and a **9.33%** gain on suboptimal datasets.
*   **Conflict Dynamics:** Analysis shows the proportion of conflicting parameters differs significantly across tasks (e.g., basketball, coffee-push, disassemble, door-close, door-lock), and conflict ratios change dynamically during training, rendering fixed-sparsity strategies ineffective.

---
**Paper Quality Score:** 9/10  
**References:** 7 citations