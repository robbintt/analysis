---
title: 'From Pixels to Facts (Pix2Fact): Benchmarking Multi-Hop Reasoning for Fine-Grained
  Visual Fact Checking'
arxiv_id: '2602.00593'
source_url: https://arxiv.org/abs/2602.00593
generated_at: '2026-02-06T05:07:25'
quality_score: 8
citation_count: 32
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# From Pixels to Facts (Pix2Fact): Benchmarking Multi-Hop Reasoning for Fine-Grained Visual Fact Checking

*Yifan Jiang; Cong Zhang; Bofei Zhang; Yifan Yang; Bingzhang Wang; Yew-Soon Ong*

---

> ### **Quick Facts**
>
> *   **Benchmark Name:** Pix2Fact
> *   **Dataset Volume:** 1,000 High-Resolution (4K+) Images
> *   **Scenarios Covered:** 8 Daily-life Categories
> *   **Human Baseline Accuracy:** 56%
> *   **SOTA VLM Accuracy:** 24.0%
> *   **Performance Gap:** 32% (Absolute)
> *   **Expert Annotation Time:** 35–40 minutes/question
> *   **Quality Score:** 8/10

---

### Executive Summary

Current state-of-the-art (SOTA) Vision-Language Models (VLMs) demonstrate proficiency in basic object recognition but fail to achieve human-level comprehension in scenarios requiring deep cognitive synthesis. The core problem addressed is the inability of these models to perform fine-grained visual fact-checking that relies on multi-hop reasoning. Specifically, current architectures struggle to bridge the gap between pixel-level visual grounding and knowledge-intensive reasoning, preventing them from answering complex questions that require integrating detailed visual observations with external knowledge.

To address this, the authors introduce **Pix2Fact**, a novel benchmark designed to evaluate the synergy between expert-level perception and knowledge-based reasoning. The innovation lies in the dataset's rigorous construction and its unique problem formulation. The benchmark comprises 1,000 high-resolution (4K+) images across eight daily-life scenarios, curated through a three-stage pipeline and annotated by PhD-level experts. Technically, the tasks are designed around a "Unique Information Bridge" logic structure, forcing models to move beyond surface-level feature matching.

Empirical evaluation highlights a stark performance disparity between automated systems and human capabilities. While human participants achieved a baseline accuracy of 56%, the collective performance of SOTA VLMs reached only 24.0%, resulting in a substantial 32% absolute performance gap. Even the most advanced proprietary models, including Gemini-3-Pro and GPT-5, failed to master the benchmark. These results quantitatively demonstrate that while current models can process high-resolution inputs, they lack the necessary architectural sophistication to effectively fuse detailed visual grounding with the complex, multi-hop reasoning required for fact verification.

---

## Key Findings

*   **Substantial Performance Gap:** There is a significant divide between model capabilities and human performance, with SOTA VLMs achieving only 24.0% accuracy compared to a human baseline of 56%.
*   **Integration Failure:** Current VLMs struggle to integrate detailed visual grounding with knowledge-based reasoning, often failing to synthesize information from different sources.
*   **Benchmark Difficulty:** The Pix2Fact benchmark presents a significant challenge; even top-tier models like Gemini-3-Pro and GPT-5 failed to master the dataset.
*   **Agent Limitations:** Current multimodal agents are unable to replicate human-level visual comprehension in complex, nuanced scenarios.

---

## Methodology

The research team employed a rigorous multi-stage approach to construct the Pix2Fact benchmark and evaluate existing models:

*   **Dataset Construction:** Built Pix2Fact, a visual question-answering benchmark comprising 1,000 high-resolution (4K+) images.
*   **Scenario Diversity:** The images cover 8 distinct daily-life scenarios to ensure broad applicability.
*   **Expert Annotation:** Utilized expert annotation by PhD holders and professional firms to ensure the quality and complexity of the data.
*   **Task Formulation:** Formulated tasks requiring three distinct cognitive skills:
    *   Detailed visual grounding
    *   Multi-hop reasoning
    *   External knowledge integration
*   **Comprehensive Evaluation:** Benchmarking was performed against 9 SOTA VLMs to establish a baseline comparison against human performance.

---

## Technical Details

The paper presents a Visual Question Answering (VQA) benchmark specifically engineered to test fine-grained visual grounding and multi-hop reasoning capabilities.

*   **Dataset Composition:**
    *   **Images:** 1,000 high-resolution (4K+) images.
    *   **Q&A Pairs:** 1,000 expert-curated Q&A pairs.
    *   **Categories:** 8 daily-life categories.

*   **Data Construction Pipeline:**
    *   **Stage 1:** Acquisition of high-quality images.
    *   **Stage 2:** Automated pre-screening to filter content.
    *   **Stage 3:** Expert manual review.
    *   **Annotation Intensity:** Averages 35–40 minutes per question to ensure depth and accuracy.

*   **Problem Formulation:**
    *   Requires models to bridge pixel-level details with external knowledge.
    *   Utilizes a **'Unique Information Bridge'** logic structure involving:
        1.  Visual perception
        2.  Cognitive processes
        3.  Knowledge retrieval

---

## Results

Experimental results show a significant performance gap between AI and human capabilities:

*   **Accuracy Comparison:**
    *   **Human Baseline:** 56%
    *   **SOTA VLMs:** 24.0%
    *   **Gap:** 32% absolute difference.
*   **Model Specifics:** Advanced models, specifically Gemini-3-Pro and GPT-5, failed to master the benchmark.
*   **Implication:** The results indicate that current multimodal agents struggle to integrate detailed visual grounding with knowledge-based reasoning, highlighting a critical area for future architectural improvement.

---

## Contributions

*   **Novel Benchmark:** Introduced Pix2Fact, the first benchmark to evaluate the synergy between expert-level perception and knowledge-intensive multi-hop reasoning.
*   **High Standard:** Provided a rigorously curated, high-resolution dataset establishing an expert-level standard for the community.
*   **Empirical Evidence:** Demonstrated empirically the inability of current SOTA VLMs to integrate visual details with complex knowledge.
*   **Future Development:** Established a critical testing ground for driving the development of next-generation multimodal agents.

---

**References:** 32 citations