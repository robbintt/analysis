---
title: 'Multi-Agent Deep Research: Training Multi-Agent Systems with M-GRPO'
arxiv_id: '2511.13288'
source_url: https://arxiv.org/abs/2511.13288
generated_at: '2026-02-03T07:08:12'
quality_score: 6
citation_count: 34
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Multi-Agent Deep Research: Training Multi-Agent Systems with M-GRPO

*Haoyang Hong; Jiajun Yin; Yuan Wang; Jingnan Liu; Zhe Chen; Ailing Yu; Ji Li; Zhiling Ye; Hansong Xiao; Yefei Chen; Hualei Zhou; Yun Yue; Minghui Yang; Chunxiao Guo; Junwei Liu; Peng Wei; Jinjie Gu*

> ### ðŸ“Š Quick Facts
> *   **Quality Score:** 6/10
> *   **References:** 34 Citations
> *   **Core Algorithm:** M-GRPO (Multi-agent Group Relative Policy Optimization)
> *   **Key Benchmarks:** GAIA, XBench-DeepSearch, WebWalkerQA
> *   **Performance Gain:** ~10% faster task speed than horizontal systems

---

## Executive Summary

This research addresses the complex optimization challenges inherent in training vertical multi-agent systems (MAS) composed of distinct Large Language Models (LLMs) assigned to specialized roles, such as a main planner and sub-agent executors. The core difficulty arises from structural heterogeneity, where agents operate at varying frequencies and generate asynchronous rollouts, complicating standard gradient-based learning. Furthermore, real-world distributed deployments often require agents to reside on physically separate servers, rendering standard end-to-end backpropagation infeasible. Solving these constraints is critical for advancing agent capabilities beyond the limitations of single unified models or systems relying on frozen sub-agents, which fail to leverage the full potential of role specialization.

The authors introduce **M-GRPO** (Multi-agent Group Relative Policy Optimization), a novel extension of Group Relative Policy Optimization designed specifically for heterogeneous agent environments. The approach relies on three core technical mechanisms:

1.  **Hierarchical Credit Assignment:** Computes group-relative advantages for both main and sub-agents to accurately distribute learning credit.
2.  **Trajectory Alignment:** A scheme that normalizes variable-length sub-agent invocations into fixed-size batches for consistent processing.
3.  **Decoupled Training Pipeline:** Facilitates scalable, distributed training by allowing agents to reside on separate servers that exchange only minimal statistics via a shared store, effectively eliminating the need for cross-server gradient flow.

M-GRPO demonstrates superior performance on real-world benchmarks including GAIA, XBench-DeepSearch, and WebWalkerQA, consistently outperforming baseline approaches such as single-agent GRPO and multi-agent GRPO utilizing frozen sub-agents. Quantitatively, the system achieved task execution speeds nearly 10% faster than horizontal multi-agent systems. Additionally, the results highlighted enhanced sample efficiency and training stability, validating that training distinct LLMs for specialized agent roles yields better outcomes than converging a single unified model on complex tasks.

This research significantly advances the field of multi-agent reinforcement learning by providing a viable infrastructure for optimizing heterogeneous, distributed agent systems. By validating the efficacy of specialized role training and solving the logistical challenge of decoupled optimization, M-GRPO establishes a pathway for deploying large-scale, collaborative AI architectures in practical, physically distributed environments.

---

## Key Findings

*   **Superior Performance:** M-GRPO consistently outperforms both single-agent GRPO and multi-agent GRPO (with frozen sub-agents) on real-world benchmarks, including GAIA, XBench-DeepSearch, and WebWalkerQA.
*   **Enhanced Efficiency:** The proposed method demonstrates improved stability and sample efficiency compared to existing training paradigms for multi-agent systems.
*   **Validation of Specialized Training:** Training distinct Large Language Models (LLMs) for specific agent roles (main vs. sub-agents) yields better results for specialized tasks than training a single unified model.
*   **Scalability:** The decoupled training pipeline successfully enables scalable training across distributed servers without the need for cross-server backpropagation.

---

## Methodology

The researchers propose **M-GRPO (Multi-agent Group Relative Policy Optimization)**, a hierarchical extension of Group Relative Policy Optimization designed specifically for vertical multi-agent systems consisting of a main agent (planner) and multiple sub-agents (tool executors).

The methodology addresses optimization challengesâ€”such as varying agent operation frequencies and distributed server deploymentsâ€”through three core mechanisms:

*   **Hierarchical Credit Assignment:** Computing group-relative advantages for both main and sub-agents.
*   **Trajectory Alignment:** Implementing a scheme to generate fixed-size batches from variable sub-agent invocations.
*   **Decoupled Training Pipeline:** Deploying agents on separate servers that exchange minimal statistics via a shared store.

---

## Technical Details

*   **System Architecture**
    *   Vertical Multi-Agent System (MAS) with a Main Agent ($M$) for planning/reasoning.
    *   Sub-Agents ($S_i$) for execution.
    *   Communication via a strict call-return protocol.

*   **Action Space**
    *   Hybrid of token-level textual emissions and discrete control actions.

*   **Training Framework**
    *   **M-GRPO:** Extends GRPO to train distinct LLMs for specific roles.
    *   Handles asynchronous trajectories via group-relative advantages.
    *   Supports distributed training natively.

*   **Reward Formulation**
    *   **Main Agent Reward ($R_M$):** Based on format and correctness ($\alpha_1 \cdot r_{format} + \alpha_2 \cdot r_{correct}$).
    *   **Sub-Agent Reward ($R_S$):** Includes format, Main Agent correctness, and expert evaluation ($\beta_1 \cdot r_{format} + \beta_2 \cdot r^{main}_{correct} + \beta_3 \cdot r_{expert}$).
    *   *Note:* Both return 0 for invalid formats.

---

## Contributions

*   **Optimization Framework for Heterogeneous Agents:** The paper introduces a novel solution to the problem of training distinct LLMs for different agents within a system, moving beyond the limitation of training a single unified model.
*   **M-GRPO Algorithm:** It presents a new algorithmic approach that handles the structural complexities of multi-agent systems (varying frequencies, rollout variations) through hierarchical credit assignment and trajectory alignment.
*   **Infrastructure for Distributed Training:** The work contributes a decoupled training architecture that allows for efficient optimization in real-world, distributed environments where agents are physically separated and gradient flow is restricted.

---

## Results

**Benchmarks**
*   GAIA
*   XBench-DeepSearch
*   WebWalkerQA

**Quantitative Metrics**
*   Task speed nearly 10% faster than horizontal systems.
*   Outperforms Single-agent GRPO and Multi-agent GRPO with frozen sub-agents.

**Qualitative Outcomes**
*   Improved stability.
*   Enhanced sample efficiency.
*   Scalable distributed training capabilities.
*   Better performance through role specialization.