---
title: On the Fundamental Limitations of Decentralized Learnable Reward Shaping in
  Cooperative Multi-Agent Reinforcement Learning
arxiv_id: '2511.00034'
source_url: https://arxiv.org/abs/2511.00034
generated_at: '2026-01-26T16:25:09'
quality_score: 9
citation_count: 22
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# On the Fundamental Limitations of Decentralized Learnable Reward Shaping in Cooperative Multi-Agent Reinforcement Learning

*Aditya Akella*

***

> ### **Quick Facts**
> *   **Research Focus:** Cooperative Multi-Agent Reinforcement Learning (MARL)
> *   **Proposed Method:** DMARL-RSA (Decentralized Reward Shaping)
> *   **Environment:** `simple_spread_v3` (Simple Spread)
> *   **Top Performer:** MAPPO (**+1.92** avg reward)
> *   **DMARL-RSA Performance:** **-24.20** avg reward
> *   **Performance Gap:** ~26.12 points
> *   **Quality Score:** 9/10

***

### Executive Summary

This research addresses the core challenge of achieving effective coordination in Cooperative Multi-Agent Reinforcement Learning (MARL) without relying on centralized training infrastructure. While centralized training with decentralized execution (CTDE) is the current paradigm, it implicitly assumes access to global information during the learning phase. The paper investigates whether "decentralized learnable reward shaping"—where agents learn their own intrinsic rewards to guide behavior—can theoretically substitute for centralized mechanisms.

The authors introduce **DMARL-RSA** (Decentralized MARL with Reward Shaping Architecture), a novel framework designed to test the limits of independent learning. The system operates under full decentralization, meaning no agent shares parameters, gradients, or a centralized critic. The methodology rigorously benchmarks this architecture against **MAPPO** (Centralized Training) and **IPPO** (Independent Learning) within the `simple_spread_v3` environment.

The study empirically demonstrates that decentralized learnable reward shaping fails to replicate the performance of centralized methods. A notable **"coordination paradox"** was observed: while decentralized methods achieved higher local landmark coverage rates, they suffered catastrophic global performance due to agent-agent collisions. This work reinforces the necessity of centralized coordination mechanisms to align individual learning with global objectives.

***

### Key Findings

*   **Significant Underperformance:** The proposed decentralized system (**DMARL-RSA**) significantly underperformed compared to centralized training (**MAPPO**), achieving an average reward of **-24.20** versus MAPPO's **1.92**.
*   **Parity with Independent Learning:** DMARL-RSA performed similarly to standard independent learning (**IPPO**), indicating that advanced reward shaping fails to overcome the inherent limitations of decentralized coordination.
*   **The Coordination Paradox:** A specific phenomenon was observed where decentralized methods achieved higher landmark coverage but suffered worse overall global performance due to collision penalties.
*   **Structural Barriers:** Key barriers identified include:
    *   Non-stationarity
    *   Exponential credit assignment complexity
    *   Misalignment between individual and global objectives

***

### Methodology

The researchers proposed and evaluated **DMARL-RSA**, a fully decentralized system architecture where agents learn independent reward shaping functions.

*   **Environment:** Cooperative navigation tasks within the `simple_spread_v3` environment (Multi-Agent Particle Environment).
*   **Baselines:** The system was assessed by benchmarking DMARL-RSA against:
    *   **MAPPO:** Centralized training with decentralized execution.
    *   **IPPO:** Independent learning (fully decentralized).
*   **Evaluation:** Efficacy was measured by comparing the average rewards and coordination capabilities of the proposed system against the two baselines.

***

### Technical Details

**System Architectures Evaluated**

| Architecture | Description |
| :--- | :--- |
| **MAPPO** | Centralized Training with Decentralized Execution using a shared global critic. |
| **DMARL-RSA** | Fully decentralized with individual 3-layer learnable reward shaping networks. |
| **IPPO** | Fully independent learning. |

**Hyperparameters & Configuration**

*   **Network Architecture:** 64-dim hidden layers with ReLU activation.
*   **Learning Rate:** 3e-4
*   **Discount Factor:** 0.99
*   **Environment Details:** Simple Spread (MPE) involving 3 agents and 3 landmarks.
*   **Reward Structure:**
    *   Sparse distance rewards.
    *   **-1.0 penalty** for agent-agent collisions.

***

### Results

The experimental data highlights a stark contrast between centralized and decentralized approaches.

*   **MAPPO Performance:** Achieved an average reward of **1.92**.
*   **DMARL-RSA & IPPO Performance:** Averaged **-24.20**.
*   **Performance Gap:** An approximate **26.12-point** gap indicates severe limitations in decentralized reward shaping.
*   **Causal Factors:** The failure in decentralized methods is attributed to:
    *   Non-stationarity violations.
    *   Credit assignment complexity.
    *   Reward misalignment.
*   **Local vs. Global:** Results confirm that decentralized methods achieved higher local coverage but failed in global performance due to the collision penalty.
*   **Statistical Significance:** Results were averaged over 5,000 episodes across 3 seeds.

***

### Contributions

The study establishes several important contributions to the field of Multi-Agent Reinforcement Learning:

1.  **Empirical Limits:** Establishes empirical limits for decentralized reward learning, providing quantitative evidence of its insufficiency for complex coordination.
2.  **Novel Testbed:** Introduces DMARL-RSA as a novel testbed for understanding individual reward shaping dynamics.
3.  **Theoretical Framework:** Provides a theoretical breakdown of coordination failure, specifically defining and explaining the **"coordination paradox."**
4.  **Paradigm Validation:** The findings reinforce the necessity of centralized coordination mechanisms to align individual learning with global objectives, steering future research toward centralized or hybrid solutions.

***

**Quality Score:** 9/10  
**References:** 22 citations