---
title: Scaling Agentic Reinforcement Learning for Tool-Integrated Reasoning in VLMs
arxiv_id: '2511.19773'
source_url: https://arxiv.org/abs/2511.19773
generated_at: '2026-02-03T06:32:15'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Scaling Agentic Reinforcement Learning for Tool-Integrated Reasoning in VLMs

*Meng Lu; Ran Xu; Yi Fang; Wenxuan Zhang; Yue Yu; Gaurav Srivastava; Yuchen Zhuang; Mohamed Elhoseiny; Charles Fleming; Carl Yang; Zhengzhong Tu; Yang Xie; Guanghua Xiao; Hanrui Wang; Di Jin; Wenqi Shi; Xuan Wang*

---

### ðŸ“Š Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Model** | VISTA-R1-8B |
| **Performance Gain** | +9.51% to +18.72% over SOTA baselines |
| **Benchmarks** | 11 public reasoning-intensive VQA benchmarks |
| **Training Tasks** | 7 reasoning tasks across 13 datasets |
| **Tools Available** | 26 distinct tools (perception, symbolic, interpretation) |
| **Quality Score** | 9/10 |

---

## Executive Summary

> Current Vision-Language Models (VLMs) exhibit strong capabilities in text-only reasoning but struggle to translate this proficiency into practical, tool-integrated visual applications. The primary limitation lies in their inability to effectively manage the selection, invocation, and coordination of external tools required for complex visual tasks. This deficiency prevents VLMs from functioning as autonomous agents capable of "thinking with images," resulting in a significant gap between theoretical reasoning abilities and the functional, multi-step visual interactions necessary for real-world problem-solving.
>
> To address this, the authors introduce **VISTA-Gym**, a unified training infrastructure designed to standardize and scale visual agentic reinforcement learning (RL). The environment integrates 7 real-world reasoning tasks across 13 datasets, offering a standardized API for 26 distinct toolsâ€”ranging from perception to symbolic manipulationâ€”and featuring executable interaction loops with verifiable feedback. Within this environment, the team developed **VISTA-R1-8B**, an 8-billion parameter model trained end-to-end using multi-turn trajectory sampling. The core technical advancement is a "thinking with images" paradigm, where the model utilizes end-to-end RL to learn the optimal interleaving of intrinsic reasoning steps with external tool execution, rather than relying on static augmentation or reasoning alone.
>
> VISTA-R1-8B establishes a new performance standard for 8B parameter models, outperforming state-of-the-art open-source baselines by **9.51% to 18.72%** across 11 reasoning-intensive VQA benchmarks. These results cover 5 in-domain and 6 out-of-domain tasks, demonstrating robust generalization. Specific comparative metrics illustrate the model's efficacy: VISTA-R1-8B achieves 80% accuracy on MapQA, substantially surpassing GPT-5â€™s 35%, and improves performance on UniGeo to 28% compared to InternVL3-8B's 12%. Ablation studies confirm that the proposed interleaved approach is critical, revealing that direct tool augmentation alone degrades accuracy, while reasoning-only models yield limited gains.
>
> This research provides compelling evidence that end-to-end reinforcement learning within a unified environment is a viable and scalable method for teaching VLMs to integrate tool-use directly into their reasoning loops. By open-sourcing VISTA-Gym, the authors address a critical infrastructure shortage in the field, providing a reproducible platform for future research into visual agents.

---

## Key Findings

*   **Performance Leadership:** The `VISTA-R1-8B` model outperforms state-of-the-art baselines by **9.51% to 18.72%** across 11 public reasoning-intensive VQA benchmarks.
*   **The Tool-Use Gap:** Current Vision-Language Models struggle with practical tool selection, invocation, and coordination despite possessing strong text-only reasoning capabilities.
*   **Efficacy of End-to-End RL:** End-to-end reinforcement learning within a unified environment is highly effective for teaching VLMs to interleave tool-use with agentic reasoning.
*   **Scalability:** Multi-step visual interactions (thinking with images) can be successfully scaled and improved through structured training environments.

---

## Methodology

The research methodology focused on creating a standardized environment for training and optimizing agent behavior:

1.  **Environment Construction (VISTA-Gym):** The team constructed VISTA-Gym, a scalable training environment unifying **7 real-world multimodal reasoning tasks** sourced from **13 datasets**.
2.  **Standardization:** This environment provides a standardized interface for visual tools, executable interaction loops, verifiable feedback signals, and efficient trajectory logging.
3.  **Model Training (VISTA-R1):** The model was trained using visual agentic reinforcement learning.
4.  **Data Generation:** The process utilized multi-turn trajectory sampling for interaction data generation.
5.  **Optimization:** End-to-end reinforcement learning was employed to optimize the interleaving of tool usage with reasoning steps.

---

## Contributions

*   **VISTA-Gym:** A novel, unified training infrastructure designed to incentivize tool-integrated visual reasoning. It addresses the critical lack of standardized environments for visual agentic RL.
*   **VISTA-R1:** A model architecture that successfully integrates tool-use capabilities directly into the reasoning loop of VLMs, overcoming current limitations in tool coordination.
*   **Benchmarking and Validation:** Comprehensive evidence establishing VISTA-Gym as an effective training ground for complex visual reasoning, setting a new performance standard for 8B parameter models.

---

## Technical Details

### Infrastructure
*   **Name:** VISTA-Gym
*   **Type:** Unified, extensible environment for end-to-end RL of VLMs.
*   **Scope:** Supports 7 reasoning tasks across 13 public datasets.
*   **API:** Exposes a standardized API with **26 tools** for:
    *   Perception
    *   Symbolic manipulation
    *   Interpretation
*   **Performance:** Utilizes multithreading for scalable trajectory collection.

### Model Architecture
*   **Agent:** VISTA-R1-8B (8 billion parameters).
*   **Training Method:** End-to-end RL within VISTA-Gym.
*   **Paradigm:** "Thinking with images" â€“ interleaving intrinsic reasoning with tool execution.
*   **Optimization:** Utilizes tool-selection prior knowledge to guide the process.

---

## Results

The VISTA-R1-8B model demonstrated superior performance across both in-domain and out-of-domain tasks:

*   **Overall Performance:** Surpassed state-of-the-art open-source baselines by **9.51% to 18.72%** across 11 benchmarks (5 in-domain, 6 out-of-domain).
*   **Ablation Studies:**
    *   **Direct Tool Augmentation:** Found to degrade accuracy.
    *   **Reasoning-Only:** Yielded limited gains.
    *   **Combined Approach (w/ T&R):** Significantly improved performance.
*   **Benchmark Specifics:**
    *   **MapQA:** VISTA-R1-8B reached **80%** vs. GPT-5 at 35%.
    *   **ChartQA:** VISTA-R1-8B reached **32%** vs. GPT-5 at 24%.
    *   **UniGeo:** VISTA-R1-8B reached **28%** vs. InternVL3-8B at 12%.
    *   **MapQA (vs InternVL3):** VISTA-R1-8B reached **36%** vs. InternVL3-8B at 20%.

---

**References:** 40 citations
**Quality Score:** 9/10