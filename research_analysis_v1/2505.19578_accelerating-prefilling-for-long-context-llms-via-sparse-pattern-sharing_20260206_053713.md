---
title: Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing
arxiv_id: '2505.19578'
source_url: https://arxiv.org/abs/2505.19578
generated_at: '2026-02-06T05:37:13'
quality_score: 9
citation_count: 22
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing

*Dan Peng; Zhihui Fu; Zewen Ye; Zhuoran Song; Jun Wang*

---

> **ðŸ“Š Quick Facts**
>
> *   **Quality Score:** 9/10
> *   **Models Tested:** Llama-3-8B, Qwen2.5-7B
> *   **Benchmark:** InfiniteBench
> *   **Method Name:** SharePrefill
> *   **Key Insight:** Inter-head attention similarity allows for sparse pattern sharing
> *   **Primary Benefit:** Reduces quadratic complexity during prefilling without accuracy loss

---

## ðŸ’¡ Executive Summary

The prefilling phase of long-context Large Language Models (LLMs) presents a significant computational bottleneck due to the quadratic complexity inherent in standard attention mechanisms. While existing sparse attention methods attempt to mitigate this, they frequently fail because they rely on either rigid, predefined patterns or inaccurate estimations that do not capture true attention dynamics. This limitation is critical because it restricts the feasibility of deploying long-context models in latency-sensitive applications. The paper addresses the fundamental challenge of accelerating the prefilling stage without sacrificing the model's ability to accurately process and reason over extended contexts.

The authors introduce "SharePrefill," a novel optimization framework that leverages a specific theoretical insight: attention patterns demonstrate strong and consistent inter-head similarity across diverse inputs. Technically, the method designates a subset of "Anchor Heads" to compute full dense attention, while the remaining heads utilize precise sparse patterns derived from these anchors, thereby avoiding redundant independent calculations. The approach specifically critiques and rejects pooling-based estimation due to two mathematical failure modesâ€”the disregard for token alignment and the smoothing of high/low attention valuesâ€”opting instead to rely on the stability of similarity relationships (quantified via Jaccard Similarity) to approximate full attention behavior efficiently.

Evaluations conducted on Llama-3-8B and Qwen2.5-7B models using the InfiniteBench benchmark demonstrate that SharePrefill achieves Pareto improvements over state-of-the-art baselines, including FlashAttn, MInference, and FlexPrefill. For the Llama-3-8B model, the method achieved an InfiniteBench score of approximately 40 with a latency of ~15 seconds, significantly outperforming baselines that required 20â€“35 seconds for similar or lower scores. Likewise, on the Qwen2.5-7B model, SharePrefill secured a score of ~33 at ~15 seconds latency, whereas FlashAtten required ~30 seconds for comparable performance and other baselines suffered from degraded accuracy.

This research establishes a new state-of-the-art balance in the trade-off between inference speed and model accuracy, demonstrating that high efficiency does not necessitate a sacrifice in precision. By addressing the quadratic complexity of the prefilling phase through pattern sharing rather than lossy approximation, the work provides a viable path for the broader adoption of long-context LLMs. The findings influence the field by validating that redundancy in attention heads can be exploited for optimization, potentially reshaping how future architectures approach the balance between computational cost and reasoning capability.

---

## Key Findings

*   **Limitations of Current Methods:** Existing sparse attention methods often fail to capture true attention dynamics because they rely on predefined patterns or inaccurate estimations.
*   **Consistency in Patterns:** Attention patterns demonstrate strong inter-head similarity, and this similarity remains consistent across diverse inputs.
*   **Selective Computation:** Full attention computation is only required for a small subset of heads, while the remaining heads can utilize shared, accurate patterns.
*   **Performance Superiority:** The proposed approach achieves superior or comparable speedup relative to state-of-the-art methods while delivering the best overall accuracy.

---

## Methodology

The authors propose a sparse attention mechanism based on **Sparse Pattern Sharing** to accelerate the prefilling phase of long-context inference. The methodology leverages observed inter-head similarity to avoid computing full attention for every head by sharing computed, accurate patterns across multiple heads.

Computationally expensive full attention calculations are restricted to a small subset of heads, and these shared patterns are used to approximate the dynamic behavior of the full attention mechanism without the quadratic computational cost.

---

## Technical Details

The approach, named **SharePrefill**, utilizes sparse pattern sharing via head similarity.

*   **Core Mechanism:** Designates a subset of **'Anchor Heads'** to compute full dense attention. The remaining heads use the precise sparse patterns derived from these anchors.
*   **Theoretical Basis:** Relies on the consistency of similarity relationships between heads across different inputs.
*   **Critique of Pooling:** The technique critiques pooling-based estimation for two mathematical failure modes:
    1.  **Disregard for Token Alignment:** Pooled vectors ignore positional alignment, making pooled dot products unequal to average attention scores.
    2.  **Smoothing of Values:** Pooling dilutes extreme attention scores (both high and low).
*   **Similarity Metric:** Employs **Jaccard Similarity** to quantify pattern overlap, identifying significant redundancy where inter-head similarity scores exceed 0.5.

---

## Results

Experiments conducted on Llama-3-8B and Qwen2.5-7B models using the InfiniteBench benchmark demonstrated a Pareto improvement over baselines (FlashAttn, MInference, FlexPrefill).

*   **Llama-3-8B Performance:**
    *   Achieved an InfiniteBench score of **~40**.
    *   Latency of **~15s**.
    *   *Comparison:* Outperformed baselines requiring ~20sâ€“35s for similar or lower scores.
*   **Qwen2.5-7B Performance:**
    *   Achieved a score of **~33**.
    *   Latency of **~15s**.
    *   *Comparison:* FlashAttn required ~30s for comparable performance; other baselines showed degraded accuracy.
*   **Qualitative Analysis:** Confirmed that specific heads (e.g., L18, H4) exhibited consistent 'staircase-like' patterns across tasks like En.Dia and Code.Debug.

---

## Contributions

*   **Theoretical Insight:** A novel observation regarding the inherent and consistent inter-head similarity of attention patterns.
*   **Optimization Framework:** A highly accurate sparse attention mechanism that addresses the quadratic complexity of the prefilling phase in long-context LLMs.
*   **Performance Benchmarking:** Establishment of a new state-of-the-art balance in the trade-off between inference speed and model accuracy, demonstrating that high efficiency does not necessitate a sacrifice in precision.

---

**References:** 22 citations