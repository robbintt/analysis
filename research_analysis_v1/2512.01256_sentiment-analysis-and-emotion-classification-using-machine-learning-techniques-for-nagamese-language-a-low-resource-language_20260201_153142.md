# Sentiment Analysis and Emotion Classification using Machine Learning Techniques for Nagamese Language -- A Low-resource Language

*Ekha Morang; Surhoni A. Ngullie; Sashienla Longkumer; Teisovi Angami*

---

> ### üìã Quick Facts
>
> *   **Target Language:** Nagamese (Naga Pidgin)
> *   **Language Family:** Assamese-lexified creole / Indo-Aryan
> *   **Corpus Size:** ~25,000 tokens
> *   **Lexicon Size:** 1,195 words
> *   **Best Performing Model:** SVM (81.8% Accuracy)
> *   **Resource Availability:** Zero prior work (First of its kind)
> *   **Quality Score:** 9/10

---

### üìù Executive Summary

This paper addresses the critical lack of Natural Language Processing (NLP) resources for Nagamese, an Assamese-lexified creole spoken in Northeast India. As a low-resource language with a Subject-Object-Verb (SOV) structure, Nagamese has historically had zero dedicated research in sentiment analysis or emotion classification. This absence presents a significant technical barrier to automated text understanding, preventing effective processing of digital communication for the language's speakers and leaving a void in the broader computational study of under-represented creole languages.

The study‚Äôs primary innovation is the manual construction and curation of the first sentiment polarity lexicon for Nagamese, derived from a custom corpus of approximately 25,000 tokens sourced from online news and religious texts. This lexicon comprises 1,195 unique words categorized as Positive (162), Negative (162), or Neutral (871). Technically, the methodology employed specific feature engineering techniques, combining **TF-IDF vectorization** with **lexicon-based sentiment scores** to train classical supervised machine learning models‚Äîspecifically Naive Bayes (NB) and Support Vector Machines (SVM)‚Äîfor polarity detection and emotion classification.

The research yielded strong quantitative results, demonstrating that SVM outperformed Naive Bayes in both tasks. The **SVM model achieved an accuracy of 81.8%** with an **F1-score of 0.81**, while the **Naive Bayes model secured an accuracy of 76.5%** and an **F1-score of 0.76**. While the lexicon analysis revealed that approximately 73% of the dataset is neutral, the models successfully leveraged the balanced positive and negative subsets to establish a functional baseline, proving that sufficient signal exists within the limited corpora for effective classification.

This work significantly influences the field of NLP by validating that classical machine learning techniques can effectively bridge the digital divide for low-resource languages without requiring massive datasets or deep learning architectures. By providing the first annotated dataset and sentiment lexicon for Nagamese, the authors deliver a foundational asset that enables future research in opinion mining and digital humanities for the region. The study serves as a blueprint for processing other indigenous or creole languages, demonstrating that resource-efficient methods can successfully extend computational linguistics capabilities to under-represented languages.

---

## Key Findings

*   **Research Gap Identification:** The study successfully identified and addressed a complete void in prior research, with zero existing works on sentiment analysis for the Nagamese language.
*   **Resource Creation:** Created a comprehensive sentiment polarity lexicon consisting of **1,195 words**, providing a foundational linguistic resource for the language.
*   **Model Capability:** Demonstrated the capability to detect sentiment polarity and identify basic emotions using classical approaches tailored for low-resource environments.
*   **Algorithm Effectiveness:** Showed that classical machine learning techniques‚Äîspecifically Naive Bayes (NB) and Support Vector Machines (SVM)‚Äîare highly effective for this low-resource language.
*   **Feature Engineering Success:** Validated that combining **TF-IDF vectorization** with lexicon-based sentiment scores provides sufficient signal for accurate classification.

---

## Research Contributions

*   **First of its Kind:** Represents the first known attempt at sentiment analysis and emotion classification for the Nagamese language (Naga Pidgin).
*   **NLP Extension:** Successfully extends Natural Language Processing capabilities to a low-resource, Assamese-lexified creole language.
*   **Resource Provision:** Contributes a valuable new linguistic resource to the academic community in the form of the 1,195-word sentiment lexicon and annotated dataset.
*   **Methodological Blueprint:** Provides a reproducible methodology for processing other indigenous or creole languages using resource-efficient classical methods.

---

## Technical Details

| Aspect | Details |
| :--- | :--- |
| **Target Language** | Nagamese (Naga Pidgin) |
| **Language Structure** | SOV (Subject-Object-Verb) |
| **Phonology** | 28 phonemes with specific syllabic patterns |
| **Corpus Size** | ~25,000 tokens (1,195 unique words) |
| **Data Sources** | Online news, religious texts |
| **Lexicon Composition** | 162 Positive, 162 Negative, 871 Neutral |
| **Algorithms Used** | Naive Bayes (NB), Support Vector Machine (SVM) |
| **Features** | TF-IDF Vectorization, Lexicon-based sentiment scores |

---

## Methodology

The research followed a structured pipeline designed specifically for low-resource environments:

1.  **Data Collection:** A custom corpus was constructed from online news and religious texts, totaling approximately 25,000 tokens.
2.  **Lexicon Construction:** Researchers manually curated a domain-specific sentiment lexicon of 1,195 unique words, labeling them as Positive, Negative, or Neutral.
3.  **Feature Engineering:** Features were engineered by extracting data from the custom lexicon and combining it with linguistic features (specifically TF-IDF).
4.  **Model Training:** Supervised machine learning approaches were employed, training models on the engineered features using Naive Bayes and Support Vector Machines (SVM) algorithms.
5.  **Classification:** The trained models were tasked with sentiment polarity detection and emotion classification.

---

## Results

*   **Lexicon Coverage:** The sentiment lexicon covers the entire set of 1,195 unique words, with approximately **73%** labeled as Neutral.
*   **Model Performance - SVM:**
    *   **Accuracy:** 81.8%
    *   **F1-Score:** 0.81
*   **Model Performance - Naive Bayes:**
    *   **Accuracy:** 76.5%
    *   **F1-Score:** 0.76
*   **Qualitative Outcomes:** The study successfully established the first sentiment polarity lexicon and annotated dataset for the language.
*   **Comparative Analysis:** Classical ML techniques proved effective, with SVM demonstrating superior performance over Naive Bayes for this specific task.

---

**Paper Rating:** 9/10
**References:** 4 Citations