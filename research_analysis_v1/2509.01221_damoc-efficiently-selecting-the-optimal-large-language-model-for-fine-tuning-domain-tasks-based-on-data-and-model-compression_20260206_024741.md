---
title: 'DaMoC: Efficiently Selecting the Optimal Large Language Model for Fine-tuning
  Domain Tasks Based on Data and Model Compression'
arxiv_id: '2509.01221'
source_url: https://arxiv.org/abs/2509.01221
generated_at: '2026-02-06T02:47:41'
quality_score: 9
citation_count: 26
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# DaMoC: Efficiently Selecting the Optimal Large Language Model for Fine-tuning Domain Tasks Based on Data and Model Compression

*Wei Huang; Huang Wei; Yinggui Wang*

---

> ### ðŸ“Š Quick Facts
>
> *   **Efficiency Gain:** ~20x reduction in training time
> *   **Validation Scope:** 4 distinct datasets (Medical, Financial, General Q&A, Reading Comprehension)
> *   **Core Strategy:** Dual-level compression (Data Level & Model Level)
> *   **Key Innovation:** Sparse Merging paradigm to preserve model capabilities
> *   **Reliability Score:** 9/10

---

## Executive Summary

Selecting the optimal Large Language Model (LLM) for domain-specific fine-tuning presents a significant computational bottleneck. Organizations typically rely on either fully fine-tuning numerous candidate modelsâ€”which is prohibitively expensive and time-consumingâ€”or using few-shot prompting as a performance proxy. However, this research highlights that few-shot prompting is frequently an unreliable predictor of fine-tuning success, leading to suboptimal model selection. The industry lacks an efficient method to evaluate a model's true potential for specific tasks (such as medical or financial reasoning) without incurring the full resource costs of training.

The authors introduce **DaMoC (Data and Model Compression)**, a three-stage framework designed to identify the best LLM for a task using a fraction of the standard computational resources. DaMoC employs a dual-level compression strategy: at the **Data Level**, it utilizes a taxonomy of filtering methods (distribution-aware, quality-aware, and hybrid), combined with token compression and iterative rewriting, to increase the density of key information in training text. At the **Model Level**, the framework assesses layer importance using cosine similarity scores to perform structural pruning. Crucially, it implements a "**Sparse Merging**" paradigm that merges the task vector of a pruned layer into the preceding layer. This technique mitigates distribution shifts and preserves the original model's capabilities, allowing for rapid evaluation of the model's post-fine-tuning performance.

Empirical validation across four distinct datasetsâ€”medical Q&A, financial Q&A, general Q&A, and reading comprehensionâ€”demonstrates that DaMoC achieves approximately a **20-fold reduction in training time** compared to standard full fine-tuning approaches. The framework effectively preserved model performance despite significant layer removal. Furthermore, the study provided concrete evidence of the limitations of few-shot prompting; notably, on the BillSum dataset, the `Llama3.1-8B` model ranked 8th in zero-shot performance but surged to 2nd after full fine-tuning. DaMoC successfully predicted this potential while maintaining high information density through data-level optimizations.

This research significantly advances the field of efficient machine learning by decoupling the model selection process from exhaustive computational expenditure. By categorizing data filtering methods and validating that compressed training maintains ranking stability, DaMoC provides a practical, cost-effective solution for enterprises and researchers deploying domain-specific LLMs. The framework enables rapid, accurate benchmarking of candidate models, reducing the barrier to entry for high-performance domain adaptation and ensuring that computational resources are invested only in the most promising architectures.

---

## Key Findings

*   **Optimal Model Identification:** The proposed DaMoC framework effectively identifies the optimal Large Language Model (LLM) for fine-tuning domain-specific tasks.
*   **Computational Efficiency:** The methodology achieves significant computational efficiency, reducing training time by approximately **20-fold** compared to standard approaches.
*   **Versatile Validation:** Extensive validation on four distinct datasetsâ€”medical Q&A, financial Q&A, general Q&A, and reading comprehensionâ€”confirms the framework's versatility.
*   **Capability Preservation:** The sparse merging paradigm successfully preserves the original model's capabilities despite the removal of less important layers.
*   **Information Density:** Data-level optimizations, such as token compression and iterative rewriting, successfully enhance the density of key information in the training text.

---

## Methodology

The DaMoC framework utilizes a dual-level compression strategy to optimize model selection.

### Data Level
*   **Filtering Taxonomy:** Establishes a methodology classification (distribution-aware, quality-aware, and hybrid).
*   **Token Compression:** Employs token compression to enhance key token density.
*   **Iterative Rewriting:** Uses iterative rewriting mechanisms to optimize text expression.

### Model Level
*   **Layer Assessment:** Assesses layer importance using similarity scores to perform layer pruning.
*   **Sparse Merging:** Implements a sparse merging paradigm to preserve original model capabilities during the pruning process.

---

## Technical Details

DaMoC is a three-stage framework for identifying optimal LLMs for domain-specific fine-tuning without full training costs:

**1. Data Filtering**
Investigates 12 methods (Distribution-Aware, Quality-Aware, Hybrid) to reduce data volume while maintaining ranking stability.

**2. Token Compression**
*   Compresses question and answer prompts using a **Budget Controller** and perplexity calculations (`Baichuan2-7B-Chat-4bits`).
*   Goal: Minimize embedding distance (BERTScore).
*   Includes an iterative rewriting mechanism (`Baichuan2-13B-Chat`) and a negative feedback loop if similarity is low.

**3. Model Pruning**
*   Utilizes layer-wise pruning based on **Layer Importance Scoring** (cosine similarity of input/output activations).
*   Implements **Sparse Merging** to merge the task vector of a pruned layer into the preceding layer to mitigate distribution shifts.

---

## Contributions

1.  **Framework Introduction:** Introduced the DaMoC Framework, a novel Data and Model Compression Framework designed to quickly identify the optimal LLM for fine-tuning.
2.  **Systematic Categorization:** Established a systematic categorization of LLM data filtering methods, distinguishing between distribution-aware, quality-aware, and hybrid techniques.
3.  **Resource Reduction:** Demonstrated that combining data-level compression with model-level pruning drastically reduces training resource requirements while maintaining model effectiveness.
4.  **Empirical Evidence:** Provided empirical evidence across diverse domains (medical, financial, general) to validate the efficacy of the proposed selection and compression methods.

---

## Results

*   **Unreliability of Few-Shot Prompting:** Few-shot prompting is an unreliable proxy for fine-tuning performance. For example, `Llama3.1-8B` ranked 8th in Zero-shot but rose to 2nd after full fine-tuning on the BillSum dataset.
*   **Efficiency Metrics:** DaMoC achieves approximately a 20-fold reduction in training time compared to full fine-tuning.
*   **Validation:** Validation was conducted on Medical, Financial, General Q&A, and Reading Comprehension datasets.
*   **Stability:** The framework preserves model capabilities via Sparse Merging and enhances information density through data-level optimizations to ensure stable model selection.

---

**References:** 26 citations
**Quality Score:** 9/10