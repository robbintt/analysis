---
title: 'CodeChemist: Functional Knowledge Transfer for Low-Resource Code Generation
  via Test-Time Scaling'
arxiv_id: '2510.00501'
source_url: https://arxiv.org/abs/2510.00501
generated_at: '2026-01-27T16:28:30'
quality_score: 8
citation_count: 10
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# CodeChemist: Functional Knowledge Transfer for Low-Resource Code Generation via Test-Time Scaling

*Nanyang Technological University, Xi‚Äôan Jiaotong University, Beihang University; Aishan Liu, Tianlin Li, Xiaoyu Zhang, Kaixin Wang, Xianglong Liu*

---

### üìã Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Quality Score** | 8/10 |
| **Languages Tested** | 14 Low-Resource Programming Languages |
| **Retraining Required** | None (Zero-shot) |
| **Avg. Performance Boost (GPT-3.5)** | +19.46% |
| **Avg. Performance Boost (Llama-3)** | +14.67% |
| **Reference Count** | 10 Citations |

---

## üìù Executive Summary

Large Language Models (LLMs) demonstrate a significant performance gap when generating code for low-resource programming languages (LRPLs) compared to high-resource languages like Python. This disparity stems from the severe scarcity of training data for niche languages, which hinders the models' ability to produce syntactically and logically correct code. Addressing this issue through conventional methods‚Äîsuch as model retraining or fine-tuning‚Äîis computationally expensive and often impractical due to the lack of high-quality datasets. This limitation restricts the utility of LLMs in specialized domains and creates a barrier for developers working with emerging or legacy languages.

**CodeChemist** introduces a novel test-time scaling framework that enables functional knowledge transfer to LRPLs without modifying model parameters. The approach technically operates through a three-stage pipeline that decouples functional reasoning from syntax generation. First, the model synthesizes and executes code in a high-resource language (e.g., Python) to self-generate a comprehensive suite of input-output test cases that capture the task's underlying logic. Second, to address the syntax volatility of LRPLs, the framework employs multi-temperature hedged sampling to produce a diverse array of candidate code snippets in the target language. Finally, an execution-based selection mechanism evaluates these candidates against the self-generated functional test cases, selecting the solution with the highest pass rate to ensure correctness.

Evaluations across **14 low-resource programming languages** demonstrate that CodeChemist significantly outperforms existing test-time scaling baselines and standard generation methods. The framework delivers substantial performance gains, boosting the Pass@1 accuracy of **GPT-3.5-Turbo by an average of 19.46%** and improving **Llama-3-8B by 14.67%**. These results highlight CodeChemist's ability to effectively bridge the performance gap, providing consistent accuracy improvements across different model architectures purely at inference time.

The significance of CodeChemist lies in validating that high-quality code generation for low-resource languages can be achieved through test-time computation rather than reliance on extensive, language-specific training data. By successfully decoupling functional logic (verified via high-resource languages) from language-specific syntax, this framework offers a cost-effective, scalable solution for deploying coding assistants. This approach lowers the barrier to entry for supporting diverse programming languages in industrial and academic settings, transforming the requirement from massive data acquisition to intelligent inference-time resource allocation.

---

## üîë Key Findings

*   **Superior Performance:** CodeChemist outperforms existing test-time scaling approaches in generating code for low-resource programming languages.
*   **Performance Boost:** The framework significantly improves code generation performance for languages that suffer from limited training data.
*   **No Retraining Required:** These performance improvements are achieved without requiring any model retraining, offering an efficient inference-time solution.
*   **Effective Knowledge Transfer:** The study demonstrates the viability of transferring functional knowledge from high-resource languages to low-resource languages.

---

## ‚öôÔ∏è Methodology

The CodeChemist framework operates as a **two-step test-time scaling process**, designed to bridge the gap between high-resource logic and low-resource syntax.

1.  **Test Case Generation:**
    *   The framework initially generates and executes code in **high-resource programming languages** (which have abundant training data) to create test cases.
    *   These cases encapsulate the underlying functional requirements of the task, ensuring the logic is sound before syntax generation begins.

2.  **Hedged Sampling and Selection:**
    *   For the target low-resource language, CodeChemist employs **multi-temperature hedged sampling** to generate diverse code snippets.
    *   It then evaluates these snippets against the functional test cases created in step one, selecting the final output based on the highest pass rate.

---

## üî© Technical Details

CodeChemist is a test-time scaling framework for low-resource programming languages that operates without model retraining. It features a robust **three-stage pipeline**:

1.  **Test Case Generation:**
    *   Uses high-resource languages to create valid Input/Output (I/O) pairs.
    *   Utilizes *code synthesis*, *input synthesis*, *execution validation*, and *majority voting* to ensure test case quality.

2.  **Hedged Sampling:**
    *   Implemented in low-resource languages.
    *   Uses *multi-temperature hedging* to generate a diverse set of candidate solutions to handle syntax uncertainty.

3.  **Execution-based Selection:**
    *   Candidates are executed against the synthesized test cases.
    *   Selects the code snippet with the highest pass rate as the final output.

**Evaluation Metrics:**
*   **Pass Rate:** The primary selection criteria for the final code output.
*   **Consensus/Majority Vote Confidence:** Used for filtering test cases to ensure reliability.

---

## üöÄ Contributions

*   **Novel Framework:** Introduced CodeChemist, a new and efficient framework designed to bridge the performance gap between high-resource and low-resource programming languages in CodeLLMs.
*   **Functional Knowledge Transfer:** Proposed a unique mechanism that utilizes self-generated test cases from high-resource languages to verify and guide code generation in low-resource languages.
*   **Efficiency Optimization:** Demonstrated that effective specialized code generation can be achieved via test-time scaling techniques (hedged sampling and execution-based verification), removing the computational cost and data requirements associated with model retraining or fine-tuning.

---

## üìä Results

Reported findings indicate superiority over existing test-time scaling approaches with **zero retraining**, demonstrating effective knowledge transfer from high-resource to low-resource languages.

*   **GPT-3.5-Turbo:** Pass@1 accuracy improved by an average of **19.46%**.
*   **Llama-3-8B:** Pass@1 accuracy improved by an average of **14.67%**.

While specific quantitative metrics for all languages are extensive, the evaluation logic defines success through high Pass Rates and strong Consensus/Majority Vote Confidence during test case filtering.