---
title: LLM serving engines experience long cumulative wait times,
arxiv_id: '2502.13965'
source_url: https://arxiv.org/abs/2502.13965
generated_at: '2026-01-27T21:22:12'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# LLM serving engines experience long cumulative wait times

*Michael Luo, General Programs, Chi Wang, Colin Cai, Xiaoxiang Shi, Tianjun Zhang, An Efficient, Yichuan Wang, Justin Wong, Serving Engine*

---

## üìä Quick Facts

| Metric | Details |
| :--- | :--- |
| **Proposed System** | Autellix |
| **Performance** | 4‚Äì15√ó improvement in program throughput |
| ** primary Bottleneck** | Head-of-Line (HOL) Blocking (80.4% of latency) |
| **Core Innovation** | Dependency-aware scheduling & program-level optimization |
| **Test Environment** | NVIDIA A100 80GB, LLaMA-3.1-8B |
| **Workloads Analyzed** | Chatbots, ReAct, Map-Reduce, MCTS |
| **Quality Score** | **9/10** |

---

> ## üìù Executive Summary
>
> Current Large Language Model (LLM) serving engines are facing critical inefficiencies as applications shift from simple chatbots to dynamic, agentic programs. These sophisticated workflows require numerous interdependent LLM calls, and existing systems fail to account for the complex dependencies within them. This failure results in severe **Head-of-Line (HOL) blocking**, where long-running requests stall shorter tasks, causing excessive cumulative wait times. As wait times become the dominant latency factor, current infrastructure renders advanced agentic workflows inefficient.
>
> To address this, the authors introduce **Autellix**, a serving system that elevates agentic programs to "first-class citizens." Autellix models these programs as Dynamic Directed Acyclic Graphs (DAGs) and utilizes a runtime process table to track call relationships. Key features include an **interception mechanism for context enrichment** and two distinct scheduling algorithms (for single-threaded and distributed environments) that leverage preemption and prioritization based on call history.
>
> Benchmark tests on an NVIDIA A100 80GB GPU using LLaMA-3.1-8B demonstrated that Autellix achieves a **4‚Äì15√ó improvement in program throughput** compared to state-of-the-art baselines like vLLM. The system successfully handled 10 additional concurrent LLM calls in steady-state tests and significantly reduced wait-to-execution ratios for short tasks. This research establishes a new performance standard, proving that program-level optimization delivers order-of-magnitude gains without sacrificing latency.

---

## üîë Key Findings

*   **Shift in Paradigm:** LLM applications are evolving from simple, single-turn chatbots to dynamic, agentic programs that require numerous interdependent calls.
*   **Dependency Ignorance:** Existing serving systems suffer from long wait times because they ignore the dependencies between distinct LLM calls within a program.
*   **Primary Latency Cause:** The root cause of latency is identified as Head-of-Line (HOL) blocking, occurring at both the request level and the program level.
*   **Significant Performance Gains:** The proposed system, Autellix, achieves a **4‚Äì15√ó improvement in program throughput** over state-of-the-art systems like vLLM.

---

## üèõÔ∏è Methodology

The authors developed **Autellix**, a specialized serving system designed to treat programs as first-class citizens rather than isolated requests. The research methodology focused on three main areas:

1.  **System Design:** Implementation of an **interception mechanism** specifically for context enrichment, allowing the system to manage context retention more effectively than standard caching.
2.  **Algorithm Development:** Designing two distinct scheduling algorithms tailored for different environments:
    *   **Single-threaded programs**
    *   **Distributed programs**
3.  **Optimization Strategy:** Both scheduling algorithms utilize preemption and prioritization based on the history of completed calls to optimize execution flow.

---

## ‚öôÔ∏è Technical Details

The Autellix architecture introduces several technical innovations to manage agentic workflows:

*   **Program Modeling:** Agentic programs are treated similarly to OS processes and modeled as **Dynamic Directed Acyclic Graphs (DAGs)**.
*   **Runtime State:** The architecture dynamically builds an internal state known as a **process table** during runtime.
    *   **Nodes:** Represent individual LLM calls.
    *   **Edges:** Represent dependencies between calls.
*   **Scheduling Policy:** To combat Head-of-Line (HoL) blocking at both call and program levels, the system employs a **Multi-Level Feedback Queue (MLFQ)**.
    *   Features **preemption** to prioritize shorter tasks.
*   **Caching Strategy:**
    *   **Intra-program caching:** Emphasized heavily due to high context overlap (hit rates approaching 1.0).
    *   **Inter-program caching:** De-emphasized (hit rates between 0.0‚Äì0.2).

---

## üìà Results

The validation of Autellix involved comprehensive benchmarking against state-of-the-art baselines (vLLM):

*   **Throughput:** Achieved a **4‚Äì15√ó improvement** in program throughput across diverse workloads.
*   **Concurrency:** In steady-state tests, Autellix handled **10 additional concurrent LLM calls** compared to vLLM's First-Come-First-Served (FCFS) policy.
*   **Bottleneck Analysis:** Confirmed that wait time is the primary bottleneck, accounting for **80.4% of total time at Load 0.90**. Autellix successfully reduced wait-to-execution ratios for short calls.
*   **Cache Efficiency:** Validated the strategy of focusing on intra-program caching, which yielded significantly higher hit rates.
*   **Tested Workloads:** Chatbots, ReAct, Map-Reduce, and Monte Carlo Tree Search (MCTS).

---

## ‚úÖ Contributions

*   **Optimization Framework:** Introduced a novel framework that explicitly accounts for dependencies between LLM calls, a factor largely ignored by previous systems.
*   **Scheduling Algorithms:** Developed two new scheduling algorithms that leverage program history to effectively address HOL blocking.
*   **Performance Validation:** Validated the approach through extensive benchmarking, establishing a new performance standard by improving throughput by an order of magnitude without increasing latency.

---
*References: 40 citations*