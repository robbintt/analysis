---
title: Agentic Reasoning and Tool Integration for LLMs via Reinforcement Learning
arxiv_id: '2505.01441'
source_url: https://arxiv.org/abs/2505.01441
generated_at: '2026-02-03T12:28:29'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Agentic Reasoning and Tool Integration for LLMs via Reinforcement Learning

*Joykirat Singh; Raghav Magazine; Yash Pandya; Akshay Nambi*

---

## üìä Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Framework** | ARTIST (Agentic Reasoning and Tool Integration in Self-improving Transformers) |
| **Performance Gain** | Up to **22%** absolute improvement over base models |
| **Training Method** | Outcome-based Reinforcement Learning (Group Relative Policy Optimization) |
| **Key Innovation** | Loss masking strategy to exclude tool outputs from gradient propagation |
| **Validation** | WebArena, WindowsAgentArena, Code Interpreters, Web Search |
| **Quality Score** | 9/10 |

---

## üìù Executive Summary

Current Large Language Models (LLMs) face significant limitations in executing complex, multi-step reasoning and integrating external tools effectively. These models typically require resource-intensive, step-by-step supervision to learn agentic capabilities, which hinders their scalability and adaptability. Addressing this inefficiency is critical for advancing autonomous agents that can operate in dynamic environments without the need for costly, granular human guidance at every stage of the decision-making process.

The paper introduces **ARTIST** (Agentic Reasoning and Tool Integration in Self-improving Transformers), a unified framework that combines agentic reasoning with reinforcement learning (RL) to optimize decision-making without intermediate labels. Technically, ARTIST employs outcome-based RL, specifically utilizing Group Relative Policy Optimization (GRPO) to maximize final rewards. The architecture features a specialized prompt structure encompassing Internal Reasoning, Tool Queries, and Tool Outputs, coupled with a unique loss masking strategy. This strategy excludes tool output tokens from loss computation, ensuring gradients propagate only through the model's reasoning and actions, thereby enabling the autonomous management of observation-action loops.

The ARTIST framework demonstrates substantial performance gains, achieving up to a **22% absolute improvement** over base models specifically on mathematical reasoning and multi-turn function calling benchmarks. Beyond these specific metrics, the model exhibits robust capabilities in autonomous tool management, deeper chain-of-thought reasoning, and self-correction. The system was successfully validated across diverse environments, including WebArena, WindowsAgentArena, code interpreters, and web search engines, proving its ability to dynamically adapt reasoning depth and tool usage at test time.

This work establishes agentic RL combined with tool integration as a vital advancement for creating robust, interpretable, and generalizable AI systems. By demonstrating that LLMs can learn sophisticated tool usage strategies solely from outcome-based rewards, the research sets a new standard for the capabilities of RL-trained agentic systems. This approach effectively eliminates the dependency on step-level supervision, paving the way for the development of highly autonomous agents capable of iterative refinement and complex problem-solving in real-world scenarios.

---

## üîë Key Findings

*   **Significant Performance Gains:** ARTIST achieves up to **22% absolute improvement** over base models on mathematical reasoning and multi-turn function calling benchmarks.
*   **Autonomous Tool Orchestration:** Enables LLMs to autonomously determine which tools to use, when to use them, and how to invoke them without explicit instructions.
*   **Deepened Reasoning:** The agentic reinforcement learning approach leads to deeper reasoning chains and more effective tool utilization compared to standard approaches.
*   **Cost-Efficient Training:** The model learns robust strategies for environment interaction relying solely on outcome-based RL, eliminating the need for costly step-level supervision.
*   **Dynamic Adaptation:** The model demonstrates the ability to adapt its reasoning depth dynamically at test time.

---

## üß† Methodology

The researchers introduce **ARTIST**, a unified framework integrating three core components: agentic reasoning, reinforcement learning (RL), and tool integration.

*   **Outcome-Based RL:** Unlike traditional methods that require intermediate step-by-step supervision, ARTIST utilizes outcome-based RL. This allows the model to learn from the final results of an action sequence, facilitating the development of adaptive, multi-step reasoning capabilities.
*   **Dynamic Interaction:** The methodology encourages the model to dynamically interact with external tools and environments, viewing tool usage as a core component of the reasoning trajectory rather than an external appendage.
*   **Unified Framework:** By tightly coupling reasoning with tool use within an RL loop, the system moves beyond the limitations of static LLMs, creating a more fluid problem-solving process.

---

## ‚öôÔ∏è Technical Details

### Architecture & Loop
The ARTIST framework treats tool usage as an intrinsic part of the reasoning trajectory. It operates on a continuous loop consisting of:
1.  **Observation**
2.  **Reasoning / Action**
3.  **Tool Interaction**
4.  **Reward**

### Prompt Structure
The system utilizes a specific prompt template to organize the agentic process, containing segments for:
*   **Internal Reasoning**
*   **Tool Queries**
*   **Tool Outputs**
*   **Final Answer**

### Training Algorithm (GRPO)
The model is trained using **Group Relative Policy Optimization (GRPO)** to ensure sample efficiency and stability.
*   **Objective:** Optimizes an objective using clipped importance sampling ratios.
*   **Constraint:** Includes a KL divergence penalty to maintain policy stability.

### Critical Innovation: Loss Masking
A key technical contribution is the implementation of a loss masking strategy.
*   **Mechanism:** Tokens generated by tool outputs are excluded during loss computation.
*   **Benefit:** This ensures that gradients propagate *only* through the model-generated reasoning and actions, preventing the model from being penalized for external tool errors or inconsistencies.

---

## üìà Results

*   **Benchmark Success:** Achieved up to 22% absolute improvement on mathematical reasoning and multi-turn function calling tasks.
*   **Autonomous Behavior:** The model successfully determined tool usage and developed deeper, more coherent chains of thought.
*   **Robustness:** Exhibited self-correction capabilities and iterative refinement during problem solving.
*   **Validation Environments:** The architecture was validated across complex environments including:
    *   Web Browsers (WebArena)
    *   Operating Systems (WindowsAgentArena)
    *   Code Interpreters
    *   Web Search Engines

---

## üèÅ Contributions

*   **Novel Architecture:** Introduced ARTIST, a new architecture that tightly couples agentic reasoning with RL, moving beyond the limitations of static LLMs.
*   **New Frontier:** Established agentic RL combined with tool integration as a powerful new frontier for creating robust, interpretable, and generalizable problem-solving systems.
*   **Performance Standard:** Set a new standard for the capabilities of RL-trained agentic systems by demonstrating substantial performance gains on challenging tasks without step-level supervision.

---

**References:** 40 citations