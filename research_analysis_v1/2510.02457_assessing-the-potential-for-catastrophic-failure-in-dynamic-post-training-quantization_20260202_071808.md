# Assessing the Potential for Catastrophic Failure in Dynamic Post-Training Quantization

*Logan Frank; Paul Ardis*

---

> ### üìã Quick Facts
>
> *   **Quality Score:** 8/10
> *   **Citations:** 40 References
> *   **Key Risks:** Up to **75.80%** accuracy reduction in brittle models.
> *   **Method:** Knowledge Distillation (KD) + Reinforcement Learning (RL).
> *   **Core Insight:** Specific "inflection point" layers act as triggers for system-wide collapse.

---

## üìù Executive Summary

Dynamic Post-Training Quantization (DPTQ) is a widely adopted technique for optimizing neural network efficiency‚Äîreducing memory and latency‚Äîwithout the high computational cost of retraining. The field has traditionally operated under the assumption that DPTQ introduces only minor, predictable performance degradation. This paper challenges that assumption by revealing that **DPTQ can induce catastrophic failure modes**, where model accuracy collapses completely rather than dipping gracefully.

The authors demonstrate that these drastic failures are not random but are heavily dependent on the interaction between specific network architectures and dynamic bit-width allocation policies when encountering certain input distributions. This poses a severe risk for safety-critical systems, such as autonomous vehicles or medical devices, where a minor efficiency trade-off is acceptable, but a total loss of accuracy is not.

To systematically identify these failure points, the authors developed a framework combining Knowledge Distillation (KD) and Reinforcement Learning (RL). Experiments on ResNet18, MobileNetV4, and RegNetX-1.6GF demonstrated a stark divergence in quantized performance while maintaining near-identical floating-point accuracy. While "Robust" models showed negligible changes, "Brittle" models suffered catastrophic accuracy drops of up to 75.80%.

This research significantly shifts the paradigm for deploying quantized models, establishing that efficiency optimizations cannot be assumed safe without rigorous validation. It serves as a cautionary advocacy for safety, ensuring that the pursuit of model compression does not inadvertently introduce unexpected and dangerous failure modes.

---

## üîç Key Findings

*   **Catastrophic Potential:** Identified the existence of detrimental network-policy pairs causing catastrophic failure in dynamic PTQ rather than minor degradation.
*   **Significant Variance:** Vulnerable models showed a **10% to 65%** accuracy reduction compared to less than 2% in robust models.
*   **Input Dependency:** Drastic performance reduction is heavily dependent on the distribution of inputs encountered during inference.
*   **Vulnerability Mapping:** Successfully identified initial points of highest vulnerability within networks that contribute to these failures.

---

## üõ†Ô∏è Methodology

The authors formulated a combined task using **Knowledge Distillation (KD)** and **Reinforcement Learning (RL)** to simultaneously learn a specific neural network and a corresponding bit-width policy pair.

Key aspects of the approach include:
*   **Co-training:** Simultaneous learning of the neural network and bit-width policy.
*   **Worst-Case Analysis:** The setup focuses on worst-case scenarios, specifically seeking out input distributions and model configurations that maximize performance degradation under quantization.

---

## üß© Technical Details

The paper proposes a framework for Dynamic Post-Training Quantization (DPTQ) to create 'network-policy pairs' consisting of a **Robust network ($f_R$)** and a **Brittle/Detrimental network ($f_D$)**.

### Phase 1: White Box Model Retrieval
*   Uses Knowledge Distillation from a black box teacher ($f_B$) to a student ($f_N$).
*   Utilizes KL-divergence, Temperature ($\tau$), and **Mixup** (specifically 'Patient and Consistent' KD without ground truth cross-entropy).

### Phase 2: Policy-Aware Training
*   $f_N$ initializes $f_R$ and $f_D$.
*   A **Policy Network ($\theta$)**‚Äîa small CNN‚Äîpredicts bit-widths for $L$ dynamic layers (output $R^{L \times O}$ logits).

### Phase 3: Composite Loss & Gradient Splitting
*   **Loss Function:** $L = L_{\text{Hinge}} + L_{\text{KD}}$ (equal weights).
    *   $L_{\text{Hinge}}$: Trains the policy for accuracy effects.
    *   $L_{\text{KD}}$: Ensures fidelity to $f_N$.
*   **Optimization:**
    *   $\theta$ (Policy) optimized via $L_{\text{Hinge}}$.
    *   Networks optimized via total $L$.

### Phase 4: Bit-Width Budget Constraint
*   Enforces a Bit-Width Budget ($C$).
*   Solves a **Multiple-Choice Knapsack Problem (MCKP)** using Dynamic Programming on policy outputs.

---

## üìä Contributions

1.  **Failure Mode Analysis:** Provided empirical evidence and a framework for assessing and generating worst-case failure scenarios in dynamic PTQ.
2.  **Safety Advocacy:** Emphasized the critical need for caution and rigorous robustness checks when deploying quantized models in safety-critical environments.
3.  **Vulnerability Mapping:** Offered an initial exploration into specific points of highest vulnerability within neural networks subject to low-precision representations.

---

## üß™ Results & Analysis

Experiments on **ResNet18**, **MobileNetV4**, and **RegNetX-1.6GF** demonstrated divergent quantized performance while maintaining FP accuracy.

| Model | Configuration | FP Accuracy | Robust Quantized ($\Delta$) | Brittle Quantized ($\Delta$) |
| :--- | :--- | :--- | :--- | :--- |
| **ResNet18** | V=III, B | 87.87% / 85.58% | **+0.08%** | **-64.78%** |
| **MobileNetV4** | V=I, B | 86.43% / 82.61% | **-2.37%** | **-75.80%** |
| **RegNetX-1.6GF** | V=I, B | 86.73% / 75.39% | **-0.82%** | **-34.26%** |

### Layer Vulnerability Analysis
The analysis identified specific failure locations relative to network depth:
*   **ResNet18:** ~75% depth (23.60% transitory points).
*   **RegNetX:** Last 10% (27.09% transitory points).
*   **MobileNetV4:** Earlier layers (67.32% transitory points).

**Conclusion:** Quantizing specific 'inflection point' layers is sufficient to trigger catastrophic failure.

---

*Document generated from research analysis.*