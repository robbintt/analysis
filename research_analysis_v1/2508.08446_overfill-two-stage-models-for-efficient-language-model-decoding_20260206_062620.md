---
title: 'OverFill: Two-Stage Models for Efficient Language Model Decoding'
arxiv_id: '2508.08446'
source_url: https://arxiv.org/abs/2508.08446
generated_at: '2026-02-06T06:26:20'
quality_score: 8
citation_count: 23
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# OverFill: Two-Stage Models for Efficient Language Model Decoding

*Woojeong Kim; Junxiong Wang; Jing Nathan Yan; Mohamed Abdelfattah; Alexander M. Rush*

---

> ### ðŸ“Š Quick Facts
> *   **Quality Score:** 8/10
> *   **Citations:** 23 references
> *   **Top Performance Gain:** 83.2% (3B-to-1B config vs. standalone 1B model)
> *   **Core Innovation:** Two-stage asymmetric decoupling architecture
> *   **Primary Benefit:** Optimizes accuracy-efficiency trade-off by aligning model complexity with hardware bottlenecks

---

## Executive Summary

Large Language Model (LLM) inference is fundamentally constrained by a mismatch between computational resources and the distinct requirements of generation phases. The inference process consists of a prefill stage (processing input) which is compute-bound and benefits from parallelization, and a decode stage (generating tokens) which is memory-bound and limited by bandwidth. Standard decoder-only models utilize the same full parameter set for both stages, resulting in inefficiency where the heavy computational overhead of the full model is underutilized during the sequential, memory-intensive decode phase.

The paper introduces **OverFill**, a two-stage asymmetric inference architecture designed to decouple the prefill and decode phases. In Stage 1, the full model parameters ($\theta$) are leveraged to process the input sequence and build the Key-Value (KV) cache, taking full advantage of compute-bound parallelism. In Stage 2, the system switches to a smaller, dense pruned sub-network ($\theta'$) for autoregressive generation, drastically reducing memory bandwidth demands.

OverFill delivers substantial performance improvements over standalone pruned baselines while maintaining minimal latency overhead. Specifically, the 3B-to-1B configuration outperformed a standalone 1B pruned model by 83.2%, while the 8B-to-3B configuration outperformed a standalone 3B model by 79.2%. The method also achieves performance parity with models trained from scratch but requires significantly less training data.

This work challenges the standard uniform approach to decoder-only models, validating that decoupling inference stages optimizes the accuracy-efficiency trade-off better than current practices.

---

## Key Findings

*   ðŸš€ **Significant Performance over Pruned Models:** The OverFill 3B-to-1B configuration outperforms standalone 1B pruned models by **83.2%**, while the 8B-to-3B configuration outperforms 3B pruned models by **79.2%**.
*   âœ… **Parity with Scratch Training:** OverFill achieves performance levels comparable to models trained from scratch, but requires significantly less training data.
*   âš¡ **Latency Efficiency:** By leveraging more compute during the prefill stage, the method improves generation quality while maintaining minimal latency overhead.
*   ðŸ§© **Stage Decoupling Efficacy:** The study validates that decoupling the prefill and decode stages optimizes the accuracy-efficiency trade-off better than current uniform decoder-only models.

---

## Methodology

The paper proposes **OverFill**, a two-stage inference framework designed to decouple the prefill and decode phases of Large Language Models (LLMs):

*   **Stage 1 (Prefill):** Utilizes a full model to process system and user inputs, taking advantage of compute-bound parallel processing.
*   **Stage 2 (Decode):** Switches to a smaller, dense pruned model to generate tokens sequentially, addressing the memory-bound nature of the decoding stage.

---

## Technical Details

OverFill employs a two-stage asymmetric decoupling architecture to address LLM inference bottlenecks.

### Architecture & Processing
*   **Prefill Stage (Compute-Bound):** Uses full model parameters ($\theta$) to process the input sequence and build the Key-Value cache.
*   **Decode Stage (Memory-Bound):** Uses a pruned sub-network ($\theta'$) for autoregressive generation to reduce memory bandwidth.

### Pruning Strategy
*   **Technique:** Structured width pruning is utilized to maintain cache compatibility (preserving dimension $D$).
*   **Process:** The pruning process aggregates activations using L2 norm across batches and mean across sequences to retain the top $(1-P)\%$ channels.

### Training Methodology
*   **Approach:** The full model is frozen; only the pruned decoder is trained.
*   **Objective:** Utilizes teacher forcing to maximize the likelihood of output tokens $y$.

---

## Contributions

*   **Architectural Innovation:** Introduction of a decoupled inference architecture that treats prefill and decode stages separately, challenging the standard uniform approach of decoder-only models.
*   **Optimization of Computational Profiles:** A strategy that aligns model complexity with the specific hardware bottlenecks of each inference stage (compute-bound vs. memory-bound).
*   **Data Efficiency:** A demonstration that high-performance models can be achieved via this two-stage configuration without the extensive data requirements typically associated with training models from scratch.

---

## Results

OverFill was evaluated using configurations including pruning a 3.21B model to 0.52B (P=0.70) and an 8.03B model to 3.19B (P=0.43).

*   **Performance:** OverFill significantly outperforms standalone pruned models.
*   **Dataset:** Main experiments utilized the Infinity-Instruct dataset (7M instances), while OpenHermes-2.5 was used for pruning ratio sweeps.
*   **Outcome:** The 3B-to-1B configuration outperformed a standalone 1B model by 83.2%, and the 8B-to-3B configuration outperformed a standalone 3B model by 79.2%.