---
title: 'Wikontic: Constructing Wikidata-Aligned, Ontology-Aware Knowledge Graphs with
  Large Language Models'
arxiv_id: '2512.00590'
source_url: https://arxiv.org/abs/2512.00590
generated_at: '2026-02-06T01:43:02'
quality_score: 9
citation_count: 23
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Wikontic: Constructing Wikidata-Aligned, Ontology-Aware Knowledge Graphs with Large Language Models

*Alla Chepurova; Aydar Bulatov; Mikhail Burtsev; Yuri Kuratov*

---

### üìä Quick Facts

| Metric | Value |
| :--- | :--- |
| **MINE-1 Retention** | 86% (State-of-the-Art) |
| **Output Tokens** | < 1,000 (High Efficiency) |
| **Cost Reduction** | 3x vs AriGraph; >95% vs GraphRAG |
| **HotpotQA F1** | 76.0 |
| **MuSiQue F1** | 59.8 |
| **Quality Score** | 9/10 |

---

## üìù Executive Summary

This research addresses the inherent difficulties in constructing high-quality Knowledge Graphs (KGs) from unstructured text using Large Language Models (LLMs). Current approaches frequently struggle with a trade-off between information density and computational cost, often resulting in verbose, unstructured outputs that suffer from hallucinations and entity duplication. This is a critical issue for technical applications, as integrating structured knowledge into LLMs is essential for enabling complex reasoning and reducing factual errors, yet existing methods remain too expensive or inconsistent for scalable deployment.

Wikontic introduces a novel, multi-stage pipeline that shifts the focus from generating raw text to constructing ontology-aware KGs through strict adherence to Wikidata constraints. The technical implementation involves three distinct stages: Candidate Triplet Extraction, Ontology-aware Triplet Refinement, and Entity Normalization & Deduplication. By leveraging a schema derived from 2,464 Wikidata factual properties and employing recursive expansion of 'instance of' and 'subclass of' relations, the system enforces rigorous type and relation constraints. This ensures that generated triplets are verifiable, logically consistent, and semantically indexed, effectively anchoring LLM outputs to a structured global ontology.

The proposed pipeline achieves state-of-the-art performance with exceptional efficiency, securing 86% information retention on the MINE-1 benchmark. Wikontic is highly cost-effective, requiring fewer than 1,000 output tokens to construct a KG‚Äîa 3x reduction compared to AriGraph and over a 95% reduction compared to GraphRAG. In reasoning tasks utilizing triplets-only inputs, the method achieved 76.0 F1 on HotpotQA and 59.8 F1 on MuSiQue, matching or surpassing retrieval-augmented generation baselines. Additionally, the generated graphs are compact and high-quality, with 96% of triplets on the MuSiQue dataset containing the correct answer entity.

The significance of this work lies in establishing that high-quality, ontology-aware triplets can serve as a sufficient, standalone knowledge source for complex reasoning, thereby reducing dependency on large, unstructured text contexts. Wikontic offers a scalable solution that integrates verifiable, structured knowledge into LLMs without prohibitive computational overhead. By demonstrating that rigorous ontology alignment improves both intrinsic graph quality and downstream reasoning, this research paves the way for more reliable, efficient, and verifiable AI systems.

---

## üîë Key Findings

*   **State-of-the-Art Retention:** Achieves **86%** information retention on the MINE-1 benchmark.
*   **Unmatched Efficiency:** Requires fewer than 1,000 output tokens, representing a **3x reduction** compared to AriGraph and over a **95% reduction** compared to GraphRAG.
*   **Reasoning Performance:** In triplets-only reasoning, the method achieves **76.0 F1** on HotpotQA and **59.8 F1** on MuSiQue, matching or surpassing retrieval-augmented generation baselines.
*   **High Graph Quality:** Generated Knowledge Graphs are compact, ontology-consistent, and well-connected. Notably, **96%** of generated triplets contain the correct answer entity on the MuSiQue dataset.

---

## üõ†Ô∏è Methodology

Wikontic implements a structured, multi-stage process designed to maximize information density while minimizing computational overhead.

*   **Structured Pipeline:** Utilizes a rigorous process for constructing KGs from open-domain text.
*   **Extraction & Constraint:** Involves extracting candidate triplets with qualifiers and strictly enforcing Wikidata-based type and relation constraints.
*   **Normalization:** Applies entity normalization techniques to address duplication common in LLM-generated outputs.
*   **Ontology Alignment:** Prioritizes alignment with existing ontologies (specifically Wikidata) to ensure structure and verifiability.

---

## ‚öôÔ∏è Technical Details

Wikontic is a multi-stage pipeline integrating LLMs with Wikidata-derived ontological constraints, entity normalization, and iterative refinement.

### Pipeline Architecture
The system operates through three primary stages:
1.  **Candidate Triplet Extraction:** Capturing raw triplets with metadata (qualifiers) and entity types.
2.  **Ontology-aware Triplet Refinement:** Refinement using type constraints and relation validation to ensure ontology compatibility.
3.  **Entity Normalization & Deduplication:** Cleaning the graph to ensure entity consistency.

### Schema & Tools
*   **Wikidata Integration:** The schema is derived from Wikidata, utilizing **2,464 factual properties**.
*   **Recursive Logic:** Uses recursive expansion of 'instance of' and 'subclass of' relations for constraint logic.
*   **Indexing:** Employs Contriever embeddings and MongoDB Atlas Vector Search for semantic indexing.

---

## üìà Results

The pipeline demonstrates exceptional performance across efficiency, retention, and reasoning benchmarks.

*   **Efficiency:** Requires **< 1,000 output tokens** for KG construction (3x fewer than AriGraph and >95% reduction compared to GraphRAG).
*   **Benchmark Performance:**
    *   **MINE-1:** 86% information retention (SOTA).
    *   **HotpotQA:** 76.0 F1.
    *   **MuSiQue:** 59.8 F1.
*   **Graph Integrity:** Compact and ontology-consistent graphs with high relevance to target queries.

---

## üí° Contributions

*   **Paradigm Shift:** Shifts the focus of KG usage in LLMs from mere auxiliary text retrieval to improving the intrinsic quality and structure of the Knowledge Graphs.
*   **Scalability:** Offers a scalable solution for integrating high-quality, structured knowledge into LLMs, addressing the trade-off between information density and computational cost.
*   **Reasoning Efficiency:** Provides evidence that high-quality, ontology-aware triplets can serve as a sufficient knowledge source for complex reasoning tasks, reducing dependency on large, unstructured text contexts.

---

**Quality Score:** 9/10 | **References:** 23 citations