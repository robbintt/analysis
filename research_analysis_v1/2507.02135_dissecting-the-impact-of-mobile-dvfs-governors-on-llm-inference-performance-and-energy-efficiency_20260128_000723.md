---
title: Dissecting the Impact of Mobile DVFS Governors on LLM Inference Performance
  and Energy Efficiency
arxiv_id: '2507.02135'
source_url: https://arxiv.org/abs/2507.02135
generated_at: '2026-01-28T00:07:23'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# Dissecting the Impact of Mobile DVFS Governors on LLM Inference Performance and Energy Efficiency

*Jian Li, Zongpu Zhang, Pranab Dash, Haibing Guan, Shanghai Jiao, Qiang Xu, Charlie Hu*

> ### **Quick Facts: Key Metrics**
> *   **Latency Overhead:** Up to **40.4%** longer latency with uncoordinated governors vs. optimal settings.
> *   **TTFT Reduction:** **7.0% – 16.9%** improvement using the proposed FUSE governor.
> *   **TPOT Reduction:** **25.4% – 36.8%** improvement using the proposed FUSE governor.
> *   **Test Hardware:** Google Pixel 7/7 Pro, iPhone 14 Pro (Dataset: ShareGPT).
> *   **Impact:** Achieves significant speedups without increasing the energy-per-token budget.

---

## Executive Summary

Efficiently deploying Large Language Models (LLMs) on mobile devices is severely hindered by uncoordinated power management mechanisms. Current mobile systems rely on independent Dynamic Voltage and Frequency Scaling (DVFS) governors for the CPU, GPU, and memory, a configuration the authors term the "uncoordinated triplet." Because these components operate in isolation, the system suffers from suboptimal frequency pairings that create significant bottlenecks during inference. This lack of coordination introduces substantial latency overhead and energy waste, limiting the viability of responsive on-device AI execution under strict thermal and power constraints.

The researchers propose **FUSE**, a unified, energy-aware governor architecture designed to holistically manage the frequency of the hardware triplet. The innovation is grounded in a detailed root cause analysis performed using the `llama.cpp` framework, which characterizes LLM inference into compute-intensive **Prefill** and memory-intensive **Decode** stages. By identifying hardware constraints—such as shallow CPU-GPU command queues—FUSE dynamically synchronizes hardware frequencies.

Empirical evaluations on commercial mobile hardware demonstrate that standard uncoordinated governors suffer from up to **40.4%** higher latency compared to optimal frequency configurations at identical energy consumption levels. Implementing FUSE yields substantial performance gains without increasing energy usage. These results confirm that the system achieves significant speedups while strictly maintaining energy-per-token efficiency, paving the way for more responsive generative AI on mobile edge devices.

---

## Key Findings

*   **The "Uncoordinated Triplet" Problem:** Standard CPU, GPU, and Memory DVFS governors operate independently. This lack of coordination causes energy inefficiencies and performance bottlenecks during LLM inference.
*   **Significant Latency Overhead:** Uncoordinated governors lead to up to **40.4%** longer latency when compared to optimal frequency combinations at the same energy consumption level.
*   **FUSE Solution:** The proposed unified governor (FUSE) successfully coordinates hardware frequencies.
*   **Performance Gains:**
    *   Reduces Time-to-First-Token (TTFT) latency by **7.0% – 16.9%**.
    *   Reduces Time-Per-Output-Token (TPOT) latency by **25.4% – 36.8%**.
*   **Energy Parity:** These performance improvements are achieved while maintaining the same Energy-Per-Token, effectively decoupling speed from power consumption.

---

## Methodology

The research follows a three-phase empirical approach:

1.  **Baseline Establishment:** The team conducted a measurement study on commercial mobile phones using a State-of-the-the-Art (SOTA) LLM framework. The goal was to evaluate current energy efficiency and establish a performance baseline.
2.  **Root Cause Analysis:** The researchers dissected the behavior and interplay of independent mobile governors to understand the source of inefficiencies. This involved analyzing how hardware power management systems interact during inference.
3.  **Design and Evaluation:** Based on the analysis, the team designed **FUSE**, a unified energy-aware governor. It was evaluated using the ShareGPT dataset across various mobile LLM models to quantify latency reductions relative to energy consumption.

---

## Technical Details

### System Architecture & Bottlenecks
*   **Triplet Power Management:** Identifies inefficiencies caused by uncoordinated management of CPU, GPU, and Memory on devices like the Google Pixel 7/7 Pro.
*   **CPU-GPU Interaction:** Utilizing `llama.cpp` with OpenCL, the system is bottlenecked by a shallow hardware queue (max depth 2). This requires constant CPU activity to feed the GPU, creating a dependency that independent governors fail to manage optimally.

### LLM Inference Stages
The study breaks inference into two distinct stages requiring different hardware optimizations:
*   **Prefill Stage:** Compute-intensive.
*   **Decode Stage:** Memory-intensive (relying on KV-cache).

### Hardware Configurations
The study analyzed the following frequency ranges on commercial mobile silicon:

| Component | Frequency Range |
| :--- | :--- |
| **CPU** | 500 MHz – 2850 MHz |
| **GPU** | 151 MHz – 848 MHz |
| **Memory** | 421 MHz – 3172 MHz |

---

## Results & Contributions

### Empirical Results
*   **Baseline Inefficiency:** Uncoordinated governors resulted in up to **40.4%** longer latency compared to optimal configurations. In extreme cases, this inefficiency could deplete a battery (iPhone 14 Pro tested) after only **490 to 590 prompts**.
*   **FUSE Performance:**
    *   **TTFT:** 7.0% – 16.9% reduction.
    *   **TPOT:** 25.4% – 36.8% reduction.
    *   **Energy:** Maintained energy-per-token efficiency.

### Research Contributions
1.  **Diagnostic Analysis:** Provided the first detailed diagnostic analysis of how standard, independent mobile DVFS governors negatively impact LLM inference performance.
2.  **Quantitative Benchmarking:** Established concrete benchmarks, identifying a **40.4%** latency overhead directly attributable to uncoordinated power management.
3.  **Unified Architecture:** Developed **FUSE**, a novel unified energy-aware governor that coordinates hardware frequencies to achieve significant latency reductions without increasing the mobile device's energy budget.

---
**Document Statistics**
*   Quality Score: 9/10
*   Citations: 40 references