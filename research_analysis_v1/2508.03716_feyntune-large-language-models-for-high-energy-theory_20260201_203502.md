# FeynTune: Large Language Models for High-Energy Theory

*Paul Richmond; Prarit Agarwal; Borun Chowdhury; Vasilis Niarchos; Constantinos Papageorgakis*

---

## üìë Executive Summary

The application of Large Language Models (LLMs) to theoretical High-Energy Physics is hindered by a domain gap; general commercial models lack the requisite depth for specialized sub-fields, while existing open-source models fail to capture necessary scientific nuances. This paper addresses the challenge of effectively adapting AI for high-entropy, technically precise domains like High-Energy Theory (hep-th), High-Energy Phenomenology (hep-ph), and General Relativity and Quantum Cosmology (gr-qc). The authors investigate whether smaller, domain-specific models can be fine-tuned to rival the performance of large proprietary systems, thereby removing barriers to AI adoption in specialized scientific research.

The authors introduce **"FeynTune,"** a suite of 20 specialized models derived from the 8-billion parameter Llama-3.1 architecture. To efficiently adapt the base model, they employed two distinct Low-Rank Adaptation (LoRA) approaches: LoRA-QKV and LoRA-all. The methodology rigorously isolates the impact of domain relevance by incorporating control datasets from disparate fields (q-bio, cs) alongside the target physics categories. Training was conducted on arXiv abstracts through August 2024 using 3x NVIDIA A100 40GB GPUs with 4-bit precision for base weights and 16-bit for LoRA adapters, utilizing Flash Attention 2.

The study demonstrates that **all fine-tuned variants outperformed the base Llama-3.1 model**, which achieved a perplexity of 11.20. While the LoRA-all variant achieved the absolute lowest perplexity of 9.83, the LoRA-QKV method generally yielded superior perplexity scores across the broader evaluation set. Human evaluations indicated that these models exhibited higher entropy‚Äîsignaling reduced repetition‚Äîand successfully utilized domain-specific technical language. When benchmarked against major commercial LLMs (ChatGPT, Claude, Gemini, and DeepSeek), the open-source FeynTune models demonstrated parity in technical proficiency. This research validates that smaller, transparent models can match the technical proficiency of large proprietary systems through targeted domain adaptation.

---

### ‚ö° Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Base Architecture** | Llama-3.1 8B |
| **Total Variants** | 20 specialized models |
| **Best Perplexity** | 9.83 (LoRA-all) |
| **Semantic Similarity** | 0.90 ¬± 0.07 (Fine-tuned) |
| **Training Hardware** | 3x NVIDIA A100 40GB |
| **Precision** | 4-bit (Base) / 16-bit (Adapters) |
| **Quality Score** | 7/10 |

---

## üîë Key Findings

*   **Universal Improvement:** All 20 fine-tuned variants outperformed the base Llama-3.1 model on hep-th abstract completion tasks.
*   **Domain Specificity:** Models trained specifically on High-Energy Physics categories (hep-th, hep-ph, gr-qc) were significantly more effective for domain-specific tasks compared to the generic base model.
*   **Competitive Benchmarking:** The study benchmarked specialized models against leading commercial LLMs, including **ChatGPT**, **Claude**, **Gemini**, and **DeepSeek**, showing competitive technical proficiency.
*   **Impact of Data Source:** A comparative study training models on abstracts from disparate fields (q-bio, cs) versus target physics fields confirmed that domain relevance is a critical driver of performance.

---

## üõ†Ô∏è Methodology

The research methodology focused on efficiently adapting a foundational architecture to the rigorous demands of theoretical physics.

*   **Foundation Model:** Utilized the 8-billion parameter **Llama-3.1** model.
*   **Fine-Tuning Strategy:** Employed two distinct Low-Rank Adaptation (LoRA) approaches to modify the base model efficiently:
    *   **LoRA-QKV**
    *   **LoRA-all**
*   **Dataset Composition:** Training data consisted of arXiv abstracts through August 2024. This included various combinations of High-Energy Physics sub-fields alongside control datasets from disparate fields to isolate the impact of domain relevance.
*   **Experimental Design:** The study generated 20 fine-tuned variants, manipulating both dataset sizes and specific categories of training data to evaluate performance variations.

---

## ‚öôÔ∏è Technical Details

### System Architecture
*   **Base Model:** Meta Llama 3.1 8B
*   **Adaptation:** Low-Rank Adaptation (LoRA)
    *   Variants: LoRA-QKV, LoRA-all
*   **Mechanism:** Flash Attention 2

### Hardware Configuration
*   **GPUs:** 3x NVIDIA A100 40GB
*   **Precision:**
    *   Base Weights: 4-bit
    *   LoRA Adapters: 16-bit

### Hyperparameters
| Parameter | Value |
| :--- | :--- |
| LoRA Rank | 8 |
| Alpha | 32 |
| Optimizer | AdamW |
| Batch Size | 16 |
| Epochs | 4 |
| Learning Rate | Warm-up to 3e-4 with cosine decay |

### Data Splitting
*   **Task:** Abstract Completion
*   **Split Ratio:** 70% Training / 15% Validation / 15% Testing
*   **Data Source:** arXiv abstracts (Publicly available up to August 2024)

---

## üìä Results

*   **Baseline Performance:** The base Llama 3.1 model achieved a perplexity of **11.20**.
*   **Fine-Tuned Performance:** 
    *   Fine-tuned models generally outperformed the baseline.
    *   **LoRA-all** achieved the lowest perplexity at **9.83** (dataset s3).
    *   **LoRA-QKV** generally achieved better perplexity scores than LoRA-all across the broader evaluation.
*   **Semantic Analysis:** Semantic similarity improved marginally from **0.88 ¬± 0.08** (baseline) to **0.90 ¬± 0.07** (fine-tuned).
*   **Human Evaluation:**
    *   Fine-tuned models exhibited **higher entropy** (reduced repetition).
    *   Successfully utilized domain-specific technical language.
    *   Performed competitively against commercial LLMs in technical proficiency.
*   **Caveat:** While technically proficient, the authors note the models were not 100% factually accurate.

---

## üèÜ Contributions

*   **FeynTune Suite:** Introduction of a suite of 20 specialized Large Language Models tailored specifically for theoretical High-Energy Physics.
*   **Comparative Analysis:** Provided a comprehensive performance evaluation that bridges the gap between open-source specialized models and proprietary commercial LLMs.
*   **Empirical Insights:** Offered empirical evidence regarding how fine-tuning on specific sub-domains and varying dataset sizes affects the capability of LLMs to complete complex physics texts, providing a reproducible blueprint for the scientific community.

---

**Quality Score:** 7/10  
**References:** 32 Citations