---
title: Data-to-text (D2T) generation (Lin et al
arxiv_id: '2502.12372'
source_url: https://arxiv.org/abs/2502.12372
generated_at: '2026-01-28T01:36:10'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# Data-to-Text (D2T) Generation

*Soumyajit Roy, John Buscema, Utpal Garain, Joy Mahapatra*

> ### ðŸ“Š Quick Facts
> *   **Model Scale:** 70 Million to 13 Billion Parameters
> *   **LLM Families Analyzed:** Pythia, OPT, BLOOM
> *   **Datasets Covered:** 5 (E2E, ViGGO, WikiTableText, DART, WebNLG)
> *   **Core Finding:** Factual inconsistency scales **exponentially**, not via power law.
> *   **Validation:** 3-Stage Statistical Framework (CV, F-test, Vuongâ€™s Test)

---

## ðŸ“‘ Executive Summary

### Problem: Factual Inconsistency in Scaling Data-to-Text Generation
This research addresses a critical gap in understanding how factual inconsistency in Data-to-Text (D2T) generation scales with model size, a distinct issue that differs from generalization error. While standard scaling laws suggest that generalization errors follow a predictable power-law decay as model parameters increase, the industry has largely assumed this "bigger is better" dynamic applies to factual reliability as well. The paper investigates this assumption, emphasizing that the veracity of generated text in structured data tasks cannot be guaranteed by brute-force parameter scaling alone.

### Innovation: Statistical Validation of Scaling Hypotheses
The key innovation is a rigorous three-stage statistical validation framework designed to compare competing scaling hypotheses. The authors formally model the relationship between LLM size ($x$) and factual inconsistency ($f(x)$) by comparing a Power Law against an Exponential distribution. The framework utilizes Maximum Likelihood Estimation (MLE) with Huber Loss for parameter fitting to statistically determine the superior model.

### Results: Exponential Increase in Hallucinations
Empirical testing on three decoder-only LLM families yielded conclusive evidence. The study found that factual inconsistency follows an **exponential scaling law** (superior fit $p < 0.005$ via Vuongâ€™s test), indicating that hallucination severity increases exponentially as parameter count grows, rather than improving. This finding was validated using four specific state-of-the-art consistency metrics: **ALIGN SCORE, QAFACTEVAL, SUMMAC-C, and FactCC**.

### Impact: A Strategic Pivot for AI Reliability
These findings challenge the prevailing narrative surrounding scaling laws. Because factual inconsistency scales exponentially (increasing with size) rather than decreasing via a power law, the study demonstrates that simply scaling up model parameters is an ineffectiveâ€”and potentially detrimentalâ€”strategy for ensuring reliability in D2T systems. This insight necessitates a strategic pivot in AI development, advocating for trustworthiness-specific architectures and training objectives.

---

## ðŸ”‘ Key Findings

*   **Exponential Scaling Identified:** Factual inconsistency in Data-to-text (D2T) generation follows exponential scaling relative to LLM size, contrary to the widely assumed power law scaling.
*   **Distinct Behavior:** Factual inconsistency exhibits a distinct and more severe scaling behavior compared to previous scaling laws focused on generalization error.
*   **Broad Empirical Validation:** Evaluation involving three popular LLM families and five D2T datasets confirms that larger models do not resolve factual inconsistency via standard power law dynamics.
*   **Statistical Confirmation:** A specific statistical validation framework was utilized to successfully identify exponential scaling as the superior fit for the data.

---

## ðŸ§ª Methodology

The study explicitly compared power law and exponential scaling to determine which best describes the relationship between LLM size and factual inconsistency.

*   **Comparative Analysis:** Direct comparison between Power Law and Exponential models.
*   **Validation Framework:** A rigorous three-stage statistical validation framework was employed:
    1.  **Predictive Performance:** Estimated via 5-fold cross-validation.
    2.  **Goodness-of-Fit:** Assessed via an F-test ($p < 0.05$) on log-transformed models.
    3.  **Comparative Analysis:** Conducted via Vuongâ€™s Likelihood-Ratio Test ($p < 0.005$).
*   **Scope:** The empirical study analyzed three popular LLM families across five D2T datasets.
*   **Measurement:** Factual inconsistency was measured inversely using four state-of-the-art consistency metrics.

---

## ðŸ› ï¸ Technical Details

### Scaling Law Models
The paper proposes two models to describe the relationship between LLM size ($x$) and factual inconsistency ($f(x)$):

*   **Power Law ($M_{pow}$):** $f(x) = Ax^\alpha + B$
*   **Exponential ($M_{exp}$):** $F(x) = Ce^{\beta x} + D$

### Parameter Estimation
*   **Method:** Maximum Likelihood Estimation (MLE)
*   **Loss Function:** Huber Loss ($\delta = 1$) to handle outliers effectively.

### Statistical Validation Stages
1.  **Predictive Performance:** 5-fold cross-validation.
2.  **Goodness-of-Fit:** F-test ($p < 0.05$) on log-transformed models to check fit quality.
3.  **Comparative Analysis:** Vuongâ€™s Likelihood-Ratio Test ($p < 0.005$) to strictly distinguish between Power Law and Exponential hypotheses.

### Target Systems & Evaluation
*   **Architectures:** Decoder-only LLM families (**Pythia**, **OPT**, **BLOOM**).
*   **Tasks:** Data-to-Text tasks including Graph-to-text, Table-to-text, and MR-to-text.
*   **Metrics:** ALIGN SCORE, QAFACTEVAL, and SUMMA C-C.

---

## ðŸ“ˆ Results

The core finding is that factual inconsistency in Data-to-Text generation follows **exponential scaling** relative to model size, contradicting the standard power law assumption. Consequently, hallucination severity increases exponentially as parameter count grows.

**Evaluation Scope:**
*   **Model Range:** 70 million to 13 billion parameters.
*   **Datasets:**
    *   E2E (~37K pairs)
    *   ViGGO (7K instances)
    *   WikiTableText (~13K pairs)
    *   DART (~70K triplets)
    *   WebNLG (~38K samples)
*   **Model Families:**
    *   Pythia (8 models: 70M-12B)
    *   OPT (6 models: 130M-13B)
    *   BLOOM (5 models: 0.56M-7B)

**Statistical Significance:**
*   Relied on the **F-statistic** derived from Sum of Squared Residuals (SSR).
*   Validated using **Vuongâ€™s test statistic** ($V_{stat}$).

---

## ðŸ“ Contributions

*   **Research Gap Identification:** Identified that LLM size specifically impacts factual inconsistency in D2T tasks differently than generalization error, shifting focus to trustworthiness.
*   **Evidence-Based Conclusion:** Provided the first evidence-based conclusion that factual inconsistency scales exponentially, challenging the industry standard assumption of power law scaling.
*   **Methodological Framework:** Introduced and validated a statistical framework for analyzing scaling laws in NLP, offering a template for future rigorous validation of model behaviors.
*   **Reliability Advancement:** Advanced the understanding of reliability in D2T systems by quantifying how increasing model parameters affects the veracity of generated text.

---

**Quality Score:** 8/10  
**References:** 40 citations