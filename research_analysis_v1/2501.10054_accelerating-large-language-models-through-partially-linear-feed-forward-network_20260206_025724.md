---
title: Accelerating Large Language Models through Partially Linear Feed-Forward Network
arxiv_id: '2501.10054'
source_url: https://arxiv.org/abs/2501.10054
generated_at: '2026-02-06T02:57:24'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Accelerating Large Language Models through Partially Linear Feed-Forward Network

*Gansen Hu; Zhaoguo Wang; Jinglin Wei; Wei Huang; Haibo Chen*

---

### ðŸ“‹ Quick Facts

| Metric | Value |
| :--- | :--- |
| **Parameter Reduction** | 80% (FFN Specific) |
| **Inference Speedup** | 1.6x (vLLM) / 1.4x (HuggingFace) |
| **Accuracy Trade-off** | 10.9% |
| **Baseline Comparison** | +65% accuracy vs. Wanda/RIA |
| **Quality Score** | 9/10 |

---

## Executive Summary

Large Language Models (LLMs) present significant deployment challenges due to their massive parameter counts and the associated computational overhead. This paper addresses the critical bottleneck of Feed-Forward Networks (FFNs), which constitute 67â€“80% of total model parameters. The research identifies that the primary inhibitor of inference speed is not merely arithmetic computation but the I/O overhead required to load these dense parameters, which consumes nearly 80% of total inference time. Consequently, reducing the size of FFNs is essential for accelerating LLM inference, yet traditional compression methods often struggle to maintain accuracy at high compression ratios.

The authors introduce **TARDIS**, a novel optimization framework inspired by compiler constant folding, which aims to merge FFN matrices to minimize parameter count. A significant hurdle in applying constant folding to LLMs is the prevalence of non-linear activation functions (GELU, SiLU), which typically prevent such algebraic simplification. TARDIS solves this through a hybrid approach: **"Partial Linear Approximation,"** which treats activation functions as linear within the narrow, high-frequency input ranges where most data clusters, and **"Dynamic Fallback,"** which utilizes an online predictor to revert to non-linear computation for outlier inputs. This mechanism theoretically enables an 87.5% reduction in FFN parameters without the catastrophic accuracy loss associated with naive linearization.

Empirical testing demonstrates that TARDIS substantially outperforms state-of-the-art pruning techniques (Wanda and RIA). The method achieves an 80% reduction in FFN parameters with a relatively minor accuracy trade-off of 10.9%, whereas naive linear approximation results in a 75% accuracy drop. In real-world deployments, these structural optimizations translate to significant performance gains: a 7B model achieved a 1.6x end-to-end speedup using the vLLM serving system and a 1.4x speedup with standard HuggingFace implementations. Furthermore, under high compression ratios, TARDIS delivered up to 65% higher accuracy than competing pruning baselines.

This work establishes a new optimization frontier for LLM compression by bridging the gap between compiler theory and deep learning architecture. By validating a method that offers a superior Pareto frontier balancing compression, speed, and accuracy, TARDIS provides a viable alternative to standard pruning and quantization. The findings are particularly significant for production environments, as they highlight that addressing I/O bottlenecks via structural reduction is as critical as optimizing computational operations, paving the way for more efficient serving of large-scale models.

---

## Key Findings

*   **Significant Parameter Reduction:** TARDIS achieves an **80% reduction** in parameters specifically within the feed-forward networks of Large Language Models.
*   **Superiority Over Pruning:** The method significantly outperforms state-of-the-art pruning techniques (Wanda and RIA), delivering up to **65% higher accuracy** under high compression ratios.
*   **Real-World Speedup:** In real-world deployments of a 7B model, TARDIS achieved a **1.6x end-to-end inference speedup** with the vLLM serving system and a **1.4x speedup** with the standard HuggingFace implementation.
*   **Maintained Model Integrity:** The approach maintains model integrity with only a **10.9% accuracy trade-off** despite substantial parameter reduction and speed gains.

---

## Methodology

The proposed method, **TARDIS**, draws inspiration from **constant folding** in compiler optimization to reduce model parameters. To address the challenge of modern LLMs using complex non-linear activation functions (like GELU)â€”which typically prevent direct constant foldingâ€”the methodology employs a two-step mechanism:

1.  **Partial Linear Approximation:** It approximates non-linear activation functions as linear functions specifically within input ranges that occur with high frequency.
2.  **Dynamic Fallback:** For outlier inputs that fall outside these frequent ranges, an online predictor is employed to dynamically revert to the original, non-linear computations.

---

## Technical Details

*   **Target Architecture:** Focuses on Feed-Forward Networks (FFNs), which account for **67-80%** of total model parameters.
*   **Optimization Mechanism:** Adapts constant folding to merge two FFN matrices by approximating non-linear activation functions (GELU, SiLU) as linear.
*   **Core Insight:** Activation inputs are concentrated in a narrow range, allowing linearization for the majority of cases.
*   **I/O Bottleneck:** Identifies parameter loading as the primary inference bottleneck. FFN parameter loading consumes **78.2%** of total inference time (analysis on Falcon-7B showed FFN blocks took 2.1s vs 0.9s for attention).
*   **Theoretical Reduction:** Enables a theoretical **87.5% reduction** in FFN parameters.

---

## Contributions

*   **Novel Optimization Perspective:** Introduces a new approach to LLM compression by applying the compiler optimization concept of constant folding to neural network architectures.
*   **Solution for Non-Linear Activations:** Solves the limitation of applying constant folding to non-linear activations by developing a framework that partially linearizes frequent inputs while dynamically handling outliers.
*   **Validation of Efficiency:** Provides comprehensive empirical evidence demonstrating that this approach offers a better Pareto frontier between compression, speed, and accuracy compared to existing pruning methods (Wanda, RIA).

---

## Results

**Compression vs. Accuracy**
*   **TARDIS:** 80% parameter reduction with only a 10.9% accuracy trade-off.
*   **Naive Linear Approximation:** Results in a catastrophic 75% accuracy drop.
*   **Comparison:** Delivers up to 65% higher accuracy than baselines (Wanda, RIA) at high compression ratios.

**Inference Speed**
*   **vLLM Serving:** 1.6x end-to-end speedup.
*   **HuggingFace:** 1.4x speedup.

**Performance Analysis (Falcon-7B)**
*   Confirmed I/O dominance, with FFN parameter loading taking significantly longer than attention computation.

---

*Report generated based on 40 citations. Quality Score: 9/10*