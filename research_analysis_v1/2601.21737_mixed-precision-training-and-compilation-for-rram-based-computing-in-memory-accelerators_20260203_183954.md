---
title: Mixed-Precision Training and Compilation for RRAM-based Computing-in-Memory
  Accelerators
arxiv_id: '2601.21737'
source_url: https://arxiv.org/abs/2601.21737
generated_at: '2026-02-03T18:39:54'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Mixed-Precision Training and Compilation for RRAM-based Computing-in-Memory Accelerators

*Rebecca Pelke; Joel Klein; Jose Cubero-Cascante; Nils Bosbach; Jan Moritz Joseph; Rainer Leupers*

---

> ### **Quick Facts**
> 
> *   **Peak Performance:** Achieved a **2.48x** inference speedup compared to state-of-the-art CIM solutions.
> *   **Accuracy Impact:** Maintained model integrity with a maximum accuracy loss of only **0.086%**.
> *   **Optimization Method:** Utilizes a Reinforcement Learning (RL)-based search strategy (enhanced HAQ).
> *   **Architecture:** 1T1R RRAM crossbar with differential mapping.
> *   **Key Innovation:** Supports sub-8-bit quantization (down to 2-bit) to bridge the hardware-software gap.

---

## Executive Summary

Resistive Random Access Memory (RRAM) based Computing-in-Memory (CIM) architectures offer significant potential for accelerating deep learning workloads, but current software stacks fail to fully exploit these hardware capabilities. A primary inefficiency lies in the disconnect between compiler support and hardware potential; existing CIM compilers often restrict quantization to 8 bits or higher, ignoring the ability of RRAM crossbars to handle lower precision. This limitation results in suboptimal latency, requiring multiple compute cycles per Matrix-Vector Multiplication (MVM) and inefficient weight storage across multiple cells.

The authors introduce a comprehensive mixed-precision training and compilation framework designed to bridge the gap between RRAM hardware capabilities and software support. The core technical innovation is a Reinforcement Learning (RL)-based search strategy, built upon an enhanced HAQ (Hardware-Aware Automated Quantization) backend, which autonomously navigates the vast combinatorial search space to identify optimal quantization parameters for each neural network layer. The system employs Quantization-Aware Training (QAT) via the Brevitas framework, utilizing range-based linear quantization on a 1T1R crossbar structure with differential mapping to ensure resilience against wire parasitics.

The proposed framework demonstrates superior performance compared to state-of-the-art CIM solutions, achieving a peak inference speedup of **2.48x** against an 8-bit baseline and exceeding **2.20x** speedup for 4-bit cell configurations. Crucially, these significant latency reductions were achieved with negligible impact on model integrity; the maximum accuracy loss reported was a mere 0.086%, with specific instances actually yielding a slight accuracy gain of 0.536%. This research validates the feasibility of aggressive low-bit quantization, providing a scalable blueprint for future CIM toolchains.

---

## Key Findings

*   **Significant Speedup:** Achieves up to a **2.48x** speedup compared to existing state-of-the-art CIM solutions.
*   **Negligible Accuracy Loss:** Maintains model integrity with a maximum accuracy loss of only **0.086%**, with some cases showing slight accuracy gains.
*   **Efficiency Optimization:** Addresses inefficiency by storing weights in a single crossbar cell and reducing compute cycles per MVM.
*   **Compiler Gap:** Highlights that current CIM compilers fail to support quantization below 8 bits despite hardware capabilities being present.
*   **Pareto Superiority:** Demonstrates a superior Pareto frontier for latency versus accuracy trade-offs.

---

## Methodology

The authors propose a **mixed-precision training and compilation framework** specifically designed for RRAM-based Computing-in-Memory architectures. The core of the methodology relies on a **Reinforcement Learning (RL)-based strategy** to navigate the massive search space for quantization parameters.

*   **Autonomous Optimization:** The RL agent autonomously identifies optimal configurations that trade off latency against accuracy.
*   **Framework Integration:** Utilizes the Brevitas framework for Quantization-Aware Training (QAT) and mixed-precision implementation.
*   **Hardware-Aware Constraints:** The optimization process operates under specific hardware constraints, such as fixing the first and last layers at 8-bit to ensure stability.

---

## Technical Details

**Architecture & Structure**
*   **Crossbar Type:** 1T1R (One Transistor, One Resistor) structure.
*   **Mapping Scheme:** Differential mapping (2N cells per column) utilized for high resilience against wire parasitics.
*   **Simulation Specs:** 256x256 crossbar size, 56μs write latency, and 1.4μs MVM latency.

**Computation & Quantization**
*   **Logic:** Computation involves binary inputs and bit-sliced multi-bit weights.
*   **Quantization Type:** Range-based linear quantization (symmetric for weights).
*   **DAC Resolution:** 1-bit DAC resolution.
*   **Bit-Width Constraints:** Weight bit-widths are restricted to multiples of cell resolution.

**Optimization Algorithm**
*   **Strategy:** Reinforcement Learning (enhanced HAQ backend).
*   **Objective:** Find a policy maximizing reward by balancing speed and accuracy.
*   **Efficiency:** Achieved an 8.4% runtime speedup during the RL phase for VGG-16 compared to the original HAQ backend.

---

## Results

*   **Inference Speedup:**
    *   Exceeded **2.20x** against an 8-bit baseline for 4-bit cells.
    *   Peak speedup reached **2.48x**.
*   **Speedup-to-Accuracy-Loss (S/AL) Scores:**
    *   ResNet-18 (2-bit cells): **3.373**
    *   ResNet-18 (4-bit cells): **2.924**
*   **Accuracy Metrics:**
    *   Maximum reported loss: **0.086%**
    *   Best-case accuracy gain: **0.536%**
*   **Model-Specific Outcomes:**
    *   **ViT-B/32:** Dense layers successfully reduced to 4-bit to minimize latency, while MatMul layers largely remained at 8-bit to preserve performance.

---

## Contributions

1.  **Comprehensive Framework:** Introduction of a holistic mixed-precision training and compilation flow bridging hardware capabilities and software support.
2.  **RL-Based Mechanism:** Development of a reinforcement learning-based mechanism to efficiently solve the complex combinatorial problem of selecting quantization parameters.
3.  **Validation of Low-Bit Quantization:** Demonstration of a superior Pareto frontier for latency versus accuracy, proving the feasibility and benefit of low-bit quantization below 8 bits.

---

**Quality Score:** 9/10
**References:** 40 citations