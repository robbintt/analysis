---
title: 'Learn the Ropes, Then Trust the Wins: Self-imitation with Progressive Exploration
  for Agentic Reinforcement Learning'
arxiv_id: '2509.22601'
source_url: https://arxiv.org/abs/2509.22601
generated_at: '2026-02-03T07:08:11'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Learn the Ropes, Then Trust the Wins: Self-imitation with Progressive Exploration for Agentic Reinforcement Learning

*Yulei Qin; Xiaoyu Tan; Zhengbao He; Gang Li; Haojia Lin; Zongyi Li; Zihan Xu; Yuchen Shi; Siqi Cai; Renting Rui; Shaofei Cai; Yuzheng Cai; Xuan Zhang; Sheng Ye; Ke Li; Xing Sun*

***

> ### **QUICK FACTS**
>
> *   **Algorithm:** SPEAR (Self-imitation with Progressive Exploration for Agentic RL)
> *   **Key Innovation:** Curriculum-based exploration-exploitation strategy
> *   **Performance Gain:** Up to **20.7%** in WebShop; Up to **6.1%** in AIME
> *   **Computational Overhead:** 10%-25% extra theoretical complexity (negligible runtime)
> *   **Baseline Architecture:** Dr.BoT
> *   **Quality Score:** 8/10

***

## Executive Summary

This research addresses the critical instability and inefficiency inherent in training Large Language Model (LLM) agents using Reinforcement Learning (RL). Specifically, it tackles the exploration-exploitation trade-off within the Reason-and-Act (ReAct) paradigm, where agents interacting with complex environments often struggle to balance learning new tool interactions (exploration) with refining successful behaviors (exploitation).

Standard RL approaches frequently suffer from **entropy collapse**—becoming too greedy too quickly—or **runaway divergence**, leading to mode collapse and severe distribution shifts. This instability hinders the deployment of reliable agentic systems in industrial settings where consistent performance is paramount.

The paper introduces **SPEAR** (Self-imitation with Progressive Exploration for Agentic RL), a plug-and-play module built upon Self-Imitation Learning (SIL). It utilizes a curriculum-based two-phase strategy:

1.  **Exploration Phase:** Employs intrinsic reward shaping and dynamic entropy management to encourage high-entropy, skill-level discovery of tool interactions.
2.  **Exploitation Phase:** Transitions to a replay buffer to reinforce successful tactics via self-imitation while converging entropy for precise action execution.

By modeling the agent-environment interaction as a Markov Decision Process (MDP) and avoiding mechanical entropy maximization, SPEAR ensures agents "learn the ropes" before "trusting the wins." The study establishes **Dr.BoT** as a robust industrial baseline and validates the efficacy of progressive, curriculum-based exploration strategies for LLM agents.

***

## Key Findings

*   **Significant Performance Uplift:** SPEAR improves performance by up to **20.7%** in agent tasks (WebShop) and **6.1%** in mathematical reasoning (AIME benchmarks).
*   **High Computational Efficiency:** The method requires only **10%-25%** extra theoretical complexity compared to base models, with negligible runtime overhead.
*   **Superior Training Stability:** Offers robust stability by balancing exploration and exploitation without suffering from entropy collapsing or runaway divergence.
*   **Industrial Viability:** Demonstrates a scalable, computationally efficient solution suitable for industrial applications requiring complex reasoning and tool use.

***

## Methodology

The researchers propose **SPEAR**, built upon Self-Imitation Learning (SIL). The framework utilizes a curriculum scheduling strategy divided into two distinct phases:

*   **Exploration Phase:** Utilizes intrinsic reward shaping to encourage diverse tool interactions and skill acquisition.
*   **Exploitation Phase:** Uses a replay buffer to strengthen successful tactics, thereby avoiding mechanical entropy maximization.

This approach effectively decouples skill acquisition from tactic refinement.

***

## Technical Details

**System Architecture & Paradigm**
*   **Framework:** Operates within the Reason-and-Act (ReAct) paradigm using RL to optimize LLM agents.
*   **Modeling:** Models agent-environment interaction as a Markov Decision Process (MDP).
*   **Integration:** Designed as a 'plug-and-play' module that can be integrated into existing systems.

**Core Strategy: Progressive Exploration**
*   **Dynamic Entropy Management:**
    *   *Early Stages:* Prioritize increasing entropy for skill-level exploration.
    *   *Advanced Stages:* Shift to converging entropy for action-based exploration.
*   **Learning Mechanism:** Integrates curriculum-scheduled self-imitation learning (SIL) with intrinsic reward shaping.

**Baseline System**
*   **Dr.BoT:** The baseline architecture used to validate SPEAR. It harmonizes DAPO, Dr.GRPO, and Group Relative Policy Optimization (GRPO).

***

## Contributions

*   **Novel Strategy:** Introduction of a progressive, curriculum-based exploration-exploitation strategy specifically designed for agentic RL.
*   **Industrial Baseline:** Establishment of Dr.BoT as a robust industrial RL baseline for future research.
*   **Comprehensive Validation:** Provided extensive empirical validation across diverse domains, including interactive environments and mathematical reasoning tasks.
*   **Scalability:** Demonstration of a scalable, computationally efficient solution that is viable for real-world industrial applications.

***

## Results

*   **Agentic Tasks:** Achieved a **20.7%** improvement on the WebShop benchmark.
*   **Mathematical Reasoning:** Achieved a **6.1%** improvement on AIME Benchmarks.
*   **Efficiency:** Adds only 10%-25% theoretical complexity with negligible impact on runtime.
*   **Stability Metrics:** Successfully balanced exploration and exploitation, preventing:
    *   Entropy collapsing
    *   Runaway divergence
    *   Severe distribution shifting
    *   Mode collapse

***

**Paper Quality Score:** 8/10  
**References:** 40 citations