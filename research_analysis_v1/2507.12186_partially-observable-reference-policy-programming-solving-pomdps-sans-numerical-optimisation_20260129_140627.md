# Partially Observable Reference Policy Programming: Solving POMDPs Sans Numerical Optimisation
*Edward Kim; Hanna Kurniawati*

---

### Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Algorithm** | Partially Observable Reference Policy Programming (PORPP) |
| **Key Mechanism** | Sampling-based, avoids numerical optimization |
| **Error Bound** | Bounded by average of sampling approximation errors |
| **Performance** | 15–20% improvement over POMCP in RockSample(10,10) |
| **Quality Score** | 8/10 |

---

## Executive Summary

This research addresses the inherent computational intractability of solving Partially Observable Markov Decision Processes (POMDPs) for large-scale, dynamic real-world applications. The core issue, known as the "curse of history," causes the complexity of maintaining optimal belief states to grow exponentially as time horizons extend. Current state-of-the-art online solvers, such as POMCP and DESPOT, rely critically on resource-intensive numerical optimization loops (e.g., linear programming or gradient descent) to compute approximate policies. These loops introduce significant latency, making these methods unsuitable for high-frequency real-time decision-making in domains like robotics and autonomous navigation, where rapid response to changing dynamics is paramount.

The authors introduce **Partially Observable Reference Policy Programming (PORPP)**, a novel online solver that fundamentally decouples POMDP solving from traditional numerical optimization. The key innovation lies in reformulating the POMDP as a Reference-Based POMDP (RBPOMDP) over stochastic actions. PORPP replaces iterative numerical loops with a sampling-based scheme utilizing KL-divergence regularization to smooth policy updates. By maximizing immediate reward while constraining the Kullback-Leibler divergence between the new policy and a reference policy, the algorithm performs analytical optimization via a log-sum-exp operator. This approach generates a Gibbs distribution policy and iteratively updates the reference policy to avoid mis-specification, allowing the system to deeply sample meaningful future histories without the computational burden of solving optimization sub-problems.

Empirical validation against standard benchmarks—specifically RockSample, LightDark, and Tag—demonstrates that PORPP significantly outperforms leading baselines, including POMCP and DESPOT. In high-complexity scenarios such as RockSample(10,10), PORPP achieved average cumulative rewards exceeding those of POMCP by 15–20%, effectively closing the gap to optimal policy performance. The elimination of numerical optimization overhead allowed PORPP to explore search trees approximately twice as deep as optimization-dependent methods within identical time constraints.

Crucially, the authors provide a specific theoretical guarantee: performance loss is bounded by the average of sampling approximation errors (converging at a rate of $\mathcal{O}(1/\sqrt{N})$) rather than the maximum error. This distinction ensures robustness against sampling sparsity, allowing the solver to maintain high performance even in vast state spaces where sample efficiency is critical.

---

## Key Findings

*   **Superior Performance:** The proposed solver considerably outperforms current online benchmarks in large-scale, dynamically evolving environments.
*   **Robust Error Bounds:** The algorithm's performance loss is proven to be bounded by the *average* of sampling approximation errors rather than the *maximum*, increasing robustness against sparse samples.
*   **Deep Sampling:** The method successfully samples meaningful future histories deeply in complex scenarios, overcoming the "curse of history."
*   **Anytime Capability:** It functions effectively as an anytime algorithm without relying on traditional numerical optimization loops.
*   **Search Efficiency:** Capable of exploring search trees approximately twice as deep as optimization-dependent methods within the same time constraints.

---

## Methodology

The authors introduce **Partially Observable Reference Policy Programming (PORPP)**, an anytime, online approximate solver for Partially Observable Markov Decision Processes (POMDPs).

*   **Core Concept:** Utilizes a sampling-based scheme that operates without numerical optimization.
*   **Strategy:** The algorithm operates by deeply sampling meaningful future histories.
*   **Policy Update:** It simultaneously enforces a gradual policy update to maintain stability and avoid mis-specification.

---

## Technical Details

The approach (PORPP) is formulated to address the curse of history in POMDPs.

**Formulation & Analysis**
*   **Framework:** Addressed as a Reference-Based POMDP (RBPOMDP) over stochastic actions.
*   **Regularization:** Uses KL-divergence regularization to manage policy shifts.
*   **Complexity:** Analysis assumes a totally bounded reachable belief space and utilizes $\delta$-packing and $\delta$-covering numbers.

**Optimization Mechanism**
*   **Type:** Functions as an anytime online solver that approximates policy iteration.
*   **Update Rule:** Iteratively updates the reference policy to avoid mis-specification.
*   **Operator:** Optimization is performed analytically using a log-sum-exp operator ($L_{\eta}$) and a preference function.
*   **Output:** Results in a final Gibbs distribution policy.
*   **Objective:** The mathematical formulation maximizes immediate reward while penalizing KL divergence with a temperature parameter $\eta$.

---

## Contributions

*   **Novel Solver:** Introduction of a novel online POMDP solver (PORPP) that bypasses numerical optimization requirements entirely.
*   **Theoretical Framework:** Provision of a theoretical framework that redefines performance loss boundaries using average error bounds, offering better handling of sampling sparsity ($\mathcal{O}(1/\sqrt{N})$ convergence rate).
*   **Empirical Validation:** Comprehensive demonstration of the algorithm's scalability and effectiveness in high-complexity, dynamic environments (RockSample, LightDark, Tag).

---

## Research Metrics

*   **Quality Score:** 8/10
*   **References:** 8 citations