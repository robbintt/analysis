---
title: 'LLM-SrcLog: Towards Proactive and Unified Log Template Extraction via Large
  Language Models'
arxiv_id: '2512.04474'
source_url: https://arxiv.org/abs/2512.04474
generated_at: '2026-01-27T21:58:32'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# LLM-SrcLog: Towards Proactive and Unified Log Template Extraction via Large Language Models

*Jiaqi Sun, Jian Cao, Guangtao Xue, Shanghai Jiao, Wei Li, Shiyou Qian*

---

### ðŸ“Š Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Quality Score** | 9/10 |
| **References** | 40 Citations |
| **Key Datasets** | Hadoop, Zookeeper, Sunfire-Compute |
| **Performance Gain** | +8â€“35% F1-score vs. Traditional Parsers |
| **Speed** | ~1,000Ã— faster than per-log LLM parsing |
| **Approach Type** | Proactive Source-Centric Analysis |

---

## Executive Summary

### ðŸš¨ Problem
Traditional log parsing relies on heuristic-based analysis of raw log data, a reactive approach that often suffers from instability and inaccuracy. Existing state-of-the-art parsers like Drain, Spell, and IPLoM struggle to handle complex semantic scenarios such as string concatenation and conditional logging, frequently requiring extensive manual tuning to maintain acceptable accuracy levels. This fragility limits the reliability of automated log analysis, which is critical for system monitoring, debugging, and anomaly detection in complex software environments.

### ðŸ’¡ Innovation
LLM-SrcLog introduces a paradigm shift by moving from reactive raw log parsing to **proactive extraction directly from source code** before execution. The framework treats template extraction as a unified code-understanding task rather than a string-matching problem. Its architecture integrates a Cross-function Static Code Analyzer to reconstruct logging contexts and an LLM-based White-box Template Extractor that uses prompt engineering to semantically distinguish constant strings from dynamic variables. The design also incorporates a Post-processing Module and a Black-box Template Extractor (utilizing Drain3) to handle scenarios where source code is unavailable, ensuring comprehensive coverage.

### ðŸ“ˆ Results
In evaluations across industrial datasets including Hadoop, Zookeeper, and Sunfire-Compute, LLM-SrcLog demonstrated superior performance, boosting F1-scores by **2â€“17% over LLM-based baselines** and **8â€“35% compared to traditional heuristic parsers**. Crucially, the framework achieved these accuracy gains with remarkable efficiency, operating approximately **1,000Ã— faster** than naive per-log LLM parsing while maintaining latency comparable to traditional data-driven methods. It also exhibited robust generalization across various logging libraries and programming languages without the need for manual tuning.

### ðŸš€ Impact
This research represents a significant advancement in log management by validating proactive extraction as a superior alternative to reactive parsing. By effectively resolving log instability at the source and automating the identification of templates across diverse coding environments, LLM-SrcLog offers a highly scalable solution that addresses the root causes of parsing errors. This approach establishes a new standard for leveraging Large Language Models in system observability, likely influencing future tooling to adopt source-code-centric methodologies for operational intelligence.

---

## Key Findings

*   **Superior Accuracy:** LLM-SrcLog significantly outperforms traditional state-of-the-art parsers (Drain, Spell, IPLoM) in both grouping and parsing accuracy.
*   **Source-Centric Stability:** It resolves the problem of log instability by extracting templates directly from source code rather than inferring them from raw logs.
*   **Robust Generalization:** The approach demonstrates robust generalization capabilities across various logging libraries and programming languages without the need for manual tuning.
*   **Complex Semantic Handling:** It effectively handles complex semantic scenarios that traditionally challenge parsers, such as string concatenation and conditional logging.

---

## Methodology

The proposed methodology shifts the focus from post-hoc log analysis to pre-execution code analysis. The core process involves:

*   **Proactive Analysis:** The system utilizes proactive source code analysis to identify and analyze logging statements *before* the code is executed.
*   **LLM Processing:** It employs Large Language Models (LLMs) to process code snippets. Advanced prompt engineering is used to help the model distinguish between constant strings (which form the static template) and variables (which represent dynamic parameters).
*   **Unified Task:** The method reframes template extraction as a unified code-understanding task, moving away from traditional string-matching heuristics.

---

## Technical Details

LLM-SrcLog is a proactive, source-centric framework that combines white-box and black-box parsing techniques. Its architecture is composed of four main modules:

1.  **Cross-function Static Code Analyzer:**
    *   Responsible for reconstructing the full context of logging statements.
    *   Analyzes code across function boundaries to understand variable scope and data flow.

2.  **LLM-based White-box Template Extractor:**
    *   Leverages the semantic understanding power of LLMs to generate templates.
    *   Uses prompt engineering to semantically differentiate between template constants and dynamic variables.

3.  **Post-processing Module:**
    *   Refines the output of the LLM.
    *   Functions strictly to distinguish constants from variables, ensuring template precision.

4.  **Black-box Template Extractor:**
    *   Utilizes Drain3 (an improved version of the Drain parser).
    *   Acts as a fallback mechanism for scenarios where source code is unavailable, ensuring the framework remains versatile.

---

## Contributions

*   **Paradigm Shift:** The paper proposes a fundamental shift from reactive raw log parsing to proactive extraction from source code.
*   **Novel Framework:** It introduces LLM-SrcLog, the first comprehensive framework utilizing LLMs specifically for log template extraction from source code.
*   **Comprehensive Evaluation:** The authors provide an extensive evaluation on large-scale open-source projects, validating the effectiveness of the proactive approach against traditional heuristic methods.

---

## Results

Evaluations conducted on Hadoop, Zookeeper, and Sunfire-Compute industrial datasets yielded the following outcomes:

*   **Accuracy Improvements:**
    *   **2â€“17%** improvement in F1-scores compared to LLM baselines.
    *   **8â€“35%** improvement compared to traditional parsers (Drain, Spell, IPLoM).
*   **Performance Efficiency:**
    *   Approximately **1,000Ã— faster** than naive per-log LLM parsing.
    *   Latency is comparable to traditional data-driven methods.
*   **Generalization:** Showed strong performance across different environments without requiring manual parameter tuning.