# A Statistical Method for Attack-Agnostic Adversarial Attack Detection with Compressive Sensing Comparison

*Chinthana Wimalasuriya; Spyros Tragoudas*

---

## üìä Quick Facts

| Metric | Details |
| :--- | :--- |
| **Base Model** | ResNet18 |
| **Datasets** | CIFAR-10, CIFAR-100, Truncated TinyImageNet (50 classes) |
| **Detection Rate** | Near-perfect across tested attacks |
| **Key Innovation** | Statistical Class Identity Matching via Compressive Sensing |
| **False Positives** | 0% on CIFAR-10 (Threshold $T=5$) |
| **Attack Types** | FGSM, PGD, Square Attack, DeepFool, Carlini-Wagner |

---

## üìù Executive Summary

> Adversarial attacks pose a critical threat to the reliability of Deep Neural Networks (DNNs), exploiting subtle perturbations in input data to induce misclassification. Current detection mechanisms often struggle with generalization; they are typically tailored to specific, known attack vectors and fail against unseen or "zero-day" threats. Furthermore, state-of-the-art detectors frequently suffer from high false positive rates, which undermines trust and renders them impractical for deployment in safety-critical or high-throughput real-world systems.

This paper introduces a statistical framework for **Attack-Agnostic Adversarial Attack Detection** that utilizes a dual-network architecture comprising a Primary Network (trained on raw images) and a Redundant Network (trained on JPEG2000 compressed images at 80% quality) to suppress adversarial noise.

The core technical novelty, **"Statistical Class Identity Matching,"** operates in two phases. First, during a pre-deployment phase, the system establishes a baseline identity by extracting feature vectors from clean data and calculating distributions using KL Divergence and Mann-Whitney U tests. Second, at runtime, the system augments inputs with random noise and generates distance vectors for both networks. It computes an "**Adversarial Possibility Metric**" ($P_A$), defined as the L2 Norm between the raw network's statistical output and that of the compressed network, identifying attacks based on behavioral disparities rather than known signatures.

The proposed method was validated using a ResNet18 model on CIFAR-10, CIFAR-100, and a truncated TinyImageNet against a comprehensive suite of attacks including FGSM, PGD, Square Attack, DeepFool, and Carlini-Wagner. The compressed network effectively mitigated attack impact; for instance, under an FGSM attack ($\epsilon=0.02$), the raw network accuracy dropped to 62.72% while the compressed network maintained 81.59%.

The primary significance of this research is the delivery of a robust, attack-agnostic defense that remains effective against novel and diverse adversarial strategies without requiring prior knowledge of the attack model. By shifting the computational burden to a pre-deployment baseline phase, the approach offers operational efficiency suitable for real-time monitoring. The drastic reduction in false positives addresses a major barrier to the commercial adoption of adversarial defenses, providing a practical solution for securing deployed machine learning systems in environments where reliability is paramount.

---

## üîë Key Findings

*   **Near-Perfect Detection Accuracy:** The proposed method achieves near-perfect detection rates across a wide variety of adversarial attack types.
*   **Reduction in False Positives:** The approach significantly lowers the rate of false positives compared to existing state-of-the-art techniques, achieving **0% false positives** on the CIFAR-10 dataset.
*   **Real-World Applicability:** The system is reliable for real-world scenarios, offering effective real-time adversarial detection capabilities due to its efficient pre-deployment baseline setup.

---

## üõ†Ô∏è Methodology

The proposed solution utilizes a statistical framework that establishes a detection baseline prior to the neural network's deployment. It generates a specific metric to indicate the presence of adversarial attacks by **quantitatively comparing the behavioral outputs** of a compressed neural network against an uncompressed version of the same network.

This comparison relies on analyzing how the networks diverge from the statistical "identity" of clean data, rather than trying to recognize specific attack patterns.

---

## üìÅ Contributions

*   **Attack-Agnostic Solution:** Addresses the limitation of current methods by developing a technique capable of detecting unseen and diverse attack types with high accuracy, rather than being limited to known threats.
*   **Novel Detection Metric:** Introduces a unique mechanism for detecting adversarial presence based on the disparities in behavior between a compressed/uncompressed neural network pair.
*   **Operational Efficiency:** Enables effective real-time monitoring through a pre-deployment baseline, distinguishing the method as a practical solution for deployed systems.

---

## ‚öôÔ∏è Technical Details

### System Architecture
The approach employs a dual-network architecture:
*   **Primary Network:** Trained on raw images.
*   **Redundant / Denoising Network:** Trained exclusively on **JPEG2000 compressed images (80% quality)** to suppress adversarial noise.

### Core Concept: Statistical Class Identity Matching
The system detects deviations from clean data distributions rather than specific attack models. The process involves two distinct phases:

#### Phase 1: Pre-Deployment Identity Creation
1.  Extracts average feature vectors from the penultimate layer of clean image subsets.
2.  Calculates **symmetric KL Divergence** between training and test sets.
3.  Performs **Mann-Whitney U tests** to determine p-values ($MWid$).
4.  Repeat this process $I$ times to build a robust class distribution identity.

#### Phase 2: Runtime Detection
1.  **Augment** the input with random noise.
2.  Generate distance vectors ($V_R$ and $V_C$) derived from p-values and divergences from both the Raw and Compressed networks.
3.  Compute the **Adversarial Possibility Metric ($P_A$)** as the L2 Norm between these vectors:
    $$P_A = L2Norm(V_R, V_C)$$
4.  **Decision:** A sample is marked adversarial if $P_A$ exceeds a defined threshold $T$.

---

## üìà Results

### Experimental Setup
*   **Datasets:** CIFAR-10, CIFAR-100, Truncated TinyImageNet (50 classes).
*   **Base Model:** ResNet18.
*   **Attacks Tested:** FGSM, PGD, Square Attack, DeepFool, Carlini-Wagner.
*   **Hyperparameters:** 50 identity iterations, sample size of 10,000 images.

### Performance Metrics
**Classification Accuracy (CIFAR-10):**
*   **Clean Data:** Raw Network (91.39%) vs. Compressed Network (90.27%).
*   **Under FGSM Attack ($\epsilon=0.02$):**
    *   Raw Network Dropped to: **62.72%**
    *   Compressed Network Maintained: **81.59%**
    *   *This validates the noise suppression effect of the compression.*

**Detection Thresholds & False Positives:**
*   **CIFAR-10:** Threshold $T=5$ (Result: **0% False Positives**)
*   **CIFAR-100:** Threshold $T=8.3$
*   **TinyImageNet:** Threshold $T=8.7$

The paper reports **near-perfect detection rates** across all attack types with drastically lower false positives compared to state-of-the-art methods.

---

## üìå Evaluation

*   **Quality Score:** 8/10
*   **References:** 2 citations