---
title: Lossless Compression of Large Language Model-Generated Text via Next-Token
  Prediction
arxiv_id: '2505.06297'
source_url: https://arxiv.org/abs/2505.06297
generated_at: '2026-01-27T22:27:28'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# Lossless Compression of Large Language Model-Generated Text via Next-Token Prediction

*Yu Mao, Chun Jason, Holger Pirk (Imperial College, Abu Dhabi)*

---

### ðŸ“Š Quick Facts

| Metric | Value |
| :--- | :--- |
| **Compression Rate** | >20Ã— (LLM) vs ~3Ã— (Gzip) |
| **Performance Gain** | 6.7Ã— improvement over Gzip |
| **Daily Output (ChatGPT)** | ~100 billion words |
| **Projected Volume** | 16,000 TB (within a decade) |
| **Study Scale** | 14 LLMs, 8 Datasets |
| **Top Dataset Categories** | Code (571), Linguistics (471), Art (432) |

---

## Executive Summary

> With the rapid proliferation of Large Language Models (LLMs), the internet is seeing an unprecedented surge in machine-generated content, currently estimated at 100 billion words daily from ChatGPT alone. This explosion poses a critical storage and transmission challenge, particularly as projections indicate LLM-generated data on platforms like HuggingFace could reach 16,000 TB within a decade.
>
> The core issue is that conventional lossless compression algorithms, such as Gzip, are designed for natural human language and rigid structures. They perform poorly on LLM-generated data, which lacks the same redundancy and statistical regularities, rendering traditional methods inefficient for the growing volume of synthetic text.
>
> This paper introduces a novel lossless compression method that repurposes LLMs as highly effective compressors for their own output. The innovation exploits the inherent next-token prediction training objective of LLMs. Specifically, the approach leverages the fact that LLM-generated text exhibits high predictability when fed back into an LLM, effectively creating a "self-compression" loop. The method utilizes the model's probability distribution over tokens as the basis for encoding, transforming the generative capability of the model into a semantic-aware entropy coding mechanism.
>
> In extensive experiments across 14 representative LLMs and 8 distinct datasets spanning Code, Linguistics, Art, and Science, the LLM-based approach demonstrated superior performance. The proposed method achieved compression rates exceeding 20Ã—, significantly outperforming the Gzip baseline, which achieved only approximately 3Ã— compression. This represents a performance improvement factor of 6.7Ã— over the state-of-the-art general-purpose compressor. As LLMs increasingly contribute to significant volumes of scholarly work and code, these findings provide a foundational framework for future data storage infrastructure, highlighting the necessity for model-aware compression techniques in an AI-dominated digital landscape.

---

## Key Findings

*   **Significant Outperformance:** LLM-based prediction methods achieve compression rates exceeding **20Ã—**, significantly outperforming the state-of-the-art lossless compressor Gzip, which achieves approximately **3Ã—**.
*   **High Predictability:** LLM-generated text exhibits high predictability when fed back into LLMs due to next-token prediction training, making the models highly effective compressors for their own output.
*   **Consistent Performance:** The superior performance of LLM-based compressors is consistent across different model scales and diverse dataset categories.
*   **Inefficiency of Traditional Methods:** Conventional lossless compression algorithms are less effective on LLM-generated data due to its complexity and lack of rigid structure.

---

## Methodology

The approach leverages the inherent next-token prediction capability of LLMs to perform lossless compression, treating the model's probability distribution over tokens as the basis for encoding.

*   **Scope:** Extensive experiments were conducted using **14 representative LLMs**.
*   **Datasets:** Evaluated against **8 distinct LLM-generated datasets**.
*   **Benchmarking:** Performance was directly benchmarked against the Gzip compression algorithm to demonstrate relative efficacy.

---

## Technical Details

**Theoretical Foundation**
*   **Core Mechanism:** LLM-based prediction methods for lossless compression.
*   **Principle:** LLMs are effective compressors for their own output due to next-token prediction training.

**Target Data Classification**
*   **Category:** 'LLM-Generated Text' is distinct from conventional machine-generated data (due to semantic context) and human data (due to automatic, high-volume generation).
*   **Taxonomy:** The architecture is designed to handle a diverse taxonomy of data, focusing on:
    *   Code
    *   Linguistics
    *   Art
    *   Science

---

## Results

*   **Compression Factor:** The LLM-based compressor achieves compression rates exceeding **20Ã—**.
*   **Comparative Performance:** Significantly outperforms the Gzip baseline (~3Ã—) by a factor of **6.7Ã—**.
*   **Data Volume Metrics:**
    *   ChatGPT produces approximately **100 billion words daily**.
    *   Over **1%** of 2023 scholarly articles (17.5% of CS papers, 16.9% of peer reviews) were written with LLM assistance.
*   **Growth Projections:** Estimates suggest LLM-generated data on HuggingFace will grow to **16,000 TB** within a decade.
*   **Dataset Distribution:**
    *   Code: 571
    *   Linguistics: 471
    *   Art: 432

---

## Contributions

*   **First Investigation:** This work represents the first investigation into effective lossless compression methods specifically designed for LLM-generated data.
*   **Validation of Predictability:** The study identifies and validates the high predictability of LLM-generated text via next-token prediction as a unique property enabling superior compression.
*   **Benchmark Establishment:** The paper establishes that LLM-based compressors can outperform traditional general-purpose compressors like Gzip by a significant margin (6.7Ã—).

---
**Quality Score:** 9/10 | **References:** 40 citations