---
title: 'TokMem: Tokenized Procedural Memory for Large Language Models'
arxiv_id: '2510.00444'
source_url: https://arxiv.org/abs/2510.00444
generated_at: '2026-02-03T12:27:01'
quality_score: 6
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# TokMem: Tokenized Procedural Memory for Large Language Models

*Zijun Wu; Yongchang Hao; Lili Mou*

---

> ### üìÑ Executive Summary
> 
> Large Language Models (LLMs) face significant inefficiencies when handling recurring procedural tasks. Current solutions, such as relying on implicit knowledge storage or external retrieval-augmented generation (RAG), are burdened by repeated context overhead. This results in quadratic computational costs due to self-attention over long contexts, substantial retrieval latency, and constraints from finite context windows. Furthermore, standard fine-tuning is parameter-inefficient and prone to catastrophic interference.
> 
> **TokMem** addresses these challenges by introducing a tokenized procedural memory framework that stores recurring procedures as compact, trainable embedding vectors. The architecture utilizes "memory tokens" serving as dual-purpose encodings for addressing and control signals. Crucially, the Transformer backbone remains frozen while only memory embeddings are trained. During inference, TokMem achieves **O(1) procedural invocation overhead**, avoiding the cost of re-processing long contexts. Stability in continual learning is maintained through a renormalization process that aligns new embeddings with existing ones. Evaluations on **Qwen (0.5B)** and **Llama (up to 8B)** demonstrate that TokMem consistently outperforms RAG on atomic and compositional tasks, offering a scalable, low-overhead mechanism for dynamic procedural execution.

---

### üöÄ Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Models Tested** | Qwen (0.5B), Llama (up to 8B) |
| **Key Datasets** | Super-Natural Instructions, Function-calling dataset |
| **Computational Complexity** | O(1) Procedural Invocation |
| **Trainable Parameters** | Memory Embeddings Only (Backbone Frozen) |
| **Quality Score** | 6/10 |
| **References** | 40 Citations |

---

### üîë Key Findings

*   **Superior Performance:** TokMem consistently outperforms Retrieval-Augmented Generation (RAG) on both atomic recall and compositional function-calling tasks.
*   **Zero Context Overhead:** The approach eliminates repeated context overhead by maintaining a **constant-size overhead** during generation.
*   **High Parameter Efficiency:** Achieves efficiency by keeping the backbone model frozen and utilizing significantly fewer parameters than standard fine-tuning methods.
*   **Continual Adaptation:** Successfully supports the addition of new procedures without causing interference with existing memories (catastrophic forgetting).

---

### üß† Methodology

The authors introduce **TokMem**, a framework designed to equip LLMs with explicit, tokenized procedural memory.

*   **Core Concept:** Recurring procedures are stored as compact, trainable embeddings rather than text within the context window.
*   **Memory Tokens:** These function as dual-purpose encodings, containing both an "address" (to locate the procedure) and a "control signal" (to guide execution).
*   **Frozen Backbone:** The large language model's parameters remain frozen. Only the memory embeddings are trained, allowing for modular injection.
*   **Modular Injection:** During inference, specific memory tokens are injected into the model to enable targeted behaviors based on the input query.

---

### ‚öôÔ∏è Technical Details

The TokMem architecture is built for efficiency and stability in dynamic environments.

**Architecture**
*   **Memory Bank:** A collection of trainable embedding vectors.
*   **Encoding:** Each procedure is encoded as a single vector associated with a special index.
*   **Backbone:** The Transformer structure remains static (frozen).

**Training**
*   **Optimization:** Only memory embeddings are updated.
*   **Loss Function:** Utilizes standard next-token prediction loss on interleaved query and response sequences.

**Inference**
*   **Efficiency:** Achieves **O(1) procedural invocation overhead**.
*   **Mechanism:** The query is used to internally recall specific memory tokens, bypassing the need to re-process long procedural contexts and avoiding quadratic computational costs.

**Continual Learning Stability**
*   **Renormalization:** A specific process rescales new active embeddings to align with the scale of inactive memories.
*   **Goal:** Prevents new procedures from dominating the model's attention or interfering with established knowledge.

---

### üìä Experimental Results

**Experimental Setup**
*   **Models:** Qwen (0.5B) and Llama family (up to 8B parameters).
*   **Tasks:**
    *   **Atomic Memory Recall:** Tested using the Super-Natural Instructions dataset.
    *   **Compositional Memory Recall:** Tested using a custom function-calling dataset.

**Outcomes**
*   **Performance:** TokMem demonstrated superior performance compared to RAG across both evaluation scenarios.
*   **Memory Footprint:** Confirmed elimination of repeated context overhead and maintenance of a constant-size footprint during generation.
*   **Scalability:** Validated the ability to add new procedures continually without degrading performance on existing tasks.

---

### üèÜ Contributions

1.  **Novel Architecture:** Establishes a system providing LLMs with **explicit procedural memory**, moving beyond implicit knowledge storage.
2.  **Scalable Alternative:** Introduces a modular alternative to prompt engineering and fine-tuning, allowing specific skills to be managed independently.
3.  **Optimization:** Develops a solution specifically optimized for recurring procedural tasks, addressing the scaling limitations and computational costs of standard prompts.