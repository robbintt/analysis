# PARQ: Piecewise-Affine Regularized Quantization

*Lisa Jin; Jianhao Ma; Zechun Liu; Andrey Gromov; Aaron Defazio; Lin Xiao*

---

### ðŸ“Š Quick Facts

| **Metric** | **Detail** |
| :--- | :--- |
| **Quality Score** | 8/10 |
| **Citations** | 9 |
| **Optimization** | Aggregate Proximal (AProx) |
| **Key Result** | **82.3%** Top-1 on Swin-Base (W3/A3) |
| **Theoretical Basis** | Convex, piecewise-affine regularization |

---

## Executive Summary

This research addresses the theoretical and computational limitations of current Quantization-Aware Training (QAT) methods for large-scale deep learning models. While standard techniques like the Straight-Through Estimator (STE) are ubiquitous in industry, they rely on heuristics that suffer from optimization issues such as parameter trapping and lack a rigorous mathematical foundation explaining their convergence. The core challenge for technical audiences is the absence of a principled optimization framework that can effectively drive continuous weights toward discrete, low-bit values necessary for efficient deployment without sacrificing model accuracy.

The key innovation is the **PARQ framework**, which formulates quantization as a convex optimization problem using a piecewise-affine regularizer (PAR) rather than non-convex regularization strategies. The approach minimizes an objective defined as $\min f(w) + \Psi(w)$, where $\Psi(w)$ is a convex, nonsmooth function designed to force weights to cluster at precise quantization levels. To solve this, the authors introduce the **Aggregate Proximal (AProx) Stochastic Gradient method**, which treats the proximal map of PAR as a 'soft quantization' operator. The authors provide a significant theoretical bridge by proving that the widely used STE is actually the asymptotic form of their proposed PARQ method and offer a proof of last-iterate convergence for the AProx optimizer under convex loss conditions.

Experimental evaluations demonstrate that PARQ achieves competitive or superior performance compared to established baselines across both CNNs and Vision Transformer architectures. On ImageNet classification with ResNet-18 using 2-bit weights and 4-bit activations, PARQ achieves a top-1 accuracy of **69.3%**, outperforming the LSQ baseline. For ResNet-50 under the same aggressive quantization settings (W2/A4), PARQ maintains high performance with **75.7%** accuracy. The framework also proves effective on transformers; on the Swin-Base model with 3-bit weights and activations, PARQ attains **82.3%** top-1 accuracy, significantly closing the gap with the full-precision model and exceeding standard heuristic methods.

The significance of this paper lies in its establishment of a mathematically rigorous foundation for quantization, moving the field away from heuristic-based tuning toward optimization-centric strategies. By formalizing the convex link between regularization and STE, the authors validate a ubiquitous industry practice with solid theoretical evidence. The introduction of the PAR regularizer and the AProx optimizer provides researchers with a robust tool for model compression that minimizes the performance gap between full-precision and quantized models.

---

## Key Findings

*   **Effective Regularization:** Convex, piecewise-affine regularization (PAR) successfully induces model parameters to cluster toward discrete values.
*   **Optimizer Convergence:** The aggregate proximal stochastic gradient method (AProx) achieves **last-iterate convergence** when minimizing PAR-regularized loss functions.
*   **Theoretical Validation:** The straight-through estimator (STE) is established mathematically as the **asymptotic form** of the proposed PARQ method.
*   **Broad Applicability:** PARQ demonstrates competitive performance on vision tasks involving both convolutional and transformer architectures without the trapping issues common in nonconvex methods.

---

## Methodology

The authors introduce a framework for Quantization-Aware Training (QAT) focused on large-scale models. The approach involves:

1.  **Objective Function:** Minimizing a loss function regularized by a convex, piecewise-affine regularizer (PAR).
2.  **Optimization Strategy:** Solving the objective using an aggregate proximal stochastic gradient method (AProx).
3.  **Discretization:** Driving parameters toward discrete clusters without relying on heuristics, using the proximal map as a 'soft quantization' operator.

---

## Contributions

1.  **Principled Alternative:** Provides a mathematically principled alternative to heuristic-based quantization by formalizing how regularization forces parameter discretization.
2.  **Theoretical Bridge:** Offers a theoretical bridge validating the Straight-Through Estimator (STE) as the asymptotic limit of the PARQ algorithm.
3.  **New Optimizer:** Contributes the **AProx optimizer**, tailored for this regularization landscape with a proof of last-iterate convergence.

---

## Technical Details

*   **Framework:** Principled Quantization-Aware Training (QAT) based on convex optimization.
*   **Objective:**
    $$ \min f(w) + \Psi(w) $$
    *   $f(w)$: The loss function.
    *   $\Psi(w)$: A convex and nonsmooth **Piecewise-Affine Regularizer** designed to force weights to cluster at quantization levels.
*   **Algorithm:** **Aggregate Proximal (AProx) Stochastic Gradient Method**.
*   **Key Properties:**
    *   The proximal map of PAR acts as a 'soft quantization' operator.
    *   The method proves **last-iterate convergence** for convex losses.
    *   Optimality conditions are defined to encourage parameter clustering at discrete values.

---

## Results

Experimental findings indicate that the PARQ method demonstrates competitive performance compared to standard approaches (STE/BinaryConnect) and post-training quantization baselines. The experiments cover vision tasks involving both CNNs and Vision Transformers.

**Performance Highlights:**
*   **ResNet-18 (W2/A4):** Achieved **69.3%** top-1 accuracy, outperforming the LSQ baseline.
*   **ResNet-50 (W2/A4):** Achieved **75.7%** top-1 accuracy, maintaining high performance in aggressive low-bit regimes.
*   **Swin-Base (W3/A3):** Achieved **82.3%** top-1 accuracy, significantly closing the gap with the full-precision model and exceeding standard heuristic methods.

The authors claim their approach effectively induces clustering toward discrete values without the trapping issues of nonconvex methods, achieving mild performance degradation even in low-bit regimes.