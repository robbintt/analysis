# On the Adversarial Robustness of Learning-based Conformal Novelty Detection

*Daofu Zhang; Mehrdad Pournaderi; Hanne M. Clifford; Yu Xiang; Pramod K. Varshney*

---

### ðŸ“Š Quick Facts & Key Metrics

| **Metric** | **Detail** |
| :--- | :--- |
| **Target Framework** | AdaDetect (Conformal Prediction) |
| **Focus Area** | False Discovery Rate (FDR) Guarantees |
| **Attack Setting** | Black-box (Query access only) |
| **Attack Algorithms** | HSJA, Boundary Attack |
| **Primary Metric** | FDR (Baseline $\alpha=0.1$) |
| **Max FDR Degradation** | Up to $\sim0.71$ (from $\sim0.08$) |
| **Datasets Tested** | Credit Card, Shuttle, KDDCup99, Mammography |

---

## Executive Summary

This research exposes a critical vulnerability in AdaDetect, a state-of-the-art framework used for novelty detection in safety-critical systems like medical diagnosis and fraud detection. AdaDetect relies on conformal prediction to guarantee a specific False Discovery Rate (FDR)â€”essentially providing a mathematical promise that only a small percentage of alerts will be false positives. The paper demonstrates that these statistical guarantees are fragile; when subjected to adversarial perturbations, the systemâ€™s error control mechanisms collapse. This is a vital concern because operators in high-stakes environments rely on FDR guarantees to trust automated anomaly scores. If an attacker can artificially induce a high volume of false positives, they can undermine the integrity of the entire decision-making pipeline without triggering standard security alarms.

The authors introduce a dual-method approach to validate these vulnerabilities, moving from theoretical bounds to practical exploits. First, they established an "oracle attack setting" to mathematically derive the upper bound of damageâ€”the "statistical cost"â€”quantifying the absolute worst-case degradation of FDR controls. To demonstrate this is not merely theoretical, they developed a practical surrogate attack scheme that operates in a black-box setting. This approach requires only query access to the detectorâ€™s output labels, eliminating the need for internal model knowledge. The attackers train a surrogate model to mimic the target detectorâ€™s behavior and then use optimization algorithms to generate adversarial inputs. This strategy effectively bridges the gap between abstract mathematical possibility and real-world exploitability.

Experimental validation on synthetic and real-world benchmarks confirms severe degradation of statistical safety. Under idealized "oracle" attacks, the FDR skyrocketed from a baseline of ~0.08 to between 0.67 and 0.71. The practical surrogate attacks yielded similarly catastrophic results; for instance, on the Credit Card dataset, FDR rose from 0.08 to 0.64, and on KDDCup99, from 0.04 to 0.67. Most critically, the attacks identified a dangerous failure mode: while FDR increased drastically, the model's detection power (true positive rate) also increased. In the Mammography dataset, detection power jumped from 0.48 to 0.80 while false positives surged. This creates a deceptive scenario where the system appears to perform better due to higher detection rates, while actually suffering from catastrophic inflation of false alarms.

This study represents the first systematic investigation into the adversarial robustness of conformal novelty detection, revealing a fundamental fragility in error-controlled methods. The findings expose a severe real-world danger: a **Denial-of-Service via False Alarms** strategy. By pushing FDR levels to 60-70%, an attacker can flood an organizationâ€™s monitoring systems with erroneous alerts. This induces "alert fatigue," causing human operators to ignore the system entirely, which in turn allows actual threats to pass unnoticed. Consequently, this research establishes a critical benchmark for the field and necessitates a paradigm shift toward "robust novelty detection" algorithms capable of preserving statistical integrity under active siege.

---

## Key Findings

*   **Vulnerability of FDR Guarantees:** While AdaDetect provides rigorous statistical guarantees under normal conditions, adversarial perturbations can significantly degrade its False Discovery Rate (FDR) control.
*   **Sustained Detection Power:** The attack schemes successfully increase the FDR (false positives) *without* compromising the model's high detection power (true positives), creating a particularly dangerous failure mode.
*   **Theoretical Bound on Statistical Cost:** The research establishes an upper bound that quantifies the "statistical cost" of an attack, defining the worst-case degradation potential of FDR.
*   **Fundamental Limitations Exposed:** The study reveals that current error-controlled novelty detection methods possess inherent weaknesses against adversarial examples.

---

## Methodology

The study follows a structured pipeline from theoretical formulation to empirical validation:

1.  **Target Analysis**:  
    Focuses specifically on **AdaDetect**, a learning-based framework for novelty detection that utilizes conformal prediction to ensure finite-sample FDR control.

2.  **Theoretical Formulation**:  
    The authors first establish an idealized **"oracle attack setting"** to quantify worst-case FDR degradation and derive theoretical upper bounds on the statistical impact of attacks.

3.  **Practical Attack Implementation**:  
    Motivated by the theoretical formulation, the researchers developed a practical, effective attack scheme that operates with **only query access** to the detector's output labels (black-box setting).

4.  **Evaluation Protocol**:  
    The vulnerability of AdaDetect was systematically assessed by coupling the proposed formulations with two popular, complementary black-box adversarial algorithms. The experiments were conducted on both synthetic and real-world datasets.

---

## Contributions

*   **Novelty in Robustness Analysis:** This work presents the first investigation into the adversarial robustness of learning-based conformal novelty detection, specifically targeting the statistical guarantees of AdaDetect.
*   **Theoretical Framework:** The paper introduces a novel "oracle attack setting" which provides a theoretical upper bound on the statistical cost of adversarial perturbations on FDR-controlled systems.
*   **Practical Attack Construction:** The authors bridge the gap between theory and practice by proposing a query-based attack scheme that exploits the derived bounds, demonstrating that theoretical vulnerabilities are exploitable in real-world scenarios.
*   **Benchmark for Future Research:** By systematically exposing the fragility of error-controlled methods, the paper establishes a critical benchmark and motivates the development of robust novelty detection algorithms.

---

## Technical Details

**System Overview**
AdaDetect is a learning-based novelty detection framework using conformal p-values for finite-sample False Discovery Rate (FDR) control. It relies on data exchangeability under the null hypothesis.

**Attack Formulations**
*   **Oracle Attack:** An idealized worst-case scenario assuming full knowledge of the detector to quantify maximum FDR degradation.
*   **Surrogate Attack:** A practical approach using query access and a PU (Positive-Unlabeled) learning framework to train surrogate score functions. This attack relies on **HSJA** (Hop-Skip-Jump Attack) and **Boundary Attack** algorithms.

**Statistical Bound**
The authors derived a theoretical upper bound for the statistical cost (FDR degradation) expressed as:

$$ \alpha + m_a \cdot \frac{1}{20} \sum_{i=1}^{20} \left( \frac{1}{e^{R(i)}} \vee 1 \right) $$

*   Where $\alpha$ is the target significance level (set to $0.1$ in experiments).
*   $m_a$ represents the number of adversarial instances.

**Experimental Setup**
*   **Datasets:** Synthetic (Gaussian/Non-Gaussian), Credit Card, Shuttle, KDDCup99, Mammography.
*   **Models:** Random Forest ($n=5000, m=1000, m_a=200$).

---

## Results

**FDR Degradation**
*   **Oracle Attacks:** Increased FDR from a baseline of ~0.08 to a range of **0.67 â€“ 0.71**.
*   **Surrogate Attacks:** Showed significant FDR increases across all datasets:
    *   **Credit Card:** 0.08 $\rightarrow$ 0.64
    *   **KDDCup99:** 0.04 $\rightarrow$ 0.67

**Detection Power**
*   Detection Power generally increased alongside FDR. For example, in the **Mammography** dataset, power increased from 0.48 to 0.80.
*   **Performance Note:** Boundary attacks often outperformed HSJA in this context.

**Key Implication**
The attacks induce a dangerous failure mode where the **FDR skyrockets (up to 70%)** while Power increases. This makes the system appear superficially better ("more detections") while actually suffering from a massive inflation of false alarms, potentially deceiving operators.

---
*Document Quality Score: 9/10 | References: 40 citations*