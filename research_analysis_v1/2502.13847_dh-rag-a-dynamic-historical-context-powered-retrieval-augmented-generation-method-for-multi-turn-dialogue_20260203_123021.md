---
title: 'DH-RAG: A Dynamic Historical Context-Powered Retrieval-Augmented Generation
  Method for Multi-Turn Dialogue'
arxiv_id: '2502.13847'
source_url: https://arxiv.org/abs/2502.13847
generated_at: '2026-02-03T12:30:21'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# DH-RAG: A Dynamic Historical Context-Powered Retrieval-Augmented Generation Method for Multi-Turn Dialogue

*Feiyuan Zhang; Dezhi Zhu; James Ming; Yilun Jin; Di Chai; Liu Yang; Han Tian; Zhaoxin Fan; Kai Chen*

---

> **QUICK FACTS**
> *   **Core Innovation:** Dynamic Historical Information database vs. Static Knowledge Bases.
> *   **Key Performance:** 215.38% relative increase in BLEU score on MobileCS2 compared to Self-RAG.
> *   **Reasoning Depth:** Average Chain of Thought (CoT) length of 1.73 steps.
> *   **Hardware Setup:** 8 NVIDIA 3090 GPUs.
> *   **Quality Score:** 9/10

---

## Executive Summary

Traditional Retrieval-Augmented Generation (RAG) systems predominantly rely on static external knowledge bases, creating a significant "context gap" in multi-turn dialogue scenarios. Because these systems treat each query in isolation, they fail to effectively utilize the evolving information and conversational history necessary to maintain coherence over long interactions. This limitation results in responses that lack relevance and logical continuity as the dialogue progresses, making it difficult for standard RAG architectures to handle the complex, context-dependent nature of human-like conversation.

To address this, the paper proposes **DH-RAG**, a bi-component architecture inspired by human cognitive processes that utilizes a Dynamic Historical Information database rather than static knowledge alone. The system introduces a **History-Learning based Query Reconstruction Module** that employs Historical Query Clustering, Hierarchical Matching, and Chain of Thought (CoT) Tracking—with an average reasoning chain length of 1.73 steps—to reformulate queries based on past context. This is paired with a **Dynamic History Information Updating Module** that continuously refreshes the historical context as the dialogue progresses, ensuring that both the retrieval and generation processes are informed by the conversation's cumulative history.

Experimental evaluations demonstrate that DH-RAG significantly outperforms conventional Large Language Models (LLMs) and standard RAG baselines like Self-RAG across multiple benchmarks. In domain-specific QA (MobileCS2), DH-RAG achieved a BLEU score of 4.10 and an F1 score of 27.83, representing relative increases of 215.38% and 58.13%, respectively, compared to Self-RAG. On modified open-domain QA (PopQA Mod), it scored a BLEU of 49.20 and an F1 of 68.76. While the model incurs a higher computational cost (peak memory ~4.2 GB vs. ~3.0 GB for Self-RAG), ablation studies validate the necessity of the design; removing the Dynamic History Information Updating Module caused BLEU scores to crash from 44.99 to 0.43 on TopiOCQA, highlighting the critical importance of the dynamic updating mechanism. This research bridges the context gap inherent in traditional RAG systems by validating that dynamic historical information provides substantial benefits over static knowledge base reliance.

---

## Key Findings

*   **Performance Superiority:** DH-RAG significantly outperforms conventional models and standard RAG baselines across multiple benchmark tests.
*   **Quality Enhancement:** The system enhances dialogue quality metrics, specifically improving **response relevance** and **coherence**.
*   **Validation of Dynamic Context:** The study validates that leveraging dynamic historical information provides substantial benefits over traditional RAG methods that rely solely on static knowledge bases.

---

## Methodology

DH-RAG utilizes a bi-component architecture centered on a **Dynamic Historical Information database**, inspired by human cognitive processes.

*   **History-Learning based Query Reconstruction Module:** Employs specific techniques to reformulate queries based on past context:
    *   Historical Query Clustering
    *   Hierarchical Matching
    *   Chain of Thought (CoT) Tracking
*   **Dynamic History Information Updating Module:** Continuously refreshes the historical context during dialogues to ensure relevance.

---

## Technical Details

### Core Approach
DH-RAG (Dynamic Historical Context-Powered RAG) is designed specifically for multi-turn dialogue. It utilizes dynamic historical context rather than relying on static knowledge bases alone, integrating historical dialogue information directly into the retrieval and generation process.

### Key Architectural Components

| Component | Function |
| :--- | :--- |
| **History-Learning based Query Reconstruction** | Utilizes a historical database to reformulate queries based on past context. |
| **Result Integration Component** | Combines retrieved information with the current dialogue context. |
| **Dynamic History Information Updating** | Updates the historical context dynamically as the dialogue progresses. |
| **Hierarchical Matching** | Used for precise matching of queries with relevant documents. |
| **Chain of Thought (CoT) Tracking** | Implements a reasoning process with an average chain length of **1.73 steps** (peaking at 2-3 steps). |

### Experimental Setup
*   **Hardware:** 8 NVIDIA 3090 GPUs
*   **Baselines:**
    *   *Standard LLMs:* Llama 2 7B, Llama 3 70B, Mistral 7B, ChatGPT 4o-mini
    *   *RAG Methods:* BM25, Self-RAG

---

## Performance Analysis

### Benchmark Results
DH-RAG showed robust performance across various domains, with particularly strong results in domain-specific and modified open-domain QA tasks.

| Benchmark | BLEU Score | F1 Score | Notes |
| :--- | :--- | :--- | :--- |
| **MobileCS2** (Domain-Specific) | **4.10** | **27.83** | +215.38% BLEU / +58.13% F1 vs SelfRAG |
| **PopQA Mod** (Open-Domain) | **49.20** | **68.76** | High performance on modified open QA |
| **TriviaQA Mod** | **30.97** | **57.20** | Strong retrieval and generation |
| **CoQA** (Conversational) | **12.86** | **32.57** | Competitive in conversational flows |
| **TopiOCQA** (Conversational) | **10.06** | **40.56** | Maintains context over open topics |

### Efficiency Analysis
While performance improved, computational overhead increased compared to Self-RAG.

| Metric | DH-RAG | Self-RAG (Baseline) |
| :--- | :--- | :--- |
| **Runtime** | 1198.37s | 1125.85s |
| **Memory Usage (Avg)** | 2186.9 MB | 1725.9 MB |
| **Memory Usage (Peak)** | **4192.5 MB** | 3018.6 MB |
| **Retrieval Time (Avg)** | 0.25s | 0.15s |

---

## Ablation Study

The study analyzed the impact of removing specific modules to verify their necessity. The **Dynamic History Information Updating Module** proved to be the most critical component.

| Component Removed | Dataset | BLEU Drop | Impact |
| :--- | :--- | :--- | :--- |
| **Result Integration** | PopQA | 44.99 $\rightarrow$ 5.38 | Severe degradation in synthesis. |
| **Dynamic History Updating** | PopQA | 44.99 $\rightarrow$ 3.87 | Critical for maintaining context. |
| **Dynamic History Updating** | TopiOCQA | 11.58 $\rightarrow$ 0.43 | Catastrophic failure without updates. |
| **Chain of Thought (CoT)** | PopQA | 44.99 $\rightarrow$ 23.62 | Significant drop in reasoning. |
| **Chain of Thought (CoT)** | TopiOCQA | 11.58 $\rightarrow$ 5.70 | Reduced coherence. |
| **Hierarchical Matching** | PopQA | 44.99 $\rightarrow$ 43.88 | Minimal impact. |

---

## Contributions & Conclusion

### Research Contributions
1.  **Bridging the Context Gap:** Addresses the limitations of traditional RAG systems by utilizing dynamic historical information.
2.  **Novel Framework:** Proposes a dual-module framework that mimics human cognitive processes.
3.  **Advancement in Multi-Turn Dialogue:** Establishes a new technical approach for maintaining context and coherence in long conversations.

### Conclusion
By establishing a novel technical approach for maintaining context and coherence, DH-RAG advances the field of multi-turn dialogue systems and offers a robust framework that mimics human cognitive processes. The findings provide a foundation for future developments in conversational AI where context retention and dynamic reasoning are paramount.

---

**Quality Score:** 9/10 | **References:** 40 citations