# Quantile-Optimal Policy Learning under Unmeasured Confounding

*Zhongren Chen; Siyu Chen; Zhengling Qi; Xiaohong Chen; Zhuoran Yang*

---

> ### üìä Quick Facts
>
> *   **Quality Score:** 8/10
> *   **References:** 40 Citations
> *   **Convergence Rate:** $\tilde{\mathscr{O}}(n^{-1/2})$
> *   **Key Technique:** Causal-assisted framework using Instrumental Variables (IV) & Negative Controls (NC)
> *   **Primary Context:** Offline Reinforcement Learning & Causal Inference

---

## üìù Executive Summary

### **The Problem**
This research tackles a critical challenge in offline decision-making: learning optimal policies when observational data is plagued by **unmeasured confounding**. Standard methods often fail here because they rely on the assumption that all relevant confounders are measured (ignorability). The complexity is further compounded by the objective of optimizing for **quantiles** (e.g., minimizing downside risk) rather than just average outcomes. This requires navigating a triad of obstacles:
1.  Bias from unobserved confounders.
2.  The inherent non-linearity of quantile functions.
3.  Insufficient data coverage (overlap) that risks evaluating policies on underrepresented actions.

### **The Innovation**
The authors propose a **"causal-assisted" framework** that integrates instrumental variables (IV) and negative controls (NC) to identify causal effects despite unobserved confounders. Technically, the approach transforms quantile objectives into **nonlinear functional integral equations**, solved via minimax estimation and nonparametric modeling.

To handle data sparsity, the framework employs a **pessimistic principle**, constructing conservative lower-bound estimates of policy value and selecting policies that maximize these bounds. Crucially, to solve computational intractability, a specific regularization method is introduced to constrain the optimization space, allowing efficient solving of high-dimensional nonparametric problems.

### **The Results**
The paper establishes rigorous theoretical guarantees, demonstrating that the proposed methods achieve **near-parametric convergence rates**. Specifically:
*   **Convergence Rate:** $\tilde{\mathscr{O}}(n^{-1/2})$
*   **Optimality Gap:** $\tilde{O}(n^{-1/2})$

These results hold under mild coverage assumptions, confirming that the method is both robust to noise/sparsity and sample-efficient without requiring prohibitively large datasets.

### **The Impact**
This work bridges nonparametric causal inference and offline reinforcement learning, providing one of the first sample-efficient algorithms for quantile optimization in confounded settings. It has significant practical value in high-stakes fields where minimizing tail risks is essential:
*   **Healthcare:** Determining treatment protocols that minimize severe adverse events, even with unrecorded patient factors.
*   **Economics:** Designing policies that guard against financial downside risk using historical data with hidden influencers.

---

## üîë Key Findings

*   **Sample-Efficient Optimality:** The proposed causal-assisted methods achieve a convergence rate of $\tilde{\mathscr{O}}(n^{-1/2})$ for quantile-optimal policy estimation under mild coverage assumptions.
*   **Resolution of Unobserved Confounding:** The framework successfully addresses unobserved confounding by integrating instrumental variables and negative controls.
*   **Robustness to Coverage Issues:** The proposed methods mitigate risks associated with insufficient coverage through conservative (pessimistic) policy estimates.
*   **Computational Tractability:** A novel regularized policy learning method ensures the approach is computationally feasible.

---

## üß© Methodology

The researchers propose a **'causal-assisted' framework** for offline policy learning in the presence of unmeasured confounders. The approach is structured as follows:

1.  **Causal Identification:**
    Utilization of instrumental variables (IV) and negative controls (NC) to formulate valid quantile objectives despite the presence of hidden confounders.

2.  **Equation Transformation:**
    Quantile objectives are transformed into nonlinear functional integral equations. These are solved via minimax estimation and nonparametric models.

3.  **Coverage Mitigation:**
    To address insufficient data coverage (low overlap), the method constructs conservative estimates. The final policy is selected by maximizing these pessimistic estimates.

4.  **Computational Optimization:**
    Introduction of a novel regularized policy learning method to enhance computational amenity and handle the complexity of the optimization space.

---

## üìÅ Technical Details

| Component | Description |
| :--- | :--- |
| **Core Objective** | Quantile-optimal policy learning in offline settings with unobserved confounders. |
| **Primary Challenges** | Non-linearity of quantile objective, unobserved confounding bias, insufficient data coverage. |
| **Causal Tools** | Uses Instrumental Variables (IV) and Negative Controls (NC) to disentangle causal relationships. |
| **Estimation Strategy** | Solves nonlinear functional integral equations via **minimax estimation** and **nonparametric models**. |
| **Overlap Strategy** | Employs the **principle of pessimism**; constructs conservative policy estimates and maximizes lower-bound estimates. |
| **Computational Innovation** | Introduces a novel regularization method to the policy learning process for computational tractability. |
| **Theoretical Extension** | Extends existing nonparametric IV and NC frameworks to handle nonlinear quantile functions. |

---

## üèÜ Contributions

*   **Novel Framework for Confounded Data:** Establishes one of the first sample-efficient algorithms for learning quantile-optimal policies in the presence of unmeasured confounding.
*   **Integration of Causal Inference and Offline RL:** Contributes a unique theoretical synthesis employing instrumental variables and negative controls to solve nonlinear functional integral equations.
*   **Theoretical Guarantees:** Provides rigorous theoretical proofs establishing near-parametric convergence rates ($\tilde{\mathscr{O}}(n^{-1/2})$), addressing a critical gap in the theoretical understanding of quantile optimization under confounding.

---

## üìà Results

Theoretical results indicate the proposed methods achieve a convergence rate of $\tilde{\mathscr{O}}(n^{-1/2})$ for quantile-optimal policy estimation, with an optimality gap of $\tilde{O}(n^{-1/2})$.

*   **Assumptions:** These guarantees hold under a mild coverage assumption on the offline dataset.
*   **Robustness:** The framework successfully resolves unobserved confounding bias and mitigates risks associated with insufficient data coverage.
*   **Significance:** The authors assert this is the first work to propose sample-efficient policy learning algorithms for estimating quantile-optimal policies in the presence of unmeasured confounding.