---
title: 'EffGen: Enabling Small Language Models as Capable Autonomous Agents'
arxiv_id: '2602.00887'
source_url: https://arxiv.org/abs/2602.00887
generated_at: '2026-02-03T12:36:21'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# EffGen: Enabling Small Language Models as Capable Autonomous Agents

*Gaurav Srivastava; Aafiya Hussain; Chi Wang; Yingyan Celine Lin; Xuan Wang*

---

> ### ðŸ“Š Quick Facts
>
> *   **Benchmarks Evaluated:** 13
> *   **Context Compression Rate:** 70â€“80%
> *   **SLM Performance Gain:** +11.2% (at 1.5B parameters)
> *   **LLM Performance Gain:** +7.9% (at 32B parameters)
> *   **Routing Improvement:** Outperforms baselines by 6.4â€“11.2%
> *   **Quality Score:** 9/10
> *   **Total Citations:** 40

---

## Executive Summary

While Large Language Models (LLMs) have demonstrated significant potential as autonomous agents, their deployment is often constrained by high costs, latency, and privacy concerns associated with reliance on cloud APIs. Small Language Models (SLMs) offer a promising alternative for local, privacy-preserving execution but typically lack the context window and reasoning capabilities required for complex agentic behaviors, particularly in tool-calling and task decomposition.

This paper addresses the critical challenge of bridging this performance gap, aiming to enable resource-constrained SLMs to function as capable, efficient autonomous agents without external dependencies. The authors introduce **effGen**, an open-source framework designed around a four-pronged architecture optimized for SLM constraints. The core innovation is an SLM-aware prompt optimization module that utilizes a five-step pipeline to reduce token usage by **70â€“80%** while maintaining semantic integrity.

The framework employs a complexity-based routing mechanism that evaluates tasks based on five factors to dynamically select execution paths before runtime. In comparative evaluations across 13 benchmarks, effGen outperformed established agent frameworks including LangChain, AutoGen, and Smolagents, demonstrating superior success rates, faster execution speeds, and lower memory consumption.

---

## Key Findings

*   **Benchmark Superiority:** The effGen framework outperforms established agent frameworks (LangChain, AutoGen, and Smolagents) across **13 benchmarks**, demonstrating higher success rates, faster execution speeds, and reduced memory consumption.
*   **SLM Optimization:** Prompt optimization provides significant performance gains for Small Language Models, specifically an **11.2% improvement** at the 1.5B parameter scale.
*   **LLM Routing Benefits:** Complexity-based routing benefits Large Language Models more substantially, yielding a **7.9% improvement** at the 32B parameter scale.
*   **Combined Synergy:** When combined, prompt optimization and complexity routing provide consistent performance gains across all model scales (from 1.5B to 32B parameters).
*   **Context Compression:** The framework achieves a **context compression rate of 70â€“80%** through prompt optimization while successfully preserving the semantic integrity of the tasks.

---

## Methodology

The researchers developed **effGen**, an open-source agentic framework specifically engineered to optimize Small Language Models (SLMs) for local deployment. The methodology employs a four-pronged technical architecture:

1.  **Prompt Optimization:** Enhances tool-calling capabilities by compressing prompts.
2.  **Task Decomposition:** Intelligently breaks down complex queries into parallel or sequential subtasks based on dependency analysis.
3.  **Complexity-Based Routing:** Utilizes a five-factor analysis system for pre-execution decisions.
4.  **Unified Memory System:** Integrates short-term, long-term, and vector-based storage.

Additionally, the approach unifies multiple agent protocols (**MCP, A2A, ACP**) to facilitate cross-protocol communication.

---

## Technical Details

### Agent Definition
EffGen treats Small Language Model (SLM) constraints as primary design requirements, defining an agent as a tuple:
$$A = (M, T, R, D, \mathcal{M}, \phi)$$

### Prompt Optimization ($\phi$)
The framework employs SLM-aware prompt optimization with specific parameters for different model sizes (TINY, SMALL, MEDIUM, LARGE) using a five-step pipeline:
1.  Compression
2.  Simplification
3.  Redundancy removal
4.  Bullet formatting
5.  Context truncation

### Complexity Analyzer & Routing
*   **Analyzer ($C(q)$):** Uses weighted factors (task length, requirements, domain breadth, tools, reasoning depth) to predict task difficulty.
*   **Routing Logic ($R$):** Selects execution paths based on complexity scores:
    *   SINGLE
    *   PARALLEL
    *   SEQUENTIAL
    *   HIERARCHICAL

### Task Decomposition ($D$)
Performed locally using strategies such as parallel, sequential, hierarchical, or hybrid execution.

---

## Contributions

*   **Comprehensive Framework:** Introduction of a comprehensive, open-source framework that enables SLMs to function as capable autonomous agents, offering a cost-effective and privacy-preserving alternative to API-dependent LLM systems.
*   **Novel Tool-Calling:** A novel mechanism for tool-calling that drastically reduces token usage (**70-80% compression**) while maintaining task semantics.
*   **Dynamic Decomposition:** A task decomposition strategy that dynamically determines parallel versus sequential execution based on subtask dependencies.
*   **Routing Model:** A complexity-based routing model that uses five distinct factors to guide pre-execution decisions.
*   **Unified Systems:** The development of a unified memory system and a protocol-agnostic communication layer that bridges MCP, A2A, and ACP protocols.

---

## Performance Results

EffGen outperforms established frameworks (LangChain, AutoGen, Smolagents) across 13 benchmarks, demonstrating faster execution speeds and reduced memory consumption.

*   **Cost Scaling:** Expected cost scales as $O(|M| \cdot L \cdot k)$.
*   **Component Impact:**
    *   **11.2% improvement** from prompt optimization at the 1.5B scale.
    *   **7.9% improvement** from complexity routing at the 32B scale.
    *   The prompt optimization module achieves a **70â€“80% context compression rate** while preserving semantic integrity.
*   **Routing Efficiency:** Complexity-based routing outperforms baseline strategies by **6.4â€“11.2%**.

---

**Paper Quality Score:** 9/10 | **References:** 40 citations