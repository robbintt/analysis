---
title: 'MoR: Mixture Of Representations For Mixed-Precision Training'
arxiv_id: '2512.22804'
source_url: https://arxiv.org/abs/2512.22804
generated_at: '2026-02-03T19:35:58'
quality_score: 8
citation_count: 30
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# MoR: Mixture Of Representations For Mixed-Precision Training

*Bor-Yiing Su; Peter Dykas; Mike Chrzanowski; Jatin Chhugani*

***

> ### **Quick Facts**
> *   **Model Tested:** Nemotron-3 8B
> *   **Quantization Achievement:** 98.38% of tensors in FP8
> *   **Training Scale:** 1 Trillion tokens
> *   **Core Innovation:** Group Amax Mantissa (GAM) Scaling
> *   **Target Formats:** E4M3, E5M2, BF16
> *   **Framework:** Megatron-LM

***

## Executive Summary

Training large language models (LLMs) necessitates substantial computational resources, driving the adoption of mixed-precision training using low-bit formats like FP8. However, aggressively reducing tensor precision often degrades model accuracy due to the presence of numerical outliers. Existing solutions typically preserve quality by employing fine-grained partitioning strategies to isolate these outliers, but these methods introduce significant implementation complexity that hinders hardware adoption. This paper addresses the challenge of maximizing low-precision utilization to improve training efficiency while maintaining model quality, specifically by achieving high accuracy without relying on complex architectural partitioning.

The authors propose **Mixture-of-Representations (MoR)**, a dynamic quantization framework that analyzes tensor properties to select appropriate numerical formats in real-time. The core innovation is **Group Amax Mantissa (GAM) Scaling**, which decouples the mantissa and exponent of scaling factors to handle outliers effectively. By combining the wide dynamic range of E8M0 with high-precision mantissa characteristics, GAM prevents numerical saturation without necessitating separate data paths for outliers. MoR utilizes concrete algorithms to dynamically iterate from aggressive formats like FP8 to conservative fallbacks like BF16 based on acceptance metrics such as mean relative error. While the framework supports operation at both tensor and sub-tensor levels, it is designed to eliminate the need for the complex fine-grained partitioning typically required to manage outlier values.

The framework was evaluated on the Nemotron-3 8B model within the Megatron-LM framework, training on 1 Trillion tokens across Nemotron4 and NemotronH datasets. Experiments demonstrated that MoR successfully quantized **98.38% of tensors** to the FP8 format (E4M3/E5M2) while maintaining baseline model quality. The method matched the performance of state-of-the-art techniques that require fine-grained partitioning, achieving robust results across various learning rates and batch sizes. Notably, this high level of precision was achieved with a simplified implementation profile compared to existing complex methods.

This research advances efficient AI training by proving that high-accuracy, low-precision training does not depend on fine-grain partitioning architectures. By simplifying the handling of outliers through GAM scaling, MoR improves the robustness of mixed-precision training and reduces hardware implementation barriers. The framework establishes a pathway for adopting future ultra-low-precision formats, such as NVFP4, offering a scalable solution for reducing the computational cost of developing next-generation foundation models.

***

## Key Findings

*   **High Quantization Efficiency:** The proposed MoR framework achieves state-of-the-art results by successfully quantizing **98.38%** of tensors to the FP8 format while maintaining model quality.
*   **Simplified Architecture:** The approach demonstrates that high-accuracy FP8 training can be achieved without the need for fine-grain partitioning, matching the performance of existing complex methods.
*   **Dynamic Adaptation:** Dynamic, property-aware quantization proves effective in preserving model quality across various datasets and quantization partition strategies.
*   **Future-Proofing:** The framework shows potential to improve the robustness of low-precision training and can be combined with other methods to leverage even lower precision formats like NVFP4.

***

## Methodology

The researchers developed **Mixture-of-Representations (MoR)**, a novel quantization framework designed for mixed-precision training. The methodology operates at both per-tensor and sub-tensor levels, utilizing a dynamic analysis process to evaluate a tensor's numerical properties.

Based on these properties, concrete algorithms automatically select the most appropriate numerical representation in real-time, choosing dynamically between FP8 and BF16 formats. This universal approach is designed to be adaptable across different quantization partition strategies and datasets.

***

## Contributions

*   **Framework Introduction:** Introduced MoR, a preliminary framework for dynamic, property-aware quantization that functions at both per-tensor and sub-tensor granularities.
*   **Dynamic Algorithms:** Proposed and validated specific algorithms that dynamically switch between FP8 and BF16 based on tensor analysis rather than static assignment.
*   **Efficiency Maximization:** Demonstrated a method to significantly increase the proportion of tensors using low-precision formats (FP8) to over 98% without sacrificing model quality or relying on fine-grain partitioning.
*   **Pathway to Lower Precision:** Highlighted a pathway toward future training methods by showing the approach's compatibility with emerging lower-precision number formats such as NVFP4.

***

## Technical Details

The paper proposes MoR (Mixture Of Representations), a framework for mixed-precision training.

*   **Core Innovation: Group Amax Mantissa (GAM) Scaling**
    *   Decouples the mantissa and exponent of scaling factors.
    *   Combines the wide dynamic range of E8M0 with high-precision mantissa characteristics.
    *   Operates at both group and block levels to prevent saturation.

*   **Dynamic Selection Strategy**
    *   MoR dynamically selects quantization strategies for tensor partitions.
    *   Iterates from aggressive (e.g., E4M3) to conservative (e.g., BF16) formats based on acceptance metrics.

*   **Granularity Strategies**
    *   **Tensor-Level MoR:** Uses a single type per tensor based on mean relative error.
    *   **Sub-Tensor-Level MoR:** Uses mixed types per block, potentially requiring upcasting for hardware compatibility.

*   **Target Formats**
    *   Primary low-precision targets: **E4M3** and **E5M2**.
    *   Fallback format: **BF16**.

***

## Results & Evaluation

Experiments were performed on the **Nemotron-3 8B** model within the **Megatron-LM** framework.

*   **Scope:** Fake quantization was applied to linear layers (QKV, Projection, FC1, FC2) for activations, weights, and gradients.
*   **Training Configuration:** The model was trained for **1 Trillion tokens** using two configurations (Nemotron4 and NemotronH datasets) with varying learning rates and batch sizes.
*   **Outcome:** The MoR framework successfully quantized 98.38% of tensors to FP8 format while maintaining model quality.
*   **Comparison:** It matched the performance of complex existing methods requiring fine-grained partitioning, achieving state-of-the-art results with improved robustness.

***

**Document Statistics**
*   **Quality Score:** 8/10
*   **References:** 30 citations