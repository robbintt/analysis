---
title: 'PLANET: A Collection of Benchmarks for Evaluating LLMs'' Planning Capabilities'
arxiv_id: '2504.14773'
source_url: https://arxiv.org/abs/2504.14773
generated_at: '2026-02-03T13:38:25'
quality_score: 8
citation_count: 26
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# PLANET: A Collection of Benchmarks for Evaluating LLMs' Planning Capabilities

*Haoming Li; Zhaoliang Chen; Jonathan Zhang; Fei Liu*

---

> ### ðŸ“Š Quick Facts & Key Metrics
>
> *   **GPT-4 Success Rate (WebArena):** 14% vs. 78% Human Baseline
> *   **WebShop Scale:** 1.18 million products & 12,087 instructions
> *   **VirtualHome Programs:** 5,193 synthetic & 2,821 human programs
> *   **TravelPlanner Capacity:** 4 million entries & 1,225 queries
> *   **VisualWebArena Score:** ~25.2
> *   **PlanBench Instances:** 600 Blocksworld instances
> *   **Reference Count:** 26 citations

---

## Executive Summary

The field of Large Language Model (LLM) planning currently suffers from a fragmented understanding of evaluation standards, creating significant barriers to comparing algorithm performance across domains. Without a comprehensive categorization of the diverse testbeds in use, it is difficult to isolate specific deficiencies in agent reasoning capabilities. This paper addresses this gap by introducing **PLANET**, a systematic taxonomy of planning benchmarks designed to clarify the landscape, facilitate standardized comparisons, and highlight critical deficiencies in existing evaluation methodologies.

The key innovation of this work is the PLANET framework, a taxonomy that organizes benchmarks based on functional categories and specific architectural requirements rather than merely listing tasks. Technically, the study grounds planning in Markov Decision Processes (MDPs) and categorizes environmentsâ€”embodied settings, web navigation, scheduling, games and puzzles, and task automationâ€”by analyzing their distinct state-action spaces and interaction modalities. For instance, the framework distinguishes between environments requiring discrete symbolic manipulation, such as embodied tasks, and those demanding multimodal processing, such as visual web navigation. This structural logic provides a methodology for mapping algorithmic strengths to the specific constraints and demands of each domain.

The study provides a detailed statistical breakdown of major datasets and reveals significant performance gaps between current models and human baselines. Dataset statistics include VirtualHome (5,193 synthetic programs), WebShop (1.18 million products), and TravelPlanner (4 million entries). Performance benchmarks indicate a GPT-4-based agent achieved only a 14% success rate on WebArena compared to a 78% human baseline. Further testing shows that LLM scheduling performance degrades as complexity increases, while results on VisualWebArena yielded a task-specific score of 25.2, underscoring the persistent challenges of multimodal planning and complex reasoning.

The PLANET framework serves as a vital roadmap for the future development of LLM planning agents by providing clear guidance on selecting appropriate benchmarks for evaluating specific algorithm types. By identifying gaps in the current landscape and demonstrating that optimal planning strategies generally require fewer resources than ad-hoc methods, this work directs the field toward more efficient and robust architectural designs. This comprehensive synthesis establishes a foundational reference for standardizing the evaluation of planning capabilities, ultimately accelerating progress in creating agents capable of complex, multi-step reasoning.

---

## Key Findings

*   **Fragmented Landscape:** There is currently a lack of comprehensive understanding regarding existing planning benchmarks, making cross-domain algorithm performance comparison difficult.
*   **Systematic Categorization:** Existing benchmarks can be systematically organized into five distinct domains:
    *   Embodied environments
    *   Web navigation
    *   Scheduling
    *   Games and puzzles
    *   Everyday task automation
*   **Identified Gaps:** The analysis revealed potential deficiencies within the current landscape of planning testbeds used for algorithm development.
*   **Resource Efficiency:** Optimal planning strategies generally require fewer computational resources than ad-hoc methods.

---

## Methodology

The authors conducted a comprehensive examination and analysis of a wide range of existing planning benchmarks. The study focused on:

1.  **Identification:** Pinpointing commonly used testbeds for algorithm development.
2.  **Organization:** Classifying these testbeds into specific functional categories.
3.  **Assessment:** Evaluating the current state of the field to identify deficiencies and gaps in testing methodologies.

---

## Core Contributions

*   **Categorization Framework:** A structured classification of planning benchmarks into five key areas (embodied, web, scheduling, games/puzzles, automation).
*   **Algorithm Recommendations:** Guidance on selecting the most appropriate benchmarks for evaluating various types of planning algorithms.
*   **Future Development Roadmap:** Insights and recommendations designed to guide the creation and improvement of future planning benchmarks.

---

## Technical Details

### Formal Definition
Planning is formally defined as a Markov Decision Process (MDP) with state transitions modeled as:
$$p_\theta(s_{t+1}|s_t, a_t)$$

### Architectural Domains

*   **Embodied Environments**
    *   *Blocksworld:* Utilizes discrete actions (e.g., pick-up, stack).
    *   *VirtualHome:* Uses symbolic programs for simulation.
    *   *ALFWorld / TextWorld:* Interaction via text commands.

*   **Web Navigation**
    *   *WebArena / VisualWebArena:* Decomposes goals into interactions.
    *   *Requirement:* VisualWebArena requires multimodal architectures to process both visual and textual inputs.

*   **Scheduling**
    *   *TravelPlanner:* Chains LLMs with external tools/APIs to handle complex constraints.

*   **Games and Puzzles**
    *   *Strategy:* Employs **Randomized Strategic Trace Pruning** using A* Search traces.
    *   *Masking Strategy (4-Level):*
        *   **D1:** Masks close nodes.
        *   **D2:** Adds cost nodes.
        *   **D3:** Adds create nodes.
        *   **D4:** Masks the entire trace.

---

## Performance Results

**Dataset Statistics:**
*   **VirtualHome:** 2,821 human programs, 5,193 synthetic programs.
*   **WebShop:** 1.18 million products, 12,087 instructions.
*   **WebArena:** 812 tasks.
*   **VisualWebArena:** 910 tasks.
*   **TravelPlanner:** 4 million entries, 1,225 queries.
*   **PlanBench:** 600 Blocksworld instances.

**Performance Benchmarks:**
*   **WebArena:** GPT-4-based agent achieved a **14%** success rate versus a **78%** human baseline.
*   **Scheduling:** LLM performance degrades significantly as problem complexity increases.
*   **VisualWebArena:** Reported a performance metric of approximately **25.2**.

---

**Document Quality Score:** 8/10