# Community size rather than grammatical complexity better predicts Large Language Model accuracy in a novel Wug Test
*Nikoleta Pantelidou; Evelina Leivada; Raquel Montero; Paolo Morosi*

---

> ### üìä Quick Facts
>
> *   **Quality Score:** 8/10
> *   **Models Evaluated:** 6 (ChatGPT-3.4, ChatGPT-4, Grok 3, DeepSeek, Mistral, BERT*)
> *   **Languages Studied:** Catalan, English, Greek, Spanish
> *   **Methodology:** Multilingual Wug Test
> *   **Key Predictor:** Community Size > Linguistic Complexity
> *   **Statistical Tool:** R (v4.5.1) Mixed-effects logistic regression
> *   **References:** 40 Citations
>
> *\*BERT was excluded from analysis as an outlier.*

---

## üìã Executive Summary

This research addresses the fundamental question of whether Large Language Models (LLMs) possess a genuine linguistic competence that mirrors human understanding, or if their apparent proficiency is merely a byproduct of statistical pattern matching on massive datasets. Specifically, the paper investigates conflicting theories regarding the drivers of LLM accuracy: does performance correlate with the inherent structural complexity of a language, or is it primarily determined by the volume of available training data (community size)?

To disentangle these variables, the authors developed a multilingual adaptation of the "Wug Test," evaluating six LLMs across four typologically distinct languages while benchmarking results against human speaker data. The study found that while aggregated LLM performance did not differ significantly from human baselines ($p=0.77$), **community size** was identified as the superior predictor of accuracy over grammatical complexity (AIC: 277.858 vs. 290.659). Notably, models significantly outperformed humans in English ($p=0.0081$) but struggled with lower-resource Catalan.

These findings refine the theoretical debate regarding LLM cognitive capabilities by establishing that model accuracy is heavily contingent upon data resource wealth rather than an inherent sensitivity to grammatical structure. By highlighting the performance gap in lower-resource languages, the study underscores the pervasive issue of data inequality, warning that high performance in high-resource languages should not be extrapolated as evidence of universal linguistic competence.

---

## üîç Key Findings

*   **Human-like Morphological Generalization:** LLMs demonstrated the ability to generalize morphological processes to unseen words with accuracy comparable to human speakers.
*   **Predictors of Accuracy:** Model accuracy aligned more closely with the size of the linguistic community and the availability of training data than with the grammatical complexity of the language.
*   **Resource Disparity:** High-resource languages (**Spanish** and **English**) yielded significantly higher accuracy than lower-resourced languages (**Catalan** and **Greek**).
*   **Driver of Performance:** The findings suggest that LLM performance is driven primarily by the richness of linguistic resources rather than an inherent sensitivity to grammatical complexity, resulting in a competence that resembles human linguistic behavior only superficially.

---

## üß™ Methodology

The study employed a rigorous experimental design to isolate the factors influencing LLM morphological generalization:

*   **Task Design:** A multilingual adaptation of the classic "Wug Test" was utilized to assess morphological generalization capabilities using novel words (nonce words).
*   **Models and Languages:** The research evaluated six distinct Large Language Models across four partially unrelated languages: **Catalan**, **English**, **Greek**, and **Spanish**.
*   **Benchmarking:** Model results were directly compared against performance data from human speakers to evaluate the approximation of human competence.

---

## ‚öôÔ∏è Technical Details

| Component | Description |
| :--- | :--- |
| **Models Tested** | ChatGPT-3.4, ChatGPT-4, Grok 3, BERT (excluded as outlier), DeepSeek, Mistral |
| **Languages** | Catalan, English, Greek, Spanish |
| **Task** | Nominal inflection of nonce words (Wug Test adaptation) |
| **Scoring Metric** | Binary accuracy based on morphophonological rules |
| **Complexity Metrics** | Fusion and Informativity (Structural dimensions) |
| **Community Size** | Proxy for training data availability |
| **Analysis Software** | R (v4.5.1) |
| **Statistical Method** | Mixed-effects logistic regression with random effects for Participants, Items, and Models |
| **Model Selection** | Akaike Information Criterion (AIC) |
| **Post-hoc Analysis** | Tukey adjustments |

---

## üìà Results

The analysis provided clear statistical insights into the performance gap between humans and models across different languages.

*   **Human Baselines:** Significant cross-linguistic variation was observed.
    *   Lowest: English (**84.8%**)
    *   Highest: Spanish (**95.7%**)
*   **LLM vs. Human Performance:** Aggregate LLM performance (excluding BERT) did not differ significantly from humans ($p=0.77$). **ChatGPT-4** specifically matched human baselines.
*   **Language-Specific Performance:** A significant interaction revealed that LLMs outperformed humans in **English** ($p=0.0081$) but not in other languages. LLMs struggled specifically with **Catalan** compared to English and Spanish.
*   **Predictive Factors:** Both Linguistic Complexity and Community Size were significant predictors of accuracy. However, **Community Size was the better predictor** (lower AIC: 277.858 vs 290.659).
*   **Interaction Effect:** A significant interaction indicated that for languages with similar community sizes, higher complexity predicted better performance.

---

## üí° Contributions

*   **Refining Theoretical Claims:** The study refines previous literature by quantifying the impact of data availability (community size) versus structural complexity on LLM performance, establishing community size as the superior predictor.
*   **Distinction Between Competence and Performance:** It contributes to the ongoing debate regarding LLM linguistic abilities by clarifying that high performance in generalization tasks may reflect data-driven pattern matching rather than a deep sensitivity to grammatical structure.
*   **Cross-Linguistic Evaluation:** By including languages with varying levels of digital resources (Spanish/English vs. Catalan/Greek), the paper provides empirical evidence of how data inequality affects model efficacy in low-resource linguistic contexts.