# Dynamic Sparse Attention on Mobile SoCs

*Wangsong Yin; Daliang Xu; Mengwei Xu; Gang Huang; Xuanzhe Liu*

<div align="center">

### ðŸ“Š Quick Facts

| Metric | Value |
| :--- | :--- |
| **Quality Score** | 8/10 |
| **References** | 40 Citations |
| **Avg Accuracy Drop (Standard NPU)** | 18 Percentage Points (pp) |
| **Max Accuracy Drop (Qwen2-1.5B)** | 25.5 pp |
| **Sparsity Threshold** | >80% of attention scores < 0.01 |

</div>

***

## Executive Summary

> **The Challenge:** Deploying Large Language Models (LLMs) on mobile System-on-Chips (SoCs) faces a critical "fallback" bottleneck. While mobile NPUs are efficient for dense matrix multiplication, attention mechanisms suffer from quantization sensitivity under INT8 precision. This forces frameworks to offload attention operations to general-purpose CPU or GPU cores, degrading user experience and complicating system scheduling.
>
> **The Solution:** This paper introduces **shadowAttn**, a system-algorithm co-designed sparse attention module tailored for mobile NPUs. Instead of full attention calculation, shadowAttn dynamically identifies and processes only critical tokens. Its core innovation is "NPU-based pilot compute," which utilizes the NPU to mask the overhead of token estimationâ€”a process that typically causes latency spikes in sparse attention.
>
> **The Impact:** Experimental results show that forcing attention onto the NPU without shadowAttn causes an average accuracy drop of 18 percentage points compared to Float32 baselines. shadowAttn successfully achieves performance parity with state-of-the-art frameworks while significantly reducing reliance on CPU/GPU resources, validating the necessity of software-hardware co-design for edge AI.

***

## Key Findings

*   **The "Fallback" Phenomenon:** Current state-of-the-art frameworks face a significant bottleneck where the attention operator falls back from the specialized NPU to general-purpose CPU/GPU due to quantization sensitivity, resulting in degraded user experience.
*   **Efficiency via Sparsity:** The proposed system, **shadowAttn**, achieves high efficiency by calculating attention sparsely on only a tiny portion of tokens, rather than processing the full matrix.
*   **Overhead Hiding:** The system effectively hides the overhead of estimating important tokens by utilizing an NPU-based "pilot compute," a strategy necessary to avoid the latency penalties seen in naive sparse attention.
*   **Resource Optimization:** shadowAttn delivers performance comparable to state-of-the-art frameworks while utilizing significantly fewer CPU/GPU resources, easing system scheduling constraints.

***

## Methodology

The authors propose **shadowAttn**, a system-algorithm co-designed sparse attention module specifically tailored for the constraints of Mobile SoCs. The methodology focuses on shifting the computational burden from general-purpose processors (CPU/GPU) to the NPU through the following strategies:

*   **Sparse Calculation:** Instead of full attention calculation, the system performs sparse calculations solely on identified important tokens.
*   **NPU-based Pilot Compute:** To accomplish this efficiently, the approach employs a "pilot compute" phase on the NPU to mask the cost of token estimation.
*   **Hardware-Aware Optimizations:** The implementation utilizes specific optimization techniques to balance computational load and model accuracy:
    *   NPU compute graph bucketing
    *   A head-wise NPU-CPU/GPU pipeline
    *   Per-head fine-grained sparsity ratios

***

## Technical Details

### System Constraints
The approach addresses specific Mobile NPU limitations that hinder current LLM deployment:
*   **Static Graphs:** Difficulty handling dynamic computation patterns.
*   **INT8 Focus:** Hardware optimized primarily for 8-bit integers.
*   **Quantization Sensitivity:** Forcing attention operations onto the NPU results in accuracy degradation.

### The shadowAttn Solution
The proposed solution utilizes an NPU-centric sparse attention architecture employing 'Pilot Compute' to hide overhead.

#### Workflow Stages
1.  **Estimation Stage:** Calculates attention scores and identifies top-k important tokens.
2.  **Attention Stage:** Computes the final output by processing only the retained tokens identified in the previous stage.

***

## Results

Experiments reveal the severity of the quantization issue and the performance characteristics of the proposed solution:

### Accuracy Degradation (Standard NPU vs. Float32 CPU/GPU)
Forcing NPU-based attention without specialized handling results in significant accuracy loss:
*   **Average Drop:** 18 percentage points (pp)
*   **PhoneLM-0.5B:** 14.7 pp drop
*   **Qwen2-1.5B:** 25.5 pp drop

### Sparsity Analysis
*   High skewness in attention was confirmed, with over **80% of scores below 0.01**. This validates the feasibility of pruning computations to only the most critical tokens.

### Latency Performance
*   **Naive Sparse Attention:** Fails to improve end-to-end performance (latency ranging **0.32s to 0.73s**) because the Estimation Stage overhead is too high.
*   **shadowAttn:** Successfully addresses this latency through pilot compute and architectural optimizations.

***

## Contributions

*   **Problem Identification:** Identified and analyzed the "fallback" phenomenon in mobile LLMs, where quantization sensitivity forces the attention operator off specialized NPU hardware.
*   **System Design:** Designed shadowAttn, a co-designed sparse attention module that maintains high accuracy through minimal reliance on CPU/GPU resources.
*   **Architectural Innovation:** Introduced specific innovations for handling sparse attention on hardware-constrained devices:
    *   NPU compute graph bucketing
    *   Head-wise pipelining
    *   Pilot compute mechanisms
*   **Performance Validation:** Demonstrated a system that achieves parity with state-of-the-art framework performance while operating under highly limited CPU/GPU resource constraints.