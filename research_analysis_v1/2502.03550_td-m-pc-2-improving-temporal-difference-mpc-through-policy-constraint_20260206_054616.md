---
title: 'TD-M(PC)$^2$: Improving Temporal Difference MPC Through Policy Constraint'
arxiv_id: '2502.03550'
source_url: https://arxiv.org/abs/2502.03550
generated_at: '2026-02-06T05:46:16'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# TD-M(PC)$^2$: Improving Temporal Difference MPC Through Policy Constraint

*Haotian Lin; Pengcheng Wang; Jeff Schneider; Guanya Shi*

---

> ### âš¡ Quick Facts
>
> * **Quality Score:** 9/10
> * **References:** 40 Citations
> * **Core Problem:** Value overestimation in MBRL due to policy mismatch
> * **Primary Metric:** Value Approximation Error
> * **Key Performance Gains:** Error reduced from 2159% (baseline) to stable levels in 61-DoF tasks

---

## Executive Summary

This research addresses a critical instability issue in Model-Based Reinforcement Learning (MBRL) algorithms that integrate planning with learned value or policy priors. Specifically, methods relying on standard Soft Actor-Critic (SAC)-style policy iteration often suffer from persistent and severe value overestimation. The authors demonstrate that this is not merely a numerical artifact but a fundamental structural mismatch between the data generation policy used by the planner and the learned policy prior used for value estimation. This issue poses a significant barrier to scaling MBRL to complex, high-dimensional control tasks where accurate value estimation is essential for convergence.

The authors propose **TD-M(PC)$^2$**, a minimalist modification to the existing "TD-MPC2" framework that introduces a policy regularization term into the learning objective. Technically, this term constrains the policy by penalizing "Out-Of-Distribution (OOD)" queries, thereby forcing the policy to remain closer to the data distribution generated by the planner. This reduction in "OOD" queries mitigates the structural policy mismatch identified in the analysis. The method is implemented as a low-overhead addition to existing architectures that utilize Model Predictive Path Integral (MPPI) control and Temporal Difference learning, requiring no additional computational resources while theoretically bounding the performance gap via distributional shift analysis.

The proposed method achieves substantial performance improvements over the strong TD-MPC2 baseline, particularly in high-dimensional environments. The results highlight a severe scalability issue in existing approaches: while low-dimensional tasks like Hopper-Stand (4-DoF) exhibit a manageable 15% value approximation error, high-dimensional tasks show catastrophic overestimation. Specifically, baseline errors reached 231% for Dog-Trot (36-DoF) and a staggering 2159% for the h1hand-run-v0 humanoid task (61-DoF). TD-M(PC)$^2$ effectively mitigates this overestimation, stabilizing value learning and enabling successful convergence in complex 61-Degree-of-Freedom control scenarios where the baseline fails.

---

## Key Findings

*   **Structural Policy Mismatch:** Existing MBRL algorithms combining planning with learned value/policy priors suffer from persistent value overestimation. This issue is theoretically and empirically linked to a structural mismatch between the data generation policy and the learned policy prior.
*   **Mitigation via Regularization:** Implementing a simple policy regularization term effectively reduces Out-Of-Distribution (OOD) queries. This mitigates the mismatch and stabilizes value learning.
*   **High-Dimensional Performance:** The proposed method achieves significant performance improvements over strong baselines like TD-MPC2, with particularly large gains observed in complex 61-Degree-of-Freedom (DoF) humanoid control tasks.

---

## Methodology

The authors propose a minimalist modification to existing model-based reinforcement learning frameworks (specifically those relying on SAC-style policy iteration).

1.  **Objective:** Introduce a policy regularization term into the learning objective to constrain the policy.
2.  **Mechanism:** Reduce out-of-distribution (OOD) queries to address the structural mismatch between the planner's data generation and the learned policy prior.
3.  **Implementation:** The method is implemented as a low-overhead addition to existing architectures, requiring no additional computational resources.

---

## Contributions

*   **Root Cause Analysis:** Provides a rigorous analysis identifying the structural policy mismatch between planner-bootstrapped data and learned policy priors as the fundamental cause of value overestimation in current MBRL methods.
*   **TD-M(PC)$^2$ Algorithm:** Introduces a novel policy constraint strategy that improves value learning efficiency by minimizing OOD queries without adding computational burden.
*   **Benchmarking:** Demonstrates that the proposed regularization technique substantially outperforms the current state-of-the-art (TD-MPC2) in continuous control tasks, setting a new benchmark for high-dimensional control.

---

## Technical Details

### Architecture
*   **Base Framework:** TD-MPC2.
*   **Components:** Integrates a learned world model with Model Predictive Path Integral (MPPI) control and Temporal Difference learning.
*   **Learned Components:** Jointly learns an encoder, latent dynamics, a reward model, and an action-value function.

### Training & Loss
*   **Loss Function:** Composite loss involving dynamics reconstruction (MSE) and cross-entropy losses for reward and Q-values.
*   **Optimization:** Employs stop-gradients on target states.

### Theoretical Analysis
*   **Policy Mismatch:** Identifies a structural issue where the data collection policy and the value estimation policy differ, leading to distributional shift.
*   **Solution (TD-M(PC)$^2$):** Introduces a policy regularization term to penalize Out-Of-Distribution queries.
*   **Theorem 3.1:** Bounds the performance gap, highlighting that value approximation error is amplified by the planning horizon and discount factor.

---

## Results

The primary metric used is **Value Approximation Error** ($E_{\rho_0}[\hat{V} - V^\pi]$), comparing the function estimation against the true discounted return.

The analysis indicates a severe scalability issue with standard methods (TD-MPC2):

| Task | Degrees of Freedom (DoF) | Error Rate |
| :--- | :---: | :--- |
| **Hopper-Stand** | 4-DoF | 15% |
| **Dog-Trot** | 36-DoF | 231% |
| **h1hand-slide-v0** | 61-DoF | 746% |
| **h1hand-run-v0** | 61-DoF | 2159% |

**Conclusion:** TD-MPC2 fails to converge to ground truth in high-dimensional environments, correlating with poor policy performance. TD-M(PC)$^2$ resolves these errors, stabilizing the learning process.