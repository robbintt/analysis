# GC-KBVQA: A New Four-Stage Framework for Enhancing Knowledge Based Visual Question Answering Performance

*Mohammad Mahdi Moradi; Sudhir Mudur*

---

### ðŸ“Š Quick Facts

| **Metric** | **Details** |
| :--- | :--- |
| **Quality Score** | 8/10 |
| **References** | 40 Citations |
| **Framework Type** | 4-Stage Zero-Shot |
| **Core Model** | Llama-3-8B-Instruct |
| **Key Innovation** | Grounding Question-Aware Caption Generation |
| **Embedding Speed** | 14,200 sentences/sec |

---

## Executive Summary

**Problem**
Knowledge-Based Visual Question Answering (KB-VQA) faces a critical bottleneck in retrieving relevant external information while avoiding data that is misleading or irrelevant. Existing solutions typically require resource-intensive multimodal training or task-specific fine-tuning, resulting in high computational costs and significant deployment barriers. This paper addresses the challenge of achieving high-performance KB-VQA in a zero-shot setting, proving that effective visual reasoning does not necessitate expensive end-to-end model training.

**Innovation**
The authors propose **GC-KBVQA**, a novel four-stage framework that leverages Large Language Models (LLMs) as implicit knowledge sources. The methodology begins with *Grounding Question-Aware Caption Generation*, employing KeyBERT (thresholds > 0.4) to perform keyword-guided visual grounding and utilizing an ensemble of VLMs (LLaVA and InstructBlip). This is followed by *Knowledge Integration*, where the system mitigates irrelevant information through a rigorous filtering mechanism: Llama-3-8B-Instruct distills the question, and captions are ranked against it using `all-MiniLM-L6-v2` vectors via cosine similarity. The pipeline proceeds to *LLM Prompting*â€”constructing informative prompts using the top three filtered captions and two generated in-context QA pairsâ€”and concludes with *Zero-Shot Inference*.

**Results**
The framework achieves state-of-the-art results in zero-shot settings compared to competing KB-VQA methods. Quantitative analysis through ablation studies highlights specific performance drivers: generating exactly two QA pairs was found to be optimal, while the dual VLM ensemble successfully compensated for individual model biases. The system demonstrates high computational efficiency, with the embedding model capable of encoding 14,200 sentences per second.

**Impact**
This research significantly shifts the KB-VQA paradigm away from monolithic, fine-tuned models toward efficient, modular inference frameworks. By eliminating the need for task-specific training, GC-KBVQA lowers computational costs and entry barriers for deploying advanced visual QA systems. It validates that high-level reasoning can be effectively offloaded to general-purpose LLMs while maintaining state-of-the-art accuracy.

---

## Key Findings

*   **Zero-Shot Superiority:** The GC-KBVQA framework achieves significantly improved performance compared to competing methods while operating in a zero-shot setting, requiring no end-to-end multimodal training.
*   **Cost Efficiency:** By leveraging general-purpose pre-trained LLMs, the method eliminates the need for task-specific fine-tuning, thereby reducing computational costs and deployment complexity.
*   **Mitigated Hallucination:** Integrating question-aware captions with external knowledge sources effectively mitigates the issue of irrelevant or misleading auxiliary information common in previous KB-VQA approaches.
*   **Optimal Efficiency:** The system achieves a balance between performance and speed, with the embedding model processing 14,200 sentences per second.

---

## Methodology

The researchers introduce **GC-KBVQA**, a novel four-stage framework designed to enhance Knowledge-Based Visual Question Answering by better utilizing Large Language Models (LLMs).

1.  **Grounding Question-Aware Caption Generation**
    Generates compact, detailed, and context-rich captions specific to the question to ensure relevance.
2.  **Knowledge Integration**
    Combines the generated captions with relevant external information to build a comprehensive knowledge base.
3.  **LLM Prompting**
    Constructs informative prompts using the fused data from the integration stage.
4.  **Zero-Shot Inference**
    Allows the LLM to perform tasks without explicit end-to-end training, relying on the pre-trained capabilities of the model.

---

## Technical Details

GC-KBVQA is a zero-shot, cost-efficient framework using `Llama-3-8B-Instruct` as the core LLM. The pipeline leverages a dual VLM ensemble and specific hyperparameters for robustness.

**Pipeline Architecture & Parameters**

| Stage | Component | Configuration & Thresholds |
| :--- | :--- | :--- |
| **1. Visual Grounding** | Keyword-Guided (KeyBERT) | â€¢ Relevance > **0.4**<br>â€¢ Detection > **0.25**<br>â€¢ Intersection Removal > **0.9** |
| **2. Captioning** | VLM Ensemble | â€¢ Models: **LLaVA** + **InstructBlip**<br>â€¢ Filter: `all-MiniLM-L6-v2` (384-dim)<br>â€¢ Sorting: Cosine Similarity |
| **3. Context Prep** | QA Pairs Gen | â€¢ Source: Top **3** filtered captions<br>â€¢ Quantity: **2** in-context examples<br>â€¢ Distilled by: Llama-3-8B-Instruct |
| **4. Inference** | Answer Gen | â€¢ Inputs: Original question, filtered captions, generated QA pairs<br>â€¢ Model: Llama-3-8B-Instruct |

---

## Contributions

*   **Four-Stage Framework:** Development of GC-KBVQA, a framework that enables zero-shot VQA by utilizing LLMs as implicit knowledge sources without the constraints of explicit knowledge bases or end-to-end training.
*   **Grounding Question-Aware Captioning:** Introduction of a novel caption generation method that ensures precise, relevant, and compact context.
*   **Performance & Accessibility:** Demonstrated that a framework can achieve state-of-the-art performance while lowering deployment barriers by removing expensive task-specific fine-tuning.

---

## Results

*   **Zero-Shot Performance:** The framework achieves significantly improved performance compared to competing KB-VQA methods without task-specific training.
*   **Ablation Study: VLM Ensemble:** Validated that using a dual VLM ensemble (LLaVA and InstructBlip) yields the best results, complementing individual model biases.
*   **Ablation Study: Filtering:** The question-distillation filtering step significantly improves accuracy by removing irrelevant visual data.
*   **Hyperparameter Optimization:** Determined that generating exactly two QA pairs provides the optimal balance between efficiency and effectiveness; more pairs lead to information saturation.

---

**Quality Score:** 8/10
**References:** 40 Citations