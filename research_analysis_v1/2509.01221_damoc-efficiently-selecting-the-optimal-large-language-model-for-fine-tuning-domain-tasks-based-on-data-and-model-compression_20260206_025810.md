---
title: 'DaMoC: Efficiently Selecting the Optimal Large Language Model for Fine-tuning
  Domain Tasks Based on Data and Model Compression'
arxiv_id: '2509.01221'
source_url: https://arxiv.org/abs/2509.01221
generated_at: '2026-02-06T02:58:10'
quality_score: 8
citation_count: 26
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# DaMoC: Efficiently Selecting the Optimal Large Language Model for Fine-tuning Domain Tasks Based on Data and Model Compression

*Wei Huang; Huang Wei; Yinggui Wang*

---

> ### ðŸ“Š Quick Facts
>
> *   **Quality Score:** 8/10
> *   **Computational Savings:** ~20-fold reduction in training time
> *   **Validation Scope:** 4 distinct datasets (Medical, Financial, General Q&A, Reading Comprehension)
> *   **Core Strategy:** Data Filtering + Token Compression + Model Pruning
> *   **Citations:** 26 references

---

## Executive Summary

Selecting the optimal open-source Large Language Model (LLM) for domain-specific fine-tuning presents a significant computational bottleneck for researchers and practitioners. With the rapid expansion of available models, identifying which architecture will yield the highest performance on a specific downstream task traditionally requires expensive full fine-tuning for every candidate. This paper addresses the challenge of efficiently identifying the best model without the prohibitive cost of exhaustive training. The authors highlight that current proxy methods, such as few-shot prompting, are unreliable for this task, as performance rankings in prompting scenarios frequently contradict the results obtained after full fine-tuning.

The authors introduce **DaMoC** (Data and Model Compression), a unified framework that accelerates the model selection process by evaluating a "compressed proxy" of the fine-tuning process rather than the full model. The methodology operates through a three-pronged strategy: **Data Filtering**, **Token Compression**, and **Model Pruning**.

*   **Data Filtering:** Employs a novel taxonomy of data filtering methodsâ€”classified as distribution-aware, quality-aware, or hybridâ€”to reduce training volume while maintaining performance correlation.
*   **Token Compression:** Utilizes token compression driven by BERTScore minimization and perplexity calculations, supplemented by iterative text rewriting via a larger model (Baichuan2-13B-Chat) to optimize input data.
*   **Model Pruning:** Calculates a Layer Importance Score based on the cosine similarity of input/output activations to perform layer-wise pruning, avoiding MHA/MLP components to mitigate ZeRO-3 memory overhead, and uses a sparse merging paradigm to preserve the original model's capabilities.

Experimental validation across four distinct datasets demonstrates that DaMoC achieves a substantial reduction in computational overhead, saving approximately **20-fold in training time** compared to standard fine-tuning. Crucially, this efficiency does not come at the cost of accuracy; the framework successfully identifies the best-performing model in the majority of test cases. The results underscore the volatility of few-shot prompting as a selection metric, noting that rankings from Zero-shot or One-shot evaluations often contradict Full Fine-tuning results. DaMoCâ€™s combination of data-level optimizations and model pruning proved robust, ensuring that the performance of the compressed proxy correlates strongly with the full model's potential.

The DaMoC framework addresses a critical scalability issue in natural language processing by enabling rapid, cost-effective prototyping of domain-specific LLMs. By establishing a rigorous taxonomy for data filtering and validating that compressed proxies can reliably predict full-model performance, this research provides a practical blueprint for optimizing model selection workflows.

---

## Key Findings

*   **Rapid Identification:** The DaMoC framework enables the rapid identification of the optimal Large Language Model (LLM) for fine-tuning domain-specific tasks from a pool of open-source models.
*   **Significant Efficiency Gains:** The proposed approach achieves a significant reduction in computational overhead, saving approximately **20-fold in training time** while successfully selecting the best-performing model.
*   **Cross-Domain Versatility:** The methodology is validated across four distinct datasetsâ€”Medical Q&A, Financial Q&A, General Q&A, and Reading Comprehensionâ€”demonstrating versatility across different domains.
*   **Preserved Capabilities:** Combining data-level optimizations (token compression and rewriting) with model-level pruning (layer removal) effectively preserves the original model's capabilities during the selection process.

---

## Methodology

The DaMoC framework operates on two distinct levels to optimize the selection and fine-tuning process:

### 1. Data Level
*   **Taxonomy Establishment:** Establishes a taxonomy of data filtering methodologies divided into:
    *   Distribution-aware
    *   Quality-aware
    *   Hybrid approaches
*   **Optimization Techniques:** Employs token compression and text rewriting to optimize the dataset before training begins.

### 2. Model Level
*   **Layer Assessment:** Assesses layer importance using similarity scores to prune less important layers.
*   **Capability Preservation:** Utilizes a sparse merging paradigm to preserve the original model's capabilities even after removing layers.

---

## Technical Details

The DaMoC (Data and Model Compression) framework selects optimal open-source LLMs for domain-specific fine-tuning through a specific three-pronged strategy:

1.  **Data Filtering**
    *   Uses 12 distinct methods categorized as Distribution-aware, Quality-aware, and Hybrid.
    *   Goal: Reduce training volume while maintaining performance ranking.

2.  **Token Compression**
    *   Optimizes the training phase by compressing questions and answers using **BERTScore minimization** and **perplexity calculations**.
    *   Utilizes iterative rewriting via **Baichuan2-13B-Chat** if scores fall below a threshold.

3.  **Model Pruning**
    *   Focuses on **Layer-wise pruning** (avoiding MHA/MLP due to ZeRO-3 overhead).
    *   Uses a **Layer Importance Score** based on the cosine similarity of input/output activations.
    *   Implements a **sparse merging paradigm** to merge task vectors of pruned layers with preceding unpruned layers.

---

## Results

*   **Unreliability of Prompting:** Experimental findings show that few-shot prompting is an unreliable proxy for selecting fine-tuning models. Rankings from Zero-shot/One-shot/Two-shot often contradict Full Fine-tuning results (e.g., Llama3.1-8B ranked 8th in Zero-shot but 2nd in Full Fine-tuning on BillSum).
*   **Performance Correlation:** DaMoC achieves approximately a 20-fold reduction in training time compared to standard fine-tuning while successfully identifying the best-performing model.
*   **Validation Success:** The framework was validated across Medical, Financial, General Q&A, and Reading Comprehension datasets.
*   **Robustness:** The combination of data-level optimizations and model pruning preserves original capabilities and ensures compressed proxy performance correlates well with full model performance.

---

## Contributions

*   **DaMoC Framework:** Introduction of a unified framework that addresses the challenge of optimal LLM selection for downstream domain tasks by integrating data and model compression techniques.
*   **Data Filtering Taxonomy:** A novel classification system for LLM data filtering that distinguishes between distribution-aware, quality-aware, and hybrid methodologies.
*   **Efficiency Optimization:** A contribution to computational efficiency in model selection, demonstrating that training time can be reduced by a factor of 20 without sacrificing the ability to identify the highest-performing model.

---

## Evaluation & References

**Quality Score:** 8/10
**References:** 26 citations