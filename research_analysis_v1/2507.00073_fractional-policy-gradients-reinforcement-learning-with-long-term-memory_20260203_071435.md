---
title: 'Fractional Policy Gradients: Reinforcement Learning with Long-Term Memory'
arxiv_id: '2507.00073'
source_url: https://arxiv.org/abs/2507.00073
generated_at: '2026-02-03T07:14:35'
quality_score: 9
citation_count: 5
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Fractional Policy Gradients: Reinforcement Learning with Long-Term Memory

*Urvi Pawar; Kunal Telangi*

---

> ### ðŸ“Š Quick Facts
>
> *   **Sample Efficiency:** +35â€“68% improvement vs. baselines
> *   **Variance Reduction:** 24â€“52% reduction vs. baselines
> *   **Computational Complexity:** O(1) Time & Memory
> *   **Core Innovation:** Fractional Calculus for Non-Markovian RL
> *   **Quality Score:** 9/10

---

## Executive Summary

Standard Reinforcement Learning (RL) policy gradient methods rely heavily on Markovian assumptions, positing that future states depend solely on the present state and action. This limitation often prevents agents from effectively capturing long-term temporal dependencies inherent in complex environments, leading to high variance in gradient estimates and poor sample efficiency. Addressing these inefficiencies is critical for the advancement of RL, as high variance slows convergence and requires prohibitive amounts of data to achieve stable performance in real-world applications.

The authors introduce **Fractional Policy Gradients (FPG)**, a novel framework that integrates fractional calculus into policy optimization to model long-range dependencies. Technically, the method utilizes Caputo fractional derivatives to reformulate policy gradients and employs a Fractional Bellman equation weighted by Riemann-Liouville kernels to establish power-law memory correlations. To solve the typically high computational cost of fractional calculus, the authors developed a constant-time recursive scheme for calculating fractional temporal-difference (TD) errors. This innovation reduces complexity from linear time $O(t)$ to constant time $O(1)$, allowing the framework to leverage fractional dynamics without introducing computational overhead.

FPG demonstrates significant performance improvements over state-of-the-art baselines, achieving a 35â€“68% increase in sample efficiency and a 24â€“52% reduction in variance. Theoretically, the authors prove that FPG achieves an asymptotic variance reduction of order $O(t^{-\alpha})$ while preserving the convergence properties of standard policy gradients. Crucially, the recursive computation technique ensures that these gains are realized efficiently; the algorithm maintains constant time and memory complexity ($O(1)$) and utilizes adaptive stabilization mechanisms to ensure training stability throughout the process.

This research establishes a mathematically grounded mechanism for incorporating long-term memory into RL, effectively moving beyond the constraints of Markovian assumptions. By demonstrating that fractional calculus can be applied to policy optimization without computational penalty, FPG provides a viable pathway for solving the persistent issues of high variance and sampling inefficiency. This work influences the field by validating power-law temporal correlations as a tool for agent optimization, potentially inspiring future applications of fractional calculus in deep learning and control theory.

---

## Key Findings

*   **Asymptotic Variance Reduction:** Theoretical analysis demonstrates that Fractional Policy Gradients (FPG) achieve an asymptotic variance reduction of order $O(t^{-\alpha})$ compared to standard policy gradients while strictly preserving convergence properties.
*   **Significant Performance Gains:** Empirical validation reveals substantial performance improvements, with specific gains of **35â€“68%** in sample efficiency and **24â€“52%** in variance reduction compared to state-of-the-art baselines.
*   **Zero Computational Overhead:** The framework successfully leverages long-range dependencies and fractional calculus without introducing computational overhead, utilizing a recursive computation technique that maintains constant time and memory requirements.
*   **Power-Law Correlations:** FPG establishes power-law temporal correlations between state transitions, effectively addressing the limitations inherent in the Markovian assumptions of standard approaches.

---

## Methodology

The authors propose a reinforcement learning framework that integrates fractional calculus into policy optimization through the following steps:

1.  **Fractional Integration:** The methodology involves utilizing Caputo fractional derivatives to reformulate policy gradients, thereby enabling the modeling of long-term temporal dependencies.
2.  **Efficient Computation:** Additionally, it develops an efficient recursive technique to calculate fractional temporal-difference (TD) errors, ensuring the algorithm operates with constant time and memory complexity ($O(1)$).

---

## Technical Details

*   **Core Concept:** Fractional Policy Gradients (FPG) integrates fractional calculus into reinforcement learning using the Caputo Fractional Derivative to define optimization dynamics.
*   **Memory Modeling:** The method introduces a Fractional Bellman equation with weights derived from Riemann-Liouville kernels to incorporate power-law memory.
*   **Discretization:** Continuous operators are discretized using the GrÃ¼nwald-Letnikov equivalence.
*   **Optimization Scheme:** To address computational complexity, a **Constant-Time Recursive Scheme** reformulates the fractional TD-error into a recursive update rule dependent only on the current and previous errors, reducing complexity from O(t) to O(1).
*   **Temporal Correlations:** The framework establishes power-law temporal correlations to retain long-term dependencies.

---

## Results

*   **Efficiency Metrics:** FPG demonstrates a 35â€“68% improvement in sample efficiency and a 24â€“52% reduction in variance compared to state-of-the-art baselines.
*   **Complexity Management:** The recursive computation technique ensures constant time and memory complexity (O(1)) with no additional computational overhead.
*   **Stability:** The method preserves the convergence properties of standard policy gradients and utilizes adaptive stabilization mechanisms to ensure training stability.

---

## Contributions

*   **Novel Framework:** Introduction of the Fractional Policy Gradients (FPG) framework, which provides a mathematically grounded mechanism for long-term temporal modeling in reinforcement learning.
*   **Overcoming Limitations:** The work addresses the high variance and inefficient sampling issues of standard policy gradients by moving beyond Markovian assumptions through power-law temporal correlations.
*   **Rigorous Validation:** It provides rigorous theoretical proof of variance reduction and convergence, alongside empirical evidence of significant sample efficiency gains over existing state-of-the-art methods.

---

**Quality Score:** 9/10
**References:** 5 citations