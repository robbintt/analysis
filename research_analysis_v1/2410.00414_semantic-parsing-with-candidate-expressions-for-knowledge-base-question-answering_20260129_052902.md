# Semantic Parsing with Candidate Expressions for Knowledge Base Question Answering

*Daehwan Nam; Gary Geunbae Lee*

---

> ### ⚡ Quick Facts: Metrics & Specs
> 
> *   **Datasets:** KQA Pro, Overnight
> *   **Hardware:** AMD EPYC 7502, NVIDIA RTX A5000
> *   **Top Speed Improvement:** ~2x speedup on Overnight dataset (115.91 ms vs. 223.16 ms baseline)
> *   **Peak Accuracy:** 87.6% Exact Match (Overnight), 64.8% Execution Accuracy (KQA Pro - Strong Supervision)
> *   **Key Innovations:** Candidate Expressions, Sub-type Inference, Mask Caching Algorithm
> *   **Quality Score:** 9/10

---

## Executive Summary

Current semantic parsing approaches for Knowledge Base Question Answering (KBQA) that utilize sequence-to-sequence pre-trained language models (PLMs) face a critical scalability limitation. While these models are linguistically powerful, they lack robust mechanisms to effectively integrate the massive schema information inherent in large-scale Knowledge Bases. Without the ability to leverage this deep context during generation, parsers struggle to produce valid KB elements, often resulting in invalid logical forms or hallucinations that fail to execute. This disconnect between the generative capabilities of PLMs and the rigid structural requirements of KBs presents a significant barrier to achieving high accuracy in real-world QA scenarios.

To resolve this, the authors introduce a grammar framework augmented with **"candidate expressions,"** which embed KB information directly into the parsing constraints. This approach employs a dual-layered constrained decoding mechanism:
1.  **Types:** Ensures structural validity.
2.  **Candidate Expressions:** Ensures semantic validity specific to the KB.

Technically, the method treats logical forms as token sequences and utilizes production rules enhanced with these expressions to guide the generation toward valid executable queries. To address the computational overhead typically associated with constrained decoding, the paper implements **"sub-type inference"** and **"union types"** to refine action sequences, alongside a **"mask caching algorithm"** that caches valid actions during beam search to significantly reduce latency.

The proposed method was rigorously evaluated on the **KQA Pro** and **Overnight** benchmarks, demonstrating robust performance gains against strong baselines like BART KOPL and GRAPHQ IR. On the KQA Pro dataset under weak supervision, the model achieved an execution accuracy of **60.5%**, outperforming the BART KOPL baseline's 56.5%. These accuracy improvements were consistent under strong supervision settings, where the proposed model reached **64.8%** compared to the baseline's 59.6%. Similarly, on the Overnight dataset, the approach achieved an Exact Match score of **87.6%**, surpassing the BART baseline's 85.9%.

Crucially, the efficiency optimizations yielded substantial speed improvements: on KQA Pro, decoding time improved to **152.52 ms** versus the baseline's 172.00 ms. On the Overnight dataset, the approach achieved a decoding time of **115.91 ms** compared to the baseline's 223.16 ms—nearly a 2x speedup.

This research significantly advances the field of semantic parsing by successfully reconciling the generative flexibility of PLMs with the strict constraints of large-scale Knowledge Bases. By demonstrating that accuracy can be improved without—and in fact while significantly reducing—inference latency, the work challenges the traditional trade-off between precision and speed in constrained generation.

---

## Key Findings

*   **Integration Gap:** Traditional grammar-based semantic parsers using sequence-to-sequence PLMs lack mechanisms to effectively utilize large-scale Knowledge Base (KB) information.
*   **Candidate Expressions:** Augmenting grammars with 'candidate expressions' provides necessary constraints that help the parser generate valid KB elements, leading to increased accuracy.
*   **Robust Accuracy:** The proposed approach robustly improves accuracy on **KQA Pro** and **Overnight** benchmarks under both strong and weak supervision regimes.
*   **Efficiency Gains:** 'Sub-type inference' and a 'mask caching algorithm' significantly enhance decoding speed, mitigating the overhead typically associated with constrained decoding.

---

## Methodology

The research methodology is built upon a sequence-to-sequence framework enhanced with specific constraints to handle the complexity of large KBs.

1.  **Base Architecture:** The system utilizes a sequence-to-sequence (seq2seq) pre-trained language model (PLM) as the foundation for the semantic parser.
2.  **Grammar Augmentation:** The authors propose augmenting standard grammars with 'candidate expressions' designed specifically for large KBs.
3.  **Dual-Layer Decoding:** The methodology employs constrained decoding where parser actions are predicted based on two factors:
    *   **Types:** To ensure structural validity.
    *   **Candidate Expressions:** To ensure semantic validity.
4.  **Optimization Rules:** Specialized rules are introduced to handle complex logic and speed up inference:
    *   **Sub-type inference:** Refines type handling.
    *   **Union types:** Manages complex structures.
    *   **Mask caching algorithm:** Optimizes the inference process.

---

## Technical Details

The technical implementation focuses on treating logical forms as token sequences while enforcing strict grammatical and semantic rules.

*   **Framework:**
    *   Built on sequence-to-sequence Pre-trained Language Models (PLMs).
    *   Treats logical forms as linearized token sequences.

*   **Parsing Strategy:**
    *   Uses **grammar-based parsing** with defined production rules.
    *   Implements **constrained decoding** to guarantee both syntactic and semantic validity of the output.

*   **Key Innovations:**
    *   **Candidate Expressions:** Integrated directly into the grammar to use large-scale Knowledge Base information as generation constraints.
    *   **Sub-type Inference:** Refines type handling, effectively reducing the length of action sequences required for parsing.
    *   **Union Types:** Employed to handle complex structural requirements within the logical forms.
    *   **Mask Caching Algorithm:** Caches valid actions during the decoding process to drastically reduce the computational overhead of beam search.

---

## Experimental Results

The proposed approach was tested against BART KOPL and GRAPHQ IR baselines using **AMD EPYC 7502** and **NVIDIA RTX A5000** hardware.

### Accuracy Performance
*   **Weak Supervision:** Accuracy plateaus were observed between **60% and 80%**.
*   **Strong Supervision:** Significant improvements over baselines, with execution accuracy on KQA Pro reaching **64.8%** and Exact Match on Overnight reaching **87.6%**.

### Decoding Speed & Latency
The optimizations yielded substantial reductions in inference time.

*   **KQA Pro Dataset:**
    *   Proposed Method: **152.52 ms**
    *   Baseline: 172.00 ms
*   **Overnight Dataset:**
    *   Proposed Method: **115.91 ms**
    *   Baseline: 223.16 ms (Representing a nearly **2x speedup**)

### Optimization Impact
*   **Mask Caching:** Significantly reduced latency.
    *   *Example ($type-$):* 155.80 ms with caching vs. 218.76 ms without.
*   **Sub-type Inference:** Provided moderate speedups by effectively shortening the required action sequences.

---

## Contributions

*   **Grammar Framework:** Developed a novel grammar framework augmented with candidate expressions, enabling semantic parsers to leverage large-scale KB information effectively during constrained decoding.
*   **Structural Innovations:** Introduced "sub-type inference" and "union types" to handle logical forms more effectively and reduce sequence length.
*   **Efficiency Optimization:** Implemented a "mask caching algorithm" that successfully mitigates the speed overhead associated with constrained decoding.
*   **Validation & Open Source:** Validated the approach through empirical testing showing significant accuracy boosts in KBQA tasks and released the code as open source.

---

*References: 40 citations*