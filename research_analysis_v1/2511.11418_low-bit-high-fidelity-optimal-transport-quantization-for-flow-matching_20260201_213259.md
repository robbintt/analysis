# Low-Bit, High-Fidelity: Optimal Transport Quantization for Flow Matching

*Dara Varam · Diaa A. Abuhani · Imran Zualkernan · Raghad AlDamani · Lujain Khalil*

> **Quick Facts**
> 
> | **Metric** | **Value** |
> | :--- | :--- |
> | **Quality Score** | 8/10 |
> | **Compression Ratio** | 16×–32× (vs. FP32) |
> | **Target Bitwidth** | 2–3 bits per parameter |
> | **Validation Datasets** | 5 (MNIST, FashionMNIST, CIFAR-10, CelebA, ImageNet) |
> | **Key Metrics** | SSIM, PSNR |
> | **Theoretical Contribution** | First degradation bounds for flow-based models |
> | **Method Core** | 2-Wasserstein distance minimization |
> | **References** | 34 citations |

---

## Executive Summary

Flow Matching models represent the current state-of-the-art for continuous-time generative modeling, yet their deployment on resource-constrained edge devices remains prohibitive due to massive memory and computational requirements. This paper introduces a **mathematically principled Optimal Transport (OT) quantization framework** that enables aggressive 2–3 bit compression without catastrophic generative collapse.

By minimizing the **2-Wasserstein distance** between full-precision and quantized weight distributions—rather than using conventional heuristic bucketing—this approach preserves the global geometric structure essential for stable ODE/SDE trajectory dynamics. The method achieves **viable generation quality at 16×–32× compression ratios** where uniform, piecewise, and logarithmic quantization schemes fail completely, challenging the prevailing assumption that generative models require high-precision parameters.

Empirical validation demonstrates that at 2-bit precision, OT quantization maintains **SSIM scores of ~0.8** on MNIST, CIFAR-10, and CelebA (versus <0.4 for baselines) and **PSNR exceeding 40 dB** on MNIST (versus <30 dB for competitors). This work establishes the first theoretical degradation bounds specific to flow-based architectures, providing formal guarantees for ultra-low bit deployment and enabling practical edge deployment of high-fidelity generative AI.

---

## Key Findings

* **Ultra-low bit viability** — OT-based quantization preserves visual generation quality and latent space stability at aggressive compression rates of **2–3 bits per parameter**, whereas uniform, piecewise, and logarithmic quantization schemes fail catastrophically at equivalent bit depths.

* **Theoretical degradation bounds** — The analysis provides rigorous **upper bounds on generative performance degradation** specifically induced by weight quantization in Flow Matching architectures, a first for flow-based generative models.

* **Consistent cross-dataset performance** — Empirical validation across five benchmark datasets of varying complexity demonstrates that OT-based methods maintain fidelity regardless of dataset complexity, from simple MNIST to high-resolution ImageNet.

* **OT superiority via Wasserstein geometry** — Minimizing the **2-Wasserstein distance** between quantized and original weight distributions outperforms conventional quantization approaches by preserving the probability distribution's global geometry rather than local approximation error.

---

## Methodology

### Core Framework
The approach adapts **post-training quantization (PTQ)** to Flow Matching models through an Optimal Transport formulation. Instead of treating quantization as local approximation, the method minimizes the **2-Wasserstein distance** between the probability distributions of full-precision and quantized weights, preserving the distributional geometry critical for flow stability.

### Comparative Analysis
Systematic evaluation against three baseline quantization paradigms:
* **Uniform quantization** — Standard linear bucketing
* **Piecewise Linear (PWL)** — Non-uniform segment-based approximation  
* **Logarithmic (LogBase2)** — Log-scale compression schemes

### Theoretical Characterization
Derivation of **analytical upper bounds** on generative degradation to characterize the theoretical limits of quantization-induced performance loss specific to continuous-time flow architectures and their ODE/SDE dynamics.

### Empirical Validation
Comprehensive testing across five diverse benchmarks:
1. MNIST
2. FashionMNIST  
3. CIFAR-10
4. CelebA
5. ImageNet

Assessment focuses on generation quality (SSIM, PSNR) and latent space stability under quantized conditions.

---

## Technical Specifications

| **Parameter** | **Specification** |
| :--- | :--- |
| **Optimization Objective** | Minimize 2-Wasserstein distance between original and quantized weight distributions |
| **Mathematical Framework** | Optimal Transport (OT) theory |
| **Target Architecture** | Flow Matching models (continuous-time generative flows) |
| **Preservation Target** | Latent space trajectory stability and ODE/SDE dynamics |
| **Quantization Depth** | 2–3 bits per parameter (Ultra-low precision) |
| **Compression Ratios** | 4×–16× (vs. 8-bit baselines) / 16×–32× (vs. FP32) |
| **Baseline Comparators** | Uniform, PWL (Piecewise Linear), LogBase2 |
| **Evaluation Metrics** | SSIM (Structural Similarity Index), PSNR (Peak Signal-to-Noise Ratio) |

---

## Empirical Results

### Critical 2–4 Bit Regime
In the ultra-low bitwidth range where deployment feasibility is determined, OT quantization demonstrates **2×–4× effective quality advantage** over heuristic methods:

* **MNIST**: SSIM approaches **0.8** at 2 bits (OT) versus **~0.2** for all baselines; PSNR maintains **>40 dB** versus sharp drop to **<30 dB**
* **FashionMNIST**: Maximum SSIM **~0.6** (OT) versus **<0.1–0.2** for baselines  
* **CIFAR-10 & CelebA**: SSIM preserved in **0.8 range** (OT) versus **<0.4** for baselines
* **ImageNet**: Stable generation maintained where comparators exhibit collapse

### Convergence Behavior
At bitwidths **≥5–6 bits**, all quantization methods converge to comparable performance. However, in the critical **2–4 bit regime**, OT uniquely enables viable deployment without catastrophic fidelity loss or flow trajectory disruption.

### Cross-Dataset Stability
OT demonstrates **stable relative performance across the dataset complexity spectrum**, from simple 28×28 grayscale (MNIST) to high-resolution facial imagery (CelebA) and complex natural scenes (ImageNet), confirming robust preservation of Wasserstein-2 geometry.

---

## Core Contributions

1. **Domain Extension** — First adaptation of OT-based quantization to Flow Matching generative models, successfully transferring a principled mathematical framework from discriminative to generative architectures.

2. **Theoretical Characterization** — Introduction of the first **analytical bounds on quantization degradation** specific to flow-based generative models, providing theoretical guarantees for low-bit deployment.

3. **Precision Paradigm Shift** — Demonstration that Flow Matching models can operate effectively at **2–3 bit precision** without catastrophic collapse, challenging prevailing assumptions that generative models require high-precision parameters.

4. **Compression Methodology** — Validation of **Wasserstein-distance minimization** as a superior, mathematically principled compression strategy compared to heuristic quantization schemes.

5. **Edge Deployment Enablement** — Practical framework for deploying high-fidelity generative models on resource-constrained edge and embedded AI systems through aggressive yet stable parameter compression.

---

## Research Assessment

**Quality Score:** 8/10  
**Citation Count:** 34 references  

This work establishes a new standard for principled quantization in generative architectures, providing both the theoretical foundations and practical methodology necessary for democratizing access to resource-intensive generative AI on edge devices.