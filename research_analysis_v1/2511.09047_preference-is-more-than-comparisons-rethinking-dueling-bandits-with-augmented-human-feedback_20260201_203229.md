# Preference is More Than Comparisons: Rethinking Dueling Bandits with Augmented Human Feedback

*Shengbo Wang; Hong Sun; Ke Li*

---

### üìë Quick Facts

| Metric | Details |
| :--- | :--- |
| **Quality Score** | 8/10 |
| **References** | 40 Citations |
| **Core Algorithm** | IPEA-HF (Interactive Preference Elicitation with Augmented Human Feedback) |
| **Key Innovations** | Augmented Confidence Bounds, Model-Free Framework |
| **Evaluation Metric** | Cumulative Regret |
| **Primary Domains** | Recommendation Systems, Multi-Objective Optimization, LLM Alignment |

---

## Executive Summary

Current Dueling Bandit (DB) algorithms used for Interactive Preference Elicitation (IPE) face critical performance bottlenecks due to their reliance on parametric reward models. These models are inherently inefficient when human feedback is sparse and are highly vulnerable to model misspecification, often leading to incorrect convergence. This limitation is a significant hurdle in high-stakes domains such as recommendation systems and Large Language Model (LLM) alignment, where acquiring sufficient direct human comparisons to fit complex parametric models is often prohibitively expensive and resource-intensive.

The authors propose **IPEA-HF** (Interactive Preference Elicitation with Augmented Human Feedback), a model-free DB framework that replaces rigid parametric assumptions with a feedback augmentation mechanism. Technically, the approach utilizes a graph structure to extract dependencies between arms, allowing the algorithm to incorporate "related observations" as surrogate feedback. The core innovation is the **Augmented Confidence Bound**, which recalculates confidence intervals using an Augmented Observation Count ($n_{i,j}(t)$) that sums direct observations and weighted related observations. This mechanism ensures mathematical validity through Generalized Concentration Properties, dynamically decreasing the weight of indirect data as direct feedback accumulates, thereby tightening bounds more efficiently than standard methods.

The proposed IPEA-RUCB and IPEA-DTS algorithms were evaluated against standard context-free baselines (RUCB, DTS) and parametric baselines (MaxInP, COLSTIM, VACDB) using Cumulative Regret as the primary metric. Evaluations across Item Recommendation (Sushi, Car Preference), Multi-Objective Optimization (DTLZ2, DTLZ7), and LLM Response Optimization (Anthropic H-H) demonstrated distinct performance advantages. While the parametric VACDB model exhibited high regret due to premature convergence, IPEA algorithms achieved significantly lower regret trajectories. Specifically, the methods proved more sample-efficient, successfully accelerating convergence within 200 rounds for multi-objective tasks (DTLZ2/7) and maintaining robust performance over 2,000 rounds in recommendation simulations, effectively outperforming standard methods on the H-H dataset for LLM optimization.

This research establishes a fundamental paradigm shift in Interactive Preference Elicitation, moving the field from model-dependent approaches to robust, model-free frameworks driven by feedback augmentation. By providing a rigorous theoretical analysis of regret trade-offs, the authors prove that leveraging indirect feedback allows for provably efficient learning even with sparse data. The significance of this work extends to cutting-edge AI applications, offering a scalable solution for optimizing LLM responses and other complex systems where relying solely on direct human comparison is a bottleneck, thereby broadening the practical applicability of dueling bandits.

---

## Key Findings & Contributions

### üîç Key Findings
*   **Inefficiency of Parametric Models:** Existing dueling bandit (DB) algorithms relying on parametric reward models are inefficient and vulnerable to model misspecification due to sparse human feedback and rigid assumptions.
*   **Superiority of Augmentation:** Addressing sparse feedback through feedback augmentation provides a more robust solution than heavy reliance on parametric models.
*   **Broad Competitiveness:** The prototype algorithm achieved competitive results across diverse Interactive Preference Elicitation (IPE) benchmarks, including recommendation systems, multi-objective optimization, and LLM response optimization.
*   **Theoretical Support:** Model-free DB frameworks enhanced with augmented feedback mechanisms can support provably efficient IPE in broader applications.

### ‚ú® Primary Contributions
*   **Paradigm Shift:** Introduction of a novel paradigm for sparse feedback shifting from parametric reward modeling to feedback augmentation within Interactive Preference Elicitation (IPE).
*   **Algorithmic Innovation:** Creation of "augmented confidence bounds" designed for a model-free DB framework under generalized concentration properties.
*   **Theoretical Framework:** Development of a rigorous theoretical framework via regret analysis characterizing the multi-factored trade-offs of the approach.
*   **Validation:** Broad empirical validation expanding the scope of dueling bandits to modern domains, specifically LLM response optimization.

---

## Methodology & Technical Details

### Framework Overview
The authors propose a **model-free Dueling Bandit (DB) framework** utilizing feedback augmentation to mitigate sparse human feedback, bypassing the need for rigid parametric reward models. The architecture is composed of four iterative components:

1.  **Augmented Confidence Bounds:** Recalculating intervals using direct and augmented feedback.
2.  **Pair Selection:** Utilizing algorithms like RUCB or DTS to choose the next comparison pair.
3.  **Dependency Extraction:** Employing a graph structure to establish relationships (edges) between arms.
4.  **Feedback Augmentation:** Updating the graph and observation counts based on new data.

### Mathematical Formulation

The method defines the core mechanics of the framework through the following calculations:

*   **Augmented Observation Count:**
    $$n_{i,j}(t) = n^d_{i,j}(t) + n^r_{i,j}(t)$$
    *Where $n^d$ represents direct observations and $n^r$ represents related observations.*

*   **Augmented Mean Estimation:**
    Calculated using a weighted sum of direct and related wins normalized by $\eta$.

*   **Augmented Confidence Bounds:**
    Derived directly from the augmented observation counts.

### Robustness Mechanism
The framework ensures robustness by assigning weaker influence weights to related observations. These weights decrease as direct observations accumulate, preventing indirect data from skewing results once sufficient ground truth is available.

---

## Evaluation Results

### Experimental Setup
*   **Primary Metric:** Cumulative Regret (Lower is better).
*   **Simulations:** Up to 2,000 interaction rounds for recommendation tasks; 200 rounds for multi-objective tasks.
*   **Compared Algorithms:**
    *   *Proposed:* **IPEA-RUCB**, **IPEA-DTS**
    *   *Standard Context-Free:* RUCB, DTS
    *   *Parametric Baselines:* MaxInP, COLSTIM, VACDB

### Benchmarks
1.  **Item Recommendation:** Sushi dataset, Car Preference dataset.
2.  **Multi-Objective Optimization:** DTLZ2, DTLZ7.
3.  **LLM Response Optimization:** Anthropic H-H dataset.

### Performance Outcomes
*   **Superior Regret Trajectories:** The proposed IPEA algorithms consistently outperformed all baselines, achieving significantly lower regret.
*   **Vulnerability of Parametrics:** Parametric models like VACDB exhibited increasingly high regret due to model misspecification and premature convergence.
*   **Sample Efficiency:** IPEA algorithms improved sample efficiency by tightening confidence intervals faster than methods relying solely on direct observations.
*   **Key Successes:** The most distinct advantages were observed on the complex **DTLZ7** multi-objective tasks and the **LLM optimization** benchmarks.