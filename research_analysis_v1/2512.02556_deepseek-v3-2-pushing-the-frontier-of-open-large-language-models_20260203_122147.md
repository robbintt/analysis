---
title: 'DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models'
arxiv_id: '2512.02556'
source_url: https://arxiv.org/abs/2512.02556
generated_at: '2026-02-03T12:21:47'
quality_score: 9
citation_count: 8
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models

*DeepSeek-AI; Aixin Liu; Aoxue Mei; Bangcai Lin; Bing Xue; Bingxuan Wang; Bingzheng Xu; Bochao Wu; Bowei Zhang; Chaofan Lin; Chen Dong; Chengda Lu; Chenggang Zhao; Chengqi Deng; Chenhao Xu; Chong Ruan; Damai Dai; Daya Guo; Dejian Yang; Deli Chen; Erhang Li; Fangqi Zhou; Fangyun Lin; Fucong Dai; Guangbo Hao; Guanting Chen; Guowei Li; H. Zhang; Hanwei Xu; Hao Li; Haofen Liang; Haoran Wei; Haowei Zhang; Haowen Luo; Haozhe Ji; Honghui Ding; Hongxuan Tang; Huanqi Cao; Huazuo Gao; Hui Qu; Hui Zeng; Jialiang Huang; Jiashi Li; Jiaxin Xu; Jiewen Hu; Jingchang Chen; Jingting Xiang; Jingyang Yuan; Jingyuan Cheng; Jinhua Zhu; Jun Ran; Junguang Jiang; Junjie Qiu; Junlong Li; Junxiao Song; Kai Dong; Kaige Gao; Kang Guan; Kexin Huang; Kexing Zhou; Kezhao Huang; Kuai Yu; Lean Wang; Lecong Zhang; Lei Wang; Liang Zhao; Liangsheng Yin; Lihua Guo; Lingxiao Luo; Linwang Ma; Litong Wang; Liyue Zhang; M. S. Di; M. Y Xu; Mingchuan Zhang; Minghua Zhang; Minghui Tang; Mingxu Zhou; Panpan Huang; Peixin Cong; Peiyi Wang; Qiancheng Wang; Qihao Zhu; Qingyang Li; Qinyu Chen; Qiushi Du; Ruiling Xu; Ruiqi Ge; Ruisong Zhang; Ruizhe Pan; Runji Wang; Runqiu Yin; Runxin Xu; Ruomeng Shen; Ruoyu Zhang; S. H. Liu; Shanghao Lu; Shangyan Zhou; Shanhuang Chen; Shaofei Cai; Shaoyuan Chen; Shengding Hu; Shengyu Liu; Shiqiang Hu; Shirong Ma; Shiyu Wang; Shuiping Yu; Shunfeng Zhou; Shuting Pan; Songyang Zhou; Tao Ni; Tao Yun; Tian Pei; Tian Ye; Tianyuan Yue; Wangding Zeng; Wen Liu; Wenfeng Liang; Wenjie Pang; Wenjing Luo; Wenjun Gao; Wentao Zhang; Xi Gao; Xiangwen Wang; Xiao Bi; Xiaodong Liu; Xiaohan Wang; Xiaokang Chen; Xiaokang Zhang; Xiaotao Nie; Xin Cheng; Xin Liu; Xin Xie; Xingchao Liu; Xingkai Yu; Xingyou Li; Xinyu Yang; Xinyuan Li; Xu Chen; Xuecheng Su; Xuehai Pan; Xuheng Lin; Xuwei Fu; Y. Q. Wang; Yang Zhang; Yanhong Xu; Yanru Ma; Yao Li; Yao Li; Yao Zhao; Yaofeng Sun; Yaohui Wang; Yi Qian; Yi Yu; Yichao Zhang; Yifan Ding; Yifan Shi; Yiliang Xiong; Ying He; Ying Zhou; Yinmin Zhong; Yishi Piao; Yisong Wang; Yixiao Chen; Yixuan Tan; Yixuan Wei; Yiyang Ma; Yiyuan Liu; Yonglun Yang; Yongqiang Guo; Yongtong Wu; Yu Wu; Yuan Cheng; Yuan Ou; Yuanfan Xu; Yuduan Wang; Yue Gong; Yuhan Wu; Yuheng Zou; Yukun Li; Yunfan Xiong; Yuxiang Luo; Yuxiang You; Yuxuan Liu; Yuyang Zhou; Z. F. Wu; Z. Z. Ren; Zehua Zhao; Zehui Ren; Zhangli Sha; Zhe Fu; Zhean Xu; Zhenda Xie; Zhengyan Zhang; Zhewen Hao; Zhibin Gou; Zhicheng Ma; Zhigang Yan; Zhihong Shao; Zhixian Huang; Zhiyu Wu; Zhuoshu Li; Zhuping Zhang; Zian Xu; Zihao Wang; Zihui Gu; Zijia Zhu; Zilin Li; Zipeng Zhang; Ziwei Xie; Ziyi Gao; Zizheng Pan; Zongqing Yao; Bei Feng; Hui Li; J. L. Cai; Jiaqi Ni; Lei Xu; Meng Li; Ning Tian; R. J. Chen; R. L. Jin; S. S. Li; Shuang Zhou; Tianyu Sun; X. Q. Li; Xiangyue Jin; Xiaojin Shen; Xiaosha Chen; Xinnan Song; Xinyi Zhou; Y. X. Zhu; Yanping Huang; Yaohui Li; Yi Zheng; Yuchen Zhu; Yunxian Ma; Zhen Huang; Zhipeng Xu; Zhongyu Zhang; Dongjie Ji; Jian Liang; Jianzhong Guo; Jin Chen; Leyi Xia; Miaojun Wang; Mingming Li; Peng Zhang; Ruyi Chen; Shangmian Sun; Shaoqing Wu; Shengfeng Ye; T. Wang; W. L. Xiao; Wei An; Xianzu Wang; Xiaowen Sun; Xiaoxiang Wang; Ying Tang; Yukun Zha; Zekai Zhang; Zhe Ju; Zhen Zhang; Zihua Qu*

> ### ðŸ“Š Quick Facts & Key Metrics
>
> *   **Olympiad Performance:** ðŸ¥‡ Gold Medal in **IMO 2025** & **IOI 2025**
> *   **Benchmark Superiority:** Surpasses GPT-5; comparable to Gemini-3.0-Pro
> *   **Tool Decathlon (Pass@1):** 99.2%
> *   **Codeforces Rating:** 2708
> *   **HMMT 2025 Score:** 88.3%
> *   **Compute Budget:** >10% of pre-training cost allocated to RL
> *   **Paper Quality:** 9/10

---

## Executive Summary

This research addresses the dual challenge of achieving state-of-the-art reasoning capabilities while maintaining computational efficiency and robust agentic behavior in Large Language Models (LLMs). Specifically, it tackles the difficulties of handling long-context scenariosâ€”which often incur prohibitive computational costsâ€”and the limitations of current models in generalizing across complex, interactive tool-use environments. The problem is significant because it represents the critical barrier preventing open-source models from matching the performance of proprietary frontier systems; solving it requires a departure from standard scaling laws to harmonize high-efficiency inference with advanced reasoning and reliable instruction following.

The paper introduces three primary technical innovations to bridge this gap. First, **DeepSeek Sparse Attention (DSA)** optimizes the attention mechanism to substantially reduce computational complexity for long sequences without degrading performance. Second, the authors implemented a stable **Reinforcement Learning (RL) protocol** that aggressively scales post-training compute to over 10% of the pre-training cost, a method proven to enhance reasoning proficiency. Third, they developed a "**Cold-Start Agentic Pipeline**" for large-scale task synthesis; this pipeline systematically generates training data by unifying reasoning with tool-use, creating over 85,000 co-tasks across 1,800 distinct environments to improve generalization and robustness.

DeepSeek-V3.2 demonstrates benchmark superiority, with the specialized "**Speciale**" variant surpassing GPT-5 and achieving reasoning proficiency comparable to Gemini-3.0-Pro. In high-stakes evaluations, the model secured gold-medal performance in the 2025 International Mathematical Olympiad (IMO) and the International Olympiad in Informatics (IOI). Even the standard DeepSeek-V3.2 model performs on par with GPT-5, establishing a new performance ceiling for open-weight models.

The significance of this work lies in its validation that open models can compete with and exceed proprietary frontier AI through architectural efficiency and innovative post-training strategies. By successfully scaling reinforcement learning for reasoning, the paper establishes a new precedent for the role of post-training compute in model development.

---

## Key Findings

*   **Benchmark Superiority:** DeepSeek-V3.2-Speciale surpasses GPT-5 and achieves reasoning proficiency comparable to Gemini-3.0-Pro.
*   **Olympiad Success:** Secured gold-medal performance in both the 2025 International Mathematical Olympiad (IMO) and the International Olympiad in Informatics (IOI).
*   **Efficiency-Performance Balance:** DeepSeek-V3.2 successfully harmonizes high computational efficiency with superior reasoning and agent performance.
*   **Agentic Robustness:** Demonstrates substantial improvements in generalization and instruction-following robustness within complex, interactive environments.
*   **Competitive Baseline Performance:** The standard DeepSeek-V3.2 model performs comparably to GPT-5.

---

## Methodology

*   **DeepSeek Sparse Attention (DSA):** An efficient attention mechanism implemented to reduce computational complexity while maintaining model performance in long-context scenarios.
*   **Scalable Reinforcement Learning:** A robust reinforcement learning protocol utilized, coupled with scaling of post-training compute, to enhance reasoning capabilities.
*   **Large-Scale Agentic Task Synthesis Pipeline:** A novel data synthesis pipeline developed to systematically generate training data at scale, integrating reasoning into tool-use scenarios.

---

## Technical Details

| Component | Description |
| :--- | :--- |
| **Attention Mechanism** | **DSA (DeepSeek Attention):** Designed to reduce computational complexity for long sequences, ensuring efficiency without performance degradation. |
| **Training Pipeline** | **Cold-Start Agentic Pipeline:** Based on DeepSeek-V3 methodology to unify reasoning and tool-use processes. |
| **Reinforcement Learning** | **Stable RL Protocol:** Developed for post-training expansion with a computational budget exceeding 10% of the pre-training cost. |
| **Data Synthesis** | **Large-Scale Task Synthesis:** Generated over 1,800 distinct environments and 85,000 co-tasks to significantly improve model generalization. |

---

## Results

| Benchmark / Metric | Score / Result |
| :--- | :--- |
| **IMO 2025** | ðŸ¥‡ Gold Medal |
| **IOI 2025** | ðŸ¥‡ Gold Medal |
| **Tool Decathlon (Pass@1)** | 99.2% |
| **Codeforces Rating** | 2708 |
| **HMMT 2025** | 88.3% |
| **HLE Text-only Subset (Pass@1)** | 74.9% |
| **Comparison to GPT-5** | Standard model is comparable; **Speciale variant surpasses GPT-5**. |

---

## Contributions

*   **Efficient Attention Mechanisms:** Introduced DeepSeek Sparse Attention (DSA), contributing to the field's ability to handle long contexts with reduced computational overhead.
*   **Advanced Reasoning via RL:** Demonstrated the viability of scaling post-training compute through reinforcement learning to achieve state-of-the-art reasoning results, rivaling proprietary frontier models.
*   **Agentic AI Infrastructure:** Contributed a scalable framework for creating agentic training data, addressing the challenge of robust tool use and generalization in interactive environments.

---

**Paper Quality Score:** 9/10
**References:** 8 citations