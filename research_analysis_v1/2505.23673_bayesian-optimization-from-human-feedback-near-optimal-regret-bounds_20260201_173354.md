# Bayesian Optimization from Human Feedback: Near-Optimal Regret Bounds

*Aya Kayal; Sattar Vakili; Laura Toni; Da-shan Shiu; Alberto Bernacchia*

---

### üìã Quick Facts

| Metric | Details |
| :--- | :--- |
| **Quality Score** | 8/10 |
| **Total Citations** | 40 References |
| **Key Regret Bound** | $\tilde{O}(\sqrt{\Gamma(T)T})$ |
| **Algorithm Used** | MaxMinLCB |
| **Feedback Model** | Bradley-Terry-Luce (BTL) |
| **Real-World Dataset** | Yelp (Philly Restaurants) |

---

> ## EXECUTIVE SUMMARY
>
> **1. Problem**
> This research addresses a fundamental theoretical gap in Human-in-the-Loop optimization: specifically, whether Bayesian Optimization can maintain high efficiency when restricted to pairwise preference feedback rather than precise scalar rewards. In many real-world applications (e.g., recommender systems or generative AI), humans can easily compare two options ("Do you prefer A or B?") but struggle to provide accurate numerical scores. The paper tackles the Bayesian Optimization from Human Feedback (BOHF) problem to determine if this qualitative feedback inevitably leads to higher sample complexity or statistical inefficiency compared to standard BO methods that utilize exact numerical values.
>
> **2. Innovation**
> The key innovation is the derivation of significantly tighter regret bounds for BOHF by utilizing the Bradley-Terry-Luce (BTL) feedback model within a kernelized Bayesian framework. Technically, the authors move beyond general high-level bounds to establish specific, kernel-dependent performance guarantees. They construct high-probability confidence bounds in a linear kernel space (RKHS), using operator norms to control matrix terms and employing a bias-variance decomposition to rigorously quantify uncertainty. By incorporating sub-Gaussian noise concentration inequalities and bounding the RKHS norm, they formulate a confidence parameter that accounts for eigenvalues and regularization, allowing the algorithm to navigate the search space using only comparative data.
>
> **3. Results**
> The paper establishes a near-optimal cumulative regret bound of $\tilde{O}(\sqrt{\Gamma(T)T})$, where $\Gamma(T)$ is the maximum information gain dependent on the kernel. Crucially, this matches the order-optimal rates of conventional Bayesian Optimization, proving that preferential samples are theoretically just as efficient as scalar-valued samples. Validation was performed using the MaxMinLCB algorithm on both synthetic benchmarks (RKHS functions with Squared Exponential and Mat√©rn kernels, and a 1D Ackley function) and a real-world dataset. In the real-world scenario utilizing Yelp data for Philadelphia restaurants (275 actions, 20 users, features from 32-dimensional OpenAI embeddings), the method effectively minimized Simple Regret relative to the optimal utility function.
>
> **4. Impact**
> This work significantly influences the field by bridging the theoretical gap between preference-based learning and standard Bayesian Optimization. It provides the tightest known regret bounds for the BOHF problem to date, formally validating the use of cost-effective preference-based queries. By demonstrating that reduced feedback models do not impose a penalty on the sample complexity order, the paper provides strong theoretical backing for the use of human feedback in complex optimization tasks. This foundation enhances the viability of interactive machine learning systems in domains where acquiring precise scalar rewards is expensive, unreliable, or impossible.

---

## Key Findings

The paper establishes a significantly tighter regret bound for Bayesian Optimization from Human Feedback (BOHF) of $\tilde{O}(\sqrt{\Gamma(T)T})$.

*   **Tighter Regret Bounds:** Achieves a bound of $\tilde{O}(\sqrt{\Gamma(T)T})$, which is significantly tighter than previous theoretical guarantees.
*   **Order-Optimal Rates:** Despite operating with pairwise preferences, the sample complexity matches the order-optimal rates of conventional Bayesian Optimization for common kernels.
*   **Equivelency of Feedback:** Theoretically proves that preferential samples are just as efficient as scalar-valued samples.
*   **Kernel-Specific Performance:** The performance guarantee relies on kernel-specific complexity terms, specifically the maximum information gain.

## Methodology

The research operates within the Bayesian Optimization framework adapted for Human Feedback (BOHF).

*   **Feedback Model:** Utilizes the **Bradley-Terry-Luce (BTL)** model. The learner observes a preference between two actions at each time step rather than a precise scalar reward.
*   **Theoretical Derivation:** Incorporates kernel-specific complexity terms, specifically focusing on the **maximum information gain ($\Gamma(T)$)** to control the exploration-exploitation trade-off.

## Contributions

*   **Tightest Known Bounds:** Provides the tightest known regret bounds for the BOHF problem to date.
*   **Bridging the Gap:** Establishes the equivalence of sample complexities between preference-based learning and standard BO.
*   **Validation of Reduced Feedback:** Validates the use of reduced feedback models, showing that cost-effective preference-based queries do not impose a penalty on sample complexity order.

## Technical Details

The paper presents a rigorous framework for Bayesian Optimization with Human Feedback (BOHF), focusing on theoretical guarantees within linear kernel spaces.

### Framework & Analysis
*   **Space:** High-probability confidence bounds derived in a linear kernel space.
*   **Uncertainty Bounds:** Uses operator norms to control matrix terms.
*   **Noise Handling:** Handles sub-Gaussian noise via concentration inequalities.

### Key Components
*   **Feature Mapping & Gram Matrix:** Critical for analyzing the relationship between actions.
*   **Bias-Variance Decomposition:** Used to derive the confidence parameter.

### Quantitative Results
*   **Confidence Parameter ($LB$):**
    $$LB + L^2 \cdot \sqrt{\left(\frac{2\kappa}{\lambda}\right) \cdot \log\left(\frac{2}{\delta}\right)}$$
    *Incorporates bounds on the RKHS norm, eigenvalues, and regularization.*
*   **Final Regret Bound:** Achieves $\tilde{O}(\sqrt{\Gamma(T)T})$, matching standard Bayesian Optimization rates.

## Results

### Experimental Setup
*   **Algorithm:** MaxMinLCB.
*   **Success Metric:** Simple Regret relative to the optimal utility function.

### Benchmarks
*   **Synthetic:**
    *   RKHS test functions using **Squared Exponential** and **Mat√©rn kernels**.
    *   **1D Ackley function** on the domain $[-5, 5]$ with preference scaling $[-3, 3]$.

### Real-World Data (Yelp Dataset)
*   **Context:** Philadelphia restaurants.
*   **Scale:** Filtered to **275 actions** (restaurants) and **20 users** (2,563 reviews).
*   **Features:** 32-dimensional **OpenAI TEXT-EMBEDDING-3-LARGE** embeddings.
*   **Utility:** Normalized to the range $[-3, 3]$.