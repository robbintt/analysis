# Abduction of Domain Relationships from Data for VQA

*Al Mehdi Saadat Chowdhury; Paulo Shakarian; Gerardo I. Simari*

***

> ### **Quick Facts**
> 
> **Dataset Evaluated:** GQA
> **Core Algorithm:** FAST-DAP
> **Framework Type:** Neurosymbolic (ASP)
> **Accuracy Improvement:** 59.98% â†’ 81.01%
> **Data Requirements:** Low (Few-shot learning)
> **Orthogonality:** Compatible with existing KG augmentation strategies

***

## Executive Summary

Visual Question Answering (VQA) systems utilizing neurosymbolic approaches like Answer Set Programming (ASP) face a critical limitation: while they excel at logical reasoning, the underlying logic programs often lack the specific domain relationships required to answer queries about complex images. To be effective, a VQA system must understand not just the objects present, but how they interact; however, manually encoding every conceivable relationship or relying entirely on static knowledge bases is often impractical. This creates a "knowledge gap" where the logic remains sound, but the system cannot answer correctly because it lacks the necessary domain facts connecting visual constructs.

This research addresses that gap by introducing a mechanism for *abducing* (inferring) domain relationships directly from historical data rather than relying solely on pre-existing knowledge bases. The authors formalize the VQA task within a first-order logic framework, treating images and queries as ASP programs composed of facts, helper rules, and missing domain relationships.

To recover these missing relationships, they propose the **FAST-DAP algorithm**, an efficient abductive solver that identifies failing queries, isolates the missing concepts causing the failure, and generates assignments that entail the ground truth. Crucially, the method employs support-based regularization to guard against overfitting, allowing the system to derive generalizable relationships without requiring exhaustive training data.

Experimental evaluation on the GQA dataset demonstrates the efficacy of this approach, yielding a substantial improvement in query answering accuracy. The proposed method increased performance from a baseline of 59.98% to 81.01%, marking a significant leap in capability. Furthermore, the framework proved to be highly data-efficient, achieving these robust results while requiring only a few historical examples to learn from. The study also confirmed that the approach is orthogonal to existing Knowledge Graph augmentation strategies, meaning it successfully enhances performance without conflicting with current state-of-the-art methods. The primary significance of this work lies in its ability to bridge the gap between symbolic reasoning and data-driven learning in VQA.

***

## Key Findings

*   **Significant Accuracy Boost:** The proposed implementation significantly improves the accuracy of query answering in Visual Question Answering (VQA) systems.
*   **High Data Efficiency:** The approach achieves robust results while requiring only a few examples to learn from, distinguishing it from data-hungry deep learning models.
*   **Orthogonal Integration:** The method is orthogonal to existing techniques, meaning it can be used to complement, rather than replace, current knowledge augmentation strategies.
*   **Overcoming ASP Limitations:** By abducing domain relationships from past examples, the system successfully overcomes the limitation of ASP programs lacking domain data.

***

## Methodology

The researchers tackle the problem of missing domain knowledge in VQA through the following logical approach:

*   **Representation:** Visual data (images) and textual queries are represented as Answer Set Programming (ASP) programs.
*   **Core Process:** The methodology relies on **abduction**, a logical inference process, to derive domain relationships regarding "image constructs" from a set of past examples.
*   **Formalization:** The study involves formally framing this abduction problem.
*   **Validation:** A baseline approach and a concrete implementation were developed to validate the theory.

***

## Technical Details

The paper proposes a neurosymbolic framework for Visual Question Answering (VQA) using Answer Set Programming (ASP) to perform abduction on scene graphs.

*   **Logical Formalization:** The task is formalized using first-order logic, dividing the program into:
    *   **Facts:** Representing the image and query.
    *   **Helper Rules:** General logical constraints.
    *   **Domain Relationships:** The missing links to be inferred.
*   **Handling Incomplete Knowledge:** The system employs fallback rules with *negation as failure* to manage situations where specific data is missing.
*   **Core Algorithm (FAST-DAP):** This algorithm efficiently generates domain relationships through three steps:
    1.  Identifying failing queries.
    2.  Isolating missing concepts.
    3.  Finding assignments that entail the ground truth.
*   **Regularization:** The model utilizes **support-based regularization** to avoid overfitting to the few provided examples.

***

## Results

The evaluation of the method on the **GQA dataset** yielded the following outcomes:

*   **Accuracy Metric:** Query answering accuracy improved significantly from a baseline of **59.98%** to **81.01%**.
*   **Efficiency:** The approach demonstrates high data efficiency, requiring only a few historical examples to achieve these results.
*   **Compatibility:** The method is shown to be orthogonal to existing Knowledge Graph augmentation strategies, implying it can be stacked with other methods for cumulative benefits.

***

## Contributions

*   **Novel Problem Formulation:** Addresses the specific challenge of VQA where image and query representations (ASP programs) lack necessary domain data.
*   **Abductive Framework:** Introduces a mechanism to abduce (infer) missing domain relationships from historical data rather than relying solely on pre-existing knowledge bases.
*   **Data-Efficient Solution:** Provides a highly efficient implementation that improves accuracy without the need for large datasets, distinguishing it from heavy deep learning reliance.

***

**Document Statistics**
*   **Quality Score:** 9/10
*   **References:** 8 citations