---
title: 'RHINO: Learning Real-Time Humanoid-Human-Object Interaction from Human Demonstrations'
arxiv_id: '2502.13134'
source_url: https://arxiv.org/abs/2502.13134
generated_at: '2026-02-03T06:43:41'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# RHINO: Learning Real-Time Humanoid-Human-Object Interaction from Human Demonstrations

*Jingxiao Chen, Xinyao Li, Jiahang Cao, Zhengbang Zhu, Wentao Dong, Minghuan Liu, Ying Wen, Yong Yu, Liqing Zhang, Weinan Zhang*

---

> ### **Quick Facts**
>
> *   **Quality Score:** 8/10
> *   **References:** 40 Citations
> *   **Validation:** Real humanoid robot physical validation
> *   **Datasets:** Dining (14 classes), Office (17 classes)
> *   **Peak Accuracy:** 100% (Wave gesture)
> *   **Core Innovation:** Unified multi-modal reactive planning

---

## Executive Summary

Existing frameworks for humanoid interaction often rely on rigid, multi-stage pipelines that lack the flexibility for real-time responsiveness. This is a critical limitation because effective Humanoid-Human-Object Interaction (HHOI) requires robots to process multi-modal signals instantly and adapt to dynamic human interruptions, rather than executing pre-planned trajectories without feedback. The inability to seamlessly integrate language, vision, and motion in real-time hinders the deployment of humanoids in complex, unstructured environments where safety and adaptability are paramount.

The **RHINO** framework introduces a unified hierarchical architecture that decouples intention inference from physical execution to solve these rigidity issues. By integrating language, visual, and motion data through a high-level planner and a low-level controller, RHINO learns reactive skills directly from human-human-object demonstrations and teleoperation data.

A key technical component is the "Human Details Integration" module, which allows the system to distinguish between nuanced interaction types—such as differentiating between placing an object and handing it over—while employing reactive planning to generate dynamic motion.

Validated on a physical humanoid robot using Dining (14 classes) and Office (17 classes) datasets, RHINO demonstrated high precision in gesture recognition, achieving **100%** accuracy for waving and approximately **96%** for shaking hands. Ablation studies highlighted the necessity of the proposed modules; removing the "Human Details" component resulted in critical failures, including 838 misclassifications for the 'Settle Cap' action. Furthermore, RHINO significantly outperformed the GPT-4o-mini baseline, which struggled with contextual understanding, misclassifying ~50% of 'Idle' states as active actions and achieving only ~31% accuracy on social gestures.

This research represents a significant advancement in generalizable HHOI, moving the field beyond basic locomotion to complex, interactive behaviors. By enabling real-time reaction to human instructions and interruptions within a single, unified framework, RHINO establishes a new standard for safety and flexibility in human-robot collaboration.

---

## Key Findings

*   **Real-Time Reactivity:** The RHINO framework empowers humanoid robots with real-time reaction capabilities, allowing for immediate response to human instructions and interruptions.
*   **Unified Multi-Modal Processing:** The system effectively unifies multi-modal human signals—specifically language, images, and motions—into a single framework for reactive motion and manipulation.
*   **Physical Validation:** Successful implementation on a real humanoid robot demonstrated the framework's effectiveness, flexibility, and safety in practical scenarios.

---

## Methodology

The researchers utilized a **hierarchical learning framework** designed to bridge the gap between high-level understanding and low-level physical execution.

*   **High-Level Planner:** Responsible for inferring human intentions based on multi-modal inputs.
*   **Low-Level Controller:** Executes reactive motions and manipulation tasks based on the planner's output.
*   **Data Sources:** The framework learns reaction skills by leveraging:
    *   Human-human-object demonstrations.
    *   Teleoperation data.
*   **Unified Architecture:** Simultaneously addresses three critical pillars:
    1.  Reactive motion.
    2.  Instruction-based manipulation.
    3.  Safety concerns.

---

## Technical Details

**Framework Definition**
RHINO is a comprehensive framework for **Learning Real-Time Humanoid-Human-Object Interaction**. It processes the following multi-modal inputs:
*   **Language**
*   **Images**
*   **Motions**

**Key Architectural Components**
*   **Human Details Integration Module:** A specific module designed to distinguish between subtle interaction types (e.g., differentiating between placing an object vs. handing it over to a person).
*   **Reactive Planning:** Utilized for dynamic motion generation that adapts to real-time changes.
*   **Validation Environment:** The system was physically validated on a real humanoid robot platform.

---

## Contributions

*   **Overcoming Pipeline Rigidity:** Addresses limitations of existing multi-stage interaction models by prioritizing real-time feedback and allowing human interruption at any time.
*   **Generalizable HHOI Solution:** Proposed as a general solution for humanoid-human-object interaction that encompasses complex, interactive behaviors beyond basic locomotion.
*   **Decoupled Control Strategy:** Introduces a strategy that separates intention inference from physical execution, enabling the learning of complex reaction skills directly from demonstrations.

---

## Results & Evaluation

The framework was evaluated using **Dining (14 classes)** and **Office (17 classes)** datasets.

**Performance Metrics**
*   **Gesture Precision:**
    *   **Wave:** 100%
    *   **Shake Hands:** ~96%
*   **Interaction Handling:** Demonstrated robust performance in handling complex interactions.

**Ablation Study**
*   **Condition:** Removed "Human Details" module.
*   **Outcome:**
    *   Critical failures in disambiguating interactions (e.g., **838 misclassifications** for 'Settle Cap').
    *   High false positive rates during Idle states.

**Baseline Comparison (GPT-4o-mini)**
*   **Contextual Understanding:** Poor performance, misclassifying **~50%** of 'Idle' instances as active actions.
*   **Social Gesture Accuracy:** Low accuracy of **~31%** on tasks like Shake Hands.