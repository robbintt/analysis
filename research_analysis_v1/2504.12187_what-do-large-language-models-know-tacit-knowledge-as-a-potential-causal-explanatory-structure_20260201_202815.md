# What Do Large Language Models Know? Tacit Knowledge as a Potential Causal-Explanatory Structure

*C√©line Budding*

---

## üìä Quick Facts

| Category | Details |
| :--- | :--- |
| **Document Type** | Theoretical / Conceptual Analysis |
| **Core Subject** | Epistemology of Large Language Models (LLMs) |
| **Key Framework** | Martin Davies (1990) Tacit Knowledge Constraints |
| **Architecture Focus** | Transformers (Attention, MLP Modules) |
| **Quality Score** | 8/10 |
| **Citations** | 0 |

---

## üìù Executive Summary

The paper addresses the fundamental epistemological question of whether Large Language Models (LLMs) possess genuine "knowledge" or merely function as statistical mimics. Specifically, it challenges the philosophical skepticism articulated by Martin Davies (1990), which posits that connectionist neural networks are incapable of acquiring "tacit knowledge"‚Äîa form of implicit, explanatory knowledge defined by semantic description, syntactic structure, and causal systematicity. Resolving this ambiguity is critical for the field; without validating that LLMs possess structurally grounded knowledge, internal model reasoning remains opaque, hampering the development of reliable safety mechanisms and interpretable interventions.

The key innovation lies in operationalizing Martin Davies‚Äô three philosophical constraints specifically within the Transformer architecture, effectively bridging analytic philosophy with technical AI interpretability. Rather than conducting new experiments, the author applies a rigorous theoretical framework to existing empirical findings, arguing that MLP (Multi-Layer Perceptron) modules function as associative Key-Value memories. In this structure, keys represent input subjects and values represent memorized factual properties, mapping directly onto Davies' requirements. This theoretical defense asserts that LLMs organize data to satisfy the rigorous criteria for tacit knowledge, rather than relying on superficial statistical correlations.

The author's analysis of existing causal tracing experiments confirms that LLMs satisfy all three constraints for tacit knowledge. Specifically, the evaluation identifies that factual associations are structurally localized within MLP modules, with a distinct concentration in the middle layers of the network. Furthermore, the analysis of model editing interventions‚Äîsuch as successfully altering the Eiffel Tower‚Äôs location‚Äîdemonstrates causal systematicity; modifications to internal representations are not isolated but propagate effectively to semantically related prompts while maintaining grammatical integrity. This confirms that the model‚Äôs knowledge is structurally represented and causally efficacious.

By definitively establishing that LLMs possess causal systematicity, this work shifts the epistemological understanding of modern AI from statistical mimicry to systems capable of tacit knowledge. This theoretical validation provides researchers and engineers with a robust framework for describing and intervening upon model behaviors, significantly enhancing the precision of interpretability and safety engineering.

---

## üîç Key Findings

*   **Acquisition of Tacit Knowledge:** The research demonstrates that LLMs are capable of acquiring 'tacit knowledge' as defined by Martin Davies (1990), directly refuting the original assertion that neural networks cannot possess this capability.
*   **Constraint Satisfaction:** Specific architectural features of LLMs satisfy the three required constraints for tacit knowledge:
    *   **Semantic Description**
    *   **Syntactic Structure**
    *   **Causal Systematicity**
*   **Framework Validity:** Tacit knowledge is established as a valid conceptual framework for describing, explaining, and intervening upon LLM behaviors.

---

## üß© Methodology

This paper utilizes a **theoretical and conceptual analysis approach** rather than empirical experimentation. The process involved:

1.  **Literature Review:** Evaluating the philosophical constraints and definitions established by Martin Davies (1990).
2.  **Architectural Mapping:** Assessing the properties of Large Language Models (specifically Transformer architectures) against these philosophical criteria.
3.  **Constraint Testing:** Specifically testing architectures against the three core criteria regarding semantic description, syntactic structure, and causal systematicity.

---

## üõ† Technical Details

| Component | Description |
| :--- | :--- |
| **Architecture** | Large Language Models based on **Transformers**, utilizing attention mechanisms for long-range dependencies and optimized for next-word prediction. |
| **Operational Framework** | Tacit knowledge is operationalized using **Martin Davies' framework** (Semantic Description, Syntactic Structure, Causal Systematicity). |
| **Knowledge Representation** | Unlike symbolic AI, these models rely on network weights. The paper hypothesizes that **MLP modules** act as **Key-Value pairs**: <br>‚Ä¢ **Keys:** Represent input subjects <br>‚Ä¢ **Values:** Represent memorized properties |
| **Comparison** | Distinct from symbolic AI approaches; relies on distributed representations within network weights. |

---

## üöÄ Contributions

*   **Theoretical Refutation:** Offers a direct counter-argument to the view that neural networks cannot acquire tacit knowledge.
*   **Epistemological Clarity:** Provides a precise definition for what LLMs 'know,' moving beyond "statistical correlation" arguments.
*   **Operational Framework:** Establishes a practical link between philosophical theory and AI application, enabling researchers and engineers to use tacit knowledge as a tool for:
    *   Describing behaviors
    *   Explaining model outputs
    *   Performing interventions on LLMs

---

## üìà Results

*   **Localization of Associations:** Causal tracing experiments identified that **MLP layers** (particularly middle layers) represent factual associations as key-value memories.
*   **Successful Manipulation:** Model editing experiments successfully manipulated these internal representations (e.g., changing the Eiffel Tower's location).
*   **Demonstration of Systematicity:** The successful edits demonstrated **causal systematicity** and generalization to semantically similar prompts.
*   **Structural Representation:** Results confirm that LLMs explicitly learn grammatical syntax, while strongly suggesting that semantic aspects of knowledge are also structurally represented.

---

**Assessment Quality Score:** 8/10
**References Included in Analysis:** 0