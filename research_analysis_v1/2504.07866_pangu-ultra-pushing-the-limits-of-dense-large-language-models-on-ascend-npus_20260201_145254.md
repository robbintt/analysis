# Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs

*Yichun Yin, Wenyong Huang, Kaikai Song, Yehui Tang, Xueyu Wu, Wei Guo, Peng Guo, Yaoyuan Wang, Xiaojun Meng, Yasheng Wang, Dong Li, Can Chen, Dandan Tu, Yin Li, Fisher Yu, Ruiming Tang, Yunhe Wang, Baojun Wang, Bin Wang, Bo Wang, Boxiao Liu, Changzheng Zhang, Duyu Tang, Fei Mi, Hui Jin, Jiansheng Wei, Jiarui Qin, Jinpeng Li, Jun Zhao, Liqun Deng, Lin Li, Minghui Xu, Naifu Zhang, Nianzu Zheng, Qiang Li, Rongju Ruan, Shengjun Cheng, Tianyu Guo, Wei He, Wei Li, Weiwen Liu, Wulong Liu, Xinyi Dai, Yonghan Dong, Yu Pan, Yue Li, Yufei Wang, Yujun Li, Yunsheng Ni, Zhe Liu, Zhenhe Zhang, Zhicheng Liu*

---

> ### ðŸ“Š Quick Facts
> *   **Model Parameters:** 135 Billion (Dense)
> *   **Architecture:** Dense Transformer (94 Layers)
> *   **Training Data:** 13.2 Trillion Tokens
> *   **Hardware Infrastructure:** 8,192 Ascend NPUs
> *   **Key Innovation:** Depth-Scaled Sandwich-Norm (DSSN)
> *   **Performance:** Outperforms Llama 405B; Competitive with DeepSeek-R1

---

## Executive Summary

This research addresses the dual challenge of optimization instability in ultra-deep dense Large Language Models (LLMs) and the expansion of hardware interoperability beyond the dominant GPU ecosystem. As models scale beyond 100 billion parameters, deep networks often suffer from training failures such as loss spikes and divergence, exacerbated by increased depth. Furthermore, efficient training at this scale has been restricted to specific, often proprietary, hardware stacks. This paper is significant because it aims to prove that a dense 135-billion parameter model can be trained stably on Ascend NPUs, rivaling the performance of significantly larger sparse models (like DeepSeek-R1) and massive dense architectures (like Llama 405B), challenging the prevailing assumption that sheer parameter scale or sparsity is strictly necessary for state-of-the-art results.

The core technical innovation is the introduction of **Depth-Scaled Sandwich-Norm (DSSN)** and a **"TinyInit"** strategy designed to ensure stability in 94-layer architectures. DSSN mitigates training loss spikes by scaling normalization gamma parameters relative to network depth, while TinyInit adjusts the standard deviation of initialization based on both network depth and width. The model utilizes a dense Transformer configuration with 135 billion parameters, Group Query Attention (GQA), SwiGLU activation, and a domain-aware tokenizer with 153,376 tokens. A critical systems-level achievement was the execution of distributed training across 8,192 Ascend NPUs using advanced system optimizations. The methodology also employed a rigorous post-training phase specifically engineered to enhance reasoning capabilities, moving beyond standard pre-training.

Pangu Ultra demonstrated robust convergence, successfully resolving training loss spikes through DSSN, and delivered compelling performance against state-of-the-art baselines. Pre-trained on a massive corpus of 13.2 trillion tokens, the 135B dense model significantly outperformed Llama 405B and Mistral Large 2 on standard benchmarks. Crucially, Pangu Ultra achieved performance competitive with DeepSeek-R1, a much larger sparse model, effectively rivaling architectures that rely on massive sparsity or significantly higher parameter counts. These quantitative results validate that the architectural innovations allowed the model to maximize the utility of its dense parameters, achieving efficiency and stability that surpass much larger counterparts.

The significance of this work is twofold: it offers a viable alternative hardware path for training frontier models and introduces a generalizable algorithmic improvement for deep network stability. By providing a comprehensive proof-of-concept for training 100B+ parameter dense models on Ascend NPUs, the study diversifies the infrastructure landscape for AI development, reducing reliance on single-vendor ecosystems. Moreover, by demonstrating that a smaller, well-trained dense model can match or exceed the performance of much larger sparse models, the paper challenges current architectural trends and establishes Depth-Scaled Sandwich-Norm as a critical technique for future deep learning research.

---

## Key Findings

*   **Superior Performance:** Pangu Ultra (135B) significantly outperforms state-of-the-art dense LLMs such as Llama 405B and Mistral Large 2.
*   **Competitive with Sparse Models:** Achieves performance competitive with the larger sparse model DeepSeek-R1.
*   **Training Stability:** Successfully resolved training stability issues and loss spikes through 'depth-scaled sandwich normalization'.
*   **Hardware Validation:** Validated the capability of Ascend NPUs to efficiently train dense models with over 100 billion parameters.

---

## Technical Details

**Architecture & Configuration**
*   **Parameters:** 135 Billion
*   **Layers:** 94
*   **Hidden Dimension:** 12,288
*   **Feed-Forward Network:** SwiGLU activation with an intermediate size of 28,672
*   **Attention Mechanism:** Group Query Attention (GQA) with 96 query heads and 8 KV heads
*   **Tokenizer:** Domain-aware tokenizer with 153,376 tokens

**Training Innovations**
*   **Depth-Scaled Sandwich-Norm (DSSN):** Scales normalization gamma parameters by network depth to mitigate loss spikes.
*   **TinyInit:** An initialization strategy scaling standard deviation by depth and width to stabilize deep networks.

**System Specs**
*   **Hardware:** Ascend NPUs
*   **Scale:** 8,192 NPUs

---

## Methodology

The study utilized a dense Transformer architecture configured with 135 billion parameters. To ensure training stability, the researchers introduced 'depth-scaled sandwich normalization' to mitigate loss spikes. The model was pre-trained on a massive corpus of 13.2 trillion diverse tokens, followed by a post-training phase to enhance reasoning. Training was executed on a distributed system of 8,192 Ascend NPUs using various system-level optimizations.

---

## Contributions

*   **Algorithmic Innovation:** Introduced 'depth-scaled sandwich normalization' as a solution for deep neural network stability.
*   **Hardware Ecosystem Proof-of-Concept:** Provided comprehensive proof-of-concept for training 100B+ parameter dense models on the Ascend NPU ecosystem.
*   **Architectural Efficiency:** Demonstrated that smaller, well-trained dense models can rival the performance of much larger sparse architectures.
*   **System Scaling:** Showcased advances in system scaling by coordinating training across thousands of NPUs.

---

## Results

Pangu Ultra significantly outperforms state-of-the-art dense models Llama 405B and Mistral Large 2, and achieves competitive performance with the sparse model DeepSeek-R1. The Depth-Scaled Sandwich-Norm (DSSN) technique successfully resolved training loss spikes and divergence issues typical of 94-layer dense models. The training process also validated the efficiency of Ascend NPUs in training dense models exceeding 100 billion parameters.

---

**Quality Score:** 8/10 | **References:** 40 citations