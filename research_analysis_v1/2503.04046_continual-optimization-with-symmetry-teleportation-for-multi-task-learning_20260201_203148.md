# Continual Optimization with Symmetry Teleportation for Multi-Task Learning

*Zhipeng Zhou; Ziqiao Meng; Pengcheng Wu; Peilin Zhao; Chunyan Miao*

***

> ### ðŸ“Š Quick Facts & Metrics
>
> *   **Core Methodology:** Continual Optimization with Symmetry Teleportation (COST)
> *   **Primary Mechanism:** Low-Rank Adapters (LoRA) for loss-equivalent teleportation
> *   **Key Datasets:** CelebA (40-task), NYUv2 (3-task)
> *   **Performance:** Conflict Ratio reduced to **0.007 - 0.011**
> *   **Optimization:** Achieved Loss Invariance ($L_t$) of 0.914 and Sharpness ($L_g$) of 0.924
> *   **Type:** Plug-and-play optimization module
> *   **Quality Score:** 8/10

***

## Executive Summary

Multi-Task Learning (MTL) seeks to improve model generalization by learning multiple tasks simultaneously, but it frequently encounters two critical optimization bottlenecks: **gradient conflict** and **task imbalance**. These issues arise when the gradient directions for different tasks oppose one another, forcing the model to oscillate or converge to suboptimal solutions that favor dominant tasks while neglecting others. Traditional remedies, such as loss or gradient reweighting, attempt to balance these competing forces but often fail to resolve the fundamental geometric incompatibilities within the loss landscape, leading to unstable training processes.

To address these challenges, the authors propose **Continual Optimization with Symmetry Teleportation (COST)**, a novel optimization paradigm that resolves conflicts by relocating the model to alternative, loss-equivalent points on the landscape rather than manipulating gradients directly. Practically implemented via Low-Rank Adapters (LoRA), COST employs a dual-objective function to navigate the model's position: one term minimizes task loss fluctuation to ensure "loss invariance," while the other maximizes sharpness as a proxy for finding favorable gradient directions. This approach is complemented by a historical trajectory reuse strategy, which preserves the benefits of advanced optimizer states (like momentum) even after the model parameters are teleported to a new location.

Experimental evaluations on the CelebA (40-task) and NYUv2 (3-task) datasets demonstrate COST's ability to effectively suppress optimization conflicts and enhance downstream performance. The method achieved a Conflict Ratio between **0.007 and 0.011** over 600 iterationsâ€”near-zero compared to standard baselinesâ€”indicating minimal interference between tasks. These optimization stability gains translated to measurable improvements in task-specific accuracy and F1 scores over state-of-the-art baselines. The dual-objective optimization proved stable, with Loss Invariance ($L_t$) decreasing from 0.922 to 0.914 and Sharpness ($L_g$) increasing to 0.924, while qualitative analysis confirmed COST successfully reached the Pareto front in scenarios where Linear Scalarization failed.

COST represents a significant shift in MTL research, moving the focus from gradient manipulation to restructuring the optimization path via symmetry teleportation. By effectively decoupling conflict resolution from the gradient descent steps, COST offers a robust solution to the long-standing problem of task interference. Its implementation as a plug-and-play LoRA module makes it a versatile and practical tool that enhances existing state-of-the-art MTL methods without requiring architectural changes.

---

## Key Findings

*   **Conflict Resolution via Teleportation:** The proposed COST method effectively addresses optimization conflict and task imbalance in Multi-Task Learning (MTL) by relocating the model to alternative, loss-equivalent points on the loss landscape rather than relying on reweighting losses or gradients.
*   **Practical LoRA Implementation:** Implementation via Low-Rank Adapters (LoRA) allows for practical, convergent, and loss-invariant teleportation.
*   **Versatile Integration:** COST serves as a versatile, plug-and-play solution that enhances existing MTL methods without the need for architectural overhauls.
*   **Optimizer Benefits Preserved:** The method incorporates a historical trajectory reuse strategy to ensure the continual utilization of advanced optimizers' benefits (like momentum) despite parameter shifts.

---

## Methodology

The research introduces **'Continual Optimization with Symmetry Teleportation' (COST)**, a novel optimization framework designed specifically for Multi-Task Learning.

1.  **Symmetry Teleportation:** Instead of adjusting gradients, the method identifies conflict scenarios during optimization and teleports the model to a different point on the loss landscape. This new point yields an equivalent loss value but presents fewer conflicts.
2.  **LoRA-Based Objectives:** Practically, teleportation is achieved using Low-Rank Adapters (LoRA). Specific objectives are designed that are both convergent and loss-invariant.
3.  **Historical Trajectory Reuse:** The methodology includes a strategy for reusing historical trajectories to maintain the advantages provided by advanced optimizers (e.g., Adam) throughout the training process, ensuring that teleportation does not discard optimizer momentum.

---

## Contributions

The research makes four distinct contributions to the field of Multi-Task Learning:

1.  **Novel Optimization Paradigm:** Proposes a shift from traditional gradient/loss reweighting to a symmetry-based teleportation approach.
2.  **Technical Implementation:** Introduces a technical implementation utilizing Low-Rank Adapters (LoRA) to facilitate practical and efficient symmetry teleportation via convergent, loss-invariant objectives.
3.  **Trajectory Preservation:** Introduces a historical trajectory reuse strategy to preserve the benefits of advanced optimizers despite parameter teleportation.
4.  **Broad Applicability:** Demonstrates broad applicability through a plug-and-play module that improves the performance of existing state-of-the-art MTL methods.

---

## Technical Details

**Core Mechanism**
*   **Symmetry Teleportation via LoRA:** Utilizes Low-Rank Adapters to relocate the model to a 'loss-equivalent' point with favorable gradients.

**Objective Function**
*   **Dual-Objective:** Optimization is driven by `L_lora = L_t - gamma L_g`.
*   **Loss Invariance ($L_t$):** Ensures stability by minimizing task loss fluctuation.
*   **Gradient Maximization ($L_g$):** Uses Sharpness as a proxy for gradient maximization via random sampling to avoid computationally expensive Hessian calculation.

**Imbalance Handling**
*   **Weighting Scheme:** Task imbalance is managed using a weighting scheme `R` based on inverse gradient norms.

---

## Results

### Experimental Setup
*   **Datasases:** CelebA (40-task) and NYUv2 (3-task).
*   **Metrics:** Conflict Ratio, Dominated Conflict, K/2 Condition.

### Quantitative Results
*   **Conflict Suppression:** Conflict ratio ranged between **0.007 and 0.011** over 600 iterations, indicating near-zero conflict.
*   **Optimization Stability:** Loss Invariance ($L_t$) decreased from 0.922 to 0.914, while Sharpness ($L_g$) increased to 0.924.

### Qualitative Results
*   **Pareto Efficiency:** COST successfully reached the Pareto front in toy examples where Linear Scalarization (LS) failed.

***

**Report Generated based on 32 Citations**
**Quality Assessment:** 8/10