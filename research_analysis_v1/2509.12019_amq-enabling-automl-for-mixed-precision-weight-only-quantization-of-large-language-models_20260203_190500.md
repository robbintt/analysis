---
title: 'AMQ: Enabling AutoML for Mixed-precision Weight-Only Quantization of Large
  Language Models'
arxiv_id: '2509.12019'
source_url: https://arxiv.org/abs/2509.12019
generated_at: '2026-02-03T19:05:00'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# AMQ: Enabling AutoML for Mixed-precision Weight-Only Quantization of Large Language Models

*Authors: Sangjun Lee; Seung-taek Woo; Jungyu Jin; Changhun Lee; Eunhyeok Park*

---

### ðŸ“Š Quick Facts

| Metric | Details |
| :--- | :--- |
| **Quality Score** | 9/10 |
| **Citations** | 40 |
| **Models Tested** | Llama 2 (7B, 13B) |
| **Hardware** | NVIDIA L40S GPUs |
| **Key Performance** | 1.4xâ€“1.5x speedup over BitStack |
| **Effective Precision** | ~3.3 bits (avg) |
| **Llama 2-7B Perplexity** | **5.75** (vs. GPTQ-4bit's 6.16) |
| **Llama 2-7B Accuracy** | **70.42%** (vs. BitStack's 69.91%) |

---

## Executive Summary

**Problem**
Deploying Large Language Models (LLMs) on resource-constrained hardware is severely bottlenecked by the massive memory footprint of high-precision weights. While weight-only quantization reduces bit-widths, determining the optimal layer-wise assignment of bit-widthsâ€”known as mixed-precision quantizationâ€”is computationally intractable. The search space for assigning precision across layers exceeds $10^{100}$ possible configurations, rendering conventional black-box optimization methods infeasible. This creates a critical barrier for maximizing model quality under strict memory constraints, as manually tuning these layers or applying uniform quantization fails to capture the varying sensitivity of different network components.

**Innovation**
The authors propose **AMQ** (Automated Mixed-Precision Weight-Only Quantization), an AutoML framework that formulates quantization as a discrete combinatorial optimization task using the NSGA-II algorithm. To navigate the massive search space efficiently, AMQ introduces a composite strategy of four technical components:
*   **Search Space Pruning:** Eliminates unpromising configurations based on prior knowledge.
*   **Quantization Proxy:** Bypasses expensive format conversions.
*   **Quality Predictor:** Estimates performance without costly full validation.
*   **Iterative Search-and-Update:** Ensures stable convergence.
This approach leverages the observation that layer sensitivity varies significantly, allowing the system to converge on optimal solutions that balance compression and accuracy.

**Results**
Experimental evaluations on Llama 2 (7B and 13B) models demonstrate that AMQ identifies configurations on the Pareto frontier, achieving performance superior to established baselines. Operating at an effective average precision of approximately 3.3 bits, AMQ outperforms uniform 4-bit baselines and BitStack. Specifically, AMQ delivered an inference speedup of **1.4x to 1.5x** compared to BitStack by avoiding the latency penalties associated with BitStack's slow weight reconstruction.

**Impact**
This research transforms a previously intractable optimization problem into a viable, automated workflow for LLM deployment. By drastically reducing search time and resource requirements, AMQ establishes a new standard for balancing model compression with the retention of reasoning capabilities. The release of the AMQ codebase provides a practical tool that facilitates the immediate deployment of compact, high-performing LLMs, offering a path forward for edge computing and real-time applications.

---

## Key Findings

*   **Tractable Optimization:** AMQ successfully addresses the challenge of deploying LLMs under strict memory constraints by optimizing the trade-off between model quality and memory usage.
*   **Massive Search Space:** The framework effectively navigates a combinatorial search space exceeding $10^{100}$ possible configurations, a scale that renders conventional black-box optimization infeasible.
*   **Pareto Frontier Achievement:** By efficiently exploring the quality-efficiency landscape, AMQ is capable of reaching the Pareto frontier, identifying models that are both compact (memory-efficient) and high-performing.
*   **Layer Variance:** The approach relies on the observation that quantization sensitivity varies significantly across layers, allowing for non-uniform optimization.

---

## Methodology

The authors propose **AMQ**, an AutoML framework designed to assign layer-wise quantization bit-widths. To overcome the computational intractability of the search space, the methodology integrates four specific technical components:

1.  **Search Space Pruning**
    Utilizes prior knowledge to exclude unpromising configurations early in the process.
2.  **Quantization Proxy**
    Implements a bypass mechanism to avoid costly format conversions during the search phase.
3.  **Quality Predictor**
    Utilizes a predictive model to minimize the computational overhead of evaluating model quality.
4.  **Iterative Search-and-Update Strategy**
    Employs a cyclical optimization process to ensure fast and stable convergence to the best configurations.

---

## Technical Details

*   **Objective:** Find optimal layer-wise quantization bit-width assignments for LLMs to maximize quality under memory constraints.
*   **Formulation:** Discrete combinatorial optimization task.
*   **Algorithm:** Uses NSGA-II to build a Pareto frontier.
*   **Core Innovation:** A composite strategy to handle a search space exceeding $10^{100}$ combinations without exhaustive search.
*   **Layer Sensitivity:** The method exploits the fact that different layers in an LLM react differently to quantization, allowing for mixed-precision assignment rather than uniform bit-width reduction.

---

## Contributions

*   **Solving the Intractable:** Provides a viable solution to the problem of optimizing mixed-precision quantization for LLMs, where standard optimization techniques fail due to the exponential size of the search space.
*   **Algorithmic Efficiency:** Introduces a composite algorithmic strategy (pruning, proxy, prediction, and iterative updating) that drastically reduces the search time and resource requirements for quantization.
*   **Benchmark Performance:** Demonstrates the ability to automatically identify models on the Pareto frontier, establishing a new standard for balancing compression and accuracy in LLM deployment.
*   **Open Source:** Contributes the AMQ codebase to the public domain, facilitating further research and practical application.

---

## Results & Evaluation

**Experimental Setup**
*   **Models:** Llama 2 (7B and 13B)
*   **Hardware:** NVIDIA L40S GPUs

**Performance Metrics**
*   Zero-shot Average Accuracy (ARC-Easy, ARC-Challenge, PIQA, HellaSwag, WinoGrande, BoolQ)
*   Perplexity on WikiText-2
*   Memory Usage (GB)
*   Inference Speed (Tokens/s)

**Outcomes**
*   **Efficiency:** AMQ efficiently navigates the massive search space compared to black-box methods.
*   **Non-Uniformity:** Demonstrates high non-uniformity in layer sensitivity, validating the mixed-precision approach.
*   **Optimization:** Successfully reaches the Pareto frontier and optimizes inference speed better than methods like BitStack (which suffer from slow weight reconstruction).