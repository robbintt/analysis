---
title: 'Preserve-Then-Quantize: Balancing Rank Budgets for Quantization Error Reconstruction
  in LLMs'
arxiv_id: '2602.02001'
source_url: https://arxiv.org/abs/2602.02001
generated_at: '2026-02-03T20:21:29'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Preserve-Then-Quantize: Balancing Rank Budgets for Quantization Error Reconstruction in LLMs

*Yoonjun Cho; Dongjae Jeon; Soeun Kim; Moongyu Jeon; Albert No*

---

> ### ðŸ“Š Quick Facts
> *   **Performance Gain:** +5.9 percentage points on GLUE benchmark (2-bit QPEFT).
> *   **Key Innovation:** "Preserve-Then-Quantize" strategy with Structured Residual Reconstruction (SRR).
> *   **Perplexity (LLaMA-2 7B):** 10.72 (SRR) vs. 10.87 (Baseline).
> *   **Compatibility:** Quantizer-agnostic (works with MXINT, GPTQ, QuIP#).
> *   **Quality Score:** 9/10

---

## Executive Summary

Current Quantization Error Reconstruction (QER) methods for Large Language Models (LLMs) suffer from a fundamental inefficiency in rank budget allocation. These existing approaches typically dedicate the full rank budget to reconstructing quantization errors, a strategy that fails when weight matrices are low-rank or when dominant weight directions are corrupted. Furthermore, Quantized Parameter-Efficient Fine-Tuning (QPEFT) faces significant challenges regarding training stability.

The paper introduces **Structured Residual Reconstruction (SRR)**, a novel framework built on a "Preserve-Then-Quantize" strategy. Instead of using the entire rank budget for error correction, SRR strategically splits it: it preserves the top-$k$ singular subspace of activation-scaled weights and quantizes the residual component, utilizing the remaining rank budget ($r-k$) to reconstruct the quantization error of that residual. The preservation rank $k$ is determined by a theory-guided criterion that balances quantization-exposed energy against unrecoverable error.

SRR delivers substantial performance gains over existing baselines like QERA. In 2-bit QPEFT scenarios on the GLUE benchmark, SRR outperformed QERA by **5.9 percentage points**. On generative models, it achieved lower perplexity and higher accuracy on LLaMA-2 7B and LLaMA-3.1 8B, while demonstrating faster training convergence.

---

## Key Findings

*   **Suboptimal Existing Methods:** Current Quantization Error Reconstruction (QER) methods are inefficient as they allocate the full rank budget to error reconstruction. This approach fails on low-rank weights or when dominant directions are corrupted.
*   **Strategic Rank Splitting:** The proposed **Structured Residual Reconstruction (SRR)** framework achieves lower perplexity by strategically splitting the rank budget rather than allocating it entirely to reconstruction.
*   **Fine-Tuning Stability:** SRR enhances fine-tuning stability for Quantized Parameter-Efficient Fine-Tuning (QPEFT) through the implementation of gradient scaling.
*   **Benchmark Performance:** The method achieves an average performance gain of **5.9 percentage points** on the GLUE benchmark in 2-bit QPEFT scenarios.

---

## Methodology

The paper proposes **Structured Residual Reconstruction (SRR)**, a rank-allocation framework based on a 'Preserve-Then-Quantize' strategy.

1.  **Preservation:** The framework preserves the top-$k$ singular subspace of activation-scaled weights.
2.  **Quantization:** It quantizes the residual component of the weights.
3.  **Reconstruction:** The remaining rank budget ($r-k$) is utilized to reconstruct the quantization error of the residual component.
4.  **Rank Selection:** The preservation rank $k$ is determined by a theory-guided criterion designed to balance quantization-exposed energy against unrecoverable error.

---

## Technical Details

*   **Core Mechanism:** Structured Residual Reconstruction (SRR) using a 'Preserve-Then-Quantize' mechanism.
*   **Rank Allocation:**
    *   **Preserved Rank ($k^*$):** Used for dominant weight directions.
    *   **Residual Rank ($r - k^*$):** Used for error reconstruction.
*   **Adaptivity:** Rank selection ($k^*$) is adaptive based on projection type and QERA-exact scaling.
*   **Gradient Scaling:** Applies a fixed gradient scaling factor ($\gamma = 0.1$) to top-$k^*$ directions to ensure stability in Quantized PEFT.
*   **Compatibility:** The method is quantizer-agnostic and is compatible with MXINT, GPTQ, and QuIP#.

---

## Contributions

*   **Theoretical Framework:** Introduction of a theoretical framework for rank allocation to select the optimal preservation rank $k$.
*   **Novel Parameterization:** A new parameterization strategy that decouples dominant subspace preservation from residual quantization.
*   **Bridging PTQ and QPEFT:** Successfully bridges Post-Training Quantization (PTQ) and QPEFT by demonstrating SRR's dual-purpose utility in reducing accuracy loss and improving gradient stability.

---

## Results

### GLUE Benchmark (RoBERTa-base with MXINT)
*   **2-bit:** SRR (78.43) vs QERA (72.51) â€” **+5.9 pp**
*   **3-bit:** SRR (81.62) vs QERA (76.96) â€” **+4.7 pp**
*   **4-bit:** SRR (84.62) vs QERA (83.15) â€” **+1.5 pp**

### LLaMA-2 7B (2-bit)
*   **SlimPajama PPL:** 10.72 (SRR) vs 10.87 (Baseline)
*   **GSM8K Accuracy:** 18.29% (SRR) vs 16.83% (Baseline)

### LLaMA-3.1 8B (2-bit)
*   **PPL:** 19.62 (SRR) vs 21.94 (Baseline)
*   **Accuracy:** 26.87% (SRR) vs 25.11% (Baseline) â€” **+1.7 pp**

### Training & Ablation
*   **Convergence:** SRR showed faster training convergence and consistent perplexity reduction on GPTQ and QuIP#.
*   **Ablation Study:** Gradient scaling alone (QERA+SGP) lowered scores (71.35 vs QERA 72.51), validating the necessity of the rank splitting structure.

---

*Quality Score: 9/10*
*References: 40 citations*