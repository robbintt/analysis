---
title: Benchmarking Post-Training Quantization of Large Language Models under Microscaling
  Floating Point Formats
arxiv_id: '2601.09555'
source_url: https://arxiv.org/abs/2601.09555
generated_at: '2026-02-03T18:56:36'
quality_score: 8
citation_count: 27
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Benchmarking Post-Training Quantization of Large Language Models under Microscaling Floating Point Formats

*Manyi Zhang; Ji-Fu Li; Zhongao Sun; Haoli Bai; Hui-Ling Zhen; Zhenhua Dong; Xianzhi Yu*

---

> ### üìä Quick Facts
>
> *   **Algorithms Evaluated:** 7+ distinct PTQ methods
> *   **Benchmarks Covered:** 15 different evaluation sets
> *   **Model Families:** 3 (including LLMs & Multimodal)
> *   **Key Formats:** MXFP8 (E4M3) & MXFP4 (E2M1)
> *   **Technical Focus:** Block-level scaling (Block size 32, UE8M0)

---

## üìù Executive Summary

As Large Language Models (LLMs) grow in size, efficient deployment via quantization is critical, yet the vast majority of existing research focuses exclusively on integer-based quantization. The Microscaling Floating-Point (MXFP) data type‚Äîrecently introduced as an OCP standard for AI‚Äîoffers a compelling alternative by utilizing block-level scaling to maintain dynamic range, but its behavior under Post-Training Quantization (PTQ) remains largely unexplored. This paper addresses the significant gap in understanding how existing PTQ algorithms translate to MXFP formats, specifically investigating whether the methods optimized for integers can effectively handle the unique error characteristics of floating-point block scaling without costly retraining.

The authors provide the first large-scale systematic benchmark of PTQ algorithms adapted for MXFP formats, evaluating over seven distinct algorithms across four major paradigms: Channel-wise Transformation (e.g., SmoothQuant), Error Compensation (e.g., GPTQ), Rotational Transformation (e.g., QuaRot), and Affine Transformation (e.g., FlatQuant). Technically, the study focuses on MXFP8 (E4M3) and MXFP4 (E2M1) formats using a shared exponent scale per block of 32 elements. A key technical contribution is the identification of the scaling factor as the primary source of error in MXFP4 implementations; to address this, the authors propose a "pre-scale optimization" strategy that adjusts the scaling factor prior to quantization to significantly mitigate accuracy degradation.

The evaluation across models like Llama-3.1-8B and Qwen2.5-VL-7B reveals that while MXFP8 consistently achieves near-lossless performance, MXFP4 suffers from substantial accuracy degradation and remains difficult to implement effectively. Specifically, W8A8 configurations were found to be consistently lossless (‚â§1% degradation), while W4A4 configurations were deemed "risky" with recovery rates as low as 86.37%. The study highlights W4A8 as a critical inflection point where effective PTQ algorithms can recover performance, demonstrated by openPangu-Embedded-7B improving from 95.44% to 98.60% accuracy. Furthermore, Error Compensation and Affine Transformation paradigms were identified as the most compatible with MXFP, while multimodal sensitivity was found to be dominated by the language model component rather than the vision encoder.

This research provides a crucial roadmap for the industry‚Äôs shift from integer to floating-point quantization standards, offering actionable evidence for hardware designers and model engineers considering the adoption of MXFP. By establishing that specific algorithmic paradigms (Error Compensation and Affine) outperform others in the MXFP context, the study allows practitioners to prioritize effective optimization strategies. The findings effectively validate MXFP8 as a safe, high-efficiency format for broad deployment while cautioning that MXFP4 requires further algorithmic refinement‚Äîspecifically regarding scaling factor optimization‚Äîbefore it can be considered viable for production-grade LLMs.

---

## üîë Key Findings

*   **Format Performance Disparity:** MXFP8 formats consistently achieve near-lossless performance, whereas MXFP4 formats introduce substantial accuracy degradation and remain technically challenging to implement effectively.
*   **Algorithm Dependence:** The effectiveness of Post-Training Quantization (PTQ) under MXFP is heavily dependent on format compatibility, with specific algorithmic paradigms proving consistently more effective than others.
*   **Cross-Modal Consistency:** PTQ performance trends remain highly consistent across different model families and modalities; notably, in multimodal LLMs, quantization sensitivity is dominated by the language model component rather than the vision encoder.
*   **Scaling Factor Optimization:** The scaling factor is identified as a critical source of error in MXFP4, and its impact can be significantly mitigated through a simple pre-scale optimization strategy.

---

## üî¨ Methodology

The authors conducted a systematic investigation and benchmarking study of Post-Training Quantization (PTQ) algorithms specifically adapted for Microscaling Floating-Point (MXFP) formats. This evaluation encompassed a wide scope of parameters, including:

*   **Algorithms:** Over 7 distinct PTQ algorithms
*   **Benchmarks:** 15 different evaluation benchmarks
*   **Models:** 3 separate families of Large Language Models (LLMs)

---

## ‚ú® Contributions

*   **Bridging the Research Gap:** Addresses the lack of exploration regarding the applicability and behavior of existing PTQ algorithms under MXFP formats, shifting focus from the predominant integer quantization research.
*   **Large-Scale Comparative Analysis:** Provides a broad dataset across algorithms, benchmarks, and model architectures within the MXFP context.
*   **Actionable Optimization Strategies:** Offers actionable insights for adapting PTQ methods to MXFP, specifically identifying the scaling factor as a bottleneck for MXFP4 and proposing a pre-scale optimization strategy to improve accuracy.

---

## ‚öôÔ∏è Technical Details

### Format Specifications
*   **Microscaling Floating Point (MXFP):** A block-level number format utilizing a shared scale per block.
*   **Configuration:** Block size 32, UE8M0.
*   **Variants Studied:**
    *   **MXFP8:** E4M3
    *   **MXFP4:** E2M1

### PTQ Paradigms Evaluated
The study evaluated four primary transformation paradigms:
1.  **Channel-wise Transformation:** SmoothQuant, AWQ
2.  **Error Compensation:** GPTQ, MR-GPTQ
3.  **Rotational Transformation:** QuaRot, SpinQuant
4.  **Affine Transformation:** FlatQuant

### Configuration & Optimization
*   **Notation:** Quantization configurations follow the `W{bits}A{bits}[KV{bits}]` format.
*   **Optimization Strategy:** The study employs a pre-scale optimization strategy to mitigate scaling factor errors in MXFP4.

---

## üìà Results

### Evaluation Scope
*   **Models:** Llama-3.1-8B, openPangu-Embedded-7B, and Qwen2.5-VL-7B.
*   **Tasks:** Language modeling, reasoning, and multimodal tasks.

### Performance Categorization
*   **Lossless (‚â§1%):** W8A8 is consistently lossless. MXFP8 achieves near-lossless performance.
*   **Fair (1-3%):** W4A8 acts as an inflection point where PTQ improves recovery (e.g., openPangu from 95.44% to 98.60%), though reasoning tasks remain risky.
*   **Risky (‚â•3%):** W4A4 is highly risky, with recovery rates ranging from 86.37% to 97.36%.

### Algorithm Compatibility
Error compensation and affine transformations are identified as most compatible with MXFP.

---

**Quality Score:** 8/10  
**References:** 27 citations