---
title: 'Sub-MoE: Efficient Mixture-of-Expert LLMs Compression via Subspace Expert
  Merging'
arxiv_id: '2506.23266'
source_url: https://arxiv.org/abs/2506.23266
generated_at: '2026-02-06T05:06:10'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Sub-MoE: Efficient Mixture-of-Expert LLMs Compression via Subspace Expert Merging

*Lujun Li; Zhu Qiyuan; Jiacheng Wang; Wei Li; Hao Gu; Sirui Han; Yike Guo*

---

> ### ðŸ“Š Quick Facts: Report Card
> 
> *   **Primary Focus**: Mixture-of-Experts (MoE) Model Compression
> *   **Core Technique**: Joint Singular Value Decomposition (SVD) & Subspace Merging
> *   **Top Performance Metric**: Retains **96%** of original performance with **25%** expert reduction (Mixtral-8x7B)
> *   **Aggressive Compression**: Maintains **86%** performance with **50%** expert reduction
> *   **Tested Architectures**: Mixtral, DeepSeek, Qwen-1.5|3
> *   **Quality Score**: 9/10

---

## Executive Summary

### Problem
Mixture-of-Experts (MoE) models provide a scalable architecture for large language models (LLMs) but introduce significant computational overhead due to their massive parameter count. While compressing these models via expert pruning or merging is essential for efficient deployment, current methods face a critical bottleneck: **"parameter conflicts."** Because experts become highly specialized, their weights often diverge significantly (low inter-expert similarity). Consequently, existing techniques that rely on direct weight averaging or naive clustering inadvertently merge conflicting parameters, leading to substantial performance degradation.

### Innovation
The authors propose **"Sub-MoE,"** a novel compression framework that resolves parameter conflicts by merging experts within a shared subspace rather than in direct weight space. The innovation lies in a two-phase process:
1.  **Adaptive Expert Clustering**: Groups experts based on output cosine similarity.
2.  **Subspace Expert Merging**: Utilizes Joint Singular Value Decomposition (SVD) on concatenated expert weights. By decomposing weights into a shared orthogonal basis (U-matrix) and specific singular vectors (V-matrices), the method isolates shared knowledge from expert-specific features. These components are then merged based on router activation frequencies.

### Results
Sub-MoE demonstrates superior performance over state-of-the-art methods like SEER-MoE and MC-SMoE across major architectures including Mixtral, DeepSeek, and Qwen-1.5|3. In testing on the Mixtral-8x7B model, the framework achieved a 25% reduction in experts while retaining **96%** of the original model's performance on zero-shot benchmarks. Even under aggressive compression with a 50% expert reduction, the model maintained **86%** of its baseline performance.

### Impact
This research establishes a new efficiency frontier for MoE model compression, theoretically validating that expert specialization does not preclude aggressive structural reduction. By effectively disentangling shared and specific parameters, Sub-MoE mitigates the primary trade-off between model size and inference capability. The framework is designed to be compatible with intra-expert compression techniques, allowing for integration into broader optimization pipelines.

---

## Key Findings

*   **High Compression with Minimal Performance Loss**: Sub-MoE achieves significant expert reduction (25% and 50%) on Mixtral-8x7B while maintaining **96%** and **86%** of the original model's performance on zero-shot benchmarks, respectively.
*   **Superiority Over Existing Methods**: The proposed framework significantly outperforms current state-of-the-art expert pruning and merging methods across tested models (Mixtral, DeepSeek, and Qwen-1.5|3).
*   **Resolution of Parameter Conflicts**: The core finding is that parameter conflicts arising from expert specialization can be effectively mitigated by extracting shared subspaces via joint Singular Value Decomposition (SVD).
*   **Compatibility with Further Optimization**: The framework is designed to align with intra-expert compression techniques, allowing for additional inference optimization beyond expert merging.

---

## Methodology

The Sub-MoE framework operates through a two-phase process designed to compress Mixture-of-Experts (MoE) models by aligning experts within a shared subspace.

### Phase 1: Adaptive Expert Clustering
Experts are grouped based on functional coherence using K-means clustering. The metric utilized is the cosine similarity of expert outputs, ensuring that experts performing similar functions are identified for potential merging.

### Phase 2: Subspace Expert Merging
Joint Singular Value Decomposition (SVD) is performed on concatenated expert weights (Experts Union Decomposition). This process involves:
1.  Extracting a shared **U-matrix** (shared subspace) across experts within the same cluster to reduce conflicting parameters.
2.  Merging the expert-specific **V components** based on frequency.
3.  Reconstructing the experts using the merged V-matrix within the shared subspace.

---

## Technical Details

**Pipeline Architecture**
Sub-MoE proposes a two-stage pipeline to compress Mixture-of-Experts models without performance degradation.

**Stage 1: Adaptive Expert Clustering**
*   Groups experts using K-means based on the average cosine similarity of their outputs over representative input tokens.
*   Dynamically optimizes cluster counts per layer.

**Stage 2: Subspace Expert Merging**
*   Addresses parameter conflicts caused by low inter-expert similarity (typically ranging between 0.1 and 0.3).
*   Employs Joint SVD on concatenated weight matrices to extract a shared orthogonal basis and singular values, aligning experts into a common subspace.
*   Merges the right singular vectors based on expert utilization frequency derived from router activation probabilities to reconstruct the final merged weights.

---

## Contributions

*   **Theoretical Insight**: Identifies "parameter conflicts" arising from expert specialization as a fundamental bottleneck in current expert merging methods and proposes a subspace-based solution to disentangle shared and specific components.
*   **Novel Framework**: Introduces "Sub-MoE," a comprehensive compression framework that uniquely combines adaptive clustering with joint SVD to fuse experts effectively without degrading model capabilities.
*   **Empirical Validation**: Provides extensive experimental evidence on major MoE architectures (Mixtral, DeepSeek, Qwen), establishing a new efficiency frontier by demonstrating that high retention of zero-shot performance is feasible despite aggressive expert reduction.

---

## Results Overview

| Model | Compression Rate | Performance Retention | Comparison |
| :--- | :--- | :--- | :--- |
| **Mixtral-8x7B** | 25% Reduction | 96% | Outperforms SEER-MoE & MC-SMoE |
| **Mixtral-8x7B** | 50% Reduction | 86% | Outperforms SEER-MoE & MC-SMoE |
| **DeepSeek / Qwen** | Varied | Superior to Baselines | Validates cross-architecture capability |

**Analysis**: The study identifies inter-expert similarity as typically ranging between **0.1 and 0.3**, highlighting the necessity of subspace alignment over simple weight averaging. Sub-MoE maintains high performance while achieving significant compression, proving that expert specialization does not preclude effective model compression.

---

**References:** 40 citations
**Quality Score:** 9/10