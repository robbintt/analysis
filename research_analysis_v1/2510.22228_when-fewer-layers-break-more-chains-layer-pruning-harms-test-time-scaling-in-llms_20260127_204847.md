---
title: 'When Fewer Layers Break More Chains: Layer Pruning Harms Test-Time Scaling
  in LLMs'
arxiv_id: '2510.22228'
source_url: https://arxiv.org/abs/2510.22228
generated_at: '2026-01-27T20:48:47'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# When Fewer Layers Break More Chains: Layer Pruning Harms Test-Time Scaling in LLMs

*Guinan Su, Max Planck, King Abdullah, Marco Canini, Keyu Wang, Lu Yin, Intelligent Systems, Tian Lyu, Shiwei Liu, Jonas Geiping*

---

> ### ðŸ“Š Quick Facts
> *   **Quality Score:** 8/10
> *   **References:** 40 Citations
> *   **Models Analyzed:** Qwen3-8B, Llama-3-8B
> *   **Method:** Layer Pruning via ShortGPT
> *   **Key Risk:** Catastrophic collapse in reasoning capabilities with minimal layer removal.

---

## Executive Summary

This research addresses a critical vulnerability in Large Language Model (LLM) compression techniques, specifically examining how layer pruning impacts test-time scaling. While layer pruning is a widely adopted strategy for reducing computational costs, prior evaluations have focused predominantly on static benchmarks, overlooking how structural damage affects dynamic, reasoning-intensive processes. The authors investigate whether the architectural simplification required for efficiency fundamentally compromises the complex reasoning mechanisms needed for test-time scaling.

The key innovation lies in a specialized evaluation framework that prioritizes test-time scaling behaviors over static accuracy metrics. The researchers analyze two distinct scaling regimes: **Sequential Scaling** (extending reasoning chains) and **Parallel Scaling** (generating diverse candidates). Furthermore, the authors perform a mechanistic analysis to diagnose failure modes within the modelâ€™s trajectory, identifying specific phenomena such as Recurring Loops, Reduced Trajectory Diversity, and Diminished Self-Reflection.

The findings reveal a **severe and asymmetric degradation** in model capabilities following pruning. While knowledge-intensive tasks (MMLU) showed only gradual performance decline, long-chain reasoning tasks (AIME24) suffered catastrophic collapse, dropping to near-zero accuracy with the removal of just 10% of layers. The study further demonstrated that standard recovery mechanisms are ineffective; neither Supervised Fine-Tuning (SFT) via LoRA nor full-parameter fine-tuning could restore the lost capabilities. This work suggests that reasoning capabilities are structurally fragile and cannot be easily relearned, providing critical guidance for future architecture designs and compression strategies.

---

## Key Findings

*   **Severe Impairment:** Test-time scaling is significantly damaged even when pruning a minimal number of layers (e.g., 10%).
*   **Asymmetric Degradation:** Long-chain reasoning capabilities collapse, while knowledge-intensive tasks remain relatively stable.
*   **Ineffective Recovery:** Standard Supervised Fine-Tuning (SFT), including LoRA and full-parameter tuning, fails to recover lost reasoning capabilities.
*   **Structural Fragility:** Reasoning mechanisms are fundamentally fragile to structural architectural damage.

---

## Technical Details

| Component | Description |
| :--- | :--- |
| **Base Model** | Qwen3-8B / Llama-3-8B |
| **Pruning Method** | Layer Pruning (primarily using **ShortGPT**) |
| **Selection Metric** | Block Influence (BI) scores used to identify and remove low-contribution layers. |
| **Scaling Regimes** | **Sequential Scaling** (extending reasoning chains) and **Parallel Scaling** (generating multiple candidates). |
| **Recovery Mechanisms** | Supervised Fine-Tuning (SFT) via LoRA and full-parameter fine-tuning. |
| **Diagnosed Failures** | Recurring Loops, Reduced Trajectory Diversity, Diminished Self-Reflection. |

---

## Methodology

The researchers employed a comprehensive approach to evaluate the structural integrity of pruned models:

*   **Test-Time Scaling Framework:** Shifted the focus from static metrics to test-time scaling capabilities.
*   **Comparative Benchmarking:** Evaluated performance discrepancy between long reasoning tasks (e.g., AIME24) and knowledge-intensive tasks (e.g., MMLU).
*   **Recovery Testing:** Tested standard SFT as a potential mechanism to restore lost capabilities post-pruning.
*   **Mechanistic Analysis:** Traced underlying causes of fragility by analyzing model behaviors and trajectories.

---

## Results

Experimental results highlight a stark contrast between knowledge retention and reasoning ability:

*   **Asymmetric Performance:** Knowledge tasks (MMLU) degraded gradually, whereas long-chain reasoning tasks (AIME24) collapsed after pruning a **single layer**.
*   **Catastrophic Collapse:** Accuracy on AIME24 dropped to near-zero with only 10% of layers removed.
*   **Contradiction of Prior Work:** These findings contradict prior claims (e.g., ShortGPT reporting 85% accuracy retention after 25% pruning).
*   **Failed Recovery:** Experiments demonstrated that recovery mechanisms (LoRA and full-parameter fine-tuning) fail to effectively restore test-time scaling capabilities.
*   **Evaluation Benchmarks:** Used MMLU, GSM8K, MATH500, and AIME24, employing LLM-as-a-Judge scoring for the latter two.

---

## Contributions

*   **Risk Identification:** Highlights a specific, critical risk of layer pruning for reasoning-intensive LLMs.
*   **Mechanistic Insights:** Provides the first insights into the mechanisms causing test-time scaling fragility.
*   **Strategic Guidance:** Offers guidance for rethinking architecture design and compression strategies to preserve reasoning.
*   **Open Resources:** Provides open-source resources for community replication.

---

**Report generated based on analysis of:** *When Fewer Layers Break More Chains: Layer Pruning Harms Test-Time Scaling in LLMs*