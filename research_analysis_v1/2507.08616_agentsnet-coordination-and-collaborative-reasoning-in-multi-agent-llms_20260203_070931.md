---
title: 'AgentsNet: Coordination and Collaborative Reasoning in Multi-Agent LLMs'
arxiv_id: '2507.08616'
source_url: https://arxiv.org/abs/2507.08616
generated_at: '2026-02-03T07:09:31'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# AgentsNet: Coordination and Collaborative Reasoning in Multi-Agent LLMs

*Florian Gr√∂tschla; Luis M√ºller; Jan T√∂nshoff; Mikhail Galkin; Bryan Perozzi*

---

> ### üìä Quick Facts
>
> *   **Max Network Size:** Validated up to 100 agents
> *   **Benchmark Tasks:** 5 (Leader Election, Consensus, Maximal Matching, (Delta + 1)-Coloring, Vertex Cover)
> *   **Graph Models:** Small World, Scale Free, Delaunay
> *   **Cost Efficiency:** $0.30 ‚Äì $0.80 USD per repeat
> *   **Performance Drop:** >85% (small networks) $\to$ <40% (large networks)
> *   **Quality Score:** 9/10
> *   **References:** 40 citations

---

## üìù Executive Summary

Current research into Multi-Agent Large Language Models (LLMs) is significantly constrained by the scale of existing evaluation frameworks, which are typically limited to networks of only 2 to 5 agents. This limitation creates a critical blind spot regarding the capabilities of frontier models in complex, real-world environments that require large-scale coordination. While LLMs demonstrate strong collaborative reasoning in small, homogeneous networks, it is unclear whether these capabilities generalize to larger topologies. This paper addresses the gap in understanding how strategy formation, self-organization, and communication protocols degrade‚Äîor persist‚Äîas network size scales, a vital factor for deploying reliable multi-agent systems in distributed computing scenarios.

The authors introduce **"AgentsNet,"** a novel benchmark inspired by distributed systems and graph theory that employs a generative problem creation protocol to evaluate practically unlimited network sizes. Unlike static benchmarks, AgentsNet translates fundamental distributed computing problems into five distinct agentic tasks: Leader Election, Consensus, Maximal Matching, (Delta + 1)-Coloring, and Vertex Cover. To ensure real-world applicability, the benchmark utilizes three specific graph models: Small World (Watts-Strogatz), Scale Free (Preferential Attachment), and Delaunay (Geometric Graphs). This approach shifts the focus from simple multi-turn dialogue to topological awareness, testing an agent's ability to reason about its position and neighbors within a complex structure.

The study successfully validated evaluation setups involving up to 100 agents across diverse network structures, evaluating specific frontier models including GPT-4o, Claude 3.5 Sonnet, Llama-3-70B-Instruct, and Gemini-1.5 Pro. Performance was measured using the "Mean AGENTS NET score," which revealed that while models achieved over 85% success in small networks, this figure plummeted to below 40% in 100-agent configurations, quantifying the severe degradation of coordination at scale. Cost-efficiency analysis noted API costs per repeat ranging from $0.30 to $0.80 USD. The study identified Claude 3.5 Sonnet and GPT-4o as Pareto-optimal models, offering the best balance between performance fidelity and computational cost, while protocols established in homogeneous networks consistently failed to translate effectively to larger, complex topologies.

AgentsNet establishes a new standard for evaluating multi-agent systems by providing the first scalable framework capable of assessing hundreds of agents, thereby moving the field beyond toy examples. By exposing the inability of current LLMs to maintain coordination fidelity at scale, this work defines the empirical limits of contemporary collaborative reasoning. This contribution is significant for future research, as it provides a rigorous methodology for testing self-organization and strategy formation, guiding the development of next-generation architectures that must operate effectively in large-scale, distributed environments.

---

## üîë Key Findings

*   **Scalability Issues:** Frontier LLMs demonstrate strong collaborative reasoning in small networks but exhibit significant performance degradation as network size scales up.
*   **Protocol Transferability:** Agents can agree on basic protocols in homogeneous networks, but this does not translate effectively to larger topologies.
*   **Benchmark Limitations:** Existing multi-agent benchmarks are generally limited to 2-5 agents.
*   **Validation Success:** The AgentsNet benchmark successfully validated evaluation setups involving up to 100 agents.

---

## üõ†Ô∏è Methodology

The authors introduced **AgentsNet**, a benchmark inspired by distributed systems and graph theory designed to measure strategy formation, self-organization, and communication.

*   **Benchmark Design:** Utilizes homogeneous networks requiring protocol agreement.
*   **Scaling Configuration:** Involved scaling configurations up to 100 agents to test performance correlation with network size.
*   **Generative Protocol:** Uses a generative problem creation protocol to scale beyond traditional limits.

---

## ‚öôÔ∏è Technical Details

The benchmark evaluates scalable coordination and collaborative reasoning in multi-agent LLMs. It translates fundamental distributed computing problems into agentic tasks using the following specifications:

| Component | Description |
| :--- | :--- |
| **Core Protocol** | Generative problem creation protocol to scale beyond the 2-5 agent limit. |
| **Agentic Tasks** | 1. Leader Election<br>2. Consensus<br>3. Maximal Matching<br>4. (Delta + 1)-Coloring<br>5. Vertex Cover |
| **Graph Models** | **Small World (Watts-Strogatz):** Models social networks.<br>**Scale Free (Preferential Attachment):** Models web-like structures.<br>**Delaunay (Geometric Graphs):** Models spatial relationships. |

---

## üìà Results

*   **Validation:** The study validated evaluation setups with up to 100 agents across diverse network structures.
*   **Metrics:** Performance is measured by the **Mean AGENTS NET score**.
*   **Cost Efficiency:** API costs ranged from $0.30 to $0.80 USD per repeat.
*   **Model Performance:** Identified Pareto-optimal models among frontier LLMs from Google, Meta, OpenAI, and Anthropic.
*   **Key Observation:** While frontier LLMs show strong reasoning in small networks, they exhibit significant performance degradation as network size scales up. Protocols agreed upon in homogeneous networks do not translate effectively to larger, complex topologies.

---

## ‚ú® Contributions

*   **Topological Awareness:** Provided a benchmark focusing on self-organization within network topologies.
*   **Scalable Framework:** Offered an evaluation framework addressing the gap in existing research by supporting practically unlimited network sizes.
*   **Empirical Analysis:** Delivered empirical analysis on the performance limits of frontier LLMs in large-scale setups up to 100 agents.

---

**References:** 40 citations