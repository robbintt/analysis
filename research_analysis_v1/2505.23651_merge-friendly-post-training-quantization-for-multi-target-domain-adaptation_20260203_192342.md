---
title: Merge-Friendly Post-Training Quantization for Multi-Target Domain Adaptation
arxiv_id: '2505.23651'
source_url: https://arxiv.org/abs/2505.23651
generated_at: '2026-02-03T19:23:42'
quality_score: 9
citation_count: 12
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Merge-Friendly Post-Training Quantization for Multi-Target Domain Adaptation

*Juncheol Shin; Minsang Seok; Seonggon Kim; Eunhyeok Park*

***

## üìù Executive Summary

This paper addresses the critical challenge of merging quantized neural networks in multi-target domain adaptation scenarios. While model merging allows for the efficient combination of models adapted to different domains, standard post-training quantization (PTQ) introduces discretization effects and domain restrictions that create "error barriers" on the loss surface. These barriers hinder the smooth integration of models, rendering existing merging techniques ineffective and causing significant performance degradation when attempting to merge low-precision models.

To overcome these limitations, the authors propose **HDRQ (Hessian and Distant Regularizing Quantization)**, a post-training quantization framework explicitly designed to facilitate model merging. HDRQ works by actively flattening the loss surface during quantization to remove error barriers and minimizing deviation from the source pre-trained model. Technically, it employs Noise-Based Quantization, which trains the model using original weights plus sampled quantization noise to perform implicit Hessian regularization, thereby penalizing sharp curvature. Additionally, it utilizes Weight Distance Regularization to minimize the L2-norm between source and target weights, ensuring compatibility, while an advanced merging strategy uses cosine similarity to filter low-quality weights.

The method demonstrates superior performance across several benchmarks compared to standard quantization baselines. In semantic segmentation tasks (GTA to Cityscapes, W6A6), HDRQ achieved 61.54 mIoU, nearly matching the FP32 baseline of 61.69 mIoU and outperforming BRECQ (60.86) and QDrop (61.32). This work represents the first comprehensive study to identify and address the specific challenges of merging quantized models in multi-target domain adaptation, offering a practical pathway for deploying highly efficient, merged models without sacrificing accuracy.

***

## üìä Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Proposed Method** | HDRQ (Hessian and Distant Regularizing Quantization) |
| **Primary Application** | Multi-Target Domain Adaptation |
| **Key Innovation** | Flattening loss surfaces via "error barrier" analysis |
| **Quality Score** | **9/10** |
| **Top Result (Seg)** | **61.54 mIoU** (GTA to Cityscapes, W6A6) |
| **Top Result (MTDA)**| **68.10 Accuracy** (Office-Home, Harmonic Mean) |

***

## üîë Key Findings

*   **The Merging Challenge:** Applying standard quantization introduces domain restrictions and discretization effects, rendering model merging non-trivial.
*   **Error Barriers:** Analyzing quantization through the lens of 'error barriers' provides critical insights into why merging quantized models is difficult.
*   **Flattening the Surface:** Facilitating smooth merging requires flattening the loss surface specifically during the quantization process.
*   **Minimizing Deviation:** Effective merge-friendly quantization requires minimal deviation from the source pre-trained model weights.
*   **Research Gap:** Prior research lacked a specific focus on the complexities of merging quantized models in multi-target domain adaptation.

***

## üõ†Ô∏è Methodology

The authors propose **HDRQ (Hessian and distant regularizing quantization)**, a novel post-training quantization framework tailored for multi-target domain adaptation. The methodology is built on three core principles:

1.  **Leveraging Hessian Information:** Utilizing curvature information to guide the quantization process.
2.  **Distant Regularization:** Applying constraints to ensure the quantized model remains compatible with the source model.
3.  **Optimization Strategy:** Optimizing the process to minimize the deviation from source pre-trained model weights while actively flattening the loss surface to remove barriers for smooth merging.

***

## ‚öôÔ∏è Technical Details

The HDRQ method facilitates model merging by specifically flattening the loss surface during quantization. It consists of the following technical components:

### 1. Noise-Based Quantization
*   **Mechanism:** The model is trained using original weights plus quantization noise sampled from a uniform distribution.
*   **Purpose:** This performs implicit Hessian regularization.
*   **Effect:** It minimizes the expected loss and actively penalizes sharp curvature in the loss landscape.

### 2. Weight Distance Regularization
*   **Mechanism:** Minimizes the L2-norm between source and target weights.
*   **Purpose:** To ensure compatibility between the pre-trained source model and the adapted target models.

### 3. Advanced Merging Strategy
*   **Mechanism:** Uses cosine similarity metrics to filter out low-quality weights during the merge phase.

***

## üèÜ Contributions

*   **Problem Identification:** This is the first study to identify and address the specific challenges of merging quantized models in multi-target domain adaptation.
*   **Theoretical Framework:** Introduced 'error barriers' analysis to theoretically understand quantization's impact on the merging process.
*   **Algorithmic Innovation:** Presented HDRQ, a merge-friendly post-training quantization algorithm designed to overcome error barriers.
*   **Empirical Evidence:** Provided extensive experimental validation to demonstrate the method's effectiveness over existing baselines.

***

## üìà Results

### Semantic Segmentation
*   **Task:** GTA to Cityscapes (W6A6)
    *   **HDRQ:** 61.54 mIoU
    *   **FP32 Baseline:** 61.69 mIoU
    *   **BRECQ:** 60.86 mIoU
    *   **QDrop:** 61.32 mIoU
*   **Task:** GTA to IDD
    *   **HDRQ:** 51.53 mIoU

### Multi-Target Domain Adaptation
*   **Dataset:** Office-Home (Task A to R,C,P)
    *   **HDRQ:** 68.10 Harmonic Mean Accuracy

### Qualitative Analysis
*   Results confirmed that HDRQ produces a significantly **flatter loss surface** compared to standard baselines, validating the theoretical framework.

***

## üìö References & Evaluation

*   **References:** 12 citations
*   **Quality Score:** 9/10