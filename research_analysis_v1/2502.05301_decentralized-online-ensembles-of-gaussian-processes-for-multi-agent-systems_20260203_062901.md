---
title: Decentralized Online Ensembles of Gaussian Processes for Multi-Agent Systems
arxiv_id: '2502.05301'
source_url: https://arxiv.org/abs/2502.05301
generated_at: '2026-02-03T06:29:01'
quality_score: 8
citation_count: 29
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Decentralized Online Ensembles of Gaussian Processes for Multi-Agent Systems

*Fernando Llorente; Daniel Waxman; Petar M. DjuriÄ‡*

---

> ### ðŸ“„ QUICK FACTS
> | Metric | Value |
> | :--- | :--- |
> | **Quality Score** | 8/10 |
> | **References** | 29 Citations |
> | **Core Technique** | Random Feature Approximations |
> | **Ensembling Method** | Online Bayesian Model Averaging |
> | **Evaluation Metric** | MSE Ratio (vs. SVI GP) |

---

## Executive Summary

This paper addresses the computational and statistical challenges of performing Gaussian Process (GP) regression within fully decentralized multi-agent systems where a central fusion center is unavailable. GPs are highly valued for providing rigorous uncertainty quantification, but their standard implementation suffers from $O(N^3)$ computational complexity and requires centralized data aggregation, making them unsuitable for distributed environments. While frequentist methods have been proposed to approximate GPs in distributed settings, there remains a significant gap in scalable Bayesian solutions that can handle both distributed inference and automated hyperparameter selection. This lack of decentralized Bayesian tools limits the deployment of robust, uncertainty-aware models in large-scale networks such as distributed sensor arrays or collaborative robotic teams.

The authors introduce a fully decentralized, asymptotically exact algorithm for computing random feature (RF) approximations of GPs. The core technical innovation lies in a novel ensembling scheme based on Online Bayesian Model Averaging (BMA) for Bayesian Multiple Kernel Learning. By utilizing random features, the method transforms the non-parametric GP problem into a parametric form amenable to distributed consensus. To manage hyperparameter selection without central coordination, the algorithm recursively computes marginal likelihoods and employs two distinct weight update schemes: Local BMA, which relies on private agent data, and Independent Consensus BMA, which approximates the global marginal likelihood through distributed consensus protocols. This allows agents to dynamically weigh and combine different kernel models based on streaming data.

The proposed method was rigorously evaluated against baselines including AdaRaker, DOMKL, NPAE, and Stochastic Variational Inference GPs (SVI GP), using the Mean Squared Error (MSE) Ratio relative to the SVI GP baseline (where values < 1.0 indicate superior performance). On the simulated Kin40k dataset, the Independent Consensus BMA scheme achieved an MSE ratio of approximately 0.95, significantly outperforming AdaRaker (~1.25) and DOMKL (~1.2). Similarly, on the real-world Weather dataset, the method attained an MSE ratio of roughly 0.98, compared to AdaRaker (~1.3) and DOMKL (~1.1). Furthermore, a direct comparison of the two proposed schemes revealed that Independent Consensus BMA consistently outperformed Local BMA, demonstrating that sharing global marginal likelihood information via consensus leads to more accurate hyperparameter selection and predictive performance.

This research significantly advances the state of decentralized learning by providing a scalable, fully Bayesian alternative to existing frequentist methods. By successfully bridging the gap between centralized GP theory and the constraints of multi-agent systems, the work enables the deployment of principled probabilistic models in distributed environments where privacy and bandwidth prevent data aggregation. The introduction of a decentralized, asymptotically exact algorithm for online ensembling establishes a new foundation for adaptive model selection in distributed networks, with broad implications for applications requiring reliable uncertainty quantification, such as environmental monitoring, autonomous navigation, and distributed IoT sensing.

---

## Key Findings

*   **Decentralized Solution Development:** The authors successfully developed a fully decentralized solution for computing random feature approximations of Gaussian processes (GPs) that is asymptotically exact.
*   **Hyperparameter Innovation:** The proposed algorithm effectively addresses the challenge of hyperparameter selection through a novel ensembling scheme based on Online Bayesian Model Averaging.
*   **Validation Success:** The method was validated against both Bayesian and frequentist baselines, demonstrating efficacy on both simulated and real-world datasets.
*   **Bayesian Advancement:** The research provides a scalable Bayesian alternative to existing frequentist methods, filling a significant gap in decentralized multi-agent learning.

---

## Methodology

The research proposes a decentralized framework designed for multi-agent systems with the following core components:

*   **Random Feature Approximations:** The core methodology relies on Random Feature Approximations to enable the scalable and distributed computation of Gaussian Processes.
*   **Ensembling Scheme:** To manage model complexity and hyperparameter selection within this distributed setting, the approach implements an ensembling scheme for Bayesian Multiple Kernel Learning.
*   **Online Bayesian Model Averaging:** This ensembling is driven by Online Bayesian Model Averaging, allowing the system to dynamically weigh and combine different models.

---

## Contributions

1.  **Decentralized GP Implementation:**
    Introduction of a fully decentralized, asymptotically exact algorithm for computing random feature approximations of Gaussian processes, which significantly expands the availability of Bayesian solutions in distributed settings.

2.  **Automated Hyperparameter Handling:**
    Development of a mechanism to handle hyperparameter choices in decentralized environments via an ensembling scheme utilizing Online Bayesian Model Averaging for Bayesian multiple kernel learning.

3.  **Advancement of Bayesian Distributed Learning:**
    Bridging the gap between frequentist and Bayesian approaches by offering a flexible, scalable solution specifically tailored for the constraints of multi-agent systems.

---

## Technical Details

The paper proposes a fully decentralized solution for computing random feature (RF) approximations of Gaussian processes (GPs) in multi-agent systems, designed to be asymptotically exact without a central fusion center.

*   **Aggregation:** It aggregates local statistics from agents over a graph.
*   **Ensembling Strategy:** Uses an ensembling scheme with Online Bayesian Model Averaging (BMA) for hyperparameter selection.
*   **Recursive Computation:** The marginal likelihood is computed recursively, utilizing two weight update schemes:
    *   **Local BMA:** Uses private data.
    *   **Independent Consensus BMA:** Approximates global marginal likelihood via consensus.

---

## Results

Performance is evaluated using the Mean Squared Error (MSE) Ratio with respect to a Stochastic Variational Inference GP (SVI GP) baseline, where lower values indicate better performance.

*   **Baselines:** The proposed method is compared against AdaRaker, DOMKL, NPAE, and SVI GPs.
*   **Performance Bound:** NPAE serves as a performance bound for similar decentralized GP algorithms, as naive online learning with it is prohibitively expensive.
*   **Validation:** The method was validated on simulated and real-world datasets, demonstrating efficacy against the baselines.

---

**Quality Score:** 8/10
**References:** 29 citations