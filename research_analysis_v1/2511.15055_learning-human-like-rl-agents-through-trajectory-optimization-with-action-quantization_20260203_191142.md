---
title: Learning Human-Like RL Agents Through Trajectory Optimization With Action Quantization
arxiv_id: '2511.15055'
source_url: https://arxiv.org/abs/2511.15055
generated_at: '2026-02-03T19:11:42'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Learning Human-Like RL Agents Through Trajectory Optimization With Action Quantization

*Jian-Ting Guo; Yu-Cheng Chen; Ping-Chun Hsieh; Kuo-Hao Ho; Po-Wei Huang; Ti-Rong Wu; I-Chen Wu*

> ###  Quick Facts
> *   **Quality Score:** 8/10
> *   **References:** 40 Citations
> *   **Core Framework:** Macro Action Quantization (MAQ)
> *   **Key Technique:** Vector-Quantized VAE (VQ-VAE)
> *   **Benchmarks:** D4RL Adroit
> *   **Algorithms Used:** IQL, SAC, RLPD

---

##  Executive Summary

Standard reinforcement learning (RL) agents often achieve high task performance but exhibit unnatural, "robotic" behaviors because they optimize solely for reward maximization without regard for human motion patterns. This disconnect reduces the interpretability and trustworthiness of agents, particularly in applications requiring human-robot collaboration where safety and natural movement are paramount.

This paper addresses the fundamental challenge of aligning high-performing RL agents with human-like behavioral patterns. The key innovation is the **Macro Action Quantization (MAQ)** framework, which redefines human-likeness as a trajectory optimization problem constrained within a "human manifold" rather than a simple reward maximization task. Technically, MAQ employs a Conditional Vector-Quantized Variational Autoencoder (VQ-VAE) to distill human demonstrations into a discrete codebook of latent embeddings, representing "macro actions." This approach reduces action space complexity from exponential dimensions ($n^H$) to a fixed codebook size $K$ and utilizes a Human-Like Receding-Horizon Control (HRC) strategy for segment-level planning.

Functioning as a modular wrapper, MAQ allows off-the-shelf RL algorithms to select discrete codebook indices, which are then decoded into coherent, human-like action sequences. Evaluated on the D4RL Adroit benchmarks, MAQ was successfully integrated into standard algorithms including IQL, SAC, and RLPD. It consistently outperformed baseline methods, achieving both higher task completion success rates and significantly improved trajectory similarity scores. In human evaluation studies, MAQ secured the highest rankings for human-likeness, eliminating common RL artifacts such as "shaking and spinning" while maintaining high performance.

---

##  Key Findings

*   **Superior Human-Likeness:** The proposed Macro Action Quantization (MAQ) framework significantly improves the human-likeness of reinforcement learning agents, outperforming standard approaches.
*   **Benchmark Success:** In experiments using the D4RL Adroit benchmarks, MAQ achieved higher trajectory similarity scores and secured the highest human-likeness rankings in human evaluation studies.
*   **Behavioral Correction:** The framework addresses the issue of unnatural behaviors in reward-driven RL agents (such as "shaking and spinning"), thereby enhancing the interpretability and trustworthiness of the agents.
*   **High Compatibility:** MAQ demonstrates high compatibility and can be easily integrated into various off-the-shelf RL algorithms without complex modifications.

---

##  Methodology

*   **Problem Formulation:**
    *   The researchers formulate human-likeness as a trajectory optimization problem.
    *   The objective function is designed to align action sequences with human behavior while simultaneously maximizing rewards.

*   **Implementation Strategy:**
    *   The method adapts classic receding-horizon control to facilitate efficient human-like learning.

*   **Core Mechanism:**
    *   The approach utilizes a **Vector-Quantized VAE (VQ-VAE)** to distill human demonstrations into discrete macro actions.
    *   These discrete actions form the basis of the MAQ framework.

---

##  Contributions

1.  **Novel Formulation:** Redefining the pursuit of human-like agents by framing it as a trajectory optimization task rather than relying solely on standard reward maximization.
2.  **Framework Innovation:** Introducing the **MAQ (Macro Action Quantization)** framework, which leverages VQ-VAE to create a structured latent space of macro actions derived from human data.
3.  **Empirical Validation:** Providing comprehensive evidence on the D4RL Adroit benchmarks that MAQ effectively bridges the gap between high-performance RL and human-like behavioral patterns.
4.  **Practical Utility:** Establishing MAQ as a modular, plug-and-play solution that can augment existing RL algorithms to improve human alignment.

---

##  Technical Details

The MAQ framework operates by constraining the trajectory optimization problem to a 'human manifold' and shifting control to high-level 'macro actions.'

### Core Components
*   **Action Quantization:** Uses a **Conditional VQVAE** to distill human behavior into a discrete codebook of latent embeddings.
*   **Complexity Reduction:** Reduces action space complexity from $n^H$ to a fixed codebook size $K$.
*   **Planning Strategy:** Employs **Human-Like Receding-Horizon Control (HRC)** for segment-level planning over a short horizon $H$.

### VQVAE Process
1.  **Input:** Takes a state and a sequence of primitive actions.
2.  **Quantization:** Quantizes the latent vector to the nearest codebook entry.
3.  **Reconstruction:** Reconstructs the macro action based on the codebook entry.

### Loss Function
The VQVAE is trained with a composite loss function comprising:
*   Reconstruction loss
*   Codebook loss
*   Commitment loss

$$L = ||m - \hat{m}||^2 + ||sg[e] - e_k||^2 + \beta ||e - sg[e_k]||^2$$

### Integration
MAQ acts as a wrapper for standard RL algorithms. The policy selects discrete codebook indices, which are subsequently decoded into macro actions for execution.

---

##  Results

The proposed framework was evaluated on the **D4RL Adroit benchmarks** using offline human demonstrations. MAQ was integrated into IQL, SAC, and RLPD algorithms for testing.

*   **Performance Metrics:** The framework achieved higher success rates in task completion and higher trajectory similarity scores compared to baseline methods.
*   **Human Evaluation:** In subjective studies, MAQ obtained the highest human-likeness rankings.
*   **Qualitative Improvements:**
    *   Improved interpretability and trustworthiness.
    *   Reduction of unnatural behaviors.
    *   Correction of specific RL artifacts such as 'shaking and spinning'.
*   **Compatibility:** Results demonstrated high compatibility, allowing easy integration into various RL algorithms without complex modifications.