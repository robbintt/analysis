---
title: Goal-Oriented Skill Abstraction for Offline Multi-Task Reinforcement Learning
arxiv_id: '2507.06628'
source_url: https://arxiv.org/abs/2507.06628
generated_at: '2026-02-03T18:50:01'
quality_score: 9
citation_count: 24
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Goal-Oriented Skill Abstraction for Offline Multi-Task Reinforcement Learning

*Jinmin He; Kai Li; Yifan Zang; Haobo Fu; Qiang Fu; Junliang Xing; Jian Cheng*

---

> ### ðŸ“Š Quick Facts
> | Metric | Detail |
> | :--- | :--- |
> | **Quality Score** | 9/10 |
> | **Citations** | 24 |
> | **Benchmarks** | MetaWorld (MT30, MT50, ML45) |
> | **Optimal Horizon** | H=10 |
> | **Codebook Size** | M=16 |
> | **Embedding Size** | \|Z\|=64 |
> | **Fine-tuning Speed** | 3k iterations (ML45) |

---

## Executive Summary

### Problem
This research addresses the challenge of **Offline Multi-Task Reinforcement Learning (RL)**, specifically focusing on robotic manipulation scenarios where agents must learn to solve diverse tasks using only static, pre-collected datasets without online environment interaction. This problem is critical for real-world robotics, where trial-and-error learning is often unsafe or prohibitively expensive. The primary difficulty lies in effectively abstracting reusable knowledge from heterogeneous data to achieve positive transfer across tasks, a challenge compounded by the distribution shifts inherent in offline learning and the complexity of managing sub-optimal data.

### Innovation
The authors propose **GO-Skill (Goal-Oriented Skill Abstraction)**, a novel framework designed to mimic human-like knowledge abstraction by decomposing complex tasks into reusable, discrete skills. Technically, the architecture utilizes a Goal Encoder and Vector Quantization (VQ) to construct a discrete Skill Codebook, which serves as a library of low-level actions. The key innovation lies in the three-stage methodology: **Skill Extraction** codifies skills via reconstruction and VQ losses; **Skill Refinement** uniquely addresses the "class imbalance" problemâ€”where broadly applicable skills overshadow rare, task-specific onesâ€”by employing Focal Loss and resampling techniques; and **Hierarchical Policy Integration**, where a high-level policy orchestrates these skills over a defined horizon. The framework also includes MH-GO-Skill, a variant using distinct heads to mitigate gradient conflicts during multi-task optimization.

### Results
GO-Skill demonstrates **state-of-the-art performance** on the MetaWorld benchmark, outperforming established baselines (including MTDT, PromptDT, MTDIFF-P, and HarmoDT) across MT30, MT50, and ML45 benchmarks in both Near-Optimal and Sub-Optimal settings. Ablation studies on the MT50 (Near-Optimal) benchmark validate the necessity of the framework's components: the full GO-Skill model achieved a score of **3159.1**, significantly outperforming variants without reached-goal history (2952.1) or without Vector Quantization (2905.0). Furthermore, the framework proved highly efficient, capable of fine-tuning on the ML45 benchmark in only **3k iterations**, with optimal performance achieved using a skill horizon of 10 and a codebook size of 16.

### Impact
The significance of this work lies in establishing a robust mechanism for **knowledge transfer in offline hierarchical RL**. By explicitly identifying and resolving the class imbalance problem in skill learning, the authors ensure that both general and specialized skills are effectively acquired. This advancement improves the viability of deploying multi-task RL agents in data-constrained environments, reducing the dependency on massive online interaction. The framework's ability to maintain performance scalability from MT30 to MT50 suggests it is a strong foundational approach for developing general-purpose robotic policies from static datasets.

---

## Key Findings

*   **Effective Offline Multi-Task Learning:** The GO-Skill framework demonstrates the ability to solve multiple tasks using pre-collected static datasets without any online environment interaction.
*   **Superior Benchmark Performance:** Shows superior performance on the MetaWorld benchmark across diverse robotic manipulation tasks.
*   **Human-Like Abstraction:** Successfully mimics human-like knowledge abstraction by extracting reusable skills to share knowledge across different tasks.
*   **Class Imbalance Mitigation:** The inclusion of a skill enhancement phase effectively mitigates class imbalances between broadly applicable and task-specific skills.

---

## Methodology

The authors propose **Goal-Oriented Skill Abstraction (GO-Skill)**, a framework for offline multi-task reinforcement learning. The methodology consists of three core components:

1.  **Skill Extraction & Representation**
    Uses a goal-oriented process and vector quantization to codify reusable skills into a discrete library.
2.  **Skill Refinement**
    A skill enhancement phase designed to address class imbalances between general and specific skills.
3.  **Hierarchical Policy Integration**
    A high-level policy that dynamically orchestrates discrete low-level skills to accomplish specific tasks.

---

## Contributions

*   **Framework Introduction:** Introduction of GO-Skill, a novel framework for offline multi-task reinforcement learning that enhances knowledge transfer and task performance without online interaction.
*   **Library Construction:** Development of a discrete skill library construction technique using goal-oriented skill extraction and vector quantization.
*   **Class Imbalance Resolution:** Identification and resolution of the class imbalance problem in skill learning through a dedicated skill enhancement phase.
*   **Orchestration Strategy:** Creation of a hierarchical orchestration strategy that enables a high-level policy to effectively manage abstracted skills without requiring environmental interaction.

---

## Technical Details

**Architecture Components**
*   **Goal Encoder:** Generates reached-goal representations.
*   **Skill Codebook:** A discrete codebook using Vector Quantization.
*   **Skill Decoder:** Responsible for generating actions.
*   **Skill-Based Policy:** Selects skill indices every `H` steps.

**Training Procedure**
*   **Skill Extraction:** Focuses on minimizing reconstruction and VQ (Vector Quantization) losses.
*   **Skill Enhancement:** Mitigates class imbalance by updating only the decoder.
*   **Policy Learning:** Utilizes Focal Loss for effective training.

**Transfer Learning & Variants**
*   **Transfer Strategy:** The Goal Encoder and Codebook are frozen while the Decoder and Policy are fine-tuned.
*   **MH-GO-Skill:** A variant that uses distinct heads to mitigate gradient conflicts.

---

## Results

*   **Benchmark Performance:**
    *   Outperforms state-of-the-art baselines (MTDT, PromptDT, MTDIFF-P, HarmoDT) on MetaWorld benchmarks (MT30, MT50, ML45).
    *   Demonstrates strong performance in both **Near-Optimal** and **Sub-Optimal** settings.
    *   Maintains scalability on MT50.
*   **Ablation Studies (MT50 - Near-Optimal):**
    *   **Full GO-Skill:** 3159.1 Â± 71.0
    *   **Without Reached-Goal history:** 2952.1
    *   **Without Vector Quantization:** 2905.0
    *   **Class Imbalance Mitigation:** Focal Loss + Resampling achieved 3159.1.
*   **Efficiency:**
    *   Achieves efficient fine-tuning on ML45 with only **3k iterations**.
*   **Hyperparameters:**
    *   Optimal Skill Horizon (**H**): 10
    *   Optimal Codebook Size (**M**): 16
    *   Optimal Embedding Size (**|Z|**): 64