---
title: 'Scaling Competence, Shrinking Reasoning: Cognitive Signatures in Language
  Model Learning'
arxiv_id: '2511.21743'
source_url: https://arxiv.org/abs/2511.21743
generated_at: '2026-02-03T13:04:48'
quality_score: 9
citation_count: 27
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Scaling Competence, Shrinking Reasoning: Cognitive Signatures in Language Model Learning

*Authors: Mukul Singh; Ananya Singha; Arjun Radhakrishna; Sumit Gulwani*

---

> ### ðŸ“Š Quick Facts
>
> *   **Quality Score:** 9/10
> *   **References:** 27 Citations
> *   **Core Domains:** Code Generation, Mathematical Reasoning, Regex Synthesis, Logical QA
> *   **Key Concept:** Inverted-U Trajectory of Reasoning Tokens
> *   **Training Method:** Supervised Fine-Tuning (SFT)
> *   **Primary Analogy:** Reasoning Tokens â‰ˆ Human Working Memory

---

## Executive Summary

This research addresses the opacity of learning dynamics in Large Language Models (LLMs), specifically regarding how reasoning capabilities emerge and evolve during fine-tuning. While LLMs demonstrate impressive performance in complex tasks like code generation and mathematical reasoning, the field lacks a clear theoretical framework to explain the internal cognitive shifts a model undergoes while acquiring these skills. The central problem is understanding whether explicit intermediate reasoningâ€”often manifested as "Chain of Thought"â€”is a permanent requirement for problem-solving or a transient mechanism used solely during the acquisition of competence. Understanding this distinction is critical for optimizing training efficiency and interpreting model behavior.

The key innovation is the establishment of a novel framework that maps LLM training dynamics onto the human psychological **"Four Stages of Competence" model** (Unconscious Incompetence to Unconscious Competence). Technically, the study operationalizes this by treating "reasoning tokens"â€”intermediate steps generated during problem-solvingâ€”as a functional analog to human working memory, while model parameters serve as long-term memory. Contrary to previous assumptions, the researchers utilized **Supervised Fine-Tuning (SFT)** rather than Reinforcement Learning to train models across four distinct domains: code generation, mathematical reasoning, regex synthesis, and logical question answering. By tracking the length and correctness of these reasoning traces relative to model performance over time, they introduce a new methodological approach to diagnosing the internal state of the learning system.

The study revealed a consistent **"Inverted-U Trajectory"** in the length of reasoning tokens across all tested domains. During early training, reasoning length expands as performance improves, peaking at the "conscious competence" stage. As training progresses to "unconscious competence," the reasoning traces contract significantlyâ€”shrinking by approximately **50% to 80%** depending on the datasetâ€”indicating the task has been internalized. Crucially, ablation analysis demonstrated **"Scaffold Redundancy"**: models retained high performance, often exceeding 95% of their original accuracy, even when reasoning tokens were removed post-training. This confirms that the explicit generation of intermediate steps acts as a temporary scaffold that becomes redundant once the skill is encoded into the model's weights.

This work significantly influences the field by redefining the role of reasoning in LLMs from a static output feature to a dynamic training signal. The proposed link between LLM dynamics and human cognitive psychology provides a robust interpretative lens for researchers to understand model learning processes. Practically, these findings offer actionable insights for training engineering: monitoring the shrinking of reasoning tokens can serve as a concrete metric for diagnosing training stages and identifying optimal early stopping points. This suggests that the goal of training reasoning models may be to reach a state of "unconscious competence" where the need for explicit, computationally expensive reasoning is minimized without sacrificing performance.

---

## Key Findings

*   **Alignment with Human Learning:** Language model training dynamics align with the human **'Four Stages of Competence'**, progressing from unconscious incompetence to unconscious competence.
*   **Inverted-U Trajectory:** The length of reasoning tokens follows a specific trajectory: it expands as performance improves, peaks at the 'conscious competence' stage, and declines as the task is internalized.
*   **Temporary Scaffolding:** Explicit reasoning acts as a temporary scaffold for learning. Models retain high performance even when reasoning tokens are removed after training.
*   **Cognitive Parallel:** The study draws a functional parallel between reasoning tokens in LLMs and human working memory.

---

## Methodology

The researchers analyzed model behavior during task-specific fine-tuning, mapping observed training dynamics onto the **'Four Stages of Competence'** framework. The methodology focused on:

1.  **Token Tracking:** Measuring 'reasoning tokens'â€”the intermediate steps generated during problem-solvingâ€”specifically analyzing their length relative to model performance over time.
2.  **Ablation Analysis:** Evaluating model performance after the removal of reasoning tokens post-training to test for scaffold redundancy.
3.  **Domain Coverage:** Conducting experiments across code generation, mathematical reasoning, regex synthesis, and logical question answering using Supervised Fine-Tuning (SFT).

---

## Technical Details

**Theoretical Framework**
*   Maps LM training progression to four stages:
    1.  Unconscious Incompetence
    2.  Conscious Incompetence
    3.  Conscious Competence
    4.  Unconscious Competence
*   **Functional Equivalence:**
    *   Reasoning Tokens â‰ˆ Human Working Memory
    *   Model Parameters â‰ˆ Long-term Memory

**Operationalization**
*   Reasoning is defined as the generation of intermediate tokens to decompose problems into inferential steps.
*   **Central Hypothesis:** Reasoning is a transient phase used to acquire skills, which are eventually internalized into the model's weights, reducing the necessity for explicit reasoning tokens.

---

## Results

The study tracked four specific metrics: length of reasoning traces, response accuracy, correctness of reasoning traces, and non-reasoning performance.

*   **Inverted-U Trajectory:** Confirmed the primary finding where token length expands early in training, peaks at the 'conscious competence' stage, and contracts as the model reaches 'unconscious competence'.
*   **Scaffold Redundancy:** Researchers observed that models maintain high performance even when reasoning tokens are removed in later stages.
*   **Domain-Agnostic Consistency:** The pattern of expansion, peaking, and contraction was consistent across all tested domains (code, math, regex, logic), demonstrating a universal learning dynamic.

---

## Contributions

*   **Novel Cognitive Link:** Establishes a novel link between LLM training dynamics and human cognitive psychology by analogizing reasoning tokens to working memory.
*   **New Training Metrics:** Proposes metrics based on reasoning token dynamics to diagnose training stages, identify convergence, and determine optimal early stopping points.
*   **Actionable Insights:** Offers insights for reasoning model training, suggesting that monitoring the 'shrinking' of reasoning can signal successful task internalization.