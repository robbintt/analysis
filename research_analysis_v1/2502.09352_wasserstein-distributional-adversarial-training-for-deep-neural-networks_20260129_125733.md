# Wasserstein Distributional Adversarial Training for Deep Neural Networks

*Xingjian Bai; Guangyi He; Yifan Jiang; Jan Obloz*

---

### **Quick Facts**

| Metric | Detail |
| :--- | :--- |
| **Quality Score** | 8/10 |
| **Citations** | 14 |
| **Dataset** | CIFAR-10 (50k images) |
| **Core Innovation** | TRADES extension for Wasserstein Distributional Robustness |
| **Key Boost** | +25.07% Wasserstein Robustness ($W_2$) |
| **Cost Efficiency** | No massive synthetic data required for fine-tuning |

---

## **Executive Summary**

Current adversarial training methods, such as TRADES, primarily focus on **pointwise threats**, defending models against individual input attacks bounded by metrics like $l_\infty$. However, this narrow focus often leaves models vulnerable to **distributional threats**, where the underlying data distribution shifts—modeled here via Wasserstein distance ($W_2$).

This paper addresses the critical gap between pointwise and distributional robustness, highlighting that a model secure against single-instance perturbations is not necessarily secure against broader, more realistic distributional shifts. The authors propose extending the TRADES framework to support **Wasserstein distributionally robust optimization (DRO)**. They formulate this as a min-max optimization over couplings, utilizing first-order sensitivity analysis to derive optimal attack directions.

Evaluations on five pre-trained models from the RobustBench benchmark (CIFAR-10) demonstrate significant improvements in both distributional and pointwise robustness ($+14.57\%$ to $+25.07\%$ in $W_2$ and $+13.56\%$ to $+20.02\%$ in $l_\infty$). Although these gains resulted in a trade-off in standard Clean Accuracy, the method achieves these results using **only the original training dataset (50k images)**, making it a highly scalable solution for enhancing deployed models.

---

## **Key Findings**

*   **Dual Robustness Enhancement:** The proposed method successfully enhances Wasserstein distributional robustness in deep neural networks **without compromising** existing pointwise robustness, bridging a critical gap in defense strategies.
*   **Broad Applicability:** It is effective for a wide range of pre-trained networks, including successful models on the RobustBench benchmark.
*   **Efficiency over Scale:** Improvements are less pronounced for models pre-trained on massive synthetic datasets (20–100M images), yet the method still boosts performance significantly using only the original, smaller training dataset (50k images).
*   **No Retraining Required:** Large-scale model improvements can be achieved via fine-tuning, eliminating the need for expensive retraining from scratch or reliance on massive synthetic data during the fine-tuning phase.

---

## **Methodology**

The authors extend the **TRADES method** (traditionally used for pointwise attacks) to defend against distributional attack threats.

*   **Sensitivity Analysis:** The approach relies on sensitivity analysis applied to Wasserstein distributionally robust optimization problems to gauge and improve stability against distribution shifts.
*   **Fine-Tuning Protocol:** The methodology includes an efficient fine-tuning technique designed for deployment onto pre-existing models, allowing practitioners to upgrade robustness without training from scratch.
*   **Optimization Strategy:** The framework utilizes efficient computational strategies to manage the cost of distributional robustness, making it practical for real-world application.

---

## **Contributions**

1.  **Framework Extension:**
    Expansion of adversarial training strategies to specifically address distributional threats via Wasserstein distributionally robust optimization.

2.  **Practical Deployment:**
    Introduction of a computationally efficient fine-tuning procedure that allows practitioners to upgrade the robustness of existing, deployed models.

3.  **Dual Robustness Validation:**
    Demonstration through RobustBench experiments that it is possible to improve distributional robustness while strictly maintaining pointwise robustness.

---

## **Technical Details**

### **Threat Model & Formulation**
The method employs a **Wasserstein distributional threat model ($W_2$)** rather than standard pointwise $l_\infty$ attacks. It is formulated as a min-max optimization over couplings $\pi$:

$$
\inf_{\theta} \left[ \mathbb{E}_P[L(f_\theta(x), y)] + \beta \sup_{\pi \in \Pi_2(P, \delta)} \mathbb{E}_\pi[\tilde{L}(f_\theta(x), f_\theta(x'))] \right]
$$

### **Attack Direction Calculation**
It utilizes **first-order sensitivity analysis** to calculate a sensitivity term $\Upsilon$ and derive the attack direction:

$$
T(x) = \Upsilon^{-1} \text{sgn}(\nabla_{x'} J_\theta(x, x, y)) \|\nabla_{x'} J_\theta(x, x, y)\|_1
$$

### **Loss Functions**
The strategy uses a **split loss approach**:
*   **ReDLR Loss:** Used for the attack step to focus on boundary samples.
*   **Cross-Entropy (CE):** Used for the optimization step.

### **Computational Efficiency**
*   **W-PGD-Budget:** Estimates global sensitivity using a data subset and moving average to calculate a **dynamic batch budget $\delta_B$**, managing computational costs effectively.
*   **Protocol:** Designed for fine-tuning pre-trained robust networks using random weight perturbations to mitigate overfitting.

---

## **Results**

Experiments were conducted on **CIFAR-10** using 5 pre-trained robust models from RobustBench with $\delta = 8/255$.

### **Performance Improvements**

*   **Wasserstein Distributional Robustness ($W_2$):**
    *   Significant boosts ranging from **+14.57% to +25.07%**.
    *   *Wang et al.:* Improved from 52.14% $\to$ 77.21%.
    *   *Cui et al.:* Improved from 53.16% $\to$ 76.54%.

*   **Pointwise Robustness ($W_\infty$):**
    *   Substantial increases without compromise, ranging from **+13.56% to +20.02%**.
    *   *Wang et al.:* Improved from 70.62% $\to$ 90.64%.
    *   *Cui et al.:* Improved from 70.92% $\to$ 90.49%.

*   **Standard Clean Accuracy:**
    *   A trade-off was observed (e.g., Wang et al. dropped from 92.44% $\to$ 78.19%).

### **Data Efficiency**
Notably, the fine-tuning method achieved these gains using only the original **50k CIFAR-10 images**, avoiding the need for the massive synthetic datasets (20–100M images) required during the initial pre-training phases of other methods.