# Improving MoE Compute Efficiency by Composing Weight and Data Sparsity

*Maciej Kilian; Oleg Mkrtchyan; Luke Zettlemoyer; Akshat Shrivastava; Armen Aghajanyan*

---

> ### ðŸ“Š Quick Facts
> *   **Quality Score:** 8/10
> *   **Citations:** 40
> *   **Core Mechanism:** Zero-Compute "Null" Experts
> *   **Primary Domain:** Vision-Language Models (VLMs)
> *   **Efficiency Gain:** Iso-FLOP improvements via composed sparsity

---

## Executive Summary

Current Mixture-of-Experts (MoE) architectures rely primarily on **weight sparsity**â€”activating a fixed subset of experts per tokenâ€”to reduce computational costs. This approach leaves significant efficiency untapped by failing to leverage **data sparsity**, where experts could ignore less informative tokens entirely. Implementing data sparsity in autoregressive models has been historically difficult due to strict causality constraints and the risk of train-inference mismatch, where models trained on full data behave inconsistently when inference requires selective processing. This paper addresses the challenge of unifying weight and data sparsity to maximize compute efficiency, specifically targeting the modal imbalances in vision-language modeling where information density varies significantly between text and vision.

The authors introduce **"Composed Sparsity,"** a framework that integrates zero-compute "null experts" into standard causal token-choice MoE layers. Technically, the router selects top-$k$ scores from a combined pool of real and null experts; tokens routed to a null expert effectively skip computation, allowing dynamic allocation between 0 and $k$ real experts. Crucially, this resolves the train-inference mismatch by applying a standard load balancing loss uniformly to all experts, including null ones. By incentivizing the router to send low-information tokens to null experts during the training phase, the model learns intrinsic data sparsity that remains consistent at inference.

Under strict iso-FLOPs conditionsâ€”matching the expected floating-point operations of weight-sparse-only baselinesâ€”the composed sparsity approach achieved a measurable reduction in training loss and superior downstream performance metrics. The experiments quantified a distinct asymmetry in compute allocation: **vision tokens were routed to zero-compute (null) experts significantly more aggressively than text tokens.** This differential routing rate indicates the model successfully detected lower information density in vision tokens relative to text, dynamically optimizing the compute budget without manual intervention and outperforming traditional weight-sparse models within the same computational constraints.

This research resolves a fundamental conflict in efficient model design by proving that data sparsity can be effectively harnessed in autoregressive models without violating causality or introducing discrepancies between training and inference. The validation of null experts as a practical mechanism for dynamic resource allocation offers a critical path forward for scaling Vision-Language Models (VLMs), which often struggle with inherent modality imbalances. By enabling models to automatically identify and skip low-value computation, this work paves the way for future large language models to achieve higher performance with reduced computational overhead.

---

## Key Findings

*   **Superior Efficiency:** Combining weight sparsity (activating a subset of experts) with data sparsity (experts processing a subset of tokens) yields superior compute efficiency, lower training loss, and better downstream performance at matched expected FLOPs.
*   **Modality-Aware Allocation:** In vision-language model training, the model automatically learns to allocate compute based on data modality, routing vision tokens to zero-compute experts more aggressively than text tokens.
*   **Causality Resolution:** The proposed approach resolves causality constraints by recovering the benefits of data sparsity within causal token-choice Mixture-of-Experts (MoE) without violating causality or creating a train-inference mismatch.

---

## Methodology

The researchers propose enhancing standard causal token-choice Mixture-of-Experts (MoE) layers through the following strategic modifications:

1.  **Integration of Null Experts:** The framework introduces **zero-compute (null) experts** into the routing pool alongside standard active experts.
2.  **Retained Routing Logic:** The system retains token-choice routing where tokens are assigned to null experts to consume no computation.
3.  **Load Balancing:** A standard load balancing objective is utilized to train the model to uniformly use all experts. This ensures data sparsity is achieved in expectation while maintaining the causal properties required for autoregressive modeling.

---

## Technical Details

The proposed architecture formalizes routing through a matrix with dual budget constraints. Below are the core technical components:

| Component | Description |
| :--- | :--- |
| **Composed Sparsity** | A formalized routing approach limiting experts per token (*Weight Sparsity* via per-token budget) and tokens per expert (*Data Sparsity* via per-expert budget). This allows variable compute allocation without violating aggregate FLOP budgets. |
| **Null Experts** | Modification of standard Token-Choice MoE by adding zero-compute experts. The router selects top-k scores from a pool of real and null experts; if a token is routed to a null expert, computation is skipped, allowing between 0 and k real experts per token. |
| **Causality Preservation** | Preserves causality by allowing independent token selection based on current representation, avoiding future-token dependency. |
| **Training Objective** | Uses a standard load balancing loss applied to all experts (including null ones), incentivizing the model to send low-information tokens to null experts. |
| **Target Application** | Specifically targeted at Vision-Language Models (VLMs) to handle modality imbalance. |

---

## Contributions

*   **Integration of Sparsity Types:** A novel method to compose weight and data sparsity within a single framework, addressing limitations of previous data sparsity implementations in autoregressive settings.
*   **Zero-Compute Routing Mechanism:** Introduction of "null experts" as a mechanism to eliminate computation for specific tokens dynamically, validating that standard load balancing objectives can effectively train models to utilize these skip-connections.
*   **Advancement in Vision-Language Models:** A solution for handling data heterogeneity in VLMs, demonstrating that the architecture can implicitly identify and mitigate the computational load of low-information tokens.

---

## Results

*   **Iso-FLOPs Performance:** Under matched compute (iso-FLOPs) conditions, the composed sparsity approach achieves lower training loss and better downstream performance compared to weight-sparse-only baselines.
*   **Asymmetric Routing Behavior:** In vision-language training, the router learns to allocate compute asymmetrically:
    *   **Vision Tokens:** Routed to zero-compute (null) experts more aggressively (indicating lower information density).
    *   **Text Tokens:** Routed to active experts more frequently.
*   **Dense Capabilities:** The proposed solution contains denser configurations than weight-sparse-only baselines, preserving dense capabilities while offering sparse efficiency, and consistently outperforms those baselines.