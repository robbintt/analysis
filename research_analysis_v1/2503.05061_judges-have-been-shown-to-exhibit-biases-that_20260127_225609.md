---
title: Judges have been shown to exhibit biases that
arxiv_id: '2503.05061'
source_url: https://arxiv.org/abs/2503.05061
generated_at: '2026-01-27T22:56:09'
quality_score: 9
citation_count: 21
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# Judges have been shown to exhibit biases that
*Judge Without, Varshini Reddy, Massachusetts Institute, Kensho Technologies, No Free, Charles Lovering, Chris Tanner, Michael Krumdick, Seth Ebner*

---

### ðŸ“Š Quick Facts

| Metric | Value |
| :--- | :--- |
| **Quality Score** | 9/10 |
| **Total Citations** | 21 |
| **Dataset Size** | 1,200 LLM responses |
| **Human Consensus** | 87% |
| **Benchmark Error Found** | 35% (Original MT-Bench) |
| **Inference Method** | Chain-of-Thought, Self-Consistency (5x) |

---

## Executive Summary

This paper addresses a critical vulnerability in the "LLM-as-a-Judge" paradigm, where automated evaluator models are increasingly trusted to benchmark candidate systems. While aggregate evaluation scores often show high agreement with human annotators, the authors reveal that these high-level statistics obscure a fundamental bias: **judges lack the capability to accurately grade responses to questions they themselves cannot solve.** This problem is significant because it creates a blind spot in evaluation pipelines, particularly for complex reasoning tasks, leading to inflated performance scores for capabilities that the judge model does not actually possess.

The innovation lies in a rigorous diagnostic framework combining a novel human-annotated dataset with controlled experimental conditions to isolate the specific factors impacting grading accuracy. The researchers released a dataset of 1,200 responses, evaluated against (C)MT-Bench and a new, challenging financial benchmark (**BFF-Bench**) comprising 80 two-turn expert questions. The methodology systematically tests three reference conditionsâ€”none, self-generated (synthetic), and human-writtenâ€”utilizing Chain-of-Thought prompting and Self-Consistency (5 inferences) to ensure the evaluation strictly focuses on "correctness" rather than stylistic preference.

Empirical results demonstrate a strong correlation between a model's ability to answer a question and its ability to grade it, with judges failing specifically on the subset of questions they cannot answer. A key revelation was a **35% error rate in the original MT-Bench reference answers**, underscoring the fragility of existing benchmarks. While human annotation achieved 87% majority consensus, the experiments proved that providing high-quality human references resolves grading inaccuracies more effectively than scaling the judge model. Specifically, smaller judges equipped with strong human references outperformed larger judges relying on synthetic or weak references.

These findings have substantial implications for the field, suggesting that the industry's reliance on aggregate metrics is masking critical evaluation failures. The study establishes that the "quality over compute" principle applies to evaluation: investing in high-quality human reference answers yields better accuracy than upgrading to larger, more expensive judge models.

---

## Key Findings

*   **Correlation Between Answering and Grading:** There is a strong connection between an LLM's ability to correctly answer a question and its ability to accurate grading responses to that same question.
*   **Hidden Weakness in Aggregate Scores:** While aggregate-level statistics may suggest high agreement with human annotators, LLM Judges specifically struggle to evaluate the subset of questions they are incapable of answering correctly.
*   **Superiority of Human References:** Providing the judge with a correct, human-written reference answer effectively addresses the issue of grading accuracy.
*   **Quality Over Compute:** The quality of the reference answer is more critical than the size of the judge model.

---

## Methodology

The research framework is designed to isolate the factors impacting grading accuracy through a combination of novel dataset creation and comparative analysis.

*   **Dataset Creation:** The researchers constructed and publicly released a human-annotated dataset containing correctness labels for 1,200 LLM responses.
*   **Data Sourcing:** Questions were sourced from a combination of existing datasets and a novel, challenging benchmark developed specifically for this analysis (**BFF-Bench**).
*   **Evaluation Framework:** The study evaluated LLM Judges specifically on their ability to grade the "correctness" of responses to conversational questions, rather than just stylistic preferences.
*   **Comparative Analysis:** The research involved an in-depth analysis of how reference quality impacts performance, comparing weak versus strong judges under conditions of high-quality (human) versus lower-quality (synthetic) reference materials.

---

## Technical Details

*   **Framework:** LLM-as-a-Judge (Candidate generates response; Judge evaluates via Single or Pairwise grading).
*   **Reference Conditions:** Three distinct settings were tested:
    1.  None
    2.  Self (Generated by the judge)
    3.  Human
*   **Benchmarks:**
    *   **(C)MT-Bench:** A manually corrected version of the standard MT-Bench.
    *   **BFF-Bench:** 80 two-turn Business/Finance questions created by experts.
*   **Annotation Method:** Performed by in-house financial analysts using holistic grading ( labels: Incorrect, Correct, Not Sure).
*   **Inference Configuration:**
    *   Chain-of-Thought (CoT)
    *   Self-Consistency with 5 inferences
    *   Temperature set to 0.7

---

## Results

*   **Benchmark Integrity:** Manual analysis revealed a **35% error rate** in the original MT-Bench reference answers.
*   **Dataset Composition:** The final dataset consists of 1,036 Single pairs and 604 Pairwise pairs.
*   **Human Reliability:** Human annotation reliability achieved 87% majority consensus (with 54% unanimous agreement).
*   **Experimental Scale:** The experiment involved 6 candidate models (e.g., GPT-4o, Llama 3.3 70B) and 5 judge models, resulting in 1,200 generated responses.
*   **Validation of Hypothesis:** Results confirmed the strong correlation between answering and grading capability, the failure of LLMs to correctly grade questions they cannot answer, and the resolution of accuracy issues via human references.

---

## Contributions

*   **Public Dataset:** The release of a specialized human-annotated dataset designed to test the correctness evaluation capabilities of LLMs.
*   **Novel Benchmark:** The introduction of **BFF-Bench**, a new challenging benchmark for testing LLM capabilities.
*   **Diagnostic Insight:** Identification of the specific limitation where LLMs fail to grade questions they cannot answer, challenging the reliance on aggregate accuracy metrics.
*   **Best Practice Recommendation:** Validation of a simple but effective solutionâ€”providing high-quality human-written reference answersâ€”and evidence that this method is more effective than upgrading to a more powerful judge model reliant on synthetic references.