# Learning from Loss Landscape: Generalizable Mixed-Precision Quantization via Adaptive Sharpness-Aware Gradient Aligning
*Lianbo Ma; Jianlun Ma; Yuee Zhou; Guoyang Xie; Qiang He; Zhichao Lu*

---

### ðŸ“Š Quick Facts

| Metric | Details |
| :--- | :--- |
| **Method** | Adaptive Sharpness-Aware Gradient Aligning (ASGA) |
| **Core Strategy** | Proxy-to-Target Transfer (CIFAR10 $\to$ ImageNet) |
| **Efficiency Gain** | Up to **150%** faster than baselines |
| **Top-1 Accuracy Gain** | **+1.0%** (ImageNet) / **+1.5%** (CIFAR10) |
| **Bit-widths** | {2, 3, 4, 5, 6} MobileNet/VGG; {2, 3, 4, 6} ResNet |
| **Quality Score** | **8/10** |

---

## Executive Summary

Current Mixed-Precision Quantization (MPQ) methods suffer from prohibitive computational costs because they require searching for optimal bit-width configurations and performing fine-tuning directly on large-scale datasets like ImageNet. This dependency on massive data creates a bottleneck, as the search space for quantization policies is vast, and the process of training on millions of images is resource-intensive. A critical challenge remains the inability to effectively transfer quantization policies learned on small, manageable proxy datasets to large-scale target models without significant performance degradation, limiting the accessibility of efficient neural network deployment.

The authors introduce **Adaptive Sharpness-Aware Gradient Aligning (ASGA)**, a transferable framework that optimizes MPQ policies on small proxy datasets (e.g., CIFAR10) for generalization to large-scale targets. ASGA utilizes a three-pronged strategy:

1.  **Sharpness-Aware Minimization (SAM)** to explore the loss landscape and seek flatter minima associated with better generalization.
2.  **Implicit Gradient Direction Alignment** to mitigate conflicts between optimization objectives.
3.  **Adaptive Perturbation Radius** to accelerate convergence.

Technically, the method fixes the first and last layers to 8 bits while searching for bit-width candidates $\{2, 3, 4, 5, 6\}$ for MobileNet-V2 and VGG16, and $\{2, 3, 4, 6\}$ for ResNet. It employs an initial perturbation radius of 0.1 and balances performance against complexity using a hyper-parameter $\beta$.

ASGA successfully correlates sharpness metrics on proxy datasets with accuracy on target datasets, delivering measurable improvements over baselines. In ablation studies comparing EdMIPS against EdMIPS + ASGA, Top-1 accuracy on the ImageNet Subset increased by 1.0% (from 66.3% to 67.3%), and Top-5 accuracy rose by 1.3%. On CIFAR10, Top-1 accuracy improved by 1.5% (from 66.4% to 67.9%).

**Crucially, the framework achieves up to 150% greater computational efficiency during the search phase compared to existing methods and reduces overhead by requiring only weight adjustment on the target dataset rather than a full retraining process.**

This research shifts the MPQ paradigm by decoupling the policy search process from large-scale data dependencies, effectively eliminating the need for expensive large-scale fine-tuning. By validating that policies optimized for flatness on proxy datasets can transfer to complex target datasets, the authors provide a cost-effective pathway to deploy high-performance, quantized neural networks.

---

## Key Findings

*   **Generalization Capability:** The proposed method successfully generalizes quantization policies searched on small-scale datasets (CIFAR10) to large-scale datasets (ImageNet) without losing accuracy.
*   **Computational Efficiency:** Achieves an efficiency improvement of **up to 150%** compared to existing baselines.
*   **Cost Reduction:** Eliminates expensive large-scale quantization fine-tuning by reducing computational overhead.
*   **Simplified Deployment:** Reduces overhead by only requiring model weight adjustment on the target dataset rather than a full retraining process.

---

## Methodology

The research proposes a **transferable optimization framework** where Mixed Precision Quantization (MPQ) policy search is conducted on a proxy small dataset. The approach utilizes a three-pronged strategy:

1.  **Sharpness-Aware Minimization (SAM):** Utilized to enhance generalization by actively exploring the loss landscape.
2.  **Implicit Gradient Direction Alignment:** Mitigates conflicts between optimization objectives to ensure stable convergence.
3.  **Adaptive Perturbation Radius:** A technique to accelerate optimization speed specifically during the search phase.

---

## Technical Details

**Approach Name:** Adaptive Sharpness-Aware Gradient Aligning (ASGA)

**Core Principle:**
*   Searches for quantization policies on small-scale proxy datasets and generalizes to large-scale target datasets by modeling the loss landscape.
*   Focuses on **sharpness minimization** to achieve flatter minima, which is theoretically linked to improved generalization.

**Configuration & Parameters:**
*   **Perturbation Radius:** Initial radius set to **0.1**.
*   **Hyper-parameter:** Uses parameter $\beta$ to balance model performance and computational complexity.
*   **Fixed Layers:** First and last layers are fixed to **8 bits**.

**Bit-width Candidates:**
*   **Models:** MobileNet-V2, VGG16
    *   *Candidates:* {2, 3, 4, 5, 6}
*   **Models:** ResNet
    *   *Candidates:* {2, 3, 4, 6}

---

## Contributions

*   **Cost-Effective Paradigm:** Introduction of a Mixed-Precision Quantization (MPQ) paradigm that decouples policy search from large-scale data dependency.
*   **Domain Application:** Application of sharpness-aware minimization to the quantization domain to specifically improve policy transferability.
*   **Conflict Resolution:** Resolution of gradient conflicts in multi-objective optimization through implicit gradient direction alignment.
*   **Convergence Speed:** Development of an adaptive perturbation radius technique to speed up the search process convergence.

---

## Results

### Accuracy Improvements (Ablation Studies: EdMIPS vs. EdMIPS + ASGA)

| Dataset | Metric | Baseline (EdMIPS) | **Proposed (EdMIPS + ASGA)** | Improvement |
| :--- | :--- | :--- | :--- | :--- |
| **ImageNet Subset** | Top-1 | 66.3% | **67.3%** | **+1.0%** |
| **ImageNet Subset** | Top-5 | - | - | **+1.3%** |
| **CIFAR10** | Top-1 | 66.4% | **67.9%** | **+1.5%** |

### Efficiency & Analysis
*   **Computational Efficiency:** Up to **150%** improvement in search speed compared to baselines.
*   **Process Optimization:** Requires only weight adjustment instead of full retraining.
*   **Loss Landscape:** Visualizations confirmed the method successfully finds flatter minima compared to baselines.
*   **Correlation:** Established a verifiable correlation between sharpness metrics on proxy datasets and accuracy on target datasets.

---
*Quality Score: 8/10 | References: 26 citations*