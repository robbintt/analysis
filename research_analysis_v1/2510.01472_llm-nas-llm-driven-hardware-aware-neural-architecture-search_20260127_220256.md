---
title: 'LLM-NAS: LLM-driven Hardware-Aware Neural Architecture Search'
arxiv_id: '2510.01472'
source_url: https://arxiv.org/abs/2510.01472
generated_at: '2026-01-27T22:02:56'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# LLM-NAS: LLM-driven Hardware-Aware Neural Architecture Search

*Hengyi Zhu, Shaoyi Huang, Grace Li, Stevens Institute of Technology, Technical University*

---

> ### âš¡ Quick Facts
>
> *   **Search Efficiency:** Reduced from ~10 GPU days to **minutes**
> *   **Latency Improvement:** Up to **54%** lower latency than baselines
> *   **Prediction Accuracy:** Spearman's rank correlation of **~0.90**
> *   **Quality Score:** 9/10
> *   **References:** 40 Citations

---

## Executive Summary

This paper addresses the dual challenges of computational inefficiency and "exploration bias" inherent in current Neural Architecture Search (NAS) methodologies. Traditional hardware-aware NAS techniques typically require multiple GPU days of computation and rely on supernet training, making them prohibitively expensive. Furthermore, existing LLM-driven NAS approaches tend to converge on a narrow subset of architectures (exploration bias), failing to discover optimal solutions across the diverse range of hardware latency constraints required for real-world deployment.

The authors introduce **LLM-NAS**, a novel framework that integrates LLM reasoning with training-free evaluation to optimize the search process. The system consists of three core components: a **Complexity-Driven Partitioning Engine** that divides the search space into disjoint niches to enforce diversity; an **LLM-Powered Architecture Prompt Co-Evolution Operator** that uses an LLM as a reasoning engine to update a Knowledge Base of design heuristics and perform guided crossover and mutation; and a **Zero-Cost Predictor** that leverages an XGBoost regressor trained on 13 zero-cost proxies to estimate accuracy with high correlation, eliminating the need for training.

Evaluated on the **HW-NAS-Bench**, LLM-NAS achieves superior Hypervolume (HV) and lower Inverted Generational Distance (IGD) compared to baseline methods, indicating better convergence and solution diversity. The framework successfully identifies architectures that reduce latency by up to **54%** while maintaining accuracy levels comparable to baselines. Crucially, the method drastically improves search efficiency, reducing discovery time from multiple GPU days to just minutes, while achieving near-complete coverage across the full spectrum of latency ranges.

The significance of this research lies in its dramatic reduction of search costs by orders of magnitude without the need for supernet training, effectively democratizing high-performance neural architecture search. By successfully mitigating exploration bias and outperforming baselines in hardware-aware metrics, LLM-NAS establishes a new paradigm for efficient, resource-constrained model development.

---

## Key Findings

*   **Superior Performance Metrics:** Achieves overall higher Hypervolume (HV) and lower Inverted Generational Distance (IGD) on the HW-NAS-Bench compared to baseline methods.
*   **Hardware Optimization:** Delivers architectures with **up to 54% lower latency** than baselines while maintaining similar accuracy levels.
*   **Extreme Efficiency:** Drastically reduces search time from multiple GPU days (e.g., ~10 days for FairNAS) to just **minutes**.
*   **Bias Mitigation:** Successfully mitigates exploration bias to enable the discovery of architectures across diverse latency ranges.

---

## Methodology

The study proposes LLM-NAS, an LLM-driven Neural Architecture Search framework designed to optimize hardware-aware architecture generation. The methodology consists of three distinct components:

1.  **Complexity-Driven Partitioning Engine:** Divides the search space by complexity levels to enforce diversity and prevent mode collapse.
2.  **LLM-Powered Architecture Prompt Co-Evolution Operator:** Iteratively updates a knowledge base of design heuristics and performs guided evolution using Large Language Models.
3.  **Zero-Cost Predictor:** Employed to estimate performance without training candidate networks from scratch, significantly accelerating the search process.

---

## Technical Details

The proposed LLM-NAS framework is built upon three main technical pillars:

1.  **Complexity-Driven Search Space Partitioning**
    *   Divides the search space into disjoint niches based on model complexity (specifically the count of 3x3 convolutions).
    *   Primary goal is to mitigate exploration bias by ensuring diverse architectural candidates are considered.

2.  **LLM-Powered Partitioned Co-Evolution**
    *   Operates across the defined niches.
    *   Uses an LLM as a reasoning engine to update a Knowledge Base (long-term memory).
    *   Generates new architectures via Crossover and Mutation operators guided by the LLM.

3.  **Training-Free Objective Evaluation**
    *   Utilizes a lookup table for latency data (via HW-NAS-Bench).
    *   Implements an XGBoost regressor trained on 13 zero-cost proxies to predict accuracy.
    *   Achieves a Spearman's rank correlation of approximately **0.90**, validating the training-free approach.

---

## Core Contributions

*   **Bias Characterization:** Identification and characterization of the 'exploration bias' inherent in standard LLM-driven NAS approaches.
*   **Novel Framework:** Introduction of a new framework featuring a partitioning strategy and a prompt co-evolution mechanism that dynamically updates design knowledge.
*   **Efficiency Advancement:** Significant reduction in search costs by orders of magnitude through the elimination of supernet training, while outperforming baselines in hardware-aware metrics.

---

## Results

*   **Convergence & Diversity:** The method achieves higher Hypervolume (HV) and lower Inverted Generational Distance (IGD) compared to baselines on HW-NAS-Bench.
*   **Latency Reduction:** Delivers architectures with up to **54% lower latency** while maintaining similar accuracy.
*   **Speed:** Search efficiency is drastically improved, reducing time from multiple GPU days to just **minutes**.
*   **Coverage:** LLM-NAS successfully achieves near-complete coverage across the full spectrum of latency ranges.

---
*Analysis Score: 9/10 | References: 40 citations mentioned*