---
title: LLM Agents Are Hypersensitive to Nudges
arxiv_id: '2505.11584'
source_url: https://arxiv.org/abs/2505.11584
generated_at: '2026-02-03T12:25:16'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# LLM Agents Are Hypersensitive to Nudges
*Manuel Cherep; Pattie Maes; Nikhil Singh*

---

### ðŸ“Š Quick Facts

> **Quality Score:** 8/10  
> **References:** 40 Citations  
> **Models Tested:** GPT-3.5-Turbo, GPT-4o, o3-Mini, Claude 3 Haiku, Claude 3.5 Sonnet, Gemini 1.5 Pro/Flash  
> **Total Processed:** ~1 Billion Tokens  
> **Experimental Scale:** 320â€“340 Trials per condition  
> **Cost Metric:** 2 Points per information reveal  

---

## Executive Summary

As Large Language Models (LLMs) are increasingly deployed as autonomous agents in complex, high-stakes environments, a critical assumption is that they function as rational or human-neutral decision-makers. This paper addresses the concern that LLMs possess inherent behavioral biases that render them unreliable in sequential decision-making tasks. Specifically, the authors investigate how "choice architecture"â€”the way choices are presented through nudges like default options or suggestionsâ€”affects LLM outputs. If LLMs are excessively susceptible to these subtle contextual cues, their utility as objective agents is compromised, potentially leading to suboptimal or irrational outcomes in real-world applications ranging from financial planning to healthcare triage.

The study introduces a rigorous behavioral evaluation framework, implementing a text-based sequential decision-making paradigm designed to simulate complex trade-offs. Technically, the task requires agents to select from "baskets" ($B_i$) to maximize a reward function ($r = p \cdot B_i$) based on tabular data. To succeed, agents must strategically manage a costâ€“benefit trade-off: they incur a specific cost (2 points) whenever they choose to reveal hidden information. This setup clarifies the agent's goal as active information gathering rather than simple classification. The innovation lies in benchmarking state-of-the-art modelsâ€”including GPT-4o, Claude 3.5 Sonnet, and Gemini 1.5 Proâ€”against established human baselines and a "resource-rational" model. The authors systematically expose these agents to canonical nudges (defaults and suggestions) and attempt to mitigate biases using advanced prompting strategies like Zero-shot Chain-of-Thought and Few-Shot learning (12 examples) to isolate behavioral susceptibilities.

The experiments reveal that LLM agents demonstrate "hypersensitivity" to nudges, exhibiting significantly higher susceptibility to default options and suggestions compared to human choice distributions. Across 340 trials for default configurations and 320 trials for suggestion configurations, models frequently displayed divergent and suboptimal information-gathering strategies, often incurring high costs to reveal irrelevant data while missing critical context. Crucially, the study found that models were swayed by "prize idiosyncrasy"â€”irrelevant details about the prizesâ€”rather than adhering to rational decision-making principles. Standard prompt engineering techniques failed to correct these behavioral flaws; neither Zero-shot Chain-of-Thought nor Few-Shot prompting successfully bridged the gap between LLM and human behavior or reduced the models' reactivity to nudges. However, the study found that nudges optimized using a human resource-rational model were effective in improving the performance of specific LLMs, suggesting that external environmental design can partially compensate for internal irrationality.

This research fundamentally challenges the perception of LLMs as neutral processors of information, establishing that choice architecture exerts a powerful, often distorting influence on autonomous agent behavior. For the field, this implies that current prompting techniques are insufficient for ensuring robust decision-making capabilities. Consequently, the authors argue for a paradigm shift where rigorous behavioral testing is a prerequisite for deploying LLMs in autonomous roles. The findings suggest that future systems must either be architected with "LLM-specific" nudges derived from resource-rational models or undergo extensive behavioral fine-tuning to function reliably in environments where subtle context variations can drastically alter outcomes.

---

## Key Findings

*   **Hypersensitivity to Nudges:** LLM agents demonstrate significantly higher susceptibility to canonical nudges (default options, suggestions, and information highlighting) compared to human choice distributions.
*   **Suboptimal Information Strategies:** The models exhibit divergent and often suboptimal strategies for gathering information, frequently incurring substantial costs or lacking necessary context.
*   **Limitations of Prompting:** Advanced prompting strategies like zero-shot chain-of-thought and few-shot prompting do **not** resolve the fundamental hypersensitivity of the models to nudges.
*   **Influence of Irrelevant Factors:** LLM performance is influenced by irrelevant factors, such as the specific idiosyncrasy of available prizes, rather than purely rational decision-making.
*   **Efficacy of Optimized Nudges:** Nudges optimized using a human resource-rational model can successfully improve the performance of certain LLM models.

---

## Methodology

The study employs a **case study approach** utilizing specific LLM models evaluated within a multi-attribute tabular decision-making problem. This environment is designed to simulate complex, sequential choice environments.

The methodology involves:
*   **Exposure to Canonical Nudges:** Testing variables such as default options and suggestions.
*   **Prompting Interventions:** Implementing zero-shot chain-of-thought and few-shot prompting to test robustness.
*   **Benchmarking:** Comparing LLM outputs against human choice distributions and performance metrics optimized via a human resource-rational model.

---

## Technical Details

*   **Paradigm:** Text-based sequential decision-making (replicating Callaway et al., 2023).
*   **Objective:** Agents choose baskets ($B_i$) to maximize reward ($r = p \cdot B_i$) while minimizing information costs.
*   **Cost Structure:** 2 points per reveal.
*   **Data Representation:** Markdown tables.
*   **Models Tested:**
    *   GPT-3.5-Turbo
    *   GPT-4o
    *   o3-Mini
    *   Claude 3 Haiku
    *   Claude 3.5 Sonnet
    *   Gemini 1.5 Pro/Flash
*   **Parameters:** Temperature set to 0.2.
*   **Prompting Strategies:**
    *   BASE
    *   Zero-shot Chain-of-Thought (COT)
    *   FEW-SHOT (12 examples)
*   **Nudge Configurations:**
    *   Defaults (most points)
    *   Suggested alternatives (early/late)
    *   Optimal nudges (derived from resource-rational model)

---

## Results

*   **Significant Hypersensitivity:** LLM agents displayed significantly higher susceptibility to nudges than human baselines.
*   **Information Gathering Failures:** Models often incurred high costs without optimizing the cost-benefit trade-off, revealing irrelevant data while missing key context.
*   **Prompting Failure:** Chain-of-Thought and Few-Shot prompting failed to resolve hypersensitivity or bridge the gap between LLM and human behavior.
*   **Successful Intervention:** Nudges optimized using a human resource-rational model successfully improved the performance of specific LLMs.
*   **Trial Configuration:** The setup involved 340 trials per condition for defaults and 320 trials per condition for suggestions across various grid configurations.

---

## Contributions

*   **Evidence of Non-Neutrality:** Provides empirical evidence that LLM agents are not neutral decision-makers but are highly reactive to choice architecture, often more so than humans.
*   **Behavioral Deviations:** Highlights specific areas where LLMs deviate from rational or human-like behavior, specifically regarding information acquisition costs and sensitivity to irrelevant reward features.
*   **Prompting Limitations:** Contributes to the understanding of how current prompting techniques fall short in correcting behavioral susceptibilities.
*   **Testing Standards:** Establishes the necessity for rigorous behavioral testing as a prerequisite for deploying LLMs as autonomous agents in high-stakes environments.