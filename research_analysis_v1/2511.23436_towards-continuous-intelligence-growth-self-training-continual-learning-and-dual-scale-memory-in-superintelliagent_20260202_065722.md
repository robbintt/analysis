# Towards Continuous Intelligence Growth: Self-Training, Continual Learning, and Dual-Scale Memory in SuperIntelliAgent

*Jianzhe Lin; Zeyu Pan; Yun Zhu; Ruiqi Song; Jining Yang*

---

> ### **Quick Facts**
> *   **Quality Score:** 8/10
> *   **Human Annotation Reliance:** 0%
> *   **References:** 40 Citations
> *   **Core Architecture:** Learner-Verifier Loop
> *   **Key Innovation:** Inference as Lifelong Optimization

---

## Executive Summary

Current Large Language Models (LLMs) are fundamentally static post-deployment, requiring expensive and resource-intensive retraining cycles to adapt. A primary barrier to artificial general intelligence is "catastrophic forgetting," where models lose previously acquired knowledge when learning new tasks. This research addresses the critical need for autonomous, lifelong learning systems that can evolve continuously without external human intervention, effectively transforming inference from a passive generation task into an active, lifelong optimization process.

SuperIntelliAgent introduces a "Learner-Verifier Loop," a dual-model architecture that decouples generation from reasoning by pairing a small, trainable diffusion model (the Learner) with a frozen, larger LLM (the Verifier). As the Learner generates candidates, the Verifier utilizes step-by-step reasoning to evaluate them, automatically creating Direct Preference Optimization (DPO) pairs without human labels. Crucially, the architecture separates **memory storage** from **management strategy**: it employs a dual-scale memory system for retention—integrating short-term in-context traces with long-term on-the-fly fine-tuning—while a distinct replay buffer manages this storage by constructing adaptive curricula to reinforce verifiable progress.

Empirical evaluation demonstrated positive performance deltas across all tested benchmarks, with the system achieving **0% reliance on human annotation** by deriving training signals exclusively from verifier feedback. The implementation of the replay buffer and dual-scale memory quantitatively mitigated catastrophic forgetting, allowing the model to successfully consolidate new knowledge while retaining previously learned capabilities. These results validate the framework's ability to maintain stability and drive continuous improvement using strictly synthetic data.

This research establishes a fundamental, reliable unit for intelligence growth: the coupling of a trainable learner with a reasoning-capable verifier. By proving that a minimal autonomous setup can drive continuous self-improvement, the paper shifts the paradigm from training as a discrete phase to training as a lifelong process. This infrastructure-agnostic approach offers a viable path toward deploying low-resource agents capable of adapting and evolving in dynamic environments without constant human oversight.

---

## Key Findings

*   **Continuous Self-Improvement:** SuperIntelliAgent enables continuous improvement of a small model without human annotation by treating inference loops as a lifelong optimization process.
*   **Automated Training Signals:** The learner improved across all benchmarks using automatically generated Direct Preference Optimization (DPO) pairs derived solely from verifier feedback.
*   **Fundamental Intelligence Unit:** The research establishes that pairing a trainable learner with a frozen, reasoning-capable verifier creates a fundamental, reliable unit for growing intelligence.
*   **Mitigation of Catastrophic Forgetting:** The combination of dual-scale memory and a replay buffer successfully reinforces recent learning and forms adaptive curricula, allowing the system to consolidate knowledge without catastrophic forgetting.

---

## Methodology

SuperIntelliAgent employs a dual-model, self-supervised architecture designed to function autonomously:

*   **Architecture Components:**
    *   **The Learner:** A trainable small diffusion model responsible for generating candidate outputs.
    *   **The Verifier:** A frozen Large Language Model responsible for evaluating outputs using step-by-step reasoning.
*   **The Self-Training Loop:**
    1.  The Learner generates candidate outputs.
    2.  The Verifier evaluates candidates to produce chosen/rejected pairs.
    3.  Direct Preference Optimization (DPO) is applied to convert inputs into pseudo-training signals.
*   **Memory & Curriculum Management:**
    *   **Dual-Scale Memory:** Utilizes short-term in-context memory for immediate context and long-term on-the-fly fine-tuning for persistent knowledge.
    *   **Replay Buffer:** A curriculum management strategy that retains samples showing verifiable progress to reinforce learning.

---

## Technical Details

| Component | Description |
| :--- | :--- |
| **Architecture** | **Learner-Verifier Loop** consisting of a trainable small 'learner' model and a frozen larger 'verifier' model. |
| **Optimization Process** | Treats inference as a lifelong optimization process utilizing **Direct Preference Optimization (DPO)**. |
| **Data Generation** | DPO pairs are generated automatically from verifier feedback without human annotation. |
| **Memory System** | Features a dual-scale memory system (in-context traces + on-the-fly fine-tuning) and adaptive curricula generation. |

---

## Core Contributions

*   **Agentic Framework:** Introduction of an infrastructure-agnostic framework that transforms ordinary inference loops into a continuous, autonomous learning process without external data annotation.
*   **Novel Architecture:** Proposal of a novel architecture coupling a small diffusion model with a large frozen LLM to facilitate continual intelligence growth via preference optimization.
*   **Advanced Memory System:** Development of a dual-scale memory system combining in-context traces and on-the-fly fine-tuning with a dedicated replay buffer for knowledge consolidation.
*   **Empirical Validation:** Proof that a minimal setup of a learner and a reasoning verifier is sufficient to drive significant performance improvements across benchmarks.

---

## Results & Evaluation

*   **Benchmark Performance:** The learner demonstrated performance improvement across **all benchmarks tested**.
*   **Knowledge Retention:** The system successfully avoided catastrophic forgetting, effectively retaining previous knowledge while integrating new data.
*   **Autonomy:** Achieved continuous improvement with **0% reliance on human annotation**, using exclusively synthetic data generated from verifier feedback.