# Performative Policy Gradient: Optimality in Performative Reinforcement Learning

*Debabrota Basu; Udvas Das; Brahim Driss; Uddalak Mukherjee*

| Metric | Detail |
| :--- | :--- |
| **Quality Score** | 8/10 |
| **References** | 40 Citations |
| **Algorithm** | PePG (Performative Policy Gradient) |
| **Core Concept** | Performative Optimality vs. Stability |
| **Framework** | PeMDP (Performative Markov Decision Process) |

---

## Executive Summary

Standard Reinforcement Learning (RL) operates under the assumption that the environment follows a static Markov Decision Process (MDP) where transition dynamics and reward functions are independent of the agent’s policy. However, in many real-world systems—such as financial markets, autonomous driving, or recommendation systems—the deployment of a policy fundamentally alters the environment itself, a phenomenon known as **performativity**.

Existing research in performative RL has largely focused on achieving **stability**, ensuring that the learning process converges to a fixed point where the policy no longer changes. The critical gap this paper addresses is that stability does not equate to optimality; a stable policy may be suboptimal if it fails to account for how its actions reshape the underlying dynamics.

The authors introduce the **Performative Policy Gradient (PePG)**, the first policy gradient algorithm designed specifically to achieve performative optimality rather than mere stability. Central to this innovation is the formulation of the Performative MDP (PeMDP), where transition kernels and reward functions are explicit functions of the deployed policy $\pi$. Technically, the authors derive performative counterparts to two fundamental RL theorems: the *Performance Difference Lemma* and the *Policy Gradient Theorem*. These new theoretical tools decompose value differences into a classical RL term and a "performative shift term," which accounts for the changes in rewards and transitions induced by the policy. Relying on softmax parametrization, PePG maximizes the Performative Value Function, allowing the agent to optimize its behavior while simultaneously considering how that behavior influences the environment.

The study provides both theoretical and empirical validation of the PePG algorithm. Theoretically, the authors prove that PePG converges to performatively optimal policies under softmax parametrization, covering scenarios both with and without entropy regularization. Empirically, experiments compared PePG against standard Policy Gradient (ERM) and existing performative methods focused only on stability. The results demonstrated that PePG consistently outperformed both baselines. Notably, as performative strength increased, the performance gap between standard ERM and the performative optimum widened significantly, while PePG maintained convergence to the optimal policy.

This research represents a significant paradigm shift in Reinforcement Learning, moving the field from analyzing performative stability to achieving performative optimality.

---

## Key Findings

*   **Algorithm Performance:** The Performative Policy Gradient (PePG) algorithm successfully converges to performatively optimal policies.
*   **Novelty:** PePG is the first policy gradient algorithm specifically designed to handle performativity within Reinforcement Learning.
*   **Theoretical Proof:** Convergence to optimality is rigorously proven under softmax parametrization, valid for cases both with and without entropy regularization.
*   **Empirical Superiority:** Experiments demonstrate that PePG outperforms standard policy gradient algorithms (ERM) and existing performative RL methods that aim only for stability.
*   **Impact of Performativity:** Results indicate that as performative strength increases, the performance gap between standard ERM and the performative optimum widens, highlighting the necessity of performative-aware algorithms.

---

## Methodology

The authors' methodology centers on the theoretical extension of fundamental RL theorems to account for performativity:

1.  **Derivation of Theorems:** The authors derived performative counterparts to the classic Performance Difference Lemma and the Policy Gradient Theorem.
2.  **Algorithm Formulation:** Based on these new theorems, they formulated the **PePG algorithm** to optimize policies specifically in environments where the agent's actions influence the underlying dynamics (performative environments).
3.  **Parametrization Focus:** The analysis focuses heavily on **softmax parametrization**.
4.  **Regularization Analysis:** The study includes rigorous theoretical analysis regarding entropy regularization to prove convergence stability.

---

## Technical Details

The technical foundation of the paper is built upon the concept of the **Performative Markov Decision Process (PeMDP)**.

*   **Model Structure (PeMDP):** Unlike static MDPs, the PeMDP defines transition kernels and reward functions as dependent on the deployed policy $\pi$.
*   **Objective Function:** The algorithm maximizes the **Performative Value Function**, denoted as $V^{\pi}_{\pi}(s)$.
*   **Gradient Decomposition:** utilizes the **Performative Performance Difference Lemma** to decompose value differences into two components:
    *   A **classical term** (standard RL value difference).
    *   A **performative shift term** (accounting for shifts in rewards and transitions caused by the policy).
*   **Assumptions:**
    *   Bounded rewards.
    *   Lipschitz continuous environments.

---

## Contributions

*   **Theoretical Tools:** Provided performative versions of the Performance Difference Lemma and Policy Gradient Theorem, expanding the theoretical toolkit available for performative RL.
*   **Paradigm Shift:** Advanced the field by moving beyond the goal of performative stability to achieve **performative optimality**.
*   **Practical Application:** Introduced the practical PePG algorithm, enabling RL agents to operate effectively in performative environments.
*   **Validation:** Established empirical evidence proving that performative-aware algorithms yield superior results compared to standard RL or stability-focused methods.

---

## Results

The experimental evaluation focused on comparing PePG against baseline methods across varying degrees of performativity.

*   **Baselines:** Standard Policy Gradient (ERM) and existing performative methods targeting stability.
*   **Metric:** Average Reward.
*   **Variable:** Performative Strength ($\alpha \in \{0.0, 0.2, 0.5, 0.8\}$).
*   **Outcome:**
    *   PePG converged to performatively optimal policies across all tested strengths.
    *   PePG outperformed both standard PG and stability-focused methods when performative effects were present.
    *   The gap between standard ERM performance and the performative optimum widened as performative strength ($\alpha$) increased, validating the necessity of the PePG approach.