---
title: Reciprocal Space Attention for Learning Long-Range Interactions
arxiv_id: '2510.13055'
source_url: https://arxiv.org/abs/2510.13055
generated_at: '2026-01-26T16:27:38'
quality_score: 8
citation_count: 0
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# Reciprocal Space Attention for Learning Long-Range Interactions

*Alvaro Vazquez, Atul C. Thakur, Ganesh Sivaraman, Hariharan Ramasubramanian*

---

### üìä Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Core Innovation** | Reciprocal Space Attention (RSA) |
| **Computational Complexity** | Linear ($O(N)$) vs. Standard Quadratic ($O(N^2)$) |
| **Theoretical Basis** | Convolution Theorem & Fast Fourier Transform (FFT) |
| **Effective Interaction Range** | > 30 Angstroms |
| **Primary Application** | N-body problems, Fluid dynamics, Material science |
| **Underlying Framework** | MACE (Message Passing Neural Networks) |
| **Quality Score** | **8/10** |

---

## üìù Executive Summary

Simulating physical and geometric systems involving long-range interactions presents a significant computational challenge, as standard Transformer architectures suffer from quadratic complexity in their self-attention mechanisms, making them prohibitively expensive for large-scale systems. Conversely, common Graph Neural Networks (GNNs) and message-passing frameworks rely on local receptive fields, failing to capture critical global dependencies such as electrostatics or dispersion forces. This limitation hinders the accuracy of learned models in physical simulations, where many systems of interest‚Äîsuch as N-body problems and turbulent flows‚Äîrely fundamentally on interactions that span the entire domain.

The authors introduce **"Reciprocal Space Attention" (RSA)**, a novel architecture integrated into the MACE (Message Passing Neural Networks) framework as a Long-Range ("LR") component to overcome these limitations. The method leverages the Convolution Theorem to shift interaction computations from Euclidean space to the spectral domain. The process begins by projecting atomic features via learnable weight matrices, followed by a transformation into reciprocal space using the **Fast Fourier Transform (FFT)**. In this spectral domain, attention is computed through efficient element-wise multiplications, allowing the model to process all pairwise interactions simultaneously. Crucially, the results are transformed back to real space via an **inverse FFT**, incorporating global context into the original data structure. This LR module operates in parallel with local message-passing blocks‚Äîwhich utilize a radial cutoff of 5-6 √Ö‚Äîsumming their contributions to update node states while maintaining translational invariance through Bloch phase factors.

The proposed LR-MACE model demonstrates superior accuracy in capturing long-range physical interactions that standard methods miss. In $S_N2$ reaction simulations, local-only models (SR-MACE) failed to reproduce DFT energy profiles beyond 10 Angstroms, whereas the RSA-augmented model accurately modeled asymptotic electrostatic interactions. For dimer binding curves within a 30 Angstrom cubic box, the RSA module correctly recovered long-range asymptotics. Furthermore, in phosphorene exfoliation tests, the model achieved accuracy comparable to DFT+MBD methods without requiring explicit empirical corrections for dispersion forces.

This work establishes a critical theoretical bridge between deep learning architectures and fundamental physics principles. By offering a scalable solution that decouples global receptive fields from quadratic computational costs, the approach enables the modeling of complex systems at scales previously unattainable for learned models.

---

## üîë Key Findings

*   **Global Interaction Efficiency:** The approach enables the learning of long-range interactions in physical and geometric systems with significantly higher computational efficiency compared to standard attention mechanisms.
*   **Linear Complexity:** The proposed method reduces the computational complexity of capturing global dependencies, effectively overcoming the quadratic bottleneck associated with traditional self-attention mechanisms in Transformers.
*   **State-of-the-Art Performance:** The model achieves superior or competitive accuracy on benchmarks involving fluid dynamics, particle systems, and other physical simulations where long-range forces are critical.

---

## üß™ Methodology

The core methodology relies on shifting the computational domain to handle global interactions efficiently:

1.  **Reciprocal Space Transformation**
    The core method shifts the computation of interactions from real space (Euclidean) to reciprocal space (spectral/frequency domain) using the Fast Fourier Transform (FFT).

2.  **Spectral Attention**
    By applying the Convolution Theorem, the method computes attention by performing element-wise multiplications in the frequency domain. This allows for the simultaneous processing of all pairwise interactions regardless of distance.

3.  **Inverse Transformation**
    The results are transformed back to real space via an inverse FFT, preserving the original data structure while incorporating global context.

---

## ‚öôÔ∏è Technical Details

The proposed architecture augments the standard MACE framework with specific configurations to handle long-range dependencies:

*   **Base Framework:** Built upon the MACE (Message Passing Neural Networks) framework.
*   **Architecture:** Augments the standard Short-Range ("SR") architecture with a Long-Range ("LR") component.
*   **Components:**
    *   **RSA ("Reciprocal Space Attention"):** Handles global interactions.
    *   **FPE ("Fourier Positional Encoding"):** Integrated to process node features.
*   **Processing Flow:** LR processes node features in parallel with local Message Passing blocks; contributions are summed to update node states.
*   **Physical Constraints:** Respects translational invariance through the use of Bloch phase factors.
*   **Hyperparameters:**
    *   **Radial Cutoff ("SR"):** 5-6 Angstroms.
    *   **Message Passing Layers:** 2 layers.
    *   **RSA Smearing Width ($\sigma$):** 5 Angstroms.
    *   **Total "SR" Receptive Field:** 10-12 Angstroms.
*   **Feature Projection:** Atomic features are projected via learnable weight matrices into a specific dimension.

---

## üìà Results

The model was rigorously tested against standard baselines (like SR-MACE) and ground truth data (DFT):

*   **$S_N2$ Reactions:** Beyond 10 Angstroms, the standard SR-MACE failed to reproduce DFT energy profiles. Conversely, LR-MACE accurately modeled the asymptotic electrostatic interactions.
*   **Dimer Binding Curves:** In a 30 Angstrom cubic box, the RSA module correctly recovered long-range asymptotics.
*   **Phosphorene Exfoliation:** LR-MACE achieved accuracy comparable to DFT+MBD (Many-Body Dispersion) for dispersion forces without requiring explicit empirical corrections.
*   **Bulk Water Dynamics:** Tested at 300K across 1,593 configurations. The model provided stable trajectories and excellent structural agreement with baselines.
*   **Overall Outcome:** The method achieves state-of-the-art accuracy with linear complexity, effectively handling ranges up to at least 30 Angstroms.

---

## üåü Contributions

*   **Novel Architecture:** Introduction of "Reciprocal Space Attention," a new architecture designed specifically to handle global receptive fields without the computational penalties of vanilla Transformers or Graph Neural Networks (GNNs).
*   **Physical Grounding:** The work establishes a theoretical bridge between deep learning architectures and fundamental physics principles (specifically the use of Fourier space for long-range potentials), making the model particularly suitable for scientific machine learning applications.
*   **Scalability:** The contribution offers a scalable solution for modeling complex systems (such as N-body problems or turbulent flows) where the range of interaction is global, enabling larger-scale simulations than previously possible with learned models.

---
**References:** 0 citations