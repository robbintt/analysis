# Dagstuhl Perspectives Workshop 24352 -- Conversational Agents: A Framework for Evaluation (CAFE): Manifesto

*Christine Bauer; Li Chen; Nicola Ferro; Norbert Fuhr; Avishek Anand; Timo Breuer; Guglielmo Faggioli; Ophir Frieder; Hideo Joho; Jussi Karlgren; Johannes Kiesel; Bart P. Knijnenburg; Aldo Lipani; Lien Michiels; Andrea Papenmeier; Maria Soledad Pera; Mark Sanderson; Scott Sanner; Benno Stein; Johanne R. Trippas; Karin Verspoor; Martijn C Willemsen*

---

> ### üìä Quick Facts
> - **Quality Score:** 9/10
> - **References:** 40 Citations
> - **Document Type:** Manifesto / Framework Proposal
> - **Key innovation:** CAFE Evaluation Framework
> - **Core Model:** CONIAC (CONversational Information Access Agents)

---

## üìã Executive Summary

The evaluation of Conversational Information Access Agents (CONIAC) currently suffers from a lack of standardized theoretical foundations and fragmented assessment methodologies. Existing evaluation paradigms often rely on static, turn-based metrics borrowed from traditional information retrieval, which fail to capture the dynamic, stateful, and continuous nature of conversational interactions. This discrepancy is a significant barrier to progress in the field, as it prevents accurate comparison of systems, obscures the impact of mixed-initiative strategies, and limits the understanding of user experience over time. Without a unified framework, it is impossible to rigorously assess how well these agents serve stakeholder objectives or handle complex, context-dependent user tasks.

The findings presented emerged from a collaborative synthesis of expert domain knowledge during the Dagstuhl Perspectives Workshop 24352. The primary innovation is the **CAFE (Conversational Agents Framework for Evaluation)**, supported by the theoretical **CONIAC World Model**, which abstracts the environment using a hierarchical architecture. This model consists of a Process Layer that governs continuous conversational events (Start, End, User, System) to support temporal flexibility, and a System Layer that functions as a stateful machine maintaining a Shared State of World Knowledge, User Information, and State Tracking. Bridging these components, the Evaluation Layer allows CAFE to operationalize the model by deploying evaluation probes that function synchronously or asynchronously across six dimensions: Stakeholder Goals, User Aspects, Tasks, Criteria, Methodology, and Measures.

As the document is a manifesto and framework proposal rather than an experimental study, it does not report quantitative performance data (e.g., F1 scores). Instead, its results are qualitative, defining a new structure for assessment categories and proposing a shift from summative to formative evaluation. The framework specifies context-dependent criteria, distinguishing between "fast-time response" for simple queries and "comprehensibility" for complex tasks. It further differentiates between stationary methodologies (evaluating single interactions) and continuous methodologies (incorporating physiological tracking or longitudinal analysis). These definitions provide the specific metrics and boundaries required for future empirical validation of the framework.

The CAFE framework represents a foundational shift in the field of conversational agents by providing the first comprehensive attempt to integrate stakeholder objectives, user attributes, and dynamic task complexities into a single evaluation model. By establishing a common vocabulary and a structured approach to measuring temporal and contextual performance, this manifesto addresses a critical gap in current research. Its significance lies in its potential to guide the development of future benchmarks and standardized testing protocols, moving the industry away from static benchmarks toward rigorous, context-aware evaluation methods that reflect the real-world utility of conversational systems.

---

## üîç Key Findings

*   **CONIAC Definition:** The workshop successfully defined CONIAC (CONversational Information Access Agents) and identified its unique features compared to traditional systems.
*   **World Model Proposal:** Proposed a 'world model' to abstract the environment and operational context, allowing for a theoretical grounding of agent interactions.
*   **New Standard (CAFE):** Established the CAFE framework to standardize system assessment, moving away from fragmented methods.
*   **Holistic Scope:** Defined a holistic evaluation scope that integrates three critical elements:
    *   Stakeholder objectives
    *   User tasks
    *   User attributes

---

## üèóÔ∏è Research Contributions

The primary contributions of this workshop report are theoretical and structural, providing the necessary vocabulary and architecture for future evaluation standards:

1.  **The CONIAC World Model:** A theoretical model designed to facilitate a high-level understanding of the conversational agent domain.
2.  **The CAFE Framework:** A six-component framework designed to standardize how systems are assessed. It consists of:
    *   Stakeholder goals
    *   User tasks
    *   User attributes
    *   Evaluation criteria
    *   Evaluation methodology
    *   Measures

---

## ‚öôÔ∏è Technical Details

The paper introduces the **CONIAC (CONversational Information Access Agents) World Model**, a hierarchical architecture designed to handle the complexity of conversational systems.

### Architecture Layers

1.  **Process Layer**
    *   Defines interactions as continuous conversational events.
    *   **Components:** Start, End, User, System.
    *   **Features:**
        *   Focuses on a single objective.
        *   Supports temporal flexibility.
        *   Enables mixed-initiative strategies.

2.  **System Layer**
    *   Functions as a stateful machine.
    *   **Core Component:** Maintains a **Shared State**.
    *   **Sub-components:**
        *   World Knowledge
        *   User Information
        *   State Tracking

3.  **Evaluation Layer**
    *   Bridges the Process and System layers.
    *   Utilizes **CAFE Evaluation Framework**.
    *   **Mechanism:** Deploys evaluation probes that operate:
        *   Synchronously (real-time)
        *   Asynchronously (post-hoc)
    *   **Dimensions:** Operates across six dimensions to assess performance.

---

## üìà Results & Implications

As this document is a manifesto and framework proposal, it does not provide specific experimental results or quantitative performance data (e.g., F1 scores). Instead, it defines qualitative assessment categories and proposes a fundamental shift in evaluation philosophy:

### Shift in Evaluation Type
*   **From:** Summative evaluation (assessing at the end).
*   **To:** Formative evaluation (assessing during development and interaction).

### Key Assessment Categories
*   **Temporal Dimensions:** Focuses on performance changes over time.
*   **Assessment Scope:** Ranges from micro-level details to macro-level interactions.
*   **Context-Specific Metrics:**
    *   *Simple queries:* Fast-time response.
    *   *Complex tasks:* Comprehensibility.
*   **Methodology Distinction:**
    *   **Stationary:** Single-interaction evaluation.
    *   **Continuous:** Longitudinal analysis including physiological tracking.

---

## üìù Methodology

The findings were derived through a **Collaborative Deep Discussion** during the *Dagstuhl Perspectives Workshop 24352*. In this setting, experts collectively:
1.  Analyzed conceptual boundaries of CONIAC.
2.  Synthesized domain knowledge from various participants.
3.  Constructed a structured evaluation framework (CAFE) based on consensus.

---

## üìö Metrics & Quality

| Metric | Value |
| :--- | :--- |
| **Quality Score** | 9/10 |
| **References** | 40 Citations |
| **Research Type** | Conceptual / Manifesto |