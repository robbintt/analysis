---
title: 'GR-Agent: Adaptive Graph Reasoning Agent under Incomplete Knowledge'
arxiv_id: '2512.14766'
source_url: https://arxiv.org/abs/2512.14766
generated_at: '2026-02-03T07:16:24'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# GR-Agent: Adaptive Graph Reasoning Agent under Incomplete Knowledge

*Dongzhuoran Zhou; Yuqicheng Zhu; Xiaxia Wang; Hongkuan Zhou; Jiaoyan Chen; Steffen Staab; Yuan He; Evgeny Kharlamov*

---

> ### üìä Quick Facts
> - **Quality Score:** 9/10
> - **References:** 40 citations
> - **Method Type:** Training-free Agent
> - **Datasets Used:** Family & FB15k-237
> - **Split Ratio:** 8:1:1
> - **Core Focus:** Multi-hop reasoning over incomplete knowledge graphs

---

## üìù Executive Summary

Current research in Knowledge Graph Question Answering (KGQA) suffers from a critical evaluation gap: most existing benchmarks rely on complete knowledge graphs, failing to account for the inherent incompleteness of real-world data. This limitation allows state-of-the-art models to excel at shallow retrieval without demonstrating true reasoning capabilities. This paper addresses the fragility of these methods, demonstrating that performance degrades significantly when direct supporting facts are missing, thereby highlighting the need for systems capable of robust, multi-hop reasoning under uncertain conditions.

The authors introduce a two-fold innovation comprising a novel benchmark construction methodology and the **Adaptive Graph Reasoning Agent (GR-Agent)**. The benchmark simulates real-world incompleteness by utilizing AMIE3 to mine high-confidence Horn rules and strategically removing "head" triples (direct answers) while preserving "body" triples (reasoning paths), with GPT-4 employed to generate corresponding questions. GR-Agent addresses the reasoning challenge as a training-free, agent-environment interaction. It utilizes a defined action space of graph reasoning tools‚Äîspecifically GetNeighbors, FilterByRelation, and FilterByEntityType‚Äîcombined with a mutable memory mechanism to operate at the relation-path level. This architecture enables the system to adaptively track evidence and infer answers without requiring model training.

Evaluations were conducted on the Family and FB15k-237 datasets. The experiments revealed that standard training-based methods, such as PullNet and GraftNet, suffer catastrophic performance drops in incomplete settings‚Äîoften falling from approximately 60% accuracy in complete settings to near 0% when head triples are removed. Similarly, non-training LLM baselines like CoT and ReAct failed to maintain performance. In contrast, **GR-Agent demonstrated high robustness across both settings**, matching the accuracy of state-of-the-art training-based methods in complete settings while retaining significant performance (50-60% range) in incomplete settings where other models failed.

This research significantly shifts the KGQA paradigm from evaluating shallow retrieval to assessing complex reasoning over incomplete knowledge structures. By exposing the vulnerability of current models to missing facts, the paper establishes a rigorous new standard for evaluating robustness in KGQA.

---

## üîç Key Findings

*   **Benchmark Limitation:** Most existing KGQA benchmarks rely on complete knowledge graphs, which limits evaluation to shallow retrieval and ignores real-world incompleteness.
*   **Performance Fragility:** Existing methods suffer significant performance degradation when direct facts are missing, revealing limited reasoning capabilities.
*   **Superior Performance:** The proposed GR-Agent outperforms non-training baselines and matches training-based methods without requiring model training.
*   **Robustness:** GR-Agent maintains high performance in both complete and incomplete KG settings, effectively forcing multi-hop reasoning rather than direct lookup.

---

## ‚ú® Contributions

*   **Novel Benchmark Methodology:** A new approach for constructing KGQA benchmarks that reflects incomplete KGs, shifting the evaluation focus from shallow retrieval to multi-hop reasoning over missing facts.
*   **Adaptive Graph Reasoning Agent:** The introduction of GR-Agent, which reframes KGQA as an interactive agent problem utilizing graph tools and memory.
*   **Empirical Validation:** Demonstrates the fragility of current state-of-the-art methods under incomplete conditions and establishes GR-Agent as a robust, training-free alternative.

---

## üõ†Ô∏è Methodology

The paper proposes a comprehensive two-fold framework involving both benchmark construction and an agent-based solution:

1.  **Benchmark Construction:**
    *   Simulates KG incompleteness by removing direct supporting triples ("head" triples).
    *   Ensures alternative reasoning paths ("body" triples) remain intact to allow for multi-hop reasoning.
    *   Utilizes AMIE3 for rule mining and GPT-4 for question generation.

2.  **Agent-Based Solution (GR-Agent):**
    *   Formalizes KGQA as an agent-environment interaction.
    *   **Environment:** An interactive environment constructed directly from the KG.
    *   **Action Space:** A defined set of graph reasoning tools.
    *   **Memory:** A mechanism to track supporting evidence and reasoning history.

---

## ‚öôÔ∏è Technical Details

*   **Rule Mining:** Utilizes **AMIE3** to mine high-confidence Horn rules for benchmark construction.
*   **Triple Strategy:** Implements a specific removal strategy where head triples are removed while body triples are kept to simulate missing information.
*   **Question Generation:** Employs **GPT-4** to generate questions corresponding to the constructed reasoning paths.
*   **Architecture:** GR-Agent is **training-free** and operates on an interactive KG environment with mutable memory.
*   **Reasoning Level:** Operates at the **relation-path level**.
*   **Datasets & Splits:**
    *   **Family Dataset:** 2,165 questions (Split: 1,749 Train / 218 Val / 198 Test).
    *   **FB15k-237 Dataset:** 5,449 questions (Split: 4,374 Train / 535 Val / 540 Test).
    *   **Split Ratio:** Consistent 8:1:1 ratio across datasets.

---

## üìà Results

**Dataset Statistics:**
The study utilized the Family and FB15k-237 datasets, containing 2,165 and 5,449 questions respectively. These datasets featured varying numbers of complete and incomplete triples alongside mined rules to test different reasoning scenarios.

**Performance Analysis:**
*   **Baseline Failure:** Baseline methods showed marked performance degradation in incomplete settings, often collapsing when direct facts were unavailable.
*   **GR-Agent Success:**
    *   Outperformed non-training baselines significantly.
    *   Matched the accuracy of training-based methods (e.g., PullNet) in complete settings.
    *   Maintained robust performance in incomplete settings (approx. 50-60% accuracy) where competing models failed.
*   **Reasoning Verification:** Results confirmed that GR-Agent successfully forces multi-hop reasoning rather than relying on simple direct lookups.