# ARISE: An Adaptive Resolution-Aware Metric for Test-Time Scaling Evaluation in Large Reasoning Models

*Zhangyue Yin; Qiushi Sun; Zhiyuan Zeng; Zhiyuan Yu; Qipeng Guo; Xuanjing Huang; Xipeng Qiu*

***

> ### ðŸ“Š Quick Facts
>
> *   **Metric Name:** ARISE (Adaptive Resolution-Aware Integrated Scoring Engine)
> *   **Primary Focus:** Test-time scaling efficiency in Large Reasoning Models
> *   **Top Performing Model:** Claude Opus
> *   **Key Innovation:** Penalization of negative scaling behaviors
> *   **Domains Validated:** Math, Code, Agentic Tasks

***

## Executive Summary

### **The Problem**
Current evaluation frameworks for Large Reasoning Models (LRMs) rely predominantly on static accuracy metrics measured at fixed computational budgets. This approach is fundamentally inadequate for characterizing model performance under the dynamic resource allocation strategies inherent to test-time scaling. Consequently, the field lacks a standardized metric for compute efficiency, leading to an evaluation gap where "negative scaling"â€”increased inference compute actively degrading performanceâ€”goes obscured.

### **The Innovation**
The authors introduce **ARISE** (Adaptive Resolution-aware Scaling Evaluation), a novel metric designed to evaluate performance across a continuous spectrum of test-time compute budgets. ARISE captures the complete scaling trajectory via fine-grained area-under-the-curve integration. Its architecture incorporates:
*   A **sample-level awareness mechanism**.
*   A rigorous **mathematical penalty for negative scaling**.
*   A **dynamic sampling mechanism** to mitigate noise from accuracy fluctuations.

### **The Results**
Comprehensive validation revealed significant disparities in scaling efficiency among state-of-the-art models, demonstrating that increased compute does not guarantee linear returns. **Claude Opus** exhibited superior scaling characteristics compared to peers. Crucially, ARISE successfully identified and quantified negative scaling behaviors, distinguishing models with reliable profiles from those with unstable resource utilization.

### **The Impact**
This research shifts the research priority from static accuracy to the computational efficiency of resource utilization. By rigorously penalizing negative scaling, ARISE prevents models from benefiting from inefficient brute-force strategies, setting a new standard for benchmarking reasoning models and guiding the design of future compute-efficient architectures.

***

## Key Findings

*   **Fine-Grained Measurement:** ARISE provides a reliable and fine-grained measurement of test-time scaling capabilities across diverse domains.
*   **Non-Uniform Performance:** Significant variations exist in scaling efficiency among state-of-the-art reasoning models, showing that adding compute does not guarantee uniform performance gains.
*   **Model Superiority:** **Claude Opus** exhibits superior scaling characteristics compared to other contemporary models.
*   **Negative Scaling:** The study confirms the existence of negative scaling behaviors (where increased compute degrades performance) and demonstrates that ARISE successfully penalizes them.

***

## Methodology

The authors introduce **ARISE**, a novel metric designed to assess dynamic computational resource allocation during inference. The methodology focuses on three main components:

1.  **Metric Design:** The approach incorporates a sample-level awareness mechanism specifically designed to penalize negative scaling behaviors.
2.  **Noise Mitigation:** It utilizes a dynamic sampling mechanism to minimize the impact of noise resulting from accuracy fluctuations.
3.  **Validation:** The method was rigorously validated through comprehensive experiments evaluating state-of-the-art reasoning models across multiple domains, including:
    *   Mathematics
    *   Coding
    *   Agentic tasks

***

## Technical Details

**Core Technology:** Adaptive Resolution-Aware Integrated Scoring Engine (ARISE)

**System Architecture:**
*   **Continuous Spectrum Evaluation:** Evaluates performance across a continuous spectrum of test-time compute budgets rather than discrete static points.
*   **Trajectory Capture:** Captures the scaling trajectory to show exactly how model performance evolves as the compute budget increases.
*   **Penalty Mechanism:** Includes a mathematical penalty for 'negative scaling' (performance degradation with increased compute).
*   **Fine-Grained Measurement:** Likely employs area-under-the-curve (AUC) or integral approaches to discriminate efficient scaling capabilities.

***

## Contributions

*   **Standardized Framework:** Provides a standardized, systematic framework for comparing test-time scaling capabilities across reasoning models.
*   **Robust Assessment:** Advances metric design by offering a robust assessment standard that accounts for inference process instability.
*   **Scaling Efficiency Definition:** Contributes a rigorous definition of scaling efficiency that penalizes negative scaling, preventing models from benefiting inefficiently.
*   **Benchmarking Insights:** Offers new insights that shift the focus from static accuracy to the efficiency of compute utilization.

***

## Results

*   **Claude Opus Performance:** Demonstrated superior scaling characteristics compared to state-of-the-art reasoning models.
*   **Diminishing Returns:** Results indicated non-uniform gains, showing that adding compute does not guarantee linear returns across different architectures.
*   **Differentiation:** ARISE successfully differentiated models with reliable scaling from those with volatile or negative scaling slopes.
*   **Robustness:** The metric was validated across diverse domains (Math, Code, Agentic), proving robust as a general-purpose evaluator.
*   **Negative Scaling Quantification:** Successfully quantified and penalized negative scaling behaviors where extended inference lowered accuracy.

***

**Quality Score:** 7/10  
**References:** 0 citations