# A Graph Perspective to Probe Structural Patterns of Knowledge in Large Language Models

*Utkarsh Sahu; Zhisheng Qi; Yongjia Lei; Ryan A. Rossi; Franck Dernoncourt; Nesreen K. Ahmed; Mahantesh M Halappanavar; Yao Ma; Yu Wang*

---

### ðŸ“Š Quick Facts Sidebar

| Metric | Details |
| :--- | :--- |
| **Research Rating** | 6/10 |
| **Total References** | 36 Citations |
| **Models Evaluated** | GPT-3.5, GPT-4o, Gemini-2.5 Flash, LLaMA3.3-70B, DeepSeek-V3 |
| **Key Datasets** | T-Rex, WD50K, PharmKG8K, MVPKG |
| **Core Method** | Graph Machine Learning (GNN) & Message Passing |
| **Primary Metric** | Knowledge Homophily ($H_{v_i}$) |

---

## Executive Summary

Current research on Large Language Models (LLMs) predominantly focuses on knowledge access capabilities while neglecting the structural organization of that information within the network. This oversight presents a critical inefficiency: standard probing and fine-tuning strategies often treat data as isolated points, ignoring the topological context that connects entities. To optimize model reliability and training efficiency, it is necessary to map the distribution of knowledge within neural networks rather than relying on random sampling or treating knowledge in a vacuum.

This paper addresses this challenge by rigorously analyzing the relationship between graph topology and knowledge retention, establishing a quantitative link between structural properties and what the model "knows." The authors introduce a graph-centric framework to quantify LLM knowledge at both triplet and entity levels, mapping it against structural properties such as node degree and clustering coefficient.

The key technical innovation is the identification and quantification of **"knowledge homophily,"** where topologically close entities tend to possess similar levels of knowledgeability. Leveraging this principle, the authors employ Graph Machine Learning (GNNs) to predict an entityâ€™s knowledgeability based on its local neighbors. This mechanism powers a "knowledge checking" pipeline that strategically identifies and selects triplets likely to be unknown to the LLM, enabling targeted data augmentation. Experiments across general and specialized datasets confirmed high knowledge homophily and demonstrated that fine-tuning on graph-selected triplets results in superior model performance compared to baseline methods.

---

## Key Findings

*   **Graph-Knowledge Correlation:** There is a quantifiable relationship between the knowledge stored in LLMs and graph structural properties (e.g., node degree) analyzed at both triplet and entity levels.
*   **Knowledge Homophily:** The study uncovers a phenomenon called "knowledge homophily," where topologically close entities tend to possess similar levels of knowledgeability within the model.
*   **Neighbor-Based Estimation:** Graph Machine Learning models can effectively estimate an entity's knowledge level by leveraging information from its local neighbors.
*   **Optimized Fine-Tuning:** Using graph-based models to identify and select triplets that are less known to the LLMs creates a superior dataset for fine-tuning, leading to better model performance.

---

## Methodology

The authors adopt a **graph-centric analysis framework** characterized by the following stages:

1.  **Quantification:** The team begins by quantifying LLM knowledge at two distinct levels: the triplet level and the entity level. This data is then correlated with graph structural properties like node degree.
2.  **Modeling:** Building on the discovery of knowledge homophily, they employ Graph Machine Learning (GML) models. These models are trained to estimate the knowledgeability of an entity based on its local topological neighbors.
3.  **Application:** Finally, they utilize this model to perform **"knowledge checking."** This involves strategically selecting tripletsâ€”specifically those less known to the LLMâ€”to create optimized fine-tuning datasets.

---

## Contributions

*   **Structural Analysis of Knowledge:** Shifts the research focus from general knowledge access to the structural organization of knowledge within neural networks.
*   **Identification of Knowledge Homophily:** Defines and characterizes a specific structural pattern describing how knowledge is distributed across connected entities.
*   **Application to Knowledge Checking:** Introduces a novel application of graph models for automatically identifying gaps in LLM knowledge using topological cues.
*   **Improved Fine-Tuning Strategy:** Demonstrates a practical pipeline for improving LLM performance by fine-tuning on data selected based on graph structural insights rather than random selection.

---

## Technical Details

The technical implementation of the research involves the following components and protocols:

*   **Graph Representation:** Models LLM knowledge as a graph $G=(V, R, F)$.
*   **Structural Metrics:** Utilizes node degree and clustering coefficient as primary structural features.
*   **Knowledge Quantization:**
    *   **Triplet Level:** Quantized using LLM prompts and mapped to binary values.
    *   **Entity Level:** Aggregated from triplet scores to calculate entity-level knowledgeability.
*   **Metrics:**
    *   **Knowledge Homophily ($H_{v_i}$):** Defined to measure the similarity of knowledge between topological neighbors.
*   **Model Architecture:**
    *   Utilizes a **GNN-based regression architecture**.
    *   Components include Message Passing (MP) and Feature Transformation (TR).
*   **Inputs & Training:**
    *   Inputs include text embeddings or one-hot encodings.
    *   Training utilizes **Mean Squared Error (MSE)** as the loss function to predict entity knowledgeability for unprobed entities.

---

## Results

Experiments evaluated models (GPT-3.5, GPT-4o, Gemini-2.5 Flash, LLaMA3.3-70B, DeepSeek-V3) on general datasets (T-Rex, WD50K) and specialized datasets (PharmKG8K, MVPKG).

*   **Distribution of Knowledge:**
    *   Knowledgeability scores showed a **trimodal distribution**.
    *   **Specialized datasets:** Low coverage (peak at 0.0).
    *   **General datasets:** High coverage (peak at 1.0).
    *   **Temporal Impact:** Inclusion of temporal information shifted distributions towards lower knowledgeability.
*   **Homophily Scores:**
    *   Consistently high (**> 0.5**) with a peak around **0.8**.
    *   Slightly reduced when temporal data was included.
*   **Correlation:** A positive correlation was found between node degree and knowledgeability.
*   **Prediction Accuracy:** The GNN model effectively estimated unknown entity knowledgeability.

---
**Document Rating:** 6/10 | **References:** 36 Citations