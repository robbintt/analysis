---
title: 'WebSeer: Training Deeper Search Agents through Reinforcement Learning with
  Self-Reflection'
arxiv_id: '2510.18798'
source_url: https://arxiv.org/abs/2510.18798
generated_at: '2026-02-03T12:42:53'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# WebSeer: Training Deeper Search Agents through Reinforcement Learning with Self-Reflection

*Guanzhong He; Zhen Yang; Jinxin Liu; Bin Xu; Lei Hou; Juanzi Li*

---

> ### ðŸ“Š QUICK FACTS
>
> *   **Model Size:** 14B Parameters
> *   **HotpotQA Score:** 72.3% Accuracy
> *   **SimpleQA Score:** 90.0% Accuracy
> *   **Max Chain Length:** Up to 50 steps
> *   **Quality Score:** 9/10
> *   **Citations:** 40

---

## Executive Summary

Current Agentic Retrieval-Augmented Generation (RAG) systems struggle to solve complex, open-domain, multi-hop questions due to shallow tool usage and error propagation. Existing models often rely on short chains of reasoning, limiting their ability to explore the web deeply or recover from incorrect information retrieval. This creates a critical bottleneck where agents cannot verify findings or backtrack effectively, leading to accumulated errors and poor performance on tasks requiring sustained, deep exploration.

WebSeer addresses these limitations through a novel reinforcement learning framework augmented with a self-reflection mechanism. The architecture employs a unified two-stage training process, necessitating a "cold start" phase using Supervised Fine-Tuning (SFT) with multi-turn rejection sampling. This initialization is essential to provide a stable policy foundation before moving to the second stage, Self-Reflective Reinforcement Learning (SRRL), which operates on a large-scale dataset annotated with reflection patterns.

Technically, the agent operates on a sequential chain structureâ€”looping through generation, tool invocation, parameter extraction, and observation integrationâ€”equipped with a toolkit including a Search Engine, Webpage Reader, and Code Executor. The key innovation lies in the self-reflection component, which enables the model to cross-verify information and backtrack, extending tool-use chains up to 50 steps without losing coherence. Utilizing a single 14B parameter model, WebSeer achieved state-of-the-art performance on major benchmarks, securing 72.3% accuracy on HotpotQA and 90.0% on SimpleQA. This research significantly advances the field of autonomous web agents by proving that deep exploration and self-correction are feasible within a unified training framework.

---

## Key Findings

*   **State-of-the-Art Performance:** Achieved leading results on **HotpotQA (72.3%)** and **SimpleQA (90.0%)** utilizing a single 14B parameter model.
*   **Extended Search Depth:** Successfully extends tool-use chains far beyond existing RL limitations, reaching up to 50 steps while maintaining coherence.
*   **Robust Generalization:** Demonstrates strong performance on out-of-distribution datasets, indicating high adaptability.
*   **Error Mitigation:** Effectively reduces error accumulation through the implementation of a self-reflection mechanism.

---

## Methodology

The WebSeer approach is built upon a reinforcement learning framework designed to enhance the depth and reliability of web search agents.

*   **Reinforcement Learning with Self-Reflection:** The core methodology integrates a self-reflection mechanism into the RL loop, allowing the agent to critique its own actions and adjust its trajectory.
*   **Dataset Construction:** A large-scale dataset was constructed specifically annotated with reflection patterns to train the model on how to think about its own search process.
*   **Two-Stage Training Framework:**
    1.  **Cold Start Initialization:** Utilizes Supervised Fine-Tuning (SFT) to establish a stable baseline policy.
    2.  **Reinforcement Learning:** Transitioning to RL specifically tailored for complex web environments.

---

## Technical Details

**Architecture & Design**
*   **Model Type:** Agentic Retrieval-Augmented Generation (RAG).
*   **Parameters:** 14B.
*   **Design Goal:** Open-domain, multi-hop question answering.

**Training Framework**
*   **Unified Approach:** Combines "cold start" initialization with advanced RL.
*   **Stage 1:** Supervised Fine-Tuning (SFT) via multi-turn rejection sampling.
*   **Stage 2:** Self-Reflective Reinforcement Learning (SRRL).

**Operational Workflow**
The system utilizes a sequential chain structure consisting of a continuous loop of:
1.  **Generation**
2.  **Tool Invocation**
3.  **Parameter Extraction**
4.  **Observation Integration**

**Toolkit Components**
*   Search Engine
*   Webpage Reader
*   Code Executor
*   Answer Submission tool

**Key Features**
*   **Self-Reflection:** Enables cross-verification of information and backtracking capabilities.
*   **Deep Exploration:** Capable of maintaining tool-use chains extending up to 50 steps.

---

## Results

WebSeer demonstrated superior performance compared to existing baselines, particularly in scenarios requiring deep reasoning and long-horizon planning.

| Benchmark | Metric | Score | Significance |
| :--- | :--- | :--- | :--- |
| **HotpotQA** | Accuracy | **72.3%** | State-of-the-Art performance |
| **SimpleQA** | Accuracy | **90.0%** | State-of-the-Art performance |
| **Search Depth** | Max Steps | **50** | Significant improvement over shallow baselines (e.g., Search-r1) |
| **Generalization** | OOD Data | **Strong** | Effective mitigation of error accumulation |

---

## Contributions

1.  **Novel Agent Architecture:** Introduction of the WebSeer architecture specifically designed to address shallow tool usage and error propagation.
2.  **Dataset Creation:** Release of a large-scale dataset annotated with reflection patterns to support future research.
3.  **Unified Training Framework:** Development of a two-stage framework bridging the gap between cold-start supervision and reinforcement learning.
4.  **Benchmark Leadership:** Achievement of state-of-the-art performance on complex QA tasks using a parameter-efficient 14B model.

---
*Analysis based on 40 citations.*