# A Variance-Reduced Cubic-Regularized Newton for Policy Optimization

*Cheng Sun; Zhen Zhang; Shaofu Yang*

***

## ðŸ“Š Quick Facts

| **Metric** | **Detail** |
| :--- | :--- |
| **Quality Score** | 9/10 |
| **Total Citations** | 40 |
| **Sample Complexity** | $\tilde{\mathcal{O}}(\epsilon^{-3})$ |
| **Algorithm** | VR-CR-PN (Variance-Reduced Cubic-Regularized Policy Newton) |
| **Key Innovation** | First integration of Hessian-aided variance reduction with second-order policy optimization |
| **Test Environment** | CartPole-v1 (50k sample budget) |

***

## Executive Summary

Reinforcement learning (RL) agents optimizing policies with non-convex objective functions frequently encounter saddle points, which often lead to suboptimal convergence. While second-order optimization methods are theoretically superior for escaping these saddle points, their practical application has been hindered by prohibitive sample complexity and high variance. Existing approaches typically rely on importance sampling to mitigate distribution shiftâ€”a method that introduces impractical assumptionsâ€”or exhibit sample complexities that scale unfavorably with the length of the time horizon. Addressing these inefficiencies is critical for developing RL algorithms that are both sample-efficient and robust in complex, long-horizon decision-making tasks.

The authors introduce **VR-CR-PN** (Variance-Reduced Cubic-Regularized Policy Newton), a novel algorithm that integrates cubic regularization within a policy Newton framework to facilitate variance reduction. The core technical advancement is a Hessian-aided variance reduction mechanism that employs a periodic two-stage strategy: an outer loop computes a large-batch baseline, while an inner loop applies control-variates corrections. A distinct innovation is the development of a novel Hessian estimator that maintains a uniform upper bound independent of the time horizon. This estimator allows VR-CR-PN to optimize the truncated expected return by solving cubic-regularization subproblems, effectively decoupling the algorithm's performance from trajectory length without relying on unrealistic importance sampling.

Theoretically, VR-CR-PN achieves a sample complexity of $\tilde{\mathcal{O}}(\epsilon^{-3})$ to reach an $\epsilon$-second-order stationary point, improving the dependency on $\epsilon$ from the previous state-of-the-art bound of $\tilde{\mathcal{O}}(\epsilon^{-3.5})$. Empirical evaluations on the CartPole-v1 environment validate these theoretical gains; within a constrained sample budget of 50,000 interactions, VR-CR-PN maintained a consistent advantage in average cumulative return compared to standard Cubic-Regularized Policy Newton (CR-PN), outperforming the baseline by margins of 0.25 to 0.5. Additionally, the algorithm demonstrated significantly reduced outcome variability with narrower confidence bands, indicating superior stability and faster convergence compared to baseline methods.

This research establishes a new theoretical benchmark for second-order optimization in reinforcement learning as the first algorithm to successfully integrate Hessian-aided variance reduction directly with second-order policy optimization. By successfully resolving the distribution shift problem without reliance on importance sampling, the method provides a more robust and practically applicable framework for non-convex RL. The achievement of horizon-independent sample complexity suggests that VR-CR-PN can scale effectively to long-horizon tasks without a linear increase in data requirements, paving the way for the adoption of advanced second-order methods in high-dimensional, real-world control systems.

***

## Key Findings

*   **Improved Sample Complexity:** The proposed VR-CR-PN algorithm achieves a sample complexity of **$\tilde{\mathcal{O}}(\epsilon^{-3})$** to reach an $\epsilon$-second-order stationary point. This significantly improves upon the previous best result of $\tilde{\mathcal{O}}(\epsilon^{-3.5})$.
*   **Horizon-Independent Performance:** Through a novel Hessian estimator, the algorithm achieves performance that maintains a uniform upper bound independent of the time horizon.
*   **Robustness to Distribution Shift:** The approach addresses the distribution shift problem effectively without relying on potentially unrealistic importance sampling assumptions.
*   **Algorithmic First:** This is the first algorithm to successfully integrate Hessian-aided variance reduction directly with second-order policy optimization frameworks.

***

## Methodology

The paper proposes **VR-CR-PN** (Variance-Reduced Cubic-Regularized Policy Newton), a second-order optimization approach designed specifically for reinforcement learning. The methodology centers on three main pillars:

1.  **Integration of Frameworks:** It combines cubic regularization with a policy Newton framework.
2.  **Hessian-Aided Variance Reduction:** A variance reduction mechanism is incorporated that utilizes Hessian information to stabilize updates.
3.  **Novel Hessian Estimator:** A new estimator for the expected return function is designed to maintain a uniform upper bound independent of the time horizon. This allows the algorithm to operate under general nonconvex conditions.

This methodology allows VR-CR-PN to optimize the truncated expected return without incurring the high costs typically associated with long-horizon dependency.

***

## Technical Details

VR-CR-PN is a second-order policy optimization method for non-convex reinforcement learning. It modifies the standard Cubic-Regularized Newton method by adding a variance reduction mechanism.

**Optimization Subproblem**
The algorithm optimizes the truncated expected return by solving the following cubic-regularization subproblem at step $t$:

$$h_t \in \text{argmin}_h \{ g_t^\top h + \frac{1}{2} h^\top H_t h + \frac{M}{6} \|h\|^3 \}$$

**Two-Stage Estimation Strategy**
VR-CR-PN employs a periodic two-stage strategy for gradient estimation to reduce variance:
*   **Outer Loop:** Computes a large-batch baseline.
*   **Inner Loop:** Applies a Hessian-aided correction using control variates.

**Key Characteristics**
*   **Hessian Estimator:** Designed to achieve horizon-independent sample complexity.
*   **Hyperparameters:**
    *   Cubic regularization: $M=30L_3$
    *   Inner loop length: $S$
    *   Scaled batch sizes: $b_g, b_H, B_g$

***

## Core Contributions

### 1. Algorithmic Innovation
Introduction of VR-CR-PN as the first algorithm to merge Hessian-aided variance reduction with second-order policy optimization frameworks.

### 2. Theoretical Advancement
Establishment of a tighter sample complexity bound ($\tilde{\mathcal{O}}(\epsilon^{-3})$) for finding second-order stationary points compared to existing methods ($\tilde{\mathcal{O}}(\epsilon^{-3.5})$).

### 3. Technical Estimator
Development of a novel Hessian estimator with a uniform upper bound that decouples sample complexity from the horizon length.

### 4. Practical Robustness
Theoretical validation of a second-order approach that overcomes the 'distribution shift' problem without unrealistic importance sampling assumptions.

***

## Results

### Theoretical Results
*   **Convergence:** VR-CR-PN converges to an $\epsilon$-second-order stationary point with a sample complexity of **$\tilde{\mathcal{O}}(\epsilon^{-3})$**.
*   **Improvement:** This represents an improvement of $\tilde{\mathcal{O}}(\epsilon^{-0.5})$ over the previous state-of-the-art ($\tilde{\mathcal{O}}(\epsilon^{-3.5})$).
*   **Samples:** Total gradient and Hessian samples are both $\tilde{\mathcal{O}}(\epsilon^{-3})$.

### Empirical Results (CartPole-v1)
*   **Sample Budget:** 50,000 samples.
*   **Performance Advantage:** VR-CR-PN maintained a performance advantage of **0.25 to 0.5** over standard CR-PN.
*   **Stability:** Demonstrated significantly reduced outcome variability with narrower confidence bands, indicating more stable and faster convergence.