# A Mathematical Philosophy of Explanations in Mechanistic Interpretability -- The Strange Science Part I.i
*Kola Ayonrinde; Louis Jaburi*

---

> ### ðŸ“Š Quick Facts
> *   **Quality Score:** 9/10
> *   **Total Citations:** 40
> *   **Research Type:** Theoretical / Mathematical Philosophy
> *   **Core Paradigm:** Strange Science
> *   **Key Principle:** Explanatory Optimism

---

## Executive Summary

This paper addresses a fundamental theoretical vacuum in the field of Mechanistic Interpretability (MI): the lack of a rigorous epistemological framework defining what constitutes a valid explanation of neural network behavior. The authors identify the "**Strange Paradox**", a situation where researchers possess precise knowledge of a model's implementation (weights and code) yet lack a corresponding understanding of its high-level behavior. Without a formal definition of "explanation," the field risks producing unscientific or unfalsifiable narratives.

The authors argue that establishing a mathematical philosophy is essential to justify why MI is possible and to delineate its boundaries as a scientific discipline. The key innovation is the formalization of the "**Explanatory View Hypothesis**" (EVH), which posits that neural networks inherently possess extractable, implicit explanations. A strict taxonomy is introduced defining valid MI explanations as **Model-level**, **Ontic** (referencing real internal states), **Causal-Mechanistic**, and **Falsifiable**.

Technically, the paper adopts a "Strange Science" paradigm, applying natural science methods to formal mathematical objects. It utilizes **Marrâ€™s Levels of Analysis** to focus on the Algorithmic and Computational levels, defining structure through the lens of compressibility. This framework treats models as "proto-explainers" containing runnable algorithmic mechanisms that must be aligned with human-understandable concepts. 

Theoretical contributions include the "**Principle of Explanatory Optimism**", conjecturing that specific optimism is a necessary precondition for MI success, and the principle of "**Generalization as Structure**", which deduces that generalization capability implies the existence of underlying internal structure. Ultimately, this work supplies the necessary philosophical "bedrock" for MI, transitioning it from a collection of disparate techniques into a cohesive, mathematically grounded science.

---

## Key Findings

*   **The Explanatory View Hypothesis (EVH):** Suggests that Mechanistic Interpretability is valid primarily because neural networks inherently possess extractable 'implicit explanations'.
*   **Rigorous Definitions:** "Explanatory Faithfulness" is mathematically defined as the measure of how accurately an explanation represents a model.
*   **Strategic Conjecture:** The "Principle of Explanatory Optimism" is formulated as a necessary precondition for the field of MI to succeed; specific optimism is required to drive research forward.

---

## Methodology

The paper employs a **theoretical and mathematical philosophy framework** rather than empirical experimentation. 

*   **Approach:** Utilizes conceptual argumentation to support the existence of explanations within neural architectures.
*   **Taxonomy Creation:** Creates a formal taxonomy with specific attributes to define the practice of Mechanistic Interpretability.
*   **Core Attributes:** Model-level, Ontic, Causal-Mechanistic, and Falsifiable.

---

## Technical Details

The paper proposes a specific set of definitions and frameworks to ground the science of interpretability:

*   **Paradigm:** "**Strange Science**" â€” Interpretability viewed as a natural science applied to formal objects.
*   **The Paradox:** The "**Strange Paradox**" characterizes the state of having precise implementation knowledge (code/weights) but lacking behavioral understanding.
*   **Framework:** Adopts **Marr's Levels of Analysis**, specifically targeting the **Algorithmic** and **Computational Levels** over the Implementation level.
*   **Definitions:**
    *   **Representations:** Defined via correspondence with input features.
    *   **Structure:** Defined by the compressibility of the generating process (fewer bits than raw observations).
*   **The "Explanatory View":** Treats models as '**proto-explainers**' with implicit mechanisms, requiring explanations to be 'runnable' at the algorithmic level.

---

## Results & Metrics

As a theoretical paper, it establishes qualitative metrics and conjectures rather than empirical benchmarks.

*   **Defined Metrics:**
    *   **Explanatory Faithfulness:** An algorithmic-level property requiring internal algorithm alignment and specific locatability.
    *   **Compression:** Understanding is quantified as the ability to express a generating process in fewer bits than raw observations.
*   **Theoretical Results:**
    *   **Explanatory View Hypothesis:** Neural networks possess extractable implicit explanations.
    *   **Principle of Explanatory Optimism:** Optimism is necessary for MI success.
    *   **Generalization as Structure:** Generalization implies the formation of internal structure.

---

## Contributions

*   **Formal Definition of MI:** Establishes that valid MI explanations must be Model-level, Ontic, Causal-Mechanistic, and Falsifiable.
*   **Field Demarcation:** Provides a limitation analysis that distinguishes MI from other interpretability paradigms and outlines the field's inherent limits.
*   **Theoretical Foundation:** Introduces the Explanatory View Hypothesis and the Principle of Explanatory Optimism to justify methodology and set preconditions for future research.