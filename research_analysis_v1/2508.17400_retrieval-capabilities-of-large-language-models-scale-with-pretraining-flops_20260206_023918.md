---
title: Retrieval Capabilities of Large Language Models Scale with Pretraining FLOPs
arxiv_id: '2508.17400'
source_url: https://arxiv.org/abs/2508.17400
generated_at: '2026-02-06T02:39:18'
quality_score: 8
citation_count: 27
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Retrieval Capabilities of Large Language Models Scale with Pretraining FLOPs

*Jacob Portes; Connor Jennings; Erica Ji Yuen; Sasha Doubov; Michael Carbin*

---

### ðŸ“Š Quick Facts

| Metric | Value |
| :--- | :--- |
| **Quality Score** | 8/10 |
| **Model Scale** | 125M to 7B parameters |
| **Training Data** | 1B to >2T tokens |
| **Key Benchmark** | Zero-shot BEIR (nDCG@10) |
| **References** | 27 citations |

---

## Executive Summary

> **Overview:** This research addresses the critical question of whether Large Language Models (LLMs) can effectively serve as general-purpose retrievers, a role traditionally dominated by specialized encoder architectures like BERT. As LLMs increasingly power retrieval-augmented generation (RAG) systems, it is essential to determine if their retrieval capabilities are emergent, unpredictable properties or if they follow predictable scaling laws.
>
> **Key Innovation:** The key innovation of this study is the empirical validation that retrieval performance in decoder-only LLMs adheres to predictable scaling laws governed by pretraining FLOPs (Floating Point Operations). The researchers demonstrate this by converting Mosaic Pretrained Transformer (MPT) decoder-only models into embedding models via average pooling of final hidden representations and evaluating them on zero-shot BEIR tasks.
>
> **Implications:** A significant technical contribution is the identification of In-Context Learning (ICL) as a robust proxy metric for retrieval capability; the study establishes a strong correlation between ICL scores and retrieval performance, allowing for the estimation of retrieval quality without running full retrieval benchmarks. These findings have significant implications for the future development of LLM-based retrievers, shifting the focus from designing specialized architectures to optimizing compute investment.

---

## Key Findings

*   **Predictable Scaling Laws:** Retrieval performance on zero-shot BEIR tasks scales predictably and positively with increases in LLM size, training duration, and estimated pretraining FLOPs.
*   **Correlation with In-Context Learning:** There is a strong correlation between In-Context Learning (ICL) scores and retrieval scores across various retrieval tasks.
*   **Compute Determines Capability:** The study confirms that the retrieval capabilities of LLMs are not emergent or random but are directly tied to the computational investment (FLOPs) during the pretraining phase.
*   **Generalization across Scales:** The observed scaling trends hold true across a vast range of model sizes (from 125M to 7B parameters) and training data volumes (from 1 billion to >2 trillion tokens).

---

## Methodology

The researchers utilized a benchmarking methodology to evaluate retrieval capabilities across a wide spectrum of model scales and training regimes.

1.  **Scope of Analysis:** Analyzed models ranging from 125 million to 7 billion parameters.
2.  **Training Variants:** Examined models pretrained on datasets varying from 1 billion tokens to over 2 trillion tokens.
3.  **Evaluation Metrics:** Assessed performance using zero-shot BEIR tasks.
4.  **Correlation Analysis:** Calculated and correlated performance metrics against estimated pretraining FLOPs.
5.  **ICL Investigation:** Investigated the relationship between retrieval performance and In-Context Learning (ICL) scores.

---

## Technical Details

**Model Architecture & Data**
*   **Models:** Utilized MPT (Mosaic Pretrained Transformer) decoder-only models.
*   **Scale:** Ranged from 125M to 7B parameters.
*   **Training Volume:** Pretrained on 1 billion to >2 trillion tokens.
*   **Ratios:** Token-to-parameter ratios spanning 20 to 500.

**Approach & Configuration**
*   **Conversion:** Converts decoder-style LLMs into embedding models via average pooling of final hidden representations.
*   **Intermediate Stage:** Omitted intermediate pretraining.
*   **Fine-tuning Dataset:** MS MARCO dataset (500,000 query-passage pairs for 1 epoch).
*   **Loss Function:** InfoNCE Loss with cosine similarity.
*   **Negatives:** 15 BM25-mined hard negatives.
*   **Hyperparameters:** Fixed hyperparameters without sweeps.

---

## Results

*   **Scaling Consistency:** Retrieval performance on zero-shot BEIR tasks (average nDCG@10) scales predictably with increased model size, training duration, and estimated pretraining FLOPs.
*   **Correlation Validation:** A strong correlation is observed between In-Context Learning (ICL) scores and retrieval scores.
*   **Performance Improvements:** Increasing model size and training tokens consistently improves BEIR scores.
*   **Architecture Comparison:** 7B parameter decoders trained on 2T tokens outperform BERT-style architectures on MTEB.
*   **Optimization Insight:** Higher token-to-parameter ratios may be more optimal for retrieval than Chinchilla-optimal ratios.

---

## Contributions

*   **Empirical Evidence for Retrieval Scaling:** Provides concrete evidence that retrieval performance adheres to scaling laws governed by compute (FLOPs), similar to other NLP capabilities.
*   **Proxy Metrics Development:** Identifies In-Context Learning as a strong correlate to retrieval performance, offering a potential proxy metric for evaluating the retrieval capabilities of new models without running full retrieval benchmarks.
*   **Guidance for Retriever Development:** Informs the future development of LLM-based retrievers by establishing that increased computational investment in pretraining reliably yields better retrieval performance.

---

### References
*27 citations*