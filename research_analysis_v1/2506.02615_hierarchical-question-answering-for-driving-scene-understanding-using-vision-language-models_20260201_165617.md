# Hierarchical Question-Answering for Driving Scene Understanding Using Vision-Language Models

*Safaa Abdullahi Moallim Mohamud; Minjin Baek; Dong Seog Han*

---

> ### ðŸ“Š Quick Facts
>
> *   **Model Architecture:** Compact BLIP VLM
> *   **Parameters:** 384.7 Million
> *   **Environment:** ROS 2, NVIDIA RTX 4090, PyTorch
> *   **Training:** 15 Epochs, Learning Rate 1e-6
> *   **Total Accuracy:** **94.81%** (vs. 72.10% baseline)
> *   **Evaluation Metric:** Lingo-Judge (Threshold $\ge$ 85%)
> *   **Inference:** Lower latency than GPT-4o; real-time viable

---

## Executive Summary

Autonomous driving systems require precise, high-level scene understanding to navigate complex environments safely, necessitating a balance between deep visual interpretation and computational efficiency. While state-of-the-art large Vision-Language Models (VLMs) like GPT-4o offer exceptional descriptive capabilities, they incur prohibitive inference costs and latency, making them unsuitable for real-time, edge-deployed applications.

This paper addresses this challenge by proposing a **Hierarchical Question-Answering (QA) framework** built around a compact BLIP VLM containing 384.7 million parameters. The core technical innovation is a tree-based navigation system that decomposes scene understanding into structured high-level and specific sub-questions. Instead of a brute-force approach, the system employs a **dynamic inference mechanism** that selectively skips irrelevant branches of the question tree based on real-time context, minimizing computational overhead. Furthermore, the framework utilizes handcrafted templates for final synthesis, avoiding resource-intensive generative decoding.

The proposed system achieved a total accuracy of **94.81%** using the Lingo-Judge metric, significantly outperforming the pre-trained baseline of 72.10% (a 22.71% improvement). This performance rivals GPT-4o while ensuring real-time operation. This research challenges the assumption that massive models are strictly necessary for high-performance autonomous driving tasks, establishing a new paradigm for cost-efficient computer vision in intelligent transportation systems.

---

## Key Findings

*   **Competitiveness with SOTA:** The system achieves performance comparable to **GPT-4o** in capturing key scene details, despite using a significantly smaller model architecture.
*   **Significant Efficiency Gains:** The framework delivers lower inference times than large proprietary models while balancing cost-efficiency with detailed visual interpretation.
*   **Real-Time Viability:** Qualitative results confirm the system's ability to identify essential driving elements with minimal latency, making it suitable for deployment in moving vehicles.
*   **Effective Grounding:** Fine-tuning a compact VLM on a geographically specific custom dataset effectively captures local driving-related visual elements that generic models might miss.

---

## Methodology

The paper proposes a hierarchical question-answering (QA) framework utilizing a compact Vision-Language Model (VLM). The methodology relies on four core steps:

1.  **Fine-Tuning:** A compact VLM is fine-tuned on a custom dataset specific to the vehicle's operational area to enhance local visual grounding.
2.  **Hierarchical Decomposition:** The system breaks down the scene understanding task into high-level and detailed sub-questions organized in a structured tree format.
3.  **Inference Optimization:** The system navigates the question tree dynamically, skipping irrelevant branches to minimize computational overhead and processing time.
4.  **Synthesis:** Extracted answers are converted into coherent descriptions using handcrafted templates rather than generative decoding, saving resources.

---

## Core Contributions

*   **Hierarchical QA Strategy:** Introduction of a tree-based navigation system that decomposes scene understanding into logical stages, reducing complexity.
*   **Cost-Efficiency without Quality Loss:** Demonstration that compact VLMs can rival the performance of massive proprietary models while drastically reducing inference time.
*   **Dynamic Inference Mechanism:** Development of a logic-based skipping mechanism that optimizes computational resources by avoiding unnecessary processing of sub-questions.

---

## Technical Details

**System Architecture & Environment**
*   **Logic:** Dynamic question selection (tree-based navigation) vs. brute-force processing.
*   **Platform:** ROS 2 (Robot Operating System).
*   **Hardware:** NVIDIA RTX 4090 GPU.
*   **Framework:** PyTorch.

**Model Specifications**
*   **Base Architecture:** Compact BLIP Vision-Language Model (VLM).
*   **Trainable Parameters:** 384.7 million.

**Training Protocol**
*   **Dataset:** Geographically specific custom dataset.
*   **Learning Rate:** 1e-6.
*   **Epochs:** 15.
*   **Batch Size:** 16.

**Evaluation Metrics**
*   **Metric:** Lingo-Judge.
*   **Threshold:** Similarity score $\ge$ 85%.

---

## Performance Results

The fine-tuned VLM demonstrated substantial improvements over the pre-trained baseline across all categories.

### Accuracy Comparison

| Category | Fine-Tuned VLM | Pre-trained Baseline | Change |
| :--- | :---: | :---: | :---: |
| **Total Accuracy** | **94.81%** | 72.10% | **+22.71%** |
| None | 98.85% | 79.53% | +19.32% |
| No | 95.65% | 59.71% | +35.94% |
| Yes | 67.10% | 52.63% | +14.47% |
| Other | 53.33% | 26.66% | +26.67% |

### Inference Analysis
*   **Latency:** The Hierarchical QA system delivers lower latency than statically processing all 41 questions.
*   **Comparison:** Confirmed faster processing speeds compared to large models like GPT-4o, ensuring real-time viability on the specified hardware.

---

**Quality Score:** 9/10  
**References:** 27 citations