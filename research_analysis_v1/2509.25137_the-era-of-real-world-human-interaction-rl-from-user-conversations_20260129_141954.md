# The Era of Real-World Human Interaction: RL from User Conversations

*Chuanyang Jin; Jing Xu; Bo Liu; Leitian Tao; Olga Golovneva; Tianmin Shu; Wenting Zhao; Xian Li; Jason Weston*

---

> ### ðŸ“Š Quick Facts & Metrics
>
> *   **Core Dataset:** WildChat (reconstructed as *WildLlamaChat*)
> *   **Architecture:** RLHI (Reinforcement Learning from Human Interaction)
> *   **Top Performance:** 77.9% length-controlled win rate (AlpacaEval 2.0)
> *   **User Feedback Signal:** 26.51% of messages are re-attempts (rising to 83.15% after Turn 5)
> *   **Contextual Diversity:** 0.865 (Superior to HelpSteer2 and HH-RLHF)
> *   **Base Model:** Llama-3.1-8B-Instruct

---

## Executive Summary

Current Reinforcement Learning from Human Feedback (RLHF) methodologies predominantly rely on static, expert-annotated datasets to align large language models (LLMs). This approach presents a scalability bottleneck and fails to capture the dynamic, personalized nuances inherent in real-world deployment.

This paper introduces **Reinforcement Learning from Human Interaction (RLHI)**, a novel framework designed to extract learning signals directly from natural user conversations (specifically the WildChat dataset). RLHI utilizes a dual-method architecture unified under **Persona-conditioned Direct Preference Optimization (Persona-DPO)**:

1.  **RLHI with User-Guided Rewrites:** Constructs preference pairs by comparing original unsatisfactory responses against user-revised "re-attempts."
2.  **RLHI with User-Based Rewards:** Employs a reward model conditioned on long-term user interaction history (personas) to rank candidate responses.

To ensure data integrity, the authors reconstructed the dataset as "WildLlamaChat" to avoid training on model-generated outputs (like GPT). Experimental analysis demonstrates that natural user conversation data is both abundant and effective, with RLHI variants outperforming strong baselines in personalization and instruction-following. This research represents a paradigm shift from expensive, static expert annotation toward scalable, organic learning from dynamic user interactions.

---

## Key Findings

*   **Superiority in Personalization:** Both variants of RLHI outperformed strong baselines on the WildChat evaluation, specifically in personalization and instruction-following tasks.
*   **Cross-Domain Performance:** Feedback derived from user conversations was shown to enhance performance on reasoning benchmarks, demonstrating generalizability.
*   **Efficacy of Organic Data:** Natural human interaction collected from the wild offers scalable and effective supervision, reducing the need for expensive expert annotation.
*   **Persona-Driven Alignment:** Linking long-term user interaction histories to immediate preferences facilitates multifaceted alignment, allowing models to adapt to specific user styles.

---

## Methodology

The authors propose Reinforcement Learning from Human Interaction (RLHI), a paradigm designed to learn directly from in-the-wild user conversations. The approach employs two complementary methods unified via **Persona-conditioned Preference Optimization**:

*   **RLHI with User-Guided Rewrites:** This method targets "re-attempts with feedback." It constructs preference pairs by revising unsatisfactory outputs based on user follow-ups, subject to quality filtering.
*   **RLHI with User-Based Rewards:** This method samples N candidates conditioned on the user persona and scores them using a reward model that is conditioned on long-term interaction history to form preference pairs.

---

## Technical Details

**Core Concept**
RLHI trains language models using in-the-wild user conversations and long-term user history (personas) rather than static expert prompts.

**Dual-Method Architecture**
1.  **Rewrites:** Targets specific "re-attempts with feedback" to construct preference pairs (rewrite vs. original).
2.  **Rewards:** Samples candidates conditioned on user persona; scored by a persona-conditioned reward model.

**Training & Data Processing**
*   **Algorithm:** Persona-conditioned Direct Preference Optimization (Persona-DPO).
*   **Data Reconstruction:** *WildLlamaChat* was created by regenerating assistant responses using Llama-3.1-8B-Instruct to eliminate reliance on GPT outputs.
*   **Reasoning Synthesis:** Reasoning data was synthesized using PRM800K, where simulated users identify errors in MATH solutions.

**Experimental Parameters**
*   **Sampling:** T=0.6, top-p=0.9
*   **Candidate Generation:** 64 candidates sampled using Athene-RM-8B
*   **Training Data:** Utilized 10,000 erroneous solutions from PRM800K

---

## Results

*   **Engagement Analysis:** In the WildChat-1M dataset, **26.51%** of user messages were identified as re-attempts with feedback. This rate rises significantly to **83.15%** after turn 5, indicating high user engagement in refining outputs.
*   **Diversity Metrics:** WildChat demonstrated higher contextual diversity (**0.865**) compared to HelpSteer2 (0.848) and HH-RLHF (0.751).
*   **Benchmark Performance:**
    *   The **RLHI with User-Based Rewards** variant achieved a **77.9%** length-controlled win rate on AlpacaEval 2.0.
    *   Both RLHI variants outperformed baselines in personalization and instruction-following on the WildChat User Eval.
*   **Reasoning Capabilities:** Integration of reasoning data (PRM800K) successfully enhanced performance on mathematical reasoning benchmarks.

---

## Contributions

*   **Paradigm Shift:** Establishes RLHI as a fundamental shift from static expert-annotated feedback toward learning from dynamic natural interaction.
*   **Novel Framework:** Provides a technical framework for integrating user rewrites and interaction history into the alignment process (Persona-DPO).
*   **Demonstrated Scalability:** Shows that high-quality personalized alignment can be achieved without expensive expert annotation, validating the utility of "wild" data.

---

**Paper Quality Score:** 8/10
**References:** 35 citations