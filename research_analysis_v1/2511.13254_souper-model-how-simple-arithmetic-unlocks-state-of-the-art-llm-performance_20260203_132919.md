---
title: 'Souper-Model: How Simple Arithmetic Unlocks State-of-the-Art LLM Performance'
arxiv_id: '2511.13254'
source_url: https://arxiv.org/abs/2511.13254
generated_at: '2026-02-03T13:29:19'
quality_score: 8
citation_count: 37
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Souper-Model: How Simple Arithmetic Unlocks State-of-the-Art LLM Performance

*Shalini Maiti; Amar Budhiraja; Bhavul Gauri; Gaurav Chaurasia; Anton Protopopov; Alexis Audran-Reiss; Michael Slater; Despoina Magka; Tatiana Shavrina; Roberta Raileanu; Yoram Bachrach*

***

> ### **Quick Facts**
>
> *   **Methodology**: Soup Of Category Experts (SoCE)
> *   **Key Innovation**: Non-uniform weighted averaging vs. standard uniform averaging
> *   **Top Result**: **80.68%** accuracy on Berkeley Function Calling Leaderboard (70B models)
> *   **Resource Efficiency**: Achieves SOTA performance without expensive retraining
> *   **Core Insight**: Benchmark categories exhibit low inter-correlation, allowing for distinct 'expert' models

***

## Executive Summary

Large Language Models (LLMs) require robust performance across a diverse range of capabilities, including multilingual understanding, tool calling, and mathematical reasoning. However, improving these capabilities typically necessitates expensive retraining or fine-tuning cycles. While "model souping"—the post-training technique of averaging weights from multiple models—offers a potential low-cost alternative, existing methods rely heavily on uniform averaging. This approach often fails to account for the specialized strengths of individual models and can lead to performance regression in specific tasks.

This paper addresses the need for a resource-efficient method to maximize performance and robustness across distinct domains without the prohibitive computational cost of retraining. The authors introduce the **Soup Of Category Experts (SoCE)**, a principled framework that advances model souping by shifting from uniform averaging to optimized, non-uniform weighted averaging. SoCE utilizes benchmark composition analysis, specifically employing Pearson correlation metrics, to identify clusters of tasks that exhibit low performance inter-correlation. This analysis reveals that different models often possess "expert" capabilities in distinct, often anti-correlated domains. By leveraging cooperative game theory, the framework optimizes the selection and weighting of these expert models, merging them in a way that prioritizes specific strengths rather than diluting them through a general average.

Empirical evaluation demonstrates that SoCE achieves state-of-the-art results on the Berkeley Function Calling Leaderboard (BFCL). For 70B parameter models, SoCE attained 80.68% accuracy, surpassing the previous best of 78.56%, while 8B models reached 76.50% compared to the prior 72.37%. On the MGSM mathematical reasoning benchmark (7B class), SoCE achieved 51.7% accuracy, outperforming the best individual model (50.9%) and correcting the performance regression caused by uniform souping. Ablation studies confirmed the efficacy of the approach, showing that weight optimization provided relative improvements of +2.28% for 70B models and +3.44% for 8B models.

The significance of this research lies in demonstrating that state-of-the-art LLM performance can be unlocked through arithmetic optimization rather than additional training data or compute. By validating that benchmark categories often exhibit low inter-correlation—identifying areas of both high redundancy and distinct expertise (as low as 0.07 correlation)—SoCE provides a generalizable, cost-effective strategy for model composition.

***

## Key Findings

*   **Performance Boost**: The proposed Soup Of Category Experts (SoCE) method improves performance and robustness across multiple domains, specifically enhancing multilingual capabilities, tool calling, and mathematical reasoning.
*   **State-of-the-Art Results**: Achieves top rankings on the Berkeley Function Calling Leaderboard, setting new benchmarks for the industry.
*   **Cost Efficiency**: Performance can be significantly maximized without expensive retraining by applying non-uniform weighted averaging to model weights.
*   **Low Inter-Correlation**: Benchmark categories generally exhibit low inter-correlations in model performance. This allows specific models to act as 'experts' for distinct category clusters rather than relying on a generalized average.

***

## Methodology

The authors propose **Soup Of Category Experts (SoCE)**, a principled framework for model souping that utilizes benchmark composition analysis to identify optimal model candidates.

*   **Beyond Uniform Averaging**: Unlike previous methods relying on uniform averaging, this approach leverages the observation that different benchmark categories show low performance inter-correlations.
*   **Expert Identification**: It identifies specific 'expert' models for these weakly-correlated clusters.
*   **Optimized Combination**: These expert models are combined using optimized, non-uniform weighted averaging to maximize specific strengths.

***

## Technical Details

The SoCE approach utilizes model souping—a post-training method averaging weights of models with the same architecture—combined with non-uniform weighted averaging.

| Component | Description |
| :--- | :--- |
| **Core Mechanism** | Model souping with non-uniform weighted averaging. |
| **Analysis Tool** | Employs **Pearson correlation analysis** to identify 'expert' models for distinct competency domains. |
| **Selection Strategy** | Selects models based on anti-correlated category clusters to minimize redundancy. |
| **Optimization Theory** | Utilizes **cooperative game theory** and correlation metrics to optimize selection and weighting. |
| **Comparison** | Improves upon standard uniform and greedy souping strategies. |

***

## Results

The method was evaluated on BFCL, MGSM, infinity-Bench, and FLORES-101.

### Berkeley Function Calling Leaderboard (BFCL)
*   **70B Models**: Achieved **80.68%** accuracy (vs. 78.56% previous best).
*   **8B Models**: Achieved **76.50%** accuracy (vs. 72.37% previous best).

### MGSM (Mathematical Reasoning)
*   **7B Models**: Achieved **51.7%** accuracy, improving upon the best individual model (50.9%).
*   **Note**: Uniform souping caused regression in this benchmark, whereas SoCE provided gains.

### Ablation Studies
*   Weight optimization provided relative improvements of **+2.28%** for 70B models and **+3.44%** for 8B models.

### Correlation Analysis
*   Identified **high redundancy** in some areas (0.96-0.98 correlation).
*   Identified **distinct expertise** in others (0.07 correlation), validating the need for expert selection.

***

## Contributions

1.  **Novel Technique**: Introduction of SoCE, a new model souping technique that uses benchmark composition to guide the selection and averaging of model weights.
2.  **Advanced Averaging**: Advancement of averaging techniques by shifting from standard uniform averaging to optimized, non-uniform weighted averaging based on specific model strengths.
3.  **Resource Efficiency**: Demonstration of efficiency as a pre- and post-training strategy that boosts performance and robustness without resource-intensive retraining.
4.  **Validation**: Benchmark validation through empirical demonstration of achieving state-of-the-art performance on the Berkeley Function Calling Leaderboard.

***
**Quality Score:** 8/10 | **References:** 37 citations