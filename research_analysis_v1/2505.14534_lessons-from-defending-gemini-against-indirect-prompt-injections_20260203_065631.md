---
title: Lessons from Defending Gemini Against Indirect Prompt Injections
arxiv_id: '2505.14534'
source_url: https://arxiv.org/abs/2505.14534
generated_at: '2026-02-03T06:56:31'
quality_score: 4
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Lessons from Defending Gemini Against Indirect Prompt Injections

*Chongyang Shi; Sharon Lin; Shuang Song; Jamie Hayes; Ilia Shumailov; Itay Yona; Juliette Pluto; Aneesh Pappu; Christopher A. Choquette-Choo; Milad Nasr; Chawin Sitawarin; Gena Gibson; Andreas Terzis; John "Four" Flynn*

***

> ### üìä Quick Facts & Metrics
>
> *   **Quality Score:** 4/10
> *   **Total Citations:** 40
> *   **Target Model:** Gemini 2.5
> *   **Key Metrics Monitored:**
>     *   Attack Success Rate (ASR)
>     *   Data Exfiltration Rate
>     *   False Positive / False Negative Rates
> *   **Core Focus:** Indirect Prompt Injection (IPI) & Tool-Use Security

***

## üìë Executive Summary

This paper addresses the security vulnerabilities introduced by integrating Large Language Models (LLMs) with external tools and untrusted data sources, specifically focusing on **Indirect Prompt Injection (IPI)**. As models like Gemini adopt function-calling capabilities to interact with the world‚Äîprocessing emails, web pages, or documents‚Äîthe boundary between data and instructions blurs. IPI attacks exploit this by embedding malicious directives within retrieved content, causing the model to execute unauthorized actions, such as data exfiltration, while the user believes the system is performing a benign task. This is a distinct threat from direct jailbreaking, as the malicious payload enters the system context via a trusted tool pathway, bypassing standard safety alignments that typically filter direct user inputs.

The authors present a continuous, adaptive adversarial evaluation framework designed to simulate sophisticated white-box attackers. Unlike static evaluations, this framework utilizes automated red-teaming agents that iteratively probe the model across different iterations (past, current, and future) to identify persistent weaknesses. Technically, the research implements a **defense-in-depth architecture** combining input/output filters with adversarial fine-tuning. The core innovation involves training the model‚Äîspecifically Gemini 2.5‚Äîto internally recognize and reject IPI attacks found within tool outputs. This approach hardens the model's reasoning capabilities, allowing it to distinguish between legitimate data content and malicious instructions even when the adversary is aware of the underlying defense logic.

Evaluations utilizing specific metrics‚ÄîAttack Success Rate (ASR), Data Exfiltration Rate, and False Positive/Negative Rates‚Äîdemonstrated that standard tool-use integrations are highly vulnerable to adaptive adversaries. The study found that baseline non-adaptive defenses, while effective against static threats, were frequently bypassed by attackers who understood the system's logic, leading to high ASR. However, the implementation of adversarial fine-tuning yielded measurable improvements; the fine-tuned model achieved a quantifiable reduction in both ASR and Data Exfiltration Rates compared to baseline models. Furthermore, the defense successfully lowered False Negative rates (missing an attack) without significantly inflating False Positive rates (rejecting safe inputs), thereby maintaining the model's operational utility.

The findings establish that security for tool-augmented LLMs cannot be a one-time fix but requires an iterative, cross-generational testing lifecycle. The authors argue that continuous evaluation is essential for identifying vulnerabilities that persist across model versions. By operationalizing this defense-in-depth strategy, the paper provides a methodological blueprint for the industry on how to safeguard agentic systems against IPI. However, the research also highlights inherent limitations, including the computational cost of continuous red-teaming and the reality that adaptive defense is an ongoing arms race; resilience is best achieved by integrating adversarial training directly into the model development lifecycle rather than relying solely on external filters.

***

## üîë Key Findings

*   **Vulnerability in Tool-Use:** The integration of function-calling and tool-use capabilities in LLMs introduces significant security risks, particularly when tools require access to untrusted data sources.
*   **Indirect Prompt Injection Efficacy:** Adversaries can successfully embed malicious instructions within untrusted data, causing models to deviate from user expectations.
*   **Impact of Continuous Evaluation:** Deploying a continuous, adaptive adversarial evaluation framework contributes to increasing the resilience of models like Gemini against manipulation.
*   **Cross-Generational Robustness:** Continuous testing must span past, current, and future model versions to effectively identify and mitigate vulnerabilities.

***

## üõ† Methodology

The authors utilized an **adversarial evaluation framework** designed to simulate a sophisticated adversary. This framework employs a suite of **adaptive attack techniques** that run continuously against various iterations of the Gemini models (past, current, and future). This approach allows for the ongoing assessment of the model's adversarial robustness in real-world scenarios involving tool-use and untrusted data access.

***

## üß† Technical Details

The research proposes a defense-in-depth architecture for LLMs with tool-use capabilities, focusing on Indirect Prompt Injection (IPI) from untrusted sources aimed at data exfiltration.

### System Architecture
*   **Focus:** Defense-in-depth architecture for LLMs utilizing tool-use.
*   **Threat Vector:** Indirect Prompt Injection (IPI) leading to data exfiltration.

### Evaluation & Training
*   **Automated Red-Teaming:** Utilizes a specialized dataset of prompt injections.
*   **Adversarial Fine-Tuning:** Specifically applied to Gemini 2.5 to improve internal recognition of attacks.
*   **Layered Defense:** The final approach layers adversarial training with other defense mechanisms.

### Defense Strategies Tested
1.  **Non-adaptive Baseline Defenses:** Standard filtering mechanisms.
2.  **Adaptive Attack Evaluation:** Testing against adversaries who know the defense logic.
3.  **Adversarial Fine-Tuning:** Training the model to internally recognize and reject malicious instructions.

***

## üìà Results

The study confirms that standard tool-use integration poses significant security risks, allowing adversaries to embed malicious instructions successfully.

*   **Baseline Failure:** Defenses effective against non-adaptive attacks often fail against adaptive attacks.
*   **Efficacy of Fine-Tuning:** Adversarial fine-tuning significantly improved resilience, with Gemini 2.5 demonstrating a better understanding of indirect prompt injections and mitigating attacks that bypassed standard defenses.
*   **Operational Utility:** The study highlights the necessity of continuous testing across model generations.
*   **Metrics:** Success was measured via **Attack Success Rate (ASR)**, **Data Exfiltration Rate**, and **False Positive/Negative Rates**.

***

## üèÜ Contributions

*   **Evaluation Framework:** The paper details Google DeepMind‚Äôs specific approach and institutional framework for evaluating the adversarial robustness of large language models equipped with function-calling capabilities.
*   **Security Insights:** It provides an analysis of how indirect prompt injections operate within the context of tool-use, highlighting risks associated with untrusted data inputs.
*   **Best Practices:** The report shares "lessons learned" from the defense of Gemini, offering the broader community insights into how continuous, adaptive testing can be operationalized to improve model safety and resilience.