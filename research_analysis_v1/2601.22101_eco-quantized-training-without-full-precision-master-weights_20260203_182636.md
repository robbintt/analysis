---
title: 'ECO: Quantized Training without Full-Precision Master Weights'
arxiv_id: '2601.22101'
source_url: https://arxiv.org/abs/2601.22101
generated_at: '2026-02-03T18:26:36'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# ECO: Quantized Training without Full-Precision Master Weights

*Mahdi Nikdan; Amir Zandieh; Dan Alistarh; Vahab Mirrokni*

---

### üìä Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Model Range** | 30M to 16B Parameters |
| **Architectures** | Transformers, Gemma, DeepSeek-MoE |
| **Precision Support** | FP8, INT4 |
| **Memory Efficiency** | ~25% reduction (12 ‚Üí 9 bytes/param) |
| **Quality Score** | 9/10 |

---

## üìã Executive Summary

Standard quantized training techniques typically require maintaining a full-precision (FP32) "master weight" copy of the model parameters alongside the quantized weights used for computation. This dual-weight requirement significantly increases static memory overhead, creating a bottleneck for training large-scale architectures, particularly Sparse Mixture of Experts (SMoE) models. As model sizes grow into the billions of parameters, this memory tax limits the ability to train on available hardware, making the elimination of master weights a critical challenge for enabling efficient large-scale model training.

The authors introduce **ECO (Error-Compensating Optimizer)**, the first optimizer designed to perform quantized training without retaining full-precision master weights. Technically, ECO applies gradient updates directly to the quantized parameters rather than a high-precision buffer. To compensate for the precision loss that typically causes divergence in this scenario, ECO utilizes an error-feedback loop integrated into the optimizer's momentum. After each update, the quantization error‚Äîthe difference between the updated weight and its quantized state‚Äîis calculated and injected back into the momentum term. This mechanism leverages the observation that consecutive quantization errors are strongly aligned, effectively correcting for drift without requiring additional memory storage.

ECO was validated across models ranging from 30M to 16B parameters, including Gemma and DeepSeek-MoE architectures, using FP8 and INT4 formats. In pre-training a 2.1B SMoE model, ECO achieved near-lossless accuracy in FP8 (validation loss 2.7615 vs. 2.7554 baseline) and successfully enabled convergence in INT4 where naive removal of master weights diverged. The method reduced peak memory usage by approximately 25% (from 12 to 9 bytes per parameter). In fine-tuning DeepSeek-MoE-16B, ECO matched the training loss curves of master-weight baselines and achieved comparable zero-shot accuracy (MMLU: 38.41, HellaSwag: 78.88, ARC-C: 49.15).

This research establishes a theoretical guarantee that master-weight-free training can converge to a constant-radius neighborhood of the optimum, addressing a critical gap in low-memory training theory. By shifting the Pareto frontier for static memory versus validation loss, ECO enables the training of significantly larger models on existing hardware. This advancement is particularly significant for the future of Sparse MoE models, where memory efficiency is paramount, offering a pathway to reduce costs and resource barriers for state-of-the-art model development.

---

## üîç Key Findings

*   **Elimination of Master Weights:** ECO removes the requirement for high-precision master weight buffers during quantized training, significantly reducing memory overhead‚Äîparticularly beneficial for Sparse Mixture of Experts (SMoE) models.
*   **Robust Convergence:** Theoretical analysis demonstrates that ECO converges to a constant-radius neighborhood of the optimum, whereas naive removal of master weights leads to errors inversely proportional to the learning rate.
*   **Broad Applicability and Scale:** The method was validated across a wide range of model sizes (30M to 16B parameters), architectures (Transformers, Gemma, DeepSeek-MoE), and precision formats (FP8, INT4).
*   **Pareto Frontier Improvement:** ECO achieves near-lossless accuracy comparable to baselines using master weights while significantly shifting the Pareto frontier for static memory versus validation loss.

---

## ‚öôÔ∏è Methodology

The **Error-Compensating Optimizer (ECO)** functions by applying gradient updates directly to quantized parameters rather than maintaining a separate high-precision master copy.

The core mechanism involves an **error-feedback loop**:
1.  Weights are updated based on gradients.
2.  The weights are immediately quantized.
3.  The resulting quantization error is calculated.
4.  This error is injected into the optimizer's momentum.

This process compensates for the precision loss without introducing additional memory overhead for storing full-precision weights.

---

## üöÄ Contributions

*   **Algorithm Innovation:** Introduction of ECO, the first optimizer capable of quantized training without full-precision master weights by utilizing an error-feedback mechanism integrated into the optimizer momentum.
*   **Theoretical Guarantees:** A rigorous proof that ECO maintains convergence stability (to a constant-radius neighborhood) under standard assumptions and decaying learning rates, addressing a critical theoretical gap in low-memory training.
*   **Empirical Validation:** Comprehensive evidence that master-weight-free training can match the performance of standard baselines across both pretraining and fine-tuning tasks for large-scale models, including dense Transformers and Sparse MoE architectures.

---

## üîß Technical Details

*   **Memory Optimization:** Eliminates high-precision master weight buffers via an error-feedback loop using the optimizer's momentum.
*   **Convergence Properties:** Theoretically converges to a constant-radius neighborhood of the optimum; naive removal leads to errors inversely proportional to the learning rate.
*   **Quantization Support:** Supports tensor-wise weight-only quantization for **FP8** and **INT4** formats.
*   **Rounding Modes:** Compatible with Round-to-Nearest (RTN) and Stochastic Rounding (SR).
*   **Error Alignment:** Leverages the observation that consecutive quantization errors exhibit strong alignment (high cosine similarity, relative norm near 1).

---

## üìà Experimental Results

**Pre-Training (SMoE 2.1B)**
*   **FP8 Performance:** Achieved near-lossless recovery (Val Loss: **2.7615** vs 2.7554 baseline).
*   **INT4 Performance:** Enabled convergence where naive removal diverged.
*   **Memory:** Reduced peak memory usage by ~25% (from 12 to 9 bytes/parameter).

**Fine-Tuning (DeepSeek-MoE-16B)**
ECO matched training loss curves and achieved comparable or better zero-shot accuracy:

| Benchmark | ECO Score | Baseline (Master Weights) |
| :--- | :--- | :--- |
| **MMLU** | 38.41 | Comparable |
| **HellaSwag** | 78.88 | Comparable |
| **ARC-C** | 49.15 | Comparable |

**Gemma 3 1B**
*   Achieved validation loss comparable to master weight baselines.

---
* **Quality Score:** 9/10
* **References:** 40 citations