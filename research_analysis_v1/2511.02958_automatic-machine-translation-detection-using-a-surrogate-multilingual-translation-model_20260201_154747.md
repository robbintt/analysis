# Automatic Machine Translation Detection Using a Surrogate Multilingual Translation Model

*Authors: Cristian GarcÃ­a-Romero; Miquel EsplÃ -Gomis; Felipe SÃ¡nchez-MartÃ­nez*

> ### ðŸ“Š Quick Facts
> * **Quality Score:** 9/10
> * **Best Accuracy:** 72.92% (Aggregated Dev)
> * **SOTA Improvement:** >5 percentage points
> * **Key Model:** NLLB 3.3B (10th Decoder Block)
> * **Primary Dataset:** WMT (de-en, ru-en, en-de, en-ru, de-es, es-de)

---

## Executive Summary

The proliferation of machine-generated content on the internet presents a critical data integrity challenge for training Machine Translation (MT) systems. As web-scraped corpora increasingly contain synthetic translations rather than human-authored text, MT models risk training on machine-generated output. This reliance on "synthetic noise" creates a feedback loop that degrades the quality and robustness of subsequent translation models.

This paper addresses the urgent need for effective filtering mechanisms to distinguish between Human Translation (HT) and Machine Translation (MT) to preserve the fidelity of training data. The authors propose **SMaTD** (Surrogate Multilingual Translation Detection), a novel framework that leverages the internal representations of a pre-trained multilingual MT model rather than relying on external linguistic features.

The approach frames the detection task as binary classification, utilizing a frozen NLLB encoder-decoder model as a feature extractor. Technically, the system processes source sentences via the encoder and target sentences via the decoder using teacher forcing. It extracts hidden states from a specific decoder block (specifically the 10th layer in the best-performing configuration) and projects them through a Transformer classifier utilizing BERT-style first-token pooling. An advanced variant, **SMaTD+LM**, further refines classification by prepending the `[CLS]` token representation from a frozen, fine-tuned Language Model to the surrogate features.

In experiments conducted on WMT data across language pairs including de-en, ru-en, and de-es, the proposed method significantly outperformed existing state-of-the-art techniques. The NLLB 3.3B model configuration achieved an aggregate accuracy of **72.92%** on development sets, surpassing baseline embedding layer approaches which yielded only 58.70%.

This research offers a crucial solution for data curation, enabling developers to clean parallel corpora of synthetic noise and prevent quality degradation in MT models. By exploiting the internal mechanics of a multilingual surrogate model, the work closes the performance gap in detection capabilities for non-English language pairs, ensuring that future systems train on authentic human language.

---

## Key Findings

*   **Data Integrity Crisis**: Training data collected from the Internet for MT systems contains substantial machine-generated translations rather than human-authored text. Overreliance on this synthetic content degrades the quality of resulting translation models.
*   **Superior Performance**: The proposed detection method outperforms current state-of-the-art techniques with **accuracy improvements of at least 5 percentage points**.
*   **Cross-Lingual Efficacy**: The approach shows particularly strong results for **non-English language pairs**, effectively closing the performance gap in translation detection tasks across diverse languages.
*   **Perplexity Observation**: Results indicate that machine-translated text exhibits **lower per-word perplexity** than human-translated text, providing a useful linguistic characteristic for detection.

---

## Methodology

The authors employ a **surrogate multilingual MT model** as the foundation for their detection system. Unlike traditional methods that rely on external linguistic features, this method directly exploits the internal representations (hidden states or embeddings) of the surrogate model.

The system processes source and target sentences through an encoder and a teacher-forcing decoder, respectively. These internal representations are then analyzed to classify and distinguish between sentences translated by humans and those generated by machines.

---

## Technical Details

The **SMaTD** approach frames HT vs. MT detection as a binary classification problem using a frozen NLLB encoder-decoder model as a surrogate feature extractor.

**Architecture & Processing:**
*   **Input Processing**: Source sentences are processed via the encoder, while target sentences are processed via the decoder using teacher forcing.
*   **Feature Extraction**: Hidden states are extracted from a specific decoder block.
*   **Classification**: The extracted states are projected and processed by a **Transformer classifier** with absolute positional embeddings. It utilizes a BERT-style first token output for final prediction.

**Variant Architecture (SMaTD+LM):**
*   Integrates a frozen, fine-tuned Language Model by prepending its `[CLS]` token representation to the surrogate features for final classification.

---

## Results

Experiments were conducted on WMT data involving pairs including de-en, ru-en, en-de, en-ru, de-es, and es-de.

**Performance Comparison:**

| Configuration | Layer Used | Accuracy (Aggregated Dev) |
| :--- | :--- | :--- |
| **NLLB 3.3B (Best)** | 10th Decoder Block | **72.92%** |
| Baseline | Embedding Layer | 58.70% |

*   The NLLB 3.3B model utilizing the 10th decoder block achieved the best performance.
*   This configuration significantly outperformed the simple embedding layer approach by a large margin.
*   The proposed method established a new benchmark, outperforming current SOTA by at least 5 percentage points.

---

## Contributions

*   **Novel Detection Framework**: Introduction of a new framework for automatic machine translation detection that leverages the internal mechanics of a multilingual MT model.
*   **Data Filtering Solution**: Provision of a practical solution to remove non-human translations from training sets, directly addressing the issue of 'synthetic noise' in parallel corpora.
*   **Advancement of Cross-Lingual Efficacy**: Significant improvement in handling non-English language pairs, closing the performance gap in translation detection tasks across diverse languages.

> **Total Citations:** 19