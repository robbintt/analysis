---
title: Test-Time Alignment of LLMs via Sampling-Based Optimal Control in pre-logit
  space
arxiv_id: '2510.26219'
source_url: https://arxiv.org/abs/2510.26219
generated_at: '2026-01-27T16:32:23'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# Test-Time Alignment of LLMs via Sampling-Based Optimal Control in pre-logit space

*Hiroshi Takahashi, The University, Toyohashi University, Though Bo, Sekitoshi Kanai, Tsukasa Yoshida, Haru Kuroki, Kazumune Hashimoto*

---

> ### **QUICK FACTS**
>
> *   **Method:** Adaptive Importance Sampling on Pre-logits (**AISP**)
> *   **Core Technique:** Sampling-based Model Predictive Control (MPPI)
> *   **Training Required:** None (Zero parameter updates)
> *   **Key Innovation:** Optimization in pre-logit space vs. discrete token space
> *   **Dataset Reduction:** Eliminates the need for 349,000 prompts (required by RE-Control)
> *   **Performance:** **27.02%** Win Rate (AlpacaEval 2 LC) vs. 24.36% (BoN) and 22.97% (RE-Control)

---

## EXECUTIVE SUMMARY

**Problem**
Aligning Large Language Models (LLMs) to follow instructions and behave safely has traditionally relied on Reinforcement Learning from Human Feedback (RLHF), a process that is computationally expensive and requires extensive fine-tuning of model parameters. While test-time alignment methods offer an alternative by modifying behavior during inference, existing solutions suffer from critical inefficiencies. Methods like Best-of-N require generating numerous complete responses to find a satisfactory output, and recent control-theoretic approaches like RE-Control necessitate training a separate value function on massive datasets (>349,000 prompts). The field currently lacks alignment strategies that are both computationally efficient at inference time and devoid of the overhead associated with auxiliary model training.

**Innovation**
The authors introduce **Adaptive Importance Sampling on Pre-logits (AISP)**, a training-free method grounded in sampling-based optimal control (Model Predictive Control). Unlike prior approaches that operate in the discrete token space or rely on learned value networks, AISP operates directly in the continuous **pre-logit space ($z_t$)**. The technique applies stochastic Gaussian perturbations to the pre-logits to minimize a cost function comprising negative expected rewards and a KL-divergence term (minimizing Free Energy). By leveraging Adaptive Importance Sampling, AISP calculates the optimal mean trajectory for these perturbations, effectively steering the model's hidden states to maximize output quality without updating network weights.

**Results**
AISP demonstrates superior performance both in terms of reward quality and sample efficiency compared to Best-of-N (BoN) sampling and RE-Control. It completely eliminates the dependency on large training datasets; whereas RE-Control requires a dataset of 349,000 prompts, AISP operates strictly at test-time. In head-to-head evaluations on **Llama-3-8B-Instruct** using **AlpacaEval 2 (Length Controlled)**:
*   **AISP:** 27.02% Win Rate
*   **Best-of-N:** 24.36% Win Rate
*   **RE-Control:** 22.97% Win Rate
Furthermore, AISP proves highly sample efficient, achieving a **25.78%** win rate even at reduced sampling costs.

**Impact**
This research significantly advances the paradigm of test-time alignment by successfully bridging control theory with LLM inference. By shifting the focus of optimization to the pre-logit space, AISP decouples alignment performance from the need for backpropagation or value function approximation. This reduces resource barriers for aligning massive models, potentially enabling dynamic, high-performance alignment in resource-constrained environments or for proprietary models where parameter access is restricted.

---

## KEY FINDINGS

*   **Superior Efficiency:** The proposed AISP method achieves higher overall rewards and better sample efficiency compared to best-of-n sampling and existing reward-based test-time alignment methods.
*   **Viable Control Mechanism:** Gaussian perturbations applied to pre-logits are an effective mechanism for controlling model behavior without altering weights.
*   **Optimal Mean Calculation:** The optimal mean for perturbation to maximize expected rewards can be effectively determined using importance sampling techniques.

---

## METHODOLOGY

The authors propose **Adaptive Importance Sampling on Pre-logits (AISP)**, a method grounded in sampling-based model predictive control with stochastic control inputs. The core operational shift is moving from the output token space to the pre-logit space.

*   **Pre-logit Operation:** AISP operates within the pre-logit space rather than manipulating final tokens, allowing for continuous optimization.
*   **Gaussian Perturbation:** The methodology involves applying Gaussian perturbation to the pre-logits to explore potential high-reward trajectories.
*   **Objective Function:** The process seeks to maximize expected rewards with respect to the perturbation mean.
*   **Optimization:** The optimal mean is calculated using an importance sampling technique, iteratively refining the model's hidden states.

---

## TECHNICAL DETAILS

*   **Algorithm:** AISP is a training-free test-time alignment method using sampling-based optimal control (MPPI).
*   **State Space:** Operates in the pre-logit space denoted as $z_t$.
*   **Perturbation:** Applies a stochastic Gaussian perturbation:
    $$v_t \sim \mathcal{N}(u_t, \sigma^2 I)$$
*   **Cost Function:** The objective minimizes the following cost function, linked to minimizing Free Energy:
    $$J(x, U) = -\mathbb{E}_{V \sim Q_{U,\sigma^2}}[r(x, y(V))] + \lambda D_{\text{KL}}(Q_{U,\sigma^2} | P)$$
*   **Optimization:** Uses Adaptive Importance Sampling to update the mean trajectory $U$ based on reward-weighted samples.
*   **Constraints:**
    *   No LLM parameter updates required.
    *   No value function training required.

---

## CONTRIBUTIONS

*   **Computational Barrier Removal:** Addresses the computational barrier of fine-tuning by introducing a high-performance test-time alignment method that requires **no parameter updates**.
*   **Cross-Disciplinary Integration:** Integrates concepts from control theory (specifically Model Predictive Control) with LLM inference to optimize token generation.
*   **Paradigm Shift:** Shifts the focus of alignment from the output token space to the **pre-logit space**, utilizing stochastic perturbations and importance sampling to steer the model.

---

## RESULTS

*   **Performance:** AISP achieves higher overall rewards and superior sample efficiency compared to Best-of-N (BoN) sampling and RE-Control.
*   **Dataset Elimination:** Eliminates the need for the training dataset of **349,000 prompts** required by RE-Control for value function training.
*   **Computational Cost:** Operates strictly at test-time, avoiding the heavy computational costs of standard RLHF fine-tuning and the storage/training overheads of value networks.

---

**Quality Score:** 8/10
**References:** 40 citations