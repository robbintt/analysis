---
title: 'Chatty-KG: A Multi-Agent AI System for On-Demand Conversational Question Answering
  over Knowledge Graphs'
arxiv_id: '2511.2094'
source_url: https://arxiv.org/abs/2511.20940
generated_at: '2026-02-03T06:52:53'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Chatty-KG: A Multi-Agent AI System for On-Demand Conversational Question Answering over Knowledge Graphs

*Reham Omar; Abdelghny Orogat; Ibrahim Abdelaziz; Omij Mangukiya; Panos Kalnis; Essam Mansour*

---

> ### ðŸ“Š Quick Facts
>
> *   **Performance Boost:** +23.1% F1 Score and +24.5% P@1 vs. state-of-the-art.
> *   **Latency Reduction:** Up to **50%** reduction in query translation latency compared to GraphRAG.
> *   **Architecture:** Training-free, Multi-Agent framework.
> *   **Compatibility:** LLM-agnostic (supports both commercial and open-weight models).
> *   **Quality Score:** 9/10
> *   **References:** 40 citations

---

## Executive Summary

Integrating Large Language Models (LLMs) with Knowledge Graphs (KGs) for conversational query answering faces critical bottlenecks regarding dialogue coherence and computational efficiency. Existing state-of-the-art systems, such as KGQAn, EDGQA, and GraphRAG, primarily rely on graph serializationâ€”converting structured graph data into linear text. This methodology degrades retrieval precision, inflates memory overhead, and introduces significant latency. Additionally, current architectures often fail to maintain context across multi-turn interactions and typically require task-specific fine-tuning. This dependency on extensive retraining renders adaptation to dynamic, private, or domain-specific knowledge bases costly and difficult, limiting practical enterprise deployment.

Chatty-KG addresses these limitations through a novel, **training-free multi-agent architecture** that employs a hybrid execution strategy to eliminate the need for graph serialization. Rather than flattening graph structures, the system utilizes a collaboration of task-specialized LLM agents to generate structured SPARQL queries on-demand, merging the semantic flexibility of Retrieval-Augmented Generation (RAG) with the precision of formal query languages. The framework delegates specific responsibilities to distinct agents for contextual interpretation, entity and relation linking, and query planning. By utilizing a hybrid prompting approachâ€”zero-shot for classification tasks and few-shot Chain-of-Thought reasoning for complex synthesisâ€”Chatty-KG operates as a modular, LLM-agnostic framework. This design functions with both commercial and open-weight models without fine-tuning, preserving the structural integrity of the underlying data.

Evaluations on the ConvQuestions dataset demonstrate that Chatty-KG delivers quantifiable performance gains over state-of-the-art baselines. The system improves F1 scores by **23.1%** and Precision at 1 (P@1) by **24.5%** compared to the best competing methods, achieving these results in both single-turn and multi-turn settings. Furthermore, the framework addresses the latency bottlenecks associated with serialization; compared to GraphRAG, Chatty-KG reduces query translation latency by up to **50%**, enabling faster response times without sacrificing accuracy. This performance validates the system's ability to resolve coreference issues and maintain coherence across extended dialogues, offering a measurable efficiency advantage over fine-tuned models.

This research establishes a scalable, cost-effective framework for reliable Question Answering over Knowledge Graphs (KGQA), significantly lowering the barriers to deployment for enterprise applications. By removing the dependency on expensive model fine-tuning and minimizing pre-processing requirements, Chatty-KG offers a robust solution for organizations managing private or rapidly evolving data sources. The system's ability to support high-precision, low-latency multi-turn dialogue marks a pivotal shift toward maintainable, knowledge-intensive AI architectures. This approach successfully bridges the gap between LLM generative capabilities and KG structural reliability, providing a practical path forward for industries requiring accurate, on-demand conversational access to complex structured data.

---

## Key Findings

*   **Superior Performance:** Significantly outperforms state-of-the-art baselines in both single-turn and multi-turn settings, achieving higher F1 and P@1 scores.
*   **Robust Compatibility:** Demonstrates strong compatibility with both commercial and open-weight LLMs without requiring fine-tuning.
*   **Modular Design:** Supports evolving Knowledge Graphs, allowing the system to adapt to data changes without retraining the model.
*   **Efficiency:** Achieves low-latency query translation while successfully maintaining dialogue coherence across multi-turn conversations.

---

## Methodology

The authors propose **Chatty-KG**, a multi-agent architecture composed of task-specialized LLM agents that collaborate to solve distinct sub-problems. The approach utilizes a hybrid execution strategy that combines Retrieval-Augmented Generation (RAG) with structured SPARQL query generation, explicitly avoiding the pitfalls of graph data serialization.

The system assigns specialized agents specific responsibilities, including:
*   Contextual interpretation
*   Entity and relation linking
*   Efficient query planning

---

## Technical Details

The system is structured as a multi-agent AI framework using a training-free paradigm that relies on intrinsic LLM capabilities.

*   **Paradigm:** Training-free; relies on intrinsic LLM capabilities without task-specific fine-tuning.
*   **Compatibility:** LLM-agnostic; supports both commercial and open-weight models.
*   **Adaptability:** Supports evolving Knowledge Graphs without retraining.
*   **Prompting Strategy:** Hybrid approach:
    *   *Zero-shot:* Used for simple agents (classification/routing).
    *   *Few-shot/Chain-of-Thought:* Used for complex agents (reasoning/synthesis).
*   **Pipeline Implementation:** Standard KGQA stages executed via agents:
    *   Dialogue Understanding
    *   Linking
    *   Answer Generation
*   **Differentiation:** Unlike GraphRAG, Chatty-KG utilizes **on-demand query translation** to preserve structural integrity and reduce memory overhead.

---

## Results

Chatty-KG achieves significantly higher F1 scores and superior Precision at 1 (P@1) compared to state-of-the-art baselines in both single-turn and multi-turn settings, while also achieving low-latency query translation.

**Comparative Analysis:**
*   **Multi-turn Support:** Successfully supports multi-turn dialogue (unlike KGQAn and EDGQA).
*   **Real-time Performance:** Offers real-time response times with no training requirement and modular/minimal pre-processing.
*   **Efficiency Gains:** Addresses high latency issues found in KGQAn and EDGQA, and mitigates the high memory consumption and low precision issues found in GraphRAG.

---

## Contributions

*   **Unified Framework:** Presents a unified framework bridging LLMs and KGs by combining LLM flexibility with KG reliability.
*   **Solving Limitations:** Addresses current limitations by avoiding structural serialization and overcoming single-turn, latency, and coreference issues found in existing systems.
*   **Scalability:** Establishes a scalable and extensible approach for reliable multi-turn Question Answering over private or domain-specific Knowledge Graphs.