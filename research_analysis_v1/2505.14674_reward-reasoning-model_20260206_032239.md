---
title: Reward Reasoning Model
arxiv_id: '2505.14674'
source_url: https://arxiv.org/abs/2505.14674
generated_at: '2026-02-06T03:22:39'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

***

# Reward Reasoning Model

*Jiaxin Guo; Zewen Chi; Li Dong; Qingxiu Dong; Xun Wu; Shaohan Huang; Furu Wei*

---

### ðŸ“Š Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Quality Score** | 9/10 |
| **References** | 40 Citations |
| **Base Architecture** | Qwen2 Transformer-decoder |
| **Training Method** | Reinforcement Learning (GRPO) |
| **Key Performance** | GPQA: +14.1 percentage points (26.8% â†’ 40.9%) |

---

## Executive Summary

> **The Challenge:** Current reward models, which are essential for aligning Large Language Models (LLMs), typically rely on direct prediction mechanisms that lack the nuance required for complex reasoning tasks. This limitation results in suboptimal performance when evaluating sophisticated queries. Furthermore, while increasing computational resources at inference time (test-time compute) offers a theoretical path to better performance, the challenge of effectively utilizing this additional compute to enhance reward accuracy remains unresolved.
>
> **The Solution:** The authors introduce **Reward Reasoning Models (RRMs)**, a framework that fundamentally shifts the paradigm from direct reward prediction to a reasoning-first approach. Built on a Qwen2 Transformer-decoder architecture, RRMs treat reward modeling as a text completion task where the model must first generate a Chain-of-Thought (CoT) reasoning trace before producing a final judgment in LaTeX format. The technical novelty lies in a reinforcement learning frameworkâ€”specifically utilizing Group Relative Policy Optimization (GRPO) and a rule-based reward functionâ€”that enables "self-evolved" reasoning.
>
> **The Impact:** RRMs demonstrate superior performance compared to existing standards across multiple benchmarks, including MMLU-Pro, MATH, and GPQA. In a rigorous validation of the model's utility, using RRM as a reward signal for reinforcement learning on unlabeled data yielded significant improvements, boosting GPQA performance from 26.8% to 40.9%â€”an increase of 14.1 percentage points. Additionally, the study confirmed that RRMs successfully leverage adaptive test-time compute, specifically enhancing reward accuracy on complex queries that require deeper analysis. This research establishes a significant milestone in AI alignment by solving the test-time compute challenge and eliminating the dependency on costly human-annotated reasoning data.

---

## Key Findings

*   **Superior Benchmark Performance:** Reward Reasoning Models (RRMs) outperform existing standards on reward modeling benchmarks across diverse domains.
*   **Adaptive Compute Utilization:** RRMs effectively leverage additional test-time compute to enhance reward accuracy, specifically by adaptively applying it to complex queries.
*   **Reasoning-Driven Accuracy:** By implementing a chain-of-thought reasoning process before generating final rewards, RRMs achieve higher accuracy in complex scenarios compared to direct reward prediction models.
*   **Self-Evolved Learning:** The model achieves high-level reasoning capabilities without explicit reasoning traces in training data, relying instead on a self-evolved learning process.

---

## Methodology

The researchers introduced Reward Reasoning Models (RRMs), a framework designed to execute a deliberate reasoning process prior to generating final rewards.

1.  **Chain-of-Thought (CoT) Integration:** The methodology utilizes CoT reasoning to allow the model to analyze complex queries deeply.
2.  **Test-Time Compute Structuring:** The model is structured to use additional computational resources at inference time for nuanced inputs.
3.  **Reinforcement Learning Framework:** The approach employs an RL framework that fosters 'self-evolved' reward reasoning capabilities without the need for explicitly labeled reasoning traces.

---

## Technical Details

| Component | Description |
| :--- | :--- |
| **Architecture** | **Qwen2 Transformer-decoder** |
| **Task Framing** | Reward modeling is treated as a text completion/reasoning task. |
| **Input/Output** | Accepts a single query and two candidate responses. Generates a chain-of-thought reasoning trace followed by a final judgment in **LaTeX format**. |
| **Training Base** | Deepseek-R1 distilled models. |
| **Learning Algorithm** | **Reward Reasoning via Reinforcement Learning** using **Group Relative Policy Optimization (GRPO)**. |
| **Reward Function** | Rule-based (+1 for correct, -1 for incorrect). |
| **Multi-Response Strategies** | Supports **ELO Rating**, **Knockout Tournament**, and **Majority Voting**. |

---

## Contributions

*   **Solving the Test-Time Compute Challenge:** Addressing the open challenge of effectively utilizing test-time compute to enhance reward model performance.
*   **Novel Architecture for Reward Modeling:** Introducing RRMs to shift the paradigm from direct reward prediction to a reasoning-first approach.
*   **Training Innovation:** Developing a reinforcement learning framework that enables the acquisition of reasoning skills without expensive human-annotated reasoning traces.
*   **Open Source Resources:** Contributing pretrained reward reasoning models made publicly available to the research community.

---

## Results

*   **Benchmark Dominance:** RRM outperforms previous reward models in average accuracy on the **MMLU-Pro**, **MATH**, and **GPQA** benchmarks.
*   **Significant Performance Lift:** When used as a reward signal for reinforcement learning on unlabeled data, RRM improved GPQA performance from **26.8% to 40.9%** (+14.1 percentage points).
*   **Adaptive Compute Success:** The model effectively leverages adaptive test-time compute to enhance reward accuracy specifically for complex queries.