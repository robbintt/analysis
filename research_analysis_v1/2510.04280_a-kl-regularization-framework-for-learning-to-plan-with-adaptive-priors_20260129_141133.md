# A KL-regularization framework for learning to plan with adaptive priors
*Ãlvaro Serra-Gomez, Daniel Jarne Ornia, Dhruva Tirumala, Thomas Moerland*

---

> ### ðŸ“Š Quick Facts
> | Metric | Detail |
> | :--- | :--- |
> | **Quality Score** | 8/10 |
> | **References** | 40 Citations |
> | **Framework** | PO-MPC (Policy Optimization-Model Predictive Control) |
> | **Planner** | Model Predictive Path Integral (MPPI) |
> | **Benchmarks** | DeepMind Control Suite (7 tasks), HumanoidBench (14 tasks) |
> | **Training Time** | 7â€“15 hours (1e6 time-steps on NVIDIA A100) |

---

## Executive Summary

Model-Based Reinforcement Learning (MBRL), particularly using Model Predictive Path Integral (MPPI) planners, faces a fundamental **distribution mismatch** between the planner's optimal action distribution and the learned sampling policy. In high-dimensional continuous control tasks, this misalignment creates a bottleneck where the learned policy fails to fully capitalize on the planner's guidance. This limitation reduces sample efficiency and hinders long-term performance, often leading to convergence issues or convergence to suboptimal local minima.

The authors introduce **Policy Optimization-Model Predictive Control (PO-MPC)**, a unified framework that formulates the learning problem as KL-regularized RL. The core technical innovation treats the MPPI planner's action distribution as an adaptive prior, maximizing expected returns while minimizing the KL divergence between the learned policy and the planner. To mitigate variance from stale data in online settings, the architecture introduces a learned "intermediate prior" to represent the planning policy within the KL-regularization objective.

This approach theoretically unifies the field, demonstrating that state-of-the-art methods like TD-MPC2 and BMPC are merely special cases (specifically, boundary conditions) of this generalized PO-MPC family. PO-MPC was evaluated on 21 high-dimensional continuous control tasks, achieving substantial gains in both sample efficiency and final performance compared to strong baselines.

---

## Key Findings

*   **Enhanced Alignment:** Aligning the learned sampling policy with the MPPI planner's distribution enhances accuracy and long-term performance.
*   **Flexible Prior Integration:** Integrating the planner's action distribution as a prior provides flexibility in trading off return maximization and KL divergence minimization.
*   **SOTA Performance:** Extended configurations yield significant performance improvements, advancing the state of the art in MPPI-based reinforcement learning.
*   **Theoretical Unification:** Previous approaches using learned policies as proposal distributions are identified as special cases of the proposed PO-MPC family.

---

## Methodology

The authors introduce **Policy Optimization-Model Predictive Control (PO-MPC)**, a unified framework for KL-regularized Model-Based Reinforcement Learning. The approach operates on the principle that the planning agent and the learned policy must remain synchronized.

1.  **Adaptive Priors:** The MPPI planner's action distribution is treated as an adaptive prior within the policy optimization process.
2.  **KL-Regularization:** The framework minimizes the Kullback-Leibler (KL) divergence between the learned policy and the planner's distribution. This aligns the agent's behavior with the planner's optimal guidance.
3.  **Objective Function:** The process simultaneously maximizes returns (rewards) while minimizing the divergence from the prior, ensuring a balance between exploitation of known rewards and exploration guided by the planner.

---

## Contributions

*   **Theoretical Unification:** Consolidates recent MPPI-based reinforcement learning methods into a single PO-MPC family, showing how disparate methods relate to one another.
*   **Methodological Generalization:** Clarifies the mathematical relationship between return maximization and planner-guided regularization, introducing new optimization variations.
*   **State-of-the-Art Advancement:** Provides empirical evidence that extended configurations of the PO-MPC framework outperform existing methods, setting new benchmarks for sample efficiency and performance.

---

## Technical Details

**Framework Architecture**
*   **Core Model:** PO-MPC utilizes a TD-MPC2 base World Model.
*   **Components:** The architecture consists of a latent state encoder, a Bootstrap Action Value Function, and a learned Sampling Policy.
*   **Planner:** It employs Model Predictive Path Integral (MPPI) Control, a sample-based stochastic optimal control method.

**Optimization Strategy**
*   **Intermediate Prior:** To prevent variance caused by stale data in online settings, the framework introduces a learned "intermediate prior" to represent the planning policy within the KL-regularization objective.
*   **Mathematical Formulation:** The objective function is designed to maximize returns while minimizing divergence from this prior.

**Theoretical Context**
*   **Problem Addressed:** Resolves distribution mismatch problems and prevents collapse to local minima.
*   **Special Cases:** Previous methods are unified under this framework:
    *   **TD-MPC2:** Corresponds to $\lambda = 0$.
    *   **BMPC:** Corresponds to $\lambda = \infty$.

---

## Results

**Experimental Setup**
*   **Benchmarks:** Experiments were performed on the **DeepMind Control Suite** (7 tasks, including Humanoid and Dog) and **HumanoidBench** (14 tasks).
*   **Domains:** High-dimensional continuous control.
*   **Hardware:** Training was completed on a single NVIDIA A100 GPU.

**Performance Metrics**
*   **Efficiency:** The system processed 1e6 time-steps within 7 to 15 hours.
*   **Outcomes:** The paper reports substantial gains in both sample efficiency and final performance compared to state-of-the-art baselines (TD-MPC2 and BMPC).
*   **Conclusion:** Extended configurations of PO-MPC were shown to advance the state of the art in MPPI-based RL.