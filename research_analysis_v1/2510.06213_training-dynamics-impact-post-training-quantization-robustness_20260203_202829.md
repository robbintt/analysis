---
title: Training Dynamics Impact Post-Training Quantization Robustness
arxiv_id: '2510.06213'
source_url: https://arxiv.org/abs/2510.06213
generated_at: '2026-02-03T20:28:29'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Training Dynamics Impact Post-Training Quantization Robustness

*Albert Catalan-Tatjer; Niccolò Ajroldi; Jonas Geiping*

---

### ⚡ Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Model Scale** | Up to 32 Billion Parameters |
| **Training Data** | Up to 15 Trillion Tokens |
| **Quantization** | GPTQ (3-bit & 4-bit), AWQ, BitsAndBytes |
| **Key Focus** | Learning Rate Schedules vs. Quantization Error |
| **Quality Score** | 9/10 |

---

## Executive Summary

Post-training quantization (PTQ) is critical for the efficient deployment of large language models (LLMs), yet models often suffer significant performance degradation when compressed to lower bit-widths, such as 3-bit or 4-bit precision. The field has struggled to identify the root causes of this loss in robustness, with prevailing assumptions suggesting that the sheer scale of training data might be a contributing factor.

This research addresses the urgent need to understand the mechanisms behind quantization errors, challenging the conventional wisdom that training on increasingly massive datasets inherently compromises a model's ability to be effectively quantized. The key innovation is the identification of a complex interplay between **learning rate schedules** and other training hyperparameters as the primary driver of quantization robustness, effectively decoupling this issue from dataset scale.

The authors employ a rigorous methodology combining large-scale analysis of open-source models (up to 32B parameters) with controlled experimentation. The study isolates the **"decay phase"** of training as the critical moment where validation loss ceases to be a reliable proxy for quantization performance. To mitigate this, the authors propose and validate specific interventions: optimizing learning rate schedules to maintain higher rates for longer durations and implementing Stochastic Weight Averaging (SWA).

Ultimately, these findings pave the way for more accessible LLM deployment, ensuring that future models can retain accuracy even under strict quantization constraints.

---

## Key Findings

*   **Hyperparameter Interplay:** Quantization errors in large-scale language models are driven by a complex interplay between the **learning rate** and other training hyperparameters, rather than model size alone.
*   **Divergence Phenomenon:** Once learning rates decay, **validation loss** and **quantization error** diverge; this phenomenon is largely independent of the scale of training data.
*   **Data Scale Independence:** Increasing dataset scale does **not** inherently compromise quantization effectiveness, directly challenging previous assumptions in the field.
*   **Strategic Interventions:** Strategic interventions on training hyperparameters can successfully modulate and improve quantization robustness at scale.

---

## Methodology

The research employs a multi-faceted approach to isolate causal links between training dynamics and quantization error:

*   **Large-Scale Analysis**
    Conducted a comprehensive analysis of quantization degradation across training trajectories of open-source language models. This study covered massive scales, analyzing models with up to **32 billion parameters** trained on **15 trillion tokens**.

*   **Controlled Experimentation**
    Trained proprietary models in controlled environments processing up to **100 billion tokens**. This allowed the researchers to test specific interventions on training dynamics without external variables.

*   **Correlative Assessment**
    Accurately assessed the relationship between training dynamics (specifically learning rate decay), validation loss, and quantization performance to move beyond correlation and identify causal links.

---

## Technical Details

### Scope and Models
*   **Model Families:** Analyzed six modern LLM families, specifically focusing on **SmolLM3**.
*   **Checkpoints:** Utilized hundreds of intermediate checkpoints to track degradation over time.
*   **Scale:** Up to 32 billion parameters and 15 trillion training tokens.

### Quantization Strategy
*   **Method:** Post-Training Quantization (PTQ).
*   **Algorithm:** Primary focus on **GPTQ**.
*   **Precision:** 3-bit and 4-bit precision.
*   **Technique:** Reconstruction error minimization with calibration data.
*   **Generalization:** The framework generalizes to **AWQ** and **BitsAndBytes (BNB)**.
*   **Optimizations:** Assumes standard optimizations like grouping and outlier processing.

### Training Dynamics
*   **Schedules:** Contrasted **Cosine Decay** with **Trapezoidal** (Warmup-Stable-Decay) schedules.
*   **Focus Area:** The transition between constant and annealing phases.

### Proposed Interventions
1.  **Optimized Learning Rate Schedules:** Maintaining larger learning rates for longer periods.
2.  **Weight Averaging:** Utilizing averaging techniques to smooth the trajectory.

---

## Results & Analysis

Experiments revealed a distinct **divergence phenomenon** where validation loss decreases while quantization error rises sharply during the learning rate decay phase.

*   **SmolLM3 Case Study:** In the SmolLM3 family, 3-bit quantization error spiked from approximately $3 \times 10^{-2}$ to $9 \times 10^{-2}$, while validation loss simultaneously dropped from ~3.0 to ~2.6.
*   **Learning Rate Impact:** Maintaining larger learning rates for longer durations was found to significantly reduce quantization error.
*   **Scale Independence:** The degradation of quantization robustness was determined to be independent of training data scale, debunking the assumption that larger token counts cause quantization sensitivity.
*   **Intervention Success:** Optimized LR schedules and weight averaging successfully achieved lower quantization error without sacrificing model performance.

---

## Research Contributions

*   **Clarified Mechanisms of Robustness:** Provided clarity on the mechanisms underlying quantization robustness, shifting the field's focus from data scale to training dynamics.
*   **Identification of Divergence:** Identified the critical point at which validation loss ceases to be a reliable proxy for quantization error (post-learning rate decay).
*   **Actionable Interventions:** Demonstrated that training hyperparameters can be strategically configured to favor quantization quality without necessarily sacrificing model performance or training scale.

---

**References:** 40 citations
**Quality Score:** 9/10