---
title: 'ViVa: Video-Trained Value Functions for Guiding Online RL from Diverse Data'
arxiv_id: '2503.1821'
source_url: https://arxiv.org/abs/2503.18210
generated_at: '2026-02-03T06:56:45'
quality_score: 9
citation_count: 34
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# ViVa: Video-Trained Value Functions for Guiding Online RL from Diverse Data

*Nitish Dashora; Dibya Ghosh; Sergey Levine*

---

> ### **Quick Facts**
>
> *   **Quality Score:** 9/10
> *   **References:** 34 citations
> *   **Performance:** Achieved ~80-90% success rate on Adroit Door task (vs. ~20-40% for baselines).
> *   **Key Datasets:** Ego4D, Epic Kitchens, BridgeData.
> *   **Architecture:** ResNet-50 CNN + MLP for intent-conditioned value functions.

---

## Executive Summary

### Problem
Online reinforcement learning (RL) in sparse-reward environments faces a critical bootstrapping problem: agents struggle to learn meaningful behaviors without dense feedback. While offline RL typically addresses this by relying on large datasets of expert demonstrations, such task-specific data is often scarce and expensive to acquire. Furthermore, manual reward shaping is labor-intensive and prone to reward hacking. This paper addresses the bottleneck of data scarcity by investigating whether the vast quantities of diverse, non-expert video data available on the internet—such as unstructured human interactions or even failure cases—can be utilized to guide online RL agents without requiring action labels or specific expert trajectories.

### Innovation
The authors introduce **ViVa (Video-trained Value Functions)**, a pre-training pipeline that learns a visual value function entirely from passive, unlabeled video data. Technically, the method trains an intent-conditioned value function, $V_\phi(s, g)$, using a shared ResNet-50 CNN architecture to process observations and goals, followed by an MLP to predict scalar values. The key innovation lies in the training objective: ViVa employs Temporal Difference (TD) learning with synthetic rewards based on state-goal distance, treating unlabeled video frames as states and subsequent frames as goals. This allows the model to learn general reachability and goal-conditioned values without action labels. During the online phase, these pre-trained parameters initialize the critic of a Soft Actor-Critic (SAC) agent, effectively providing an intrinsic learning signal derived from the pre-trained video understanding to guide the actor.

### Results
ViVa demonstrates significant performance improvements over prior state-of-the-art methods like R3M and MVP across Adroit and Franka Kitchen benchmarks. Specifically, on the Adroit Door task, ViVa achieved approximately 80-90% success rates, compared to roughly 20-40% for baseline methods. The approach exhibits log-linear scaling, where performance improves consistently as the dataset size increases, and successfully generalizes to goal positions unseen during training. Notably, the method showed positive transfer when pre-trained on massive human video datasets like Ego4D and Epic Kitchens, proving robust even when trained on data containing failures or off-task interactions.

### Impact
This research represents a significant step toward decoupling online RL guidance from the dependency on expert offline data. By establishing a framework to leverage diverse, passive video observations—including failures and undirected interactions—ViVa expands the utility of unstructured video datasets for robotic learning. The approach provides a general, automated alternative to domain-specific reward shaping, reducing the need for manual engineering of heuristics. Ultimately, this work suggests that the abundance of internet video can serve as a scalable prior for general goal-reaching behaviors, facilitating more efficient sample learning in complex, sparse-reward environments.

---

## Key Findings

*   **Efficacy of Diverse Data:** Video-trained value functions successfully guide online reinforcement learning (RL) using a wide variety of non-expert, diverse data sources, including Internet recordings, off-task demonstrations, and failure examples.
*   **Positive Transfer from Pre-training:** The method demonstrates significant positive transfer when pre-trained on human video data, improving online performance without the need for task-specific expert data.
*   **Generalization and Scaling:** The approach exhibits the ability to generalize to goals that were unseen during the training phase and scales effectively as the size of the dataset increases.

---

## Methodology

*   **Data-Driven Guidance:** Instead of relying on hand-engineered heuristics or scarce expert offline data, the authors utilize widely available passive video data.
*   **Intent-Conditioned Value Functions:** The core technical mechanism involves learning a model of optimal goal-conditioned value using "intent-conditioned value functions" trained on diverse videos.
*   **Reward Integration:** These learned goal-conditioned values are incorporated directly into the reward signal to guide the online RL agent, effectively providing a fine-grained learning signal that nudges the policy toward the optimal solution.

---

## Technical Details

**Pre-training Pipeline**
ViVa proposes a pipeline that learns a visual value function $V_\phi(s, g)$ entirely from offline, diverse video data without action labels to initialize the critic in an online reinforcement learning algorithm (SAC).

**Architecture**
*   **Visual Backbone:** Utilizes a shared ResNet-50 CNN to process both observations and goals.
*   **Prediction Head:** Concatenated embeddings are passed through an MLP to predict a scalar value.

**Training Methodology**
*   **Objective:** Employs Temporal Difference (TD) learning with synthetic rewards based on state-goal distance.
*   **Data Handling:** Treats unlabeled video frames as states and subsequent frames as goals to learn reachability. This approach naturally handles failures and off-task data.
*   **Datasets:** Trained on massive datasets including Ego4D, Epic Kitchens, and BridgeData.

**Online Fine-tuning**
*   **Initialization:** Pre-trained parameters initialize the SAC Critic.
*   **Policy Training:** The Actor is trained from scratch during the online phase.

---

## Contributions

*   **Removing Expert Data Dependency:** The paper presents a methodology that decouples online RL guidance from the requirement of expert offline data with reward signals, addressing the bottleneck of data scarcity in sparse-reward environments.
*   **Leveraging Passive Observations:** It establishes a framework for utilizing diverse, passive video data (such as undirected interaction or failures) to model general goal-reaching behaviors, expanding the utility of unstructured video datasets.
*   **Automated Reward Shaping:** The approach offers a general, inexpensive, and automated alternative to domain-specific reward shaping, reducing the need for manual engineering of heuristics.

---

## Results

**Evaluation Metrics**
Evaluated on Adroit and Franka Kitchen benchmarks using metrics such as:
*   Success Rate
*   Sample Efficiency (area under the success rate curve)
*   Average Return

**Performance Highlights**
*   **vs. State-of-the-Art:** ViVa significantly outperforms prior methods like R3M and MVP.
*   **Adroit Door Task:** Achieved ~80-90% success rate compared to ~20-40% for baselines.
*   **Scalability:** Demonstrated log-linear scaling of performance with data size.
*   **Robustness:** Showed robustness to failure data and zero-shot generalization to unseen goal positions.
*   **Transfer Learning:** Key results include positive transfer from human video datasets (Ego4D).