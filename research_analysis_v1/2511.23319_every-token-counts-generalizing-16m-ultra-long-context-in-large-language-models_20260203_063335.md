---
title: 'Every Token Counts: Generalizing 16M Ultra-Long Context in Large Language
  Models'
arxiv_id: '2511.23319'
source_url: https://arxiv.org/abs/2511.23319
generated_at: '2026-02-03T06:33:35'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Every Token Counts: Generalizing 16M Ultra-Long Context in Large Language Models
*Xiang Hu; Zhanchao Zhou; Ruiqi Liang; Zehuan Li; Wei Wu; Jianguo Li*

***

### üìä Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Model Name** | HSA-UltraLong |
| **Parameters** | 8 Billion (Mixture-of-Experts) |
| **Training Scale** | >8 Trillion Tokens |
| **Max Context** | 16 Million Tokens |
| **Retrieval Accuracy** | >90% @ 16M Context |
| **Complexity** | Linear (via Hierarchical Sparse Attention) |
| **Quality Score** | 9/10 |

***

> ## üìù Executive Summary
>
> Current Large Language Models (LLMs) face a fundamental computational bottleneck due to the quadratic complexity of standard attention mechanisms, restricting practical context windows to thousands of tokens. While extending context length is critical for applications requiring massive data ingestion‚Äîsuch as analyzing entire codebases or long-form documents‚Äîexisting methods struggle to balance efficiency with retrieval accuracy. The challenge lies in designing an architecture that maintains high performance on in-context retrieval tasks while scaling to millions of tokens without prohibitive computational costs.
>
> The researchers introduce **Hierarchical Sparse Attention (HSA)**, a novel architecture designed to achieve linear complexity while preserving the ability to perform random access within ultra-long sequences. HSA operates by fusing Sliding-Window Attention (SWA) for local context processing with a global retrieval mechanism. The text is divided into fixed-length chunks ($S=64$), where a learnable query vector selects the top-$K$ relevant chunks for inter-chunk fusion. A critical technical innovation is the hybrid positional encoding strategy: utilizing Rotary Positional Embeddings (RoPE) for the local SWA pathway to capture fine-grained order, and "NoPE" (No Positional Encoding) for the global HSA pathway to enhance length extrapolation. This architecture is implemented in **HSA-UltraLong**, an 8-billion parameter Mixture-of-Experts (MoE) model.
>
> HSA-UltraLong demonstrates remarkable generalization capabilities, trained on over 8 trillion tokens with a pre-training context of only 8K and mid-training of 32K, yet successfully inferring on contexts up to **16 million tokens**. Empirical evaluations show near-perfect accuracy (~100%) on the Single-Needle-In-A-Haystack (S-NIAH) test at 16M context length and greater than 90% accuracy on most complex in-context retrieval tasks at the same scale. The model performs comparably to full-attention baselines up to 32K tokens. Ablation studies underscore the importance of the NoPE strategy; baselines using RoPE for global retrieval, such as NSA, suffered catastrophic failures in accuracy (dropping to 4.0% on MQ-NIAH at 64K), validating the authors' design choices.
>
> This research significantly advances the state of the art in efficient long-context modeling by demonstrating that sparse attention mechanisms can effectively generalize to lengths hundreds of times larger than their training data. By decoupling training context length from inference length and achieving linear complexity, HSA-UltraLong provides a viable blueprint for deploying LLMs in scenarios requiring ultra-long context windows without the memory and compute overhead of dense attention.

***

## üîç Key Findings

*   **Ultra-Long Generalization:** The HSA-UltraLong model successfully generalizes to handle contexts up to **16 million tokens**.
*   **High Retrieval Accuracy:** Achieves over **90% accuracy** on in-context retrieval tasks at the 16M token scale.
*   **Efficiency:** Performs comparably to full-attention baselines despite using sparse attention mechanisms.
*   **Scalability:** Demonstrates effectiveness at scale through an 8B-parameter Mixture-of-Experts (MoE) model trained on over 8 trillion tokens.

***

## üß† Methodology

The researchers developed **Hierarchical Sparse Attention (HSA)**, a mechanism designed to provide sparsity, random-access flexibility, and length generalization.

*   **Integration:** HSA was integrated into a standard Transformer to create the HSA-UltraLong model.
*   **Architecture:** The system is an 8B-parameter Mixture-of-Experts (MoE) architecture.
*   **Training Protocol:** Trained on over 8 trillion tokens with rigorous evaluation on both in-domain and out-of-domain context lengths.

***

## ‚öôÔ∏è Technical Details

### Core Architecture
*   **Model:** HSA-UltraLong (8B-parameter MoE).
*   **Complexity:** Utilizes Hierarchical Sparse Attention (HSA) for linear complexity.
*   **Fusion Strategy:** Fuses **Sliding-Window Attention (SWA)** for local retrieval and **HSA** for global retrieval.

### HSA Specifics
*   **Chunking Strategy:** Implements fixed-length chunks ($S=64$).
*   **Retrieval Mechanism:** Uses end-to-end learnable retrieval via a query vector to select top-$K$ relevant chunks.
*   **Attention Computation:** Employs a two-stage process:
    1.  Intra-chunk attention.
    2.  Inter-chunk fusion.
*   **Optimization:** Implements Query-Key Normalization (QK-Norm) for stability and KV Cache Sharing among HSA modules for memory optimization.

### Positional Encoding Strategy (Hybrid)
*   **SWA (Local):** Uses RoPE (Rotary Positional Embeddings).
*   **HSA (Global):** Uses NoPE (No Positional Encoding) to improve extrapolation capabilities.

***

## üìà Results

### Generalization & Scale
*   **Training Volume:** Trained on over 8 trillion tokens.
*   **Progressive Training:** Pre-trained with 8K context, mid-trained to 32K.
*   **Inference Length:** Successfully generalizes to **16 million tokens** (hundreds of times the training length).

### Performance Metrics
*   **S-NIAH @ 16M:** Achieved near-perfect accuracy (~100%).
*   **In-Context Retrieval @ 16M:** Maintained >90% accuracy on most tasks.
*   **Baseline Comparison:** Performs comparably to full-attention baselines at lengths up to 32K.

### Ablation Studies
*   **Comparison with NSA:** Baseline models (e.g., NSA with RoPE) showed significant accuracy drops at long contexts.
*   **Validation:** MQ-NIAH accuracy dropped to **4.0%** for NSA with RoPE at 64K context. This validated the design choice of using NoPE for global retrieval to significantly improve extrapolation capabilities.

***

## üèÜ Contributions

*   **Theoretical Framework:** Presents a framework for efficient ultra-long context modeling.
*   **Architecture Introduction:** Introduces the Hierarchical Sparse Attention (HSA) architecture.
*   **Empirical Validation:** Provides validation via the HSA-UltraLong model, demonstrating high accuracy at extreme context lengths.
*   **Future Guidance:** Outlines experimental insights and open problems to guide future research in the field.

***

**Quality Score:** 9/10  
**References:** 40 citations