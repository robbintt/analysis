# Think Global, Act Local: Bayesian Causal Discovery with Language Models in Sequential Data

*Prakhar Verma; David Arbour; Sunav Choudhary; Harshita Chopra; Arno Solin; Atanu R. Sinha*

---

### üìä Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Framework** | BLANCE (Bayesian LM-Augmented Causal Estimation) |
| **Core Approach** | Hybrid Bayesian / "Data-First" |
| **Key Innovation** | Shift from DAGs to Partial Ancestral Graphs (PAGs) |
| **Models Tested** | GPT-3.5 Turbo, GPT-4o |
| **Datasets** | EARTHQUAKE, ASIA |
| **Top Performance** | Precision **1.0**, SHD **0.0** |
| **Quality Score** | 8/10 |
| **Citations** | 40 References |

---

## üìù Executive Summary

Causal discovery in sequential data presents a complex challenge due to the temporal dependencies of time-series data and the frequent scarcity of human expertise. Relying solely on observational data introduces significant ambiguity driven by latent confounders, while relying on Large Language Models (LMs) exposes researchers to hallucinations, inconsistencies, and biases. Consequently, there is a critical need for methodologies that can integrate the broad, global knowledge embedded in LMs with the rigor of local, sequential observational data without succumbing to the specific pitfalls of either source.

The authors introduce **BLANCE** (Bayesian LM-Augmented Causal Estimation), a hybrid "data-first" Bayesian framework that treats LMs as "noisy experts" rather than infallible oracles. Technically, BLANCE fuses sequential observational data with probabilistic LM priors, explicitly modeling uncertainty from both sampling bias and LM noise. A key theoretical innovation is the shift from Directed Acyclic Graphs (DAGs) to Partial Ancestral Graphs (PAGs), which allows the model to explicitly represent and handle causal ambiguities induced by latent confounders. Furthermore, BLANCE employs a Sequential Optimization Scheme‚Äîan Active Information Retrieval strategy‚Äîto adaptively query the LM only on the most informative edges, thereby grounding global knowledge in local data to perform robust Bayesian parameter estimation.

Experiments on the EARTHQUAKE and ASIA datasets utilizing GPT-3.5 turbo and GPT-4o demonstrated that standard LM prompting methods fail to balance precision and recall. For instance, baseline Pairwise prompting with GPT-4o on the ASIA dataset yielded a Precision of only 0.42 and an SHD of 11.2. In contrast, BLANCE effectively filtered out false positives and resolved structural ambiguities. Specifically, on the ASIA dataset, BLANCE achieved a perfect Precision of 1.0 and an SHD of 0.0, dramatically outperforming the baseline. Similarly, on the EARTHQUAKE dataset, BLANCE achieved a Precision of 1.0 and an SHD of 0.0, compared to the baseline Pairwise prompting which achieved a Precision of 0.69 and an SHD of 1.8.

This research significantly advances the field of causal discovery by establishing a rigorous statistical mechanism to mitigate LM hallucinations. By shifting the standard representation from DAGs to PAGs within a Bayesian context, BLANCE offers a realistic approach to handling uncertainty and latent variables in real-world sequential data. The framework establishes a new paradigm for fusing AI-generated knowledge with domain-specific data, enabling reliable automated causal discovery even in environments where data is scarce or observational.

---

## üîë Key Findings

*   **Superior Integration:** BLANCE outperforms prior causal discovery methods by effectively integrating external knowledge (from LMs) with observational data.
*   **Robustness to Hallucinations:** The system demonstrates robustness to common Language Model deficiencies‚Äîsuch as hallucinations, inconsistencies, and biases‚Äîby treating LMs as **noisy experts**.
*   **Global-to-Local Grounding:** The framework successfully grounds global knowledge from LMs using local data to resolve ambiguities that would otherwise be impossible to clarify.
*   **Extended Capability:** The application of the framework extends robustly beyond just structural discovery to Bayesian parameter estimation.
*   **High Precision, Low Error:** In head-to-head comparisons, BLANCE achieved perfect Precision (1.0) and zero Structural Hamming Distance (0.0) on test datasets, whereas baseline methods suffered from low precision and dense false positive graphs.

---

## üß† Methodology

The research proposes a sophisticated hybrid approach to handle the nuances of sequential data and AI-generated knowledge:

*   **Hybrid Bayesian Framework:** Utilizes a probabilistic structure to fuse sequential observational data with knowledge retrieved from Language Models.
*   **Dual Noise Management:** Explicitly accounts for noise originating from two distinct sources:
    1.  **Data limitations** (Sampling bias)
    2.  **Language Model limitations** (Hallucinations/Bias)
*   **PAGs over DAGs:** Employs **Partial Ancestral Graphs (PAGs)** instead of standard Directed Acyclic Graphs (DAGs). This shift allows the system to represent causal ambiguities specifically caused by latent confounders.
*   **Sequential Optimization Scheme:** Implements an adaptive selection process that identifies and queries the LM only on the most informative edges, optimizing efficiency and accuracy.

---

## ‚öôÔ∏è Technical Details

**System Architecture: BLANCE**
*Bayesian LM-Augmented Causal Estimation*

*   **Core Philosophy:** A "data-first" approach that treats Language Models as **noisy experts** rather than ground truth sources.
*   **Uncertainty Handling:** Addresses two specific types of uncertainty:
    *   *Data-induced uncertainty:* Sampling bias within the observational data.
    *   *LM-induced uncertainty:* Hallucinations and inconsistencies within the model outputs.
*   **Representation Shift:** Moves representation from Directed Acyclic Graphs (DAGs) to **Partial Ancestral Graphs (PAGs)** to explicitly handle ambiguity and latent confounders.
*   **Workflow Pipeline:**
    1.  **Initialization:** Uses FCI (Fast Causal Inference) to build an initial PAG.
    2.  **Optimization:** Applies sequential optimization to determine optimal LM queries.
    3.  **Grounding:** Grounds LM priors using local data to validate global assumptions.
    4.  **Estimation:** Performs Bayesian parameter estimation that incorporates the noisy LM priors.

---

## üìà Experimental Results

Experiments were conducted on **EARTHQUAKE** and **ASIA** datasets using **GPT-3.5 Turbo** and **GPT-4o**. The study compared BLANCE against Pairwise and Triplet prompting baselines using **SHD** (Structural Hamming Distance), **Precision**, and **Recall**.

**Baseline Performance (Standard LM Methods):**
*   Achieved near-perfect Recall (often **1.0**).
*   Suffered from extremely low Precision.
*   Produced graphs that were too dense with many false positives.

| Dataset | Model | Method | SHD | Precision | Recall |
| :--- | :--- | :--- | :--- | :--- | :--- |
| **EARTHQUAKE** | GPT-4o | Pairwise (Baseline) | 1.8 | 0.69 | 1.0 |
| **EARTHQUAKE** | GPT-4o | Triplet (Baseline) | 6.2 | 0.39 | 1.0 |
| **EARTHQUAKE** | **-** | **BLANCE** | **0.0** | **1.0** | **-** |
| | | | | | |
| **ASIA** | GPT-4o | Pairwise (Baseline) | 11.2 | 0.42 | 1.0 |
| **ASIA** | GPT-4o | Triplet (Baseline) | 26.2 | 0.23 | 1.0 |
| **ASIA** | **-** | **BLANCE** | **0.0** | **1.0** | **-** |

---

## üèÜ Core Contributions

1.  **BLANCE Architecture:** Introduction of a novel Bayesian framework designed for causal discovery in sequential data environments where human expertise is scarce.
2.  **Theoretical Representation Shift:** A move from DAGs to **PAGs** within a Bayesian context, allowing the model to mathematically accommodate ambiguity and latent variables.
3.  **Hallucination Mitigation:** A mechanism that treats LM outputs as noisy probabilistic sources that require statistical validation against observational data.
4.  **Active Information Retrieval:** Development of a query strategy that optimizes interaction with the LM by focusing only on the most informative edges.

---

**Quality Score:** 8/10
**References:** 40 Citations