---
title: Improving the Straight-Through Estimator with Zeroth-Order Information
arxiv_id: '2510.23926'
source_url: https://arxiv.org/abs/2510.23926
generated_at: '2026-02-03T20:18:26'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Improving the Straight-Through Estimator with Zeroth-Order Information
*Ningfeng Yang; Tor M. Aamodt*

---

> ### ðŸ“Š Quick Facts
>
> *   **Quality Score:** 9/10
> *   **Citations:** 40
> *   **Key Improvement:** 1â€“8% accuracy increase (DeiT), 1â€“22 point perplexity reduction (LLaMA)
> *   **Efficiency Gain:** 796Ã— reduction in computation vs. n-SPSA
> *   **Target Application:** Quantization-Aware Pre-Training (QAPT)

---

## Executive Summary

Training neural networks with quantized parameters is essential for efficient deployment but remains fundamentally challenging because the discrete nature of quantization breaks the chain rule required for backpropagation. Current solutions face a critical trade-off: the **Straight-Through Estimator (STE)** is computationally efficient but introduces significant bias by approximating gradients, leading to suboptimal convergence. Conversely, **Zeroth-Order (ZO)** optimization methods provide unbiased gradients but require excessive computation (many forward passes) to estimate them, rendering them impractical for large-scale models.

This paper addresses the need for a gradient estimator that balances the low computational cost of STE with the convergence robustness of ZO methods. The authors introduce **First-Order-Guided Zeroth-Order Gradient Descent (FOGZO)**, a hybrid optimization algorithm that leverages the strengths of both first-order and zeroth-order methods.

FOGZO utilizes the STE gradient not as the final update, but as a high-quality directional guide. Technically, the algorithm mixes a normalized STE gradient with a random noise vector using a decayed mixing ratio ($\beta$) to construct query vectors. It then estimates the true gradient via finite differences along these vectors. This approach effectively mitigates the bias inherent in STE while drastically reducing the computational overhead of pure ZO methods. Computationally, FOGZO requires only one standard backward pass and two additional forward passes ($n=1$), making it significantly more efficient than traditional ZO baselines.

Empirical validation demonstrates that FOGZO achieves a superior trade-off between model quality and training efficiency across diverse architectures. Compared to standard STE, FOGZO improves accuracy by 1â€“8% on Vision Transformers (DeiT) and 1â€“2% on ResNet architectures. In LLMs, it achieved a perplexity reduction of 1â€“22 points. Crucially, FOGZO achieved a **796Ã— reduction** in computation compared to the unbiased n-SPSA zeroth-order method on benchmark tasks.

---

## Methodology

The authors propose **First-Order-Guided Zeroth-Order Gradient Descent (FOGZO)**, a hybrid optimization strategy designed to train neural networks with quantized parameters. The core methodology revolves around resolving the limitations of existing approaches:

*   **The Hybrid Approach:** FOGZO utilizes the Straight-Through Estimator (STE) as a source of high-quality, albeit biased, first-order gradient information.
*   **Integration with ZO:** It integrates Zeroth-Order (ZO) gradient descent, which provides unbiased gradients but is typically computationally expensive.
*   **Optimization Goal:** By guiding the process with first-order information, FOGZO aims to mitigate the bias found in STE while significantly reducing the computational overhead required by pure ZO methods.

---

## Key Findings

*   **Performance vs. STE:**
    *   FOGZO improves accuracy by **1â€“8%** on DeiT Tiny/Small models.
    *   Improves accuracy by **1â€“2%** on ResNet 18/50 compared to the standard Straight-Through Estimator (STE).
*   **Performance on LLMs:**
    *   When applied to LLaMA models (up to 0.3 billion parameters), FOGZO achieves a **perplexity reduction of 1â€“22 points** compared to STE.
*   **Computational Efficiency vs. ZO:**
    *   FOGZO achieves a **796Ã— reduction in computation** compared to the n-SPSA (a zeroth-order method) to reach the same loss value on a 2-layer MLP using MNIST.
*   **Overall Trade-off:**
    *   The method empirically demonstrates a superior tradeoff between model quality and training time in Quantization-Aware Pre-Training scenarios.

---

## Contributions

*   **Algorithmic Innovation:** Introduction of FOGZO, a novel optimization algorithm that effectively bridges first-order (STE) and zeroth-order descent methods to solve the quantization training problem.
*   **Bias-Cost Trade-off Resolution:** A theoretical and practical solution that resolves the conflict between the high bias of STE and the high computational cost of traditional zeroth-order methods.
*   **Broad Empirical Validation:** Demonstrated efficacy across multiple architectures, including Vision Transformers (DeiT), Convolutional Neural Networks (ResNet), and Large Language Models (LLaMA).
*   **Efficiency Benchmarking:** Establishment of a significant efficiency benchmark, showing orders of magnitude reduction in computation costs compared to standard zeroth-order baselines like n-SPSA.

---

## Technical Details

**Algorithm Definition**
FOGZO (First-Order-Guided Zeroth-Order) is a biased gradient estimator for Quantization-Aware Pre-Training (QAPT) that combines the efficiency of the Straight-Through Estimator (STE) with the robustness of Zeroth-Order (ZO) optimization.

**Mechanism**
*   **Mixing:** The algorithm mixes a normalized STE gradient with a random noise vector using a decayed mixing ratio $\beta$ to form query vectors.
*   **Estimation:** It then estimates the gradient via finite differences.

**Computational Cost**
*   **Requirement:** Requires one standard forward-backward pass plus $2n$ forward passes.
*   **Efficiency:** With $n=1$, the overhead is limited to only two additional forward passes compared to standard STE.

---

## Results

FOGZO demonstrates consistent improvements across various model architectures:

*   **Vision Transformers (DeiT):** Accuracy improvement of 1â€“8%.
*   **Convolutional Networks (ResNet):** Accuracy improvement of 1â€“2% on ResNet-18/50.
*   **Large Language Models (LLaMA):** Perplexity reduction of 1â€“22 points (up to 0.3B parameters).
*   **Efficiency Benchmark:** Compared to the unbiased n-SPSA method, FOGZO achieved a 796Ã— reduction in computation required to reach a target loss value on an MLP benchmark.