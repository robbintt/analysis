# Fusing Rewards and Preferences in Reinforcement Learning

*Sadegh Khorasani; Saber Salehkaleybar; Negar Kiyavash; Matthias Grossglauser*

---

> ### ðŸ“Š Quick Facts
>
> | Metric | Detail |
> | :--- | :--- |
> | **Quality Score** | 8/10 |
> | **Total Citations** | 25 |
> | **Core Algorithm** | Dual-Feedback Actor (DFA) |
> | **Theoretical Link** | Soft Actor-Critic (SAC) |
> | **Key Environments** | MuJoCo (Continuous), GridWorld (Stochastic) |

---

## Executive Summary

This research addresses the fundamental challenge of integrating sparse environment rewards with dense human preferences in reinforcement learning (RL). Standard RLHF methods typically rely on a separate reward model to bridge the gap between scalar rewards and comparative data, introducing architectural fragmentation that leads to instability and inefficiency. This disconnect is particularly problematic in stochastic environments where defining an accurate scalar reward function is difficult, often resulting in noisy intermediate models that degrade performance.

The authors introduce the **Dual-Feedback Actor (DFA)**, a novel algorithm that fuses individual reward signals and pairwise preference data into a unified update rule. The methodology employs **Direct Preference Modeling (DPM)**, utilizing the policyâ€™s log-probabilities directly within a Bradley-Terry model to calculate preference probabilities, thereby bypassing the need for a separate reward model.

The paper's most significant theoretical contribution is the formal proof that **DFA is theoretically equivalent to Soft Actor-Critic (SAC)** under the Bradley-Terry model. Empirical results demonstrate that DFA achieves performance parity with state-of-the-art methods while offering superior stability across benchmarks, effectively removing the failure points associated with managing separate, potentially mis-specified models.

---

## Key Findings

*   **Theoretical Equivalence:** The Dual-Feedback Actor (DFA) is proven to be theoretically equivalent to Soft Actor-Critic (SAC) when operating under the Bradley-Terry model.
*   **Enhanced Stability:** DFA demonstrates superior stability compared to standard methods, matching or exceeding SAC performance in control environments.
*   **Performance in Stochastic Environments:** Using semi-synthetic data, DFA outperforms standard RLHF baselines in stochastic environments, approaching Oracle performance with true rewards.
*   **Robustness:** The method establishes a robust baseline for preference-augmented RL by effectively removing the architectural complexity of separate reward modeling.

---

## Methodology

The researchers developed the **Dual-Feedback Actor (DFA)** algorithm to resolve the fragmentation between reward signals and preference data. The approach consists of three main pillars:

1.  **Unified Update Rule:** DFA fuses individual reward signals and pairwise preference data into a single, coherent update rule, rather than treating them as separate training phases.
2.  **Direct Preference Modeling (DPM):** The method employs DPM using the policy's log-probabilities directly. This bypasses the computational overhead and potential errors associated with training a separate reward model.
3.  **Hybrid Feedback Integration:** The methodology accommodates the flexible integration of preferences derived from both human annotators and synthetic data generated via Q-values, allowing the agent to learn from both sparse and dense feedback.

---

## Technical Details

### Algorithm: Dual-Feedback Actor (DFA)

DFA is a reinforcement learning algorithm that incorporates human feedback without estimating a scalar reward function.

*   **Feedback State:** It utilizes state-wise feedback defined as $D_{pref} = \{(s_k, a^+_k, a^-_k)\}$.
*   **Probability Modeling:** Preference probability is modeled directly using the policyâ€™s probability densities based on the Bradley-Terry model:
    $$P_\theta(a^+ \succ a^- | s) = \frac{\pi_\theta(a^+|s)^\alpha}{\pi_\theta(a^+|s)^\alpha + \pi_\theta(a^-|s)^\alpha}$$
*   **Objective Function:** The algorithm minimizes the negative log-likelihood of preferences.

### Signal Fusion Mechanism

To fuse numeric rewards with preference data, DFA synthesizes preference pairs from the agent's replay buffer. This is achieved by comparing the Q-values of actions in neighboring states, effectively translating scalar rewards into comparative data.

### Theoretical Differentiation

*   **Alignment:** The method is theoretically aligned with the entropy-regularized RL solution (Soft Actor-Critic).
*   **Distinction:** Unlike DPO (Direct Preference Optimization) or SPO (Stochastic Policy Optimization), DFA avoids the use of a global KL (Kullback-Leibler) regularizer.

---

## Results

The performance of DFA was evaluated across continuous control and stochastic environments:

*   **Control Environments (MuJoCo):**
    *   DFA demonstrated superior stability and matched the performance of SAC.
    *   **HalfCheetah:** DFA converged to average returns of approximately **12,300**, closely tracking SACâ€™s ~12,600.
    *   **Ant & Hopper:** DFA showed similar performance parity in these environments.
*   **Stochastic Environments (GridWorld):**
    *   Using semi-synthetic data, DFA significantly outperformed standard RLHF baselines.
    *   DFA achieved **normalized returns exceeding 0.90**, approaching the Oracle agentâ€™s perfect score of 1.0.
    *   Competing PPO-based RLHF methods typically plateaued below **0.6** due to reward modeling noise.

---

## Contributions

*   **Unified Update Mechanism:** Introduced a mechanism that combines rewards and preferences without the need for a separate reward model.
*   **Theoretical Foundation:** Provided a formal theoretical proof connecting preference-based learning to entropy-regularized policy optimization.
*   **Empirical Validation:** Demonstrated the effectiveness of hybrid feedback (human and synthesized) across GridWorld and continuous control environments.

---

**Paper Rating:** 8/10  
**References:** 25 citations