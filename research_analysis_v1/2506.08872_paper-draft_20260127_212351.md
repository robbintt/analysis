---
title: Paper draft
arxiv_id: '2506.08872'
source_url: https://arxiv.org/abs/2506.08872
generated_at: '2026-01-27T21:23:51'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# Paper Draft

*College Wellesley, Yuan Wellesley, Liao Mass*

***

> ### ðŸ“Š Quick Facts
>
> *   **Study Type:** Longitudinal & Crossover
> *   **Duration:** 4 Months
> *   **Participants:** 54 (Initial), 18 (Crossover)
> *   **Neurotech:** 32-electrode EEG (992 pairs)
> *   **Primary Metric:** Dynamic Directed Transfer Function (dDTF)
> *   **Comparison:** LLM vs. Search Engine vs. Brain-only

## Executive Summary

This paper addresses the critical gap in understanding the specific neurophysiological consequences of integrating Large Language Models (LLMs) into cognitive tasks, distinguishing them from traditional tools like search engines. While LLMs are known to enhance the immediate quality of output, the research investigates the hidden cost of "cognitive offloading." The central problem is determining whether the convenience of generative AI comes at the expense of internal cognitive engagement to a greater degree than standard information retrieval, potentially leading to long-term degradation in memory retention, learning capabilities, and a diminished sense of ownership over oneâ€™s work. Unlike prior studies that focus on single-session metrics, this study seeks to understand the trajectory of cognitive adaptation over extended periods.

The study introduces a rigorous longitudinal, multi-modal assessment framework that triangulates neurophysiological data, linguistic analysis, and subjective user experience over a four-month period. Technically, the research utilized a 32-electrode Electroencephalography (EEG) setup to capture functional connectivity across 992 electrode pairs, employing the Dynamic Directed Transfer Function (dDTF) on Alpha band oscillations to map neural coupling. Methodologically, the paper is significant for its sample size and durationâ€”54 participants across initial sessions, with a subset of 18 in a fourth sessionâ€”and its three-arm design comparing LLM (ChatGPT), Search Engine (Google), and Brain-only (no tools) groups. A key technical innovation is the implementation of a crossover protocol in the final session, which allowed for the observation of neural adaptations as participants transitioned from relying on AI back to independent work.

The findings reveal a distinct hierarchy of cognitive effort, with the LLM group demonstrating significantly weaker neural coupling than both the Search Engine and Brain-only groups. Quantitatively, the LLM group recorded a Total dDTF sum of 2.222 compared to the Brain-only group's 2.730, with the latter exhibiting stronger connectivity in 79 instances versus the LLM group's 42. In terms of statistical robustness, the Brain-only group showed a much higher density of significant connections, with a normalized sum of 0.053 versus 0.009 for the LLM group at the $p < 0.001$ threshold; specific pathways such as P7 to T8 also reflected this reduction (0.0092 vs. 0.0529). Critically, longitudinal data indicated that the LLM group suffered degraded learning over time, displaying impaired recall of essays written minutes prior. The crossover session further highlighted these deficits: transitioning away from LLMs resulted in under-engaged brain networks, whereas introducing LLMs to non-users caused a spike in connectivity. Additionally, LLM users reported a reduced sense of ownership and produced more homogeneous linguistic outputs despite achieving high essay scores.

This research provides the first empirical neurophysiological evidence differentiating the cognitive costs of LLMs from those of Search Engines, establishing a clear link between generative AI and the downscaling of internal brain network connectivity. By demonstrating that improved output metrics do not equate to enhanced learningâ€”and that prior LLM reliance may hamper independent problem-solvingâ€”the paper offers a vital cautionary perspective for the integration of AI in education. The discovery of the "LLM-to-Brain" connectivity deficit versus the "Brain-to-LLM" spike suggests that cognitive offloading creates a lasting physiological impact, challenging the assumption that AI tools are merely benign aids to productivity and highlighting the risk of cognitive atrophy in users who over-rely on automation.

---

## Key Findings

*   **Cognitive Offloading:** LLM usage causes significant cognitive offloading, evidenced by the weakest neural coupling compared to Search Engine and Brain-only groups, despite producing high-scoring essays.
*   **Degraded Learning:** Over four months, the LLM group demonstrated degraded learning and memory skills, including impaired recall of essays written minutes prior.
*   **Reduced Ownership:** Participants using LLMs reported a reduced sense of ownership over their work, whereas Search Engine users reported stronger ownership.
*   **Adaptation Deficits:** Transitioning away from LLMs (LLM-to-Brain) led to under-engaged brain networks, while introducing LLMs to non-users (Brain-to-LLM) caused a spike in network connectivity.
*   **Homogeneous Output:** NLP analysis revealed that LLM reliance leads to homogeneous and standardized linguistic outputs.

## Methodology

The study employed a longitudinal and crossover design to measure the long-term cognitive effects of AI tool usage.

*   **Design & Duration:**
    *   Conducted over four months.
    *   54 participants across three initial sessions.
    *   Subset of 18 participants in a fourth crossover session.
*   **Group Segmentation:**
    *   **LLM:** Using ChatGPT.
    *   **Search Engine:** Using Google only (AI-overviews disabled).
    *   **Brain-only:** No digital tools.
*   **Protocol:**
    *   Implemented a crossover protocol in Session 4 where LLM users switched to no tools (LLM-to-Brain) and Brain-only users switched to LLMs (Brain-to-LLM).
*   **Data Collection:**
    *   **EEG:** To measure neural connectivity.
    *   **NLP:** For linguistic analysis.
    *   **Qualitative:** Post-session interviews.
    *   **Performance:** Scoring by humans and AI.

## Technical Details

*   **Study Duration:** 4 months
*   **Participants:** 54 randomly assigned to three groups:
    1.  LLM (ChatGPT only)
    2.  Search Engine (Google only)
    3.  Brain-only (No external tools)
*   **Equipment:**
    *   32-electrode EEG setup.
    *   Generated **992 electrode pairs**.
*   **Signal Processing:**
    *   Utilized **Dynamic Directed Transfer Function (dDTF)** on Alpha band oscillations.
    *   Filtered using a **p-value < 0.05** threshold.
    *   Data normalized and aggregated across three sessions.

## Results

The study observed a statistically significant reduction in cognitive effort among LLM users.

**Neural Connectivity Comparisons (LLM vs. Brain-only):**

*   **Total dDTF Sum:**
    *   LLM: 2.222
    *   Brain-only: 2.730
*   **Stronger Connectivity Instances:**
    *   Brain-only: 79 instances
    *   LLM: 42 instances
*   **Statistical Significance (p < 0.001):**
    *   Brain-only: 0.053 density
    *   LLM: 0.009 density
*   **Specific Pathway Example (P7 to T8):**
    *   LLM: 0.0092
    *   Brain-only: 0.0529

**Qualitative Outcomes:**
*   LLM users produced more homogeneous essays.
*   LLM users reported a reduced sense of ownership.
*   LLM users demonstrated degraded content recall despite high essay performance scores.

## Contributions

*   **Empirical Evidence:** Provides empirical neurophysiological evidence of the cognitive costs and potential 'cognitive atrophy' associated with LLM use in education.
*   **Hierarchy of Effort:** Establishes a hierarchy of cognitive effort (**Brain-only > Search Engine > LLM**) showing how external support scales down internal brain network connectivity.
*   **Adaptation Deficits Identification:** Identifies cognitive adaptation deficits, demonstrating that prior reliance on LLMs can hamper independent problem-solving when the tool is removed.
*   **Multi-modal Framework:** Introduces a multi-modal assessment framework that triangulates neural data, linguistic output, and subjective experience to evaluate the impact of AI on cognition.

***
**Quality Score:** 8/10 | **References:** 40 citations