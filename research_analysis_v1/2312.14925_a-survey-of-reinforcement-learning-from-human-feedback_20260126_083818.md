---
title: A Survey of Reinforcement Learning from Human Feedback
arxiv_id: '2312.14925'
source_url: https://arxiv.org/abs/2312.14925
generated_at: '2026-01-26T08:38:18'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# A Survey of Reinforcement Learning from Human Feedback

*Human Feedback, Artificial Intelligence, Reinforcement Learning, Research Center, Duke Kunshan, German Research, Timo Kaufmanntimo, Digital Innovation, Viktor Bengs, Paul Wengpaul*

***

### QUICK FACTS

| Metric | Value |
| :--- | :--- |
| **Quality Score** | 8/10 |
| **Total Citations** | 40 |
| **Research Type** | Literature Survey |
| **Core Domains** | LLMs, Robotics, Control Theory |
| **Key Algorithms** | PPO, TRPO, DPO, RRHF |

***

> **EXECUTIVE SUMMARY**
>
> This paper addresses the challenge of aligning reinforcement learning systems with complex human values, a task where traditional engineered reward functions often fail due to "reward hacking" and an inability to capture nuance. Reinforcement Learning from Human Feedback (RLHF) is defined as a reinforcement learning variant that utilizes human feedback rather than engineered reward functions to enhance system performance and adaptivity. This alignment is critical in safety-sensitive domains, particularly Large Language Models (LLMs) and autonomous robotics, because scalar metrics cannot easily represent the subtleties of desired human behavior.
>
> The authors’ primary contribution is a technical synthesis that organizes existing research into a functional decomposition of the system, explicitly structuring the workflow into a five-stage pipeline. These stages include: Theoretical Foundations (Preference-Based MDPs); Feedback Engineering (addressing cold-start initialization and feedback typologies); Data Pipelines (utilizing active learning); Reward Model Architecture (distinguishing utility from reward learning); and Policy Optimization.
>
> Crucially, the analysis contrasts traditional actor-critic methods like PPO and TRPO—which optimize against a learned reward model—against modern approaches such as Direct Preference Optimization (DPO) and Rank Response from Human Feedback (RRHF), which optimize the policy directly on preference data. As the paper is a literature survey, it does not present novel experimental data but establishes a structured taxonomy of evaluation criteria essential for benchmarking alignment systems. The authors categorize results into three distinct metric classes: Reward Function Evaluation, Policy Performance Metrics, and Human-Centric Metrics.
>
> The significance of this work lies in its holistic scope, bridging the historical origins of RLHF in control theory and robotics with its current application in LLM development. By situating modern generative AI breakthroughs within established engineering principles, the paper provides a standardized technical reference that contextualizes the rapid evolution of algorithms, offering the field a rigorous vocabulary and a structured lens for evaluating alignment.

***

## Key Findings

*   **Definition:** RLHF is a reinforcement learning variant that utilizes human feedback rather than engineered reward functions.
*   **Primary Value:** It enhances system performance and adaptability while aligning objectives with human values.
*   **LLM Success:** RLHF is identified as a decisive factor in the success of Large Language Models (LLMs).
*   **Cross-Domain Roots:** Fundamental RLHF techniques originate from control and robotics, demonstrating broad applicability beyond just generative AI.

## Methodology

The authors employ a comprehensive **literature survey methodology**. Rather than presenting new experimental results, the article synthesizes existing research to provide a high-level overview of RLHF fundamentals. The review covers the technique across various domains, specifically emphasizing:

1.  **Origins:** Control theory and robotics.
2.  **Modern Application:** Large Language Models (LLMs).
3.  **Core Focus:** Algorithm-human feedback interactions and current research trends.

## Technical Details

The paper structures RLHF into a comprehensive **five-stage pipeline**:

### 1. Theoretical Foundation
*   Formalized as **Preference-Based MDPs**.
*   Features a distinct Reward Learning phase.

### 2. Feedback Engineering
*   Analyzes feedback typologies.
*   Addresses cold-start initialization strategies.
*   Examines hybrid combination logic.

### 3. Data Pipeline
*   Utilizes **active learning** for query efficiency.
*   Focuses on addressing human labeling bottlenecks.

### 4. Reward Model Architecture
*   **Distinctions:** Separates utility learning from reward learning.
*   **Input Representations:** Analyzes trajectory history and state-action inputs.
*   **Techniques:** Covers feedback efficiency methods.

### 5. Policy Optimization
*   **Algorithm Adaptation:** Covers PPO and TRPO.
*   **Contextual Bandit:** Framing within the bandit context.
*   **Direct Optimization:** Covers Direct Policy Optimization (DPO) and Rank Response from Human Feedback (RRHF).

## Contributions

*   **Holistic Scope:** Documents the origins and applications of RLHF in control and robotics, moving beyond the current LLM hype.
*   **Fundamental Synthesis:** Clarifies the mechanisms of how RL agents interact with human feedback.
*   **Resource for Researchers:** Provides a structured overview of main research trends and principles to aid future study.

## Evaluation Criteria & Results

As this is a survey paper, specific quantitative experimental results are not provided. Instead, the analysis identifies key evaluation criteria and metric types discussed in the literature:

### Reward Function Evaluation Metrics
*   Reward Prediction Accuracy
*   Generalization / Out-of-Distribution (OOD) performance
*   Consistency

### Policy Performance Metrics
*   Win Rate
*   KL-Divergence
*   Sample Efficiency

### Human-Centric Metrics
*   Labeler Efficiency
*   Feedback Quality