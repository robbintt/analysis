---
title: What Is Your Agent's GPA? A Framework for Evaluating Agent Goal-Plan-Action
  Alignment
arxiv_id: '2510.08847'
source_url: https://arxiv.org/abs/2510.08847
generated_at: '2026-01-26T12:44:30'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# What Is Your Agent's GPA? A Framework for Evaluating Agent Goal-Plan-Action Alignment

*Independent School, Allison Sihan, Logical Consistency, Daniel Huang, Anupam Datta, Stanford University, Shayak Sen, Goal Ful, Nirvika Choudhury, Nikhil Vytla*

> ### ðŸ“Š Quick Facts
> | Metric | Value |
> | :--- | :--- |
> | **Error Coverage** | 100% (TRAIL/GAIA) |
> | **Localization Precision** | 86% |
> | **LLM-Judge Agreement** | 80% â€“ 95%+ |
> | **Reliability (Krippendorff's $\alpha$)** | 0.628 â€“ 0.934 |
> | **Core Metrics** | 5 Diagnostic Metrics |
> | **Validation Datasets** | TRAIL/GAIA & Snowflake Intelligence |

---

## Executive Summary

Evaluating autonomous AI agents presents a significant challenge because current methodologies rely heavily on binary success/failure metrics that offer little insight into the root causes of failure. As agents are tasked with increasingly complex, multi-step objectives, this lack of granularity makes it difficult for developers to distinguish whether a failure stems from poor goal formulation, flawed planning logic, or inefficient execution. This opacity hinders the debugging and refinement process, creating a need for a more systematic approach to assessing the intricate alignment between an agent's goals, plans, and actions.

The paper introduces the **Agent GPA (Goal-Plan-Action)** framework, a novel evaluation paradigm designed to decompose agent performance into five distinct, diagnostic metrics: Goal Fulfillment, Logical Consistency, Plan Quality, Plan Adherence, and Execution Efficiency (plus Tool Selection). Technically, the framework utilizes the **TruLens OSS** library and employs **Claude-3.5-Sonnet** as an "LLM-as-a-Judge" to evaluate OpenTelemetry traces. By implementing specialized judges with specific system instructions and few-shot examples, the system can automatically parse execution logs, remove redundancies to manage context limits, and provide precise, structured feedback on specific operational failures.

Empirical validation on the public TRAIL/GAIA benchmark and an internal Snowflake Intelligence production dataset demonstrates the framework's robustness and accuracy. Agent GPA achieved **100% coverage of errors** in the TRAIL/GAIA dataset (570 errors) and demonstrated error localization precision of **86%** when compared to human ground truth. The automated judges showed strong reliability with human annotators, identifying between 80% and over 95% of errors; specifically, Execution Efficiency achieved a Krippendorff's alpha of 0.934, while the proprietary dataset showed 82% overall agreement and an alpha of 0.882 for Execution Efficiency.

This research provides a critical shift from opaque, outcome-based evaluation to a transparent, process-driven diagnostic model, significantly influencing how AI agents are tested and improved. By proving that automated LLM-judges can effectively scale to match human-level annotation in fine-grained analysis, Agent GPA offers a practical solution for maintaining high standards in production environments. The framework not only facilitates the targeted optimization of agent behaviors but also establishes a new standard for assessing the logical coherence and operational efficiency of autonomous systems.

---

## Key Findings

*   **Comprehensive Error Coverage:** The Agent GPA framework comprehensively covers a broad range of agent failures, achieving **100% coverage** of agent errors on the public TRAIL/GAIA benchmark dataset.
*   **Automated Evaluation Support:** The framework supports automated evaluation using LLM-judges, which demonstrate strong alignment with human annotations, covering between **80% and over 95%** of identified errors.
*   **Precision in Error Localization:** The methodology enables precise error localization with **86% agreement** compared to human ground truth, facilitating targeted improvements in agent performance.
*   **Broad Validation Success:** Validation was successful across two distinct environments: the public TRAIL/GAIA benchmark and an internal dataset for a production-grade data agent (Snowflake Intelligence).

---

## Methodology

The researchers proposed the **Agent GPA (Goal-Plan-Action)** framework, an evaluation paradigm designed to assess an agent's performance across its entire operational loop: setting goals, devising plans, and executing actions.

The methodology implements five distinct metrics to diagnose specific failures:
1.  **Goal Fulfillment**
2.  **Logical Consistency**
3.  **Execution Efficiency**
4.  **Plan Quality**
5.  **Plan Adherence**

The approach was empirically validated by benchmarking against two datasets (TRAIL/GAIA and an internal production dataset) and measuring the agreement between LLM-judges and human annotators.

---

## Technical Details

The implementation of the Agent GPA framework relies on a sophisticated technical stack to interpret agent behavior:

| Component | Specification |
| :--- | :--- |
| **Framework Dimensions** | Evaluates across **Goal**, **Plan**, and **Action**. |
| **Core Library** | **TruLens OSS**. |
| **LLM-as-a-Judge** | **Claude-4-Sonnet** (configured with high reasoning effort). |
| **Evaluation Strategy** | Uses system instructions, few-shot examples, and structured output generation. |
| **Specialized Judges** | Six dedicated judges are employed: |
| | â€¢ **PQ**: Plan Quality |
| | â€¢ **PA**: Plan Adherence |
| | â€¢ **GF**: Goal Fulfillment |
| | â€¢ **LC**: Logical Consistency |
| | â€¢ **EE**: Execution Efficiency |
| | â€¢ **TS**: Tool Selection |
| **Data Pre-processing** | Involves traversing **OpenTelemetry traces** and removing duplicate messages to manage context window limits effectively. |

---

## Contributions

*   **A Holistic Evaluation Framework:** Introduction of a systematic paradigm (**Agent GPA**) that moves beyond simple success/failure metrics to evaluate the intricate connection between goals, plans, and actions.
*   **Granular Diagnostic Metrics:** The definition of five specific metrics that allow for fine-grained analysis of agent behavior and failure modes.
*   **Scalable Automated Evaluation:** Demonstration that LLM-judges can effectively replace or supplement human annotation in evaluating complex agent behaviors, offering a scalable solution with high agreement rates (up to **95%+**).

---

## Results

The framework was rigorously tested, yielding the following performance metrics:

**On TRAIL/GAIA (117 traces, 570 errors):**
*   **Coverage:** 100% of error types identified.
*   **Identification Accuracy:** 95% (267 out of 281 errors).
*   **Localization Precision:** 86%.
*   **Reliability (Krippendorff's $\alpha$):**
    *   Execution Efficiency: 0.934
    *   Plan Quality: 0.628

**On Snowflake Intelligence Dataset (17 traces):**
*   **Overall Agreement:** 82% with human annotators.
*   **Execution Efficiency:** 0.882 accuracy, 0.81 $\alpha$.
*   **Logical Consistency:** 0.765 accuracy, 0.66 $\alpha$.

---

**Paper Quality Score:** 8/10
**References:** 40 citations