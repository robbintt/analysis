# Off-Policy Actor-Critic for Adversarial Observation Robustness: Virtual Alternative Training via Symmetric Policy Evaluation

*Kosuke Nakanishi; Akihiro Kubo; Yuji Yasui; Shin Ishii*

---

### ðŸ“Š Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Quality Score** | 7/10 |
| **Total Citations** | 40 |
| **Key Framework** | VALT (Virtual Alternative Training) |
| **Test Environments** | MuJoCo (HalfCheetah, Hopper, Walker2d, Ant) |
| **Core Innovation** | Symmetric Policy Evaluation & Soft-Constrained Optimization |

---

## Executive Summary

Current Reinforcement Learning (RL) methods for achieving adversarial observation robustness face a fundamental computational bottleneck. Traditional approaches rely on adversarial training where the agent and adversary update policies through tight coupling, restricting robustness training almost exclusively to on-policy methods. This limitation necessitates constant environmental interaction for every update, rendering robustness computationally expensive and sample-inefficient. This restriction is particularly problematic for deployment in complex, real-world environments where data collection is costly or dangerous, making existing robust RL solutions impractical for widespread use.

The authors introduce **Virtual Alternative Training (VALT)**, a novel off-policy actor-critic framework that overcomes these limitations by reframing adversarial learning as a soft-constrained optimization problem. The core technical innovation is the identification of "symmetric policy evaluation," a theoretical property demonstrating that the value functions for the agent and adversary satisfy a specific symmetry. By leveraging this property and replacing the traditional tight coupling with soft-constrained optimization, VALT decouples the training loop. This architecture allows the agent and adversary to perform independent "virtual updates" using existing replay buffers, eliminating the need for synchronous environmental sampling and enabling robustness training in an off-policy setting.

Evaluations across MuJoCo benchmarks (HalfCheetah, Hopper, Walker2d, and Ant) demonstrated that VALT variants (VALT-EPS and VALT-SOFT) achieve robust performance where previous off-policy methods failed. While the VALT variants maintained stability and matched the performance of LSTM-based PPO baselines, the experiments highlighted severe instability in competing methods. Specifically, the WocaR-SAC baseline experienced training collapse as attack schedules increased, failing completely at durations of $2.0 \times 10^5$ steps on the HalfCheetah environment. The authors attributed this failure to poor value estimation caused by the absence of the adversary during policy improvementâ€”a failure mode VALT avoids by decoupling the updates.

This research fundamentally shifts the paradigm of robust RL by proving that adversarial robustness can be effectively achieved through off-policy methods rather than being restricted to on-policy interaction. By eliminating the bottleneck of coupled training loops, VALT drastically improves sample efficiency and reduces the computational cost of training robust agents. The paper contributes both theoretically, through the establishment of symmetric policy evaluation, and practically, via the open-source release of VALT-SAC.

---

## Key Findings

*   **Inefficiency in Current Methods:** Identified that current robust RL methods are inefficient due to mutual dependencies between the agent and the adversary.
*   **Off-Policy Feasibility:** Demonstrated that off-policy feasibility for adversarial robustness is achievable, removing the need for constant environmental interactions.
*   **Symmetric Policy Evaluation:** Discovered a symmetric property of policy evaluation that exists between the agent and the adversary.
*   **Optimization Reframing:** Showed that adversarial learning can be reframed as a soft-constrained optimization problem, successfully decoupling training from the environment loop.

---

## Methodology

The authors propose **Virtual Alternative Training (VALT)**, a framework centered around an off-policy actor-critic architecture.

*   **Core Architecture:** VALT utilizes a soft-constrained optimization approach instead of traditional alternating learning, which typically requires direct environmental engagement.
*   **Virtual Updates:** The approach leverages symmetric policy evaluation to perform updates virtually without generating new environment samples, enhancing data efficiency.

---

## Technical Details

### Proposed Algorithms
*   **VALT (Virtual Alternative Training):** The primary framework achieving off-policy feasibility via soft-constrained optimization.
*   **Variants:**
    *   **VALT-EPS**
    *   **VALT-SOFT**

### Baselines Compared
*   **LSTM-based PPO variants:** Robust-PPO, ATLA-PPO, WocaR-PPO.
*   **Off-policy methods:** Robust-SAC, SAC-PPO.
*   **WocaR-SAC:** Explored via two implementations mixing worst-case objectives.

### Training Mechanics
*   **Attack Scale Scheduling:** The attack scale is scheduled linearly from 0.0.
    *   *Example Duration:* $2.0 \times 10^5$ steps for the HalfCheetah environment.
*   **Mixture Rate:** The $\kappa_{worst}$ mixture rate is increased from 0.0 during training.

---

## Results

Experiments were conducted on MuJoCo tasks (**HalfCheetah, Hopper, Walker2d, Ant**) measuring **Average Episodic Return** and **Sample Efficiency**.

*   **DDPG vs. SAC:** DDPG was found to be inadequate compared to SAC.
*   **VALT Performance:** VALT variants (VALT-EPS, VALT-SOFT) demonstrated competitive sample efficiency against robust baselines.
*   **SAC-PPO:** The SAC-PPO baseline suffered from significant sample inefficiency.
*   **WocaR-SAC Failure:** Analysis revealed that while initially successful, WocaR-SAC experienced **training collapse** as attack scales increased. This was attributed to the lack of an adversary during policy improvement causing poor value estimation, unlike in on-policy settings.

---

## Contributions

*   **Elimination of Bottlenecks:** Removed the mutual dependency bottleneck that previously hindered off-policy methods in adversarial robust RL.
*   **Sample Efficiency:** Achieved significant improvement in sample efficiency by removing the need for additional environmental interactions.
*   **Dual Contribution:**
    *   **Theoretical:** Advancements in symmetric policy evaluation for robustness.
    *   **Practical:** Release of a robust, open-source implementation (VALT_SAC).