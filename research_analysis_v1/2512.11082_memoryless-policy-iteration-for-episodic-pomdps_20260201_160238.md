# Memoryless Policy Iteration for Episodic POMDPs
*Roy van Zuijlen; Duarte Antunes*

***

> ### ðŸ“Š Quick Facts
> | Metric | Details |
> | :--- | :--- |
> | **Quality Score** | 8/10 |
> | **References** | 14 Citations |
> | **Domain** | Reinforcement Learning (POMDPs) |
> | **Setting** | Episodic, Model-Based & Model-Free |

***

> ## ðŸ“ Executive Summary
> 
> This research addresses the computational intractability of solving Episodic Partially Observable Markov Decision Processes (POMDPs), a critical challenge where agents must make decisions based on incomplete or noisy information. Standard solutions often fail due to the "curse of dimensionality" inherent in belief-state tracking, which requires maintaining probability distributions over all possible states. Furthermore, because observations in POMDPs are non-Markovian, directly applying efficient policy iterationâ€”a standard technique in fully observable environmentsâ€”has been theoretically problematic.
> 
> The authors introduce **"Periodic Memoryless Policy Iteration,"** a novel framework that optimizes direct mappings from observations to actions, thereby bypassing the need for computationally expensive belief-state tracking. The methodâ€™s core technical contribution is **"Pattern Optimization,"** a theoretical advance that identifies specific periodic schedules for alternating between single-stage policy improvements and evaluations. By dictating the exact rhythm and order of these updates, the pattern ensures that the policy value improves monotonically despite the non-Markovian nature of the observations.
> 
> **Performance & Impact:**
> Experimental results validate the framework's efficiency, demonstrating substantial computational speedups compared to policy-gradient baselines and specialized algorithms. While computing optimal memoryless policies remains NP-hard, the proposed method offers a tractable compromise: it achieved values within 20% of the optimal POMDP value in 60% of benchmark problems against Mixed-Integer Linear Programming (MILP) baselines. By accepting bounded suboptimality, the algorithm achieves tractability in large-scale domains where traditional exact solvers are computationally prohibitive, successfully bridging the gap between model-based and model-free reinforcement learning.

***

## Key Findings

*   **Monotonic Improvement:** Developed a family of policy-iteration algorithms for episodic POMDPs that guarantees monotonic improvement in policy performance.
*   **Periodic Optimization:** Identified specific periodic patterns for alternating policy improvement and evaluation that maximize computational efficiency.
*   **Computational Speedups:** Demonstrated substantial speedups compared to traditional policy-gradient baselines and specialized algorithms.
*   **Unified Efficacy:** The approach is proven effective in both model-based settings (using a known model) and model-free settings (learning directly from data).
*   **Belief Bypass:** Successfully operates in the output space rather than the belief space, avoiding the curse of dimensionality.

## Methodology

The research introduces a framework designed to solve POMDPs using memoryless policies, shifting the focus from complex belief states to direct observation outputs.

*   **Memoryless Policy Framework:** Solves POMDPs using policies in the output space rather than the belief space.
*   **Periodic Policy Iteration:** Implements a cycle alternating between single-stage output-based policy improvements and policy evaluations.
*   **Pattern Optimization:** Theoretically identifies specific update patterns (rhythms of improvement and evaluation) that maximize efficiency.
*   **Model-Free Extension:** Capable of estimating values directly from data, enabling the learning of memoryless policies without a known environment model.

## Contributions

*   **Theoretical Breakthrough:** Overcame the theoretical challenge of applying policy iteration to non-Markovian output processes.
*   **Dimensionality Reduction:** Provided a computationally efficient solution that avoids the curse of dimensionality associated with traditional belief-space methods.
*   **Unified Framework:** Bridged the gap between model-based and model-free settings, offering a unified approach for reinforcement learning in partially observable environments.

## Technical Details

| Aspect | Specification |
| :--- | :--- |
| **Problem Scope** | Episodic POMDPs with finite states, actions, and observations. |
| **Policy Constraint** | Time-varying deterministic memoryless policies. |
| **Core Algorithm** | **Periodic Memoryless Policy Iteration (PMPI)** - Updates policy stages sequentially based on a periodic sequence to maximize expected episodic return. |
| **Key Metrics** | Relies on the **Observation-Action Value** and **Cost Index** for optimization. |
| **Optimization Strategy** | Efficient recomputation strategies based on defined periodic patterns. |
| **Model-Free Modes** | Includes **State-Informed** methods (using state data) and **Observation-Only** methods. |

## Results

*   **Computational Efficiency:** Achieved substantial computational speedups over policy-gradient baselines and specialized algorithms.
*   **Convergence:** Theoretically guarantees monotonic improvement and convergence to a local optimum.
*   **Benchmark Performance:**
    *   Mixed-Integer Linear Programming (MILP) policies achieved values within 20% of the optimal POMDP value in 60% of problems.
    *   Computing optimal memoryless policies remains NP-hard, validating the need for the proposed tractable approach.
*   **Versatility:** Validated effectiveness across Model-Based, Model-Free, State-Informed, and Observation-Only settings.