---
title: Evaluating Metalinguistic Knowledge in Large Language Models across the World's
  Languages
arxiv_id: '2602.02182'
source_url: https://arxiv.org/abs/2602.02182
generated_at: '2026-02-04T15:41:23'
quality_score: 9
citation_count: 12
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Evaluating Metalinguistic Knowledge in Large Language Models across the World's Languages

*Tja≈°a Arƒçon; Matej Klemen; Marko Robnik-≈†ikonja; Kaja Dobrovoljc*

---

## üìä Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Top Model Accuracy** | 0.367 (GPT-4o) |
| **Languages Covered** | 2,600+ |
| **Linguistic Domains** | Phonology, Morphology, Lexicon, Syntax |
| **Key Predictor** | Wikipedia Size (Resource availability) |
| **Quality Score** | 9/10 |

---

## Executive Summary

This paper addresses the critical gap in understanding whether Large Language Models (LLMs) possess genuine metalinguistic knowledge‚Äîthe explicit ability to reason about linguistic structures such as phonology, morphology, and syntax‚Äîor if they merely replicate surface-level statistical patterns. While LLMs have demonstrated high proficiency in text generation for high-resource languages, their capacity for generalized grammatical reasoning across the world's diverse linguistic landscape remains largely unproven. This matters because it distinguishes between true cognitive understanding of language rules and data-driven mimicry, a distinction that becomes increasingly complex as models scale.

The authors introduce a comprehensive evaluation framework built upon the **World Atlas of Language Structures (WALS)**, creating an open-source benchmark that covers over 2,600 languages. Technically, the study moves beyond standard fluency or translation metrics by utilizing targeted prompting to assess explicit, metalinguistic reasoning across four distinct domains: phonology, morphology, lexicon, and syntax. This approach isolates the model's ability to describe and analyze linguistic features rather than simply process them. The methodology further employs multi-dimensional analysis and predictor modeling to quantify the influence of external variables, such as digital resource availability and genealogical relationships, on model performance.

The empirical findings reveal significant limitations in current LLM capabilities. The state-of-the-art **GPT-4o** achieved a modest accuracy of 0.367, failing to outperform a majority-class baseline, which suggests a lack of robust metalinguistic competence. Performance varied substantially across linguistic domains: lexical features yielded the highest accuracy, while phonological features were the lowest, a discrepancy largely attributed to the differences in online visibility of these features. The study identified a strong positive association between a language's digital status and model accuracy, with Wikipedia size serving as the most informative predictor‚Äîsignificantly more so than geographical or sociolinguistic factors. Additionally, while larger models generally outperformed smaller ones, accuracy dropped sharply for low-resource languages, particularly on tasks requiring abstract reasoning.

This research challenges the assumption that scaling LLMs automatically results in generalizable grammatical competence, providing evidence that metalinguistic knowledge is fragmented and heavily dependent on training data availability rather than innate reasoning capabilities. By releasing a broad-scale benchmark dataset, the authors equip the research community with a standardized tool to track progress in multilingual understanding. The findings highlight urgent performance gaps in low-resource languages, signaling a need for future architectures that prioritize linguistic diversity and reduce reliance on a language's digital footprint or Wikipedia presence to achieve competence.

---

## Key Findings

*   **Limited Metalinguistic Knowledge:** Current LLMs possess limited explicit knowledge of language structures. GPT-4o, the state-of-the-art model tested, achieved only moderate accuracy (0.367) and failed to outperform the majority-class baseline.
*   **Domain-Specific Variance:** Accuracy varies significantly across linguistic domains. **Lexical features** showed the highest accuracy, while **phonological features** showed the lowest. This variance is partly attributed to differences in online visibility and representation in training data.
*   **Digital Status Correlation:** There is a strong association between a language's digital status and model accuracy. Languages with a higher digital presence perform significantly better than those with low digital resources.
*   **Predictors of Success:** Resource-related indicators, specifically **Wikipedia size**, are more informative predictors of accuracy than geographical or sociolinguistic factors.

---

## Technical Details

**Evaluation Framework**
*   **Foundation:** Utilizes the World Atlas of Language Structures (WALS) to assess explicit, metalinguistic knowledge.
*   **Scope:** Covers over 2,600 languages.
*   **Domains:** Assesses four distinct areas:
    *   Phonology
    *   Morphology
    *   Lexicon
    *   Syntax

**Methodological Approach**
*   **Metalinguistic Reasoning:** Focuses on prompting for explicit reasoning about language, distinct from surface-level competence.
*   **Analysis Types:**
    *   **Multi-dimensional Analysis:** Examines performance variations across different linguistic domains.
    *   **Predictor Analysis:** Compares the impact of resource-related factors (e.g., Wikipedia size) against geographical and sociolinguistic factors.

---

## Methodology & Results

### Methodology
The researchers employed a rigorous evaluation setup using the following metrics and comparisons:
*   **Evaluation Metrics:** Accuracy and Macro F1 scores.
*   **Baselines:** Performance was measured against both majority-class and chance baselines to ensure statistical significance.
*   **Analysis Dimensions:** The study examined overall performance alongside specific variations across linguistic domains (lexical, phonological, etc.) and language-related factors (digital resources, genealogy).

### Results
The testing phase yielded several critical insights into model behavior:
*   **Baseline Failure:** GPT-4o could not surpass the majority-class baseline, indicating a lack of deep grammatical reasoning.
*   **Visibility Impact:** The discrepancy between lexical (high visibility) and phonological (low visibility) accuracy confirms that models rely on data frequency rather than abstract rules.
*   **Resource Reliance:** Wikipedia size was identified as a dominant predictor, confirming that "resource-rich" languages dominate model performance.
*   **Scaling Trends:** While larger models generally outperform smaller ones, this advantage diminishes for low-resource languages, especially when tasks require abstract reasoning capabilities.

---

## Contributions

This research makes three primary contributions to the field of Natural Language Processing:

1.  **Benchmark Dataset:** Contributed a comprehensive, open-source benchmark dataset designed specifically for evaluating metalinguistic knowledge across a diverse range of the world's languages.
2.  **Empirical Evidence:** Provided evidence that LLMs' metalinguistic knowledge is fragmented. The findings suggest this knowledge is shaped by data availability rather than representing generalizable, innate grammatical competence.
3.  **Focus on Diversity:** Highlighted significant performance gaps in low-resource languages. This underscores the need to develop future LLMs that prioritize greater global linguistic diversity rather than relying solely on high-resource data.

---
*Paper Analysis References: 12 citations*