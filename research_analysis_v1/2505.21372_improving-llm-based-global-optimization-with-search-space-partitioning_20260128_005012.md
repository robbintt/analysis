---
title: Improving LLM-based Global Optimization with Search Space Partitioning
arxiv_id: '2505.21372'
source_url: https://arxiv.org/abs/2505.21372
generated_at: '2026-01-28T00:50:12'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# Improving LLM-based Global Optimization with Search Space Partitioning

*Andrej Schwanke, Large Language, Aaron Klein, Frank Hutter, Fabio Ferreira, Arber Zela, Prior Labs, David Salinas, Lyubomir Ivanov*

***

### ðŸ“Š Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Acronym** | HOLLM (Hierarchical Optimization with Large Language Models) |
| **Quality Score** | 9/10 |
| **References** | 40 Citations |
| **Key Innovation** | Bandit-LLM Synergy & Search Space Partitioning |
| **Primary Domain** | Expensive Blackbox Optimization / AutoML |

***

## Executive Summary

> This research tackles the critical challenge of optimizing expensive blackbox functions using Large Language Models (LLMs). While LLMs have demonstrated strong generative capabilities, they face significant limitations in high-dimensional search spaces and scenarios lacking strong domain-specific priors. Specifically, LLMs often exhibit high sampling bias and sparse coverage, failing to meet the rigorous exploration-exploitation demands of global optimization. This is a substantial barrier in fields like automated machine learning (AutoML) and complex scientific discovery, where traditional methods such as Bayesian Optimization (BO) are computationally expensive and Evolutionary Algorithms (EAs) suffer from the curse of dimensionality.
>
> The authors introduce **HOLLM** (Hierarchical Optimization with Large Language Models), a novel framework that synergizes adaptive spatial partitioning with LLM generation. Technically, HOLLM recursively partitions the search domain using KD-trees, creating localized subregions treated as "meta-arms." It employs a bandit-inspired scoring mechanism to guide region selection, balancing exploitation (via maximum improvement), geometric exploration (via hypervolume), and statistical uncertainty (via UCB-V). Once a region is selected, the LLM is constrained to sample points strictly within the localized bounding box. By focusing the LLM's generative capacity on high-potential subregions rather than the global domain, HOLLM effectively mitigates sampling bias and sparsity without requiring explicit domain knowledge.
>
> In benchmarking experiments using the BBOB suite, HOLLM consistently matched or surpassed leading global optimization methods, including Bayesian Optimization and CMA-ES. On complex functions such as the 8D Rosenbrock and Rastrigin, HOLLM achieved significantly lower Simple Regret ($r_t$) and Cumulative Regret ($R_t$) compared to standard LLM baselines, successfully converging on global optima where global sampling failed to progress. Spatial coverage experiments on 8D hypercubes provided quantitative evidence of this improvement: HOLLMâ€™s partitioning approach reduced the Hausdorff distance ($d_H$) to near-zero, approximating uniform coverage, whereas global LLM sampling maintained a high Hausdorff distance ($d_H > 0.5$), confirming persistent bias. Additionally, in two-minima optimization challenges, HOLLMâ€™s localized bounding box strategy successfully targeted both minima, demonstrating robustness in multimodal landscapes.
>
> The significance of this work lies in demonstrating that LLMs can perform high-quality candidate generation without relying on explicit domain-specific priors, thereby broadening the scope of problems solvable by generative AI. The proposed "Bandit-LLM Synergy" provides a robust new mechanism for managing the exploration-exploitation trade-off in algorithmic search. By proving that restricting LLM sampling to local subregions overcomes the curse of dimensionality, this research establishes a foundational shift in utilizing LLMs for complex mathematical optimization, setting a new precedent for hybridizing classical algorithmic structures with foundation models.

***

## Key Findings

*   **Superior Performance:** The proposed algorithm, HOLLM, consistently matches or surpasses leading global optimization methods on standard benchmarks.
*   **Outperforms LLM Baselines:** HOLLM substantially outperforms existing global LLM-based sampling strategies.
*   **Addresses Limitations:** The method effectively mitigates common LLM struggles in high-dimensional search spaces and scenarios lacking domain-specific priors.
*   **Quality Generation:** The system produces high-quality candidate points within specific search regions without the need for explicit domain knowledge.

***

## Methodology

The researchers propose **HOLLM**, a novel global optimization framework designed for expensive blackbox functions. The approach operates through two distinct stages:

1.  **Search Space Partitioning**
    The search space is divided into promising subregions. These subregions are treated as 'meta-arms' to be evaluated for potential.

2.  **Hybrid Selection and Generation**
    The system utilizes a bandit-inspired scoring mechanism to select subregions, aiming to perfectly balance **exploration** and **exploitation**.
    *   *Action:* Once a region is selected, a Large Language Model (LLM) proposes candidate points specifically within that localized area.

***

## Technical Details

**Algorithm:** HOLLM (Hierarchical Optimization with Large Language Models)
**Type:** Blackbox optimization method combining adaptive spatial partitioning (KD-trees) with LLM generative capabilities.

### 5-Step Workflow

1.  **Partition**
    Uses KD-tree splitting on the dimension with the largest variance to create subregions.
2.  **Score**
    Applies a bandit-inspired utility calculation balancing three factors:
    *   **Exploitation:** Via max improvement.
    *   **Geometric Exploration:** Via hypervolume.
    *   **Statistical Exploration:** Via UCB-V.
3.  **Select**
    Chooses the most promising regions based on the scoring step.
4.  **Sample**
    Generates candidates within subregion bounding boxes using the LLM.
5.  **Evaluate**
    Assesses the performance of the generated points.

**Key Constraint:** The approach restricts LLM sampling to local subregions to mitigate bias and sparsity issues typically found in high-dimensional spaces.

***

## Results

### Performance Metrics
*   **Hausdorff Distance ($d_H$):** Measure of spatial coverage.
*   **Simple Regret ($r_t$):** Error of the best point found.
*   **Cumulative Regret ($R_t$):** Total error over time.

### Experimental Outcomes

*   **Spatial Coverage:** In 2D and 8D hypercube experiments, LLM sampling with partitioning significantly reduced Hausdorff distance and approximated uniform coverage, unlike global sampling which showed high bias ($d_H > 0.5$).
*   **Two-Minima Optimization:** Sampling within specific bounding boxes successfully targeted minima, whereas global sampling failed.
*   **General Benchmarks:** HOLLM matches or surpasses Bayesian Optimization and Evolutionary Algorithms, effectively mitigating the curse of dimensionality.

***

## Contributions

*   **Algorithmic Innovation:** Introduction of HOLLM, a new algorithm that enhances LLM-driven sampling by integrating it with a search space partitioning strategy.
*   **Bandit-LLM Synergy:** A novel mechanism that utilizes bandit-inspired scoring to guide LLM sampling, optimizing the exploration-exploitation trade-off for expensive blackbox optimization.
*   **Reduced Dependency:** Demonstration that high-quality candidate generation can be achieved by LLMs within localized subregions without relying on explicit domain-specific priors, broadening the applicability of LLMs in complex optimization tasks.