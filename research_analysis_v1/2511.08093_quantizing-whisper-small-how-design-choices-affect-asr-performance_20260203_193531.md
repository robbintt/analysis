---
title: 'Quantizing Whisper-small: How design choices affect ASR performance'
arxiv_id: '2511.08093'
source_url: https://arxiv.org/abs/2511.08093
generated_at: '2026-02-03T19:35:31'
quality_score: 9
citation_count: 0
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Quantizing Whisper-small: How design choices affect ASR performance

*Arthur SÃ¶hler; Julian Irigoyen; Andreas SÃ¸eborg Kirkedal*

---

> ### ðŸ“Š Quick Facts
> *   **Subject:** Whisper-small ASR Model
> *   **Primary Method:** Post-Training Quantization (PTQ)
> *   **Best Performing Technique:** Dynamic int8 (via Optimum-Quanto)
> *   **Max Model Compression:** 71% (nf4/int3)
> *   **Optimal Compression:** 57% (Dynamic int8)
> *   **Benchmark Dataset:** LibriSpeech (test-clean, test-other)
> *   **Libraries Analyzed:** PyTorch, Optimum-Quanto, HQQ, bitsandbytes

---

## Executive Summary

The deployment of state-of-the-art Automatic Speech Recognition (ASR) models like OpenAIâ€™s Whisper on resource-constrained edge hardware is currently hindered by substantial memory and computational demands. This research addresses the critical challenge of compressing these large Transformer-based models without sacrificing transcription accuracy. As edge computing grows in importance for latency-sensitive and private applications, the ability to optimize model footprint and inference speed via quantizationâ€”without the prohibitive cost of model retrainingâ€”is a vital engineering necessity.

The study presents a unified, cross-library benchmarking framework that systematically evaluates Post-Training Quantization (PTQ) techniques on the Whisper-small architecture. Unlike previous isolated evaluations, this work disentangles the effects of specific design variablesâ€”including quantization scheme, granularity, and bit-widthâ€”by comparing four distinct libraries: PyTorch, Optimum-Quanto, HQQ, and bitsandbytes.

The evaluation yielded distinct performance trade-offs depending on the quantization strategy. **Dynamic int8 quantization**, particularly via the Optimum-Quanto library, demonstrated the optimal balance, reducing model size by **57%** while paradoxically improving Word Error Rate (WER) compared to the baseline on LibriSpeech benchmarks. In contrast, aggressive low-bit formats (nf4 and int3) achieved higher compression rates of up to 71% but suffered significant accuracy degradation, especially in noisy conditions (test-other). Furthermore, the study revealed that standard Static Quantization methods consistently underperformed, likely due to fundamental incompatibilities with Whisperâ€™s architecture.

This research offers significant practical value for the deployment of ASR technologies in production environments, particularly on edge devices. By validating specific PTQ workflows that maintain or improve accuracy without retraining, the authors provide a clear pathway for engineers to reduce inference costs and model size effectively.

---

## Key Findings

*   **Optimal Trade-off:** Dynamic int8 quantization using the Optimum-Quanto library provides the best balance between efficiency and performance, reducing model size by **57%** while actually improving the Word Error Rate (WER) compared to the baseline.
*   **Static Quantization Limitations:** Static quantization methods underperformed in this study, likely due to incompatibilities with Whisper's Transformer architecture.
*   **Aggressive Compression Trade-offs:** Lower-bit formats such as nf4 and int3 can achieve up to **71%** model compression, but this comes at the cost of accuracy, specifically degrading performance in noisy conditions (test-other).
*   **Efficiency Without Retraining:** Carefully selected Post-Training Quantization (PTQ) methods can significantly lower inference costs and model size without the need for model retraining.

---

## Methodology

The researchers conducted a unified, cross-library evaluation of Post-Training Quantization (PTQ) applied to the Whisper-small model. The study disentangled the effects of specific design variables, including:

*   Quantization scheme
*   Method
*   Granularity
*   Bit-width

The analysis compared four distinct librariesâ€”**PyTorch**, **Optimum-Quanto**, **HQQ**, and **bitsandbytes**â€”and assessed performance using the LibriSpeech dataset across both *test-clean* and *test-other* benchmarks.

---

## Technical Details

*   **Model Architecture:** Whisper-small Transformer-based ASR model.
*   **Optimization Technique:** Post-Training Quantization (PTQ) applied without model retraining.
*   **Evaluated Techniques:**
    *   Dynamic int8
    *   Static Quantization (noted as architecturally incompatible)
    *   Low-bit formats (nf4 and int3)
*   **Primary Tool:** The Optimum-Quanto library was utilized to achieve optimal results.

---

## Results

*   **Dynamic int8:** Achieved a 57% reduction in model size and an **improvement** in Word Error Rate (WER) compared to the baseline.
*   **Low-bit formats (nf4/int3):** Achieved up to 71% model compression but suffered accuracy degradation on the *test-other* dataset.
*   **Static Quantization:** Demonstrated underperformance and incompatibility.
*   **Inference Costs:** PTQ methods significantly lowered computational requirements.

---

## Contributions

*   **Unified Benchmarking:** Provided a comprehensive, cross-library comparison of PTQ techniques on a state-of-the-art ASR model, offering a standardized view of tool performance.
*   **Architecture-Specific Insights:** Clarified how specific quantization design choices interact with the Transformer architecture of Whisper models to affect ASR outcomes.
*   **Edge Deployment Enablement:** Demonstrated a practical pathway for deploying high-accuracy large speech models on resource-constrained edge hardware by validating methods that drastically reduce size and computational demand.

---

**Quality Score:** 9/10