---
title: 'MoLEx: Mixture of Layer Experts for Finetuning with Sparse Upcycling'
arxiv_id: '2503.11144'
source_url: https://arxiv.org/abs/2503.11144
generated_at: '2026-02-03T18:47:35'
quality_score: 7
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# MoLEx: Mixture of Layer Experts for Finetuning with Sparse Upcycling

*Rachel S. Y. Teo; Tan M. Nguyen*

***

> ### üìä Quick Facts
>
> | Metric | Detail |
> | :--- | :--- |
> | **Quality Score** | 7/10 |
> | **References** | 40 Citations |
> | **Paradigm** | Sparse Upcycling / PEFT |
> | **Key Benchmarks** | GLUE (NLU), E2E NLG (Generation) |
> | **Performance Highlight** | 83.8% on GLUE (RoBERTa-large) |
> | **Computational Overhead** | Minimal (Parallel Processing) |

***

## üìù Executive Summary

Fine-tuning large pre-trained language models (PLMs) for specific downstream tasks is computationally expensive and resource-intensive. While Parameter-Efficient Fine-Tuning (PEFT) methods have emerged to mitigate these costs, they often fail to fully leverage the diverse, complementary linguistic features encoded at different depths of a pre-trained network.

The authors introduce **MoLEx (Mixture of Layer Experts)**, a novel framework based on "sparse upcycling" that treats entire pre-trained layers as distinct experts. **Sparse upcycling** is defined here as a technique that reuses pre-trained layers without adding new ones. Technically, MoLEx freezes the original model parameters and employs a shared, learnable router to select a specific "guest" layer at each step using a Top-1 gating mechanism. The output is calculated as a weighted blend of the original layer's transformation and the selected guest layer's output, governed by a learnable scalar.

By training only the router and the mixing scalar, MoLEx functions as a weighted ensemble of layer compositions, theoretically providing robustness against perturbations while enabling conditional computation.

**Performance Highlights:**
*   **GLUE Benchmark:** Using RoBERTa-large, MoLEx achieves an average score of **83.8%**, outperforming strong PEFT baselines like LoRA and BitFit.
*   **Efficiency:** Closely matches full fine-tuning performance while maintaining the parameter count of the original backbone.
*   **Robustness:** Layer probe analysis confirms more well-informed predictions via effective information exchange.

This work establishes sparse upcycling as a new PEFT paradigm, optimizing the trade-off between computational cost and performance without increasing model size.

***

## üîë Key Findings

*   **Superior Performance:** Achieves better fine-tuning accuracy than standard baselines while keeping the effective parameter count constant.
*   **Minimal Overhead:** Introduces negligible computational costs by utilizing parallel processing of layer experts.
*   **Broad Applicability:** Demonstrates strong results on both the GLUE benchmark (NLU) and the E2E NLG Challenge (generation), effectively combining with other PEFT methods.
*   **Enhanced Information Flow:** Generates more well-informed predictions by facilitating information exchange between different layers of the network.

***

## üß† Methodology

The authors propose the **Mixture of Layer Experts (MoLEx)** framework. Unlike traditional sparse mixture of experts (SMoE) models that might replace MLPs, MoLEx treats individual pre-trained model layers as the experts.

The process involves:
1.  **Conditional Computation:** During fine-tuning, the model dynamically selects and mixes specific layers.
2.  **Feature Leverage:** This selection allows the model to leverage different types of linguistic information inherent to different depths.
3.  **Dynamic Exchange:** The architecture enables the dynamic exchange of information across the network depth.

***

## ‚öôÔ∏è Technical Details

MoLEx is a sparse upcycling method designed for Parameter-Efficient Fine-Tuning (PEFT). It views entire pre-trained layers as experts rather than modifying sub-components.

**Architecture & Components:**
*   **Router:** Uses a shared learnable router defined as $g(z_t) = W z_t + b$.
*   **Gating Mechanism:** Utilizes **Top-1 gating** to select a specific "guest" layer, denoted as $v_t$.

**Mathematical Formulation:**
The layer update rule is defined as:

$$z_{t+1} = z_t + \alpha u_t(z_t; \theta^{(0)}_t) + (1 - \alpha) v_t(z_t)$$

*   $z_t$: Input at step $t$
*   $u_t$: Original layer transformation
*   $v_t$: Selected guest layer transformation
*   $\alpha$: A learnable scalar that balances the original and expert layers.

**Training & Theory:**
*   **Parameter Efficiency:** The method reuses original pre-trained parameters ($\theta^{(0)}_t$). Only the router parameters ($W, b$) and the scalar $\alpha$ are trained.
*   **Interpretation:** Theoretically viewed as a weighted ensemble of layer compositions.
*   **Robustness:** Proven to provide $\epsilon$-robustness.

***

## üìà Results

The paper validates the MoLEx framework through rigorous testing on standard NLP benchmarks:

*   **Benchmarks:** Evaluated on GLUE (General Language Understanding Evaluation) and the E2E NLG Challenge (data-to-text generation).
*   **Comparative Performance:** MoLEx outperforms strong PEFT baselines (specifically LoRA and BitFit) and closely matches the performance of full fine-tuning.
*   **Resource Usage:** Maintains the parameter count of the original pre-trained backbone (plus negligible router overhead).
*   **Evaluation Metrics:** Focus included robustness, zero-shot transfer capabilities, and layer probe analysis to verify information utilization.

***

## üöÄ Contributions

*   **New PEFT Paradigm:** Introduces "sparse upcycling," a novel technique for reusing pre-trained layers without architectural expansion.
*   **Theoretical Perspective:** Provides a theoretical foundation viewing layers as extractors of distinct, complementary features.
*   **Resource Optimization:** Optimizes the trade-off between cost and performance without increasing the number of parameters.
*   **Reproducibility:** Publicly releases code to support reproducibility and further research.