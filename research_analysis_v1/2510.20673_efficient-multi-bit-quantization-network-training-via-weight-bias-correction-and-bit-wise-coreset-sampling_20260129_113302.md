# Efficient Multi-bit Quantization Network Training via Weight Bias Correction and Bit-wise Coreset Sampling

*Jinhee Kim; Jae Jun An; Kang Eun Jeon; Jong Hwan Ko*

---

> ### üìä Quick Facts
> *   **Training Speedup:** Up to **7.88x** faster than standard approaches.
> *   **Fine-Tuning:** **Eliminated** completely for intermediate precisions.
> *   **Architecture Support:** Validated on ResNet and Vision Transformers.
> *   **Datasets:** CIFAR-10/100 and ImageNet-1K.
> *   **Improvement vs. SOTA:** Outperforms TDDS by **4.43%** accuracy at 80% pruning rate.
> *   **Quality Score:** 8/10

---

## üìë Executive Summary

Training multi-bit quantization networks presents a significant computational bottleneck in deep learning deployment. Existing methods typically require separate training passes or intensive fine-tuning stages for each target precision (bit-width) to recover accuracy lost during quantization. This linear scaling of training costs with the number of supported precisions is inefficient and resource-prohibitive, hindering the development of versatile models that must operate across diverse hardware constraints with varying computational capabilities.

The authors propose a novel framework combining architectural optimization and data-centric efficiency to decouple training costs from the number of quantization precisions. The first key innovation, **Weight Bias Correction**, mathematically aligns the distribution of quantized weights with full-precision weights using statistical scaling and shifting (defined as $w_q' = \sqrt{\frac{V[w]}{V[w_q]}}(w_q + (E[w] - E[w_q]))$). This correction harmonizes activation statistics, enabling the use of a single shared batch normalization layer across all bit-widths and eliminating the need for precision-specific fine-tuning. The second innovation, **Bit-wise Coreset Sampling**, utilizes gradient-based importance scores to construct dynamic, informative subsets of training data for each bit-width, drastically reducing the data volume required for effective training.

Validated on ResNet and Vision Transformer architectures using CIFAR-10/100 and ImageNet-1K datasets, the proposed framework achieved substantial efficiency gains while maintaining high model utility. The method realized training speed-ups of **7.88x** on CIFAR-10 and **7.61x** on CIFAR-100 compared to standard approaches. Specifically, on CIFAR-10, the model achieved **92.97%** accuracy within 1.52 GPU hours, and on CIFAR-100, it reached **70.26%** accuracy within 1.47 GPU hours. Furthermore, the bit-wise coreset strategy demonstrated superior data efficiency, outperforming the previous state-of-the-art method (TDDS) by **4.43%** in average accuracy at an 80% pruning rate.

This research represents a critical advancement in the practical deployment of quantized neural networks by addressing the prohibitive training overhead associated with multi-bit systems. By removing the necessity for intermediate fine-tuning stages and drastically reducing data requirements through intelligent coreset selection, the methodology offers a scalable pathway to develop flexible, hardware-agnostic models.

---

## üîë Key Findings

*   **Significant Training Efficiency:** Reduces training time by up to **7.88x** compared to standard dedicated training methods.
*   **Elimination of Fine-Tuning:** Removes the need for extra fine-tuning stages for intermediate precisions, streamlining the deployment pipeline.
*   **High Model Utility:** Achieves competitive or superior accuracy across tested benchmarks despite the reduced training overhead.
*   **Broad Applicability:** Validated on modern architectures including **ResNet** and **Vision Transformer**, as well as standard datasets like **CIFAR-10/100** and **ImageNet-1K**.

---

## ‚öôÔ∏è Methodology

The proposed framework utilizes two primary techniques to optimize multi-bit quantization training:

1.  **Weight Bias Correction**
    *   Facilitates shared batch normalization across different bit-widths.
    *   Neutralizes quantization bias by aligning activation distributions.
    *   Ensures that the statistical properties of quantized weights match those of full-precision weights.

2.  **Bit-wise Coreset Sampling**
    *   Constructs compact, informative subsets of training data.
    *   Utilizes gradient-based importance scores to select the most valuable data points.
    *   Maintains model performance while significantly reducing the volume of training data required.

---

## ‚ú® Contributions

*   **Optimization of Multi-bit Training:** Addresses training overhead by decoupling computational cost from the number of supported precisions.
*   **Architectural Innovation via Bias Correction:** Introduces a weight bias correction mechanism that unifies parameter sharing without the need for fine-tuning.
*   **Data-Centric Training Efficiency:** Proposes a novel bit-wise coreset sampling strategy that utilizes gradient-based importance for intelligent data selection.

---

## üî¨ Technical Details

### Framework Overview
The paper introduces a comprehensive training framework for multi-bit quantization networks designed to lower the barrier for deploying flexible quantized models.

### Weight Bias Correction
This component addresses activation distribution mismatches caused by weight quantization biases.
*   **Mechanism:** Directly corrects weights via scale and shift adjustments to align with full-precision weights.
*   **Benefit:** Allows for a single shared batch normalization layer regardless of bit-width.
*   **Formula:**
    $$w_q' = \sqrt{\frac{V[w]}{V[w_q]}}(w_q + (E[w] - E[w_q]))$$
    *(Where $V$ denotes variance and $E$ denotes expectation)*

### Bit-wise Coreset Sampling
This component reduces training overhead by optimizing data usage.
*   **Strategy:** Uses dynamic subsets for each bit-width, which are redrawn periodically to handle temporal drift.
*   **Knowledge Transfer:** Utilizes a sequential bit-wise training scheme; gradients between bit-widths show high alignment (angles < 28 degrees), facilitating implicit knowledge transfer.

---

## üìà Results

### CIFAR-10
*   **With Coreset:** Achieved **92.97%** average accuracy with a training time of **1.52 GPU hours**.
    *   Represents a **7.88x speedup** over Dedicated training methods.
*   **Without Coreset:** Reached **93.46%** accuracy.

### CIFAR-100
*   **With Coreset:** Achieved **70.26%** accuracy with a training time of **1.47 GPU hours**.
    *   Represents a **7.61x speedup** over Dedicated training methods.
*   **Without Coreset:** Reached **71.84%** accuracy.

### Comparative Performance
*   The proposed bit-wise coreset strategy outperformed the previous State-of-the-Art (SOTA) method (**TDDS**) by **4.43%** in average accuracy on CIFAR-10 at an 80% pruning rate.

---

### üìö References & Rating
*   **References:** 40 citations
*   **Quality Score:** 8/10