# Hidden in plain sight: VLMs overlook their visual representations

*Stephanie Fu; Tyler Bonnen; Devin Guillory; Trevor Darrell*

---

> ### ğŸ’¡ Executive Summary
>
> This research identifies a critical disconnect in Vision-Language Models (VLMs): while these models possess powerful internal visual encoders, they frequently fail to leverage this information effectively during end-to-end processing. Standard VLMs often perform adequately on text-heavy tasks but exhibit severe deficiencies in vision-centric benchmarks such as depth estimation and correspondence.
>
> The study introduces a diagnostic **"Testbed" evaluation framework** designed to isolate modality integration capabilities. By contrasting "Standard Visual Evaluation" (direct readouts) against "VLM Evaluation" (full model), the researchers demonstrate a universal and drastic performance degradation. The core conclusion is that the **LLM component is the primary bottleneck**, failing to interpret projected visual tokens and defaulting to language priors rather than grounding responses in visual reality. This work challenges the assumption that scaling up visual encoders automatically leads to better visual understanding, shifting the focus to better projection mechanisms and training alignment.

---

## ğŸ“Š Quick Facts

| Metric | Details |
| :--- | :--- |
| **Quality Score** | 9/10 |
| **References** | 28 Citations |
| **Primary Bottleneck** | Large Language Model (LLM) Component |
| **Key Benchmark** | Depth Estimation & Correspondence |
| **Core Issue** | Zero correlation between encoder strength and VLM output |

---

## ğŸ” Key Findings

*   **Performance Gap:** VLMs perform substantially worse than direct readouts of their visual encoders on vision-centric benchmarks, often dropping to **near-chance levels**.
*   **The LLM Bottleneck:** The primary bottleneck is the language model component, not the visual representations or the task prompts themselves.
*   **Reliance on Priors:** VLMs tend to rely heavily on language priors from the LLM component rather than grounding their responses in the actual visual inputs.
*   **Ineffective Utilization:** Even though high-quality visual representations are accessible within the model, the VLM pipeline fails to leverage them effectively for final resolution.

---

## ğŸ§ª Methodology

The researchers employed a structured comparative analysis framework to isolate integration capabilities:

1.  **Comparative Framework:** Isolated capabilities by comparing end-to-end VLMs against "direct readouts" of their internal visual encoders.
2.  **Benchmarking:** Conducted rigorous testing on vision-heavy benchmarks, specifically **Depth Estimation** and **Correspondence Tasks**.
3.  **Diagnostic Analysis:** The study was broken down into three specific diagnostic parts:
    *   Degradation of vision representations.
    *   Brittleness to variations in task prompts.
    *   The specific role of the language model in resolution.

---

## âš™ï¸ Technical Details

### System Architecture
*   **Vision Encoder:** Pretrained model (e.g., CLIP, DINOv2).
*   **Projector:** Connects visual data to language.
*   **Large Language Model (LLM):** Processes the projected information.

### Evaluation Framework
*   **"Testbed" Protocol:** Designed to isolate visual capabilities using "Vision-Centric Tasks" which are solvable purely by visual input.
*   **Contrasting Modes:**
    *   *Standard Visual Evaluation:* Linear Probing of raw encoder representations.
    *   *VLM Evaluation:* Using the full model with text prompts.

### Failure Analysis Focus
1.  Vision representation quality.
2.  Sensitivity to prompt variations.
3.  The LLM's inherent capability to interpret projected representations.

---

## ğŸ“ˆ Results

The study revealed a universal performance degradation when shifting from visual probing to full VLM evaluation. There was found to be **zero correlation** between standalone encoder performance (e.g., DINOv2) and final VLM performance.

### Performance Degradation Metrics

| Benchmark | Encoder Readout | VLM Performance | Drop |
| :--- | :---: | :---: | :--- |
| **Depth Estimation** | ~0.8 | ~0.2 â€“ 0.3 | Significant |
| **3D Object Awareness** | ~0.8 | ~0.2 | ~75% |
| **Low-level Matching** | >0.8 | ~0.4 â€“ 0.6 | ~30-50% |
| **Object Affordance** | ~0.6 â€“ 0.8 | ~0.4 â€“ 0.5 | Moderate |
| **Semantic Correspondence**| ~0.8 | ~0.0 â€“ 0.2 | Near Chance |

---

## ğŸ† Contributions

*   **Diagnostic Framework:** Identifies the LLM bottleneck as the specific cause of visual grounding failures in open-source VLMs.
*   **Evaluation Protocol:** Presents a benchmark designed to measure actual visual understanding rather than linguistic guessing.
*   **Modality Insights:** Provides critical insights demonstrating that strong visual representations inside a model do not guarantee they are used effectively in the final output.

---

**Paper Rating:** 9/10 | **References:** 28