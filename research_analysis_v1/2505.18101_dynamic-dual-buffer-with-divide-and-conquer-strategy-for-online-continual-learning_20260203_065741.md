---
title: Dynamic Dual Buffer with Divide-and-Conquer Strategy for Online Continual Learning
arxiv_id: '2505.18101'
source_url: https://arxiv.org/abs/2505.18101
generated_at: '2026-02-03T06:57:41'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Dynamic Dual Buffer with Divide-and-Conquer Strategy for Online Continual Learning

*Congren Dai; Huichi Zhou; Jiahao Huang; Zhenxuan Zhang; Fanwen Wang; Yijian Gao; Guang Yang; Fei Ye*

---

> ### ðŸ“Š Quick Facts
> *   **Framework Name:** ODEDM (Online Dynamic Expandable Dual Memory)
> *   **Key Innovation:** Divide-and-Conquer (DAC) Optimization Strategy
> *   **Primary Datasets:** CIFAR10, CIFAR100, TINYIMG
> *   **Top Result (CIFAR10, Buf 500):** 47.3% ACC5 (DER++ w/ ODEDM)
> *   **Scalability:** Tested up to Buffer 5120
> *   **Quality Score:** 9/10

---

## Executive Summary

Online Continual Learning (OCL) faces the critical challenge of catastrophic forgetting, where neural networks lose previously acquired knowledge upon learning new tasks from sequential data streams. This issue is compounded by the constraints of limited memory capacity and the high computational overhead associated with updating memory buffers in real-time, particularly when dealing with imbalanced data distributions. Existing rehearsal-based methods often struggle to balance the retention of semantically rich historical information with the computational cost required to maintain and update these memories efficiently, creating a bottleneck for deploying adaptable, high-performance models in dynamic environments.

To address these limitations, the authors propose the **Online Dynamic Expandable Dual Memory (ODEDM)** framework, a plug-and-play architecture built upon a Dual Memory system and a Divide-and-Conquer (DAC) optimization strategy. ODEDM separates memory into a short-term component for immediate processing and a prototype-anchored long-term memory for sustained knowledge retention. Technically, the framework employs K-means clustering to identify prototypes and utilizes an optimal transport mechanism (Sinkhorn distance) for sample selection, ensuring that semantically critical data is preserved. Crucially, the DAC strategy decomposes the memory update process into smaller, manageable subproblems, significantly reducing computational overhead without sacrificing the integrity of the stored knowledge.

ODEDM achieves state-of-the-art performance across standard and imbalanced OCL benchmarks, outperforming established baselines such as DER++, VR-MCL, and POCL. On CIFAR10 with a buffer size of 500, DER++ enhanced with ODEDM achieved an ACC5 of **47.3%**, a substantial increase over the standard DER++ (39.9%) and CoPE (42.9%). The framework demonstrates effective scalability; with a buffer size of 5120, POCL+ODEDM reached **66.74%** accuracy. Ablation studies further highlight the importance of the prototype strategy, as reducing prototypes to a single instance caused performance to collapse to 7.78% compared to 42.69% with six prototypes.

This research establishes a new efficiency and accuracy standard for rehearsal-based continual learning by demonstrating that complex memory management can be optimized without excessive computational cost. The introduction of the Divide-and-Conquer strategy offers a practical solution to the scalability issues often plaguing OCL systems. Furthermore, because ODEDM functions as a compatible, plug-and-play module, it can be readily integrated into existing architectures to enhance adaptability.

---

## Key Findings

*   **State-of-the-Art Performance:** The proposed ODEDM framework achieves superior results in both standard and imbalanced Online Continual Learning (OCL) settings compared to recent baselines (DER family, VR-MCL, POCL).
*   **Mitigation of Catastrophic Forgetting:** The design effectively preserves knowledge by utilizing a prototype-anchored long-term memory structure that maintains semantically rich information.
*   **Computational Efficiency:** The Divide-and-Conquer (DAC) optimization strategy successfully reduces the computational overhead of memory updates without compromising model performance.
*   **Robustness to Imbalance:** The framework demonstrates strong adaptability in imbalanced data scenarios, ensuring critical information is retained.

---

## Methodology

The authors introduce the **Online Dynamic Expandable Dual Memory (ODEDM)**, a framework designed to manage sequentially arriving data in OCL. The methodology relies on three core components:

1.  **Dual Memory Architecture:** Utilizes a short-term memory for immediate processing and a prototype-anchored long-term memory for stable retention.
2.  **Prototype-Based Management:** Employs a K-means-based approach for prototype identification and an optimal transport-based mechanism (Sinkhorn distance) for sample selection to prioritize semantically critical data.
3.  **Divide-and-Conquer (DAC) Optimization:** Decomposes memory updates into smaller subproblems to significantly reduce overhead.

The framework is designed as a **plug-and-play module** compatible with existing rehearsal-based approaches.

---

## Technical Details

| Component | Description |
| :--- | :--- |
| **Framework Name** | Online Dynamic Expandable Dual Memory (ODEDM) |
| **Core Objective** | Mitigate catastrophic forgetting in OCL via prototype-anchored memory. |
| **Optimization Strategy** | Divide-and-Conquer (DAC) for efficient memory updates. |
| **Distance Metrics** | Utilizes **L2** and **Sinkhorn** distances for distribution alignment. |
| **Scaling Parameter** | **ODEDM-X**: Defines prototype counts, scaling with buffer size (e.g., Buffer 200: 6, 13, 19; Buffer 5120: 160, 320, 480). |
| **Clustering** | K-means for prototype identification. |

---

## Results

The framework was evaluated on CIFAR10, CIFAR100, and TINYIMG using ACC5 and ACC Task-IL metrics.

### Performance Highlights

*   **CIFAR10 (Buffer 500):** DER++ w/ ODEDM achieved **47.3%** ACC5, outperforming standard DER++ (39.9%) and CoPE (42.9%).
*   **Scalability (Buffer Size):**
    *   Buffer 200: 42.69%
    *   Buffer 500: 47.31%
    *   Buffer 5120: 54.49% (DER++) / 66.74% (POCL+ODEDM)
*   **Imbalanced Settings:** On imbalanced CIFAR10, POCL (ODEDM-160) scored **62.01%**.
*   **Task-IL Results:**
    *   CIFAR10 (Buffer 5120): POCL (ODEDM) reached **92.60 Â± 1.96**.
    *   CIFAR100 (Buffer 5120): POCL (ODEDM) reached **72.64 Â± 1.17**.

### Ablation Studies
Studies confirmed the necessity of sufficient prototypes:
*   **ODEDM-1:** Performance dropped significantly to **7.78%**.
*   **ODEDM-6:** Performance recovered to **42.69%**.

---

## Contributions

*   **Novel Architecture:** Introduction of ODEDM, a new dual-memory architecture that dynamically expands and organizes memory using cluster prototypes to handle diverse data streams.
*   **Advanced Retention Strategy:** Development of a strategy combining K-means for prototype identification and optimal transport for sample prioritization.
*   **Optimization Proposal:** Proposal of the Divide-and-Conquer (DAC) strategy to optimize the memory update process for better computational feasibility.
*   **Benchmark Establishment:** Demonstration of superior adaptability and accuracy across multiple datasets, setting a new state-of-the-art standard for rehearsal-based OCL.

---
**References:** 40 citations