# Separating the what and how of compositional computation to enable reuse and continual learning

*Haozhe Shan; Sun Minni; Lea Duncker*

---

### ðŸ“Š Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Quality Score** | 8/10 |
| **References** | 40 Citations |
| **Efficiency** | 50% fewer parameters than unconstrained baselines |
| **Tasks Evaluated** | 6 Neuroscience tasks |
| **Algorithm** | Unsupervised Online Incremental EM |

---

## Executive Summary

> **The Challenge:** Standard Recurrent Neural Networks (RNNs) face "catastrophic forgetting" in continual learning, where new skills overwrite old ones. Unlike biological systems, they also struggle with compositional generalizationâ€”recombining known primitives to solve novel problems.
>
> **The Solution:** This paper introduces a novel "two-system" architecture that strictly separates the inference of task context (**"What"**) from the execution of computations (**"How"**).
> *   The **"What"** system uses a probabilistic generative model with unsupervised online learning (Incremental EM) to infer the latent computational context.
> *   The **"How"** system is a Contextually Modulated Low-Rank RNN that dynamically composes parameters based on the "What" system's output.
>
> **The Outcome:** Evaluated across six complex neuroscience tasks, the framework achieved near-perfect performance (~1.0) without catastrophic forgetting, regardless of task order. It outperformed standard baselines (Adam, EWC, OWP) while using half the parameters, offering a mechanistic solution to the stability-plasticity dilemma.

---

## Key Findings

*   **Successful Continual Learning:** The two-system framework enables RNNs to learn new tasks sequentially without catastrophic forgetting.
*   **Efficient Context Inference:** The 'what' system incrementally builds a vocabulary of task epochs and infers latent computational context via unsupervised online learning.
*   **Compositional Generalization:** The model achieves fast generalization to unseen tasks by reusing low-rank RNN components.
*   **Knowledge Transfer:** The system demonstrates competitive performance and facilitates both forward and backward transfer of skills.

---

## Methodology

The research utilizes a novel **two-system architecture** applied to RNNs. This approach mirrors biological cognitive processes by separating identification from action.

### The "What" System (Inference)
*   **Function:** Identifies task structure and infers context.
*   **Mechanism:** Utilizes a probabilistic generative model trained via an **unsupervised online Incremental EM algorithm**.
*   **Goal:** To determine *what* task is currently being performed based on input data.

### The "How" System (Implementation)
*   **Function:** Executes computations based on the inferred context.
*   **Mechanism:** An RNN that uses **dynamically composed low-rank components**.
*   **Goal:** To determine *how* to perform the task by assembling the appropriate neural machinery provided by the "What" system.

---

## Technical Details

### Architecture Overview
The paper proposes a strict separation between computational context and implementation:

| Component | Technical Specification |
| :--- | :--- |
| **Generative Task Model** | A probabilistic graphical model with latent variables for task epochs (`$z_t$`) and trial conditions (`$x$`). |
| **Training Algorithm** | Unsupervised online Incremental Expectation-Maximization (EM). |
| **Implementation** | Contextually Modulated Low-Rank RNN. |
| **Dynamic Weighting** | Recurrent weights (`$W^{rec}$`) are dynamically computed as weighted sums of low-rank context-specific components. |
| **Input Source** | Weights are derived from posterior beliefs generated by the "What" system. |

### Evaluation Protocol
*   **Tasks:** 6 neuroscience tasks.
*   **Conditions:** Variable epoch durations and trial conditions to test robustness.
*   **Baselines Compared:** General RNN (Adam/EWC/OWP), Neuromodulated RNN, Hypernetwork RNN.

---

## Results

*   ** continual Learning:** The framework successfully maintained high test performance (near **1.0**) across all tasks, irrespective of presentation order.
*   **Baseline Comparison:** Significantly outperformed baselines (General RNN, Neuromodulated RNN, Hypernetwork RNN) which exhibited sharp performance degradation or capacity limitations.
*   **Parameter Efficiency:** Achieved superior results using only **half the parameters** of unconstrained General RNN baselines.
*   **Inference Robustness:** Demonstrated the ability to infer latent contexts robustly using only input data.

---

## Contributions

*   **Architectural Innovation:** Introduces a conceptual separation of inference ('what') and implementation ('how') to facilitate skill reuse.
*   **Mechanism for Computation Reuse:** Demonstrates flexible creation, learning, and reuse of computational primitives via generative models and low-rank RNNs.
*   **Solution to Catastrophic Forgetting:** Provides a mechanistic solution for continual learning where explicit contextual inference protects prior knowledge.
*   **Modeling Compositionality:** Offers a systematic framework to describe compositionality in cognitive tasks, bridging neuroscience and AI.