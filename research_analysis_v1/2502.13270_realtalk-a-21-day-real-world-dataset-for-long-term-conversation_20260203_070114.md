---
title: 'REALTALK: A 21-Day Real-World Dataset for Long-Term Conversation'
arxiv_id: '2502.1327'
source_url: https://arxiv.org/abs/2502.13270
generated_at: '2026-02-03T07:01:14'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# REALTALK: A 21-Day Real-World Dataset for Long-Term Conversation

*Dong-Ho Lee; Adyasha Maharana; Jay Pujara; Xiang Ren; Francesco Barbieri*

---

> ### ðŸ“Š Quick Facts
> *   **Dataset Duration:** 21 Days
> *   **Participants:** 10 native US speakers (aged 18â€“25)
> *   **Volume:** Avg. 894.4 turns, 21.9 sessions, and 17,109.8 tokens per conversation
> *   **Data Type:** Multimodal (Text + Internet-sourced images)
> *   **Annotations:** 728 Memory Probing QA pairs
> *   **Benchmark Tasks:** Persona Simulation & Memory Probing

---

## Executive Summary

Conversational AI has been hampered by a reliance on short-term, synthetic datasets that fail to capture the nuances of human interaction, creating a "reality gap" where large language models (LLMs) struggle to sustain long-term relationships. While LLMs excel at generating coherent text in controlled settings, they often lack the emotional intelligence, stable persona consistency, and long-term memory required for interactions spanning weeks or months. This research addresses the critical scarcity of authentic, longitudinal data, which is essential for training agents capable of maintaining coherent and context-aware interactions over extended periods, thereby moving beyond the limitations of current benchmarks.

To bridge this gap, the authors introduce **REALTALK**, a novel, large-scale corpus consisting of authentic, 21-day messaging exchanges between 10 native US speakers. This multimodal dataset includes both text and internet-sourced images, with participants required to exchange a minimum of 50 messages daily to ensure high verbosity. The researchers established a specialized evaluation framework comprising two benchmark tasks: **Persona Simulation**, which tests a model's ability to adopt a specific user's identity based on chat history, and **Memory Probing**, which assesses information retrieval through 728 manually annotated QA pairs involving multi-hop reasoning, temporal analysis, and commonsense.

The dataset demonstrates substantial improvements in scale and density over previous benchmarks. Comparative analysis against LLM-generated synthetic dialogues, specifically referencing LoCoMo, revealed that real-world interactions exhibit significantly higher emotional diversity and persona stability. Evaluation of current state-of-the-art models highlighted significant performance gaps: while models struggle to simulate specific users based solely on dialogue history, fine-tuning on specific user chats was shown to improve persona emulation. However, models continued to face marked difficulties in recalling and leveraging long-term context, particularly in tasks requiring temporal reasoning and multi-hop memory retrieval.

---

## Key Findings

*   **Emotional & Persona Superiority:** Real-world dialogues exhibit significantly more diverse emotional expressions and persona stability compared to LLM-generated synthetic dialogues.
*   **Simulation Challenges:** Current models struggle to simulate specific users based solely on dialogue history; however, fine-tuning on specific user chats improves persona emulation.
*   **Memory Deficits:** Existing models face significant difficulties in recalling and leveraging long-term context within authentic conversation scenarios.

---

## Methodology

The research methodology focused on constructing the **REALTALK corpus**, a collection of authentic messaging app dialogues spanning 21 days. The team conducted a comparative analysis between this real-world data and LLM-generated conversations, focusing specifically on:

1.  **Emotional Intelligence Attributes**
2.  **Persona Consistency**

Additionally, the authors defined two specific benchmark tasks to evaluate model performance:
*   **Persona Simulation:** Testing the ability to continue a conversation as a specific user.
*   **Memory Probing:** Testing the ability to retrieve information from past interactions.

---

## Contributions

*   **The REALTALK Dataset:** A novel, large-scale corpus of real-world, long-term conversations addressing the scarcity of non-synthetic data.
*   **Analytical Framework:** A detailed comparative analysis characterizing the challenges of real-world dialogue, specifically emotional diversity and persona consistency.
*   **Evaluation Benchmarks:** Two standardized tasks (Persona Simulation and Memory Probing) providing a rigorous framework for assessing long-term dependency and user emulation.

---

## Technical Details

### Dataset Composition
The REALTALK dataset is designed to be high-density and multimodal, capturing the intricacies of daily digital communication.

| Feature | Specification |
| :--- | :--- |
| **Participants** | 10 native US speakers (aged 18â€“25) |
| **Duration** | 21-day period |
| **Volume Requirement** | Minimum of 50 messages exchanged daily |
| **Content Type** | Casual, friendly small talk; Multimodal (text and internet-sourced images) |

### Annotation Framework
To enable rigorous testing, the dataset includes a comprehensive annotation scheme:

*   **Memory Probing QA Pairs:** 728 total pairs, categorized into:
    *   **MULTI-HOP**
    *   **TEMPORAL REASONING**
    *   **COMMONSENSE**
*   **Event Annotations:** Documentation of past, future, or ongoing events per session.

### Evaluation Setup
The evaluation framework compares real-world dialogues directly against LLM-simulated ones (referencing the LoCoMo benchmark). Metrics are derived to measure:
1.  Emotional Intelligence
2.  Persona Consistency
3.  Long-term Context Retrieval

---

## Results

REALTALK achieves significantly higher verbosity than previous benchmarks. The dataset is confirmed as a multimodal, crowdsourced resource with the following average statistics per conversation:

*   **894.4 turns**
*   **21.9 sessions**
*   **17,109.8 tokens**

The study confirms that real-world dialogues exhibit significantly more diverse emotional expressions and persona stability compared to LLM-generated synthetic dialogues. While fine-tuning assists in persona emulation, current models still struggle to simulate specific users without it and face marked difficulties in recalling and leveraging long-term context in authentic scenarios.

---

**Quality Score:** 8/10  
**References:** 40 citations