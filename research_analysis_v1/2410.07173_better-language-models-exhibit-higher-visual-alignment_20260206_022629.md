---
title: Better Language Models Exhibit Higher Visual Alignment
arxiv_id: '2410.07173'
source_url: https://arxiv.org/abs/2410.07173
generated_at: '2026-02-06T02:26:29'
quality_score: 8
citation_count: 25
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Better Language Models Exhibit Higher Visual Alignment

*Jona Ruthardt; Gertjan J. Burghouts; Serge Belongie; Yuki M. Asano*

---

> ### ðŸ“Š Quick Facts
> * **Quality Score:** 8/10
> * **Training Efficiency:** Under 1 GPU-hour
> * **Data Efficiency:** 563k image-caption pairs
> * **ImageNet Accuracy:** 51%
> * **Cross-Lingual Dominance:** 38.7% (Chinese) vs 1.4% (CLIP)

---

## Executive Summary

Current state-of-the-art Vision-Language (V-L) models, such as CLIP, typically rely on massive datasets and extensive computational resources for training, creating high barriers to entry and inefficiency. Furthermore, there is a lack of systematic understanding regarding which architectural characteristics of text-only Large Language Models (LLMs) translate most effectively to visual grounding. Specifically, the field lacks clarity on how general language capabilities correlate with visual generalization and how existing models can be adapted to handle cross-lingual scenarios without suffering significant performance degradation.

This paper addresses these challenges by investigating the intrinsic alignment between language and vision and seeking a method to leverage this alignment without prohibitive training costs. The core innovation is **"ShareLock"** (Shared Vision-Language-Locked Tuning), a lightweight fusion framework designed to maximize the utility of pre-trained backbones.

The method employs a late-fusion architecture where both the vision and language backbones remain frozen; only a single, small learnable projection network is optimized. By utilizing a symmetric contrastive loss function with a fixed temperature, ShareLock aligns the frozen representations of image and text encoders. This allows for the integration of powerful, pre-trained text-only LLMs with vision encoders without the need for expensive end-to-end fine-tuning or large-scale paired image-text data.

The study demonstrates that decoder-based LLMs exhibit superior visual alignment compared to encoder-based models, with a Pearson correlation coefficient of **0.768** between general language capability (MMLU-Pro score) and visual generalization performance. The ShareLock method achieved remarkable efficiency and performance, reaching 51% accuracy on ImageNet using only 563k image-caption pairs and less than one GPU-hour of training. Furthermore, the model showed massive improvements in cross-lingual transfer learning, achieving 38.7% top-1 accuracy on Chinese image classification compared to CLIPâ€™s 1.4%, and demonstrated above-CLIP performance in fine-grained compositional reasoning tasks.

This research significantly impacts the field by decoupling high-performance V-L modeling from massive computational and data requirements. The findings establish that better language models are naturally better aligned with the visual world, suggesting that future research should prioritize scaling language capabilities over simply hoarding larger paired datasets.

---

## Key Findings

*   **Architecture Matters:** Decoder-based language models exhibit significantly stronger visual alignment compared to encoder-based models.
*   **Performance Correlation:** There is a direct correlation between language modeling performance and visual generalization capabilities.
*   **Extreme Efficiency:** The proposed "ShareLock" method achieves robust performance with drastically reduced data requirements, reaching **51% accuracy on ImageNet** using only **563k image-caption pairs** and **under one GPU-hour** of training.
*   **Cross-Lingual Superiority:** ShareLock dramatically outperforms standard models like CLIP in cross-lingual settings, achieving **38.7% top-1 accuracy** on Chinese image classification compared to CLIPâ€™s 1.4%.
*   **Compositional Reasoning:** The model exhibited 'above-CLIP' performance in fine-grained compositional reasoning tasks.

---

## Methodology

The researchers utilized a discriminative vision-language framework to systematically evaluate visual alignment. This involved incorporating frozen representations from various text-only large language models (LLMs) to measure zero-shot generalization to novel concepts.

Based on the observation that better language models align better with the visual world, they developed **"ShareLock"**. This is a lightweight fusion method designed to integrate frozen vision and language backbones without requiring extensive fine-tuning or large-scale paired data.

---

## Technical Details

| Component | Specification |
| :--- | :--- |
| **Method Name** | Shared Vision-Language-Locked Tuning (ShareLock) |
| **Architecture** | Late-fusion architecture with frozen vision and language backbones |
| **Trainable Parameters** | Only a single learnable projection network is optimized |
| **Loss Function** | Symmetric contrastive loss function with a fixed temperature |
| **Inference Strategy** | Maximizing similarity between image embeddings and projected text embeddings |
| **Evaluation Protocol** | Strict zero-shot generalization using disjoint training and testing classes (no concept leakage) |

---

## Contributions

1.  **Systematic Evaluation:** Provided the first systematic evaluation of how well text-only LLMs align with the visual world, identifying decoder architectures and language modeling performance as key predictors of visual success.
2.  **Method Innovation:** Introduced "ShareLock," a novel method that minimizes the need for expensive paired image-text data and compute resources while maintaining high performance.
3.  **Solving the Language Barrier:** Demonstrated that fusing strong LLMs with vision backbones via ShareLock effectively solves the "language barrier" in vision-language models, evidenced by massive performance gains in non-English image classification compared to existing state-of-the-art models like CLIP.

---

## Results

*   **Data Efficiency:** ShareLock achieved 51% accuracy on ImageNet using only 563k image-caption pairs and under one GPU-hour of training.
*   **Cross-Lingual Transfer:** Significantly outperformed standard CLIP in cross-lingual tasks, achieving 38.7% top-1 accuracy on Chinese Image Classification compared to CLIP's 1.4%.
*   **Model Architecture:** Decoder-based language models demonstrated stronger visual alignment than encoder-based models.
*   **Statistical Correlation:** A Pearson correlation coefficient of 0.768 was found between general language capability (MMLU-Pro score) and visual generalization performance.

---

**References:** 25 citations