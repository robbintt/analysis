# A Survey of Adversarial Defenses in Vision-based Systems: Categorization, Methods and Challenges

*Nandish Chattopadhyay; Abdul Basit; Bassem Ouni; Muhammad Shafique*

---

> ### **Quick Facts**
> **Quality Score:** 9/10  
> **References:** 40 Citations  
> **Focus:** Image Classification & Object Detection  
> **Review Type:** 'Systematic Literature Review 

---

## Executive Summary

Adversarial attacks pose a critical threat to the trustworthiness of vision-based machine learning systems, exploiting vulnerabilities through white-box, black-box, and physical-world manipulation methods. As these attacks grow in sophistication, the landscape of available mitigation strategies has become increasingly fragmented across both image classification and object detection tasks. While various defenses exist, they often possess inherent limitations and fail to provide comprehensive protection against the full scope of adversarial threats required for practical deployment. This paper addresses the urgent need to organize and analyze this disparate field to understand why current defenses are insufficient for securing regular and trustworthy AI systems.

The primary innovation of this work is a comprehensive systematization of knowledge that establishes a structured taxonomy for adversarial defense mechanisms. The authors conduct a systematic literature review to categorize defenses into three distinct classes: **Model Modification** (e.g., Defensive Distillation, Parseval Networks), **Pre-Processing/Sample Modification** (e.g., ComDefend, Feature Distillation), and **Defenses Against Patches** (e.g., ODDR). This taxonomy is contextualized within a schematic representation of the entire machine learning pipeline, allowing for effective benchmarking across different vision tasks. Furthermore, the authors provide a practical mapping that associates specific defense methods with the adversarial attack types and datasets where they are most effective, creating a framework for structured comparison and analysis.

The survey synthesizes reported quantitative performance data from the literature, indicating that specific defense strategies offer robustness in standard testing scenarios, though their efficacy against adaptive attacks varies. According to the surveyed studies, **Defensive Distillation** proved highly effective in reducing adversarial success rates to less than 0.5% and gradient sensitivity by a factor of $10^{30}$. In the pre-processing domain, **ComDefend** reportedly surpassed state-of-the-art performance on MNIST, CIFAR10, and ImageNet, while **Feature Distillation** achieved high efficiency with minimal accuracy loss. For object detection and patch-based attacks, **ODDR** is reported to maintain clean accuracy with only a 1-2% reduction, and **Local Gradient Smoothing** demonstrated high resistance against Backward Pass Differentiable Approximation (BPDA) attacks on ImageNet. Additionally, **Parseval Networks** were found to outperform vanilla models on CIFAR and SVHN datasets.

This research significantly influences the field by providing a cohesive architecture for evaluating and understanding the state of adversarial defense across classification and object detection. The gap analysis presented underscores the reality that while current defenses mitigate specific risks, they are not yet sufficient for fully robust, trustworthy AI systems, particularly when facing adaptive adversaries. By mapping the limitations of existing technologies against specific attack vectors, this survey directs future research toward the development of more comprehensive solutions. It serves as an essential resource for technical stakeholders aiming to implement security measures in vision systems, highlighting the necessary research directions to bridge the divide between theoretical defense mechanisms and practical deployment.

---

## Key Findings

*   **Threat Diversity:** Adversarial attacks threaten trustworthy machine learning through diverse vectors, including white-box, black-box, and physical-world manipulation.
*   **Fragmented Landscape:** The field of adversarial defense is fragmented, with various mitigation approaches available for image classification and object detection, each carrying specific advantages and inherent limitations.
*   **Deployment Gap:** While current defenses offer a degree of protection, they suffer from critical shortcomings in fully addressing the scope of adversarial threats necessary for regular, practical, and trustworthy AI deployment.

---

## Methodology

The authors implemented a rigorous research strategy comprising the following steps:

1.  **Systematization of Knowledge:** Conducted a comprehensive organization of existing research concepts.
2.  **Systematic Literature Review:** Reviewed state-of-the-art defense techniques with a specific focus on image classification and object detection.
3.  **Categorization:** Structured the comparison and analysis by categorizing defense techniques into distinct functional groups.
4.  **Schematic Mapping:** Created a visual representation of defense categories within the context of the overall machine learning pipeline to aid in benchmarking.
5.  **Effectiveness Mapping:** Mapped specific defense methods to the types of adversarial attacks and datasets where they demonstrate the highest effectiveness.

---

## Contributions

*   **Structured Taxonomy:** A comprehensive categorization of adversarial defense methods facilitating the comparison and understanding of different techniques.
*   **Contextual Framework:** A schematic representation of defense categories within the broader machine learning pipeline, serving as a benchmarking tool.
*   **Practical Mapping:** A detailed mapping linking defenses to specific adversarial attack types (e.g., white-box, black-box, physical) and relevant benchmark datasets.
*   **Gap Analysis:** An evaluation of current defense scopes and limitations, highlighting necessary research directions for future trustworthy AI systems.

---

## Technical Details

The analysis categorizes defense mechanisms into three primary areas for image classification:

### 1. Model Modification
These approaches alter the model architecture or training process to increase robustness.
*   **Defensive Distillation:** Utilizes softened softmax temperature to reduce model sensitivity to small perturbations.
*   **Parseval Networks:** Constrains Lipschitz constants via Parseval tight frames to stabilize the learning process.
*   **SafetyNet:** Architectures specifically designed to complicate the generation of adversarial examples.

### 2. Pre-Processing and Sample Modification
These techniques clean or modify input data before it reaches the classifier.
*   **ComDefend:** Uses ComCNN and ResCNN networks for compression and reconstruction of inputs to remove perturbations.
*   **Feature Distillation:** Employs JPEG compression combined with defensive quantization and refinement.
*   **Dimension Reduction:** Reduces input complexity to mitigate adversarial noise.

### 3. Defenses Against Patches
Focused on mitigating localized, physically realizable patch attacks.
*   **ODDR:** A pipeline involving fragmentation, segregation, and neutralization of patches.
*   **Local Gradient Smoothing:** Regularizes gradients specifically in noisy regions to smooth out patch effects.
*   **Jujutsu:** A two-stage technique designed to defend against patch attacks.

---

## Results

The analysis highlights specific quantitative outcomes for the surveyed defense methods:

*   **Defensive Distillation:** Achieved a reduction in adversarial success rates to **< 0.5%** and reduced gradients by a factor of **$10^{30}$**.
*   **Parseval Networks:** Outperformed vanilla models on **CIFAR** and **SVHN** datasets.
*   **SafetyNet:** Demonstrated significant resilience against Type I and Type II attacks.
*   **ComDefend:** Surpassed state-of-the-art techniques on **MNIST, CIFAR10, and ImageNet**.
*   **Feature Distillation:** Provided high defense efficiency with minimal loss of accuracy.
*   **ODDR:** Maintained clean accuracy with a minor reduction of only **1-2%**.
*   **Local Gradient Smoothing:** Demonstrated the highest resistance against **BPDA (Backward Pass Differentiable Approximation)** on ImageNet.
*   **Dimension Reduction:** Empirical data supports this as an effective hypothesis for defense.