---
title: 'MUA-RL: Multi-turn User-interacting Agent Reinforcement Learning for agentic
  tool use'
arxiv_id: '2508.18669'
source_url: https://arxiv.org/abs/2508.18669
generated_at: '2026-02-03T13:36:54'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# MUA-RL: Multi-turn User-interacting Agent Reinforcement Learning for agentic tool use

*Weikang Zhao; Xili Wang; Chengdi Ma; Lingbin Kong; Zhaohua Yang; Mingxiang Tuo; Xiaowei Shi; Yitao Zhai; Xunliang Cai*

---

### üìä Quick Facts

| Metric | Details |
| :--- | :--- |
| **Top Model** | MUA-RL-32B (Based on Qwen3) |
| **Key Algorithm** | Group Relative Policy Optimization (GRPO) |
| **User Simulator** | GPT-4o-2024-11-20 |
| **Best Score (TAU2 Retail)** | 67.3 |
| **Best Score (ACEBench)** | 82.5 |
| **Training Epochs** | 25 (on TAU1-Bench data) |

---

## üìù Executive Summary

This research addresses the critical challenge of training reinforcement learning (RL) agents for agentic tool use in environments characterized by dynamic, stochastic, and uncertain user demands. Traditional RL approaches for tool-using agents typically rely on static environments or assume clear, immediate objectives, failing to account for the ambiguity and iterative clarification required in real-world interactions. This limitation creates a "training gap" where agents are not equipped to handle the nuances of multi-turn conversations, leading to suboptimal performance when user intent is not explicitly defined or changes over time.

The authors introduce **MUA-RL (Multi-turn User-interacting Agent Reinforcement Learning)**, a novel framework that integrates an LLM-simulated user directly into the RL training loop to create a dynamic, bidirectional interaction environment. Technically, the framework formulates tasks using tuples $(T, M, O)$ representing tool sets, messages, and observations, and utilizes the Group Relative Policy Optimization (GRPO) algorithm. A key technical contribution is the implementation of a "loss mask strategy" that excludes tool execution results and user messages from the loss calculation; this forces the model to focus its learning capacity on decision-making and communication strategies rather than content generation.

Evaluations based on Qwen3 models (8B, 14B, and 32B) demonstrate that the MUA-RL-32B model achieves competitive performance, outperforming or matching significantly larger open-source models in non-thinking settings. MUA-RL represents a significant methodological shift in agentic AI by successfully bridging the gap between static tool execution and dynamic user interaction, establishing a new paradigm for training agents that prioritize proactive communication to resolve uncertainty.

---

## üîë Key Findings

*   **Competitive Performance:** The **MUA-RL-32B** model achieved strong benchmark scores, including **67.3** on TAU2 Retail, **45.4** on TAU2 Airline, and **82.5** on ACEBench Agent.
*   **Efficiency:** The model outperformed or matched significantly larger open-source models, particularly in non-thinking settings, demonstrating high parameter efficiency.
*   **Multi-turn Interaction:** The framework enables effective multi-turn interaction capabilities, allowing agents to refine user understanding and resolve complex queries iteratively.
*   **Simulated User Efficacy:** The integration of LLM-simulated users (GPT-4o) proved effective in bridging the interaction gap found in existing static RL approaches.

---

## üß© Methodology

The researchers introduced **MUA-RL**, a reinforcement learning framework designed to train agents in agentic tool use scenarios.

*   **Simulated Environment:** The core innovation is the integration of an LLM-simulated user directly into the training loop. This allows the model to train autonomously within dynamic environments.
*   **Stochastic Demands:** Unlike static approaches, MUA-RL focuses on environments characterized by stochastic and uncertain user demands.
*   **Iterative Problem Solving:** The methodology emphasizes efficient communication to refine understanding while simultaneously invoking tools, relying on iterative, communication-based problem solving.

---

## ‚öôÔ∏è Technical Details

**Framework Architecture**
*   **Task Formulation:** Tasks are formulated using tuples $(T, M, O)$ representing:
    *   $T$: Tool sets
    *   $M$: Messages
    *   $O$: Observations
*   **Loss Mask Strategy:** A specific strategy is employed that excludes tool execution results and user messages from the loss calculation. This ensures the model focuses learning on **decision-making** rather than content generation.

**Training Pipeline**
1.  **Cold-start Supervised Fine-Tuning (SFT):** Initial training phase using ~2,000 trajectories (Batch size: 128, Peak LR: 5e-6).
2.  **RL Training:**
    *   **Algorithm:** Group Relative Policy Optimization (GRPO) within a VolcEngine RL (VeRL) environment.
    *   **Data:** Utilizes TAU1-Bench data for 25 epochs.
    *   **Hyperparameters:** Batch size of 32, KL coefficient of 0.001, maximum of 30 interaction turns.
    *   **Reward Mechanism:** Sparse reward based solely on ultimate task completion.

**Data Synthesis**
*   **LLM-Simulated Execution:** Involves distinct roles for Agent, User, and Tool LLMs.
*   **Model Context Protocol (MCP):** Supports execution via MCP standards.
*   **Simulator:** The user simulator is powered by **GPT-4o-2024-11-20**.

---

## üèÜ Contributions

*   **Novel Framework:** Introduction of MUA-RL, the first framework in the field to incorporate LLM-simulated users into the RL training cycle.
*   **Solving Dynamic Demands:** Successfully addresses the challenge of dynamic and uncertain user demands through iterative, communication-based problem solving.
*   **Bridging the Gap:** Addresses the limitation of existing RL approaches by bridging the training gap regarding dynamic user interaction.

---

## üìà Experimental Results

The models were evaluated based on the Qwen3 architecture across three sizes (8B, 14B, and 32B).

| Benchmark | Metric | Score (MUA-RL-32B) |
| :--- | :--- | :--- |
| **TAU2** | Retail | **67.3** |
| **TAU2** | Airline | **45.4** |
| **ACEBench** | Agent | **82.5** |

**Training Configuration:**
*   Base Models: Qwen3-8B, Qwen3-14B, Qwen3-32B
*   RL Duration: 25 Epochs on TAU1-Bench data
*   Interaction Limit: Max 30 turns per interaction

---

**Quality Score:** 8/10  
**References:** 40 citations