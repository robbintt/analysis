# Selective Sinkhorn Routing for Improved Sparse Mixture of Experts

*Duc Anh Nguyen; Huu Binh Ta; Nhuan Le Duc; Tan M. Nguyen; Toan Tran*

---

> ### ðŸ“Š Quick Facts
>
> *   **Quality Score:** 9/10
> *   **Citations Referenced:** 40
> *   **WikiText-103 Perplexity:** 19.58
> *   **CIFAR-100 Accuracy:** 81.33%
> *   **Training Overhead:** Activated on only 0.1% â€“ 1% of steps
> *   **Extra Parameters:** None

---

## Executive Summary

**Problem**
Sparse Mixture-of-Experts (SMoE) models suffer from "routing collapse," an instability where the gating mechanism fails to distribute tokens effectively. This causes a few experts to dominate computation, degrading performance. Existing solutions often rely on auxiliary balancing losses, which introduce difficult-to-tune hyperparameters, increase training overhead, and frequently cause objective misalignment with the primary task.

**Innovation**
The authors propose **Selective Sinkhorn Routing (SSR)**, formulating token-to-expert assignment as an Entropy-Regularized Optimal Transport problem solved via the Sinkhorn-Knopp algorithm. Instead of relying solely on hard index selection, the method derives gating scores directly from the transport map values. Crucially, SSR uses a concurrent training strategy, applying the routing mechanism selectively to only **0.1% to 1% of training steps** per epoch. This enforces balanced expert utilization through marginal constraints without auxiliary loss functions.

**Results**
SSR achieves superior performance and robustness across language modeling and image classification benchmarks without adding trainable parameters. The method demonstrates faster convergence than baselines, achieving a perplexity of **19.58 on WikiText-103** and **81.33% accuracy on CIFAR-100**. The research highlights that while noise injection tricks benefit training in other methods, they often degrade inference performance; SSR bypasses this issue, maintaining robustness without noise-based regularization.

**Impact**
This research provides a mathematically principled solution to routing collapse that eliminates heuristic auxiliary losses. By significantly reducing the computational overhead associated with Optimal Transport and aligning the routing objective directly with the task, SSR offers a robust, parameter-efficient architecture, shifting the focus from loss term engineering to architectural optimization.

---

## Key Findings

*   **Enhanced Performance via OT:** Incorporating a minimal degree of Optimal Transport (OT)-based routing enhances Sparse Mixture-of-Experts (SMoE) performance without auxiliary balancing losses or extra trainable parameters.
*   **Direct Gating Derivation:** Deriving gating scores directly from the transport map enables more effective token-to-expert balancing.
*   **Superior Metrics:** The method achieves faster training convergence, higher accuracy, and increased robustness across both language modeling and image classification tasks.
*   **Lightweight Alternative:** It offers a significantly reduced training overhead compared to previous Sinkhorn-based methods.

---

## Methodology

*   **Optimal Transport Formulation:** The authors formulate the token-to-expert assignment problem as an Optimal Transport (OT) problem.
*   **Inherent Constraints:** Specific constraints are incorporated into the OT formulation to inherently ensure balanced expert utilization.
*   **SSR Mechanism:** A mechanism called Selective Sinkhorn Routing (SSR) is proposed to replace auxiliary loss functions with a lightweight Sinkhorn-based algorithm. This manages token assignments while preserving flexibility.

---

## Contributions

*   **Theoretical Framework:** Provides a theoretical framework by formulating token-to-expert assignment as an optimal transport problem.
*   **Simplified Architecture (SSR):** Proposes Selective Sinkhorn Routing (SSR), which removes the need for auxiliary losses and noisy gating parameters, effectively solving objective misalignment issues.
*   **Overhead Reduction:** Addresses limitations of existing Sinkhorn methods by significantly reducing training overhead while simultaneously improving model robustness against input corruption and overall accuracy.

---

## Technical Details

The approach introduces **Selective Sinkhorn Routing (SSR)** to address routing collapse in Sparse Mixture-of-Experts (SMoE) without auxiliary balancing losses.

1.  **Entropy-Regularized OT:** The method formulates token-to-expert assignment as an Entropy-Regularized Optimal Transport problem solved via the Sinkhorn-Knopp algorithm.
2.  **Objective Function:** The optimization targets cost and entropy subject to marginal constraints ensuring a balanced distribution.
3.  **Transport Map Innovation:** The key innovation involves using the values of the transport map to calculate routing weights instead of just hard selection of indices.
4.  **Concurrent Training Strategy:** A concurrent training strategy is utilized where **Softmax** trains the gating weight matrices ($W_g$) while SSR handles assignment.
5.  **Selective Application:** The algorithm is applied selectively to only **0.1% to 1%** of training steps per epoch to ensure efficiency.

---

## Results

*   **Efficiency:** SSR is reported as a lightweight alternative requiring activation on only 0.1% to 1% of steps for performance gains.
*   **Performance:** Achieves faster training convergence and higher accuracy in Language Modeling (WikiText-103) and Image Classification (CIFAR-100).
*   **Robustness:** Demonstrates increased robustness to input corruption.
*   **Parameter Efficiency:** Achieves results without adding extra trainable parameters or auxiliary loss functions.
*   **Noise Trade-off:** An observation noted is that noise addition tricks beneficial for training in other methods have a negative impact on inference performance; SSR avoids this pitfall.

---
**References:** 40 citations