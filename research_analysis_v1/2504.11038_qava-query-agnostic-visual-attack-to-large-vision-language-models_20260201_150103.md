# QAVA: Query-Agnostic Visual Attack to Large Vision-Language Models

*Yudong Zhang; Ruobing Xie; Jiansheng Chen; Xingwu Sun; Zhanhui Kang; Yu Wang*

***

> ### ðŸ“„ Quick Facts
>
> | Metric | Detail |
> | :--- | :--- |
> | **Quality Score** | 7/10 |
> | **References** | 20 Citations |
> | **Target Models** | BLIP-2, InstructBLIP (White-box); LLaVA, MiniGPT-4 (Transferability) |
> | **Attack Types** | PGD-l_inf, C&W-l_2 |
> | **Dataset** | VQA v2 Validation Set ('32+50' subset) |

***

## Executive Summary

Current adversarial attack methodologies for Large Vision-Language Models (LVLMs) suffer from fundamental brittleness because they rely on optimizing perturbations against a specific, known image-question pair. In practical deployment scenariosâ€”such as an attacker uploading a malicious image to a public platformâ€”the attacker cannot predict the specific textual queries users will eventually ask. Consequently, existing attacks fail to induce errors when the query deviates from the one used during optimization, causing model accuracy to remain stable despite the attack. This limitation renders current vulnerability assessments incomplete, as models appear robust only under unrealistic, controlled conditions where the prompt is known ahead of time, masking a significant security blind spot.

The authors propose **QAVA (Query-Agnostic Visual Attack)**, a framework that generates adversarial perturbations independent of the textual query. Technically, QAVA targets the Q-former alignment module found in models like BLIP-2 and InstructBLIP. Rather than optimizing a loss based on a specific answer, QAVA maximizes the Mean Squared Error (MSE) between the Q-former features of clean and perturbed images. This disrupts the internal visual representations sufficiently to cause incorrect responses regardless of the input text. To ensure the perturbation generalizes across diverse user intents, the optimization process employs gradient-based methods (PGD and C&W) guided by surrogate question sampling strategies, which simulate a wide distribution of potential user queries during the generation phase.

The researchers evaluated QAVA on the VQA v2 validation set, targeting BLIP-2 and InstructBLIP in white-box settings and testing transferability on black-box targets like LLaVA and MiniGPT-4. The results demonstrate that QAVA maintains a high Attack Success Rate (ASR) across a wide range of unseen questions, achieving performance parity with "oracle" attacks that possess full foreknowledge of the target question. This research significantly broadens the scope of visual adversarial attacks, transitioning from theoretical, task-specific constraints to practical, real-world applicability. By uncovering the dependency of current LVLMs on query-context for stable visual recognition, the paper exposes a critical vulnerability in deployed systems.

***

## Key Findings

*   **Brittleness of Traditional Attacks:** Traditional adversarial attacks on LVLMs fail effectively when different questions are asked about the same image.
*   **Query-Agnostic Success:** QAVA creates adversarial examples that cause incorrect responses even without prior knowledge of the specific user question.
*   **Performance Parity:** QAVA achieves attack effectiveness comparable to scenarios where the target question is fully known ("oracle" scenarios).
*   **Practical Efficiency:** The method significantly enhances the efficiency of attacking images in practical settings by decoupling the attack from the query optimization loop.

***

## Methodology

The researchers introduce **QAVA (Query-Agnostic Visual Attack)**, a novel adversarial framework designed for Large Vision-Language Models (LVLMs).

*   **Core Philosophy:** Unlike traditional Visual Question Answering (VQA) attacks that optimize perturbations against a fixed image-question pair, QAVA optimizes the visual perturbation to be independent of the query.
*   **Objective:** The goal is to craft robust adversarial images that mislead the model for a wide range of potential questions, rather than a single specific interaction.
*   **Surrogate Sampling:** To achieve this, the method utilizes surrogate question sampling strategies to simulate potential user queries during the optimization phase.

***

## Technical Details

*   **Target Architecture:** The approach specifically targets Large Vision-Language Models (LVLMs) that utilize a **Q-former alignment module** (e.g., InstructBLIP, BLIP-2).
*   **Optimization Strategy**: QAVA generates adversarial examples to disrupt internal representations for *any* question.
*   **Loss Function:** It maximizes the **Mean Squared Error (MSE)** between Q-former features of clean and perturbed images.
*   **Sampling Strategies:** Uses surrogate question sampling techniques including:
    *   **VQG** (Visual Question Generation)
    *   **RSQN** (Random Sampling of Question Nouns)
    *   **RSQt** (Random Sampling of Questions)
*   **Attack Algorithms:** Employs gradient-based attacks, specifically **PGD** (Projected Gradient Descent) and **C&W** (Carlini & Wagner).
*   **Hyperparameters:** Adjusted parameters were used, such as setting constant **c to 0.005** to account for the smaller QAVA loss magnitude.

***

## Results

Experiments were conducted using the **VQA v2 validation set** (specifically a '32+50' subset), utilizing **VQA Accuracy** as the primary metric.

*   **White-box Targets:** BLIP-2 and InstructBLIP.
*   **Transferability Targets:** LLaVA and MiniGPT-4.
*   **Efficacy:** QAVA generated adversarial images that caused incorrect responses across multiple questions without prior knowledge. Standard attacks failed catastrophically when questions changed, often resulting in accuracy metrics identical to clean images.
*   **Comparison:** QAVA achieved performance parity with white-box attacks that had full question knowledge, validating its robustness.
*   **Efficiency:** The attack demonstrated substantial efficiency gains by reducing the overhead from a per-query optimization loop to a single, one-time generation process.

***

## Contributions

*   **Broadened Scope:** Shifts the scope of visual adversarial attacks from specific, known tasks to practical, unknown query scenarios.
*   **Vulnerability Discovery:** Uncovers previously overlooked security vulnerabilities in LVLMs, specifically highlighting the dependency of current models on query-context for visual recognition.
*   **Real-World Threat Demonstration:** Demonstrates a heightened level of visual adversarial threat that is viable in real-world applications where an attacker cannot predict specific user questions.

***

**Quality Score:** 7/10  
**References:** 20 citations