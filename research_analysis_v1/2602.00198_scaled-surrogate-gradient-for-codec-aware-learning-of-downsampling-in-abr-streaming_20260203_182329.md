---
title: 'SCALED : Surrogate-gradient for Codec-Aware Learning of Downsampling in ABR
  Streaming'
arxiv_id: '2602.00198'
source_url: https://arxiv.org/abs/2602.00198
generated_at: '2026-02-03T18:23:29'
quality_score: 9
citation_count: 29
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# SCALED : Surrogate-gradient for Codec-Aware Learning of Downsampling in ABR Streaming
*Esteban Pesnel; Julien Le Tanou; Michael Ropert; Thomas Maugey; Aline Roumy*

---

> ### ðŸ“Š Quick Facts
>
> | Metric | Value |
> | :--- | :--- |
> | **Quality Score** | 9/10 |
> | **BD-BR Improvement** | 5.19% (PSNR) |
> | **References** | 29 Citations |
> | **Core Innovation** | Data-driven Surrogate Gradients |
> | **Training Type** | End-to-End with Non-Differentiable Codecs |

---

## Executive Summary

This research addresses the fundamental challenge of optimizing Adaptive Bitrate (ABR) streaming pipelines in an end-to-end fashion. The core obstacle lies in the non-differentiable nature of standard, industrial video codecs, which prevents the flow of gradients required to train deep learning components, such as neural downsamplers. Existing solutions typically rely on differentiable proxy codecs to approximate this behavior; however, these approximations create a gap between the training environment and real-world deployment, resulting in suboptimal performance when the system interacts with actual, non-differentiable codecs.

The authors propose **SCALED** (Surrogate-gradient for Codec-Aware Learning of Downsampling), a framework that enables end-to-end training using real, standard video codecs without requiring differentiable approximations. The key technical innovation is the use of **data-driven surrogate gradients** derived from actual compression errors. By analyzing how input changes affect the codec's output error, the method generates gradients that bypass the non-differentiable codec block. This approach allows the system to perform "codec-aware" learning, optimizing the pre-codec downsampling module specifically for the behavior of the target standard codec.

The SCALED framework demonstrates significant performance improvements over codec-agnostic training baselines. Specifically, the method achieves a **5.19% improvement in BD-BR** (Bjontegaard Delta Bitrate) using PSNR as the quality metric. The authors report that these gains are consistent across the entire rate-distortion convex hull and remain robust across multiple downsampling ratios, validating the reliability of the approach under varying bitrate constraints.

This work represents a paradigm shift in video streaming optimization, moving the field away from differentiable proxy models toward methodologies that incorporate the true behavior of standard codecs during training. It provides the first quantitative evidence (to the authors' knowledge) demonstrating the specific inefficiencies of training with standard codecs without tailored gradient handling. By solving the gradient obstruction problem, SCALED establishes a new standard for codec-aware learning, ensuring that training objectives are tightly aligned with real-world deployment performance.

---

## Methodology

The authors introduce the **SCALED** framework to address the challenge of optimizing the ABR streaming pipeline by utilizing real, non-differentiable video codecs during the training process.

*   **The Challenge:** Standard video codecs are non-differentiable, blocking gradient flow in neural network training.
*   **The Solution:** Instead of replacing standard codecs with differentiable approximations, the method bypasses the non-differentiability obstacle by employing **data-driven surrogate gradients**.
*   **The Mechanism:** These gradients are calculated based on the actual compression errors observed from the real codec, allowing for gradient-based optimization without approximating the codec itself.

---

## Key Findings

*   **Performance Gain:** The proposed framework achieves a **5.19% improvement in BD-BR (PSNR)** compared to codec-agnostic training approaches.
*   **Consistency:** The performance gains are consistent across the entire rate-distortion convex hull.
*   **Scalability:** Improvements hold true across multiple downsampling ratios.
*   **Real-World Application:** The framework successfully enables end-to-end training using real, non-differentiable standard video codecs, eliminating the reliance on differentiable proxy models.
*   **Objective Alignment:** Using data-driven surrogate gradients derived from actual compression errors effectively aligns training objectives with real-world deployment performance.

---

## Technical Details

**Framework Name:** SCALED (Surrogate-gradient for Codec-Aware Learning of Downsampling)

**Core Technology:**
*   **Surrogate Gradient Method:** Achieves codec-aware learning without modifying the internal code of standard codecs.
*   **End-to-End Training:** Maintains the training loop integrity while utilizing real, non-differentiable standard video codecs.
*   **Elimination of Proxies:** Removes the need for differentiable proxy models.

**Gradient Mechanism:**
*   **Data-Driven:** Gradients are derived from actual compression errors.
*   **Alignment:** Explicitly designed to align training objectives with real-world deployment performance.

---

## Contributions

*   **Solves Gradient Obstruction:** Provides a novel solution to the fundamental problem of gradient obstruction in video streaming pipelines, enabling end-to-end optimization with standard, non-differentiable codecs.
*   **Paradigm Shift:** Moves the field away from differentiable proxy codecs (which are merely approximations) toward a methodology that captures the true behavior of standard codecs during training.
*   **Quantitative Evidence:** Offers the first evidence (to the authors' knowledge) demonstrating the specific inefficiencies of using standard codecs during training without tailored gradient handling, and establishes the quantitative superiority of codec-aware learning.

---

## Results

The framework was rigorously tested against baseline methods:

*   **BD-BR (PSNR):** Achieved a **5.19% improvement** over codec-agnostic training approaches.
*   **Rate-Distortion Hull:** Performance gains were verified as consistent across the entire convex hull.
*   **Downsampling Ratios:** The method proved robust across various downsampling configurations.