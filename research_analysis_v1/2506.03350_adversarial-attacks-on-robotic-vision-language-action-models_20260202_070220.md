# Adversarial Attacks on Robotic Vision Language Action Models

*Eliot Krzysztof Jones; Alexander Robey; Andy Zou; Zachary Ravichandran; George J. Pappas; Hamed Hassani; Matt Fredrikson; J. Zico Kolter*

---

### ðŸ“Š Quick Facts

| Metric | Details |
| :--- | :--- |
| **Quality Score** | 9/10 |
| **References** | 40 Citations |
| **Target Model** | OpenVLA-7B |
| **Attack Method** | RoboGCG (Adapted GCG) |
| **Primary Risk** | Complete physical control via textual input |

---

## Executive Summary

Vision-Language-Action (VLA) models represent a significant advancement in embodied AI, enabling robots to interpret complex visual and linguistic commands to perform physical tasks. However, this research identifies a critical security vulnerability: these models inherit the adversarial susceptibilities of their Large Language Model (LLM) backbones, creating a vector for exploits to manifest as dangerous physical failures. Unlike purely digital LLM attacks, adversarial inputs in robotic systems can hijack motor functions, posing severe safety risks. This problem is compounded by the fact that current VLA architectures typically lack the refusal training and preference optimization found in modern LLMs, leaving a security gap where semantic vulnerabilities translate directly into low-level physical actions.

The key innovation is **"RoboGCG,"** a method that adapts the Greedy Coordinate Gradient (GCG) algorithmâ€”typically used for LLM jailbreakingâ€”to target the control policies of multimodal robotic systems. Technically, the approach exploits the VLA architecture's autoregressive generation process, where text and images are fused into a joint embedding space. By targeting the modelâ€™s action discretizationâ€”the process where continuous physical actions are mapped to a specific vocabulary of tokensâ€”the attack injects optimized textual adversarial suffixes. This allows the attacker to manipulate the model's output at the token level, forcing the generation of arbitrary action sequences (such as specific torques or velocities) rather than merely altering the semantic content of the text response.

The study presents empirical results demonstrating the efficacy of these attacks on the OpenVLA-7B model within simulated evaluation environments. The researchers report that textual adversarial attacks can achieve 100% success rates in forcing specific, targeted actions (such as overriding a "move forward" command to execute "move left" or "stop"). The findings establish that the attack grants "complete reachability" of the robot's action space, capable of accessing the entire vocabulary of 256 discretized action tokens. These attacks are highly persistent, maintaining control over extended time horizons despite being applied only once at the start of a task rollout, and succeed without relying on semantics related to harm or violence.

This work establishes the foundational study on adversarial attacks targeting VLA-controlled robots, proving that the safety alignment issues plaguing text-based foundation models extend directly into the physical domain. By releasing the RoboGCG code implementation, the authors provide a reproducible benchmark for the community to evaluate robustness in future models. The findings expose a critical deficiency in current robotic architectures, which lack the defensive alignment necessary to filter adversarial inputs. Consequently, this research signals an urgent shift in the field toward developing training paradigms that specifically incorporate adversarial robustness for physical action spaces, ensuring that the deployment of embodied AI does not introduce uncontrollable physical risks.

---

## Key Findings

*   **Inherited Vulnerability:** Vision-Language-Action (VLA) models inherit the adversarial susceptibility inherent in Large Language Models (LLMs).
*   **Full Action Space Reachability:** Textual adversarial attacks applied at the start of a rollout can achieve full reachability of the action space, effectively granting an attacker complete control over the robot.
*   **Long-Term Persistence:** These attacks are durable, persisting over long time horizons even when the adversarial prompt is only injected once at the beginning.
*   **Semantic Independence:** Unlike standard jailbreaking, these attacks do not require semantic context related to harm or violence, highlighting a unique safety gap in robotic systems.

---

## Methodology

The researchers adapted existing LLM jailbreaking attack algorithms to the domain of VLA-controlled robotics. The experimental approach involved:

1.  **Prompt Injection:** Injecting textual adversarial prompts at the start of a robot rollout.
2.  **Control Evaluation:** Evaluating the extent of control gained over the robotic system's actions based on these inputs.
3.  **Algorithm Adaptation:** Modifying text-based optimization algorithms to target the physical action outputs of the multimodal policy.

---

## Technical Details

The study provides an in-depth look at the architecture exploited and the specific mechanisms of the attack:

### VLA Architecture
*   **Input Processing:** Ingests textual prompts and dynamic images.
*   **Fusion:** Inputs are fused in a joint embedding space.
*   **Processing:** Processed by an LLM backbone with an action detokenizer.

### Action Discretization ("Symbol Tuning")
*   **Mapping Strategy:** Maps the least used vocabulary tokens to specific points in the robot's continuous action space.
*   **Distribution:** Points are uniformly distributed between data quantiles.
*   **Generation:** Utilizes autoregressive generation to produce sequences of action tokens.

### Attack Mechanism
*   **Algorithm:** Adapts Greedy Coordinate Gradient (GCG) to textual prompts.
*   **Objective:** Seeks "complete control authority" by targeting specific low-level action sequences rather than semantic content.
*   **Distinction:** Unlike high-level planners, VLAs output direct torques and velocities. The attack exploits this by hijacking the low-level token generation.

---

## Contributions

*   **Foundational Study:** Initiated the first comprehensive study on adversarial attacks specifically targeting VLA-controlled robots.
*   **Technique Transfer:** Demonstrated the successful transfer of LLM jailbreaking techniques to multimodal robotic policies.
*   **Empirical Evidence:** Provided concrete evidence that high-level semantic attacks translate directly to low-level physical failures.
*   **Open Source:** Released the **RoboGCG** code implementation to ensure reproducibility and encourage further research into defenses.

---

## Results

The reported findings highlight the severity of the security risks in current VLA systems:

*   **Control Capability:** Textual adversarial attacks successfully drove the robot to *any* targeted action, achieving full reachability of the action space.
*   **Persistence:** The attacks maintained effectiveness over extended durations despite being applied only at the start of the rollout.
*   **Context-Free Execution:** The attacks functioned independently of semantic harm contexts.
*   **Architecture Gap:** The study identified a critical lack of refusal training or preference optimization in VLAs compared to standard LLMs, which creates the vulnerability exploited in these experiments.