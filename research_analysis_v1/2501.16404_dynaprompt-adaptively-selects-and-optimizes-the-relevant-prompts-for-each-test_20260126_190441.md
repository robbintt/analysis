---
title: DynaPrompt adaptively selects and optimizes the relevant prompts for each test
arxiv_id: '2501.16404'
source_url: https://arxiv.org/abs/2501.16404
generated_at: '2026-01-26T19:04:41'
quality_score: 6
citation_count: 39
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# DynaPrompt adaptively selects and optimizes the relevant prompts for each test

*Jiayi Shen, Xiaohongshu Inc, Jiayin Cai, Shilin Yan, Tsinghua University, Jack Hong, Zehao Xiao, Qi Wang, Yao Hu, Xiaolong Jiang*

---

> ### ðŸ“‹ Quick Facts
>
> *   **Quality Score:** 6/10
> *   **Total Citations:** 39
> *   **Primary Dataset:** ImageNet-A
> *   **Base Architecture:** CLIP (Frozen Encoders)
> *   **Core Innovation:** Dynamic Prompt Buffer & Entropy Minimization
> *   **Key Problem Solved:** Prompt Collapse in streaming environments

---

## Executive Summary

This research investigates the instability of **Online Test-Time Adaptation (Online TPT)**, specifically targeting the phenomenon known as "prompt collapse." In vision-language models processing continuous data streams without access to training data, naive approaches that continuously tune a single global prompt suffer from severe error accumulation. As the model processes sequential inputs from non-stationary distributions, this "collapse" causes catastrophic degradation in performance, rendering standard adaptation methods unreliable for real-world, dynamic environments.

The authors introduce **DynaPrompt**, a buffer-based framework designed to decouple the adaptation process from error accumulation. Rather than tuning a single prompt, DynaPrompt maintains a dynamic buffer of historical prompts. To address the limitations of previous methods, the framework selects the most relevant prompt subset for each input based on feature similarity, ensuring alignment with the current data context. Once selected, the prompt subset is optimized via entropy minimization to maintain high prediction confidence, while a buffer management strategy periodically replaces inactive prompts to preserve the relevance of the global knowledge base.

Experimental results on the **ImageNet-A** dataset quantify the severity of prompt collapse and the efficacy of the proposed solution. Naive Online TPT methods demonstrated catastrophic instability, with accuracy degrading from competitive levels to effectively 0% by the conclusion of the test sequence. In contrast, DynaPrompt successfully mitigated this drop, sustaining performance comparable to stable baselines that reset prompts for each sample. An Oracle analysis further validates the approach, confirming that the selective utilization of relevant historical information is the critical factor in maintaining accuracy without succumbing to noise accumulation.

DynaPrompt establishes a practical buffer-management paradigm for prompt tuning, specifically resolving the trade-off between model plasticity and stability in online scenarios. By introducing a mechanism that selectively optimizes prompts based on feature relevance, this work enables the deployment of VLMs in streaming environments without requiring expensive retraining.

---

## Key Findings

*   **Identification of Prompt Collapse:** The study highlights that naive Online TPT suffers from severe error accumulation, leading to "prompt collapse."
*   **Catastrophic Degradation:** Experimental findings show that without intervention, accuracy on ImageNet-A drops from competitive levels to nearly **0%** by the end of a test sequence.
*   **Validation of Selection Strategy:** Oracle analysis confirms that utilizing relevant online information significantly improves performance, validating that selecting beneficial prompts is key to avoiding degradation.
*   **Stability Maintenance:** Unlike naive tuning, baseline TPT maintains stable accuracy by resetting prompts for each sample, a principle DynaPrompt improves upon by maintaining a history of relevant prompts rather than full resets.

---

## Methodology

The proposed method utilizes a **CLIP model** with frozen encoders. Instead of tuning a single global prompt, the method employs a "Buffer-based Framework."

*   **Dynamic Prompt Buffer ($V_n$):** A storage mechanism with a maximum size $M$ used to retain historical prompts.
*   **Subset Selection ($S_n$):** For each test input, the framework selects a subset of prompts from the buffer based on two distinct metrics:
    *   **Prediction Entropy ($D_{ent}$):** Used to ensure confidence is maintained compared to the initial prompt.
    *   **Probability Difference ($D_{pro}$):** Used to ensure the prompt remains sensitive to changes in the input.
*   **Optimization:** The selected prompts are optimized using entropy minimization.
*   **Buffer Management:** Inactive prompts are periodically replaced to ensure the buffer remains relevant to the current data distribution.

---

## Technical Details

DynaPrompt addresses 'prompt collapse' in online test-time adaptation by adaptively selecting relevant historical prompts from a buffer rather than tuning a single global prompt. It utilizes a CLIP model with frozen encoders and a dynamic prompt buffer ($V_n$) with a maximum size $M$.

The method selects a subset of prompts ($S_n$) based on two metrics:
1.  **Prediction Entropy ($D_{ent}$):** Ensures confidence compared to the initial prompt.
2.  **Probability Difference ($D_{pro}$):** Ensures sensitivity to input changes.

Prompts are optimized using entropy minimization, and the buffer is managed by replacing inactive prompts to maintain a diverse and relevant set of adaptation parameters.

---

## Results

Experimental comparisons on **ImageNet-A** revealed the following:

*   **Naive Online TPT:** Suffered from severe error accumulation, with accuracy dropping from competitive levels to nearly **0%** by the end of the test sequence.
*   **Baseline TPT:** Maintained stable accuracy by resetting prompts for each sample (serving as a stable but less adapted baseline).
*   **DynaPrompt:** Successfully avoided degradation, sustaining performance closer to the stable baseline while benefiting from adaptation.
*   **Oracle Analysis:** Confirmed that utilizing relevant online information significantly improves performance, validating the hypothesis that the selective strategy effectively avoids noise accumulation.

---

## Research Contributions

*   **Buffer-Management Paradigm:** Establishes a practical buffer-management strategy specifically for prompt tuning in online scenarios.
*   **Plasticity vs. Stability Trade-off:** Resolves the conflict between the model's ability to adapt (plasticity) and its ability to maintain accurate performance over time (stability).
*   **Streaming VLM Deployment:** Provides a foundation for deploying Vision-Language Models (VLMs) in dynamic, streaming environments without the need for expensive retraining processes.
*   **Prompt Integrity:** Prioritizes the preservation of prompt integrity alongside the necessity for continuous learning.
*   **Oracle Validation:** Provides rigorous theoretical validation through Oracle analysis regarding the benefits of selective historical information usage.