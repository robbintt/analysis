# On the existence of consistent adversarial attacks in high-dimensional linear classification

*Matteo Vilucchio; Lenka ZdeborovÃ¡; Bruno Loureiro*

---

### ðŸ“Š Quick Facts

| **Metric** | **Detail** |
| :--- | :--- |
| **Quality Score** | 9/10 |
| **References** | 40 Citations |
| **Core Context** | High-dimensional binary classification |
| **Key Metric** | Consistent Robust Error ($E^{cns}_{rob}$) |
| **Training Method** | Robust Empirical Risk Minimization (RE-ERM) |
| **Model Types** | Well-specified & Latent Space Models |

---

## Executive Summary

> This research addresses the critical challenge of distinguishing between inherent generalization errors and true adversarial vulnerability in high-dimensional binary classification. Standard robust error metrics often conflate misclassifications caused by limited data or model expressivity with errors arising from adversarial perturbations that preserve the ground-truth label. This distinction is vital because the sources of these errors are fundamentally different; understanding true adversarial risk requires isolating perturbations that do not alter the underlying data label yet still fool the classifier.

**The Key Innovation**
The introduction of a novel metric, **Consistent Robust Error ($E^{cns}_{rob}$)**, which quantifies model vulnerability specifically to "consistent adversarial attacks"â€”perturbations that change the model's prediction while retaining the correct ground-truth label. Technically, the authors employ an exact asymptotic characterization within a high-dimensional regime to analyze linear classifiers with various link functions (logit, probit, noiseless) trained via Robust Empirical Risk Minimization (RE-ERM). Geometrically, they define these consistent attacks as perturbations orthogonal to the target weights within an $\ell_q$-ball, allowing them to rigorously isolate adversarial susceptibility from generalization gaps in both well-specified and latent space models.

**Implications & Results**
The study reveals that vulnerability patterns for consistent attacks diverge significantly from those identified by standard robust error measures. Notably, the authors found that **overparameterization increases a model's vulnerability** to label-preserving perturbations, challenging the assumption that more parameters inherently improve robustness. Empirical simulations using $n=10^3$ samples confirmed theoretical predictions with high precision. Furthermore, the results demonstrated a **sharp threshold effect**: as dimensionality $d$ increases (e.g., to 1000), the probability of a consistent attack existing transitions from a smooth curve to a near-deterministic step function. The transition behavior is also heavily dependent on the choice of norm $q$, highlighting specific geometric vulnerabilities in high-dimensional spaces.

This work significantly advances the theoretical understanding of adversarial robustness by providing a rigorous benchmark for separating generalization errors from true adversarial sensitivity. By establishing that overparameterization can exacerbate vulnerability to consistent attacks, the paper challenges prevailing beliefs about model capacity and security. These findings compel the field to reconsider current evaluation metrics for robustness and offer a precise framework for analyzing the trade-offs between accuracy and security in high-dimensional statistical learning.

---

## Key Findings

*   **Distinction of Errors**
    *   Research distinguishes adversarial attacks from misclassifications caused by limited data or model expressivity by focusing on **"consistent adversarial attacks"** (perturbations that preserve ground-truth labels).
*   **Divergence from Standard Metrics**
    *   Vulnerability patterns associated with consistent adversarial attacks differ fundamentally from those identified by standard robust error measures.
*   **Impact of Overparameterization**
    *   As models become overparameterized, their vulnerability to label-preserving perturbations grows, challenging the assumption that more parameters inherently improve robustness.
*   **Asymptotic Behavior**
    *   The exact asymptotic characterization of the proposed metric holds true across both well-specified models and latent space models.

---

## Methodology

The research is situated within the framework of high-dimensional binary classification, focusing on statistical effects from limited data. The authors employ an exact and rigorous asymptotic characterization to analyze the behavior of the introduced error metric. This theoretical analysis is applied to two distinct model settings:

1.  **Well-specified models**
2.  **Latent space models**

---

## Contributions

*   **Novel Error Metric**
    *   Introduction of a precise error metric that quantifies model vulnerability specifically to consistent adversarial attacks, isolating it from generalization errors.
*   **Rigorous Theoretical Characterization**
    *   Provision of an exact asymptotic analysis of model vulnerability in high-dimensional settings, serving as a rigorous benchmark for understanding adversarial risks.
*   **Insight into Model Sensitivity**
    *   Theoretical advancement in understanding adversarial sensitivity, specifically establishing the link between overparameterization and increased vulnerability to adversarial attacks.

---

## Technical Details

*   **Classifier Scope**
    *   The paper distinguishes between consistent and inconsistent attacks on binary classifiers, focusing on a high-dimensional asymptotic regime.
*   **Link Functions Analyzed**
    *   Linear classifiers analyzed with various link functions: **Logit**, **Probit**, and **Noiseless**.
*   **Geometric Definition**
    *   *Consistent attacks* are defined as perturbations orthogonal to target weights, forming a hyperplane section of the $\ell_q$-ball.
*   **Existence Condition**
    *   The study derives an existence condition for these attacks involving the attack budget $\epsilon$ and the orthogonal component of model weights.
*   **Key Metrics**
    *   **Robust Error ($E_{rob}$)**
    *   **Consistent Robust Error ($E^{cns}_{rob}$)**
*   **Training Protocol**
    *   Training performed using **Robust Empirical Risk Minimization (RE-ERM)**.

---

## Results

*   **Theoretical Alignment**
    *   Theoretical predictions showed perfect alignment with empirical simulations based on $n=10^3$ samples.
*   **Norm Sensitivity ($q$)**
    *   Experiments measuring the probability of attack existence revealed that the choice of norm $q$ significantly impacts the transition curves.
*   **Sharp Threshold Effect**
    *   Results demonstrated a sharp threshold effect: as dimensionality $d$ increases (e.g., to 1000), the probability of a consistent attack existing transitions from a smooth curve to a near-deterministic step function.
*   **High-Dimensional Determinism**
    *   The step function behavior indicates a clear vulnerability threshold in high-dimensional spaces compared to the probabilistic uncertainty in low dimensions.