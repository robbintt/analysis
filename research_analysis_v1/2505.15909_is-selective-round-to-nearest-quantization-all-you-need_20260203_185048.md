---
title: Is (Selective) Round-To-Nearest Quantization All You Need?
arxiv_id: '2505.15909'
source_url: https://arxiv.org/abs/2505.15909
generated_at: '2026-02-03T18:50:48'
quality_score: 8
citation_count: 38
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Is (Selective) Round-To-Nearest Quantization All You Need?
*Alex Kogan*

---

> **üìä QUICK FACTS**
> *   **Quality Score:** 8/10
> *   **References:** 38 Citations
> *   **Testbed:** 8x Nvidia H100 GPUs
> *   **Software Stack:** vLLM (v0.6.4), PyTorch 2.5.1
> *   **Key Innovation:** Selective Precision RTN with Marlin Kernels
> *   **Performance:** Near-identical accuracy to FP16/BF16 with 4x speedup

---

## üìù Executive Summary

The machine learning community has largely favored advanced Post-Training Quantization (PTQ) methods‚Äîsuch as GPTQ, AWQ, and outlier handling techniques‚Äîunder the assumption that simple Round-to-Nearest (RTN) quantization is insufficient for maintaining the accuracy of Large Language Models (LLMs). While these complex methods reduce model size, they introduce significant computational overhead, require calibration data, and risk overfitting.

This paper addresses the trade-off between quantization complexity and inference efficiency, questioning whether the industry's pursuit of sophisticated quantization algorithms is necessary when simpler methods might offer comparable accuracy with superior speed and lower implementation costs. The key innovation proposed is a refined approach to standard RTN quantization, augmented by a **selective precision strategy**.

RTN is a data-free method that scales weights and rounds them to the nearest integer without calibration. To mitigate potential accuracy drops, the author proposes selectively increasing the data precision format (e.g., from 4-bit to 8-bit) only for specific layers or modules that are sensitive to quantization, rather than applying uniform high precision across the entire model. This approach is implemented using highly optimized **Marlin kernels** for mixed-precision GEMM (W4A16 and W8A16).

Experiments demonstrate that the proposed **RTN-8 method** achieves performance nearly identical to FP16/BF16 baselines (e.g., Llama-3.1 8B Average Accuracy: 66.13 vs 66.07). Throughput tests show Marlin kernels achieved close to the theoretical optimal 4x speedup over standard 16-bit GEMM calculations. This research significantly challenges the established consensus, validating RTN as a viable, high-performance alternative that bypasses calibration costs and complexity.

---

## üîë Key Findings

*   **Cost Efficiency:** Round-to-Nearest (RTN) quantization is significantly cheaper to apply compared to recent advanced quantization methods.
*   **Superior Throughput:** RTN can achieve token generation throughput that is better than that of more complex quantization alternatives.
*   **Accuracy Parity:** Despite its simplicity, RTN demonstrates accuracy levels similar to more sophisticated techniques.
*   **Selective Enhancement:** The accuracy of RTN can be incrementally enhanced by selectively increasing the data precision format of specific model layers and modules.

---

## ‚öôÔ∏è Methodology

The study utilizes an implementation of RTN based on the recent **Marlin kernels**, likely optimizing for low-bit integer matrix multiplication. The methodology involves a targeted approach where the data precision format is increased specifically for certain model layers and modules rather than applying uniform high precision across the entire model.

---

## üõ† Technical Details

*   **Core Mechanism:**
    *   Utilizes **Round-to-Nearest (RTN)** quantization.
    *   A data-free method that scales model weights and rounds them to the nearest integer.
    *   Requires **no calibration data or processes**.
    *   Can be applied on-the-fly during model loading, enabling large models to run on hardware with limited memory.

*   **Optimization Strategy:**
    *   **Selective Precision Increase:** Accuracy is improved by selectively increasing precision for specific layers only.
    *   **Uniform Weight Treatment:** Unlike outlier handling methods (e.g., BitsAndBytes), this approach treats weights uniformly to avoid computational overhead.

*   **Implementation & Hardware:**
    *   Leverages **Marlin kernels** for high-performance mixed-precision GEMM (W4A16 and W8A16).
    *   Hides dequantization overhead by overlapping data access latency with floating-point operations via asynchronous memory access and optimized layouts.

*   **Comparison to Alternatives:**
    *   **Vs. Advanced PTQ (AWQ/GPTQ):** Avoids calibration data costs and overfitting risks.
    *   **Vs. Outlier Handling:** Avoids the computational overhead associated with non-uniform weight processing.

---

## üìà Results

*   **Experimental Setup:**
    *   **Hardware:** 8x Nvidia H100 GPUs.
    *   **Software:** vLLM framework (v0.6.4) and PyTorch 2.5.1.
    *   **Models Evaluated:** Llama, Phi, and Mistral.
    *   **Benchmarks:** Wikitext perplexity and zero-shot accuracy across 6 tasks.

*   **Accuracy (Llama-3.1 8B):**
    *   RTN-8 achieved nearly identical performance to the FP16/BF16 baseline.
    *   **Average Accuracy:** 66.13 (RTN-8) vs 66.07 (Baseline).
    *   **Wikitext Perplexity:** 8.65 (RTN-8) vs 8.64 (Baseline).

*   **Performance:**
    *   Marlin kernels achieved close to optimal **4x speedup** over 16-bit GEMM calculations for batch sizes of 16‚Äì32 inputs.
    *   The time required to apply RTN quantization is very fast, taking only **milliseconds to seconds** depending on model size.

---

## üöÄ Contributions

1.  **Challenges Consensus:** The work challenges the established consensus in the machine learning community that advanced quantization methods are inherently superior to RTN in performance aspects.
2.  **Validates Simplicity:** It validates RTN as a viable and practical choice for quantizing Large Language Models (LLMs), shifting the focus back to simpler techniques.
3.  **Novel Strategy:** It demonstrates a specific strategy (selective precision increase) to mitigate the accuracy limitations of simple RTN without sacrificing its cost and speed benefits.

---

**Paper Rating:** 8/10 | **Citations:** 38