# Quant-Trim in Practice: Improved Cross-Platform Low-Bit Deployment on Edge NPUs

*Rayen Dhahri; Steffen Urban*

---

## eee Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Quality Score** | 6/10 |
| **References** | 40 Citations |
| **Primary Solution** | Quant-Trim (Training-phase technique) |
| **Target Deployment** | Edge NPUs, SoCs, GPUs |
| **Precision Support** | INT8, INT4 |
| **Export Format** | Standard ONNX (No custom operators) |

---

## eee Executive Summary

Edge AI deployment faces a critical bottleneck due to **vendor fragmentation**, where proprietary compiler backends implement quantization inconsistently across GPUs, SoCs, and NPUs. This heterogeneity prevents the seamless porting of low-bit neural networks, forcing developers to bridge the accuracy gap between floating-point and integer models through unsustainable engineering overhead. Currently, maintaining performance across diverse hardware ecosystems requires per-hardware retraining or manual model refactoring, creating significant operational inefficiencies in the MLOps pipeline.

The authors introduce **Quant-Trim**, a training-phase technique designed to generate hardware-agnostic checkpoints by decoupling model training from specific deployment constraints. The method relies on two core mechanisms: **Progressive Fake Quantization**, which employs a curriculum learning schedule with distinct warmup, quartic, and quadratic ramp phases to interpolate forward passes between FP32 and quantized precision; and **Reverse Pruning**, which clips outlier weights based on a robust quantile ($p_{clip}=0.95$) to control scale inflation. This approach supports symmetric weight quantization and asymmetric activation quantization, exporting to standard ONNX without custom operators or vendor-specific graph modifications.

Evaluations across various hardware backends confirm that Quant-Trim effectively narrows the performance gap between floating-point models and low-bit integer deployments (INT8 and INT4). Technical analysis reveals specific structural improvements within the models, including compressed weight tails and narrower activation ranges, which validate the method's ability to maintain consistent accuracy stability without per-backend retraining. The study demonstrates competitive system-level performance across GPUs, SoCs, and NPUs, specifically measuring efficiency in latency, throughput, energy consumption, and operational cost.

This work establishes a universal **"train-once, deploy-anywhere"** framework that drastically reduces the engineering complexity associated with edge AI deployment. By solving vendor fragmentation through hardware-neutral checkpoints, Quant-Trim sets a new standard for evaluating edge models, shifting the focus from pure accuracy to a comprehensive view of system-level efficiency. This approach streamlines MLOps workflows by eliminating the need to maintain multiple model variants for different hardware targets, offering a scalable solution for the edge AI market.

---

## e Key Findings

*   **Hardware-Neutral Checkpoints:** Quant-Trim generates checkpoints that maintain consistent accuracy across different edge NPUs and vendor compiler backends, solving the issue of vendor fragmentation.
*   **Reduced Precision Gap:** It effectively narrows the performance gap between floating-point (FP) models and low-bit integer deployments (INT8/INT4).
*   **Eliminated Retraining:** The approach reduces engineering overhead by removing the need for per-backend retraining or manual model refactoring.
*   **Comprehensive Validation:** Performance is validated on holistic edge metrics, including latency, throughput, energy consumption, and operational cost.

---

##  Methodology

Quant-Trim is a training-phase technique designed to decouple model training from specific hardware deployment constraints. It is configuration-agnostic, supporting symmetric/asymmetric schemes and various bit-widths without vendor-specific graph modifications.

The methodology relies on two core mechanisms:

1.  **Progressive Fake Quantization (PFQ):**
    *   Aligns the training process with the integer grid used during deployment.
    *   Ensures the model learns to robustly handle quantization noise before actual deployment.

2.  **Reverse Pruning:**
    *   Controls outlier-driven scale inflation without compromising the model's ability to learn.
    *   Focuses on preserving the precision of bulk weights by managing statistical outliers.

---

## fe Technical Details

### Core Training Procedure
The paper proposes Quant-Trim as a training-time procedure to generate hardware-agnostic checkpoints robust to vendor compiler heterogeneity and precision regimes (INT8/INT4).

#### 1. Progressive Fake Quantization
This mechanism employs a **curriculum learning schedule** using a global blending coefficient ($\lambda_t$) to interpolate forward passes between FP32 and quantized precision. The schedule consists of three distinct phases:
*   **Warmup**
*   **Quartic Ramp**
*   **Quadratic Ramp**

#### 2. Reverse Pruning (Scale Control)
This process clips outlier weights at a threshold $\tau_{\ell,t}$ based on **robust quantiles** (typically $p_{clip} = 0.95$). This prevents scale inflation and preserves the precision of bulk weights.

### Quantization Parameters
*   **Weights:** Symmetric quantization (zero-point $z=0$) based on upper quantiles.
*   **Activations:** Asymmetric quantization based on ranges between lower and upper quantiles.
*   **Statistics:** Empirical quantiles are computed on sub-samples and updated via **Exponential Moving Average (EMA)**.
*   **Export:** The final model exports to standard ONNX without custom operators.

---

##  Results

*   **Consistency:** Quant-Trim produces hardware-neutral checkpoints that maintain consistent accuracy across diverse edge NPUs and vendor compiler backends.
*   **Efficiency:** Successfully narrows the performance gap between floating-point and low-bit integer deployments while eliminating per-backend retraining overhead.
*   **Structural Analysis:** Scale distribution analysis shows compressed weight tails and narrower activation ranges.
*   **Hardware Benchmarks:** Validated on GPUs, System-on-Chips (SoCs), and Neural Processing Units (NPUs).

---

##  Contributions

1.  **Solving Vendor Fragmentation:** Addresses the challenge of differing 'black box' vendor compilers.
2.  **Algorithmic Innovation:** Introduces the specific mechanisms of progressive fake quantization and reverse pruning.
3.  **Universal Deployment Framework:** Provides a unified training approach enabling a 'train-once, deploy-anywhere' workflow.
4.  **Holistic Performance Metrics:** Establishes a broad evaluation standard by measuring accuracy alongside system-level edge metrics.