---
title: Binary Verification for Zero-Shot Vision
arxiv_id: '2511.10983'
source_url: https://arxiv.org/abs/2511.10983
generated_at: '2026-02-03T19:29:04'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Binary Verification for Zero-Shot Vision

*Jeffrey Liu; Rongbin Hu*

---

> ### ðŸ“Š Quick Facts
>
> * **Quality Score:** 9/10
> * **References:** 40 Citations
> * **Approach:** Training-free, inference-time workflow
> * **Backbone Used:** LLaVA-1.5-7B
> * **Key Improvement:** RefCOCO+ testB accuracy rose from **62.0%** to **84.3%**
> * **Primary Tasks:** Referring Expression Grounding, Spatial Reasoning, BLINK-Jigsaw

---

## Executive Summary

Current Vision-Language Models (VLMs) face significant reliability challenges in zero-shot scenarios, particularly when tasked with open-ended generation. Directly generating descriptive answers or performing complex spatial reasoning often results in high error rates, as off-the-shelf models lack the specific task knowledge required for precise outputs without fine-tuning. This limitation is a critical barrier to deploying VLMs in real-world applications where high accuracy is essential but task-specific training data is unavailable or costly to obtain.

The authors introduce a training-free, inference-time algorithm called the **Boolean Resolution Pipeline** to enhance VLM performance without architectural modifications. The workflow operates in three stages:

1.  **Quantization:** Converts open-ended queries into Multiple-Choice Questions (MCQs).
2.  **Binarization:** Breaks MCQs into individual True/False verification questions.
3.  **Deterministic Resolution:** Selects the correct candidate based on a logic check.

Crucially, this resolution step selects an option only if exactly one verification returns 'True'; if multiple or zero options return 'True', the system reverts to standard MCQ selection as a fallback. The authors support this methodology with a "hardness ladder" theoretical framework, formally proving that Boolean resolution is analytically more effective than direct MCQ selection or open-ended generation.

The proposed method was evaluated on Referring Expression Grounding (RefCOCO+/RefCOCOg), Spatial Reasoning, and BLINK-Jigsaw tasks. Experiments demonstrated substantial accuracy gains. For instance, on the RefCOCO+ testB set, accuracy rose from 62.0% (open-ended) to 81.2% (MCQ), finally reaching 84.3% with the Boolean pipeline. This research represents a paradigm shift from model-centric solutions to algorithm-centric solutions, offering an immediately practical tool for improving the reliability of existing VLMs.

---

## Key Findings

*   **Significant Performance Gains:** The proposed workflow yields substantial improvements across all evaluated tasks, including referring expression grounding, spatial reasoning, and BLINK-Jigsaw.
*   **Progressive Query Refinement:**
    *   Converting open-ended queries to **Multiple-Choice Questions (MCQs)** results in substantial accuracy gains.
    *   Further processing these into **binary (True/False)** questions provides an additional, consistent boost.
*   **Broad Applicability:** The same single workflow is effective across diverse vision tasks without modification, demonstrating broad applicability for zero-shot scenarios.
*   **Analytical Superiority:** Boolean resolution (asking True/False questions) is analytically proven to be more effective than direct MCQ selection or open-ended generation.

---

## Methodology

The authors propose a training-free, inference-time workflow designed for off-the-shelf Vision-Language Models (VLMs). The process consists of three distinct phases:

1.  **Quantization**
    Open-ended queries are transformed into Multiple-Choice Questions (MCQs) with explicit options. This constrains the output space, reducing ambiguity.

2.  **Binarization**
    MCQs are broken down into individual True/False verification questions for each candidate option. This simplifies the decision-making process for the model.

3.  **Deterministic Resolution**
    A candidate is selected if exactly one verification returns 'True.' If the condition is not met (i.e., zero or multiple options return 'True'), the system reverts to standard MCQ selection as a fallback mechanism.

---

## Technical Details

The paper proposes a unified, task-agnostic **Boolean Resolution Pipeline** designed to enhance zero-shot vision capabilities through progressive query refinement.

### Architecture Stages
The pipeline architecture consists of three sequential stages:

1.  **Open-ended Generation:** The raw input is processed by the VLM.
2.  **MCQ Conversion (Quantization):** The open-ended prompt is reformatted into a structured selection task.
3.  **Binary Verification:** The core innovation stage. This utilizes Boolean resolution rather than generative selection.

### Core Innovation
The central technical advancement is the shift toward **Boolean resolution**. This approach utilizes a single workflow without requiring task-specific fine-tuning or architectural changes to the underlying VLM.

---

## Contributions

*   **Practical "Drop-in" Solution:** The paper introduces a solution that emphasizes inference-time algorithm design over task-specific training or model fine-tuning, making it immediately applicable to existing models.
*   **Theoretical Framework:** The authors establish a theoretical framework ('hardness ladder') that formalizes how open-ended vision queries can be quantized into MCQs and further binarized into True/False verifications.
*   **Theoretical Analysis:** The research provides a theoretical analysis explaining *why* Boolean resolution mechanisms boost accuracy in zero-shot vision tasks, grounding the empirical results in logic.

---

## Results

The method was rigorously evaluated across three distinct benchmarks using **accuracy** as the primary metric.

### Evaluation Tasks
*   Referring Expression Grounding (RefCOCO+/RefCOCOg)
*   Spatial Reasoning
*   BLINK-Jigsaw

### Performance Highlights
Experiments using the **LLaVA-1.5-7B** backbone demonstrated specific, quantifiable gains:

*   **RefCOCO+ testB:**
    *   Open-ended: 62.0%
    *   MCQ: 81.2%
    *   **Boolean Pipeline: 84.3%**
*   **RefCOCOg:**
    *   Improved from 74.1% to 89.0% following the full workflow.

The workflow achieved significant improvements across all tasks, proving its broad applicability in zero-shot scenarios without the need for task-specific adjustments.