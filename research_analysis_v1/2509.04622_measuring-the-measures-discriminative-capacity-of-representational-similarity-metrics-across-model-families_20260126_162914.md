---
title: 'Measuring the Measures: Discriminative Capacity of Representational Similarity
  Metrics Across Model Families'
arxiv_id: '2509.04622'
source_url: https://arxiv.org/abs/2509.04622
generated_at: '2026-01-26T16:29:14'
quality_score: 8
citation_count: 29
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# Measuring the Measures: Discriminative Capacity of Representational Similarity Metrics Across Model Families

*Meenakshi Khosla, Yiqing Bo, Shreya Saha, Jialin Wu*

> ### ⚡ Quick Facts: Key Metrics at a Glance
>
> *   **Models Evaluated:** 35 Vision Models
> *   **Architectures:** CNNs, ViT, Swin Transformers, ConvNeXt
> *   **Training Regimes:** Supervised, Self-supervised
> *   **Top Performing Metric:** Representational Similarity Analysis (RSA)
> *   **Highest D-Prime Score:** 3.95 (RSA)
> *   **Evaluation Criteria:** D-Prime, Silhouette Score, ROC-AUC

---

## Executive Summary

In the field of deep learning and computational neuroscience, representational similarity metrics—such as Centered Kernel Alignment (CKA), Procrustes analysis, and Representational Similarity Analysis (RSA)—are fundamental tools for comparing the internal representations of neural networks. Despite their widespread use in comparing model architectures and aligning models with biological brains, a critical gap remains in understanding their discriminative capacity: the ability of these metrics to reliably distinguish between different model families and training regimes, rather than merely indicating absolute similarity.

This paper addresses the lack of systematic evaluation regarding how effectively these metrics can separate distinct architectural classes. The authors introduce a novel evaluation framework that moves beyond raw similarity scores to focus on **"separability."** This framework assesses how well a given metric clusters models from the same family (e.g., different CNNs) while separating those from different families (e.g., CNNs vs. Vision Transformers). 

The study, which evaluated 35 vision models across four architectural families and two training paradigms, revealed distinct performance hierarchies. Representational Similarity Analysis (RSA) demonstrated the highest overall discriminative capacity, achieving a D-Prime of 3.95, a Silhouette Score of 0.56, and a ROC-AUC of 0.9257. The findings indicate that discriminative capacity generally increases as metrics impose more stringent alignment constraints, with RSA proving particularly effective due to its geometric sensitivity. This research provides the field’s first systematic comparison of representational similarity metrics through the lens of separability, offering practical, evidence-based guidance for researchers.

---

## Key Findings

*   **Constraint-based Separability:** The discriminative capacity of representational similarity metrics systematically increases as the metrics impose more stringent alignment constraints.
*   **Ranking of Mapping-based Methods:** Among mapping-based approaches, *soft-matching* achieves the highest separability, followed by *Procrustes alignment* and *linear predictivity*.
*   **Efficacy of Non-fitting Methods:** Non-fitting methods, such as Representational Similarity Analysis (RSA), were found to yield strong separability across different model families.
*   **Generalizability:** The findings hold valid across diverse architectures (CNNs, Vision Transformers, Swin Transformers, ConvNeXt) and training regimes (supervised vs. self-supervised).

---

## Methodology

The researchers developed a **quantitative framework** to evaluate the discriminative power of representational similarity measures. This framework assesses the ability of metrics to separate different model families by utilizing three complementary separability measures:

1.  **D-prime:** Borrowed from signal detection theory.
2.  **Silhouette coefficients:** Measuring cohesion and separation.
3.  **ROC-AUC:** Evaluating classification performance.

This systematic evaluation was applied to commonly used metrics—including **RSA**, **linear predictivity**, **Procrustes**, and **soft matching**—across a variety of model architectures and training regimes.

---

## Technical Details

| Aspect | Description |
| :--- | :--- |
| **Objective** | Evaluate the discriminative capacity of representational similarity metrics to distinguish between different model families. |
| **Dataset** | 35 vision models categorized into four architectural families (CNNs, ViT, Swin Transformers, ConvNeXt). |
| **Training Paradigms** | Supervised and Self-supervised. |
| **Evaluation Set** | ImageNet-1k test set. |
| **Methods Tested** | RSA (Representational Similarity Analysis), Soft Matching, Procrustes Alignment, Linear Predictivity. |
| **Quantification Metrics** | D-Prime, Silhouette Score, ROC-AUC. |

---

## Results

The study quantified the separability of each method, revealing a clear hierarchy in discriminative capacity.

### Performance Ranking

| Metric | D-Prime | Silhouette Score | ROC-AUC |
| :--- | :---: | :---: | :---: |
| **RSA** | **3.95** | **0.56** | **0.9257** |
| **SoftMatch** | 3.59 | 0.30 | 0.8989 |
| **Procrustes** | 2.96 | 0.22 | 0.9004 |
| **Linear Predictivity** | 2.09 | 0.14 | 0.8273 |

### Analysis
*   **RSA** demonstrated the highest discriminative capacity across all evaluation metrics.
*   **SoftMatch** followed in performance, outperforming Procrustes alignment and Linear Predictivity.
*   The results indicate that **geometric sensitivity** (RSA) is more reliable for distinguishing architectures and training regimes than flexible linear mappings.
*   There is a positive correlation between the stringency of alignment constraints and the resulting discriminative capacity.

---

## Contributions

*   **Introduction of a Novel Evaluation Framework:** The paper introduces a quantitative framework specifically designed to assess representational similarity metrics based on their ability to separate model families, rather than just their absolute similarity scores.
*   **First Systematic Comparison:** The study provides the field's first systematic comparison of similarity metrics viewed through the lens of separability.
*   **Guidance for Metric Selection:** By clarifying the relative sensitivity and discriminative capacity of different metrics, the research offers practical guidance for selecting appropriate measures in large-scale model-to-model and model-to-brain comparisons.

---

**Quality Score:** 8/10
**References:** 29 citations