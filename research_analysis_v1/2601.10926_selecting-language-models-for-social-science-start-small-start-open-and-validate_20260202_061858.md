# Selecting Language Models for Social Science: Start Small, Start Open

*Dustin S. Stoltz; Marshall A. Taylor; Sanuj Kumar*

---

## üìå Quick Facts

| Metric | Value |
| :--- | :--- |
| **Recommendation** | Start Small, Start Open |
| **Key Validation Method** | Ex-Post Validation |
| **Data Center Energy Use (US)** | 4% (2024) ‚Üí 12% (Projected 2028) |
| **Inference Energy Cost** | ~10x a traditional search query |
| **Context Scaling Impact** | Doubling length = 4x resource requirement |
| **Optimization Strategy** | Quantization (32-bit to 8-bit) |
| **Quality Score** | 8/10 |
| **References** | 31 Citations |

---

## Executive Summary

This paper addresses the lack of standardized criteria for social scientists selecting appropriate Language Models (LMs) for research. Current selection methods frequently prioritize high parameter counts and general performance metrics over scientific rigor, leading to the adoption of massive, proprietary "black box" models that hinder reproducibility and transparency.

The authors challenge the assumption that larger, closed models are inherently superior for scientific inquiry. Instead, they argue for a framework grounded in **validity, reliability, and replicability**. The core proposal is a **"Start Small, Start Open"** paradigm, advocating for a strategic shift toward smaller, open-source models.

**Key Innovations:**
*   **Selection Framework:** Evaluates models based on openness, environmental footprint, and architectural fit.
*   **Delimited Benchmarks:** Datasets constructed with high human agreement thresholds (95%) to validate the entire computational pipeline *ex-post*.
*   **Efficiency Analysis:** Highlights substantial environmental costs and technical scaling laws, demonstrating that optimization techniques like quantization can save RAM without significant quality loss.

By advocating for smaller, open-source models and rigorous validation, this framework democratizes access to research tools, enhances reproducibility, and ensures that language models serve the explicit epistemic standards of social science.

---

## Key Findings

*   **Scientific Rigor Over Benchmarks:** Social scientists must perform *ex-post* validation rather than relying on general benchmarks which often fail to meet specific epistemic standards.
*   **Replicability as Priority:** Replicability is a more critical criterion for model selection than general performance metrics.
*   **Holistic Validation:** The necessity of task-specific validation requires validating the **entire computational pipeline** rather than relying on pre-trained capabilities.
*   **The "Small and Open" Advantage:** A preference for 'small and open' models is superior for social science research due to superior reproducibility and transparency compared to proprietary black boxes.

---

## Methodology

The study employs a dual-pronged analytical approach:

1.  **Conceptual Framework Analysis:**
    *   Applies core scientific concepts of **validity, reliability, reproducibility, and replicability**.
    *   Uses these concepts as a lens to rigorously evaluate language models.

2.  **Attribute-Based Evaluation:**
    *   Assesses the significance of:
        *   **Model Openness:** Source code and weights availability.
        *   **Model Footprint:** Environmental and computational costs.
        *   **Training Data Composition:** Origin and nature of data.
        *   **Architectures & Fine-tuning:** Structural variants and adaptation methods.

---

## Contributions

*   **Selection Framework:**
    *   Provides a structured guideline for social scientists to navigate the LLM marketplace based on scientific standards rather than marketing hype.
*   **The 'Start Small, Start Open' Paradigm:**
    *   Advocates for a strategic shift toward smaller, open-source models to ensure transparency and reduce environmental impact.
*   **Validation Protocol Proposal:**
    *   Proposes constructing **'delimited benchmarks'** designed to demonstrate the validity of specific computational pipelines with high human agreement thresholds.

---

## Technical Details

### üèóÔ∏è Core Architecture
*   **Definition:** Probabilistic algorithms utilizing neural networks to predict the next token.
*   **Components:** Tokenizers, embeddings, and a stack of matrix layers.
*   **Architectural Variants:**
    *   Encoder-only
    *   Decoder-only
    *   Encoder-decoder

### üîÑ Training Pipeline
1.  **Pretraining:** On generic corpora (e.g., Common Crawl).
2.  **Fine-tuning:** On specific tasks.
3.  **Output:** Language modeling block.

### üìê Context & Scaling
*   **Context Management:** Uses Rotational Position Embeddings (RoPE) to handle long contexts.
*   **Vulnerabilities:**
    *   Quadratic scaling issues.
    *   "Lost in the middle" dilution.
*   **Scaling Law:** Doubling context length increases resource requirements by a factor of **four (4x)**.

### ‚ö° Optimization & Risks
*   **Quantization:** Reduces bit precision (e.g., 32-bit to 8-bit) to save RAM (Formula: `Parameters x Bits`), often without quality loss.
*   **Synthetic Data Risks:** Training on synthetic data risks **'Model Collapse'** due to reduced variance.
*   **Validation:** Involves 'Ex-Post' analysis using benchmarks like GLUE and MMLU.

---

## Results

*   **Benchmark Validity:** Studies indicate minimal time savings for tasks like Radiology AI and scrutiny over the claimed novelty of materials discoveries.
*   **Benchmark Construction:** Ambiguity thresholds are set by crowdworker agreement (**95%** for clear-cut values).
*   **Environmental Metrics:**
    *   US data centers increased energy usage from **2% (2018)** to **4% (2024)**.
    *   Projected to reach up to **12% by 2028**.
    *   Inference for ChatGPT consumes approximately **ten times** the energy of a traditional search query.
*   **Computational Scaling:**
    *   Doubling context length increases resource requirements by a factor of four.
    *   Historical parameter counts show GPT-2 evolving from 124M to 1.5B parameters.
    *   Fine-tuning performance generally improves with increased data volume.

---

## Evaluation

**Quality Score:** 8/10  
**Total References:** 31 citations