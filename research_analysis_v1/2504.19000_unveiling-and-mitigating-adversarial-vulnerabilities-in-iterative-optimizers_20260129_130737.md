# Unveiling and Mitigating Adversarial Vulnerabilities in Iterative Optimizers

*Elad Sofer; Tomer Shaked; Caroline Chaux; Nir Shlezinger*

---

### Quick Facts

| Metric | Details |
| :--- | :--- |
| **Quality Score** | 9/10 |
| **References** | 40 Citations |
| **Key Frameworks** | Deep Unfolding (LISTA, LADMM) |
| **Dataset** | Yale Face Database |
| **Iterations ($T$)** | 6 |
| **Regularization ($\rho$)** | 0.01 |

---

## Executive Summary

### Problem
This research addresses a critical security gap regarding the susceptibility of non-learned iterative optimizers—fundamental algorithms in signal processing such as ISTA and ADMM—to adversarial attacks. While deep neural networks are recognized as vulnerable to imperceptible input perturbations, this paper demonstrates that classical, hand-crafted optimization algorithms possess a universal vulnerability of similar magnitude. This issue is of paramount importance because these solvers form the backbone of critical applications in sparse recovery and matrix factorization; their compromise implies that the mathematical foundations relied upon in scientific computing and engineering are not inherently secure, creating a risk where small, targeted noise in input data can lead to catastrophic failures in output accuracy.

### Innovation
The key innovation proposed is **"Adversarial-Aware Unfolding,"** a defense mechanism that bridges classical signal processing with modern machine learning robustness techniques. The authors utilize the framework of deep unfolding to convert non-learned, fixed iterative algorithms (like ISTA and ADMM) into trainable deep network architectures (specifically LISTA and LADMM with $T=6$ iterations). Technical analysis reveals that adversarial attacks function by perturbing the input signal ($x \rightarrow x + \delta$), thereby altering the optimization objective surface and shifting the minima sought by the algorithm. The proposed defense mitigates this by applying adversarial training to the unfolded models; this process implicitly reduces the optimizer’s Lipschitz continuity constant, which determines the sensitivity of proximal gradient optimizers, thereby stabilizing the convergence path against input perturbations.

### Results
The study validates its findings through numerical experiments on Sparse Recovery (formulated as a LASSO problem with regularization $\rho=0.01$) and RobustPCA (using the AccAltProj optimizer on the Yale Face Database). In Sparse Recovery tests involving 5-sparse vectors over 1200 trials, adversarial perturbations set to $\epsilon=0.025$ produced inputs indistinguishable from clean data but caused high Euclidean distortion in estimates and slowed convergence for vanilla ISTA. Under BIM, CW, and NIFGSM attacks, RPCA tests resulted in imperceptible input alterations yet induced specific output degradation such as blurring key facial features. The proposed **"Robust-LISTA"** and **"Robust-LADMM"** models demonstrated enhanced robustness compared to their vanilla counterparts with measurable performance retention on clean data. Notably, while vanilla LADMM proved more resilient than vanilla ADMM, vanilla LISTA was observed to be more sensitive than ISTA; empirical analysis confirmed that adversarial training successfully reduced the normalized Lipschitz constant, directly correlating with lower adversarial sensitivity.

### Impact
This paper significantly broadens the understanding of adversarial vulnerabilities by extending the scope beyond learned black-box models to include fundamental, non-learned decision rules and mathematical solvers. It provides rigorous theoretical insight into the relationship between an optimizer’s architecture—specifically proximal gradient methods—and its susceptibility to input perturbations. By demonstrating that machine learning strategies like adversarial training can be effectively ported to classical signal processing algorithms via deep unfolding, the work establishes a new paradigm for securing iterative optimization. This influence is likely to drive future research in designing intrinsically robust solvers, ensuring that the reliance on optimization algorithms in safety-critical systems is no longer a potential weak point against adversarial manipulation.

---

## Key Findings

*   **Universal Vulnerability:** Non-learned iterative optimizers possess a universal vulnerability to adversarial examples, similar to traditional machine learning models.
*   **Attack Mechanism:** Adversarial attacks function by altering the optimization objective surface, which effectively shifts the minima sought by the algorithm.
*   **Defense Strategy:** Deep unfolding enables the use of adversarial training to significantly enhance the robustness of these optimizers.
*   **Theoretical Validation:** Theoretical analysis confirms that learning processes directly impact adversarial sensitivity in proximal gradient optimizers.

---

## Methodology

The study employs a multi-faceted approach to analyze and mitigate vulnerabilities:

1.  **Framework Application:** Utilizes the **deep unfolding** framework to analyze non-learned iterative optimizers as machine learning models.
2.  **Interaction Analysis:** Investigates the specific interaction between adversarial perturbations and the optimization objective surface.
3.  **Mathematical Rigor:** Applies rigorous mathematical analysis specifically to proximal gradient optimizers to derive theoretical bounds.
4.  **Experimental Support:** Supports theoretical findings with extensive numerical experiments.

---

## Contributions

*   **Broadened Understanding:** Expands the definition of adversarial vulnerabilities to include fundamental, non-learned decision rules and iterative algorithms.
*   **Novel Defense Mechanism:** Introduces a defense that bridges signal processing and machine learning, allowing researchers to 'train' optimizers for robustness.
*   **Theoretical Insight:** Provides rigorous theoretical insight into the relationship between optimizer architecture and susceptibility to adversarial attacks.

---

## Technical Details

### Attack Mechanics
*   Adversarial attacks function by perturbing the input signal ($x \rightarrow x + \delta$).
*   This perturbation alters the optimization objective surface.
*   Sensitivity varies significantly based on the specific solver used.

### Problem Formulations
*   **Sparse Recovery:** Formulated as a LASSO problem using the $\ell_1$ norm ($\rho=0.01$).
*   **RPCA:** Uses a non-convex formulation to separate low-rank and sparse matrices via the AccAltProj optimizer.

### Mitigation Strategy: Adversarial-Aware Unfolding
*   Unfolds standard optimizers into trainable ML models:
    *   ISTA $\rightarrow$ LISTA / Robust-LISTA
    *   ADMM $\rightarrow$ LADMM / Robust-LADMM
*   Configuration: $T=6$ iterations.
*   Models are trained explicitly on adversarial examples.

### Theoretical Basis
*   The paper validates that the **Lipschitz continuity constant** determines the sensitivity of proximal gradient optimizers.
*   Adversarial training implicitly reduces this constant, thereby reducing sensitivity.

---

## Results

### Sparse Recovery Experiments
*   **Setup:** Tests on 5-sparse vectors (1200 trials) using BIM and NIFGSM attacks.
*   **Input:** Perturbations at $\epsilon=0.025$ were indistinguishable from clean data.
*   **Output:** Significant increase in Euclidean distortion between estimates and noticeably slowed ISTA convergence.

### RPCA Experiments (Yale Face Database)
*   **Attacks:** BIM and CW attacks caused imperceptible input perturbations.
*   **Damage:** Severe output degradation, specifically blurring key facial features.

### Model Performance
*   **Robust Variants:** 'Robust-LISTA' and 'Robust-LADMM' showed higher robustness than vanilla counterparts with only minor clean data performance loss.
*   **Sensitivity Anomalies:**
    *   LISTA was sometimes *more* sensitive than vanilla ISTA.
    *   LADMM was *more* resilient than vanilla ADMM.
*   **Empirical Analysis:** Confirmed that adversarial training reduces the normalized Lipschitz constant, directly correlating with reduced adversarial sensitivity.