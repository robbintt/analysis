---
title: 'VLMQ: Efficient Post-Training Quantization for Large Vision-Language Models
  via Hessian Augmentation'
arxiv_id: '2508.03351'
source_url: https://arxiv.org/abs/2508.03351
generated_at: '2026-02-03T19:15:02'
quality_score: 9
citation_count: 31
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# VLMQ: Efficient Post-Training Quantization for Large Vision-Language Models via Hessian Augmentation

*Yufei Xue; Yushi Huang; Jiawei Shao; Jun Zhang*

---

### üìä Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Focus** | Post-Training Quantization (PTQ) for VLMs |
| **Key Innovation** | Hessian Augmentation via Importance-Aware Objective |
| **Model Scale** | 0.5B ‚Äì 32B Parameters |
| **Top Gain** | **+16.45%** on MME-RealWorld (2-bit) |
| **Benchmarks** | 8 Benchmarks Validated |
| **Quality Score** | 9/10 |

---

## üìù Executive Summary

This research addresses the critical challenge of efficiently quantizing Large Vision-Language Models (VLMs) using Post-Training Quantization (PTQ). While Hessian-based PTQ methods have proven effective for Large Language Models (LLMs), they fail when applied to VLMs due to a **"modality discrepancy."** This discrepancy arises because VLMs process a massive number of vision tokens compared to limited text tokens. Existing methods treat all tokens uniformly, ignoring this imbalance and the resulting redundancy in visual data, which leads to significant performance degradation when compressing models to low-bit formats.

The authors propose **VLMQ**, the first importance-aware PTQ framework specifically designed to handle the architectural nuances of VLMs. The core technical innovation is an **Importance-Aware Objective** that augments the standard Hessian matrix with a diagonal token-level importance weighting matrix, denoted as $G$. By modifying the Hessian calculation to $eH = X G X^T$, VLMQ applies differential weighting to tokens during the optimization process, effectively mitigating visual over-representation. To maintain computational efficiency, the framework computes these importance factors via a lightweight block-wise backward pass, utilizing the $l_1$-norm of gradients derived from localized block losses between full-precision and semi-quantized attention outputs.

VLMQ achieves state-of-the-art performance across eight benchmarks for models ranging from 0.5B to 32B parameters. In aggressive low-bit settings (2-bit), the framework delivers a substantial **16.45%** improvement on the MME-RealWorld benchmark compared to current baselines. Additionally, in a pilot study utilizing Qwen2-VL-7B-Instruct quantized to INT3, VLMQ achieved 88.90% accuracy on the DocVQA task, outperforming the standard baseline (88.09%) and the best manual random weighting strategy (88.48%). These metrics validate the framework's ability to preserve multimodal reasoning capabilities even under significant compression.

This work significantly advances the field by formally identifying and defining "modality discrepancy" as a fundamental bottleneck in existing quantization literature. By establishing a theoretical link between token-level perturbations and importance factors, VLMQ provides a robust, mathematically grounded solution for minimizing quantization error in multimodal systems. The ability to perform low-bit quantization without sacrificing accuracy enables the deployment of massive VLMs on resource-constrained edge devices, setting a new standard for future research and development in efficient multimodal AI.

---

## üîë Key Findings

*   **Modality Discrepancy:** Identified that existing Hessian-based PTQ methods for LLMs fail on VLMs because they ignore the difference between limited text tokens and excessive vision tokens.
*   **State-of-the-Art Performance:** VLMQ achieves SOTA performance across **8 benchmarks** and models ranging from **0.5B to 32B parameters**, particularly excelling in low-bit settings.
*   **Significant Improvement:** Under **2-bit quantization**, VLMQ delivers a **16.45% improvement** on the MME-RealWorld benchmark compared to baselines.
*   **Efficiency:** The framework effectively addresses vision token redundancy without sacrificing computational efficiency by utilizing a lightweight backward pass.

---

## ‚öôÔ∏è Methodology

The authors propose VLMQ, an importance-aware Post-Training Quantization framework tailored for VLMs. The methodology consists of two primary components:

*   **Importance-Aware Objective:** Optimizes a novel objective function to generate an enhanced Hessian matrix. This matrix incorporates token-level importance factors, allowing for differential token weighting during quantization while remaining compatible with parallelized weight updates.
*   **Lightweight Factor Computation:** Calculates token-level importance factors via a single lightweight block-wise backward pass. This is based on a theoretical connection to token-level perturbations.

---

## üîß Technical Details

VLMQ addresses Hessian-based PTQ failures in VLMs caused by **Visual Over-Representation** using an Importance-Aware Objective.

*   **Core Formula:** Introduces a diagonal token-level importance weighting matrix $G$ to refine the Hessian calculation:
    $$eH = X G X^T$$
*   **Granularity:** The method operates at token-level granularity.
*   **Computation:** Computes importance factors via a lightweight backward pass using the **$l_1$-norm of gradients**. These gradients are derived from a localized block loss between full-precision and semi-quantized attention outputs.

---

## üìà Results

VLMQ demonstrates robust empirical validation across various model sizes and tasks:

*   **Overall Performance:** Achieved State-of-the-Art (SOTA) performance across 8 benchmarks for models ranging from 0.5B to 32B parameters.
*   **Low-Bit Gains:** Under 2-bit quantization, delivered a **16.45% improvement** on the MME-RealWorld benchmark compared to baselines.
*   **Case Study (Qwen2-VL-7B-Instruct-INT3):**
    *   **VLMQ Accuracy:** 88.90%
    *   **Baseline Accuracy:** 88.09%
    *   **Best Manual Random Weighting:** 88.48%
    *   *Task:* DocVQA

---

## ‚ú® Contributions

*   **Problem Definition:** Identifies and formally defines **'modality discrepancy'** as a critical failure point for current LLM-centric PTQ methods.
*   **Framework Innovation:** Introduces **VLMQ**, the first importance-aware PTQ framework designed to handle the architectural and token distribution differences of VLMs.
*   **Theoretical Foundation:** Provides a theoretical foundation by connecting token-level perturbations to importance factors for precise error minimization.
*   **Comprehensive Validation:** Offers extensive empirical validation across a wide spectrum of model sizes (0.5B‚Äì32B), establishing a new performance standard for low-bit VLM quantization.

---
*Quality Score: 9/10 | References: 31 citations*