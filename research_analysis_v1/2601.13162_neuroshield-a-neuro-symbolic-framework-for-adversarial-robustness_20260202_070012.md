# NeuroShield: A Neuro-Symbolic Framework for Adversarial Robustness

*Ali Shafiee Sarvestani; Jason Schmidt; Arman Roohi*

---

> ### ðŸ“Š Quick Facts
>
> *   **Quality Score:** 9/10
> *   **Dataset:** GTSRB (German Traffic Sign Recognition Benchmark)
> *   **Core Architecture:** Modified ResNet18
> *   **Training Efficiency:** 10 Epochs
> *   **Key Performance:** +18.1% (FGSM) & +17.35% (PGD) Adversarial Accuracy
> *   **Perturbation Budget:** $\epsilon = 8/255$

---

## Executive Summary

Deep Neural Networks (DNNs) suffer from two critical limitations that hinder their deployment in safety-critical environments: susceptibility to adversarial attacks and a lack of interpretability. While standard adversarial training is the prevailing defense, it often requires substantial computational resources and typically degrades model accuracy on clean, unperturbed data. This research addresses the need for a defensive mechanism that does not force a trade-off between robustness and standard performance, while also making the model's decision-making process more transparent and logically grounded.

The paper introduces **NeuroShield**, a neuro-symbolic framework that integrates symbolic rule supervision directly into the neural training pipeline (designated as DesignII). Technically, the system employs a Modified ResNet18 backbone that preserves spatial resolution (using $3 \times 3$ kernels) and features a multi-head architecture with a classifier and parallel attribute heads (lightweight MLPs). The core innovation lies in the training objective, which combines standard Cross-Entropy with a Semantic Loss and a Symbolic Logic Loss. These losses enforce domain knowledge encoded as logical constraints (e.g., relationships between shape and color attributes) directly onto the feature vector, ensuring the model's predictions are logically consistent during end-to-end differentiation.

Evaluated on the GTSRB dataset against Fast Gradient Sign Method (FGSM) and Projected Gradient Descent (PGD) attacks ($\epsilon = 8/255$), the NeuroShield models achieved significant improvements over standard adversarial training baselines. The FGSM-Neuro-Symbolic and PGD-Neuro-Symbolic variants improved adversarial accuracy by **18.1%** and **17.35%**, respectivelyâ€”representing a robustness gain roughly three times larger than standard methods relative to a clean-training baseline. Crucially, these improvements were achieved without any reduction in accuracy on clean samples. Furthermore, the PGD-Neuro-Symbolic model, trained for only 10 epochs, demonstrated comparable or superior robustness to heavy transformer-based defenses like LNL-MoEx.

This research validates the hypothesis that incorporating symbolic reasoning via logical constraints is an effective pathway toward robust AI, addressing both adversarial vulnerability and the "black box" nature of DNNs simultaneously. By demonstrating that a ResNet18-based model with symbolic supervision can outperform complex transformer architectures requiring extensive augmentation, NeuroShield challenges the assumption that state-of-the-art robustness necessitates massive computational resources.

---

## Key Findings

*   **Superior Robustness Gains:** The proposed FGSM-Neuro-Symbolic and PGD-Neuro-Symbolic models improved adversarial accuracy by **18.1%** and **17.35%** respectively over standard adversarial-training baselines.
*   **Multi-fold Improvement:** The robustness gains achieved by the neuro-symbolic approach were roughly **three times larger** than those provided by standard adversarial training when measured relative to a clean-training baseline.
*   **Retention of Clean Accuracy:** The framework achieved these significant adversarial improvements **without reducing accuracy** on clean samples.
*   **Efficiency vs. Heavy Architectures:** The PGD-Neuro-Symbolic variant (using a ResNet18 backbone trained for only 10 epochs) attained comparable or superior robustness to transformer-based defenses like LNL-MoEx, which require heavy architectures and extensive data augmentation.

---

## Methodology

The research employs a structured approach to integrating logic into deep learning:

*   **Framework:** A neuro-symbolic framework (labeled as **DesignII**) that integrates symbolic rule supervision directly into neural network training.
*   **Knowledge Encoding:** Domain knowledge is encoded as logical constraints over appearance attributes, specifically **shape** and **color**.
*   **Loss Functions:** The framework enforces these logical constraints through Semantic and Symbolic logic losses applied during the training process.
*   **Evaluation Protocol:** Robustness was evaluated on the **GTSRB dataset** against Fast Gradient Sign Method (FGSM) and Projected Gradient Descent (PGD) attacks using a standard $l_{\infty}$ perturbation budget of $\epsilon = 8/255$.

---

## Technical Specifications

### Architecture
*   **Backbone:** Modified ResNet18.
    *   Initial $7 \times 7$ convolution replaced with a $3 \times 3$ kernel.
    *   Max pooling removed to preserve spatial resolution.
*   **Multi-Head Setup:**
    *   **Classifier Head:** Standard output.
    *   **Symbolic Attribute Heads:** Parallel, lightweight MLPs operating on a shared 512-dimensional feature vector.

### Training & Optimization
*   **Differentiability:** The system is fully end-to-end differentiable.
*   **Symbolic Supervision:** Utilizes dictionaries encoding soft implications.
*   **Loss Components:**
    1.  **Cross-Entropy:** Standard classification loss.
    2.  **Semantic Loss:** KL divergence for equivalent class consistency.
    3.  **Symbolic Logic Loss:** Ensures consistency between class and attribute confidence.
*   **Adversarial Generation:** Examples are generated on-the-fly using FGSM or PGD with a perturbation budget of $\epsilon = 8/255$.

---

## Research Contributions

1.  **Dual-Problem Solution:** Addresses the two critical limitations of deep neural networksâ€”adversarial vulnerability and lack of interpretabilityâ€”simultaneously within a single framework.
2.  **Validation of Symbolic Reasoning:** Provides empirical evidence that incorporating symbolic reasoning (via logical constraints) is an effective path toward robust AI, outperforming standard data-driven adversarial training.
3.  **Resource-Efficient Defense:** Challenges the necessity of heavy computational resources for strong defenses, demonstrating that a ResNet18-based model with symbolic supervision can outperform complex transformer-based methods.

---

**Document Statistics**
*   **Quality Score:** 9/10
*   **Total References:** 31 citations