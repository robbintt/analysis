---
title: Dataset Distillation for Quantum Neural Networks
arxiv_id: '2503.17935'
source_url: https://arxiv.org/abs/2503.17935
generated_at: '2026-02-06T04:24:32'
quality_score: 9
citation_count: 23
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Dataset Distillation for Quantum Neural Networks

*Koustubh Phalak; Junde Li; Swaroop Ghosh*

***

> ### ðŸ’¡ Quick Facts
> *   **Quality Score:** 9/10
> *   **References:** 23 Citations
> *   **Core Innovation:** Application of dataset distillation to Quantum Neural Networks (QNNs).
> *   **Performance Gap:** Maintains parity with classical baselines within a 2-4% margin.
> *   **Key Benefit:** Significant reduction in quantum execution costs and training time.

---

## Executive Summary

Training Quantum Neural Networks (QNNs) presents a significant computational bottleneck due to the high execution costs and extended training times associated with running gradient descent on quantum hardware. As QNNs are typically trained on large classical datasets, the sheer volume of data required exacerbates these issues, leading to prohibitive financial and time expenses.

This paper addresses the critical challenge of reducing the resource overhead of QNN training while maintaining model performance, seeking to bridge the efficiency gap between quantum and classical machine learning paradigms. The authors introduce the first application of dataset distillation to the quantum computing domain, compressing large classical datasets like MNIST and CIFAR-10 into significantly smaller, synthetic subsets.

The core innovation is a specialized Quantum LeNet architecture that integrates residual connections and a trainable Hermitian observable within its Parametric Quantum Circuit (PQC). To ensure numerical stability during the distillation processâ€”a known challenge in quantum gradientsâ€”the methodology incorporates a non-trainable Hermitian component. This framework allows the QNN to learn from distilled data rather than full datasets, drastically reducing the number of quantum executions required.

The proposed Quantum LeNet demonstrated strong performance parity with classical baselines. On MNIST, the distilled-data model achieved 91.9% accuracy compared to a 94% classical baseline (a -2.1% gap), while on CIFAR-10 it reached 50.3% against a 54% baseline (a -3.7% gap). While the introduction of the non-trainable Hermitian observable ensured process stability, it incurred a marginal trade-off in optimization capacity, resulting in accuracy reductions of up to 1.8% on MNIST and 1.3% on CIFAR-10.

Crucially, the approach successfully minimized the number of gradient descent steps and quantum circuit executions, confirming substantial efficiency gains. This research establishes dataset distillation as a viable strategy for mitigating the high costs of quantum machine learning.

---

## Key Findings

*   **Effective Distillation for QNNs:** Dataset distillation can be effectively applied to Quantum Neural Networks, yielding a significantly smaller set of highly informative training data while maintaining performance comparable to using the original full dataset.
*   **Strong Performance Parity:** The proposed quantum LeNet model achieved post-inferencing accuracies of **91.9%** on MNIST and **50.3%** on Cifar-10. These results are reasonably close to the classical LeNet baselines of 94% and 54%, respectively.
*   **Cost and Time Efficiency:** By reducing the amount of training data, the approach minimizes the number of required gradient descent steps and quantum executions, effectively addressing the high time and financial costs associated with training QNNs.
*   **Stability vs. Accuracy Trade-off:** Introducing a non-trainable Hermitian observable successfully ensured stability during the distillation process, though it resulted in a marginal reduction in accuracy (up to **1.8%** for MNIST and **1.3%** for Cifar-10).

---

## Methodology

The researchers utilized a dataset distillation framework specifically tailored for Quantum Neural Networks to compress large classical datasets (MNIST and Cifar-10) into smaller, synthetic subsets.

*   **Core Architecture:** Implementation of a novel quantum variant of the classical LeNet model.
*   **Circuit Design:** Incorporates residual connections and a trainable Hermitian observable within its Parametric Quantum Circuit (PQC).
*   **Stability Mechanism:** Introduction of a non-trainable Hermitian component to guarantee stability throughout the distillation process.
*   **Validation:** The distilled datasets were used to train the quantum models, with performance evaluated against classical LeNet baselines.

---

## Technical Details

**Core Technique**
*   **Dataset Distillation with Quantum LeNet:** A method for compressing image classification datasets into smaller subsets for efficient training.

**Architecture Components**
*   **Residual Connections:** Integrated into the quantum model to enhance learning capabilities.
*   **Hermitian Observables:**
    *   *Trainable:* Part of the standard PQC design.
    *   *Non-trainable:* Implemented specifically to stabilize the distillation process.

**Optimization & Constraints**
*   **Stability Mechanism:** The non-trainable Hermitian observable limits optimization capacity slightly but is necessary for numerical stability in quantum gradients.
*   **Efficiency Goal:** Reducing computational overhead by minimizing gradient descent steps and quantum circuit executions.

---

## Experimental Results

The study evaluated the Quantum LeNet against classical baselines on two major datasets:

| Dataset | Quantum Model Accuracy | Classical Baseline | Performance Gap | Stability Cost (Accuracy Loss) |
| :--- | :---: | :---: | :---: | :---: |
| **MNIST** | 91.9% | 94% | -2.1% | Up to 1.8% |
| **CIFAR-10** | 50.3% | 54% | -3.7% | Up to 1.3% |

**Outcome:** Significant efficiency gains were observed through reduced training data volume, gradient descent steps, and quantum circuit executions, with an acceptable trade-off in accuracy.

---

## Core Contributions

1.  **Novel Application of Distillation to QNNs:** Introduces the concept of dataset distillation to the quantum computing domain, offering a solution to the bottleneck of high execution costs and training time associated with QNNs.
2.  **Architecture Innovation:** Proposes a specialized quantum LeNet architecture enhanced with residual connections and trainable Hermitian observables, expanding the toolkit for quantum circuit design.
3.  **Validation of Quantum-Classic Parity:** Provides empirical evidence that QNNs trained on distilled data can achieve inference accuracy comparable to classical models, validating the feasibility of using distilled datasets to reduce quantum resource overhead without significant performance degradation.

---

**Document Rating:** 9/10 | **References:** 23 Citations