---
title: 'Xmodel-2.5: 1.3B Data-Efficient Reasoning SLM'
arxiv_id: '2511.19496'
source_url: https://arxiv.org/abs/2511.19496
generated_at: '2026-02-03T13:46:03'
quality_score: 8
citation_count: 7
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Xmodel-2.5: 1.3B Data-Efficient Reasoning SLM
*Yang Liu; Xiaolong Zhong; Ling Jiang*

---

### üìä Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Model Size** | 1.3 Billion Parameters |
| **Training Data** | 1.4 Trillion Tokens |
| **Architecture** | Decoder-only, Deep-and-thin |
| **Reasoning Boost** | **+4.58%** (Muon Optimizer) |
| **Efficiency Gain** | **~30%** Throughput (FP8) |
| **License** | Apache-2.0 (Open Source) |
| **Tokenizer** | DeepSeek-v3 |

---

> ### üìù Executive Summary
>
> Developing high-performance Small Language Models (SLMs) suitable for edge and cost-sensitive deployments remains a significant challenge, particularly when optimizing for reasoning capabilities within a limited parameter budget. Training efficiency is a major bottleneck, as standard optimization techniques often fail to maximize reasoning performance without substantial computational overhead. Furthermore, the process of hyperparameter tuning for billion-parameter models is notoriously expensive and resource-intensive, creating a need for methodologies that can transfer optimal settings from smaller, computationally affordable proxy models to larger production-scale systems.
>
> The authors introduce **Xmodel-2.5**, a 1.3B-parameter decoder-only model employing a "deep-and-thin" architecture with Pre-RMSNorm, SwiGLU, and parameter-tied word embeddings. The core technical innovation lies in a hybrid optimization strategy and the use of **Maximal-update Parameterization ($\mu$P)**. The team utilized $\mu$P to derive hyperparameters from a 20M-parameter proxy model, successfully transferring them to the 1.3B scale. Additionally, they implemented a hybrid optimizer schedule: AdamW is used during the initial Warmup and Stable phases for training stability, followed by a switch to the **Muon optimizer** during the Decay phase to enhance reasoning sharpness. The training pipeline also utilizes FP8-mixed precision and a 1.4T-token curriculum divided into Warmup, Stable, and Decay phases.
>
> The experimental validation demonstrates significant improvements in both performance and training efficiency. Switching to the Muon optimizer during the decay phase yielded a substantial **4.58% improvement** on a 13-task reasoning average benchmark. From an efficiency standpoint, the implementation of FP8-mixed precision training increased throughput by approximately **30%** with no observable degradation in accuracy. The study also confirmed the viability of the $\mu$P approach, as hyperparameters tuned on the small proxy model transferred effectively to the full 1.3B model, validating the method's scalability over the 1.4T-token training run.

---

## üîç Key Findings

*   **Optimizer Innovation:** Switching the optimizer from AdamW to Muon during the decay phase resulted in a **4.58% improvement** on a 13-task reasoning average.
*   **Scalable Tuning:** Maximal-update parameterization ($\mu$P) enabled hyper-parameters tuned on a 20M-parameter proxy to transfer successfully to a 1.3B-parameter model.
*   **Precision Efficiency:** FP8-mixed-precision training balanced accuracy with throughput, increasing training speed by ~30% without degrading performance.
*   **Robust Curriculum:** The model achieved strong reasoning capabilities using a 1.4T-token **Warmup--Stable--Decay** curriculum.

---

## üõ†Ô∏è Methodology

The development of Xmodel-2.5 focused on efficiency and reasoning transferability through the following approach:

*   **Architecture:** Development of a 1.3B-parameter SLM using a parameter-tied word-embedding architecture.
*   **Parameterization:** Utilized maximal-update parameterization ($\mu$P) derived from a smaller 20M proxy model to scale hyperparameters effectively.
*   **Curriculum:** Training involved a massive 1.4T-token curriculum split into three distinct phases:
    1.  **Warmup**
    2.  **Stable**
    3.  **Decay**
*   **Optimization Strategy:** Implemented a hybrid optimization schedule using AdamW in the early phases for stability and transitioning to Muon in the late phase for performance.
*   **Compute Efficiency:** Employed FP8-mixed precision throughout the training process.

---

## ‚öôÔ∏è Technical Details

**Architecture & Components**
*   **Type:** 1.3B-parameter decoder-only model.
*   **Design:** "Deep-and-thin" architecture.
*   **Normalization:** Pre-RMSNorm.
*   **Activation:** SwiGLU components.
*   **Tokenizer:** DeepSeek-v3.

**Training Infrastructure**
*   **Precision:** FP8-mixed precision.
*   **Attention:** Transitioned from Flash-Attention v2 to a CuDNN backend.
*   **Parameterization:** Maximal-Update Parameterization ($\mu$P) with attention-logits scaling.

**Optimization Schedule**
*   **Hybrid Optimizer:** Starts with **AdamW** (early stages) and switches to **Muon** (late decay stage).
*   **Schedule:** Three-phase Warmup--Stable--Decay over 1.4T tokens.
*   **Data Mixing:** Domain-weighted data mixing strategy.

---

## üìà Results

*   **Reasoning Performance:** Switching to the Muon optimizer during the decay phase yielded a **+4.58%** improvement on the 13-task reasoning average benchmark.
*   **Training Efficiency:** The use of FP8-mixed precision increased training throughput by approximately **30%** with no observable degradation in accuracy.
*   **Hyperparameter Transfer:** Hyper-parameters tuned on a 20M-parameter proxy model transferred successfully to the 1.3B production model.
*   **Scale:** The model was successfully trained on a total of **1.4T tokens**.

---

## üöÄ Contributions

*   **Open Source:** Full release of checkpoints, training recipes, and evaluation code under the **Apache-2.0** license.
*   **Optimization Technique:** Introduction and verification of a hybrid optimization technique combining AdamW stability with late Muon sharpening.
*   **Cost-Effective AI:** Demonstration of a cost-effective, high-performance SLM optimized specifically for edge and cost-sensitive deployments.

---

**Paper Quality Score:** 8/10  
**References:** 7 citations