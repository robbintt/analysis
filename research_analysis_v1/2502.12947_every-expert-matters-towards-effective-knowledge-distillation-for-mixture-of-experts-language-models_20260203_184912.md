---
title: 'Every Expert Matters: Towards Effective Knowledge Distillation for Mixture-of-Experts
  Language Models'
arxiv_id: '2502.12947'
source_url: https://arxiv.org/abs/2502.12947
generated_at: '2026-02-03T18:49:12'
quality_score: 8
citation_count: 26
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Every Expert Matters: Towards Effective Knowledge Distillation for Mixture-of-Experts Language Models

*Gyeongman Kim; Gyouk Chu; Eunho Yang*

---

> ### ðŸ“Š Quick Facts
> ---
> *   **Quality Score:** 8/10
> *   **Citations:** 26 references
> *   **Teacher Model:** Llama-MoE-3.5B
> *   **Student Model:** Sheared-Llama
> *   **Key Metric:** Activated experts account for **<50%** of gate probabilities
> *   **Optimal Configuration:** Activating all experts ($k=16$)
> *   **Training Objective:** Reverse KL Divergence

---

## Executive Summary

Knowledge Distillation (KD) is the standard technique for compressing large language models into smaller student models, yet it remains fundamentally suboptimal for Mixture-of-Experts (MoE) architectures. The core issue lies in the sparse activation nature of MoEs, where typically only Top-$k$ experts are utilized during a forward pass. Conventional KD methods rely solely on the outputs of these activated experts, effectively ignoring the vast majority of parameters within the non-activated experts. This results in a significant loss of information, as the student model fails to learn the valuable knowledge embedded in the dormant expert pool, leading to inefficient model compression and subpar student performance.

To address this, the authors introduce a tailored KD framework designed to extract knowledge from the entire MoE layer. The key technical innovation is **Knowledge Augmentation (KA)**, a mechanism that forces the teacher model to perform $M$ forward passes rather than a single pass. During these passes, KA probabilistically selects $N-1$ experts and applies a masking function to set non-selected logits to $-\infty$. This process ensures broad coverage of the expert pool. Furthermore, the framework incorporates a **Student-Aware Router (SAR)** and utilizes Reverse KL Divergence for training, allowing the student to learn from a holistic representation of the teacherâ€™s capabilities.

Experimental validation using Llama-MoE models quantified the inefficiency of standard routing, revealing that activated experts consistently account for less than 50% of gate probabilities. In distillation experiments where a Llama-MoE-3.5B teacher trained a Sheared-Llama student, the proposed method demonstrated a clear performance advantage. The researchers observed that increasing the number of active experts ($k$) during distillation monotonically improved Average ROUGE-L scores. Notably, activating all experts ($k=16$) achieved the highest performance, empirically validating the hypothesis that knowledge from non-activated experts is critical for effective student learning.

---

## Key Findings

*   **Value of Dormant Experts:** Experts within MoE models that are *not* activated during a standard forward pass possess valuable, distinct knowledge that can significantly benefit student models.
*   **Suboptimal Conventional KD:** Traditional Knowledge Distillation techniques are ineffective for compressing MoE models because they fail to leverage the knowledge contained in non-activated experts.
*   **Superior Performance:** The proposed methods, which extract knowledge from the entire expert pool rather than a sparse subset, consistently outperform traditional KD techniques.

---

## Methodology

The researchers developed a tailored Knowledge Distillation framework specifically designed for MoE teacher models. The approach consists of two primary components:

1.  **Knowledge Augmentation (KA):**
    *   Designed to sample experts multiple times to achieve broader knowledge coverage.
    *   Forces the teacher to perform $M$ forward passes to generate diverse representations.
2.  **Student-Aware Router (SAR):**
    *   A mechanism that utilizes all experts during the distillation process.
    *   Optimizes knowledge transfer by dynamically adjusting expert weights based on the student's needs.

---

## Technical Details

The proposed method addresses the inefficiency of standard KD on MoE models, where Top-$k$ activation ignores over 50% of expert probability mass.

### Knowledge Augmentation (KA) Mechanism
*   **Selection Strategy:** KA selects $N-1$ experts per layer via a probabilistic mechanism:
    *   Top $N-1$ with probability $1-\lambda$
    *   Sampled $N-1$ with probability $\lambda$
*   **Masking Function:** Utilizes a masking function $KA(v, E)$ to set non-selected logits to $-\infty$.
*   **Training Process:** The teacher performs $M$ forward passes to generate diverse representations; the student is trained using **Reverse KL Divergence**.

### MoE Architecture Specifics
*   **Gating Mechanism:** Employs Noisy Top-k Gating.
*   **Load Balancing Loss:** The model optimizes using a load balancing loss defined as:
    $$L_b = CV(m)^2 + CV(P)^2$$
    *Where $CV$ represents the coefficient of variation.*

---

## Results

Experiments conducted on Llama-MoE models provided the following empirical insights:

*   **Routing Inefficiency:** Activated experts consistently account for less than 50% of total gate probabilities, validating the "wasted capacity" concern.
*   **Distillation Performance:** In experiments using Llama-MoE-3.5B as the teacher and Sheared-Llama as the student:
    *   Increasing the number of active experts ($k$) monotonically improved Average ROUGE-L scores.
    *   Activating all experts ($k=16$) achieved the **highest performance**.
*   **Validation:** The results empirically validate that knowledge residing in non-activated experts is critical for maximizing student model potential.

---

## Contributions

This research makes three distinct contributions to the field of NLP and model compression:

1.  **First Instance of MoE-Specific KD:** Presents the first intuitive, MoE-specific KD methods, addressing a previously underexplored area in model compression.
2.  **Paradigm Shift:** Shifts the distillation paradigm from sparse selection to **holistic knowledge extraction**, where every expert contributes to the student's learning.
3.  **Empirical Evidence:** Provides extensive experimentation proving that leveraging the full capacity of MoE architecture leads to more effective model compression compared to standard methods.