---
title: 'TD-M(PC)$^2$: Improving Temporal Difference MPC Through Policy Constraint'
arxiv_id: '2502.03550'
source_url: https://arxiv.org/abs/2502.03550
generated_at: '2026-02-06T05:37:44'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# TD-M(PC)$^2$: Improving Temporal Difference MPC Through Policy Constraint

*Haotian Lin; Pengcheng Wang; Jeff Schneider; Guanya Shi*

---

> ### ðŸ“Š Quick Facts
> **Quality Score:** 9/10  
> **Citations:** 40  
> **Key Innovation:** Policy regularization to mitigate planner-policy mismatch  
> **Performance Highlight:** Reduces value overestimation error from 2159% to manageable levels in high-DoF tasks  
> **Computational Overhead:** None (Lightweight addition)

---

## Executive Summary

This research addresses a critical instability in state-of-the-art model-based reinforcement learning (MBRL) algorithms, specifically those relying on planner-generated data like TD-MPC2. The authors identify that these methods suffer from persistent and severe **value overestimation**, a phenomenon where the agent overestimates the value of its states, leading to suboptimal or catastrophic performance. The root cause is identified as a **structural policy mismatch**: the data generation policy (the planner) differs from the learned policy prior used to train the value function. This discrepancy induces a distributional shift, resulting in out-of-distribution (OOD) queries that destabilize learning. This issue is particularly detrimental in high-dimensional control tasks, rendering existing algorithms ineffective for complex applications like humanoid robotics where error amplification is most acute.

The key innovation proposed is **TD-M(PC)$^2$**, a minimalist modification that introduces a policy regularization term to the standard MBRL framework. Rather than altering the network architecture, the method constrains the planner to align with the learned policy prior, effectively reducing OOD queries and stabilizing the value learning process. Theoretically, the authors provide a bound on the suboptimality of the H-step lookahead policy, demonstrating that in standard architectures, value approximation errors are significantly amplified by the planning horizon and discount factor. By adding policy regularization, the method mitigates this amplification without adding computational overhead, maintaining the efficiency of the underlying system.

The study quantifies the magnitude of value overestimation using the metric $E_{\rho_0}[\hat{V} - V^\pi]$, revealing a strong correlation between action space dimensionality and error magnitude. In low-dimensional tasks like Hopper-Stand (4-DoF), the error was relatively contained at 15%. However, in high-dimensional environments, the errors were massive: Dog-Trot (36-DoF) exhibited 231% error, while h1hand-slide-v0 and h1hand-run-v0 (both 61-DoF) showed errors of 746% and 2159%, respectively. TD-M(PC)$^2$ successfully corrects these errors, achieving superior performance over TD-MPC2 baselines and establishing new state-of-the-art results in complex continuous control benchmarks where previous methods failed.

The significance of this work lies in its resolution of a fundamental structural flaw in planner-based MBRL, enabling robust performance in high-dimensional spaces that were previously inaccessible. By demonstrating that policy constraints effectively eliminate the distributional shift causing value overestimation, the authors provide a computationally efficient path toward scalable model-based control.

---

## Key Findings

*   **Persistent Value Overestimation:** Existing model-based RL algorithms using planner-generated data suffer from a systematic overestimation of state values.
*   **Root Cause Identification:** This overestimation is caused by a structural policy mismatch between the data generation policy and the learned policy prior.
*   **Mitigation via Regularization:** Policy regularization mitigates this mismatch by reducing out-of-distribution (OOD) queries, which stabilizes value learning.
*   **Superior Performance:** The method achieves superior performance over baselines like TD-MPC2, most notably in high-dimensional humanoid control tasks.

---

## Methodology

The researchers propose a minimalist modification to existing model-based RL frameworks by introducing a **policy regularization term**. This term aligns the learned policy with the data distribution to reduce out-of-distribution (OOD) queries caused by the planner-policy mismatch. The approach is implemented as a lightweight addition on top of standard architectures without requiring additional computational overhead.

---

## Contributions

*   **Theoretical Insight:** Identifies structural policy mismatch as the root cause of value overestimation in model-based methods using planner-generated data.
*   **Algorithmic Innovation:** Proposes a policy regularization technique to constrain the policy and reduce OOD queries without changing the underlying architecture.
*   **Performance Advancement:** Establishes new benchmarks for continuous control, significantly outperforming state-of-the-art baselines like TD-MPC2 in complex, high-dimensional environments.
*   **Efficiency:** Offers a solution that improves performance and data efficiency while maintaining computational parity with existing methods.

---

## Technical Details

The paper addresses a structural policy mismatch in standard TD-MPC2 where the planner policy ($\pi_H$) used for data collection differs from the nominal policy ($\pi$) used to train the value function. This divergence causes distributional shift and persistent overestimation.

### Baseline Architecture
The baseline employs a latent world model consisting of:
*   State representation: $z$
*   Dynamics: $d$
*   Reward: $R$
*   Stochastic nominal policy: $\pi$
*   Q-function: $\hat{Q}$

These components are trained via a joint loss minimizing reconstruction and Bellman errors.

### Proposed Solution: TD-M(PC)$^2$
The proposed solution introduces **policy regularization** to constrain the planner and reduce Out-of-Distribution queries.

### Theoretical Analysis
**Theorem 3.1** bounds the suboptimality of the H-step lookahead policy. It demonstrates that suboptimality depends on:
1.  Model error
2.  Planner suboptimality
3.  Value approximation error

Crucially, the value approximation error is amplified by a factor of:
$$ \frac{\gamma^H(1 + \gamma^2)}{(1 - \gamma)^2} $$

---

## Results

The study quantifies value overestimation using the metric $E_{\rho_0}[\hat{V} - V^\pi]$. The data reveals a direct correlation between action space dimensionality (DoF) and the magnitude of the error.

| Task | Degrees of Freedom (DoF) | Value Overestimation Error |
| :--- | :---: | :---: |
| **Hopper-Stand** | 4 | 15% |
| **Dog-Trot** | 36 | 231% |
| **h1hand-slide-v0** | 61 | 746% |
| **h1hand-run-v0** | 61 | 2159% |

**Conclusion:** Value errors accumulate and are amplified by the planner in high-dimensional environments, leading to failure in humanoid benchmarks. TD-M(PC)$^2$ effectively addresses these massive overestimations to enable successful control.

---

**Quality Score:** 9/10  
**References:** 40 citations