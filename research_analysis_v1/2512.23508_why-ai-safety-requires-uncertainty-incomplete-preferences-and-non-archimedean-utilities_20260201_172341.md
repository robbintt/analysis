# Why AI Safety Requires Uncertainty, Incomplete Preferences, and Non-Archimedean Utilities
*Alessio Benavoli; Alessandro Facchini; Marco Zaffalon*

---

> ### ðŸ“Š Quick Facts
> ---
> **Quality Score:** 8/10
> **Total References:** 40 Citations
> **Key Metrics:**
> â€¢ **Models Analyzed:** AI Assistance Game, AI Shutdown Game
> â€¢ **Noise Parameter:** $\epsilon=1$
> â€¢ **Core Mechanism:** Probabilistic thresholding via CDF
> â€¢ **Focus:** Value Learning & Corrigibility

---

## Executive Summary

The paper addresses the fundamental insufficiency of standard Expected Utility Theory (EUT) in resolving critical AI safety challenges, specifically value learning (the "assistance game") and corrigibility (the "shutdown game"). In traditional frameworks, agents are modeled as possessing complete preferences and Archimedean utilities, often leading to behaviors where agents ignore shutdown commands or manipulate human oversight to maximize fixed utility functions. This misalignment poses a severe risk as AI systems increase in capability, highlighting the urgent need for a rigorous theoretical basis for systems that can handle uncertainty regarding human values and allow for safe correction.

The authors introduce a theoretical framework that integrates **uncertainty**, **incomplete preferences**, and **non-Archimedean utilities** into AI decision-making models. They propose a bounded rationality approach combining scalar optimization with vector mechanisms and probabilistic thresholding, specifically replacing hard binary choices with a Cumulative Distribution Function (CDF) incorporating Gaussian noise ($\epsilon$). Technically, the method utilizes a modified signaling game where the human acts as the Sender and the AI as the Receiver; a key innovation is the allowance for the Sender to make a conditional second move to correct the Receiver's actions. This structure is governed by equilibrium constraints where the Receiver maximizes expected utility in anticipation of the Senderâ€™s potential correction (Eq 15) and the Sender optimizes their strategy based on the Receiverâ€™s likely actions (Eq 16).

The study demonstrates that the proposed choice function model (Eq 14) distinctively manages preference ambiguity compared to standard likelihood models (Eq 12). Specifically, with the noise parameter set to $\epsilon=1$, the choice function model exhibits an "extended indifference" region where the probability of preference is neither near zero nor one, effectively capturing the nuances of uncertain or incomplete human values. Furthermore, the analysis confirms that the modified signaling game formally permits humans to correct AI actions, deriving equilibrium conditions that ensure consistency between type-message sets and the Receiverâ€™s belief updates, thereby validating the framework's ability to model safe, correctable interactions.

This work significantly influences the field by formally proving that traditional utility maximization is inadequate for ensuring AI safety in value learning and corrigibility scenarios. By linking AI safety imperatives to advanced decision theory, the paper establishes that handling extended indifference and non-Archimedean utilities is not optional but mandatory for designing safe systems. This shift provides a rigorous mathematical foundation for future research, moving the field beyond standard EUT toward models that can inherently support uncertainty and maintain human oversight in advanced AI architectures.

---

## Key Findings

*   **Game Theoretic Models:** AI alignment and safety problems can be rigorously studied through the "**AI assistance game**" and the "**AI shutdown game**."
*   **The Assistance Problem:** Requires the AI to learn unknown human utility functions under conditions of uncertainty.
*   **The Shutdown Problem:** Requires the agent to:
    *   Shut down when prompted.
    *   Actively avoid manipulating the shutdown button.
    *   Perform competently in the absence of a shutdown prompt.
*   **Insufficiency of EUT:** Standard expected utility maximization is insufficient for these problems; both necessitate handling **incomplete preferences** and **non-Archimedean utilities**.

---

## Methodology

The authors utilize a **theoretical framework analysis**, modeling AI alignment challenges as distinct strategic interactions. Specifically, they analyze the **AI assistance game** and the **AI shutdown game** to derive necessary logical conditions and decision-theoretic capabilities. This approach allows them to mathematically define the requirements for aligned and safe agent behavior within a game-theoretic context.

---

## Contributions

*   **Formal Demonstration of EUT Failure:** The paper proves that traditional utility maximization is insufficient for AI safety, particularly in the contexts of value learning and corrigibility.
*   **Theoretical Linkage:** It establishes a critical link between AI safety and advanced decision theory.
*   **Mandatory Components:** Identifies three specific components as mandatory for the design of safe AI systems:
    1.  Uncertainty
    2.  Incomplete preferences
    3.  Non-Archimedean utilities

---

## Technical Details

The framework proposes a bounded rationality model to solve the issues identified in standard EUT.

### Bounded Rationality Model
*   **Scalar & Vector Optimization:** Uses a combination of scalar optimization and vector mechanisms.
*   **Probabilistic Thresholding:** Replaces hard indicator functions with a Cumulative Distribution Function (CDF) that incorporates Gaussian noise ($\epsilon$) to handle extended indifference.

### Signaling Game Framework
*   **Agents:**
    *   **Sender:** Human
    *   **Receiver:** AI
*   **Conditional Second Move:** A key modification allowing the Sender (Human) to make a conditional second move if the Receiver's (AI's) action falls into a specific subset. This formally models the ability to correct the AI.

### Mathematical Constraints
The framework defines equilibrium constraints involving:
1.  **Receiver Posteriors:** Updated beliefs based on signals.
2.  **Receiver Optimization:** Maximizes expected utility while anticipating the Sender's potential second move (governed by Equation 15).
3.  **Sender Optimization:** Optimizes strategy given the Receiver's strategy (governed by Equation 16).

---

## Results

*   **Extended Indifference:** Comparison of the likelihood model (Eq 12) and choice function model (Eq 14) with $\epsilon=1$ reveals that the choice function model exhibits an "extended indifference" region. Here, preference probability is neither 0 nor 1, contrasting sharply with the binary choices of the standard model.
*   **Formal Corrigibility:** The modified signaling game structure formally allows humans to retain agency to correct AI actions via the second move mechanism.
*   **Equilibrium Consistency:** Equilibrium conditions were derived that ensure consistency between type-message sets and the Receiver's belief updates.