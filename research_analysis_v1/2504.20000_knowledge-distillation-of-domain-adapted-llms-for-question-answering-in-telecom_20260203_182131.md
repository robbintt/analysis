---
title: Knowledge Distillation of Domain-adapted LLMs for Question-Answering in Telecom
arxiv_id: '2504.2'
source_url: https://arxiv.org/abs/2504.20000
generated_at: '2026-02-03T18:21:31'
quality_score: 7
citation_count: 27
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Knowledge Distillation of Domain-adapted LLMs for Question-Answering in Telecom

*Rishika Sen; Sujoy Roychowdhury; Sumit Soman; H. G. Ranjani; Srikhetra Mohanty*

---

> ### **Quick Facts**
>
> *   **Quality Score:** 7/10
> *   **Citations:** 27 references
> *   **Dataset:** TeleQuAD
> *   **Model Families:** Llama and Mistral
> *   **Evaluation Scope:** 14 distinct metrics (N-gram, Embedding, LLM-based)

---

## Executive Summary

Deploying Large Language Models (LLMs) in vertical domains like telecommunications presents a critical challenge: balancing the high computational cost of large models with the need for domain-specific accuracy. While Knowledge Distillation (KD) offers a solution by transferring knowledge from a large "teacher" model to a smaller "student" model, significant ambiguity remains regarding the optimal workflow for domain adaptation. Practitioners currently lack consensus on whether to fine-tune the teacher, the student, or both models prior to the distillation process, and how the vocabulary alignment between these architectures influences the final outcome. This paper addresses this gap by systematically investigating the most effective strategies for adapting LLMs to telecom-specific Question-Answering (QA) tasks, providing a rigorous analysis of workflow trade-offs.

The study introduces a comprehensive experimental framework that evaluates three distinct Supervised Fine-Tuning (SFT) strategies applied before the distillation phase: fine-tuning the teacher only, the student only, or both models. Crucially, the research dissects the interaction between these adaptation strategies and the architectural relationship between the models, specifically comparing scenarios with shared vocabularies versus different vocabularies. To handle these differing architectures, the authors utilize two specific KD algorithms: **Vanilla KD** (using KL Divergence loss) for same-vocabulary pairs and **Dual Space KD (DSKD)** (using Cross-Model Attention) for different-vocabulary pairs (e.g., Llama vs. Mistral). The technical innovation lies in isolating the impact of vocabulary configuration to determine its effect on the efficacy of domain adaptation.

Evaluation on the TeleQuAD dataset utilized a robust suite of 14 metricsâ€”encompassing N-gram (BLEU variants, ROUGE-L), embedding-based (BERTScore, Cosine Similarity), and LLM-based (RAGAs faithfulness and correctness) measures. The results demonstrate that applying SFT to both the teacher and student models yields the highest performance across all metric categories. A key finding clarifies that applying SFT to the teacher model alone significantly improves performance specifically when the teacher and student share the same vocabulary, and this improvement holds true regardless of the KD algorithm used. While the "Teacher-only" SFT strategy was generally outperformed by the combined approach, it provided substantial gains; in scenarios where vocabularies differ, the DSKD algorithm serves as an effective alternative to Vanilla KD. The study further confirmed that performance improvements are more statistically significant in same-vocabulary setups, with consistent trends observed across all 14 evaluation metrics.

This research provides clear, actionable guidelines for optimizing LLM deployment in resource-constrained, specialized domains. By establishing that fine-tuning both model components is generally the superior strategy, the paper resolves a key workflow ambiguity for engineers working on domain adaptation. Furthermore, the work highlights the often-overlooked critical role of vocabulary alignment in determining the statistical significance of distillation gains, showing that same-vocabulary setups yield more reliable improvements. The introduction of a multi-faceted evaluation benchmark utilizing 14 distinct metrics sets a new standard for assessing distilled models, encouraging the field to move beyond single-metric analysis toward a more holistic understanding of model performance in domain-specific QA.

---

## Key Findings

*   **SFT of Teacher Model:** Supervised Fine-tuning (SFT) of the teacher model alone improves the performance of the distilled model specifically when the teacher and student models share the same vocabulary, regardless of the KD algorithm or evaluation metric used.
*   **Optimal Adaptation Strategy:** Fine-tuning both the teacher and student models prior to Knowledge Distillation generally yields the best performance across all evaluation metrics.
*   **Statistical Significance:** While SFT of both models leads to better overall results, the statistical significance of this improvement is dependent on the vocabulary configuration of the teacher models.
*   **Vocabulary Dependence:** The impact of domain adaptation strategies on distillation performance is heavily influenced by whether the teacher and student models utilize the same or different vocabularies.

---

## Methodology

The study focuses on the domain-specific task of **Question-Answering (QA)** within the telecom sector. The researchers systematically experimented with three distinct Supervised Fine-tuning (SFT) strategies applied prior to Knowledge Distillation:

1.  SFT of the teacher only
2.  SFT of the student only
3.  SFT of both models

The study analyzed the impact of two specific variables:
*   **Vocabulary Alignment:** Same vs. different vocabularies between models.
*   **KD Algorithms:** Vanilla KD vs. Dual Space KD.

A multi-faceted evaluation was conducted using **14 different metrics** categorized into:
*   N-gram metrics
*   Embedding-based metrics
*   LLM-based metrics

---

## Technical Details

The paper focuses on optimizing Knowledge Distillation (KD) for Question-Answering in the telecom domain by introducing domain adaptation strategies prior to distillation.

### Algorithms Compared
*   **Vanilla KD:** Uses KL Divergence loss. Designed for models with the same vocabulary.
*   **DSKD (Dual Space KD):** Utilizes Cross-Model Attention. Designed for models with different vocabularies.

### Core Approach
The research involves applying Supervised Fine-Tuning (SFT) to the models before the KD phase, testing the following configurations:
*   Teacher-only SFT
*   Student-only SFT
*   Both models SFT
*   Neither model SFT

### Experimental Setup
*   **Model Families:** Llama and Mistral.
*   **Analysis Focus:** The impact of vocabulary alignment on distillation efficacy.

---

## Results

Evaluation on the TeleQuAD dataset utilized a comprehensive set of metrics:

*   **N-gram metrics:** BLEU variants, ROUGE-L
*   **Embedding-based metrics:** Cosine Similarity, BERTScore
*   **Oracle-LLM metrics:** RAGAs (including faithfulness and correctness)

**Summary of Outcomes:**
*   **Performance:** Applying Supervised Fine-Tuning (SFT) to both teacher and student models yields the highest performance across all metrics.
*   **Teacher-only Impact:** Teacher-only SFT significantly improves performance, especially with the same vocabulary.
*   **Algorithm Efficacy:** The DSKD algorithm serves as a useful alternative for different vocabularies when SFT is limited.
*   **Significance:** Performance improvements are more statistically significant in same-vocabulary setups, with consistent trends observed across all metric types.

---

## Contributions

*   **Domain Adaptation Strategy Clarification:** The paper addresses the ambiguity regarding which components (teacher, student, or both) require domain adaptation in Knowledge Distillation workflows for specialized tasks.
*   **Empirical Analysis of Vocabulary Impact:** It provides insights into how vocabulary alignment between teacher and student models interacts with domain adaptation and distillation algorithms.
*   **Comprehensive Evaluation Benchmark:** The work establishes a rigorous evaluation framework for distilled models by utilizing a diverse set of 14 metrics, offering a more holistic view of model performance than traditional single-metric approaches.

---

## References

*   **Total Citations:** 27