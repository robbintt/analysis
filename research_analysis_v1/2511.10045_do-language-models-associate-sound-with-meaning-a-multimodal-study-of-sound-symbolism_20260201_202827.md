# Do Language Models Associate Sound with Meaning? A Multimodal Study of Sound Symbolism

*Jinhong Jeong; Sunghyun Lee; Jaeyoung Lee; Seonah Han; Youngjae Yu*

***

> ### ðŸ“Š Quick Facts
>
> * **Dataset Size:** 10,982 words (8,052 natural + 2,930 pseudo-words)
> * **Languages Covered:** English, French, Japanese, Korean
> * **Semantic Dimensions:** 25 distinct pairs
> * **Model Type:** Multimodal Large Language Models (MLLMs)
> * **Quality Score:** **9/10**

***

## Executive Summary

This research addresses the fundamental question of whether Multimodal Large Language Models (MLLMs) possess the ability to associate sound with meaningâ€”a linguistic phenomenon known as sound symbolism or phonetic iconicity. While human languages frequently contain non-arbitrary mappings between phonemes and semantics (e.g., mimetic words), it is unclear whether deep learning models capture these relationships or rely solely on statistical correlations. This problem is significant because it touches on the core issue of symbolic grounding in AI; understanding how models interpret auditory features is essential for improving their interpretability and ensuring their processing of language aligns with human cognitive patterns.

The study introduces a comprehensive multimodal evaluation framework anchored by the novel **LEX-ICON dataset**, comprising 10,982 words, including 8,052 natural mimetic words from English, French, Japanese, and Korean, and 2,930 systematically constructed pseudo-words. Technically, the approach evaluates model performance across 25 semantic dimensions using the semantic differential method, testing inputs via original text, IPA transcription, and audio waveforms. The key technical innovation lies in the mechanistic analysis of "phonosemantic attention patterns," where the authors employ layer-wise analysis to calculate attention fraction scores at the phoneme level. This allows for granular tracing of how models selectively prioritize iconic sounds during processing, revealing the internal evolution of phonetic interpretation.

The experiments demonstrate that MLLMs exhibit phonetic intuitions consistent with established linguistic research across all 25 semantic categories. The models successfully detected phonetic iconicity not only in textual formats but also in auditory inputs, indicating a robust cross-modal understanding of sound-meaning relationships. Specifically, the layer-wise analysis successfully mapped the progression of phonosemantic associations through the model's layers, providing quantitative evidence that MLLMs selectively allocate attention to iconic phonemes when deriving meaning from the LEX-ICON dataset.

This study establishes sound symbolism as a viable, new methodology for probing the interpretability of auditory information processing in MLLMs. By providing the first large-scale, quantitative analysis of phonetic iconicity in artificial intelligence, the work bridges the gap between computational linguistics and cognitive science. The release of the extensive, multilingual LEX-ICON dataset offers a valuable resource for future research into multimodal grounding, fundamentally advancing the field's understanding of how language models encode and interpret the sensory properties of human language.

***

## Key Findings

*   **Alignment with Linguistics:** MLLMs exhibit phonetic intuitions that align with existing linguistic research across up to **25 semantic categories**.
*   **Selective Attention:** Analysis reveals specific **"phonosemantic attention patterns"** showing models selectively focus on iconic phonemes.
*   **Cross-Modal Detection:** MLLMs detect phonetic iconicity across multiple input modalities, including textual forms and auditory formats.
*   **Granular Processing:** The study successfully mapped layer-wise processing, providing granular insight into the evolution of phonetic interpretation.

***

## Methodology

The researchers utilized a multimodal evaluation framework centered on the **LEX-ICON dataset**, which comprises 8,052 mimetic words from four natural languages and 2,930 pseudo-words annotated with semantic features. 

The study evaluated model performance on phonetic iconicity across **25 semantic dimensions** using both textual and auditory inputs. The technical approach involved layer-wise analysis, specifically measuring **phoneme-level attention fraction scores** to interpret model prioritization of specific sounds.

***

## Technical Details

*   **Dataset Composition (LEX-ICON):**
    *   **Total Words:** 10,982
    *   **Natural Mimetic Words:** 8,052 (English, French, Japanese, Korean)
    *   **Constructed Words:** 2,930 pseudo-words
*   **Evaluation Framework:**
    *   **Method:** Semantic differential method
    *   **Dimensions:** 25 distinct pairs of semantic features
*   **Target Architecture:**
    *   Multimodal Large Language Models (MLLMs) integrating audio-modality input.
*   **Input Modalities Tested:**
    *   Original Text
    *   IPA transcription
    *   Audio waveforms
*   **Analytical Approach:**
    *   Layer-wise analysis
    *   Calculation of "attention fraction scores" to identify phonosemantic attention patterns.

***

## Results

MLLMs demonstrated phonetic intuitions consistent with existing linguistic research across up to 25 semantic categories. The models successfully detected phonetic iconicity across multiple input modalities, including textual forms (original text and IPA) and auditory formats.

Mechanistic findings revealed **'phonosemantic attention patterns'**, providing evidence that models selectively focus on iconic phonemes. The research successfully mapped layer-wise processing to show the evolution of phoneme-meaning associations.

***

## Contributions

*   **New Probing Method:** Establishes sound symbolism as a new method for probing how MLLMs interpret auditory information in human language.
*   **Resource Release:** Presents the **LEX-ICON dataset**, an extensive, multilingual, and multimodal dataset designed to study phonetic iconicity.
*   **Interdisciplinary Bridge:** Bridges AI and cognitive linguistics by providing the first large-scale, quantitative analysis of phonetic iconicity regarding MLLM interpretability.

***

**References:** 40 citations  
**Quality Score:** 9/10