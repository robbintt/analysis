# Robust Fine-Tuning from Non-Robust Pretrained Models: Mitigating Suboptimal Transfer With Adversarial Scheduling

*Jonas NgnawÃ©; Maxime Heuillet; Sabyasachi Sahoo; Yann Pequignot; Ola Ahmad; Audrey Durand; FrÃ©dÃ©ric Precioso; Christian GagnÃ©*

---

> ## ðŸ“ Executive Summary
>
> This research addresses the critical issue of **"suboptimal transfer"** that occurs when applying robust fine-tuning (RFT) to non-robust pretrained models. While standard models pretrained on large datasets (e.g., ImageNet) generalize well, they lack robustness to adversarial attacks. Ideally, fine-tuning these models with adversarial training (AT) would yield robust task-specific performance; however, the paper demonstrates that introducing a robust objective early in the fine-tuning process impedes task adaptation. This phenomenon causes significant performance degradation and can lead to complete transfer failure, creating a substantial barrier to deploying robust models in scenarios where robust pretraining is computationally prohibitive.
>
> To mitigate transfer failure, the authors propose **"Epsilon-Scheduling,"** a heuristic that dynamically modulates the strength of adversarial perturbations during training. Unlike standard RFT-fix, which applies a fixed perturbation strength ($\epsilon$) throughout the entire training process, Epsilon-Scheduling employs a two-hinge linear schedule. This approach begins fine-tuning with zero or low perturbation strength to allow the model to adapt to the new task ("onset of task adaptation") before gradually introducing stronger perturbations. Additionally, the paper introduces **"Expected Robustness,"** a new metric defined as the expectation of accuracy across a spectrum of perturbation levels from zero to a target threshold, providing a more holistic assessment of the accuracy-robustness trade-off than single-point evaluation.
>
> Extensive experiments across six models and five datasets validate the efficacy of the proposed method. The results confirm that robust fine-tuning with fixed perturbations causes significant delays in task adaptation, whereas the scheduled approach maintains high clean accuracy while improving robustness. This work provides a practical solution for transferring knowledge from standard non-robust checkpoints to robust downstream models without the prohibitive computational costs of robust pretraining.

---

### ðŸ“Š Quick Facts

*   **Analysis Quality Score:** 9/10
*   **Total References:** 40 Citations
*   **Models Evaluated:** 6 (ResNet-50, Swin, etc.)
*   **Datasets Tested:** 5 (Cars, Caltech, Aircraft, etc.)
*   **Key Innovation:** Epsilon-Scheduling Heuristic
*   **New Metric:** Expected Robustness

---

## Key Findings

*   **Suboptimal Transfer:** Fine-tuning non-robust pretrained models with a robust objective often leads to poor performance and sometimes complete transfer failure.
*   **Early Training Impediment:** Applying a robust objective too early in training impedes task adaptation, preventing optimal transfer later in the process.
*   **Epsilon-Scheduling Success:** The proposed "Epsilon-Scheduling" method successfully prevents suboptimal transfer and outperforms standard approaches across six models and five datasets.
*   **Consistent Trade-offs:** The new "expected robustness" metric demonstrates consistent improvements in the accuracy-robustness trade-off across various perturbation strengths.

## Methodology

The researchers conducted a systematic examination of Robust Fine-Tuning (RFT) using non-robust pretrained models. Their approach included three main components:

1.  **Problem Analysis:** Investigating the failure modes associated with standard fine-tuning techniques.
2.  **Heuristic Introduction:** Developing "Epsilon-Scheduling," a mechanism to modulate the strength of adversarial perturbations dynamically during training.
3.  **Evaluation:** Assessing performance using a bespoke "expected robustness" metric designed to evaluate models across a spectrum of perturbation levels, validated by extensive experiments on multiple architectures.

## Contributions

*   **Mechanism Characterization:** Defined and characterized the mechanism of "suboptimal transfer" in RFT failure modes.
*   **Training Heuristic:** Introduced "Epsilon-Scheduling," a novel training schedule that varies perturbation strength over time to facilitate better task adaptation.
*   **Holistic Metric:** Proposed "expected robustness," a new evaluation metric for a more comprehensive assessment of the robustness-accuracy trade-off.

## Technical Details

The paper provides a deep dive into the mechanics of transfer failure and the proposed solution:

### 1. Suboptimal Transfer
The study identifies 'Suboptimal Transfer' as a phenomenon where robust fine-tuning of non-robust backbones actively impedes task adaptation.

### 2. Baseline (RFT-fix)
The baseline method employs classical Adversarial Training (PGD) with a **fixed perturbation strength** ($\epsilon$) equal to the target evaluation threshold ($\epsilon_g$) during the full fine-tuning process.

### 3. Solution: Epsilon-Scheduling
*   **Type:** Dynamic two-hinge linear schedule.
*   **Function:** Adjusts perturbation strength over time.
*   **Goal:** Allow task adaptation before introducing strong perturbations.

### 4. New Metrics
*   **Expected Robustness:** Defined as the expectation of accuracy across the range from zero to $\epsilon_g$.
*   **Onset of Task Adaptation:** Defined as the specific epoch where validation clean accuracy exceeds 5%.

## Performance Results

Experiments were conducted on 6 models (including ResNet-50, Swin) and 5 datasets (including Cars, Caltech, Aircraft) under moderate ($4/255$) and high ($8/255$) perturbations.

### R50-Caltech Dataset Performance

The following table highlights the significant improvements achieved by the scheduler compared to the fixed baseline:

| Target Perturbation ($\epsilon_g$) | Method | Clean Accuracy | Expected Robustness |
| :--- | :--- | :--- | :--- |
| **4/255** | Baseline (RFT-fix) | 67.5% | 53.7% |
| **4/255** | **Epsilon-Scheduling** | **76.6%** | **55.7%** |
| **8/255** | Baseline (RFT-fix) | 53.6% | 39.9% |
| **8/255** | **Epsilon-Scheduling** | **67.6%** | **44.0%** |

**Key Observation:** Robust fine-tuning (RFT-fix) resulted in significant delays in task adaptation compared to standard fine-tuning. Stronger perturbations caused longer delays and more severe performance degradation, which the Epsilon-Scheduling method effectively mitigated.