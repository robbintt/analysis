---
title: 'Klear-AgentForge: Forging Agentic Intelligence through Posttraining Scaling'
arxiv_id: '2511.05951'
source_url: https://arxiv.org/abs/2511.05951
generated_at: '2026-02-03T13:24:06'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Klear-AgentForge: Forging Agentic Intelligence through Posttraining Scaling

*Qi Wang; Hongzhi Zhang; Jia Fu; Kai Fu; Yahui Liu; Tinghai Zhang; Chenxi Sun; Gangwei Jiang; Jingyi Tang; Xingguang Ji; Yang Yue; Jingyuan Zhang; Fuzheng Zhang; Kun Gai; Guorui Zhou*

---

### ðŸ“Š Quick Facts

| Metric | Details |
| :--- | :--- |
| **Model Name** | Klear-Qwen3-AgentForge-8B |
| **Base Architecture** | Qwen3-8B |
| **Parameters** | 8 Billion |
| **Training Strategy** | 2-Stage (SFT + Multi-turn RL) |
| **BFCL V3 Score** | 56.7 |
| **SWE Verified Score** | 40.0 |
| **Retail Bench Score** | 41.5 |
| **Quality Score** | 9/10 |

---

## Executive Summary

**Problem**
This research addresses the critical challenge of efficiently developing high-performance "agentic" Large Language Models (LLMs)â€”systems capable of autonomous reasoning, complex decision-making, and interaction with external tools and environments. While proprietary models have demonstrated advanced agentic capabilities, the open-source community has largely lacked the detailed methodologies and standardized pipelines required to replicate this success. This paper investigates whether specific post-training scaling strategies can enable a relatively small, dense model (8B parameters) to rival the performance of significantly larger architectures, thereby reducing the computational barrier to entry for creating state-of-the-art AI agents.

**Innovation**
The core innovation is "Klear-AgentForge," a comprehensive, fully open-source post-training pipeline built upon the Qwen3-8B architecture. The technical approach employs a rigorous two-stage process: Stage 1 utilizes Supervised Fine-Tuning (SFT) with high-quality synthetic data and specialized Reasoning SFT to establish foundational agentic capabilities. Stage 2 implements a sophisticated multi-turn Reinforcement Learning (RL) framework designed to refine the model's ability to interact with external tools across complex tasks. Key technical differentiators include the use of disaggregated RL with steptime breakdown, a hybrid reward mechanism mixing turn-level and outcome rewards, and a comparative analysis of multi-task RL versus model merging strategies.

**Results**
The Klear-Qwen3-AgentForge-8B model achieved state-of-the-art performance against similar-sized open-source models, notably outperforming the Qwen3-8B (Thinking) baseline, and delivered competitive results against proprietary giants. On the Berkeley Function Calling Leaderboard (BFCL V3), the model scored 56.7, surpassing the Qwen3-8B (Thinking) baseline. In the Retail Bench, it achieved a score of 41.5â€”a substantial improvement over the base model's 25.0 and approaching GPT-4.1's score of 49.4. Most notably, on the SWE Verified benchmark for coding proficiency, the AgentForge model scored 40.0, slightly exceeding GPT-4.1 (39.4) and more than doubling the performance of the base Qwen3-8B (Thinking) model (17.3).

**Impact**
The significance of this work lies in the empirical validation that smaller models can achieve elite agentic performance through targeted post-training scaling, challenging the necessity of massive parameter counts for complex tool-use and coding tasks. By releasing the entire pipeline as open-source, the authors provide a reproducible blueprint for the community, bridging a significant knowledge gap in how to effectively train agents using synthetic data and multi-turn RL. This contribution sets a new benchmark for open-source agentic intelligence and suggests that future developments may focus more heavily on data efficiency and training algorithms rather than simply scaling model size.

---

## Key Findings

*   **State-of-the-Art Performance:** The Klear-Qwen3-AgentForge-8B model achieves SOTA results on agentic benchmarks when compared to similar-sized LLMs.
*   **Competitive with Giants:** Despite its smaller size, the model remains competitive with significantly larger proprietary models.
*   **Methodology Validation:** The study confirms that a combination of Supervised Fine-Tuning (SFT) on synthetic data and multi-turn Reinforcement Learning (RL) effectively unlocks potential for diverse agentic tasks.
*   **Domain Proficiency:** Exclusive experiments confirmed the model's high proficiency in both **tool use** and **coding** domains.

---

## Methodology

The research pipeline initiates with the **Qwen3-8B base model** and utilizes a specialized two-stage post-training approach:

1.  **Stage 1: Supervised Fine-Tuning (SFT)**
    *   Utilizes synthetic data to build foundational agentic capabilities.
2.  **Stage 2: Multi-turn Reinforcement Learning (RL)**
    *   Enhances the model's ability to interact with external tools and environments across complex tasks.
3.  **Evaluation**
    *   Includes rigorous testing on agentic benchmarks specifically within the tool use and coding domains.

---

## Technical Details

The implementation leverages a sophisticated two-stage post-training scaling method:

*   **Architecture:** Built upon the Qwen3-8B foundation.
*   **Stage 1 (SFT):**
    *   **Data Utilization:** Synthetic data combined with specific Reasoning SFT data.
    *   **Goal:** Capability acquisition and foundational building.
*   **Stage 2 (Multi-turn RL):**
    *   **Objective:** Refine agentic behaviors and decision-making processes.
    *   **Techniques:**
        *   Disaggregated RL with steptime breakdown.
        *   Hybrid reward mechanism mixing turn-level and outcome rewards.
        *   Comparative analysis of multi-task RL vs. model merging.
*   **Explorations:**
    *   Test-time scaling strategies.
    *   Agentic Task Formalization using custom-built environments.

---

## Results

The Klear-AgentForge-8B model demonstrated significant improvements across major benchmarks:

*   **Berkeley Function Calling Leaderboard (BFCL V3):**
    *   **Score:** 56.7
    *   **Comparison:** Outperformed Qwen3-8B (Thinking).
*   **Retail Bench:**
    *   **Score:** 41.5
    *   **Comparison:** Significantly outperformed Qwen3-8B (Thinking) (25.0); performed competitively against GPT-4.1 (49.4).
*   **SWE Verified:**
    *   **Score:** 40.0
    *   **Comparison:** Matched/slightly exceeded GPT-4.1 (39.4); significantly outperformed base Qwen3-8B (Thinking) (17.3).

---

## Contributions

*   **Open Source Pipeline:** Released a comprehensive, fully open-source post-training pipeline to address the lack of details in the community.
*   **High-Performance Model:** Introduced Klear-Qwen3-AgentForge, a model capable of high-level interaction with external tools.
*   **Empirical Evidence:** Provided research-backed evidence that specific post-training scaling strategies allow smaller models to rival the performance of significantly larger architectures.

---
**References:** 40 citations