# Overcoming Growth-Induced Forgetting in Task-Agnostic Continual Learning

*Yuqing Zhao; Jiannong Cao; Divya Saxena; Xiaoyun Liu; Changlin Song; Bo Yuan; Julie McCann*

---

> ## üìä Quick Facts
> | Metric | Detail |
> | :--- | :--- |
> | **Method** | SparseGrow |
> | **Focus** | Task-Agnostic Continual Learning |
> | **Key Mechanism** | Gradient & Parameter Sparsity |
> | **Quality Score** | 8/10 |
> | **Citations** | 13 |

---

## üìù Executive Summary

This research addresses a critical failure mode in task-agnostic continual learning (CL) termed "**growth-induced forgetting**" (GIF). While dynamic network expansion is a common strategy to accommodate new data streams, the authors demonstrate that uncontrolled or improperly managed growth actually degrades the performance of learned knowledge during inference. This issue is significant because it challenges the prevailing assumption that simply increasing model capacity is sufficient to balance stability and plasticity, revealing that without precise management, expansion can actively disrupt previously acquired representations.

To overcome GIF, the authors propose "**SparseGrow**," a framework that prioritizes specific structural mechanisms over standard regularization schemes. The method technically operates through three coordinated components:
1.  **Dynamic Capacity Expansion**, which evaluates neuron significance via the L1 norm of pre-activation outputs to determine when and where to add capacity;
2.  **Selective Freezing**, which locks the original backbone parameters during expansion to train only new neurons and a shared task-agnostic head;
3.  **Gradient Gating and Sparse Initialization**. Unlike traditional approaches relying solely on penalty terms, SparseGrow utilizes gradient gating to explicitly preserve critical parameters and sparse initialization to control the plasticity of new neurons.

SparseGrow demonstrated superior performance over state-of-the-art competitors across multiple benchmarks. On the **Split-CIFAR-100** dataset (10 tasks), the method achieved **78.5%** accuracy, significantly outperforming the leading competitor ANML (73.2%) and a static baseline (44.8%). Similarly, on **TinyImageNet**, SparseGrow attained **65.4%** accuracy compared to ANML's 60.1%. Crucially, ablation studies confirmed that uncontrolled growth leads to significant accuracy drops on previous tasks, which the authors' selective freezing and gating strategies successfully mitigated. Furthermore, the model maintained approximately 50% parameter sparsity, offering a more favorable efficiency trade-off than Progressive Networks.

The significance of this work lies in formally defining the phenomenon of growth-induced forgetting and establishing that controlled expansion via gradient gating and sparse initialization‚Äîrather than unbounded growth‚Äîis essential for sustainable continual learning. By validating the necessity of these specific sparsity mechanisms in task-agnostic settings, SparseGrow provides a blueprint for developing more resource-efficient deep learning systems.

---

## üîë Key Findings

*   **Identification of Growth-Induced Forgetting:** Improper application of model growth in task-agnostic continual learning (CL) leads to 'growth-induced forgetting,' where learned knowledge is severely degraded during inference.
*   **Sparsity as a Critical Factor:** Managing both gradient and parameter sparsity is key to simultaneously promoting model adaptability to new data and retaining existing knowledge.
*   **Efficacy of Controlled Expansion:** Uncontrolled growth is detrimental, whereas controlled layer expansion is necessary for maintaining performance across many tasks.
*   **Validation of SparseGrow:** The SparseGrow method effectively achieves high adaptability to new data while minimizing knowledge loss.

---

## üß† Methodology

The paper proposes **SparseGrow**, a method for task-agnostic continual learning that manages model growth through two primary mechanisms:

*   **Gradient Sparsity:** Achieved via layer expansion combined with gradient gating to preserve critical parameters.
*   **Parameter Sparsity:** Promoted via sparse initialization and sparse training to control model plasticity.

---

## üõ†Ô∏è Technical Details

**Method Name:** SparseGrow

The approach addresses **Growth-Induced Forgetting** by combining dynamic network capacity with strict sparsity regularization and a selective freezing strategy for task-agnostic settings.

### Core Mechanisms

1.  **Dynamic Capacity Expansion**
    *   Evaluates **Neuron Significance** (calculated as the average L1 norm of pre-activation outputs).
    *   Preserves important neurons while adding new ones for novel data distributions.

2.  **Selective Freezing**
    *   To mitigate feature distortion caused by training new neurons, the original backbone parameters are frozen during expansion.
    *   Only the weights of newly added neurons and the shared task-agnostic head are optimized.

3.  **Dual Sparsity Regularization**
    *   The loss function incorporates two distinct penalty terms:
        *   **Parameter Sparsity term:** L1 penalty on weights.
        *   **Gradient Sparsity term:** L2 penalty on gradients.
    *   Serves to enforce efficiency and stability.

---

## ‚úÖ Contributions

*   **Problem Formalization:** Defined the issue of 'growth-induced forgetting' in task-agnostic CL.
*   **Novel Framework:** Introduced SparseGrow, a framework integrating gradient gating and sparse training to stabilize model growth.
*   **Empirical Validation:** Provided comprehensive experimental evidence validating the necessity of controlled layer expansion and the effectiveness of sparse growth strategies.

---

## üìà Results

**Evaluation Metrics:**
*   Average Accuracy (AA)
*   Backward Transfer (BWT)
*   Parameter Efficiency

**Performance Breakdown:**

*   **Split-CIFAR-100 (10 Tasks):**
    *   **SparseGrow:** 78.5% accuracy
    *   **ANML (Best Competitor):** 73.2% accuracy
    *   **Static Baseline:** 44.8% accuracy
*   **TinyImageNet:**
    *   **SparseGrow:** 65.4% accuracy
    *   **ANML:** 60.1% accuracy
*   **Growth-Induced Forgetting (GIF) Evaluation:**
    *   Ablation studies confirmed that 'Uncontrolled Growth' causes accuracy drops on previous tasks.
    *   'Selective Freezing' successfully mitigates these drops.
*   **Efficiency:**
    *   The method maintained approximately **50% parameter sparsity**.
    *   Offers a better trade-off than Progressive Nets.

---
**References:** 13 citations | **Quality Score:** 8/10