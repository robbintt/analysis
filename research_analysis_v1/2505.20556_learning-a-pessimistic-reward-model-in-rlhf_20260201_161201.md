# Learning a Pessimistic Reward Model in RLHF

*Yinglun Xu, Hangoo Kang, Tarun Suresh, Yuxuan Wan, Gagandeep Singh*

***

> ### **Quick Facts**
> - **Method:** PET (Pessimistic Reward Fine-Tuning)
> - **Key Dataset:** TL;DR Summarization
> - **Model Architecture:** Pythia-1b
> - **Quality Score:** 7/10
> - **References:** 38 Citations
> - **Core Innovation:** Removes need for KL regularization via pessimistic reward modeling

***

## Executive Summary

In offline Reinforcement Learning from Human Feedback (RLHF), a fundamental challenge is "reward hacking," where an agent maximizes scores from an imperfect reward model without actually improving output quality. Standard practice relies on Kullback-Leibler (KL) regularization to constrain the policy, preventing it from deviating too far from a reference model. While effective, this artificially restricts the policy's search space, potentially excluding higher-quality solutions that require significant deviation from the initial data distribution.

This paper introduces **Pessimistic Reward Fine-Tuning (PET)**, a framework that shifts the burden of robustness from the policy to the reward model. Instead of relying on KL penalties, PET formulates the problem as a minimax optimization: `min_r max_pi (V_r(pi) - V_r(pi_ref)) + beta * L_D(r)`. In this formulation, the reward model is trained to be "pessimistic"â€”conservative against the worst-case policy behaviors generated via rejection sampling.

The researchers validated PET using Pythia-1b models on the TL;DR summarization dataset. Experimental results demonstrated that PET successfully learns high-quality policies entirely without KL regularization. Crucially, PET maintained high performance even when the policy exhibited high KL divergence from the base model, a condition under which traditional KL-dependent methods typically suffer from severe reward collapse. This work challenges the long-held intuition that KL regularization is a strict requirement for stable offline RLHF, suggesting a new paradigm for alignment where developers can push models further beyond their initial capabilities.

***

## Key Findings

*   **Elimination of Regularization:** PET prevents reward hacking in offline RLHF without requiring KL regularization, removing a standard architectural constraint.
*   **High Divergence with High Performance:** PET enables the learning of high-quality policies that maintain high performance despite exhibiting high KL divergence from the reference model.
*   **Robustness via Pessimism:** Optimizing a policy on a pessimistic reward model allows an agent to greedily search for high rewards without suffering from reward hacking.
*   **Empirical Validation:** Testing on the TL;DR summarization dataset confirmed that a high-quality policy can be learned on a pessimistic reward model without added regularization constraints.

***

## Methodology

The authors introduce **PET (Pessimistic reward fine-tuning)**, a novel method for offline RLHF. Instead of relying on the standard pipeline of training an imperfect reward model and applying KL regularization to constrain the policy, PET fine-tunes a pessimistic reward model.

The policy is then optimized directly against this pessimistic model. This approach shifts the mechanism for preventing reward hacking from relying on policy constraints (KL penalties) to relying on the inherent robustness of the pessimistic reward model.

***

## Contributions

*   **Formulation of Pessimistic Reward Modeling:** The paper establishes the feasibility of learning a pessimistic reward model specifically to counter reward hacking in RLHF.
*   **Challenging Traditional Intuition:** The work challenges the prevailing intuition that KL regularization is strictly necessary to prevent reward hacking, demonstrating that robustness can be achieved through the reward modeling phase alone.
*   **Expanded Policy Search Space:** By removing the need for KL constraints, the method allows for the discovery of effective policies that deviate significantly from the original dataset distribution (high KL divergence), which were previously excluded.

***

## Technical Details

The paper proposes PET, a three-step framework to address reward hacking in offline RLHF without KL regularization.

### Framework Steps
1.  **Standard Reward Modeling:** Initial training of the reward model.
2.  **Pessimistic Fine-Tuning:** Adversarially fine-tuning the reward model against a policy induced by rejection sampling.
3.  **Policy Optimization:** The final optimization of the policy against the hardened reward model.

### Optimization Logic
The approach formulates robust RLHF as a minimax optimization problem:

$$ \min_r \max_\pi (V_r(\pi) - V_r(\pi_{ref})) + \beta \cdot L_D(r) $$

*   $r$: The reward model.
*   $\pi$: The Rejection Sampling policy.
*   The value gap represents relative performance.
*   The prediction loss ($L_D$) constrains accuracy.

### Implementation & Differentiators
*   **Update Cycle:** Updates the policy using Rejection Sampling (best-of-N), samples responses, and updates the reward model by minimizing a loss combining reward differences and prediction loss.
*   **Key Differentiators:** The absence of KL regularization and the use of adversarial training.

***

## Results

**Experimental Setup:**
*   **Dataset:** TL;DR summarization.
*   **Models:** Pythia-1b models; Qwen-2.5-32B-Instruct used as a judge.
*   **Hyperparameters:** Rejection Sampling $n=64$; Temperatures $T=0.7$ (base) and $T=0.1$ (reference); Pessimism coefficient $1/\beta=0.1$.
*   **Evaluation:** Win Rate against a human reference.

**Baselines:**
Compared against PPO-KL, Best-of-N, DPO, RPO, and chi-PO.

**Outcomes:**
Results confirmed that PET enables learning high-quality policies without KL regularization. Unlike traditional methods that degrade, PET maintains high performance even with high KL divergence.