---
title: 'MindFlow+: A Self-Evolving Agent for E-Commerce Customer Service'
arxiv_id: '2507.18884'
source_url: https://arxiv.org/abs/2507.18884
generated_at: '2026-02-03T13:53:07'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# MindFlow+: A Self-Evolving Agent for E-Commerce Customer Service

*Ming Gong; Xucheng Huang; Ziheng Xu; Vijayan K. Asari*

---

> ### ðŸ“Š Quick Facts
> *   **Model Architecture:** Self-evolving Agent (LLM + Imitation Learning + Offline RL)
> *   **Base Models:** Qwen2.5 (0.5B, 1.5B, 3B, 7B)
> *   **Top Performance:** 94.00% AI Contribution Ratio (7B Model)
> *   **Context Window:** 4096 Tokens
> *   **Training Hardware:** 8 NVIDIA A800 GPUs
> *   **Novel Metric:** AI Contribution Ratio
> *   **Quality Score:** 8/10

---

## Executive Summary

Traditional e-commerce customer service systems rely heavily on rigid intent-based frameworks that struggle to manage the fluid, multi-turn, and context-dependent nature of real-world user interactions. These static approaches often fail when queries deviate from predefined scripts, resulting in poor contextual relevance and task accuracy. As e-commerce environments become increasingly dynamic, there is a critical need for systems that can adaptively reason, utilize external tools, and align responses with specific business goals, rather than simply matching patterns.

MindFlow+ addresses these limitations through a self-evolving agent architecture that integrates Large Language Models (LLMs) with Imitation Learning and Offline Reinforcement Learning (RL). The technical core consists of two data-centric mechanisms: **Tool-Augmented Demonstration Construction**, which utilizes ReAct-style reasoning to teach the model effective tool usage, and **Reward-Conditioned Data Modeling**, which aligns outputs with domain-specific goals via reward signals. The system employs a Stateâ€“Rewardâ€“Action ($s, r, a$) sequence representation and contrastive learning on positive and negative responses, enabling models ranging from 0.5B to 7B parameters to learn knowledge-enhanced, agentic behaviors through Supervised Fine-Tuning (SFT).

The model was evaluated using a novel metric, the **AI Contribution Ratio**, designed to quantify the specific value added by the AI in dialogue generation. MindFlow+ significantly outperformed baseline models across all sizes: the 0.5B model achieved 72.67%, the 1.5B model reached 85.33%, the 3B model hit 87.33%, and the 7B model scored 94.00%. Notably, the 1.5B MindFlow+ model surpassed a 7B pretrained baseline, demonstrating the efficiency of the training paradigm. Ablation studies confirmed that the specific $s, r, a$ token order outperformed alternatives ($r, s, a$) and that the removal of factual context caused a significant performance drop, validating the necessity of the tool-augmented demonstrations.

This research marks a significant advancement in the shift from static, intent-based chatbots to adaptive, self-evolving agents capable of complex reasoning in commercial environments. By validating the unification of imitation learning and offline RL within a data-centric framework, the authors provide a scalable blueprint for deploying LLMs in high-stakes, domain-specific scenarios. Furthermore, the introduction of the AI Contribution Ratio provides the field with a more granular evaluation metric, allowing researchers and practitioners to better assess the tangible utility and involvement of AI agents in human-AI collaboration workflows.

---

## Key Findings

*   **Superior Performance:** MindFlow+ outperforms strong baseline models in real-world e-commerce environments, demonstrating superior capabilities in contextual relevance, flexibility, and task accuracy.
*   **Overcoming Traditional Limits:** The integration of Large Language Models (LLMs) with tool reasoning and reward-guided learning successfully overcomes the limitations of traditional intent-based systems in handling dynamic, multi-turn interactions.
*   **Novel Evaluation Metric:** The proposed **'AI Contribution Ratio'** effectively serves as a novel quantitative metric for evaluating the specific involvement and contribution of the AI in generating dialogue responses.
*   **Data-Centric Mechanisms:** Data-centric mechanismsâ€”specifically tool-augmented demonstration and reward-conditioned modelingâ€”are critical for aligning model behavior with domain-specific goals and effective tool usage.

---

## Methodology

The research presents a self-evolving dialogue agent designed to learn domain-specific behaviors through a hybrid learning approach. The methodology relies on two primary components:

1.  **Core Architecture:** Combines Large Language Models (LLMs) with Imitation Learning and Offline Reinforcement Learning (RL).
2.  **Tool-Augmented Demonstration Construction:** Exposes the model to knowledge-enhanced, agentic interactions (ReAct-style) to ensure effective tool use.
3.  **Reward-Conditioned Data Modeling:** Utilizes reward signals to align generated responses with task-specific goals.
4.  **Evaluation:** The model is tested on real-world e-commerce conversation datasets, measured against the newly introduced AI Contribution Ratio metric alongside standard performance indicators.

---

## Technical Details

**System Architecture**
*   **Type:** Self-evolving dialogue agent for e-commerce.
*   **Learning Paradigm:** Supervised Fine-Tuning (SFT) on a hybrid, reward-guided corpus using a data-centric approach.
*   **Unified Framework:** Unifies Imitation Learning (knowledge-augmented static context + ReAct-based agentic demonstrations) and Reward-Based Supervision via offline RL.

**Data Representation & Loss Functions**
*   **Sequence Representation:** Employs a Stateâ€“Rewardâ€“Action ($s, r, a$) sequence representation.
*   **Loss Functions:** Minimizes standard cross-entropy loss and uses contrastive learning with positive and negative responses.

**Hyperparameters & Hardware**
*   **Base Models:** Qwen2.5 (0.5B to 7B parameters).
*   **Context Window:** 4096 tokens.
*   **Learning Rate:** 2e-5.
*   **Training Epochs:** 4.
*   **Compute Infrastructure:** Trained on 8 NVIDIA A800 GPUs.

---

## Contributions

*   **MindFlow+ Agent:** The development of a novel, self-evolving agent architecture capable of handling complex, dynamic e-commerce customer service interactions better than traditional intent-based systems.
*   **Data-Centric Learning Mechanisms:** The introduction of specific techniques for tool-augmented demonstration construction (for reasoning and tool use) and reward-conditioned data modeling (for goal alignment).
*   **AI Contribution Ratio:** The proposal of a new evaluation metric designed to quantify and assess the specific contribution of the AI agent within the context of dialogue generation.

---

## Results

*   **AI Contribution Ratio Performance:** MindFlow+ (Unified) significantly outperformed baselines across different sizes:
    *   **0.5B:** 72.67%
    *   **1.5B:** 85.33% (Surpassed the 7B pretrained baseline)
    *   **3B:** 87.33%
    *   **7B:** 94.00%
*   **Ablation Study (Token Order):** The **{s, r, a}** token order consistently outperformed the alternative **{r, s, a}** configuration.
*   **Generalization:** Cross-domain evaluation demonstrated robust generalization with consistently high ratios.
*   **Knowledge Ablation:** Ablation on knowledge sources confirmed that removing factual context caused a significant performance drop, highlighting the importance of context.

---

**Quality Score:** 8/10  
**References:** 40 citations