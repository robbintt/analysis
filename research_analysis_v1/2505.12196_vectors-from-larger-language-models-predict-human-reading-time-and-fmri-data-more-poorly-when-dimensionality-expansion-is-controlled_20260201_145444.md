# Vectors from Larger Language Models Predict Human Reading Time and fMRI Data More Poorly when Dimensionality Expansion is Controlled

*Yi-Chien Lin; Hongao Zhu; William Schuler*

---

> ### ðŸ“Š Quick Facts
>
> *   **Quality Score:** 8/10
> *   **Citations:** 9 references
> *   **Model Scale:** 70M to 66B parameters
> *   **Dimensions ($d_{\text{model}}$):** 512 to 9216
> *   **Data Scope (Reading):** ~48k to 192k held-out points
> *   **Data Scope (fMRI):** ~51k to 76 held-out points
> *   **Evaluation Metrics:** Linear Regression, Pearson correlation ($r$)

---

## Executive Summary

This paper resolves a fundamental conflict in cognitive AI research regarding whether increasing the size of Large Language Models (LLMs) improves their ability to model human sentence processing. Previous studies produced contradictory evidence: those utilizing word prediction probabilities (surprisal) demonstrated "inverse scaling," where larger models performed worse, whereas studies using whole vector embeddings suggested "positive scaling." Resolving this discrepancy is critical for determining if simply scaling up parameters is a valid strategy for achieving cognitive plausibility in AI systems.

The authors identify a statistical confound as the likely driver of previous positive scaling findings: as LLMs scale from 70M to 66B parameters, their vector dimensionality ($d_{\text{model}}$) expands from 512 to 9216. This increased dimensionality provides regression models with more degrees of freedom, artificially inflating fit regardless of the semantic quality of the representations.

To isolate the effect of model scale from feature quantity, the study introduced a rigorous control methodology comparing pre-trained vectors against untrained baseline models of identical dimensions. The technical evaluation utilized final-layer vectors averaged to the word level, applying Linear Regression and Pearson correlation ($r$)â€”normalized by 0.32 for fMRI datasetsâ€”to assess predictive fit against preprocessed reading time (SPR, Eye-tracking) and fMRI data convolved with a Hemodynamic Response Function (HRF).

The study validated the inverse scaling hypothesis across datasets ranging from approximately 48,000 to 192,000 held-out points for reading data and 51,000 to 76 points for fMRI data. When dimensionality was controlled, smaller architectures (e.g., Pythia 70M, $d=512$) consistently outperformed massive models (e.g., OPT 66B, $d=9216$). The results showed that the superior performance initially observed in high-dimensional vectors was spurious; once benchmarked against high-dimensional untrained baselines, the correlation coefficients for larger models dropped significantly.

Consequently, the study confirmed that increasing model scale results in a quantifiable decrease in predictive accuracy for human reading times and brain imaging data. These findings challenge the conjecture that improvements in LLM word prediction accuracy linearly translate to better fits with human psychometric data. By demonstrating that previous "positive scaling" results were artifacts of uncontrolled dimensionality expansion rather than evidence of cognitive alignment, the paper suggests a fundamental divergence between LLM architectures and human sentence processing mechanisms. This indicates that merely increasing model complexity is not a solution for mapping LLMs to human cognition, urging the field to fundamentally rethink model design objectives and training paradigms rather than relying on scale alone.

---

## Key Findings

*   **Inverse Scaling Validated for Vectors:** When the larger number of predictors (dimensions) in vectors from larger LLMs is controlled for, larger models predict human reading time and fMRI data more poorly, confirming an inverse scaling relationship.
*   **Debunking Positive Scaling via Vectors:** Previous studies that showed positive scaling using whole vectors were likely confounded by the expansion of dimensionality.
*   **Alignment Worsens with Scale:** The misalignment between LLM architectures and human sentence processing mechanisms increases as model size grows.
*   **Consistency with Surprisal Metrics:** The inversed scaling observed using whole vectors aligns with previous findings that used word prediction probability (surprisal).

---

## Methodology

The study assessed the relationship between LLM size and psychometric fit using reading time and fMRI brain imaging data. Researchers utilized entire vector representations extracted from LLMs of varying sizes and implemented statistical controls for dimensionality expansion to isolate the effect of model scale from the effect of feature quantity.

---

## Technical Details

*   **Investigation Focus:** Analysis of whether positive scaling in LLMs predicting human reading/brain data is genuine or a statistical confound caused by increased vector dimensionality ($d_{\text{model}}$).
*   **The Confound:** Larger models offer more predictors, giving regression models more degrees of freedom.
*   **Control Method:** Isolates dimensionality by comparing pre-trained vectors against untrained models.
*   **Data Preprocessing:**
    *   **SPR:** Filtered 100ms-3000ms.
    *   **Eye-tracking:** Go-past duration, excluding initial/final/unfixated words.
    *   **fMRI:** HRF convolution applied.
*   **Partitioning:**
    *   50% Fit
    *   25% Exploratory
    *   25% Held-out
    *   *Note:* 5-fold partitioning used for Pereira dataset.
*   **Models Used:** GPT-2, GPT-Neo, OPT, and Pythia (70M to 66B parameters; $d_{\text{model}}$ 512 to 9216).
*   **Predictors:** Final-layer vectors averaged to word-level.
*   **Evaluation:** Linear Regression and Pearson correlation ($r$), normalized by 0.32 for Pereira.

---

## Results

Post-preprocessing, dataset sizes ranged from ~48k to 192k held-out points for reading data, and ~51k to 76 for fMRI. Key findings validate **'Inverse Scaling'**: when accounting for the dimensionality confound, larger models predict human data more poorly.

*   Previous positive scaling results were likely spurious correlations driven by predictor degrees of freedom.
*   Misalignment between LLMs and human sentence processing increases with scale, aligning with observations based on 'surprisal'.
*   **Specific Model Performance:**
    *   **Small:** Pythia 70M (Dimensions: 512)
    *   **Medium:** OPT 2.7B
    *   **Large:** OPT 66B (Dimensions: 9216)

---

## Contributions

*   **Challenges Linearity Assumption:** The study challenges the conjecture that improvements in LLM word prediction accuracy linearly translate to better fits with human psychometric data.
*   **Resolves Discrepancies:** It addresses the discrepancy between studies showing inverse scaling and positive scaling by demonstrating that the latter was an artifact of uncontrolled dimensionality.
*   **Guides Future Research:** The findings suggest that simply increasing model complexity is not the solution to mapping LLMs to human cognition, indicating a fundamental architectural or objective-based divergence.