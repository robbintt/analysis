---
title: 'Not There Yet: Evaluating Vision Language Models in Simulating the Visual
  Perception of People with Low Vision'
arxiv_id: '2508.10972'
source_url: https://arxiv.org/abs/2508.10972
generated_at: '2026-02-04T15:56:50'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Not There Yet: Evaluating Vision Language Models in Simulating the Visual Perception of People with Low Vision

*Rosiana Natalie; Wenqian Xu; Ruei-Che Chang; Rada Mihalcea; Anhong Guo*

---

> ### ðŸ“Š Quick Facts
> | Metric | Value |
> | :--- | :--- |
> | **Model Evaluated** | GPT-4o |
> | **Human Participants** | 40 |
> | **Dataset Size** | 709 open-ended descriptions, 4,170 multiple-choice responses |
> | **Agreement (Minimal Context)** | 0.59 |
> | **Agreement (Combined Context)** | 0.70 |
> | **Statistical Significance** | p < 0.0001 |
> | **Quality Score** | 9/10 |
> | **References** | 40 Citations |

---

## Executive Summary

This research addresses the critical challenge of using Vision Language Models (VLMs) to accurately simulate the visual perception of individuals with low vision. While VLMs hold immense potential for improving accessibility applicationsâ€”such as automated auditing of visual content or generating personalized descriptionsâ€”they often fail to strictly adhere to specific user constraints. The core issue is that models tend to revert to "standard" vision inference patterns rather than maintaining the persona of a user with specific visual limitations. This gap in simulation fidelity limits the reliability of AI agents in accessibility contexts, where accurate representation of user experience is essential for inclusive design.

The authors present the first comprehensive study and benchmark dataset designed to evaluate VLMs as simulators of visual disability. Technically, the study involved collecting ground-truth data from 40 low-vision participants, resulting in a dataset of 709 open-ended descriptions and 4,170 multiple-choice responses. Using GPT-4o, the researchers constructed simulated agents and manipulated prompt engineering variables to identify optimal simulation strategies. The key innovation is the "Combined Context" approach, which integrates detailed vision profile information with concrete examples of image responses. This method is designed to "anchor" the model, forcing it to adjust its inference processing to match specific visual capabilities rather than relying on its pre-trained, general-purpose vision biases.

The evaluation demonstrated that standard "Minimal Context" promptingâ€”where visual limitations are merely specifiedâ€”results in poor agreement with human participants (0.59 agreement score). Conversely, the "Combined Context" strategy significantly improved simulation performance, achieving an agreement score of 0.70 (p < 0.0001). The study also found that single-shot prompting is highly efficient; providing a single example combining open-ended and multiple-choice responses yielded significant gains, while adding more examples provided diminishing returns. For context, the human participants completed tasks with a baseline accuracy of 60.4% on multiple-choice questions, providing descriptions averaging 27.2 words over a 70.2-minute session.

This work significantly advances the accessibility domain by establishing the first rigorous framework for assessing how well VLMs can simulate the visual experiences of people with disabilities. Beyond the novel benchmark dataset, the paper provides actionable, evidence-based guidelines for prompt engineering, demonstrating that high-fidelity simulation requires the explicit combination of user profile data and few-shot examples. These findings highlight a fundamental limitation in current VLM behaviorâ€”the tendency to ignore specific constraints unless explicitly anchoredâ€”and offer a pathway toward developing more reliable, user-centric AI agents for accessibility applications.

---

## Key Findings

*   **Limited Simulation with Minimal Context:** VLMs (specifically GPT-4o) infer visual capabilities beyond specified limitations, resulting in low agreement scores (**0.59**).
*   **Necessity of Combined Context:** Providing vision information and example image responses in isolation is insufficient, but combining both significantly increases agreement with human responses (**0.70**, p < 0.0001).
*   **Efficiency of Single-Shot Prompting:** A single example combining open-ended and multiple-choice responses yields significant performance gains, while additional examples offer minimal statistical benefit.
*   **General Inference Bias:** Models tend to revert to general inference patterns rather than adhering to specific user constraints unless explicitly anchored by descriptive vision data and concrete response examples.

---

## Methodology

The researchers employed a rigorous three-step process to evaluate simulation fidelity:

1.  **Data Collection:** A benchmark dataset was compiled via a survey study involving **40 low vision participants**. This process collected detailed vision profiles and recorded responses to various images.
2.  **Agent Simulation:** Using **GPT-4o**, the researchers constructed simulated agents for each participant. This phase involved manipulating prompt engineering variables, specifically testing the inclusion of vision information and example image responses.
3.  **Evaluation:** The study assessed simulation performance by measuring the agreement between VLM-generated responses and the ground-truth answers provided by human participants.

---

## Technical Details

**Model Evaluated**
*   GPT-4o

**Data Collection**
*   **Interface:** Custom accessible web interface.
*   **Volume:** 709 open-ended descriptions and 4,170 multiple-choice responses.

**Prompting Strategies**
*   **Minimal Context:** Specifying visual limitations only.
*   **Combined Context:** Integrating vision info with examples to anchor the model.
*   **Efficiency Testing:** Comparing Single-Shot vs. Few-Shot performance.

---

## Results

**Human Participant Baseline**
*   **Average Duration:** 70.2 minutes
*   **Description Length:** Average of 27.2 words
*   **Multiple-Choice Accuracy:** 60.4%

**GPT-4o Performance**
*   **Minimal Context Agreement:** 0.59
*   **Combined Context Agreement:** 0.70 (Statistically significant improvement, p < 0.0001)
*   **Prompting Efficiency:** Single-shot prompting was effective, with diminishing returns observed on additional examples.

**Critical Insight**
Without explicit anchoring, models default to general inference patterns associated with standard vision.

---

## Contributions

*   **Accessibility Domain Research:** The first study to investigate the capability of VLMs to simulate the visual perception of individuals with disabilities (low vision).
*   **Benchmark Dataset:** A novel dataset containing detailed vision information and corresponding image perception data from low vision users.
*   **Prompt Engineering Guidelines:** Actionable insights into effectively simulating specific user experiences, demonstrating that high-fidelity simulation requires a specific combination of user profile data and few-shot examples.

---
*Analysis based on 40 citations.*