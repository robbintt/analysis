---
title: Agentic Entropy-Balanced Policy Optimization
arxiv_id: '2510.14545'
source_url: https://arxiv.org/abs/2510.14545
generated_at: '2026-02-03T06:46:25'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Agentic Entropy-Balanced Policy Optimization

*Guanting Dong; Licheng Bao; Zhongyuan Wang; Kangzhi Zhao; Xiaoxi Li; Jiajie Jin; Jinghan Yang; Hangyu Mao; Fuzheng Zhang; Kun Gai; Guorui Zhou; Yutao Zhu; Ji-Rong Wen; Zhicheng Dou*

**Quality Score:** 9/10 | **References:** 40 citations

---

### ðŸ“Š Quick Facts

*   **Model Architecture:** Qwen3-14B
*   **Training Efficiency:** 1K RL Samples
*   **GAIA Pass@1:** 47.6%
*   **Humanity's Last Exam Pass@1:** 11.2%
*   **WebWalker Pass@1:** 43.0%
*   **Benchmark Scope:** Outperformed 7 mainstream RL algorithms across 14 datasets.

---

## Executive Summary

Training Large Language Model (LLM)-based web agents using Reinforcement Learning (RL) presents a unique challenge in balancing exploration with stability. While entropy regularization is typically used to encourage exploration, this paper identifies that in long-horizon, multi-turn tool-use scenarios, an excessive reliance on entropy signals leads to "training collapse." Specifically, high-entropy statesâ€”such as repetitive or tool-calling loopsâ€”tend to occur consecutively, causing gradient instability and a concentration of rollout samples on a limited number of trajectories. This issue severely hampers the agent's ability to learn diverse and effective policies for complex web navigation tasks.

To address this, the authors propose **Agentic Entropy-Balanced Policy Optimization (AEPO)**, a novel algorithm that manages entropy dynamically during both data collection and policy updates. The approach consists of two core mechanisms:

1.  **Dynamic Entropy-Balanced Rollout:** Adaptively allocates global and branch sampling budgets via entropy pre-monitoring and imposes a penalty on consecutive high-entropy steps to prevent repetitive loops.
2.  **Entropy-Balanced Policy Optimization:** Modifies the objective function by inserting a stop-gradient operation into the high-entropy clipping term. This preserves and rescales gradients for high-entropy tokens while incorporating entropy-aware advantage estimation to prioritize learning on high-uncertainty tokens.

AEPO demonstrates superior performance and data efficiency, consistently outperforming 7 mainstream RL algorithms across 14 challenging datasets. Remarkably, using only 1K RL samples to train the Qwen3-14B model, AEPO achieved Pass@1 scores of 47.6% on GAIA, 11.2% on Humanity's Last Exam, and 43.0% on WebWalker. In contrast, baseline experiments with the ARPO algorithm showed that 93.4% of rollout branches were concentrated on merely 1-3 trajectories, with 56.5% of high-entropy turns occurring consecutively. AEPO successfully mitigated these collapse issues, significantly improving rollout sampling diversity while maintaining stable policy entropy.

The primary significance of this research lies in validating that sophisticated web agent capabilities can be achieved with extremely limited RL data (1K samples), challenging the necessity for massive datasets in agent training. By providing a mathematically grounded solution to the problem of entropy-induced collapse, AEPO establishes a scalable framework for developing robust, long-horizon autonomous agents.

---

## Key Findings

*   **Superior Performance:** AEPO consistently outperforms 7 mainstream reinforcement learning algorithms across 14 challenging datasets.
*   **High Efficiency with Low Data:** Utilizing only 1K RL samples, the Qwen3-14B model trained with AEPO achieved impressive Pass@1 scores:
    *   **47.6%** on GAIA
    *   **11.2%** on Humanity's Last Exam
    *   **43.0%** on WebWalker
*   **Mitigation of Training Collapse:** The method successfully addresses the issue of training collapse caused by excessive reliance on entropy signals in long-horizon, multi-turn tool-use scenarios.
*   **Balanced Diversity and Stability:** AEPO improves rollout sampling diversity while maintaining stable policy entropy, thereby facilitating scalable web agent training.

---

## Methodology

The paper introduces Agentic Entropy-Balanced Policy Optimization (AEPO), a reinforcement learning algorithm designed to balance entropy signals during both the data collection (rollout) and learning (policy update) phases. The approach consists of two core components:

*   **Dynamic Entropy-Balanced Rollout Mechanism:**
    *   Adaptively allocates global and branch sampling budgets through entropy pre-monitoring.
    *   Imposes a branch penalty on consecutive high-entropy tool-call steps to prevent collapse.

*   **Entropy-Balanced Policy Optimization:**
    *   An update mechanism that inserts a stop-gradient operation into the high-entropy clipping term.
    *   Preserves and rescales gradients on high-entropy tokens.
    *   Incorporates entropy-aware advantage estimation to prioritize learning on high-uncertainty tokens.

---

## Technical Details

**Core Objective**
The paper proposes AEPO, an agentic reinforcement learning algorithm for LLM-based web agents that balances entropy during rollout generation and policy updates. It aims to maximize expected reward while constraining policy divergence from a reference model using KL divergence.

**Architecture Components**
1.  **Dynamic Entropy-Balanced Rollout:**
    *   Addresses high-entropy rollout collapse.
    *   Utilizes entropy monitoring, balanced beaming, and consecutive branch penalties.
2.  **Entropy-Balanced Policy Optimization:**
    *   Addresses gradient clipping on high-entropy tokens.
    *   Employs stop-gradient operations and entropy-aware advantage estimation.

**Evaluation Environment**
*   Web Search Engines
*   Web Browsers
*   Code Executors

---

## Contributions

*   **Problem Identification:** The authors provide a deep analysis of the specific challenges entropy poses in Agentic RL, specifically how excessive reliance on entropy signals can lead to training collapse.
*   **Algorithmic Innovation:** The proposal of AEPO offers a novel solution that manages entropy dynamically during rollouts and mathematically adjusts gradient updates.
*   **Empirical Validation:** The research demonstrates that sophisticated web agent capabilities can be achieved with very few RL samples (1K), validating the efficiency and scalability of the proposed method for complex, long-horizon tasks.

---

## Results

*   **Baseline Failures (ARPO):** Pilot experiments revealed that the baseline ARPO had 56.5% of high-entropy turns occurring consecutively and 93.4% of rollout branches concentrated on only 1-3 trajectories, with significant early gradient clipping.
*   **AEPO Success:** AEPO, using the Qwen3-14B model and only 1K RL samples, achieved:
    *   Pass@1 of **47.6%** on GAIA
    *   Pass@1 of **11.2%** on Humanity's Last Exam
    *   Pass@1 of **43.0%** on WebWalker
*   **Comparison:** The method outperformed 7 mainstream reinforcement learning algorithms across 14 datasets, successfully balancing rollout sampling diversity and maintaining stable policy entropy.